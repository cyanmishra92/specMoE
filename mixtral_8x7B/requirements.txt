# Mixtral 8x7B MoE Requirements
# For A100/A6000 GPU deployment

# Core ML frameworks
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Transformers and model loading
transformers>=4.35.0
accelerate>=0.20.0
bitsandbytes>=0.41.0
peft>=0.5.0

# Dataset processing
datasets>=2.14.0
tokenizers>=0.14.0

# Scientific computing
numpy>=1.24.0
scipy>=1.11.0
pandas>=2.0.0
scikit-learn>=1.3.0

# Visualization
matplotlib>=3.7.0
seaborn>=0.12.0
plotly>=5.15.0

# Progress and logging
tqdm>=4.65.0
tensorboard>=2.13.0
wandb>=0.15.0

# System monitoring
psutil>=5.9.0
GPUtil>=1.4.0
nvidia-ml-py3>=7.352.0

# Development tools
jupyter>=1.0.0
ipykernel>=6.25.0
black>=23.0.0
flake8>=6.0.0

# HuggingFace ecosystem
huggingface-hub>=0.17.0
safetensors>=0.3.0

# Optional: For advanced features
deepspeed>=0.10.0
flash-attn>=2.3.0