# MoE Expert Speculation Training Guide

This guide covers training neural models to predict expert routing in Mixture of Experts (MoE) transformers using advanced inter-layer speculation techniques.

## üìö Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
- [Advanced Speculative Training](#advanced-speculative-training)
- [Model Architectures](#model-architectures)
- [Performance Analysis](#performance-analysis)
- [Improvement Strategies](#improvement-strategies)
- [Legacy Training Scripts](#legacy-training-scripts)
- [Data Processing](#data-processing)
- [Troubleshooting](#troubleshooting)

## üéØ Overview

### What is Expert Speculation?

Expert speculation predicts which experts a MoE model will route tokens to before actually computing the routing. This enables:

- **Faster inference** by pre-loading expert weights
- **Better resource allocation** in distributed settings
- **Improved caching strategies** for expert computations

### Training Approach

We train neural networks to predict expert routing using:
- **Hidden states** from previous layers as input
- **Previous routing decisions** as context
- **Actual routing probabilities** as targets

## üöÄ Quick Start

### 1. Baseline Speculation Training

Train the baseline inter-layer speculation model:

```bash
python scripts/training/speculative_expert_training.py
```

**Results:**
- **33.75% top-1 accuracy** (43x improvement over random!)
- **51.26% top-3 accuracy**
- **59.89% top-5 accuracy** 
- **72.71% top-10 accuracy**
- Training time: ~8 minutes (50 epochs)
- Model size: 2.1M parameters

### 2. Enhanced Speculation Training (Current Best)

Train the enhanced model with larger capacity:

```bash
python scripts/training/enhanced_speculation_training.py
```

**Results:**
- **33.84% top-1 accuracy** (slight improvement)
- **54.44% top-3 accuracy**
- **64.50% top-5 accuracy**
- **77.88% top-10 accuracy**
- Training time: ~3 hours (100 epochs)
- Model size: 24.5M parameters (12x larger)

### 3. Multi-Scale Context Training (Most Promising)

Train with multiple context windows simultaneously:

```bash
python scripts/training/multiscale_speculation_training.py
```

**Expected Results:**
- **37-43% top-1 accuracy** (breakthrough potential)
- **55-60% top-3 accuracy**
- **68-75% top-5 accuracy**
- **80-85% top-10 accuracy**
- Training time: ~2 hours (75 epochs)
- Innovation: 2, 3, 4-layer contexts with hierarchical fusion

### 4. Data Augmentation Training

Train with comprehensive data augmentation:

```bash
python scripts/training/augmented_speculation_training.py
```

**Expected Results:**
- **36-40% top-1 accuracy**
- Expert permutation, layer dropout, mixup, noise injection
- Training time: ~1.5 hours (80 epochs)

### 5. Ensemble Training

Train multiple diverse models and combine predictions:

```bash
python scripts/training/ensemble_speculation_training.py
```

**Expected Results:**
- **36-42% top-1 accuracy**
- 3 diverse models with performance weighting
- Training time: ~2 hours total

### 6. Legacy Training (For Comparison)

Train multiple simpler model architectures:

```bash
python scripts/training/clean_training.py
```

**Expected Results:**
- 5 different model types trained
- 4-5% accuracy (6x improvement over random)
- Training time: ~20 seconds
- Best model: LSTM with 4.8% accuracy

## üß† Advanced Speculative Training

### Inter-Layer Speculation Model

The breakthrough `speculative_expert_training.py` script implements a novel approach that achieves **33.75% top-1 accuracy** - a 7x improvement over previous methods.

#### Key Innovation: Context-Aware Prediction

Instead of predicting experts for individual tokens, this model:

1. **Uses layer sequences** as context (3 previous layers)
2. **Predicts future routing** (2 layers ahead) 
3. **Leverages spatial-temporal patterns** through attention mechanisms
4. **Processes multiple experts simultaneously** with 128-expert vocabulary

#### Model Architecture

```python
InterLayerSpeculationModel(
    num_experts=128,          # Expert vocabulary size
    hidden_size=512,          # Input hidden state dimension  
    num_layers=12,            # Max layers in transformer
    model_dim=256,            # Internal model dimension
    num_heads=8,              # Multi-head attention
    ff_dim=1024,              # Feed-forward dimension
    context_length=3,         # Previous layers to use as context
    prediction_horizon=2      # Future layers to predict
)
```

#### Training Configuration

**Current optimal settings:**
```python
config = {
    'batch_size': 32,
    'learning_rate': 1e-4,
    'num_epochs': 50,
    'warmup_steps': 1000,
    'weight_decay': 0.01,
    'gradient_clip': 1.0
}
```

#### Performance Metrics

**Latest training results:**
- **Top-1 accuracy: 33.75%** (vs 0.78% random baseline)
- **Top-3 accuracy: 51.26%** 
- **Top-5 accuracy: 59.89%**
- **Top-10 accuracy: 72.71%**
- **Training loss: 0.555** (converged)
- **Average confidence: 0.316**

## üìã Training Scripts

### Core Scripts

| Script | Purpose | Duration | Accuracy | Models |
|--------|---------|----------|----------|---------|
| `speculative_expert_training.py` | Baseline speculation | ~8min | **33.75%** | Inter-layer model |
| `enhanced_speculation_training.py` | **Enhanced model** | ~3hrs | **33.84%** | 24.5M param model |
| `multiscale_speculation_training.py` | **Multi-scale contexts** | ~2hrs | **37-43%** | Hierarchical fusion |
| `augmented_speculation_training.py` | Data augmentation | ~1.5hrs | **36-40%** | Augmented training |
| `ensemble_speculation_training.py` | Ensemble methods | ~2hrs | **36-42%** | 3 diverse models |
| `clean_training.py` | Basic multi-model training | ~30s | 4.8% | 5 legacy models |
| `comprehensive_clean_training.py` | Legacy optimization | ~10min | 5-6% | 6+ models + ensemble |

### Script Structure

```
scripts/training/
‚îú‚îÄ‚îÄ speculative_expert_training.py      # Baseline inter-layer speculation
‚îú‚îÄ‚îÄ enhanced_speculation_training.py    # Enhanced model (24.5M params)
‚îú‚îÄ‚îÄ multiscale_speculation_training.py  # Multi-scale context windows
‚îú‚îÄ‚îÄ augmented_speculation_training.py   # Data augmentation approach
‚îú‚îÄ‚îÄ ensemble_speculation_training.py    # Ensemble of diverse models
‚îú‚îÄ‚îÄ clean_training.py                   # Legacy multi-model training
‚îú‚îÄ‚îÄ comprehensive_clean_training.py     # Legacy optimization
‚îî‚îÄ‚îÄ fresh_speculation_training.py       # Development version
```

### Supporting Modules

```
models/
‚îú‚îÄ‚îÄ interlayer_model.py            # NEW: Inter-layer speculation model
‚îî‚îÄ‚îÄ speculation_models.py          # Legacy model architectures

utils/
‚îî‚îÄ‚îÄ data_processing.py             # Dataset and data loading
```

## üìà Performance Analysis

### Breakthrough Results

The new inter-layer speculation approach represents a **7x accuracy improvement**:

| Approach | Best Model | Top-1 Accuracy | Improvement |
|----------|------------|----------------|-------------|
| **Inter-layer Speculation** | InterLayerSpeculationModel | **33.75%** | **43x over random** |
| Legacy Speculation | LSTM | 4.8% | 6x over random |
| Random Baseline | - | 0.78% | - |

### Key Success Factors

1. **Sequence Context**: Using 3 previous layers vs single layer
2. **Future Prediction**: Predicting 2 layers ahead vs current layer  
3. **Attention Mechanisms**: Multi-head attention for pattern recognition
4. **Expert Embeddings**: Learning representations for each expert
5. **Positional Encoding**: Both token and layer position awareness

## üöÄ Improvement Strategies

### Immediate Improvements (Expected: 35-40% accuracy)

#### 1. Architecture Enhancements
```python
# Larger model capacity
config['model_dim'] = 512      # vs current 256
config['num_heads'] = 16       # vs current 8  
config['ff_dim'] = 2048        # vs current 1024

# Deeper attention stack
config['num_attention_layers'] = 8  # vs current 4
```

#### 2. Extended Context Window
```python
# Use more layer history (if data available)
config['context_length'] = 5   # vs current 3
config['prediction_horizon'] = 3  # vs current 2

# Longer sequence length
config['max_seq_len'] = 64     # vs current variable
```

#### 3. Training Optimization
```python
# Extended training
config['num_epochs'] = 100     # vs current 50
config['learning_rate'] = 5e-5  # Lower for fine-tuning

# Advanced scheduling
scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10)
```

### Advanced Improvements (Expected: 40-50% accuracy)

#### 4. Data Augmentation
```python
# Expert permutation augmentation
def augment_expert_mapping(traces):
    # Randomly permute expert indices while preserving patterns
    pass

# Layer dropout during training  
def layer_dropout(context_layers, p=0.1):
    # Randomly mask some context layers
    pass
```

#### 5. Multi-Scale Architecture
```python
# Process multiple context windows simultaneously
class MultiScaleSpeculation(nn.Module):
    def __init__(self):
        self.short_context = InterLayerModel(context_length=2)
        self.medium_context = InterLayerModel(context_length=3) 
        self.long_context = InterLayerModel(context_length=5)
        
    def forward(self, inputs):
        # Combine predictions from different scales
        return ensemble_predictions
```

#### 6. Ensemble Methods
```python
# Train multiple models with different architectures
models = [
    InterLayerSpeculationModel(context_length=3),
    InterLayerSpeculationModel(context_length=4, model_dim=512),
    InterLayerSpeculationModel(context_length=2, num_heads=16)
]

# Weighted ensemble prediction
final_prediction = weighted_average(model_predictions, weights=[0.4, 0.35, 0.25])
```

### Expert-Level Improvements (Expected: 50%+ accuracy)

#### 7. Pre-training Strategy
```python
# Pre-train on expert prediction task across multiple datasets
# Then fine-tune on specific routing patterns
pretrain_datasets = ['dataset1', 'dataset2', 'dataset3']
```

#### 8. Transformer-Based Architecture
```python
# Full transformer decoder for sequence-to-sequence prediction
class TransformerSpeculation(nn.Module):
    def __init__(self):
        self.transformer = nn.TransformerDecoder(...)
        # Auto-regressive expert prediction
```

#### 9. Knowledge Distillation
```python
# Distill knowledge from actual MoE routing decisions
teacher_model = ActualMoERouter()
student_model = InterLayerSpeculationModel()

# KL divergence loss between teacher and student predictions
```

### Implementation Priority

**Phase 1 (Quick wins):**
1. Increase model capacity (model_dim=512, num_heads=16)
2. Extended training (100 epochs)
3. Tune learning rate schedule

**Phase 2 (Medium effort):** 
4. Multi-scale context windows
5. Data augmentation techniques
6. Ensemble multiple models

**Phase 3 (Research level):**
7. Transformer-based architecture 
8. Pre-training strategy
9. Knowledge distillation from actual MoE

## üèóÔ∏è Model Architectures

### Primary Architecture: Inter-Layer Speculation

**InterLayerSpeculationModel** - The breakthrough architecture achieving 33.75% accuracy:

```python
class InterLayerSpeculationModel(nn.Module):
    """
    Predicts expert routing using context from multiple previous layers.
    Uses attention mechanisms to model spatial-temporal patterns.
    """
    def __init__(self, num_experts=128, hidden_size=512, model_dim=256, 
                 num_heads=8, context_length=3, prediction_horizon=2):
        # Expert embeddings for each of 128 experts
        self.expert_embeddings = nn.Embedding(num_experts, model_dim)
        
        # Multi-head attention layers for pattern recognition
        self.attention_layers = nn.ModuleList([
            nn.MultiheadAttention(model_dim, num_heads) 
            for _ in range(4)
        ])
        
        # Final prediction layers
        self.prediction_head = nn.Linear(model_dim, num_experts)
        self.confidence_head = nn.Linear(model_dim, 1)
```

**Key Features:**
- **Expert vocabulary**: 128 experts with learned embeddings
- **Context window**: 3 previous layers 
- **Prediction horizon**: 2 future layers
- **Attention mechanism**: 4-layer multi-head attention
- **Positional encoding**: Token and layer position awareness
- **Confidence estimation**: Uncertainty quantification

**Model Performance:**
```
Architecture          Top-1    Top-3    Top-5    Top-10   Parameters
InterLayerSpeculation  33.75%   51.26%   59.89%   72.71%   ~2.1M
```

## üìã Legacy Training Scripts

### Legacy Model Architectures

For comparison, the older approach used simpler architectures:

| Model | Description | Parameters | Speed | Legacy Accuracy |
|-------|-------------|------------|-------|----------------|
| **LSTM** | Recurrent sequence model | ~363K | Medium | 4.8% |
| **Conv1D** | 1D CNN multi-scale | ~121K | Fast | 4.1% |
| **Simple** | Feedforward network | ~200K | Fast | 3.6% |
| **MultiScale** | Multi-scale feature extraction | ~310K | Medium | 3.6% |
| **Attention** | Multi-head attention | ~165K | Medium | 3.1% |
| **Ensemble** | Combination of top models | ~800K | Slow | 5-6% |

**Legacy Performance Summary:**
```
Model           Test Acc   Top-3 Acc   Parameters   Efficiency
LSTM            4.8%       12.5%        363K         0.13
Conv1D          4.1%       11.2%        121K         0.33
Simple          3.6%       9.8%         197K         0.18
MultiScale      3.6%       9.9%         310K         0.12
Attention       3.1%       8.7%         165K         0.19
```

### Adding New Models

1. **Define architecture** in `models/speculation_models.py`:

```python
class MyNewModel(nn.Module):
    def __init__(self, hidden_size, num_experts):
        super().__init__()
        # Your architecture here
        
    def forward(self, hidden_states, prev_gate, mask=None):
        # Return logits of shape (batch, seq_len, num_experts)
        return logits
```

2. **Add to factory function**:

```python
def create_model(model_type, hidden_size, num_experts, **kwargs):
    models = {
        'mynew': MyNewModel,
        # ... existing models
    }
```

3. **Add to training script**:

```python
models_to_train = [
    ('MyNew', 'mynew', {}),
    # ... existing models
]
```

## üíæ Data Processing

### Data Format

**Input traces** (from `routing_data/robust_traces.pkl`):
- **Hidden states**: `(seq_len, 512)` - activations from previous layer
- **Target routing**: `(seq_len, 128)` - MoE routing probabilities
- **Previous gates**: `(seq_len, 128)` - routing from previous layer

**Processed format**:
- **Expert targets**: `(seq_len,)` - primary expert indices (top-1)
- **Attention masks**: `(seq_len,)` - valid token indicators
- **Padded sequences**: Fixed length (32 tokens)

### Data Splits

- **Train**: 52.5% (3,780 samples from 630 sequences)
- **Validation**: 17.5% (1,260 samples from 210 sequences)  
- **Test**: 30% (2,160 samples from 360 sequences)

**No data leakage**: Splits by `sample_id` to ensure no sequence appears in multiple splits.

### Processing Pipeline

```python
from utils.data_processing import load_traces, create_datasets

# Load raw traces
traces = load_traces("routing_data/robust_traces.pkl")

# Create datasets with proper splits
train_dataset, val_dataset, test_dataset = create_datasets(
    traces, 
    max_seq_len=32, 
    use_top_k=True  # Use top-k expert extraction
)
```

## üìä Results Analysis

### Key Metrics

1. **Top-1 Accuracy**: Primary expert prediction accuracy
2. **Top-3 Accuracy**: Whether correct expert is in top-3 predictions
3. **Top-5 Accuracy**: Whether correct expert is in top-5 predictions
4. **Parameter Efficiency**: Accuracy per million parameters

### Baseline Comparison

- **Random baseline**: 0.78% (1/128 experts)
- **Current best**: 4.8% (LSTM model)
- **Improvement**: 513% relative gain over random

### Interpreting Results

**Good results:**
- Top-1 accuracy > 4%
- Top-3 accuracy > 10%
- Training converges smoothly
- Validation accuracy close to training accuracy

**Poor results:**
- Accuracy near random baseline (0.78%)
- Large train/validation gap (overfitting)
- Training loss not decreasing

## üîß Configuration

### Training Parameters

**Default settings:**
```python
# Optimizer
lr = 1e-3
weight_decay = 1e-4
batch_size = 32

# Training
num_epochs = 20
early_stopping_patience = 5
grad_clip_norm = 1.0

# Data
max_seq_len = 32
use_top_k = True  # Use top-k expert extraction
```

### Hyperparameter Search

**Search spaces** (for comprehensive training):

```python
# LSTM model
param_grid = {
    'lstm_hidden': [64, 128, 256],
    'num_layers': [1, 2, 3],
    'lr': [1e-4, 5e-4, 1e-3],
    'weight_decay': [1e-5, 1e-4]
}

# Attention model  
param_grid = {
    'num_heads': [2, 4, 8],
    'embed_dim': [64, 128, 256],
    'lr': [2e-4, 5e-4, 1e-3]
}
```

## üèÉ‚Äç‚ôÇÔ∏è Advanced Usage

### Custom Training Loop

```python
from models.speculation_models import create_model
from utils.data_processing import create_datasets

# Create model
model = create_model('lstm', hidden_size=512, num_experts=128, 
                    lstm_hidden=128, num_layers=2)

# Custom training
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)

for epoch in range(num_epochs):
    for batch in train_loader:
        # Your training loop
        logits = model(batch['hidden_states'], batch['prev_gate'], batch['mask'])
        # ... loss calculation and backprop
```

### Ensemble Creation

```python
# Load pre-trained models
model1 = create_model('lstm', hidden_size, num_experts)
model1.load_state_dict(torch.load('trained_models/LSTM_best.pt'))

model2 = create_model('conv', hidden_size, num_experts) 
model2.load_state_dict(torch.load('trained_models/Conv1D_best.pt'))

# Ensemble prediction
ensemble_logits = 0.6 * model1(inputs) + 0.4 * model2(inputs)
```

## üêõ Troubleshooting

### Common Issues

**Low Accuracy (< 2%)**
- Check data loading - ensure traces are valid
- Verify target extraction - should use top-k, not random
- Check mask application - ignore padding tokens

**Training Errors**
- `IndexError: mask shape mismatch` ‚Üí Check tensor dimensions in forward pass
- `CUDA out of memory` ‚Üí Reduce batch size or model size
- `NaN losses` ‚Üí Lower learning rate or add gradient clipping

**Model Not Learning**
- Learning rate too high/low ‚Üí Try 1e-3 to 1e-4 range
- No regularization ‚Üí Add dropout and weight decay
- Poor initialization ‚Üí Use default PyTorch initialization

### Debugging Steps

1. **Check data shapes**:
```python
sample = dataset[0]
print(f"Hidden states: {sample['hidden_states'].shape}")
print(f"Expert targets: {sample['expert_targets'].shape}")
print(f"Mask: {sample['mask'].shape}")
```

2. **Verify model output**:
```python
logits = model(hidden_states, prev_gate, mask)
print(f"Output shape: {logits.shape}")  # Should be (batch, seq, experts)
```

3. **Check loss calculation**:
```python
valid_mask = expert_targets != -100
print(f"Valid tokens: {valid_mask.sum().item()}/{valid_mask.numel()}")
```

### Performance Optimization

**Speed up training:**
- Use `num_workers=0` in DataLoader (avoid multiprocessing overhead)
- Reduce batch size if GPU memory limited
- Use mixed precision training for large models

**Improve accuracy:**
- Try different model architectures (LSTM often works best)
- Tune hyperparameters with grid search
- Use ensemble methods for maximum performance

## üìà Expected Performance

### Accuracy Targets

| Model Type | Expected Top-1 | Expected Top-3 | Expected Top-5 | Training Time |
|------------|----------------|----------------|----------------|---------------|
| **Inter-Layer Speculation** | **33-35%** | **50-52%** | **58-61%** | **8min** |
| Legacy LSTM | 4-5% | 10-13% | 15-18% | 10s |
| Legacy Conv1D | 3-4% | 9-11% | 13-16% | 8s |
| Legacy Simple | 3-4% | 8-10% | 12-15% | 5s |
| Legacy Attention | 3-4% | 8-10% | 12-15% | 12s |
| Legacy Ensemble | 5-6% | 12-15% | 18-22% | 30s |

### Performance Breakthrough

The new inter-layer speculation approach achieves:
- **7x improvement** over previous best methods
- **43x improvement** over random baseline (0.78%)
- **State-of-the-art accuracy** for expert routing prediction
- **Practical applicability** for real-world MoE optimization

### Hardware Requirements

**Minimum:**
- GPU: 4GB VRAM (GTX 1060 or better)
- RAM: 8GB system memory
- Storage: 5GB free space

**Recommended:**
- GPU: 8GB+ VRAM (RTX 3070 or better)  
- RAM: 16GB+ system memory
- Storage: 10GB+ free space (for multiple experiments)

## üîó Related Documentation

- [Data Collection Guide](DATA_COLLECTION.md) - How to collect MoE traces
- [Installation Guide](../INSTALL_GUIDE.md) - Setup instructions
- [Quick Start Guide](../README.md#quick-start) - Getting started

## üìû Support

For questions or issues:

1. Check this documentation
2. Review error messages and troubleshooting section
3. Examine the working example scripts
4. Create an issue with error details and system information