=== ORIGINAL PDF: 2504.00520v1_SCRec_A_Scalable_Computational_Storage_System_with.pdf ===\n\nRaw text length: 77874 characters\nCleaned text length: 77055 characters\nNumber of segments: 47\n\n=== CLEANED TEXT ===\n\nJOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 1 SCRec: A Scalable Computational Storage System with Statistical Sharding and Tensor-train Decomposition for Recommendation Models Jinho Yang, Graduate Student Member, IEEE, Ji-Hoon Kim, Graduate Student Member, IEEE, Joo-Young Kim, Senior Member, IEEE, Abstract Deep Learning Recommendation Models (DLRMs) play a crucial role in delivering personalized content across web applications such as social networking and video streaming. However, with improvements in performance, the parameter size of DLRMs has grown to terabyte (TB) scales, accompanied by memory bandwidth demands exceeding TB s levels. Furthermore, the workload intensity within the model varies based on the target mechanism, making it difficult to build an optimized recommendation system. In this paper, we propose SCRec, a scalable computational storage recommendation system that can handle TB-scale industrial DLRMs while guaranteeing high bandwidth requirements. SCRec utilizes a software framework that features a mixed-integer programming (MIP)-based cost model, efficiently fetching data based on data access patterns and adaptively configuring memory-centric and compute-centric cores. Additionally, SCRec integrates hardware acceleration cores to enhance DLRM computations, particularly allowing for the high-performance reconstruction of approximated embedding vectors from extremely compressed tensor-train (TT) format. By combining its software framework and hardware accelera- tors, while eliminating data communication overhead by being implemented on a single server, SCRec achieves substantial improvements in DLRM inference performance. It delivers up to 55.77 speedup compared to a CPU-DRAM system with no loss in accuracy and up to 13.35 energy efficiency gains over a multi-GPU system. Index Terms deep learning recommendation model, near-data processing, statistical sharding, tensor-train decomposition. I. INTRODUCTION R RECOMMENDATION systems are widely used in so- cial network services and video streaming platforms to provide personalized and preferred content to consumers as described in Fig.1. They are also employed in search engines to offer differentiated search services [1] [5]. Supporting rec- ommendation services for such a large number of consumers demands significant computation resources. For example, more than 80 of Meta s data center resources are allocated to recommendation system inference, while over 50 are utilized for training these systems [6]. Traditional recommendation systems relied on collaborative filtering techniques, such as content filtering using matrix factorization [7] [10]. However, with advancements in deep neural networks (DNNs), deep learning recommendation mod- els (DLRMs) that combine embedding tables (EMBs) and This work was supported by Samsung Electronics Co., Ltd.. Manuscript received MM dd, yyyy; revised MM dd, yyyy. Application Music Music Movie Movie Game Game Sport Sport Sport Shop Shop Food Food Music Movie Game Sport Shop Food Recommendation System Feedback Rec. List User Like Dislike Watch Buy Fig. 1. Overview of recommendation system. multi-layer perceptron (MLP) layers have become a dom- inant approach in recommendation systems. These models are widely adopted in data centers, with recent focuses on both software-level and hardware-level optimizations [11] [17]. DLRMs process categorical data like website visit histories through EMBs, and numerical data such as user age through MLP layers. This combination has demonstrated superior recommendation performance, making DLRM the industry standard in recommendation systems. However, their model size and memory bandwidth requirements have grown rapidly with the recommendation performance. Specifically, the number of EMBs, which accounts for the majority of a DLRM s size, has increased to multiple terabytes (TBs) of parameters in memory devices, with bandwidth requirement rising to TB s level [18] [20]. In addition to the inherent characteristics of the DLRM algorithm, the workload intensity within the model varies depending on how it is applied in the recommendation system, making it difficult to build the optimized system [21]. There are two primary mechanisms in recommendation systems: content filtering and content ranking. The former is the pro- cess of selecting a list of recommended content for users where embedding layer processing dominates, resulting in a memory-centric workload. In contrast, the latter determines the order in which the recommended content is displayed to users where MLP layer processing dominates, leading to a compute-centric workload. As a result, it is crucial to configure optimized recommendation systems with focusing on the targeting mechanism of the DLRM, to achieve high utilization of the system resources. Since traditional CPU- DRAM systems have struggled to meet the demands of modern DLRMs, it highlights the need for systems that can adaptively handle extensive memory and computational requirements. Research has been conducted to address the memory and computational challenges of DLRM using solid-state drives (SSDs). Recent studies [22] [25] have explored in-storage arXiv:2504.00520v1 [cs.AR] 1 Apr 2025 JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 2 processing (ISP) and near-memory processing (NMP) to per- form embedding layer computations within standalone SSDs. This approach facilitates the storage of TBs of EMBs and accelerates embedding layer operations by utilizing the inter- nal memory bandwidth of SSDs, providing significantly faster performance compared to conventional SSD usage. However, the internal bandwidth of SSDs remains substantially lower than that of volatile memory, such as high-bandwidth memory (HBM), rendering them insufficient to meet the growing memory bandwidth demands of DLRMs. Furthermore, the high computational complexity of the MLP layer poses sig- nificant challenges for SSD-based ISP, making it impractical to implement a complete DLRM system. Another approach to addressing both memory capacity and bandwidth is the use of multi-GPU systems. Recent works [26] [29] have implemented data parallelism and model parallelism to distribute the MLP and EMBs of the DLRM across multiple GPUs. Although this approach achieves highly performant systems, it also comes with a few challenging issues. The first issue is the large number of GPUs required to run a single DLRM. For instance, constructing a DLRM system with TBs of parameters would require dozens of NVIDIA H100 GPUs, each equipped with 80 GB of HBM, resulting in enormous system costs. The second issue is the underutilization of GPU compute resources for embedding- dominant DLRM applications, leading to inefficiency in the entire system. Lastly, multiple GPU nodes must be intercon- nected to run the entire DLRM system at TB scale in parallel, leading to increased network complexity and overhead. The overall DLRM performance degradation is inevitable due to complex network configuration among GPU nodes, with data communication overhead among them. To be precise, all-to- all communication is required during model inference, where each GPU requests embedding vectors from different GPUs, causing significant system bottlenecks. To mitigate these challenges, recent work [19] has statisti- cally analyzed DLRM characteristics and proposed a sharding technique that stores frequently accessed embedding vectors in the GPU s HBM, while placing rarely accessed embed- ding vectors in the host s DRAM. This approach effectively maintains a comparable system memory bandwidth while reducing the required number of GPUs to run the DLRM. However, challenges such as high system cost, computing resource underutilization, and complexity of network topology still persist in the GPU-based DLRM systems. To address these challenges, we propose SCRec, a scalable computational storage system that can handle TB-scale indus- trial DLRMs while providing high bandwidth requirements. SCRec leverages SmartSSD devices, which integrate an SSD with a field-programmable gate array (FPGA) for near-data processing (NDP), effectively utilizing the SSD s large capac- ity alongside the high bandwidth of DRAM and block RAM (BRAM) on the FPGA. The detailed contributions of our work are as follows. SCRec utilizes a software framework with two pri- mary features statistical sharding and adaptive accel- eration core mapping leveraging a mixed-integer pro- gramming (MIP)-based cost model. By analyzing data EMB_1 EMB_N Embedding Layer Input Data Prediction Feature Interaction Layer Bottom MLP Emb. Vector Emb. Vector Emb. Vector Pooling Pooling Pooling Sparse Features View History ... Dense Features Region Age ... Top MLP Emb. Vector Pooling EMB_3 EMB_2 Item Rate Fig. 2. Architecture of deep learning recommendation model. access patterns of DLRM input features, SCRec optimizes SmartSSD memory bandwidth by placing hot data in high-bandwidth memory devices such as BRAM and DRAM, and cold data in SSD. Additionally, it deter- mines the optimal configuration of memory-centric and compute-centric acceleration cores based on the work- load intensity analysis for each layer, ensuring a high- utilization DLRM system. We designed custom acceleration cores capable of boost- ing up EMB and MLP computations in DLRM and implemented them onto the FPGA chip of the SmartSSD device. Especially, the EMB core utilizes the tensor-train (TT) format to compress the GB-sized EMBs signifi- cantly into the megabyte (MB)-level, enabling the high- performance reconstruction of approximated embedding vectors. This also complements the device s DRAM ca- pacity limitations while enhancing the overall system s memory bandwidth. We integrated the software framework and hardware accelerators to develop a multi-SmartSSD system capable of running the entire DLRM operations. The system is configured within a single server, eliminating the need for complex network setup and eliminating data com- munication overhead. Additionally, within the server, it minimizes host communication overhead through peer-to- peer (P2P) data transfer between the SSD and FPGA chip. Therefore, SCRec can significantly reduce both inter- node and intra-node communication overhead. Our evaluation demonstrates that SCRec achieves signifi- cant improvements in inference performance, achieving up to 55.77 speed-up compared to a CPU-DRAM system with no loss in accuracy and up to 13.35 improvement in energy efficiency over a multi-GPU system. The remainder of this paper is organized as follows: Section II provides background information on the DLRM and TT- format; Section III describes SCRec, including its software framework and hardware architecture; Section IV presents the evaluation results of SCRec; Section V discusses related works on memory caching, sharding, and TT decomposition; and Section VI concludes the paper. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 3 Den 1 Den 2 ... Den N Den 1 Den 2 ... Den N Dense Features bs bs N Spar 1 Spar 2 ... Spar M Spar 1 Spar 2 ... Spar M Sparse Features bs Input Data MLP Param. Bot MLP Layer 0 Bot MLP Layer 0 EMB Param. Embedding Table 0 Embedding Table 0 MLP Param. Top MLP Layer 0 Top MLP Layer 0 All-to-all Feature Interact. Device 1 Device 1 Device 1 Feat 1 Top MLP Feature Interact. Device 1 Feat 1 Top MLP Feature Interact. Device 2 Device 2 Device 2 Feat 2 Top MLP Feature Interact. Device 2 Feat 2 Top MLP Feature Interact. Device N Device N Device N Feat N Top MLP Feature Interact. Device N Feat N Top MLP Predict 1 Data Parallelism Data Parallelism Data Parallelism Model Parallelism Model Parallelism Model Parallelism Den 1 Bot MLP Device 1 Device 1 Device 1 Spar 4 EMB 4 Spar 4 EMB 4 Den 1 Bot MLP Device 1 Spar 4 EMB 4 Den 2 Bot MLP Device 2 Device 2 Device 2 Spar 8 EMB 8 Spar 8 EMB 8 Den 2 Bot MLP Device 2 Spar 8 EMB 8 Den N Bot MLP Device N Device N Device N Spar M EMB M Spar M EMB M Den N Bot MLP Device N Spar M EMB M Data Parallelism Data Parallelism Data Parallelism Predict 2 Predict N Fig. 3. Hybrid-parallel DLRM processing. II. BACKGROUND A. DLRM Architecture and Hybird-parallel Processing DLRM is a DNN-based recommendation model widely utilized across various industries, as depicted in Fig.2. It processes user and content data as input to predict user interactions with specific content, aiming to estimate the click-through rate (CTR). The model primarily consists of an embedding layer with multiple EMBs, MLP layers, and a feature interaction layer. In the DLRM, dense features and sparse features serve as inputs to the MLP layer and the embedding layer, respectively. Dense features, also referred to as continuous features, represent numerical data such as a user s age or geographic location. Sparse features, also known as categorical features, include data like website visit history or content category IDs. Sparse features are specifically represented as one-hot or multi-hot binary vectors, which are used to retrieve latent vectors from the EMBs. These latent vectors are then aggregated through a pooling operation to produce a single vector. The outputs of the bottom MLP layer and the embedding layer are integrated through concatenation in the feature interaction layer, followed by computation in the top MLP layer to produce the final prediction. A key strength of DLRM is its ability to use multiple EMBs to map discrete, categorical features into their corresponding vector representations, allowing it to effectively capture semantically meaningful representations of the target features [30]. The need for parallel processing across multiple devices in the DLRM arises from the model s inherent characteristics. First, EMBs, which account for the majority of DLRM pa- rameters, can number hundreds, with the total size of the em- bedding layer exceeding several TBs. Consequently, memory devices utilizing DRAM and HBM must split and store the EMBs across multiple devices. Second, as the MLP layer is compute-bound and the embedding layer is memory-bound, optimizing the system with respect to workload intensity is essential to prevent hardware underutilization. To address these challenges, hybrid-parallel processing is applied to optimize DLRM execution across multiple devices, as depicted in Fig.3. Data parallelism is used for the MLP layer, whereas model parallelism is applied to the embedding layer. Specifically, dense features are divided among devices using a batch-wise split, and EMBs are configured with a table-wise split. In this setup, an all-to-all operation ensures that each device holds all Decompose Decompose Decompose EMB Table Reshape Reshape Reshape Reshaped Table TT Core 0 TT Core 1 TT Core 2 Row [ I1 I2 I3 ] Row [ I1 I2 I3 ] Dim [ J1 J2 J3 ] Dim [ J1 J2 J3 ] TT dim of TT cores 3 TT ranks [R] 4 TT dim of TT cores 3 TT ranks [R] 4 I3 J3 I3 J3 I1 J1 I1 J1 I1 J1 I1 J1 R1 R1 R1 R1 I2 J2 I2 J2 I3 J3 I3 J3 R3 Fig. 4. Process of representing an embedding table in TT-format. pooled latent vectors, as the feature interaction layer requires pooling results from all EMBs. Subsequently, the feature interaction layer and top MLP layer are processed using data parallelism, finally producing the CTR results. Hybrid-parallel processing in DLRM is a powerful approach that reduces the computational intensity of MLP operations while overcoming memory capacity limitations and increasing effective memory bandwidth. B. Tensor-train Decomposition TT decomposition [31] is a low-rank approximation method that decomposes high-dimensional tensors into lower- dimensional representations, enabling them to be approxi- mated with significantly less data. The decomposition process involves the following steps: A d-dimensional tensor T RI1 I2 Id, where Ik is the size of dimension k, can be represented as a sequential product of tensors G with lower dimension d, using the following equation 1. T (i1, i2, . . . , id) G1(:, i1, :)G2(:, i2, :) . . . Gd(:, id, :) (1) The decomposed tensor Gk RRk 1 Ik Rk is referred to as a TT-core, has a TT-rank Rk, where the first and last TT- cores have TT-rank of 1 to ensure that the sequential product of TT-cores results in a scalar, meaning R0 Rk 1. A matrix-form EMB can be represented using TT-format, as illustrated in Fig.4. If the EMB is represented as E RI J , where I Qd k 1 ik refers to the row length and J Qd k 1 jk refers to embedding dimension. the EMB can be reshaped into a d-dimensional tensor T R(i1j1) (i2j2) (idjd). This re- shaped tensor can then be decomposed into multiple TT-cores. Consequently, every element of the EMB can be approximated as a value obtained through the sequential product of TT-cores, which can be computed using Equation 2: E(i, j) T (m1, m2, . . . , md) G1(:, m1, :)G2(:, m2, :) . . . Gd(:, md, :) (2) where the EMB row is defined as i Pd k 1 ik Qd l k 1 Il, the EMB dimension value by j Pd k 1 jk Qd l k 1 Jl, and the reshaped tensor index by md Id id jd, with the ranges 0 id Id 1 and 0 jd Jd 1 [32]. The use of TT decomposition to compress DNN model parameters has significantly reduced the need for large mem- ory capacity. This method has proven particularly effective, achieving exceptionally high compression ratios up to thou- sands of times for DLRMs, especially for EMBs, which range from hundreds of GBs to severals TBs. Additionally, JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 4 MLP TT-core matmul. operation acceleration (enhancing DLRM throughput ) Runtime Device Host Remapping layer Host Remapping layer SmartSSD Cluster EMB SmartSSD N DRAM NAND flash (4TB) SSD ctlr. PCIe switch FPGA chip BRAM EMB core EMB S.SSD 1 EMB param. MLP S.SSD M MLP param. MLP S.SSD 1 MLP param. Random Sampling Dense feat. Sparse feat. Dataset Model Config. of EMB table EMB dimension of EMB rows Software Framework of MLP layers MLP dimension DLRM Top MLP layer Bot MLP layer EMB layer Adaptive hardware resource mapping (optimizing system based on DLRM workload intensity ) Statistics CDF PF TT CR Latency Statistics CDF PF TT CR Latency Scalable Resource Manager EMB S.SSD 1 EMB 4 Row [DRAM, TT, SSD] EMB 4 Row [DRAM, TT, SSD] EMB S.SSD N EMB J Row [DRAM, TT, SSD] EMB J Row [DRAM, TT, SSD] 1, ..., M 1, ..., N EMB S.SSD idx. MLP S.SSD idx. 1, ..., M 1, ..., N EMB S.SSD idx. MLP S.SSD idx. SmartSSD Spec. of devices Memory size Memory BW. SmartSSD Spec. of devices Memory size Memory BW. Core Performance Simulator EMB Core TT matmul. MLP Core Top MLP Bot. MLP Top MLP Bot. MLP Address Remapper P2P communication (reducing host communication overhead ) Inst. Compiler Param. Mapper Core Mapper EMB core MLP core MLP core Data Statistic Analyzer Fig. 5. Overview of SCRec. fetching parameters in TT-format to on-chip memory reduces the off-chip memory footprint, thereby enhancing the model throughput. III. SCREC A. Overview of SCRec SCRec integrates a software framework that configures an optimized DLRM system based on DLRM s workload intensity, along with a custom hardware accelerator to enhance model performance. SCRec consists of three main pipelines in the offline phase: the Data Statistic Analyzer (DSA), the scal- able Resource Manager (SRM), and the Address Remapper. Detailed descriptions of each stage are provided as follows: Data Statistic Analyzer DSA identifies the statistical characteristics of a dataset and uses them as parameters for the cost model in the subsequent SRM task, providing the necessary information for analyzing workload. Specifically, the DSA utilizes a randomly subsampled input dataset, the configuration of the DLRM, and the hardware specifications of the SmartSSD to derive key metrics, as explained in Section III-B. SCRec simulates hardware acceleration logic mapped to the SmartSSD using a cycle-accurate performance simulator. This simulation measures the latency required to retrieve embedding vectors in an EMB core and the latency of MLP operations in an MLP core while considering the off-chip memory footprint. These metrics are also used as parameters to build the cost model in the SRM. Scalable Resource Manager Based on the statistics ob- tained from the DSA, the SRM applies a MIP-based cost model to enable adaptive hardware core allocation and model parameter mapping on the SmartSSDs, based on the workload intensity of DLRM. A key feature of the SRM is its three- level sharding strategy, which stores hot embedding vectors with high access frequency in FPGA DRAM and BRAM, while cold embedding vectors with low access frequency are stored in the SSD. This approach leverages the large memory capacity of SSDs and the high memory bandwidth of DRAM. Notably, by storing EMBs in TT-format within FPGA BRAM, even large EMBs can be accommodated in on- chip memory, effectively overcoming the capacity limitations of FPGA DRAM, as discussed in Section III-C. Additionally, the SRM enables scalable, hardware-based latency optimizing by adaptively allocating MLP and EMB cores based on the workload intensity of the DLRM. Using the results generated by the SRM, heterogeneous computation cores are mapped to the SmartSSD, and DLRM parameters are fetched. Simul- taneously, the instruction compiler generates instructions for multi-device execution. Addresss Remapper In the address remapper, a remapping layer is implemented to translate logical memory addresses, enabling access to sharded EMBs stored in corresponding devices. During DLRM execution, this remapping layer is loaded into host DRAM at runtime, facilitating memory ac- cess when the host requests embedding layer access from SmartSSDs. The implementation details of the remapping layer are discussed in Section III-D. Hardware Accelerator On the hardware side, the MLP core and EMB core are designed to accelerate computations in the MLP layer and embedding layer, respectively. In par- ticular, the primary function of the EMB core is to generate approximated embedding vectors represented in the TT-format while storing them on-chip, as detailed in Section III-E. Addi- tionally, P2P communication in SmartSSD is implemented via a PCIe switch to directly connect the SSD and FPGA chip, minimizing host communication overhead through NDP. B. Data Statistic Analyzer The goal of DSA is to derive the parameters required for the cost model used in the SRM. This is achieved using sub- sampled data, the model configuration, and a core performance simulator. The DSA computes essential parameters for con- structing the cost model, including the cumulative distribution function (CDF), pooling factor (PF), TT compression ratio (CR), and layer operation latency. In SCRec, statistics for the Meta Embedding Lookup Synthetic (MELS) dataset [33] from 2021 and 2022, which simulates the embedding layer of Meta s industrial DLRM, were analyzed. The results of this data profiling are presented in Fig.6(a) and Fig.6(b), respectively. Detailed explanations of each statistical metric are provided below: JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 5 CDF [ ] Pooling Factor TT Compression Ratio Cumulative Rows [ ] (a) (b) Cumulative Rows [ ] EMB Index CDF [ ] Pooling Factor TT Compression Ratio EMB Index EMB Index EMB Index CDF [ ] Pooling Factor TT Compression Ratio Cumulative Rows [ ] (a) (b) Cumulative Rows [ ] EMB Index CDF [ ] Pooling Factor TT Compression Ratio EMB Index EMB Index EMB Index Fig. 6. Characteristic of sparse featrues in Meta embedding lookup synthetic dataset. (a) Year 2021 (b) Year 2022 Cumulative Distribution Function CDF is used to describe how frequently different rows in EMBs are accessed within the data sample, as shown in the left graph of Fig.6. Specifically, we analyzed the 856 EMBs from the 2021 dataset and the 788 EMBs from the 2022 dataset, plotting graphs of the access statistics. These graphs reveal that the EMB access pattern follows a flipped power-law distribution, where a small portion of the EMB accounts for the majority of total accesses. This observation suggests that employing a multi- level memory hierarchy with hot data placed in smaller, high-bandwidth memory and cold data stored in larger, low- bandwidth memory can significantly improve EMB access performance. Pooling Factor PF refers to the number of embedding vectors accessed for a particular sparse feature in a given data sample. PF directly impacts memory bandwidth demand, as a higher PF requires accessing more EMB rows, thereby increasing the memory load. The middle graphs in Fig.6 illustrate the average PF, showing a wide range from 0 to 118 in the 2021 dataset and from 0 to 175 in the 2022 dataset. Due to the significant variance in PF across EMBs, PF becomes a critical factor in effectively partitioning EMBs across devices. TT Compression Ratio TT CR refers to the ratio that quantifies how much the data can be compressed when an EMB is represented in TT-format. It is expressed as CR EMB size TT core size. Calculating the CR in the DSA is crucial because TT-cores must fit within the limited capacity of BRAM, which is only a few MBs. Additionally, the CR depends on various factors, including the rank of the TT-core, the dimensions of the EMBs, and the percentage of rows in the EMB selected for compression. For example, the graph on the right side of Fig.6 shows the CR when the entire EMB is compressed with an embedding dimension of 64 and a TT-rank of 4. In some EMBs, the TT-represented EMB surpasses the original size, while in others, the CR reaches values in the thousands. This variation arises because larger EMBs generally achieve higher CR. Given this characteristic, it is crucial to construct a cost model that incorporates the TABLE I DESCRIPTIONS OF SRM COST MODEL PARAMETERS Parameter Description M Number of SmartSSDs J Number of EMBs BS Batch Size BS mini Mini Batch Size of MLP Core Capbram FPGA BRAM Capacity per SmartSSD Capdram FPGA DRAM Capacity per SmartSSD Capssd SSD Capacity per SmartSSD ttt TT-core Matrix multiplication Latency tdram DRAM Read Latency tssd SSD Read Latency tmlp top Top MLP Layer Processing Latency tmlp bot Bottom MLP Layer Processing Latency ICDFj Inverse CDF of EMB j stepj Step Size for Piecewise Linear Interpolation of EMB j tt cmj TT-cores Size of EMB j pfj Average Pooling Factor of EMB j row lenj Row Length of EMB j dimj Embedding Dimension of EMB j embj Overall Data Size of EMB j dfj Data Format Byte Size of EMB j hot thrj Hot Data Threshold Percentage of EMB j TT CR of each EMB and its resulting compressed size. This ensures that TT-format EMBs can be effectively selected and compressed to fit within the FPGA s on-chip memory in the SmartSSDs using the cost model, thereby optimizing memory usage and access. Layer Operation Latency The latency of generating ap- proximated embedding vectors from TT-cores, along with the processing latency of the top and bottom MLP layers, is obtained through the core performance simulator and used as constraints in the cost model. Decoupling the cost model from the latency measurement simulator reduces the computational overhead of the cost model, while the cycle-accurate design of the simulator ensures precise latency estimations. Notably, when calculating the latency of MLP layers, the off-chip memory footprint is also considered, resulting in even more accurate latency measurements. C. Scalable Resource Manager SRM aims to achieve configuring optimized DLRM system through a MIP-based cost model, leveraging the characteristics of the DLRM profiled by the DSA. The core concept of SRM is to minimize EMB access latency by utilizing a three- level memory hierarchy and the TT-format. Specifically, EMBs requiring high bandwidth are fetched into the DRAM and BRAM of the FPGA, while those with lower bandwidth requirements are stored in the SSD. This strategy enables the embedding layer parameters to take advantage of the large capacity of SSDs while benefiting from the high bandwidth of DRAM and BRAM. Moreover, by storing EMBs approxi- mated in the highly compressed TT-format within the BRAM, it becomes possible to fit GB-sized EMBs into the MB- sized on-chip memory, effectively increasing the embedding layer s effective bandwidth. We refer to this technique as three-level sharding, and the following explanation details the implementation of the cost model used to apply this sharding scheme. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 6 Objective Function To optimize the latency of DLRM using multiple SmartSSD devices, we define an objective function to minimize the cost, as described in Equation 3, with the parameters outlined in Table I. minimize C subject to cfnt cmlp top C (3) cfnt refers to the cost of the layer preceding the feature interaction layer, while cmlp top represents the cost of the top MLP layer. Since the bottom MLP layer and the embedding layer, which together determine cfnt, are processed in parallel, the cost function is designed to minimize the sum of the maximum cost between these two independently processed layers and the cost of the top MLP layer. Device Allocation The constraints for assigning cores ca- pable of computing the MLP and embedding layers are as follows: 1 X m dm M 1 m M (4) dm {0, 1} m M (5) dm represents the type of hardware core to be mapped onto the FPGA chip of the SmartSSD. Specifically, when dm 0, the constraint ensures that an MLP core is mapped, whereas when dm 1, it enforces the mapping of an EMB core. Equation 4 controls the allocation of heterogeneous cores, preventing all SmartSSDs from being exclusively assigned to either EMB cores or MLP cores. This allocation ensures that all layers of the DLRM can be effectively accelerated within the multi-SmartSSD system. Embedding Table Allocation To enable model paral- lelism by partitioning the EMB parameters across SmartSSDs mapped to EMB cores, the following constraints are defined: X m pmj 1 j J (6) X j pmj dm X j pmj m M (7) pmj {0, 1} m M (8) pmjis a binary variable that indicates whether EMB j is assigned to SmartSSD m. In Equation 6, a constraint is imposed to ensure that each specific EMB is assigned to its corresponding SmartSSD. Equation 7 ensures that EMBs are only assigned to SmartSSDs that have been allocated EMB cores, serving as a constraint to prevent the incorrect assignment of EMBs to SmartSSDs with MLP cores. By utilizing these constraints, SCRec achieves model parallelism in the embedding layer through table-wise splitting of EMBs in the SRM. Three-level Sharding The EMBs assigned to SmartSSDs are split row-wise based on their access frequency patterns and are fetched into the FPGA s DRAM, BRAM, and SSD memory devices. The constraints are structured as follows to minimize EMB access latency: X i x dramji ICDFj(i) row lenj dimj dfj mem dramj j J (9) X i x dramji i stepj pct dramj j J (10) X i x dramji 1 j J (11) x dramji {0, 1} i 0, . . . , stepj j J (12) The above four equations constrain which rows of EMB j are fetched into the FPGA DRAM and determine the size of those rows. The variable x dram serves as a binary variable to compute the pct dram variable, which determines , which specifies the percentage of rows from EMB j are stored in DRAM, as described in Equation 10. At this point, the step size for EMB j is set to min(row lenj, 100). For the ICDF, the CDF obtained from the DSA is inverted and the interpolation interval is determined by stepj using piecewise linear interpolation, reducing the computational complexity of the cost model. Equation 9, formulated using the ICDF, calculates the size of EMB j to be fetched into DRAM, with mem dramj serving as a constraint on the FPGA DRAM capacity of the SmartSSD. X i x ptr ttji ICDFj(i) row lenj dimj dfj mem dramj mem ttj j J (13) X i x dramji i stepj X i x ttji i stepj X i x ptr ttji i stepj j J (14) X i x ttji i stepj pct ttj j J (15) X i x ttji X i x ptr ttji 1 j J (16) x ttji {0, 1} i 0, . . . , stepj j J (17) x ptr ttji {0, 1} i 0, . . . , stepj j J (18) The above six equations define constraints for determin- ing which rows of EMB j are fetched into BRAM in the compressed TT-format and the size of those rows before compression. The variable x tt is a binary variable used to calculate the percentage of rows from EMB j stored in BRAM, as described in Equation 15, while x ptr tt is a binary variable used to calculate the size of the EMB rows allocated to BRAM. Unlike Equations 9 12, the EMB rows stored in TT-format are positioned between the rows stored in DRAM and SSD. Thus, the starting and ending row indices of the EMB stored in TT-format can be derived using the x dram and x ptr tt binary variables via Equations 14 and 15. Additionally, the size of EMB j before compression into TT-format, denoted as mem ttj, is calculated using the ICDF in Equation 13, and this variable serves as a constraint on the BRAM capacity of the SmartSSD. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 7 X i x ptr ttji ICDFj X i x dramji ICDFj X i x row ttji i stepj j J (19) X i x row ttji 1 j J (20) x row ttji {0, 1} i 0, . . . , stepj j J (21) The above three equations define constraints for calculating the x row tt binary variable, which is necessary to determine the size of EMB j when represented across multiple TT-cores. Using x row tt, the cost model calculate the size of the EMBs compressed into TT-format, enabling the formulation of BRAM capacity constraints. pct dramj pct ttj hot thrj j J (22) Equation 22 imposes constraints to prevent excessive TT- format compression, ensuring that less frequently accessed embedding vectors are stored in the SSD. The hot thrj is assigned a value for each EMB. A value of 1 is used when the EMB row size is relatively small, whereas a value less than 1 is applied for significantly larger row sizes, ensuring that EMB parameters from larger tables are fetched from the SSD. Memory Capacity The constraints in the EMB allocation process determine how each EMB j is split at the row level and the extent of memory capacity it occupies when assigned to a SmartSSD. Additionally, a condition must be imposed to ensure that the size of the EMBs fetched to the device does not exceed the device s memory capacity. The constraints are defined as follows: row lenj dimj dfj embj j J (23) X j pmj mem dramj CapDRAM m M (24) X j pmj (embj mem dramj mem ttj) CapSSD m M (25) X j x row ttij tt cmj(i) dfj tt capj j J (26) X i pmj tt capj CapBRAM m M (27) Equations 23 25 limit the capacity of EMBs allocated to the DRAM and SSD of SmartSSD m, while Equations 26 and 27 restrict the capacity of EMBs compressed in TT-format. In these constraints, the tt cmj used in Equation 26 refers to the compressed size, which depends on the percentage of the total EMB rows compressed in TT-format. Similar to the ICDF, this value is obtained through piecewise linear interpolation during DSA stage. Embedding Vector Access Latency To optimize processing overhead, constraints can be formulated to link the embedding vector access latency of each memory device to the batch size. This is achieved utilizing the PF obtained from the DSA and the variables derived from the three-level sharding process. The resulting equations are as follows: (pfj BS) (pct dramj tdram) c dramj j J (28) (pfj BS) (pct ttj ttt) c ttj j J (29) (pfj BS) ((1 pct dramj pct ttj) tssd) c ssdj j J (30) X j pmj c dramj c dramm m M (31) X j pmj c ttj c ttm m M (32) X j pmj c ssdj c ssdm m M (33) Equations 28 30 calculate latency cost of EMB j allocated to DRAM, BRAM, and SSD. Additionally, Equations 31 33 define the constraints for determining the latency of each memory tier in the SmartSSD, using the pmj derived earlier in the EMB allocation process. By applying these constraints, a precise cost model for the embedding layer can be constructed, taking the specific characteristics of DLRM into account. MLP Layer Latency In addition to the embedding layer s cost constraints, the latency cost of the top MLP layer and the bottom MLP layer can also be formulated with the following constraints: tmlp top (BS BS mini) cmlp top X m (1 dm) m M (34) tmlp bot (BS BS mini) cmlp bot X m (1 dm) m M (35) In the Equations 34 and 35, tmlp top and tmlp bot are values obtained from the cycle-accurate MLP core simulator, and BS mini refers to the batch size that can be processed by the MLP core in a single tile operation. The value of BS mini is configurable, as the MLP core is equipped with multiple computing cores. The details of the hardware architecture will be discussed in Section III-E. Cost Definition To define the objective function for opti- mizing the DLRM, constraints for the previously mentioned variables were formulated. Using these constraints, the equa- tions for the corresponding layers of the DLRM were estab- lished as follows: {c dramm, c ttm, c ssdm} cemb m M (36) {cmlp bot, cemb} cfnt (37) The constraint in Equation 36 defines the cost of the embedding layer, cemb, while the constraint in Equation 37 defines cfnt which represents the maximum cost between the bottom MLP layer and the embedding layer. Using cfnt and cmlp top, the objective function for Equation 3 in the SRM can be formulated. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 8 (a) Top Controller Vector Pooling Unit EMB Core DMA TT Computation Unit Array Ctlr. TT_AMem TT_BMem TT_CMem PE PE PE PE PE PE PE Array 32 16 Reshaper Main Pool. DRAM SSD TT Post-pooling Top Controller Interconnect Controller MLP Core Interconnect Interconnect Interconnect (b) WMem BMem IOMem Shared Mem. DMA Array Ctlr. MLP Computation Unit PE PE PE PE PE PE PE Array 16 8 Bias Adder Act. Func. Fig. 7. Hardware architecture of computing cores. (a) EMB core (b) MLP core By constructing the cost model as described above, embed- ding layer partitioning and custom hardware core allocation can be achieved, enabling the implementation of an optimized hybrid-parallel processing system based on DLRM workload intensity. D. Address Remapping Address remapping is essential step for facilitating access to memory-allocated EMBs, which are partitioned by the SRM, using EMB access indices derived from input sparse features. The length of the remapping table generated in this stage matches the row length of the EMB. When the sparse feature index accesses the remapping table, a 32-bit remapped address is retrieved, which is then decoded to read the embedding vectors allocated in the corresponding memory devices. In detail, the remapped address is composed of {device id[1:0], emb idx[29:0]}, where a device id of 0 indicates the embed- ding vector is fetched from DRAM, 1 from BRAM, and 2 from SSD. By utilizing the remapping table loaded into a host, SCRec enables three-level sharding to be performed in a fine- grained manner. E. Hardware Accelerator Allocation The objective of the core mapper is to allocate either an MLP core for processing MLP layers or an EMB core for processing embedding layers based on dm, derived from solv- ing the cost model of SRM. The core mapper s applicability within SCRec is closely tied to the hardware architecture of the SmartSSD, which is equipped with an FPGA chip featuring reconfigurable hardware. This architecture enables adaptive programming of hardware configurations optimized for specific tasks prior to runtime. By leveraging this reconfig- urable capability, hardware accelerators tailored to the DLRM workload can be mapped onto the SmartSSD. EMB Core EMB core fetches embedding vectors stored in TT-format, as well as those stored in DRAM and SSD, to perform pooling operations. It operates based on a 32- bit floating-point (FP) processing element (PE), as shown in Fig.7(a). The key module of the EMB core is the TT Algorithm 1 TT-core Matrix-multiplication Sequence Input: Gk (TT-core of EMB table) Output: V (Approximated EMB vector) Parameter: K (TT-dim.), R (TT-rank), J (EMB dim.) 1: Initialization 2: for k 1 to K do 3: for i 1 to Ik do 4: if k 1 then 5: Uk[i] Unfold(Gk[:, i, :, :], ( 1, R)) 6: TT AMem Fetch(Uk[i]) 7: else 8: Uk[i] Unfold(Gk[:, i, :, :], (R, 1)) 9: TT CMem Fetch(Uk[i]) 10: end if 11: end for 12: end for 13: Runtime computation 14: for k 1 to K do 15: Matrix multiplication 16: if k 1 then 17: Tk MatMul(Uk[ik], Uk 1[ik 1]) 18: else 19: Tk MatMul(Rk, Uk 1[ik 1]) 20: end if 21: Matrix reshaping 22: if k K then 23: Rk 1 Reshape(Tk, ( 1, rank)) 24: TT BMem Fetch(Rk 1) 25: else 26: V Reshape(Tk, (J)) 27: end if 28: end for computation unit (CU), which accelerates sequential matrix multiplication operations within the TT-core to generate ap- proximated embedding vectors. While Equations 1 and 2 in Section II describe the calculation of a single element of an embedding vector, the TT CU computes the entire embedding vector simultaneously by processing the product of a 3-dimensional tensor using Equation 38: E(i, :) G1(:, i1, :, :)G2(:, i2, :, :) . . . Gd(:, id, :, :) (38) where TT-core Gk RRk 1 Ik Jk Rk with R0 Rk 1, the EMB row is defined as i Pd k 1 ik Qd l k 1 Il with the ranges 0 id Id 1. To perform 3D tensor multiplications on the EMB core, which is implemented with a 2D PE array, the 3D TT-core is unfolded into a 2D matrix and loaded into the on-chip memory during initialization. During sequential TT-core matrix multiplications, intermediate reshaping is per- formed to ensure conformability, as detailed in Algorithm 1. Specifically, the multiply-accumulate (MAC) operations in the EMB core are executed using an output-stationary systolic array, processing the entire matrix in 16 32-sized output tiles iteratively. The input memory for the PE array is de- signed with dual channels in the column direction, effectively JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 9 TABLE II EXPERIMENT SETUP Server CPU Intel Xeon Silver 4310 2.1GHz GPU Nvidia A40 GDDR6 48GB DRAM DDR4 SDRAM 256GB 3200Mbps SSD Simulator (MQSim) Mem. Device Samsung 860 EVO 3.84 TB NAND Flash Samsung V-NAND V4 (TLC, 64-layer) PCIe Interface Single Port PCIe Gen3 4 DRAM Cache LPDDR4 SDRAM 2GB 1866Mbps Read Time (tR) 45µs Page Size 16KB DRAM Simulator (Ramulator) Mem. Device DDR4 SDRAM 4GB 2400Mbps FPGA Chip Name AMD Kintex Ultrascale KU15P On-chip Mem. Cap. BRAM - 4.325MB URAM - 4.5MB of DSP Slices 1968 Core Freq. 200MHz hiding the reshaping overhead within the memory read latency. Additionally, the vector pooling unit (VPU) independently performs pooling operations for each memory device, and the final pooled embedding vector is produced through the post- pooling module. The proposed EMB core not only accelerates TT computations but also supports pooling operations on embedding vectors stored across multi-tier memory devices, enabling complete embedding layer computations within the SmartSSD. MLP Core The MLP core is designed to accelerate both the top and bottom MLP layers of the DLRM, utilizing a 32-bit FP PE, as illustrated in Fig.7(b). The MLP CU consists of an 8 16 PE array, performing matrix multiplication operations in tiles using output-stationary computation. Input values for the MLP layer are read from IOMem and flow into the PE array in the column direction, while weight parameters are read from WMem and flow into the PE array in the row direction. Once partial outputs are computed in the PE array, the bias adder module adds bias values from BMem, and the activation func- tion module applies a rectified linear unit (ReLU) operation before writing the results back to IOMem. The MLP core contains four CUs in total, with configurable input and weight data distribution from the interconnect, enabling the system to select between latency-optimized or throughput-optimized computation. Specifically, if input data is broadcast to all CUs and weight parameters are split so that each CU receives different weights, the MLP layer is processed in a latency- optimized manner. Conversely, if different input data is fed to each CU while weight parameters are broadcast to all CUs, the MLP layer is processed in a throughput-optimized manner. As a result, the MLP core in SCRec not only accelerates MLP operations but also supports configurable layer processing, significantly enhancing MLP processing performance. IV. EVALUATION A. Experiment Setup We established an experimental environment using a GPU server and cycle-accurate simulators to evaluate the effective- ness of implementing DLRM in SCRec, with detailed specifi- TABLE III DLRM EVALUATION DATASET SPECIFICATION Dataset Criteo Kaggle Meta Synthetic 2021 2022 MLP Layer O X X of EMB Table 26 856 788 Avg. EMB Rows 1,298,560 2,720,716 4,841,017 Avg. PF 1 8.34 13.6 of Data Sample 45,840,617 65,536 131,072 cations outlined in Table II. DLRM training and inference were conducted on a server equipped with Nvidia A40 GPUs to assess accuracy based on Meta DLRM [34] when applying the TT-format to the embedding layer. Additionally, we utilized Ramulator [35] and MQSim [36], simulators designed to evaluate the performance of DRAM and SSD, respectively, to measure processing performance during SCRec s runtime operation. To compare SCRec with a multi-GPU system, we measured the power consumption of SSD and DRAM using SimpleSSD [37] and VAMPIRE [38]. FPGA power consump- tion was profiled using the Xilinx Vivado power analysis tool to evaluate the power of the implemented core logic. We also designed an in-house cycle-accurate core simulator, similar to the memory simulators, to measure performance for simulating the hardware design implemented on the FPGA chip. To synchronize the memory simulators and the core simulator, modifications were made to enable transmission and reception of control signals and data between simulators via inter-process communication (IPC) using the TCP protocol [39]. This synchronization ensured accurate emulation of real- time system behavior by aligning the simulators at each cycle. Regarding the datasets used for evaluation, large-scale in- dustrial DLRM datasets are not publicly available. Therefore, we utilized the Criteo Display Advertising (CDA) dataset [40] and the MELS dataset [33]. As the characteristics of these datasets differ, as shown in Table III, the appropriate dataset was selected based on the evaluation metrics. Specifically, the CDA dataset supports the evaluation of the entire DLRM, including both the embedding and MLP layers. However, due to significant differences in the size and access patterns of its embedding layer compared to industrial DLRMs, it was primarily used for assessing DLRM accuracy and the performance of acceleration cores in our evaluation. Con- versely, while the MELS dataset is limited to evaluating the embedding layer without the MLP layers, it offers insights into addressing key challenges currently faced by industrial DLRMs. Consequently, we focused on using the MELS dataset to evaluate the energy efficiency and performance of the industrial DLRM implementation, with a specific emphasis on the embedding layer. To evaluate the SCRec software framework, we derived statistical parameters for the SRM from the DSA using sub- sampled data from the CDA and MELS datasets. For the CDA dataset, a 1 randomly subsampled portion was used to extract these parameters. In contrast, for the MELS dataset, given its smaller data sample size, we increased the subsampling rate to 10 to enhance statistical reliability. In the SRM, the Gurobi optimizer [41] was employed to construct the MIP-based cost JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 10 LUT LUTRAM FF BRAM URAM DSP PE Array 209.5K (40.08 ) 30.7K (19.08 ) 406.9K (38.93 ) 0 (0 ) 0 (0 ) 1536 (78.05 ) Reshaper 413 (0.08 ) 0 (0 ) 935 (0.09 ) 0 (0 ) 0 (0 ) 0 (0 ) Arr Ctlr. 248 (0.05 ) 0 (0 ) 299 (0.03 ) 0 (0 ) 0 (0 ) 0 (0 ) 13 (0.01 ) 1.3K (0.79 ) 5.2K (0.49 ) 114 (11.59 ) 128 (100 ) 0 (0 ) 18.2K (3.49 ) 1.3K (0.79 ) 32.5K (3.11 ) 0 (0 ) 0 (0 ) 198 (10.06 ) 627 (0.12 ) 0 (0 ) 570 (0.05 ) 0 (0 ) 0 (0 ) 0 (0 ) 229.0K (43.82 ) 33.3K (20.66 ) 446.5K (42.70 ) 114 (11.59 ) 128 (100 ) 1734 (88.11 ) Vector Pooling Unit Etc. Total EMB Core TT CU TT_Mem LUT LUTRAM FF BRAM URAM DSP PE Array 215.8K (41.30 ) 29.3K (18.20 ) 417.2K (39.91 ) 0 (0 ) 0 (0 ) 1536 (78.05 ) Bias Adder 5.1K (0.98 ) 416 (0.26 ) 2.5K (0.24 ) 0 (0 ) 0 (0 ) 64 (3.25 ) Act Func. 520 (0.10 ) 0 (0 ) 247 (0.02 ) 0 (0 ) 0 (0 ) 0 (0 ) Array Ctlr. 2.2K (0.44 ) 0 (0 ) 145 (0.01 ) 0 (0 ) 0 (0 ) 0 (0 ) 4.8K (0.86 ) 0 (0 ) 2.8K (0.27 ) 488 (49.59 ) 128 (100 ) 0 (0 ) 153 (0.03 ) 0 (0 ) 219 (0.02 ) 16 (1.63 ) 0 (0 ) 0 (0 ) 228.4K (43.69 ) 29.7K (18.46 ) 423.1K (40.47 ) 504 (51.22 ) 128 (100 ) 1600 (81.30 ) MLP Core MLP CU ( 0 3) Shared Mem Etc. Total (a) (b) Fig. 8. FPGA layout and resource utilization on SmartSSD. (a) EMB core (b) MLP core model. Specifically, the hot thr was empirically set to 1 if its value was below 0.01 of the largest EMB row in the entire EMB and to 0.99 if it exceeded this threshold. To generate TT-formatted EMBs, the t3nsor library [42] was utilized, applying a TT-rank of 4 to achieve higher data compression performance. B. Hardware Implementation Results To verify the feasibility of implementing the hardware cores of SCRec on a SmartSSD, we synthesized and performed place-and-route operations for the computation kernels of the EMB core and MLP core at a target frequency of 200 MHz. The implementation details and results are shown in Fig.8(a) and Fig.8(b), respectively. Both cores demonstrated the highest utilization in the CU module, primarily due to the hardware computation data format being based on single- precision FP. To achieve this, the PE modules instantiate FP multipliers and FP accumulators synthesized using Xilinx s FP Intellectual Property (IP). This design optimizes hardware by utilizing approximately 78 of the DSP slices, leveraging look-up tables (LUTs) and flip-flops (FFs). For the EMB core, the vector pooling unit (VPU) pools embedding vectors read from each memory device using FP adders, resulting in about 10 DSP slice utilization. Additionally, the post-pooling unit within the VPU computes the average pooling vector using an FP divider. In the MLP core, the bias adder module performs vector addition between the matrix multiplication result and the bias parameter using the FP adder module, which utilizes approximately 3 of the DSP slices. The difference in DSP slice utilization between non-CU modules and the CU modules arises from their differing workloads; non-CU modules perform element-wise operations, resulting 60 50 40 30 20 10 0 0 1 2 3 4 5 6 7 RM0 RM1 RM2 RM3 Relative Performance (times) of EMB Smart SSDs Embedding Dimension ( : 16 : 32 : 64 : 128 : 256) : of EMB SmartSSDs RM0 RM1 RM2 RM3 Bottom MLP 13-64-32-D 13-128-64-D 13-256-128-D 13-512-256-D Top MLP X-64-16-1 X-128-32-1 X-256-64-1 X-512-128-1 D: embedding dimension X: (26 1) embedding dimension (a) (b) Fig. 9. Performance comparison between CPU-DRAM system and SCRec. (a) DLRM configuration (b) Relative performance and EMB core allocation in a lower computational load. As a result, these modules are designed to consume relatively less hardware resources. C. Performance We evaluated SCRec using the CDA dataset by analyzing core allocation variations based on the layer workload in the DLRM and comparing its performance to a CPU-DRAM system. This evaluation utilized the inference per second (IPS) metric with a batch size of 128, using 8 SmartSSDs. The DLRM configuration applied four different setups, as de- scribed in Fig.9(a), with each model containing an embedding layer composed of 26 EMBs. In the DLRM, the input layer for the top MLP layer was determined by concatenating the outputs of the 26 EMBs and the bottom MLP output, following the constraints of the Meta DLRM [34]. Analyzing the graph in Fig.9(b), SCRec significantly outperforms the CPU-DRAM system when the embedding layer workload exceeds that of the MLP layers. Specifically, for an embedding dimension of 16, SCRec shows a performance improvement of 10.37 - 55.77 across the four recommendation models (RMs). This enhancement is attributed to SCRec s ability to leverage multiple SmartSSD devices, enabling model parallelism in processing the embedding layer and thereby increasing pro- cessing bandwidth compared to the CPU-DRAM system. Within each RM, as the embedding dimension increases, the relative performance of SCRec declines. This is because a larger embedding dimension increases the processing overhead of the top MLP layer, indicating that SCRec is more effective for embedding-dominant DLRM workloads than for MLP- dominant ones. Regarding core allocation for the SmartSSDs, as the workload of the MLP layer grows relative to that of the embedding layer, the number of SmartSSDs assigned to EMB cores decreases, while the number assigned to MLP cores increases. This demonstrates that by optimizing the cost model in its software stack, SCRec effectively mitigates low JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 11 0 Relative Energy Efficiency (times) 64 128 256 512 64 128 256 512 Embedding Dimension of GPUs 2 4 6 8 10 12 14 280 240 200 160 120 80 40 0 MELS 2021 MELS 2022 : of GPUs System Type ( : Multi-GPU : SCRec [Ours]) System Type ( : Multi-GPU : SCRec [Ours]) Fig. 10. Energy efficiency comparison between multi-GPU system and SCRec. core utilization caused by imbalances in processing overheads between the embedding and MLP layers. These results indicate that SCRec can optimize the system based on workload intensity across various DLRM configurations and improve processing performance through hybrid-parallel processing. D. Energy Efficiency We evaluated the energy efficiency of SCRec, deployed on 8 SmartSSDs, against a multi-GPU system by varying embedding dimensions and utilizing the MELS 2021 and 2022 datasets. The batch size was set to 1024, and model parallelism was applied similarly in both systems, with EMBs split table- wise and distributed to GPUs in a round-robin manner based on their index. Due to the large size of the model parameters in MELS, which could not fit into a single server, a hash table was applied to the EMBs during measurement to limit the maximum index of the EMBs. Energy efficiency was measured using the IPS per watt (IPS W) metric to assess rel- ative performance. As shown in Fig.10, SCRec demonstrated superior energy efficiency compared to the multi-GPU system across both datasets, except when the embedding dimension was set to 64. As the embedding dimension increased, the improvement in energy efficiency became more pronounced. Specifically, with an embedding dimension of 512, SCRec achieved energy efficiency improvements of 6.14 for the 2021 dataset and 13.35 for the 2022 dataset. This improve- ment stems from the fact that as the embedding dimension increases, the number of GPUs required to store DLRM parameters grows significantly. In contrast, SCRec can fully accommodate DLRM parameters within the 32 TB capacity of the SmartSSDs. Moreover, SCRec compensates for hot embedding vectors that cannot be fetched to DRAM due to capacity limitations by compressing them using the TT-format, enabling GB s-level bandwidth. These findings highlight that SCRec provides an energy-efficient solution for the embedding layers of industrial-scale DLRMs compared to multi-GPU systems. Unlike systems requiring dozens or even hundreds of GPUs with complex network topologies, SCRec achieves energy efficiency by utilizing multiple SmartSSD devices within a single server, eliminating network communication overhead. 100 Inference Per Second (IPS) MELS 2021 101 102 103 104 1 2 4 8 1 2 4 8 of SmartSSDs MELS 2022 Sharding strategy ( : one-level : two-level : three-level [SCRec]) Fig. 11. Performance-based ablation study using Meta embedding lookup synthetic dataset. E. Ablation Study We evaluated the performance changes in SCRec based on the sharding levels applied, varying the number of SmartSSDs and using the MELS 2021 and 2022 datasets, which in- clude only embedding layer access patterns. An embedding dimension of 256 and a batch size of 1024 were used. For one-level sharding, only SSD was utilized, while for two- level sharding, both SSD and DRAM were employed. As shown in Fig.11, increasing the sharding level by incorporating additional memory devices to store EMBs, while maintaining the same number of SmartSSDs, leads to improved inference performance. For the 2021 dataset, the three-level sharding scheme applied in SCRec achieved performance improvements of 141.34 and 10.11 over one-level and two-level sharding, respectively, when 8 SmartSSDs were used. With a single SmartSSD, the improvements were 6.55 and 3.02 , respec- tively. Similarly, for the 2022 dataset, three-level sharding delivered performance gains of 161.77 and 15.48 over one- level and two-level sharding, respectively, with 8 SmartSSDs, and 6.31 and 3.14 with a single SmartSSD. The variation in performance improvements with higher sharding levels, while keeping the number of SmartSSDs constant, can be attributed to several factors. Transitioning from one-level to two-level sharding allows frequently accessed embedding vectors to be fetched from DRAM, mitigating the performance degradation caused by the lower bandwidth of SSDs. Advancing from two-level to three-level sharding compensates for DRAM s capacity limitations by storing highly compressed embedding vectors in TT-format. This approach effectively hides SSD access latency by overlapping it with the access latencies of DRAM and BRAM, resulting in superior performance with three-level sharding. Moreover, increasing the number of SmartSSDs in SCRec s three-level sharding further enhances performance due to the application of model parallelism in the embedding layer. As the number of devices grows, the effective bandwidth increases proportionally. Notably, for both datasets, the most significant performance improvements occur when the number of SmartSSDs increases from 1 to 2, with gains of 17.64 and 11.46 , respectively. This is because fewer SmartSSDs exacerbate the capacity limitations when JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 12 4 8 16 32 16 78.02 (-0.78) 78.14 (-0.66) 78.23 (-0.57) 78.40 (-0.40) 32 78.08 (-0.73) 78.26 (-0.55) 78.35 (-0.46) 78.47 (-0.34) 64 77.95 (-0.84) 78.17 (-0.62) 78.27 (-0.52) 78.45 (-0.34) 16 78.84 ( 0.04) 78.86 ( 0.06) 78.85 ( 0.05) 78.85 ( 0.05) 32 78.89 ( 0.08) 78.89 ( 0.08) 78.90 ( 0.09) 78.90 ( 0.09) 64 78.97 ( 0.18) 78.94 ( 0.15) 78.99 ( 0.20) 78.97 ( 0.18) [ ] EMB dim. TT-rank All-TT SCRec (Ours) Fig. 12. Accuracy comparison. (Numbers in parentheses represent accuracy difference from vanilla-DLRM) fetching hot embedding vectors into high-bandwidth memory devices. These results demonstrate that three-level sharding in SCRec not only addresses DRAM capacity limitations but also effectively hides SSD access latency, significantly enhancing the performance of the embedding layer in industrial-scale DLRMs. F. Accuracy Using low-rank approximation techniques such as TT de- composition significantly increases the data CR. However, applying the TT-format introduces information loss, and as the TT-rank decreases, the quality of data representation deteriorates, resulting in lower model accuracy. Therefore, minimizing accuracy drop becomes a critical focus when compressing the embedding layer in the DLRM using the TT-format. To evaluate accuracy when part of the EMBs is represented in the TT-format with SCRec, we varied the embedding dimension and rank, as shown in Fig.12. For this evaluation, we used the RM3 configuration described in Fig.9, based on the CDA dataset. When all embedding layers were replaced with the TT-format, and inference accuracy was measured after training, the accuracy drop was highest at a TT-rank of 4 across all embedding dimensions, ranging from - 0.84 to -0.73 . In contrast, the accuracy drop was lowest at a TT-rank of 32, ranging from -0.40 to -0.34 , demonstrating that higher TT-ranks improve the data quality of EMBs and reduce accuracy loss. SCRec, on the other hand, exhibited a slight accuracy increase of 0.04 to 0.20 across all experiments. This improvement occurs because SCRec does not apply the TT-format to all EMBs. Instead, it stores most frequently accessed embedding vectors in DRAM, while less frequently accessed ones are stored in the TT-format. Conse- quently, only a small portion of embedding vector accesses are impacted by accuracy loss. These findings indicate that in SCRec, using a TT-rank of 4 achieves high compression rates while maintaining high-throughput access to the embedding layer without any accuracy drop. Beyond the CDA dataset, SCRec facilitates partial compression of EMBs in large-scale industrial DLRMs using the TT-format, suggesting its potential for effective application in these scenarios while mitigating accuracy loss. V. RELATED WORKS A. Multi-tier Memory Caching and Sharding Due to the massive size of the embedding layer and the irregular access patterns with a power-law distribution in DLRM, many studies have focused on leveraging multi- tier memory and caching schemes to reduce EMB access latency. Scratch-Pipe [43] utilizes a 2-level memory hierarchy consisting of host CPU memory and GPU memory, caching embedding vectors in GPU memory to overcome the limited memory capacity of GPUs. This approach enables the embed- ding layer to be trained at GPU memory speed. AIBox [20] integrates GPU and SSD devices to store large embedding layers, implementing an SSD cache management system to provide low-latency access to embedding layers stored in SSDs. This allows industrial-scale recommendation models to be trained on a single node. In addition to caching methods, sharding schemes that split model parameters across multiple devices have also been extensively researched. RecShard [19] leverages multiple GPUs, placing frequently accessed data in the GPU s HBM and less frequently accessed data in unified virtual memory (UVM), which encompasses host DRAM and GPU HBM. By employing a cost model before runtime, RecShard effectively partitions the DLRM across GPU devices, significantly en- hancing training throughput. Similarly, AutoShard [44] uti- lizes a cost model to estimate table computation costs and incorporates deep reinforcement learning (DRL) to address the EMB partitioning challenge across multiple GPUs, achieving balanced sharding among GPUs. B. Tensor-train Decomposition In addition to the challenges posed by large EMB sizes in DLRM, the substantial vocabulary size in transformer models presents a significant hurdle in natural language processing (NLP). To address this, TT-embedding [42] was introduced to efficiently compress EMBs using tensor-train (TT) decom- position [31]. This method enables seamless integration into any deep learning framework and supports training via back- propagation. TT-Rec [32] applies the TT-embedding format to effectively compress EMBs in DLRM, introducing a scheme that initializes TT-core weights based on a sampled Gaussian distribution. It demonstrated negligible accuracy loss when tested on the CDA and Criteo Terabyte datasets. Similarly, EL-Rec [45] utilized the TT-format to compress the embedding layer and developed a TT-based training pipeline system. This approach mitigated host communication latency between the CPU and GPU, improving training performance on a single GPU. VI. CONCLUSION In this paper, we present SCRec, a scalable computational storage system with statistical sharding and TT decomposition for recommendation models. On the software side, SCRec leverages three-level statistical sharding to meet the high bandwidth requirements of DLRM by storing hot data in high- bandwidth memory devices and cold data in a low-bandwidth memory device. Additionally, SCRec can adaptively configure memory-centric and compute-centric cores based on DLRM workload intensity, thereby configuring optimized system. On the hardware side, SCRec implements custom hardware accel- eration cores to enhance DLRM computations. In particular, it JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 13 enables the high-performance reconstruction of approximated embedding vectors from significantly compressed TT-format, complementing memory capacity limitations while enhancing memory bandwidth. By integrating the software framework with the hardware accelerators into our system using mul- tiple SmartSSDs, SCRec achieved up to 55.77 inference performance improvement compared to a CPU-DRAM system and up to 13.35 energy efficiency improvement compared to a multi-GPU system. These results validate that large- scale DLRMs can be implemented on a single server with high performance and low energy costs, eliminating data communication overhead. REFERENCES [1] P. Resnick and H. R. Varian, Recommender systems, Communications of the ACM, vol. 40, no. 3, pp. 56 58, 1997. [2] S. K. Raghuwanshi and R. K. Pateriya, Recommendation systems: techniques, challenges, application, and evaluation, in Soft Computing for Problem Solving: SocProS 2017, Volume 2. Springer, 2019, pp. 151 164. [3] Z. Z. Darban and M. H. Valipour, Ghrs: Graph-based hybrid recom- mendation system with application to movie recommendation, Expert Systems with Applications, vol. 200, p. 116850, 2022. [4] P. Covington, J. Adams, and E. Sargin, Deep neural networks for youtube recommendations, in Proceedings of the 10th ACM conference on recommender systems, 2016, pp. 191 198. [5] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir et al., Wide deep learning for recommender systems, in Proceedings of the 1st workshop on deep learning for recommender systems, 2016, pp. 7 10. [6] M. Naumov, J. Kim, D. Mudigere, S. Sridharan, X. Wang, W. Zhao, S. Yilmaz, C. Kim, H. Yuen, M. Ozdal et al., Deep learning training in facebook data centers: Design of scale-up and scale-out systems, arXiv preprint arXiv:2003.09518, 2020. [7] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry, Using collaborative filtering to weave an information tapestry, Communications of the ACM, vol. 35, no. 12, pp. 61 70, 1992. [8] Y. Hu, Y. Koren, and C. Volinsky, Collaborative filtering for implicit feedback datasets, in 2008 Eighth IEEE international conference on data mining. Ieee, 2008, pp. 263 272. [9] Y. Koren, R. Bell, and C. Volinsky, Matrix factorization techniques for recommender systems, Computer, vol. 42, no. 8, pp. 30 37, 2009. [10] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, Item-based collabo- rative filtering recommendation algorithms, in Proceedings of the 10th international conference on World Wide Web, 2001, pp. 285 295. [11] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini et al., Deep learning recommendation model for personalization and recommenda- tion systems, arXiv preprint arXiv:1906.00091, 2019. [12] H.-J. M. Shi, D. Mudigere, M. Naumov, and J. Yang, Compositional embeddings using complementary partitions for memory-efficient rec- ommendation systems, in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining, 2020, pp. 165 175. [13] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazelwood, M. Hempstead, B. Jia et al., The architectural implications of facebook s dnn-based personalized recommendation, in 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2020, pp. 488 501. [14] A. Firoozshahian, J. Coburn, R. Levenstein, R. Nattoji, A. Kamath, O. Wu, G. Grewal, H. Aepala, B. Jakka, B. Dreyer et al., Mtia: First generation silicon targeting meta s recommendation systems, in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1 13. [15] S. Agarwal, C. Yan, Z. Zhang, and S. Venkataraman, Bagpipe: Ac- celerating deep recommendation model training, in Proceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 348 363. [16] Y. Kwon, Y. Lee, and M. Rhu, Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning, in Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, 2019, pp. 740 753. [17] V. Gupta, D. Choudhary, P. Tang, X. Wei, X. Wang, Y. Huang, A. Ke- jariwal, K. Ramchandran, and M. W. Mahoney, Training recommender systems at scale: Communication-efficient model and data parallelism, in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining, 2021, pp. 2928 2936. [18] W. Zhao, D. Xie, R. Jia, Y. Qian, R. Ding, M. Sun, and P. Li, Distributed hierarchical gpu parameter server for massive scale deep learning ads systems, Proceedings of Machine Learning and Systems, vol. 2, pp. 412 428, 2020. [19] G. Sethi, B. Acun, N. Agarwal, C. Kozyrakis, C. Trippel, and C.- J. Wu, Recshard: statistical feature-based memory optimization for industry-scale neural recommendation, in Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2022, pp. 344 358. [20] W. Zhao, J. Zhang, D. Xie, Y. Qian, R. Jia, and P. Li, Aibox: Ctr prediction model training on a single node, in Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019, pp. 319 328. [21] U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G.-Y. Wei, H.- H. S. Lee, D. Brooks, and C.-J. Wu, Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference, in 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2020, pp. 982 995. [22] X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, Rm- ssd: In-storage computing for large-scale recommendation inference, in 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022, pp. 1056 1070. [23] M. Kim and S. Lee, Reducing tail latency of dnn-based recommender systems using in-storage processing, in Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems, 2020, pp. 90 97. [24] M. Wilkening, U. Gupta, S. Hsia, C. Trippel, C.-J. Wu, D. Brooks, and G.-Y. Wei, Recssd: near data processing for solid state drive based recommendation inference, in Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2021, pp. 717 729. [25] H. Wan, X. Sun, Y. Cui, C.-L. Yang, T.-W. Kuo, and C. J. Xue, Flashembedding: storing embedding tables in ssd for large-scale recom- mender systems, in Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems, 2021, pp. 9 16. [26] Z. Wang, Y. Wei, M. Lee, M. Langer, F. Yu, J. Liu, S. Liu, D. G. Abel, X. Guo, J. Dong et al., Merlin hugectr: Gpu-accelerated recommender system training and inference, in Proceedings of the 16th ACM Con- ference on Recommender Systems, 2022, pp. 534 537. [27] Y. Xiao, S. Zhao, Z. Zhou, Z. Huan, L. Ju, X. Zhang, L. Wang, and J. Zhou, G-meta: Distributed meta learning in gpu clusters for large-scale recommender systems, in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 2023, pp. 4365 4369. [28] D. Mudigere, Y. Hao, J. Huang, Z. Jia, A. Tulloch, S. Sridharan, X. Liu, M. Ozdal, J. Nie, J. Park et al., Software-hardware co-design for fast and scalable training of deep learning recommendation models, in Proceedings of the 49th Annual International Symposium on Computer Architecture, 2022, pp. 993 1011. [29] Z. Wang, Y. Wang, J. Deng, D. Zheng, A. Li, and Y. Ding, Rap: Resource-aware automated gpu sharing for multi-gpu recommendation model training and input preprocessing, in Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, 2024, pp. 964 979. [30] Y. Kwon, Y. Lee, and M. Rhu, Tensor casting: Co-designing algorithm- architecture for personalized recommendation training, in 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2021, pp. 235 248. [31] I. V. Oseledets, Tensor-train decomposition, SIAM Journal on Scien- tific Computing, vol. 33, no. 5, pp. 2295 2317, 2011. [32] C. Yin, B. Acun, C.-J. Wu, and X. Liu, Tt-rec: Tensor train compression for deep learning recommendation models, Proceedings of Machine Learning and Systems, vol. 3, pp. 448 462, 2021. [33] Facebook Research, Embedding lookup synthetic dataset, 2021, accessed: 2021-12-09. [Online]. Available: facebookresearch dlrm datasets [34] Facebook Research, An implementation of a deep learning recommendation model (dlrm), 2019, accessed: 2019-03-21. [Online]. Available: [35] Y. Kim, W. Yang, and O. Mutlu, Ramulator: A fast and extensible dram simulator, IEEE Computer architecture letters, vol. 15, no. 1, pp. 45 49, 2015. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 14 [36] A. Tavakkol, J. G omez-Luna, M. Sadrosadati, S. Ghose, and O. Mutlu, {MQSim}: A framework for enabling realistic studies of modern {Multi-Queue}{SSD} devices, in 16th USENIX Conference on File and Storage Technologies (FAST 18), 2018, pp. 49 66. [37] M. Jung, J. Zhang, A. Abulila, M. Kwon, N. Shahidi, J. Shalf, N. S. Kim, and M. Kandemir, Simplessd: Modeling solid state drives for holistic system simulation, IEEE Computer Architecture Letters, vol. 17, no. 1, pp. 37 41, 2017. [38] S. Ghose, A. G. Yaglikc i, R. Gupta, D. Lee, K. Kudrolli, W. X. Liu, H. Hassan, K. K. Chang, N. Chatterjee, A. Agrawal et al., What your dram power models are not telling you: Lessons from a detailed experimental study, Proceedings of the ACM on Measurement and Analysis of Computing Systems, vol. 2, no. 3, pp. 1 41, 2018. [39] V. Cerf and R. Kahn, A protocol for packet network intercommunica- tion, IEEE Transactions on communications, vol. 22, no. 5, pp. 637 648, 1974. [40] Crtieo AI Lab, Kaggle display advertising dataset, 2014, accessed: 2014-06-25. [Online]. Available: criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz [41] Gurobi Optimization, Gurobi optimizer reference manual, 2024. [On- line]. Available: [42] O. Hrinchuk, V. Khrulkov, L. Mirvakhabova, E. Orlova, and I. Oseledets, Tensorized embedding layers for efficient model compression, arXiv preprint arXiv:1901.10787, 2019. [43] Y. Kwon and M. Rhu, Training personalized recommendation systems from (gpu) scratch: Look forward not backwards, in Proceedings of the 49th Annual International Symposium on Computer Architecture, 2022, pp. 860 873. [44] D. Zha, L. Feng, B. Bhushanam, D. Choudhary, J. Nie, Y. Tian, J. Chae, Y. Ma, A. Kejariwal, and X. Hu, Autoshard: Automated embedding table sharding for recommender systems, in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 4461 4471. [45] Z. Wang, Y. Wang, B. Feng, D. Mudigere, B. Muthiah, and Y. Ding, El-rec: Efficient large-scale recommendation model training via tensor- train embedding table, in SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2022, pp. 1 14. Jinho Yang Jinho Yang (Graduate Student Mem- ber, IEEE) received the B.S. degree in electrical engineering from Hanyang University, Seoul, South Korea, in 2022, and the M.S. degree in electrical en- gineering from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2025, where he is currently pursuing the Ph.D. degree. His research interests include a hardware accel- erator for machine learning, hardware-software co- design, and a near-memory processing system. Ji-Hoon Kim Ji-Hoon Kim (Graduate Student Member, IEEE) received the B.S. degree in electrical engineering from Kyung-Hee University, Suwon, South Korea, in 2017 and the M.S. and Ph.D. degree in electrical engineering from the Korea Advanced Institute of Science and Technology (KAIST), Dae- jeon, South Korea, in 2019 and 2024, respectively. His research interests include AI ML hardware accelerator (ASIC FPGA) design, efficient AI ML system design, energy-efficient processing-in near- memory architecture, and hardware software code- sign for DNN processing. Joo-Young Kim Joo-Young Kim (Senior Member, IEEE) received the B.S., M.S., and Ph. D degrees in electrical engineering from Korea Advanced Insti- tute of Science and Technology (KAIST), Daejeon, South Korea, in 2005, 2007, and 2010, respectively. He is currently an Associate Professor in the School of Electrical Engineering, KAIST, and the Director of the AI Semiconductor Systems Research Center, KAIST. His research interests span various aspects of hardware design, including chip design, com- puter architecture, domain-specific accelerators, and hardware software co-design. Before joining KAIST, he was a Hardware Engineering Leader at Microsoft Azure, Redmond, WA, USA, working on hardware acceleration for cloud services such as machine learning, data storage, and networking. He founded an AI fabless startup, HyperAccel, in Jan 2023 to build innova- tive AI processors solutions for large-language-model(LLM)-based generative AI, making it sustainable for everyone.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nJOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 1 SCRec: A Scalable Computational Storage System with Statistical Sharding and Tensor-train Decomposition for Recommendation Models Jinho Yang, Graduate Student Member, IEEE, Ji-Hoon Kim, Graduate Student Member, IEEE, Joo-Young Kim, Senior Member, IEEE, Abstract Deep Learning Recommendation Models (DLRMs) play a crucial role in delivering personalized content across web applications such as social networking and video streaming. However, with improvements in performance, the parameter size of DLRMs has grown to terabyte (TB) scales, accompanied by memory bandwidth demands exceeding TB s levels. Furthermore, the workload intensity within the model varies based on the target mechanism, making it difficult to build an optimized recommendation system. In this paper, we propose SCRec, a scalable computational storage recommendation system that can handle TB-scale industrial DLRMs while guaranteeing high bandwidth requirements. SCRec utilizes a software framework that features a mixed-integer programming (MIP)-based cost model, efficiently fetching data based on data access patterns and adaptively configuring memory-centric and compute-centric cores. Additionally, SCRec integrates hardware acceleration cores to enhance DLRM computations, particularly allowing for the high-performance reconstruction of approximated embedding vectors from extremely compressed tensor-train (TT) format. By combining its software framework and hardware accelera- tors, while eliminating data communication overhead by being implemented on a single server, SCRec achieves substantial improvements in DLRM inference performance. It delivers up to 55.77 speedup compared to a CPU-DRAM system with no loss in accuracy and up to 13.35 energy efficiency gains over a multi-GPU system. Index Terms deep learning recommendation model, near-data processing, statistical sharding, tensor-train decomposition. I. INTRODUCTION R RECOMMENDATION systems are widely used in so- cial network services and video streaming platforms to provide personalized and preferred content to consumers as described in Fig.1. They are also employed in search engines to offer differentiated search services [1] [5]. Supporting rec- ommendation services for such a large number of consumers demands significant computation resources. For example, more than 80 of Meta s data center resources are allocated to recommendation system inference, while over 50 are utilized for training these systems [6].\n\n--- Segment 2 ---\nSupporting rec- ommendation services for such a large number of consumers demands significant computation resources. For example, more than 80 of Meta s data center resources are allocated to recommendation system inference, while over 50 are utilized for training these systems [6]. Traditional recommendation systems relied on collaborative filtering techniques, such as content filtering using matrix factorization [7] [10]. However, with advancements in deep neural networks (DNNs), deep learning recommendation mod- els (DLRMs) that combine embedding tables (EMBs) and This work was supported by Samsung Electronics Co., Ltd.. Manuscript received MM dd, yyyy; revised MM dd, yyyy. Application Music Music Movie Movie Game Game Sport Sport Sport Shop Shop Food Food Music Movie Game Sport Shop Food Recommendation System Feedback Rec. List User Like Dislike Watch Buy Fig. 1. Overview of recommendation system. multi-layer perceptron (MLP) layers have become a dom- inant approach in recommendation systems. These models are widely adopted in data centers, with recent focuses on both software-level and hardware-level optimizations [11] [17]. DLRMs process categorical data like website visit histories through EMBs, and numerical data such as user age through MLP layers. This combination has demonstrated superior recommendation performance, making DLRM the industry standard in recommendation systems. However, their model size and memory bandwidth requirements have grown rapidly with the recommendation performance. Specifically, the number of EMBs, which accounts for the majority of a DLRM s size, has increased to multiple terabytes (TBs) of parameters in memory devices, with bandwidth requirement rising to TB s level [18] [20]. In addition to the inherent characteristics of the DLRM algorithm, the workload intensity within the model varies depending on how it is applied in the recommendation system, making it difficult to build the optimized system [21]. There are two primary mechanisms in recommendation systems: content filtering and content ranking. The former is the pro- cess of selecting a list of recommended content for users where embedding layer processing dominates, resulting in a memory-centric workload. In contrast, the latter determines the order in which the recommended content is displayed to users where MLP layer processing dominates, leading to a compute-centric workload. As a result, it is crucial to configure optimized recommendation systems with focusing on the targeting mechanism of the DLRM, to achieve high utilization of the system resources.\n\n--- Segment 3 ---\nIn contrast, the latter determines the order in which the recommended content is displayed to users where MLP layer processing dominates, leading to a compute-centric workload. As a result, it is crucial to configure optimized recommendation systems with focusing on the targeting mechanism of the DLRM, to achieve high utilization of the system resources. Since traditional CPU- DRAM systems have struggled to meet the demands of modern DLRMs, it highlights the need for systems that can adaptively handle extensive memory and computational requirements. Research has been conducted to address the memory and computational challenges of DLRM using solid-state drives (SSDs). Recent studies [22] [25] have explored in-storage arXiv:2504.00520v1 [cs.AR] 1 Apr 2025 JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 2 processing (ISP) and near-memory processing (NMP) to per- form embedding layer computations within standalone SSDs. This approach facilitates the storage of TBs of EMBs and accelerates embedding layer operations by utilizing the inter- nal memory bandwidth of SSDs, providing significantly faster performance compared to conventional SSD usage. However, the internal bandwidth of SSDs remains substantially lower than that of volatile memory, such as high-bandwidth memory (HBM), rendering them insufficient to meet the growing memory bandwidth demands of DLRMs. Furthermore, the high computational complexity of the MLP layer poses sig- nificant challenges for SSD-based ISP, making it impractical to implement a complete DLRM system. Another approach to addressing both memory capacity and bandwidth is the use of multi-GPU systems. Recent works [26] [29] have implemented data parallelism and model parallelism to distribute the MLP and EMBs of the DLRM across multiple GPUs. Although this approach achieves highly performant systems, it also comes with a few challenging issues. The first issue is the large number of GPUs required to run a single DLRM. For instance, constructing a DLRM system with TBs of parameters would require dozens of NVIDIA H100 GPUs, each equipped with 80 GB of HBM, resulting in enormous system costs. The second issue is the underutilization of GPU compute resources for embedding- dominant DLRM applications, leading to inefficiency in the entire system.\n\n--- Segment 4 ---\nFor instance, constructing a DLRM system with TBs of parameters would require dozens of NVIDIA H100 GPUs, each equipped with 80 GB of HBM, resulting in enormous system costs. The second issue is the underutilization of GPU compute resources for embedding- dominant DLRM applications, leading to inefficiency in the entire system. Lastly, multiple GPU nodes must be intercon- nected to run the entire DLRM system at TB scale in parallel, leading to increased network complexity and overhead. The overall DLRM performance degradation is inevitable due to complex network configuration among GPU nodes, with data communication overhead among them. To be precise, all-to- all communication is required during model inference, where each GPU requests embedding vectors from different GPUs, causing significant system bottlenecks. To mitigate these challenges, recent work [19] has statisti- cally analyzed DLRM characteristics and proposed a sharding technique that stores frequently accessed embedding vectors in the GPU s HBM, while placing rarely accessed embed- ding vectors in the host s DRAM. This approach effectively maintains a comparable system memory bandwidth while reducing the required number of GPUs to run the DLRM. However, challenges such as high system cost, computing resource underutilization, and complexity of network topology still persist in the GPU-based DLRM systems. To address these challenges, we propose SCRec, a scalable computational storage system that can handle TB-scale indus- trial DLRMs while providing high bandwidth requirements. SCRec leverages SmartSSD devices, which integrate an SSD with a field-programmable gate array (FPGA) for near-data processing (NDP), effectively utilizing the SSD s large capac- ity alongside the high bandwidth of DRAM and block RAM (BRAM) on the FPGA. The detailed contributions of our work are as follows. SCRec utilizes a software framework with two pri- mary features statistical sharding and adaptive accel- eration core mapping leveraging a mixed-integer pro- gramming (MIP)-based cost model. By analyzing data EMB_1 EMB_N Embedding Layer Input Data Prediction Feature Interaction Layer Bottom MLP Emb. Vector Emb. Vector Emb. Vector Pooling Pooling Pooling Sparse Features View History ... Dense Features Region Age ... Top MLP Emb. Vector Pooling EMB_3 EMB_2 Item Rate Fig. 2. Architecture of deep learning recommendation model.\n\n--- Segment 5 ---\n2. Architecture of deep learning recommendation model. access patterns of DLRM input features, SCRec optimizes SmartSSD memory bandwidth by placing hot data in high-bandwidth memory devices such as BRAM and DRAM, and cold data in SSD. Additionally, it deter- mines the optimal configuration of memory-centric and compute-centric acceleration cores based on the work- load intensity analysis for each layer, ensuring a high- utilization DLRM system. We designed custom acceleration cores capable of boost- ing up EMB and MLP computations in DLRM and implemented them onto the FPGA chip of the SmartSSD device. Especially, the EMB core utilizes the tensor-train (TT) format to compress the GB-sized EMBs signifi- cantly into the megabyte (MB)-level, enabling the high- performance reconstruction of approximated embedding vectors. This also complements the device s DRAM ca- pacity limitations while enhancing the overall system s memory bandwidth. We integrated the software framework and hardware accelerators to develop a multi-SmartSSD system capable of running the entire DLRM operations. The system is configured within a single server, eliminating the need for complex network setup and eliminating data com- munication overhead. Additionally, within the server, it minimizes host communication overhead through peer-to- peer (P2P) data transfer between the SSD and FPGA chip. Therefore, SCRec can significantly reduce both inter- node and intra-node communication overhead. Our evaluation demonstrates that SCRec achieves signifi- cant improvements in inference performance, achieving up to 55.77 speed-up compared to a CPU-DRAM system with no loss in accuracy and up to 13.35 improvement in energy efficiency over a multi-GPU system. The remainder of this paper is organized as follows: Section II provides background information on the DLRM and TT- format; Section III describes SCRec, including its software framework and hardware architecture; Section IV presents the evaluation results of SCRec; Section V discusses related works on memory caching, sharding, and TT decomposition; and Section VI concludes the paper. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO.\n\n--- Segment 6 ---\nJOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 3 Den 1 Den 2 ... Den N Den 1 Den 2 ... Den N Dense Features bs bs N Spar 1 Spar 2 ... Spar M Spar 1 Spar 2 ... Spar M Sparse Features bs Input Data MLP Param. Bot MLP Layer 0 Bot MLP Layer 0 EMB Param. Embedding Table 0 Embedding Table 0 MLP Param. Top MLP Layer 0 Top MLP Layer 0 All-to-all Feature Interact. Device 1 Device 1 Device 1 Feat 1 Top MLP Feature Interact. Device 1 Feat 1 Top MLP Feature Interact. Device 2 Device 2 Device 2 Feat 2 Top MLP Feature Interact. Device 2 Feat 2 Top MLP Feature Interact. Device N Device N Device N Feat N Top MLP Feature Interact. Device N Feat N Top MLP Predict 1 Data Parallelism Data Parallelism Data Parallelism Model Parallelism Model Parallelism Model Parallelism Den 1 Bot MLP Device 1 Device 1 Device 1 Spar 4 EMB 4 Spar 4 EMB 4 Den 1 Bot MLP Device 1 Spar 4 EMB 4 Den 2 Bot MLP Device 2 Device 2 Device 2 Spar 8 EMB 8 Spar 8 EMB 8 Den 2 Bot MLP Device 2 Spar 8 EMB 8 Den N Bot MLP Device N Device N Device N Spar M EMB M Spar M EMB M Den N Bot MLP Device N Spar M EMB M Data Parallelism Data Parallelism Data Parallelism Predict 2 Predict N Fig. 3. Hybrid-parallel DLRM processing. II. BACKGROUND A. DLRM Architecture and Hybird-parallel Processing DLRM is a DNN-based recommendation model widely utilized across various industries, as depicted in Fig.2. It processes user and content data as input to predict user interactions with specific content, aiming to estimate the click-through rate (CTR). The model primarily consists of an embedding layer with multiple EMBs, MLP layers, and a feature interaction layer. In the DLRM, dense features and sparse features serve as inputs to the MLP layer and the embedding layer, respectively. Dense features, also referred to as continuous features, represent numerical data such as a user s age or geographic location.\n\n--- Segment 7 ---\nIn the DLRM, dense features and sparse features serve as inputs to the MLP layer and the embedding layer, respectively. Dense features, also referred to as continuous features, represent numerical data such as a user s age or geographic location. Sparse features, also known as categorical features, include data like website visit history or content category IDs. Sparse features are specifically represented as one-hot or multi-hot binary vectors, which are used to retrieve latent vectors from the EMBs. These latent vectors are then aggregated through a pooling operation to produce a single vector. The outputs of the bottom MLP layer and the embedding layer are integrated through concatenation in the feature interaction layer, followed by computation in the top MLP layer to produce the final prediction. A key strength of DLRM is its ability to use multiple EMBs to map discrete, categorical features into their corresponding vector representations, allowing it to effectively capture semantically meaningful representations of the target features [30]. The need for parallel processing across multiple devices in the DLRM arises from the model s inherent characteristics. First, EMBs, which account for the majority of DLRM pa- rameters, can number hundreds, with the total size of the em- bedding layer exceeding several TBs. Consequently, memory devices utilizing DRAM and HBM must split and store the EMBs across multiple devices. Second, as the MLP layer is compute-bound and the embedding layer is memory-bound, optimizing the system with respect to workload intensity is essential to prevent hardware underutilization. To address these challenges, hybrid-parallel processing is applied to optimize DLRM execution across multiple devices, as depicted in Fig.3. Data parallelism is used for the MLP layer, whereas model parallelism is applied to the embedding layer. Specifically, dense features are divided among devices using a batch-wise split, and EMBs are configured with a table-wise split.\n\n--- Segment 8 ---\nData parallelism is used for the MLP layer, whereas model parallelism is applied to the embedding layer. Specifically, dense features are divided among devices using a batch-wise split, and EMBs are configured with a table-wise split. In this setup, an all-to-all operation ensures that each device holds all Decompose Decompose Decompose EMB Table Reshape Reshape Reshape Reshaped Table TT Core 0 TT Core 1 TT Core 2 Row [ I1 I2 I3 ] Row [ I1 I2 I3 ] Dim [ J1 J2 J3 ] Dim [ J1 J2 J3 ] TT dim of TT cores 3 TT ranks [R] 4 TT dim of TT cores 3 TT ranks [R] 4 I3 J3 I3 J3 I1 J1 I1 J1 I1 J1 I1 J1 R1 R1 R1 R1 I2 J2 I2 J2 I3 J3 I3 J3 R3 Fig. 4. Process of representing an embedding table in TT-format. pooled latent vectors, as the feature interaction layer requires pooling results from all EMBs. Subsequently, the feature interaction layer and top MLP layer are processed using data parallelism, finally producing the CTR results. Hybrid-parallel processing in DLRM is a powerful approach that reduces the computational intensity of MLP operations while overcoming memory capacity limitations and increasing effective memory bandwidth. B. Tensor-train Decomposition TT decomposition [31] is a low-rank approximation method that decomposes high-dimensional tensors into lower- dimensional representations, enabling them to be approxi- mated with significantly less data. The decomposition process involves the following steps: A d-dimensional tensor T RI1 I2 Id, where Ik is the size of dimension k, can be represented as a sequential product of tensors G with lower dimension d, using the following equation 1. T (i1, i2, . . . , id) G1(:, i1, :)G2(:, i2, :) . . .\n\n--- Segment 9 ---\n. . Gd(:, id, :) (1) The decomposed tensor Gk RRk 1 Ik Rk is referred to as a TT-core, has a TT-rank Rk, where the first and last TT- cores have TT-rank of 1 to ensure that the sequential product of TT-cores results in a scalar, meaning R0 Rk 1. A matrix-form EMB can be represented using TT-format, as illustrated in Fig.4. If the EMB is represented as E RI J , where I Qd k 1 ik refers to the row length and J Qd k 1 jk refers to embedding dimension. the EMB can be reshaped into a d-dimensional tensor T R(i1j1) (i2j2) (idjd). This re- shaped tensor can then be decomposed into multiple TT-cores. Consequently, every element of the EMB can be approximated as a value obtained through the sequential product of TT-cores, which can be computed using Equation 2: E(i, j) T (m1, m2, . . . , md) G1(:, m1, :)G2(:, m2, :) . . . Gd(:, md, :) (2) where the EMB row is defined as i Pd k 1 ik Qd l k 1 Il, the EMB dimension value by j Pd k 1 jk Qd l k 1 Jl, and the reshaped tensor index by md Id id jd, with the ranges 0 id Id 1 and 0 jd Jd 1 [32]. The use of TT decomposition to compress DNN model parameters has significantly reduced the need for large mem- ory capacity. This method has proven particularly effective, achieving exceptionally high compression ratios up to thou- sands of times for DLRMs, especially for EMBs, which range from hundreds of GBs to severals TBs. Additionally, JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 4 MLP TT-core matmul. operation acceleration (enhancing DLRM throughput ) Runtime Device Host Remapping layer Host Remapping layer SmartSSD Cluster EMB SmartSSD N DRAM NAND flash (4TB) SSD ctlr.\n\n--- Segment 10 ---\nNN, MM YYYY 4 MLP TT-core matmul. operation acceleration (enhancing DLRM throughput ) Runtime Device Host Remapping layer Host Remapping layer SmartSSD Cluster EMB SmartSSD N DRAM NAND flash (4TB) SSD ctlr. PCIe switch FPGA chip BRAM EMB core EMB S.SSD 1 EMB param. MLP S.SSD M MLP param. MLP S.SSD 1 MLP param. Random Sampling Dense feat. Sparse feat. Dataset Model Config. of EMB table EMB dimension of EMB rows Software Framework of MLP layers MLP dimension DLRM Top MLP layer Bot MLP layer EMB layer Adaptive hardware resource mapping (optimizing system based on DLRM workload intensity ) Statistics CDF PF TT CR Latency Statistics CDF PF TT CR Latency Scalable Resource Manager EMB S.SSD 1 EMB 4 Row [DRAM, TT, SSD] EMB 4 Row [DRAM, TT, SSD] EMB S.SSD N EMB J Row [DRAM, TT, SSD] EMB J Row [DRAM, TT, SSD] 1, ..., M 1, ..., N EMB S.SSD idx. MLP S.SSD idx. 1, ..., M 1, ..., N EMB S.SSD idx. MLP S.SSD idx. SmartSSD Spec. of devices Memory size Memory BW. SmartSSD Spec. of devices Memory size Memory BW. Core Performance Simulator EMB Core TT matmul. MLP Core Top MLP Bot. MLP Top MLP Bot. MLP Address Remapper P2P communication (reducing host communication overhead ) Inst. Compiler Param. Mapper Core Mapper EMB core MLP core MLP core Data Statistic Analyzer Fig. 5. Overview of SCRec. fetching parameters in TT-format to on-chip memory reduces the off-chip memory footprint, thereby enhancing the model throughput. III. SCREC A. Overview of SCRec SCRec integrates a software framework that configures an optimized DLRM system based on DLRM s workload intensity, along with a custom hardware accelerator to enhance model performance.\n\n--- Segment 11 ---\nIII. SCREC A. Overview of SCRec SCRec integrates a software framework that configures an optimized DLRM system based on DLRM s workload intensity, along with a custom hardware accelerator to enhance model performance. SCRec consists of three main pipelines in the offline phase: the Data Statistic Analyzer (DSA), the scal- able Resource Manager (SRM), and the Address Remapper. Detailed descriptions of each stage are provided as follows: Data Statistic Analyzer DSA identifies the statistical characteristics of a dataset and uses them as parameters for the cost model in the subsequent SRM task, providing the necessary information for analyzing workload. Specifically, the DSA utilizes a randomly subsampled input dataset, the configuration of the DLRM, and the hardware specifications of the SmartSSD to derive key metrics, as explained in Section III-B. SCRec simulates hardware acceleration logic mapped to the SmartSSD using a cycle-accurate performance simulator. This simulation measures the latency required to retrieve embedding vectors in an EMB core and the latency of MLP operations in an MLP core while considering the off-chip memory footprint. These metrics are also used as parameters to build the cost model in the SRM. Scalable Resource Manager Based on the statistics ob- tained from the DSA, the SRM applies a MIP-based cost model to enable adaptive hardware core allocation and model parameter mapping on the SmartSSDs, based on the workload intensity of DLRM. A key feature of the SRM is its three- level sharding strategy, which stores hot embedding vectors with high access frequency in FPGA DRAM and BRAM, while cold embedding vectors with low access frequency are stored in the SSD. This approach leverages the large memory capacity of SSDs and the high memory bandwidth of DRAM. Notably, by storing EMBs in TT-format within FPGA BRAM, even large EMBs can be accommodated in on- chip memory, effectively overcoming the capacity limitations of FPGA DRAM, as discussed in Section III-C. Additionally, the SRM enables scalable, hardware-based latency optimizing by adaptively allocating MLP and EMB cores based on the workload intensity of the DLRM. Using the results generated by the SRM, heterogeneous computation cores are mapped to the SmartSSD, and DLRM parameters are fetched.\n\n--- Segment 12 ---\nNotably, by storing EMBs in TT-format within FPGA BRAM, even large EMBs can be accommodated in on- chip memory, effectively overcoming the capacity limitations of FPGA DRAM, as discussed in Section III-C. Additionally, the SRM enables scalable, hardware-based latency optimizing by adaptively allocating MLP and EMB cores based on the workload intensity of the DLRM. Using the results generated by the SRM, heterogeneous computation cores are mapped to the SmartSSD, and DLRM parameters are fetched. Simul- taneously, the instruction compiler generates instructions for multi-device execution. Addresss Remapper In the address remapper, a remapping layer is implemented to translate logical memory addresses, enabling access to sharded EMBs stored in corresponding devices. During DLRM execution, this remapping layer is loaded into host DRAM at runtime, facilitating memory ac- cess when the host requests embedding layer access from SmartSSDs. The implementation details of the remapping layer are discussed in Section III-D. Hardware Accelerator On the hardware side, the MLP core and EMB core are designed to accelerate computations in the MLP layer and embedding layer, respectively. In par- ticular, the primary function of the EMB core is to generate approximated embedding vectors represented in the TT-format while storing them on-chip, as detailed in Section III-E. Addi- tionally, P2P communication in SmartSSD is implemented via a PCIe switch to directly connect the SSD and FPGA chip, minimizing host communication overhead through NDP. B. Data Statistic Analyzer The goal of DSA is to derive the parameters required for the cost model used in the SRM. This is achieved using sub- sampled data, the model configuration, and a core performance simulator. The DSA computes essential parameters for con- structing the cost model, including the cumulative distribution function (CDF), pooling factor (PF), TT compression ratio (CR), and layer operation latency. In SCRec, statistics for the Meta Embedding Lookup Synthetic (MELS) dataset [33] from 2021 and 2022, which simulates the embedding layer of Meta s industrial DLRM, were analyzed. The results of this data profiling are presented in Fig.6(a) and Fig.6(b), respectively.\n\n--- Segment 13 ---\nIn SCRec, statistics for the Meta Embedding Lookup Synthetic (MELS) dataset [33] from 2021 and 2022, which simulates the embedding layer of Meta s industrial DLRM, were analyzed. The results of this data profiling are presented in Fig.6(a) and Fig.6(b), respectively. Detailed explanations of each statistical metric are provided below: JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 5 CDF [ ] Pooling Factor TT Compression Ratio Cumulative Rows [ ] (a) (b) Cumulative Rows [ ] EMB Index CDF [ ] Pooling Factor TT Compression Ratio EMB Index EMB Index EMB Index CDF [ ] Pooling Factor TT Compression Ratio Cumulative Rows [ ] (a) (b) Cumulative Rows [ ] EMB Index CDF [ ] Pooling Factor TT Compression Ratio EMB Index EMB Index EMB Index Fig. 6. Characteristic of sparse featrues in Meta embedding lookup synthetic dataset. (a) Year 2021 (b) Year 2022 Cumulative Distribution Function CDF is used to describe how frequently different rows in EMBs are accessed within the data sample, as shown in the left graph of Fig.6. Specifically, we analyzed the 856 EMBs from the 2021 dataset and the 788 EMBs from the 2022 dataset, plotting graphs of the access statistics. These graphs reveal that the EMB access pattern follows a flipped power-law distribution, where a small portion of the EMB accounts for the majority of total accesses. This observation suggests that employing a multi- level memory hierarchy with hot data placed in smaller, high-bandwidth memory and cold data stored in larger, low- bandwidth memory can significantly improve EMB access performance. Pooling Factor PF refers to the number of embedding vectors accessed for a particular sparse feature in a given data sample. PF directly impacts memory bandwidth demand, as a higher PF requires accessing more EMB rows, thereby increasing the memory load. The middle graphs in Fig.6 illustrate the average PF, showing a wide range from 0 to 118 in the 2021 dataset and from 0 to 175 in the 2022 dataset. Due to the significant variance in PF across EMBs, PF becomes a critical factor in effectively partitioning EMBs across devices.\n\n--- Segment 14 ---\nThe middle graphs in Fig.6 illustrate the average PF, showing a wide range from 0 to 118 in the 2021 dataset and from 0 to 175 in the 2022 dataset. Due to the significant variance in PF across EMBs, PF becomes a critical factor in effectively partitioning EMBs across devices. TT Compression Ratio TT CR refers to the ratio that quantifies how much the data can be compressed when an EMB is represented in TT-format. It is expressed as CR EMB size TT core size. Calculating the CR in the DSA is crucial because TT-cores must fit within the limited capacity of BRAM, which is only a few MBs. Additionally, the CR depends on various factors, including the rank of the TT-core, the dimensions of the EMBs, and the percentage of rows in the EMB selected for compression. For example, the graph on the right side of Fig.6 shows the CR when the entire EMB is compressed with an embedding dimension of 64 and a TT-rank of 4. In some EMBs, the TT-represented EMB surpasses the original size, while in others, the CR reaches values in the thousands. This variation arises because larger EMBs generally achieve higher CR.\n\n--- Segment 15 ---\nIn some EMBs, the TT-represented EMB surpasses the original size, while in others, the CR reaches values in the thousands. This variation arises because larger EMBs generally achieve higher CR. Given this characteristic, it is crucial to construct a cost model that incorporates the TABLE I DESCRIPTIONS OF SRM COST MODEL PARAMETERS Parameter Description M Number of SmartSSDs J Number of EMBs BS Batch Size BS mini Mini Batch Size of MLP Core Capbram FPGA BRAM Capacity per SmartSSD Capdram FPGA DRAM Capacity per SmartSSD Capssd SSD Capacity per SmartSSD ttt TT-core Matrix multiplication Latency tdram DRAM Read Latency tssd SSD Read Latency tmlp top Top MLP Layer Processing Latency tmlp bot Bottom MLP Layer Processing Latency ICDFj Inverse CDF of EMB j stepj Step Size for Piecewise Linear Interpolation of EMB j tt cmj TT-cores Size of EMB j pfj Average Pooling Factor of EMB j row lenj Row Length of EMB j dimj Embedding Dimension of EMB j embj Overall Data Size of EMB j dfj Data Format Byte Size of EMB j hot thrj Hot Data Threshold Percentage of EMB j TT CR of each EMB and its resulting compressed size. This ensures that TT-format EMBs can be effectively selected and compressed to fit within the FPGA s on-chip memory in the SmartSSDs using the cost model, thereby optimizing memory usage and access. Layer Operation Latency The latency of generating ap- proximated embedding vectors from TT-cores, along with the processing latency of the top and bottom MLP layers, is obtained through the core performance simulator and used as constraints in the cost model. Decoupling the cost model from the latency measurement simulator reduces the computational overhead of the cost model, while the cycle-accurate design of the simulator ensures precise latency estimations. Notably, when calculating the latency of MLP layers, the off-chip memory footprint is also considered, resulting in even more accurate latency measurements. C. Scalable Resource Manager SRM aims to achieve configuring optimized DLRM system through a MIP-based cost model, leveraging the characteristics of the DLRM profiled by the DSA.\n\n--- Segment 16 ---\nNotably, when calculating the latency of MLP layers, the off-chip memory footprint is also considered, resulting in even more accurate latency measurements. C. Scalable Resource Manager SRM aims to achieve configuring optimized DLRM system through a MIP-based cost model, leveraging the characteristics of the DLRM profiled by the DSA. The core concept of SRM is to minimize EMB access latency by utilizing a three- level memory hierarchy and the TT-format. Specifically, EMBs requiring high bandwidth are fetched into the DRAM and BRAM of the FPGA, while those with lower bandwidth requirements are stored in the SSD. This strategy enables the embedding layer parameters to take advantage of the large capacity of SSDs while benefiting from the high bandwidth of DRAM and BRAM. Moreover, by storing EMBs approxi- mated in the highly compressed TT-format within the BRAM, it becomes possible to fit GB-sized EMBs into the MB- sized on-chip memory, effectively increasing the embedding layer s effective bandwidth. We refer to this technique as three-level sharding, and the following explanation details the implementation of the cost model used to apply this sharding scheme. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 6 Objective Function To optimize the latency of DLRM using multiple SmartSSD devices, we define an objective function to minimize the cost, as described in Equation 3, with the parameters outlined in Table I. minimize C subject to cfnt cmlp top C (3) cfnt refers to the cost of the layer preceding the feature interaction layer, while cmlp top represents the cost of the top MLP layer. Since the bottom MLP layer and the embedding layer, which together determine cfnt, are processed in parallel, the cost function is designed to minimize the sum of the maximum cost between these two independently processed layers and the cost of the top MLP layer. Device Allocation The constraints for assigning cores ca- pable of computing the MLP and embedding layers are as follows: 1 X m dm M 1 m M (4) dm {0, 1} m M (5) dm represents the type of hardware core to be mapped onto the FPGA chip of the SmartSSD.\n\n--- Segment 17 ---\nSince the bottom MLP layer and the embedding layer, which together determine cfnt, are processed in parallel, the cost function is designed to minimize the sum of the maximum cost between these two independently processed layers and the cost of the top MLP layer. Device Allocation The constraints for assigning cores ca- pable of computing the MLP and embedding layers are as follows: 1 X m dm M 1 m M (4) dm {0, 1} m M (5) dm represents the type of hardware core to be mapped onto the FPGA chip of the SmartSSD. Specifically, when dm 0, the constraint ensures that an MLP core is mapped, whereas when dm 1, it enforces the mapping of an EMB core. Equation 4 controls the allocation of heterogeneous cores, preventing all SmartSSDs from being exclusively assigned to either EMB cores or MLP cores. This allocation ensures that all layers of the DLRM can be effectively accelerated within the multi-SmartSSD system. Embedding Table Allocation To enable model paral- lelism by partitioning the EMB parameters across SmartSSDs mapped to EMB cores, the following constraints are defined: X m pmj 1 j J (6) X j pmj dm X j pmj m M (7) pmj {0, 1} m M (8) pmjis a binary variable that indicates whether EMB j is assigned to SmartSSD m. In Equation 6, a constraint is imposed to ensure that each specific EMB is assigned to its corresponding SmartSSD. Equation 7 ensures that EMBs are only assigned to SmartSSDs that have been allocated EMB cores, serving as a constraint to prevent the incorrect assignment of EMBs to SmartSSDs with MLP cores. By utilizing these constraints, SCRec achieves model parallelism in the embedding layer through table-wise splitting of EMBs in the SRM. Three-level Sharding The EMBs assigned to SmartSSDs are split row-wise based on their access frequency patterns and are fetched into the FPGA s DRAM, BRAM, and SSD memory devices.\n\n--- Segment 18 ---\nBy utilizing these constraints, SCRec achieves model parallelism in the embedding layer through table-wise splitting of EMBs in the SRM. Three-level Sharding The EMBs assigned to SmartSSDs are split row-wise based on their access frequency patterns and are fetched into the FPGA s DRAM, BRAM, and SSD memory devices. The constraints are structured as follows to minimize EMB access latency: X i x dramji ICDFj(i) row lenj dimj dfj mem dramj j J (9) X i x dramji i stepj pct dramj j J (10) X i x dramji 1 j J (11) x dramji {0, 1} i 0, . . . , stepj j J (12) The above four equations constrain which rows of EMB j are fetched into the FPGA DRAM and determine the size of those rows. The variable x dram serves as a binary variable to compute the pct dram variable, which determines , which specifies the percentage of rows from EMB j are stored in DRAM, as described in Equation 10. At this point, the step size for EMB j is set to min(row lenj, 100). For the ICDF, the CDF obtained from the DSA is inverted and the interpolation interval is determined by stepj using piecewise linear interpolation, reducing the computational complexity of the cost model. Equation 9, formulated using the ICDF, calculates the size of EMB j to be fetched into DRAM, with mem dramj serving as a constraint on the FPGA DRAM capacity of the SmartSSD. X i x ptr ttji ICDFj(i) row lenj dimj dfj mem dramj mem ttj j J (13) X i x dramji i stepj X i x ttji i stepj X i x ptr ttji i stepj j J (14) X i x ttji i stepj pct ttj j J (15) X i x ttji X i x ptr ttji 1 j J (16) x ttji {0, 1} i 0, . . . , stepj j J (17) x ptr ttji {0, 1} i 0, . . .\n\n--- Segment 19 ---\n. . , stepj j J (18) The above six equations define constraints for determin- ing which rows of EMB j are fetched into BRAM in the compressed TT-format and the size of those rows before compression. The variable x tt is a binary variable used to calculate the percentage of rows from EMB j stored in BRAM, as described in Equation 15, while x ptr tt is a binary variable used to calculate the size of the EMB rows allocated to BRAM. Unlike Equations 9 12, the EMB rows stored in TT-format are positioned between the rows stored in DRAM and SSD. Thus, the starting and ending row indices of the EMB stored in TT-format can be derived using the x dram and x ptr tt binary variables via Equations 14 and 15. Additionally, the size of EMB j before compression into TT-format, denoted as mem ttj, is calculated using the ICDF in Equation 13, and this variable serves as a constraint on the BRAM capacity of the SmartSSD. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 7 X i x ptr ttji ICDFj X i x dramji ICDFj X i x row ttji i stepj j J (19) X i x row ttji 1 j J (20) x row ttji {0, 1} i 0, . . . , stepj j J (21) The above three equations define constraints for calculating the x row tt binary variable, which is necessary to determine the size of EMB j when represented across multiple TT-cores. Using x row tt, the cost model calculate the size of the EMBs compressed into TT-format, enabling the formulation of BRAM capacity constraints. pct dramj pct ttj hot thrj j J (22) Equation 22 imposes constraints to prevent excessive TT- format compression, ensuring that less frequently accessed embedding vectors are stored in the SSD. The hot thrj is assigned a value for each EMB. A value of 1 is used when the EMB row size is relatively small, whereas a value less than 1 is applied for significantly larger row sizes, ensuring that EMB parameters from larger tables are fetched from the SSD.\n\n--- Segment 20 ---\nThe hot thrj is assigned a value for each EMB. A value of 1 is used when the EMB row size is relatively small, whereas a value less than 1 is applied for significantly larger row sizes, ensuring that EMB parameters from larger tables are fetched from the SSD. Memory Capacity The constraints in the EMB allocation process determine how each EMB j is split at the row level and the extent of memory capacity it occupies when assigned to a SmartSSD. Additionally, a condition must be imposed to ensure that the size of the EMBs fetched to the device does not exceed the device s memory capacity. The constraints are defined as follows: row lenj dimj dfj embj j J (23) X j pmj mem dramj CapDRAM m M (24) X j pmj (embj mem dramj mem ttj) CapSSD m M (25) X j x row ttij tt cmj(i) dfj tt capj j J (26) X i pmj tt capj CapBRAM m M (27) Equations 23 25 limit the capacity of EMBs allocated to the DRAM and SSD of SmartSSD m, while Equations 26 and 27 restrict the capacity of EMBs compressed in TT-format. In these constraints, the tt cmj used in Equation 26 refers to the compressed size, which depends on the percentage of the total EMB rows compressed in TT-format. Similar to the ICDF, this value is obtained through piecewise linear interpolation during DSA stage. Embedding Vector Access Latency To optimize processing overhead, constraints can be formulated to link the embedding vector access latency of each memory device to the batch size. This is achieved utilizing the PF obtained from the DSA and the variables derived from the three-level sharding process.\n\n--- Segment 21 ---\nEmbedding Vector Access Latency To optimize processing overhead, constraints can be formulated to link the embedding vector access latency of each memory device to the batch size. This is achieved utilizing the PF obtained from the DSA and the variables derived from the three-level sharding process. The resulting equations are as follows: (pfj BS) (pct dramj tdram) c dramj j J (28) (pfj BS) (pct ttj ttt) c ttj j J (29) (pfj BS) ((1 pct dramj pct ttj) tssd) c ssdj j J (30) X j pmj c dramj c dramm m M (31) X j pmj c ttj c ttm m M (32) X j pmj c ssdj c ssdm m M (33) Equations 28 30 calculate latency cost of EMB j allocated to DRAM, BRAM, and SSD. Additionally, Equations 31 33 define the constraints for determining the latency of each memory tier in the SmartSSD, using the pmj derived earlier in the EMB allocation process. By applying these constraints, a precise cost model for the embedding layer can be constructed, taking the specific characteristics of DLRM into account. MLP Layer Latency In addition to the embedding layer s cost constraints, the latency cost of the top MLP layer and the bottom MLP layer can also be formulated with the following constraints: tmlp top (BS BS mini) cmlp top X m (1 dm) m M (34) tmlp bot (BS BS mini) cmlp bot X m (1 dm) m M (35) In the Equations 34 and 35, tmlp top and tmlp bot are values obtained from the cycle-accurate MLP core simulator, and BS mini refers to the batch size that can be processed by the MLP core in a single tile operation. The value of BS mini is configurable, as the MLP core is equipped with multiple computing cores. The details of the hardware architecture will be discussed in Section III-E. Cost Definition To define the objective function for opti- mizing the DLRM, constraints for the previously mentioned variables were formulated.\n\n--- Segment 22 ---\nThe details of the hardware architecture will be discussed in Section III-E. Cost Definition To define the objective function for opti- mizing the DLRM, constraints for the previously mentioned variables were formulated. Using these constraints, the equa- tions for the corresponding layers of the DLRM were estab- lished as follows: {c dramm, c ttm, c ssdm} cemb m M (36) {cmlp bot, cemb} cfnt (37) The constraint in Equation 36 defines the cost of the embedding layer, cemb, while the constraint in Equation 37 defines cfnt which represents the maximum cost between the bottom MLP layer and the embedding layer. Using cfnt and cmlp top, the objective function for Equation 3 in the SRM can be formulated. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 8 (a) Top Controller Vector Pooling Unit EMB Core DMA TT Computation Unit Array Ctlr. TT_AMem TT_BMem TT_CMem PE PE PE PE PE PE PE Array 32 16 Reshaper Main Pool. DRAM SSD TT Post-pooling Top Controller Interconnect Controller MLP Core Interconnect Interconnect Interconnect (b) WMem BMem IOMem Shared Mem. DMA Array Ctlr. MLP Computation Unit PE PE PE PE PE PE PE Array 16 8 Bias Adder Act. Func. Fig. 7. Hardware architecture of computing cores. (a) EMB core (b) MLP core By constructing the cost model as described above, embed- ding layer partitioning and custom hardware core allocation can be achieved, enabling the implementation of an optimized hybrid-parallel processing system based on DLRM workload intensity. D. Address Remapping Address remapping is essential step for facilitating access to memory-allocated EMBs, which are partitioned by the SRM, using EMB access indices derived from input sparse features. The length of the remapping table generated in this stage matches the row length of the EMB. When the sparse feature index accesses the remapping table, a 32-bit remapped address is retrieved, which is then decoded to read the embedding vectors allocated in the corresponding memory devices.\n\n--- Segment 23 ---\nThe length of the remapping table generated in this stage matches the row length of the EMB. When the sparse feature index accesses the remapping table, a 32-bit remapped address is retrieved, which is then decoded to read the embedding vectors allocated in the corresponding memory devices. In detail, the remapped address is composed of {device id[1:0], emb idx[29:0]}, where a device id of 0 indicates the embed- ding vector is fetched from DRAM, 1 from BRAM, and 2 from SSD. By utilizing the remapping table loaded into a host, SCRec enables three-level sharding to be performed in a fine- grained manner. E. Hardware Accelerator Allocation The objective of the core mapper is to allocate either an MLP core for processing MLP layers or an EMB core for processing embedding layers based on dm, derived from solv- ing the cost model of SRM. The core mapper s applicability within SCRec is closely tied to the hardware architecture of the SmartSSD, which is equipped with an FPGA chip featuring reconfigurable hardware. This architecture enables adaptive programming of hardware configurations optimized for specific tasks prior to runtime. By leveraging this reconfig- urable capability, hardware accelerators tailored to the DLRM workload can be mapped onto the SmartSSD. EMB Core EMB core fetches embedding vectors stored in TT-format, as well as those stored in DRAM and SSD, to perform pooling operations. It operates based on a 32- bit floating-point (FP) processing element (PE), as shown in Fig.7(a). The key module of the EMB core is the TT Algorithm 1 TT-core Matrix-multiplication Sequence Input: Gk (TT-core of EMB table) Output: V (Approximated EMB vector) Parameter: K (TT-dim. ), R (TT-rank), J (EMB dim.)\n\n--- Segment 24 ---\nThe key module of the EMB core is the TT Algorithm 1 TT-core Matrix-multiplication Sequence Input: Gk (TT-core of EMB table) Output: V (Approximated EMB vector) Parameter: K (TT-dim. ), R (TT-rank), J (EMB dim.) 1: Initialization 2: for k 1 to K do 3: for i 1 to Ik do 4: if k 1 then 5: Uk[i] Unfold(Gk[:, i, :, :], ( 1, R)) 6: TT AMem Fetch(Uk[i]) 7: else 8: Uk[i] Unfold(Gk[:, i, :, :], (R, 1)) 9: TT CMem Fetch(Uk[i]) 10: end if 11: end for 12: end for 13: Runtime computation 14: for k 1 to K do 15: Matrix multiplication 16: if k 1 then 17: Tk MatMul(Uk[ik], Uk 1[ik 1]) 18: else 19: Tk MatMul(Rk, Uk 1[ik 1]) 20: end if 21: Matrix reshaping 22: if k K then 23: Rk 1 Reshape(Tk, ( 1, rank)) 24: TT BMem Fetch(Rk 1) 25: else 26: V Reshape(Tk, (J)) 27: end if 28: end for computation unit (CU), which accelerates sequential matrix multiplication operations within the TT-core to generate ap- proximated embedding vectors. While Equations 1 and 2 in Section II describe the calculation of a single element of an embedding vector, the TT CU computes the entire embedding vector simultaneously by processing the product of a 3-dimensional tensor using Equation 38: E(i, :) G1(:, i1, :, :)G2(:, i2, :, :) . . . Gd(:, id, :, :) (38) where TT-core Gk RRk 1 Ik Jk Rk with R0 Rk 1, the EMB row is defined as i Pd k 1 ik Qd l k 1 Il with the ranges 0 id Id 1.\n\n--- Segment 25 ---\n. Gd(:, id, :, :) (38) where TT-core Gk RRk 1 Ik Jk Rk with R0 Rk 1, the EMB row is defined as i Pd k 1 ik Qd l k 1 Il with the ranges 0 id Id 1. To perform 3D tensor multiplications on the EMB core, which is implemented with a 2D PE array, the 3D TT-core is unfolded into a 2D matrix and loaded into the on-chip memory during initialization. During sequential TT-core matrix multiplications, intermediate reshaping is per- formed to ensure conformability, as detailed in Algorithm 1. Specifically, the multiply-accumulate (MAC) operations in the EMB core are executed using an output-stationary systolic array, processing the entire matrix in 16 32-sized output tiles iteratively. The input memory for the PE array is de- signed with dual channels in the column direction, effectively JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 9 TABLE II EXPERIMENT SETUP Server CPU Intel Xeon Silver 4310 2.1GHz GPU Nvidia A40 GDDR6 48GB DRAM DDR4 SDRAM 256GB 3200Mbps SSD Simulator (MQSim) Mem. Device Samsung 860 EVO 3.84 TB NAND Flash Samsung V-NAND V4 (TLC, 64-layer) PCIe Interface Single Port PCIe Gen3 4 DRAM Cache LPDDR4 SDRAM 2GB 1866Mbps Read Time (tR) 45µs Page Size 16KB DRAM Simulator (Ramulator) Mem. Device DDR4 SDRAM 4GB 2400Mbps FPGA Chip Name AMD Kintex Ultrascale KU15P On-chip Mem. Cap. BRAM - 4.325MB URAM - 4.5MB of DSP Slices 1968 Core Freq. 200MHz hiding the reshaping overhead within the memory read latency. Additionally, the vector pooling unit (VPU) independently performs pooling operations for each memory device, and the final pooled embedding vector is produced through the post- pooling module. The proposed EMB core not only accelerates TT computations but also supports pooling operations on embedding vectors stored across multi-tier memory devices, enabling complete embedding layer computations within the SmartSSD.\n\n--- Segment 26 ---\nAdditionally, the vector pooling unit (VPU) independently performs pooling operations for each memory device, and the final pooled embedding vector is produced through the post- pooling module. The proposed EMB core not only accelerates TT computations but also supports pooling operations on embedding vectors stored across multi-tier memory devices, enabling complete embedding layer computations within the SmartSSD. MLP Core The MLP core is designed to accelerate both the top and bottom MLP layers of the DLRM, utilizing a 32-bit FP PE, as illustrated in Fig.7(b). The MLP CU consists of an 8 16 PE array, performing matrix multiplication operations in tiles using output-stationary computation. Input values for the MLP layer are read from IOMem and flow into the PE array in the column direction, while weight parameters are read from WMem and flow into the PE array in the row direction. Once partial outputs are computed in the PE array, the bias adder module adds bias values from BMem, and the activation func- tion module applies a rectified linear unit (ReLU) operation before writing the results back to IOMem. The MLP core contains four CUs in total, with configurable input and weight data distribution from the interconnect, enabling the system to select between latency-optimized or throughput-optimized computation. Specifically, if input data is broadcast to all CUs and weight parameters are split so that each CU receives different weights, the MLP layer is processed in a latency- optimized manner. Conversely, if different input data is fed to each CU while weight parameters are broadcast to all CUs, the MLP layer is processed in a throughput-optimized manner. As a result, the MLP core in SCRec not only accelerates MLP operations but also supports configurable layer processing, significantly enhancing MLP processing performance. IV. EVALUATION A. Experiment Setup We established an experimental environment using a GPU server and cycle-accurate simulators to evaluate the effective- ness of implementing DLRM in SCRec, with detailed specifi- TABLE III DLRM EVALUATION DATASET SPECIFICATION Dataset Criteo Kaggle Meta Synthetic 2021 2022 MLP Layer O X X of EMB Table 26 856 788 Avg.\n\n--- Segment 27 ---\nEVALUATION A. Experiment Setup We established an experimental environment using a GPU server and cycle-accurate simulators to evaluate the effective- ness of implementing DLRM in SCRec, with detailed specifi- TABLE III DLRM EVALUATION DATASET SPECIFICATION Dataset Criteo Kaggle Meta Synthetic 2021 2022 MLP Layer O X X of EMB Table 26 856 788 Avg. EMB Rows 1,298,560 2,720,716 4,841,017 Avg. PF 1 8.34 13.6 of Data Sample 45,840,617 65,536 131,072 cations outlined in Table II. DLRM training and inference were conducted on a server equipped with Nvidia A40 GPUs to assess accuracy based on Meta DLRM [34] when applying the TT-format to the embedding layer. Additionally, we utilized Ramulator [35] and MQSim [36], simulators designed to evaluate the performance of DRAM and SSD, respectively, to measure processing performance during SCRec s runtime operation. To compare SCRec with a multi-GPU system, we measured the power consumption of SSD and DRAM using SimpleSSD [37] and VAMPIRE [38]. FPGA power consump- tion was profiled using the Xilinx Vivado power analysis tool to evaluate the power of the implemented core logic. We also designed an in-house cycle-accurate core simulator, similar to the memory simulators, to measure performance for simulating the hardware design implemented on the FPGA chip. To synchronize the memory simulators and the core simulator, modifications were made to enable transmission and reception of control signals and data between simulators via inter-process communication (IPC) using the TCP protocol [39]. This synchronization ensured accurate emulation of real- time system behavior by aligning the simulators at each cycle. Regarding the datasets used for evaluation, large-scale in- dustrial DLRM datasets are not publicly available. Therefore, we utilized the Criteo Display Advertising (CDA) dataset [40] and the MELS dataset [33]. As the characteristics of these datasets differ, as shown in Table III, the appropriate dataset was selected based on the evaluation metrics. Specifically, the CDA dataset supports the evaluation of the entire DLRM, including both the embedding and MLP layers.\n\n--- Segment 28 ---\nAs the characteristics of these datasets differ, as shown in Table III, the appropriate dataset was selected based on the evaluation metrics. Specifically, the CDA dataset supports the evaluation of the entire DLRM, including both the embedding and MLP layers. However, due to significant differences in the size and access patterns of its embedding layer compared to industrial DLRMs, it was primarily used for assessing DLRM accuracy and the performance of acceleration cores in our evaluation. Con- versely, while the MELS dataset is limited to evaluating the embedding layer without the MLP layers, it offers insights into addressing key challenges currently faced by industrial DLRMs. Consequently, we focused on using the MELS dataset to evaluate the energy efficiency and performance of the industrial DLRM implementation, with a specific emphasis on the embedding layer. To evaluate the SCRec software framework, we derived statistical parameters for the SRM from the DSA using sub- sampled data from the CDA and MELS datasets. For the CDA dataset, a 1 randomly subsampled portion was used to extract these parameters. In contrast, for the MELS dataset, given its smaller data sample size, we increased the subsampling rate to 10 to enhance statistical reliability. In the SRM, the Gurobi optimizer [41] was employed to construct the MIP-based cost JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 10 LUT LUTRAM FF BRAM URAM DSP PE Array 209.5K (40.08 ) 30.7K (19.08 ) 406.9K (38.93 ) 0 (0 ) 0 (0 ) 1536 (78.05 ) Reshaper 413 (0.08 ) 0 (0 ) 935 (0.09 ) 0 (0 ) 0 (0 ) 0 (0 ) Arr Ctlr.\n\n--- Segment 29 ---\nNN, NO. NN, MM YYYY 10 LUT LUTRAM FF BRAM URAM DSP PE Array 209.5K (40.08 ) 30.7K (19.08 ) 406.9K (38.93 ) 0 (0 ) 0 (0 ) 1536 (78.05 ) Reshaper 413 (0.08 ) 0 (0 ) 935 (0.09 ) 0 (0 ) 0 (0 ) 0 (0 ) Arr Ctlr. 248 (0.05 ) 0 (0 ) 299 (0.03 ) 0 (0 ) 0 (0 ) 0 (0 ) 13 (0.01 ) 1.3K (0.79 ) 5.2K (0.49 ) 114 (11.59 ) 128 (100 ) 0 (0 ) 18.2K (3.49 ) 1.3K (0.79 ) 32.5K (3.11 ) 0 (0 ) 0 (0 ) 198 (10.06 ) 627 (0.12 ) 0 (0 ) 570 (0.05 ) 0 (0 ) 0 (0 ) 0 (0 ) 229.0K (43.82 ) 33.3K (20.66 ) 446.5K (42.70 ) 114 (11.59 ) 128 (100 ) 1734 (88.11 ) Vector Pooling Unit Etc. Total EMB Core TT CU TT_Mem LUT LUTRAM FF BRAM URAM DSP PE Array 215.8K (41.30 ) 29.3K (18.20 ) 417.2K (39.91 ) 0 (0 ) 0 (0 ) 1536 (78.05 ) Bias Adder 5.1K (0.98 ) 416 (0.26 ) 2.5K (0.24 ) 0 (0 ) 0 (0 ) 64 (3.25 ) Act Func. 520 (0.10 ) 0 (0 ) 247 (0.02 ) 0 (0 ) 0 (0 ) 0 (0 ) Array Ctlr.\n\n--- Segment 30 ---\nTotal EMB Core TT CU TT_Mem LUT LUTRAM FF BRAM URAM DSP PE Array 215.8K (41.30 ) 29.3K (18.20 ) 417.2K (39.91 ) 0 (0 ) 0 (0 ) 1536 (78.05 ) Bias Adder 5.1K (0.98 ) 416 (0.26 ) 2.5K (0.24 ) 0 (0 ) 0 (0 ) 64 (3.25 ) Act Func. 520 (0.10 ) 0 (0 ) 247 (0.02 ) 0 (0 ) 0 (0 ) 0 (0 ) Array Ctlr. 2.2K (0.44 ) 0 (0 ) 145 (0.01 ) 0 (0 ) 0 (0 ) 0 (0 ) 4.8K (0.86 ) 0 (0 ) 2.8K (0.27 ) 488 (49.59 ) 128 (100 ) 0 (0 ) 153 (0.03 ) 0 (0 ) 219 (0.02 ) 16 (1.63 ) 0 (0 ) 0 (0 ) 228.4K (43.69 ) 29.7K (18.46 ) 423.1K (40.47 ) 504 (51.22 ) 128 (100 ) 1600 (81.30 ) MLP Core MLP CU ( 0 3) Shared Mem Etc. Total (a) (b) Fig. 8. FPGA layout and resource utilization on SmartSSD. (a) EMB core (b) MLP core model. Specifically, the hot thr was empirically set to 1 if its value was below 0.01 of the largest EMB row in the entire EMB and to 0.99 if it exceeded this threshold. To generate TT-formatted EMBs, the t3nsor library [42] was utilized, applying a TT-rank of 4 to achieve higher data compression performance. B. Hardware Implementation Results To verify the feasibility of implementing the hardware cores of SCRec on a SmartSSD, we synthesized and performed place-and-route operations for the computation kernels of the EMB core and MLP core at a target frequency of 200 MHz. The implementation details and results are shown in Fig.8(a) and Fig.8(b), respectively.\n\n--- Segment 31 ---\nHardware Implementation Results To verify the feasibility of implementing the hardware cores of SCRec on a SmartSSD, we synthesized and performed place-and-route operations for the computation kernels of the EMB core and MLP core at a target frequency of 200 MHz. The implementation details and results are shown in Fig.8(a) and Fig.8(b), respectively. Both cores demonstrated the highest utilization in the CU module, primarily due to the hardware computation data format being based on single- precision FP. To achieve this, the PE modules instantiate FP multipliers and FP accumulators synthesized using Xilinx s FP Intellectual Property (IP). This design optimizes hardware by utilizing approximately 78 of the DSP slices, leveraging look-up tables (LUTs) and flip-flops (FFs). For the EMB core, the vector pooling unit (VPU) pools embedding vectors read from each memory device using FP adders, resulting in about 10 DSP slice utilization. Additionally, the post-pooling unit within the VPU computes the average pooling vector using an FP divider. In the MLP core, the bias adder module performs vector addition between the matrix multiplication result and the bias parameter using the FP adder module, which utilizes approximately 3 of the DSP slices. The difference in DSP slice utilization between non-CU modules and the CU modules arises from their differing workloads; non-CU modules perform element-wise operations, resulting 60 50 40 30 20 10 0 0 1 2 3 4 5 6 7 RM0 RM1 RM2 RM3 Relative Performance (times) of EMB Smart SSDs Embedding Dimension ( : 16 : 32 : 64 : 128 : 256) : of EMB SmartSSDs RM0 RM1 RM2 RM3 Bottom MLP 13-64-32-D 13-128-64-D 13-256-128-D 13-512-256-D Top MLP X-64-16-1 X-128-32-1 X-256-64-1 X-512-128-1 D: embedding dimension X: (26 1) embedding dimension (a) (b) Fig. 9. Performance comparison between CPU-DRAM system and SCRec. (a) DLRM configuration (b) Relative performance and EMB core allocation in a lower computational load.\n\n--- Segment 32 ---\nPerformance comparison between CPU-DRAM system and SCRec. (a) DLRM configuration (b) Relative performance and EMB core allocation in a lower computational load. As a result, these modules are designed to consume relatively less hardware resources. C. Performance We evaluated SCRec using the CDA dataset by analyzing core allocation variations based on the layer workload in the DLRM and comparing its performance to a CPU-DRAM system. This evaluation utilized the inference per second (IPS) metric with a batch size of 128, using 8 SmartSSDs. The DLRM configuration applied four different setups, as de- scribed in Fig.9(a), with each model containing an embedding layer composed of 26 EMBs. In the DLRM, the input layer for the top MLP layer was determined by concatenating the outputs of the 26 EMBs and the bottom MLP output, following the constraints of the Meta DLRM [34]. Analyzing the graph in Fig.9(b), SCRec significantly outperforms the CPU-DRAM system when the embedding layer workload exceeds that of the MLP layers. Specifically, for an embedding dimension of 16, SCRec shows a performance improvement of 10.37 - 55.77 across the four recommendation models (RMs). This enhancement is attributed to SCRec s ability to leverage multiple SmartSSD devices, enabling model parallelism in processing the embedding layer and thereby increasing pro- cessing bandwidth compared to the CPU-DRAM system. Within each RM, as the embedding dimension increases, the relative performance of SCRec declines. This is because a larger embedding dimension increases the processing overhead of the top MLP layer, indicating that SCRec is more effective for embedding-dominant DLRM workloads than for MLP- dominant ones. Regarding core allocation for the SmartSSDs, as the workload of the MLP layer grows relative to that of the embedding layer, the number of SmartSSDs assigned to EMB cores decreases, while the number assigned to MLP cores increases. This demonstrates that by optimizing the cost model in its software stack, SCRec effectively mitigates low JOURNAL OF LATEX CLASS FILES, VOL. NN, NO.\n\n--- Segment 33 ---\nThis demonstrates that by optimizing the cost model in its software stack, SCRec effectively mitigates low JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 11 0 Relative Energy Efficiency (times) 64 128 256 512 64 128 256 512 Embedding Dimension of GPUs 2 4 6 8 10 12 14 280 240 200 160 120 80 40 0 MELS 2021 MELS 2022 : of GPUs System Type ( : Multi-GPU : SCRec [Ours]) System Type ( : Multi-GPU : SCRec [Ours]) Fig. 10. Energy efficiency comparison between multi-GPU system and SCRec. core utilization caused by imbalances in processing overheads between the embedding and MLP layers. These results indicate that SCRec can optimize the system based on workload intensity across various DLRM configurations and improve processing performance through hybrid-parallel processing. D. Energy Efficiency We evaluated the energy efficiency of SCRec, deployed on 8 SmartSSDs, against a multi-GPU system by varying embedding dimensions and utilizing the MELS 2021 and 2022 datasets. The batch size was set to 1024, and model parallelism was applied similarly in both systems, with EMBs split table- wise and distributed to GPUs in a round-robin manner based on their index. Due to the large size of the model parameters in MELS, which could not fit into a single server, a hash table was applied to the EMBs during measurement to limit the maximum index of the EMBs. Energy efficiency was measured using the IPS per watt (IPS W) metric to assess rel- ative performance. As shown in Fig.10, SCRec demonstrated superior energy efficiency compared to the multi-GPU system across both datasets, except when the embedding dimension was set to 64. As the embedding dimension increased, the improvement in energy efficiency became more pronounced. Specifically, with an embedding dimension of 512, SCRec achieved energy efficiency improvements of 6.14 for the 2021 dataset and 13.35 for the 2022 dataset. This improve- ment stems from the fact that as the embedding dimension increases, the number of GPUs required to store DLRM parameters grows significantly. In contrast, SCRec can fully accommodate DLRM parameters within the 32 TB capacity of the SmartSSDs.\n\n--- Segment 34 ---\nThis improve- ment stems from the fact that as the embedding dimension increases, the number of GPUs required to store DLRM parameters grows significantly. In contrast, SCRec can fully accommodate DLRM parameters within the 32 TB capacity of the SmartSSDs. Moreover, SCRec compensates for hot embedding vectors that cannot be fetched to DRAM due to capacity limitations by compressing them using the TT-format, enabling GB s-level bandwidth. These findings highlight that SCRec provides an energy-efficient solution for the embedding layers of industrial-scale DLRMs compared to multi-GPU systems. Unlike systems requiring dozens or even hundreds of GPUs with complex network topologies, SCRec achieves energy efficiency by utilizing multiple SmartSSD devices within a single server, eliminating network communication overhead. 100 Inference Per Second (IPS) MELS 2021 101 102 103 104 1 2 4 8 1 2 4 8 of SmartSSDs MELS 2022 Sharding strategy ( : one-level : two-level : three-level [SCRec]) Fig. 11. Performance-based ablation study using Meta embedding lookup synthetic dataset. E. Ablation Study We evaluated the performance changes in SCRec based on the sharding levels applied, varying the number of SmartSSDs and using the MELS 2021 and 2022 datasets, which in- clude only embedding layer access patterns. An embedding dimension of 256 and a batch size of 1024 were used. For one-level sharding, only SSD was utilized, while for two- level sharding, both SSD and DRAM were employed. As shown in Fig.11, increasing the sharding level by incorporating additional memory devices to store EMBs, while maintaining the same number of SmartSSDs, leads to improved inference performance. For the 2021 dataset, the three-level sharding scheme applied in SCRec achieved performance improvements of 141.34 and 10.11 over one-level and two-level sharding, respectively, when 8 SmartSSDs were used. With a single SmartSSD, the improvements were 6.55 and 3.02 , respec- tively. Similarly, for the 2022 dataset, three-level sharding delivered performance gains of 161.77 and 15.48 over one- level and two-level sharding, respectively, with 8 SmartSSDs, and 6.31 and 3.14 with a single SmartSSD.\n\n--- Segment 35 ---\nWith a single SmartSSD, the improvements were 6.55 and 3.02 , respec- tively. Similarly, for the 2022 dataset, three-level sharding delivered performance gains of 161.77 and 15.48 over one- level and two-level sharding, respectively, with 8 SmartSSDs, and 6.31 and 3.14 with a single SmartSSD. The variation in performance improvements with higher sharding levels, while keeping the number of SmartSSDs constant, can be attributed to several factors. Transitioning from one-level to two-level sharding allows frequently accessed embedding vectors to be fetched from DRAM, mitigating the performance degradation caused by the lower bandwidth of SSDs. Advancing from two-level to three-level sharding compensates for DRAM s capacity limitations by storing highly compressed embedding vectors in TT-format. This approach effectively hides SSD access latency by overlapping it with the access latencies of DRAM and BRAM, resulting in superior performance with three-level sharding. Moreover, increasing the number of SmartSSDs in SCRec s three-level sharding further enhances performance due to the application of model parallelism in the embedding layer. As the number of devices grows, the effective bandwidth increases proportionally. Notably, for both datasets, the most significant performance improvements occur when the number of SmartSSDs increases from 1 to 2, with gains of 17.64 and 11.46 , respectively. This is because fewer SmartSSDs exacerbate the capacity limitations when JOURNAL OF LATEX CLASS FILES, VOL. NN, NO.\n\n--- Segment 36 ---\nThis is because fewer SmartSSDs exacerbate the capacity limitations when JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 12 4 8 16 32 16 78.02 (-0.78) 78.14 (-0.66) 78.23 (-0.57) 78.40 (-0.40) 32 78.08 (-0.73) 78.26 (-0.55) 78.35 (-0.46) 78.47 (-0.34) 64 77.95 (-0.84) 78.17 (-0.62) 78.27 (-0.52) 78.45 (-0.34) 16 78.84 ( 0.04) 78.86 ( 0.06) 78.85 ( 0.05) 78.85 ( 0.05) 32 78.89 ( 0.08) 78.89 ( 0.08) 78.90 ( 0.09) 78.90 ( 0.09) 64 78.97 ( 0.18) 78.94 ( 0.15) 78.99 ( 0.20) 78.97 ( 0.18) [ ] EMB dim. TT-rank All-TT SCRec (Ours) Fig. 12. Accuracy comparison. (Numbers in parentheses represent accuracy difference from vanilla-DLRM) fetching hot embedding vectors into high-bandwidth memory devices. These results demonstrate that three-level sharding in SCRec not only addresses DRAM capacity limitations but also effectively hides SSD access latency, significantly enhancing the performance of the embedding layer in industrial-scale DLRMs. F. Accuracy Using low-rank approximation techniques such as TT de- composition significantly increases the data CR. However, applying the TT-format introduces information loss, and as the TT-rank decreases, the quality of data representation deteriorates, resulting in lower model accuracy. Therefore, minimizing accuracy drop becomes a critical focus when compressing the embedding layer in the DLRM using the TT-format. To evaluate accuracy when part of the EMBs is represented in the TT-format with SCRec, we varied the embedding dimension and rank, as shown in Fig.12. For this evaluation, we used the RM3 configuration described in Fig.9, based on the CDA dataset.\n\n--- Segment 37 ---\nTo evaluate accuracy when part of the EMBs is represented in the TT-format with SCRec, we varied the embedding dimension and rank, as shown in Fig.12. For this evaluation, we used the RM3 configuration described in Fig.9, based on the CDA dataset. When all embedding layers were replaced with the TT-format, and inference accuracy was measured after training, the accuracy drop was highest at a TT-rank of 4 across all embedding dimensions, ranging from - 0.84 to -0.73 . In contrast, the accuracy drop was lowest at a TT-rank of 32, ranging from -0.40 to -0.34 , demonstrating that higher TT-ranks improve the data quality of EMBs and reduce accuracy loss. SCRec, on the other hand, exhibited a slight accuracy increase of 0.04 to 0.20 across all experiments. This improvement occurs because SCRec does not apply the TT-format to all EMBs. Instead, it stores most frequently accessed embedding vectors in DRAM, while less frequently accessed ones are stored in the TT-format. Conse- quently, only a small portion of embedding vector accesses are impacted by accuracy loss. These findings indicate that in SCRec, using a TT-rank of 4 achieves high compression rates while maintaining high-throughput access to the embedding layer without any accuracy drop. Beyond the CDA dataset, SCRec facilitates partial compression of EMBs in large-scale industrial DLRMs using the TT-format, suggesting its potential for effective application in these scenarios while mitigating accuracy loss. V. RELATED WORKS A. Multi-tier Memory Caching and Sharding Due to the massive size of the embedding layer and the irregular access patterns with a power-law distribution in DLRM, many studies have focused on leveraging multi- tier memory and caching schemes to reduce EMB access latency. Scratch-Pipe [43] utilizes a 2-level memory hierarchy consisting of host CPU memory and GPU memory, caching embedding vectors in GPU memory to overcome the limited memory capacity of GPUs. This approach enables the embed- ding layer to be trained at GPU memory speed. AIBox [20] integrates GPU and SSD devices to store large embedding layers, implementing an SSD cache management system to provide low-latency access to embedding layers stored in SSDs.\n\n--- Segment 38 ---\nThis approach enables the embed- ding layer to be trained at GPU memory speed. AIBox [20] integrates GPU and SSD devices to store large embedding layers, implementing an SSD cache management system to provide low-latency access to embedding layers stored in SSDs. This allows industrial-scale recommendation models to be trained on a single node. In addition to caching methods, sharding schemes that split model parameters across multiple devices have also been extensively researched. RecShard [19] leverages multiple GPUs, placing frequently accessed data in the GPU s HBM and less frequently accessed data in unified virtual memory (UVM), which encompasses host DRAM and GPU HBM. By employing a cost model before runtime, RecShard effectively partitions the DLRM across GPU devices, significantly en- hancing training throughput. Similarly, AutoShard [44] uti- lizes a cost model to estimate table computation costs and incorporates deep reinforcement learning (DRL) to address the EMB partitioning challenge across multiple GPUs, achieving balanced sharding among GPUs. B. Tensor-train Decomposition In addition to the challenges posed by large EMB sizes in DLRM, the substantial vocabulary size in transformer models presents a significant hurdle in natural language processing (NLP). To address this, TT-embedding [42] was introduced to efficiently compress EMBs using tensor-train (TT) decom- position [31]. This method enables seamless integration into any deep learning framework and supports training via back- propagation. TT-Rec [32] applies the TT-embedding format to effectively compress EMBs in DLRM, introducing a scheme that initializes TT-core weights based on a sampled Gaussian distribution. It demonstrated negligible accuracy loss when tested on the CDA and Criteo Terabyte datasets. Similarly, EL-Rec [45] utilized the TT-format to compress the embedding layer and developed a TT-based training pipeline system. This approach mitigated host communication latency between the CPU and GPU, improving training performance on a single GPU. VI. CONCLUSION In this paper, we present SCRec, a scalable computational storage system with statistical sharding and TT decomposition for recommendation models. On the software side, SCRec leverages three-level statistical sharding to meet the high bandwidth requirements of DLRM by storing hot data in high- bandwidth memory devices and cold data in a low-bandwidth memory device.\n\n--- Segment 39 ---\nCONCLUSION In this paper, we present SCRec, a scalable computational storage system with statistical sharding and TT decomposition for recommendation models. On the software side, SCRec leverages three-level statistical sharding to meet the high bandwidth requirements of DLRM by storing hot data in high- bandwidth memory devices and cold data in a low-bandwidth memory device. Additionally, SCRec can adaptively configure memory-centric and compute-centric cores based on DLRM workload intensity, thereby configuring optimized system. On the hardware side, SCRec implements custom hardware accel- eration cores to enhance DLRM computations. In particular, it JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 13 enables the high-performance reconstruction of approximated embedding vectors from significantly compressed TT-format, complementing memory capacity limitations while enhancing memory bandwidth. By integrating the software framework with the hardware accelerators into our system using mul- tiple SmartSSDs, SCRec achieved up to 55.77 inference performance improvement compared to a CPU-DRAM system and up to 13.35 energy efficiency improvement compared to a multi-GPU system. These results validate that large- scale DLRMs can be implemented on a single server with high performance and low energy costs, eliminating data communication overhead. REFERENCES [1] P. Resnick and H. R. Varian, Recommender systems, Communications of the ACM, vol. 40, no. 3, pp. 56 58, 1997. [2] S. K. Raghuwanshi and R. K. Pateriya, Recommendation systems: techniques, challenges, application, and evaluation, in Soft Computing for Problem Solving: SocProS 2017, Volume 2. Springer, 2019, pp. 151 164. [3] Z. Z. Darban and M. H. Valipour, Ghrs: Graph-based hybrid recom- mendation system with application to movie recommendation, Expert Systems with Applications, vol. 200, p. 116850, 2022. [4] P. Covington, J. Adams, and E. Sargin, Deep neural networks for youtube recommendations, in Proceedings of the 10th ACM conference on recommender systems, 2016, pp. 191 198.\n\n--- Segment 40 ---\n[4] P. Covington, J. Adams, and E. Sargin, Deep neural networks for youtube recommendations, in Proceedings of the 10th ACM conference on recommender systems, 2016, pp. 191 198. [5] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir et al., Wide deep learning for recommender systems, in Proceedings of the 1st workshop on deep learning for recommender systems, 2016, pp. 7 10. [6] M. Naumov, J. Kim, D. Mudigere, S. Sridharan, X. Wang, W. Zhao, S. Yilmaz, C. Kim, H. Yuen, M. Ozdal et al., Deep learning training in facebook data centers: Design of scale-up and scale-out systems, arXiv preprint arXiv:2003.09518, 2020. [7] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry, Using collaborative filtering to weave an information tapestry, Communications of the ACM, vol. 35, no. 12, pp. 61 70, 1992. [8] Y. Hu, Y. Koren, and C. Volinsky, Collaborative filtering for implicit feedback datasets, in 2008 Eighth IEEE international conference on data mining. Ieee, 2008, pp. 263 272. [9] Y. Koren, R. Bell, and C. Volinsky, Matrix factorization techniques for recommender systems, Computer, vol. 42, no. 8, pp. 30 37, 2009. [10] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl, Item-based collabo- rative filtering recommendation algorithms, in Proceedings of the 10th international conference on World Wide Web, 2001, pp. 285 295. [11] M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park, X. Wang, U. Gupta, C.-J.\n\n--- Segment 41 ---\nM. Shi, J. Huang, N. Sundaraman, J. Park, X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini et al., Deep learning recommendation model for personalization and recommenda- tion systems, arXiv preprint arXiv:1906.00091, 2019. [12] H.-J. M. Shi, D. Mudigere, M. Naumov, and J. Yang, Compositional embeddings using complementary partitions for memory-efficient rec- ommendation systems, in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining, 2020, pp. 165 175. [13] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazelwood, M. Hempstead, B. Jia et al., The architectural implications of facebook s dnn-based personalized recommendation, in 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2020, pp. 488 501. [14] A. Firoozshahian, J. Coburn, R. Levenstein, R. Nattoji, A. Kamath, O. Wu, G. Grewal, H. Aepala, B. Jakka, B. Dreyer et al., Mtia: First generation silicon targeting meta s recommendation systems, in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1 13. [15] S. Agarwal, C. Yan, Z. Zhang, and S. Venkataraman, Bagpipe: Ac- celerating deep recommendation model training, in Proceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 348 363. [16] Y. Kwon, Y. Lee, and M. Rhu, Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning, in Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, 2019, pp. 740 753.\n\n--- Segment 42 ---\n[16] Y. Kwon, Y. Lee, and M. Rhu, Tensordimm: A practical near-memory processing architecture for embeddings and tensor operations in deep learning, in Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, 2019, pp. 740 753. [17] V. Gupta, D. Choudhary, P. Tang, X. Wei, X. Wang, Y. Huang, A. Ke- jariwal, K. Ramchandran, and M. W. Mahoney, Training recommender systems at scale: Communication-efficient model and data parallelism, in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining, 2021, pp. 2928 2936. [18] W. Zhao, D. Xie, R. Jia, Y. Qian, R. Ding, M. Sun, and P. Li, Distributed hierarchical gpu parameter server for massive scale deep learning ads systems, Proceedings of Machine Learning and Systems, vol. 2, pp. 412 428, 2020. [19] G. Sethi, B. Acun, N. Agarwal, C. Kozyrakis, C. Trippel, and C.- J. Wu, Recshard: statistical feature-based memory optimization for industry-scale neural recommendation, in Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2022, pp. 344 358. [20] W. Zhao, J. Zhang, D. Xie, Y. Qian, R. Jia, and P. Li, Aibox: Ctr prediction model training on a single node, in Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019, pp. 319 328. [21] U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G.-Y. Wei, H.- H. S. Lee, D. Brooks, and C.-J. Wu, Deeprecsys: A system for optimizing end-to-end at-scale neural recommendation inference, in 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2020, pp. 982 995. [22] X.\n\n--- Segment 43 ---\n982 995. [22] X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, Rm- ssd: In-storage computing for large-scale recommendation inference, in 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022, pp. 1056 1070. [23] M. Kim and S. Lee, Reducing tail latency of dnn-based recommender systems using in-storage processing, in Proceedings of the 11th ACM SIGOPS Asia-Pacific Workshop on Systems, 2020, pp. 90 97. [24] M. Wilkening, U. Gupta, S. Hsia, C. Trippel, C.-J. Wu, D. Brooks, and G.-Y. Wei, Recssd: near data processing for solid state drive based recommendation inference, in Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, 2021, pp. 717 729. [25] H. Wan, X. Sun, Y. Cui, C.-L. Yang, T.-W. Kuo, and C. J. Xue, Flashembedding: storing embedding tables in ssd for large-scale recom- mender systems, in Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems, 2021, pp. 9 16. [26] Z. Wang, Y. Wei, M. Lee, M. Langer, F. Yu, J. Liu, S. Liu, D. G. Abel, X. Guo, J. Dong et al., Merlin hugectr: Gpu-accelerated recommender system training and inference, in Proceedings of the 16th ACM Con- ference on Recommender Systems, 2022, pp. 534 537. [27] Y. Xiao, S. Zhao, Z. Zhou, Z. Huan, L. Ju, X. Zhang, L. Wang, and J. Zhou, G-meta: Distributed meta learning in gpu clusters for large-scale recommender systems, in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 2023, pp. 4365 4369.\n\n--- Segment 44 ---\n[27] Y. Xiao, S. Zhao, Z. Zhou, Z. Huan, L. Ju, X. Zhang, L. Wang, and J. Zhou, G-meta: Distributed meta learning in gpu clusters for large-scale recommender systems, in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 2023, pp. 4365 4369. [28] D. Mudigere, Y. Hao, J. Huang, Z. Jia, A. Tulloch, S. Sridharan, X. Liu, M. Ozdal, J. Nie, J. Park et al., Software-hardware co-design for fast and scalable training of deep learning recommendation models, in Proceedings of the 49th Annual International Symposium on Computer Architecture, 2022, pp. 993 1011. [29] Z. Wang, Y. Wang, J. Deng, D. Zheng, A. Li, and Y. Ding, Rap: Resource-aware automated gpu sharing for multi-gpu recommendation model training and input preprocessing, in Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, 2024, pp. 964 979. [30] Y. Kwon, Y. Lee, and M. Rhu, Tensor casting: Co-designing algorithm- architecture for personalized recommendation training, in 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2021, pp. 235 248. [31] I. V. Oseledets, Tensor-train decomposition, SIAM Journal on Scien- tific Computing, vol. 33, no. 5, pp. 2295 2317, 2011. [32] C. Yin, B. Acun, C.-J. Wu, and X. Liu, Tt-rec: Tensor train compression for deep learning recommendation models, Proceedings of Machine Learning and Systems, vol. 3, pp. 448 462, 2021. [33] Facebook Research, Embedding lookup synthetic dataset, 2021, accessed: 2021-12-09. [Online]. Available: facebookresearch dlrm datasets [34] Facebook Research, An implementation of a deep learning recommendation model (dlrm), 2019, accessed: 2019-03-21. [Online].\n\n--- Segment 45 ---\nAvailable: facebookresearch dlrm datasets [34] Facebook Research, An implementation of a deep learning recommendation model (dlrm), 2019, accessed: 2019-03-21. [Online]. Available: [35] Y. Kim, W. Yang, and O. Mutlu, Ramulator: A fast and extensible dram simulator, IEEE Computer architecture letters, vol. 15, no. 1, pp. 45 49, 2015. JOURNAL OF LATEX CLASS FILES, VOL. NN, NO. NN, MM YYYY 14 [36] A. Tavakkol, J. G omez-Luna, M. Sadrosadati, S. Ghose, and O. Mutlu, {MQSim}: A framework for enabling realistic studies of modern {Multi-Queue}{SSD} devices, in 16th USENIX Conference on File and Storage Technologies (FAST 18), 2018, pp. 49 66. [37] M. Jung, J. Zhang, A. Abulila, M. Kwon, N. Shahidi, J. Shalf, N. S. Kim, and M. Kandemir, Simplessd: Modeling solid state drives for holistic system simulation, IEEE Computer Architecture Letters, vol. 17, no. 1, pp. 37 41, 2017. [38] S. Ghose, A. G. Yaglikc i, R. Gupta, D. Lee, K. Kudrolli, W. X. Liu, H. Hassan, K. K. Chang, N. Chatterjee, A. Agrawal et al., What your dram power models are not telling you: Lessons from a detailed experimental study, Proceedings of the ACM on Measurement and Analysis of Computing Systems, vol. 2, no. 3, pp. 1 41, 2018. [39] V. Cerf and R. Kahn, A protocol for packet network intercommunica- tion, IEEE Transactions on communications, vol. 22, no. 5, pp. 637 648, 1974. [40] Crtieo AI Lab, Kaggle display advertising dataset, 2014, accessed: 2014-06-25. [Online].\n\n--- Segment 46 ---\n[40] Crtieo AI Lab, Kaggle display advertising dataset, 2014, accessed: 2014-06-25. [Online]. Available: criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz [41] Gurobi Optimization, Gurobi optimizer reference manual, 2024. [On- line]. Available: [42] O. Hrinchuk, V. Khrulkov, L. Mirvakhabova, E. Orlova, and I. Oseledets, Tensorized embedding layers for efficient model compression, arXiv preprint arXiv:1901.10787, 2019. [43] Y. Kwon and M. Rhu, Training personalized recommendation systems from (gpu) scratch: Look forward not backwards, in Proceedings of the 49th Annual International Symposium on Computer Architecture, 2022, pp. 860 873. [44] D. Zha, L. Feng, B. Bhushanam, D. Choudhary, J. Nie, Y. Tian, J. Chae, Y. Ma, A. Kejariwal, and X. Hu, Autoshard: Automated embedding table sharding for recommender systems, in Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022, pp. 4461 4471. [45] Z. Wang, Y. Wang, B. Feng, D. Mudigere, B. Muthiah, and Y. Ding, El-rec: Efficient large-scale recommendation model training via tensor- train embedding table, in SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2022, pp. 1 14. Jinho Yang Jinho Yang (Graduate Student Mem- ber, IEEE) received the B.S. degree in electrical engineering from Hanyang University, Seoul, South Korea, in 2022, and the M.S. degree in electrical en- gineering from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2025, where he is currently pursuing the Ph.D. degree. His research interests include a hardware accel- erator for machine learning, hardware-software co- design, and a near-memory processing system.\n\n--- Segment 47 ---\ndegree in electrical en- gineering from Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea, in 2025, where he is currently pursuing the Ph.D. degree. His research interests include a hardware accel- erator for machine learning, hardware-software co- design, and a near-memory processing system. Ji-Hoon Kim Ji-Hoon Kim (Graduate Student Member, IEEE) received the B.S. degree in electrical engineering from Kyung-Hee University, Suwon, South Korea, in 2017 and the M.S. and Ph.D. degree in electrical engineering from the Korea Advanced Institute of Science and Technology (KAIST), Dae- jeon, South Korea, in 2019 and 2024, respectively. His research interests include AI ML hardware accelerator (ASIC FPGA) design, efficient AI ML system design, energy-efficient processing-in near- memory architecture, and hardware software code- sign for DNN processing. Joo-Young Kim Joo-Young Kim (Senior Member, IEEE) received the B.S., M.S., and Ph. D degrees in electrical engineering from Korea Advanced Insti- tute of Science and Technology (KAIST), Daejeon, South Korea, in 2005, 2007, and 2010, respectively. He is currently an Associate Professor in the School of Electrical Engineering, KAIST, and the Director of the AI Semiconductor Systems Research Center, KAIST. His research interests span various aspects of hardware design, including chip design, com- puter architecture, domain-specific accelerators, and hardware software co-design. Before joining KAIST, he was a Hardware Engineering Leader at Microsoft Azure, Redmond, WA, USA, working on hardware acceleration for cloud services such as machine learning, data storage, and networking. He founded an AI fabless startup, HyperAccel, in Jan 2023 to build innova- tive AI processors solutions for large-language-model(LLM)-based generative AI, making it sustainable for everyone.\n\n