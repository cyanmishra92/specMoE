=== ORIGINAL PDF: 2504.18628v1_Periodic_Online_Testing_for_Sparse_Systolic_Tensor.pdf ===\n\nRaw text length: 34236 characters\nCleaned text length: 34067 characters\nNumber of segments: 18\n\n=== CLEANED TEXT ===\n\nPeriodic Online Testing for Sparse Systolic Tensor Arrays Christodoulos Peltekis Electrical and Computer Engineering Democritus University of Thrace, Greece Chrysostomos Nicopoulos Electrical and Computer Engineering University of Cyprus, Cyprus Giorgos Dimitrakopoulos Electrical and Computer Engineering Democritus University of Thrace, Greece Abstract Modern Machine Learning (ML) applications often benefit from structured sparsity, a technique that efficiently reduces model complexity and simplifies handling of sparse data in hardware. Sparse systolic tensor arrays specifically designed to accelerate these structured-sparse ML models play a pivotal role in enabling efficient computations. As ML is increasingly integrated into safety-critical systems, it is of paramount importance to ensure the reliability of these systems. This paper introduces an online error-checking technique capable of detecting and locating permanent faults within sparse systolic tensor arrays before computation begins. The new technique relies on merely four test vectors and exploits the weight values already loaded within the systolic array to comprehensively test the system. Fault-injection campaigns within the gate- level netlist, while executing three well-established Convolutional Neural Networks (CNN), validate the efficiency of the proposed approach, which is shown to achieve very high fault coverage, while incurring minimal performance and area overheads. Index Terms Error-checking, Fault-tolerance, Structured sparsity, Sparse systolic tensor array I. INTRODUCTION Machine Learning (ML) techniques have enjoyed widespread proliferation in many different fields and are rapidly becoming ubiquitous. However, as ML models grow in size and complexity, they demand significant computational power and memory, making deployment on resource- constrained devices challenging. Model sparsification [1] addresses this issue by reducing the number of weights, keeping only essential non-zero parameters. This approach enhances efficiency, reduces memory footprint, accelerates inference, and lowers energy consumption. These benefits make ML models more practical for real-time applications and deployment on edge devices. There are two main types of sparsity. Under unstructured sparsity [2], there are no constraints on the distribution of the non-zero elements in the input, as abstractly illustrated in Fig. 1(a). Consequently, extensive meta-data and multiple indexes are required to encode the non-zero locations. On the contrary, structured sparsity [3], [4] alleviates the meta-data cost and complexity by enforcing a constraint on the maximum possible number of non-zero elements present in every fixed- size block of consecutive input elements. Structured sparsity is typically defined by the notation N:M, which indicates that in each block of M consecutive elements, at most N may This work was supported by a research grant from Siemens EDA to Democritus University of Thrace for High Level Synthesis Research for Systems-on-Chip Fig. 1. Example of (a) unstructured sparsity; and (b) structured block sparsity of 2:4 (i.e., up to 2 non-zero elements in every 4 consecutive elements in each column) and their respective packed storage with their associated bit masks. be non-zero. Fig. 1(b) shows an example of a 2:4 structured sparse input matrix. The positions of the non-zero elements within each block is decided by the stored 4-bit masks. To accelerate directly in hardware the computations of structured- sparse data, sparse systolic arrays have been proposed, which take full advantage of the unique characteristics of structured block sparsity [5], [6], [7], [8]. The pervasive adoption of ML in safety-critical application domains e.g., automotive [9], medicine [10], aviation [11], etc. elevates the importance of the reliability and or fault- tolerance attributes of the employed hardware accelerators. Typically, fault-tolerant systems rely on some form of fault- detection technique capability that triggers reactive action(s) upon fault detection. Periodic online testing techniques allow for expedited in-the-field fault detection and can be very effective in rapidly detecting faults in mission-critical systems that cannot tolerate delayed or offline fault detection. This paper proposes a novel periodic online self-testing methodology to detect permanent faults in sparse systolic arrays prior to the commencement of the actual calculations by the running application. Hence, a possible fault is identified before any erroneous (thus, wasteful) work is performed. The proposed technique exploits the presence of already-stored data within the array to minimize the number of required test vectors. Faults that occur during the computation can also be detected by Algorithm-Based Fault Tolerance (ABFT) techniques [12], [13], [14], [15], which concurrently perform checks alongside the application computations. Under ABFT, faults are detected after a computation is completed. The proposed technique is inspired by the work in [16], which explores online self-testing in dense systolic array archi- tectures and reuses the stationary weights already loaded into the systolic array for testing purposes. In a similar vein, the arXiv:2504.18628v1 [cs.AR] 25 Apr 2025 Fig. 2. A sparse systolic tensor array that employs the weight-stationary dataflow; i.e., the weights are pre-loaded into the Tensor Processing Elements (TPE) and remain stationary during the operations. The inputs and outputs flow in the horizontal (west-to-east) and vertical (north-to-south) directions, respectively. technique proposed here also reuses the existing weight values, but it targets sparse systolic arrays with their distinctive Tensor Processing Elements (TPE) and attributes that necessitate a more complex testing procedure, as compared to [16]. The contribution of this work can be summarized as follows: By utilizing the weight values of the running application during the testing phase, the presented technique requires merely four test vectors to provide very high coverage against permanent faults, which minimizes the time lost for on-line testing before the initiation of computation. The proposed approach reuses the systolic array s exist- ing storage elements to minimize the incurred hardware overhead, as compared to other self-testing techniques that use scan chains [17]. To assess the achieved fault coverage of the proposed technique, we employ random fault injections at the gate level, while executing three established Convolutional Neural Networks (CNN) applications. The obtained re- sults demonstrate very high achieved coverage across all three benchmarks, thereby validating the effectiveness of the new lightweight online checking mechanism. Ad- ditionally, the incurred overheads to the system perfor- mance and the hardware area are shown to be minimal. II. BACKGROUND: SPARSE SYSTOLIC TENSOR ARRAYS A sparse systolic tensor array computes the matrix product C A W of a dense input matrix A and a structured-sparse weight matrix W. It adopts the same systolic array architecture as a conventional dense array, but, instead of using a scalar Processing Element (PE) as its building block, it employs a more complex Tensor PE, aptly called TPE [5]. Scalar PEs in a Weight-Stationary (WS) dataflow accept a single input value and use a single local (pre-loaded) weight value to perform a Multiply-Accumulate (MAC) operation per cycle. In contrast, Tensor PEs take a block of M consecutive input values and use up to N local weight values to perform up to N multiplications in each MAC operation. This micro-architecture supports N:M structured block sparsity. The architecture of a two- row sparse tensor array is illustrated in Fig. 2. This array is configurable and can support both 2:4 and 1:4 structured sparsities. As shown in the figure, each TPE consists of two weight registers storing the (up to) two non-zero weight elements identified within every four elements of the pruned weight matrix. These weight registers are loaded with the appropriate weights during a distinct weight-loading phase, as dictated by the WS dataflow. During the computation phase, four consecutive input values from the same row of matrix A are fed into every TPE from the West side. To perform matrix multiplication, up to two of the four input elements are selected (using multiplexers) and multiplied with the locally- stored weights. The selection of the suitable input value(s) is facilitated by two 4-to-1 multiplexers that are controlled by the column indexes of the stored weights. The resultant products in each TPE are accumulated downwards across each column of the tensor array. When structured sparsity of 1:4 is enabled, only one of the two multiplexers and multipliers will be activated in each TPE. This process iterates for all incoming rows of matrix A; different rows of A arrive at each TPE in blocks of four consecutive elements, and the stationary weights dictate which ones are selected for computation. III. FAULT DETECTION IN SPARSE TENSOR ARRAYS The first step toward a fault-tolerant sparse tensor array is a fast and light-weight (in terms of both hardware overhead and the impact on application performance) fault-detection mechanism that can trigger an appropriate reaction. We hereby propose an online test technique that periodically checks every time a new group of weights is loaded the systolic array TPEs for permanent errors. For simplicity, let us assume that a permanent fault can only occur within the storage elements, i.e., the registers, of each TPE, as shown in Fig. 3. Hence, a permanent fault may afflict one of the following registers: 1) The so called activation registers, which store the incom- ing blocks of consecutive input elements and propagate them along the horizontal (west-to-east) direction in each row of the array. 2) The N weight registers that store the stationary weights. 3) The weight-index registers that control the multiplexers and select the up to N appropriate input (activation) elements out of the M-element input block. 4) The output registers that store the accumulated sums and propagate them downwards along the vertical direction (north-to-south) in each column of the array. Note that the simplifying assumption that permanent faults may only occur in the above-mentioned registers does not ignore faults manifesting in the remaining logic of a PE. As can be seen in Fig. 3, the micro-architecture of the TPE indicates that such faults will yield an erroneous result in either the multiplication and or the accumulation steps, which will be captured as errors in the output register of each TPE. All paths of the internal TPE logic are funneled into the output register at the bottom right of each TPE. Hence, error coverage of the output register will also detect faults occurring within the remaining TPE logic. Fig. 3. The four types of registers in a single TPE of a sparse systolic array. The proposed online checking mechanism detects and lo- cates permanent (stuck-at) faults through a simple self-testing sequence. The sequence consists of four different tests, exe- cuted consecutively, with each test requiring a single test vec- tor. Said test vectors are carefully crafted to reveal permanent faults occurring in any register within any TPE of the systolic array. The four test vectors use the weight values that are already stored (by the running application) in the array, thereby immensely reducing the test-latency overhead. The weights are not affected in any way and there is never a need to reload any data after a test session concludes. The application simply resumes normal operation. The testing sequence is initiated after the weight-loading phase of each tile1 is completed. At this point, the pre-defined test vectors are fed as inputs to the systolic array and propagate through the array; i.e., they are multiplied-and-accumulated with the local weight values in each TPE along the way, until the final outputs are produced at the bottom of each column of the array (south edge). A single testing session is depicted in Fig. 4 for the case that the inputs to each TPE are fed as groups of M 4 elements. Each test vector is fed into every row of TPEs in the array. For each test vector, there is an accompanying value that must be fed into the sum (adder) input of the top-row TPEs of the array. The test vectors and the corresponding values fed into the top-row TPE adders are shown in Table I. The last two tests use the same test vector. When fed into the array, the test vectors trigger the calcula- tion of a weighted sum of all the weights already stored in the TPEs across each column of the array. This weighted sum is output at the bottom of each column and is then compared with the corresponding golden reference. This comparison is facilitated by the existing accumulators at the south edge of the SA, highlighted in red in Fig. 4. In other words, the comparison is simply an addition operation. Due to the use of the existing, specific weight values, it is not possible to ensure fault coverage for all possible paths in the design, i.e., achieve 100 fault coverage. Nevertheless, if the test response is error free, there is very high certainty that, for the currently-loaded weight tile, there will be no error 1Since the input and weight matrices of ML applications are typically larger than the size of the systolic array, the multiplication is performed gradually and progressively by loading tiles of these matrices, whereby a tile corresponds to the size of the systolic array accelerator. Fig. 4. The four test vectors are applied periodically to the sparse systolic tensor array to detect permanent faults within any one register. TABLE I THE 3 UNIQUE TEST VECTORS EMPLOYED BY THE PROPOSED ONLINE CHECKING METHODOLOGY Test number Test vector Top-row TPE sum input 1 [1, 1, 1, 1] 0 2 [ 1, 1, 1, 1] 1 3,4 [1, 2, 3, 4] 0 affecting the output of the array. In other words, functional cor- rectness is ensured with very high certainty for this particular weight tile. The test must then be repeated when the next tile is loaded, and so forth. Also, the localization granularity is limited to a single column of the systolic array. In other words, the proposed mechanism can locate the faulty column of the array, but it cannot identify the specific TPE where the fault is located. A. Testing for faults in the weight and output registers The first test vector computes the sum V of the weight values across all TPEs in each column j, i.e., Vj P i wij. Index i refers to all the weight registers across column j. For this first test, the golden reference value equals GVj X i wij The minus sign is included so that the addition of the com- puted test sum Vj with the golden reference value GVj should result in a zero value in the absence of a fault within each column j. The second test vector used in the second test of each test session computes the bit-wise complement of the sum of the weight values across all TPEs in each column j, i.e., Vj. This is because the second test subtracts one (the input value fed into the top-row TPE) from the negative sum (due to the 1s in the test vector) of the weights of each column. By the 2 s complement definition, the subtraction of 1 from a negative signed operand effectively computes its bit-wise complement: Vj Vj 1 Vj 1 Vj Thus, when the bit-wise complement of the sum of weights, Vj is added to a golden reference value of GVj X i wij ( error-free Vj) the result should be 1 (i.e., a bit string of all 1s in the output value, since we use 2 s complement arithmetic) in the absence of a fault, since Vj Vj 1. In summary, under fault-free operation, the results of the first test vector at the south edge of the array should be 0 for all the columns, while the results of the second test vector should be 1 (all 1s in the output values). If there is any discrepancy in these first two tests, further checking is needed to locate the fault. As described in [16] for dense systolic arrays, this fault localization is achieved by examining the outputs of Test 1 and Test 2 prior and after the comparison with the golden reference values, and checking whether they are bit-wise complementary, or not. Based on these bit-wise complementary checks, the fault can be localized using the cases depicted in Table II. Note that the localization identifies the register type that is faulty within a particular column of the array, but it cannot identify the specific TPE in the column where the faulty register is located. TABLE II LOCALIZATION OF FAULTS BY CHECKING IF THE SYSTOLIC ARRAY OUTPUTS AND THE COMPARISON (WITH THE GOLDEN REFERENCE) OUTPUTS OF TESTS 1 AND 2 ARE BIT-WISE COMPLEMENTARY Test 1 and 2 Complementary Not Complementary outputs before comparison with Complementary golden reference Test 1 and 2 Complementary Not Not results after comparison with Complementary Complementary golden reference Fault Weight Output Comparison Location register Register Adder B. Testing for faults in the weight-index registers The third test vector as used in the third test of each test session targets potential faults in the weight-index registers of each TPE in the array. The vector computes the weighted sum of the weight values, multiplied by their index position in the M-element block, as follows: Vj X i (idxij wij) Similar to the first test, the golden value reference is the negative of this sum, i.e., GVj Vj Consequently, if a fault occurs in any of the weight-index registers within a column of the array, a wrong element of the input test vector [1, 2, 3, 4] (assuming here that M 4) will be selected to be multiplied with a stationary weight, thereby resulting in an erroneous sum at the bottom of the column. C. Testing for faults in the input activation registers The first three test vectors target faults that affect the vertical flow in the systolic array; i.e., faults that only affect the end result at the bottom of a column. The fourth test vector targets faults within the input activation registers of the horizontal flow within the array (see Fig. 3). This test is more complex in sparse arrays than in dense arrays, since an erroneous activation value due to a fault within an activation register may not be selected for calcu- lation until several columns later. Thus, we introduce another, more sophisticated test, as compared to [16], where a fault in any activation register will be coerced to manifest itself as multiple errors in consecutive columns. This is achieved by utilizing a new control signal (labeled test 4 and shown in red in Fig. 4) and masking gates at the outputs of the weight-index registers (also shown in red in Fig. 4). When the new control signal is de-asserted, the masking gates are transparent, i.e., they let the output of the weight-index register pass through. The control signal is asserted only in the clock cycle that Test 4 is performed and it activates the masking gates. These force the multiplexers to select one specific value from the M-element input block. Specifically, in the first column, the first input element is selected (by the OR masking gate in the top-most position); in the second column, the second input element is selected (the OR gate is one position below), and so forth. This will force the erroneous value to manifest at the output (south edge) of a column periodically, every M columns. That is, errors will be observed in multiple columns, all spaced M columns apart. Such behavior indicates that the fault occurred in an activation register in one of the M columns located just before (to the left of) the first error appearance. IV. EXPERIMENTAL RESULTS The experimental evaluation aims to quantitatively assess: (a) the fault-detection capability of the proposed online check- ing mechanism; (b) the impact on system performance in a fault-free environment; and (c) the hardware area cost. An 8 8 sparse systolic tensor array was fully implemented in SystemVerilog RTL and augmented with the proposed checking mechanism. This systolic array operates on 16- bit integer quantized inputs and weights executing single- batch inference of CNNs that require matrix multiplications of different sizes. To retain accuracy, the additions in each column of the array are performed at a 32-bit width. The array was synthesized with Cadence s digital implementation flow using a 28 nm standard-cell library. It operates at a clock frequency of 1 GHz. Three well-known CNNs were used for all the experiments: ResNet50 [18], DenseNet121 [19] and VGG16 [20]. Since the focus of this evaluation is on fault-detection, the experiments were conducted at the synthesized gate-level netlist. To calculate the achieved fault coverage, the Hope sequential fault-simulator [21] was employed, which performs exhaustive fault simulation at the gate-level of the entire sys- tolic array. Specifically, Hope performs a single-fault injection campaign for each point in the given netlist and checks if Fig. 5. The fault coverage achieved after completion of each layer of ResNet50 [18]. The fault coverage converges quite rapidly to a high value. TABLE III FAULT COVERAGE ACHIEVED BY THE PROPOSED ONLINE TESTING MECHANISM FOR THREE WELL-KNOWN CNN APPLICATIONS AND THE LAYER OF CONVERGENCE FOR EACH APPLICATION. App Total of layers Fault Coverage Convergence layer ResNet50 49 94.2 7 DenseNet121 120 94.3 2 VGG16 13 94.1 5 the application of the applied test-vectors (in our case: the 4 test vectors and all the weight values in the CNN application) produce an output different from the fault-free output. When a new tile of weights is loaded, four distinct tests are performed in consecutive clock cycles using the four proposed test vectors applied as inputs to the array. Consequently, the achieved fault coverage increases progressively as more and more weight tiles are loaded during the execution of the CNN application. Fig. 5 depicts this increase in the achieved fault coverage as all the layers of ResNet50 [18] pass through the systolic array and a testing session is performed on each new weight tile. After the first CNN layer is completed, the fault coverage is quite low, at 88.7 . However, the fault coverage increases rapidly within the first three layers. At the end of the third layer, there is a knee, beyond which the increase in the fault coverage is minimal. A convergence point is reached after completion of the seventh layer, after which the changes in fault coverage are negligible. Table III summarizes the achieved fault coverages and convergence points for all three examined CNN applications. As shown, all three CNN applications converge quite rapidly (after only a few layers) to an average fault coverage of 94.2 . Obviously, since the proposed mechanism uses only 4 test vectors and the existing weights in each loaded weight tile, the achieved gate-level stuck-at fault coverage cannot reach 100 ; the weights are not sufficient to exercise all stuck-at faults. Our observations indicate that the vast majority of uncovered faults are paths located inside the multipliers of each TPE, which are not activated due to the constant weight values. However, as previously mentioned, even if the gate-level fault coverage is not closer to 100 , an error-free test response indicates with very high certainty functional correctness for the currently-loaded weight tile; i.e., there will be no error affecting the output of the array during the execution of this particular weight tile. Fig. 6. The impact on application latency of the proposed periodic online testing methodology. The results are normalized to the latency of the sparse systolic array with no error-checking mechanism. To evaluate the latency overhead introduced by the pro- posed periodic online testing mechanism, the total runtime of each CNN application in the presence of online testing was measured. An online test session was triggered for each newly-loaded weight tile throughout the entire execution of the application. This measured runtime was compared to the runtime achieved without any online testing, i.e., the execution latency achieved on a baseline sparse systolic tensor array. The results for all three CNN applications are shown in Fig. 6. The proposed checking mechanism adds only marginal latency overhead of 0.5 2 to the applications total runtimes. This is due to the use of only 4 test vectors, which merely add a 4-cycle latency overhead for every executed weight tile. The hardware area overhead to support the proposed mech- anism is also minimal. As shown in red in Fig. 2, the extra hardware added comprises the single test 4 control signal, three AND gates and one OR gate per weight-index register in each TPE, and a multiplexer at the input of the bottom accumulators of each column, at the south edge of the SA. The total additional hardware accounts for 3 of the total area of the sparse systolic tensor array. V. RELATED WORK There are numerous efforts aimed at protecting systolic array architectures from faults, which can be categorized into three main approaches: (a) detection-only methods that identify faults and discard faulty results; (b) detection and localization methods that isolate erroneous hardware modules after detecting faults; and (c) detection, localization, and correction methods that also attempt to correct faults for faster system recovery. These fault-tolerant methodologies are applied to dense systolic arrays and vary in complexity and effectiveness. For fault detection, the work in [22] uses connections between weight and activation registers to form new scan- chains, reducing hardware complexity and power consump- tion. Another method [23] employs ABFT to detect faults caused by voltage reductions in low-power systolic arrays, while [24] uses extra accumulators to implement ABFT di- rectly on Intel s Tiled Matrix Multiplication (TMUL) units. In a similar vein, [25] adapts the ABFT methodology to the unique characteristics of sparse systolic tensor arrays and to Graph Convolutional Networks (GCN) [26]. These methods primarily focus on identifying faults without correcting them, thereby preventing faulty results from impacting system oper- ations. More advanced methodologies not only detect, but also localize and correct faults. Techniques like those in [27] and [28] combine fault-aware pruning and retraining to min- imize accuracy degradation and to bypass critical faults in Tensor Processing Units (TPUs). Additionally, the work in [29] tests near-threshold systolic array architectures without ex- tra hardware, and RunSafer [16] uses specific test vectors to protect against permanent faults with minimal latency overhead. Correction methods like STRAIT [17] use self- test and recovery architectures to address faults, employing weight pruning and row column swapping to maintain accu- racy. Another approach, in [30], focuses on error detection and correction in Transformer networks, mitigating out-of-range neuron outputs through saturation or zeroing, thus ensuring minimal accuracy degradation even under high error rates. Finally, the work in [31] introduces fault-injection frameworks to analyze the impact of various factors on fault propagation in systolic arrays, further enhancing fault-tolerance strategies. VI. CONCLUSIONS AND FUTURE WORK This work addresses the need for fault-tolerant ML acceler- ator hardware in safety-critical applications like autonomous vehicles, medicine, and aviation. The proposed periodic online testing targets sparse systolic tensor arrays used for structured- sparse ML models. Each test session requires only four test vectors, the model s existing weights, and simple comparisons with precomputed reference values to detect permanent faults. Unlike ABFT, testing occurs before execution, preventing wasted work. Moreover, the location of any detected fault is also identified at column-level granularity. The experi- mental evaluation demonstrates high stuck-at fault coverage with minimal impact on application performance and minimal additional hardware. As a direction for future work, we plan to enhance the current test-vector set by incorporating a small number of strategically selected random vectors. These will be designed to exercise previously unreachable paths, thereby further improving overall fault coverage without significantly increasing testing overhead. REFERENCES [1] T. Hoefler et al., Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks, The Journal of Machine Learning Research, vol. 22, no. 1, pp. 10 882 11 005, 2021. [2] U. Evci et al., Rigging the lottery: Making all tickets winners, in Inter. Conf. on Machine Learning, Jul. 2020, pp. 2943 2952. [3] A. Mishra and other, Accelerating sparse deep neural networks, arXiv preprint arXiv:2104.08378, 2021. [4] A. Zhou et al., Learning N:M fine-grained structured sparse neural networks from scratch, in Inter. Conf. on Learning Representations (ICLR), May 2021. [5] Z.-G. Liu et al., Systolic tensor array: An efficient structured-sparse gemm accelerator for mobile CNN inference, IEEE Comp. Arch. Letters, vol. 19, no. 1, pp. 34 37, 2020. [6] G. Jeong et al., Vegeta: Vertically-integrated extensions for sparse dense gemm tile acceleration on cpus, in IEEE Inter. Symp. on High- Performance Comp. Arch. (HPCA), Feb. 2023, pp. 259 272. [7] Z.-G. Liu et al., S2TA: Exploiting structured sparsity for energy- efficient mobile CNN acceleration, in IEEE Inter. Symp. on High- Performance Comp. Arch. (HPCA), Apr. 2022, pp. 573 586. [8] C. Peltekis, V. Titopoulos, C. Nicopoulos, and G. Dimitrakopoulos, DeMM: A decoupled matrix multiplication engine supporting relaxed structured sparsity, IEEE Comput. Archit. Lett., p. 17 20, 2024. [9] R. Salay, R. Queiroz, and K. Czarnecki, An analysis of ISO 26262: Using machine learning safely in automotive software, 2017. [10] D. Sarvamangala and R. V. Kulkarni, Convolutional neural networks in medical image understanding: a survey, Evolutionary intelligence, vol. 15, no. 1, pp. 1 22, 2022. [11] L. Ma and S. Tian, A hybrid cnn-lstm model for aircraft 4d trajectory prediction, IEEE access, vol. 8, pp. 134 668 134 680, 2020. [12] K.-H. Huang and J. A. Abraham, Algorithm-based fault tolerance for matrix operations, IEEE Trans. on Computers, vol. C-33, no. 6, pp. 518 528, 1984. [13] Abraham, Banerjee, C.-Y. Chen, Fuchs, S.-Y. Kuo, and N. Reddy, Fault tolerance techniques for systolic arrays, Computer, vol. 20, no. 7, pp. 65 75, 1987. [14] P. Wu, Q. Guan, N. DeBardeleben, S. Blanchard, D. Tao, X. Liang, J. Chen, and Z. Chen, Towards practical algorithm based fault tolerance in dense linear algebra, in Proc. of the ACM Intern. Symp.on High- Performance Parallel and Distributed Computing, 2016, p. 31 42. [15] D. Filippas, N. Margomenos, N. Mitianoudis, C. Nicopoulos, and G. Dimitrakopoulos, Low-cost online convolution checksum checker, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 30, no. 2, pp. 201 212, 2022. [16] E. Vacca, G. Ajmone, and L. Sterpone, Runsafer: A novel runtime fault detection approach for systolic array accelerators, in IEEE Intern. Conf. on Comp. Design (ICCD), 2023, pp. 596 604. [17] H. Lee, J. Kim, J. Park, and S. Kang, Strait: Self-test and self-recovery for ai accelerator, IEEE Trans. on Computer-Aided Design of Integr. Circ. and Syst., 2023. [18] K. He et al., Deep residual learning for image recognition, in IEEE Conf. on Comp. Vision and Pattern Recognition(CVPR), Jun 2016. [19] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, Densely connected convolutional networks, in IEEE Conf. on Comp. Vision and Pattern Recogn. (CVPR), 2017, pp. 4700 4708. [20] K. Simonyan and A. Zisserman, Very Deep Convolutional Networks for Large-Scale Image Recognition, in Inter. Conf. on Learning Repre- sentations (ICLR), 2015. [21] H. K. Lee and D. S. Ha, HOPE: An efficient parallel fault simulator for synchronous sequential circuits, IEEE Trans. on Computer-Aided Design of Integr. Circ.s and Syst., vol. 15, no. 9, pp. 1048 1058, 1996. [22] U. S. Solangi, M. Ibtesam, M. A. Ansari, J. Kim, and S. Park, Test architecture for systolic array of edge-based ai accelerator, IEEE Access, vol. 9, pp. 96 700 96 710, 2021. [23] M. Safarpour, R. Inanlou, and O. Silv en, Algorithm level error detection in low voltage systolic array, IEEE Trans. on Circ. and Syst. II, vol. 69, no. 2, pp. 569 573, 2021. [24] S. Bal, C. S. Mummidi, V. Da Cruz Ferreira, S. Srinivasan, and S. Kundu, A novel fault-tolerant architecture for tiled matrix multi- plication, in Design, Automation Test in Europe (DATE), 2023. [25] C. Peltekis, D. Filippas, and G. Dimitrakopoulos, Error checking for sparse systolic tensor arrays, in IEEE Inter. Conf. on AI Circ. and Syst. (AICAS), 2024. [26] C. Peltekis and G. Dimitrakopoulos, GCN-ABFT: Low-cost online error checking for graph convolutional networks, IEEE Trans. on Computer-Aided Design of Integr. Circ. and Syst., 2024. [27] J. J. Zhang, T. Gu, K. Basu, and S. Garg, Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator, in IEEE VLSI Test Symp. (VTS), 2018, pp. 1 6. [28] M. Sadi and U. Guin, Test and yield loss reduction of ai and deep learning accelerators, IEEE Trans. on Computer-Aided Design of Integr. Circ. and Syst., vol. 41, no. 1, pp. 104 115, 2021. [29] S. Lee, J. Park, S. Park, H. Kim, and S. Kang, A new zero-overhead test method for low-power ai accelerators, IEEE Trans. on Circ. and Syst. II, pp. 1 5, 2023. [30] K. Ma, C. Amarnath, and A. Chatterjee, Error resilient transformers: A novel soft error vulnerability guided approach to error checking and suppression, in IEEE European Test Symp. (ETS), 2023, pp. 1 6. [31] U. K. Agarwal, A. Chan, A. Asgari, and K. Pattabiraman, Towards reliability assessment of systolic arrays against stuck-at faults, in IEEE IFIP Intern. Conf. on Dependable Syst. and Networks (DSN-S), 2023, pp. 230 236.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nPeriodic Online Testing for Sparse Systolic Tensor Arrays Christodoulos Peltekis Electrical and Computer Engineering Democritus University of Thrace, Greece Chrysostomos Nicopoulos Electrical and Computer Engineering University of Cyprus, Cyprus Giorgos Dimitrakopoulos Electrical and Computer Engineering Democritus University of Thrace, Greece Abstract Modern Machine Learning (ML) applications often benefit from structured sparsity, a technique that efficiently reduces model complexity and simplifies handling of sparse data in hardware. Sparse systolic tensor arrays specifically designed to accelerate these structured-sparse ML models play a pivotal role in enabling efficient computations. As ML is increasingly integrated into safety-critical systems, it is of paramount importance to ensure the reliability of these systems. This paper introduces an online error-checking technique capable of detecting and locating permanent faults within sparse systolic tensor arrays before computation begins. The new technique relies on merely four test vectors and exploits the weight values already loaded within the systolic array to comprehensively test the system. Fault-injection campaigns within the gate- level netlist, while executing three well-established Convolutional Neural Networks (CNN), validate the efficiency of the proposed approach, which is shown to achieve very high fault coverage, while incurring minimal performance and area overheads. Index Terms Error-checking, Fault-tolerance, Structured sparsity, Sparse systolic tensor array I. INTRODUCTION Machine Learning (ML) techniques have enjoyed widespread proliferation in many different fields and are rapidly becoming ubiquitous. However, as ML models grow in size and complexity, they demand significant computational power and memory, making deployment on resource- constrained devices challenging. Model sparsification [1] addresses this issue by reducing the number of weights, keeping only essential non-zero parameters. This approach enhances efficiency, reduces memory footprint, accelerates inference, and lowers energy consumption. These benefits make ML models more practical for real-time applications and deployment on edge devices. There are two main types of sparsity. Under unstructured sparsity [2], there are no constraints on the distribution of the non-zero elements in the input, as abstractly illustrated in Fig. 1(a). Consequently, extensive meta-data and multiple indexes are required to encode the non-zero locations.\n\n--- Segment 2 ---\n1(a). Consequently, extensive meta-data and multiple indexes are required to encode the non-zero locations. On the contrary, structured sparsity [3], [4] alleviates the meta-data cost and complexity by enforcing a constraint on the maximum possible number of non-zero elements present in every fixed- size block of consecutive input elements. Structured sparsity is typically defined by the notation N:M, which indicates that in each block of M consecutive elements, at most N may This work was supported by a research grant from Siemens EDA to Democritus University of Thrace for High Level Synthesis Research for Systems-on-Chip Fig. 1. Example of (a) unstructured sparsity; and (b) structured block sparsity of 2:4 (i.e., up to 2 non-zero elements in every 4 consecutive elements in each column) and their respective packed storage with their associated bit masks. be non-zero. Fig. 1(b) shows an example of a 2:4 structured sparse input matrix. The positions of the non-zero elements within each block is decided by the stored 4-bit masks. To accelerate directly in hardware the computations of structured- sparse data, sparse systolic arrays have been proposed, which take full advantage of the unique characteristics of structured block sparsity [5], [6], [7], [8]. The pervasive adoption of ML in safety-critical application domains e.g., automotive [9], medicine [10], aviation [11], etc. elevates the importance of the reliability and or fault- tolerance attributes of the employed hardware accelerators. Typically, fault-tolerant systems rely on some form of fault- detection technique capability that triggers reactive action(s) upon fault detection. Periodic online testing techniques allow for expedited in-the-field fault detection and can be very effective in rapidly detecting faults in mission-critical systems that cannot tolerate delayed or offline fault detection. This paper proposes a novel periodic online self-testing methodology to detect permanent faults in sparse systolic arrays prior to the commencement of the actual calculations by the running application. Hence, a possible fault is identified before any erroneous (thus, wasteful) work is performed. The proposed technique exploits the presence of already-stored data within the array to minimize the number of required test vectors.\n\n--- Segment 3 ---\nHence, a possible fault is identified before any erroneous (thus, wasteful) work is performed. The proposed technique exploits the presence of already-stored data within the array to minimize the number of required test vectors. Faults that occur during the computation can also be detected by Algorithm-Based Fault Tolerance (ABFT) techniques [12], [13], [14], [15], which concurrently perform checks alongside the application computations. Under ABFT, faults are detected after a computation is completed. The proposed technique is inspired by the work in [16], which explores online self-testing in dense systolic array archi- tectures and reuses the stationary weights already loaded into the systolic array for testing purposes. In a similar vein, the arXiv:2504.18628v1 [cs.AR] 25 Apr 2025 Fig. 2. A sparse systolic tensor array that employs the weight-stationary dataflow; i.e., the weights are pre-loaded into the Tensor Processing Elements (TPE) and remain stationary during the operations. The inputs and outputs flow in the horizontal (west-to-east) and vertical (north-to-south) directions, respectively. technique proposed here also reuses the existing weight values, but it targets sparse systolic arrays with their distinctive Tensor Processing Elements (TPE) and attributes that necessitate a more complex testing procedure, as compared to [16]. The contribution of this work can be summarized as follows: By utilizing the weight values of the running application during the testing phase, the presented technique requires merely four test vectors to provide very high coverage against permanent faults, which minimizes the time lost for on-line testing before the initiation of computation. The proposed approach reuses the systolic array s exist- ing storage elements to minimize the incurred hardware overhead, as compared to other self-testing techniques that use scan chains [17]. To assess the achieved fault coverage of the proposed technique, we employ random fault injections at the gate level, while executing three established Convolutional Neural Networks (CNN) applications. The obtained re- sults demonstrate very high achieved coverage across all three benchmarks, thereby validating the effectiveness of the new lightweight online checking mechanism. Ad- ditionally, the incurred overheads to the system perfor- mance and the hardware area are shown to be minimal. II.\n\n--- Segment 4 ---\nAd- ditionally, the incurred overheads to the system perfor- mance and the hardware area are shown to be minimal. II. BACKGROUND: SPARSE SYSTOLIC TENSOR ARRAYS A sparse systolic tensor array computes the matrix product C A W of a dense input matrix A and a structured-sparse weight matrix W. It adopts the same systolic array architecture as a conventional dense array, but, instead of using a scalar Processing Element (PE) as its building block, it employs a more complex Tensor PE, aptly called TPE [5]. Scalar PEs in a Weight-Stationary (WS) dataflow accept a single input value and use a single local (pre-loaded) weight value to perform a Multiply-Accumulate (MAC) operation per cycle. In contrast, Tensor PEs take a block of M consecutive input values and use up to N local weight values to perform up to N multiplications in each MAC operation. This micro-architecture supports N:M structured block sparsity. The architecture of a two- row sparse tensor array is illustrated in Fig. 2. This array is configurable and can support both 2:4 and 1:4 structured sparsities. As shown in the figure, each TPE consists of two weight registers storing the (up to) two non-zero weight elements identified within every four elements of the pruned weight matrix. These weight registers are loaded with the appropriate weights during a distinct weight-loading phase, as dictated by the WS dataflow. During the computation phase, four consecutive input values from the same row of matrix A are fed into every TPE from the West side. To perform matrix multiplication, up to two of the four input elements are selected (using multiplexers) and multiplied with the locally- stored weights. The selection of the suitable input value(s) is facilitated by two 4-to-1 multiplexers that are controlled by the column indexes of the stored weights. The resultant products in each TPE are accumulated downwards across each column of the tensor array. When structured sparsity of 1:4 is enabled, only one of the two multiplexers and multipliers will be activated in each TPE.\n\n--- Segment 5 ---\nThe resultant products in each TPE are accumulated downwards across each column of the tensor array. When structured sparsity of 1:4 is enabled, only one of the two multiplexers and multipliers will be activated in each TPE. This process iterates for all incoming rows of matrix A; different rows of A arrive at each TPE in blocks of four consecutive elements, and the stationary weights dictate which ones are selected for computation. III. FAULT DETECTION IN SPARSE TENSOR ARRAYS The first step toward a fault-tolerant sparse tensor array is a fast and light-weight (in terms of both hardware overhead and the impact on application performance) fault-detection mechanism that can trigger an appropriate reaction. We hereby propose an online test technique that periodically checks every time a new group of weights is loaded the systolic array TPEs for permanent errors. For simplicity, let us assume that a permanent fault can only occur within the storage elements, i.e., the registers, of each TPE, as shown in Fig. 3. Hence, a permanent fault may afflict one of the following registers: 1) The so called activation registers, which store the incom- ing blocks of consecutive input elements and propagate them along the horizontal (west-to-east) direction in each row of the array. 2) The N weight registers that store the stationary weights. 3) The weight-index registers that control the multiplexers and select the up to N appropriate input (activation) elements out of the M-element input block. 4) The output registers that store the accumulated sums and propagate them downwards along the vertical direction (north-to-south) in each column of the array. Note that the simplifying assumption that permanent faults may only occur in the above-mentioned registers does not ignore faults manifesting in the remaining logic of a PE. As can be seen in Fig. 3, the micro-architecture of the TPE indicates that such faults will yield an erroneous result in either the multiplication and or the accumulation steps, which will be captured as errors in the output register of each TPE. All paths of the internal TPE logic are funneled into the output register at the bottom right of each TPE. Hence, error coverage of the output register will also detect faults occurring within the remaining TPE logic. Fig. 3.\n\n--- Segment 6 ---\nFig. 3. The four types of registers in a single TPE of a sparse systolic array. The proposed online checking mechanism detects and lo- cates permanent (stuck-at) faults through a simple self-testing sequence. The sequence consists of four different tests, exe- cuted consecutively, with each test requiring a single test vec- tor. Said test vectors are carefully crafted to reveal permanent faults occurring in any register within any TPE of the systolic array. The four test vectors use the weight values that are already stored (by the running application) in the array, thereby immensely reducing the test-latency overhead. The weights are not affected in any way and there is never a need to reload any data after a test session concludes. The application simply resumes normal operation. The testing sequence is initiated after the weight-loading phase of each tile1 is completed. At this point, the pre-defined test vectors are fed as inputs to the systolic array and propagate through the array; i.e., they are multiplied-and-accumulated with the local weight values in each TPE along the way, until the final outputs are produced at the bottom of each column of the array (south edge). A single testing session is depicted in Fig. 4 for the case that the inputs to each TPE are fed as groups of M 4 elements. Each test vector is fed into every row of TPEs in the array. For each test vector, there is an accompanying value that must be fed into the sum (adder) input of the top-row TPEs of the array. The test vectors and the corresponding values fed into the top-row TPE adders are shown in Table I. The last two tests use the same test vector. When fed into the array, the test vectors trigger the calcula- tion of a weighted sum of all the weights already stored in the TPEs across each column of the array. This weighted sum is output at the bottom of each column and is then compared with the corresponding golden reference. This comparison is facilitated by the existing accumulators at the south edge of the SA, highlighted in red in Fig. 4. In other words, the comparison is simply an addition operation. Due to the use of the existing, specific weight values, it is not possible to ensure fault coverage for all possible paths in the design, i.e., achieve 100 fault coverage.\n\n--- Segment 7 ---\nIn other words, the comparison is simply an addition operation. Due to the use of the existing, specific weight values, it is not possible to ensure fault coverage for all possible paths in the design, i.e., achieve 100 fault coverage. Nevertheless, if the test response is error free, there is very high certainty that, for the currently-loaded weight tile, there will be no error 1Since the input and weight matrices of ML applications are typically larger than the size of the systolic array, the multiplication is performed gradually and progressively by loading tiles of these matrices, whereby a tile corresponds to the size of the systolic array accelerator. Fig. 4. The four test vectors are applied periodically to the sparse systolic tensor array to detect permanent faults within any one register. TABLE I THE 3 UNIQUE TEST VECTORS EMPLOYED BY THE PROPOSED ONLINE CHECKING METHODOLOGY Test number Test vector Top-row TPE sum input 1 [1, 1, 1, 1] 0 2 [ 1, 1, 1, 1] 1 3,4 [1, 2, 3, 4] 0 affecting the output of the array. In other words, functional cor- rectness is ensured with very high certainty for this particular weight tile. The test must then be repeated when the next tile is loaded, and so forth. Also, the localization granularity is limited to a single column of the systolic array. In other words, the proposed mechanism can locate the faulty column of the array, but it cannot identify the specific TPE where the fault is located. A. Testing for faults in the weight and output registers The first test vector computes the sum V of the weight values across all TPEs in each column j, i.e., Vj P i wij. Index i refers to all the weight registers across column j. For this first test, the golden reference value equals GVj X i wij The minus sign is included so that the addition of the com- puted test sum Vj with the golden reference value GVj should result in a zero value in the absence of a fault within each column j. The second test vector used in the second test of each test session computes the bit-wise complement of the sum of the weight values across all TPEs in each column j, i.e., Vj.\n\n--- Segment 8 ---\nFor this first test, the golden reference value equals GVj X i wij The minus sign is included so that the addition of the com- puted test sum Vj with the golden reference value GVj should result in a zero value in the absence of a fault within each column j. The second test vector used in the second test of each test session computes the bit-wise complement of the sum of the weight values across all TPEs in each column j, i.e., Vj. This is because the second test subtracts one (the input value fed into the top-row TPE) from the negative sum (due to the 1s in the test vector) of the weights of each column. By the 2 s complement definition, the subtraction of 1 from a negative signed operand effectively computes its bit-wise complement: Vj Vj 1 Vj 1 Vj Thus, when the bit-wise complement of the sum of weights, Vj is added to a golden reference value of GVj X i wij ( error-free Vj) the result should be 1 (i.e., a bit string of all 1s in the output value, since we use 2 s complement arithmetic) in the absence of a fault, since Vj Vj 1. In summary, under fault-free operation, the results of the first test vector at the south edge of the array should be 0 for all the columns, while the results of the second test vector should be 1 (all 1s in the output values). If there is any discrepancy in these first two tests, further checking is needed to locate the fault. As described in [16] for dense systolic arrays, this fault localization is achieved by examining the outputs of Test 1 and Test 2 prior and after the comparison with the golden reference values, and checking whether they are bit-wise complementary, or not. Based on these bit-wise complementary checks, the fault can be localized using the cases depicted in Table II. Note that the localization identifies the register type that is faulty within a particular column of the array, but it cannot identify the specific TPE in the column where the faulty register is located.\n\n--- Segment 9 ---\nBased on these bit-wise complementary checks, the fault can be localized using the cases depicted in Table II. Note that the localization identifies the register type that is faulty within a particular column of the array, but it cannot identify the specific TPE in the column where the faulty register is located. TABLE II LOCALIZATION OF FAULTS BY CHECKING IF THE SYSTOLIC ARRAY OUTPUTS AND THE COMPARISON (WITH THE GOLDEN REFERENCE) OUTPUTS OF TESTS 1 AND 2 ARE BIT-WISE COMPLEMENTARY Test 1 and 2 Complementary Not Complementary outputs before comparison with Complementary golden reference Test 1 and 2 Complementary Not Not results after comparison with Complementary Complementary golden reference Fault Weight Output Comparison Location register Register Adder B. Testing for faults in the weight-index registers The third test vector as used in the third test of each test session targets potential faults in the weight-index registers of each TPE in the array. The vector computes the weighted sum of the weight values, multiplied by their index position in the M-element block, as follows: Vj X i (idxij wij) Similar to the first test, the golden value reference is the negative of this sum, i.e., GVj Vj Consequently, if a fault occurs in any of the weight-index registers within a column of the array, a wrong element of the input test vector [1, 2, 3, 4] (assuming here that M 4) will be selected to be multiplied with a stationary weight, thereby resulting in an erroneous sum at the bottom of the column. C. Testing for faults in the input activation registers The first three test vectors target faults that affect the vertical flow in the systolic array; i.e., faults that only affect the end result at the bottom of a column. The fourth test vector targets faults within the input activation registers of the horizontal flow within the array (see Fig. 3). This test is more complex in sparse arrays than in dense arrays, since an erroneous activation value due to a fault within an activation register may not be selected for calcu- lation until several columns later. Thus, we introduce another, more sophisticated test, as compared to [16], where a fault in any activation register will be coerced to manifest itself as multiple errors in consecutive columns.\n\n--- Segment 10 ---\nThis test is more complex in sparse arrays than in dense arrays, since an erroneous activation value due to a fault within an activation register may not be selected for calcu- lation until several columns later. Thus, we introduce another, more sophisticated test, as compared to [16], where a fault in any activation register will be coerced to manifest itself as multiple errors in consecutive columns. This is achieved by utilizing a new control signal (labeled test 4 and shown in red in Fig. 4) and masking gates at the outputs of the weight-index registers (also shown in red in Fig. 4). When the new control signal is de-asserted, the masking gates are transparent, i.e., they let the output of the weight-index register pass through. The control signal is asserted only in the clock cycle that Test 4 is performed and it activates the masking gates. These force the multiplexers to select one specific value from the M-element input block. Specifically, in the first column, the first input element is selected (by the OR masking gate in the top-most position); in the second column, the second input element is selected (the OR gate is one position below), and so forth. This will force the erroneous value to manifest at the output (south edge) of a column periodically, every M columns. That is, errors will be observed in multiple columns, all spaced M columns apart. Such behavior indicates that the fault occurred in an activation register in one of the M columns located just before (to the left of) the first error appearance. IV. EXPERIMENTAL RESULTS The experimental evaluation aims to quantitatively assess: (a) the fault-detection capability of the proposed online check- ing mechanism; (b) the impact on system performance in a fault-free environment; and (c) the hardware area cost. An 8 8 sparse systolic tensor array was fully implemented in SystemVerilog RTL and augmented with the proposed checking mechanism. This systolic array operates on 16- bit integer quantized inputs and weights executing single- batch inference of CNNs that require matrix multiplications of different sizes. To retain accuracy, the additions in each column of the array are performed at a 32-bit width. The array was synthesized with Cadence s digital implementation flow using a 28 nm standard-cell library. It operates at a clock frequency of 1 GHz.\n\n--- Segment 11 ---\nThe array was synthesized with Cadence s digital implementation flow using a 28 nm standard-cell library. It operates at a clock frequency of 1 GHz. Three well-known CNNs were used for all the experiments: ResNet50 [18], DenseNet121 [19] and VGG16 [20]. Since the focus of this evaluation is on fault-detection, the experiments were conducted at the synthesized gate-level netlist. To calculate the achieved fault coverage, the Hope sequential fault-simulator [21] was employed, which performs exhaustive fault simulation at the gate-level of the entire sys- tolic array. Specifically, Hope performs a single-fault injection campaign for each point in the given netlist and checks if Fig. 5. The fault coverage achieved after completion of each layer of ResNet50 [18]. The fault coverage converges quite rapidly to a high value. TABLE III FAULT COVERAGE ACHIEVED BY THE PROPOSED ONLINE TESTING MECHANISM FOR THREE WELL-KNOWN CNN APPLICATIONS AND THE LAYER OF CONVERGENCE FOR EACH APPLICATION. App Total of layers Fault Coverage Convergence layer ResNet50 49 94.2 7 DenseNet121 120 94.3 2 VGG16 13 94.1 5 the application of the applied test-vectors (in our case: the 4 test vectors and all the weight values in the CNN application) produce an output different from the fault-free output. When a new tile of weights is loaded, four distinct tests are performed in consecutive clock cycles using the four proposed test vectors applied as inputs to the array. Consequently, the achieved fault coverage increases progressively as more and more weight tiles are loaded during the execution of the CNN application. Fig. 5 depicts this increase in the achieved fault coverage as all the layers of ResNet50 [18] pass through the systolic array and a testing session is performed on each new weight tile. After the first CNN layer is completed, the fault coverage is quite low, at 88.7 . However, the fault coverage increases rapidly within the first three layers. At the end of the third layer, there is a knee, beyond which the increase in the fault coverage is minimal. A convergence point is reached after completion of the seventh layer, after which the changes in fault coverage are negligible. Table III summarizes the achieved fault coverages and convergence points for all three examined CNN applications.\n\n--- Segment 12 ---\nA convergence point is reached after completion of the seventh layer, after which the changes in fault coverage are negligible. Table III summarizes the achieved fault coverages and convergence points for all three examined CNN applications. As shown, all three CNN applications converge quite rapidly (after only a few layers) to an average fault coverage of 94.2 . Obviously, since the proposed mechanism uses only 4 test vectors and the existing weights in each loaded weight tile, the achieved gate-level stuck-at fault coverage cannot reach 100 ; the weights are not sufficient to exercise all stuck-at faults. Our observations indicate that the vast majority of uncovered faults are paths located inside the multipliers of each TPE, which are not activated due to the constant weight values. However, as previously mentioned, even if the gate-level fault coverage is not closer to 100 , an error-free test response indicates with very high certainty functional correctness for the currently-loaded weight tile; i.e., there will be no error affecting the output of the array during the execution of this particular weight tile. Fig. 6. The impact on application latency of the proposed periodic online testing methodology. The results are normalized to the latency of the sparse systolic array with no error-checking mechanism. To evaluate the latency overhead introduced by the pro- posed periodic online testing mechanism, the total runtime of each CNN application in the presence of online testing was measured. An online test session was triggered for each newly-loaded weight tile throughout the entire execution of the application. This measured runtime was compared to the runtime achieved without any online testing, i.e., the execution latency achieved on a baseline sparse systolic tensor array. The results for all three CNN applications are shown in Fig. 6. The proposed checking mechanism adds only marginal latency overhead of 0.5 2 to the applications total runtimes. This is due to the use of only 4 test vectors, which merely add a 4-cycle latency overhead for every executed weight tile. The hardware area overhead to support the proposed mech- anism is also minimal. As shown in red in Fig. 2, the extra hardware added comprises the single test 4 control signal, three AND gates and one OR gate per weight-index register in each TPE, and a multiplexer at the input of the bottom accumulators of each column, at the south edge of the SA. The total additional hardware accounts for 3 of the total area of the sparse systolic tensor array.\n\n--- Segment 13 ---\n2, the extra hardware added comprises the single test 4 control signal, three AND gates and one OR gate per weight-index register in each TPE, and a multiplexer at the input of the bottom accumulators of each column, at the south edge of the SA. The total additional hardware accounts for 3 of the total area of the sparse systolic tensor array. V. RELATED WORK There are numerous efforts aimed at protecting systolic array architectures from faults, which can be categorized into three main approaches: (a) detection-only methods that identify faults and discard faulty results; (b) detection and localization methods that isolate erroneous hardware modules after detecting faults; and (c) detection, localization, and correction methods that also attempt to correct faults for faster system recovery. These fault-tolerant methodologies are applied to dense systolic arrays and vary in complexity and effectiveness. For fault detection, the work in [22] uses connections between weight and activation registers to form new scan- chains, reducing hardware complexity and power consump- tion. Another method [23] employs ABFT to detect faults caused by voltage reductions in low-power systolic arrays, while [24] uses extra accumulators to implement ABFT di- rectly on Intel s Tiled Matrix Multiplication (TMUL) units. In a similar vein, [25] adapts the ABFT methodology to the unique characteristics of sparse systolic tensor arrays and to Graph Convolutional Networks (GCN) [26]. These methods primarily focus on identifying faults without correcting them, thereby preventing faulty results from impacting system oper- ations. More advanced methodologies not only detect, but also localize and correct faults. Techniques like those in [27] and [28] combine fault-aware pruning and retraining to min- imize accuracy degradation and to bypass critical faults in Tensor Processing Units (TPUs). Additionally, the work in [29] tests near-threshold systolic array architectures without ex- tra hardware, and RunSafer [16] uses specific test vectors to protect against permanent faults with minimal latency overhead. Correction methods like STRAIT [17] use self- test and recovery architectures to address faults, employing weight pruning and row column swapping to maintain accu- racy.\n\n--- Segment 14 ---\nAdditionally, the work in [29] tests near-threshold systolic array architectures without ex- tra hardware, and RunSafer [16] uses specific test vectors to protect against permanent faults with minimal latency overhead. Correction methods like STRAIT [17] use self- test and recovery architectures to address faults, employing weight pruning and row column swapping to maintain accu- racy. Another approach, in [30], focuses on error detection and correction in Transformer networks, mitigating out-of-range neuron outputs through saturation or zeroing, thus ensuring minimal accuracy degradation even under high error rates. Finally, the work in [31] introduces fault-injection frameworks to analyze the impact of various factors on fault propagation in systolic arrays, further enhancing fault-tolerance strategies. VI. CONCLUSIONS AND FUTURE WORK This work addresses the need for fault-tolerant ML acceler- ator hardware in safety-critical applications like autonomous vehicles, medicine, and aviation. The proposed periodic online testing targets sparse systolic tensor arrays used for structured- sparse ML models. Each test session requires only four test vectors, the model s existing weights, and simple comparisons with precomputed reference values to detect permanent faults. Unlike ABFT, testing occurs before execution, preventing wasted work. Moreover, the location of any detected fault is also identified at column-level granularity. The experi- mental evaluation demonstrates high stuck-at fault coverage with minimal impact on application performance and minimal additional hardware. As a direction for future work, we plan to enhance the current test-vector set by incorporating a small number of strategically selected random vectors. These will be designed to exercise previously unreachable paths, thereby further improving overall fault coverage without significantly increasing testing overhead. REFERENCES [1] T. Hoefler et al., Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks, The Journal of Machine Learning Research, vol. 22, no. 1, pp. 10 882 11 005, 2021. [2] U. Evci et al., Rigging the lottery: Making all tickets winners, in Inter. Conf. on Machine Learning, Jul. 2020, pp. 2943 2952. [3] A. Mishra and other, Accelerating sparse deep neural networks, arXiv preprint arXiv:2104.08378, 2021.\n\n--- Segment 15 ---\n2943 2952. [3] A. Mishra and other, Accelerating sparse deep neural networks, arXiv preprint arXiv:2104.08378, 2021. [4] A. Zhou et al., Learning N:M fine-grained structured sparse neural networks from scratch, in Inter. Conf. on Learning Representations (ICLR), May 2021. [5] Z.-G. Liu et al., Systolic tensor array: An efficient structured-sparse gemm accelerator for mobile CNN inference, IEEE Comp. Arch. Letters, vol. 19, no. 1, pp. 34 37, 2020. [6] G. Jeong et al., Vegeta: Vertically-integrated extensions for sparse dense gemm tile acceleration on cpus, in IEEE Inter. Symp. on High- Performance Comp. Arch. (HPCA), Feb. 2023, pp. 259 272. [7] Z.-G. Liu et al., S2TA: Exploiting structured sparsity for energy- efficient mobile CNN acceleration, in IEEE Inter. Symp. on High- Performance Comp. Arch. (HPCA), Apr. 2022, pp. 573 586. [8] C. Peltekis, V. Titopoulos, C. Nicopoulos, and G. Dimitrakopoulos, DeMM: A decoupled matrix multiplication engine supporting relaxed structured sparsity, IEEE Comput. Archit. Lett., p. 17 20, 2024. [9] R. Salay, R. Queiroz, and K. Czarnecki, An analysis of ISO 26262: Using machine learning safely in automotive software, 2017. [10] D. Sarvamangala and R. V. Kulkarni, Convolutional neural networks in medical image understanding: a survey, Evolutionary intelligence, vol. 15, no. 1, pp. 1 22, 2022. [11] L. Ma and S. Tian, A hybrid cnn-lstm model for aircraft 4d trajectory prediction, IEEE access, vol. 8, pp. 134 668 134 680, 2020. [12] K.-H. Huang and J. A. Abraham, Algorithm-based fault tolerance for matrix operations, IEEE Trans. on Computers, vol. C-33, no. 6, pp. 518 528, 1984.\n\n--- Segment 16 ---\n6, pp. 518 528, 1984. [13] Abraham, Banerjee, C.-Y. Chen, Fuchs, S.-Y. Kuo, and N. Reddy, Fault tolerance techniques for systolic arrays, Computer, vol. 20, no. 7, pp. 65 75, 1987. [14] P. Wu, Q. Guan, N. DeBardeleben, S. Blanchard, D. Tao, X. Liang, J. Chen, and Z. Chen, Towards practical algorithm based fault tolerance in dense linear algebra, in Proc. of the ACM Intern. Symp.on High- Performance Parallel and Distributed Computing, 2016, p. 31 42. [15] D. Filippas, N. Margomenos, N. Mitianoudis, C. Nicopoulos, and G. Dimitrakopoulos, Low-cost online convolution checksum checker, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 30, no. 2, pp. 201 212, 2022. [16] E. Vacca, G. Ajmone, and L. Sterpone, Runsafer: A novel runtime fault detection approach for systolic array accelerators, in IEEE Intern. Conf. on Comp. Design (ICCD), 2023, pp. 596 604. [17] H. Lee, J. Kim, J. Park, and S. Kang, Strait: Self-test and self-recovery for ai accelerator, IEEE Trans. on Computer-Aided Design of Integr. Circ. and Syst., 2023. [18] K. He et al., Deep residual learning for image recognition, in IEEE Conf. on Comp. Vision and Pattern Recognition(CVPR), Jun 2016. [19] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, Densely connected convolutional networks, in IEEE Conf. on Comp. Vision and Pattern Recogn. (CVPR), 2017, pp. 4700 4708. [20] K. Simonyan and A. Zisserman, Very Deep Convolutional Networks for Large-Scale Image Recognition, in Inter. Conf. on Learning Repre- sentations (ICLR), 2015.\n\n--- Segment 17 ---\nConf. on Learning Repre- sentations (ICLR), 2015. [21] H. K. Lee and D. S. Ha, HOPE: An efficient parallel fault simulator for synchronous sequential circuits, IEEE Trans. on Computer-Aided Design of Integr. Circ.s and Syst., vol. 15, no. 9, pp. 1048 1058, 1996. [22] U. S. Solangi, M. Ibtesam, M. A. Ansari, J. Kim, and S. Park, Test architecture for systolic array of edge-based ai accelerator, IEEE Access, vol. 9, pp. 96 700 96 710, 2021. [23] M. Safarpour, R. Inanlou, and O. Silv en, Algorithm level error detection in low voltage systolic array, IEEE Trans. on Circ. and Syst. II, vol. 69, no. 2, pp. 569 573, 2021. [24] S. Bal, C. S. Mummidi, V. Da Cruz Ferreira, S. Srinivasan, and S. Kundu, A novel fault-tolerant architecture for tiled matrix multi- plication, in Design, Automation Test in Europe (DATE), 2023. [25] C. Peltekis, D. Filippas, and G. Dimitrakopoulos, Error checking for sparse systolic tensor arrays, in IEEE Inter. Conf. on AI Circ. and Syst. (AICAS), 2024. [26] C. Peltekis and G. Dimitrakopoulos, GCN-ABFT: Low-cost online error checking for graph convolutional networks, IEEE Trans. on Computer-Aided Design of Integr. Circ. and Syst., 2024. [27] J. J. Zhang, T. Gu, K. Basu, and S. Garg, Analyzing and mitigating the impact of permanent faults on a systolic array based neural network accelerator, in IEEE VLSI Test Symp. (VTS), 2018, pp. 1 6. [28] M. Sadi and U. Guin, Test and yield loss reduction of ai and deep learning accelerators, IEEE Trans. on Computer-Aided Design of Integr. Circ. and Syst., vol.\n\n--- Segment 18 ---\nCirc. and Syst., vol. 41, no. 1, pp. 104 115, 2021. [29] S. Lee, J. Park, S. Park, H. Kim, and S. Kang, A new zero-overhead test method for low-power ai accelerators, IEEE Trans. on Circ. and Syst. II, pp. 1 5, 2023. [30] K. Ma, C. Amarnath, and A. Chatterjee, Error resilient transformers: A novel soft error vulnerability guided approach to error checking and suppression, in IEEE European Test Symp. (ETS), 2023, pp. 1 6. [31] U. K. Agarwal, A. Chan, A. Asgari, and K. Pattabiraman, Towards reliability assessment of systolic arrays against stuck-at faults, in IEEE IFIP Intern. Conf. on Dependable Syst. and Networks (DSN-S), 2023, pp. 230 236.\n\n