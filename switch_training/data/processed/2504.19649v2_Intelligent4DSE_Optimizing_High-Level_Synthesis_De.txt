=== ORIGINAL PDF: 2504.19649v2_Intelligent4DSE_Optimizing_High-Level_Synthesis_De.pdf ===\n\nRaw text length: 70836 characters\nCleaned text length: 70237 characters\nNumber of segments: 47\n\n=== CLEANED TEXT ===\n\narXiv:2504.19649v2 [cs.LG] 5 Jun 2025 INTELLIGENT4DSE: OPTIMIZING HIGH-LEVEL SYNTHESIS DESIGN SPACE EXPLORATION WITH GRAPH NEURAL NETWORKS AND LARGE LANGUAGE MODELS Lei Xu, Shanshan Wang Department of Computer Science Shantou University Shantou, China {24lxu, Emmanuel Casseau IRISA Unviersity of Rennes 1 Rennes, France Chenglong Xiao Department of Computer Science Shantou University Shantou, China ABSTRACT High-level synthesis (HLS) design space exploration (DSE) is an optimization process in electronic design automation (EDA) that systematically explores high-level design configurations to achieve Pareto-optimal hardware implementations balancing performance, area, and power (PPA). To opti- mize this process, HLS prediction tasks often employ message-passing neural networks (MPNNs), leveraging complex architectures to achieve high accuracy. These predictors serve as evaluators in the DSE process, effectively bypassing the time-consuming estimations traditionally required by HLS tools. However, existing models often prioritize structural complexity and minimization of training loss, overlooking task-specific characteristics. Additionally, while meta-heuristic al- gorithms are widely used in DSE, they typically require extensive domain-specific knowledge to design operators and time-consuming tuning. To address these limitations, we propose CoGNNs- LLMMH, a framework that integrates a graph neural network with task-adaptive message passing and large language model-enhanced meta-heuristic algorithms. As a predictive model, CoGNNs directly leverages intermediate representations generated from source code after compiler front-end processing, enabling prediction of quality of results (QoR) without invoking HLS tools. Due to its strong adaptability to tasks, CoGNNs can be tuned to predict post-HLS and post-implementation outcomes, effectively bridging the gap between high-level abstractions and physical implemen- tation characteristics. CoGNNs achieves state-of-the-art accuracy in post-HLS QoR prediction, reducing mean prediction errors by 2.8 for latency and 3.4 for resource utilization compared to baseline models. For post-implementation prediction tasks, CoGNNs demonstrates the lowest prediction errors, with average reductions of 17.6 for flip-flop (FF) usage, 33.7 for critical path (CP) delay, 26.3 for power consumption, 38.3 for digital signal processor (DSP) utilization, and 40.8 for BRAM usage. Furthermore, LLMMH variants can generate superior Pareto fronts compared to meta-heuristic algorithms simulated annealing algorithm (SA), genetic algorithm (GA) and ant colony optimization (ACO) in terms of average distance from the reference set (ADRS) with improvements of 89.34 , 85.90 and 88.07 , respectively. Code and models are available at 1 Introduction High-level synthesis (HLS) has emerged as a modern alternative to traditional RTL-based VLSI design. HLS enables users to leverage high-level programming languages, such as C C , for hardware design tasks, offering synthesis options to fine-tune results. Proven in practice, HLS provides numerous advantages in hardware design and has become the mainstream approach, particularly for FPGA implementations. To enhance HLS effectiveness, most EDA tools offer a variety of synthesis options, called "knobs," allowing users to control the synthesis outcomes. In commercial HLS tools, synthesis directives are typically added to the source code as comments or pragmas. By evaluating various Corresponding Author: Chenglong Xiao. Running Title for Header Figure 1: A comprehensive overview of the HLS DSE process, using the atax application from the PolyBench benchmark suite. The HLS tool can be replaced by predictive models. directive combinations, HLS produces a wide range of hardware implementations, facilitating the exploration of trade-offs between conflicting objectives such as performance (latency) and cost (area, power). Fig. 1 presents an overview of high-level synthesis design space exploration. The workflow begins with C C behavioral descriptions augmented with pragma directives and parameterized optimization targets. These quality of results (QoR) metrics (e.g., latency, resource utilization) of specifications can be obtained using HLS tools or machine learning based models. A design space exploration engine then selects novel pragma combinations through multi- objective optimization, generating updated behavioral descriptions for subsequent iterations. This cyclic refinement process continues until the convergence to a Pareto-optimal configuration set that optimally balances competing design constraints. Therefore, accurate QoR prediction of each design and efficient design space exploration are the most crucial tasks in HLS DSE. Regarding the two key tasks involved in HLS DSE, we identify the following critical issues: Problem 1 (High-accuracy QoR prediction): Current QoR prediction methodologies predominantly employ graph representation learning, where pragma-annotated source code is transformed into control-data flow graph (CDFG), followed by sophisticated GNNs architectures extracting topological features for predictive modeling. GNNs based on the message-passing mechanism, commonly termed MPNNs, operate by iteratively updating node features through neighborhood information aggregation. Although these MPNNs demonstrate strong accuracy in conventional classi- fication and regression tasks, their performance in QoR prediction tasks remains limited, often failing to surpass the discriminative power of the 1-Weisfeiler-Lehman (1-WL) algorithm [1]. This limitation comes from their restricted expressiveness, leading to sub-optimal performance in complex prediction tasks [2]. In addition, conventional MPNNs suffer from inherent long-range dependency issues and over-smoothing phenomena [3], severely constraining GNN models accuracy and generalization capabilities in QoR prediction tasks. The problems existing in the above-mentioned MPNNs all seriously affect its QoR prediction accuracy in post-HLS and post-implementation. A critical observation reveals that existing GNN models seldom excel in both post-HLS and post-implementation QoR prediction tasks, inherently linked to the architectural limitations of MPNNs. Problem 2 (Multi-objective design exploration): DSE methods systematically explore the design space to identify Pareto-optimal configurations that achieve optimal trade-offs between competing objectives such as performance and cost. In DSE, meta-heuristic algorithms (genetic algorithms [4], simulated annealing [5], ant colony optimization [6]) have profound applications and have achieved remarkable success. One critical issue lies in the inherent challenge for meta-heuristic algorithms to identify superior design configurations within vast design spaces while exploring 2 Running Title for Header a limited number of design candidates. While traditional meta-heuristics demonstrate satisfactory performance on benchmarks with constrained design spaces, their effectiveness fundamentally degrades when handling exploration tasks encompassing millions or even hundreds of millions of configurations. Under such scale constraints, the optimization objective shifts from identifying the absolute optimal configuration to discovering acceptably competitive solutions within strictly bounded exploration iterations and time budgets. Conventional approaches relying solely on parameter configurations and stochastic operators prove inadequate for achieving viable design solutions through sparse exploration of the combinatorial space. Another critical challenge stems from the parameter sensitivity inherent in metaheuristic algorithms. For instance, in genetic algorithm (GA), improper configuration of crossover and mutation rates can lead to insufficient exploration of subspaces. Similarly, simulated annealing algorithm (SA) requires careful calibration of neighborhood ranges excessively large ranges exacerbate exploration inefficiencies, while overly constrained ranges promote premature convergence to local optima. Ant colony optimization algorithm (ACO) presents greater parametric complexity through pheromone matrix-guided probabilistic path selection mechanisms, which involve more intricate parameter configurations in their probabilistic selection formulae. To address the aforementioned issues, we propose CoGNNs-LLMMH, a unified framework that synergizes task-adaptive graph neural networks [1] with large language model (LLM)-driven meta-heuristic algorithms. Our key contributions are as follows: 1) We adopt a novel graph neural network architecture (CoGNNs) with an adaptive message-passing mechanism as our core prediction models for accurate QoR prediction. Unlike traditional GNNs with fixed message- passing rules, CoGNNs dynamically reconstructs graph topologies based on task objectives, enabling targeted feature propagation and mitigating over-smoothing. The adapted CoGNNs can be tuned to predict both post-HLS and post-implementation QoR metrics with the highest accuracy compared to SOTA works. 2) We propose an LLM-driven meta-heuristic (LLMMH) framework for efficient DSE. The evaluated meta- heuristic algorithms includes GA, SA, and ACO. By leveraging LLMs learning and reasoning capabilities with carefully engineered prompts, LLMMH significantly reduces domain expertise dependency and achieves prominent improvement in Pareto front quality compared to baseline algorithms. Moreover, the proposed LLMMH framework can be adapted to many other meta-heuristic algorithms for HLS DSE. 3) To the best of our knowledge, the proposed LLMMH is the first framework that integrates LLM with meta- heuristic algorithms for HLS DSE. This approach represents a significant shift from prior methods in HLS DSE. Notably, it has demonstrated promising ability to enhance the quality of DSE outcomes. We hope that our findings will inspire further exploration of LLM-based techniques to tackle challenges in HLS DSE. 2 Related work Recent advances in QoR prediction have been driven by machine learning and GNNs. Foundational approaches employ learnable parameters and loss minimization to predict optimal configurations, with notable contributions including systematic regression model analysis for HLS feature selection [7] and accelerator evaluation framework MPSeeker [8]. Makrani et al. [9] employ Minerva, an automated hardware optimization tool that determines near- optimal tool configurations through static timing analysis and heuristic algorithms, optimizing either peak throughput or throughput-to-area ratio. Pyramid framework leverages this database to train an ensemble ML model that maps HLS-reported features to Minerva s optimization outcomes.Building on these foundations, GNN-based methods have emerged as the dominant paradigm, typically converting source code into intermediate representation (IR) graphs for feature extraction exemplified by hybrid control-data flow graphs [10] and hierarchy-aware GNNs [11]. Recent studies demonstrate progressive advancements in GNN-based hardware design optimization. Wu et al. [12] develop performance modeling frameworks using graph neural networks to represent C C programs as computational graphs. Extending this paradigm, Zhao et al. [13] establish GNNHLS as an HLS-specific benchmark with six GNN models across four topological datasets. While Lin et al. [14] propose power-aware edge-centric GNNs with dynamic power modeling through edge neighborhood aggregation, Kuang et al. [15] introduce hierarchical HGP architectures for post-implementation PPA prediction. Complementing these approaches, Sohrabizadeh et al. [16] present HARP s hierarchical graph representation with auxiliary nodes for pragma-optimized HLS designs. DSE is fundamentally formulated as a multi-objective optimization problem (MOOP), where pre-trained prediction models can serve as evaluators to guide heuristic search algorithms toward Pareto-optimal solutions. Wang et al. [17] automate meta-heuristic parameter configuration through behavioral analysis of C-based descriptions, while Goswami et al. [18] demonstrate GBRT-based models achieve comparable accuracy to exhaustive logic synthesis. To enable a precise identification of Pareto-optimal designs, Hong et al. [19] develop contrastive learning to predict the dominance relationships between designs. A reinforcement learning-driven DSE approach is presented in [20]. Yao et al. [21] propose a decomposition based DSE approach to minimize synthesis iterations. To accelerate ASIC DSE, Rashid et 3 Running Title for Header al. [22] introduce FPGA-targeted transfer learning and a dedicated multithreaded parallel HLS design space explorer. Zou et al. [23] propose FlexWalker, a multi-objective HLS design space exploration framework that employs upper confidence bound (UCB)-coordinated heterogeneous regression models for design quality prediction across parameter configurations, augmented with probabilistic sampling and elastic Pareto frontiers to mitigate regression modeling inaccuracies. In summary, SOTA prediction methodologies in HLS depend primarily on GNNs, leveraging their inference capabilities to achieve high-precision predictions on unseen datasets. Dominant GNN architectures that include graph convolutional networks (GCNs) [24], graph attention networks (GATs) [25], and graph isomorphism networks (GINs) [2] are rooted in message-passing mechanisms. However, inherent limitations of MPNNs, such as long-range dependency failures and over-smoothing phenomena, critically degrade prediction accuracy, partially explaining the persistent performance gaps in HLS QoR estimation. The recent GNN architecture CoGNNs [1] demonstrates adaptive capability with dynamic message-passing mechanism, which is particularly beneficial for HLS prediction tasks. In this work, we adopt CoGNNs for predicting post-HLS metrics. Meta-heuristics have been the predominant approaches and have achieved significant progress in solving HLS DSE over the past decades, and they continue to undergo diverse and flourishing development. The main problems of the traditional meta-heuristic algorithms lie in the difficulty of the designed random operator to escape the local optimum and the numerous difficulties in parameter tuning. Inspired by the works proposed in [26, 27], we propose a LLM-driven metaheuristic framework for HLS DSE, which takes advantage of LLMs powerful information comprehension capabilities to interpret the functional roles of various pragmas and their impacts on the final routing outcomes. 3 Preliminaries 3.1 Problem Formulation HLS DSE can be formulated as a MOOP, where the primary objective is to identify a set of Pareto-optimal configurations that simultaneously minimize latency and resource utilization. Given a behavioral description and optional synthesis directives [pragma1, pragma2, ..., pragman], the design space DS can be formulated as the Cartesian product of the combination of pragmas. The definition of Pareto frontier Pf is as follows [28]: A(df) A(di) and L(df) L(di) (1) where df Pf, di DS and Pf DS. The function A( ) and L( ) represent the resource utilization and the latency of df and di. A design configuration dj Pf is considered Pareto-optimal if no other configuration in the search space dominates it, that is, no alternative design simultaneously achieves both lower area utilization and reduced latency. Therefore, our objective is to efficiently identify the Pareto-optimal set without resorting to exhaustive searches across the entire configuration space and invoking computationally expensive HLS tools. 3.2 Graph Representation of Source Code To facilitate GNNs in capturing the features of the source code, we construct hierarchical graph representations that preserve both syntactic structures and semantic dependencies. A prevalent approach nowadays is to use LLVM [29] to convert the source code into IR, and then utilize ProGraML [30] to transform the IR into a control data flow graph. LLVM instructions encompass numerous low-level operations such as addition operations and memory reading operations. These operations are converted into certain nodes in the CDFG. If an operation needs to be performed on a certain variable, the corresponding nodes will be connected through some directed edges. 3.3 Graph Neural Network A simple graph neural network extracts the features of each node during the training process and aggregates the messages from its neighbors to update the node features. Specifically, at GNN s l-th layer, node v s feature representation hl v will be updated by aggregating (AGG) node v s feature representation hl 1 v in the previous layer and all the neighbor messages hl 1 w , where w N(v) and N(v) is a neighbor set of node v. To make this process learnable, the result obtained above needs to be multiplied by the learnable weight matrix W l 1. hl v σ W l 1hl 1 v W lψ hl 1 w , w N(v) (2) where σ is a activation function to provide non-linearity and ψ is aggregation function like mean, W l 1 and W l are learnable matrices. 4 Running Title for Header 3.4 Gumbel-softmax Estimator In CoGNNs [1], the action network dynamically adjusts the node states based on task-specific characteristics, enabling adaptive reconstruction of the computational graph topology. Specifically, the action network performs a node-level classification task to predict discrete state assignments, where the optimal state is selected via argmax over the predicted probabilities. However, since node states follow a categorical distribution, this prediction task is inherently non- differentiable. To address this, CoGNNs employs the Gumbel-Softmax estimator [31], which provides a continuous relaxation of the discrete state selection process, thereby enabling end-to-end gradient-based optimization. This approach ensures both the dynamic adaptability of the graph structure and the trainability of the overall framework. This estimator approximates the stochastic action sampling in a differentiable and continuous manner. We consider a set of actions Υ, which can be represented as a probability vector p R Υ and the vector p stores the probability values of different actions. Let q be a categorical variable with class probabilities p1, p2, ..., p Υ and we assume that the categorical samples are encoded as k-dimensional one-hot vectors. The Gumbel-Max is a special reparametrization trick that provides an efficient way to draw samples q from a categorical distribution with class probabilities p : q onehot argmax k (gk log pk) (3) where g1, g2, ..., g Υ are samples drawn from Gumbel(0, 1). We can use the softmax function to replace the non- differentiable argmax function to generate the Gumbel-softmax scores sk and τ is the softmax temperature: sk exp ((gk log pk) τ) P Υ i 1 exp ((gi log pi) τ) (4) By incorporating the Gumbel-Softmax estimator, the action network becomes fully differentiable, enabling its seamless integration into the gradient-based optimization process. This differentiability allows CoGNNs to dynamically adapt node-level actions such as edge pruning or feature aggregation based on task-specific objectives. Consequently, the framework learns data graph features in a topology-aware manner, capturing both local structural patterns and global semantic dependencies. This adaptive capability is particularly advantageous for HLS prediction tasks, where design configurations exhibit heterogeneous and hierarchical characteristics. 3.5 Meta-heuristic Algorithms for DSE In DSE, GA [4] are widely adopted as the optimization backbone, where each design configuration is encoded as a gene and the complete set forms a chromosome Cr. The exploration begins with an initial population of parent configurations, which undergo crossover and mutation operations guided by predefined probabilities to generate offspring by modifying pragma values (e.g., loop unrolling factors, array partitioning schemes). SA [6] simulates the natural annealing process by dynamically calculating the probability of accepting either inferior solutions or neighboring configurations during each iteration, with acceptance criteria governed by current temperature parameters. ACO [5] emulates ant colony behavioral mechanisms by constructing pheromone matrices for potential pragma values. During each iteration, it probabilistically selects pragma configurations through pheromone-guided stochastic sampling using sophisticated probabilistic models, thereby generating solutions with enhanced quality potential. 4 Methodology Fig. 2 provides an overview of the proposed CoGNNs-LLMMH. The proposed framework integrates four key stages: dataset generation, model training, inference, and DSE. During dataset generation, labels are extracted from both HLS reports (e.g., loop initiation intervals, resource estimates) and implementation reports (e.g., post-place-and-route resource usage, timing closure) to enhance prediction accuracy. For model training, a subset of the dataset is used to train CoGNNs composed of an environment network (global context capture), an action network (dynamic topology adjustment), and a multi-layer perceptron (QoR metric mapping) which iteratively improves prediction accuracy through hierarchical feature extraction. In the inference stage, the trained CoGNNs model predicts QoR metrics for unseen configurations, enabling rapid evaluation without invocations of the HLS tool. Finally, the DSE stage employs modified meta-heuristic algorithms enhanced by LLMs to identify near-Pareto-optimal solutions. 4.1 Dataset Generation The C C source code is first converted to an intermediate representation (IR) using LLVM, followed by the generation of a CDFG through ProGraML [30], which incorporates HLS directives for enhanced semantic representation. The 5 Running Title for Header Figure 2: The CoGNNs-LLMMH framework integrates data generation (combining HLS reports and implementation reports), predictive model training via task-adaptive graph neural networks, and DSE using an LLM-enhanced evolutionary algorithm. The CoGNNs can be tuned to predict post-HLS QoR metrics and post-implementation QoR metrics. CDFG generated through this compilation process serves as the foundational input to the CoGNNs prediction model. Each design configuration is then synthesized using an HLS tool to extract latency and resource utilization metrics. Although HLS reports provide reasonably accurate estimates, we further refine the dataset by incorporating post- implementation metrics, resulting in a more realistic and comprehensive training set. This dual-source labeling strategy combining HLS and implementation reports ensures high-fidelity ground truth for model training. We train our prediction model on datasets from [15] and [32]. For the GNN-DSE dataset [32], the labels for each benchmark are extracted from the HLS reports, and the CDFGs are generated by the compilation front-end. For the HGBO-DSE dataset [15], the labels are derived from the implementation reports, and the CDFGs come from the HLS process. The two aforementioned datasets differ not only in their label sources but also in the benchmarks used for their construction. By conducting experiments on two distinct datasets, we can comprehensively validate the performance of CoGNNs and evaluate its efficiency in handling complex tasks. 4.2 Cooperative Graph Neural Networks The approach proposed in [1] introduces a dynamic message-passing mechanism by assigning each node one of four states Standard (S), Listen (L), Broadcast (B), or Isolate (I), which govern information flow: nodes in the Standard state both receive and propagate information, Listen nodes only receive, Broadcast nodes only propagate, and Isolate nodes remain inactive. This state-based approach transforms node feature updates into a two-step process: 1) each node selects its optimal state based on task-specific objectives, and 2) updates its features accordingly. As illustrated in Fig. 3, CoGNNs implements this mechanism through dual MPNNs: an action network that determines node states and an environment network that performs context-aware feature aggregation. A complete CoGNN (π, η) model consists of environment networks π and action networks η. These two MPNNs, working in tandem, can better adapt to multi-target prediction tasks. Action networks π help each node determine the 6 Running Title for Header Figure 3: An abstract illustration of the CoGNNs architecture, where a dedicated MLP is constructed for each prediction target. way information flows in and out, while environment networks η update the features of each node according to the state of the node. Therefore, CoGNNs update the feature representation hv of each node v according to the following steps. To begin with, the action networks π will obtain the action of each node. For a set of actions Φ {S, L, B, I}, the action of node v can be drawn from a categorical distribution pv R Φ by aggregating the information of its neighbors N(v). pv π (hv, N(v)) (5) Next, we input the probability vector pv into the Gumbel-softmax Estimator to obtain the action of the node av. We can use environment networks η to update the feature representation hv and obtain h v. h v η (hv, N) (6) where N is related to the value of av: N {hu u N(v), au S B} , av L S {} , av B I (7) For the environment and action networks, we employ a combination of aggregation models (e.g., MEANGNN, SUMGNN) to iteratively update node feature representations. To construct the graph-level embedding, we replace CoGNNs direct node feature summation with an attention-based mechanism [33], which assigns importance scores tv to each node v based on its relevance to the HLS prediction task. This approach captures the heterogeneous contributions of nodes (e.g., loop headers vs. memory operations) and computes the final graph-level vector hG by weighted summation. hG n X i 1 expMLP1(hi) Pn j 1 expMLP1(hj) ! MLP2 (hi) (8) where n is the total number of nodes in the data graph, and MLP1 and MLP2 are used to map hi and hj to R. In the prediction phase, we set up MLPs for each prediction target. Compared to SOTA works such as [15], this method only requires training one prediction model to predict all targets. 4.3 Large Language Model aided Meta-Heuristic Algorithms Traditional meta-heuristic algorithms, such as GA, SA and ACO, have demonstrated strong performance in DSE. Meta-heuristic algorithms, however, suffer from inherent limitations including intricate parameter configurations and suboptimal performance on benchmarks with expansive design spaces. To address this limitations, we leverage large language models (LLMs) to assist meta-heuristic algorithms in generating offspring. Fig. 4 illustrates the proposed LLM-driven meta-heuristic framework for HLS DSE. The proposed LLMMH framework consists of two main components: 1) the general process of meta-heuristics (the lower section of Fig. 4) 2) generating new solutions using LLM (the upper section of Fig. 4). LMMH begins by randomly generating a set of initial solutions to create the first population using LLM with a tailored prompt. It then follows an iterative update process. In each iteration, LMMH constructs a prompt to guide the LLM in generating offspring solutions. These offspring solutions are then evaluated using trained GNN models, and we select top solutions from the offspring solutions and the current population as new 7 Running Title for Header Figure 4: An overview of LLMMH framework for HLS DSE. The LLMMH framework integrates prompt design, which includes task descriptions, solution examples, and task instructions, along with an LLM-enhanced workflow. This workflow consists of initialization, LLM-guided solution generation, and GNN-based evaluation. In this study, we present and evaluate three LLM-driven meta-heuristics: LLMGA, LLMSA, and LLMACO. The LLMMH framework can be adapted to various other meta-heuristic algorithms to support efficient HLS DSE. population for the next iteration. The process continues until a predefined number of iterations is reached, and finally, the Pareto solutions found are returned. In the proposed framework, we carefully design prompts tailored to the dataset characteristics and incorporate the Few-Shot method to guide the LLM in performing generating the initial population and new offspring for meta-heuristic algorithms, which are one of the most important steps involved in meta-heuristic algorithms. LangChain2, an open- source framework, facilitates the development of applications utilizing LLMs by offering tools, components, and interfaces for integrating LLMs with external data sources and services. We employ LangChain to configure our LLMs, necessitating prompt designs that adhere to its format. The prompts are categorized into system messages and input messages. The system messages inform the LLMs about the task s content and requirements, while the input messages transmit data for each LLM API call. Within the system messages, we design three sections: task description, solution examples, task instruction. The task description section briefly introduces HLS DSE background to activate LLMs relevant knowledge modules. Solution examples provide input-output instances to constrain LLMs output formatting. Task instructions guide LLMs in solution generation, maintaining direct correspondence with pseudocode components. Since LangChain is used to invoke the LLM API, the input messages are designed to include only parameters relevant to the design space. Algorithm 1 provides an overview of the proposed LLM-enhanced genetic algorithm (LLMGA). The inputs of the algorithm include the parameters required for LLM prompts and those related to the design space, while the output is the Pareto-optimal set Pest. First we construct prompts based on parameters N and V (lines 2-3), then instruct 2 8 Running Title for Header Algorithm 1 LLMGA algorithm. Input: Design space ds, design space size S, pragma possible values Vp, maximum number of explorations Nse, population size N, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t; Output: Pareto optimal solution set Pest; 1: n 1, nost 0; 2: prompts employing prompt engineering techniques based on N, Vp to establish LLM prompt; 3: P use prompts to instruct LLM to initialize N solutions; 4: Fitness Evaluator (P, code); 5: while n Nse do 6: prompts employing prompt engineering techniques based on N, Vp, Fitness, P to establish LLM prompt; 7: Pnew use prompts to instruct LLM to generate new offspring solutions; 8: P, Fitness, nost Selector(P, Pnew, code, nost); 9: if nost Nost, t 0 then 10: t t vd ; 11: end if 12: n n N ; 13: end while 14: Pest the best solutions in P; 15: return Pest Algorithm 2 LLMSA algorithm. Input: Design space ds, design space size S, pragma possible values Vp, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t, population size N, initial temperature Ti, stop iteration temperature Ts, cooling rate rcs; Output: Pareto optimal solution set Pest; 1: nt Ti, nost 0; 2: P, Fitness establish prompt to instruct LLM to initialize N solutions and evaluate each solution in P; 3: while nt Ts do 4: Ptemp, fitness, nost establish LLM prompt to instruct LLM to generate neighbor solutions Pnew, evaluate each solution in Pnew and select the top solutions into Ptemp; 5: Cost Fitnessavg fitnessavg; 6: if Cost then 7: P Ptemp, Fitness fitness with Prob 1 exp Cost nt ; 8: else 9: P Ptemp, Fitness fitness; 10: end if 11: if nost Nost, t 0 then 12: t t vd ; 13: end if 14: nt nt 1 rcs ; 15: end while 16: Pest the best solutions in P; 17: return Pest LLMs to generate an initial population, which is evaluated by a pre-trained model to obtain Fitness (line 4). The process then enters an iterative phase: prompts are dynamically rebuilt using current P and Fitness to guide LLMs in generating new solutions Pnew. The exploration engine employs the pre-trained predictive models to assess the Fitness of each new solution in the current population and selects top configurations, updating both the current population P and returning the fitness of population Fitness (line 8). The fitness is primarily composed of two key metrics: estimated latency derived from critical path analysis and resource utilization metrics including LUT, DSP, FF and BRAM usage. The temperature parameter in LLMs governs the stochasticity of text generation during sampling: elevated temperature values induce higher randomness in output generation, while reduced temperature settings promote deterministic responses. This mechanism directly mirrors the exploration-exploitation trade-off inherent in DSE optimization processes. To better balance the exploration-exploitation trade-off, we implement an adaptive temperature mechanism specifically through dynamic updating of LLMs temperature parameters (lines 9-11) [34]. This process 9 Running Title for Header Algorithm 3 LLMACO algorithm. Input: Design space ds, design space size S, pragma possible values Vp, maximum number of explorations Nse, the number of ants Nant, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t, pheromone evaporation ratio ρ; Output: Pareto optimal solution set Pest; 1: n 1, nost 0; 2: Initializes a pheromone matrix Pm based on the shape of Vp; 3: P, Fitness establish prompt to instruct LLM to initialize N solutions and evaluate each solution in P; 4: while n Nse do 5: P, Fitness, nost establish LLM prompt to instruct LLM to generate new solutions Pnew based on pheromone matrix Pm, evaluate each solution in Pnew and select the top solutions into P; 6: Pheromone evaporation Pm ρ Pm; 7: Update pheromone matrix Pm τi(c) τi(c) 0.1; 8: if nost Nost, t 0 then 9: t t vd ; 10: end if 11: n n Nant ; 12: end while 13: Pest the best solutions in P; 14: return Pest repeats until the loop terminates, at which point the solutions in P are added to Pest, and the Pareto-optimal solution set Pest is returned (lines 14-15). Algorithm 2 presents an overview of LLM-enhanced simulated annealing algorithm (LLMSA). LLMSA maintains fundamental alignment with LLMGA for integrating LLM in lines 2 and 4 (the only difference is that we instruct LLM to generate neigbor solutions as new solutions). The algorithm implements a conditional check on value Cost: if Cost 0, it computes the probability for potentially accepting inferior solutions. if Cost 0, immediate acceptance occurs (indicating newly generated solutions surpass the current optimal set). This dual-branch mechanism enables dynamic balance between exploration ( Cost 0 cases) and exploitation ( Cost 0 scenarios) during optimization iterations(lines 5-10). The probability formula in line 11 exhibits temperature-dependent behavior: acceptance probability for inferior solutions increases with higher temperatures and decreases with lower thermal states. Algorithm 3 presents an overview of LLM-enhanced ant colony optimization algorithm (LLMACO). To preserve ACO s core characteristics, the algorithm 3 retains the pheromone matrix mechanism. It initializes a pheromone matrix Pm encoding concentration values for each pragma option (line 2). While maintaining similar LLMGA operations, the prompt construction phase explicitly injects pheromone matrix data to LLMs (lines 3-5). Before each iteration concludes, pheromone evaporation and concentration updates are applied to values in current elite solutions (lines 6-7). Subsequent procedures mirror LLMGA, crucially replacing traditional ACO s probabilistic selection formulas (requiring complex computations) with LLM-driven decision-making. 5 Experiments 5.1 Experimental Setup In the experiments, the datasets are sourced from SOTA works GNN-DSE [32] and HGBO-DSE [15]. The GNN-DSE dataset, containing latency and resource usage labels, is extracted from HLS reports, while the HGBO-DSE dataset with PPA results is derived from post-implementation reports. Since the proposed method can be used for predicting both the post-HLS QoRs and the post-implementation QoRs, we compare it with SOTA works on the aforementioned two datasets respectively. To evaluate the proposed DSE algorithm, we use the most accurate prediction model based on experimental evaluation as the estimator for DSE, which collaborates with LLMMH in Section 4.3 to ultimately obtain the Pareto-optimal solution set. The applications in the GNN-DSE dataset are sourced from MachSuite [35] and Polyhedral [36] benchmarks, which involve 7 applications for training, testing, and validation and 5 for DSE exploration, while the HGBO-DSE dataset includes 10 applications from MachSuite, with 6 for training, testing, and validation and 4 for inference. The experiments are conducted on the AMD Ultrascale MPSoC ZCU104. The benchmarks are synthesized using Vitis-HLS 2022.1 and Vivado 2022.1 to collect ground-truth latency and resource utilization metrics, which serve as training labels for the model. 10 Running Title for Header Table 1: The training applications used for post-HLS QoR prediction. Kernel name Description Pragmas Design configs aes a common block cipher 3 45 gemm-blocked A blocked version of matrix multiplication 9 2,314 gemm-ncubed O(n3) algorithm for dense matrix multiplication. 7 7,792 nw A dynamic programming algorithm for optimal sequence alignment 6 15,288 spmv-crs Sparse matrix-vector multiplication, using variable-length neighbor lists 3 114 spmv-ellpack Sparse matrix-vector multiplication, using fixed-size neighbor lists 3 114 stencil3d A three-dimensional stencil computation 7 7,591 Table 2: RMSE loss of different CoGNNs on unseen applications. The top models are marked in bold. Model Latency LUT DSP FF BRAM All CoGNNs(α,α) 0.4315 0.0043 0.0114 0.0155 0.0270 0.4897 CoGNNs(α,β) 0.7101 0.0084 0.0141 0.0279 0.0333 0.7937 CoGNNs(β,β) 0.5077 0.0053 0.0091 0.0146 0.0353 0.5720 CoGNNs(β,α) 0.3557 0.0039 0.0075 0.0152 0.0244 0.4067 CoGNNs(γ,β) 0.6748 0.0067 0.0167 0.0229 0.0331 0.7541 CoGNNs(γ,γ) 0.5825 0.0063 0.0126 0.0252 0.0324 0.6592 CoGNNs(α,γ) 0.4131 0.0174 0.0276 0.1193 0.0297 0.6072 CoGNNs(β,γ) 0.6456 0.0112 0.0248 0.0434 0.0346 0.7596 Both datasets are randomly divided into 70 for training, 15 for testing, and 15 for validation. In the experiments, CoGNNs environment network has three layers and the action network has two layers, with MEANGNN, SUMGNN, and GCN as the main GNN types selectable for both networks, from which the most suitable combination for the QoR prediction task is chosen. Meanwhile, the hidden layer dimension of all models is set to 128, and the Adam optimizer is used, with the GNN-DSE dataset experiments use a batch size of 64, a learning rate of 0.001, and over 500 iterations, while the HGBO-DSE dataset experiments employing a batch size of 128, a learning rate of 0.001, and over 250 iterations. To be consistent with GNN-DSE [32] and HGBO-DSE [15], Root Mean Squared Error (RMSE) loss is used to quantify the post-HLS QoR prediction quality, and Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) are used to evaluate the accuracy of the models for the post-implementation QoR prediction. The CoGNNs model proposed in this paper, along with the baseline models and the DSE algorithms, are all implemented in Python. 5.2 Evaluation of Post-HLS QoR Prediction Accuracy In this experiment, to validate the prediction accuracy of CoGNNs for estimating post-HLS QoR metrics, we lever- age the architectural flexibility of CoGNNs to design eight combinations of environment and action networks for predicting latency, LUT, DSP, FF and BRAM usage, comparing their performance against baseline models. For clarity, we denote MEANGNNs [37], SUMGNNs [37], and GCN [24] as α, β, γ, respectively, to describe different CoGNNs configurations. The SUMGNNs and MEANGNNs mentioned here are two fundamental variants of MPNNs that differ in their neighborhood aggregation functions, with SUMGNNs using summation-based aggregation and MEANGNNs employing mean-based aggregation. Additionally, we evaluate baseline models including GNN-DSE [32], HGP SAGE GF [15], IronMan-Pro [20], PNA-R [12], and PowerGear [14]. While PNA-R and PowerGear are primarily designed for post-implementation prediction tasks (PowerGear proposes GNN to predict power), we adapt these baseline models for post-HLS scenarios by modifying their input dimensions to align with the feature space of the GNN-DSE dataset and restructuring their architectures using the PyTorch Geometric framework. These adaptations enable competitive performance in post-HLS prediction tasks, demonstrating the models flexibility across different hardware abstraction levels. As shown in Table 1, the HLS prediction dataset primarily comprises benchmarks such as aes and gemm-blocked. GNN-DSE [32] employs a Redis-based service to implement an online dataset provisioning mechanism, ensuring strict separation between training and inference datasets while maintaining benchmark diversity across both data splits. As shown in Table 2, we conducted extensive experiments on this dataset and recorded the RMSE of models. RMSE is quared-error metric measuring prediction accuracy via root-averaged discrepancies between predicted and actual values, preserving data units. For latency prediction, CoGNNs(β, α) achieves the best performance, followed by CoGNNs(α, γ) with prediction errors of 0.3557 and 0.4131, respectively. In contrast, CoGNNs predicts multiple targets within an integrated framework, achieving high accuracy overall. For DSP and FF resource prediction, CoGNNs(β, α) 11 Running Title for Header Figure 5: Reduction rates of RMSE loss achieved by CoGNNs(β,α) over other CoGNNs (For FF, we present the reduction rates of RMSE loss achieved by CoGNNs(β,β) over other CoGNNs). The bar chart quantifies each model s prediction error reduction ratios (PERR) relative to the top-performing model, while the line chart tracks the evolving PERR trend across evaluation metrics. Table 3: RMSE loss of CoGNNs(β,α) and SOTA models on unseen applications. The top models are marked in bold. Model Latency LUT DSP FF BRAM All GNN-DSE[32] 0.5359 0.0762 0.1253 0.0632 0.0515 0.8521 HGP SAGE GF[15] 0.9519 0.0152 0.0270 0.0895 0.0362 1.1197 IronMan-Pro[20] 0.6778 0.0081 0.0161 0.0399 0.0326 0.7745 PNA-R[12] 1.3728 0.0144 0.0307 0.0811 0.0610 1.5600 PowerGear[14] 1.3956 0.0177 0.0290 0.0908 0.0432 1.5764 CoGNNs(β,α) 0.3557 0.0039 0.0075 0.0152 0.0244 0.4067 and CoGNNs(β, β) deliver the best results. For the prediction of LUT and BRAM, CoGNNs(β, α)and CoGNNs(β, β) achieve the smallest prediction errors. Although CoGNNs(β, α) has a slightly higher prediction error (0.0006) than CoGNNs(β, β) in FF prediction, its overall error is 0.083 lower than CoGNNs(α, α). Thus, we conclude that CoGNNs(β, α) exhibits the best CoGNNs combination on the GNN-DSE dataset. As empirically validated in Fig. 5, the CoGNN(β, α) demonstrates statistically significant error reduction across multiple prediction targets when compared with other configurations of CoGNNs. In comparison with other combination, CoGNN(β, α) demonstrates statistically significant reductions in prediction errors across all metrics except FF prediction targets. Specifically regarding the Latency metric, CoGNN(β, α) achieves a minimum RMSE reduction ratio of 13.89 . For DSP indicators, the maximum prediction error reduction reaches 72.83 when using CoGNNs. Notably, CoGNNs maintains consistent superiority in LUT and BRAM metrics, exhibiting an average prediction error reduction of 44.57 and 23.64 compared to alternative models. As shown in Table 3, CoGNNs(β, α) also exhibits exceptional prediction capabilities compared to SOTA. Compared to GNN-DSE, HGP SAGE GF, IronMan-Pro, PNA-R, and PowerGear, the reduction rates of RMSE values are 47.69 , 36.32 , 52.51 , 26.07 , and 25.80 of those of the respective baseline models. This superior performance stems from CoGNNs task-specific feature adaptation, as highlighted in its design. All other CoGNNs combinations outperform the baseline models except IronMan-Pro and GNN-DSE. This not only underscores CoGNNs transferability, but also demonstrates its ability to dynamically reconfigure the computational graph topology based on task objectives, enabling more effective feature extraction and improved prediction accuracy. Unlike traditional MPNNs, which focus primarily on minimizing loss during training, CoGNNs prioritizes the extraction of graph-level and node-level features to achieve 12 Running Title for Header Figure 6: Reduction rates of RMSE loss achieved by CoGNNs(β,α) over baseline models. Table 4: The benchmarks used for post-implementation QoR prediction. Kernel name Description Func Array Loop Params aes a common block cipher 12 2 11 30 bfs-bulk Data-oriented version of breadth-first search 1 4 3 25 fft-strided Recursive formulation of the Fast Fourier Transform 1 4 2 27 gemm-ncubed O(n3) algorithm for dense matrix multiplication. 1 3 3 27 md-knn n-body molecular dynamics, using k-nearest neighbors to compute only local forces 1 7 2 42 nw A dynamic programming algorithm for optimal sequence alignment 1 6 7 54 sort-radix Sorts an integer array by comparing 4-bits blocks at a time 7 4 11 72 spmv-ellpack Sparse matrix-vector multiplication, using fixed-size neighbor lists 1 4 2 25 stencil3d A three-dimensional stencil computation 1 2 9 43 viterbi A dynamic programing method for computing probabilities on a Hidden Markov model 1 6 7 52 task objectives, with loss reduction being a natural consequence of this process. As evidenced by Fig. 6, CoGNN architectures demonstrate statistically significant improvements over benchmark baseline models. The CoGNN(β, α) variant exhibits superior performance across all evaluation metrics, particularly achieving at least 33.63 RMSE reduction in latency measurements. With particular strength in hardware utilization metrics, the framework achieves peak error reduction of 94.01 for DSP and mean error reductions of 74.44 (LUT), 77.08 (FF), and 42.78 (BRAM) respectively, outperforming existing approaches in FPGA resource estimation accuracy. In this experiment, the combination of CoGNNs components significantly influences prediction accuracy. We observe that when the environment and action networks utilize simpler MPNNs like SUMGNN and MEANGNN, prediction errors are substantially lower compared to using more complex models such as GCN. This phenomenon is fundamentally tied to CoGNNs operational mechanism(it dynamically selects graph topologies to regulate message flow between nodes). In contrast, complex MPNNs like GCN inherently prioritize the importance of information from neighboring nodes, which contradicts CoGNNs design philosophy. CoGNNs autonomously learns which neighbors information is relevant for each node, effectively disregarding messages from unimportant neighbors. Consequently, integrating MPNNs like GCN, which predefine neighbor importance, results in suboptimal performance within the CoGNNs framework. 5.3 Evaluation of Post-Implementation QoR Prediction Accuracy To further validate the accuracy of the proposed CoGNNs, we also evaluate its performance on post-implementation QoR prediction. For LUT, FF, CP and POWER, we employ MAPE as the loss function. For DSP and BRAM where zero-value occurrences exist, we adopt MAE to avoid division-by-zero errors during model optimization.The HGBO- 13 Running Title for Header Table 5: MAPE and MAE of CoGNN(β, α) and SOTA models on unseen applications. MAPE( ) MAE Model LUT FF CP POWER DSP BRAM HGP SAGE GF[15] 9.05 9.87 9.87 13.60 1.4282 0.1606 IronMan-Pro[20] 7.35 6.30 10.98 16.34 0.6198 0.1917 PowerGear[14] - - - 11.61 - - PNA-R[12] 6.40 6.52 14.03 - - - CoGNNs(β, α) 6.82 5.98 7.54 10.01 0.5339 0.1035 DSE dataset provides labels for LUT, DSP, FF, BRAM, CP and POWER, all extracted from implementation reports. In this experiment, we compare the MAPE and MAE of CoGNNs(β, α) on the GNN-DSE dataset against baseline models including HGP SAGE GF, IronMan-Pro, PowerGear and PNA-R. It is important to note that PowerGear was initially designed to predict power, while PNA-R was created to predict metrics such as LUT, FF, and CP. In accordance with the experiments conducted in [15], we only present the results for the target metrics of PowerGear and PNA-R. Notably, the HGBO-DSE dataset has a node feature dimension of 15, significantly smaller than the GNN-DSE dataset s 153-dimensional features. To account for this, we reduce the number of iterations, while ensuring that each model undergoes at least 250 training epochs. It is worth noting that HGBO-DSE s work does not address and evaluate the model generalization, including for all the reproduced baseline models. In this experiment, we partition the dataset into two subsets comprising 10 applications. As shown in Table 4, six applications (aes, bfs, fft, gemm, md, nw) are used for training, while the remaining applications are reserved as unseen applications for inference. As shown in Table 5, it can be observed that CoGNNs(β,α) achieves the smallest prediction errors for FF, CP, POWER, DSP, and BRAM compared to baseline models, with values of 5.98 , 7.54 , and 10.01 in terms of MAPE, 0.5339 and 0.1033 in terms of MAE, respectively. For LUT prediction, PNA-R exhibits the lowest error at 6.40, while CoGNNs(β,α) closely follows with 6.82, demonstrating a marginal difference. Notably, in POWER prediction, CoGNNs(β,α) reduces the error to 10.01 , outperforming PowerGear, a model specifically designed for power prediction, which achieves 11.61 . The performance of HGP SAGE GF on unseen applications underscores that complex model architectures or mere structural stacking cannot enhance performance in HLS or implementation prediction tasks. In summary, the task-adaptive CoGNN demonstrates exceptional performance in predicting post-place-and-route resource usage and exhibits strong generalization capabilities. In general, as can be seen from Section 5.2 and Section 5.3, CoGNNs(β, α) achieves the best prediction performance on these two datasets. Facts have proven that in the post-HLS QoR prediction and the post-implementation QoR prediction, complex model structures like PNA-R may not yield excellent prediction results. The key lies in improving the model structure according to the task objectives and extracting high-dimensional features from the CDFG, which can lead to better prediction performance. 5.4 Evaluation of Design Space Exploration In the DSE experiment, we applied our proposed LLMMH framework to explore five unseen applications (atax, doitgen, gemm-p, heat-3d, and mvt) that were excluded from the training phase, utilizing the CoGNNs(β, α) model trained in Section 5.3 as the evaluator. For each application, the design configurations encompass all possible combinations of the pragma values and the C C code. We implemented a Python-based explorer to convert these configurations into graph data via LLVM and ProGraML, enabling input into the prediction model. The evaluation results are stored, and the top-ranked configurations are identified through iterative refinement. The primary objective of DSE is to identify the Pareto-optimal configuration set. To quantify the quality of the approximate Pareto-optimal set, we employ ADRS metric [28]: ADRS(Γ, Ω) 1 Γ X λ Γ min µ Ωf (λ, µ) (9) f(λ, µ) max A(λ) A(µ) A(µ) , L(λ) L(µ) L(µ) (10) A(i) 1 4 FFi FFmax LUTi LUTmax BRAMi BRAMmax DSPi DSPmax (11) L(i) 1 Latencyi (12) 14 Running Title for Header Table 6: The unseen applications and the number of design configurations used in DSE. Benchmark Description Pragmas Design configs atax Matrix Transpose and Vector Multiplication 5 3,354 doitgen Multi-resolution analysis kernel (MADNESS) 6 179 gemm-p Matrix-multiply C alpha.A.B beta.C 8 409,905 heat-3d Heat equation over 3D data domain 11 71,511 mvt Matrix Vector Product and Transpose 8 3,059,001 Figure 7: Convergence Curves: Mean ADRS achieved by LLMMHs, EA, SA, ACO as the number of explored designs increases. where Γ represents the reference Pareto-optimal set, Ωdenotes the approximate Pareto-optimal set, and the function f computes the distance between λ and µ. Equations 11 and 12 formally define the functionality of A( ) and L( ) respectively, where FFmax, LUTmax, BRAMmax, and DSPmax represent the maximum available resources on the target FPGA platform. The framework quantifies design performance through latency measurements as the primary evaluation metric. A lower ADRS value indicates a higher accuracy of the approximate Pareto-optimal set relative to the reference. To assess the performance of LLMMH varaints (LLMGA, LLMSA and LLMACO) , we compare them with three meta-heuristic algorithms: GA [4], SA [5], ACO [6], as referenced in [17]. The reference Pareto-optimal set is collected from the results generated by the exhaustive DSE algorithm proposed in [32]. To ensure fair comparison of algorithm performance, the parameter configurations for SA, GA, ACO and LLMMH variants maintain unified settings: both population size N and maximum number of explorations Nse are set to identical values across all algorithms. Additionally, due to the excessively large design space sizes of certain benchmarks such as mvt, we have imposed a one-hour exploration time limit for each benchmark. The adaptive parameter scheme is dynamically adjusted based on the design space size S as follows: (Nse, N) (0.00005S, 30) S 107 (0.005S, 30) 105 S 106 (0.05S, 30) 104 S 105 (0.3S, 30) 500 S 104 (0.5S, 10) S 500 (13) 15 Running Title for Header Table 7: ADRS Results and Runtime on Unseen Applications. Algorithm atax heat-3d doitgen gemm-p mvt Average ADRS Overall Runtime (s) SA 0 0.4053 0.0669 0.5567 0.5620 0.3182 213 GA 0 0.3532 0 0.2936 0.5557 0.2405 132 ACO 0.0043 0.2946 0 0.6081 0.5138 0.2842 151 LLMGA(deepseek-r1) 0.0086 0.1483 0 0.1633 0.3844 0.1409 9491 LLMGA(gpt-4o) 0 0.2085 0.0790 0.1837 0.4206 0.1784 4961 LLMGA(gpt-4.1) 0.0029 0.3182 0 0.2258 0.1549 0.1404 4752 LLMGA(o3-mini) 0 0.2829 0.0717 0.2007 0.3223 0.1755 8427 LLMSA(deepseek-r1) 0.0086 0.1053 0 0.1235 0.1668 0.0808 9836 LLMSA(gpt-4o) 0.0028 0.1481 0 0.1389 0.3619 0.1304 4317 LLMSA(gpt-4.1) 0 0.3237 0 0.2519 0.2416 0.1635 4931 LLMSA(o3-mini) 0.0028 0.2878 0.0245 0.2239 0.4565 0.1991 8206 LLMACO(deepseek-r1) 0.0086 0 0 0.0096 0.1516 0.0339 9998 LLMACO(gpt-4o) 0 0 0 0.0361 0.4297 0.0932 4716 LLMACO(gpt-4.1) 0 0.2096 0 0.2401 0.2131 0.1326 4158 LLMACO(o3-mini) 0 0.1863 0 0.2135 0.4084 0.1616 7659 LLMACO(deepseek-r1) Improv. over SA 89.34 LLMACO(deepseek-r1) Improv. over GA 85.90 LLMACO(deepseek-r1) Improv. over ACO 88.07 In the experiments, we employ four foundation models (deepseek-r1 [38], gpt-4o [39], gpt-4.1 [40], o3-mini [41]) as surrogate models. In total, we evaluate 12 combinations of LLMMH (3 meta-heuristics 4 LLMs). As shown in Table 7, metaheuristics (GA, SA, ACO) achieve ADRS exceeding 0.5 on the mvt benchmark with large design space, aligning with our theoretical analysis of their exploration limitations. In contrast, our LLMMH framework demonstrates superior performance LLMACO(deepseek-r1) achieves ADRS as low as 0.0339. On small-scale benchmarks such as atax, LLMMH variants maintain comparable solution quality with meta-heuristic methods. Comprehensive experimental results demonstrate that LLMACO(deepseek-r1) achieves the optimal performance with the lowest ADRS, showing improvements of 89.34 , 85.90 , and 88.07 over SA, GA, and GA respectively. While LLMMH algorithms exhibit marginally inferior performance compared to traditional metaheuristics on small-scale design space benchmark due to LLMs inherent hallucination issues (a longstanding challenge in NLP where models generate semantically plausible but technically invalid outputs), holistic evaluation reveals the framework s superior overall performance across metrics. This demonstrates LLMMH s effectiveness in balancing exploration-exploitation tradeoffs despite occasional localized solution quality variations. This quantitatively verifies that our LLMMH framework not only resolves inherent limitations of meta-heuristics but also substantially outperforms them in solution quality. Fig. 7 illustrates convergence curves of high-performing LLMMH variants versus GA, SA, and EA. It can be observed that, for benchmarks with small design spaces, all algorithms demonstrate comparable convergence to Pareto-approximate optimal solutions within limited iterations. However, this comparative analysis quantitatively validates our core hypothesis: traditional meta-heuristics fundamentally underperform in exploring superior configurations under strict iteration budgets for large design spaces, whereas LLM-augmented operators effectively mitigate this limitation through intelligent solution generation, as evidenced by their accelerated convergence trajectories. Based on the overall runtime of each algorithm on five benchmarks (see Table 7), we also observed that the computational overhead of LLMMH variants remains significant due to the inference latency of large language models (particularly o3-mini and deepseek-r1), compounded by per-iteration API calls and network delays. 6 Conclusion In this paper, we present the CoGNNs-LLMMH framework, which comprises a GNN prediction model that dynamically adapts message-passing strategies based on task-specific characteristics and LLM-driven meta-heuristic algorithms for DSE. CoGNNs enhances LLMMH s exploration capabilities by providing high-precision QoR predictions for design configurations without relying on time-consuming EDA flows. Experimental results demonstrate that both CoGNNs and LLMMH outperform baseline models across multiple metrics. This work presents the first systematic implementation of LLMs in DSE. Beyond demonstrating LLMs capability to autonomously execute complex operations traditionally requiring human expertise, we aim to address the limitations of conventional heuristic-based methodologies that 16 Running Title for Header dominate current DSE practices. More significantly, our LLMMH framework demonstrates superior performance compared to traditional heuristic approaches. The proposed paradigm shift of leveraging LLMs for domain-specific knowledge-intensive tasks establishes a new methodology with promising potential for widespread adoption in DSE research and applications. The current implementation of LLMMH presents valuable opportunities for enhancement in future endeavors. By addressing specific areas for improvement, we can significantly elevate its effectiveness and impact. 1) Improving the selection of LLMs The current DSE experiment implemented multiple LLMs, though the investigation does not comprehensively encompass all established model archetypes. Notably, the hypothesis that LLMs specifically designed for mathematical reasoning and code generation might demonstrate superior configuration generation capabilities compared to conversational LLMs in DSE contexts remains to be rigorously validated through systematic comparative analysis. 2) Reducing the runtime of LLMMH In the experiments, we employed multiple LLMs, necessitating the implementation of Langchain for online LLM orchestration. However, the algorithms within our proposed LLMMH framework currently exhibit elevated runtime overhead, primarily attributed to the per-iteration API calls through Langchain interfaces and the substantial parameter sizes of LLMs. This bottleneck is not insurmountable potential optimizations include establishing multi-threaded API connections or developing locally deployed distilled LLMs specialized for DSE tasks (given the correlation between LLM inference latency and parameter count). These approaches hold significant promise for runtime reduction, constituting a key focus of our ongoing research efforts. 3) Optimizing prompt engineering techniques Experimental results empirically demonstrate that the output quality of LLMs exhibits strong dependency on prompt configuration design. Dedicated prompt engineering techniques, such as self-reflective prompts [42] and directional-stimulus prompts [43], may lead to more significant performance enhancements in LLMMH s evolutionary optimization processes. References [1] Ben Finkelshtein, Xingyue Huang, Michael Bronstein, and Ceylan. Cooperative graph neural networks. In Proceedings of the 41st International Conference on Machine Learning, ICML 24. JMLR.org, 2024. [2] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. [3] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [4] Benjamin Carrion Schafer. Parallel high-level synthesis design space exploration for behavioral ips of exact latencies. ACM Trans. Des. Autom. Electron. Syst., 22(4), May 2017. [5] Benjamin Carrion Schafer. Probabilistic multiknob high-level synthesis design space exploration acceleration. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 35(3):394 406, 2016. [6] Benjamin Carrion Schafer, Takashi Takenaka, and Kazutoshi Wakabayashi. Adaptive simulated annealer for high level synthesis design space exploration. In 2009 International Symposium on VLSI Design, Automation and Test, pages 106 109, 2009. [7] Steve Dai, Yuan Zhou, Hang Zhang, Ecenur Ustun, Evangeline FY Young, and Zhiru Zhang. Fast and accurate estimation of quality of results in high-level synthesis with machine learning. In 2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), pages 129 132. IEEE, 2018. [8] Guanwen Zhong, Alok Prakash, Siqi Wang, Yun Liang, Tulika Mitra, and Smail Niar. Design space exploration of fpga-based accelerators with multi-level parallelism. In Design, Automation Test in Europe Conference Exhibition (DATE), 2017, pages 1141 1146. IEEE, 2017. [9] Hosein Mohammadi Makrani, Farnoud Farahmand, Hossein Sayadi, Sara Bondi, Sai Manoj Pudukotai Dinakarrao, Houman Homayoun, and Setareh Rafatirad. Pyramid: Machine learning framework to estimate the optimal timing and resource usage of a high-level synthesis design. In 2019 29th International Conference on Field Programmable Logic and Applications (FPL), pages 397 403. IEEE, 2019. [10] Lorenzo Ferretti, Andrea Cini, Georgios Zacharopoulos, Cesare Alippi, and Laura Pozzi. Graph neural networks for high-level synthesis design space exploration. ACM Transactions on Design Automation of Electronic Systems, 28(2):1 20, 2022. 17 Running Title for Header [11] Mingzhe Gao, Jieru Zhao, Zhe Lin, and Minyi Guo. Hierarchical source-to-post-route qor prediction in high-level synthesis with gnns. In 2024 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1 6. IEEE, 2024. [12] Nan Wu, Hang Yang, Yuan Xie, Pan Li, and Cong Hao. High-level synthesis performance prediction using gnns: Benchmarking, modeling, and advancing. In Proceedings of the 59th ACM IEEE Design Automation Conference, pages 49 54, 2022. [13] Chenfeng Zhao, Clayton Faber, Roger Chamberlain, and Xuan Zhang. Hlperf: demystifying the performance of hls-based graph neural networks with dataflow architectures. ACM transactions on reconfigurable technology and systems, 18(1):1 26, 2024. [14] Zhe Lin, Zike Yuan, Jieru Zhao, Wei Zhang, Hui Wang, and Yonghong Tian. Powergear: Early-stage power estimation in fpga hls via heterogeneous edge-centric gnns. in 2022 design, automation test in europe conference exhibition (date). IEEE, Virtual, pages 1341 1346, 2022. [15] Huizhen Kuang, Xianfeng Cao, Jingyuan Li, and Lingli Wang. Hgbo-dse: Hierarchical gnn and bayesian optimization based hls design space exploration. In 2023 International Conference on Field Programmable Technology (ICFPT), pages 106 114. IEEE, 2023. [16] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. Harnessing gnns for robust representation learning in high-level synthesis. IEEE Transactions on Circuits and Systems for Artificial Intelligence, 1(2):114 127, 2024. [17] Zi Wang and Benjamin Carrion Schafer. Machine leaming to set meta-heuristic specific parameters for high-level synthesis design space exploration. in 2020 57th acm ieee design automation conference (dac). 1 6, 2020. [18] Pingakshya Goswami, Benjamin Carrion Schaefer, and Dinesh Bhatia. Machine learning based fast and accurate high level synthesis design space exploration: From graph to synthesis. Integration, 88:116 124, 2023. [19] Huiliang Hong, Chenglong Xiao, and Shanshan Wang. Rethinking high-level synthesis design space exploration from a contrastive perspective. In 2024 IEEE 42nd International Conference on Computer Design (ICCD), pages 179 182. IEEE, 2024. [20] Nan Wu, Yuan Xie, and Cong Hao. Ironman-pro: Multiobjective design space exploration in hls via reinforcement learning and graph neural network-based modeling. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(3):900 913, 2023. [21] Yuan Yao, Huiliang Hong, Shanshan Wang, and Chenglong Xiao. Decomposition based estimation of distribution algorithm for high-level synthesis design space exploration. Integration, 100:102292, 2025. [22] Md Imtiaz Rashid and Benjamin Carrion Schafer. Fast and inexpensive high-level synthesis design space exploration: Machine learning to the rescue. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(11):3939 3950, 2023. [23] Zheyuan Zou, Cheng Tang, Lei Gong, Chao Wang, and Xuehai Zhou. Flexwalker: An efficient multi-objective design space exploration framework for hls design. In 2024 34th International Conference on Field-Programmable Logic and Applications (FPL), pages 126 132, 2024. [24] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [25] Petar Veliˇckovi c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [26] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers. In 2024 IEEE Congress on Evolutionary Computation (CEC), pages 1 8. IEEE, 2024. [27] Elliot Meyerson, Mark J Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. ACM Transactions on Evolutionary Learning, 4(4):1 40, 2024. [28] Benjamin Carrion Schafer and Zi Wang. High-level synthesis design space exploration: Past, present, and future. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 39(10):2628 2639, 2019. [29] Chris Lattner and Vikram Adve. Llvm: A compilation framework for lifelong program analysis transformation. In International symposium on code generation and optimization, 2004. CGO 2004., pages 75 86. IEEE, 2004. [30] Chris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP O Boyle, and Hugh Leather. Pro- graml: A graph-based program representation for data flow analysis and compiler optimizations. In International Conference on Machine Learning, pages 2244 2253. PMLR, 2021. 18 Running Title for Header [31] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. [32] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. Automated accelerator optimization aided by graph neural networks. In Proceedings of the 59th ACM IEEE Design Automation Conference, DAC 22, page 55 60, 2022. [33] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. [34] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. [35] Brandon Reagen, Robert Adolf, Yakun Sophia Shao, Gu-Yeon Wei, and David Brooks. Machsuite: Benchmarks for accelerator design and customized architectures. In 2014 IEEE International Symposium on Workload Characterization (IISWC), pages 110 119. IEEE, 2014. [36] T. Yuki et al. Polybenchc, 4.2.1, 2010. [37] William L Hamilton. Graph representation learning. Morgan Claypool Publishers, 2020. [38] DeepSeek-AI, Daya Guo, Dejian Yang, and . Haowei Zhanget et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [39] OpenAI, :, Aaron Hurst, Adam Lerer, and . Adam P. Goucher et al. Gpt-4o system card, 2024. [40] OpenAI. Openai gpt-4-1, 2025. Accessed: 2025-03-21. [41] OpenAI. Openai o3-mini, 2025. Accessed: 2025-03-21. [42] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 8634 8652. Curran Associates, Inc., 2023. [43] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. Guiding large language models via directional stimulus prompting. NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. [44] Vijay Prakash Dwivedi, Ladislav Rampášek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. Advances in Neural Information Processing Systems, 35:22326 22340, 2022. 19\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2504.19649v2 [cs.LG] 5 Jun 2025 INTELLIGENT4DSE: OPTIMIZING HIGH-LEVEL SYNTHESIS DESIGN SPACE EXPLORATION WITH GRAPH NEURAL NETWORKS AND LARGE LANGUAGE MODELS Lei Xu, Shanshan Wang Department of Computer Science Shantou University Shantou, China {24lxu, Emmanuel Casseau IRISA Unviersity of Rennes 1 Rennes, France Chenglong Xiao Department of Computer Science Shantou University Shantou, China ABSTRACT High-level synthesis (HLS) design space exploration (DSE) is an optimization process in electronic design automation (EDA) that systematically explores high-level design configurations to achieve Pareto-optimal hardware implementations balancing performance, area, and power (PPA). To opti- mize this process, HLS prediction tasks often employ message-passing neural networks (MPNNs), leveraging complex architectures to achieve high accuracy. These predictors serve as evaluators in the DSE process, effectively bypassing the time-consuming estimations traditionally required by HLS tools. However, existing models often prioritize structural complexity and minimization of training loss, overlooking task-specific characteristics. Additionally, while meta-heuristic al- gorithms are widely used in DSE, they typically require extensive domain-specific knowledge to design operators and time-consuming tuning. To address these limitations, we propose CoGNNs- LLMMH, a framework that integrates a graph neural network with task-adaptive message passing and large language model-enhanced meta-heuristic algorithms. As a predictive model, CoGNNs directly leverages intermediate representations generated from source code after compiler front-end processing, enabling prediction of quality of results (QoR) without invoking HLS tools. Due to its strong adaptability to tasks, CoGNNs can be tuned to predict post-HLS and post-implementation outcomes, effectively bridging the gap between high-level abstractions and physical implemen- tation characteristics. CoGNNs achieves state-of-the-art accuracy in post-HLS QoR prediction, reducing mean prediction errors by 2.8 for latency and 3.4 for resource utilization compared to baseline models.\n\n--- Segment 2 ---\nDue to its strong adaptability to tasks, CoGNNs can be tuned to predict post-HLS and post-implementation outcomes, effectively bridging the gap between high-level abstractions and physical implemen- tation characteristics. CoGNNs achieves state-of-the-art accuracy in post-HLS QoR prediction, reducing mean prediction errors by 2.8 for latency and 3.4 for resource utilization compared to baseline models. For post-implementation prediction tasks, CoGNNs demonstrates the lowest prediction errors, with average reductions of 17.6 for flip-flop (FF) usage, 33.7 for critical path (CP) delay, 26.3 for power consumption, 38.3 for digital signal processor (DSP) utilization, and 40.8 for BRAM usage. Furthermore, LLMMH variants can generate superior Pareto fronts compared to meta-heuristic algorithms simulated annealing algorithm (SA), genetic algorithm (GA) and ant colony optimization (ACO) in terms of average distance from the reference set (ADRS) with improvements of 89.34 , 85.90 and 88.07 , respectively. Code and models are available at 1 Introduction High-level synthesis (HLS) has emerged as a modern alternative to traditional RTL-based VLSI design. HLS enables users to leverage high-level programming languages, such as C C , for hardware design tasks, offering synthesis options to fine-tune results. Proven in practice, HLS provides numerous advantages in hardware design and has become the mainstream approach, particularly for FPGA implementations. To enhance HLS effectiveness, most EDA tools offer a variety of synthesis options, called "knobs," allowing users to control the synthesis outcomes. In commercial HLS tools, synthesis directives are typically added to the source code as comments or pragmas. By evaluating various Corresponding Author: Chenglong Xiao. Running Title for Header Figure 1: A comprehensive overview of the HLS DSE process, using the atax application from the PolyBench benchmark suite. The HLS tool can be replaced by predictive models. directive combinations, HLS produces a wide range of hardware implementations, facilitating the exploration of trade-offs between conflicting objectives such as performance (latency) and cost (area, power). Fig. 1 presents an overview of high-level synthesis design space exploration.\n\n--- Segment 3 ---\nFig. 1 presents an overview of high-level synthesis design space exploration. The workflow begins with C C behavioral descriptions augmented with pragma directives and parameterized optimization targets. These quality of results (QoR) metrics (e.g., latency, resource utilization) of specifications can be obtained using HLS tools or machine learning based models. A design space exploration engine then selects novel pragma combinations through multi- objective optimization, generating updated behavioral descriptions for subsequent iterations. This cyclic refinement process continues until the convergence to a Pareto-optimal configuration set that optimally balances competing design constraints. Therefore, accurate QoR prediction of each design and efficient design space exploration are the most crucial tasks in HLS DSE. Regarding the two key tasks involved in HLS DSE, we identify the following critical issues: Problem 1 (High-accuracy QoR prediction): Current QoR prediction methodologies predominantly employ graph representation learning, where pragma-annotated source code is transformed into control-data flow graph (CDFG), followed by sophisticated GNNs architectures extracting topological features for predictive modeling. GNNs based on the message-passing mechanism, commonly termed MPNNs, operate by iteratively updating node features through neighborhood information aggregation. Although these MPNNs demonstrate strong accuracy in conventional classi- fication and regression tasks, their performance in QoR prediction tasks remains limited, often failing to surpass the discriminative power of the 1-Weisfeiler-Lehman (1-WL) algorithm [1]. This limitation comes from their restricted expressiveness, leading to sub-optimal performance in complex prediction tasks [2]. In addition, conventional MPNNs suffer from inherent long-range dependency issues and over-smoothing phenomena [3], severely constraining GNN models accuracy and generalization capabilities in QoR prediction tasks. The problems existing in the above-mentioned MPNNs all seriously affect its QoR prediction accuracy in post-HLS and post-implementation. A critical observation reveals that existing GNN models seldom excel in both post-HLS and post-implementation QoR prediction tasks, inherently linked to the architectural limitations of MPNNs. Problem 2 (Multi-objective design exploration): DSE methods systematically explore the design space to identify Pareto-optimal configurations that achieve optimal trade-offs between competing objectives such as performance and cost.\n\n--- Segment 4 ---\nA critical observation reveals that existing GNN models seldom excel in both post-HLS and post-implementation QoR prediction tasks, inherently linked to the architectural limitations of MPNNs. Problem 2 (Multi-objective design exploration): DSE methods systematically explore the design space to identify Pareto-optimal configurations that achieve optimal trade-offs between competing objectives such as performance and cost. In DSE, meta-heuristic algorithms (genetic algorithms [4], simulated annealing [5], ant colony optimization [6]) have profound applications and have achieved remarkable success. One critical issue lies in the inherent challenge for meta-heuristic algorithms to identify superior design configurations within vast design spaces while exploring 2 Running Title for Header a limited number of design candidates. While traditional meta-heuristics demonstrate satisfactory performance on benchmarks with constrained design spaces, their effectiveness fundamentally degrades when handling exploration tasks encompassing millions or even hundreds of millions of configurations. Under such scale constraints, the optimization objective shifts from identifying the absolute optimal configuration to discovering acceptably competitive solutions within strictly bounded exploration iterations and time budgets. Conventional approaches relying solely on parameter configurations and stochastic operators prove inadequate for achieving viable design solutions through sparse exploration of the combinatorial space. Another critical challenge stems from the parameter sensitivity inherent in metaheuristic algorithms. For instance, in genetic algorithm (GA), improper configuration of crossover and mutation rates can lead to insufficient exploration of subspaces. Similarly, simulated annealing algorithm (SA) requires careful calibration of neighborhood ranges excessively large ranges exacerbate exploration inefficiencies, while overly constrained ranges promote premature convergence to local optima. Ant colony optimization algorithm (ACO) presents greater parametric complexity through pheromone matrix-guided probabilistic path selection mechanisms, which involve more intricate parameter configurations in their probabilistic selection formulae. To address the aforementioned issues, we propose CoGNNs-LLMMH, a unified framework that synergizes task-adaptive graph neural networks [1] with large language model (LLM)-driven meta-heuristic algorithms. Our key contributions are as follows: 1) We adopt a novel graph neural network architecture (CoGNNs) with an adaptive message-passing mechanism as our core prediction models for accurate QoR prediction.\n\n--- Segment 5 ---\nTo address the aforementioned issues, we propose CoGNNs-LLMMH, a unified framework that synergizes task-adaptive graph neural networks [1] with large language model (LLM)-driven meta-heuristic algorithms. Our key contributions are as follows: 1) We adopt a novel graph neural network architecture (CoGNNs) with an adaptive message-passing mechanism as our core prediction models for accurate QoR prediction. Unlike traditional GNNs with fixed message- passing rules, CoGNNs dynamically reconstructs graph topologies based on task objectives, enabling targeted feature propagation and mitigating over-smoothing. The adapted CoGNNs can be tuned to predict both post-HLS and post-implementation QoR metrics with the highest accuracy compared to SOTA works. 2) We propose an LLM-driven meta-heuristic (LLMMH) framework for efficient DSE. The evaluated meta- heuristic algorithms includes GA, SA, and ACO. By leveraging LLMs learning and reasoning capabilities with carefully engineered prompts, LLMMH significantly reduces domain expertise dependency and achieves prominent improvement in Pareto front quality compared to baseline algorithms. Moreover, the proposed LLMMH framework can be adapted to many other meta-heuristic algorithms for HLS DSE. 3) To the best of our knowledge, the proposed LLMMH is the first framework that integrates LLM with meta- heuristic algorithms for HLS DSE. This approach represents a significant shift from prior methods in HLS DSE. Notably, it has demonstrated promising ability to enhance the quality of DSE outcomes. We hope that our findings will inspire further exploration of LLM-based techniques to tackle challenges in HLS DSE. 2 Related work Recent advances in QoR prediction have been driven by machine learning and GNNs. Foundational approaches employ learnable parameters and loss minimization to predict optimal configurations, with notable contributions including systematic regression model analysis for HLS feature selection [7] and accelerator evaluation framework MPSeeker [8]. Makrani et al. [9] employ Minerva, an automated hardware optimization tool that determines near- optimal tool configurations through static timing analysis and heuristic algorithms, optimizing either peak throughput or throughput-to-area ratio.\n\n--- Segment 6 ---\nMakrani et al. [9] employ Minerva, an automated hardware optimization tool that determines near- optimal tool configurations through static timing analysis and heuristic algorithms, optimizing either peak throughput or throughput-to-area ratio. Pyramid framework leverages this database to train an ensemble ML model that maps HLS-reported features to Minerva s optimization outcomes.Building on these foundations, GNN-based methods have emerged as the dominant paradigm, typically converting source code into intermediate representation (IR) graphs for feature extraction exemplified by hybrid control-data flow graphs [10] and hierarchy-aware GNNs [11]. Recent studies demonstrate progressive advancements in GNN-based hardware design optimization. Wu et al. [12] develop performance modeling frameworks using graph neural networks to represent C C programs as computational graphs. Extending this paradigm, Zhao et al. [13] establish GNNHLS as an HLS-specific benchmark with six GNN models across four topological datasets. While Lin et al. [14] propose power-aware edge-centric GNNs with dynamic power modeling through edge neighborhood aggregation, Kuang et al. [15] introduce hierarchical HGP architectures for post-implementation PPA prediction. Complementing these approaches, Sohrabizadeh et al. [16] present HARP s hierarchical graph representation with auxiliary nodes for pragma-optimized HLS designs. DSE is fundamentally formulated as a multi-objective optimization problem (MOOP), where pre-trained prediction models can serve as evaluators to guide heuristic search algorithms toward Pareto-optimal solutions. Wang et al. [17] automate meta-heuristic parameter configuration through behavioral analysis of C-based descriptions, while Goswami et al. [18] demonstrate GBRT-based models achieve comparable accuracy to exhaustive logic synthesis. To enable a precise identification of Pareto-optimal designs, Hong et al. [19] develop contrastive learning to predict the dominance relationships between designs. A reinforcement learning-driven DSE approach is presented in [20]. Yao et al. [21] propose a decomposition based DSE approach to minimize synthesis iterations. To accelerate ASIC DSE, Rashid et 3 Running Title for Header al. [22] introduce FPGA-targeted transfer learning and a dedicated multithreaded parallel HLS design space explorer. Zou et al.\n\n--- Segment 7 ---\n[22] introduce FPGA-targeted transfer learning and a dedicated multithreaded parallel HLS design space explorer. Zou et al. [23] propose FlexWalker, a multi-objective HLS design space exploration framework that employs upper confidence bound (UCB)-coordinated heterogeneous regression models for design quality prediction across parameter configurations, augmented with probabilistic sampling and elastic Pareto frontiers to mitigate regression modeling inaccuracies. In summary, SOTA prediction methodologies in HLS depend primarily on GNNs, leveraging their inference capabilities to achieve high-precision predictions on unseen datasets. Dominant GNN architectures that include graph convolutional networks (GCNs) [24], graph attention networks (GATs) [25], and graph isomorphism networks (GINs) [2] are rooted in message-passing mechanisms. However, inherent limitations of MPNNs, such as long-range dependency failures and over-smoothing phenomena, critically degrade prediction accuracy, partially explaining the persistent performance gaps in HLS QoR estimation. The recent GNN architecture CoGNNs [1] demonstrates adaptive capability with dynamic message-passing mechanism, which is particularly beneficial for HLS prediction tasks. In this work, we adopt CoGNNs for predicting post-HLS metrics. Meta-heuristics have been the predominant approaches and have achieved significant progress in solving HLS DSE over the past decades, and they continue to undergo diverse and flourishing development. The main problems of the traditional meta-heuristic algorithms lie in the difficulty of the designed random operator to escape the local optimum and the numerous difficulties in parameter tuning. Inspired by the works proposed in [26, 27], we propose a LLM-driven metaheuristic framework for HLS DSE, which takes advantage of LLMs powerful information comprehension capabilities to interpret the functional roles of various pragmas and their impacts on the final routing outcomes. 3 Preliminaries 3.1 Problem Formulation HLS DSE can be formulated as a MOOP, where the primary objective is to identify a set of Pareto-optimal configurations that simultaneously minimize latency and resource utilization. Given a behavioral description and optional synthesis directives [pragma1, pragma2, ..., pragman], the design space DS can be formulated as the Cartesian product of the combination of pragmas.\n\n--- Segment 8 ---\n3 Preliminaries 3.1 Problem Formulation HLS DSE can be formulated as a MOOP, where the primary objective is to identify a set of Pareto-optimal configurations that simultaneously minimize latency and resource utilization. Given a behavioral description and optional synthesis directives [pragma1, pragma2, ..., pragman], the design space DS can be formulated as the Cartesian product of the combination of pragmas. The definition of Pareto frontier Pf is as follows [28]: A(df) A(di) and L(df) L(di) (1) where df Pf, di DS and Pf DS. The function A( ) and L( ) represent the resource utilization and the latency of df and di. A design configuration dj Pf is considered Pareto-optimal if no other configuration in the search space dominates it, that is, no alternative design simultaneously achieves both lower area utilization and reduced latency. Therefore, our objective is to efficiently identify the Pareto-optimal set without resorting to exhaustive searches across the entire configuration space and invoking computationally expensive HLS tools. 3.2 Graph Representation of Source Code To facilitate GNNs in capturing the features of the source code, we construct hierarchical graph representations that preserve both syntactic structures and semantic dependencies. A prevalent approach nowadays is to use LLVM [29] to convert the source code into IR, and then utilize ProGraML [30] to transform the IR into a control data flow graph. LLVM instructions encompass numerous low-level operations such as addition operations and memory reading operations. These operations are converted into certain nodes in the CDFG. If an operation needs to be performed on a certain variable, the corresponding nodes will be connected through some directed edges. 3.3 Graph Neural Network A simple graph neural network extracts the features of each node during the training process and aggregates the messages from its neighbors to update the node features.\n\n--- Segment 9 ---\nIf an operation needs to be performed on a certain variable, the corresponding nodes will be connected through some directed edges. 3.3 Graph Neural Network A simple graph neural network extracts the features of each node during the training process and aggregates the messages from its neighbors to update the node features. Specifically, at GNN s l-th layer, node v s feature representation hl v will be updated by aggregating (AGG) node v s feature representation hl 1 v in the previous layer and all the neighbor messages hl 1 w , where w N(v) and N(v) is a neighbor set of node v. To make this process learnable, the result obtained above needs to be multiplied by the learnable weight matrix W l 1. hl v σ W l 1hl 1 v W lψ hl 1 w , w N(v) (2) where σ is a activation function to provide non-linearity and ψ is aggregation function like mean, W l 1 and W l are learnable matrices. 4 Running Title for Header 3.4 Gumbel-softmax Estimator In CoGNNs [1], the action network dynamically adjusts the node states based on task-specific characteristics, enabling adaptive reconstruction of the computational graph topology. Specifically, the action network performs a node-level classification task to predict discrete state assignments, where the optimal state is selected via argmax over the predicted probabilities. However, since node states follow a categorical distribution, this prediction task is inherently non- differentiable. To address this, CoGNNs employs the Gumbel-Softmax estimator [31], which provides a continuous relaxation of the discrete state selection process, thereby enabling end-to-end gradient-based optimization. This approach ensures both the dynamic adaptability of the graph structure and the trainability of the overall framework. This estimator approximates the stochastic action sampling in a differentiable and continuous manner. We consider a set of actions Υ, which can be represented as a probability vector p R Υ and the vector p stores the probability values of different actions. Let q be a categorical variable with class probabilities p1, p2, ..., p Υ and we assume that the categorical samples are encoded as k-dimensional one-hot vectors.\n\n--- Segment 10 ---\nWe consider a set of actions Υ, which can be represented as a probability vector p R Υ and the vector p stores the probability values of different actions. Let q be a categorical variable with class probabilities p1, p2, ..., p Υ and we assume that the categorical samples are encoded as k-dimensional one-hot vectors. The Gumbel-Max is a special reparametrization trick that provides an efficient way to draw samples q from a categorical distribution with class probabilities p : q onehot argmax k (gk log pk) (3) where g1, g2, ..., g Υ are samples drawn from Gumbel(0, 1). We can use the softmax function to replace the non- differentiable argmax function to generate the Gumbel-softmax scores sk and τ is the softmax temperature: sk exp ((gk log pk) τ) P Υ i 1 exp ((gi log pi) τ) (4) By incorporating the Gumbel-Softmax estimator, the action network becomes fully differentiable, enabling its seamless integration into the gradient-based optimization process. This differentiability allows CoGNNs to dynamically adapt node-level actions such as edge pruning or feature aggregation based on task-specific objectives. Consequently, the framework learns data graph features in a topology-aware manner, capturing both local structural patterns and global semantic dependencies. This adaptive capability is particularly advantageous for HLS prediction tasks, where design configurations exhibit heterogeneous and hierarchical characteristics. 3.5 Meta-heuristic Algorithms for DSE In DSE, GA [4] are widely adopted as the optimization backbone, where each design configuration is encoded as a gene and the complete set forms a chromosome Cr. The exploration begins with an initial population of parent configurations, which undergo crossover and mutation operations guided by predefined probabilities to generate offspring by modifying pragma values (e.g., loop unrolling factors, array partitioning schemes). SA [6] simulates the natural annealing process by dynamically calculating the probability of accepting either inferior solutions or neighboring configurations during each iteration, with acceptance criteria governed by current temperature parameters. ACO [5] emulates ant colony behavioral mechanisms by constructing pheromone matrices for potential pragma values.\n\n--- Segment 11 ---\nSA [6] simulates the natural annealing process by dynamically calculating the probability of accepting either inferior solutions or neighboring configurations during each iteration, with acceptance criteria governed by current temperature parameters. ACO [5] emulates ant colony behavioral mechanisms by constructing pheromone matrices for potential pragma values. During each iteration, it probabilistically selects pragma configurations through pheromone-guided stochastic sampling using sophisticated probabilistic models, thereby generating solutions with enhanced quality potential. 4 Methodology Fig. 2 provides an overview of the proposed CoGNNs-LLMMH. The proposed framework integrates four key stages: dataset generation, model training, inference, and DSE. During dataset generation, labels are extracted from both HLS reports (e.g., loop initiation intervals, resource estimates) and implementation reports (e.g., post-place-and-route resource usage, timing closure) to enhance prediction accuracy. For model training, a subset of the dataset is used to train CoGNNs composed of an environment network (global context capture), an action network (dynamic topology adjustment), and a multi-layer perceptron (QoR metric mapping) which iteratively improves prediction accuracy through hierarchical feature extraction. In the inference stage, the trained CoGNNs model predicts QoR metrics for unseen configurations, enabling rapid evaluation without invocations of the HLS tool. Finally, the DSE stage employs modified meta-heuristic algorithms enhanced by LLMs to identify near-Pareto-optimal solutions. 4.1 Dataset Generation The C C source code is first converted to an intermediate representation (IR) using LLVM, followed by the generation of a CDFG through ProGraML [30], which incorporates HLS directives for enhanced semantic representation. The 5 Running Title for Header Figure 2: The CoGNNs-LLMMH framework integrates data generation (combining HLS reports and implementation reports), predictive model training via task-adaptive graph neural networks, and DSE using an LLM-enhanced evolutionary algorithm. The CoGNNs can be tuned to predict post-HLS QoR metrics and post-implementation QoR metrics. CDFG generated through this compilation process serves as the foundational input to the CoGNNs prediction model. Each design configuration is then synthesized using an HLS tool to extract latency and resource utilization metrics.\n\n--- Segment 12 ---\nCDFG generated through this compilation process serves as the foundational input to the CoGNNs prediction model. Each design configuration is then synthesized using an HLS tool to extract latency and resource utilization metrics. Although HLS reports provide reasonably accurate estimates, we further refine the dataset by incorporating post- implementation metrics, resulting in a more realistic and comprehensive training set. This dual-source labeling strategy combining HLS and implementation reports ensures high-fidelity ground truth for model training. We train our prediction model on datasets from [15] and [32]. For the GNN-DSE dataset [32], the labels for each benchmark are extracted from the HLS reports, and the CDFGs are generated by the compilation front-end. For the HGBO-DSE dataset [15], the labels are derived from the implementation reports, and the CDFGs come from the HLS process. The two aforementioned datasets differ not only in their label sources but also in the benchmarks used for their construction. By conducting experiments on two distinct datasets, we can comprehensively validate the performance of CoGNNs and evaluate its efficiency in handling complex tasks. 4.2 Cooperative Graph Neural Networks The approach proposed in [1] introduces a dynamic message-passing mechanism by assigning each node one of four states Standard (S), Listen (L), Broadcast (B), or Isolate (I), which govern information flow: nodes in the Standard state both receive and propagate information, Listen nodes only receive, Broadcast nodes only propagate, and Isolate nodes remain inactive. This state-based approach transforms node feature updates into a two-step process: 1) each node selects its optimal state based on task-specific objectives, and 2) updates its features accordingly. As illustrated in Fig. 3, CoGNNs implements this mechanism through dual MPNNs: an action network that determines node states and an environment network that performs context-aware feature aggregation. A complete CoGNN (π, η) model consists of environment networks π and action networks η. These two MPNNs, working in tandem, can better adapt to multi-target prediction tasks. Action networks π help each node determine the 6 Running Title for Header Figure 3: An abstract illustration of the CoGNNs architecture, where a dedicated MLP is constructed for each prediction target. way information flows in and out, while environment networks η update the features of each node according to the state of the node.\n\n--- Segment 13 ---\nAction networks π help each node determine the 6 Running Title for Header Figure 3: An abstract illustration of the CoGNNs architecture, where a dedicated MLP is constructed for each prediction target. way information flows in and out, while environment networks η update the features of each node according to the state of the node. Therefore, CoGNNs update the feature representation hv of each node v according to the following steps. To begin with, the action networks π will obtain the action of each node. For a set of actions Φ {S, L, B, I}, the action of node v can be drawn from a categorical distribution pv R Φ by aggregating the information of its neighbors N(v). pv π (hv, N(v)) (5) Next, we input the probability vector pv into the Gumbel-softmax Estimator to obtain the action of the node av. We can use environment networks η to update the feature representation hv and obtain h v. h v η (hv, N) (6) where N is related to the value of av: N {hu u N(v), au S B} , av L S {} , av B I (7) For the environment and action networks, we employ a combination of aggregation models (e.g., MEANGNN, SUMGNN) to iteratively update node feature representations. To construct the graph-level embedding, we replace CoGNNs direct node feature summation with an attention-based mechanism [33], which assigns importance scores tv to each node v based on its relevance to the HLS prediction task. This approach captures the heterogeneous contributions of nodes (e.g., loop headers vs. memory operations) and computes the final graph-level vector hG by weighted summation. hG n X i 1 expMLP1(hi) Pn j 1 expMLP1(hj) ! MLP2 (hi) (8) where n is the total number of nodes in the data graph, and MLP1 and MLP2 are used to map hi and hj to R. In the prediction phase, we set up MLPs for each prediction target. Compared to SOTA works such as [15], this method only requires training one prediction model to predict all targets.\n\n--- Segment 14 ---\nMLP2 (hi) (8) where n is the total number of nodes in the data graph, and MLP1 and MLP2 are used to map hi and hj to R. In the prediction phase, we set up MLPs for each prediction target. Compared to SOTA works such as [15], this method only requires training one prediction model to predict all targets. 4.3 Large Language Model aided Meta-Heuristic Algorithms Traditional meta-heuristic algorithms, such as GA, SA and ACO, have demonstrated strong performance in DSE. Meta-heuristic algorithms, however, suffer from inherent limitations including intricate parameter configurations and suboptimal performance on benchmarks with expansive design spaces. To address this limitations, we leverage large language models (LLMs) to assist meta-heuristic algorithms in generating offspring. Fig. 4 illustrates the proposed LLM-driven meta-heuristic framework for HLS DSE. The proposed LLMMH framework consists of two main components: 1) the general process of meta-heuristics (the lower section of Fig. 4) 2) generating new solutions using LLM (the upper section of Fig. 4). LMMH begins by randomly generating a set of initial solutions to create the first population using LLM with a tailored prompt. It then follows an iterative update process. In each iteration, LMMH constructs a prompt to guide the LLM in generating offspring solutions. These offspring solutions are then evaluated using trained GNN models, and we select top solutions from the offspring solutions and the current population as new 7 Running Title for Header Figure 4: An overview of LLMMH framework for HLS DSE. The LLMMH framework integrates prompt design, which includes task descriptions, solution examples, and task instructions, along with an LLM-enhanced workflow. This workflow consists of initialization, LLM-guided solution generation, and GNN-based evaluation. In this study, we present and evaluate three LLM-driven meta-heuristics: LLMGA, LLMSA, and LLMACO. The LLMMH framework can be adapted to various other meta-heuristic algorithms to support efficient HLS DSE. population for the next iteration. The process continues until a predefined number of iterations is reached, and finally, the Pareto solutions found are returned.\n\n--- Segment 15 ---\npopulation for the next iteration. The process continues until a predefined number of iterations is reached, and finally, the Pareto solutions found are returned. In the proposed framework, we carefully design prompts tailored to the dataset characteristics and incorporate the Few-Shot method to guide the LLM in performing generating the initial population and new offspring for meta-heuristic algorithms, which are one of the most important steps involved in meta-heuristic algorithms. LangChain2, an open- source framework, facilitates the development of applications utilizing LLMs by offering tools, components, and interfaces for integrating LLMs with external data sources and services. We employ LangChain to configure our LLMs, necessitating prompt designs that adhere to its format. The prompts are categorized into system messages and input messages. The system messages inform the LLMs about the task s content and requirements, while the input messages transmit data for each LLM API call. Within the system messages, we design three sections: task description, solution examples, task instruction. The task description section briefly introduces HLS DSE background to activate LLMs relevant knowledge modules. Solution examples provide input-output instances to constrain LLMs output formatting. Task instructions guide LLMs in solution generation, maintaining direct correspondence with pseudocode components. Since LangChain is used to invoke the LLM API, the input messages are designed to include only parameters relevant to the design space. Algorithm 1 provides an overview of the proposed LLM-enhanced genetic algorithm (LLMGA). The inputs of the algorithm include the parameters required for LLM prompts and those related to the design space, while the output is the Pareto-optimal set Pest. First we construct prompts based on parameters N and V (lines 2-3), then instruct 2 8 Running Title for Header Algorithm 1 LLMGA algorithm.\n\n--- Segment 16 ---\nThe inputs of the algorithm include the parameters required for LLM prompts and those related to the design space, while the output is the Pareto-optimal set Pest. First we construct prompts based on parameters N and V (lines 2-3), then instruct 2 8 Running Title for Header Algorithm 1 LLMGA algorithm. Input: Design space ds, design space size S, pragma possible values Vp, maximum number of explorations Nse, population size N, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t; Output: Pareto optimal solution set Pest; 1: n 1, nost 0; 2: prompts employing prompt engineering techniques based on N, Vp to establish LLM prompt; 3: P use prompts to instruct LLM to initialize N solutions; 4: Fitness Evaluator (P, code); 5: while n Nse do 6: prompts employing prompt engineering techniques based on N, Vp, Fitness, P to establish LLM prompt; 7: Pnew use prompts to instruct LLM to generate new offspring solutions; 8: P, Fitness, nost Selector(P, Pnew, code, nost); 9: if nost Nost, t 0 then 10: t t vd ; 11: end if 12: n n N ; 13: end while 14: Pest the best solutions in P; 15: return Pest Algorithm 2 LLMSA algorithm.\n\n--- Segment 17 ---\nFirst we construct prompts based on parameters N and V (lines 2-3), then instruct 2 8 Running Title for Header Algorithm 1 LLMGA algorithm. Input: Design space ds, design space size S, pragma possible values Vp, maximum number of explorations Nse, population size N, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t; Output: Pareto optimal solution set Pest; 1: n 1, nost 0; 2: prompts employing prompt engineering techniques based on N, Vp to establish LLM prompt; 3: P use prompts to instruct LLM to initialize N solutions; 4: Fitness Evaluator (P, code); 5: while n Nse do 6: prompts employing prompt engineering techniques based on N, Vp, Fitness, P to establish LLM prompt; 7: Pnew use prompts to instruct LLM to generate new offspring solutions; 8: P, Fitness, nost Selector(P, Pnew, code, nost); 9: if nost Nost, t 0 then 10: t t vd ; 11: end if 12: n n N ; 13: end while 14: Pest the best solutions in P; 15: return Pest Algorithm 2 LLMSA algorithm. Input: Design space ds, design space size S, pragma possible values Vp, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t, population size N, initial temperature Ti, stop iteration temperature Ts, cooling rate rcs; Output: Pareto optimal solution set Pest; 1: nt Ti, nost 0; 2: P, Fitness establish prompt to instruct LLM to initialize N solutions and evaluate each solution in P; 3: while nt Ts do 4: Ptemp, fitness, nost establish LLM prompt to instruct LLM to generate neighbor solutions Pnew, evaluate each solution in Pnew and select the top solutions into Ptemp; 5: Cost Fitnessavg fitnessavg; 6: if Cost then 7: P Ptemp, Fitness fitness with Prob 1 exp Cost nt ; 8: else 9: P Ptemp, Fitness fitness; 10: end if 11: if nost Nost, t 0 then 12: t t vd ; 13: end if 14: nt nt 1 rcs ; 15: end while 16: Pest the best solutions in P; 17: return Pest LLMs to generate an initial population, which is evaluated by a pre-trained model to obtain Fitness (line 4).\n\n--- Segment 18 ---\nInput: Design space ds, design space size S, pragma possible values Vp, maximum number of explorations Nse, population size N, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t; Output: Pareto optimal solution set Pest; 1: n 1, nost 0; 2: prompts employing prompt engineering techniques based on N, Vp to establish LLM prompt; 3: P use prompts to instruct LLM to initialize N solutions; 4: Fitness Evaluator (P, code); 5: while n Nse do 6: prompts employing prompt engineering techniques based on N, Vp, Fitness, P to establish LLM prompt; 7: Pnew use prompts to instruct LLM to generate new offspring solutions; 8: P, Fitness, nost Selector(P, Pnew, code, nost); 9: if nost Nost, t 0 then 10: t t vd ; 11: end if 12: n n N ; 13: end while 14: Pest the best solutions in P; 15: return Pest Algorithm 2 LLMSA algorithm. Input: Design space ds, design space size S, pragma possible values Vp, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t, population size N, initial temperature Ti, stop iteration temperature Ts, cooling rate rcs; Output: Pareto optimal solution set Pest; 1: nt Ti, nost 0; 2: P, Fitness establish prompt to instruct LLM to initialize N solutions and evaluate each solution in P; 3: while nt Ts do 4: Ptemp, fitness, nost establish LLM prompt to instruct LLM to generate neighbor solutions Pnew, evaluate each solution in Pnew and select the top solutions into Ptemp; 5: Cost Fitnessavg fitnessavg; 6: if Cost then 7: P Ptemp, Fitness fitness with Prob 1 exp Cost nt ; 8: else 9: P Ptemp, Fitness fitness; 10: end if 11: if nost Nost, t 0 then 12: t t vd ; 13: end if 14: nt nt 1 rcs ; 15: end while 16: Pest the best solutions in P; 17: return Pest LLMs to generate an initial population, which is evaluated by a pre-trained model to obtain Fitness (line 4). The process then enters an iterative phase: prompts are dynamically rebuilt using current P and Fitness to guide LLMs in generating new solutions Pnew.\n\n--- Segment 19 ---\nInput: Design space ds, design space size S, pragma possible values Vp, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t, population size N, initial temperature Ti, stop iteration temperature Ts, cooling rate rcs; Output: Pareto optimal solution set Pest; 1: nt Ti, nost 0; 2: P, Fitness establish prompt to instruct LLM to initialize N solutions and evaluate each solution in P; 3: while nt Ts do 4: Ptemp, fitness, nost establish LLM prompt to instruct LLM to generate neighbor solutions Pnew, evaluate each solution in Pnew and select the top solutions into Ptemp; 5: Cost Fitnessavg fitnessavg; 6: if Cost then 7: P Ptemp, Fitness fitness with Prob 1 exp Cost nt ; 8: else 9: P Ptemp, Fitness fitness; 10: end if 11: if nost Nost, t 0 then 12: t t vd ; 13: end if 14: nt nt 1 rcs ; 15: end while 16: Pest the best solutions in P; 17: return Pest LLMs to generate an initial population, which is evaluated by a pre-trained model to obtain Fitness (line 4). The process then enters an iterative phase: prompts are dynamically rebuilt using current P and Fitness to guide LLMs in generating new solutions Pnew. The exploration engine employs the pre-trained predictive models to assess the Fitness of each new solution in the current population and selects top configurations, updating both the current population P and returning the fitness of population Fitness (line 8). The fitness is primarily composed of two key metrics: estimated latency derived from critical path analysis and resource utilization metrics including LUT, DSP, FF and BRAM usage. The temperature parameter in LLMs governs the stochasticity of text generation during sampling: elevated temperature values induce higher randomness in output generation, while reduced temperature settings promote deterministic responses. This mechanism directly mirrors the exploration-exploitation trade-off inherent in DSE optimization processes. To better balance the exploration-exploitation trade-off, we implement an adaptive temperature mechanism specifically through dynamic updating of LLMs temperature parameters (lines 9-11) [34]. This process 9 Running Title for Header Algorithm 3 LLMACO algorithm.\n\n--- Segment 20 ---\nTo better balance the exploration-exploitation trade-off, we implement an adaptive temperature mechanism specifically through dynamic updating of LLMs temperature parameters (lines 9-11) [34]. This process 9 Running Title for Header Algorithm 3 LLMACO algorithm. Input: Design space ds, design space size S, pragma possible values Vp, maximum number of explorations Nse, the number of ants Nant, graph code code, optimal solution stagnation threshold Nost, LLM temperature decay rate vd, LLM temperature t, pheromone evaporation ratio ρ; Output: Pareto optimal solution set Pest; 1: n 1, nost 0; 2: Initializes a pheromone matrix Pm based on the shape of Vp; 3: P, Fitness establish prompt to instruct LLM to initialize N solutions and evaluate each solution in P; 4: while n Nse do 5: P, Fitness, nost establish LLM prompt to instruct LLM to generate new solutions Pnew based on pheromone matrix Pm, evaluate each solution in Pnew and select the top solutions into P; 6: Pheromone evaporation Pm ρ Pm; 7: Update pheromone matrix Pm τi(c) τi(c) 0.1; 8: if nost Nost, t 0 then 9: t t vd ; 10: end if 11: n n Nant ; 12: end while 13: Pest the best solutions in P; 14: return Pest repeats until the loop terminates, at which point the solutions in P are added to Pest, and the Pareto-optimal solution set Pest is returned (lines 14-15). Algorithm 2 presents an overview of LLM-enhanced simulated annealing algorithm (LLMSA). LLMSA maintains fundamental alignment with LLMGA for integrating LLM in lines 2 and 4 (the only difference is that we instruct LLM to generate neigbor solutions as new solutions). The algorithm implements a conditional check on value Cost: if Cost 0, it computes the probability for potentially accepting inferior solutions. if Cost 0, immediate acceptance occurs (indicating newly generated solutions surpass the current optimal set). This dual-branch mechanism enables dynamic balance between exploration ( Cost 0 cases) and exploitation ( Cost 0 scenarios) during optimization iterations(lines 5-10).\n\n--- Segment 21 ---\nif Cost 0, immediate acceptance occurs (indicating newly generated solutions surpass the current optimal set). This dual-branch mechanism enables dynamic balance between exploration ( Cost 0 cases) and exploitation ( Cost 0 scenarios) during optimization iterations(lines 5-10). The probability formula in line 11 exhibits temperature-dependent behavior: acceptance probability for inferior solutions increases with higher temperatures and decreases with lower thermal states. Algorithm 3 presents an overview of LLM-enhanced ant colony optimization algorithm (LLMACO). To preserve ACO s core characteristics, the algorithm 3 retains the pheromone matrix mechanism. It initializes a pheromone matrix Pm encoding concentration values for each pragma option (line 2). While maintaining similar LLMGA operations, the prompt construction phase explicitly injects pheromone matrix data to LLMs (lines 3-5). Before each iteration concludes, pheromone evaporation and concentration updates are applied to values in current elite solutions (lines 6-7). Subsequent procedures mirror LLMGA, crucially replacing traditional ACO s probabilistic selection formulas (requiring complex computations) with LLM-driven decision-making. 5 Experiments 5.1 Experimental Setup In the experiments, the datasets are sourced from SOTA works GNN-DSE [32] and HGBO-DSE [15]. The GNN-DSE dataset, containing latency and resource usage labels, is extracted from HLS reports, while the HGBO-DSE dataset with PPA results is derived from post-implementation reports. Since the proposed method can be used for predicting both the post-HLS QoRs and the post-implementation QoRs, we compare it with SOTA works on the aforementioned two datasets respectively. To evaluate the proposed DSE algorithm, we use the most accurate prediction model based on experimental evaluation as the estimator for DSE, which collaborates with LLMMH in Section 4.3 to ultimately obtain the Pareto-optimal solution set. The applications in the GNN-DSE dataset are sourced from MachSuite [35] and Polyhedral [36] benchmarks, which involve 7 applications for training, testing, and validation and 5 for DSE exploration, while the HGBO-DSE dataset includes 10 applications from MachSuite, with 6 for training, testing, and validation and 4 for inference.\n\n--- Segment 22 ---\nTo evaluate the proposed DSE algorithm, we use the most accurate prediction model based on experimental evaluation as the estimator for DSE, which collaborates with LLMMH in Section 4.3 to ultimately obtain the Pareto-optimal solution set. The applications in the GNN-DSE dataset are sourced from MachSuite [35] and Polyhedral [36] benchmarks, which involve 7 applications for training, testing, and validation and 5 for DSE exploration, while the HGBO-DSE dataset includes 10 applications from MachSuite, with 6 for training, testing, and validation and 4 for inference. The experiments are conducted on the AMD Ultrascale MPSoC ZCU104. The benchmarks are synthesized using Vitis-HLS 2022.1 and Vivado 2022.1 to collect ground-truth latency and resource utilization metrics, which serve as training labels for the model. 10 Running Title for Header Table 1: The training applications used for post-HLS QoR prediction. Kernel name Description Pragmas Design configs aes a common block cipher 3 45 gemm-blocked A blocked version of matrix multiplication 9 2,314 gemm-ncubed O(n3) algorithm for dense matrix multiplication. 7 7,792 nw A dynamic programming algorithm for optimal sequence alignment 6 15,288 spmv-crs Sparse matrix-vector multiplication, using variable-length neighbor lists 3 114 spmv-ellpack Sparse matrix-vector multiplication, using fixed-size neighbor lists 3 114 stencil3d A three-dimensional stencil computation 7 7,591 Table 2: RMSE loss of different CoGNNs on unseen applications. The top models are marked in bold.\n\n--- Segment 23 ---\n7 7,792 nw A dynamic programming algorithm for optimal sequence alignment 6 15,288 spmv-crs Sparse matrix-vector multiplication, using variable-length neighbor lists 3 114 spmv-ellpack Sparse matrix-vector multiplication, using fixed-size neighbor lists 3 114 stencil3d A three-dimensional stencil computation 7 7,591 Table 2: RMSE loss of different CoGNNs on unseen applications. The top models are marked in bold. Model Latency LUT DSP FF BRAM All CoGNNs(α,α) 0.4315 0.0043 0.0114 0.0155 0.0270 0.4897 CoGNNs(α,β) 0.7101 0.0084 0.0141 0.0279 0.0333 0.7937 CoGNNs(β,β) 0.5077 0.0053 0.0091 0.0146 0.0353 0.5720 CoGNNs(β,α) 0.3557 0.0039 0.0075 0.0152 0.0244 0.4067 CoGNNs(γ,β) 0.6748 0.0067 0.0167 0.0229 0.0331 0.7541 CoGNNs(γ,γ) 0.5825 0.0063 0.0126 0.0252 0.0324 0.6592 CoGNNs(α,γ) 0.4131 0.0174 0.0276 0.1193 0.0297 0.6072 CoGNNs(β,γ) 0.6456 0.0112 0.0248 0.0434 0.0346 0.7596 Both datasets are randomly divided into 70 for training, 15 for testing, and 15 for validation. In the experiments, CoGNNs environment network has three layers and the action network has two layers, with MEANGNN, SUMGNN, and GCN as the main GNN types selectable for both networks, from which the most suitable combination for the QoR prediction task is chosen.\n\n--- Segment 24 ---\nModel Latency LUT DSP FF BRAM All CoGNNs(α,α) 0.4315 0.0043 0.0114 0.0155 0.0270 0.4897 CoGNNs(α,β) 0.7101 0.0084 0.0141 0.0279 0.0333 0.7937 CoGNNs(β,β) 0.5077 0.0053 0.0091 0.0146 0.0353 0.5720 CoGNNs(β,α) 0.3557 0.0039 0.0075 0.0152 0.0244 0.4067 CoGNNs(γ,β) 0.6748 0.0067 0.0167 0.0229 0.0331 0.7541 CoGNNs(γ,γ) 0.5825 0.0063 0.0126 0.0252 0.0324 0.6592 CoGNNs(α,γ) 0.4131 0.0174 0.0276 0.1193 0.0297 0.6072 CoGNNs(β,γ) 0.6456 0.0112 0.0248 0.0434 0.0346 0.7596 Both datasets are randomly divided into 70 for training, 15 for testing, and 15 for validation. In the experiments, CoGNNs environment network has three layers and the action network has two layers, with MEANGNN, SUMGNN, and GCN as the main GNN types selectable for both networks, from which the most suitable combination for the QoR prediction task is chosen. Meanwhile, the hidden layer dimension of all models is set to 128, and the Adam optimizer is used, with the GNN-DSE dataset experiments use a batch size of 64, a learning rate of 0.001, and over 500 iterations, while the HGBO-DSE dataset experiments employing a batch size of 128, a learning rate of 0.001, and over 250 iterations.\n\n--- Segment 25 ---\nIn the experiments, CoGNNs environment network has three layers and the action network has two layers, with MEANGNN, SUMGNN, and GCN as the main GNN types selectable for both networks, from which the most suitable combination for the QoR prediction task is chosen. Meanwhile, the hidden layer dimension of all models is set to 128, and the Adam optimizer is used, with the GNN-DSE dataset experiments use a batch size of 64, a learning rate of 0.001, and over 500 iterations, while the HGBO-DSE dataset experiments employing a batch size of 128, a learning rate of 0.001, and over 250 iterations. To be consistent with GNN-DSE [32] and HGBO-DSE [15], Root Mean Squared Error (RMSE) loss is used to quantify the post-HLS QoR prediction quality, and Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) are used to evaluate the accuracy of the models for the post-implementation QoR prediction. The CoGNNs model proposed in this paper, along with the baseline models and the DSE algorithms, are all implemented in Python. 5.2 Evaluation of Post-HLS QoR Prediction Accuracy In this experiment, to validate the prediction accuracy of CoGNNs for estimating post-HLS QoR metrics, we lever- age the architectural flexibility of CoGNNs to design eight combinations of environment and action networks for predicting latency, LUT, DSP, FF and BRAM usage, comparing their performance against baseline models. For clarity, we denote MEANGNNs [37], SUMGNNs [37], and GCN [24] as α, β, γ, respectively, to describe different CoGNNs configurations. The SUMGNNs and MEANGNNs mentioned here are two fundamental variants of MPNNs that differ in their neighborhood aggregation functions, with SUMGNNs using summation-based aggregation and MEANGNNs employing mean-based aggregation. Additionally, we evaluate baseline models including GNN-DSE [32], HGP SAGE GF [15], IronMan-Pro [20], PNA-R [12], and PowerGear [14].\n\n--- Segment 26 ---\nThe SUMGNNs and MEANGNNs mentioned here are two fundamental variants of MPNNs that differ in their neighborhood aggregation functions, with SUMGNNs using summation-based aggregation and MEANGNNs employing mean-based aggregation. Additionally, we evaluate baseline models including GNN-DSE [32], HGP SAGE GF [15], IronMan-Pro [20], PNA-R [12], and PowerGear [14]. While PNA-R and PowerGear are primarily designed for post-implementation prediction tasks (PowerGear proposes GNN to predict power), we adapt these baseline models for post-HLS scenarios by modifying their input dimensions to align with the feature space of the GNN-DSE dataset and restructuring their architectures using the PyTorch Geometric framework. These adaptations enable competitive performance in post-HLS prediction tasks, demonstrating the models flexibility across different hardware abstraction levels. As shown in Table 1, the HLS prediction dataset primarily comprises benchmarks such as aes and gemm-blocked. GNN-DSE [32] employs a Redis-based service to implement an online dataset provisioning mechanism, ensuring strict separation between training and inference datasets while maintaining benchmark diversity across both data splits. As shown in Table 2, we conducted extensive experiments on this dataset and recorded the RMSE of models. RMSE is quared-error metric measuring prediction accuracy via root-averaged discrepancies between predicted and actual values, preserving data units. For latency prediction, CoGNNs(β, α) achieves the best performance, followed by CoGNNs(α, γ) with prediction errors of 0.3557 and 0.4131, respectively. In contrast, CoGNNs predicts multiple targets within an integrated framework, achieving high accuracy overall. For DSP and FF resource prediction, CoGNNs(β, α) 11 Running Title for Header Figure 5: Reduction rates of RMSE loss achieved by CoGNNs(β,α) over other CoGNNs (For FF, we present the reduction rates of RMSE loss achieved by CoGNNs(β,β) over other CoGNNs). The bar chart quantifies each model s prediction error reduction ratios (PERR) relative to the top-performing model, while the line chart tracks the evolving PERR trend across evaluation metrics.\n\n--- Segment 27 ---\nFor DSP and FF resource prediction, CoGNNs(β, α) 11 Running Title for Header Figure 5: Reduction rates of RMSE loss achieved by CoGNNs(β,α) over other CoGNNs (For FF, we present the reduction rates of RMSE loss achieved by CoGNNs(β,β) over other CoGNNs). The bar chart quantifies each model s prediction error reduction ratios (PERR) relative to the top-performing model, while the line chart tracks the evolving PERR trend across evaluation metrics. Table 3: RMSE loss of CoGNNs(β,α) and SOTA models on unseen applications. The top models are marked in bold. Model Latency LUT DSP FF BRAM All GNN-DSE[32] 0.5359 0.0762 0.1253 0.0632 0.0515 0.8521 HGP SAGE GF[15] 0.9519 0.0152 0.0270 0.0895 0.0362 1.1197 IronMan-Pro[20] 0.6778 0.0081 0.0161 0.0399 0.0326 0.7745 PNA-R[12] 1.3728 0.0144 0.0307 0.0811 0.0610 1.5600 PowerGear[14] 1.3956 0.0177 0.0290 0.0908 0.0432 1.5764 CoGNNs(β,α) 0.3557 0.0039 0.0075 0.0152 0.0244 0.4067 and CoGNNs(β, β) deliver the best results. For the prediction of LUT and BRAM, CoGNNs(β, α)and CoGNNs(β, β) achieve the smallest prediction errors. Although CoGNNs(β, α) has a slightly higher prediction error (0.0006) than CoGNNs(β, β) in FF prediction, its overall error is 0.083 lower than CoGNNs(α, α). Thus, we conclude that CoGNNs(β, α) exhibits the best CoGNNs combination on the GNN-DSE dataset. As empirically validated in Fig.\n\n--- Segment 28 ---\nThus, we conclude that CoGNNs(β, α) exhibits the best CoGNNs combination on the GNN-DSE dataset. As empirically validated in Fig. 5, the CoGNN(β, α) demonstrates statistically significant error reduction across multiple prediction targets when compared with other configurations of CoGNNs. In comparison with other combination, CoGNN(β, α) demonstrates statistically significant reductions in prediction errors across all metrics except FF prediction targets. Specifically regarding the Latency metric, CoGNN(β, α) achieves a minimum RMSE reduction ratio of 13.89 . For DSP indicators, the maximum prediction error reduction reaches 72.83 when using CoGNNs. Notably, CoGNNs maintains consistent superiority in LUT and BRAM metrics, exhibiting an average prediction error reduction of 44.57 and 23.64 compared to alternative models. As shown in Table 3, CoGNNs(β, α) also exhibits exceptional prediction capabilities compared to SOTA. Compared to GNN-DSE, HGP SAGE GF, IronMan-Pro, PNA-R, and PowerGear, the reduction rates of RMSE values are 47.69 , 36.32 , 52.51 , 26.07 , and 25.80 of those of the respective baseline models. This superior performance stems from CoGNNs task-specific feature adaptation, as highlighted in its design. All other CoGNNs combinations outperform the baseline models except IronMan-Pro and GNN-DSE. This not only underscores CoGNNs transferability, but also demonstrates its ability to dynamically reconfigure the computational graph topology based on task objectives, enabling more effective feature extraction and improved prediction accuracy. Unlike traditional MPNNs, which focus primarily on minimizing loss during training, CoGNNs prioritizes the extraction of graph-level and node-level features to achieve 12 Running Title for Header Figure 6: Reduction rates of RMSE loss achieved by CoGNNs(β,α) over baseline models. Table 4: The benchmarks used for post-implementation QoR prediction.\n\n--- Segment 29 ---\nUnlike traditional MPNNs, which focus primarily on minimizing loss during training, CoGNNs prioritizes the extraction of graph-level and node-level features to achieve 12 Running Title for Header Figure 6: Reduction rates of RMSE loss achieved by CoGNNs(β,α) over baseline models. Table 4: The benchmarks used for post-implementation QoR prediction. Kernel name Description Func Array Loop Params aes a common block cipher 12 2 11 30 bfs-bulk Data-oriented version of breadth-first search 1 4 3 25 fft-strided Recursive formulation of the Fast Fourier Transform 1 4 2 27 gemm-ncubed O(n3) algorithm for dense matrix multiplication. 1 3 3 27 md-knn n-body molecular dynamics, using k-nearest neighbors to compute only local forces 1 7 2 42 nw A dynamic programming algorithm for optimal sequence alignment 1 6 7 54 sort-radix Sorts an integer array by comparing 4-bits blocks at a time 7 4 11 72 spmv-ellpack Sparse matrix-vector multiplication, using fixed-size neighbor lists 1 4 2 25 stencil3d A three-dimensional stencil computation 1 2 9 43 viterbi A dynamic programing method for computing probabilities on a Hidden Markov model 1 6 7 52 task objectives, with loss reduction being a natural consequence of this process. As evidenced by Fig. 6, CoGNN architectures demonstrate statistically significant improvements over benchmark baseline models. The CoGNN(β, α) variant exhibits superior performance across all evaluation metrics, particularly achieving at least 33.63 RMSE reduction in latency measurements. With particular strength in hardware utilization metrics, the framework achieves peak error reduction of 94.01 for DSP and mean error reductions of 74.44 (LUT), 77.08 (FF), and 42.78 (BRAM) respectively, outperforming existing approaches in FPGA resource estimation accuracy. In this experiment, the combination of CoGNNs components significantly influences prediction accuracy. We observe that when the environment and action networks utilize simpler MPNNs like SUMGNN and MEANGNN, prediction errors are substantially lower compared to using more complex models such as GCN. This phenomenon is fundamentally tied to CoGNNs operational mechanism(it dynamically selects graph topologies to regulate message flow between nodes).\n\n--- Segment 30 ---\nWe observe that when the environment and action networks utilize simpler MPNNs like SUMGNN and MEANGNN, prediction errors are substantially lower compared to using more complex models such as GCN. This phenomenon is fundamentally tied to CoGNNs operational mechanism(it dynamically selects graph topologies to regulate message flow between nodes). In contrast, complex MPNNs like GCN inherently prioritize the importance of information from neighboring nodes, which contradicts CoGNNs design philosophy. CoGNNs autonomously learns which neighbors information is relevant for each node, effectively disregarding messages from unimportant neighbors. Consequently, integrating MPNNs like GCN, which predefine neighbor importance, results in suboptimal performance within the CoGNNs framework. 5.3 Evaluation of Post-Implementation QoR Prediction Accuracy To further validate the accuracy of the proposed CoGNNs, we also evaluate its performance on post-implementation QoR prediction. For LUT, FF, CP and POWER, we employ MAPE as the loss function. For DSP and BRAM where zero-value occurrences exist, we adopt MAE to avoid division-by-zero errors during model optimization.The HGBO- 13 Running Title for Header Table 5: MAPE and MAE of CoGNN(β, α) and SOTA models on unseen applications. MAPE( ) MAE Model LUT FF CP POWER DSP BRAM HGP SAGE GF[15] 9.05 9.87 9.87 13.60 1.4282 0.1606 IronMan-Pro[20] 7.35 6.30 10.98 16.34 0.6198 0.1917 PowerGear[14] - - - 11.61 - - PNA-R[12] 6.40 6.52 14.03 - - - CoGNNs(β, α) 6.82 5.98 7.54 10.01 0.5339 0.1035 DSE dataset provides labels for LUT, DSP, FF, BRAM, CP and POWER, all extracted from implementation reports. In this experiment, we compare the MAPE and MAE of CoGNNs(β, α) on the GNN-DSE dataset against baseline models including HGP SAGE GF, IronMan-Pro, PowerGear and PNA-R.\n\n--- Segment 31 ---\nMAPE( ) MAE Model LUT FF CP POWER DSP BRAM HGP SAGE GF[15] 9.05 9.87 9.87 13.60 1.4282 0.1606 IronMan-Pro[20] 7.35 6.30 10.98 16.34 0.6198 0.1917 PowerGear[14] - - - 11.61 - - PNA-R[12] 6.40 6.52 14.03 - - - CoGNNs(β, α) 6.82 5.98 7.54 10.01 0.5339 0.1035 DSE dataset provides labels for LUT, DSP, FF, BRAM, CP and POWER, all extracted from implementation reports. In this experiment, we compare the MAPE and MAE of CoGNNs(β, α) on the GNN-DSE dataset against baseline models including HGP SAGE GF, IronMan-Pro, PowerGear and PNA-R. It is important to note that PowerGear was initially designed to predict power, while PNA-R was created to predict metrics such as LUT, FF, and CP. In accordance with the experiments conducted in [15], we only present the results for the target metrics of PowerGear and PNA-R. Notably, the HGBO-DSE dataset has a node feature dimension of 15, significantly smaller than the GNN-DSE dataset s 153-dimensional features. To account for this, we reduce the number of iterations, while ensuring that each model undergoes at least 250 training epochs. It is worth noting that HGBO-DSE s work does not address and evaluate the model generalization, including for all the reproduced baseline models. In this experiment, we partition the dataset into two subsets comprising 10 applications. As shown in Table 4, six applications (aes, bfs, fft, gemm, md, nw) are used for training, while the remaining applications are reserved as unseen applications for inference.\n\n--- Segment 32 ---\nIn this experiment, we partition the dataset into two subsets comprising 10 applications. As shown in Table 4, six applications (aes, bfs, fft, gemm, md, nw) are used for training, while the remaining applications are reserved as unseen applications for inference. As shown in Table 5, it can be observed that CoGNNs(β,α) achieves the smallest prediction errors for FF, CP, POWER, DSP, and BRAM compared to baseline models, with values of 5.98 , 7.54 , and 10.01 in terms of MAPE, 0.5339 and 0.1033 in terms of MAE, respectively. For LUT prediction, PNA-R exhibits the lowest error at 6.40, while CoGNNs(β,α) closely follows with 6.82, demonstrating a marginal difference. Notably, in POWER prediction, CoGNNs(β,α) reduces the error to 10.01 , outperforming PowerGear, a model specifically designed for power prediction, which achieves 11.61 . The performance of HGP SAGE GF on unseen applications underscores that complex model architectures or mere structural stacking cannot enhance performance in HLS or implementation prediction tasks. In summary, the task-adaptive CoGNN demonstrates exceptional performance in predicting post-place-and-route resource usage and exhibits strong generalization capabilities. In general, as can be seen from Section 5.2 and Section 5.3, CoGNNs(β, α) achieves the best prediction performance on these two datasets. Facts have proven that in the post-HLS QoR prediction and the post-implementation QoR prediction, complex model structures like PNA-R may not yield excellent prediction results. The key lies in improving the model structure according to the task objectives and extracting high-dimensional features from the CDFG, which can lead to better prediction performance. 5.4 Evaluation of Design Space Exploration In the DSE experiment, we applied our proposed LLMMH framework to explore five unseen applications (atax, doitgen, gemm-p, heat-3d, and mvt) that were excluded from the training phase, utilizing the CoGNNs(β, α) model trained in Section 5.3 as the evaluator. For each application, the design configurations encompass all possible combinations of the pragma values and the C C code.\n\n--- Segment 33 ---\n5.4 Evaluation of Design Space Exploration In the DSE experiment, we applied our proposed LLMMH framework to explore five unseen applications (atax, doitgen, gemm-p, heat-3d, and mvt) that were excluded from the training phase, utilizing the CoGNNs(β, α) model trained in Section 5.3 as the evaluator. For each application, the design configurations encompass all possible combinations of the pragma values and the C C code. We implemented a Python-based explorer to convert these configurations into graph data via LLVM and ProGraML, enabling input into the prediction model. The evaluation results are stored, and the top-ranked configurations are identified through iterative refinement. The primary objective of DSE is to identify the Pareto-optimal configuration set. To quantify the quality of the approximate Pareto-optimal set, we employ ADRS metric [28]: ADRS(Γ, Ω) 1 Γ X λ Γ min µ Ωf (λ, µ) (9) f(λ, µ) max A(λ) A(µ) A(µ) , L(λ) L(µ) L(µ) (10) A(i) 1 4 FFi FFmax LUTi LUTmax BRAMi BRAMmax DSPi DSPmax (11) L(i) 1 Latencyi (12) 14 Running Title for Header Table 6: The unseen applications and the number of design configurations used in DSE. Benchmark Description Pragmas Design configs atax Matrix Transpose and Vector Multiplication 5 3,354 doitgen Multi-resolution analysis kernel (MADNESS) 6 179 gemm-p Matrix-multiply C alpha.A.B beta.C 8 409,905 heat-3d Heat equation over 3D data domain 11 71,511 mvt Matrix Vector Product and Transpose 8 3,059,001 Figure 7: Convergence Curves: Mean ADRS achieved by LLMMHs, EA, SA, ACO as the number of explored designs increases.\n\n--- Segment 34 ---\nTo quantify the quality of the approximate Pareto-optimal set, we employ ADRS metric [28]: ADRS(Γ, Ω) 1 Γ X λ Γ min µ Ωf (λ, µ) (9) f(λ, µ) max A(λ) A(µ) A(µ) , L(λ) L(µ) L(µ) (10) A(i) 1 4 FFi FFmax LUTi LUTmax BRAMi BRAMmax DSPi DSPmax (11) L(i) 1 Latencyi (12) 14 Running Title for Header Table 6: The unseen applications and the number of design configurations used in DSE. Benchmark Description Pragmas Design configs atax Matrix Transpose and Vector Multiplication 5 3,354 doitgen Multi-resolution analysis kernel (MADNESS) 6 179 gemm-p Matrix-multiply C alpha.A.B beta.C 8 409,905 heat-3d Heat equation over 3D data domain 11 71,511 mvt Matrix Vector Product and Transpose 8 3,059,001 Figure 7: Convergence Curves: Mean ADRS achieved by LLMMHs, EA, SA, ACO as the number of explored designs increases. where Γ represents the reference Pareto-optimal set, Ωdenotes the approximate Pareto-optimal set, and the function f computes the distance between λ and µ. Equations 11 and 12 formally define the functionality of A( ) and L( ) respectively, where FFmax, LUTmax, BRAMmax, and DSPmax represent the maximum available resources on the target FPGA platform. The framework quantifies design performance through latency measurements as the primary evaluation metric. A lower ADRS value indicates a higher accuracy of the approximate Pareto-optimal set relative to the reference. To assess the performance of LLMMH varaints (LLMGA, LLMSA and LLMACO) , we compare them with three meta-heuristic algorithms: GA [4], SA [5], ACO [6], as referenced in [17]. The reference Pareto-optimal set is collected from the results generated by the exhaustive DSE algorithm proposed in [32].\n\n--- Segment 35 ---\nTo assess the performance of LLMMH varaints (LLMGA, LLMSA and LLMACO) , we compare them with three meta-heuristic algorithms: GA [4], SA [5], ACO [6], as referenced in [17]. The reference Pareto-optimal set is collected from the results generated by the exhaustive DSE algorithm proposed in [32]. To ensure fair comparison of algorithm performance, the parameter configurations for SA, GA, ACO and LLMMH variants maintain unified settings: both population size N and maximum number of explorations Nse are set to identical values across all algorithms. Additionally, due to the excessively large design space sizes of certain benchmarks such as mvt, we have imposed a one-hour exploration time limit for each benchmark. The adaptive parameter scheme is dynamically adjusted based on the design space size S as follows: (Nse, N) (0.00005S, 30) S 107 (0.005S, 30) 105 S 106 (0.05S, 30) 104 S 105 (0.3S, 30) 500 S 104 (0.5S, 10) S 500 (13) 15 Running Title for Header Table 7: ADRS Results and Runtime on Unseen Applications.\n\n--- Segment 36 ---\nAdditionally, due to the excessively large design space sizes of certain benchmarks such as mvt, we have imposed a one-hour exploration time limit for each benchmark. The adaptive parameter scheme is dynamically adjusted based on the design space size S as follows: (Nse, N) (0.00005S, 30) S 107 (0.005S, 30) 105 S 106 (0.05S, 30) 104 S 105 (0.3S, 30) 500 S 104 (0.5S, 10) S 500 (13) 15 Running Title for Header Table 7: ADRS Results and Runtime on Unseen Applications. Algorithm atax heat-3d doitgen gemm-p mvt Average ADRS Overall Runtime (s) SA 0 0.4053 0.0669 0.5567 0.5620 0.3182 213 GA 0 0.3532 0 0.2936 0.5557 0.2405 132 ACO 0.0043 0.2946 0 0.6081 0.5138 0.2842 151 LLMGA(deepseek-r1) 0.0086 0.1483 0 0.1633 0.3844 0.1409 9491 LLMGA(gpt-4o) 0 0.2085 0.0790 0.1837 0.4206 0.1784 4961 LLMGA(gpt-4.1) 0.0029 0.3182 0 0.2258 0.1549 0.1404 4752 LLMGA(o3-mini) 0 0.2829 0.0717 0.2007 0.3223 0.1755 8427 LLMSA(deepseek-r1) 0.0086 0.1053 0 0.1235 0.1668 0.0808 9836 LLMSA(gpt-4o) 0.0028 0.1481 0 0.1389 0.3619 0.1304 4317 LLMSA(gpt-4.1) 0 0.3237 0 0.2519 0.2416 0.1635 4931 LLMSA(o3-mini) 0.0028 0.2878 0.0245 0.2239 0.4565 0.1991 8206 LLMACO(deepseek-r1) 0.0086 0 0 0.0096 0.1516 0.0339 9998 LLMACO(gpt-4o) 0 0 0 0.0361 0.4297 0.0932 4716 LLMACO(gpt-4.1) 0 0.2096 0 0.2401 0.2131 0.1326 4158 LLMACO(o3-mini) 0 0.1863 0 0.2135 0.4084 0.1616 7659 LLMACO(deepseek-r1) Improv.\n\n--- Segment 37 ---\nThe adaptive parameter scheme is dynamically adjusted based on the design space size S as follows: (Nse, N) (0.00005S, 30) S 107 (0.005S, 30) 105 S 106 (0.05S, 30) 104 S 105 (0.3S, 30) 500 S 104 (0.5S, 10) S 500 (13) 15 Running Title for Header Table 7: ADRS Results and Runtime on Unseen Applications. Algorithm atax heat-3d doitgen gemm-p mvt Average ADRS Overall Runtime (s) SA 0 0.4053 0.0669 0.5567 0.5620 0.3182 213 GA 0 0.3532 0 0.2936 0.5557 0.2405 132 ACO 0.0043 0.2946 0 0.6081 0.5138 0.2842 151 LLMGA(deepseek-r1) 0.0086 0.1483 0 0.1633 0.3844 0.1409 9491 LLMGA(gpt-4o) 0 0.2085 0.0790 0.1837 0.4206 0.1784 4961 LLMGA(gpt-4.1) 0.0029 0.3182 0 0.2258 0.1549 0.1404 4752 LLMGA(o3-mini) 0 0.2829 0.0717 0.2007 0.3223 0.1755 8427 LLMSA(deepseek-r1) 0.0086 0.1053 0 0.1235 0.1668 0.0808 9836 LLMSA(gpt-4o) 0.0028 0.1481 0 0.1389 0.3619 0.1304 4317 LLMSA(gpt-4.1) 0 0.3237 0 0.2519 0.2416 0.1635 4931 LLMSA(o3-mini) 0.0028 0.2878 0.0245 0.2239 0.4565 0.1991 8206 LLMACO(deepseek-r1) 0.0086 0 0 0.0096 0.1516 0.0339 9998 LLMACO(gpt-4o) 0 0 0 0.0361 0.4297 0.0932 4716 LLMACO(gpt-4.1) 0 0.2096 0 0.2401 0.2131 0.1326 4158 LLMACO(o3-mini) 0 0.1863 0 0.2135 0.4084 0.1616 7659 LLMACO(deepseek-r1) Improv. over SA 89.34 LLMACO(deepseek-r1) Improv.\n\n--- Segment 38 ---\nAlgorithm atax heat-3d doitgen gemm-p mvt Average ADRS Overall Runtime (s) SA 0 0.4053 0.0669 0.5567 0.5620 0.3182 213 GA 0 0.3532 0 0.2936 0.5557 0.2405 132 ACO 0.0043 0.2946 0 0.6081 0.5138 0.2842 151 LLMGA(deepseek-r1) 0.0086 0.1483 0 0.1633 0.3844 0.1409 9491 LLMGA(gpt-4o) 0 0.2085 0.0790 0.1837 0.4206 0.1784 4961 LLMGA(gpt-4.1) 0.0029 0.3182 0 0.2258 0.1549 0.1404 4752 LLMGA(o3-mini) 0 0.2829 0.0717 0.2007 0.3223 0.1755 8427 LLMSA(deepseek-r1) 0.0086 0.1053 0 0.1235 0.1668 0.0808 9836 LLMSA(gpt-4o) 0.0028 0.1481 0 0.1389 0.3619 0.1304 4317 LLMSA(gpt-4.1) 0 0.3237 0 0.2519 0.2416 0.1635 4931 LLMSA(o3-mini) 0.0028 0.2878 0.0245 0.2239 0.4565 0.1991 8206 LLMACO(deepseek-r1) 0.0086 0 0 0.0096 0.1516 0.0339 9998 LLMACO(gpt-4o) 0 0 0 0.0361 0.4297 0.0932 4716 LLMACO(gpt-4.1) 0 0.2096 0 0.2401 0.2131 0.1326 4158 LLMACO(o3-mini) 0 0.1863 0 0.2135 0.4084 0.1616 7659 LLMACO(deepseek-r1) Improv. over SA 89.34 LLMACO(deepseek-r1) Improv. over GA 85.90 LLMACO(deepseek-r1) Improv.\n\n--- Segment 39 ---\nover SA 89.34 LLMACO(deepseek-r1) Improv. over GA 85.90 LLMACO(deepseek-r1) Improv. over ACO 88.07 In the experiments, we employ four foundation models (deepseek-r1 [38], gpt-4o [39], gpt-4.1 [40], o3-mini [41]) as surrogate models. In total, we evaluate 12 combinations of LLMMH (3 meta-heuristics 4 LLMs). As shown in Table 7, metaheuristics (GA, SA, ACO) achieve ADRS exceeding 0.5 on the mvt benchmark with large design space, aligning with our theoretical analysis of their exploration limitations. In contrast, our LLMMH framework demonstrates superior performance LLMACO(deepseek-r1) achieves ADRS as low as 0.0339. On small-scale benchmarks such as atax, LLMMH variants maintain comparable solution quality with meta-heuristic methods. Comprehensive experimental results demonstrate that LLMACO(deepseek-r1) achieves the optimal performance with the lowest ADRS, showing improvements of 89.34 , 85.90 , and 88.07 over SA, GA, and GA respectively. While LLMMH algorithms exhibit marginally inferior performance compared to traditional metaheuristics on small-scale design space benchmark due to LLMs inherent hallucination issues (a longstanding challenge in NLP where models generate semantically plausible but technically invalid outputs), holistic evaluation reveals the framework s superior overall performance across metrics. This demonstrates LLMMH s effectiveness in balancing exploration-exploitation tradeoffs despite occasional localized solution quality variations. This quantitatively verifies that our LLMMH framework not only resolves inherent limitations of meta-heuristics but also substantially outperforms them in solution quality. Fig. 7 illustrates convergence curves of high-performing LLMMH variants versus GA, SA, and EA. It can be observed that, for benchmarks with small design spaces, all algorithms demonstrate comparable convergence to Pareto-approximate optimal solutions within limited iterations. However, this comparative analysis quantitatively validates our core hypothesis: traditional meta-heuristics fundamentally underperform in exploring superior configurations under strict iteration budgets for large design spaces, whereas LLM-augmented operators effectively mitigate this limitation through intelligent solution generation, as evidenced by their accelerated convergence trajectories.\n\n--- Segment 40 ---\nIt can be observed that, for benchmarks with small design spaces, all algorithms demonstrate comparable convergence to Pareto-approximate optimal solutions within limited iterations. However, this comparative analysis quantitatively validates our core hypothesis: traditional meta-heuristics fundamentally underperform in exploring superior configurations under strict iteration budgets for large design spaces, whereas LLM-augmented operators effectively mitigate this limitation through intelligent solution generation, as evidenced by their accelerated convergence trajectories. Based on the overall runtime of each algorithm on five benchmarks (see Table 7), we also observed that the computational overhead of LLMMH variants remains significant due to the inference latency of large language models (particularly o3-mini and deepseek-r1), compounded by per-iteration API calls and network delays. 6 Conclusion In this paper, we present the CoGNNs-LLMMH framework, which comprises a GNN prediction model that dynamically adapts message-passing strategies based on task-specific characteristics and LLM-driven meta-heuristic algorithms for DSE. CoGNNs enhances LLMMH s exploration capabilities by providing high-precision QoR predictions for design configurations without relying on time-consuming EDA flows. Experimental results demonstrate that both CoGNNs and LLMMH outperform baseline models across multiple metrics. This work presents the first systematic implementation of LLMs in DSE. Beyond demonstrating LLMs capability to autonomously execute complex operations traditionally requiring human expertise, we aim to address the limitations of conventional heuristic-based methodologies that 16 Running Title for Header dominate current DSE practices. More significantly, our LLMMH framework demonstrates superior performance compared to traditional heuristic approaches. The proposed paradigm shift of leveraging LLMs for domain-specific knowledge-intensive tasks establishes a new methodology with promising potential for widespread adoption in DSE research and applications. The current implementation of LLMMH presents valuable opportunities for enhancement in future endeavors. By addressing specific areas for improvement, we can significantly elevate its effectiveness and impact. 1) Improving the selection of LLMs The current DSE experiment implemented multiple LLMs, though the investigation does not comprehensively encompass all established model archetypes. Notably, the hypothesis that LLMs specifically designed for mathematical reasoning and code generation might demonstrate superior configuration generation capabilities compared to conversational LLMs in DSE contexts remains to be rigorously validated through systematic comparative analysis.\n\n--- Segment 41 ---\n1) Improving the selection of LLMs The current DSE experiment implemented multiple LLMs, though the investigation does not comprehensively encompass all established model archetypes. Notably, the hypothesis that LLMs specifically designed for mathematical reasoning and code generation might demonstrate superior configuration generation capabilities compared to conversational LLMs in DSE contexts remains to be rigorously validated through systematic comparative analysis. 2) Reducing the runtime of LLMMH In the experiments, we employed multiple LLMs, necessitating the implementation of Langchain for online LLM orchestration. However, the algorithms within our proposed LLMMH framework currently exhibit elevated runtime overhead, primarily attributed to the per-iteration API calls through Langchain interfaces and the substantial parameter sizes of LLMs. This bottleneck is not insurmountable potential optimizations include establishing multi-threaded API connections or developing locally deployed distilled LLMs specialized for DSE tasks (given the correlation between LLM inference latency and parameter count). These approaches hold significant promise for runtime reduction, constituting a key focus of our ongoing research efforts. 3) Optimizing prompt engineering techniques Experimental results empirically demonstrate that the output quality of LLMs exhibits strong dependency on prompt configuration design. Dedicated prompt engineering techniques, such as self-reflective prompts [42] and directional-stimulus prompts [43], may lead to more significant performance enhancements in LLMMH s evolutionary optimization processes. References [1] Ben Finkelshtein, Xingyue Huang, Michael Bronstein, and Ceylan. Cooperative graph neural networks. In Proceedings of the 41st International Conference on Machine Learning, ICML 24. JMLR.org, 2024. [2] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018. [3] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi- supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. [4] Benjamin Carrion Schafer. Parallel high-level synthesis design space exploration for behavioral ips of exact latencies. ACM Trans. Des. Autom. Electron. Syst., 22(4), May 2017. [5] Benjamin Carrion Schafer.\n\n--- Segment 42 ---\nSyst., 22(4), May 2017. [5] Benjamin Carrion Schafer. Probabilistic multiknob high-level synthesis design space exploration acceleration. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 35(3):394 406, 2016. [6] Benjamin Carrion Schafer, Takashi Takenaka, and Kazutoshi Wakabayashi. Adaptive simulated annealer for high level synthesis design space exploration. In 2009 International Symposium on VLSI Design, Automation and Test, pages 106 109, 2009. [7] Steve Dai, Yuan Zhou, Hang Zhang, Ecenur Ustun, Evangeline FY Young, and Zhiru Zhang. Fast and accurate estimation of quality of results in high-level synthesis with machine learning. In 2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM), pages 129 132. IEEE, 2018. [8] Guanwen Zhong, Alok Prakash, Siqi Wang, Yun Liang, Tulika Mitra, and Smail Niar. Design space exploration of fpga-based accelerators with multi-level parallelism. In Design, Automation Test in Europe Conference Exhibition (DATE), 2017, pages 1141 1146. IEEE, 2017. [9] Hosein Mohammadi Makrani, Farnoud Farahmand, Hossein Sayadi, Sara Bondi, Sai Manoj Pudukotai Dinakarrao, Houman Homayoun, and Setareh Rafatirad. Pyramid: Machine learning framework to estimate the optimal timing and resource usage of a high-level synthesis design. In 2019 29th International Conference on Field Programmable Logic and Applications (FPL), pages 397 403. IEEE, 2019. [10] Lorenzo Ferretti, Andrea Cini, Georgios Zacharopoulos, Cesare Alippi, and Laura Pozzi. Graph neural networks for high-level synthesis design space exploration. ACM Transactions on Design Automation of Electronic Systems, 28(2):1 20, 2022. 17 Running Title for Header [11] Mingzhe Gao, Jieru Zhao, Zhe Lin, and Minyi Guo. Hierarchical source-to-post-route qor prediction in high-level synthesis with gnns.\n\n--- Segment 43 ---\n17 Running Title for Header [11] Mingzhe Gao, Jieru Zhao, Zhe Lin, and Minyi Guo. Hierarchical source-to-post-route qor prediction in high-level synthesis with gnns. In 2024 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1 6. IEEE, 2024. [12] Nan Wu, Hang Yang, Yuan Xie, Pan Li, and Cong Hao. High-level synthesis performance prediction using gnns: Benchmarking, modeling, and advancing. In Proceedings of the 59th ACM IEEE Design Automation Conference, pages 49 54, 2022. [13] Chenfeng Zhao, Clayton Faber, Roger Chamberlain, and Xuan Zhang. Hlperf: demystifying the performance of hls-based graph neural networks with dataflow architectures. ACM transactions on reconfigurable technology and systems, 18(1):1 26, 2024. [14] Zhe Lin, Zike Yuan, Jieru Zhao, Wei Zhang, Hui Wang, and Yonghong Tian. Powergear: Early-stage power estimation in fpga hls via heterogeneous edge-centric gnns. in 2022 design, automation test in europe conference exhibition (date). IEEE, Virtual, pages 1341 1346, 2022. [15] Huizhen Kuang, Xianfeng Cao, Jingyuan Li, and Lingli Wang. Hgbo-dse: Hierarchical gnn and bayesian optimization based hls design space exploration. In 2023 International Conference on Field Programmable Technology (ICFPT), pages 106 114. IEEE, 2023. [16] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. Harnessing gnns for robust representation learning in high-level synthesis. IEEE Transactions on Circuits and Systems for Artificial Intelligence, 1(2):114 127, 2024. [17] Zi Wang and Benjamin Carrion Schafer. Machine leaming to set meta-heuristic specific parameters for high-level synthesis design space exploration. in 2020 57th acm ieee design automation conference (dac). 1 6, 2020. [18] Pingakshya Goswami, Benjamin Carrion Schaefer, and Dinesh Bhatia.\n\n--- Segment 44 ---\n1 6, 2020. [18] Pingakshya Goswami, Benjamin Carrion Schaefer, and Dinesh Bhatia. Machine learning based fast and accurate high level synthesis design space exploration: From graph to synthesis. Integration, 88:116 124, 2023. [19] Huiliang Hong, Chenglong Xiao, and Shanshan Wang. Rethinking high-level synthesis design space exploration from a contrastive perspective. In 2024 IEEE 42nd International Conference on Computer Design (ICCD), pages 179 182. IEEE, 2024. [20] Nan Wu, Yuan Xie, and Cong Hao. Ironman-pro: Multiobjective design space exploration in hls via reinforcement learning and graph neural network-based modeling. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(3):900 913, 2023. [21] Yuan Yao, Huiliang Hong, Shanshan Wang, and Chenglong Xiao. Decomposition based estimation of distribution algorithm for high-level synthesis design space exploration. Integration, 100:102292, 2025. [22] Md Imtiaz Rashid and Benjamin Carrion Schafer. Fast and inexpensive high-level synthesis design space exploration: Machine learning to the rescue. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(11):3939 3950, 2023. [23] Zheyuan Zou, Cheng Tang, Lei Gong, Chao Wang, and Xuehai Zhou. Flexwalker: An efficient multi-objective design space exploration framework for hls design. In 2024 34th International Conference on Field-Programmable Logic and Applications (FPL), pages 126 132, 2024. [24] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [25] Petar Veliˇckovi c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017. [26] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers.\n\n--- Segment 45 ---\n[26] Shengcai Liu, Caishun Chen, Xinghua Qu, Ke Tang, and Yew-Soon Ong. Large language models as evolutionary optimizers. In 2024 IEEE Congress on Evolutionary Computation (CEC), pages 1 8. IEEE, 2024. [27] Elliot Meyerson, Mark J Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy K Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. ACM Transactions on Evolutionary Learning, 4(4):1 40, 2024. [28] Benjamin Carrion Schafer and Zi Wang. High-level synthesis design space exploration: Past, present, and future. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 39(10):2628 2639, 2019. [29] Chris Lattner and Vikram Adve. Llvm: A compilation framework for lifelong program analysis transformation. In International symposium on code generation and optimization, 2004. CGO 2004., pages 75 86. IEEE, 2004. [30] Chris Cummins, Zacharias V Fisches, Tal Ben-Nun, Torsten Hoefler, Michael FP O Boyle, and Hugh Leather. Pro- graml: A graph-based program representation for data flow analysis and compiler optimizations. In International Conference on Machine Learning, pages 2244 2253. PMLR, 2021. 18 Running Title for Header [31] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. [32] Atefeh Sohrabizadeh, Yunsheng Bai, Yizhou Sun, and Jason Cong. Automated accelerator optimization aided by graph neural networks. In Proceedings of the 59th ACM IEEE Design Automation Conference, DAC 22, page 55 60, 2022. [33] Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015. [34] Tom B.\n\n--- Segment 46 ---\narXiv preprint arXiv:1511.05493, 2015. [34] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee- lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. [35] Brandon Reagen, Robert Adolf, Yakun Sophia Shao, Gu-Yeon Wei, and David Brooks. Machsuite: Benchmarks for accelerator design and customized architectures. In 2014 IEEE International Symposium on Workload Characterization (IISWC), pages 110 119. IEEE, 2014. [36] T. Yuki et al. Polybenchc, 4.2.1, 2010. [37] William L Hamilton. Graph representation learning. Morgan Claypool Publishers, 2020. [38] DeepSeek-AI, Daya Guo, Dejian Yang, and . Haowei Zhanget et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [39] OpenAI, :, Aaron Hurst, Adam Lerer, and . Adam P. Goucher et al. Gpt-4o system card, 2024. [40] OpenAI. Openai gpt-4-1, 2025. Accessed: 2025-03-21. [41] OpenAI. Openai o3-mini, 2025. Accessed: 2025-03-21. [42] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning.\n\n--- Segment 47 ---\n[42] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 8634 8652. Curran Associates, Inc., 2023. [43] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. Guiding large language models via directional stimulus prompting. NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. [44] Vijay Prakash Dwivedi, Ladislav Rampášek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. Advances in Neural Information Processing Systems, 35:22326 22340, 2022. 19\n\n