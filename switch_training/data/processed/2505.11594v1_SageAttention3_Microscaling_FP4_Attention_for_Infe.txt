=== ORIGINAL PDF: 2505.11594v1_SageAttention3_Microscaling_FP4_Attention_for_Infe.pdf ===\n\nRaw text length: 56299 characters\nCleaned text length: 55473 characters\nNumber of segments: 53\n\n=== CLEANED TEXT ===\n\narXiv:2505.11594v1 [cs.LG] 16 May 2025 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training Jintao Zhang , Jia Wei , Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen Tsinghua University Abstract The efficiency of attention is important due to its quadratic time complexity. We en- hance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5 speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 atten- tion can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at 5x 212 1038 3x 490s 164s FlashAttention2 (End-to-End Time: 490s on RTX5090) SageAttention3 (End-to-End Time: 164s on RTX5090) Figure 1: The upper left figure shows the kernel speedup on RTX5090. The other two figures show the end-to-end inference speedup of generating a video using HunyuanVideo on RTX5090. Note that FlashAttention3 can only run on Hopper GPUs, so FlashAttention2 is already the fastest on RTX5090. 1 Introduction Motivation. The efficiency of attention is critical for generation models, especially given their quadratic time complexity with longer sequences [1, 2]. Quantization offers an effective way to Equal Contribution. Preprint. Under review. accelerate inference by utilizing low-bit Tensor Cores in GPUs [3]. The new FP4 Tensor Cores in Blackwell GPUs deliver significantly faster performance compared to FP16 [4]. We want to propose a novel FP4 attention implementation that provides plug-and-play compatibility for inference acceleration. Beyond inference, training efficiency is equally important. However, no prior work has explored low-bit attention for training large models. To address this gap, we design a trainable 8-bit attention to explore its feasibility in training tasks. To the best of our knowledge, we are the first work that designs FP4 attention for inference and the first work to explore the feasibility of low-bit attention for training large models. Challenges. There are two primary obstacles for FP4 attention and one key difficulty for 8-bit trainable attention. First, (C1) FP4 quantization suffers from severe value limitations (only 15 representable values), making both per-tensor and per-token quantization approaches inadequate for preserving model accuracy. Second, (C2) The attention map P consists primarily of small values in the range [0, 1]. When directly quantized to FP4, these values force the scaling factors into an extremely narrow dynamic range. However, hardware requires the quantization factors to be in FP8 data type. This leads to significant accuracy loss when presenting these scale factors in FP8. Third, (C3) When employing 8-bit attention during training, we find that the attention map gradients are particularly vulnerable to quantization errors, resulting in accumulated errors in the input gradients. Our Method. To address (C1), we propose to use FP4 microscaling quantization for the two matrix multiplications in attention, i.e., QK and PV . By constraining the quantization group size to 1x16 (instead of per-tensor or per-channel), our method effectively contains outlier effects within each block while improving FP4 quantization accuracy. To overcome (C2), we propose a two-level quantization method for P to fully utilize the presentative range of the FP8 scaling factor, enhancing the quantization accuracy of P. Specifically, this approach first normalizes each token s range to [0, 448 6] through per-token quantization, then applies FP4 microscaling quantization for enhanced precision. To address (C3), we identify the most accuracy-sensitive matrix multiplication among the five in backpropagation and maintain its accuracy in FP16. Result. Our FP4 attention, named SageAttention3, could achieve 1038 TOPS on RTX5090, which is a 5 speedup than FlashAttention. Furthermore, we demonstrate that 8-bit trainable attention, named SageBwd, could achieve lossless performance when fine-tuning base models for instruction- following tasks, but is not suitable for pretraining tasks. Contribution. Our work makes the following key contributions: (1) We design the first FP4 attention to accelerate inference, achieving 1000 TOPS on RTX5090. (2) We propose the first trainable low-bit attention, enabling accelerated training with lossless fine-tuning performance, while revealing key insights for low-bit attention in training. Smoothing Micro-scaling Quantization FP4MM OnlineSoftmax Per-token Quanitzation 6 448 Micro-scaling Quantization 1 2 3 to Q K V FP16 FP4 P 4 r2Ae0pUzSaRuaF8lEKLUrcesPuNVvEv9A 8I7YwpqEZ2Q5My59yZe68deW4iTPM1py0tr6yu5dcLG5tb20V9Z7eRhGns8LoTemHcslnCPTfgdeEKj7eimDPf9njTHl IePOGx4kbBtdiEvGuz4aBO3AdJojq6cWOz8TIHkw7IyaM2qynl8yqZaxCKw MlJCtWqi oIM QjhI4YMjgCDsgSGhpw0LJiLiupgSFxNyVZxjhgJ5U1JxUjBix Qd0q6dsQHtZc5EuR06xaM3JqeBQ KEpIsJy9MFU9VZsn lnuqcsq7TehvZ7l8YgVGxP7lmyv 65O1CAxwpmpwqaZIMbI6J8uSq7ImxtfqhKUISJO4j7FY8KOcs7 bChPomqXvWUq qaUkpV7J9OmeJe3pAFbP8e5CBrHZatSrlydlKrn2ajz2McBjmiep6jiEjXU1cwf8YRnraHdanfa adUy2WePXxb2sMH64aVnA latexit ˆP ˆQ ˆK ˆV Q K V s W7QNqKcl0WoNpEjITpRTBH3Crnyb gf6Fd8YpqEV0QpIz595zZu69fhIGQjrOa85aWFxaXsmvFtbWNza3its7LRFnKeNFodx2vE9wcMg4k0ZyJB3kpR7Yz kbf mXMXbtzwVQRxdyknCe2NvFAXDgHmSqAvRb SLJafs6GXPA9eAEsyqx8UXGAGAw ZxuCIAmH8CDo6cKFg4S4HqbEpYQCHe4R4G0GWVxyvCIvaHviHZdw0a0V5CqxmdEtKbktLGAWliyksJq9NsHc 0s2J 85qT3W3Cf194zUmVuKa2L90s8z 6lQtEkOc6hoCqinRjKqOGZdMd0Xd3P5SlSHhDiFBxRPCTOtnPXZ1hqha1e9XT8TWcq Vu2Zyc3wrm5JA3Z jnMetI7KbqVcaRyXqmdm1HnsYR HNM8TVFDHU3yHuERT3i2alZkZdbdZ6qVM5pdfFvWwdRs5BH latexit sQ C mom2FWkoyndahaRKSiVK4A 41U8T 0D wjvjFNQiOiHJmXPvOTP3Xj8ORCod5zVnzc0vLC7lwsrq2vrG8XNrWYaZQnjDRYFUXLleykPRMgbUsiAX8UJ90Z wFv 8FTFW7c8SUXspxzDsjbxCKvmCeJOoi7Z51iyWn7OhlzwLXgBLMqkfF1yjhwg MGUbgCEJB CQ0tOGCwcxcR1MiEsICR3nuEeBtBlcrwiB3Sd0C7tmFD2ivPVKsZnRLQm5DSxh5pIspLCKvTbB3PtLNif OeaE91tzH9feM1Ilbihti dNPM pULRJ9HOsaBNUa0ZVx4xLpruibm5 qUqSQ0ycwj2KJ4SZVk7bGtNqmtXvfV0 E1n KlbtmcnN8K5uSQN2f45zFjQPym6lXDk LFVPzKjz2MEu9meR6ihjoa5D3AI57wbNWs0Mqsu89UK2c02 i2rIcPQ3OQ latexit sK sV u K9gG1lGQ6rUPTJCQTpRTBH3Crnyb gf6Fd8YpqEV0QpIz595zZu69fhyIVDrOa85aWFxaXsmvFtbWNza3its7zTKEsYbLAqipO17KQ9EyBtSyIC34R7Yz gLX90ruKtW56kIgqv5CTm3bE3DMVAME8SdZn26r1iySk7etnzwDWgBLPqUfEF1 gjAkO GMThCSMIBPKT0dODCQUxcF1PiEkJCxznuUSBtRlmcMjxiR Qd0q5j2JD2yjPVakanBPQmpLRxQJqI8hLC6jRbxzPtrNjfvKfaU91tQn feI2Jlbgh9i dLPO OlWLxACnugZBNcWaUdUx45Lprqib21 qkuQE6dwn IJYaVsz7bWpPq2lVvPR1 05mK VXtmcjO8q1vSgN2f45wHzaOyWylXLo5L1TMz6jz2sI9DmucJqihjgZ5D GIJzxbNSu0MuvuM9XKGc0uvi3r4QNPU5BG latexit sP FP16 FP4MM 5 M5BGBmLYMKGvTKcmBHrD7jV3zL gf6Fd8aSqMToNG3PnHvPmbn3OpHY2lZrxljYXFpeSW7mltb39jcym vNOIwES6ru6EXipZjx8zjAatLj3WigSzfcdjTWd0ruLNWyZiHgZXchyxrm8PAt7nri2Ju74thw6 Ul1epMvWEVL3MelFJQLpqYf4 FHfQwkUCHwBJGEPNmJ62ijBQkRcFxPiBCGu4wxT5EibUBajDJvYEX0HtGunbEB75RlrtUunePQKUpo4IE1IeYKwOs3U8UQ7K Y374n2VHcb09JvXxiJYbE qWbZf5Xp2qR6ONU18Cpkgzqjo3dUl0V9TNzS9VSXKIiFO4R3FB2NXKWZ9NrYl17aq3 to6 6UzFqr2b5iZ4V7ekAZd jnMeNI6KpXKxfHlcqJylo85iD s4pHmeoIL1FAn7wCPeMKzUTUS4864 0w1MqlmF9 W8fABajaTfw latexit O FP8 y ni1h9wq18l oH hXfGCGoRnZDkzLn3nJl7rx26TiwM4yWjzczOzS9kF3NLyura n1jXocJBHjNRa4QdS0rZi7js9rwhEub4YRtzb5Q17cCzjRsexU7gX4hRyDue1fednsMsQVSnLfhQCDE qe6VJpf5glE01NKngZmCAtJVDfLPaKOLAwJPHD4EIRdWIjpacGEgZC4DsbERYQcFeYIEfahLI4ZVjE Dujbp10rZX3aS89YqRmd4tIbkVLHDmkCyosIy9N0FU Us2R 8x4rT3m3Ef3t1MsjVuCK2L90n5n 1claBHo4VDU4VFOoGFkdS10S1RV5c 1LVYIcQuIk7lI8IsyU8rPutLEqnbZW0vFX1WmZOWepbkJ3uQtacDmz3FOg3qpaJaL5bP9QuUoHXUW9jGLs3zABWcoaeV jAY940s61oXar3X2kaplUs4 lvS7t Bw52lIQ latexit FP32 FP32 J iGZqKVU3PoDbvWvxD Qv DOmIJaRCckOXPuPWfm3uGnoilZb1mjLn5hcWl7HJuZXVtfSO udWIgyRivM4CL4guXCfmnvB5XQrp8Ysw4s7Q9XjTHZyoePOaR7EI HM5Cnln6PR90RPMkUS125LfSinHp9XS5DJfsIqWXuYsFNQLqQf4FbXQRgCHBEBw JGEPDmJ6WrBhISugzFxESGh4x wT5EibUBanDIfYAX37tGulrE975RlrNaNTPHojUprYI01AeRFhdZqp4l2Vuxv3mPtqe42or beg2Jlbgi9i dNPO OlWLRA9HugZBNYWaUdWx1CXRXVE3N79UJckhJE7hLsUjwkwrp302tSbWtaveOjr pjMVq YszU3wrm5JA7Z jnMWNA6KdrlYrpUKleN01FnsYBf7NM9DVHCGKurkHeI RT3g2asaNcWfcf6YamVSzjW LePgAb7OUSQ latexit FP4 FP8 !P2 Figure 2: Workflow of microscaling FP4 attention. 2 Preliminary FlashAttention. The attention computation contains two matrix multiplications and one softmax calculation: S QK , P Softmax(S), O PV . The Q, K, V are in the shape of N D, where N means the sequence length and D means the dimension of an attention head. P, S are in the shape of N N. FlashAttention divides Q to blocks {Qi} in the shape of Bq D, and divides K, V to {Ki}, {Vi} in the shape of Bkv D. Then it uses online softmax to avoid the large memory IO for S and P: Sij QiK j , Pij OnlineSoftmax(Sij), Oij PijVj. 2 Notation. For simplicity, we omit subscripts and use Q, K, V, S, P, O to denote the matrix blocks in FlashAttention, while retaining full subscript notation in Algorithm 1, 2, and 3. Quantization. Quantization is used to accelerate Matmul by converting two matrices from high- bit to low-bit with scale factors. Take INT8 quantization for Matmul AB as an example, where A and B are in FP16 data type. It can be formulated: sA max( A ) 127, ˆA A sA , sB max( B ) 127, ˆB B sB , where ˆA, ˆB are in INT8 and the others are in FP32. Then, AB ˆA ˆB sA sB, which can be accelerated by the INT8 Tensor Core. The granularity of quantization is determined by the dimensions reduced by the max operation. For example, in per- token quantization, the max is computed along each row of a matrix. In per-block quantization, the max is computed on a block of a matrix, which in our paper means a FlashAttention block. 3 FP4 Attention for Inference Acceleration This section presents our microscaling FP4 attention through three key components: (1) the fundamen- tal workflow for applying microscaling FP4 quantization to attention in Section 3.1, (2) the two-level quantization approach for the attention map in Section 3.2, and (3) critical hardware implementation optimization in Section 3.3. 0.0 0.5 1.0 Value 0 1 2 3 Frequency 1e7 (a) P Distribution Mean: 0.1758 0.00 0.05 0.10 0.15 E4M3 Value 101 102 103 104 Frequency (b) sP Distribution (Direct Quant) 0 200 400 E4M3 Value 101 102 103 104 Frequency (c) sP Distribution (Two-level Quant) Direct Quant Two-level Quant 0.000 0.002 0.004 0.006 0.008 Absolute Error (d) sP Error Compared to FP32 Mean 1 SD Direct Quant Two-level Quant 0.00 0.02 0.04 0.06 Absolute Error (e) Quantization Error of P Mean 1 SD Figure 3: Analysis of the benefit of two-level quantization. (a) shows the distribution of eP. (b) and (c) show the distribution of sP using direct quantization and two-level quantization. (d) and (e) show the error of sP and eP using direct quantization and two-level quantization. 3.1 Microscaling FP4 Attention FP4 microscaling quantization. Given a matrix X RN d, we quantize it to ˆX in FP4 data type with a scale factor matrix sX in FP8 data type. Specifically, X is partitioned into Xij R1 n blocks, where each 1 n block corresponds to one scale factor sij. The FP4 microscaling quantization ([ ˆX, sX ϕ(X)]) and dequantization (X ϕ 1( ˆX, sX)) can be formulated as follows. Quantization ϕ: sij max( X ) 6, ˆXij Xij sij (1) Dequantization ϕ 1: X ij sij ˆXij (2) Where the means FP4 rounding. FP4 microscaling quantization Matmul. Consider a matrix multiplication AB, where A and B are in FP16 precision. The speed of the Matmul is about 200 TOPS on RTX5090. In contrast, the speed of the FP4 microscaling Matmul is about 1600 TOPS, which is an 8x speedup. The FP4 microscaling Matmul instruction (FP4MM) takes four inputs, i.e., ˆA, sA, ˆB, sB, and the output C equals to the Matmul result between ϕ 1( ˆA, sA) and ϕ 1( ˆB, sB): C FP4MM( ˆA, sA, ˆB, sB) (3) Attention computation. We accelerate attention computation by applying FP4 microscaling quanti- zation to both matrix multiplications: QK and PV. ˆQ, sQ ϕ(Q), ˆK, sK ϕ(K ), S FP4MM( ˆQ, sQ, ˆK, sK) eP OnlineSoftmax(S) ˆP, sP ϕ(eP), ˆV, sV ϕ(V), O FP4MM(ˆP, sP, ˆV, sV) (4) 3 It is important to note that our hardware implementation builds on FlashAttention, where the matrices Q, K, eP, and V in our formulation correspond to FlashAttention s tiled Q, K, eP, and V blocks as described in Section 2. Additionally, to enhance the attention accuracy, we adopt the smoothing Q and K in SageAttention2 [5]. The complete algorithm is presented in Algorithm 1. Data type determination. There are two choices for the FP4 data type [6]. The first one is the NVFP4, which is in E2M1 data type and its quantization block size is 1 16 and its scale factor is in E4M3 data type. The second one is the MXFP4, which is also in E2M1 data type. However, its quantization block size is 1 32 and its scale factor is in E8M0 data type. We choose NVFP4 because the accuracy of NVFP4 is much higher than that of MXFP4 in attention quantization. Empirical results: Table 1(a) shows the accuracy of MXFP4 and NVFP4 using real Q, K, V across all layers of CogVideoX. Results indicate that the accuracy of NVFP4 outperforms that of MXFP4. Algorithm 1: Implementation of the microscaling FP4 attention. 1: Input: Matrices Q(FP16), K(FP16), V (FP16) RN d, block size Bq, Bkv. 2: Preprocessing: K K mean(K) Smoothing K of SageAttention. 3: Divide Q to Tm N Bq blocks {Qi}; divide K, and V to Tn N Bkv blocks {Ki}, {Vi} ; 4: for i 1 to Tm do 5: qi mean(Qi), (sQ, ˆQi) ϕ(Qi qi) ; Smoothing Q of SageAttention2. 6: for j in [1, Tn] do 7: (sK, ˆKj) ϕ(K j ) , (sV, ˆVj) ϕ(Vj) ; 8: Sij FP4MM( ˆQi, sQ, ˆKj, sK) GEMV( qi, K j ) ; Smoothing Q. 9: mij max(mi,j 1, rowmax(Sij)), ePij exp(Sij mij), lij emi,j 1 mij rowsum(ePij) ; 10: sP1 rowmax(ePij) (448 6), ePij ePij sP1, sP2, ˆPij ϕ(ePij); two-level quantization 11: Oij diag(emi,j 1 mij) 1Oi,j 1 FP4MM(ˆPij, sP2, ˆVj, sV) sP1 12: end for 13: Oi diag(li,Tn) 1Oi,Tn ; 14: end for 15: return O {Oi} 3.2 Two-level Scaling for eP Applying microscaling FP4 quantization for eP presents a challenge to attention accuracy. For example, Fig. 12( c) shows direct quantization severely degrades output quality, producing results substantially different from full-precision outputs. Our analysis reveals that the issue occurs because microscaling NVFP4 quantization requires the scale factor to be represented in E4M3 FP8 format [7], rather than the FP32 data type typically used for scale factors. This causes accuracy loss when the scale factor is directly converted to E4M3 format. To better understand this accuracy loss, we analyze the data distribution of eP and its scale factors in Fig. 3. Since eP is computed using online softmax [8], the values in each microscaling block ePij fall [0, 1]. Consequently, the scale factor (scale factor max(ePij) 6) ranges between 0 and 0.167. This narrow range leads to inefficient usage of E4M3 s representable range, increasing accuracy loss. To reduce accuracy loss by fully utilizing E4M3 s range, we propose a two-level quantization method for the eP matrix. Specifically, we first quantize each row of eP to [0, 448 6]. Then we apply the standard FP4 quantization ϕ for the quantized eP. The two-level quantization can be formulated as follows: sP1 rowmax(eP) (448 6), eP2 eP sP1, sP2, ˆP2 ϕ(eP2) (eP ˆP2 sP2 sP1), O FP4MM(ˆP2, sP2, ˆV, sV) sP1 (5) Where eP, eP2, and sP1 are in FP32 data type. sP2 and sV are in FP8 data type. ˆP2 and ˆV are in FP4 data type. Empirical results: As shown in Fig. 3, our two-level quantization maximizes the E4M3 range utilization for sP, thereby reducing both the numerical representation error of sP and the quantization error of eP. A more formal theoretical analysis is provided in the Appendix. Table 1(b) shows the accuracy of 4 two-level quantization against naive direct quantization, using real Q, K, V from layers of CogVideoX. Results indicate that two-level quantization boosts the accuracy. 3.3 Implementation and Optimization on Hardware Permutation for K. Unlike FP16, the FP32 accumulator s memory layout in FP4 MatMul [9] differs from its operand A s register layout (shown in Fig. 20 and 19). Performing thread shuffles to match operand A s layout would degrade kernel performance. Our solution transforms the accumulator layout (Fig. 21) by permuting the P tile s columns. To maintain correct MatMul, we correspondingly rearrange K s columns, which can be fused with the quantization kernel. Reuse shuffle. The in-kernel micro-scaling quantization of eP requires finding the max value of 16 consecutive row elements. However, as shown in Fig. 21, these 16 elements are distributed across four threads, necessitating intra-thread max reduction followed by inter-thread shuffling, significantly slowing down the kernel. We optimize this by fusing quantization with online softmax, which also computes row-wise maxima. First, we compute the max over 16 elements in S and reuse it in the subsequent softmax max-reduction. This fusion reduces redundant shuffles and max operations by 50 , yielding about 10 whole kernel speedup. Producer warp epilogue. In conventional warp-specialized kernels, consumer warps typically handle both MatMul and store operations while producers merely load inputs, with ping-pong scheduling between consumers enabling stage overlap [10]. However, register constraints make this approach infeasible for our FP4 attention kernel. Instead, we implement ping-pong scheduling between producer warps: while one producer loads inputs for the next MatMul operation, another concurrently stores outputs to global memory, with consumer warps solely responsible for transferring MatMul results from registers to shared memory. This novel design overlaps MatMul and global memory stores within register constraints, boosting throughput. 4 INT8 Attention for Training Low-bit quantization attention works, such as FlashAttention3 and SageAttention, are only for inference. In this section, we propose an INT8 attention for training, named SageBwd, which quantizes six of seven matrix multiplications in attention to INT8, achieving no performance degradation in fine-tuning tasks. Algorithm 2: Foward pass of the 8-bit attention. 1: Input: FP16 matrices Q, K, V RN d, and block size Bq, Bkv. 2: Divide Q to Tm N Bq blocks {Qi}; divide K, and V to Tn N Bkv blocks {Ki}, {Vi} ; 3: Quantization: {sQ, ˆQi} {ψ(Qi)}, {sK, ˆKi} {ψ(K i )}, {sV, ˆVi} {ψ(Vi)} ; Per-block. 4: for i 1 to Tm do 5: Oi RBq D (0), Li RBq (0), mi RBkv (0) ; 6: for j in [1, Tn] do 7: Sij MM( ˆQi, ˆKj) sQ sK ; 8: mij max(mi,j 1, rowmax(Sij)), ePij exp(Sij mij), lij emi,j 1 mij rowsum(ePij); 9: sP exp(rowmax(Sij) mij) 127, ˆPij ePij sP ; Per-token quantization. 10: Oij diag(emi,j 1 mij) 1Oi,j 1 MM(ˆPij, ˆVj) sP sV 11: end for 12: Oi diag(li,Tn) 1Oi,Tn ; 13: Li mi,Tn log(li,Tn) ; 14: end for 15: return O {Oi}, L {Li} ; 4.1 Forward There are two matrix multiplications in the forward pass of attention: S QK , O PV (6) Per-token quantization for P. Following SageAttention [11], we apply smoothing K and per-block INT8 quantization for the QK . However, for the ePV, a static per-block INT8 quantization with a 5 static scale factor of 1 127 for eP is inaccurate [11]. Fortunately, we find applying per-token INT8 quantization for ePV and per-block INT8 quantization for V can enhance the attention accuracy. Furthermore, we eliminate the need for explicit max operations on P by reusing both global and local maximum values from the online softmax computation (Line 9 in Algorithm 2). The algorithm for the forward is shown in Algorithm 2. Given our extensive use of INT8 per-block quantization in trainable attention, we formalize the process as follows. For each FlashAttention block X, the quantization process sX, ˆX ψ(X) can be formulated as: sX max( X ) 127, ˆX X sX (7) Algorithm 3: Backward pass of the 8-bit attention. 1: Input: {sQ, ˆQi}, {sK, ˆKi}, {sV, ˆVi}, O, {Li} from the forward, dO RN d, and block size Bq, Bkv ; 2: D rowsum(dO O), divide D to Tm N Bq blocks {Di}; 3: for j 1 to Tn do 4: for i in [1, Tm] do 5: Sij MM( ˆQi, ˆKj) sQ sK ; Pij exp(Sij Li) ; 6: sP, ˆPij ψ(Pij), sdO, ˆ dOi ψ(dOi) ; INT8 per-block quantization. 7: dVj dVj MM(ˆP ij, ˆ dOi) sP sdO ; 8: dPij MM(dO, V j ) ; Keep in FP16. 9: dSij Pij (dPij Di) ; sdS, ˆ dSij ψ(dSij) ; INT8 per-block quantization. 10: dQi dQi MM( ˆ dSij, ˆKj) sdS sK ; 11: dKj dKj MM( ˆ dS ij, ˆQi) sdS sQ ; 12: end for 13: end for 14: return dQ, dK, dV ; 4.2 Backward There are five matrix multiplications in the backward pass of attention: S QK , dV eP dO, dP dOV , dQ dSK, dK dS Q (8) We observe that whether applying quantizing to dOV has a significant impact on the accuracy of the gradient of Q, K. This is because the accuracy of dOV directly determines the accuracy of dP and dS (see computational dependencies in Algorithm 3). The accuracy loss in dS will continuously accumulate errors into dQ and dK during the recurrent process along the sequence length in FlashAt- tention s backward pass, meaning longer sequences lead to greater error accumulation. Therefore, we maintain dOV in FP16 while accelerating the other four matrix multiplications using INT8 per-block quantization. The algorithm for the forward is shown in Algorithm 3. Empirical results: Table 1 (c) shows the accuracy of the dQ with and without quantization of dOV . We find that the accuracy of dQ is significantly improved when keeping dOV in FP16. Table 1: Accuracy ablation using different quantization strategies. (a) Different FP4 choices Type CosSim L1 RMSE MXFP4 98.37 0.294 0.994 NVFP4 99.52 0.077 0.201 (b) Different scale strategies for eP Method CosSim L1 RMSE Direct 93.32 0.193 1.103 Two-level 99.52 0.077 0.201 (c) Different data types for dOV Method CosSim L1 RMSE INT8 97.47 0.171 2.440 FP16 99.77 0.039 0.692 6 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 Speed (TOPS) 15 16 16 OOM OOM OOM 84 89 93 94 94 95 173 198 208 212 215 214 442 467 473 479 480 479 477 530 537 548 556 559 964 1032 1022 1038 1022 1027 RTX5090, (Head dim 128, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 7 7 7 OOM OOM OOM 75 84 90 93 94 95 147 175 192 202 207 209 289 369 433 457 469 468 365 423 484 523 544 554 924 1005 1015 1020 1022 1027 RTX5090, (Head dim 128, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 4: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 Speed (TOPS) 10 10 11 OOM OOM OOM 101 106 109 111 111 111 191 206 213 218 220 220 405 441 423 428 427 427 429 492 495 492 500 504 751 812 827 839 826 825 RTX5090, (Head dim 64, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 4 5 4 OOM OOM OOM 81 92 101 105 107 109 149 174 195 207 213 217 239 312 379 404 416 421 321 400 432 463 481 491 679 750 802 803 804 806 RTX5090, (Head dim 64, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 5: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 64). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 Speed (TOPS) 17 19 OOM OOM OOM OOM 50 53 54 55 55 55 114 136 146 150 153 155 89 98 102 104 105 105 112 175 195 201 221 231 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 8 9 OOM OOM OOM OOM 44 49 52 54 55 55 81 109 130 142 148 151 68 85 95 100 103 104 110 128 190 206 214 224 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 6: Speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 Speed (TOPS) 10 10 OOM OOM OOM OOM 76 84 89 90 89 86 113 133 146 151 155 157 131 148 158 166 169 170 174 185 243 260 265 263 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 5 5 OOM OOM OOM OOM 58 71 79 82 81 80 76 103 127 143 152 155 86 110 136 149 157 160 114 116 190 248 255 263 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 7: Speed comparison between SageBwd and Baselines (RTX4090, headim 64). Table 2: End-to-end metrics comparison on various models. Model Attention CLIPSIM CLIP-T VQA-a VQA-t FScore CogvideoX Full-Precision (16bit) 0.1865 0.9968 70.476 69.875 4.780 SageAttention2 (8bit) 0.1880 0.9969 69.414 70.750 4.534 SageAttention3 (4bit) 0.1881 0.9969 69.860 70.364 4.035 Hunyuan Video Full-Precision (16bit) 0.1838 0.9993 68.998 78.891 1.4793 SageAttention2 (8bit) 0.1836 0.9993 69.497 77.019 1.4741 SageAttention3 (4bit) 0.1866 0.9993 70.552 75.440 1.232 Mochi Full-Precision (16bit) 0.1828 0.9990 61.9840 61.0000 1.8042 SageAttention2 (8bit) 0.1819 0.9990 61.0093 60.3732 1.7539 SageAttention3 (4bit) 0.1800 0.9993 61.863 59.429 1.649 Model Attention FID sFID CLIP IR Flux Full-Precision (16bit) 162.812 146.980 31.409 0.91 SageAttention2 (8bit) 163.107 146.213 31.436 0.90 SageAttention3 (4bit) 162.121 142.839 31.450 0.94 Stable-Di ffusion3.5 Full-Precision (16bit) 166.421 146.379 31.93 0.93 SageAttention2 (8bit) 164.986 148.557 32.01 0.93 SageAttention3 (4bit) 166.102 145.587 32.01 0.92 7 0 20000 40000 Step 2 4 6 8 Loss Pretrain Loss on FineWeb-Edu BF16 Llama SageBwd Llama (a) 0 200 400 600 Step 0.5 1.0 1.5 2.0 2.5 Loss Finetuning Loss on DROP BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (b) 0 200 400 600 Step 0.00 0.25 0.50 0.75 1.00 1.25 1.50 Loss Finetuning Loss on GSM8K BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (c) 0 200 400 600 Step 0.0 0.2 0.4 0.6 0.8 Loss Finetuning Loss on HELLASWAG BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (d) 0 200 400 600 Step 0.5 1.0 1.5 2.0 2.5 Loss Finetuning Loss on MMLU BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (e) Figure 8: Pretraining and Finetuing loss curves of BF16 and 8-bit atttention. Table 3: 8-bit attention finetune results on Qwen2.5 and Llama3.2 models. Model Method GSM8K(Acc ) DROP(F1 ) MMLU(Acc ) HELLASWAG(Acc ) Qwen2.5 (1.5B) BF16 0.521 0.733 0.569 0.905 SageBwd 0.520 0.734 0.574 0.911 Qwen2.5 (3B) BF16 0.601 0.785 0.640 0.944 SageBwd 0.607 0.782 0.653 0.943 Llama3.2 (1B) BF16 0.259 0.641 0.464 0.828 SageBwd 0.268 0.637 0.458 0.823 5 Experiment Main results. SageAttention3 is faster than FlashAttention and xformers by 5 and 11 on RTX5090, and maintains end-to-end metrics across various models. Furthermore, SageBwd is faster than FlashAttention and xformers by 1.67 and 3 on RTX4090, and achieves no measurable degradation in fine-tuning tasks. 5.1 Setup Models and attentions. We validate the effectiveness of SageAttention3 and SageBwd across a diverse set of representative models from language, image, and video generation. Specifically, we conduct experiments on: Qwen2.5 [12] and Llama3.2 [13] for text2text, CogvideoX [14], HunyuanVideo [15], and Mochi [16] for text2video, Flux [17], and Stable-Diffusion3.5 [18] for text2image. We compare our method with FlashAttention2 [19], xformers [20], SageAtten- tion [11], and SageAttention2 [5]. Please note that FlashAttention3 can only run on Hopper GPUs, so FlashAttention2 is already the fastest version for RTX5090 and RTX4090. Datasets, metrics, and hyperparameters. For the details about the datasets, metrics, and hyperpa- rameters we used, please refer to Appendix A.3. Implementation. We implement SageAttention3 using CUTLASS [21] and CUDA, and imple- ment SageBwd using OpenAI Triton [22]. SageAttention3 SageAttention3 Full Precision Full Precision Figure 9: Visible examples of video generation on HunyuanVideo (left) and image generation on Stable-Diffusion3.5 (right). 8 Table 4: End-to-end speedup performance using SageAttention3 and SageBwd. (a) Inference latency using SageAttention3. Model Original Sage1 Sage2 Sage3 CogvideoX (2B) 64 s 55 s 46 s 27 s HunyuanVideo 489 s 257 s 240 s 164 s (b) One iteration training latency using SageBwd. Model Original SageBwd Llama (8K) 2.1 s 1.9 s Llama (16K) 6.0 s 5.2 s 5.2 Efficiency and Effectiveness Kernel Speed. Fig. 4 and 5 show the kernel speed of SageAttention3 and baselines on RTX5090. We can see that SageAttention3 achieves 4 5 speedup over FlashAttention2 and 8 11 speedup over xformers. Fig. 6 and 7 show the forward backward speed of SageBwd and baselines on RTX4090. It shows that SageBwd achieves 1.67 speedup at most than FlashAttention2 and a higher speedup than FlashAttention2 implemented in Triton and xformers. End-to-end metrics loss of SageAttention3. In Table 2, we compare the end-to-end quality metrics on various models using SageAttention3 and other attention methods. The results demonstrate that SageAttention3 almost incurs almost no end-to-end quality loss across these models. End-to-end metrics loss of SageBwd. To evaluate the effectiveness of SageBwd on training tasks, we conduct two experiments. First, we fine-tune the base models of Qwen2.5 (3B) and Llama3.2 (1B) on GSM8K [23], DROP [24], MMLU [25], and HELLASWAG [26] datasets. Fig. 8 (b-e) shows the fine-tuning loss results, indicating that SageBwd perfectly aligns with BF16. Moreover, our evaluation of the fine-tuned models answer quality across multiple test datasets (Table 3) demonstrates that SageBwd achieves the same performance as BF16. Second, we conduct pre-training tasks on FineWeb- Edu [27] using a Llama (400M) [28] model. Fig. 8 (a) shows the loss curve, indicating that while SageBwd can achieve loss convergence, its convergence speed is relatively slow. This limitation restricts its applicability in pretraining tasks. Visible example. Fig. 9 visualizes some comparative examples of video generation on HunyuanVideo and image generation on Stable-diffsion3.5 using SageAttention3. The results demonstrate that SageAttention3 maintains full generation quality. Additional visible examples are provided in Fig. 10, 11, 13, and 14 in the Appendix. End-to-end speedup. Table 4(a) and 4(b) summarize end-to-end inference and training la- tency improvements. The results show thatSageAttention3 (Table 4(a)) achieved about 3 (HunyuanVideo) and 2.4 (CogVideoX) end-to-end inference generation speedups on RTX5090. Furthermore, SageBwd (Table 4(b)) accelerates the training of Llama (1B) by about 1.15 using 8K 16K token micro-batches on RTX4090. 6 Related Work Recent works that utilize hardware features to accelerate attention computation methods mainly include the following: FlashAttention [29] introduces tiling to reduce the GPU memory I O be- tween global memory and on-chip SRAM, achieving significant speedup. FlashAttention2 [19] improves the parallelism and warp partition strategies. FlashAttention3 [30] exclusively optimizes the kernel speed on the Hopper GPUs. xformers [20] accelerates attention using dedicated CUDA kernels. SageAttention [11] and SageAttention2 [5] accelerate attention using quantization and some novel outlier smoothing techniques. RingAttention [31] extends FlashAttention to multi-GPU Node environments. In these works, although FlashAttention3 proposes a version of FP8 attention, it has failed to be applied to video generation models in a plug-and-play way [5]. Moreover, the FP8 attention in FlashAttention3 does not support the backward pass, limiting its applicability to training tasks. Additionally, numerous efficient attention variants have emerged, including linear attention [32, 33, 34, 35, 36, 37] and sparse attention [38, 39, 40, 41, 42, 43, 2, 44, 45, 46, 47, 48]. Although these works represent promising research directions, they are orthogonal to our work. 9 7 Conclusion In this paper, we make two key contributions. Firstly, we design SageAttention3, the first mi- croscaling FP4 attention for inference acceleration, achieving 1038 TOPS on RTX5090, which is a 5 speedup than the fastest FlashAttention on RTX5090. Experiments show that SageAttention3 could accelerate various models with no end-to-end quality metrics degradation. Secondly, we introduce the first trainable 8-bit attention (SageBwd) for training acceleration and explore its feasibility in training tasks. We find that the 8-bit attention could achieve lossless performance in fine-tuning tasks, but currently has some limitations in pertaining tasks. Future Work. First, while SageBwd demonstrates faster performance than FP16 implementation, we observe a noticeable gap between its current speed and theoretical upper bounds. This gap may be caused by suboptimal Triton kernel implementations, which we plan to further optimize. Second, and more importantly, investigating the application of low-bit attention in pretraining tasks presents a promising research direction worthy of exploration. 10 References [1] A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [2] Huiqiang Jiang, YUCHENG LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [3] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. Advances in neural information processing systems, 33:883 894, 2020. [4] NVIDIA. Nvidia rtx blackwell gpu architecture. Solutions geforce blackwell nvidia-rtx-blackwell-gpu-architecture.pdf. [5] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageatten- tion2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML), 2025. [6] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. [7] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022. [8] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [9] NVIDIA. Parallel Thread Execution ISA Version 8.7. pdf ptx_isa_8.4.pdf, 2025. Accessed: 2025-05-16. [10] NVIDIA. Efficient gemm in cuda. cpp efficient_gemm.html, 2025. Accessed: 2025-05-16. [11] Jintao Zhang, Jia Wei, Pengle Zhang, Jianfei Chen, and Jun Zhu. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In The International Conference on Learning Representations, 2025. [12] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [14] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [15] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 11 [16] Genmo Team. Mochi 1. 2024. [17] Black Forest Labs. Flux. 2023. [18] Stability AI. Introducing stable diffusion 3.5. introducing-stable-diffusion-3-5, 2023. [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. [20] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hack- able transformer modelling library. 2022. [21] NVIDIA. CUTLASS: CUDA Templates for Linear Algebra Subroutines and Solvers. GitHub repository, 2023. [22] Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. MAPL 2019, page 10 19, New York, NY, USA, 2019. Association for Computing Machinery. [23] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [24] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019. [25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [27] Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. [28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [29] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with IO-awareness. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [30] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [31] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for near-infinite context. In The Twelfth International Conference on Learning Representations, 2024. [32] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 12 [33] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. [34] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition, pages 10819 10829, 2022. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156 5165. PMLR, 2020. [36] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024. [37] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024. [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE CVF international conference on computer vision, pages 10012 10022, 2021. [39] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [40] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In International Conference on Learning Representations, 2022. [41] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. [42] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. In First Workshop on Long-Context Foundation Models ICML 2024, 2024. [43] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The International Conference on Learning Representations, 2024. [44] Shashanka Venkataramanan, Amir Ghodrati, Yuki M Asano, Fatih Porikli, and Amir Habib- ian. Skip-attention: Improving vision transformers by paying less attention. In The Twelfth International Conference on Learning Representations, 2024. [45] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. [46] Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Moa: Mixture of sparse attention for automatic large language model compression. arXiv preprint arXiv:2406.14909, 2024. [47] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 13 [48] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025. [49] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [50] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740 755. Springer, 2014. [51] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 22139 22149, 2024. [52] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE CVF International Conference on Computer Vision, pages 20144 20154, 2023. [53] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. [54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [55] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7514 7528, 2021. [56] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 14 A Appendix A.1 Visible Comparison Examples Full Precision SageAttention3 Figure 10: Visible examples of image generation on Stable-Diffusion3.5. Full Precision SageAttention3 Figure 11: Visible examples of image generation on Flux. (a) Full-Precision (b) Two-level quantization (c) Direct quantization Figure 12: Visual comparison of different scale strategies for eP from CogVideoX. 15 Full Precision SageAttention3 Full Precision SageAttention3 Figure 13: Visible examples of video generation on CogVideoX. Full Precision SageAttention3 Full Precision SageAttention3 Figure 14: Visible examples of video generation on HunyuanVideo. 16 Fig. 10 and Fig. 11 show additional visual comparison examples of image generation tasks. Fig. 13 and Fig. 14 show more visual comparison examples of video generation tasks. A.2 Additional Kernel Speed Comparison Fig. 15 and Fig. 16 show the forward kernel speed of SageBwd. Fig. 17 and Fig. 18 show the backward kernel speed of SageBwd. SageBwd achieved a 2x speed up than FlashAttention in the forward propagation. SageBwd achieved a 1.2 1.6x speed up than FlashAttention in the backward propagation. 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 Speed (TOPS) 11 12 12 OOM OOM OOM 101 104 105 106 106 106 155 165 167 166 165 164 141 152 155 156 157 156 78 306 349 346 352 348 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 5 5 5 OOM OOM OOM 81 96 101 105 107 106 125 149 156 161 162 159 110 133 147 154 158 157 201 105 300 258 342 341 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 15: Forward speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 Speed (TOPS) 6 7 7 OOM OOM OOM 112 120 121 121 119 118 143 155 157 153 155 156 152 159 161 171 172 172 270 150 315 322 329 329 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 3 3 3 OOM OOM OOM 87 108 116 119 116 117 99 131 148 154 159 158 120 143 154 163 167 169 176 64 171 319 323 325 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 16: Forward speed comparison between SageBwd and Baselines (RTX4090, headim 64). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 Speed (TOPS) 22 24 OOM OOM OOM OOM 41 44 45 46 46 46 103 127 139 145 149 152 81 89 93 95 96 96 129 156 174 182 200 211 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 12 13 OOM OOM OOM OOM 37 41 44 45 46 46 71 99 122 135 144 148 62 77 86 91 94 95 97 137 172 194 193 204 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 17: Backward speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 500 Speed (TOPS) 13 14 OOM OOM OOM OOM 68 75 81 81 81 78 105 126 142 151 155 158 126 145 157 165 168 170 158 198 229 246 251 249 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 500 7 7 OOM OOM OOM OOM 51 63 70 73 73 71 69 95 120 139 149 154 79 103 132 146 154 157 103 151 196 233 241 250 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 18: Backward speed comparison between SageBwd and Baselines (RTX4090, headim 64). 17 A.3 Datasets, Metrics, and Hyperparameters Datasets. Text-to-video models are evaluated using the open-sora [49] prompt sets. Text-to-image models are assessed on COCO annotations [50]. Language models are evaluated on GSM8K [23], DROP [24], MMLU [25], and HELLASWAG [26] datasets. End-to-end metrics. For text-to-text models, we use Accuracy (Acc.) and F1-Score (F1). For text-to- video models, we evaluate the quality of generated videos on five metrics: CLIPSIM and CLIP-Temp (CLIP-T) [51] to measure the text-video alignment; (VQA-a) and (VQA-t) to assess the video aesthetic and technical quality, respectively; and Flow-score (FScore) for temporal consistency [52]. For text-to-image models, generated images are evaluated in three aspects: FID [53] and sFID [54] for fidelity evaluation, Clipscore (CLIP) [55] for text-image alignment, and ImageReward (IR) [56] for human preference. Accuracy metrics. We use three metrics to assess the accuracy of quantized attention output O compared to attention output in full-precision O: First, we flatten O and O into vectors in the shape of 1 n. Then, Cosine similarity: CosSim P OO pP O2pP O 2, Relative L1 distance: L1 P O O P O , Root mean square error: RMSE p (1 n) P(O O )2. Hyperparameters. For pretraining tasks, we use a 400M model with a hidden size of 1024, 20 layers, an intermediate size of 3072, and 16 attention heads. The training uses a learning rate of 1e-3 with linear decay over 1000 warmup steps, and each step processes 2M tokens. For finetuning tasks, we train for 700 steps using a learning rate of 3e-5 with linear decay and 100 warmup steps with a batch size of 32 on GSM8K dataset and 128 on MMLU, DROP, and HELLASWAG datasets. T0{v0, v1} T0{v2, v3} T0{v4, v5} T0{v6, v7} T1{v0, v1} T1{v2, v3} T1{v4, v5} T1{v6, v7} T2{v0, v1} T2{v2, v3} T2{v4, v5} T2{v6, v7} T3{v0, v1} T3{v2, v3} T3{v4, v5} T3{v6, v7} T0{v8, v9} T0{v10, v11} T0{v12, v13} T0{v14, v15} T1{v8, v9} T1{v10, v11} T1{v12, v13} T1{v14, v15} T2{v8, v9} T2{v10, v11} T2{v12, v13} T2{v14, v15} T0{v8, v9} T3{v10, v11} T3{v12, v13} T3{v14, v15} Figure 19: FP4 operand A register layout - rows 0 and 8, thread 0-3, entries 0-15. T0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 20: FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. T0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 21: Permuted FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. A.4 Additional Experiments Table 5 10 show Qwen2.5 (1.5B), Qwen2.5 (3B), and Llama3.2 (3B) fine-tuning results on four datasets with five different random seeds. The average and standard deviation show SageBwd is highly consistent with BF16 across various random seeds. 18 Table 5: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (1.5B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5133 0.5125 0.7316 0.7364 233 0.5027 0.5042 0.7269 0.7295 1234 0.4973 0.4973 0.7329 0.7342 5678 0.5201 0.5208 0.7340 0.7332 1 0.5049 0.5057 0.7278 0.7404 Avg 0.5077 0.5081 0.7307 0.7348 Std 0.0090 0.0089 0.0032 0.0040 Table 6: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (1.5B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.5814 0.5873 0.9089 0.9065 233 0.5746 0.5785 0.9082 0.9049 1234 0.5805 0.5836 0.9025 0.9047 5678 0.5736 0.5693 0.9112 0.9053 1 0.5830 0.5823 0.9058 0.9075 Avg 0.5786 0.5802 0.9073 0.9058 Std 0.0043 0.0069 0.0033 0.0012 Table 7: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (3B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5982 0.6232 0.7800 0.7812 233 0.5997 0.5974 0.7786 0.7812 1234 0.6156 0.6103 0.7786 0.7824 5678 0.6065 0.6012 0.7816 0.7853 1 0.6171 0.6073 0.7813 0.7832 Avg 0.6074 0.6079 0.7800 0.7827 Std 0.0001 0.0001 0.0000 0.0000 Table 8: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (3B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.6434 0.6425 0.9419 0.9402 233 0.6431 0.6437 0.9405 0.9402 1234 0.6492 0.6492 0.9414 0.9429 5678 0.6531 0.6400 0.9430 0.9440 1 0.6510 0.6454 0.9446 0.9434 Avg 0.6480 0.6442 0.9423 0.9421 Std 0.0000 0.0000 0.0000 0.0000 19 Table 9: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Llama3.2 (1B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.2722 0.2547 0.6367 0.6447 233 0.2661 0.2570 0.6456 0.6424 1234 0.2616 0.2873 0.6439 0.6352 5678 0.2684 0.2585 0.6372 0.6409 1 0.2646 0.2335 0.6393 0.6441 Avg 0.2666 0.2582 0.6405 0.6414 Std 0.0000 0.0003 0.0000 0.0000 Table 10: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Llama3.2 (3B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.4665 0.4705 0.8230 0.8319 233 0.4646 0.4560 0.8327 0.8256 1234 0.4702 0.4757 0.8202 0.8243 5678 0.4580 0.4639 0.8232 0.8276 1 0.4666 0.4691 0.8218 0.8236 Avg 0.4652 0.4670 0.8242 0.8266 Std 0.0000 0.0000 0.0000 0.0000 20\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2505.11594v1 [cs.LG] 16 May 2025 SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-bit Training Jintao Zhang , Jia Wei , Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen Tsinghua University Abstract The efficiency of attention is important due to its quadratic time complexity. We en- hance the efficiency of attention through two key contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves 1038 TOPS on RTX5090, which is a 5 speedup over the fastest FlashAttention on RTX5090. Experiments show that our FP4 atten- tion can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient 8-bit attention for both forward and backward propagation. Experiments indicate that 8-bit attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code will be available at 5x 212 1038 3x 490s 164s FlashAttention2 (End-to-End Time: 490s on RTX5090) SageAttention3 (End-to-End Time: 164s on RTX5090) Figure 1: The upper left figure shows the kernel speedup on RTX5090. The other two figures show the end-to-end inference speedup of generating a video using HunyuanVideo on RTX5090. Note that FlashAttention3 can only run on Hopper GPUs, so FlashAttention2 is already the fastest on RTX5090. 1 Introduction Motivation. The efficiency of attention is critical for generation models, especially given their quadratic time complexity with longer sequences [1, 2]. Quantization offers an effective way to Equal Contribution. Preprint. Under review. accelerate inference by utilizing low-bit Tensor Cores in GPUs [3].\n\n--- Segment 2 ---\nUnder review. accelerate inference by utilizing low-bit Tensor Cores in GPUs [3]. The new FP4 Tensor Cores in Blackwell GPUs deliver significantly faster performance compared to FP16 [4]. We want to propose a novel FP4 attention implementation that provides plug-and-play compatibility for inference acceleration. Beyond inference, training efficiency is equally important. However, no prior work has explored low-bit attention for training large models. To address this gap, we design a trainable 8-bit attention to explore its feasibility in training tasks. To the best of our knowledge, we are the first work that designs FP4 attention for inference and the first work to explore the feasibility of low-bit attention for training large models. Challenges. There are two primary obstacles for FP4 attention and one key difficulty for 8-bit trainable attention. First, (C1) FP4 quantization suffers from severe value limitations (only 15 representable values), making both per-tensor and per-token quantization approaches inadequate for preserving model accuracy. Second, (C2) The attention map P consists primarily of small values in the range [0, 1]. When directly quantized to FP4, these values force the scaling factors into an extremely narrow dynamic range. However, hardware requires the quantization factors to be in FP8 data type. This leads to significant accuracy loss when presenting these scale factors in FP8. Third, (C3) When employing 8-bit attention during training, we find that the attention map gradients are particularly vulnerable to quantization errors, resulting in accumulated errors in the input gradients. Our Method. To address (C1), we propose to use FP4 microscaling quantization for the two matrix multiplications in attention, i.e., QK and PV . By constraining the quantization group size to 1x16 (instead of per-tensor or per-channel), our method effectively contains outlier effects within each block while improving FP4 quantization accuracy. To overcome (C2), we propose a two-level quantization method for P to fully utilize the presentative range of the FP8 scaling factor, enhancing the quantization accuracy of P. Specifically, this approach first normalizes each token s range to [0, 448 6] through per-token quantization, then applies FP4 microscaling quantization for enhanced precision.\n\n--- Segment 3 ---\nBy constraining the quantization group size to 1x16 (instead of per-tensor or per-channel), our method effectively contains outlier effects within each block while improving FP4 quantization accuracy. To overcome (C2), we propose a two-level quantization method for P to fully utilize the presentative range of the FP8 scaling factor, enhancing the quantization accuracy of P. Specifically, this approach first normalizes each token s range to [0, 448 6] through per-token quantization, then applies FP4 microscaling quantization for enhanced precision. To address (C3), we identify the most accuracy-sensitive matrix multiplication among the five in backpropagation and maintain its accuracy in FP16. Result. Our FP4 attention, named SageAttention3, could achieve 1038 TOPS on RTX5090, which is a 5 speedup than FlashAttention. Furthermore, we demonstrate that 8-bit trainable attention, named SageBwd, could achieve lossless performance when fine-tuning base models for instruction- following tasks, but is not suitable for pretraining tasks. Contribution. Our work makes the following key contributions: (1) We design the first FP4 attention to accelerate inference, achieving 1000 TOPS on RTX5090. (2) We propose the first trainable low-bit attention, enabling accelerated training with lossless fine-tuning performance, while revealing key insights for low-bit attention in training.\n\n--- Segment 4 ---\nOur work makes the following key contributions: (1) We design the first FP4 attention to accelerate inference, achieving 1000 TOPS on RTX5090. (2) We propose the first trainable low-bit attention, enabling accelerated training with lossless fine-tuning performance, while revealing key insights for low-bit attention in training. Smoothing Micro-scaling Quantization FP4MM OnlineSoftmax Per-token Quanitzation 6 448 Micro-scaling Quantization 1 2 3 to Q K V FP16 FP4 P 4 r2Ae0pUzSaRuaF8lEKLUrcesPuNVvEv9A 8I7YwpqEZ2Q5My59yZe68deW4iTPM1py0tr6yu5dcLG5tb20V9Z7eRhGns8LoTemHcslnCPTfgdeEKj7eimDPf9njTHl IePOGx4kbBtdiEvGuz4aBO3AdJojq6cWOz8TIHkw7IyaM2qynl8yqZaxCKw MlJCtWqi oIM QjhI4YMjgCDsgSGhpw0LJiLiupgSFxNyVZxjhgJ5U1JxUjBix Qd0q6dsQHtZc5EuR06xaM3JqeBQ KEpIsJy9MFU9VZsn lnuqcsq7TehvZ7l8YgVGxP7lmyv 65O1CAxwpmpwqaZIMbI6J8uSq7ImxtfqhKUISJO4j7FY8KOcs7 bChPomqXvWUq qaUkpV7J9OmeJe3pAFbP8e5CBrHZatSrlydlKrn2ajz2McBjmiep6jiEjXU1cwf8YRnraHdanfa adUy2WePXxb2sMH64aVnA latexit ˆP ˆQ ˆK ˆV Q K V s W7QNqKcl0WoNpEjITpRTBH3Crnyb gf6Fd8YpqEV0QpIz595zZu69fhIGQjrOa85aWFxaXsmvFtbWNza3its7LRFnKeNFodx2vE9wcMg4k0ZyJB3kpR7Yz kbf mXMXbtzwVQRxdyknCe2NvFAXDgHmSqAvRb SLJafs6GXPA9eAEsyqx8UXGAGAw ZxuCIAmH8CDo6cKFg4S4HqbEpYQCHe4R4G0GWVxyvCIvaHviHZdw0a0V5CqxmdEtKbktLGAWliyksJq9NsHc 0s2J 85qT3W3Cf194zUmVuKa2L90s8z 6lQtEkOc6hoCqinRjKqOGZdMd0Xd3P5SlSHhDiFBxRPCTOtnPXZ1hqha1e9XT8TWcq Vu2Zyc3wrm5JA3Z jnMetI7KbqVcaRyXqmdm1HnsYR HNM8TVFDHU3yHuERT3i2alZkZdbdZ6qVM5pdfFvWwdRs5BH latexit sQ C mom2FWkoyndahaRKSiVK4A 41U8T 0D wjvjFNQiOiHJmXPvOTP3Xj8ORCod5zVnzc0vLC7lwsrq2vrG8XNrWYaZQnjDRYFUXLleykPRMgbUsiAX8UJ90Z wFv 8FTFW7c8SUXspxzDsjbxCKvmCeJOoi7Z51iyWn7OhlzwLXgBLMqkfF1yjhwg MGUbgCEJB CQ0tOGCwcxcR1MiEsICR3nuEeBtBlcrwiB3Sd0C7tmFD2ivPVKsZnRLQm5DSxh5pIspLCKvTbB3PtLNif OeaE91tzH9feM1Ilbihti dNPM pULRJ9HOsaBNUa0ZVx4xLpruibm5 qUqSQ0ycwj2KJ4SZVk7bGtNqmtXvfV0 E1n KlbtmcnN8K5uSQN2f45zFjQPym6lXDk LFVPzKjz2MEu9meR6ihjoa5D3AI57wbNWs0Mqsu89UK2c02 i2rIcPQ3OQ latexit sK sV u K9gG1lGQ6rUPTJCQTpRTBH3Crnyb gf6Fd8YpqEV0QpIz595zZu69fhyIVDrOa85aWFxaXsmvFtbWNza3its7zTKEsYbLAqipO17KQ9EyBtSyIC34R7Yz gLX90ruKtW56kIgqv5CTm3bE3DMVAME8SdZn26r1iySk7etnzwDWgBLPqUfEF1 gjAkO GMThCSMIBPKT0dODCQUxcF1PiEkJCxznuUSBtRlmcMjxiR Qd0q5j2JD2yjPVakanBPQmpLRxQJqI8hLC6jRbxzPtrNjfvKfaU91tQn feI2Jlbgh9i dLPO OlWLxACnugZBNcWaUdUx45Lprqib21 qkuQE6dwn IJYaVsz7bWpPq2lVvPR1 05mK VXtmcjO8q1vSgN2f45wHzaOyWylXLo5L1TMz6jz2sI9DmucJqihjgZ5D GIJzxbNSu0MuvuM9XKGc0uvi3r4QNPU5BG latexit sP FP16 FP4MM 5 M5BGBmLYMKGvTKcmBHrD7jV3zL gf6Fd8aSqMToNG3PnHvPmbn3OpHY2lZrxljYXFpeSW7mltb39jcym vNOIwES6ru6EXipZjx8zjAatLj3WigSzfcdjTWd0ruLNWyZiHgZXchyxrm8PAt7nri2Ju74thw6 Ul1epMvWEVL3MelFJQLpqYf4 FHfQwkUCHwBJGEPNmJ62ijBQkRcFxPiBCGu4wxT5EibUBajDJvYEX0HtGunbEB75RlrtUunePQKUpo4IE1IeYKwOs3U8UQ7K Y374n2VHcb09JvXxiJYbE qWbZf5Xp2qR6ONU18Cpkgzqjo3dUl0V9TNzS9VSXKIiFO4R3FB2NXKWZ9NrYl17aq3 to6 6UzFqr2b5iZ4V7ekAZd jnMeNI6KpXKxfHlcqJylo85iD s4pHmeoIL1FAn7wCPeMKzUTUS4864 0w1MqlmF9 W8fABajaTfw latexit O FP8 y ni1h9wq18l oH hXfGCGoRnZDkzLn3nJl7rx26TiwM4yWjzczOzS9kF3NLyura n1jXocJBHjNRa4QdS0rZi7js9rwhEub4YRtzb5Q17cCzjRsexU7gX4hRyDue1fednsMsQVSnLfhQCDE qe6VJpf5glE01NKngZmCAtJVDfLPaKOLAwJPHD4EIRdWIjpacGEgZC4DsbERYQcFeYIEfahLI4ZVjE Dujbp10rZX3aS89YqRmd4tIbkVLHDmkCyosIy9N0FU Us2R 8x4rT3m3Ef3t1MsjVuCK2L90n5n 1claBHo4VDU4VFOoGFkdS10S1RV5c 1LVYIcQuIk7lI8IsyU8rPutLEqnbZW0vFX1WmZOWepbkJ3uQtacDmz3FOg3qpaJaL5bP9QuUoHXUW9jGLs3zABWcoaeV jAY940s61oXar3X2kaplUs4 lvS7t Bw52lIQ latexit FP32 FP32 J iGZqKVU3PoDbvWvxD Qv DOmIJaRCckOXPuPWfm3uGnoilZb1mjLn5hcWl7HJuZXVtfSO udWIgyRivM4CL4guXCfmnvB5XQrp8Ysw4s7Q9XjTHZyoePOaR7EI HM5Cnln6PR90RPMkUS125LfSinHp9XS5DJfsIqWXuYsFNQLqQf4FbXQRgCHBEBw JGEPDmJ6WrBhISugzFxESGh4x wT5EibUBanDIfYAX37tGulrE975RlrNaNTPHojUprYI01AeRFhdZqp4l2Vuxv3mPtqe42or beg2Jlbgi9i dNPO OlWLRA9HugZBNYWaUdWx1CXRXVE3N79UJckhJE7hLsUjwkwrp302tSbWtaveOjr pjMVq YszU3wrm5JA7Z jnMWNA6KdrlYrpUKleN01FnsYBf7NM9DVHCGKurkHeI RT3g2asaNcWfcf6YamVSzjW LePgAb7OUSQ latexit FP4 FP8 !P2 Figure 2: Workflow of microscaling FP4 attention.\n\n--- Segment 5 ---\n(2) We propose the first trainable low-bit attention, enabling accelerated training with lossless fine-tuning performance, while revealing key insights for low-bit attention in training. Smoothing Micro-scaling Quantization FP4MM OnlineSoftmax Per-token Quanitzation 6 448 Micro-scaling Quantization 1 2 3 to Q K V FP16 FP4 P 4 r2Ae0pUzSaRuaF8lEKLUrcesPuNVvEv9A 8I7YwpqEZ2Q5My59yZe68deW4iTPM1py0tr6yu5dcLG5tb20V9Z7eRhGns8LoTemHcslnCPTfgdeEKj7eimDPf9njTHl IePOGx4kbBtdiEvGuz4aBO3AdJojq6cWOz8TIHkw7IyaM2qynl8yqZaxCKw MlJCtWqi oIM QjhI4YMjgCDsgSGhpw0LJiLiupgSFxNyVZxjhgJ5U1JxUjBix Qd0q6dsQHtZc5EuR06xaM3JqeBQ KEpIsJy9MFU9VZsn lnuqcsq7TehvZ7l8YgVGxP7lmyv 65O1CAxwpmpwqaZIMbI6J8uSq7ImxtfqhKUISJO4j7FY8KOcs7 bChPomqXvWUq qaUkpV7J9OmeJe3pAFbP8e5CBrHZatSrlydlKrn2ajz2McBjmiep6jiEjXU1cwf8YRnraHdanfa adUy2WePXxb2sMH64aVnA latexit ˆP ˆQ ˆK ˆV Q K V s W7QNqKcl0WoNpEjITpRTBH3Crnyb gf6Fd8YpqEV0QpIz595zZu69fhIGQjrOa85aWFxaXsmvFtbWNza3its7LRFnKeNFodx2vE9wcMg4k0ZyJB3kpR7Yz kbf mXMXbtzwVQRxdyknCe2NvFAXDgHmSqAvRb SLJafs6GXPA9eAEsyqx8UXGAGAw ZxuCIAmH8CDo6cKFg4S4HqbEpYQCHe4R4G0GWVxyvCIvaHviHZdw0a0V5CqxmdEtKbktLGAWliyksJq9NsHc 0s2J 85qT3W3Cf194zUmVuKa2L90s8z 6lQtEkOc6hoCqinRjKqOGZdMd0Xd3P5SlSHhDiFBxRPCTOtnPXZ1hqha1e9XT8TWcq Vu2Zyc3wrm5JA3Z jnMetI7KbqVcaRyXqmdm1HnsYR HNM8TVFDHU3yHuERT3i2alZkZdbdZ6qVM5pdfFvWwdRs5BH latexit sQ C mom2FWkoyndahaRKSiVK4A 41U8T 0D wjvjFNQiOiHJmXPvOTP3Xj8ORCod5zVnzc0vLC7lwsrq2vrG8XNrWYaZQnjDRYFUXLleykPRMgbUsiAX8UJ90Z wFv 8FTFW7c8SUXspxzDsjbxCKvmCeJOoi7Z51iyWn7OhlzwLXgBLMqkfF1yjhwg MGUbgCEJB CQ0tOGCwcxcR1MiEsICR3nuEeBtBlcrwiB3Sd0C7tmFD2ivPVKsZnRLQm5DSxh5pIspLCKvTbB3PtLNif OeaE91tzH9feM1Ilbihti dNPM pULRJ9HOsaBNUa0ZVx4xLpruibm5 qUqSQ0ycwj2KJ4SZVk7bGtNqmtXvfV0 E1n KlbtmcnN8K5uSQN2f45zFjQPym6lXDk LFVPzKjz2MEu9meR6ihjoa5D3AI57wbNWs0Mqsu89UK2c02 i2rIcPQ3OQ latexit sK sV u K9gG1lGQ6rUPTJCQTpRTBH3Crnyb gf6Fd8YpqEV0QpIz595zZu69fhyIVDrOa85aWFxaXsmvFtbWNza3its7zTKEsYbLAqipO17KQ9EyBtSyIC34R7Yz gLX90ruKtW56kIgqv5CTm3bE3DMVAME8SdZn26r1iySk7etnzwDWgBLPqUfEF1 gjAkO GMThCSMIBPKT0dODCQUxcF1PiEkJCxznuUSBtRlmcMjxiR Qd0q5j2JD2yjPVakanBPQmpLRxQJqI8hLC6jRbxzPtrNjfvKfaU91tQn feI2Jlbgh9i dLPO OlWLxACnugZBNcWaUdUx45Lprqib21 qkuQE6dwn IJYaVsz7bWpPq2lVvPR1 05mK VXtmcjO8q1vSgN2f45wHzaOyWylXLo5L1TMz6jz2sI9DmucJqihjgZ5D GIJzxbNSu0MuvuM9XKGc0uvi3r4QNPU5BG latexit sP FP16 FP4MM 5 M5BGBmLYMKGvTKcmBHrD7jV3zL gf6Fd8aSqMToNG3PnHvPmbn3OpHY2lZrxljYXFpeSW7mltb39jcym vNOIwES6ru6EXipZjx8zjAatLj3WigSzfcdjTWd0ruLNWyZiHgZXchyxrm8PAt7nri2Ju74thw6 Ul1epMvWEVL3MelFJQLpqYf4 FHfQwkUCHwBJGEPNmJ62ijBQkRcFxPiBCGu4wxT5EibUBajDJvYEX0HtGunbEB75RlrtUunePQKUpo4IE1IeYKwOs3U8UQ7K Y374n2VHcb09JvXxiJYbE qWbZf5Xp2qR6ONU18Cpkgzqjo3dUl0V9TNzS9VSXKIiFO4R3FB2NXKWZ9NrYl17aq3 to6 6UzFqr2b5iZ4V7ekAZd jnMeNI6KpXKxfHlcqJylo85iD s4pHmeoIL1FAn7wCPeMKzUTUS4864 0w1MqlmF9 W8fABajaTfw latexit O FP8 y ni1h9wq18l oH hXfGCGoRnZDkzLn3nJl7rx26TiwM4yWjzczOzS9kF3NLyura n1jXocJBHjNRa4QdS0rZi7js9rwhEub4YRtzb5Q17cCzjRsexU7gX4hRyDue1fednsMsQVSnLfhQCDE qe6VJpf5glE01NKngZmCAtJVDfLPaKOLAwJPHD4EIRdWIjpacGEgZC4DsbERYQcFeYIEfahLI4ZVjE Dujbp10rZX3aS89YqRmd4tIbkVLHDmkCyosIy9N0FU Us2R 8x4rT3m3Ef3t1MsjVuCK2L90n5n 1claBHo4VDU4VFOoGFkdS10S1RV5c 1LVYIcQuIk7lI8IsyU8rPutLEqnbZW0vFX1WmZOWepbkJ3uQtacDmz3FOg3qpaJaL5bP9QuUoHXUW9jGLs3zABWcoaeV jAY940s61oXar3X2kaplUs4 lvS7t Bw52lIQ latexit FP32 FP32 J iGZqKVU3PoDbvWvxD Qv DOmIJaRCckOXPuPWfm3uGnoilZb1mjLn5hcWl7HJuZXVtfSO udWIgyRivM4CL4guXCfmnvB5XQrp8Ysw4s7Q9XjTHZyoePOaR7EI HM5Cnln6PR90RPMkUS125LfSinHp9XS5DJfsIqWXuYsFNQLqQf4FbXQRgCHBEBw JGEPDmJ6WrBhISugzFxESGh4x wT5EibUBanDIfYAX37tGulrE975RlrNaNTPHojUprYI01AeRFhdZqp4l2Vuxv3mPtqe42or beg2Jlbgi9i dNPO OlWLRA9HugZBNYWaUdWx1CXRXVE3N79UJckhJE7hLsUjwkwrp302tSbWtaveOjr pjMVq YszU3wrm5JA7Z jnMWNA6KdrlYrpUKleN01FnsYBf7NM9DVHCGKurkHeI RT3g2asaNcWfcf6YamVSzjW LePgAb7OUSQ latexit FP4 FP8 !P2 Figure 2: Workflow of microscaling FP4 attention. 2 Preliminary FlashAttention.\n\n--- Segment 6 ---\nSmoothing Micro-scaling Quantization FP4MM OnlineSoftmax Per-token Quanitzation 6 448 Micro-scaling Quantization 1 2 3 to Q K V FP16 FP4 P 4 r2Ae0pUzSaRuaF8lEKLUrcesPuNVvEv9A 8I7YwpqEZ2Q5My59yZe68deW4iTPM1py0tr6yu5dcLG5tb20V9Z7eRhGns8LoTemHcslnCPTfgdeEKj7eimDPf9njTHl IePOGx4kbBtdiEvGuz4aBO3AdJojq6cWOz8TIHkw7IyaM2qynl8yqZaxCKw MlJCtWqi oIM QjhI4YMjgCDsgSGhpw0LJiLiupgSFxNyVZxjhgJ5U1JxUjBix Qd0q6dsQHtZc5EuR06xaM3JqeBQ KEpIsJy9MFU9VZsn lnuqcsq7TehvZ7l8YgVGxP7lmyv 65O1CAxwpmpwqaZIMbI6J8uSq7ImxtfqhKUISJO4j7FY8KOcs7 bChPomqXvWUq qaUkpV7J9OmeJe3pAFbP8e5CBrHZatSrlydlKrn2ajz2McBjmiep6jiEjXU1cwf8YRnraHdanfa adUy2WePXxb2sMH64aVnA latexit ˆP ˆQ ˆK ˆV Q K V s W7QNqKcl0WoNpEjITpRTBH3Crnyb gf6Fd8YpqEV0QpIz595zZu69fhIGQjrOa85aWFxaXsmvFtbWNza3its7LRFnKeNFodx2vE9wcMg4k0ZyJB3kpR7Yz kbf mXMXbtzwVQRxdyknCe2NvFAXDgHmSqAvRb SLJafs6GXPA9eAEsyqx8UXGAGAw ZxuCIAmH8CDo6cKFg4S4HqbEpYQCHe4R4G0GWVxyvCIvaHviHZdw0a0V5CqxmdEtKbktLGAWliyksJq9NsHc 0s2J 85qT3W3Cf194zUmVuKa2L90s8z 6lQtEkOc6hoCqinRjKqOGZdMd0Xd3P5SlSHhDiFBxRPCTOtnPXZ1hqha1e9XT8TWcq Vu2Zyc3wrm5JA3Z jnMetI7KbqVcaRyXqmdm1HnsYR HNM8TVFDHU3yHuERT3i2alZkZdbdZ6qVM5pdfFvWwdRs5BH latexit sQ C mom2FWkoyndahaRKSiVK4A 41U8T 0D wjvjFNQiOiHJmXPvOTP3Xj8ORCod5zVnzc0vLC7lwsrq2vrG8XNrWYaZQnjDRYFUXLleykPRMgbUsiAX8UJ90Z wFv 8FTFW7c8SUXspxzDsjbxCKvmCeJOoi7Z51iyWn7OhlzwLXgBLMqkfF1yjhwg MGUbgCEJB CQ0tOGCwcxcR1MiEsICR3nuEeBtBlcrwiB3Sd0C7tmFD2ivPVKsZnRLQm5DSxh5pIspLCKvTbB3PtLNif OeaE91tzH9feM1Ilbihti dNPM pULRJ9HOsaBNUa0ZVx4xLpruibm5 qUqSQ0ycwj2KJ4SZVk7bGtNqmtXvfV0 E1n KlbtmcnN8K5uSQN2f45zFjQPym6lXDk LFVPzKjz2MEu9meR6ihjoa5D3AI57wbNWs0Mqsu89UK2c02 i2rIcPQ3OQ latexit sK sV u K9gG1lGQ6rUPTJCQTpRTBH3Crnyb gf6Fd8YpqEV0QpIz595zZu69fhyIVDrOa85aWFxaXsmvFtbWNza3its7zTKEsYbLAqipO17KQ9EyBtSyIC34R7Yz gLX90ruKtW56kIgqv5CTm3bE3DMVAME8SdZn26r1iySk7etnzwDWgBLPqUfEF1 gjAkO GMThCSMIBPKT0dODCQUxcF1PiEkJCxznuUSBtRlmcMjxiR Qd0q5j2JD2yjPVakanBPQmpLRxQJqI8hLC6jRbxzPtrNjfvKfaU91tQn feI2Jlbgh9i dLPO OlWLxACnugZBNcWaUdUx45Lprqib21 qkuQE6dwn IJYaVsz7bWpPq2lVvPR1 05mK VXtmcjO8q1vSgN2f45wHzaOyWylXLo5L1TMz6jz2sI9DmucJqihjgZ5D GIJzxbNSu0MuvuM9XKGc0uvi3r4QNPU5BG latexit sP FP16 FP4MM 5 M5BGBmLYMKGvTKcmBHrD7jV3zL gf6Fd8aSqMToNG3PnHvPmbn3OpHY2lZrxljYXFpeSW7mltb39jcym vNOIwES6ru6EXipZjx8zjAatLj3WigSzfcdjTWd0ruLNWyZiHgZXchyxrm8PAt7nri2Ju74thw6 Ul1epMvWEVL3MelFJQLpqYf4 FHfQwkUCHwBJGEPNmJ62ijBQkRcFxPiBCGu4wxT5EibUBajDJvYEX0HtGunbEB75RlrtUunePQKUpo4IE1IeYKwOs3U8UQ7K Y374n2VHcb09JvXxiJYbE qWbZf5Xp2qR6ONU18Cpkgzqjo3dUl0V9TNzS9VSXKIiFO4R3FB2NXKWZ9NrYl17aq3 to6 6UzFqr2b5iZ4V7ekAZd jnMeNI6KpXKxfHlcqJylo85iD s4pHmeoIL1FAn7wCPeMKzUTUS4864 0w1MqlmF9 W8fABajaTfw latexit O FP8 y ni1h9wq18l oH hXfGCGoRnZDkzLn3nJl7rx26TiwM4yWjzczOzS9kF3NLyura n1jXocJBHjNRa4QdS0rZi7js9rwhEub4YRtzb5Q17cCzjRsexU7gX4hRyDue1fednsMsQVSnLfhQCDE qe6VJpf5glE01NKngZmCAtJVDfLPaKOLAwJPHD4EIRdWIjpacGEgZC4DsbERYQcFeYIEfahLI4ZVjE Dujbp10rZX3aS89YqRmd4tIbkVLHDmkCyosIy9N0FU Us2R 8x4rT3m3Ef3t1MsjVuCK2L90n5n 1claBHo4VDU4VFOoGFkdS10S1RV5c 1LVYIcQuIk7lI8IsyU8rPutLEqnbZW0vFX1WmZOWepbkJ3uQtacDmz3FOg3qpaJaL5bP9QuUoHXUW9jGLs3zABWcoaeV jAY940s61oXar3X2kaplUs4 lvS7t Bw52lIQ latexit FP32 FP32 J iGZqKVU3PoDbvWvxD Qv DOmIJaRCckOXPuPWfm3uGnoilZb1mjLn5hcWl7HJuZXVtfSO udWIgyRivM4CL4guXCfmnvB5XQrp8Ysw4s7Q9XjTHZyoePOaR7EI HM5Cnln6PR90RPMkUS125LfSinHp9XS5DJfsIqWXuYsFNQLqQf4FbXQRgCHBEBw JGEPDmJ6WrBhISugzFxESGh4x wT5EibUBanDIfYAX37tGulrE975RlrNaNTPHojUprYI01AeRFhdZqp4l2Vuxv3mPtqe42or beg2Jlbgi9i dNPO OlWLRA9HugZBNYWaUdWx1CXRXVE3N79UJckhJE7hLsUjwkwrp302tSbWtaveOjr pjMVq YszU3wrm5JA7Z jnMWNA6KdrlYrpUKleN01FnsYBf7NM9DVHCGKurkHeI RT3g2asaNcWfcf6YamVSzjW LePgAb7OUSQ latexit FP4 FP8 !P2 Figure 2: Workflow of microscaling FP4 attention. 2 Preliminary FlashAttention. The attention computation contains two matrix multiplications and one softmax calculation: S QK , P Softmax(S), O PV .\n\n--- Segment 7 ---\n2 Preliminary FlashAttention. The attention computation contains two matrix multiplications and one softmax calculation: S QK , P Softmax(S), O PV . The Q, K, V are in the shape of N D, where N means the sequence length and D means the dimension of an attention head. P, S are in the shape of N N. FlashAttention divides Q to blocks {Qi} in the shape of Bq D, and divides K, V to {Ki}, {Vi} in the shape of Bkv D. Then it uses online softmax to avoid the large memory IO for S and P: Sij QiK j , Pij OnlineSoftmax(Sij), Oij PijVj. 2 Notation. For simplicity, we omit subscripts and use Q, K, V, S, P, O to denote the matrix blocks in FlashAttention, while retaining full subscript notation in Algorithm 1, 2, and 3. Quantization. Quantization is used to accelerate Matmul by converting two matrices from high- bit to low-bit with scale factors. Take INT8 quantization for Matmul AB as an example, where A and B are in FP16 data type. It can be formulated: sA max( A ) 127, ˆA A sA , sB max( B ) 127, ˆB B sB , where ˆA, ˆB are in INT8 and the others are in FP32. Then, AB ˆA ˆB sA sB, which can be accelerated by the INT8 Tensor Core. The granularity of quantization is determined by the dimensions reduced by the max operation. For example, in per- token quantization, the max is computed along each row of a matrix. In per-block quantization, the max is computed on a block of a matrix, which in our paper means a FlashAttention block. 3 FP4 Attention for Inference Acceleration This section presents our microscaling FP4 attention through three key components: (1) the fundamen- tal workflow for applying microscaling FP4 quantization to attention in Section 3.1, (2) the two-level quantization approach for the attention map in Section 3.2, and (3) critical hardware implementation optimization in Section 3.3.\n\n--- Segment 8 ---\nIn per-block quantization, the max is computed on a block of a matrix, which in our paper means a FlashAttention block. 3 FP4 Attention for Inference Acceleration This section presents our microscaling FP4 attention through three key components: (1) the fundamen- tal workflow for applying microscaling FP4 quantization to attention in Section 3.1, (2) the two-level quantization approach for the attention map in Section 3.2, and (3) critical hardware implementation optimization in Section 3.3. 0.0 0.5 1.0 Value 0 1 2 3 Frequency 1e7 (a) P Distribution Mean: 0.1758 0.00 0.05 0.10 0.15 E4M3 Value 101 102 103 104 Frequency (b) sP Distribution (Direct Quant) 0 200 400 E4M3 Value 101 102 103 104 Frequency (c) sP Distribution (Two-level Quant) Direct Quant Two-level Quant 0.000 0.002 0.004 0.006 0.008 Absolute Error (d) sP Error Compared to FP32 Mean 1 SD Direct Quant Two-level Quant 0.00 0.02 0.04 0.06 Absolute Error (e) Quantization Error of P Mean 1 SD Figure 3: Analysis of the benefit of two-level quantization. (a) shows the distribution of eP. (b) and (c) show the distribution of sP using direct quantization and two-level quantization. (d) and (e) show the error of sP and eP using direct quantization and two-level quantization. 3.1 Microscaling FP4 Attention FP4 microscaling quantization. Given a matrix X RN d, we quantize it to ˆX in FP4 data type with a scale factor matrix sX in FP8 data type. Specifically, X is partitioned into Xij R1 n blocks, where each 1 n block corresponds to one scale factor sij. The FP4 microscaling quantization ([ ˆX, sX ϕ(X)]) and dequantization (X ϕ 1( ˆX, sX)) can be formulated as follows.\n\n--- Segment 9 ---\nSpecifically, X is partitioned into Xij R1 n blocks, where each 1 n block corresponds to one scale factor sij. The FP4 microscaling quantization ([ ˆX, sX ϕ(X)]) and dequantization (X ϕ 1( ˆX, sX)) can be formulated as follows. Quantization ϕ: sij max( X ) 6, ˆXij Xij sij (1) Dequantization ϕ 1: X ij sij ˆXij (2) Where the means FP4 rounding. FP4 microscaling quantization Matmul. Consider a matrix multiplication AB, where A and B are in FP16 precision. The speed of the Matmul is about 200 TOPS on RTX5090. In contrast, the speed of the FP4 microscaling Matmul is about 1600 TOPS, which is an 8x speedup. The FP4 microscaling Matmul instruction (FP4MM) takes four inputs, i.e., ˆA, sA, ˆB, sB, and the output C equals to the Matmul result between ϕ 1( ˆA, sA) and ϕ 1( ˆB, sB): C FP4MM( ˆA, sA, ˆB, sB) (3) Attention computation. We accelerate attention computation by applying FP4 microscaling quanti- zation to both matrix multiplications: QK and PV. ˆQ, sQ ϕ(Q), ˆK, sK ϕ(K ), S FP4MM( ˆQ, sQ, ˆK, sK) eP OnlineSoftmax(S) ˆP, sP ϕ(eP), ˆV, sV ϕ(V), O FP4MM(ˆP, sP, ˆV, sV) (4) 3 It is important to note that our hardware implementation builds on FlashAttention, where the matrices Q, K, eP, and V in our formulation correspond to FlashAttention s tiled Q, K, eP, and V blocks as described in Section 2.\n\n--- Segment 10 ---\nWe accelerate attention computation by applying FP4 microscaling quanti- zation to both matrix multiplications: QK and PV. ˆQ, sQ ϕ(Q), ˆK, sK ϕ(K ), S FP4MM( ˆQ, sQ, ˆK, sK) eP OnlineSoftmax(S) ˆP, sP ϕ(eP), ˆV, sV ϕ(V), O FP4MM(ˆP, sP, ˆV, sV) (4) 3 It is important to note that our hardware implementation builds on FlashAttention, where the matrices Q, K, eP, and V in our formulation correspond to FlashAttention s tiled Q, K, eP, and V blocks as described in Section 2. Additionally, to enhance the attention accuracy, we adopt the smoothing Q and K in SageAttention2 [5]. The complete algorithm is presented in Algorithm 1. Data type determination. There are two choices for the FP4 data type [6]. The first one is the NVFP4, which is in E2M1 data type and its quantization block size is 1 16 and its scale factor is in E4M3 data type. The second one is the MXFP4, which is also in E2M1 data type. However, its quantization block size is 1 32 and its scale factor is in E8M0 data type. We choose NVFP4 because the accuracy of NVFP4 is much higher than that of MXFP4 in attention quantization. Empirical results: Table 1(a) shows the accuracy of MXFP4 and NVFP4 using real Q, K, V across all layers of CogVideoX. Results indicate that the accuracy of NVFP4 outperforms that of MXFP4. Algorithm 1: Implementation of the microscaling FP4 attention. 1: Input: Matrices Q(FP16), K(FP16), V (FP16) RN d, block size Bq, Bkv. 2: Preprocessing: K K mean(K) Smoothing K of SageAttention.\n\n--- Segment 11 ---\n1: Input: Matrices Q(FP16), K(FP16), V (FP16) RN d, block size Bq, Bkv. 2: Preprocessing: K K mean(K) Smoothing K of SageAttention. 3: Divide Q to Tm N Bq blocks {Qi}; divide K, and V to Tn N Bkv blocks {Ki}, {Vi} ; 4: for i 1 to Tm do 5: qi mean(Qi), (sQ, ˆQi) ϕ(Qi qi) ; Smoothing Q of SageAttention2. 6: for j in [1, Tn] do 7: (sK, ˆKj) ϕ(K j ) , (sV, ˆVj) ϕ(Vj) ; 8: Sij FP4MM( ˆQi, sQ, ˆKj, sK) GEMV( qi, K j ) ; Smoothing Q. 9: mij max(mi,j 1, rowmax(Sij)), ePij exp(Sij mij), lij emi,j 1 mij rowsum(ePij) ; 10: sP1 rowmax(ePij) (448 6), ePij ePij sP1, sP2, ˆPij ϕ(ePij); two-level quantization 11: Oij diag(emi,j 1 mij) 1Oi,j 1 FP4MM(ˆPij, sP2, ˆVj, sV) sP1 12: end for 13: Oi diag(li,Tn) 1Oi,Tn ; 14: end for 15: return O {Oi} 3.2 Two-level Scaling for eP Applying microscaling FP4 quantization for eP presents a challenge to attention accuracy. For example, Fig. 12( c) shows direct quantization severely degrades output quality, producing results substantially different from full-precision outputs. Our analysis reveals that the issue occurs because microscaling NVFP4 quantization requires the scale factor to be represented in E4M3 FP8 format [7], rather than the FP32 data type typically used for scale factors.\n\n--- Segment 12 ---\n12( c) shows direct quantization severely degrades output quality, producing results substantially different from full-precision outputs. Our analysis reveals that the issue occurs because microscaling NVFP4 quantization requires the scale factor to be represented in E4M3 FP8 format [7], rather than the FP32 data type typically used for scale factors. This causes accuracy loss when the scale factor is directly converted to E4M3 format. To better understand this accuracy loss, we analyze the data distribution of eP and its scale factors in Fig. 3. Since eP is computed using online softmax [8], the values in each microscaling block ePij fall [0, 1]. Consequently, the scale factor (scale factor max(ePij) 6) ranges between 0 and 0.167. This narrow range leads to inefficient usage of E4M3 s representable range, increasing accuracy loss. To reduce accuracy loss by fully utilizing E4M3 s range, we propose a two-level quantization method for the eP matrix. Specifically, we first quantize each row of eP to [0, 448 6]. Then we apply the standard FP4 quantization ϕ for the quantized eP. The two-level quantization can be formulated as follows: sP1 rowmax(eP) (448 6), eP2 eP sP1, sP2, ˆP2 ϕ(eP2) (eP ˆP2 sP2 sP1), O FP4MM(ˆP2, sP2, ˆV, sV) sP1 (5) Where eP, eP2, and sP1 are in FP32 data type. sP2 and sV are in FP8 data type. ˆP2 and ˆV are in FP4 data type. Empirical results: As shown in Fig. 3, our two-level quantization maximizes the E4M3 range utilization for sP, thereby reducing both the numerical representation error of sP and the quantization error of eP. A more formal theoretical analysis is provided in the Appendix. Table 1(b) shows the accuracy of 4 two-level quantization against naive direct quantization, using real Q, K, V from layers of CogVideoX.\n\n--- Segment 13 ---\nA more formal theoretical analysis is provided in the Appendix. Table 1(b) shows the accuracy of 4 two-level quantization against naive direct quantization, using real Q, K, V from layers of CogVideoX. Results indicate that two-level quantization boosts the accuracy. 3.3 Implementation and Optimization on Hardware Permutation for K. Unlike FP16, the FP32 accumulator s memory layout in FP4 MatMul [9] differs from its operand A s register layout (shown in Fig. 20 and 19). Performing thread shuffles to match operand A s layout would degrade kernel performance. Our solution transforms the accumulator layout (Fig. 21) by permuting the P tile s columns. To maintain correct MatMul, we correspondingly rearrange K s columns, which can be fused with the quantization kernel. Reuse shuffle. The in-kernel micro-scaling quantization of eP requires finding the max value of 16 consecutive row elements. However, as shown in Fig. 21, these 16 elements are distributed across four threads, necessitating intra-thread max reduction followed by inter-thread shuffling, significantly slowing down the kernel. We optimize this by fusing quantization with online softmax, which also computes row-wise maxima. First, we compute the max over 16 elements in S and reuse it in the subsequent softmax max-reduction. This fusion reduces redundant shuffles and max operations by 50 , yielding about 10 whole kernel speedup. Producer warp epilogue. In conventional warp-specialized kernels, consumer warps typically handle both MatMul and store operations while producers merely load inputs, with ping-pong scheduling between consumers enabling stage overlap [10]. However, register constraints make this approach infeasible for our FP4 attention kernel. Instead, we implement ping-pong scheduling between producer warps: while one producer loads inputs for the next MatMul operation, another concurrently stores outputs to global memory, with consumer warps solely responsible for transferring MatMul results from registers to shared memory. This novel design overlaps MatMul and global memory stores within register constraints, boosting throughput. 4 INT8 Attention for Training Low-bit quantization attention works, such as FlashAttention3 and SageAttention, are only for inference.\n\n--- Segment 14 ---\nThis novel design overlaps MatMul and global memory stores within register constraints, boosting throughput. 4 INT8 Attention for Training Low-bit quantization attention works, such as FlashAttention3 and SageAttention, are only for inference. In this section, we propose an INT8 attention for training, named SageBwd, which quantizes six of seven matrix multiplications in attention to INT8, achieving no performance degradation in fine-tuning tasks. Algorithm 2: Foward pass of the 8-bit attention. 1: Input: FP16 matrices Q, K, V RN d, and block size Bq, Bkv. 2: Divide Q to Tm N Bq blocks {Qi}; divide K, and V to Tn N Bkv blocks {Ki}, {Vi} ; 3: Quantization: {sQ, ˆQi} {ψ(Qi)}, {sK, ˆKi} {ψ(K i )}, {sV, ˆVi} {ψ(Vi)} ; Per-block. 4: for i 1 to Tm do 5: Oi RBq D (0), Li RBq (0), mi RBkv (0) ; 6: for j in [1, Tn] do 7: Sij MM( ˆQi, ˆKj) sQ sK ; 8: mij max(mi,j 1, rowmax(Sij)), ePij exp(Sij mij), lij emi,j 1 mij rowsum(ePij); 9: sP exp(rowmax(Sij) mij) 127, ˆPij ePij sP ; Per-token quantization.\n\n--- Segment 15 ---\n2: Divide Q to Tm N Bq blocks {Qi}; divide K, and V to Tn N Bkv blocks {Ki}, {Vi} ; 3: Quantization: {sQ, ˆQi} {ψ(Qi)}, {sK, ˆKi} {ψ(K i )}, {sV, ˆVi} {ψ(Vi)} ; Per-block. 4: for i 1 to Tm do 5: Oi RBq D (0), Li RBq (0), mi RBkv (0) ; 6: for j in [1, Tn] do 7: Sij MM( ˆQi, ˆKj) sQ sK ; 8: mij max(mi,j 1, rowmax(Sij)), ePij exp(Sij mij), lij emi,j 1 mij rowsum(ePij); 9: sP exp(rowmax(Sij) mij) 127, ˆPij ePij sP ; Per-token quantization. 10: Oij diag(emi,j 1 mij) 1Oi,j 1 MM(ˆPij, ˆVj) sP sV 11: end for 12: Oi diag(li,Tn) 1Oi,Tn ; 13: Li mi,Tn log(li,Tn) ; 14: end for 15: return O {Oi}, L {Li} ; 4.1 Forward There are two matrix multiplications in the forward pass of attention: S QK , O PV (6) Per-token quantization for P. Following SageAttention [11], we apply smoothing K and per-block INT8 quantization for the QK . However, for the ePV, a static per-block INT8 quantization with a 5 static scale factor of 1 127 for eP is inaccurate [11]. Fortunately, we find applying per-token INT8 quantization for ePV and per-block INT8 quantization for V can enhance the attention accuracy. Furthermore, we eliminate the need for explicit max operations on P by reusing both global and local maximum values from the online softmax computation (Line 9 in Algorithm 2).\n\n--- Segment 16 ---\nFortunately, we find applying per-token INT8 quantization for ePV and per-block INT8 quantization for V can enhance the attention accuracy. Furthermore, we eliminate the need for explicit max operations on P by reusing both global and local maximum values from the online softmax computation (Line 9 in Algorithm 2). The algorithm for the forward is shown in Algorithm 2. Given our extensive use of INT8 per-block quantization in trainable attention, we formalize the process as follows. For each FlashAttention block X, the quantization process sX, ˆX ψ(X) can be formulated as: sX max( X ) 127, ˆX X sX (7) Algorithm 3: Backward pass of the 8-bit attention. 1: Input: {sQ, ˆQi}, {sK, ˆKi}, {sV, ˆVi}, O, {Li} from the forward, dO RN d, and block size Bq, Bkv ; 2: D rowsum(dO O), divide D to Tm N Bq blocks {Di}; 3: for j 1 to Tn do 4: for i in [1, Tm] do 5: Sij MM( ˆQi, ˆKj) sQ sK ; Pij exp(Sij Li) ; 6: sP, ˆPij ψ(Pij), sdO, ˆ dOi ψ(dOi) ; INT8 per-block quantization. 7: dVj dVj MM(ˆP ij, ˆ dOi) sP sdO ; 8: dPij MM(dO, V j ) ; Keep in FP16. 9: dSij Pij (dPij Di) ; sdS, ˆ dSij ψ(dSij) ; INT8 per-block quantization.\n\n--- Segment 17 ---\n7: dVj dVj MM(ˆP ij, ˆ dOi) sP sdO ; 8: dPij MM(dO, V j ) ; Keep in FP16. 9: dSij Pij (dPij Di) ; sdS, ˆ dSij ψ(dSij) ; INT8 per-block quantization. 10: dQi dQi MM( ˆ dSij, ˆKj) sdS sK ; 11: dKj dKj MM( ˆ dS ij, ˆQi) sdS sQ ; 12: end for 13: end for 14: return dQ, dK, dV ; 4.2 Backward There are five matrix multiplications in the backward pass of attention: S QK , dV eP dO, dP dOV , dQ dSK, dK dS Q (8) We observe that whether applying quantizing to dOV has a significant impact on the accuracy of the gradient of Q, K. This is because the accuracy of dOV directly determines the accuracy of dP and dS (see computational dependencies in Algorithm 3). The accuracy loss in dS will continuously accumulate errors into dQ and dK during the recurrent process along the sequence length in FlashAt- tention s backward pass, meaning longer sequences lead to greater error accumulation. Therefore, we maintain dOV in FP16 while accelerating the other four matrix multiplications using INT8 per-block quantization. The algorithm for the forward is shown in Algorithm 3. Empirical results: Table 1 (c) shows the accuracy of the dQ with and without quantization of dOV . We find that the accuracy of dQ is significantly improved when keeping dOV in FP16. Table 1: Accuracy ablation using different quantization strategies.\n\n--- Segment 18 ---\nWe find that the accuracy of dQ is significantly improved when keeping dOV in FP16. Table 1: Accuracy ablation using different quantization strategies. (a) Different FP4 choices Type CosSim L1 RMSE MXFP4 98.37 0.294 0.994 NVFP4 99.52 0.077 0.201 (b) Different scale strategies for eP Method CosSim L1 RMSE Direct 93.32 0.193 1.103 Two-level 99.52 0.077 0.201 (c) Different data types for dOV Method CosSim L1 RMSE INT8 97.47 0.171 2.440 FP16 99.77 0.039 0.692 6 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 Speed (TOPS) 15 16 16 OOM OOM OOM 84 89 93 94 94 95 173 198 208 212 215 214 442 467 473 479 480 479 477 530 537 548 556 559 964 1032 1022 1038 1022 1027 RTX5090, (Head dim 128, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 7 7 7 OOM OOM OOM 75 84 90 93 94 95 147 175 192 202 207 209 289 369 433 457 469 468 365 423 484 523 544 554 924 1005 1015 1020 1022 1027 RTX5090, (Head dim 128, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 4: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 128).\n\n--- Segment 19 ---\nTable 1: Accuracy ablation using different quantization strategies. (a) Different FP4 choices Type CosSim L1 RMSE MXFP4 98.37 0.294 0.994 NVFP4 99.52 0.077 0.201 (b) Different scale strategies for eP Method CosSim L1 RMSE Direct 93.32 0.193 1.103 Two-level 99.52 0.077 0.201 (c) Different data types for dOV Method CosSim L1 RMSE INT8 97.47 0.171 2.440 FP16 99.77 0.039 0.692 6 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 Speed (TOPS) 15 16 16 OOM OOM OOM 84 89 93 94 94 95 173 198 208 212 215 214 442 467 473 479 480 479 477 530 537 548 556 559 964 1032 1022 1038 1022 1027 RTX5090, (Head dim 128, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 7 7 7 OOM OOM OOM 75 84 90 93 94 95 147 175 192 202 207 209 289 369 433 457 469 468 365 423 484 523 544 554 924 1005 1015 1020 1022 1027 RTX5090, (Head dim 128, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 4: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 Speed (TOPS) 10 10 11 OOM OOM OOM 101 106 109 111 111 111 191 206 213 218 220 220 405 441 423 428 427 427 429 492 495 492 500 504 751 812 827 839 826 825 RTX5090, (Head dim 64, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 4 5 4 OOM OOM OOM 81 92 101 105 107 109 149 174 195 207 213 217 239 312 379 404 416 421 321 400 432 463 481 491 679 750 802 803 804 806 RTX5090, (Head dim 64, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 5: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 64).\n\n--- Segment 20 ---\n(a) Different FP4 choices Type CosSim L1 RMSE MXFP4 98.37 0.294 0.994 NVFP4 99.52 0.077 0.201 (b) Different scale strategies for eP Method CosSim L1 RMSE Direct 93.32 0.193 1.103 Two-level 99.52 0.077 0.201 (c) Different data types for dOV Method CosSim L1 RMSE INT8 97.47 0.171 2.440 FP16 99.77 0.039 0.692 6 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 Speed (TOPS) 15 16 16 OOM OOM OOM 84 89 93 94 94 95 173 198 208 212 215 214 442 467 473 479 480 479 477 530 537 548 556 559 964 1032 1022 1038 1022 1027 RTX5090, (Head dim 128, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 2000 7 7 7 OOM OOM OOM 75 84 90 93 94 95 147 175 192 202 207 209 289 369 433 457 469 468 365 423 484 523 544 554 924 1005 1015 1020 1022 1027 RTX5090, (Head dim 128, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 4: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 Speed (TOPS) 10 10 11 OOM OOM OOM 101 106 109 111 111 111 191 206 213 218 220 220 405 441 423 428 427 427 429 492 495 492 500 504 751 812 827 839 826 825 RTX5090, (Head dim 64, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 4 5 4 OOM OOM OOM 81 92 101 105 107 109 149 174 195 207 213 217 239 312 379 404 416 421 321 400 432 463 481 491 679 750 802 803 804 806 RTX5090, (Head dim 64, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 5: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 64). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 Speed (TOPS) 17 19 OOM OOM OOM OOM 50 53 54 55 55 55 114 136 146 150 153 155 89 98 102 104 105 105 112 175 195 201 221 231 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 8 9 OOM OOM OOM OOM 44 49 52 54 55 55 81 109 130 142 148 151 68 85 95 100 103 104 110 128 190 206 214 224 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 6: Speed comparison between SageBwd and Baselines (RTX4090, headim 128).\n\n--- Segment 21 ---\n1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 Speed (TOPS) 10 10 11 OOM OOM OOM 101 106 109 111 111 111 191 206 213 218 220 220 405 441 423 428 427 427 429 492 495 492 500 504 751 812 827 839 826 825 RTX5090, (Head dim 64, causal False) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 1K 2K 4K 8K 16K 32K Sequence Length 0 500 1000 1500 4 5 4 OOM OOM OOM 81 92 101 105 107 109 149 174 195 207 213 217 239 312 379 404 416 421 321 400 432 463 481 491 679 750 802 803 804 806 RTX5090, (Head dim 64, causal True) Torch xformers FlashAttn SageAttn1 SageAttn2 SageAttn3 Figure 5: Speed comparison between SageAttention3 and Baselines (RTX5090, headim 64). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 Speed (TOPS) 17 19 OOM OOM OOM OOM 50 53 54 55 55 55 114 136 146 150 153 155 89 98 102 104 105 105 112 175 195 201 221 231 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 8 9 OOM OOM OOM OOM 44 49 52 54 55 55 81 109 130 142 148 151 68 85 95 100 103 104 110 128 190 206 214 224 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 6: Speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 Speed (TOPS) 10 10 OOM OOM OOM OOM 76 84 89 90 89 86 113 133 146 151 155 157 131 148 158 166 169 170 174 185 243 260 265 263 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 5 5 OOM OOM OOM OOM 58 71 79 82 81 80 76 103 127 143 152 155 86 110 136 149 157 160 114 116 190 248 255 263 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 7: Speed comparison between SageBwd and Baselines (RTX4090, headim 64).\n\n--- Segment 22 ---\n1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 Speed (TOPS) 17 19 OOM OOM OOM OOM 50 53 54 55 55 55 114 136 146 150 153 155 89 98 102 104 105 105 112 175 195 201 221 231 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 8 9 OOM OOM OOM OOM 44 49 52 54 55 55 81 109 130 142 148 151 68 85 95 100 103 104 110 128 190 206 214 224 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 6: Speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 Speed (TOPS) 10 10 OOM OOM OOM OOM 76 84 89 90 89 86 113 133 146 151 155 157 131 148 158 166 169 170 174 185 243 260 265 263 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 5 5 OOM OOM OOM OOM 58 71 79 82 81 80 76 103 127 143 152 155 86 110 136 149 157 160 114 116 190 248 255 263 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 7: Speed comparison between SageBwd and Baselines (RTX4090, headim 64). Table 2: End-to-end metrics comparison on various models.\n\n--- Segment 23 ---\n1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 Speed (TOPS) 10 10 OOM OOM OOM OOM 76 84 89 90 89 86 113 133 146 151 155 157 131 148 158 166 169 170 174 185 243 260 265 263 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 5 5 OOM OOM OOM OOM 58 71 79 82 81 80 76 103 127 143 152 155 86 110 136 149 157 160 114 116 190 248 255 263 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 7: Speed comparison between SageBwd and Baselines (RTX4090, headim 64). Table 2: End-to-end metrics comparison on various models. Model Attention CLIPSIM CLIP-T VQA-a VQA-t FScore CogvideoX Full-Precision (16bit) 0.1865 0.9968 70.476 69.875 4.780 SageAttention2 (8bit) 0.1880 0.9969 69.414 70.750 4.534 SageAttention3 (4bit) 0.1881 0.9969 69.860 70.364 4.035 Hunyuan Video Full-Precision (16bit) 0.1838 0.9993 68.998 78.891 1.4793 SageAttention2 (8bit) 0.1836 0.9993 69.497 77.019 1.4741 SageAttention3 (4bit) 0.1866 0.9993 70.552 75.440 1.232 Mochi Full-Precision (16bit) 0.1828 0.9990 61.9840 61.0000 1.8042 SageAttention2 (8bit) 0.1819 0.9990 61.0093 60.3732 1.7539 SageAttention3 (4bit) 0.1800 0.9993 61.863 59.429 1.649 Model Attention FID sFID CLIP IR Flux Full-Precision (16bit) 162.812 146.980 31.409 0.91 SageAttention2 (8bit) 163.107 146.213 31.436 0.90 SageAttention3 (4bit) 162.121 142.839 31.450 0.94 Stable-Di ffusion3.5 Full-Precision (16bit) 166.421 146.379 31.93 0.93 SageAttention2 (8bit) 164.986 148.557 32.01 0.93 SageAttention3 (4bit) 166.102 145.587 32.01 0.92 7 0 20000 40000 Step 2 4 6 8 Loss Pretrain Loss on FineWeb-Edu BF16 Llama SageBwd Llama (a) 0 200 400 600 Step 0.5 1.0 1.5 2.0 2.5 Loss Finetuning Loss on DROP BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (b) 0 200 400 600 Step 0.00 0.25 0.50 0.75 1.00 1.25 1.50 Loss Finetuning Loss on GSM8K BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (c) 0 200 400 600 Step 0.0 0.2 0.4 0.6 0.8 Loss Finetuning Loss on HELLASWAG BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (d) 0 200 400 600 Step 0.5 1.0 1.5 2.0 2.5 Loss Finetuning Loss on MMLU BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (e) Figure 8: Pretraining and Finetuing loss curves of BF16 and 8-bit atttention.\n\n--- Segment 24 ---\nTable 2: End-to-end metrics comparison on various models. Model Attention CLIPSIM CLIP-T VQA-a VQA-t FScore CogvideoX Full-Precision (16bit) 0.1865 0.9968 70.476 69.875 4.780 SageAttention2 (8bit) 0.1880 0.9969 69.414 70.750 4.534 SageAttention3 (4bit) 0.1881 0.9969 69.860 70.364 4.035 Hunyuan Video Full-Precision (16bit) 0.1838 0.9993 68.998 78.891 1.4793 SageAttention2 (8bit) 0.1836 0.9993 69.497 77.019 1.4741 SageAttention3 (4bit) 0.1866 0.9993 70.552 75.440 1.232 Mochi Full-Precision (16bit) 0.1828 0.9990 61.9840 61.0000 1.8042 SageAttention2 (8bit) 0.1819 0.9990 61.0093 60.3732 1.7539 SageAttention3 (4bit) 0.1800 0.9993 61.863 59.429 1.649 Model Attention FID sFID CLIP IR Flux Full-Precision (16bit) 162.812 146.980 31.409 0.91 SageAttention2 (8bit) 163.107 146.213 31.436 0.90 SageAttention3 (4bit) 162.121 142.839 31.450 0.94 Stable-Di ffusion3.5 Full-Precision (16bit) 166.421 146.379 31.93 0.93 SageAttention2 (8bit) 164.986 148.557 32.01 0.93 SageAttention3 (4bit) 166.102 145.587 32.01 0.92 7 0 20000 40000 Step 2 4 6 8 Loss Pretrain Loss on FineWeb-Edu BF16 Llama SageBwd Llama (a) 0 200 400 600 Step 0.5 1.0 1.5 2.0 2.5 Loss Finetuning Loss on DROP BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (b) 0 200 400 600 Step 0.00 0.25 0.50 0.75 1.00 1.25 1.50 Loss Finetuning Loss on GSM8K BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (c) 0 200 400 600 Step 0.0 0.2 0.4 0.6 0.8 Loss Finetuning Loss on HELLASWAG BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (d) 0 200 400 600 Step 0.5 1.0 1.5 2.0 2.5 Loss Finetuning Loss on MMLU BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (e) Figure 8: Pretraining and Finetuing loss curves of BF16 and 8-bit atttention. Table 3: 8-bit attention finetune results on Qwen2.5 and Llama3.2 models.\n\n--- Segment 25 ---\nModel Attention CLIPSIM CLIP-T VQA-a VQA-t FScore CogvideoX Full-Precision (16bit) 0.1865 0.9968 70.476 69.875 4.780 SageAttention2 (8bit) 0.1880 0.9969 69.414 70.750 4.534 SageAttention3 (4bit) 0.1881 0.9969 69.860 70.364 4.035 Hunyuan Video Full-Precision (16bit) 0.1838 0.9993 68.998 78.891 1.4793 SageAttention2 (8bit) 0.1836 0.9993 69.497 77.019 1.4741 SageAttention3 (4bit) 0.1866 0.9993 70.552 75.440 1.232 Mochi Full-Precision (16bit) 0.1828 0.9990 61.9840 61.0000 1.8042 SageAttention2 (8bit) 0.1819 0.9990 61.0093 60.3732 1.7539 SageAttention3 (4bit) 0.1800 0.9993 61.863 59.429 1.649 Model Attention FID sFID CLIP IR Flux Full-Precision (16bit) 162.812 146.980 31.409 0.91 SageAttention2 (8bit) 163.107 146.213 31.436 0.90 SageAttention3 (4bit) 162.121 142.839 31.450 0.94 Stable-Di ffusion3.5 Full-Precision (16bit) 166.421 146.379 31.93 0.93 SageAttention2 (8bit) 164.986 148.557 32.01 0.93 SageAttention3 (4bit) 166.102 145.587 32.01 0.92 7 0 20000 40000 Step 2 4 6 8 Loss Pretrain Loss on FineWeb-Edu BF16 Llama SageBwd Llama (a) 0 200 400 600 Step 0.5 1.0 1.5 2.0 2.5 Loss Finetuning Loss on DROP BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (b) 0 200 400 600 Step 0.00 0.25 0.50 0.75 1.00 1.25 1.50 Loss Finetuning Loss on GSM8K BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (c) 0 200 400 600 Step 0.0 0.2 0.4 0.6 0.8 Loss Finetuning Loss on HELLASWAG BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (d) 0 200 400 600 Step 0.5 1.0 1.5 2.0 2.5 Loss Finetuning Loss on MMLU BF16 Qwen2.5 SageBwd Qwen2.5 BF16 Llama3.2 SageBwd Llama3.2 (e) Figure 8: Pretraining and Finetuing loss curves of BF16 and 8-bit atttention. Table 3: 8-bit attention finetune results on Qwen2.5 and Llama3.2 models. Model Method GSM8K(Acc ) DROP(F1 ) MMLU(Acc ) HELLASWAG(Acc ) Qwen2.5 (1.5B) BF16 0.521 0.733 0.569 0.905 SageBwd 0.520 0.734 0.574 0.911 Qwen2.5 (3B) BF16 0.601 0.785 0.640 0.944 SageBwd 0.607 0.782 0.653 0.943 Llama3.2 (1B) BF16 0.259 0.641 0.464 0.828 SageBwd 0.268 0.637 0.458 0.823 5 Experiment Main results.\n\n--- Segment 26 ---\nTable 3: 8-bit attention finetune results on Qwen2.5 and Llama3.2 models. Model Method GSM8K(Acc ) DROP(F1 ) MMLU(Acc ) HELLASWAG(Acc ) Qwen2.5 (1.5B) BF16 0.521 0.733 0.569 0.905 SageBwd 0.520 0.734 0.574 0.911 Qwen2.5 (3B) BF16 0.601 0.785 0.640 0.944 SageBwd 0.607 0.782 0.653 0.943 Llama3.2 (1B) BF16 0.259 0.641 0.464 0.828 SageBwd 0.268 0.637 0.458 0.823 5 Experiment Main results. SageAttention3 is faster than FlashAttention and xformers by 5 and 11 on RTX5090, and maintains end-to-end metrics across various models. Furthermore, SageBwd is faster than FlashAttention and xformers by 1.67 and 3 on RTX4090, and achieves no measurable degradation in fine-tuning tasks. 5.1 Setup Models and attentions. We validate the effectiveness of SageAttention3 and SageBwd across a diverse set of representative models from language, image, and video generation. Specifically, we conduct experiments on: Qwen2.5 [12] and Llama3.2 [13] for text2text, CogvideoX [14], HunyuanVideo [15], and Mochi [16] for text2video, Flux [17], and Stable-Diffusion3.5 [18] for text2image. We compare our method with FlashAttention2 [19], xformers [20], SageAtten- tion [11], and SageAttention2 [5]. Please note that FlashAttention3 can only run on Hopper GPUs, so FlashAttention2 is already the fastest version for RTX5090 and RTX4090. Datasets, metrics, and hyperparameters. For the details about the datasets, metrics, and hyperpa- rameters we used, please refer to Appendix A.3. Implementation.\n\n--- Segment 27 ---\nFor the details about the datasets, metrics, and hyperpa- rameters we used, please refer to Appendix A.3. Implementation. We implement SageAttention3 using CUTLASS [21] and CUDA, and imple- ment SageBwd using OpenAI Triton [22]. SageAttention3 SageAttention3 Full Precision Full Precision Figure 9: Visible examples of video generation on HunyuanVideo (left) and image generation on Stable-Diffusion3.5 (right). 8 Table 4: End-to-end speedup performance using SageAttention3 and SageBwd. (a) Inference latency using SageAttention3. Model Original Sage1 Sage2 Sage3 CogvideoX (2B) 64 s 55 s 46 s 27 s HunyuanVideo 489 s 257 s 240 s 164 s (b) One iteration training latency using SageBwd. Model Original SageBwd Llama (8K) 2.1 s 1.9 s Llama (16K) 6.0 s 5.2 s 5.2 Efficiency and Effectiveness Kernel Speed. Fig. 4 and 5 show the kernel speed of SageAttention3 and baselines on RTX5090. We can see that SageAttention3 achieves 4 5 speedup over FlashAttention2 and 8 11 speedup over xformers. Fig. 6 and 7 show the forward backward speed of SageBwd and baselines on RTX4090. It shows that SageBwd achieves 1.67 speedup at most than FlashAttention2 and a higher speedup than FlashAttention2 implemented in Triton and xformers. End-to-end metrics loss of SageAttention3. In Table 2, we compare the end-to-end quality metrics on various models using SageAttention3 and other attention methods. The results demonstrate that SageAttention3 almost incurs almost no end-to-end quality loss across these models. End-to-end metrics loss of SageBwd. To evaluate the effectiveness of SageBwd on training tasks, we conduct two experiments. First, we fine-tune the base models of Qwen2.5 (3B) and Llama3.2 (1B) on GSM8K [23], DROP [24], MMLU [25], and HELLASWAG [26] datasets. Fig.\n\n--- Segment 28 ---\nFirst, we fine-tune the base models of Qwen2.5 (3B) and Llama3.2 (1B) on GSM8K [23], DROP [24], MMLU [25], and HELLASWAG [26] datasets. Fig. 8 (b-e) shows the fine-tuning loss results, indicating that SageBwd perfectly aligns with BF16. Moreover, our evaluation of the fine-tuned models answer quality across multiple test datasets (Table 3) demonstrates that SageBwd achieves the same performance as BF16. Second, we conduct pre-training tasks on FineWeb- Edu [27] using a Llama (400M) [28] model. Fig. 8 (a) shows the loss curve, indicating that while SageBwd can achieve loss convergence, its convergence speed is relatively slow. This limitation restricts its applicability in pretraining tasks. Visible example. Fig. 9 visualizes some comparative examples of video generation on HunyuanVideo and image generation on Stable-diffsion3.5 using SageAttention3. The results demonstrate that SageAttention3 maintains full generation quality. Additional visible examples are provided in Fig. 10, 11, 13, and 14 in the Appendix. End-to-end speedup. Table 4(a) and 4(b) summarize end-to-end inference and training la- tency improvements. The results show thatSageAttention3 (Table 4(a)) achieved about 3 (HunyuanVideo) and 2.4 (CogVideoX) end-to-end inference generation speedups on RTX5090. Furthermore, SageBwd (Table 4(b)) accelerates the training of Llama (1B) by about 1.15 using 8K 16K token micro-batches on RTX4090. 6 Related Work Recent works that utilize hardware features to accelerate attention computation methods mainly include the following: FlashAttention [29] introduces tiling to reduce the GPU memory I O be- tween global memory and on-chip SRAM, achieving significant speedup. FlashAttention2 [19] improves the parallelism and warp partition strategies. FlashAttention3 [30] exclusively optimizes the kernel speed on the Hopper GPUs. xformers [20] accelerates attention using dedicated CUDA kernels.\n\n--- Segment 29 ---\nFlashAttention3 [30] exclusively optimizes the kernel speed on the Hopper GPUs. xformers [20] accelerates attention using dedicated CUDA kernels. SageAttention [11] and SageAttention2 [5] accelerate attention using quantization and some novel outlier smoothing techniques. RingAttention [31] extends FlashAttention to multi-GPU Node environments. In these works, although FlashAttention3 proposes a version of FP8 attention, it has failed to be applied to video generation models in a plug-and-play way [5]. Moreover, the FP8 attention in FlashAttention3 does not support the backward pass, limiting its applicability to training tasks. Additionally, numerous efficient attention variants have emerged, including linear attention [32, 33, 34, 35, 36, 37] and sparse attention [38, 39, 40, 41, 42, 43, 2, 44, 45, 46, 47, 48]. Although these works represent promising research directions, they are orthogonal to our work. 9 7 Conclusion In this paper, we make two key contributions. Firstly, we design SageAttention3, the first mi- croscaling FP4 attention for inference acceleration, achieving 1038 TOPS on RTX5090, which is a 5 speedup than the fastest FlashAttention on RTX5090. Experiments show that SageAttention3 could accelerate various models with no end-to-end quality metrics degradation. Secondly, we introduce the first trainable 8-bit attention (SageBwd) for training acceleration and explore its feasibility in training tasks. We find that the 8-bit attention could achieve lossless performance in fine-tuning tasks, but currently has some limitations in pertaining tasks. Future Work. First, while SageBwd demonstrates faster performance than FP16 implementation, we observe a noticeable gap between its current speed and theoretical upper bounds. This gap may be caused by suboptimal Triton kernel implementations, which we plan to further optimize. Second, and more importantly, investigating the application of low-bit attention in pretraining tasks presents a promising research direction worthy of exploration. 10 References [1] A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\n\n--- Segment 30 ---\nAttention is all you need. Advances in Neural Information Processing Systems, 2017. [2] Huiqiang Jiang, YUCHENG LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. MInference 1.0: Accelerating pre-filling for long-context LLMs via dynamic sparse attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [3] Jianfei Chen, Yu Gai, Zhewei Yao, Michael W Mahoney, and Joseph E Gonzalez. A statistical framework for low-bitwidth training of deep neural networks. Advances in neural information processing systems, 33:883 894, 2020. [4] NVIDIA. Nvidia rtx blackwell gpu architecture. Solutions geforce blackwell nvidia-rtx-blackwell-gpu-architecture.pdf. [5] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageatten- tion2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML), 2025. [6] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. [7] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433, 2022. [8] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018.\n\n--- Segment 31 ---\nOnline normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [9] NVIDIA. Parallel Thread Execution ISA Version 8.7. pdf ptx_isa_8.4.pdf, 2025. Accessed: 2025-05-16. [10] NVIDIA. Efficient gemm in cuda. cpp efficient_gemm.html, 2025. Accessed: 2025-05-16. [11] Jintao Zhang, Jia Wei, Pengle Zhang, Jianfei Chen, and Jun Zhu. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In The International Conference on Learning Representations, 2025. [12] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [14] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025.\n\n--- Segment 32 ---\nCogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [15] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Junkun Yuan, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yanxin Long, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: A systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 11 [16] Genmo Team. Mochi 1. 2024. [17] Black Forest Labs. Flux. 2023. [18] Stability AI. Introducing stable diffusion 3.5. introducing-stable-diffusion-3-5, 2023. [19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. [20] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: A modular and hack- able transformer modelling library. 2022. [21] NVIDIA.\n\n--- Segment 33 ---\n2022. [21] NVIDIA. CUTLASS: CUDA Templates for Linear Algebra Subroutines and Solvers. GitHub repository, 2023. [22] Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. MAPL 2019, page 10 19, New York, NY, USA, 2019. Association for Computing Machinery. [23] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [24] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL, 2019. [25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [26] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. [27] Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. [28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n\n--- Segment 34 ---\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [29] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with IO-awareness. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [30] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [31] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for near-infinite context. In The Twelfth International Conference on Learning Representations, 2024. [32] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 12 [33] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. [34] Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition, pages 10819 10829, 2022. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156 5165.\n\n--- Segment 35 ---\nTransformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 5156 5165. PMLR, 2020. [36] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models. arXiv preprint arXiv:2401.04658, 2024. [37] Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024. [38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE CVF international conference on computer vision, pages 10012 10022, 2021. [39] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [40] Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In International Conference on Learning Representations, 2022. [41] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. [42] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. In First Workshop on Long-Context Foundation Models ICML 2024, 2024.\n\n--- Segment 36 ---\nInfllm: Training-free long-context extrapolation for llms with an efficient context memory. In First Workshop on Long-Context Foundation Models ICML 2024, 2024. [43] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The International Conference on Learning Representations, 2024. [44] Shashanka Venkataramanan, Amir Ghodrati, Yuki M Asano, Fatih Porikli, and Amir Habib- ian. Skip-attention: Improving vision transformers by paying less attention. In The Twelfth International Conference on Learning Representations, 2024. [45] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. [46] Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Moa: Mixture of sparse attention for automatic large language model compression. arXiv preprint arXiv:2406.14909, 2024. [47] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. arXiv preprint arXiv:2502.01776, 2025. 13 [48] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025.\n\n--- Segment 37 ---\nSpargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025. [49] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [50] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740 755. Springer, 2014. [51] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 22139 22149, 2024. [52] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE CVF International Conference on Computer Vision, pages 20144 20154, 2023. [53] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. [54] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\n\n--- Segment 38 ---\nImproved techniques for training gans. Advances in neural information processing systems, 29, 2016. [55] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7514 7528, 2021. [56] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 14 A Appendix A.1 Visible Comparison Examples Full Precision SageAttention3 Figure 10: Visible examples of image generation on Stable-Diffusion3.5. Full Precision SageAttention3 Figure 11: Visible examples of image generation on Flux. (a) Full-Precision (b) Two-level quantization (c) Direct quantization Figure 12: Visual comparison of different scale strategies for eP from CogVideoX. 15 Full Precision SageAttention3 Full Precision SageAttention3 Figure 13: Visible examples of video generation on CogVideoX. Full Precision SageAttention3 Full Precision SageAttention3 Figure 14: Visible examples of video generation on HunyuanVideo. 16 Fig. 10 and Fig. 11 show additional visual comparison examples of image generation tasks. Fig. 13 and Fig. 14 show more visual comparison examples of video generation tasks. A.2 Additional Kernel Speed Comparison Fig. 15 and Fig. 16 show the forward kernel speed of SageBwd. Fig. 17 and Fig. 18 show the backward kernel speed of SageBwd. SageBwd achieved a 2x speed up than FlashAttention in the forward propagation. SageBwd achieved a 1.2 1.6x speed up than FlashAttention in the backward propagation.\n\n--- Segment 39 ---\nSageBwd achieved a 2x speed up than FlashAttention in the forward propagation. SageBwd achieved a 1.2 1.6x speed up than FlashAttention in the backward propagation. 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 Speed (TOPS) 11 12 12 OOM OOM OOM 101 104 105 106 106 106 155 165 167 166 165 164 141 152 155 156 157 156 78 306 349 346 352 348 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 5 5 5 OOM OOM OOM 81 96 101 105 107 106 125 149 156 161 162 159 110 133 147 154 158 157 201 105 300 258 342 341 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 15: Forward speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 Speed (TOPS) 6 7 7 OOM OOM OOM 112 120 121 121 119 118 143 155 157 153 155 156 152 159 161 171 172 172 270 150 315 322 329 329 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 3 3 3 OOM OOM OOM 87 108 116 119 116 117 99 131 148 154 159 158 120 143 154 163 167 169 176 64 171 319 323 325 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 16: Forward speed comparison between SageBwd and Baselines (RTX4090, headim 64).\n\n--- Segment 40 ---\n1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 Speed (TOPS) 11 12 12 OOM OOM OOM 101 104 105 106 106 106 155 165 167 166 165 164 141 152 155 156 157 156 78 306 349 346 352 348 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 5 5 5 OOM OOM OOM 81 96 101 105 107 106 125 149 156 161 162 159 110 133 147 154 158 157 201 105 300 258 342 341 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 15: Forward speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 Speed (TOPS) 6 7 7 OOM OOM OOM 112 120 121 121 119 118 143 155 157 153 155 156 152 159 161 171 172 172 270 150 315 322 329 329 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 3 3 3 OOM OOM OOM 87 108 116 119 116 117 99 131 148 154 159 158 120 143 154 163 167 169 176 64 171 319 323 325 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 16: Forward speed comparison between SageBwd and Baselines (RTX4090, headim 64). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 Speed (TOPS) 22 24 OOM OOM OOM OOM 41 44 45 46 46 46 103 127 139 145 149 152 81 89 93 95 96 96 129 156 174 182 200 211 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 12 13 OOM OOM OOM OOM 37 41 44 45 46 46 71 99 122 135 144 148 62 77 86 91 94 95 97 137 172 194 193 204 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 17: Backward speed comparison between SageBwd and Baselines (RTX4090, headim 128).\n\n--- Segment 41 ---\n1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 Speed (TOPS) 6 7 7 OOM OOM OOM 112 120 121 121 119 118 143 155 157 153 155 156 152 159 161 171 172 172 270 150 315 322 329 329 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 200 400 600 3 3 3 OOM OOM OOM 87 108 116 119 116 117 99 131 148 154 159 158 120 143 154 163 167 169 176 64 171 319 323 325 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 16: Forward speed comparison between SageBwd and Baselines (RTX4090, headim 64). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 Speed (TOPS) 22 24 OOM OOM OOM OOM 41 44 45 46 46 46 103 127 139 145 149 152 81 89 93 95 96 96 129 156 174 182 200 211 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 12 13 OOM OOM OOM OOM 37 41 44 45 46 46 71 99 122 135 144 148 62 77 86 91 94 95 97 137 172 194 193 204 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 17: Backward speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 500 Speed (TOPS) 13 14 OOM OOM OOM OOM 68 75 81 81 81 78 105 126 142 151 155 158 126 145 157 165 168 170 158 198 229 246 251 249 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 500 7 7 OOM OOM OOM OOM 51 63 70 73 73 71 69 95 120 139 149 154 79 103 132 146 154 157 103 151 196 233 241 250 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 18: Backward speed comparison between SageBwd and Baselines (RTX4090, headim 64).\n\n--- Segment 42 ---\n1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 Speed (TOPS) 22 24 OOM OOM OOM OOM 41 44 45 46 46 46 103 127 139 145 149 152 81 89 93 95 96 96 129 156 174 182 200 211 RTX4090, (Head dim 128, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 12 13 OOM OOM OOM OOM 37 41 44 45 46 46 71 99 122 135 144 148 62 77 86 91 94 95 97 137 172 194 193 204 RTX4090, (Head dim 128, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 17: Backward speed comparison between SageBwd and Baselines (RTX4090, headim 128). 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 500 Speed (TOPS) 13 14 OOM OOM OOM OOM 68 75 81 81 81 78 105 126 142 151 155 158 126 145 157 165 168 170 158 198 229 246 251 249 RTX4090, (Head dim 64, causal False) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd 1K 2K 4K 8K 16K 32K Sequence Length 0 100 200 300 400 500 7 7 OOM OOM OOM OOM 51 63 70 73 73 71 69 95 120 139 149 154 79 103 132 146 154 157 103 151 196 233 241 250 RTX4090, (Head dim 64, causal True) Torch xformers FlashAttn(Cuda) FlashAttn(Triton) SageBwd Figure 18: Backward speed comparison between SageBwd and Baselines (RTX4090, headim 64). 17 A.3 Datasets, Metrics, and Hyperparameters Datasets. Text-to-video models are evaluated using the open-sora [49] prompt sets. Text-to-image models are assessed on COCO annotations [50]. Language models are evaluated on GSM8K [23], DROP [24], MMLU [25], and HELLASWAG [26] datasets.\n\n--- Segment 43 ---\nText-to-image models are assessed on COCO annotations [50]. Language models are evaluated on GSM8K [23], DROP [24], MMLU [25], and HELLASWAG [26] datasets. End-to-end metrics. For text-to-text models, we use Accuracy (Acc.) and F1-Score (F1). For text-to- video models, we evaluate the quality of generated videos on five metrics: CLIPSIM and CLIP-Temp (CLIP-T) [51] to measure the text-video alignment; (VQA-a) and (VQA-t) to assess the video aesthetic and technical quality, respectively; and Flow-score (FScore) for temporal consistency [52]. For text-to-image models, generated images are evaluated in three aspects: FID [53] and sFID [54] for fidelity evaluation, Clipscore (CLIP) [55] for text-image alignment, and ImageReward (IR) [56] for human preference. Accuracy metrics. We use three metrics to assess the accuracy of quantized attention output O compared to attention output in full-precision O: First, we flatten O and O into vectors in the shape of 1 n. Then, Cosine similarity: CosSim P OO pP O2pP O 2, Relative L1 distance: L1 P O O P O , Root mean square error: RMSE p (1 n) P(O O )2. Hyperparameters. For pretraining tasks, we use a 400M model with a hidden size of 1024, 20 layers, an intermediate size of 3072, and 16 attention heads. The training uses a learning rate of 1e-3 with linear decay over 1000 warmup steps, and each step processes 2M tokens. For finetuning tasks, we train for 700 steps using a learning rate of 3e-5 with linear decay and 100 warmup steps with a batch size of 32 on GSM8K dataset and 128 on MMLU, DROP, and HELLASWAG datasets.\n\n--- Segment 44 ---\nThe training uses a learning rate of 1e-3 with linear decay over 1000 warmup steps, and each step processes 2M tokens. For finetuning tasks, we train for 700 steps using a learning rate of 3e-5 with linear decay and 100 warmup steps with a batch size of 32 on GSM8K dataset and 128 on MMLU, DROP, and HELLASWAG datasets. T0{v0, v1} T0{v2, v3} T0{v4, v5} T0{v6, v7} T1{v0, v1} T1{v2, v3} T1{v4, v5} T1{v6, v7} T2{v0, v1} T2{v2, v3} T2{v4, v5} T2{v6, v7} T3{v0, v1} T3{v2, v3} T3{v4, v5} T3{v6, v7} T0{v8, v9} T0{v10, v11} T0{v12, v13} T0{v14, v15} T1{v8, v9} T1{v10, v11} T1{v12, v13} T1{v14, v15} T2{v8, v9} T2{v10, v11} T2{v12, v13} T2{v14, v15} T0{v8, v9} T3{v10, v11} T3{v12, v13} T3{v14, v15} Figure 19: FP4 operand A register layout - rows 0 and 8, thread 0-3, entries 0-15.\n\n--- Segment 45 ---\nFor finetuning tasks, we train for 700 steps using a learning rate of 3e-5 with linear decay and 100 warmup steps with a batch size of 32 on GSM8K dataset and 128 on MMLU, DROP, and HELLASWAG datasets. T0{v0, v1} T0{v2, v3} T0{v4, v5} T0{v6, v7} T1{v0, v1} T1{v2, v3} T1{v4, v5} T1{v6, v7} T2{v0, v1} T2{v2, v3} T2{v4, v5} T2{v6, v7} T3{v0, v1} T3{v2, v3} T3{v4, v5} T3{v6, v7} T0{v8, v9} T0{v10, v11} T0{v12, v13} T0{v14, v15} T1{v8, v9} T1{v10, v11} T1{v12, v13} T1{v14, v15} T2{v8, v9} T2{v10, v11} T2{v12, v13} T2{v14, v15} T0{v8, v9} T3{v10, v11} T3{v12, v13} T3{v14, v15} Figure 19: FP4 operand A register layout - rows 0 and 8, thread 0-3, entries 0-15. T0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 20: FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15.\n\n--- Segment 46 ---\nT0{v0, v1} T0{v2, v3} T0{v4, v5} T0{v6, v7} T1{v0, v1} T1{v2, v3} T1{v4, v5} T1{v6, v7} T2{v0, v1} T2{v2, v3} T2{v4, v5} T2{v6, v7} T3{v0, v1} T3{v2, v3} T3{v4, v5} T3{v6, v7} T0{v8, v9} T0{v10, v11} T0{v12, v13} T0{v14, v15} T1{v8, v9} T1{v10, v11} T1{v12, v13} T1{v14, v15} T2{v8, v9} T2{v10, v11} T2{v12, v13} T2{v14, v15} T0{v8, v9} T3{v10, v11} T3{v12, v13} T3{v14, v15} Figure 19: FP4 operand A register layout - rows 0 and 8, thread 0-3, entries 0-15. T0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 20: FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. T0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 21: Permuted FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15.\n\n--- Segment 47 ---\nT0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 20: FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. T0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 21: Permuted FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. A.4 Additional Experiments Table 5 10 show Qwen2.5 (1.5B), Qwen2.5 (3B), and Llama3.2 (3B) fine-tuning results on four datasets with five different random seeds.\n\n--- Segment 48 ---\nT0{v0, v1} T1{v0, v1} T2{v0, v1} T3{v0, v1} T0{v2, v3} T1{v2, v3} T2{v2, v3} T3{v2, v3} T0{v4, v5} T1{v4, v5} T2{v4, v5} T3{v4, v5} T0{v6, v7} T1{v6, v7} T2{v6, v7} T3{v6, v7} T0{v8, v9} T1{v8, v9} T0{v8, v9} T1{v8, v9} T0{v10, v11} T1{v10, v11} T2{v10, v11} T3{v10, v11} T0{v12, v13} T1{v12, v13} T2{v12, v13} T3{v12, v13} T0{v14, v15} T1{v14, v15} T2{v14, v15} T3{v14, v15} Figure 21: Permuted FP32 accumulator register layout - rows 0 and 8, thread 0-3, entries 0-15. A.4 Additional Experiments Table 5 10 show Qwen2.5 (1.5B), Qwen2.5 (3B), and Llama3.2 (3B) fine-tuning results on four datasets with five different random seeds. The average and standard deviation show SageBwd is highly consistent with BF16 across various random seeds. 18 Table 5: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (1.5B).\n\n--- Segment 49 ---\nThe average and standard deviation show SageBwd is highly consistent with BF16 across various random seeds. 18 Table 5: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (1.5B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5133 0.5125 0.7316 0.7364 233 0.5027 0.5042 0.7269 0.7295 1234 0.4973 0.4973 0.7329 0.7342 5678 0.5201 0.5208 0.7340 0.7332 1 0.5049 0.5057 0.7278 0.7404 Avg 0.5077 0.5081 0.7307 0.7348 Std 0.0090 0.0089 0.0032 0.0040 Table 6: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (1.5B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.5814 0.5873 0.9089 0.9065 233 0.5746 0.5785 0.9082 0.9049 1234 0.5805 0.5836 0.9025 0.9047 5678 0.5736 0.5693 0.9112 0.9053 1 0.5830 0.5823 0.9058 0.9075 Avg 0.5786 0.5802 0.9073 0.9058 Std 0.0043 0.0069 0.0033 0.0012 Table 7: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (3B).\n\n--- Segment 50 ---\nSeed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5133 0.5125 0.7316 0.7364 233 0.5027 0.5042 0.7269 0.7295 1234 0.4973 0.4973 0.7329 0.7342 5678 0.5201 0.5208 0.7340 0.7332 1 0.5049 0.5057 0.7278 0.7404 Avg 0.5077 0.5081 0.7307 0.7348 Std 0.0090 0.0089 0.0032 0.0040 Table 6: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (1.5B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.5814 0.5873 0.9089 0.9065 233 0.5746 0.5785 0.9082 0.9049 1234 0.5805 0.5836 0.9025 0.9047 5678 0.5736 0.5693 0.9112 0.9053 1 0.5830 0.5823 0.9058 0.9075 Avg 0.5786 0.5802 0.9073 0.9058 Std 0.0043 0.0069 0.0033 0.0012 Table 7: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (3B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5982 0.6232 0.7800 0.7812 233 0.5997 0.5974 0.7786 0.7812 1234 0.6156 0.6103 0.7786 0.7824 5678 0.6065 0.6012 0.7816 0.7853 1 0.6171 0.6073 0.7813 0.7832 Avg 0.6074 0.6079 0.7800 0.7827 Std 0.0001 0.0001 0.0000 0.0000 Table 8: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (3B).\n\n--- Segment 51 ---\nSeed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.5814 0.5873 0.9089 0.9065 233 0.5746 0.5785 0.9082 0.9049 1234 0.5805 0.5836 0.9025 0.9047 5678 0.5736 0.5693 0.9112 0.9053 1 0.5830 0.5823 0.9058 0.9075 Avg 0.5786 0.5802 0.9073 0.9058 Std 0.0043 0.0069 0.0033 0.0012 Table 7: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Qwen2.5 (3B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5982 0.6232 0.7800 0.7812 233 0.5997 0.5974 0.7786 0.7812 1234 0.6156 0.6103 0.7786 0.7824 5678 0.6065 0.6012 0.7816 0.7853 1 0.6171 0.6073 0.7813 0.7832 Avg 0.6074 0.6079 0.7800 0.7827 Std 0.0001 0.0001 0.0000 0.0000 Table 8: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (3B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.6434 0.6425 0.9419 0.9402 233 0.6431 0.6437 0.9405 0.9402 1234 0.6492 0.6492 0.9414 0.9429 5678 0.6531 0.6400 0.9430 0.9440 1 0.6510 0.6454 0.9446 0.9434 Avg 0.6480 0.6442 0.9423 0.9421 Std 0.0000 0.0000 0.0000 0.0000 19 Table 9: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Llama3.2 (1B).\n\n--- Segment 52 ---\nSeed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.5982 0.6232 0.7800 0.7812 233 0.5997 0.5974 0.7786 0.7812 1234 0.6156 0.6103 0.7786 0.7824 5678 0.6065 0.6012 0.7816 0.7853 1 0.6171 0.6073 0.7813 0.7832 Avg 0.6074 0.6079 0.7800 0.7827 Std 0.0001 0.0001 0.0000 0.0000 Table 8: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Qwen2.5 (3B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.6434 0.6425 0.9419 0.9402 233 0.6431 0.6437 0.9405 0.9402 1234 0.6492 0.6492 0.9414 0.9429 5678 0.6531 0.6400 0.9430 0.9440 1 0.6510 0.6454 0.9446 0.9434 Avg 0.6480 0.6442 0.9423 0.9421 Std 0.0000 0.0000 0.0000 0.0000 19 Table 9: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Llama3.2 (1B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.2722 0.2547 0.6367 0.6447 233 0.2661 0.2570 0.6456 0.6424 1234 0.2616 0.2873 0.6439 0.6352 5678 0.2684 0.2585 0.6372 0.6409 1 0.2646 0.2335 0.6393 0.6441 Avg 0.2666 0.2582 0.6405 0.6414 Std 0.0000 0.0003 0.0000 0.0000 Table 10: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Llama3.2 (3B).\n\n--- Segment 53 ---\nSeed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.6434 0.6425 0.9419 0.9402 233 0.6431 0.6437 0.9405 0.9402 1234 0.6492 0.6492 0.9414 0.9429 5678 0.6531 0.6400 0.9430 0.9440 1 0.6510 0.6454 0.9446 0.9434 Avg 0.6480 0.6442 0.9423 0.9421 Std 0.0000 0.0000 0.0000 0.0000 19 Table 9: Comparison of SageBwd and BF16 performance on GSM8K and DROP across different seeds on Llama3.2 (1B). Seed GSM8K DROP SageBwd BF16 SageBwd BF16 42 0.2722 0.2547 0.6367 0.6447 233 0.2661 0.2570 0.6456 0.6424 1234 0.2616 0.2873 0.6439 0.6352 5678 0.2684 0.2585 0.6372 0.6409 1 0.2646 0.2335 0.6393 0.6441 Avg 0.2666 0.2582 0.6405 0.6414 Std 0.0000 0.0003 0.0000 0.0000 Table 10: Comparison of SageBwd and BF16 performance on MMLU and HellaSwag across different seeds on Llama3.2 (3B). Seed MMLU HellaSwag SageBwd BF16 SageBwd BF16 42 0.4665 0.4705 0.8230 0.8319 233 0.4646 0.4560 0.8327 0.8256 1234 0.4702 0.4757 0.8202 0.8243 5678 0.4580 0.4639 0.8232 0.8276 1 0.4666 0.4691 0.8218 0.8236 Avg 0.4652 0.4670 0.8242 0.8266 Std 0.0000 0.0000 0.0000 0.0000 20\n\n