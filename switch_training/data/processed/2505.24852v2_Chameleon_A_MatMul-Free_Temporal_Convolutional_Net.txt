=== ORIGINAL PDF: 2505.24852v2_Chameleon_A_MatMul-Free_Temporal_Convolutional_Net.pdf ===\n\nRaw text length: 78022 characters\nCleaned text length: 77295 characters\nNumber of segments: 49\n\n=== CLEANED TEXT ===\n\narXiv:2505.24852v2 [cs.AR] 1 Jul 2025 1 Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data Douwe den Blanken, Graduate Student Member, IEEE, and Charlotte Frenkel, Member, IEEE Abstract On-device learning at the edge enables low-latency, private personalization with improved long-term robustness and reduced maintenance costs. Yet, achieving scalable, low-power end-to-end on-chip learning, especially from real-world sequen- tial data with a limited number of examples, is an open challenge. Indeed, accelerators supporting error backpropagation optimize for learning performance at the expense of inference efficiency, while simplified learning algorithms often fail to reach acceptable accuracy targets. In this work, we present Chameleon, leveraging three key contributions to solve these challenges. (i) A unified learning and inference architecture supports few-shot learning (FSL), continual learning (CL) and inference at only 0.5 area overhead to the inference logic. (ii) Long temporal dependencies are efficiently captured with temporal convolutional networks (TCNs), enabling the first demonstration of end-to-end on- chip FSL and CL on sequential data and inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free compute array allows either matching the power consumption of state-of- the-art inference-only keyword spotting (KWS) accelerators or enabling 4.3 higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records on Omniglot for end-to- end on-chip FSL (96.8 , 5-way 1-shot, 98.8 , 5-way 5-shot) and CL (82.2 final accuracy for learning 250 classes with 10 shots), while maintaining an inference accuracy of 93.3 on the 12-class Google Speech Commands dataset at an extreme-edge power budget of 3.1 µW. Index Terms Continual learning (CL), digital accelerator, few-shot learning (FSL), keyword spotting (KWS), matrix multiplication-free (MatMul-Free), sequential data, system-on- chip (SoC), temporal convolutional network (TCN). I. INTRODUCTION T HE rise of edge computing has driven demand for deploying deep learning models on resource-constrained devices [1], forming the backbone of the Internet-of-Things (IoT) ecosystem [2]. However, most edge artificial intelligence (AI) devices focus on inference [2], relying on pre-trained models that cannot be adapted post-deployment. This lack of adaptability limits robustness to emerging features and data distribution shifts [2], making smart sensors unreliable over time [3]. Fig. 1(a) shows that while cloud-hosted retraining offers a workaround where new labeled data is uploaded from an edge device to a central server for model updates, this approach is cloud-connectivity-dependent and does not fit latency-constrained scenarios, while leading to privacy concerns [4]. On the other hand, training a model from scratch on-device incurs high energy penalties and still requires the availability of a large labeled dataset, which is not realistic at This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Douwe den Blanken and Charlotte Frenkel are with the Microelectronics Department (EEMCS Faculty), Delft University of Technology, 2628 CD Delft, Netherlands (e-mail: Inference accelerators [10, 12] Specialized digital learning accelerators [9] PE array Inference controller Complex learning logic PE array Inference controller FSL- only macro FSL CIM macros [7, 8] This work Learning integration Inference controller PE array Cloud Edge device Private features Cloud-hosted retraining Edge device On-chip FSL CL Edge device On-chip training Pre-trained net Updated network Updated network Cloud connectivity required, latency-constrained, flexible Energy-intensive, sample-hungry, offline Offline, low-latency energy, continuous, sample-efficient Updated network Large dataset Untrained net Few labeled samples Few labeled samples Unable to learn High overhead for learning No end-to- end learning (c) Deployment view (a) Solution view 5 new classes 2 samples per class On-chip training data カ カ メ メ レ レ オ オ ン ン カ Time Learned characters メ レ オ カ メ カ メ レ カ オ レ メ ン カ Continual learning Few-shot learning DNN Inference DNN DNN カ (b) Architecture view Efficient, versatile end-to-end learning Fig. 1. (a) Overview of three approaches to endow edge AI devices with the ability to adapt, where on-device few-shot learning (FSL) and contin- ual learning (CL) enable offline, low-latency, and sample-efficient learning. (b) Comparison of an inference and three learning architectures, including ours; we minimize the footprint of learning by integrating it into the inference pipeline rather than treating it separately. (c) Our architecture supports end- to-end on-chip inference, FSL, and CL at a low overhead. the edge. Therefore, enabling efficient on-device learning at the edge requires the ability to continually learn from limited data. However, over a device s lifetime, far fewer tasks are learned than inferences performed. Hence, it is key for an edge 2 AI device to support learning without degrading inference performance and efficiency (Fig. 1(b)). Yet, current efforts to extend inference accelerators with few-shot learning (FSL) capabilities incur significant area, energy and accuracy penal- ties. For example, using quantized gradient descent (GD) to implement on-chip FSL [1], [5], [6], requires maintaining all intermediate activations in memory, transposing weight matri- ces, and using costly data types such as floating-point (FP) rep- resentations [1]. Alternatively, gradient-free FSL approaches using compute-in-memory (CIM) macros [7], [8] have been proposed to accelerate distance computation between input and stored embedding vectors for FSL. However, they fail to provide end-to-end learning support as they rely on off-chip FP32 deep neural networks (DNNs) to compute high-quality embeddings. To the best of our knowledge, only one prior work computes the DNN embeddings on chip for gradient- free FSL [9]. This design adds dedicated learning hardware alongside inference modules, yielding a 2 increase in peak power, a 25- area overhead, and requires external memory to store all model parameters for learning, undermining its suitability for extreme edge scenarios. Another challenge is that state-of-the-art (SotA) on-chip FSL architectures lack the ability to handle temporal informa- tion in two key ways. First, they cannot process sequential data. All existing FSL accelerators target image classifica- tion tasks [1], [5] [9], whereas real-world applications often involve sequential data, such as keyword spotting (KWS) where audio is typically sampled at frequencies ranging from 16 kHz to 48 kHz. However, due to the limited input context length of current end-to-end KWS accelerators (60 [10] [12] to 101 [13]), feature extraction (FE) such as mel-frequency cepstral coefficients (MFCC) is typically performed for two- order-of-magnitude reductions in input sequence length, at the expense of a 50 75 area overhead [10], [14]. Hence, capturing temporal dependencies that span multiple orders of magnitude is not only key for complex temporal tasks, it also allows streamlining system design. Second, current on-chip FSL designs cannot perform sequential learning of tasks with data distributions that shift over time. This continual learning (CL) scenario is typical of edge applications, where users add features, such as new keywords, over time. While high-quality embeddings produced by off-chip embedders have recently been shown to also support CL over up to 100 classes [15], end-to-end on-chip CL is still an open challenge. Overall, due to limited on-chip memory and strict power, area, and energy budgets, no extreme-edge solution has been proposed that effectively balances efficiency and versatility for end-to-end learning on sequential data. In this work, we present Chameleon, the first work to demonstrate scalable on-chip FSL and CL on sequential data, in a fully end-to- end fashion, and without degrading the inference performance nor efficiency (Fig. 1(c)). We achieve this through three key contributions: 1) To minimize the footprint of on-device learning, we build on prototypical networks (PNs) [16] to reframe FSL using distance computation as a forward pass through an equivalent fully connected (FC) layer, thereby leading to a unified learning and inference architecture. By integrating learning within the inference process, we enable gradient-free, end-to-end FSL and CL at only 0.5 area overhead and 0.04 added latency while setting new FSL accuracy records (96.8 , 5-way 1-shot) and CL accuracy (82.2 final, 89.0 average for learning 250 classes with 10 shots) on the Omniglot [17] dataset. 2) To enable accurate FSL and CL with temporal informa- tion, a DNN that captures long-range relationships to produce high-quality embeddings is crucial. Hence, we propose to use temporal convolutional networks (TCNs), whose parameter efficiency scales better than recurrent neural networks [18], as efficient on-chip PN embedders for sequential data at the edge. Compared to SotA TCN accelerators [11], [13], [19], we expand the receptive field by 160 while cutting activation memory by 4 . 3) To enable inference at an extreme-edge power budget while facilitating a high throughput for learning, we introduce a dual-mode matrix-multiplication (MatMul)- free processing element (PE) array. Combined with system-level power gating, we achieve 3.1 µW real-time KWS or 4.3 higher peak GOPS than SotA KWS accelerators. The remainder of this article is structured as follows: Section II covers DNN training for FSL and silicon deployments for various FSL methods, while the proposed architecture of the Chameleon system-on-chip (SoC) is introduced in Section III. Section IV then presents the measurement results for FSL, CL and inference tasks, with concluding remarks in Section V. To promote reproducibility, reuse, and improvement, all code for this paper is open-source, including the accelerator source code, the training framework, and the test simulation setup.1 II. BACKGROUND To give more insight into FSL algorithms, Section II-A presents the meta-learning framework used to train DNNs for FSL [20], also known as learning to learn. Section II-B then examines various meta-learning methods and their suitability for on-chip deployment. A. Meta-Learning Framework In supervised learning, given a standard labeled dataset D {(x1, y1) , ..., (xL, yL)} with L examples xi and labels yi (Fig. 2(a)), the training Dtrain and testing Dtest splits share classes but not examples (Fig. 2(b). However, in meta- learning, entire classes are assigned to training or testing splits (Fig. 2(c)), ensuring each split has a distinct set of classes. For meta-training, a set of M tasks [21], referred to as Dmeta,train, is used to train the DNN for FSL, defined as Dmeta,train n Dsupport train , Dquery train (i)oM i 1 , Dmeta,train D. (1) The support set Dsupport train provides labeled data for the model to learn the task, while the query set Dquery train measures per- formance on the task. Both sets consist of N ways (unique classes), with k shots (examples) per class. 1 3 k 2 shots 1 shot N 3 ways Meta-testing: Dmeta,test Dsupport Dquery Full dataset: D Meta-training: Dmeta,train Dsupport Dquery k 2 shots 1 shot N 3 ways M Q Supervised training: Dtrain Supervised testing: Dtest Batch size: 12 Batch size: 6 B C train train test test (a) Labeled dataset with L samples (b) Supervised learning data splits (c) Meta-learning data splits Fig. 2. (a) Standard dataset comprising L labeled examples across multiple classes. (b) Supervised training and testing splits, illustrated for batch sizes of 12 and 6, respectively. (c) Meta-training and meta-testing splits for an example 3-way 2-shot (Dsupport) task with 1-shot query data (Dquery). Notice that the classes do not overlap between Dmeta,train and Dmeta,test. After meta-training, the learned meta-knowledge is used to learn unseen tasks from a set of Q tasks [21] provided by the target dataset Dmeta,test during meta-testing: Dmeta,test n Dsupport test , Dquery test (i)oQ i 1 , Dmeta,test D. (2) Like before, the support set Dsupport test provides data to learn the new task, while Dquery test is used to measure test performance on the task, with both sets potentially differing in ways and shots from meta-training. B. Evaluating Meta-Learning for On-Chip Deployment While a plethora of meta-learning methods exist that ex- ploit this task-based training regime, they can be categorized into three groups: parameter initialization, feed-forward, and metric-learning methods [21], which we introduce with a focus on their suitability for on-chip deployment. Parameter initialization These methods aim to find op- timal initial DNN parameters, enabling quick fine-tuning on unseen tasks with few examples [21]. Model-agnostic meta- learning (MAML) [20] is a widely used approach in this category [21], and employs nested GD. During meta-training, the inner GD loop is carried out for each new task, while the outer loop optimizes the initial parameters such that the inner loop requires only a few steps of GD to achieve high (a) Parameter initialization (c) Metric-learning (b) Feed-forward Aux. DNN DNN ζ φ φ Init. DNN ζ φ Final DNN φ On-chip GD steps DNN agnostic Gradient storage High precision data types Weight transposition Multiple forw. backw. passes Gradient-free Parameter overhead Computation overhead Recomput. per query DNN-agnostic Gradient-free Distance computation ζ φ φ Base DNN On-chip Off-chip Frozen Trained: Fig. 3. Qualitative comparison of the suitability of the three meta-learning method types for on-chip deployment. accuracy. Using the initial parameters resulting from meta- training, the deployment of MAML requires only the inner loop to be executed on-chip (Fig. 3(a)). Key advantages are that it introduces no extra learnable parameters [20] and imposes no architectural constraints on the DNN. However, a major drawback for on-chip deployment is that GD must be executed on-chip. Although proposals of initialization- based methods for edge deployment exist [1], [5], [6], they require (i) buffering all intermediate activations required for GD operation, instead of discarding them as soon as the next- layer values are computed during inference, (ii) support for on- chip weight transposition operations, and (iii) the use of more expensive data types than quantized inference would typically require, such as (block) floating-point representations [1], [6]. Additionally, the proposed quantized FSL setup in [1] requires 300 forward and backward passes to be performed for on- chip GD, significantly increasing the compute requirements for learning. Furthermore, to maintain high accuracy, [1] resorts to the Adam optimizer [22], leading to a triplication of the required on-chip weight storage. Feed-forward models These methods aim to learn a direct mapping from support examples Dsupport to parameters needed for predictions on query examples Dquery [21]. Unlike initialization-based methods, feed-forward models eliminate the need for gradient updates during deployment. However, they can introduce significant parameter overheads stemming from auxiliary DNNs or require handcrafted neural network (NN) architectures that may not align with DNNs typically supported for inference (Fig. 3(b)). A well-known example is the simple neural attentive meta-learner (SNAIL) [23], in which the model predicts, using N k support example-label pairs and a query example, the class of the query in one forward pass. SNAIL uses the same base DNN as MAML [20] but requires a second, auxiliary module that has 29 more parameters compared to the base DNN. Another method in this category is memory-augmented NNs (MANNs) [24], which use an external memory to store vector representations of previous tasks for rapid adaptation to new ones. FSL- HDnn [9] implements an on-chip MANN through hyperdi- mensional computing (HDC). In HDC, base DNN embeddings are projected into a high-dimensional space via multiplication with a random { 1, 1}-valued matrix [25] (typically 1M parameters). Classification is then performed by finding the 4 stored class encoding with the closest Hamming distance to the encoded input. In FSL-HDnn, a dedicated module performs encoding, storage, and classification, but increases on-chip power consumption by 120 relative to the power for the base DNN s forward pass. Metric learning These methods aim to learn a model that generates vector representations for each support and query sample (Fig. 3(c)), which can then be used to enable the clas- sification of unseen examples without any gradient updates, similar to feed-forward methods. A popular approach in this category is prototypical networks (PNs) [16]: a feature vector (i.e. embedding) is computed for each sample in the support set Dsupport, after which these embeddings are averaged class- wise to obtain a set of prototypes. Classification is performed by comparing the query sample embedding to the prototypes of all support classes using a distance function. The query is assigned to the class with the closest prototype. A key advan- tage of this method is that, like MAML, it is model-agnostic, requiring no additional network modules and parameters. However, unlike MAML, it is also gradient-free, with only a single extra step distance computation outside the standard NN inference pipeline. [7], [8] implement a CIM-based PN variant that stores support examples individually instead of averaging them. Only the embedding storage and distance computation classification circuits are implemented in silicon, with embedding generation relying on off-chip floating-point DNN inference. Both designs use L1 distance, causing a 7 accuracy drop in 32-way and 5 in 5-way 1-shot tasks for [7], [8], respectively, compared to an L2-based PN in FP32. III. THE CHAMELEON SYSTEM-ON-CHIP ARCHITECTURE This section describes the Chameleon accelerator SoC, as shown in Fig. 4, which highlights the key building blocks that implement our three contributions. The unified architecture for FSL, CL and inference is detailed in Section III-A, while Section III-B covers the mechanism for processing long input sequences using TCNs. Finally, Section III-C discusses the dual-mode MatMul-free PE array. A. Unified Learning and Inference Architecture Chameleon s architecture builds on the typical design of a DNN inference accelerator, featuring separate memory banks for activations (4 bits), weights (4 bits) and biases (14 bits). The activation and weight memories are connected to a 16 16 PE array tailored for efficient TCN execution. To endow this baseline inference accelerator with learning capabilities, we propose to leverage a reformulation of PNs (Fig. 5) as an equivalent fully-connected (FC) layer. We begin by defining each prototype as Pj sj k , sj k X l 1 el,j, (3) where el,j is the lth V -dimensional support embedding for the jth way and k indicates the number of shots. Build- ing on the insight of [16] by using the L2 distance func- tion and squaring it to factor out its square root term, the Accumulation, bias and ReLU Learning controller 16b 4-phase handshake input bus SPI Activation memory 256 64b (2 kB) Prototypical parameter extractor 8b 4-phase handshake output bus Weight memory 512 1024b (65 kB) Input buffer 32 64b (0.25 kB) Bias memory 128 224b (3.5 kB) Config. registers Argmax tree Dual-mode Mat- Mul-free PE array PE PE PE PE PE PE PE PE PE PE PE III-A III-C (16 16) Network address generator III-B Fig. 4. Architecture of the Chameleon SoC, building on a typical DNN in- ference accelerator design. The learning controller and prototypical parameter extractor endow this architecture with on-chip learning capabilities, while the network address generator enables long-context learning and inference using TCNs. The dual-mode MatMul-free PE array allows for switching between high-throughput and low-leakage operation. PN distance computation between a prototype Pj and a query embedding x can be reformulated as sj k x 2 D2 j V X i 1 sj i k xi !2 V X i 1 sj i 2 k2 x2 i 2sj ixi k ! . (4) As the term PV i 1 x2 i is a constant offset that only depends on x and does not influence the relative distances Dj between x and the prototypes Pj, this yields D2 j 1 k2 V X i 1 sj i 2 2 k V X i 1 sj ixi. (5) Then, after rescaling by k 2, we equivalently get D2 j bj Wj x, with bj 1 2k V X i 1 sj i 2, Wj sj. (6) Equation (6) thus corresponds to an FC layer whose output neuron j has a bias bj and a weight vector Wj, which can both be calculated from sj, the sum of the k support embeddings as per Equation (3). By reformulating PNs as an equivalent FC layer, it becomes possible to equip any inference architecture with learning ca- pabilities with minimal overhead. To support learning, only the ability to extract the equivalent FC weight and bias parameters from a prototype is required, since the resulting FC layer can then be accelerated by the existing inference datapath. Importantly, this setup is invariant to the type of off-chip meta-training performed pre-deployment: if the final model produces high-quality embeddings, it can be deployed for on-chip FSL. Moreover, unlike initialization-based and feed- forward meta-learning methods (Section II), PNs naturally extend to CL as this simply requires the storage of additional class prototypes over time. 5 ζ ζ N 2 ways DNN φ φ K 2 shots New task "Prototype" φ Input sample Embedding vectors Class-mean embedding Class 2 prototype Input embedding Fig. 5. Learning a new task using prototypes as performed in Chameleon: N ways with k shots are embedded using a fixed DNN to create N k embedding vectors: averaging these class-wise results in N prototypes. To classify an input sample, it is embedded and compared via L2 distance to the N prototypes, assigning it to the class with the minimum distance. In Chameleon, PN weight and bias extraction is handled by two dedicated modules: the learning controller and pro- totypical parameter extractor (Fig. 4, highlighted in green). The learning controller tracks states such as the current way or shot within that way, and controls the FC layer dataflow during inference, while the parameter extractor is responsible for extracting weight and bias parameters for new classes during the learning phase. The learning process is carried out in hardware as per Fig. 6, which illustrates the three steps that Chameleon follows to learn a new class (way) while maximizing reuse of the inference datapath and logic. This low-latency process only requires (k 2) V 16 1 cycles2 and scales linearly with both the number of shots and the embedding size, while inducing only a 0.5 area overhead relative to the total core area. Overall, our prototypical learning strategy enables a unified architecture for learning and inference, by reusing existing inference memories and thus eliminating the need for FSL- specific storage. It extends naturally to CL by storing new prototypes over time and can be applied to existing inference accelerator designs, with learning reduced to minimal control logic for converting prototypes into weights and biases. Unlike designs focused solely on learning, our approach enables a lightweight integration of learning into the inference process. B. Long-Context Learning and Inference Using TCNs Real-world natural data is often sequential, including bi- ological signals (e.g., heart rate, blood pressure), environ- mental data (e.g., wind speed, humidity), and perceptual data (e.g., video, audio). However, the development of algorithms for FSL and CL has historically focused on benchmarks with static data, such as images [21]. To the best of our knowledge, beyond a first proof of concept in [26] for olfactory data, no chip design has demonstrated competitive performance for sequential FSL and CL on well-established benchmarks. As our learning strategy with PNs relies on learning high- quality embeddings, we propose to extend this approach to sequential data by adopting a scalable embedder that can cap- ture long-range dependencies across full sequences. Typical DNN models for learning with temporal data range from recurrent NNs (RNNs) to transformers. On the one hand, 2 The division by 16 accounts for the 16 16 PE array. Weight mem. Bias mem. Inference flow φ Repeat k times Act. mem. 1. Embedding generation 2. Prototype conversion 3. FC parameter extraction Inference Act. mem. Embedding φ Inference flow incl. FC layer Learn. ctrl. Prototypical parameter extractor Fig. 6. FSL in Chameleon is performed in three steps. 1. For all k shots of the new class, inference is performed to compute the embeddings, which are saved in the activation memory. 2. The embeddings from the activation memory are loaded and used in the PE array to compute the prototype. 3. The prototype is converted to equivalent FC bias and weights that are stored in their respective memories. It is then possible to perform inference using the new FC layer to classify an input sample. RNNs have the advantage of scaling with O(1) memory requirements in sequence length, at the expense of stability issues for long-range dependencies. They typically fail at 1k sequence lengths [18] that are necessary, e.g., for raw-audio processing. On the other hand, transformers [27] achieve SotA performance when trained on large datasets ( 106 examples), surpassing recurrent and convolutional architectures. Yet, on smaller datasets, RNNs like LSTMs [28] typically outperform transformers [29]. Furthermore, transformer encoders, required for embedding generation, also incur O(n2) memory and time complexity, making their end-to-end deployment at a µW- level power budget an open challenge. In contrast, convolu- tional architectures like TCNs [18] offer O(log2 n) memory complexity, while iso-parameter comparisons show that they outperform both RNNs and transformers on tasks with 106 examples [29]. Hence, we propose to use TCNs [18] as efficient, scalable embedders for inference and FSL on sequential data at the edge. As shown in Fig. 7(a), TCNs employ stacked causal 1D convolutions with residual connections to ease training in deep networks [30]. They are able to capture long-range de- pendencies via dilation, which doubles at each residual block, resulting in a receptive field (R) that grows exponentially with network depth: R 1 L 2 X l 1 2l (k 1), (7) where L is the number of layers and k the kernel size. In classification scenarios, dilation also makes deeper lay- ers exponentially sparser, as illustrated by the white circles in Fig. 7(b), resulting in a streaming memory complexity of O(log2 n) when skipping these computations. However, leveraging this favorable memory scaling is nontrivial due to the computational graph structure induced by dilation and the residual connections in TCNs. We solve this by introducing two key techniques. Our first technique targets the efficient exploitation of dilation-induced sparsity in the TCN s computational graph. In TCN-CUTIE [19], 1D convolutional kernels are mapped to equivalent 2D kernels: however, this introduces 80 zero multiplications for k 2, as dilation is emulated via zero- 6 Causal Conv1D k k, dilation 2 l BN ReLU Causal Conv1D k k, dilation 2 l BN ReLU Conv1D k 1, d 1 TCN residual block (a) (b) TCN res. block k 3, l L TCN res. block k 3, l 1 FC layer FC layer ReLU Input sequence TCN layers Unused Output Residual connnection d 4 d 2 d 1 Kernel size (k): 1-15 Max. 1024 channels layer; max. 32 FC Conv1D layers Fig. 7. (a) DNN structure supported by Chameleon. Each TCN residual block contains two causal 1D convolutions, each followed by batch normalization (BN) and rectified linear unit (ReLU) activation. If input and output channels match, the Conv1D residual can be replaced with an identity. Both convolu- tional layers in a residual block share the same dilation factor d, doubling in successive blocks, starting at d 1. (b) Computational graph of a 6-layer TCN (three stacked residual blocks), where white circles indicate zero-valued activations introduced by dilation. padding, with sequence lengths limited to 24 timesteps for TCN processing. UltraTrail [13] adopts a native 1D convo- lutional kernel mapping to performs TCN inference instead, but lacks dilation support, thereby restricting its applicability to tasks requiring small receptive fields (demonstrated up to 101 timesteps). Furthermore, its weight-stationary dataflow necessitates full sequence pre-loading, which is incompatible with streaming inference of long sequences. Giraldo et al. [11] introduce a FIFO-based partial output stationary dataflow that supports dilation and reuses activations across timesteps. How- ever, since the model outputs one classification per input in the sequence and as each output is connected to a different set of TCN graph nodes, this strategy does not avoid computing the unused nodes shown in Fig. 7(b). Expanding on this FIFO- based scheme, we introduce a greedy dilation-aware execu- tion (Fig. 8), which leverages a specialized network address generator to skip redundant activations and enable greedy TCN processing with layer-wise FIFO activation storage. Fig. 8(a) presents the greedy computational graph of a four- layer TCN as executed in Chameleon. Each layer is triggered as soon as the number of available inputs matches its kernel size, and this condition cascades through subsequent layers. When additional inputs are required, control reverts to earlier layers. This scheme efficiently leverages FIFO-style activation memory, as illustrated in Fig. 8(b). The address generator ensures that the output of a new timestep always overwrites the oldest, unused one. The same mechanism applies to the input memory, where outdated inputs are automatically replaced. Through this greedy processing scheme, the total memory requirements for TCN inference are significantly reduced. Moreover, this dilation-aware processing also drastically re- duces the number of inference operations by skipping zero- valued activations that do not contribute to the final output. We demonstrate in Fig. 8(c) that Chameleon achieves 90 memory and 7 compute reduction over weight stationary (WS) non-dilation-optimized TCN inference at a sequence length of 16k (raw audio) with a fixed network size of 130k parameters (Chameleon s maximum supported size). Our second technique enables efficient handling of residual connections. As sequence lengths grow, larger dilation factors stemming from deeper networks are needed to expand the receptive field (Fig. 7), making residual connections essential for effective optimization of these deep networks [30]. While TCN-CUTIE [19] supports up to ten convolutional layers, it lacks support for residual connections, severely limiting scalability. UltraTrail [13] does support residuals but requires three separate buffers and complex memory access control logic. Giraldo et al. [11] omit residuals but still employ a ping-pong buffer setup. To eliminate the need for multi-buffer schemes to manage residual layers without sacrificing FIFO- style inference, our residual-aware register file processing (Fig. 9(a), right) uses a two-port register file that enables simultaneous reads and writes. To support asynchronous streaming input alongside ongoing processing, Chameleon stores DNN inputs in a dedicated 0.25 kB memory. By combining the two techniques described above and as shown in Fig. 9(b), Chameleon reduces activation memory by 76 , 28 , and 4 compared to prior TCN accelerators [11], [13], [19], while enabling deployment of DNNs with up to 5.5 more weights per kB of activation memory. This enables, for the first time, raw audio KWS on 16k-step inputs two orders of magnitude longer than in previous TCN accelerators. C. High-Throughput or Low-Leakage Operation Using a Dual-Mode, MatMul-Free PE Array Since learning in Chameleon is integrated within the in- ference pipeline, we propose two architectural enhancements, so as to (i) reduce the PE array and memory footprints with inference-optimized quantization, allowing for MatMul-free operation with 4-bit signed log2 weights, and (ii) endow the PE array with the flexibility to optimally support different inference and learning scenarios. Our first technique focuses on inference-optimized quan- tization. For instance, proposals for deploying parameter initialization meta-learning methods on-chip, require on- chip GD, which entails floating-point precision requirements. Murthy et al. [1] use block floating point (8-bit shared exponent, 4-bit mantissa) for weights, activations, gradients and errors. Compared to regular integer operation, additional logic for exponent management, as well as normalization and alignment is required. Wang et al. [6] demonstrates the feasibility of using signed 16-bit uniform quantization of the weights and gradients for various initialization-based meta- learning methods, but reduces memory requirements only by 7 2 Activation address 9 0 Input address Time 0 2 6 13 9 3 16 15 12 11 5 7 2 4 1 x9 x7 x8 x5 x6 x3 x4 x1 x2 x0 18 14 10 8 0 x11 x12 x10 17 19 0 1 2 3 4 5 6 5 7 8 9 10 6 9 8 11 12 13 14 0 1 2 3 4 5 6 7 8 9 10 11 12 12 15 16 9 13 17 18 19 Input seq. Input mem. Activation memory Dilation Zero-valued activations Residual connection Causal convolution Layer 2 1 0 3 Kernel index 0 1 2 0 1 2 0 1 2 0 Oldest input is overwritten FIFO Same input used in different kernel positions (a) (b) - Chameleon WS (c) 90 lower 7 lower Fig. 8. (a) Four-layer TCN with 13 inputs. Indices show greedy processing order; colors indicate per-layer memory locations. Causal convolutions and residuals are annotated, while the processing and storage of zero activations resulting from dilation (white) is skipped by Chameleon. (b) FIFO-style activation memory allocation over time in Chameleon. Each layer overwrites its oldest activation during execution. Indices show processing order; bar lengths indicate activation lifetimes. (c) Memory and compute comparison between WS TCN inference and Chameleon s strategy, using 130k parameters and producing identical outputs. (a) This work [11, 19] UltraTrail [13] 32,79 kB 32,79 kB 4 kB 2.2 kB 1.3 kB No residuals, ping-pong buffer Complex control, triple buffer 0 1 0 1 2 Conv1D Conv1D Res. TCN layer with residual path 2 kB 0 2 Low overhead, single memory 1 4 smaller memory And 2 OoM longer input At 5.5 more weights kB (b) 19 Fig. 9. (a) Comparison of residual operation steps and activation memory sizes across TCN accelerators. [11], [19] use ping-pong buffers but lack residual layer support; [13] requires a triple buffer setup for residual layers; Chameleon uses only a single dual-port memory, reducing memory and control complexity. (b) Impact of computation strategy on activation memory size, maximum number of weights kB of activation memory and input sequence length between TCN accelerators. 2 compared to learning with FP32. FSL-HDnn [9] uses brain floating point (Bfloat)-16 to generate embeddings for HDC during FSL, also saving 2 memory compared to a vanilla FP32 implementation. A deployment proposal for MAML [5] demonstrates the use of 4-bit signed uniform weights, but still requires 8- or 16-bit gradients depending on task dif- ficulty, hence increasing memory requirements by 2 4 compared inference-only quantization. As Chameleon operates in a purely forward manner due to its gradient-free learning, it can rely on a simple quantization scheme that is optimized for inference. Hence, we propose to use log2 weights [31]: this allows replacing multipliers with bit shifters, based on which we enable fully MatMul-free inference and embedding computation (Fig. 10(a)). During inference, the PE array uses an output-stationary dataflow to efficiently support our greedy PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE Adder Adder Adder Adder ReLU i0 i1 in-1 in-2 16 16 4-bit weights 16 4-bit inputs 16 4-bit outputs 18-bit acc. register Bias OPE - 4-bit log2 weight Unsigned exponent PE Residual scale Output scale 16 OPE 4-bit unsigned 12-bit signed 4-bit unsigned activation (a) PE array structure (b) Shift-only PE (c) OPE structure o0 on-1 o1 on-2 Horizontally broadcast inputs Vertically accumulate PE outputs: 16-bit signed Per-layer residual and output scale Fig. 10. (a) Diagram of the MatMul-free PE array, operating under an output- stationary dataflow. (b) Individual PE, which uses a bit shift in place of a multiplication. (c) Output PE (OPE) module, which performs input output rescaling as well as bias addition and ReLU activation. TCN processing scheme. Each cycle, it receives 16 4-bit unsigned uniform ReLU-based activations, either from the input memory (first layer) or activation memory (subsequent layers), along with 16 16 log2-quantized weights from the weight memory. We use 4-bit signed log2 weights, as they offer the same dynamic range as 8-bit signed integers while halving storage, reducing power and energy footprints [32], and maintaining floating-point accuracy [32]. Each individual PE (Fig. 10(b)) first left-shifts the horizontally broadcast input by the weight s exponent and then applies sign correction, producing a 12-bit signed output. These are summed vertically and accumulated in 18-bit signed uniform registers within output PEs (OPEs), detailed in Fig. 10(c). The OPE performs input rescaling (for residuals), bias addition, ReLU activation, and output rescaling before its output is written back to the activation memory. With inference being learning-free now, we also make learning fully multiplication-free by adapting Equation (6) for a log2 formulation, allowing for efficient PN parameter extraction. Since all weights are log2 (including sj i), the square in the bias term simplifies, thereby fully replacing multiplication by bit shifts: 8 Legend 4 4 Core logic 256b 112b 56b 128 rows Bias memory 5 128b 64b 64b 512 rows OPEs 16 16 high throughput eff. 64b 64b Always on Off during 4 4 Power domains Weight memory 256 rows 56b 16 16 Virtually stack adjacent memories to increase of rows 4 4 low-leakage, small nets (a) (b) PE array Act. memory 16b Fig. 11. (a) Comparison of simulated real-time KWS power and peak TOPS W estimates for different PE array sizes. (b) Data layout for activation, bias, and weight memories to support both 4 4 and 16 16 PE array usage, allocating LSBs of weight memories to the top-left 4 4 section. In 4 4-mode, gray memories are powered off, while the remaining weight memories are virtually stacked to double the row count. 4.3 higher 2 lower v1 v1 v2 v1 16 v2 Fig. 12. Peak GOPS, real-time KWS power and accuracy comparison on 12-class Google Speech Commands (GSC, v1 and v2) between KWS accel- erators. bj 1 2k V X i 1 2(log2 sj i) 1, Wj sj. (8) The OPE is then reused to implement the division by 2k in Equation (8), via a right shift with 2 log2(k) bits. Our second technique allows efficiently supporting dif- ferent deployment scenarios, from leakage-dominated real- time KWS inference to throughput- and dynamic-power- constrained FSL operation with large DNN embedders. For in- stance, Vocell [10] has a 36 leakage contribution (excluding analog feature extraction) during real-time KWS, similar to the 30 for UltraTrail [13] and 33 for Giraldo et al. [11] with limited throughputs of 0.13, 3.8 and 0.26 GOPS, respectively. In contrast, TinyVers [12] achieves a 4-135 higher through- put of 17.6 GOPS: however, this comes at the cost of an order- of-magnitude increase in real-time KWS power consumption. Hence, to support low-leakage operation without degrading the design s throughput, we introduce a dual-mode PE array whose size is reconfigurable, which enables Chameleon to efficiently support both µW-power real-time inference and high-throughput tasks with large DNN embedders. To determine the optimal PE array sizes for the two modes, we simulate real-time KWS and peak TOPS W performance in Fig. 11(a), assuming SRAMs dominate total system power. The analysis identifies array sizes of 4 and 16 as optimal. We couple operation in both modes with a memory layout that enables power gating of specific weight and bias memory sections, while clock gating unused PEs in 4 4-mode (Fig. 11(b)). Instead of storing DNN weights purely in row- or column-major order, the top-left 4 4 weights are stored row-major in the first two banks, followed by the remaining 12 4 weights. In 16 16 mode, all banks are active, enabling 16 16 4-bit reads per cycle. In 4 4 mode, MSB banks are power gated, leaving only the LSB banks, referred to as always-on, active. These are virtually stacked to enlarge the address space, supporting 16k weights over 1024 addresses and 512 biases over 128 addresses. Comparing Chameleon to other KWS accelerators in Fig. 12, we observe that its dual-mode operation enables a 4.3 higher peak throughput in 16 16 mode, a 16- fold increase from Chameleon s 4 4 mode. Meanwhile, in 4 4 mode, Chameleon achieves a 2 power reduction for real-time end-to-end KWS inference while maintaining a classification accuracy on par with state-of-the-art inference- only accelerators. IV. MEASUREMENTS Chameleon was taped out in TSMC 40-nm low-power (LP) CMOS technology (Fig. 13). It occupies 1.25 mm2 with a core area of 0.83 mm2, including the power rings. The SoC has two separate power domains, corresponding to (i) the core logic (using a mix of SVT and HVT standard cells) and always-on memories, and (ii) the gateable MSB memories. We first outline the software and measurement setup (Sec- tion IV-A), then present the results for FSL and CL (Sec- tion IV-B), followed by real-time KWS results (Section IV-C), and finally compare Chameleon to the SotA (Section IV-D). A. Software and Measurement Setup Our TCN models are based on the original architecture in [18], where we introduce several modifications incorpo- rating techniques from modern deep convolutional residual networks (ResNets) [30] for normalization [33], [34] and initialization [34], [35]. To deploy TCNs on Chameleon, we perform quantization- aware training (QAT) using the Brevitas framework [36], after FP32 training. QAT starts from the FP32 checkpoint with the lowest validation loss, with all parameters quantized and batch normalization (BN) layers folded into the weights of the previous layer, as per [37], [38]. Within Brevitas, we implemented a custom variable-bit quantizer for signed log2 weights and a custom requantizer to simulate overflow of the 4-bit unsigned uniform activations in Chameleon. We apply asymmetric and symmetric quantization for activations and weights respectively, both with per-tensor scaling [39]. During 9 Gateable memories (off during 4 4) Always-on memories Core logic 0.91 mm 1.12 mm ZCU104 FPGA board Daughterboard Chip (a) Chip summary (c) Test setup (b) Area breakdown ( ) (d) Chip microphotograph 0.91 mm 1.12 mm (e) Efficiency and max. frequency vs. core memory voltage Fig. 13. (a) Metrics summary of the Chameleon SoC. (b) Area breakdown of the SoC per module. (c) Test setup, including a Zynq UltraScale MPSoC ZCU104 FPGA that connects to a daughter board hosting the chip. (d) Anno- tated die photograph of the Chameleon SoC. The bottom section of the SoC, containing the gateable memories, can be powered off to enable inference with small networks using the 4 4 mode of the PE array. (e) Efficiency and maximum frequency characterization of Chameleon. quantized PN training, the TCN embeddings are using 4- bit unsigned uniform quantization, like activations (see Sec- tion III-C). The resulting prototypes are quantized using 4-bit signed log2 quantization, as they are converted to prototypical parameters as per Equation (8). Our measurement setup is shown in Fig. 13(c). A daughter board hosting the Chameleon chip connects to external power supplies and is interfaced with a Zynq UltraScale MPSoC ZCU104 evaluation board, which provides test stimuli and records the results. The complete deployment and test setups are part of the open-source repository of Chameleon. Note that all metrics that we report in the remainder of this section are obtained at room temperature. B. FSL and CL To demonstrate end-to-end on-chip learning on Chameleon using our unified learning and inference architecture, we benchmark Chameleon for FSL and CL scenarios. Benchmarking setup For both FSL and CL, we use the Omniglot dataset [17], which comprises 1,623 classes of handwritten characters from 50 alphabets (see Fig. 14, right, for selected examples). Every class has 20 examples, each of Characters from different scripts in the Omniglot dataset: Fig. 14. Sample characters from different alphabets, taken from the Omniglot dataset, including a demonstration of flattening an image from the dataset to enable sequential Omniglot on 1D sequences of pixel data. which is drawn by a different person. To align with other works on FSL, we use the Vinyals train-val-test split [40]. Following [16], [24], we augment all splits by generating new classes via 90 , 180 , and 270 rotations and resize all images to 28 28 for consistency. To adapt the images for TCN pro- cessing, we flatten each image pixelwise (see Fig. 14, bottom), effectively creating a sequential Omniglot representation. FSL results Using a 116k-parameter TCN with 14 layers, we report in Table I the classification accuracies on the Omniglot test set across 5,20-way 1,5-shot as well as 32-way 1-shot scenarios found in the state of the art. It can be seen that Chameleon outperforms existing works [7] [9] by up to 16 accuracy points, setting new records across all scenarios, even though both FSL CIM designs [7], [8] use FP32 off- chip embedders. Chameleon consumes 11.6 mW for end-to- end FSL on Omniglot at 100 MHz and 1.0 V. At 100 kHz and 0.625 V, it consumes 12.9 µW. The latencies for learning one shot of a new class are 0.59 ms and 0.54 s respectively, yielding an energy per shot of 6.84 µJ and 6.97 µJ, including the embedding phase. Since prototypical parameter extraction only takes a few clock cycles, 0.04 of the embedding computation time on Omniglot, learning and inference have effectively identical total energy, power and latency. This contrasts sharply with other on-chip training methods like CHIMERA [41], which employs low-rank training with 8-bit signed parameters and requires 103 104 RRAM update steps for learning. Similarly, Park et al. [42] use FP8 for full on-chip SGD-based training and hence could host MAML-like setups. However, their lowest reported power is already 13 higher than Chameleon s peak end-to-end power, demonstrating its incompatibility with extreme-edge operation. Alternatively, instead of learning new classes, Cioflan et al. [43] use embed- dings to adapt to the speech characteristics of end users for KWS. A microcontroller updates an embedding layer using few-shot spoken keywords, inducing a 35 FLOPS overhead compared to embedding computation alone. CL results We evaluate CL on Chameleon using the same TCN model as for FSL. CL is performed by learning one new class at a time, up to 250 classes, using 1, 2, 5, or 10 shots. As Chameleon is the only silicon-proven work to perform end-to-end fully on-chip CL, Fig. 15 compares its CL per- formance (continuous lines) to the FSL performance of prior 10 TABLE I. FSL test accuracy comparison between FSL accelerators that have reported results in silicon on the Omniglot dataset. The shows 95 confidence intervals over 100 tasks. 5-way 20-way 32-way 1-shot 5-shot 1-shot 5-shot 1-shot Kim et al. [7] 93.4 98.3 - - - SAPIENS [8] - - - - 72 FSL-HDnn [9] 79.0 - - 79.5 - This work 96.8 1.6 98.8 0.5 89.1 1.3 96.1 0.5 83.3 1.2 Uses an off-chip FP32 embedder. 5 2032 50 100 150 200 250 Number of ways 60 70 80 90 100 Accuracy ( ) 1-shot 2-shot 5-shot 10-shot [7] [8] [9] Fig. 15. End-to-end CL classification accuracies on the Omniglot dataset using Chameleon for 2 250 ways with 1, 2, 5 and 10 shots, compared to other FSL chips (points outlined in black). The shaded regions indicate 95 confidence intervals over 20 tasks. For 5 ways, shot count has little effect on accuracy; with more ways, additional shots help, with diminishing returns beyond 5 shots. works [7] [9] (single data points). Notably, in the similar few- shot class-incremental learning (FSCIL) [44] scenario, where a DNN learns a set of classes before deployment and a new set of classes online, there is currently also no scalable end-to- end fully on-chip design. For example, Karunaratne et al. [15] propose a CIM macro for MANNs targeting FSCIL: their design supports embedding aggregation (summing) and uses a 4 larger embedder than [7], enabling 100-way FSCIL with 5 shots per class. However, embeddings are still generated off- chip with an FP32 embedder and binarized before on-chip use, similar to [7], [8]. Alternatively, Wibowo et al. [45] demon- strate FSCIL with the embeddings for PNs computed on-chip, using a microcontroller. However, they require an external L3 memory to store the 8-bit (2 larger than Chameleon) DNN weights and use cosine similarity as the distance function for PNs, requiring an expensive division operation. Beyond embedding-based learning, in [26], Huo et al. demonstrate a first proof of concept for fully on-chip end-to-end FSCIL. The design uses a spiking NN with spike timing-dependent plasticity and lateral inhibition: it supports classification of nine types of gas using olfactory data and overcomes data disturbances caused by sensor drift. However, since it supports networks of only up to 4k 4-bit parameters, its suitability for more complex tasks such as Omniglot is unclear. C. Real-time KWS To characterize both (i) Chameleon s inference efficiency using its dual-mode MatMul-free PE array, and (ii) its long- context modeling capabilities using the greedy dilation-aware execution of TCNs, we conduct a series of KWS experiments. 13 87 MFCC (4 4): 3.1 µW 2 36 62 MFCC (16 16): 7.4 µW 88 5 8 Raw (16 16): 59.4 µW Dynamic power Core leakage MSB memories leakage Fig. 16. Comparison of power contributions (at 0.73 V) during real-time KWS using MFCC feature vectors in both 4 4 and 16 16 mode, as well as raw audio in 16 16 mode. Down Go Left No Off On Right Stop Up Yes Silence Unknown Predicted label Down Go Left No Off On Right Stop Up Yes Silence Unknown True label 94 88 94 92 93 94 96 98 94 96 98 84 MFCC-based KWS [93.3 ] Down Go Left No Off On Right Stop Up Yes Silence Unknown Predicted label 84 84 87 83 86 86 91 93 86 90 96 72 Raw-audio KWS [86.4 ] 0 20 40 60 80 100 True positive rate ( ) Fig. 17. Confusion matrices for MFCC-based KWS and raw audio KWS on the 12-class GSCv2 test set, including true positive rates for all keywords. Benchmarking setup We use the Google Speech Com- mands V2 (GSCv2) dataset [46], which comprises 105,829 utterances from 2,618 speakers across 35 spoken-word classes, with varying sample counts per class. Each sample is recorded for one second at 16 kHz. Ten words serve as command- like keywords: Yes, No, Up, Down, Left, Right, On, Off, Stop, and Go. Along with an unknown command class (grouping the remaining words) and a silence class (one-second audio clips from background noise files), they form the standard 12-way classification setup. We follow the same validation and testing splits as [46], [47]. To augment the training data, we apply up to 100 ms forward or backward shifts (as in [47], [48]), followed by noise addition with a probability of 0.15. Following floating-point training, we perform QAT, during which all augmentations are disabled. For PE array characterization, the audio is transformed into a 28D MFCC feature map [49] before it is fed to the DNN. To align with other KWS digital accelerators [10] [12], we use a window size of 32 ms with a 16 ms shift for MFCC-extraction, yielding a sequence of 63 timesteps. For long-context modeling, the raw audio data is used. Dual-mode PE array characterization To minimize the static-power footprint for real-time MFCC-based KWS, we use a TCN with 16.5k parameters and eight layers, which is small enough to fit in the always-on memories of Chameleon. Running this TCN in the 4 4 PE array mode results in a real-time power consumption of 3.1 µW at a clock frequency of 23.3 kHz with a core voltage of 0.73 V, achieving a classification accuracy of 93.3 on the 12-class test set of GSCv2. Executing the same network in the complete 16 16 PE array requires a clock frequency of 3.67 kHz to yield a 11 real-time power of 7.4 µW, with both the MSB memories and the core at 0.73 V. Figure 16 displays the contributions of the core leakage, MSB memories leakage and dynamic power for both real-time KWS scenarios. It can be seen that executing MFCC-based KWS with only the always-on memories leads to a power reduction of 44 compared to the 16 16 baseline. Note also that the dynamic power in the 4 4 mode is higher compared to the 16 16 mode: since the throughput in the latter is 4 higher, the clock frequency can be downscaled accordingly to achieve the same latency. Long-context modeling results In most KWS accelerator designs, the footprint of the MFCC pre-processing exceeds that of the actual DNN inference logic. For example, in the work by Shan et al. [50], the MFCC extraction accounts for 60 of the core area, while in [14] the feature extractor is 4 larger than the on-chip DNN accelerator. Similarly, the MFCC extraction module in [10] uses the same amount of area as the on-chip DNN accelerator (incurring a 2.4 area overhead when also considering the analog feature extraction). Even fully digital designs like [51] allocate 40 of the core area to MFCC logic. To the best of our knowledge, there is currently only one design operating directly on the raw audio sequence without any specific pre-processing logic: the fully analog accelerator in [52] performs inference on 8 kHz audio by applying a trained filter bank to 4 ms input windows. However, several key DNN processing steps are performed off- chip, preventing end-to-end operation. In contrast, by enabling long-context inference, our approach eliminates the need for any data-specific pre-processing block, significantly increasing the number of possible deployment scenarios. Using a 24- layer, 118k-parameter model, Chameleon achieves 86.4 test accuracy on GSCv2 (see Figure 17) from 16 kHz raw audio with 59.4 µW real-time power at a clock frequency of 532 kHz and a core MSB memory voltage of 0.73 V (Fig. 16, right). Although accuracy drops by 7 points, reflecting the challenge of capturing long-range dependencies, it remains competitive, outperforming [14] with 1.6 mm2 analog MFCC at 86.0 12- class accuracy. D. Comparison with the State of the Art Chameleon is the first SoC supporting both inference and end-to-end on-chip few-shot and continual learning: Table II compares our design with both KWS and FSL accelerators that have silicon results on GSC and Omniglot datasets, respectively. This comparison highlights the performance of Chameleon across FSL and CL on-device learning, as well as KWS inference scenarios. FSL performance Chameleon is the only end-to-end FSL chip that supports sequential data, while offering fully flexible support for embedding dimensions, number of ways, and shots. Furthermore, across all FSL evaluation scenarios, Chameleon sets a new record, despite using an embedding DNN that is 7.6 126 smaller than other FSL chips. Comparing with CIM accelerators for FSL distance computation, Kim et al. [7] implement MANNs (Section II-B) with an off-chip FP32 embedder of 7.46 MB, resulting in 3.4 and 0.5 lower accuracy than Chameleon for the 5-way 1- and 5-shot cases, respectively. SAPIENS [8] employs an RRAM-based backend to perform FSL, also using MANNs. On 32-way 1-shot Omniglot, SAPIENS achieves 72 accuracy using an FP32- based 4-layer CNN of 447 kB, which is 11.3 lower than Chameleon using an 8 larger network. To the best of our knowledge, the only other FSL accelerator with an on-chip embedder reporting metrics on Omniglot is FSL-HDnn [9], which uses HDC for FSL. It employs a Bfloat-16-based 5.5 MB ResNet-18 model for generating embeddings, achiev- ing 79 accuracy for 5-way 1-shot and 79.5 for 20-way 5-shot tasks on Omniglot. These accuracies are significantly lower than Chameleon, which uses a network 100 smaller that fits fully on-chip. Indeed, even though FSL-HDnn has a core area 12 larger than Chameleon, only 0.07 of its DNN weights can be stored on-chip at any time. Furthermore, while FSL-HDnn uses distance-based classification, similar to Chameleon, its HDC encoding adds 120 power and 26 area overhead. Chameleon fully sidesteps this costly HDC encoding by computing distances directly on the embeddings. Overall, Chameleon uses 2.5 lower power than FSL-HDnn at the same clock frequency, while reducing the end-to-end latency by 90 , yielding an energy per shot 210 lower. CL performance Chameleon is the first design to offer scalable end-to-end CL fully on-chip, establishing the first baseline for 250-way CL. Its unified learning and inference ar- chitecture, which enables a minimum CL power consumption of 12.9 µW, allows limiting the memory overhead of CL to scale with only 26 bytes per way on Omniglot (Equation (8)). This is a negligible 0.04 of the total DNN footprint. Hence, by reusing its DNN weight memory to store FC parameters for new classes, Chameleon flexibly repurposes on-chip memory to scale beyond prior class limits: even with 90 of memory allocated to the deployed TCN, it can still accommodate 250 learned Omniglot classes. In contrast, FSL CIMs like Kim et al. [7] and SAPIENS [8] are limited up to 25 and 32 classes, respectively, due to their fixed array sizes. FSL- HDnn [9] also uses a fixed separate memory to store its high- dimensional encodings, which is 80 larger than all on-chip memory in Chameleon. Yet, only up to 20-way learning is demonstrated, scaling with 8.2 kB way on Omniglot. Real-time KWS performance Chameleon matches SotA 93.3 accuracy on GSC using MFCCs, with the lowest reported real-time end-to-end power (3.1 µW), the small- est model size (8.5 kB) and the highest peak throughput (76.8 GOPS) among end-to-end KWS inference accelerators. Compared to Vocell [10], Chameleon offers 2.5 higher accuracy with 2 lower power and 600 higher peak GOPS. Versus TinyVers [12], which uses a TCN on a RISC-V SoC with a reconfigurable accelerator, Chameleon matches accuracy with a 3 smaller model, and 64 lower power. Tan et al. [52] avoid MFCC pre-processing to infer on raw audio instead: using a fully analog design, they improve raw- audio performance by 5.4 accuracy points at 1.8 lower power, with an estimated 25 higher peak efficiency compared to Chameleon. However, it does not perform end-to-end on- chip inference as several key steps of inference computation (e.g., ReLU activation, BN, stride adjustments, max pooling, activation clipping) are carried out off-chip. 12 TABLE II. Comparison with both KWS accelerators that have reported results on GSC in silicon and with FSL accelerators that have reported results on Omniglot in silicon. The shows 95 confidence intervals over 100 tasks. KWS accelerators CIM accelerators for FSL End-to-end FSL accelerators Vocell [10] JSSC 18 TinyVers [12] JSSC 23 Tan et al. [52] JSSC 25 Kim et al. [7] TCAS-II 22 SAPIENS [8] VLSI 21 FSL-HDnn [9] ESSERC 24 This work Technology Implementation Core area (mm2) On-chip memory Supply voltage Max. clock frequency 65-nm Digital 2.56 67 kB 0.6-1.2 V 8 MHz 22-nm Digital 6.25 132 kB 0.4-0.9 V 150 MHz 28-nm Analog 0.121 16 kB 0.35 0.9 V 1 MHz 40-nm LP Mixed signal 0.2 0.8 kB 0.5-1.1 V 80 MHz 40-nm Analog 0.0367 8 kB 0.85-1.1 V 200 MHz 40-nm Digital 11.3 349 kB 0.9-1.2 V 250 MHz 40-nm LP Digital 0.74 71 kB 0.6-1.1 V 150 MHz Network type Max. of on-chip weights Max. demonstrated input length Weight precision Activation precision LSTM FC 32k 62 4-bit LUT 8-bit TCN 400k 60 8-bit 8-bit FS-CNN 32.8k 8,000 4-bit 3-bit unsigned Distance computation only, off-chip FP32 embedder CNN 2.1k 28 Bfloat-16 16-bit5 TCN FCs 133k 16,000 4-bit log2 4-bit unsigned Inference support End-to-end inference Full on-chip weight storage FSL support End-to-end FSL CL support (off-chip embedder) FSL method distance metric FSL embedding dimension of FSL classes of FSL shots Do not support FSL MANNs, L1 64 1-25 1-5 MANNs, L1 32 32 1 HDC, Hamming 16-1024 2-128 1, 55 PNs, L22 1-1024 1-256 1-128 Omniglot FSL CL Model size End-to-end power End-to-end latency Do not support FSL 7.46 MB N A N A 447 kB N A N A ( 100 MHz) 5.5 MB5 27 mW 53 ms ( 100 kHz) ( 100 MHz) 59 kB 12.9 µW 11.6 mW 0.54s 0.59 ms Omniglot FSL accuracy 5-way (1, 5 shots) 20-way (1, 5-shots) 32-way, 1-shot Do not support FSL 93.4 , 98.3 - - - - 72 79.0 , - - , 79.5 - 96.8 1.6 , 98.8 0.5 89.1 1.3 , 96.1 0.5 83.3 1.2 Omniglot 250-way CL acc. 1-shot (final, avg.) 2-shot (final, avg.) 5-shot (final, avg.) 10-shot (final, avg.) Do not support continual learning 57.6 1.0 , 70.3 69.2 0.6 , 80.2 79.3 0.7 , 87.1 82.2 0.4 , 89.0 GSC 12-class KWS Pre-processing Accuracy dataset version Latency Real-time power Model size ( 250kHz) MFCC 90.87 (v1) 16 ms 10.6 µW1 16 kB ( 5 MHz) MFCC 93.3 (v1) 11 ms 193 µW 23 kB ( 1 MHz) None 91.8 (v2) 2 ms 1.73 µW 11 kB Do not support inference on temporal data ( 23.3 kHz) MFCC 93.3 (v2) 16 ms 3.1 µW 8.5 kB ( 532 kHz) None 86.4 (v2) 63 µs 59.4 µW 60 kB Peak GOPS Peak TOPS W 0.13 0.455 17.6 17 N C N C N A3 N A4 154 5.7 76.8 6.0 1 Excludes on-chip MFCC computation power. 2 Interpolated metrics. 3 Reported metrics are for distance computation only and amount to 4.855 GOPS and 27.7 TOPS W. 4 Reported metrics are for distance computation only and amount to 0.4 GOPS5 and 0.118 TOPS W. 5 Estimated metrics. V. CONCLUSION In this work, we presented Chameleon, the first scalable SoC to enable end-to-end on-chip FSL and few-shot CL on sequential data, without compromising inference accuracy nor efficiency. We achieve this through three key contributions. First, by integrating FSL and CL into the inference process, we propose a unified learning and inference architecture built on PNs [16]. The learning integration incurs only 0.04 latency and 0.5 core area overhead. Second, our greedy dilation-aware TCN execution enables the first 16 kHz raw audio KWS on GSCv2, achieving a 160 larger receptive field than prior end-to-end KWS accelerators using only 2 kB activation memory 4 less than SotA TCN accelerators, while avoiding a 60-400 area increase for audio-specific pre-processing blocks. Third, by using log2 quantization, we enable a MatMul-free PE array. Combined with its dual-mode operation through system-level power gating, this yields a peak throughput in 16 16-mode that is 4.3 higher than previous KWS accelerators or 2 lower real-time power in 4 4-mode. Chameleon establishes new accuracy records for end-to-end on-chip FSL on the Omniglot dataset (96.8 1.6 5-way 1- shot, 96.1 0.5 20-way 5-shot) at 2.5 lower power than previous on-chip embedder FSL work [9]. It is the only end- to-end FSL accelerator with all weights on-chip and enables FSL starting at 12.9 µW. Considering CL, although several designs exist, none offer a fully end-to-end SoC solution with comparable accuracy, footprint, and scalability. Hence, Chameleon establishes the first baseline for end-to-end on-chip CL on Omniglot, demonstrating 250-way few-shot CL (82.2 final accuracy for 10 shots). Beyond this learning performance, Chameleon maintains a SotA accuracy of 93.3 for 12-class GSCv2 compared to inference-only KWS accelerators. By reframing learning so that it can readily be supported by existing inference accelerators, Chameleon paves the way for efficient lifelong learning on extreme-edge devices. ACKNOWLEDGEMENT The authors thank Prof. Kofi Makinwa, Nicolas Chauvaux, Dr. Martin Lefebvre, and Dr. Adrian Kneip for their feedback, Dr. Filipe Cardoso for dicing and Nuriel Rozsa for die photos. This publication was partly financed by the Dutch Research Council (NWO) as part of the project AdaptEdge with file number 20267 in the NWO Talent Programme Veni. 13 REFERENCES [1] N. S. Murthy, P. Vrancx, N. Laubeuf, P. Debacker, F. Catthoor, and M. Verhelst, Learn to learn on chip: Hardware-aware meta-learning for quantized few-shot learning at the edge, in 2022 IEEE ACM 7th Symp. on Edge Computing (SEC), 2022, pp. 14 25. [2] L. Ravaglia, M. Rusci, D. Nadalini, A. Capotondi, F. Conti, and L. Benini, A tinyml platform for on-device continual learning with quantized latent replays, IEEE J. on Emerging and Selected Topics in Circuits and Systems, vol. 11, no. 4, pp. 789 802, 2021. [3] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man e, Concrete problems in ai safety , 2016. arXiv:1606.06565. [4] L. Pellegrini, G. Graffieti, V. Lomonaco, and D. Maltoni, Latent replay for real-time continual learning, in 2020 IEEE RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2020, pp. 10 203 10 209. [5] M. Li and X. S. Hu, A quantization framework for neural network adaption at the edge, in 2021 Des., Automat. Test in Europe Conf. Exhib. (DATE), 2021, pp. 402 407. [6] M. Wang, R. Xue, J. Lin, and Z. Wang, Exploring quantization in few-shot learning, in 2020 18th IEEE Int. New Circuits and Systems Conf. (NEWCAS), 2020, pp. 279 282. [7] S. Kim, W. Lee, S. Kim, S. Park, and D. Jeon, An in-memory computing sram macro for memory-augmented neural network, TCAS- II, vol. 69, no. 3, 2022. [8] H. Li, W.-C. Chen, A. Levy, C.-H. Wang, H. Wang, P.-H. Chen, W. Wan, H.-S. P. Wong, and P. Raina, One-shot learning with memory- augmented neural networks using a 64-kbit, 118 gops w rram-based non-volatile associative memory, in 2021 Symp. on VLSI Technology, 2021, pp. 1 2. [9] H. Yang, C. E. Song, W. Xu, B. Khaleghi, U. Mallappa, M. Shah, K. Fan, M. Kang, and T. Rosing, Fsl-hdnn: A 5.7 tops w end-to- end few-shot learning classifier accelerator with feature extraction and hyperdimensional computing, in 2024 IEEE European Solid-State Electronics Research Conf. (ESSERC), 2024, pp. 33 36. [10] J. S. P. Giraldo, S. Lauwereins, K. Badami, and M. Verhelst, Vocell: A 65-nm speech-triggered wake-up soc for 10- µ w keyword spotting and speaker verification, IEEE J. of Solid-State Circuits, vol. 55, no. 4, pp. 868 878, 2020. [11] J. S. P. Giraldo, V. Jain, and M. Verhelst, Efficient execution of temporal convolutional networks for embedded keyword spotting, IEEE Trans. on Very Large Scale Integration (VLSI) Systems, vol. 29, no. 12, pp. 2220 2228, 2021. [12] V. Jain, S. Giraldo, J. D. Roose, L. Mei, B. Boons, and M. Verhelst, Tinyvers: A tiny versatile system-on-chip with state-retentive emram for ml inference at the extreme edge, JSSC, vol. 58, no. 8, 2023. [13] P. P. Bernardo, C. Gerum, A. Frischknecht, K. L ubeck, and O. Bringmann, Ultratrail: A configurable ultralow-power tc-resnet ai accelerator for efficient keyword spotting, IEEE Trans. on Computer- Aided Design of Integrated Circuits and Systems, vol. 39, no. 11, pp. 4240 4251, 2020. [14] K. Kim, C. Gao, R. Grac a, I. Kiselev, H.-J. Yoo, T. Delbruck, and S.-C. Liu, A 23-uw keyword spotting ic with ring-oscillator-based time-domain feature extraction, IEEE J. of Solid-State Circuits, vol. 57, no. 11, pp. 3298 3311, 2022. [15] G. Karunaratne, M. Hersche, J. Langeneager, G. Cherubini, M. L. Gallo, U. Egger, K. Brew, S. Choi, I. Ok, C. Silvestre, N. Li, N. Saulnier, V. Chan, I. Ahsan, V. Narayanan, L. Benini, A. Sebastian, and A. Rahimi, In-memory realization of in-situ few-shot continual learning with a dynamically evolving explicit memory, in ESSCIRC 2022- IEEE 48th European Solid State Circuits Conf. (ESSCIRC), 2022, pp. 105 108. [16] J. Snell, K. Swersky, and R. Zemel, Prototypical networks for few- shot learning, in Advances in Neural Information Processing Systems, vol. 30, 2017. [17] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, Human-level concept learning through probabilistic program induction, Science, vol. 350, no. 6266, 2015. [18] S. Bai, J. Z. Kolter, and V. Koltun, An empirical evaluation of generic convolutional and recurrent networks for sequence modeling , 2018. arXiv:1803.01271. [19] M. Scherer, A. D. Mauro, T. Fischer, G. Rutishauser, and L. Benini, Tcn-cutie: A 1,036-top s w, 2.72-µj inference, 12.2-mw all-digital ternary accelerator in 22-nm fdx technology, IEEE Micro, vol. 43, no. 1, pp. 42 48, 2023. [20] C. Finn, P. Abbeel, and S. Levine, Model-agnostic meta-learning for fast adaptation of deep networks, in Int. Conf. on machine learning, PMLR, 2017, pp. 1126 1135. [21] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, Meta- learning in neural networks: A survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 5149 5169, 2022. [22] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization , 2014. arXiv:1412.6980. [23] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel, A Simple Neural Attentive Meta-Learner , 2018. arXiv:1707.03141. [24] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap, Meta-learning with memory-augmented neural networks, in Proc. of The 33rd Int. Conf. on Machine Learning, ser. Proc. of Machine Learning Research, vol. 48, New York, New York, USA: PMLR, Jun. 2016, pp. 1842 1850. [25] J. Morris, R. Fernando, Y. Hao, M. Imani, B. Aksanli, and T. Rosing, Locality-based encoder and model quantization for efficient hyper- dimensional computing, IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems, vol. 41, no. 4, pp. 897 907, 2022. [26] D. Huo, J. Zhang, X. Dai, J. Zhang, C. Qian, K.-T. Tang, and H. Chen, Anp-g: A 28-nm 1.04-pj sop sub-mm2 asynchronous hybrid neural network olfactory processor enabling few-shot class-incremental on- chip learning, IEEE J. of Solid-State Circuits, pp. 1 11, 2025. [27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [28] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural computation, vol. 9, no. 8, pp. 1735 1780, 1997. [29] Z. Wang, Y. Ma, Z. Liu, and J. Tang, R-transformer: Recurrent neural network enhanced transformer , 2016. arXiv:1907.05572. [30] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition , 2015. arXiv:1512.03385. [31] D. Miyashita, E. H. Lee, and B. Murmann, Convolutional neural net- works using logarithmic data representation , 2016. arXiv:1603.01025. [32] D. Przewlocka-Rus, S. S. Sarwar, H. E. Sumbul, Y. Li, and B. De Salvo, Power-of-two quantization for low bitwidth and hardware compliant neural networks , 2022. arXiv:2203.05025. [33] S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift , 2015. arXiv:1502.03167. [34] P. Goyal, Accurate, large minibatch sgd: Training imagenet in 1 hour , 2017. arXiv:1706.02677. [35] K. He, X. Zhang, S. Ren, and J. Sun, Delving deep into rectifiers: Sur- passing human-level performance on imagenet classification, in 2015 IEEE Int. Conf. on Computer Vision (ICCV), 2015, pp. 1026 1034. [36] A. Pappalardo. Xilinx brevitas. (2023). [37] R. Krishnamoorthi, Quantizing deep convolutional networks for effi- cient inference: A whitepaper , 2018. arXiv:1806.08342. [38] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, Quantization and training of neural networks for efficient integer-arithmetic-only inference, in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2018. [39] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen, and T. Blankevoort, A white paper on neural network quan- tization , 2021. arXiv:2106.08295. [40] O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu koray, and D. Wierstra, Matching networks for one shot learning, in Advances in Neural Information Processing Systems, vol. 29, 2016. [41] K. Prabhu, A. Gural, Z. F. Khan, R. M. Radway, M. Giordano, K. Koul, R. Doshi, J. W. Kustin, T. Liu, G. B. Lopes, V. Turbiner, W.-S. Khwa, Y.-D. Chih, M.-F. Chang, G. Lallement, B. Murmann, S. Mitra, and P. Raina, Chimera: A 0.92-tops, 2.2-tops w edge ai accelerator with 2-mbyte on-chip foundry resistive ram for efficient training and inference, IEEE J. of Solid-State Circuits, vol. 57, no. 4, pp. 1013 1026, 2022. [42] J. Park, S. Lee, and D. Jeon, 9.3 a 40nm 4.81tflops w 8b floating- point training processor for non-sparse neural networks using shared exponent bias and 24-way fused multiply-add tree, in 2021 IEEE Int. Solid-State Circuits Conf. (ISSCC), vol. 64, 2021, pp. 1 3. [43] C. Cioflan, L. Cavigelli, and L. Benini, Boosting keyword spot- ting through on-device learnable user speech characteristics , 2024. arXiv:2403.07802. [44] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y. Gong, Few- shot class-incremental learning, in Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2020. [45] Y. E. Wibowo, C. Cioflan, T. M. Ingolfsson, M. Hersche, L. Zhao, A. Rahimi, and L. Benini, 12 mj per class on-device online few-shot class-incremental learning, in 2024 Des., Automat. Test in Europe Conf. Exhib. (DATE), 2024, pp. 1 6. 14 [46] P. Warden, Speech commands: A dataset for limited-vocabulary speech recognition , 2018. arXiv:1804.03209. [47] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and S. Laurenzo, Streaming keyword spotting on mobile devices , 2020. arXiv:2005.06720. [48] Y. Zhang, N. Suda, L. Lai, and V. Chandra, Hello edge: Keyword spotting on microcontrollers , 2017. arXiv:1711.07128. [49] S. Davis and P. Mermelstein, Comparison of parametric represen- tations for monosyllabic word recognition in continuously spoken sentences, IEEE Trans. on acoustics, speech, and signal processing, vol. 28, no. 4, pp. 357 366, 1980. [50] W. Shan, M. Yang, T. Wang, Y. Lu, H. Cai, L. Zhu, J. Xu, C. Wu, L. Shi, and J. Yang, A 510-nw wake-up keyword-spotting chip using serial-fft-based mfcc and binarized depthwise separable cnn in 28-nm cmos, IEEE J. of Solid-State Circuits, vol. 56, no. 1, pp. 151 164, 2021. [51] B. Liu, H. Cai, Z. Wang, Y. Sun, Z. Shen, W. Zhu, Y. Li, Y. Gong, W. Ge, J. Yang, and L. Shi, A 22nm, 10.8 uw 15.1 uw dual computing modes high power-performance-area efficiency domained background noise aware keyword- spotting processor, IEEE Trans. on Circuits and Systems I: Regular Papers, vol. 67, no. 12, pp. 4733 4746, 2020. [52] F. Tan, W.-H. Yu, J. Lin, K.-F. Un, R. P. Martins, and P.-I. Mak, A 1.8 far, 2 ms decision latency, 1.73 nj decision keywords- spotting (kws) chip incorporating transfer-computing speaker verifica- tion, hybrid-if-domain computing and scalable 5t-sram, IEEE J. of Solid-State Circuits, vol. 60, no. 3, pp. 1103 1112, 2025. Douwe den Blanken (Graduate Student Member, IEEE) received the M.Sc. degree (with honors) in embedded systems from the Delft University of Technology (TU Delft), Delft, The Netherlands, in 2023, where he is currently pursuing the Ph.D. de- gree, under the supervision of Prof. C. Frenkel. His current research interests include efficient learning algorithms and their implementation in silicon, as well as the quantization and acceleration of modern DNNs. Charlotte Frenkel (Member, IEEE) received the M.Sc. degree (summa cum laude) in Electromechan- ical Engineering and the Ph.D. degree in Engineer- ing Science from Universit e catholique de Louvain (UCLouvain), Louvain-la-Neuve, Belgium in 2015 and 2020, respectively. In February 2020, she joined the Institute of Neuroinformatics, UZH and ETH Zurich, Switzerland, as a postdoctoral researcher. She is an Assistant Professor at Delft University of Technology, Delft, The Netherlands, since July 2022, and holds a Visiting Faculty Researcher position with Google since October 2024. Her research aims at bridging the bottom-up (bio-inspired) and top-down (engineering-driven) design approaches toward neuromorphic intelligence, with a focus on hardware-algorithm co-design for (Neuro)AI, digital hardware accelerators, and brain-inspired on-device learning. Dr. Frenkel received a best paper award at the IEEE International Sym- posium on Circuits and Systems (ISCAS) 2020 conference in the Neural Networks track, and her Ph.D. thesis was awarded the FNRS-FWO Nokia Bell Scientific Award 2021 and the FNRS-FWO IBM Innovation Award 2021. In 2023, she was awarded prestigious Veni and AiNed Fellowship grants from the Dutch Research Council (NWO). She presented several invited talks, including keynotes at the tinyML EMEA technical forum 2021 and at the Neuro-Inspired Computational Elements (NICE) neuromorphic conference 2021. She serves or has served as a program co-chair of NICE 2023-2024 and of the tinyML Research Symposium 2024, as a co-lead of the NeuroBench initiative for benchmarks in neuromorphic computing since 2022, as a TPC member of IEEE ESSERC for 2022-2024, and as an associate editor for the IEEE Transactions on Biomedical Circuits and Systems since 2022.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2505.24852v2 [cs.AR] 1 Jul 2025 1 Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data Douwe den Blanken, Graduate Student Member, IEEE, and Charlotte Frenkel, Member, IEEE Abstract On-device learning at the edge enables low-latency, private personalization with improved long-term robustness and reduced maintenance costs. Yet, achieving scalable, low-power end-to-end on-chip learning, especially from real-world sequen- tial data with a limited number of examples, is an open challenge. Indeed, accelerators supporting error backpropagation optimize for learning performance at the expense of inference efficiency, while simplified learning algorithms often fail to reach acceptable accuracy targets. In this work, we present Chameleon, leveraging three key contributions to solve these challenges. (i) A unified learning and inference architecture supports few-shot learning (FSL), continual learning (CL) and inference at only 0.5 area overhead to the inference logic. (ii) Long temporal dependencies are efficiently captured with temporal convolutional networks (TCNs), enabling the first demonstration of end-to-end on- chip FSL and CL on sequential data and inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free compute array allows either matching the power consumption of state-of- the-art inference-only keyword spotting (KWS) accelerators or enabling 4.3 higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records on Omniglot for end-to- end on-chip FSL (96.8 , 5-way 1-shot, 98.8 , 5-way 5-shot) and CL (82.2 final accuracy for learning 250 classes with 10 shots), while maintaining an inference accuracy of 93.3 on the 12-class Google Speech Commands dataset at an extreme-edge power budget of 3.1 µW. Index Terms Continual learning (CL), digital accelerator, few-shot learning (FSL), keyword spotting (KWS), matrix multiplication-free (MatMul-Free), sequential data, system-on- chip (SoC), temporal convolutional network (TCN). I.\n\n--- Segment 2 ---\nIndex Terms Continual learning (CL), digital accelerator, few-shot learning (FSL), keyword spotting (KWS), matrix multiplication-free (MatMul-Free), sequential data, system-on- chip (SoC), temporal convolutional network (TCN). I. INTRODUCTION T HE rise of edge computing has driven demand for deploying deep learning models on resource-constrained devices [1], forming the backbone of the Internet-of-Things (IoT) ecosystem [2]. However, most edge artificial intelligence (AI) devices focus on inference [2], relying on pre-trained models that cannot be adapted post-deployment. This lack of adaptability limits robustness to emerging features and data distribution shifts [2], making smart sensors unreliable over time [3]. Fig. 1(a) shows that while cloud-hosted retraining offers a workaround where new labeled data is uploaded from an edge device to a central server for model updates, this approach is cloud-connectivity-dependent and does not fit latency-constrained scenarios, while leading to privacy concerns [4]. On the other hand, training a model from scratch on-device incurs high energy penalties and still requires the availability of a large labeled dataset, which is not realistic at This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\n\n--- Segment 3 ---\nOn the other hand, training a model from scratch on-device incurs high energy penalties and still requires the availability of a large labeled dataset, which is not realistic at This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Douwe den Blanken and Charlotte Frenkel are with the Microelectronics Department (EEMCS Faculty), Delft University of Technology, 2628 CD Delft, Netherlands (e-mail: Inference accelerators [10, 12] Specialized digital learning accelerators [9] PE array Inference controller Complex learning logic PE array Inference controller FSL- only macro FSL CIM macros [7, 8] This work Learning integration Inference controller PE array Cloud Edge device Private features Cloud-hosted retraining Edge device On-chip FSL CL Edge device On-chip training Pre-trained net Updated network Updated network Cloud connectivity required, latency-constrained, flexible Energy-intensive, sample-hungry, offline Offline, low-latency energy, continuous, sample-efficient Updated network Large dataset Untrained net Few labeled samples Few labeled samples Unable to learn High overhead for learning No end-to- end learning (c) Deployment view (a) Solution view 5 new classes 2 samples per class On-chip training data カ カ メ メ レ レ オ オ ン ン カ Time Learned characters メ レ オ カ メ カ メ レ カ オ レ メ ン カ Continual learning Few-shot learning DNN Inference DNN DNN カ (b) Architecture view Efficient, versatile end-to-end learning Fig. 1. (a) Overview of three approaches to endow edge AI devices with the ability to adapt, where on-device few-shot learning (FSL) and contin- ual learning (CL) enable offline, low-latency, and sample-efficient learning. (b) Comparison of an inference and three learning architectures, including ours; we minimize the footprint of learning by integrating it into the inference pipeline rather than treating it separately. (c) Our architecture supports end- to-end on-chip inference, FSL, and CL at a low overhead. the edge. Therefore, enabling efficient on-device learning at the edge requires the ability to continually learn from limited data.\n\n--- Segment 4 ---\nthe edge. Therefore, enabling efficient on-device learning at the edge requires the ability to continually learn from limited data. However, over a device s lifetime, far fewer tasks are learned than inferences performed. Hence, it is key for an edge 2 AI device to support learning without degrading inference performance and efficiency (Fig. 1(b)). Yet, current efforts to extend inference accelerators with few-shot learning (FSL) capabilities incur significant area, energy and accuracy penal- ties. For example, using quantized gradient descent (GD) to implement on-chip FSL [1], [5], [6], requires maintaining all intermediate activations in memory, transposing weight matri- ces, and using costly data types such as floating-point (FP) rep- resentations [1]. Alternatively, gradient-free FSL approaches using compute-in-memory (CIM) macros [7], [8] have been proposed to accelerate distance computation between input and stored embedding vectors for FSL. However, they fail to provide end-to-end learning support as they rely on off-chip FP32 deep neural networks (DNNs) to compute high-quality embeddings. To the best of our knowledge, only one prior work computes the DNN embeddings on chip for gradient- free FSL [9]. This design adds dedicated learning hardware alongside inference modules, yielding a 2 increase in peak power, a 25- area overhead, and requires external memory to store all model parameters for learning, undermining its suitability for extreme edge scenarios. Another challenge is that state-of-the-art (SotA) on-chip FSL architectures lack the ability to handle temporal informa- tion in two key ways. First, they cannot process sequential data. All existing FSL accelerators target image classifica- tion tasks [1], [5] [9], whereas real-world applications often involve sequential data, such as keyword spotting (KWS) where audio is typically sampled at frequencies ranging from 16 kHz to 48 kHz.\n\n--- Segment 5 ---\nFirst, they cannot process sequential data. All existing FSL accelerators target image classifica- tion tasks [1], [5] [9], whereas real-world applications often involve sequential data, such as keyword spotting (KWS) where audio is typically sampled at frequencies ranging from 16 kHz to 48 kHz. However, due to the limited input context length of current end-to-end KWS accelerators (60 [10] [12] to 101 [13]), feature extraction (FE) such as mel-frequency cepstral coefficients (MFCC) is typically performed for two- order-of-magnitude reductions in input sequence length, at the expense of a 50 75 area overhead [10], [14]. Hence, capturing temporal dependencies that span multiple orders of magnitude is not only key for complex temporal tasks, it also allows streamlining system design. Second, current on-chip FSL designs cannot perform sequential learning of tasks with data distributions that shift over time. This continual learning (CL) scenario is typical of edge applications, where users add features, such as new keywords, over time. While high-quality embeddings produced by off-chip embedders have recently been shown to also support CL over up to 100 classes [15], end-to-end on-chip CL is still an open challenge. Overall, due to limited on-chip memory and strict power, area, and energy budgets, no extreme-edge solution has been proposed that effectively balances efficiency and versatility for end-to-end learning on sequential data. In this work, we present Chameleon, the first work to demonstrate scalable on-chip FSL and CL on sequential data, in a fully end-to- end fashion, and without degrading the inference performance nor efficiency (Fig. 1(c)). We achieve this through three key contributions: 1) To minimize the footprint of on-device learning, we build on prototypical networks (PNs) [16] to reframe FSL using distance computation as a forward pass through an equivalent fully connected (FC) layer, thereby leading to a unified learning and inference architecture.\n\n--- Segment 6 ---\n1(c)). We achieve this through three key contributions: 1) To minimize the footprint of on-device learning, we build on prototypical networks (PNs) [16] to reframe FSL using distance computation as a forward pass through an equivalent fully connected (FC) layer, thereby leading to a unified learning and inference architecture. By integrating learning within the inference process, we enable gradient-free, end-to-end FSL and CL at only 0.5 area overhead and 0.04 added latency while setting new FSL accuracy records (96.8 , 5-way 1-shot) and CL accuracy (82.2 final, 89.0 average for learning 250 classes with 10 shots) on the Omniglot [17] dataset. 2) To enable accurate FSL and CL with temporal informa- tion, a DNN that captures long-range relationships to produce high-quality embeddings is crucial. Hence, we propose to use temporal convolutional networks (TCNs), whose parameter efficiency scales better than recurrent neural networks [18], as efficient on-chip PN embedders for sequential data at the edge. Compared to SotA TCN accelerators [11], [13], [19], we expand the receptive field by 160 while cutting activation memory by 4 . 3) To enable inference at an extreme-edge power budget while facilitating a high throughput for learning, we introduce a dual-mode matrix-multiplication (MatMul)- free processing element (PE) array. Combined with system-level power gating, we achieve 3.1 µW real-time KWS or 4.3 higher peak GOPS than SotA KWS accelerators. The remainder of this article is structured as follows: Section II covers DNN training for FSL and silicon deployments for various FSL methods, while the proposed architecture of the Chameleon system-on-chip (SoC) is introduced in Section III. Section IV then presents the measurement results for FSL, CL and inference tasks, with concluding remarks in Section V. To promote reproducibility, reuse, and improvement, all code for this paper is open-source, including the accelerator source code, the training framework, and the test simulation setup.1 II.\n\n--- Segment 7 ---\nThe remainder of this article is structured as follows: Section II covers DNN training for FSL and silicon deployments for various FSL methods, while the proposed architecture of the Chameleon system-on-chip (SoC) is introduced in Section III. Section IV then presents the measurement results for FSL, CL and inference tasks, with concluding remarks in Section V. To promote reproducibility, reuse, and improvement, all code for this paper is open-source, including the accelerator source code, the training framework, and the test simulation setup.1 II. BACKGROUND To give more insight into FSL algorithms, Section II-A presents the meta-learning framework used to train DNNs for FSL [20], also known as learning to learn. Section II-B then examines various meta-learning methods and their suitability for on-chip deployment. A. Meta-Learning Framework In supervised learning, given a standard labeled dataset D {(x1, y1) , ..., (xL, yL)} with L examples xi and labels yi (Fig. 2(a)), the training Dtrain and testing Dtest splits share classes but not examples (Fig. 2(b). However, in meta- learning, entire classes are assigned to training or testing splits (Fig. 2(c)), ensuring each split has a distinct set of classes. For meta-training, a set of M tasks [21], referred to as Dmeta,train, is used to train the DNN for FSL, defined as Dmeta,train n Dsupport train , Dquery train (i)oM i 1 , Dmeta,train D. (1) The support set Dsupport train provides labeled data for the model to learn the task, while the query set Dquery train measures per- formance on the task. Both sets consist of N ways (unique classes), with k shots (examples) per class. 1 3 k 2 shots 1 shot N 3 ways Meta-testing: Dmeta,test Dsupport Dquery Full dataset: D Meta-training: Dmeta,train Dsupport Dquery k 2 shots 1 shot N 3 ways M Q Supervised training: Dtrain Supervised testing: Dtest Batch size: 12 Batch size: 6 B C train train test test (a) Labeled dataset with L samples (b) Supervised learning data splits (c) Meta-learning data splits Fig. 2.\n\n--- Segment 8 ---\n1 3 k 2 shots 1 shot N 3 ways Meta-testing: Dmeta,test Dsupport Dquery Full dataset: D Meta-training: Dmeta,train Dsupport Dquery k 2 shots 1 shot N 3 ways M Q Supervised training: Dtrain Supervised testing: Dtest Batch size: 12 Batch size: 6 B C train train test test (a) Labeled dataset with L samples (b) Supervised learning data splits (c) Meta-learning data splits Fig. 2. (a) Standard dataset comprising L labeled examples across multiple classes. (b) Supervised training and testing splits, illustrated for batch sizes of 12 and 6, respectively. (c) Meta-training and meta-testing splits for an example 3-way 2-shot (Dsupport) task with 1-shot query data (Dquery). Notice that the classes do not overlap between Dmeta,train and Dmeta,test. After meta-training, the learned meta-knowledge is used to learn unseen tasks from a set of Q tasks [21] provided by the target dataset Dmeta,test during meta-testing: Dmeta,test n Dsupport test , Dquery test (i)oQ i 1 , Dmeta,test D. (2) Like before, the support set Dsupport test provides data to learn the new task, while Dquery test is used to measure test performance on the task, with both sets potentially differing in ways and shots from meta-training. B. Evaluating Meta-Learning for On-Chip Deployment While a plethora of meta-learning methods exist that ex- ploit this task-based training regime, they can be categorized into three groups: parameter initialization, feed-forward, and metric-learning methods [21], which we introduce with a focus on their suitability for on-chip deployment. Parameter initialization These methods aim to find op- timal initial DNN parameters, enabling quick fine-tuning on unseen tasks with few examples [21]. Model-agnostic meta- learning (MAML) [20] is a widely used approach in this category [21], and employs nested GD.\n\n--- Segment 9 ---\nParameter initialization These methods aim to find op- timal initial DNN parameters, enabling quick fine-tuning on unseen tasks with few examples [21]. Model-agnostic meta- learning (MAML) [20] is a widely used approach in this category [21], and employs nested GD. During meta-training, the inner GD loop is carried out for each new task, while the outer loop optimizes the initial parameters such that the inner loop requires only a few steps of GD to achieve high (a) Parameter initialization (c) Metric-learning (b) Feed-forward Aux. DNN DNN ζ φ φ Init. DNN ζ φ Final DNN φ On-chip GD steps DNN agnostic Gradient storage High precision data types Weight transposition Multiple forw. backw. passes Gradient-free Parameter overhead Computation overhead Recomput. per query DNN-agnostic Gradient-free Distance computation ζ φ φ Base DNN On-chip Off-chip Frozen Trained: Fig. 3. Qualitative comparison of the suitability of the three meta-learning method types for on-chip deployment. accuracy. Using the initial parameters resulting from meta- training, the deployment of MAML requires only the inner loop to be executed on-chip (Fig. 3(a)). Key advantages are that it introduces no extra learnable parameters [20] and imposes no architectural constraints on the DNN. However, a major drawback for on-chip deployment is that GD must be executed on-chip. Although proposals of initialization- based methods for edge deployment exist [1], [5], [6], they require (i) buffering all intermediate activations required for GD operation, instead of discarding them as soon as the next- layer values are computed during inference, (ii) support for on- chip weight transposition operations, and (iii) the use of more expensive data types than quantized inference would typically require, such as (block) floating-point representations [1], [6]. Additionally, the proposed quantized FSL setup in [1] requires 300 forward and backward passes to be performed for on- chip GD, significantly increasing the compute requirements for learning. Furthermore, to maintain high accuracy, [1] resorts to the Adam optimizer [22], leading to a triplication of the required on-chip weight storage.\n\n--- Segment 10 ---\nAdditionally, the proposed quantized FSL setup in [1] requires 300 forward and backward passes to be performed for on- chip GD, significantly increasing the compute requirements for learning. Furthermore, to maintain high accuracy, [1] resorts to the Adam optimizer [22], leading to a triplication of the required on-chip weight storage. Feed-forward models These methods aim to learn a direct mapping from support examples Dsupport to parameters needed for predictions on query examples Dquery [21]. Unlike initialization-based methods, feed-forward models eliminate the need for gradient updates during deployment. However, they can introduce significant parameter overheads stemming from auxiliary DNNs or require handcrafted neural network (NN) architectures that may not align with DNNs typically supported for inference (Fig. 3(b)). A well-known example is the simple neural attentive meta-learner (SNAIL) [23], in which the model predicts, using N k support example-label pairs and a query example, the class of the query in one forward pass. SNAIL uses the same base DNN as MAML [20] but requires a second, auxiliary module that has 29 more parameters compared to the base DNN. Another method in this category is memory-augmented NNs (MANNs) [24], which use an external memory to store vector representations of previous tasks for rapid adaptation to new ones. FSL- HDnn [9] implements an on-chip MANN through hyperdi- mensional computing (HDC). In HDC, base DNN embeddings are projected into a high-dimensional space via multiplication with a random { 1, 1}-valued matrix [25] (typically 1M parameters). Classification is then performed by finding the 4 stored class encoding with the closest Hamming distance to the encoded input. In FSL-HDnn, a dedicated module performs encoding, storage, and classification, but increases on-chip power consumption by 120 relative to the power for the base DNN s forward pass. Metric learning These methods aim to learn a model that generates vector representations for each support and query sample (Fig. 3(c)), which can then be used to enable the clas- sification of unseen examples without any gradient updates, similar to feed-forward methods. A popular approach in this category is prototypical networks (PNs) [16]: a feature vector (i.e.\n\n--- Segment 11 ---\n3(c)), which can then be used to enable the clas- sification of unseen examples without any gradient updates, similar to feed-forward methods. A popular approach in this category is prototypical networks (PNs) [16]: a feature vector (i.e. embedding) is computed for each sample in the support set Dsupport, after which these embeddings are averaged class- wise to obtain a set of prototypes. Classification is performed by comparing the query sample embedding to the prototypes of all support classes using a distance function. The query is assigned to the class with the closest prototype. A key advan- tage of this method is that, like MAML, it is model-agnostic, requiring no additional network modules and parameters. However, unlike MAML, it is also gradient-free, with only a single extra step distance computation outside the standard NN inference pipeline. [7], [8] implement a CIM-based PN variant that stores support examples individually instead of averaging them. Only the embedding storage and distance computation classification circuits are implemented in silicon, with embedding generation relying on off-chip floating-point DNN inference. Both designs use L1 distance, causing a 7 accuracy drop in 32-way and 5 in 5-way 1-shot tasks for [7], [8], respectively, compared to an L2-based PN in FP32. III. THE CHAMELEON SYSTEM-ON-CHIP ARCHITECTURE This section describes the Chameleon accelerator SoC, as shown in Fig. 4, which highlights the key building blocks that implement our three contributions. The unified architecture for FSL, CL and inference is detailed in Section III-A, while Section III-B covers the mechanism for processing long input sequences using TCNs. Finally, Section III-C discusses the dual-mode MatMul-free PE array. A. Unified Learning and Inference Architecture Chameleon s architecture builds on the typical design of a DNN inference accelerator, featuring separate memory banks for activations (4 bits), weights (4 bits) and biases (14 bits). The activation and weight memories are connected to a 16 16 PE array tailored for efficient TCN execution. To endow this baseline inference accelerator with learning capabilities, we propose to leverage a reformulation of PNs (Fig. 5) as an equivalent fully-connected (FC) layer.\n\n--- Segment 12 ---\nTo endow this baseline inference accelerator with learning capabilities, we propose to leverage a reformulation of PNs (Fig. 5) as an equivalent fully-connected (FC) layer. We begin by defining each prototype as Pj sj k , sj k X l 1 el,j, (3) where el,j is the lth V -dimensional support embedding for the jth way and k indicates the number of shots. Build- ing on the insight of [16] by using the L2 distance func- tion and squaring it to factor out its square root term, the Accumulation, bias and ReLU Learning controller 16b 4-phase handshake input bus SPI Activation memory 256 64b (2 kB) Prototypical parameter extractor 8b 4-phase handshake output bus Weight memory 512 1024b (65 kB) Input buffer 32 64b (0.25 kB) Bias memory 128 224b (3.5 kB) Config. registers Argmax tree Dual-mode Mat- Mul-free PE array PE PE PE PE PE PE PE PE PE PE PE III-A III-C (16 16) Network address generator III-B Fig. 4. Architecture of the Chameleon SoC, building on a typical DNN in- ference accelerator design. The learning controller and prototypical parameter extractor endow this architecture with on-chip learning capabilities, while the network address generator enables long-context learning and inference using TCNs. The dual-mode MatMul-free PE array allows for switching between high-throughput and low-leakage operation. PN distance computation between a prototype Pj and a query embedding x can be reformulated as sj k x 2 D2 j V X i 1 sj i k xi !2 V X i 1 sj i 2 k2 x2 i 2sj ixi k ! . (4) As the term PV i 1 x2 i is a constant offset that only depends on x and does not influence the relative distances Dj between x and the prototypes Pj, this yields D2 j 1 k2 V X i 1 sj i 2 2 k V X i 1 sj ixi.\n\n--- Segment 13 ---\n. (4) As the term PV i 1 x2 i is a constant offset that only depends on x and does not influence the relative distances Dj between x and the prototypes Pj, this yields D2 j 1 k2 V X i 1 sj i 2 2 k V X i 1 sj ixi. (5) Then, after rescaling by k 2, we equivalently get D2 j bj Wj x, with bj 1 2k V X i 1 sj i 2, Wj sj. (6) Equation (6) thus corresponds to an FC layer whose output neuron j has a bias bj and a weight vector Wj, which can both be calculated from sj, the sum of the k support embeddings as per Equation (3). By reformulating PNs as an equivalent FC layer, it becomes possible to equip any inference architecture with learning ca- pabilities with minimal overhead. To support learning, only the ability to extract the equivalent FC weight and bias parameters from a prototype is required, since the resulting FC layer can then be accelerated by the existing inference datapath. Importantly, this setup is invariant to the type of off-chip meta-training performed pre-deployment: if the final model produces high-quality embeddings, it can be deployed for on-chip FSL. Moreover, unlike initialization-based and feed- forward meta-learning methods (Section II), PNs naturally extend to CL as this simply requires the storage of additional class prototypes over time. 5 ζ ζ N 2 ways DNN φ φ K 2 shots New task "Prototype" φ Input sample Embedding vectors Class-mean embedding Class 2 prototype Input embedding Fig. 5. Learning a new task using prototypes as performed in Chameleon: N ways with k shots are embedded using a fixed DNN to create N k embedding vectors: averaging these class-wise results in N prototypes. To classify an input sample, it is embedded and compared via L2 distance to the N prototypes, assigning it to the class with the minimum distance. In Chameleon, PN weight and bias extraction is handled by two dedicated modules: the learning controller and pro- totypical parameter extractor (Fig. 4, highlighted in green).\n\n--- Segment 14 ---\nIn Chameleon, PN weight and bias extraction is handled by two dedicated modules: the learning controller and pro- totypical parameter extractor (Fig. 4, highlighted in green). The learning controller tracks states such as the current way or shot within that way, and controls the FC layer dataflow during inference, while the parameter extractor is responsible for extracting weight and bias parameters for new classes during the learning phase. The learning process is carried out in hardware as per Fig. 6, which illustrates the three steps that Chameleon follows to learn a new class (way) while maximizing reuse of the inference datapath and logic. This low-latency process only requires (k 2) V 16 1 cycles2 and scales linearly with both the number of shots and the embedding size, while inducing only a 0.5 area overhead relative to the total core area. Overall, our prototypical learning strategy enables a unified architecture for learning and inference, by reusing existing inference memories and thus eliminating the need for FSL- specific storage. It extends naturally to CL by storing new prototypes over time and can be applied to existing inference accelerator designs, with learning reduced to minimal control logic for converting prototypes into weights and biases. Unlike designs focused solely on learning, our approach enables a lightweight integration of learning into the inference process. B. Long-Context Learning and Inference Using TCNs Real-world natural data is often sequential, including bi- ological signals (e.g., heart rate, blood pressure), environ- mental data (e.g., wind speed, humidity), and perceptual data (e.g., video, audio). However, the development of algorithms for FSL and CL has historically focused on benchmarks with static data, such as images [21]. To the best of our knowledge, beyond a first proof of concept in [26] for olfactory data, no chip design has demonstrated competitive performance for sequential FSL and CL on well-established benchmarks. As our learning strategy with PNs relies on learning high- quality embeddings, we propose to extend this approach to sequential data by adopting a scalable embedder that can cap- ture long-range dependencies across full sequences. Typical DNN models for learning with temporal data range from recurrent NNs (RNNs) to transformers. On the one hand, 2 The division by 16 accounts for the 16 16 PE array. Weight mem. Bias mem.\n\n--- Segment 15 ---\nWeight mem. Bias mem. Inference flow φ Repeat k times Act. mem. 1. Embedding generation 2. Prototype conversion 3. FC parameter extraction Inference Act. mem. Embedding φ Inference flow incl. FC layer Learn. ctrl. Prototypical parameter extractor Fig. 6. FSL in Chameleon is performed in three steps. 1. For all k shots of the new class, inference is performed to compute the embeddings, which are saved in the activation memory. 2. The embeddings from the activation memory are loaded and used in the PE array to compute the prototype. 3. The prototype is converted to equivalent FC bias and weights that are stored in their respective memories. It is then possible to perform inference using the new FC layer to classify an input sample. RNNs have the advantage of scaling with O(1) memory requirements in sequence length, at the expense of stability issues for long-range dependencies. They typically fail at 1k sequence lengths [18] that are necessary, e.g., for raw-audio processing. On the other hand, transformers [27] achieve SotA performance when trained on large datasets ( 106 examples), surpassing recurrent and convolutional architectures. Yet, on smaller datasets, RNNs like LSTMs [28] typically outperform transformers [29]. Furthermore, transformer encoders, required for embedding generation, also incur O(n2) memory and time complexity, making their end-to-end deployment at a µW- level power budget an open challenge. In contrast, convolu- tional architectures like TCNs [18] offer O(log2 n) memory complexity, while iso-parameter comparisons show that they outperform both RNNs and transformers on tasks with 106 examples [29]. Hence, we propose to use TCNs [18] as efficient, scalable embedders for inference and FSL on sequential data at the edge. As shown in Fig. 7(a), TCNs employ stacked causal 1D convolutions with residual connections to ease training in deep networks [30].\n\n--- Segment 16 ---\nAs shown in Fig. 7(a), TCNs employ stacked causal 1D convolutions with residual connections to ease training in deep networks [30]. They are able to capture long-range de- pendencies via dilation, which doubles at each residual block, resulting in a receptive field (R) that grows exponentially with network depth: R 1 L 2 X l 1 2l (k 1), (7) where L is the number of layers and k the kernel size. In classification scenarios, dilation also makes deeper lay- ers exponentially sparser, as illustrated by the white circles in Fig. 7(b), resulting in a streaming memory complexity of O(log2 n) when skipping these computations. However, leveraging this favorable memory scaling is nontrivial due to the computational graph structure induced by dilation and the residual connections in TCNs. We solve this by introducing two key techniques. Our first technique targets the efficient exploitation of dilation-induced sparsity in the TCN s computational graph. In TCN-CUTIE [19], 1D convolutional kernels are mapped to equivalent 2D kernels: however, this introduces 80 zero multiplications for k 2, as dilation is emulated via zero- 6 Causal Conv1D k k, dilation 2 l BN ReLU Causal Conv1D k k, dilation 2 l BN ReLU Conv1D k 1, d 1 TCN residual block (a) (b) TCN res. block k 3, l L TCN res. block k 3, l 1 FC layer FC layer ReLU Input sequence TCN layers Unused Output Residual connnection d 4 d 2 d 1 Kernel size (k): 1-15 Max. 1024 channels layer; max. 32 FC Conv1D layers Fig. 7. (a) DNN structure supported by Chameleon. Each TCN residual block contains two causal 1D convolutions, each followed by batch normalization (BN) and rectified linear unit (ReLU) activation. If input and output channels match, the Conv1D residual can be replaced with an identity. Both convolu- tional layers in a residual block share the same dilation factor d, doubling in successive blocks, starting at d 1. (b) Computational graph of a 6-layer TCN (three stacked residual blocks), where white circles indicate zero-valued activations introduced by dilation.\n\n--- Segment 17 ---\nBoth convolu- tional layers in a residual block share the same dilation factor d, doubling in successive blocks, starting at d 1. (b) Computational graph of a 6-layer TCN (three stacked residual blocks), where white circles indicate zero-valued activations introduced by dilation. padding, with sequence lengths limited to 24 timesteps for TCN processing. UltraTrail [13] adopts a native 1D convo- lutional kernel mapping to performs TCN inference instead, but lacks dilation support, thereby restricting its applicability to tasks requiring small receptive fields (demonstrated up to 101 timesteps). Furthermore, its weight-stationary dataflow necessitates full sequence pre-loading, which is incompatible with streaming inference of long sequences. Giraldo et al. [11] introduce a FIFO-based partial output stationary dataflow that supports dilation and reuses activations across timesteps. How- ever, since the model outputs one classification per input in the sequence and as each output is connected to a different set of TCN graph nodes, this strategy does not avoid computing the unused nodes shown in Fig. 7(b). Expanding on this FIFO- based scheme, we introduce a greedy dilation-aware execu- tion (Fig. 8), which leverages a specialized network address generator to skip redundant activations and enable greedy TCN processing with layer-wise FIFO activation storage. Fig. 8(a) presents the greedy computational graph of a four- layer TCN as executed in Chameleon. Each layer is triggered as soon as the number of available inputs matches its kernel size, and this condition cascades through subsequent layers. When additional inputs are required, control reverts to earlier layers. This scheme efficiently leverages FIFO-style activation memory, as illustrated in Fig. 8(b). The address generator ensures that the output of a new timestep always overwrites the oldest, unused one. The same mechanism applies to the input memory, where outdated inputs are automatically replaced. Through this greedy processing scheme, the total memory requirements for TCN inference are significantly reduced. Moreover, this dilation-aware processing also drastically re- duces the number of inference operations by skipping zero- valued activations that do not contribute to the final output. We demonstrate in Fig.\n\n--- Segment 18 ---\nMoreover, this dilation-aware processing also drastically re- duces the number of inference operations by skipping zero- valued activations that do not contribute to the final output. We demonstrate in Fig. 8(c) that Chameleon achieves 90 memory and 7 compute reduction over weight stationary (WS) non-dilation-optimized TCN inference at a sequence length of 16k (raw audio) with a fixed network size of 130k parameters (Chameleon s maximum supported size). Our second technique enables efficient handling of residual connections. As sequence lengths grow, larger dilation factors stemming from deeper networks are needed to expand the receptive field (Fig. 7), making residual connections essential for effective optimization of these deep networks [30]. While TCN-CUTIE [19] supports up to ten convolutional layers, it lacks support for residual connections, severely limiting scalability. UltraTrail [13] does support residuals but requires three separate buffers and complex memory access control logic. Giraldo et al. [11] omit residuals but still employ a ping-pong buffer setup. To eliminate the need for multi-buffer schemes to manage residual layers without sacrificing FIFO- style inference, our residual-aware register file processing (Fig. 9(a), right) uses a two-port register file that enables simultaneous reads and writes. To support asynchronous streaming input alongside ongoing processing, Chameleon stores DNN inputs in a dedicated 0.25 kB memory. By combining the two techniques described above and as shown in Fig. 9(b), Chameleon reduces activation memory by 76 , 28 , and 4 compared to prior TCN accelerators [11], [13], [19], while enabling deployment of DNNs with up to 5.5 more weights per kB of activation memory. This enables, for the first time, raw audio KWS on 16k-step inputs two orders of magnitude longer than in previous TCN accelerators.\n\n--- Segment 19 ---\n9(b), Chameleon reduces activation memory by 76 , 28 , and 4 compared to prior TCN accelerators [11], [13], [19], while enabling deployment of DNNs with up to 5.5 more weights per kB of activation memory. This enables, for the first time, raw audio KWS on 16k-step inputs two orders of magnitude longer than in previous TCN accelerators. C. High-Throughput or Low-Leakage Operation Using a Dual-Mode, MatMul-Free PE Array Since learning in Chameleon is integrated within the in- ference pipeline, we propose two architectural enhancements, so as to (i) reduce the PE array and memory footprints with inference-optimized quantization, allowing for MatMul-free operation with 4-bit signed log2 weights, and (ii) endow the PE array with the flexibility to optimally support different inference and learning scenarios. Our first technique focuses on inference-optimized quan- tization. For instance, proposals for deploying parameter initialization meta-learning methods on-chip, require on- chip GD, which entails floating-point precision requirements. Murthy et al. [1] use block floating point (8-bit shared exponent, 4-bit mantissa) for weights, activations, gradients and errors. Compared to regular integer operation, additional logic for exponent management, as well as normalization and alignment is required. Wang et al. [6] demonstrates the feasibility of using signed 16-bit uniform quantization of the weights and gradients for various initialization-based meta- learning methods, but reduces memory requirements only by 7 2 Activation address 9 0 Input address Time 0 2 6 13 9 3 16 15 12 11 5 7 2 4 1 x9 x7 x8 x5 x6 x3 x4 x1 x2 x0 18 14 10 8 0 x11 x12 x10 17 19 0 1 2 3 4 5 6 5 7 8 9 10 6 9 8 11 12 13 14 0 1 2 3 4 5 6 7 8 9 10 11 12 12 15 16 9 13 17 18 19 Input seq. Input mem.\n\n--- Segment 20 ---\n[6] demonstrates the feasibility of using signed 16-bit uniform quantization of the weights and gradients for various initialization-based meta- learning methods, but reduces memory requirements only by 7 2 Activation address 9 0 Input address Time 0 2 6 13 9 3 16 15 12 11 5 7 2 4 1 x9 x7 x8 x5 x6 x3 x4 x1 x2 x0 18 14 10 8 0 x11 x12 x10 17 19 0 1 2 3 4 5 6 5 7 8 9 10 6 9 8 11 12 13 14 0 1 2 3 4 5 6 7 8 9 10 11 12 12 15 16 9 13 17 18 19 Input seq. Input mem. Activation memory Dilation Zero-valued activations Residual connection Causal convolution Layer 2 1 0 3 Kernel index 0 1 2 0 1 2 0 1 2 0 Oldest input is overwritten FIFO Same input used in different kernel positions (a) (b) - Chameleon WS (c) 90 lower 7 lower Fig. 8. (a) Four-layer TCN with 13 inputs. Indices show greedy processing order; colors indicate per-layer memory locations. Causal convolutions and residuals are annotated, while the processing and storage of zero activations resulting from dilation (white) is skipped by Chameleon. (b) FIFO-style activation memory allocation over time in Chameleon. Each layer overwrites its oldest activation during execution. Indices show processing order; bar lengths indicate activation lifetimes. (c) Memory and compute comparison between WS TCN inference and Chameleon s strategy, using 130k parameters and producing identical outputs. (a) This work [11, 19] UltraTrail [13] 32,79 kB 32,79 kB 4 kB 2.2 kB 1.3 kB No residuals, ping-pong buffer Complex control, triple buffer 0 1 0 1 2 Conv1D Conv1D Res. TCN layer with residual path 2 kB 0 2 Low overhead, single memory 1 4 smaller memory And 2 OoM longer input At 5.5 more weights kB (b) 19 Fig. 9. (a) Comparison of residual operation steps and activation memory sizes across TCN accelerators.\n\n--- Segment 21 ---\n9. (a) Comparison of residual operation steps and activation memory sizes across TCN accelerators. [11], [19] use ping-pong buffers but lack residual layer support; [13] requires a triple buffer setup for residual layers; Chameleon uses only a single dual-port memory, reducing memory and control complexity. (b) Impact of computation strategy on activation memory size, maximum number of weights kB of activation memory and input sequence length between TCN accelerators. 2 compared to learning with FP32. FSL-HDnn [9] uses brain floating point (Bfloat)-16 to generate embeddings for HDC during FSL, also saving 2 memory compared to a vanilla FP32 implementation. A deployment proposal for MAML [5] demonstrates the use of 4-bit signed uniform weights, but still requires 8- or 16-bit gradients depending on task dif- ficulty, hence increasing memory requirements by 2 4 compared inference-only quantization. As Chameleon operates in a purely forward manner due to its gradient-free learning, it can rely on a simple quantization scheme that is optimized for inference. Hence, we propose to use log2 weights [31]: this allows replacing multipliers with bit shifters, based on which we enable fully MatMul-free inference and embedding computation (Fig. 10(a)). During inference, the PE array uses an output-stationary dataflow to efficiently support our greedy PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE PE Adder Adder Adder Adder ReLU i0 i1 in-1 in-2 16 16 4-bit weights 16 4-bit inputs 16 4-bit outputs 18-bit acc. register Bias OPE - 4-bit log2 weight Unsigned exponent PE Residual scale Output scale 16 OPE 4-bit unsigned 12-bit signed 4-bit unsigned activation (a) PE array structure (b) Shift-only PE (c) OPE structure o0 on-1 o1 on-2 Horizontally broadcast inputs Vertically accumulate PE outputs: 16-bit signed Per-layer residual and output scale Fig. 10. (a) Diagram of the MatMul-free PE array, operating under an output- stationary dataflow. (b) Individual PE, which uses a bit shift in place of a multiplication.\n\n--- Segment 22 ---\n(a) Diagram of the MatMul-free PE array, operating under an output- stationary dataflow. (b) Individual PE, which uses a bit shift in place of a multiplication. (c) Output PE (OPE) module, which performs input output rescaling as well as bias addition and ReLU activation. TCN processing scheme. Each cycle, it receives 16 4-bit unsigned uniform ReLU-based activations, either from the input memory (first layer) or activation memory (subsequent layers), along with 16 16 log2-quantized weights from the weight memory. We use 4-bit signed log2 weights, as they offer the same dynamic range as 8-bit signed integers while halving storage, reducing power and energy footprints [32], and maintaining floating-point accuracy [32]. Each individual PE (Fig. 10(b)) first left-shifts the horizontally broadcast input by the weight s exponent and then applies sign correction, producing a 12-bit signed output. These are summed vertically and accumulated in 18-bit signed uniform registers within output PEs (OPEs), detailed in Fig. 10(c). The OPE performs input rescaling (for residuals), bias addition, ReLU activation, and output rescaling before its output is written back to the activation memory. With inference being learning-free now, we also make learning fully multiplication-free by adapting Equation (6) for a log2 formulation, allowing for efficient PN parameter extraction. Since all weights are log2 (including sj i), the square in the bias term simplifies, thereby fully replacing multiplication by bit shifts: 8 Legend 4 4 Core logic 256b 112b 56b 128 rows Bias memory 5 128b 64b 64b 512 rows OPEs 16 16 high throughput eff. 64b 64b Always on Off during 4 4 Power domains Weight memory 256 rows 56b 16 16 Virtually stack adjacent memories to increase of rows 4 4 low-leakage, small nets (a) (b) PE array Act. memory 16b Fig. 11. (a) Comparison of simulated real-time KWS power and peak TOPS W estimates for different PE array sizes. (b) Data layout for activation, bias, and weight memories to support both 4 4 and 16 16 PE array usage, allocating LSBs of weight memories to the top-left 4 4 section.\n\n--- Segment 23 ---\n(a) Comparison of simulated real-time KWS power and peak TOPS W estimates for different PE array sizes. (b) Data layout for activation, bias, and weight memories to support both 4 4 and 16 16 PE array usage, allocating LSBs of weight memories to the top-left 4 4 section. In 4 4-mode, gray memories are powered off, while the remaining weight memories are virtually stacked to double the row count. 4.3 higher 2 lower v1 v1 v2 v1 16 v2 Fig. 12. Peak GOPS, real-time KWS power and accuracy comparison on 12-class Google Speech Commands (GSC, v1 and v2) between KWS accel- erators. bj 1 2k V X i 1 2(log2 sj i) 1, Wj sj. (8) The OPE is then reused to implement the division by 2k in Equation (8), via a right shift with 2 log2(k) bits. Our second technique allows efficiently supporting dif- ferent deployment scenarios, from leakage-dominated real- time KWS inference to throughput- and dynamic-power- constrained FSL operation with large DNN embedders. For in- stance, Vocell [10] has a 36 leakage contribution (excluding analog feature extraction) during real-time KWS, similar to the 30 for UltraTrail [13] and 33 for Giraldo et al. [11] with limited throughputs of 0.13, 3.8 and 0.26 GOPS, respectively. In contrast, TinyVers [12] achieves a 4-135 higher through- put of 17.6 GOPS: however, this comes at the cost of an order- of-magnitude increase in real-time KWS power consumption. Hence, to support low-leakage operation without degrading the design s throughput, we introduce a dual-mode PE array whose size is reconfigurable, which enables Chameleon to efficiently support both µW-power real-time inference and high-throughput tasks with large DNN embedders. To determine the optimal PE array sizes for the two modes, we simulate real-time KWS and peak TOPS W performance in Fig. 11(a), assuming SRAMs dominate total system power. The analysis identifies array sizes of 4 and 16 as optimal.\n\n--- Segment 24 ---\n11(a), assuming SRAMs dominate total system power. The analysis identifies array sizes of 4 and 16 as optimal. We couple operation in both modes with a memory layout that enables power gating of specific weight and bias memory sections, while clock gating unused PEs in 4 4-mode (Fig. 11(b)). Instead of storing DNN weights purely in row- or column-major order, the top-left 4 4 weights are stored row-major in the first two banks, followed by the remaining 12 4 weights. In 16 16 mode, all banks are active, enabling 16 16 4-bit reads per cycle. In 4 4 mode, MSB banks are power gated, leaving only the LSB banks, referred to as always-on, active. These are virtually stacked to enlarge the address space, supporting 16k weights over 1024 addresses and 512 biases over 128 addresses. Comparing Chameleon to other KWS accelerators in Fig. 12, we observe that its dual-mode operation enables a 4.3 higher peak throughput in 16 16 mode, a 16- fold increase from Chameleon s 4 4 mode. Meanwhile, in 4 4 mode, Chameleon achieves a 2 power reduction for real-time end-to-end KWS inference while maintaining a classification accuracy on par with state-of-the-art inference- only accelerators. IV. MEASUREMENTS Chameleon was taped out in TSMC 40-nm low-power (LP) CMOS technology (Fig. 13). It occupies 1.25 mm2 with a core area of 0.83 mm2, including the power rings. The SoC has two separate power domains, corresponding to (i) the core logic (using a mix of SVT and HVT standard cells) and always-on memories, and (ii) the gateable MSB memories. We first outline the software and measurement setup (Sec- tion IV-A), then present the results for FSL and CL (Sec- tion IV-B), followed by real-time KWS results (Section IV-C), and finally compare Chameleon to the SotA (Section IV-D). A.\n\n--- Segment 25 ---\nWe first outline the software and measurement setup (Sec- tion IV-A), then present the results for FSL and CL (Sec- tion IV-B), followed by real-time KWS results (Section IV-C), and finally compare Chameleon to the SotA (Section IV-D). A. Software and Measurement Setup Our TCN models are based on the original architecture in [18], where we introduce several modifications incorpo- rating techniques from modern deep convolutional residual networks (ResNets) [30] for normalization [33], [34] and initialization [34], [35]. To deploy TCNs on Chameleon, we perform quantization- aware training (QAT) using the Brevitas framework [36], after FP32 training. QAT starts from the FP32 checkpoint with the lowest validation loss, with all parameters quantized and batch normalization (BN) layers folded into the weights of the previous layer, as per [37], [38]. Within Brevitas, we implemented a custom variable-bit quantizer for signed log2 weights and a custom requantizer to simulate overflow of the 4-bit unsigned uniform activations in Chameleon. We apply asymmetric and symmetric quantization for activations and weights respectively, both with per-tensor scaling [39]. During 9 Gateable memories (off during 4 4) Always-on memories Core logic 0.91 mm 1.12 mm ZCU104 FPGA board Daughterboard Chip (a) Chip summary (c) Test setup (b) Area breakdown ( ) (d) Chip microphotograph 0.91 mm 1.12 mm (e) Efficiency and max. frequency vs. core memory voltage Fig. 13. (a) Metrics summary of the Chameleon SoC. (b) Area breakdown of the SoC per module. (c) Test setup, including a Zynq UltraScale MPSoC ZCU104 FPGA that connects to a daughter board hosting the chip. (d) Anno- tated die photograph of the Chameleon SoC. The bottom section of the SoC, containing the gateable memories, can be powered off to enable inference with small networks using the 4 4 mode of the PE array. (e) Efficiency and maximum frequency characterization of Chameleon.\n\n--- Segment 26 ---\nThe bottom section of the SoC, containing the gateable memories, can be powered off to enable inference with small networks using the 4 4 mode of the PE array. (e) Efficiency and maximum frequency characterization of Chameleon. quantized PN training, the TCN embeddings are using 4- bit unsigned uniform quantization, like activations (see Sec- tion III-C). The resulting prototypes are quantized using 4-bit signed log2 quantization, as they are converted to prototypical parameters as per Equation (8). Our measurement setup is shown in Fig. 13(c). A daughter board hosting the Chameleon chip connects to external power supplies and is interfaced with a Zynq UltraScale MPSoC ZCU104 evaluation board, which provides test stimuli and records the results. The complete deployment and test setups are part of the open-source repository of Chameleon. Note that all metrics that we report in the remainder of this section are obtained at room temperature. B. FSL and CL To demonstrate end-to-end on-chip learning on Chameleon using our unified learning and inference architecture, we benchmark Chameleon for FSL and CL scenarios. Benchmarking setup For both FSL and CL, we use the Omniglot dataset [17], which comprises 1,623 classes of handwritten characters from 50 alphabets (see Fig. 14, right, for selected examples). Every class has 20 examples, each of Characters from different scripts in the Omniglot dataset: Fig. 14. Sample characters from different alphabets, taken from the Omniglot dataset, including a demonstration of flattening an image from the dataset to enable sequential Omniglot on 1D sequences of pixel data. which is drawn by a different person. To align with other works on FSL, we use the Vinyals train-val-test split [40]. Following [16], [24], we augment all splits by generating new classes via 90 , 180 , and 270 rotations and resize all images to 28 28 for consistency. To adapt the images for TCN pro- cessing, we flatten each image pixelwise (see Fig. 14, bottom), effectively creating a sequential Omniglot representation.\n\n--- Segment 27 ---\nTo adapt the images for TCN pro- cessing, we flatten each image pixelwise (see Fig. 14, bottom), effectively creating a sequential Omniglot representation. FSL results Using a 116k-parameter TCN with 14 layers, we report in Table I the classification accuracies on the Omniglot test set across 5,20-way 1,5-shot as well as 32-way 1-shot scenarios found in the state of the art. It can be seen that Chameleon outperforms existing works [7] [9] by up to 16 accuracy points, setting new records across all scenarios, even though both FSL CIM designs [7], [8] use FP32 off- chip embedders. Chameleon consumes 11.6 mW for end-to- end FSL on Omniglot at 100 MHz and 1.0 V. At 100 kHz and 0.625 V, it consumes 12.9 µW. The latencies for learning one shot of a new class are 0.59 ms and 0.54 s respectively, yielding an energy per shot of 6.84 µJ and 6.97 µJ, including the embedding phase. Since prototypical parameter extraction only takes a few clock cycles, 0.04 of the embedding computation time on Omniglot, learning and inference have effectively identical total energy, power and latency. This contrasts sharply with other on-chip training methods like CHIMERA [41], which employs low-rank training with 8-bit signed parameters and requires 103 104 RRAM update steps for learning. Similarly, Park et al. [42] use FP8 for full on-chip SGD-based training and hence could host MAML-like setups. However, their lowest reported power is already 13 higher than Chameleon s peak end-to-end power, demonstrating its incompatibility with extreme-edge operation. Alternatively, instead of learning new classes, Cioflan et al. [43] use embed- dings to adapt to the speech characteristics of end users for KWS. A microcontroller updates an embedding layer using few-shot spoken keywords, inducing a 35 FLOPS overhead compared to embedding computation alone. CL results We evaluate CL on Chameleon using the same TCN model as for FSL.\n\n--- Segment 28 ---\nA microcontroller updates an embedding layer using few-shot spoken keywords, inducing a 35 FLOPS overhead compared to embedding computation alone. CL results We evaluate CL on Chameleon using the same TCN model as for FSL. CL is performed by learning one new class at a time, up to 250 classes, using 1, 2, 5, or 10 shots. As Chameleon is the only silicon-proven work to perform end-to-end fully on-chip CL, Fig. 15 compares its CL per- formance (continuous lines) to the FSL performance of prior 10 TABLE I. FSL test accuracy comparison between FSL accelerators that have reported results in silicon on the Omniglot dataset. The shows 95 confidence intervals over 100 tasks. 5-way 20-way 32-way 1-shot 5-shot 1-shot 5-shot 1-shot Kim et al. [7] 93.4 98.3 - - - SAPIENS [8] - - - - 72 FSL-HDnn [9] 79.0 - - 79.5 - This work 96.8 1.6 98.8 0.5 89.1 1.3 96.1 0.5 83.3 1.2 Uses an off-chip FP32 embedder. 5 2032 50 100 150 200 250 Number of ways 60 70 80 90 100 Accuracy ( ) 1-shot 2-shot 5-shot 10-shot [7] [8] [9] Fig. 15. End-to-end CL classification accuracies on the Omniglot dataset using Chameleon for 2 250 ways with 1, 2, 5 and 10 shots, compared to other FSL chips (points outlined in black). The shaded regions indicate 95 confidence intervals over 20 tasks. For 5 ways, shot count has little effect on accuracy; with more ways, additional shots help, with diminishing returns beyond 5 shots. works [7] [9] (single data points). Notably, in the similar few- shot class-incremental learning (FSCIL) [44] scenario, where a DNN learns a set of classes before deployment and a new set of classes online, there is currently also no scalable end-to- end fully on-chip design. For example, Karunaratne et al.\n\n--- Segment 29 ---\nNotably, in the similar few- shot class-incremental learning (FSCIL) [44] scenario, where a DNN learns a set of classes before deployment and a new set of classes online, there is currently also no scalable end-to- end fully on-chip design. For example, Karunaratne et al. [15] propose a CIM macro for MANNs targeting FSCIL: their design supports embedding aggregation (summing) and uses a 4 larger embedder than [7], enabling 100-way FSCIL with 5 shots per class. However, embeddings are still generated off- chip with an FP32 embedder and binarized before on-chip use, similar to [7], [8]. Alternatively, Wibowo et al. [45] demon- strate FSCIL with the embeddings for PNs computed on-chip, using a microcontroller. However, they require an external L3 memory to store the 8-bit (2 larger than Chameleon) DNN weights and use cosine similarity as the distance function for PNs, requiring an expensive division operation. Beyond embedding-based learning, in [26], Huo et al. demonstrate a first proof of concept for fully on-chip end-to-end FSCIL. The design uses a spiking NN with spike timing-dependent plasticity and lateral inhibition: it supports classification of nine types of gas using olfactory data and overcomes data disturbances caused by sensor drift. However, since it supports networks of only up to 4k 4-bit parameters, its suitability for more complex tasks such as Omniglot is unclear. C. Real-time KWS To characterize both (i) Chameleon s inference efficiency using its dual-mode MatMul-free PE array, and (ii) its long- context modeling capabilities using the greedy dilation-aware execution of TCNs, we conduct a series of KWS experiments. 13 87 MFCC (4 4): 3.1 µW 2 36 62 MFCC (16 16): 7.4 µW 88 5 8 Raw (16 16): 59.4 µW Dynamic power Core leakage MSB memories leakage Fig. 16.\n\n--- Segment 30 ---\n13 87 MFCC (4 4): 3.1 µW 2 36 62 MFCC (16 16): 7.4 µW 88 5 8 Raw (16 16): 59.4 µW Dynamic power Core leakage MSB memories leakage Fig. 16. Comparison of power contributions (at 0.73 V) during real-time KWS using MFCC feature vectors in both 4 4 and 16 16 mode, as well as raw audio in 16 16 mode. Down Go Left No Off On Right Stop Up Yes Silence Unknown Predicted label Down Go Left No Off On Right Stop Up Yes Silence Unknown True label 94 88 94 92 93 94 96 98 94 96 98 84 MFCC-based KWS [93.3 ] Down Go Left No Off On Right Stop Up Yes Silence Unknown Predicted label 84 84 87 83 86 86 91 93 86 90 96 72 Raw-audio KWS [86.4 ] 0 20 40 60 80 100 True positive rate ( ) Fig. 17. Confusion matrices for MFCC-based KWS and raw audio KWS on the 12-class GSCv2 test set, including true positive rates for all keywords. Benchmarking setup We use the Google Speech Com- mands V2 (GSCv2) dataset [46], which comprises 105,829 utterances from 2,618 speakers across 35 spoken-word classes, with varying sample counts per class. Each sample is recorded for one second at 16 kHz. Ten words serve as command- like keywords: Yes, No, Up, Down, Left, Right, On, Off, Stop, and Go. Along with an unknown command class (grouping the remaining words) and a silence class (one-second audio clips from background noise files), they form the standard 12-way classification setup. We follow the same validation and testing splits as [46], [47]. To augment the training data, we apply up to 100 ms forward or backward shifts (as in [47], [48]), followed by noise addition with a probability of 0.15. Following floating-point training, we perform QAT, during which all augmentations are disabled. For PE array characterization, the audio is transformed into a 28D MFCC feature map [49] before it is fed to the DNN.\n\n--- Segment 31 ---\nFollowing floating-point training, we perform QAT, during which all augmentations are disabled. For PE array characterization, the audio is transformed into a 28D MFCC feature map [49] before it is fed to the DNN. To align with other KWS digital accelerators [10] [12], we use a window size of 32 ms with a 16 ms shift for MFCC-extraction, yielding a sequence of 63 timesteps. For long-context modeling, the raw audio data is used. Dual-mode PE array characterization To minimize the static-power footprint for real-time MFCC-based KWS, we use a TCN with 16.5k parameters and eight layers, which is small enough to fit in the always-on memories of Chameleon. Running this TCN in the 4 4 PE array mode results in a real-time power consumption of 3.1 µW at a clock frequency of 23.3 kHz with a core voltage of 0.73 V, achieving a classification accuracy of 93.3 on the 12-class test set of GSCv2. Executing the same network in the complete 16 16 PE array requires a clock frequency of 3.67 kHz to yield a 11 real-time power of 7.4 µW, with both the MSB memories and the core at 0.73 V. Figure 16 displays the contributions of the core leakage, MSB memories leakage and dynamic power for both real-time KWS scenarios. It can be seen that executing MFCC-based KWS with only the always-on memories leads to a power reduction of 44 compared to the 16 16 baseline. Note also that the dynamic power in the 4 4 mode is higher compared to the 16 16 mode: since the throughput in the latter is 4 higher, the clock frequency can be downscaled accordingly to achieve the same latency. Long-context modeling results In most KWS accelerator designs, the footprint of the MFCC pre-processing exceeds that of the actual DNN inference logic. For example, in the work by Shan et al. [50], the MFCC extraction accounts for 60 of the core area, while in [14] the feature extractor is 4 larger than the on-chip DNN accelerator. Similarly, the MFCC extraction module in [10] uses the same amount of area as the on-chip DNN accelerator (incurring a 2.4 area overhead when also considering the analog feature extraction).\n\n--- Segment 32 ---\n[50], the MFCC extraction accounts for 60 of the core area, while in [14] the feature extractor is 4 larger than the on-chip DNN accelerator. Similarly, the MFCC extraction module in [10] uses the same amount of area as the on-chip DNN accelerator (incurring a 2.4 area overhead when also considering the analog feature extraction). Even fully digital designs like [51] allocate 40 of the core area to MFCC logic. To the best of our knowledge, there is currently only one design operating directly on the raw audio sequence without any specific pre-processing logic: the fully analog accelerator in [52] performs inference on 8 kHz audio by applying a trained filter bank to 4 ms input windows. However, several key DNN processing steps are performed off- chip, preventing end-to-end operation. In contrast, by enabling long-context inference, our approach eliminates the need for any data-specific pre-processing block, significantly increasing the number of possible deployment scenarios. Using a 24- layer, 118k-parameter model, Chameleon achieves 86.4 test accuracy on GSCv2 (see Figure 17) from 16 kHz raw audio with 59.4 µW real-time power at a clock frequency of 532 kHz and a core MSB memory voltage of 0.73 V (Fig. 16, right). Although accuracy drops by 7 points, reflecting the challenge of capturing long-range dependencies, it remains competitive, outperforming [14] with 1.6 mm2 analog MFCC at 86.0 12- class accuracy. D. Comparison with the State of the Art Chameleon is the first SoC supporting both inference and end-to-end on-chip few-shot and continual learning: Table II compares our design with both KWS and FSL accelerators that have silicon results on GSC and Omniglot datasets, respectively. This comparison highlights the performance of Chameleon across FSL and CL on-device learning, as well as KWS inference scenarios. FSL performance Chameleon is the only end-to-end FSL chip that supports sequential data, while offering fully flexible support for embedding dimensions, number of ways, and shots. Furthermore, across all FSL evaluation scenarios, Chameleon sets a new record, despite using an embedding DNN that is 7.6 126 smaller than other FSL chips.\n\n--- Segment 33 ---\nFSL performance Chameleon is the only end-to-end FSL chip that supports sequential data, while offering fully flexible support for embedding dimensions, number of ways, and shots. Furthermore, across all FSL evaluation scenarios, Chameleon sets a new record, despite using an embedding DNN that is 7.6 126 smaller than other FSL chips. Comparing with CIM accelerators for FSL distance computation, Kim et al. [7] implement MANNs (Section II-B) with an off-chip FP32 embedder of 7.46 MB, resulting in 3.4 and 0.5 lower accuracy than Chameleon for the 5-way 1- and 5-shot cases, respectively. SAPIENS [8] employs an RRAM-based backend to perform FSL, also using MANNs. On 32-way 1-shot Omniglot, SAPIENS achieves 72 accuracy using an FP32- based 4-layer CNN of 447 kB, which is 11.3 lower than Chameleon using an 8 larger network. To the best of our knowledge, the only other FSL accelerator with an on-chip embedder reporting metrics on Omniglot is FSL-HDnn [9], which uses HDC for FSL. It employs a Bfloat-16-based 5.5 MB ResNet-18 model for generating embeddings, achiev- ing 79 accuracy for 5-way 1-shot and 79.5 for 20-way 5-shot tasks on Omniglot. These accuracies are significantly lower than Chameleon, which uses a network 100 smaller that fits fully on-chip. Indeed, even though FSL-HDnn has a core area 12 larger than Chameleon, only 0.07 of its DNN weights can be stored on-chip at any time. Furthermore, while FSL-HDnn uses distance-based classification, similar to Chameleon, its HDC encoding adds 120 power and 26 area overhead. Chameleon fully sidesteps this costly HDC encoding by computing distances directly on the embeddings. Overall, Chameleon uses 2.5 lower power than FSL-HDnn at the same clock frequency, while reducing the end-to-end latency by 90 , yielding an energy per shot 210 lower.\n\n--- Segment 34 ---\nChameleon fully sidesteps this costly HDC encoding by computing distances directly on the embeddings. Overall, Chameleon uses 2.5 lower power than FSL-HDnn at the same clock frequency, while reducing the end-to-end latency by 90 , yielding an energy per shot 210 lower. CL performance Chameleon is the first design to offer scalable end-to-end CL fully on-chip, establishing the first baseline for 250-way CL. Its unified learning and inference ar- chitecture, which enables a minimum CL power consumption of 12.9 µW, allows limiting the memory overhead of CL to scale with only 26 bytes per way on Omniglot (Equation (8)). This is a negligible 0.04 of the total DNN footprint. Hence, by reusing its DNN weight memory to store FC parameters for new classes, Chameleon flexibly repurposes on-chip memory to scale beyond prior class limits: even with 90 of memory allocated to the deployed TCN, it can still accommodate 250 learned Omniglot classes. In contrast, FSL CIMs like Kim et al. [7] and SAPIENS [8] are limited up to 25 and 32 classes, respectively, due to their fixed array sizes. FSL- HDnn [9] also uses a fixed separate memory to store its high- dimensional encodings, which is 80 larger than all on-chip memory in Chameleon. Yet, only up to 20-way learning is demonstrated, scaling with 8.2 kB way on Omniglot. Real-time KWS performance Chameleon matches SotA 93.3 accuracy on GSC using MFCCs, with the lowest reported real-time end-to-end power (3.1 µW), the small- est model size (8.5 kB) and the highest peak throughput (76.8 GOPS) among end-to-end KWS inference accelerators. Compared to Vocell [10], Chameleon offers 2.5 higher accuracy with 2 lower power and 600 higher peak GOPS. Versus TinyVers [12], which uses a TCN on a RISC-V SoC with a reconfigurable accelerator, Chameleon matches accuracy with a 3 smaller model, and 64 lower power. Tan et al.\n\n--- Segment 35 ---\nVersus TinyVers [12], which uses a TCN on a RISC-V SoC with a reconfigurable accelerator, Chameleon matches accuracy with a 3 smaller model, and 64 lower power. Tan et al. [52] avoid MFCC pre-processing to infer on raw audio instead: using a fully analog design, they improve raw- audio performance by 5.4 accuracy points at 1.8 lower power, with an estimated 25 higher peak efficiency compared to Chameleon. However, it does not perform end-to-end on- chip inference as several key steps of inference computation (e.g., ReLU activation, BN, stride adjustments, max pooling, activation clipping) are carried out off-chip. 12 TABLE II. Comparison with both KWS accelerators that have reported results on GSC in silicon and with FSL accelerators that have reported results on Omniglot in silicon. The shows 95 confidence intervals over 100 tasks. KWS accelerators CIM accelerators for FSL End-to-end FSL accelerators Vocell [10] JSSC 18 TinyVers [12] JSSC 23 Tan et al. [52] JSSC 25 Kim et al. [7] TCAS-II 22 SAPIENS [8] VLSI 21 FSL-HDnn [9] ESSERC 24 This work Technology Implementation Core area (mm2) On-chip memory Supply voltage Max. clock frequency 65-nm Digital 2.56 67 kB 0.6-1.2 V 8 MHz 22-nm Digital 6.25 132 kB 0.4-0.9 V 150 MHz 28-nm Analog 0.121 16 kB 0.35 0.9 V 1 MHz 40-nm LP Mixed signal 0.2 0.8 kB 0.5-1.1 V 80 MHz 40-nm Analog 0.0367 8 kB 0.85-1.1 V 200 MHz 40-nm Digital 11.3 349 kB 0.9-1.2 V 250 MHz 40-nm LP Digital 0.74 71 kB 0.6-1.1 V 150 MHz Network type Max. of on-chip weights Max.\n\n--- Segment 36 ---\nclock frequency 65-nm Digital 2.56 67 kB 0.6-1.2 V 8 MHz 22-nm Digital 6.25 132 kB 0.4-0.9 V 150 MHz 28-nm Analog 0.121 16 kB 0.35 0.9 V 1 MHz 40-nm LP Mixed signal 0.2 0.8 kB 0.5-1.1 V 80 MHz 40-nm Analog 0.0367 8 kB 0.85-1.1 V 200 MHz 40-nm Digital 11.3 349 kB 0.9-1.2 V 250 MHz 40-nm LP Digital 0.74 71 kB 0.6-1.1 V 150 MHz Network type Max. of on-chip weights Max. demonstrated input length Weight precision Activation precision LSTM FC 32k 62 4-bit LUT 8-bit TCN 400k 60 8-bit 8-bit FS-CNN 32.8k 8,000 4-bit 3-bit unsigned Distance computation only, off-chip FP32 embedder CNN 2.1k 28 Bfloat-16 16-bit5 TCN FCs 133k 16,000 4-bit log2 4-bit unsigned Inference support End-to-end inference Full on-chip weight storage FSL support End-to-end FSL CL support (off-chip embedder) FSL method distance metric FSL embedding dimension of FSL classes of FSL shots Do not support FSL MANNs, L1 64 1-25 1-5 MANNs, L1 32 32 1 HDC, Hamming 16-1024 2-128 1, 55 PNs, L22 1-1024 1-256 1-128 Omniglot FSL CL Model size End-to-end power End-to-end latency Do not support FSL 7.46 MB N A N A 447 kB N A N A ( 100 MHz) 5.5 MB5 27 mW 53 ms ( 100 kHz) ( 100 MHz) 59 kB 12.9 µW 11.6 mW 0.54s 0.59 ms Omniglot FSL accuracy 5-way (1, 5 shots) 20-way (1, 5-shots) 32-way, 1-shot Do not support FSL 93.4 , 98.3 - - - - 72 79.0 , - - , 79.5 - 96.8 1.6 , 98.8 0.5 89.1 1.3 , 96.1 0.5 83.3 1.2 Omniglot 250-way CL acc.\n\n--- Segment 37 ---\nof on-chip weights Max. demonstrated input length Weight precision Activation precision LSTM FC 32k 62 4-bit LUT 8-bit TCN 400k 60 8-bit 8-bit FS-CNN 32.8k 8,000 4-bit 3-bit unsigned Distance computation only, off-chip FP32 embedder CNN 2.1k 28 Bfloat-16 16-bit5 TCN FCs 133k 16,000 4-bit log2 4-bit unsigned Inference support End-to-end inference Full on-chip weight storage FSL support End-to-end FSL CL support (off-chip embedder) FSL method distance metric FSL embedding dimension of FSL classes of FSL shots Do not support FSL MANNs, L1 64 1-25 1-5 MANNs, L1 32 32 1 HDC, Hamming 16-1024 2-128 1, 55 PNs, L22 1-1024 1-256 1-128 Omniglot FSL CL Model size End-to-end power End-to-end latency Do not support FSL 7.46 MB N A N A 447 kB N A N A ( 100 MHz) 5.5 MB5 27 mW 53 ms ( 100 kHz) ( 100 MHz) 59 kB 12.9 µW 11.6 mW 0.54s 0.59 ms Omniglot FSL accuracy 5-way (1, 5 shots) 20-way (1, 5-shots) 32-way, 1-shot Do not support FSL 93.4 , 98.3 - - - - 72 79.0 , - - , 79.5 - 96.8 1.6 , 98.8 0.5 89.1 1.3 , 96.1 0.5 83.3 1.2 Omniglot 250-way CL acc. 1-shot (final, avg.) 2-shot (final, avg.) 5-shot (final, avg.) 10-shot (final, avg.)\n\n--- Segment 38 ---\n5-shot (final, avg.) 10-shot (final, avg.) Do not support continual learning 57.6 1.0 , 70.3 69.2 0.6 , 80.2 79.3 0.7 , 87.1 82.2 0.4 , 89.0 GSC 12-class KWS Pre-processing Accuracy dataset version Latency Real-time power Model size ( 250kHz) MFCC 90.87 (v1) 16 ms 10.6 µW1 16 kB ( 5 MHz) MFCC 93.3 (v1) 11 ms 193 µW 23 kB ( 1 MHz) None 91.8 (v2) 2 ms 1.73 µW 11 kB Do not support inference on temporal data ( 23.3 kHz) MFCC 93.3 (v2) 16 ms 3.1 µW 8.5 kB ( 532 kHz) None 86.4 (v2) 63 µs 59.4 µW 60 kB Peak GOPS Peak TOPS W 0.13 0.455 17.6 17 N C N C N A3 N A4 154 5.7 76.8 6.0 1 Excludes on-chip MFCC computation power. 2 Interpolated metrics. 3 Reported metrics are for distance computation only and amount to 4.855 GOPS and 27.7 TOPS W. 4 Reported metrics are for distance computation only and amount to 0.4 GOPS5 and 0.118 TOPS W. 5 Estimated metrics. V. CONCLUSION In this work, we presented Chameleon, the first scalable SoC to enable end-to-end on-chip FSL and few-shot CL on sequential data, without compromising inference accuracy nor efficiency. We achieve this through three key contributions. First, by integrating FSL and CL into the inference process, we propose a unified learning and inference architecture built on PNs [16]. The learning integration incurs only 0.04 latency and 0.5 core area overhead. Second, our greedy dilation-aware TCN execution enables the first 16 kHz raw audio KWS on GSCv2, achieving a 160 larger receptive field than prior end-to-end KWS accelerators using only 2 kB activation memory 4 less than SotA TCN accelerators, while avoiding a 60-400 area increase for audio-specific pre-processing blocks.\n\n--- Segment 39 ---\nThe learning integration incurs only 0.04 latency and 0.5 core area overhead. Second, our greedy dilation-aware TCN execution enables the first 16 kHz raw audio KWS on GSCv2, achieving a 160 larger receptive field than prior end-to-end KWS accelerators using only 2 kB activation memory 4 less than SotA TCN accelerators, while avoiding a 60-400 area increase for audio-specific pre-processing blocks. Third, by using log2 quantization, we enable a MatMul-free PE array. Combined with its dual-mode operation through system-level power gating, this yields a peak throughput in 16 16-mode that is 4.3 higher than previous KWS accelerators or 2 lower real-time power in 4 4-mode. Chameleon establishes new accuracy records for end-to-end on-chip FSL on the Omniglot dataset (96.8 1.6 5-way 1- shot, 96.1 0.5 20-way 5-shot) at 2.5 lower power than previous on-chip embedder FSL work [9]. It is the only end- to-end FSL accelerator with all weights on-chip and enables FSL starting at 12.9 µW. Considering CL, although several designs exist, none offer a fully end-to-end SoC solution with comparable accuracy, footprint, and scalability. Hence, Chameleon establishes the first baseline for end-to-end on-chip CL on Omniglot, demonstrating 250-way few-shot CL (82.2 final accuracy for 10 shots). Beyond this learning performance, Chameleon maintains a SotA accuracy of 93.3 for 12-class GSCv2 compared to inference-only KWS accelerators. By reframing learning so that it can readily be supported by existing inference accelerators, Chameleon paves the way for efficient lifelong learning on extreme-edge devices. ACKNOWLEDGEMENT The authors thank Prof. Kofi Makinwa, Nicolas Chauvaux, Dr. Martin Lefebvre, and Dr. Adrian Kneip for their feedback, Dr. Filipe Cardoso for dicing and Nuriel Rozsa for die photos.\n\n--- Segment 40 ---\nBy reframing learning so that it can readily be supported by existing inference accelerators, Chameleon paves the way for efficient lifelong learning on extreme-edge devices. ACKNOWLEDGEMENT The authors thank Prof. Kofi Makinwa, Nicolas Chauvaux, Dr. Martin Lefebvre, and Dr. Adrian Kneip for their feedback, Dr. Filipe Cardoso for dicing and Nuriel Rozsa for die photos. This publication was partly financed by the Dutch Research Council (NWO) as part of the project AdaptEdge with file number 20267 in the NWO Talent Programme Veni. 13 REFERENCES [1] N. S. Murthy, P. Vrancx, N. Laubeuf, P. Debacker, F. Catthoor, and M. Verhelst, Learn to learn on chip: Hardware-aware meta-learning for quantized few-shot learning at the edge, in 2022 IEEE ACM 7th Symp. on Edge Computing (SEC), 2022, pp. 14 25. [2] L. Ravaglia, M. Rusci, D. Nadalini, A. Capotondi, F. Conti, and L. Benini, A tinyml platform for on-device continual learning with quantized latent replays, IEEE J. on Emerging and Selected Topics in Circuits and Systems, vol. 11, no. 4, pp. 789 802, 2021. [3] D. Amodei, C. Olah, J. Steinhardt, P. Christiano, J. Schulman, and D. Man e, Concrete problems in ai safety , 2016. arXiv:1606.06565. [4] L. Pellegrini, G. Graffieti, V. Lomonaco, and D. Maltoni, Latent replay for real-time continual learning, in 2020 IEEE RSJ Int. Conf. on Intelligent Robots and Systems (IROS), 2020, pp. 10 203 10 209. [5] M. Li and X. S. Hu, A quantization framework for neural network adaption at the edge, in 2021 Des., Automat. Test in Europe Conf. Exhib. (DATE), 2021, pp. 402 407.\n\n--- Segment 41 ---\n(DATE), 2021, pp. 402 407. [6] M. Wang, R. Xue, J. Lin, and Z. Wang, Exploring quantization in few-shot learning, in 2020 18th IEEE Int. New Circuits and Systems Conf. (NEWCAS), 2020, pp. 279 282. [7] S. Kim, W. Lee, S. Kim, S. Park, and D. Jeon, An in-memory computing sram macro for memory-augmented neural network, TCAS- II, vol. 69, no. 3, 2022. [8] H. Li, W.-C. Chen, A. Levy, C.-H. Wang, H. Wang, P.-H. Chen, W. Wan, H.-S. P. Wong, and P. Raina, One-shot learning with memory- augmented neural networks using a 64-kbit, 118 gops w rram-based non-volatile associative memory, in 2021 Symp. on VLSI Technology, 2021, pp. 1 2. [9] H. Yang, C. E. Song, W. Xu, B. Khaleghi, U. Mallappa, M. Shah, K. Fan, M. Kang, and T. Rosing, Fsl-hdnn: A 5.7 tops w end-to- end few-shot learning classifier accelerator with feature extraction and hyperdimensional computing, in 2024 IEEE European Solid-State Electronics Research Conf. (ESSERC), 2024, pp. 33 36. [10] J. S. P. Giraldo, S. Lauwereins, K. Badami, and M. Verhelst, Vocell: A 65-nm speech-triggered wake-up soc for 10- µ w keyword spotting and speaker verification, IEEE J. of Solid-State Circuits, vol. 55, no. 4, pp. 868 878, 2020. [11] J. S. P. Giraldo, V. Jain, and M. Verhelst, Efficient execution of temporal convolutional networks for embedded keyword spotting, IEEE Trans. on Very Large Scale Integration (VLSI) Systems, vol. 29, no. 12, pp. 2220 2228, 2021.\n\n--- Segment 42 ---\n12, pp. 2220 2228, 2021. [12] V. Jain, S. Giraldo, J. D. Roose, L. Mei, B. Boons, and M. Verhelst, Tinyvers: A tiny versatile system-on-chip with state-retentive emram for ml inference at the extreme edge, JSSC, vol. 58, no. 8, 2023. [13] P. P. Bernardo, C. Gerum, A. Frischknecht, K. L ubeck, and O. Bringmann, Ultratrail: A configurable ultralow-power tc-resnet ai accelerator for efficient keyword spotting, IEEE Trans. on Computer- Aided Design of Integrated Circuits and Systems, vol. 39, no. 11, pp. 4240 4251, 2020. [14] K. Kim, C. Gao, R. Grac a, I. Kiselev, H.-J. Yoo, T. Delbruck, and S.-C. Liu, A 23-uw keyword spotting ic with ring-oscillator-based time-domain feature extraction, IEEE J. of Solid-State Circuits, vol. 57, no. 11, pp. 3298 3311, 2022. [15] G. Karunaratne, M. Hersche, J. Langeneager, G. Cherubini, M. L. Gallo, U. Egger, K. Brew, S. Choi, I. Ok, C. Silvestre, N. Li, N. Saulnier, V. Chan, I. Ahsan, V. Narayanan, L. Benini, A. Sebastian, and A. Rahimi, In-memory realization of in-situ few-shot continual learning with a dynamically evolving explicit memory, in ESSCIRC 2022- IEEE 48th European Solid State Circuits Conf. (ESSCIRC), 2022, pp. 105 108. [16] J. Snell, K. Swersky, and R. Zemel, Prototypical networks for few- shot learning, in Advances in Neural Information Processing Systems, vol. 30, 2017. [17] B. M. Lake, R. Salakhutdinov, and J.\n\n--- Segment 43 ---\n30, 2017. [17] B. M. Lake, R. Salakhutdinov, and J. B. Tenenbaum, Human-level concept learning through probabilistic program induction, Science, vol. 350, no. 6266, 2015. [18] S. Bai, J. Z. Kolter, and V. Koltun, An empirical evaluation of generic convolutional and recurrent networks for sequence modeling , 2018. arXiv:1803.01271. [19] M. Scherer, A. D. Mauro, T. Fischer, G. Rutishauser, and L. Benini, Tcn-cutie: A 1,036-top s w, 2.72-µj inference, 12.2-mw all-digital ternary accelerator in 22-nm fdx technology, IEEE Micro, vol. 43, no. 1, pp. 42 48, 2023. [20] C. Finn, P. Abbeel, and S. Levine, Model-agnostic meta-learning for fast adaptation of deep networks, in Int. Conf. on machine learning, PMLR, 2017, pp. 1126 1135. [21] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, Meta- learning in neural networks: A survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 9, pp. 5149 5169, 2022. [22] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization , 2014. arXiv:1412.6980. [23] N. Mishra, M. Rohaninejad, X. Chen, and P. Abbeel, A Simple Neural Attentive Meta-Learner , 2018. arXiv:1707.03141. [24] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap, Meta-learning with memory-augmented neural networks, in Proc. of The 33rd Int. Conf. on Machine Learning, ser. Proc. of Machine Learning Research, vol. 48, New York, New York, USA: PMLR, Jun. 2016, pp. 1842 1850.\n\n--- Segment 44 ---\n2016, pp. 1842 1850. [25] J. Morris, R. Fernando, Y. Hao, M. Imani, B. Aksanli, and T. Rosing, Locality-based encoder and model quantization for efficient hyper- dimensional computing, IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems, vol. 41, no. 4, pp. 897 907, 2022. [26] D. Huo, J. Zhang, X. Dai, J. Zhang, C. Qian, K.-T. Tang, and H. Chen, Anp-g: A 28-nm 1.04-pj sop sub-mm2 asynchronous hybrid neural network olfactory processor enabling few-shot class-incremental on- chip learning, IEEE J. of Solid-State Circuits, pp. 1 11, 2025. [27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [28] S. Hochreiter and J. Schmidhuber, Long short-term memory, Neural computation, vol. 9, no. 8, pp. 1735 1780, 1997. [29] Z. Wang, Y. Ma, Z. Liu, and J. Tang, R-transformer: Recurrent neural network enhanced transformer , 2016. arXiv:1907.05572. [30] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition , 2015. arXiv:1512.03385. [31] D. Miyashita, E. H. Lee, and B. Murmann, Convolutional neural net- works using logarithmic data representation , 2016. arXiv:1603.01025. [32] D. Przewlocka-Rus, S. S. Sarwar, H. E. Sumbul, Y. Li, and B. De Salvo, Power-of-two quantization for low bitwidth and hardware compliant neural networks , 2022. arXiv:2203.05025.\n\n--- Segment 45 ---\n[32] D. Przewlocka-Rus, S. S. Sarwar, H. E. Sumbul, Y. Li, and B. De Salvo, Power-of-two quantization for low bitwidth and hardware compliant neural networks , 2022. arXiv:2203.05025. [33] S. Ioffe and C. Szegedy, Batch normalization: Accelerating deep network training by reducing internal covariate shift , 2015. arXiv:1502.03167. [34] P. Goyal, Accurate, large minibatch sgd: Training imagenet in 1 hour , 2017. arXiv:1706.02677. [35] K. He, X. Zhang, S. Ren, and J. Sun, Delving deep into rectifiers: Sur- passing human-level performance on imagenet classification, in 2015 IEEE Int. Conf. on Computer Vision (ICCV), 2015, pp. 1026 1034. [36] A. Pappalardo. Xilinx brevitas. (2023). [37] R. Krishnamoorthi, Quantizing deep convolutional networks for effi- cient inference: A whitepaper , 2018. arXiv:1806.08342. [38] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, Quantization and training of neural networks for efficient integer-arithmetic-only inference, in Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), Jun. 2018. [39] M. Nagel, M. Fournarakis, R. A. Amjad, Y. Bondarenko, M. van Baalen, and T. Blankevoort, A white paper on neural network quan- tization , 2021. arXiv:2106.08295. [40] O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu koray, and D. Wierstra, Matching networks for one shot learning, in Advances in Neural Information Processing Systems, vol. 29, 2016.\n\n--- Segment 46 ---\n[40] O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu koray, and D. Wierstra, Matching networks for one shot learning, in Advances in Neural Information Processing Systems, vol. 29, 2016. [41] K. Prabhu, A. Gural, Z. F. Khan, R. M. Radway, M. Giordano, K. Koul, R. Doshi, J. W. Kustin, T. Liu, G. B. Lopes, V. Turbiner, W.-S. Khwa, Y.-D. Chih, M.-F. Chang, G. Lallement, B. Murmann, S. Mitra, and P. Raina, Chimera: A 0.92-tops, 2.2-tops w edge ai accelerator with 2-mbyte on-chip foundry resistive ram for efficient training and inference, IEEE J. of Solid-State Circuits, vol. 57, no. 4, pp. 1013 1026, 2022. [42] J. Park, S. Lee, and D. Jeon, 9.3 a 40nm 4.81tflops w 8b floating- point training processor for non-sparse neural networks using shared exponent bias and 24-way fused multiply-add tree, in 2021 IEEE Int. Solid-State Circuits Conf. (ISSCC), vol. 64, 2021, pp. 1 3. [43] C. Cioflan, L. Cavigelli, and L. Benini, Boosting keyword spot- ting through on-device learnable user speech characteristics , 2024. arXiv:2403.07802. [44] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y. Gong, Few- shot class-incremental learning, in Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2020.\n\n--- Segment 47 ---\n[44] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y. Gong, Few- shot class-incremental learning, in Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2020. [45] Y. E. Wibowo, C. Cioflan, T. M. Ingolfsson, M. Hersche, L. Zhao, A. Rahimi, and L. Benini, 12 mj per class on-device online few-shot class-incremental learning, in 2024 Des., Automat. Test in Europe Conf. Exhib. (DATE), 2024, pp. 1 6. 14 [46] P. Warden, Speech commands: A dataset for limited-vocabulary speech recognition , 2018. arXiv:1804.03209. [47] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and S. Laurenzo, Streaming keyword spotting on mobile devices , 2020. arXiv:2005.06720. [48] Y. Zhang, N. Suda, L. Lai, and V. Chandra, Hello edge: Keyword spotting on microcontrollers , 2017. arXiv:1711.07128. [49] S. Davis and P. Mermelstein, Comparison of parametric represen- tations for monosyllabic word recognition in continuously spoken sentences, IEEE Trans. on acoustics, speech, and signal processing, vol. 28, no. 4, pp. 357 366, 1980. [50] W. Shan, M. Yang, T. Wang, Y. Lu, H. Cai, L. Zhu, J. Xu, C. Wu, L. Shi, and J. Yang, A 510-nw wake-up keyword-spotting chip using serial-fft-based mfcc and binarized depthwise separable cnn in 28-nm cmos, IEEE J. of Solid-State Circuits, vol. 56, no. 1, pp. 151 164, 2021. [51] B. Liu, H. Cai, Z. Wang, Y.\n\n--- Segment 48 ---\n151 164, 2021. [51] B. Liu, H. Cai, Z. Wang, Y. Sun, Z. Shen, W. Zhu, Y. Li, Y. Gong, W. Ge, J. Yang, and L. Shi, A 22nm, 10.8 uw 15.1 uw dual computing modes high power-performance-area efficiency domained background noise aware keyword- spotting processor, IEEE Trans. on Circuits and Systems I: Regular Papers, vol. 67, no. 12, pp. 4733 4746, 2020. [52] F. Tan, W.-H. Yu, J. Lin, K.-F. Un, R. P. Martins, and P.-I. Mak, A 1.8 far, 2 ms decision latency, 1.73 nj decision keywords- spotting (kws) chip incorporating transfer-computing speaker verifica- tion, hybrid-if-domain computing and scalable 5t-sram, IEEE J. of Solid-State Circuits, vol. 60, no. 3, pp. 1103 1112, 2025. Douwe den Blanken (Graduate Student Member, IEEE) received the M.Sc. degree (with honors) in embedded systems from the Delft University of Technology (TU Delft), Delft, The Netherlands, in 2023, where he is currently pursuing the Ph.D. de- gree, under the supervision of Prof. C. Frenkel. His current research interests include efficient learning algorithms and their implementation in silicon, as well as the quantization and acceleration of modern DNNs. Charlotte Frenkel (Member, IEEE) received the M.Sc. degree (summa cum laude) in Electromechan- ical Engineering and the Ph.D. degree in Engineer- ing Science from Universit e catholique de Louvain (UCLouvain), Louvain-la-Neuve, Belgium in 2015 and 2020, respectively. In February 2020, she joined the Institute of Neuroinformatics, UZH and ETH Zurich, Switzerland, as a postdoctoral researcher. She is an Assistant Professor at Delft University of Technology, Delft, The Netherlands, since July 2022, and holds a Visiting Faculty Researcher position with Google since October 2024.\n\n--- Segment 49 ---\nIn February 2020, she joined the Institute of Neuroinformatics, UZH and ETH Zurich, Switzerland, as a postdoctoral researcher. She is an Assistant Professor at Delft University of Technology, Delft, The Netherlands, since July 2022, and holds a Visiting Faculty Researcher position with Google since October 2024. Her research aims at bridging the bottom-up (bio-inspired) and top-down (engineering-driven) design approaches toward neuromorphic intelligence, with a focus on hardware-algorithm co-design for (Neuro)AI, digital hardware accelerators, and brain-inspired on-device learning. Dr. Frenkel received a best paper award at the IEEE International Sym- posium on Circuits and Systems (ISCAS) 2020 conference in the Neural Networks track, and her Ph.D. thesis was awarded the FNRS-FWO Nokia Bell Scientific Award 2021 and the FNRS-FWO IBM Innovation Award 2021. In 2023, she was awarded prestigious Veni and AiNed Fellowship grants from the Dutch Research Council (NWO). She presented several invited talks, including keynotes at the tinyML EMEA technical forum 2021 and at the Neuro-Inspired Computational Elements (NICE) neuromorphic conference 2021. She serves or has served as a program co-chair of NICE 2023-2024 and of the tinyML Research Symposium 2024, as a co-lead of the NeuroBench initiative for benchmarks in neuromorphic computing since 2022, as a TPC member of IEEE ESSERC for 2022-2024, and as an associate editor for the IEEE Transactions on Biomedical Circuits and Systems since 2022.\n\n