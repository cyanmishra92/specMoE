=== ORIGINAL PDF: 2503.20507v2_Harmonia_A_Multi-Agent_Reinforcement_Learning_Appr.pdf ===\n\nRaw text length: 92460 characters\nCleaned text length: 91684 characters\nNumber of segments: 57\n\n=== CLEANED TEXT ===\n\nHarmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems Rakesh Nadig Vamanan Arulchelvan Rahul Bera Taha Shahroodi Gagandeep Singh Andreas Kakolyris Mohammad Sadrosadati Jisung Park Onur Mutlu ETH Z칲rich AMD Research POSTECH Abstract Hybrid storage systems (HSS) combine multiple storage devices with diverse characteristics to achieve high performance and ca- pacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which rearranges stored data (i.e., prefetches hot data and evicts cold data) across the devices to sustain high HSS performance. Prior works focus on improving only data placement or only data migration in HSS, which leads to relatively low HSS performance. Unfortunately, no prior work tries to optimize both policies together. Our goal is to design a holistic data-management technique that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS, and thus significantly improve system performance. We demonstrate the need for multiple rein- forcement learning (RL) agents to accomplish our goal. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data- management technique that employs two lightweight autonomous RL agents, a data-placement agent and a data-migration agent, which adapt their policies for the current workload and HSS con- figuration, and coordinate with each other to improve overall HSS performance. We evaluate Harmonia on a real HSS with up to four heteroge- neous and diverse storage devices. Our evaluation using 17 data- intensive workloads on performance-optimized (cost-optimized) HSS with two storage devices shows that, on average, Harmonia outperforms the best-performing prior approach by 49.5 (31.7 ). On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0 (42.0 ). Harmonia s perfor- mance benefits come with low latency (240洧녵洧맍or inference) and storage overheads (206 KiB in DRAM for both RL agents together). We will open-source Harmonia s implementation to aid future re- search on HSS. 1 Introduction A hybrid storage system (HSS) [1 33] is a cost-effective solution for high-performance and large-capacity storage requirements of data-intensive applications such as machine learning [34 36], large language models [37 39], databases [40 42], graph processing [43], and genome analysis [44 46]. An HSS combines multiple storage devices (e.g., solid-state drives (SSDs) [47 51], hard disk drives (HDDs)) of diverse characteristics (e.g., I O bandwidth, storage capacity). A typical HSS consists of fast storage devices with limited capacities and slow storage devices with large capacities. The performance of an HSS is highly dependent on two key policies, data placement and data migration. The data-placement policy determines the best-fit storage device in the HSS for incoming I O requests. The data-migration policy rearranges data across the storage devices (i.e., prefetches frequently-accessed data to the fast device and evicts cold data to the slow device) to sustain high HSS performance, which can degrade over time due to (1) misplacement of data, and (2) changes in workload access patterns. We identify four key challenges in designing efficient data-place- ment and data-migration policies. First, workload access patterns and HSS conditions (e.g., access latencies, device capacity utiliza- tion) can change frequently in data-intensive environments, which makes it hard to optimize the policies. Second, the data-placement policy should have a low performance overhead as it operates on the critical path of I O request handling. Third, the data-migration policy needs to migrate data across storage devices in a timely man- ner without impacting the latency of incoming I O requests. Fourth, the two policies should not make conflicting decisions, which can adversely impact HSS performance and device lifetimes. Limitations of prior works. Prior works propose techniques for either data placement (e.g., [5 7, 9, 13 33, 52 68]) or data migra- tion (e.g., [7, 11, 30, 33, 69 73]), but they provide relatively low performance when employed together or alone. Our motivational study on performance- (cost-) optimized HSS configurations demon- strates that even the state-of-the-art HSS data-placement technique, Sibyl [29], only achieves 49.9 (41.4 ) of the performance of an Oracle policy (see 3 for details). We identify two key reasons for such large performance gaps. First, no prior work proposes a holis- tic approach to optimize both data placement and data migration together. Second, heuristic- and supervised-learning-based policies (e.g., [7, 13, 23, 29 31, 33, 69]) require hand-tuning and may not adapt to changes in workload characteristics and HSS states. Since there are no prior works that optimize both placement and migration, we combine prior techniques to create four ex- tended data-management techniques (see 3). Our evaluation shows that these extended techniques underperform because they are naive combinations of prior data-placement and data-migration ap- proaches that are oblivious to each other s decisions. We conclude that for high HSS performance, both data placement and migration policies need to coordinate with each other. Our goal is to design a holistic data-management technique for HSS that optimizes both data-placement and data-migration policies in a coordinated way to improve overall system perfor- mance. To this end, we propose Harmonia,1 a multi-agent online reinforcement learning (RL) based holistic approach to co-optimize HSS data placement and data migration. Need for multi-agent RL. We describe the benefits of RL for HSS data management in 3.1. Despite these benefits, a single RL agent 1Harmonia [74]: Greek goddess of harmony and balance, who brings peace to conflicts. 1 arXiv:2503.20507v2 [cs.AR] 22 Apr 2025 cannot effectively (1) co-optimize [75 78] different objectives like data placement and data migration, and (2) perform both tasks concurrently. Our motivational study demonstrates that a single- agent RL approach that performs both data placement and data migration leads to 18.8 (27.7 ) lower average performance than Sibyl, which performs only data placement, on a performance- (cost-) optimized HSS (see 3.4). This is because learning to optimize data migration interferes with the process of learning an optimal data placement policy as they are two different tasks (See 3.2). Key Idea. Harmonia employs two coordinating lightweight au- tonomous RL agents, respectively for data placement and migra- tion, to improve HSS performance. First, we design Harmonia s data-placement agent by extending the state-of-the-art placement technique, Sibyl [29]. To coordinate with the migration agent, we add new state features and improve reward structure for the place- ment agent. Second, we design Harmonia s data-migration agent to identify migration candidates and their target devices by con- tinuously monitoring previously-placed pages in HSS. Harmonia assigns rewards to the migration agent based on the impact of its mi- grations on future HSS performance. The two agents influence each other s decisions through actions that affect the HSS environment. We evaluate Harmonia on a real Linux system using seventeen data-intensive workloads and various HSS configurations with up to four devices. Compared to the best-performing prior approach, Sibyl [29], on average, Harmonia provides (1) 49.5 (31.7 ) per- formance improvement on a performance- (cost-) optimized HSS, and (2) 37.0 (42.0 ) higher performance on an HSS with three (four) devices. Harmonia incurs low latency (240洧녵洧맍or inference) and storage overheads (206 KiB for both RL agents together). This work makes the following key contributions: We show that prior data-management approaches have low HSS performance because they do not optimize both data placement and data migration, and we quantify this on a real Linux system with various HSS configurations. We propose Harmonia, a multi-agent RL-based data-management technique for HSS that co-optimizes data-placement and data- migration policies using two lightweight coordinating RL agents to improve HSS performance. Via a rigorous real-system evaluation of Harmonia on various HSS configurations, we demonstrate that Harmonia consistently outperforms state-of-the-art data-management techniques across a wide range of data-intensive workloads. 2 Background 2.1 Hybrid Storage Systems Fig. 1 presents an overview of a hybrid storage system (HSS). An HSS has two key components: (1) multiple storage devices of vary- ing characteristics (e.g., I O latency, I O bandwidth, and device capacity) and storage protocols (e.g., NVM Express (NVMe) [79] or SATA [80]), and (2) an HSS management layer that orchestrates data placement and data migration in the HSS. A typical HSS combines fast storage devices with lower capacities (i.e., high-end device in Fig. 1) and slower storage devices with larger capacities (i.e., mid-range and low-end devices in Fig. 1). HSS Management Layer. The HSS management layer has three key functions. First, it (1) provides a unified flat virtual address High-End Device Mid-Range Device Low-End Device Read Write Unified Virtual Address Space Application File System Hybrid Storage System (HSS) Data Placement Data Migration HSS Management Layer Data Migration Read Write Read Write Read Write Read Write Figure 1: Overview of a hybrid storage system. space for the applications to store data across all HSS devices, (2) maps a virtual address to a logical block address (LBA) in one of the underlying storage devices, and (3) translates application I O requests to the devices using NVMe [79] or SATA [80] protocols. The HSS management layer is similar to the md [81] feature in Linux, which provides software RAID functionality (RAID 0) when multiple storage devices are present. Second, it (1) places data (i.e., data placement) in the best-fit storage device for each incoming I O request, and (2) rearranges data (i.e., data migration) across storage devices. Prior works (e.g., [13, 29, 31]) demonstrate that data-placement and migration policies have a significant impact on HSS performance. Third, the HSS management layer stores the following metadata in the host DRAM: (1) virtual address to LBA mapping, (2) I O request characteristics (e.g., request type, request size), and (3) storage device characteristics (e.g., device capacities and their current utilization). The HSS management layer is typically implemented in the kernel space of the host OS. 2.2 Reinforcement Learning Reinforcement learning (RL) [82] is a machine learning (ML) ap- proach in which an agent learns the optimal policy for a specific objective by interacting with its environment. Fig. 2 presents the overview of an RL system. Environment Agent Action (at) Reward (rt 1) State (st) Figure 2: Overview of reinforcement learning. An RL approach is formulated using four key components. 1. State is the representation of the environment. Assuming 洧녡is the set of all possible states, 洧멇롐 洧녡is the state at time step 洧노. 2. Action. At each time step, the agent observes the current state 洧멇롐며nd performs an action 洧녩洧노from the set of all actions 洧냢. Based on the action, the environment transitions to a new state 洧멇롐 1. 3. Reward. In response to 洧녩洧노that changes environment state from 洧멇롐몂o 洧멇롐 1, the agent receives a numerical reward 洧洧노 1. 4. Policy. An agent uses policy 洧랢to determine its actions in a given state and aims to find the policy that maximizes cumulative 2 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS rewards. The optimal policy 洧랢 is determined by computing the optimal action-value function 洧녟 , also called the Q-value. The Q- value of a state-action pair, denoted as洧녟(洧녡,洧냢), reflects the predicted cumulative reward obtained by taking action A in state S. The agent makes decisions and learns to maximize the overall cumulative reward or return over time. While the agent may not maximize the benefit of each action, it aims to maximize the long- term consequences of its actions. 2.3 Multi-Agent Reinforcement Learning Multi-agent RL (e.g., [83 93]) is an extension of traditional RL where multiple RL agents interact with a shared environment and and with one another to learn optimal policies. Unlike RL with a single agent, multi-agent RL can effectively optimize multiple different tasks. A single RL agent cannot effectively learn optimal policies for different tasks because it relies on task similarity [94, 95] to transfer the learning gained from one task to another. Task similarity refers to the resemblance of tasks in their inputs, objectives, and actions. Prior works [75 78, 94, 95] show that a single RL agent cannot effectively learn optimal policies for multiple tasks with low task similarity because transferring the learning from one task to a dissimilar task can inhibit performance. 3 Motivation In this section, we describe (1) why RL is a good fit for HSS data management, (2) the need for a multi-agent RL approach, and (3) prior approaches to HSS data management and their effectiveness. 3.1 Why RL for Data Management in HSS? RL is a good fit for HSS data management for three key reasons. First, unlike heuristic and supervised-learning approaches, an RL approach can learn online without prior training using labeled data. Online learning helps the RL agent learn dynamic changes in workload access patterns and system conditions, and continuously adapt the data-placement and data-migration policies using system- level feedback without human intervention. Prior work Sibyl [29] shows that RL outperforms heuristic-(e.g., [13, 23]) and supervised- learning-based (e.g., [30, 33]) approaches for HSS data placement. However, Sibyl partially uses RL for HSS data-management (i.e., RL for data-placement and a heuristic approach for data migration) that leaves a large gap between its performance and that of an Oracle approach (as we discuss in 3.4). Second, RL can handle complex tasks such as placement and migration where each action changes the environment (e.g., de- vice capacity utilization, bandwidth utilization, garbage collection) and impacts future outcomes (e.g., I O request latencies). Unlike supervised-learning approaches, RL learns strategies that maximize long-term returns (e.g., [82, 96, 97]), i.e., overall HSS performance. Third, RL offers ease of extensibility to a wide range of HSS configurations and devices with heterogeneous characteristics with minimum designer effort (as partially demonstrated in Sibyl [29]). 3.2 Need for Multi-Agent RL for HSS Data placement and data migration are two different tasks in HSS, and a multi-agent RL approach is a great fit for optimizing these different tasks (see 2.3). Data placement and data migration in HSS are two different tasks for three key reasons. First, they operate on different data in the HSS. While data placement writes data from the current I O request to the best-fit storage device, data migration monitors previously placed data to identify candidate pages for migration and their target storage devices. Second, their objectives are differ- ent. The objective of data placement is to minimize the I O latency for each incoming request, but data migration aims to improve long-term HSS performance. Third, these two tasks are performed asynchronously with each other. Data placement is on the critical path of I O handling and is performed synchronously with applica- tion writes or updates. Data migration is a background process and it is performed asynchronously to the application s I O requests. To describe the need for data migration in addition to data place- ment, we provide two examples: (1) If a page in the fast device is rarely accessed, migrating it to the slow device can free up space for more frequently-accessed pages and improve overall HSS perfor- mance. (2) If a page on the slow device is accessed frequently (due to access pattern changes), it should be migrated to the fast device to reduce its access latency. In 3.4, we quantitatively demonstrate that a single RL agent (SAPM) shows limited performance when handling both data placement and migration in HSS. We conclude that multi-agent is a great fit for optimizing both data placement and migration in HSS, which are complex and different tasks. 3.3 Prior Approaches to HSS Data Management Our focus in this work is to co-optimize placement and migra- tion in HSS. In this section, we briefly introduce prior HSS data- management techniques. However, there is no existing technique that optimizes both placement and migration in HSS. We discuss different approaches to optimize both placement and migration by combining (or extending) prior HSS data-management techniques. Data-Placement Techniques. We evaluate two state-of-the-art HSS data-placement techniques: CDE (Cold Data Eviction) [13] and Sibyl [29]. CDE is a heuristic data-placement technique that places hot or random (cold or sequential) data in the fast (slow) device, by determining the hotness and randomness of incoming data based on past access frequency and request size, respectively. Sibyl [29] uses an RL agent to place data in the best-fit HSS storage device based on multiple features of I O requests (e.g., request type and size) and HSS conditions (e.g., device capacity, I O latency). When the fast device becomes full, both techniques evict the least-recently-used (LRU) data before placing new I O request data in the fast device. Data-Migration Techniques. We evaluate two state-of-the-art HSS data-migration techniques, K-SVM [33] and RNN-HSS [30], which have three aspects in common. First, they both leverage supervised learning to identify the target device for migration of each stored page within the HSS based on the page s access fre- quency. Second, they periodically determine migration candidates (i.e., pages) and perform data migration at fixed intervals (e.g., mi- gration is performed after every 1000 incoming requests) on the critical path of I O request handling. Third, they perform data place- ment for incoming I O requests using a fixed heuristic policy (e.g., K-SVM places incoming requests in the fast device until its capacity reaches 100 [33]). The key difference between the two techniques 3 is in the supervised-learning method used; K-SVM uses a k-means- assisted support vector machine (SVM) to classify stored pages as belonging to a storage device based on their past access frequencies; RNN-HSS uses recurrent neural networks (RNN) to predict a page s future access frequency, and migrate the page to its target device. Extended Data-Management Techniques. Since there is no ex- isting technique that optimizes both data placement and data migra- tion together, we evaluate four extended techniques to explore the design space for a holistic data-management technique. For the first two techniques, Sibyl K-SVM and Sibyl RNN-HSS, we simply com- bine Sibyl, the current best-performing data-placement technique, with the prior data-migration techniques explained above. We design the third extended data-management technique, CDE RL-Migr,2 by combining CDE, a heuristic data-placement technique, with an RL-based data-migration approach. The RL- based data-migration approach uses the same design as the data- migration agent proposed in Harmonia, including the state features and the reward structure (See 4.2). In the fourth extended technique, SAPM (Single Agent for Place- ment and Migration), we design a single RL agent to perform both data placement and data migration to analyze the effectiveness of a single RL agent at HSS data management. SAPM performs a place- ment action for every incoming I O request, and focuses on data migration only during system idle times to avoid interference with the placement task (which is on the critical path of I O request han- dling). We provide the same state features used in Harmonia (See 4.2) to SAPM and tune the hyperparameters to achieve the best performance possible with a single RL agent. We provide SAPM s RL agent (1) an immediate reward for every placement action (based on the reward of Harmonia s data-placement agent in Equation 2), and (2) a deferred reward for the migration actions (based on the reward structure of Harmonia s data-migration agent in Equation 3). We provide background in 2.3 on why a single RL agent cannot effectively optimize two different tasks. 3.4 Effectiveness of Prior Techniques Methodology. We evaluate eight prior data-management tech- niques on a real system with two HSS configurations, performance- and cost-optimized, on 17 data-intensive workloads from 3 bench- mark suites (see Table 4). See 5 for our evaluation methodology. Performance Results. Fig. 3 shows the performance of four prior and four extended HSS data-management techniques on (a) performance-optimized and (b) cost-optimized HSS. We compare their average request latencies against two ideal approaches, Fast- Only and Oracle. First, in Fast-Only, we assume that the fast storage device is large enough to accommodate the entire workload dataset. Second, Oracle [98] makes optimal data-placement and migration decisions based on complete knowledge of future I O access pat- terns of the entire workload. We assume that Oracle performs data migration during system idle times, which does not add any latency overheads. All values in Fig. 3 are normalized to Fast-Only. We make five key observations from Fig. 3. First, all prior tech- niques exhibit significant performance gaps from Oracle for every tested HSS and workload. The best-performing technique, Sibyl, 2The RL-based data-migration technique in CDE RL-Migr is a contribution of this work. We design this as there are no prior RL-based HSS data-migration techniques. 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Oracle 0 30 60 90 120 SYSTOR17 RocksDB YCSB MLPerf AVG (b) Cost-Optimized HSS (a) Performance-Optimized HSS Normalized Avg. Request Latency Figure 3: Performance of CDE, K-SVM, RNN-HSS, Sibyl, Sibyl K-SVM, Sibyl RNN-HSS, CDE RL-Migr, SAPM and Or- acle on performance-optimized (top) and cost-optimized (bot- tom) HSS configurations. Performance is shown as average I O latency normalized to Fast-Only. Lower is better. only achieves 49.9 (41.4 ) of Oracle s performance in performance- (cost-) optimized HSS, which strongly implies the need for holistic data management in HSS. Sibyl s RL-based data-placement tech- nique can adapt to workload and system changes, but its heuristic data-eviction policy on the critical path lowers its performance. Second, Sibyl outperforms all other prior techniques, improv- ing the average performance over CDE, K-SVM, and RNN-HSS by 20.7 (14.5 ), 24.7 (31.6 ), and 19.3 (19.2 ), respectively, in the performance- (cost-) optimized HSS. Sibyl outperforms CDE, K-SVM and RNN-HSS because they use heuristic- or supervised- learning-based approaches, which cannot easily adapt to changes in workload access patterns and HSS conditions. Third, Sibyl K-SVM and Sibyl RNN-HSS underperform com- pared to Sibyl. A naive combination of prior approaches performs poorly as the placement and migration policies are oblivious to each other s decisions. This leads to (1) conflicting decisions from the two policies, and (2) delayed data-placement policy convergence. Fourth, CDE RL-Migr shows lower performance than Sibyl by 4.6 (15.6 ) on average in performance- (cost-) optimized HSS. CDE RL-Migr shows comparable performance to Sibyl in read- intensive workloads (i.e., RocksDB, MLPerf) because the RL-based migration agent proactively prefetches frequently-read data to the fast device, which Sibyl does not do. However, it underperforms in write-intensive workloads (i.e., SYSTOR17) because CDE s heuris- tic data-placement policy places most incoming data in the fast storage device without considering device characteristics, result- ing in frequent migrations. CDE RL-Migr s performance suggests that an RL-based data-migration policy can benefit performance if combined with an adaptive placement policy. Fifth, SAPM shows lower performance compared to Sibyl by 18.8 (27.7 ) on average in the performance- (cost-) optimized HSS, even though SAPM uses RL for both data placement and migration. SAPM s limited performance is due to two key reasons. (1) SAPM s single RL agent is unable to learn an optimal policy for data placement and data migration concurrently, and (2) SAPM cannot identify migration candidates in a timely manner because it focuses on the data-migration task only when there are no incoming I O requests. We provide a detailed analysis on why a single RL agent cannot optimize two different tasks in 3.2. 4 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS 3.5 Our Goal Based on our observations and analyses in 3.1, 3.2, 3.3, and 3.4, we conclude that a multi-agent RL approach s ability to optimize different tasks concurrently alleviates the limitations of a single- agent RL approach. Our goal is to design a holistic HSS data-management tech- nique that optimizes both data-placement and data-migration poli- cies using a multi-agent RL approach to improve the overall HSS performance. 4 Harmonia We design Harmonia as an online multi-agent RL technique for HSS data management with two autonomous RL agents: a data- placement agent and a data-migration agent. We implement Har- monia in the HSS management layer in the host operating system. In this section, we first describe the challenges of designing a multi- agent RL-based technique for HSS data management, and then explain Harmonia s design and implementation in detail. 4.1 Challenges of Designing a Multi-Agent HSS Technique Designing a multi-agent RL-based HSS data-management tech- nique poses three key challenges. First, we need to design two autonomous RL agents to optimize the different tasks of data place- ment and migration (see 3.2). Second, we need to formulate the reward structures of the two RL agents to enable coordination be- tween them (see 4.3) and improve HSS performance. Third, having multiple RL agents can increase computational complexity and storage overhead (see 4.6). While multi-agent RL itself is not a fun- damentally new approach to system optimization, prior techniques (e.g., [84 93]) cannot be adapted directly for HSS data management because they use (1) identical RL agents with a single objective, and (2) joint-action space and shared reward functions for all RL agents. 4.2 Harmonia: RL Formulation Fig. 4 shows the overview of Harmonia s multi-agent RL framework. The key goal of the data-placement agent is to place incoming I O requests in the best-fit storage device, and that of the data-migration agent is to identify migration candidates from previously-placed pages and determine their target storage device. We design the state, action, and reward of the two agents to enable coordination between them and maximize the overall HSS performance. Data Placement Agent Action Reward (Immediate) State Data Migration Agent Action Environment Reward (Delayed) State Figure 4: RL Agents in Harmonia. State. Table 1 summarizes the state features for Harmonia s two RL agents. We design a common set of seven state features because both agents consider the current HSS conditions and data characteristics when making their decisions. The state features are represented as a 7-dimensional tuple (observation vector) for both agents: 洧녝洧노 (洧洧뉧롐_洧멇롐뒳롐洧뉧롐,洧洧뉧롐_洧노洧녽洧녷洧뉧롐,洧녩洧녫洧녫_洧녰洧녵洧노洧洧노,洧녩洧녫洧녫_洧녭洧洧뉧롐륋롐, 洧녭洧녩洧멇롐_洧녫洧녩洧녷洧노,洧녫洧녹洧洧_洧녬洧뉧롐洧노,洧녴洧녰洧녮洧_洧녰洧녵洧노洧洧노). (1) The observation vector size directly impacts Harmonia s storage and computational overheads (see 4.6), so we (1) carefully select a limited set of highly relevant features, and (2) group state feature values into a small number of bins (Table 1) based on workload characterization. For example, request size can fall into eight size classes (e.g., 4 KiB, 8 KiB, 16 KiB) requiring three bits to represent. In total, we use 32 bits for encoding the state representation. Table 1: State features used by Harmonia Feature Description of bins Encoding (bits) 洧洧뉧롐_洧노洧녽洧녷洧 Request type (read write) 2 1 洧洧뉧롐_洧멇롐뒳롐洧 Request size (in pages) 8 3 洧녩洧녫洧녫_洧녰洧녵洧노洧 Access interval of the requested page 64 8 洧녩洧녫洧녫_洧녭洧洧뉧롐 Access frequency of the requested page 64 8 洧녭洧녩洧멇롐_洧녫洧녩洧녷 Free space in the fast storage device 8 3 洧녫洧녹洧洧_洧녬洧뉧롐 Storage device where the requested page currently resides 2 1 洧녴洧녰洧녮洧_洧녰洧녵洧노洧 Migration interval of a page 64 8 Action. The action space of Harmonia s agents depends on the number of devices in the HSS. The data-placement agent receives the state features of incoming write requests, and its action is to de- termine the best-fit device to place request data. The data-migration agent continuously looks at the state features of previously-placed pages to determine their target device for migration. Therefore, the two agents do not perform their actions in synchronization. In an HSS with two devices, (1) the data-placement agent can place data in the fast or the slow device, and (2) the data-migration agent can migrate data to the fast or the slow device. The action sets of both agents are small and extensible, resulting in less computational complexity and easier multi-device scalability. We encode actions with 4 bits, allowing extensibility to more storage devices. Reward. The reward structure is key to finding an optimal policy in RL. We design the reward structures of Harmonia s agents based on two key factors. First, since we focus on HSS performance, we use I O request latencies in our reward structure for both agents. I O request latencies capture the internal state (e.g., queueing de- lays, buffer dependencies, bandwidth utilization, effects of garbage collection, read write latencies, non-blocking operations, error han- dling latencies) of storage devices in the HSS. Second, we design the reward structures to enable coordination between the two agents. For every placement action at time step 洧노, the data-placement agent receives a reward 洧녠洧녷洧녳洧녩洧녫洧뉧롐뛿롐뉧롐洧노from the environment at time step 洧노 1 that is inversely proportional to the I O request latency: 洧녠洧녷洧녳洧녩洧녫洧뉧롐뛿롐뉧롐洧노 1 洧洧노 (2) where 洧洧노is I O request latency at time step 洧노. The reward is higher (lower) if the data is placed in the fast (slow) device. We design a 5 simple reward for the data-placement agent because its objective is to minimize the latency of the ongoing I O request. The data-migration agent s objective is to maximize long-term HSS performance, which is different from the placement agent s objective (see 3.2). Unlike the immediate impact of data placement on HSS performance, the impact of data migrations (i.e., prefetching frequently-accessed data to the fast device and evicting cold data to the slow device) may be delayed due to two key reasons: (1) the application may not access the prefetched data immediately, and (2) the application may not immediately leverage the free space created in the fast device by cold data migration. Hence, we provide a delayed reward to the migration agent to capture the long-term impact of its decisions. The reward for the migration agent is: 洧녠洧녴洧녰洧녮洧_洧녬洧뉧롐뙗롐뀛롐뷣롐뉧롐 洧녵 칈洧노 洧녵 洧녰 洧노 1 洧洧녰 洧녞洧녴洧녰洧녮洧洧노 after migrating 洧논pages 0 otherwise (3) where 洧노is the current time step, 洧녵is the number of incoming I O requests whose data-placement latencies are considered, 洧洧녰is the latency of I O request 洧녰, 洧녞洧녴洧녰洧녮洧洧노is a penalty based on migra- tion and access intervals, and 洧논is the number of pages migrated before providing a reward. This reward is based on the average data- placement latencies (i.e., 칈 洧洧녰 洧녵) for 洧녵I O requests that arrive after 洧논 migration candidates are moved to their target devices, and serves as feedback for the migration decisions made previously. To avoid repeated migrations of the same page (ping-pong migrations), we add a small penalty 洧녞洧녴洧녰洧녮洧洧노whose value is inversely proportional to the average migration and access intervals (See Table 1) of 洧논pages. Based on empirical analysis (see 6.1), we set the value of 洧녵to 50 and 洧논to 10, the size of the migration queue. We use a half-precision floating-point (16-bit) representation for the reward structure of both agents to reduce the storage overhead of training data. 4.3 Coordination Between Harmonia s Agents We achieve coordination between the two agents through (1) their rewards, and (2) the effects of their actions on the HSS environment. The placement agent s reward (See Equation 2) encourages effective utilization of the fast device, which is possible only if the migration agent proactively migrates cold (hot) data to the slow (fast) device. The data-migration agent receives a delayed reward (See Equation 3) that is based on access latencies of 洧녵incoming I O requests after migrating several candidate pages. In this way, each agent s decisions influence the policy learned by the other agent. Mitigating Conflicting Decisions. To prevent conflicting place- ment and migration decisions, we use two key techniques. First, the migration agent prioritizes the monitoring of pages with high access and migration intervals, and ignores recently placed or migrated pages (e.g., if the page s access interval is 1, i.e., it was recently placed). Second, we add a small penalty based on access and migra- tion intervals to the migration agent s reward (See Equation 3) to discourage frequent page movements. 4.4 Harmonia: Design Fig. 5 shows the key components of Harmonia. We describe these key components below. I O Request from Application Placement in Fast or Slow Device in HSS Hybrid Storage System Fast Device Slow Device Data previously stored in HSS Candidate pages to migrate Migration Queue Coordination between the agents Harmonia Data-Placement Agent Training Thread Inference Thread Data-Migration Agent Training Thread Inference Thread Migration Thread Perform background migration during idle time Figure 5: Overview of Harmonia. Data-Placement Agent 1 . An RL agent that places I O request data in the best-fit storage device in the HSS 2 based on I O re- quest characteristics and HSS conditions. This agent consists of two threads. First, the data-placement training thread 3 is a background process in which the agent is trained. Second, the data-placement inference thread 4 is Harmonia s only foreground thread. In this main decision thread, the data-placement agent decides the target storage device for incoming I O request data on the critical path. Data-Migration Agent 5 . We implement the data-migration agent using two background threads to avoid interference with the I O request handling on the critical path. First, the data-migration training thread 6 is used for training the agent. Second, in the data-migration inference thread 7 , the agent performs three key tasks: (i) continuously monitors previously-placed pages in the HSS. (ii) determines migration candidates and their target devices. (iii) pushes the metadata of the migration candidates into the migration queue 8 . A page is not a migration candidate if the target device determined by the migration agent is the same as its current device. Migration Queue 8 . A queue that stores the logical page address and the target device identifier (e.g., 0 for slow device) of each migration candidate identified by the migration agent. Based on empirical analysis (see 6.1), we set the migration queue size to 10 in our experiments to migrate pages in a timely manner during system idle times and avoid stale migration candidates. Migration Thread 9 . A background process that continuously monitors the migration queue and migrates pages across devices during system idle times. Harmonia samples device bandwidth at frequent intervals using real-time tools (e.g., iostat) to perform migrations. For high-intensity workloads with short system idle times, the migration thread checks if pages related to incoming read requests are already in the migration queue and issues low-priority writes [79, 80] to immediately schedule them for migration. Harmonia s five-threaded design prevents the following back- ground processes from interfering with data placement of incoming I O requests: (1) agent training, (2) migration candidate identifica- tion, and (3) data migration across storage devices in the HSS. 4.5 Design of Harmonia s RL Agents Fig. 6 shows the detailed design of each RL agent. Both agents in Harmonia have a similar design using two threads (training and inference) for ease of implementation and low storage overhead. Inference Thread. Harmonia s RL agents use the inference thread to make decisions while collecting experiences to use as training data. In this thread, the agent (1) observes the input features (state) 6 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS Max Inference Network Policy Experience Training Dataset Periodic Weight Update Inference Thread RL Agent in Harmonia Training Thread Training Network State Action Reward Action Experience Replay Experience Buffer (stored in DRAM) a c e HSS d c f g h i b Figure 6: Design of RL Agent in Harmonia. a from the I O request data and HSS, (2) performs inference using its inference network b , (3) selects the action c that maximizes d its long-term returns, and (4) collects the state, action and reward e as experiences f in an experience buffer g . Training Thread. In this background thread, Harmonia uses col- lected experiences h from the experience buffer to train the training network i and optimize the placement or migration policy. Experience Buffer. Experiences are tuples of 洧녡洧노洧녩洧노洧,洧냢洧녫洧노洧녰洧녶洧녵, 洧녠洧뉧롐벓롐뀛롐洧녬, 洧녜洧뉧롐봻롐뫯롐洧노洧녩洧노洧 collected by the inference thread of each agent and stored in an experience buffer to use as training data. We use separate experience buffers for the two agents because they have different goals and experiences. Based on empirical analysis, we set the experience buffer size to store the most recent 1000 experiences. A larger experience buffer can capture more access patterns but comes with significant storage overhead. The experience buffers reside in the host DRAM. Harmonia uses experience replay [29, 99], where a batch of experiences is randomly sampled from the experience buffer and used as training data for the RL agent. Training and Inference networks. We separate the RL agent s training and inference networks to allow parallel execution of train- ing and inference, and remove training latency overhead from I O request handling. Training network weights are periodically copied to the inference network (after every 1000 requests) for the infer- ence network to adapt to current workload and HSS conditions. For the training and inference networks of both agents, we use an identical simple feed-forward neural network structure [29] with one hidden layer of 10 neurons [100] that uses use the swish activa- tion function [101]. Harmonia s data-placement and data-migration policies select the action with the maximum predicted Q-value. RL Algorithm. We use C51, a categorical Deep Q-Network [102] for both data-placement and data-migration agents to update the Q-values 洧녟(洧녡,洧냢) (See 2). C51 aims to learn the distribution of Q-values for each state-action pair, which helps Harmonia capture more information from the environment [29, 103]. Hyper-Parameter Tuning. We improve Harmonia s accuracy by tuning the hyper-parameters [104] of each agent using cross- validation [105] of different values. Harmonia s agents need only one-time offline hyper-parameter tuning. Table 2 shows the hyper- parameter values chosen based on empirical analysis. The discount factor (洧) determines the balance between imme- diate and future rewards, while the learning rate (洧띺) is the rate at which neural network weights are updated. The exploration rate (洧랬) balances exploration and exploitation for Harmonia s policies. Batch size determines the number of samples processed in each training iteration. Experience buffer size is the number of most recent experiences stored for training the networks. Table 2: Hyper-parameters Hyper Parameter Design Space Placement Agent Migration Agent Discount Factor (洧) 0-1 0.9 0.1 Learning Rate (洧띺) 1洧 5 1洧0 1洧 3 1洧 2 Exploration Rate (洧랬) 0-1 0.001 0.001 Batch Size 64-256 128 256 Experience Buffer Size 10-10000 1000 1000 Exploration vs. Exploitation. Harmonia s RL agents start with no prior knowledge of the workload or HSS, and make random initial decisions (explore), and use their experiences (exploit) to gradually make optimal decisions. To balance exploration and exploitation, we use the 洧랬-greedy policy [106]: the predicted best action is selected with (1-洧랬) probability, and a random action with 洧랬probability. Convergence of RL Agents in Harmonia. Figs 7(a) and 7(b) show the convergence of Harmonia s data-placement and data-migration policies in terms of reduction in training loss for the first 10000 I O requests in YCSB-B (see Table 4) on a performance-optimized HSS. 0 1 2 3 4 5 0 2 4 6 8 10 0 2 4 6 8 10 Training Loss Number of I O Requests (in thousands) (a) Data-Placement Agent (b) Data-Migration Agent Figure 7: Convergence of Harmonia s (a) Data-Placement and (b) Data-Migration RL agents shown in terms of reduction in training loss. We show only the first 10000 I O requests. We make two key observations: (1) The placement policy con- verges in less than 6000 I O requests without prior training. (2) The migration policy takes longer to converge (around 9000 I O requests) due to indirect and delayed reward to the migration agent. The convergence of Harmonia s policies indicates that the agents learn to coordinate their actions and make optimal decisions. 4.6 Overhead Analysis In this section, we provide an analysis of Harmonia s overhead in terms of (1) inference and training latencies, (2) storage overhead. 4.6.1 Latency Overhead. Harmonia has low inference and training latencies because of its (1) lightweight RL agent design, (2) sepa- ration of training and inference, and (3) background execution of multiple threads (e.g., data-migration inference thread). Harmonia s two RL agents have identical networks. We perform training and inference on host CPU as the neural networks are lightweight, and their weights fit in the CPU on-chip caches in our evaluated system. Inference latency. In Harmonia, only the placement agent s infer- ence latency affects I O latency because it is on the critical path of I O request handling. The migration agent runs in the background and its inference latency does not impact ongoing requests. Both agents inference networks have an input layer with 7 neurons (for 7 the state features listed in Table 1), one 10-neuron hidden layer and an output layer with 2 neurons representing the agent s actions (for HSS with two storage devices). Each inference requires 90 MAC operations (7 10 10 2). On our evaluated system (see Table 3), these MAC operations consume 90 CPU cycles (240洧녵洧) per core, lower than the I O read latency of a high-end SSD ( 3洧랞s) [47, 49]. Training latency. In Harmonia, RL agent training is separated from inference and executed in the background. Hence, training latency does not impact I O request latency. Harmonia computes 184,320 MAC operations for each training step. We use 16 batches per training step, where each batch of 128 training samples requires (128 7 10 128 10 2) MAC operations. This computation takes 53洧랞s on our evaluated system with 200,000 cycles per core. 4.6.2 Storage Overhead. Harmonia s storage overhead includes (1) RL agents neural networks, (2) experience buffers, (3) migration queue, (4) migration agent s reward, and (5) address mapping in- formation. First, neural network weights are represented using a half-precision floating-point format. With 90 16-bit weights, each network requires 1.5 KiB of memory. The four networks (training and inference networks for two agents) take a negligible 6 KiB of memory, so we store their weights in the CPU caches. Second, to train each RL agent, we use an experience buffer located in the DRAM that consumes 100 KiB to store 1000 experiences. Hence, the total storage overhead of two experience buffers is 200 KiB. Overall, Harmonia s RL agents require 206 KiB of memory. Third, the migra- tion queue holds the logical block address (32 bits) and target device ID (4 bits) for each queue entry. The migration queue size is 10 in our experiments, requiring 50 bytes of DRAM. Fourth, we store the latencies of 50 incoming I O requests temporarily to assign a delayed reward to the migration agent, which consumes 100B. Fifth, we maintain address mapping information for HSS devices [107]. Harmonia requires 32 bits to store the state features for each page (see Table 1). This overhead is less than 0.8 of the total storage capacity when using a 4-KiB data placement granularity. 5 Evaluation Methodology We evaluate Harmonia on a real system with an HSS with two (dual-HSS), three (tri-HSS), and four (quad-HSS) devices. The dual- HSS configurations include: (1) performance-optimized HSS, which combines a high-end SSD [47] with a mid-range SSD [48], and (2) cost-optimized HSS, which pairs the high-end SSD [47] with a low-end HDD [108]. We implement a custom block device driver in Linux for Harmonia to interface with HSS devices. We implement Harmonia using the TF-Agents [109] library. Table 3 describes our real system and the HSS configurations. Baselines. We compare Harmonia against four state-of-the-art heuristic- and machine-learning-based data-placement and migra- tion techniques, (1) CDE [13], (2) RNN-HSS (adapted from [30]), (3) K-SVM [33], and (4) Sibyl [29]. We evaluate four extended ap- proaches, Sibyl K-SVM, Sibyl RNN-HSS, CDE RL-Migr and SAPM to explore the design space of holistic data-management techniques. We also compare against two ideal approaches: (1) Fast-Only, and (2) Oracle [98] (See 3). Oracle serves as a reference to evaluate the accuracy of Harmonia s decisions as it makes optimal decisions based on knowledge of future access patterns. Table 3: Host System and HSS Configurations Host System AMD Ryzen 7 2700G[110], GHz, 8 64 32 KiB L1-I D, 4 MiB L2, 8 MiB L3, 16 GiB RDIMM DDR4 2666 MHz Storage Devices Characteristics H: Intel Optane SSD P4800X [47] 375 GB, PCIe 3.0 NVMe, SLC, R W: 2.4 2 GB s, random R W: 550000 500000 IOPS M: Intel SSD D3-S4510 [48] 1.92 TB, SATA TLC (3D), R W: 560 510 MB s, random R W: 895000 21000 IOPS L: Seagate HDD ST1000DM010 [108] 1 TB, SATA 6Gb s 7200 RPM Max. Sustained Transfer Rate: 210 MB s LSSD: ADATA SU630 SSD [111] 960 GB, SATA, TLC, R W: 520 450 MB s PMEM (Emulated): Intel Optane Persistent Memory 200 Series [112] 128 GB, Memory Mode R W: 7.45 2.25 GB s (256B) HSS Configurations Devices Performance-Optimized high-end (H) middle-end (M) Cost-Optimized high-end (H) low-end (L) HSS with PMEM Emulated PMEM (PMEM) high-end SSD (H) Tri-HSS high-end (H) middle-end (M) low-end (L) Quad-HSS high-end (H) middle-end (M) low-end SSD (LSSD) low-end HDD (L) Workloads. We select seventeen data-intensive storage workloads from SYSTOR 17 [113], RocksDB traces [114], Yahoo! Cloud Serv- ing Benchmark (YCSB) suite [115], and MLPerf Storage3 [116] from real enterprise and datacenter environments. The average workload size is approximately 50000x the fast device capacity. We choose these workloads to represent diverse I O access patterns with dif- ferent read and write ratios, I O request sizes, and inter-request times. Table 4 reports the characteristics of the chosen workloads. In our evaluation, each workload runs in a separate thread. Table 4: Characteristics of the evaluated I O traces Benchmark Suite Traces Read Avg. Request Size (KB) Avg. Inter- Request Time (洧랞s) SYSTOR17 [113] LUN0 0.2 31.7 1163.9 LUN1 0.3 34.2 1864.1 LUN2 7.6 31.1 1418.9 LUN3 3.5 42.7 734.5 LUN4 0.5 26.3 823.1 RocksDB [114] ssd-00 79.9 108.9 66.4 ssd-01 73.5 75.1 40.7 ssd-02 79.9 7.5 3.3 ssd-03 79.9 9.5 3.5 ssd-04 79.9 7.8 3.6 YCSB [115] YCSB-B 51.3 45.9 9.3 YCSB-C 47.6 54.6 6.5 YCSB-D 55.9 36.1 8.5 YCSB-E 52.1 46.6 9.6 YCSB-F 49.5 53.1 6.6 MLPerf Storage [116] ResNet50 80.0 172.6 500.1 CosmoFlow 83.4 180.1 1023.8 To evaluate Harmonia under real-world scenarios, we generate six multi-programmed workloads by running multiple workloads concurrently (see Table 5). We choose these multi-programmed workloads based on three key factors: (1) combination of read- intensive, write-intensive and mixed workloads, (2) number of 3We generated the traces by running MLPerf applications on our evaluated system. 8 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS concurrent workloads required for high I O intensity (i.e., shorter inter-request times), and (3) combination of different request sizes. Each constituent workload is executed using a single thread. For example, mix6 runs on eight concurrent threads for the eight con- stituent workloads. Multi-programmed workloads typically exhibit higher I O request intensity and varied access patterns. Table 5: Characteristics of multi-programmed workloads Mix Constituent Workloads [113 115] Description mix1 ssd-02 and LUN4 ssd-02 is read-intensive and LUN4 is write-intensive mix2 LUN1 and ssd-04 LUN0 is write-intensive and ssd-04 is read-intensive mix3 YCSB-C and YCSB-F Both have near-equal read-write ratio mix4 ssd-00, ssd-04, YCSB-A and LUN0 Two read-intensive and two write-intensive workloads mix5 ssd-00, LUN0, YCSB-C and YCSB-F Read-intensive, write-intensive and two workloads with a near-equal read-write ratio mix6 YCSB-B, YCSB-D, LUN0, LUN1, LUN4, ssd-00, ssd-02, ssd03 Two with near-equal read-write ratio, three write-intensive and three read-intensive 6 Evaluation 6.1 Performance Analysis Average I O Request Latency and End-to-End Throughput. Figs. 8(a) and 8(b) show the performance of CDE, K-SVM, RNN- HSS, Sibyl, Sibyl K-SVM, Sibyl RNN-HSS, CDE RL-Migr, SAPM, Harmonia and Oracle on performance- and cost-optimized HSS configurations. Figs. 9(a) and 9(b) show their end-to-end throughput on performance- and cost-optimized HSS, respectively. 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 30 60 90 120 SYSTOR17 RocksDB YCSB MLPerf AVG Normalized Avg. Request Latency (a) Performance-Optimized HSS (b) Cost-Optimized HSS Figure 8: Performance of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS configurations, shown as average request latency nor- malized to Fast-Only. Lower is better. We make four key observations. First, Harmonia consistently out- performs all baselines in both HSS configurations. In performance- (cost-) optimized HSS, Harmonia improves performance by 49.5 (31.7 ) and end-to-end throughput by 49.4 (156.2 ) over the best- performing prior approach, Sibyl, on average. Second, Harmonia bridges the performance gap between Sibyl and Oracle by 64.2 (64.3 ) and achieves 71.7 (62.2 ) of Oracle s throughput on average in performance- (cost-) optimized HSS. 0.0 0.2 0.4 0.6 0.8 1.0 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0.00 0.02 0.04 0.06 0.08 0.10 SYSTOR17 RocksDB YCSB MLPerf AVG Normalized End-to-End Throughput (a) Performance-Optimized HSS (b) Cost-Optimized HSS 0.20 0.48 0.16 Figure 9: End-to-end throughput of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS. Throughput values (in IOPS) are normalized to Fast- Only policy. Higher is better. Third, Harmonia shows higher performance and throughput benefits over baselines in read-intensive workloads (e.g., RocksDB, MLPerf) compared to write-intensive workloads. For example, Harmonia outperforms Sibyl by 57.5 (43.2 ) in read-intensive workloads and 41.0 (39.9 ) in write-intensive workloads. In read- intensive workloads, unlike Sibyl, Harmonia proactively prefetches frequently-read pages to the fast device to improve read perfor- mance even when there are no writes from the application, resulting in lower latencies and higher throughput. In write-intensive work- loads, while both Harmonia and Sibyl use RL-based placement, Harmonia s migration agent, in coordination with the placement agent, further improves performance by frequently migrating cold data from the fast device to accommodate new incoming requests. Fourth, Harmonia shows higher performance and throughput gains over baselines (other than Sibyl) in cost-optimized than in performance-optimized HSS. This is because (1) data migration in cost-optimized HSS is more expensive than in performance- optimized HSS due to the large latency gap between the two devices, (2) prior approaches perform migration during I O request handling, which adds significant latency overheads, and (3) Harmonia frees up space in the fast device with proactive migrations and performing migrations during system idle times. Sibyl s RL agent places only performance-critical data in the fast device, reducing the number of data evictions on the critical path. Tail Latency. Figs. 10(a) and 10(b) show the 99th and 99.99th per- centile I O latencies (tail latency) of Harmonia and baselines on performance-optimized HSS. For this analysis, we choose three representative workloads, LUN0 (write-intensive), ssd-00 (read- intensive) and YCSB-B (mixed) (see 5 for workload characteristics). We make two key observations. First, Harmonia significantly reduces tail latency compared to all baselines across the three work- loads. Harmonia reduces the 99th (99.99th) percentile I O latency by 32.3 (25.3 ) on average over Sibyl. Harmonia has lower tail latency because, unlike prior techniques, Harmonia performs data migration during system idle times to avoid interference with on- going I O requests. Second, K-SVM, RNN-HSS, Sibyl K-SVM and Sibyl RNN-HSS show very high tail latencies because they per- form a large volume of migration at fixed intervals, which adds a significant latency overhead to the ongoing I O requests. We conclude that Harmonia improves average I O latency, end- to-end throughput, and tail latency across all workloads due to its holistic and coordinated data-management approach. 9 Figure 10: Comparison of tail latencies of Harmonia and baselines in the 99th percentile (top) and 99.99th percentile (bottom) of I O requests from three representative workloads, LUN0 (write-intensive), ssd-00 (read-intensive) and YCSB-B (mixed), on performance-optimized HSS. Performance Sensitivity to Migration Queue Size. Fig. 11(a) shows the impact of migration queue size on HSS performance. We make three key observations. First, optimal migration queue size is 10 for our evaluated workloads. This may vary slightly across workloads and HSS configurations. Second, low queue depths (i.e., 5) may result in many migration candidates being dropped, which leads to performance degradation. If the migration queue size is 0, Harmonia shows poor performance because it does not perform any data migration. Third, larger-sized queues can hold more migration candidates, but the oldest candidates may become stale (i.e., past migration decisions may not correlate with current access patterns and HSS conditions), which adversely impacts HSS performance. Performance Sensitivity to Migration Agent s Reward. Fig. 11(b) shows the performance impact of the number of I O requests considered for the migration agent s delayed reward (See Equation 3). We make three key observations. First, the optimal Normalized Avg. I O Latency 25 27 29 31 33 35 0 50 100 150 200 250 Migration Queue Size (a) 0 50 100 150 200 250 Number of I O requests used for migration agent s reward (b) Figure 11: Performance sensitivity to (a) migration queue sizes, and (b) number of future incoming requests used in migration agent s reward structure. Performance is shown as average I O latency normalized to Fast-Only. Lower is better. number of requests is 50 in our evaluated configuration, but may vary slightly across workloads and HSS configurations, and depends on factors including (1) inter-request times of incoming I O requests after migration, (2) workload access patterns (e.g., access frequency and reuse distance of a page), and (3) system conditions (e.g., device capacity utilization). Second, considering fewer requests (e.g., 10) may not provide the right reward to the agent because the effect of migrations is not seen immediately in the placement latencies of these few requests. Third, if we consider more I O requests (e.g., 250), we see lower performance because of two reasons: (i) the impact of data migration is diluted if we consider a large number of incoming requests, and (ii) I O requests that arrive too far into the future after data migration may not see a direct impact. Performance on HSS with Persistent Memory (PMEM) De- vices. To demonstrate the Harmonia s extensibility to emerging persistent memory technologies, we evaluate Harmonia and base- lines on an HSS with a persistent memory device [112] and a high- end SSD [47] (see Table 3). We emulate the PMEM device using RAMDisk because these emerging devices are not easily available. Fig. 12 shows the performance of Harmonia and baselines on HSS with PMEM. Performance is shown as average request latency nor- malized to Fast-Only (i.e., PMEM device can store all data). 0 4 8 12 16 SYSTOR17 RocksDB YCSB MLPerf AVG CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle Normalized Avg. Request Latency Figure 12: Performance of Harmonia and baselines on an HSS with PMEM [112] and high-end SSD [47] shown as average request latency normalized to Fast-Only. Lower is better. We make three key observations. First, Harmonia outperforms the best-performing baseline, CDE, by 15.6 and achieves 80 of Or- acle s performance. Second, CDE outperforms other baselines in this HSS configuration as it places most data in the low-latency PMEM device. However, CDE s frequent data evictions on the critical path lower its performance compared to Harmonia. CDE performs better in read-intensive workloads because they consist of fewer writes and require fewer evictions. Third, in read-intensive workloads (i.e., RocksDB, MLPerf), Harmonia shows a higher performance improvement as it proactively prefetches data to the PMEM de- vice, which leads to more reads from the PMEM device. We discuss Harmonia s applicability to emerging memory technologies in 7. 6.2 Multi-Programmed Workloads Figs. 13(a) and 13(b) show the performance of Harmonia and base- lines on multi-programmed workloads (see Table 5) on performance- and cost-optimized HSS, respectively. We make two key observations. First, Harmonia outperforms prior approaches in all multi-programmed workloads across both HSS configurations. In performance- (cost-) optimized HSS, Har- monia (i) outperforms Sibyl by 32.8 (25.3 ), and (ii) achieves 75 (46 ) of Oracle s performance. Second, Harmonia s performance improves as the number of concurrent workloads increases. On eight concurrent workloads (e.g., mix6), Harmonia shows higher performance gains, 39.4 (41.8 ), over Sibyl compared to other multi-programmed workloads. Harmonia s RL-based policies learn access pattern variations faster than prior approaches. 10 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle (a) Performance-Optimized HSS 0 40 80 120 160 mix1 mix2 mix3 mix4 mix5 mix6 AVG (b) Cost-Optimized HSS Normalized Avg. Request Latency Figure 13: Performance of Harmonia and baselines on multi- programmed workloads on performance-optimized (top) and cost-optimized (bottom) HSS. Performance is shown as aver- age request latency normalized to Fast-Only. Lower is better. 6.3 Extensibility to Multiple Devices in HSS We evaluate Harmonia s extensibility to different HSS configura- tions by comparing its performance with the best-performing prior approach, Sibyl, on an HSS with three and four devices (see Table 3). We do not evaluate other baselines as they are designed for fixed HSS configurations and lack easy extensibility. Figs. 14(a) and 14(b) show the performance of Sibyl and Harmonia on a Tri- and Quad-HSS. We discuss Harmonia s extensibility in 7. 0 30 60 90 Sibyl Harmonia 0 30 60 90 SYSTOR17 RocksDB YCSB MLPerf AVG (a) Tri-HSS (b) Quad-HSS Normalized Avg. Request Latency Figure 14: Performance of Sibyl and Harmonia on Tri-HSS (top) and Quad-HSS (bottom). Performance is shown as aver- age request latency normalized to Fast-Only. Lower is better. We make two key observations. First, Harmonia outperforms Sibyl by 37.0 (42.0 ) on average in Tri-HSS (Quad-HSS) as Harmo- nia places performance-critical data in the high-end and mid-range devices and uses its migration policy to move cold data to slow devices during idle times. In contrast, Sibyl frequently evicts data to the low-end devices on the critical path, adding significant la- tency overhead. Second, in read-intensive workloads (e.g., RocksDB, MLPerf), Harmonia shows higher performance gains (up to 58.3 ) over Sibyl because Sibyl lacks a migration policy and relies on application updates to migrate data across devices. 6.4 Lifetime of Devices in HSS We evaluate the impact of Harmonia and baselines on storage device lifetimes by measuring their write amplification (WA). WA is the ratio of total data written (including migrations) in the HSS to data originating from the workload. A higher WA adversely impacts device lifetimes. Figs. 15(a) and 15(b) show WA of Harmonia and baselines on performance- and cost-optimized HSS, respectively. We make two key observations. First, only Sibyl and RNN-HSS have a lower WA than Harmonia. Harmonia s average WA of 1.54 (1.55) is higher than Sibyl s 1.49 (1.45) and RNN-HSS s 1.08 (1.06) in performance- (cost-) optimized HSS as it proactively prefetches data to the fast device in read-intensive workloads to improve read performance. Second, in both HSS configurations, Harmonia 0 1 2 3 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 1 2 3 SYSTOR17 RocksDB YCSB MLPerf AVG Write Amplification (b) Cost-Optimized HSS (a) Performance-Optimized HSS Figure 15: Write amplification of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS. Lower is better. has lower WA (1.24) in write-intensive workloads (i.e., SYSTOR17) compared to its WA (1.8) in read-intensive workloads (i.e., RocksDB, MLPerf). In write-intensive workloads, Harmonia learns to move data during update operations and performs fewer migrations. Write Traffic Distribution in Harmonia. Fig. 16 shows the write distribution of Harmonia s migration policy across devices in performance- and cost-optimized HSS. We make two observations. 0 25 50 75 100 SYSTOR17 RocksDB YCSB MLPerf AVG Fast Device Writes Slow Device Writes SYSTOR17 RocksDB YCSB MLPerf AVG (a) Performance- Optimized HSS (b) Cost-Optimized HSS Figure 16: Write traffic due to Harmonia s migrations on performance-optimized (left) and cost-optimized (right) HSS. First, Harmonia migrates more data to slow device as it effectively utilizes fast device for frequently-accessed data. Second, Harmonia has more writes to fast device in read-intensive workloads (i.e., RocksDB, MLPerf) as it proactively prefetches frequently-read data to the fast device even when there are no application updates. We conclude that Harmonia s adaptive policies provide higher performance at slightly higher average WA compared to Sibyl. 7 Discussion Alternate Objectives and Reward Structures. Since Harmonia s focus is to improve HSS performance, we design the RL agents reward structure based on I O latencies. We can design Harmonia for other objectives such as device lifetimes, fairness, quality of service (QoS) and energy efficiency. For example, to improve device lifetimes, the state features and reward structure can include (1) writes performed on each device, (2) device endurance in terms of program erase cycles, (3) write amplification (WA), and (4) de- vice reliability properties (e.g., NAND flash type (SLC TLC QLC), ECC capability). To manage write traffic to each device, the reward should be appropriate to the device where the data is placed or 11 migrated. Harmonia can accommodate fairness and QoS objectives by including factors such as bandwidth utilization, device queue depths and application requirements. We leave the exploration of these objectives and reward structures to future work. Extensibility of Harmonia. Harmonia is easily extensible to a large number of storage devices with minimal designer effort. When a new device is added to the HSS, we only add (1) capacity utilization of the new device as a state feature (see Table 1), and (2) an output node in each RL agent s neural network to represent the action of placing or migrating a page to the new device. However, scaling Harmonia to more devices can lead to two challenges: (1) the large state and action spaces can delay the convergence to an optimal policy, and (2) the metadata overhead increases. Applicability to Other Systems. Harmonia can be adapted to other heterogeneous systems such as tiered storage, hybrid memory, disaggregated storage and networked storage, which have diverse constraints and requirements. In a tiered storage system, placement is always performed on the top-tier device, and hence Harmonia s placement agent can be replaced by a heuristic approach. The mi- gration agent gains significance due to data migration across tiers. We can add multiple migration agents for each pair of adjacent tiers. Hybrid memory systems have strict latency constraints, and require Harmonia to be redesigned for low-latency decision-making. Dis- aggregated storage systems that use CXL [117 119] for large-scale memory pooling can leverage Harmonia s benefits when deployed on CXL fabric controller. We hope Harmonia can assist in designing adaptive policies for other heterogeneous systems. 8 Related Work In this section, we review other related work in HSS data manage- ment, and RL in storage memory systems. HSS Data Management. A large body of prior work proposes heuristic- and machine-learning-based data-placement (e.g., [5 7, 9, 13 33, 52 64]) and data-migration (e.g., [7, 11, 33, 69 73]) techniques. Hot Random Off-loading (HRO) [7] uses a 0-1 knap- sack model to allocate or migrate files between the storage devices. Vengerov [72] uses a fuzzy rule base architecture to perform data migration in a hierarchical storage system. These works do not perform (1) real system evaluation or (2) combined optimization of placement and migration policies using multiple RL agents. RL in Storage Systems. Several works (e.g., [120 134]) use RL to improve different aspects of storage systems. Prior works [120, 122] use RL to improve cloud storage utilization. RLAlloc [125] im- proves SSD throughput and Quality of Service (QoS). Other works [121, 123, 124, 126] use RL to optimize SSD maintenance activities (e.g., garbage collection) and error mitigation. Harmonia is orthogonal to all these works and can be used in conjunction with these techniques on an HSS. Multi-Agent RL Systems. Several works propose using multi- agent RL (MARL) for various system optimizations (e.g., [84 93]). Qiu et al. [85 87, 89] use MARL to improve tail latency of Server- less Function-as-a-Service (FaaS), targeting different systems and objectives than Harmonia. Jain et al. [84, 91] use MARL for dy- namic partitioning of last level cache and multi-level caches. Shen et al. [93] propose MARL-assisted cache cleaning for shingled man- aged recording drives. These techniques have four key differences from Harmonia: (1) their objectives are different from HSS data placement and migration, (2) they use identical RL agents unlike Harmonia s heterogeneous agents, (3) they use joint actions and shared rewards for all agents, and (4) they use table-based RL unlike Harmonia s neural network based RL. RLRP [90] proposes a deep RL-based replica placement and migration technique for distributed storage systems. RLRP has five key differences from Harmonia: (1) RLRP does not handle typical I O requests but focuses on replica placement, (2) RLRP s placement and migration occur at sparse in- tervals, making it unsuitable for HSS, (3) RLRP retrains its policies only when storage nodes are added or removed, while Harmonia continuously adapts its policies, (4) RLRP considers only a few state features (e.g., capacity utilization), and (5) RLRP s design has higher storage overhead than Harmonia. We conclude that these reasons make RLRP unsuitable for data placement and migration in HSS. 9 Conclusion We propose Harmonia, the first multi-agent reinforcement learning based holistic data-management technique for HSS that optimizes data placement and data migration together. We show that Har- monia s design enables coordination between the data-placement and data-migration agents to improve HSS performance. Our real- system evaluation demonstrates that Harmonia significantly out- performs prior HSS data-management techniques on four HSS con- figurations across a wide range of workloads. Harmonia is easily extensible to different HSS configurations. The performance bene- fits of Harmonia come with low latency and storage overheads. References [1] Wenjian Xiao, Huanqing Dong, Liuying Ma, Zhenjun Liu, and Qiang Zhang. HS-BAS: A Hybrid Storage System Based on Band Awareness of Shingled Write Disk. In ICCD, 2016. [2] Chunling Wang, Dandan Wang, Yupeng Chai, Chuanwen Wang, and Diansen Sun. Larger, Cheaper, but Faster: SSD-SMR Hybrid Storage Boosted by a New SMR-Oriented Cache Framework. In MSST, 2017. [3] Seongjin Lee, Youjip Won, and Sungwoo Hong. Mining-Based File Caching in a Hybrid Storage System. In JISE, 2014. [4] Wes Felter, Anthony Hylick, and John Carter. Reliability-Aware Energy Man- agement for Hybrid Storage Systems. In MSST, 2011. [5] Kai Bu, Meng Wang, Hongshan Nie, Wei Huang, and Bo Li. The Optimization of the Hierarchical Storage System Based on the Hybrid SSD Technology. In ISDEA, 2012. [6] KR Krish, Bharti Wadhwa, M Safdar Iqbal, M Mustafa Rafique, and Ali R Butt. On Efficient Hierarchical Storage for Big Data Processing. In CCGrid, 2016. [7] Lin Lin, Yifeng Zhu, Jianhui Yue, Zhao Cai, and Bruce Segee. Hot Random Off- Loading: A Hybrid Storage System with Dynamic Data Migration. In MASCOTS, 2011. [8] Junpeng Niu, Jun Xu, and Lihua Xie. Hybrid Storage Systems: A Survey of Architectures and Algorithms. In IEEE Access, 2018. [9] Jianzhe Tai, Bo Sheng, Yi Yao, and Ningfang Mi. SLA-Aware Data Migration in a Shared Hybrid Storage Cluster. In CC, 2015. [10] Jiaxin Ou, Jiwu Shu, Youyou Lu, Letian Yi, and Wei Wang. EDM: An Endurance- Aware Data Migration Scheme for Load Balancing in SSD Storage Clusters. In IPDPS, 2014. [11] Yuxia Cheng, Wenzhi Chen, Zonghui Wang, Xinjie Yu, and Yang Xiang. AMC: An Adaptive Multi-Level Cache Algorithm in Hybrid Storage Systems. In CCPE, 2015. [12] Ziliang Zong, Ribel Fares, Brian Romoser, and Joal Wood. FastStor: Data-Mining- Based Multilayer Prefetching for Hybrid Storage Systems. In CC, 2014. [13] Chihiro Matsui, Chao Sun, and Ken Takeuchi. Design of Hybrid SSDs With Storage Class Memory and NAND Flash Memory. In Proc. IEEE, 2017. [14] Feng Chen, David A Koufaty, and Xiaodong Zhang. Hystor: Making the Best Use of Solid State Drives in High Performance Storage Systems. In SC, 2011. [15] Yanfei Lv, Xuexuan Chen, Guangyu Sun, and Bin Cui. A Probabilistic Data Replacement Strategy for Flash-Based Hybrid Storage System. In APWeb, 2013. 12 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS [16] Jorge Guerra, Himabindu Pucha, Joseph S Glider, Wendy Belluomini, and Raju Rangaswami. Cost Effective Storage Using Extent Based Dynamic Tiering. In FAST, 2011. [17] Ahmed Elnably, Hui Wang, Ajay Gulati, and Peter J Varman. Efficient QoS for Multi-Tiered Storage Systems. In HotStorage, 2012. [18] Hui Wang and Peter Varman. Balancing Fairness and Efficiency in Tiered Storage Systems with Bottleneck-Aware Allocation. In FAST, 2014. [19] Gong Zhang, Lawrence Chiu, Clem Dickey, Ling Liu, Paul Muench, and Sangeetha Seshadri. Automated Lookahead Data Migration in SSD-enabled Multi-tiered Storage Systems. In MSST, 2010. [20] Xiaojian Wu and AL Narasimha Reddy. Data Organization in a Hybrid Storage System. In ICNC, 2012. [21] Ilias Iliadis, Jens Jelitto, Yusik Kim, Slavisa Sarafijanovic, and Vinodh Venkatesan. ExaPlan: Queueing-Based Data Placement and Provisioning for Large Tiered Storage Systems. In MASCOTS, 2015. [22] Yanfei Lv, Bin Cui, Xuexuan Chen, and Jing Li. Hotness-Aware Buffer Manage- ment For Flash-Based Hybrid Storage Systems. In CIKM, 2013. [23] Chihiro Matsui, Tomoaki Yamada, Yusuke Sugiyama, Yusuke Yamaga, and Ken Takeuchi. Tri-Hybrid SSD with Storage Class Memory (SCM) and MLC TLC NAND Flash Memories. Proc. IEEE, 2017. [24] Zhijie Feng, Zhiyong Feng, Xin Wang, Guozheng Rao, Yazhou Wei, and Zhiyuan Li. HDStore: An SSD HDD Hybrid Distributed Storage Scheme for Large-Scale Data. In WAIM, 2014. [25] Zhengyu Yang, Morteza Hoseinzadeh, Allen Andrews, Clay Mayers, David Thomas Evans, Rory Thomas Bolt, Janki Bhimani, Ningfang Mi, and Steven Swanson. AutoTiering: Automatic Data Placement Manager in Multi- Tier All-Flash Datacenter. In IPCCC, 2017. [26] Youngjae Kim, Aayush Gupta, Bhuvan Urgaonkar, Piotr Berman, and Anand Sivasubramaniam. HybridStore: A Cost-Efficient, High-Performance Storage System Combining SSDs and HDDs. In MASCOTS, 2011. [27] Lei Liu, Shengjie Yang, Lu Peng, and Xinyu Li. Hierarchical Hybrid Memory Management in OS for Tiered Memory Systems. In TPDS, 2019. [28] Yongping Luo, Peiquan Jin, and Shouhong Wan. Optimal Data Placement for Data-Centric Algorithms on NVM-Based Hybrid Memory. In DSAA, 2020. [29] Gagandeep Singh, Rakesh Nadig, Jisung Park, Rahul Bera, Nastaran Hajinazar, David Novo, Juan G칩mez-Luna, Sander Stuijk, Henk Corporaal, and Onur Mutlu. Sibyl: Adaptive and Extensible Data Placement in Hybrid Storage Systems Using Online Reinforcement Learning. In ISCA, 2022. [30] Thaleia Dimitra Doudali, Sergey Blagodurov, Abhinav Vishnu, Sudhanva Guru- murthi, and Ada Gavrilovska. Kleio: A Hybrid Memory Page Scheduler with Machine Intelligence. In HPDC, 2019. [31] Jinting Ren, Xianzhang Chen, Yujuan Tan, Duo Liu, Moming Duan, Liang Liang, and Lei Qiao. Archivist: A Machine Learning Assisted Data Placement Mechanism for Hybrid Storage Systems. In ICCD, 2019. [32] Peng Cheng, Yutong Lu, Yunfei Du, Zhiguang Chen, and Yang Liu. Optimizing Data Placement on Hierarchical Storage Architecture via Machine Learning. In NPC, 2019. [33] Milan Shetti, Bingzhe Li, and David Du. Machine Learning-based Adaptive Migration Algorithm for Hybrid Storage Systems. In TOS, 2019. [34] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole- Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091, 2019. [35] C.Szegedy et al. Inception-v4, inception-resnet and the impact of residual connections on learning. In CVPR, 2016. [36] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840 6851, 2020. [37] Hugo Touvron et al. Llama: Open and efficient foundation language models. CoRR, abs 2302.13971, 2023. [38] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [39] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek- v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [40] Chee-Yong Chan and Yannis E. Ioannidis. Bitmap Index Design and Evaluation. In SIGMOD, 1998. [41] FastBit: An Efficient Compressed Bitmap Index Technology. gov fastbit . [42] Redis. Redis bitmaps. [43] Scott Beamer, Krste Asanovic, and David Patterson. Direction-Optimizing Breadth-First Search. In SC, 2012. [44] Eric S Lander, Lauren M Linton, Bruce Birren, Chad Nusbaum, Michael C Zody, Jennifer Baldwin, Keri Devon, Ken Dewar, Michael Doyle, William Fitzhugh, et al. Initial Sequencing and Analysis of the Human Genome. Nature, 2001. [45] Stephen F Altschul, Warren Gish, Webb Miller, Eugene W Myers, and David J Lipman. Basic Local Alignment Search Tool. JMB, 1990. [46] Gene Myers. A Fast Bit-Vector Algorithm for Approximate String Matching Based on Dynamic Programming. JACM, 1999. [47] Intel. Intel Optane SSD DC P4801X Series, us en ark products 149365 intel-optane-ssd-dc-p4801x-series-100gb-2-5in- pcie-x4-3d-xpoint.html. [48] Intel. Intel SSD D3-S4510 Series, products memory-storage solid-state-drives data-center-ssds d3-series d3- s4510-series d3-s4510-1-92tb-2-5inch-3d2.html. [49] Samsung. Ultra-Low Latency with Samsung Z-NAND SSD, Low_Latency_with_Samsung_Z-NAND_SSD-0.pdf. [50] Samsung. Samsung SSD 980 PRO, " minisite ssd product consumer 980pro ", 2020. [51] Samsung. Samsung SSD 960 PRO NVMe M.2 512GB, "https: www.samsung.com us computing memory-storage solid-state-drives ssd- 960-pro-m-2-512gb-mz-v6p512bw". [52] Chao Sun, Kousuke Miyaji, Koh Johguchi, and Ken Takeuchi. A High Perfor- mance and Energy-Efficient Cold Data Eviction Algorithm for 3D-TSV Hybrid ReRAM MLC NAND SSD. In CAS, 2013. [53] Yang Li, Saugata Ghose, Jongmoo Choi, Jin Sun, Hui Wang, and Onur Mutlu. Utility-Based Hybrid Memory Management. In CLUSTER, 2017. [54] Neha Agarwal, David Nellans, Mark Stephenson, Mike O Connor, and Stephen W Keckler. Page Placement Strategies for GPUs Within Heterogeneous Memory Systems. In ASPLOS, 2015. [55] Neha Agarwal and Thomas F Wenisch. Thermostat: Application-Transparent Page Management for Two-Tiered Main Memory. In ASPLOS, 2017. [56] Tae Jun Ham, Bharath K Chelepalli, Neng Xue, and Benjamin C Lee. Disinte- grated Control for Energy-Efficient and Heterogeneous Memory Systems. In HPCA, 2013. [57] Reza Salkhordeh, Hossein Asadi, and Shahriar Ebrahimi. Operating System Level Data Tiering Using Online Workload Characterization. In JSC, 2015. [58] Milan Pavlovic, Nikola Puzovic, and Alex Ramirez. Data Placement in HPC Architectures With Heterogeneous Off-Chip Memory. In ICCD, 2013. [59] Justin Meza, Yixin Luo, Samira Khan, Jishen Zhao, Yuan Xie, and Onur Mutlu. A Case for Efficient Hardware Software Cooperative Management of Storage and Memory. In WEED, 2013. [60] Chiachen Chou, Aamer Jaleel, and Moinuddin K Qureshi. BATMAN: Maximiz- ing Bandwidth Utilization of Hybrid Memory Systems. MEMSYS, 2017. [61] Chenxi Wang, Huimin Cui, Ting Cao, John Zigman, Haris Volos, Onur Mutlu, Fang Lv, Xiaobing Feng, and Guoqing Harry Xu. Panthera: Holistic Memory Management for Big Data Processing over Hybrid Memories. In PLDI, 2019. [62] Luiz E Ramos, Eugene Gorbatov, and Ricardo Bianchini. Page Placement in Hybrid Memory Systems. In ICS, 2011. [63] Thaleia Dimitra Doudali, Daniel Zahka, and Ada Gavrilovska. Cori: Dancing to the Right Beat of Periodic Data Movements over Hybrid Memory Systems. In IPDPS, 2021. [64] Satyabrata Sen and Neena Imam. Machine Learning Based Design Space Explo- ration for Hybrid Main-Memory Design. In MEMSYS, 2019. [65] Chin-Hsien Wu, Cheng-Wei Huang, and Chen-Yu Chang. A Data Management Method for Databases Using Hybrid Storage Systems. ACM SIGAPP Applied Computing Review, 2019. [66] Yuan Hua Yang, Xian Bin Xu, Shui Bing He, and Yu Hua Wen. A Statistics-Based Data Placement Strategy for Hybrid Storage. Applied Mechanics and Materials, 2014. [67] Agil Yolchuyev and Janos Levendovszky. Data Chunks Placement Optimization for Hybrid Storage Systems. Future Internet, 2021. [68] John D Strunk. Hybrid Aggregates: Combining SSDs and HDDs in a Single Storage Pool. ACM SIGOPS Operating Systems Review, 2012. [69] Mingwei Lin, Riqing Chen, Jinbo Xiong, Xuan Li, and Zhiqiang Yao. Efficient Sequential Data Migration Scheme Considering Dying Data for HDD SSD Hybrid Storage Systems. IEEE Access, 2017. [70] Mingwei Lin, Riqing Chen, Li Lin, Xuan Li, and Jingchang Huang. Buffer-Aware Data Migration Scheme for Hybrid Storage Systems. IEEE Access, 2018. [71] Gong Zhang, Lawrence Chiu, and Ling Liu. Adaptive Data Migration in Multi- tiered Storage Based Cloud Environment. In IEEE CLOUD, 2010. [72] David Vengerov. A Reinforcement Learning Framework for Online Data Migra- tion in Hierarchical Storage Systems. Journal of Supercomputing, 2008. [73] Evangelos Vasilakis, Vassilis Papaefstathiou, Pedro Trancoso, and Ioannis Sour- dis. Hybrid2: Combining Caching and Migration in Hybrid Memory Systems. In HPCA, 2020. [74] Wikipedia. Harmonia, [75] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-Task Reinforcement Learning with Soft Modularization. NeurIPS, 2020. [76] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-World: A Benchmark and Evaluation for Multi- Task and Meta Reinforcement Learning. 2020. 13 [77] Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust Multitask Reinforcement Learning. NeurIPS, 2017. [78] Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning. NeurIPS, 2020. [79] NVM Express. Everything You Need to Know About the NVMe 2.0 Specifications and New Technical Proposals. [80] Serial ATA International Organization. Serial ATA Revision 3.1. io.org system files specifications SerialATA_Revision_3_1_Gold.pdf. [81] Linux. Multiple Device Driver, [82] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. 2018. [83] Lorenzo Canese, Gian Carlo Cardarilli, Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino, Marco Re, and Sergio Span. Multi-Agent Reinforcement Learning: A Review of Challenges and Applications. Applied Sciences, 2021. [84] Rahul Jain, Preeti Ranjan Panda, and Sreenivas Subramoney. Cooperative Multi- Agent Reinforcement Learning-Based Co-optimization of Cores, Caches, and On-chip Network. In TACO, 2017. [85] Haoran Qiu, Weichao Mao, Archit Patke, Chen Wang, Hubertus Franke, Zbig- niew T Kalbarczyk, Tamer Ba르r, and Ravishankar K Iyer. Reinforcement Learning for Resource Management in Multi-tenant Serverless Platforms. In EuroMLSys, 2022. [86] Haoran Qiu, Weichao Mao, Archit Patke, Chen Wang, Hubertus Franke, Zbig- niew T Kalbarczyk, Tamer Ba르r, and Ravishankar K Iyer. SIMPPO: A Scalable and Incremental Online Learning Framework for Serverless Resource Manage- ment. In SoCC, 2022. [87] Haoran Qiu, Subho S Banerjee, Saurabh Jha, Zbigniew T Kalbarczyk, and Rav- ishankar K Iyer. FIRM: An Intelligent Fine-grained Resource Management Framework for SLO-Oriented Microservices. In OSDI, 2020. [88] Weichao Mao, Haoran Qiu, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Ravi Iyer, and Tamer Basar. Multi-Agent Meta-Reinforcement Learning: Sharper Convergence Rates with Task Similarity. In NeurIPS, 2023. [89] Haoran Qiu, Weichao Mao, Chen Wang, Hubertus Franke, Zbigniew T. Kalbar- czyk, Tamer Ba르r, and Ravishankar K. Iyer. On the Promise and Challenges of Foundation Models for Learning-based Cloud Systems Management. In Workshop on ML for Systems at NeurIPS, 2023. [90] Kai Lu, Nannan Zhao, Jiguang Wan, Changhong Fei, Wei Zhao, and Tongliang Deng. RLRP: High-Efficient Data Placement with Reinforcement Learning for Modern Distributed Storage Systems. In IPDPS, 2022. [91] Rahul Jain, Preeti Ranjan Panda, and Sreenivas Subramoney. Cooperative multi-agent reinforcement learning-based co-optimization of cores, caches, and on-chip network. TACO, 2017. [92] Rahul Jain, Preeti Ranjan Panda, and Sreenivas Subramoney. A Coordinated Multi-Agent Reinforcement Learning Approach to Multi-Level Cache Co- Partitioning. In DATE, 2017. [93] Zhaoyan Shen, Yuhan Yang, Yungang Pan, Yuhao Zhang, Zhiping Jia, Xiaojun Cai, Bingzhe Li, and Zili Shao. A Multiagent Reinforcement Learning-Assisted Cache Cleaning Scheme for DM-SMR. IEEE TCAD, 2022. [94] Haitham Bou Ammar, Eric Eaton, Matthew E Taylor, Decebal Constantin Mo- canu, Kurt Driessens, Gerhard Weiss, and Karl T칲yls. An Automated Measure of MDP Similarity for Transfer in Reinforcement Learning. In AAAI, 2014. [95] James L Carroll and Kevin Seppi. Task Similarity Measures for Transfer in Reinforcement Learning Task Libraries. In IJCNN, 2005. [96] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods for Reinforcement Learning With Function Approximation. In NIPS, 1999. [97] Nan Rosemary Ke, Amanpreet Singh, Ahmed Touati, Anirudh Goyal, Yoshua Bengio, Devi Parikh, and Dhruv Batra. Modeling the Long Term Future in Model-Based Reinforcement Learning. In ICLR, 2018. [98] Mitesh R Meswani, Sergey Blagodurov, David Roberts, John Slice, Mike Igna- towski, and Gabriel H Loh. Heterogeneous Memory Architectures: A HW SW Approach for Mixing Die-stacked and Off-package Memories. In HPCA, 2015. [99] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, He- len King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement Learning. In Nature, 2015. [100] Jacques De Villiers and Etienne Barnard. Backpropagation Neural Nets with One and Two Hidden Layers. In IEEE Trans. Neural Netw. Learn. Sys, 1993. [101] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for Activation Functions. In arXiv, 2017. [102] Marc G Bellemare, Will Dabney, and R칠mi Munos. A Distributional Perspective on Reinforcement Learning. In arXiv, 2017. [103] Daniel JB Harrold, Jun Cao, and Zhong Fan. Data-Driven Battery Operation For Energy Arbitrage Using Rainbow Deep Reinforcement Learning. In Energy, 2021. [104] Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter Selec- tion For Offline Reinforcement Learning. In arXiv, 2020. [105] Sylvain Arlot and Alain Celisse. A Survey of Cross-Validation Procedures for Model Selection. SS, 2010. [106] Michel Tokic and G칲nther Palm. Value-Difference Based Exploration: Adaptive Control between Epsilon-Greedy and Softmax. In AAAI, 2011. [107] Shunsuke Tsukada, Hikaru Takayashiki, Masayuki Sato, Kazuhiko Komatsu, and Hiroaki Kobayashi. A Metadata Prefetching Mechanism for Hybrid Memory Architectures. In COOL CHIPS, 2021. [108] Seagate. Seagate Barracuda Datasheet, content datasheets pdfs 3-5-barracuda-3tbDS1900-10-1710US-en_US.pdf. [109] Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman, Ke Wang, Ekaterina Gonina, Neal Wu, Efi Kokiopoulou, Luciano Sbaiz, Jamie Smith, G치bor Bart칩k, Jesse Berent, Chris Harris, Vincent Vanhoucke, and Eugene Brevdo. TF-Agents: A Library for Reinforcement Learning in TensorFlow, 2018. [110] AMD. AMD Ryzen 7 PRO 2700 Processor, cpu amd-ryzen-7-2700. [111] ADATA. ADATA Ultimate Series: SU630, ultimate-series-su630-960gb-sata-iii-internal-2-5-solid-state-drive . [112] Intel Corporation. Intel Optane Persistent Memory 200 Series Brief. optane-persistent-memory optane-persistent-memory-200-series-brief.html, 2020. [113] Chunghan Lee, Tatsuo Kumano, Tatsuma Matsuki, Hiroshi Endo, Naoto Fuku- moto, and Mariko Sugawara. Understanding Storage Traffic Characteristics on Enterprise Virtual Desktop Infrastructure. In SYSTOR, 2017. [114] Gala Yadgar, Moshe Gabel, Shehbaz Jaffer, and Bianca Schroeder. SSD-Based Workload Characteristics and their Performance Implications. TOS, 2021. [115] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell Sears. Benchmarking Cloud Serving Systems With YCSB. In SOCC, 2010. [116] MLCommons. MLPerf Storage Benchmarks. benchmarks storage , 2024. [117] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agar- wal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanau- jia, and Prakash Chauhan. TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory. In ASPLOS, 2023. [118] Donghyun Gouk, Miryeong Kwon, Hanyeoreum Bae, Sangwon Lee, and My- oungsoo Jung. Memory Pooling With CXL. IEEE Micro, 2023. [119] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko Novakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, et al. Pond: CXL-Based Memory Pooling Systems for Cloud Platforms. In ASPLOS, 2023. [120] Kaiyang Liu, Jun Peng, Jingrong Wang, Boyang Yu, Zhuofan Liao, Zhiwu Huang, and Jianping Pan. A Learning-Based Data Placement Framework for Low Latency in Data Center Networks. In TCC, 2019. [121] Sangjin Yoo and Dongkun Shin. Reinforcement Learning-Based SLC Cache Technique for Enhancing SSD Write Performance. In HotStorage, 2020. [122] Haoyu Wang, Haiying Shen, Qi Liu, Kevin Zheng, and Jie Xu. A Reinforcement Learning Based System for Minimizing Cloud Storage Service Cost. In ICPP, 2020. [123] Wonkyung Kang, Dongkun Shin, and Sungjoo Yoo. Reinforcement Learning- Assisted Garbage Collection to Mitigate Long-Tail Latency in SSD. In TECS, 2017. [124] Wonkyung Kang and Sungjoo Yoo. Dynamic Management of Key States for Re- inforcement Learning-assisted Garbage Collection to Reduce Long Tail Latency in SSD. In DAC, 2018. [125] Mengquan Li, Chao Wu, Congming Gao, Cheng Ji, and Kenli Li. RLAlloc: A Deep Reinforcement Learning-Assisted Resource Allocation Framework for Enhanced Both I O Throughput and QoS Performance of Multi-Streamed SSDs. In DAC, 2023. [126] Jun Li, Bowen Huang, Zhibing Sha, Zhigang Cai, Jianwei Liao, Balazs Gerofi, and Yutaka Ishikawa. Mitigating Negative Impacts of Read Disturb in SSDs. TODAES, 2020. [127] Ibrahim Umit Akgun, Ali Selman Aydin, Aadil Shaikh, Lukas Velikov, and Erez Zadok. A Machine Learning Framework to Improve Storage System Performance. In HotStorage, 2021. [128] Wonkyung Kang, Dongkun Shin, and Sungjoo Yoo. Reinforcement Learning- Assisted Garbage Collection to Mitigate Long-Tail Latency in SSD. TECS, 2017. [129] Wonkyung Kang and Sungjoo Yoo. Q-Value Prediction for Reinforcement Learning Assisted Garbage Collection to Reduce Long Tail Latency in SSD. IEEE TCAD, 2019. [130] Qian Wei, Yi Li, Zhiping Jia, Mengying Zhao, Zhaoyan Shen, and Bingzhe Li. Reinforcement Learning-Assisted Management for Convertible SSDs. In DAC, 2023. [131] Jun Li, Zhigang Cai, Balazs Gerofi, Yutaka Ishikawa, and Jianwei Liao. Page Type-Aware Full-Sequence Program Scheduling via Reinforcement Learning in 14 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS High Density SSDs. IEEE TCAD, 2024. [132] Yong-Cheng Liaw, Shuo-Han Chen, and Yu-Pei Liang. Reinforcement Learning- Based Read Performance Throttling to Enhance Lifetime of 3D NAND SSD. In NVMSA, 2024. [133] Chao Wu, Cheng Ji, Qiao Li, Chenchen Fu, and Chun Jason Xue. Maximizing I O Throughput and Minimizing Performance Variation via Reinforcement Learning Based I O Merging for SSDs. In IEEE TC, 2019. [134] Jin Yong Ha, Sangjin Lee, Heon Young Yeom, and Yongseok Son. RL-Watchdog: A Fast and Predictable SSD Liveness Watchdog on Storage Systems. In USENIX ATC, 2024. 15\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nHarmonia: A Multi-Agent Reinforcement Learning Approach to Data Placement and Migration in Hybrid Storage Systems Rakesh Nadig Vamanan Arulchelvan Rahul Bera Taha Shahroodi Gagandeep Singh Andreas Kakolyris Mohammad Sadrosadati Jisung Park Onur Mutlu ETH Z칲rich AMD Research POSTECH Abstract Hybrid storage systems (HSS) combine multiple storage devices with diverse characteristics to achieve high performance and ca- pacity at low cost. The performance of an HSS highly depends on the effectiveness of two key policies: (1) the data-placement policy, which determines the best-fit storage device for incoming data, and (2) the data-migration policy, which rearranges stored data (i.e., prefetches hot data and evicts cold data) across the devices to sustain high HSS performance. Prior works focus on improving only data placement or only data migration in HSS, which leads to relatively low HSS performance. Unfortunately, no prior work tries to optimize both policies together. Our goal is to design a holistic data-management technique that optimizes both data-placement and data-migration policies to fully exploit the potential of an HSS, and thus significantly improve system performance. We demonstrate the need for multiple rein- forcement learning (RL) agents to accomplish our goal. We propose Harmonia, a multi-agent reinforcement learning (RL)-based data- management technique that employs two lightweight autonomous RL agents, a data-placement agent and a data-migration agent, which adapt their policies for the current workload and HSS con- figuration, and coordinate with each other to improve overall HSS performance. We evaluate Harmonia on a real HSS with up to four heteroge- neous and diverse storage devices. Our evaluation using 17 data- intensive workloads on performance-optimized (cost-optimized) HSS with two storage devices shows that, on average, Harmonia outperforms the best-performing prior approach by 49.5 (31.7 ). On an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0 (42.0 ). Harmonia s perfor- mance benefits come with low latency (240洧녵洧맍or inference) and storage overheads (206 KiB in DRAM for both RL agents together).\n\n--- Segment 2 ---\nOn an HSS with three (four) devices, Harmonia outperforms the best-performing prior work by 37.0 (42.0 ). Harmonia s perfor- mance benefits come with low latency (240洧녵洧맍or inference) and storage overheads (206 KiB in DRAM for both RL agents together). We will open-source Harmonia s implementation to aid future re- search on HSS. 1 Introduction A hybrid storage system (HSS) [1 33] is a cost-effective solution for high-performance and large-capacity storage requirements of data-intensive applications such as machine learning [34 36], large language models [37 39], databases [40 42], graph processing [43], and genome analysis [44 46]. An HSS combines multiple storage devices (e.g., solid-state drives (SSDs) [47 51], hard disk drives (HDDs)) of diverse characteristics (e.g., I O bandwidth, storage capacity). A typical HSS consists of fast storage devices with limited capacities and slow storage devices with large capacities. The performance of an HSS is highly dependent on two key policies, data placement and data migration. The data-placement policy determines the best-fit storage device in the HSS for incoming I O requests. The data-migration policy rearranges data across the storage devices (i.e., prefetches frequently-accessed data to the fast device and evicts cold data to the slow device) to sustain high HSS performance, which can degrade over time due to (1) misplacement of data, and (2) changes in workload access patterns. We identify four key challenges in designing efficient data-place- ment and data-migration policies. First, workload access patterns and HSS conditions (e.g., access latencies, device capacity utiliza- tion) can change frequently in data-intensive environments, which makes it hard to optimize the policies. Second, the data-placement policy should have a low performance overhead as it operates on the critical path of I O request handling. Third, the data-migration policy needs to migrate data across storage devices in a timely man- ner without impacting the latency of incoming I O requests. Fourth, the two policies should not make conflicting decisions, which can adversely impact HSS performance and device lifetimes. Limitations of prior works.\n\n--- Segment 3 ---\nFourth, the two policies should not make conflicting decisions, which can adversely impact HSS performance and device lifetimes. Limitations of prior works. Prior works propose techniques for either data placement (e.g., [5 7, 9, 13 33, 52 68]) or data migra- tion (e.g., [7, 11, 30, 33, 69 73]), but they provide relatively low performance when employed together or alone. Our motivational study on performance- (cost-) optimized HSS configurations demon- strates that even the state-of-the-art HSS data-placement technique, Sibyl [29], only achieves 49.9 (41.4 ) of the performance of an Oracle policy (see 3 for details). We identify two key reasons for such large performance gaps. First, no prior work proposes a holis- tic approach to optimize both data placement and data migration together. Second, heuristic- and supervised-learning-based policies (e.g., [7, 13, 23, 29 31, 33, 69]) require hand-tuning and may not adapt to changes in workload characteristics and HSS states. Since there are no prior works that optimize both placement and migration, we combine prior techniques to create four ex- tended data-management techniques (see 3). Our evaluation shows that these extended techniques underperform because they are naive combinations of prior data-placement and data-migration ap- proaches that are oblivious to each other s decisions. We conclude that for high HSS performance, both data placement and migration policies need to coordinate with each other. Our goal is to design a holistic data-management technique for HSS that optimizes both data-placement and data-migration policies in a coordinated way to improve overall system perfor- mance. To this end, we propose Harmonia,1 a multi-agent online reinforcement learning (RL) based holistic approach to co-optimize HSS data placement and data migration. Need for multi-agent RL. We describe the benefits of RL for HSS data management in 3.1. Despite these benefits, a single RL agent 1Harmonia [74]: Greek goddess of harmony and balance, who brings peace to conflicts.\n\n--- Segment 4 ---\nWe describe the benefits of RL for HSS data management in 3.1. Despite these benefits, a single RL agent 1Harmonia [74]: Greek goddess of harmony and balance, who brings peace to conflicts. 1 arXiv:2503.20507v2 [cs.AR] 22 Apr 2025 cannot effectively (1) co-optimize [75 78] different objectives like data placement and data migration, and (2) perform both tasks concurrently. Our motivational study demonstrates that a single- agent RL approach that performs both data placement and data migration leads to 18.8 (27.7 ) lower average performance than Sibyl, which performs only data placement, on a performance- (cost-) optimized HSS (see 3.4). This is because learning to optimize data migration interferes with the process of learning an optimal data placement policy as they are two different tasks (See 3.2). Key Idea. Harmonia employs two coordinating lightweight au- tonomous RL agents, respectively for data placement and migra- tion, to improve HSS performance. First, we design Harmonia s data-placement agent by extending the state-of-the-art placement technique, Sibyl [29]. To coordinate with the migration agent, we add new state features and improve reward structure for the place- ment agent. Second, we design Harmonia s data-migration agent to identify migration candidates and their target devices by con- tinuously monitoring previously-placed pages in HSS. Harmonia assigns rewards to the migration agent based on the impact of its mi- grations on future HSS performance. The two agents influence each other s decisions through actions that affect the HSS environment. We evaluate Harmonia on a real Linux system using seventeen data-intensive workloads and various HSS configurations with up to four devices. Compared to the best-performing prior approach, Sibyl [29], on average, Harmonia provides (1) 49.5 (31.7 ) per- formance improvement on a performance- (cost-) optimized HSS, and (2) 37.0 (42.0 ) higher performance on an HSS with three (four) devices. Harmonia incurs low latency (240洧녵洧맍or inference) and storage overheads (206 KiB for both RL agents together).\n\n--- Segment 5 ---\nCompared to the best-performing prior approach, Sibyl [29], on average, Harmonia provides (1) 49.5 (31.7 ) per- formance improvement on a performance- (cost-) optimized HSS, and (2) 37.0 (42.0 ) higher performance on an HSS with three (four) devices. Harmonia incurs low latency (240洧녵洧맍or inference) and storage overheads (206 KiB for both RL agents together). This work makes the following key contributions: We show that prior data-management approaches have low HSS performance because they do not optimize both data placement and data migration, and we quantify this on a real Linux system with various HSS configurations. We propose Harmonia, a multi-agent RL-based data-management technique for HSS that co-optimizes data-placement and data- migration policies using two lightweight coordinating RL agents to improve HSS performance. Via a rigorous real-system evaluation of Harmonia on various HSS configurations, we demonstrate that Harmonia consistently outperforms state-of-the-art data-management techniques across a wide range of data-intensive workloads. 2 Background 2.1 Hybrid Storage Systems Fig. 1 presents an overview of a hybrid storage system (HSS). An HSS has two key components: (1) multiple storage devices of vary- ing characteristics (e.g., I O latency, I O bandwidth, and device capacity) and storage protocols (e.g., NVM Express (NVMe) [79] or SATA [80]), and (2) an HSS management layer that orchestrates data placement and data migration in the HSS. A typical HSS combines fast storage devices with lower capacities (i.e., high-end device in Fig. 1) and slower storage devices with larger capacities (i.e., mid-range and low-end devices in Fig. 1). HSS Management Layer. The HSS management layer has three key functions. First, it (1) provides a unified flat virtual address High-End Device Mid-Range Device Low-End Device Read Write Unified Virtual Address Space Application File System Hybrid Storage System (HSS) Data Placement Data Migration HSS Management Layer Data Migration Read Write Read Write Read Write Read Write Figure 1: Overview of a hybrid storage system.\n\n--- Segment 6 ---\nThe HSS management layer has three key functions. First, it (1) provides a unified flat virtual address High-End Device Mid-Range Device Low-End Device Read Write Unified Virtual Address Space Application File System Hybrid Storage System (HSS) Data Placement Data Migration HSS Management Layer Data Migration Read Write Read Write Read Write Read Write Figure 1: Overview of a hybrid storage system. space for the applications to store data across all HSS devices, (2) maps a virtual address to a logical block address (LBA) in one of the underlying storage devices, and (3) translates application I O requests to the devices using NVMe [79] or SATA [80] protocols. The HSS management layer is similar to the md [81] feature in Linux, which provides software RAID functionality (RAID 0) when multiple storage devices are present. Second, it (1) places data (i.e., data placement) in the best-fit storage device for each incoming I O request, and (2) rearranges data (i.e., data migration) across storage devices. Prior works (e.g., [13, 29, 31]) demonstrate that data-placement and migration policies have a significant impact on HSS performance. Third, the HSS management layer stores the following metadata in the host DRAM: (1) virtual address to LBA mapping, (2) I O request characteristics (e.g., request type, request size), and (3) storage device characteristics (e.g., device capacities and their current utilization). The HSS management layer is typically implemented in the kernel space of the host OS. 2.2 Reinforcement Learning Reinforcement learning (RL) [82] is a machine learning (ML) ap- proach in which an agent learns the optimal policy for a specific objective by interacting with its environment. Fig. 2 presents the overview of an RL system. Environment Agent Action (at) Reward (rt 1) State (st) Figure 2: Overview of reinforcement learning. An RL approach is formulated using four key components. 1. State is the representation of the environment. Assuming 洧녡is the set of all possible states, 洧멇롐 洧녡is the state at time step 洧노. 2. Action.\n\n--- Segment 7 ---\n2. Action. At each time step, the agent observes the current state 洧멇롐며nd performs an action 洧녩洧노from the set of all actions 洧냢. Based on the action, the environment transitions to a new state 洧멇롐 1. 3. Reward. In response to 洧녩洧노that changes environment state from 洧멇롐몂o 洧멇롐 1, the agent receives a numerical reward 洧洧노 1. 4. Policy. An agent uses policy 洧랢to determine its actions in a given state and aims to find the policy that maximizes cumulative 2 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS rewards. The optimal policy 洧랢 is determined by computing the optimal action-value function 洧녟 , also called the Q-value. The Q- value of a state-action pair, denoted as洧녟(洧녡,洧냢), reflects the predicted cumulative reward obtained by taking action A in state S. The agent makes decisions and learns to maximize the overall cumulative reward or return over time. While the agent may not maximize the benefit of each action, it aims to maximize the long- term consequences of its actions. 2.3 Multi-Agent Reinforcement Learning Multi-agent RL (e.g., [83 93]) is an extension of traditional RL where multiple RL agents interact with a shared environment and and with one another to learn optimal policies. Unlike RL with a single agent, multi-agent RL can effectively optimize multiple different tasks. A single RL agent cannot effectively learn optimal policies for different tasks because it relies on task similarity [94, 95] to transfer the learning gained from one task to another. Task similarity refers to the resemblance of tasks in their inputs, objectives, and actions. Prior works [75 78, 94, 95] show that a single RL agent cannot effectively learn optimal policies for multiple tasks with low task similarity because transferring the learning from one task to a dissimilar task can inhibit performance. 3 Motivation In this section, we describe (1) why RL is a good fit for HSS data management, (2) the need for a multi-agent RL approach, and (3) prior approaches to HSS data management and their effectiveness. 3.1 Why RL for Data Management in HSS?\n\n--- Segment 8 ---\n3 Motivation In this section, we describe (1) why RL is a good fit for HSS data management, (2) the need for a multi-agent RL approach, and (3) prior approaches to HSS data management and their effectiveness. 3.1 Why RL for Data Management in HSS? RL is a good fit for HSS data management for three key reasons. First, unlike heuristic and supervised-learning approaches, an RL approach can learn online without prior training using labeled data. Online learning helps the RL agent learn dynamic changes in workload access patterns and system conditions, and continuously adapt the data-placement and data-migration policies using system- level feedback without human intervention. Prior work Sibyl [29] shows that RL outperforms heuristic-(e.g., [13, 23]) and supervised- learning-based (e.g., [30, 33]) approaches for HSS data placement. However, Sibyl partially uses RL for HSS data-management (i.e., RL for data-placement and a heuristic approach for data migration) that leaves a large gap between its performance and that of an Oracle approach (as we discuss in 3.4). Second, RL can handle complex tasks such as placement and migration where each action changes the environment (e.g., de- vice capacity utilization, bandwidth utilization, garbage collection) and impacts future outcomes (e.g., I O request latencies). Unlike supervised-learning approaches, RL learns strategies that maximize long-term returns (e.g., [82, 96, 97]), i.e., overall HSS performance. Third, RL offers ease of extensibility to a wide range of HSS configurations and devices with heterogeneous characteristics with minimum designer effort (as partially demonstrated in Sibyl [29]). 3.2 Need for Multi-Agent RL for HSS Data placement and data migration are two different tasks in HSS, and a multi-agent RL approach is a great fit for optimizing these different tasks (see 2.3). Data placement and data migration in HSS are two different tasks for three key reasons. First, they operate on different data in the HSS. While data placement writes data from the current I O request to the best-fit storage device, data migration monitors previously placed data to identify candidate pages for migration and their target storage devices. Second, their objectives are differ- ent.\n\n--- Segment 9 ---\nWhile data placement writes data from the current I O request to the best-fit storage device, data migration monitors previously placed data to identify candidate pages for migration and their target storage devices. Second, their objectives are differ- ent. The objective of data placement is to minimize the I O latency for each incoming request, but data migration aims to improve long-term HSS performance. Third, these two tasks are performed asynchronously with each other. Data placement is on the critical path of I O handling and is performed synchronously with applica- tion writes or updates. Data migration is a background process and it is performed asynchronously to the application s I O requests. To describe the need for data migration in addition to data place- ment, we provide two examples: (1) If a page in the fast device is rarely accessed, migrating it to the slow device can free up space for more frequently-accessed pages and improve overall HSS perfor- mance. (2) If a page on the slow device is accessed frequently (due to access pattern changes), it should be migrated to the fast device to reduce its access latency. In 3.4, we quantitatively demonstrate that a single RL agent (SAPM) shows limited performance when handling both data placement and migration in HSS. We conclude that multi-agent is a great fit for optimizing both data placement and migration in HSS, which are complex and different tasks. 3.3 Prior Approaches to HSS Data Management Our focus in this work is to co-optimize placement and migra- tion in HSS. In this section, we briefly introduce prior HSS data- management techniques. However, there is no existing technique that optimizes both placement and migration in HSS. We discuss different approaches to optimize both placement and migration by combining (or extending) prior HSS data-management techniques. Data-Placement Techniques. We evaluate two state-of-the-art HSS data-placement techniques: CDE (Cold Data Eviction) [13] and Sibyl [29]. CDE is a heuristic data-placement technique that places hot or random (cold or sequential) data in the fast (slow) device, by determining the hotness and randomness of incoming data based on past access frequency and request size, respectively.\n\n--- Segment 10 ---\nWe evaluate two state-of-the-art HSS data-placement techniques: CDE (Cold Data Eviction) [13] and Sibyl [29]. CDE is a heuristic data-placement technique that places hot or random (cold or sequential) data in the fast (slow) device, by determining the hotness and randomness of incoming data based on past access frequency and request size, respectively. Sibyl [29] uses an RL agent to place data in the best-fit HSS storage device based on multiple features of I O requests (e.g., request type and size) and HSS conditions (e.g., device capacity, I O latency). When the fast device becomes full, both techniques evict the least-recently-used (LRU) data before placing new I O request data in the fast device. Data-Migration Techniques. We evaluate two state-of-the-art HSS data-migration techniques, K-SVM [33] and RNN-HSS [30], which have three aspects in common. First, they both leverage supervised learning to identify the target device for migration of each stored page within the HSS based on the page s access fre- quency. Second, they periodically determine migration candidates (i.e., pages) and perform data migration at fixed intervals (e.g., mi- gration is performed after every 1000 incoming requests) on the critical path of I O request handling. Third, they perform data place- ment for incoming I O requests using a fixed heuristic policy (e.g., K-SVM places incoming requests in the fast device until its capacity reaches 100 [33]). The key difference between the two techniques 3 is in the supervised-learning method used; K-SVM uses a k-means- assisted support vector machine (SVM) to classify stored pages as belonging to a storage device based on their past access frequencies; RNN-HSS uses recurrent neural networks (RNN) to predict a page s future access frequency, and migrate the page to its target device. Extended Data-Management Techniques. Since there is no ex- isting technique that optimizes both data placement and data migra- tion together, we evaluate four extended techniques to explore the design space for a holistic data-management technique.\n\n--- Segment 11 ---\nExtended Data-Management Techniques. Since there is no ex- isting technique that optimizes both data placement and data migra- tion together, we evaluate four extended techniques to explore the design space for a holistic data-management technique. For the first two techniques, Sibyl K-SVM and Sibyl RNN-HSS, we simply com- bine Sibyl, the current best-performing data-placement technique, with the prior data-migration techniques explained above. We design the third extended data-management technique, CDE RL-Migr,2 by combining CDE, a heuristic data-placement technique, with an RL-based data-migration approach. The RL- based data-migration approach uses the same design as the data- migration agent proposed in Harmonia, including the state features and the reward structure (See 4.2). In the fourth extended technique, SAPM (Single Agent for Place- ment and Migration), we design a single RL agent to perform both data placement and data migration to analyze the effectiveness of a single RL agent at HSS data management. SAPM performs a place- ment action for every incoming I O request, and focuses on data migration only during system idle times to avoid interference with the placement task (which is on the critical path of I O request han- dling). We provide the same state features used in Harmonia (See 4.2) to SAPM and tune the hyperparameters to achieve the best performance possible with a single RL agent. We provide SAPM s RL agent (1) an immediate reward for every placement action (based on the reward of Harmonia s data-placement agent in Equation 2), and (2) a deferred reward for the migration actions (based on the reward structure of Harmonia s data-migration agent in Equation 3). We provide background in 2.3 on why a single RL agent cannot effectively optimize two different tasks. 3.4 Effectiveness of Prior Techniques Methodology. We evaluate eight prior data-management tech- niques on a real system with two HSS configurations, performance- and cost-optimized, on 17 data-intensive workloads from 3 bench- mark suites (see Table 4). See 5 for our evaluation methodology. Performance Results. Fig.\n\n--- Segment 12 ---\nPerformance Results. Fig. 3 shows the performance of four prior and four extended HSS data-management techniques on (a) performance-optimized and (b) cost-optimized HSS. We compare their average request latencies against two ideal approaches, Fast- Only and Oracle. First, in Fast-Only, we assume that the fast storage device is large enough to accommodate the entire workload dataset. Second, Oracle [98] makes optimal data-placement and migration decisions based on complete knowledge of future I O access pat- terns of the entire workload. We assume that Oracle performs data migration during system idle times, which does not add any latency overheads. All values in Fig. 3 are normalized to Fast-Only. We make five key observations from Fig. 3. First, all prior tech- niques exhibit significant performance gaps from Oracle for every tested HSS and workload. The best-performing technique, Sibyl, 2The RL-based data-migration technique in CDE RL-Migr is a contribution of this work. We design this as there are no prior RL-based HSS data-migration techniques. 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Oracle 0 30 60 90 120 SYSTOR17 RocksDB YCSB MLPerf AVG (b) Cost-Optimized HSS (a) Performance-Optimized HSS Normalized Avg. Request Latency Figure 3: Performance of CDE, K-SVM, RNN-HSS, Sibyl, Sibyl K-SVM, Sibyl RNN-HSS, CDE RL-Migr, SAPM and Or- acle on performance-optimized (top) and cost-optimized (bot- tom) HSS configurations. Performance is shown as average I O latency normalized to Fast-Only. Lower is better. only achieves 49.9 (41.4 ) of Oracle s performance in performance- (cost-) optimized HSS, which strongly implies the need for holistic data management in HSS. Sibyl s RL-based data-placement tech- nique can adapt to workload and system changes, but its heuristic data-eviction policy on the critical path lowers its performance.\n\n--- Segment 13 ---\nonly achieves 49.9 (41.4 ) of Oracle s performance in performance- (cost-) optimized HSS, which strongly implies the need for holistic data management in HSS. Sibyl s RL-based data-placement tech- nique can adapt to workload and system changes, but its heuristic data-eviction policy on the critical path lowers its performance. Second, Sibyl outperforms all other prior techniques, improv- ing the average performance over CDE, K-SVM, and RNN-HSS by 20.7 (14.5 ), 24.7 (31.6 ), and 19.3 (19.2 ), respectively, in the performance- (cost-) optimized HSS. Sibyl outperforms CDE, K-SVM and RNN-HSS because they use heuristic- or supervised- learning-based approaches, which cannot easily adapt to changes in workload access patterns and HSS conditions. Third, Sibyl K-SVM and Sibyl RNN-HSS underperform com- pared to Sibyl. A naive combination of prior approaches performs poorly as the placement and migration policies are oblivious to each other s decisions. This leads to (1) conflicting decisions from the two policies, and (2) delayed data-placement policy convergence. Fourth, CDE RL-Migr shows lower performance than Sibyl by 4.6 (15.6 ) on average in performance- (cost-) optimized HSS. CDE RL-Migr shows comparable performance to Sibyl in read- intensive workloads (i.e., RocksDB, MLPerf) because the RL-based migration agent proactively prefetches frequently-read data to the fast device, which Sibyl does not do. However, it underperforms in write-intensive workloads (i.e., SYSTOR17) because CDE s heuris- tic data-placement policy places most incoming data in the fast storage device without considering device characteristics, result- ing in frequent migrations. CDE RL-Migr s performance suggests that an RL-based data-migration policy can benefit performance if combined with an adaptive placement policy.\n\n--- Segment 14 ---\nHowever, it underperforms in write-intensive workloads (i.e., SYSTOR17) because CDE s heuris- tic data-placement policy places most incoming data in the fast storage device without considering device characteristics, result- ing in frequent migrations. CDE RL-Migr s performance suggests that an RL-based data-migration policy can benefit performance if combined with an adaptive placement policy. Fifth, SAPM shows lower performance compared to Sibyl by 18.8 (27.7 ) on average in the performance- (cost-) optimized HSS, even though SAPM uses RL for both data placement and migration. SAPM s limited performance is due to two key reasons. (1) SAPM s single RL agent is unable to learn an optimal policy for data placement and data migration concurrently, and (2) SAPM cannot identify migration candidates in a timely manner because it focuses on the data-migration task only when there are no incoming I O requests. We provide a detailed analysis on why a single RL agent cannot optimize two different tasks in 3.2. 4 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS 3.5 Our Goal Based on our observations and analyses in 3.1, 3.2, 3.3, and 3.4, we conclude that a multi-agent RL approach s ability to optimize different tasks concurrently alleviates the limitations of a single- agent RL approach. Our goal is to design a holistic HSS data-management tech- nique that optimizes both data-placement and data-migration poli- cies using a multi-agent RL approach to improve the overall HSS performance. 4 Harmonia We design Harmonia as an online multi-agent RL technique for HSS data management with two autonomous RL agents: a data- placement agent and a data-migration agent. We implement Har- monia in the HSS management layer in the host operating system. In this section, we first describe the challenges of designing a multi- agent RL-based technique for HSS data management, and then explain Harmonia s design and implementation in detail. 4.1 Challenges of Designing a Multi-Agent HSS Technique Designing a multi-agent RL-based HSS data-management tech- nique poses three key challenges.\n\n--- Segment 15 ---\nIn this section, we first describe the challenges of designing a multi- agent RL-based technique for HSS data management, and then explain Harmonia s design and implementation in detail. 4.1 Challenges of Designing a Multi-Agent HSS Technique Designing a multi-agent RL-based HSS data-management tech- nique poses three key challenges. First, we need to design two autonomous RL agents to optimize the different tasks of data place- ment and migration (see 3.2). Second, we need to formulate the reward structures of the two RL agents to enable coordination be- tween them (see 4.3) and improve HSS performance. Third, having multiple RL agents can increase computational complexity and storage overhead (see 4.6). While multi-agent RL itself is not a fun- damentally new approach to system optimization, prior techniques (e.g., [84 93]) cannot be adapted directly for HSS data management because they use (1) identical RL agents with a single objective, and (2) joint-action space and shared reward functions for all RL agents. 4.2 Harmonia: RL Formulation Fig. 4 shows the overview of Harmonia s multi-agent RL framework. The key goal of the data-placement agent is to place incoming I O requests in the best-fit storage device, and that of the data-migration agent is to identify migration candidates from previously-placed pages and determine their target storage device. We design the state, action, and reward of the two agents to enable coordination between them and maximize the overall HSS performance. Data Placement Agent Action Reward (Immediate) State Data Migration Agent Action Environment Reward (Delayed) State Figure 4: RL Agents in Harmonia. State. Table 1 summarizes the state features for Harmonia s two RL agents. We design a common set of seven state features because both agents consider the current HSS conditions and data characteristics when making their decisions.\n\n--- Segment 16 ---\nTable 1 summarizes the state features for Harmonia s two RL agents. We design a common set of seven state features because both agents consider the current HSS conditions and data characteristics when making their decisions. The state features are represented as a 7-dimensional tuple (observation vector) for both agents: 洧녝洧노 (洧洧뉧롐_洧멇롐뒳롐洧뉧롐,洧洧뉧롐_洧노洧녽洧녷洧뉧롐,洧녩洧녫洧녫_洧녰洧녵洧노洧洧노,洧녩洧녫洧녫_洧녭洧洧뉧롐륋롐, 洧녭洧녩洧멇롐_洧녫洧녩洧녷洧노,洧녫洧녹洧洧_洧녬洧뉧롐洧노,洧녴洧녰洧녮洧_洧녰洧녵洧노洧洧노). (1) The observation vector size directly impacts Harmonia s storage and computational overheads (see 4.6), so we (1) carefully select a limited set of highly relevant features, and (2) group state feature values into a small number of bins (Table 1) based on workload characterization. For example, request size can fall into eight size classes (e.g., 4 KiB, 8 KiB, 16 KiB) requiring three bits to represent. In total, we use 32 bits for encoding the state representation.\n\n--- Segment 17 ---\nFor example, request size can fall into eight size classes (e.g., 4 KiB, 8 KiB, 16 KiB) requiring three bits to represent. In total, we use 32 bits for encoding the state representation. Table 1: State features used by Harmonia Feature Description of bins Encoding (bits) 洧洧뉧롐_洧노洧녽洧녷洧 Request type (read write) 2 1 洧洧뉧롐_洧멇롐뒳롐洧 Request size (in pages) 8 3 洧녩洧녫洧녫_洧녰洧녵洧노洧 Access interval of the requested page 64 8 洧녩洧녫洧녫_洧녭洧洧뉧롐 Access frequency of the requested page 64 8 洧녭洧녩洧멇롐_洧녫洧녩洧녷 Free space in the fast storage device 8 3 洧녫洧녹洧洧_洧녬洧뉧롐 Storage device where the requested page currently resides 2 1 洧녴洧녰洧녮洧_洧녰洧녵洧노洧 Migration interval of a page 64 8 Action. The action space of Harmonia s agents depends on the number of devices in the HSS. The data-placement agent receives the state features of incoming write requests, and its action is to de- termine the best-fit device to place request data. The data-migration agent continuously looks at the state features of previously-placed pages to determine their target device for migration. Therefore, the two agents do not perform their actions in synchronization. In an HSS with two devices, (1) the data-placement agent can place data in the fast or the slow device, and (2) the data-migration agent can migrate data to the fast or the slow device. The action sets of both agents are small and extensible, resulting in less computational complexity and easier multi-device scalability. We encode actions with 4 bits, allowing extensibility to more storage devices. Reward. The reward structure is key to finding an optimal policy in RL. We design the reward structures of Harmonia s agents based on two key factors.\n\n--- Segment 18 ---\nThe reward structure is key to finding an optimal policy in RL. We design the reward structures of Harmonia s agents based on two key factors. First, since we focus on HSS performance, we use I O request latencies in our reward structure for both agents. I O request latencies capture the internal state (e.g., queueing de- lays, buffer dependencies, bandwidth utilization, effects of garbage collection, read write latencies, non-blocking operations, error han- dling latencies) of storage devices in the HSS. Second, we design the reward structures to enable coordination between the two agents. For every placement action at time step 洧노, the data-placement agent receives a reward 洧녠洧녷洧녳洧녩洧녫洧뉧롐뛿롐뉧롐洧노from the environment at time step 洧노 1 that is inversely proportional to the I O request latency: 洧녠洧녷洧녳洧녩洧녫洧뉧롐뛿롐뉧롐洧노 1 洧洧노 (2) where 洧洧노is I O request latency at time step 洧노. The reward is higher (lower) if the data is placed in the fast (slow) device. We design a 5 simple reward for the data-placement agent because its objective is to minimize the latency of the ongoing I O request. The data-migration agent s objective is to maximize long-term HSS performance, which is different from the placement agent s objective (see 3.2). Unlike the immediate impact of data placement on HSS performance, the impact of data migrations (i.e., prefetching frequently-accessed data to the fast device and evicting cold data to the slow device) may be delayed due to two key reasons: (1) the application may not access the prefetched data immediately, and (2) the application may not immediately leverage the free space created in the fast device by cold data migration. Hence, we provide a delayed reward to the migration agent to capture the long-term impact of its decisions.\n\n--- Segment 19 ---\nUnlike the immediate impact of data placement on HSS performance, the impact of data migrations (i.e., prefetching frequently-accessed data to the fast device and evicting cold data to the slow device) may be delayed due to two key reasons: (1) the application may not access the prefetched data immediately, and (2) the application may not immediately leverage the free space created in the fast device by cold data migration. Hence, we provide a delayed reward to the migration agent to capture the long-term impact of its decisions. The reward for the migration agent is: 洧녠洧녴洧녰洧녮洧_洧녬洧뉧롐뙗롐뀛롐뷣롐뉧롐 洧녵 칈洧노 洧녵 洧녰 洧노 1 洧洧녰 洧녞洧녴洧녰洧녮洧洧노 after migrating 洧논pages 0 otherwise (3) where 洧노is the current time step, 洧녵is the number of incoming I O requests whose data-placement latencies are considered, 洧洧녰is the latency of I O request 洧녰, 洧녞洧녴洧녰洧녮洧洧노is a penalty based on migra- tion and access intervals, and 洧논is the number of pages migrated before providing a reward. This reward is based on the average data- placement latencies (i.e., 칈 洧洧녰 洧녵) for 洧녵I O requests that arrive after 洧논 migration candidates are moved to their target devices, and serves as feedback for the migration decisions made previously. To avoid repeated migrations of the same page (ping-pong migrations), we add a small penalty 洧녞洧녴洧녰洧녮洧洧노whose value is inversely proportional to the average migration and access intervals (See Table 1) of 洧논pages. Based on empirical analysis (see 6.1), we set the value of 洧녵to 50 and 洧논to 10, the size of the migration queue.\n\n--- Segment 20 ---\nTo avoid repeated migrations of the same page (ping-pong migrations), we add a small penalty 洧녞洧녴洧녰洧녮洧洧노whose value is inversely proportional to the average migration and access intervals (See Table 1) of 洧논pages. Based on empirical analysis (see 6.1), we set the value of 洧녵to 50 and 洧논to 10, the size of the migration queue. We use a half-precision floating-point (16-bit) representation for the reward structure of both agents to reduce the storage overhead of training data. 4.3 Coordination Between Harmonia s Agents We achieve coordination between the two agents through (1) their rewards, and (2) the effects of their actions on the HSS environment. The placement agent s reward (See Equation 2) encourages effective utilization of the fast device, which is possible only if the migration agent proactively migrates cold (hot) data to the slow (fast) device. The data-migration agent receives a delayed reward (See Equation 3) that is based on access latencies of 洧녵incoming I O requests after migrating several candidate pages. In this way, each agent s decisions influence the policy learned by the other agent. Mitigating Conflicting Decisions. To prevent conflicting place- ment and migration decisions, we use two key techniques. First, the migration agent prioritizes the monitoring of pages with high access and migration intervals, and ignores recently placed or migrated pages (e.g., if the page s access interval is 1, i.e., it was recently placed). Second, we add a small penalty based on access and migra- tion intervals to the migration agent s reward (See Equation 3) to discourage frequent page movements. 4.4 Harmonia: Design Fig. 5 shows the key components of Harmonia. We describe these key components below. I O Request from Application Placement in Fast or Slow Device in HSS Hybrid Storage System Fast Device Slow Device Data previously stored in HSS Candidate pages to migrate Migration Queue Coordination between the agents Harmonia Data-Placement Agent Training Thread Inference Thread Data-Migration Agent Training Thread Inference Thread Migration Thread Perform background migration during idle time Figure 5: Overview of Harmonia. Data-Placement Agent 1 .\n\n--- Segment 21 ---\nI O Request from Application Placement in Fast or Slow Device in HSS Hybrid Storage System Fast Device Slow Device Data previously stored in HSS Candidate pages to migrate Migration Queue Coordination between the agents Harmonia Data-Placement Agent Training Thread Inference Thread Data-Migration Agent Training Thread Inference Thread Migration Thread Perform background migration during idle time Figure 5: Overview of Harmonia. Data-Placement Agent 1 . An RL agent that places I O request data in the best-fit storage device in the HSS 2 based on I O re- quest characteristics and HSS conditions. This agent consists of two threads. First, the data-placement training thread 3 is a background process in which the agent is trained. Second, the data-placement inference thread 4 is Harmonia s only foreground thread. In this main decision thread, the data-placement agent decides the target storage device for incoming I O request data on the critical path. Data-Migration Agent 5 . We implement the data-migration agent using two background threads to avoid interference with the I O request handling on the critical path. First, the data-migration training thread 6 is used for training the agent. Second, in the data-migration inference thread 7 , the agent performs three key tasks: (i) continuously monitors previously-placed pages in the HSS. (ii) determines migration candidates and their target devices. (iii) pushes the metadata of the migration candidates into the migration queue 8 . A page is not a migration candidate if the target device determined by the migration agent is the same as its current device. Migration Queue 8 . A queue that stores the logical page address and the target device identifier (e.g., 0 for slow device) of each migration candidate identified by the migration agent. Based on empirical analysis (see 6.1), we set the migration queue size to 10 in our experiments to migrate pages in a timely manner during system idle times and avoid stale migration candidates. Migration Thread 9 . A background process that continuously monitors the migration queue and migrates pages across devices during system idle times. Harmonia samples device bandwidth at frequent intervals using real-time tools (e.g., iostat) to perform migrations.\n\n--- Segment 22 ---\nA background process that continuously monitors the migration queue and migrates pages across devices during system idle times. Harmonia samples device bandwidth at frequent intervals using real-time tools (e.g., iostat) to perform migrations. For high-intensity workloads with short system idle times, the migration thread checks if pages related to incoming read requests are already in the migration queue and issues low-priority writes [79, 80] to immediately schedule them for migration. Harmonia s five-threaded design prevents the following back- ground processes from interfering with data placement of incoming I O requests: (1) agent training, (2) migration candidate identifica- tion, and (3) data migration across storage devices in the HSS. 4.5 Design of Harmonia s RL Agents Fig. 6 shows the detailed design of each RL agent. Both agents in Harmonia have a similar design using two threads (training and inference) for ease of implementation and low storage overhead. Inference Thread. Harmonia s RL agents use the inference thread to make decisions while collecting experiences to use as training data. In this thread, the agent (1) observes the input features (state) 6 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS Max Inference Network Policy Experience Training Dataset Periodic Weight Update Inference Thread RL Agent in Harmonia Training Thread Training Network State Action Reward Action Experience Replay Experience Buffer (stored in DRAM) a c e HSS d c f g h i b Figure 6: Design of RL Agent in Harmonia. a from the I O request data and HSS, (2) performs inference using its inference network b , (3) selects the action c that maximizes d its long-term returns, and (4) collects the state, action and reward e as experiences f in an experience buffer g . Training Thread. In this background thread, Harmonia uses col- lected experiences h from the experience buffer to train the training network i and optimize the placement or migration policy. Experience Buffer.\n\n--- Segment 23 ---\nIn this background thread, Harmonia uses col- lected experiences h from the experience buffer to train the training network i and optimize the placement or migration policy. Experience Buffer. Experiences are tuples of 洧녡洧노洧녩洧노洧,洧냢洧녫洧노洧녰洧녶洧녵, 洧녠洧뉧롐벓롐뀛롐洧녬, 洧녜洧뉧롐봻롐뫯롐洧노洧녩洧노洧 collected by the inference thread of each agent and stored in an experience buffer to use as training data. We use separate experience buffers for the two agents because they have different goals and experiences. Based on empirical analysis, we set the experience buffer size to store the most recent 1000 experiences. A larger experience buffer can capture more access patterns but comes with significant storage overhead. The experience buffers reside in the host DRAM. Harmonia uses experience replay [29, 99], where a batch of experiences is randomly sampled from the experience buffer and used as training data for the RL agent. Training and Inference networks. We separate the RL agent s training and inference networks to allow parallel execution of train- ing and inference, and remove training latency overhead from I O request handling. Training network weights are periodically copied to the inference network (after every 1000 requests) for the infer- ence network to adapt to current workload and HSS conditions. For the training and inference networks of both agents, we use an identical simple feed-forward neural network structure [29] with one hidden layer of 10 neurons [100] that uses use the swish activa- tion function [101]. Harmonia s data-placement and data-migration policies select the action with the maximum predicted Q-value. RL Algorithm. We use C51, a categorical Deep Q-Network [102] for both data-placement and data-migration agents to update the Q-values 洧녟(洧녡,洧냢) (See 2). C51 aims to learn the distribution of Q-values for each state-action pair, which helps Harmonia capture more information from the environment [29, 103]. Hyper-Parameter Tuning.\n\n--- Segment 24 ---\nC51 aims to learn the distribution of Q-values for each state-action pair, which helps Harmonia capture more information from the environment [29, 103]. Hyper-Parameter Tuning. We improve Harmonia s accuracy by tuning the hyper-parameters [104] of each agent using cross- validation [105] of different values. Harmonia s agents need only one-time offline hyper-parameter tuning. Table 2 shows the hyper- parameter values chosen based on empirical analysis. The discount factor (洧) determines the balance between imme- diate and future rewards, while the learning rate (洧띺) is the rate at which neural network weights are updated. The exploration rate (洧랬) balances exploration and exploitation for Harmonia s policies. Batch size determines the number of samples processed in each training iteration. Experience buffer size is the number of most recent experiences stored for training the networks. Table 2: Hyper-parameters Hyper Parameter Design Space Placement Agent Migration Agent Discount Factor (洧) 0-1 0.9 0.1 Learning Rate (洧띺) 1洧 5 1洧0 1洧 3 1洧 2 Exploration Rate (洧랬) 0-1 0.001 0.001 Batch Size 64-256 128 256 Experience Buffer Size 10-10000 1000 1000 Exploration vs. Exploitation. Harmonia s RL agents start with no prior knowledge of the workload or HSS, and make random initial decisions (explore), and use their experiences (exploit) to gradually make optimal decisions. To balance exploration and exploitation, we use the 洧랬-greedy policy [106]: the predicted best action is selected with (1-洧랬) probability, and a random action with 洧랬probability. Convergence of RL Agents in Harmonia. Figs 7(a) and 7(b) show the convergence of Harmonia s data-placement and data-migration policies in terms of reduction in training loss for the first 10000 I O requests in YCSB-B (see Table 4) on a performance-optimized HSS.\n\n--- Segment 25 ---\nConvergence of RL Agents in Harmonia. Figs 7(a) and 7(b) show the convergence of Harmonia s data-placement and data-migration policies in terms of reduction in training loss for the first 10000 I O requests in YCSB-B (see Table 4) on a performance-optimized HSS. 0 1 2 3 4 5 0 2 4 6 8 10 0 2 4 6 8 10 Training Loss Number of I O Requests (in thousands) (a) Data-Placement Agent (b) Data-Migration Agent Figure 7: Convergence of Harmonia s (a) Data-Placement and (b) Data-Migration RL agents shown in terms of reduction in training loss. We show only the first 10000 I O requests. We make two key observations: (1) The placement policy con- verges in less than 6000 I O requests without prior training. (2) The migration policy takes longer to converge (around 9000 I O requests) due to indirect and delayed reward to the migration agent. The convergence of Harmonia s policies indicates that the agents learn to coordinate their actions and make optimal decisions. 4.6 Overhead Analysis In this section, we provide an analysis of Harmonia s overhead in terms of (1) inference and training latencies, (2) storage overhead. 4.6.1 Latency Overhead. Harmonia has low inference and training latencies because of its (1) lightweight RL agent design, (2) sepa- ration of training and inference, and (3) background execution of multiple threads (e.g., data-migration inference thread). Harmonia s two RL agents have identical networks. We perform training and inference on host CPU as the neural networks are lightweight, and their weights fit in the CPU on-chip caches in our evaluated system. Inference latency. In Harmonia, only the placement agent s infer- ence latency affects I O latency because it is on the critical path of I O request handling. The migration agent runs in the background and its inference latency does not impact ongoing requests. Both agents inference networks have an input layer with 7 neurons (for 7 the state features listed in Table 1), one 10-neuron hidden layer and an output layer with 2 neurons representing the agent s actions (for HSS with two storage devices). Each inference requires 90 MAC operations (7 10 10 2).\n\n--- Segment 26 ---\nBoth agents inference networks have an input layer with 7 neurons (for 7 the state features listed in Table 1), one 10-neuron hidden layer and an output layer with 2 neurons representing the agent s actions (for HSS with two storage devices). Each inference requires 90 MAC operations (7 10 10 2). On our evaluated system (see Table 3), these MAC operations consume 90 CPU cycles (240洧녵洧) per core, lower than the I O read latency of a high-end SSD ( 3洧랞s) [47, 49]. Training latency. In Harmonia, RL agent training is separated from inference and executed in the background. Hence, training latency does not impact I O request latency. Harmonia computes 184,320 MAC operations for each training step. We use 16 batches per training step, where each batch of 128 training samples requires (128 7 10 128 10 2) MAC operations. This computation takes 53洧랞s on our evaluated system with 200,000 cycles per core. 4.6.2 Storage Overhead. Harmonia s storage overhead includes (1) RL agents neural networks, (2) experience buffers, (3) migration queue, (4) migration agent s reward, and (5) address mapping in- formation. First, neural network weights are represented using a half-precision floating-point format. With 90 16-bit weights, each network requires 1.5 KiB of memory. The four networks (training and inference networks for two agents) take a negligible 6 KiB of memory, so we store their weights in the CPU caches. Second, to train each RL agent, we use an experience buffer located in the DRAM that consumes 100 KiB to store 1000 experiences. Hence, the total storage overhead of two experience buffers is 200 KiB. Overall, Harmonia s RL agents require 206 KiB of memory. Third, the migra- tion queue holds the logical block address (32 bits) and target device ID (4 bits) for each queue entry. The migration queue size is 10 in our experiments, requiring 50 bytes of DRAM. Fourth, we store the latencies of 50 incoming I O requests temporarily to assign a delayed reward to the migration agent, which consumes 100B. Fifth, we maintain address mapping information for HSS devices [107]. Harmonia requires 32 bits to store the state features for each page (see Table 1).\n\n--- Segment 27 ---\nFifth, we maintain address mapping information for HSS devices [107]. Harmonia requires 32 bits to store the state features for each page (see Table 1). This overhead is less than 0.8 of the total storage capacity when using a 4-KiB data placement granularity. 5 Evaluation Methodology We evaluate Harmonia on a real system with an HSS with two (dual-HSS), three (tri-HSS), and four (quad-HSS) devices. The dual- HSS configurations include: (1) performance-optimized HSS, which combines a high-end SSD [47] with a mid-range SSD [48], and (2) cost-optimized HSS, which pairs the high-end SSD [47] with a low-end HDD [108]. We implement a custom block device driver in Linux for Harmonia to interface with HSS devices. We implement Harmonia using the TF-Agents [109] library. Table 3 describes our real system and the HSS configurations. Baselines. We compare Harmonia against four state-of-the-art heuristic- and machine-learning-based data-placement and migra- tion techniques, (1) CDE [13], (2) RNN-HSS (adapted from [30]), (3) K-SVM [33], and (4) Sibyl [29]. We evaluate four extended ap- proaches, Sibyl K-SVM, Sibyl RNN-HSS, CDE RL-Migr and SAPM to explore the design space of holistic data-management techniques. We also compare against two ideal approaches: (1) Fast-Only, and (2) Oracle [98] (See 3). Oracle serves as a reference to evaluate the accuracy of Harmonia s decisions as it makes optimal decisions based on knowledge of future access patterns.\n\n--- Segment 28 ---\nWe also compare against two ideal approaches: (1) Fast-Only, and (2) Oracle [98] (See 3). Oracle serves as a reference to evaluate the accuracy of Harmonia s decisions as it makes optimal decisions based on knowledge of future access patterns. Table 3: Host System and HSS Configurations Host System AMD Ryzen 7 2700G[110], GHz, 8 64 32 KiB L1-I D, 4 MiB L2, 8 MiB L3, 16 GiB RDIMM DDR4 2666 MHz Storage Devices Characteristics H: Intel Optane SSD P4800X [47] 375 GB, PCIe 3.0 NVMe, SLC, R W: 2.4 2 GB s, random R W: 550000 500000 IOPS M: Intel SSD D3-S4510 [48] 1.92 TB, SATA TLC (3D), R W: 560 510 MB s, random R W: 895000 21000 IOPS L: Seagate HDD ST1000DM010 [108] 1 TB, SATA 6Gb s 7200 RPM Max. Sustained Transfer Rate: 210 MB s LSSD: ADATA SU630 SSD [111] 960 GB, SATA, TLC, R W: 520 450 MB s PMEM (Emulated): Intel Optane Persistent Memory 200 Series [112] 128 GB, Memory Mode R W: 7.45 2.25 GB s (256B) HSS Configurations Devices Performance-Optimized high-end (H) middle-end (M) Cost-Optimized high-end (H) low-end (L) HSS with PMEM Emulated PMEM (PMEM) high-end SSD (H) Tri-HSS high-end (H) middle-end (M) low-end (L) Quad-HSS high-end (H) middle-end (M) low-end SSD (LSSD) low-end HDD (L) Workloads. We select seventeen data-intensive storage workloads from SYSTOR 17 [113], RocksDB traces [114], Yahoo! Cloud Serv- ing Benchmark (YCSB) suite [115], and MLPerf Storage3 [116] from real enterprise and datacenter environments. The average workload size is approximately 50000x the fast device capacity.\n\n--- Segment 29 ---\nCloud Serv- ing Benchmark (YCSB) suite [115], and MLPerf Storage3 [116] from real enterprise and datacenter environments. The average workload size is approximately 50000x the fast device capacity. We choose these workloads to represent diverse I O access patterns with dif- ferent read and write ratios, I O request sizes, and inter-request times. Table 4 reports the characteristics of the chosen workloads. In our evaluation, each workload runs in a separate thread. Table 4: Characteristics of the evaluated I O traces Benchmark Suite Traces Read Avg. Request Size (KB) Avg. Inter- Request Time (洧랞s) SYSTOR17 [113] LUN0 0.2 31.7 1163.9 LUN1 0.3 34.2 1864.1 LUN2 7.6 31.1 1418.9 LUN3 3.5 42.7 734.5 LUN4 0.5 26.3 823.1 RocksDB [114] ssd-00 79.9 108.9 66.4 ssd-01 73.5 75.1 40.7 ssd-02 79.9 7.5 3.3 ssd-03 79.9 9.5 3.5 ssd-04 79.9 7.8 3.6 YCSB [115] YCSB-B 51.3 45.9 9.3 YCSB-C 47.6 54.6 6.5 YCSB-D 55.9 36.1 8.5 YCSB-E 52.1 46.6 9.6 YCSB-F 49.5 53.1 6.6 MLPerf Storage [116] ResNet50 80.0 172.6 500.1 CosmoFlow 83.4 180.1 1023.8 To evaluate Harmonia under real-world scenarios, we generate six multi-programmed workloads by running multiple workloads concurrently (see Table 5). We choose these multi-programmed workloads based on three key factors: (1) combination of read- intensive, write-intensive and mixed workloads, (2) number of 3We generated the traces by running MLPerf applications on our evaluated system.\n\n--- Segment 30 ---\nInter- Request Time (洧랞s) SYSTOR17 [113] LUN0 0.2 31.7 1163.9 LUN1 0.3 34.2 1864.1 LUN2 7.6 31.1 1418.9 LUN3 3.5 42.7 734.5 LUN4 0.5 26.3 823.1 RocksDB [114] ssd-00 79.9 108.9 66.4 ssd-01 73.5 75.1 40.7 ssd-02 79.9 7.5 3.3 ssd-03 79.9 9.5 3.5 ssd-04 79.9 7.8 3.6 YCSB [115] YCSB-B 51.3 45.9 9.3 YCSB-C 47.6 54.6 6.5 YCSB-D 55.9 36.1 8.5 YCSB-E 52.1 46.6 9.6 YCSB-F 49.5 53.1 6.6 MLPerf Storage [116] ResNet50 80.0 172.6 500.1 CosmoFlow 83.4 180.1 1023.8 To evaluate Harmonia under real-world scenarios, we generate six multi-programmed workloads by running multiple workloads concurrently (see Table 5). We choose these multi-programmed workloads based on three key factors: (1) combination of read- intensive, write-intensive and mixed workloads, (2) number of 3We generated the traces by running MLPerf applications on our evaluated system. 8 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS concurrent workloads required for high I O intensity (i.e., shorter inter-request times), and (3) combination of different request sizes. Each constituent workload is executed using a single thread. For example, mix6 runs on eight concurrent threads for the eight con- stituent workloads. Multi-programmed workloads typically exhibit higher I O request intensity and varied access patterns.\n\n--- Segment 31 ---\nFor example, mix6 runs on eight concurrent threads for the eight con- stituent workloads. Multi-programmed workloads typically exhibit higher I O request intensity and varied access patterns. Table 5: Characteristics of multi-programmed workloads Mix Constituent Workloads [113 115] Description mix1 ssd-02 and LUN4 ssd-02 is read-intensive and LUN4 is write-intensive mix2 LUN1 and ssd-04 LUN0 is write-intensive and ssd-04 is read-intensive mix3 YCSB-C and YCSB-F Both have near-equal read-write ratio mix4 ssd-00, ssd-04, YCSB-A and LUN0 Two read-intensive and two write-intensive workloads mix5 ssd-00, LUN0, YCSB-C and YCSB-F Read-intensive, write-intensive and two workloads with a near-equal read-write ratio mix6 YCSB-B, YCSB-D, LUN0, LUN1, LUN4, ssd-00, ssd-02, ssd03 Two with near-equal read-write ratio, three write-intensive and three read-intensive 6 Evaluation 6.1 Performance Analysis Average I O Request Latency and End-to-End Throughput. Figs. 8(a) and 8(b) show the performance of CDE, K-SVM, RNN- HSS, Sibyl, Sibyl K-SVM, Sibyl RNN-HSS, CDE RL-Migr, SAPM, Harmonia and Oracle on performance- and cost-optimized HSS configurations. Figs. 9(a) and 9(b) show their end-to-end throughput on performance- and cost-optimized HSS, respectively. 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 30 60 90 120 SYSTOR17 RocksDB YCSB MLPerf AVG Normalized Avg.\n\n--- Segment 32 ---\n9(a) and 9(b) show their end-to-end throughput on performance- and cost-optimized HSS, respectively. 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 30 60 90 120 SYSTOR17 RocksDB YCSB MLPerf AVG Normalized Avg. Request Latency (a) Performance-Optimized HSS (b) Cost-Optimized HSS Figure 8: Performance of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS configurations, shown as average request latency nor- malized to Fast-Only. Lower is better. We make four key observations. First, Harmonia consistently out- performs all baselines in both HSS configurations. In performance- (cost-) optimized HSS, Harmonia improves performance by 49.5 (31.7 ) and end-to-end throughput by 49.4 (156.2 ) over the best- performing prior approach, Sibyl, on average. Second, Harmonia bridges the performance gap between Sibyl and Oracle by 64.2 (64.3 ) and achieves 71.7 (62.2 ) of Oracle s throughput on average in performance- (cost-) optimized HSS. 0.0 0.2 0.4 0.6 0.8 1.0 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0.00 0.02 0.04 0.06 0.08 0.10 SYSTOR17 RocksDB YCSB MLPerf AVG Normalized End-to-End Throughput (a) Performance-Optimized HSS (b) Cost-Optimized HSS 0.20 0.48 0.16 Figure 9: End-to-end throughput of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS. Throughput values (in IOPS) are normalized to Fast- Only policy. Higher is better.\n\n--- Segment 33 ---\nThroughput values (in IOPS) are normalized to Fast- Only policy. Higher is better. Third, Harmonia shows higher performance and throughput benefits over baselines in read-intensive workloads (e.g., RocksDB, MLPerf) compared to write-intensive workloads. For example, Harmonia outperforms Sibyl by 57.5 (43.2 ) in read-intensive workloads and 41.0 (39.9 ) in write-intensive workloads. In read- intensive workloads, unlike Sibyl, Harmonia proactively prefetches frequently-read pages to the fast device to improve read perfor- mance even when there are no writes from the application, resulting in lower latencies and higher throughput. In write-intensive work- loads, while both Harmonia and Sibyl use RL-based placement, Harmonia s migration agent, in coordination with the placement agent, further improves performance by frequently migrating cold data from the fast device to accommodate new incoming requests. Fourth, Harmonia shows higher performance and throughput gains over baselines (other than Sibyl) in cost-optimized than in performance-optimized HSS. This is because (1) data migration in cost-optimized HSS is more expensive than in performance- optimized HSS due to the large latency gap between the two devices, (2) prior approaches perform migration during I O request handling, which adds significant latency overheads, and (3) Harmonia frees up space in the fast device with proactive migrations and performing migrations during system idle times. Sibyl s RL agent places only performance-critical data in the fast device, reducing the number of data evictions on the critical path. Tail Latency. Figs. 10(a) and 10(b) show the 99th and 99.99th per- centile I O latencies (tail latency) of Harmonia and baselines on performance-optimized HSS. For this analysis, we choose three representative workloads, LUN0 (write-intensive), ssd-00 (read- intensive) and YCSB-B (mixed) (see 5 for workload characteristics). We make two key observations. First, Harmonia significantly reduces tail latency compared to all baselines across the three work- loads.\n\n--- Segment 34 ---\nWe make two key observations. First, Harmonia significantly reduces tail latency compared to all baselines across the three work- loads. Harmonia reduces the 99th (99.99th) percentile I O latency by 32.3 (25.3 ) on average over Sibyl. Harmonia has lower tail latency because, unlike prior techniques, Harmonia performs data migration during system idle times to avoid interference with on- going I O requests. Second, K-SVM, RNN-HSS, Sibyl K-SVM and Sibyl RNN-HSS show very high tail latencies because they per- form a large volume of migration at fixed intervals, which adds a significant latency overhead to the ongoing I O requests. We conclude that Harmonia improves average I O latency, end- to-end throughput, and tail latency across all workloads due to its holistic and coordinated data-management approach. 9 Figure 10: Comparison of tail latencies of Harmonia and baselines in the 99th percentile (top) and 99.99th percentile (bottom) of I O requests from three representative workloads, LUN0 (write-intensive), ssd-00 (read-intensive) and YCSB-B (mixed), on performance-optimized HSS. Performance Sensitivity to Migration Queue Size. Fig. 11(a) shows the impact of migration queue size on HSS performance. We make three key observations. First, optimal migration queue size is 10 for our evaluated workloads. This may vary slightly across workloads and HSS configurations. Second, low queue depths (i.e., 5) may result in many migration candidates being dropped, which leads to performance degradation. If the migration queue size is 0, Harmonia shows poor performance because it does not perform any data migration. Third, larger-sized queues can hold more migration candidates, but the oldest candidates may become stale (i.e., past migration decisions may not correlate with current access patterns and HSS conditions), which adversely impacts HSS performance. Performance Sensitivity to Migration Agent s Reward. Fig. 11(b) shows the performance impact of the number of I O requests considered for the migration agent s delayed reward (See Equation 3). We make three key observations. First, the optimal Normalized Avg.\n\n--- Segment 35 ---\nWe make three key observations. First, the optimal Normalized Avg. I O Latency 25 27 29 31 33 35 0 50 100 150 200 250 Migration Queue Size (a) 0 50 100 150 200 250 Number of I O requests used for migration agent s reward (b) Figure 11: Performance sensitivity to (a) migration queue sizes, and (b) number of future incoming requests used in migration agent s reward structure. Performance is shown as average I O latency normalized to Fast-Only. Lower is better. number of requests is 50 in our evaluated configuration, but may vary slightly across workloads and HSS configurations, and depends on factors including (1) inter-request times of incoming I O requests after migration, (2) workload access patterns (e.g., access frequency and reuse distance of a page), and (3) system conditions (e.g., device capacity utilization). Second, considering fewer requests (e.g., 10) may not provide the right reward to the agent because the effect of migrations is not seen immediately in the placement latencies of these few requests. Third, if we consider more I O requests (e.g., 250), we see lower performance because of two reasons: (i) the impact of data migration is diluted if we consider a large number of incoming requests, and (ii) I O requests that arrive too far into the future after data migration may not see a direct impact. Performance on HSS with Persistent Memory (PMEM) De- vices. To demonstrate the Harmonia s extensibility to emerging persistent memory technologies, we evaluate Harmonia and base- lines on an HSS with a persistent memory device [112] and a high- end SSD [47] (see Table 3). We emulate the PMEM device using RAMDisk because these emerging devices are not easily available. Fig. 12 shows the performance of Harmonia and baselines on HSS with PMEM. Performance is shown as average request latency nor- malized to Fast-Only (i.e., PMEM device can store all data). 0 4 8 12 16 SYSTOR17 RocksDB YCSB MLPerf AVG CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle Normalized Avg.\n\n--- Segment 36 ---\nPerformance is shown as average request latency nor- malized to Fast-Only (i.e., PMEM device can store all data). 0 4 8 12 16 SYSTOR17 RocksDB YCSB MLPerf AVG CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle Normalized Avg. Request Latency Figure 12: Performance of Harmonia and baselines on an HSS with PMEM [112] and high-end SSD [47] shown as average request latency normalized to Fast-Only. Lower is better. We make three key observations. First, Harmonia outperforms the best-performing baseline, CDE, by 15.6 and achieves 80 of Or- acle s performance. Second, CDE outperforms other baselines in this HSS configuration as it places most data in the low-latency PMEM device. However, CDE s frequent data evictions on the critical path lower its performance compared to Harmonia. CDE performs better in read-intensive workloads because they consist of fewer writes and require fewer evictions. Third, in read-intensive workloads (i.e., RocksDB, MLPerf), Harmonia shows a higher performance improvement as it proactively prefetches data to the PMEM de- vice, which leads to more reads from the PMEM device. We discuss Harmonia s applicability to emerging memory technologies in 7. 6.2 Multi-Programmed Workloads Figs. 13(a) and 13(b) show the performance of Harmonia and base- lines on multi-programmed workloads (see Table 5) on performance- and cost-optimized HSS, respectively. We make two key observations. First, Harmonia outperforms prior approaches in all multi-programmed workloads across both HSS configurations. In performance- (cost-) optimized HSS, Har- monia (i) outperforms Sibyl by 32.8 (25.3 ), and (ii) achieves 75 (46 ) of Oracle s performance. Second, Harmonia s performance improves as the number of concurrent workloads increases. On eight concurrent workloads (e.g., mix6), Harmonia shows higher performance gains, 39.4 (41.8 ), over Sibyl compared to other multi-programmed workloads.\n\n--- Segment 37 ---\nSecond, Harmonia s performance improves as the number of concurrent workloads increases. On eight concurrent workloads (e.g., mix6), Harmonia shows higher performance gains, 39.4 (41.8 ), over Sibyl compared to other multi-programmed workloads. Harmonia s RL-based policies learn access pattern variations faster than prior approaches. 10 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS 0 2 4 6 8 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle (a) Performance-Optimized HSS 0 40 80 120 160 mix1 mix2 mix3 mix4 mix5 mix6 AVG (b) Cost-Optimized HSS Normalized Avg. Request Latency Figure 13: Performance of Harmonia and baselines on multi- programmed workloads on performance-optimized (top) and cost-optimized (bottom) HSS. Performance is shown as aver- age request latency normalized to Fast-Only. Lower is better. 6.3 Extensibility to Multiple Devices in HSS We evaluate Harmonia s extensibility to different HSS configura- tions by comparing its performance with the best-performing prior approach, Sibyl, on an HSS with three and four devices (see Table 3). We do not evaluate other baselines as they are designed for fixed HSS configurations and lack easy extensibility. Figs. 14(a) and 14(b) show the performance of Sibyl and Harmonia on a Tri- and Quad-HSS. We discuss Harmonia s extensibility in 7. 0 30 60 90 Sibyl Harmonia 0 30 60 90 SYSTOR17 RocksDB YCSB MLPerf AVG (a) Tri-HSS (b) Quad-HSS Normalized Avg. Request Latency Figure 14: Performance of Sibyl and Harmonia on Tri-HSS (top) and Quad-HSS (bottom). Performance is shown as aver- age request latency normalized to Fast-Only. Lower is better. We make two key observations.\n\n--- Segment 38 ---\nLower is better. We make two key observations. First, Harmonia outperforms Sibyl by 37.0 (42.0 ) on average in Tri-HSS (Quad-HSS) as Harmo- nia places performance-critical data in the high-end and mid-range devices and uses its migration policy to move cold data to slow devices during idle times. In contrast, Sibyl frequently evicts data to the low-end devices on the critical path, adding significant la- tency overhead. Second, in read-intensive workloads (e.g., RocksDB, MLPerf), Harmonia shows higher performance gains (up to 58.3 ) over Sibyl because Sibyl lacks a migration policy and relies on application updates to migrate data across devices. 6.4 Lifetime of Devices in HSS We evaluate the impact of Harmonia and baselines on storage device lifetimes by measuring their write amplification (WA). WA is the ratio of total data written (including migrations) in the HSS to data originating from the workload. A higher WA adversely impacts device lifetimes. Figs. 15(a) and 15(b) show WA of Harmonia and baselines on performance- and cost-optimized HSS, respectively. We make two key observations. First, only Sibyl and RNN-HSS have a lower WA than Harmonia. Harmonia s average WA of 1.54 (1.55) is higher than Sibyl s 1.49 (1.45) and RNN-HSS s 1.08 (1.06) in performance- (cost-) optimized HSS as it proactively prefetches data to the fast device in read-intensive workloads to improve read performance. Second, in both HSS configurations, Harmonia 0 1 2 3 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 1 2 3 SYSTOR17 RocksDB YCSB MLPerf AVG Write Amplification (b) Cost-Optimized HSS (a) Performance-Optimized HSS Figure 15: Write amplification of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS. Lower is better.\n\n--- Segment 39 ---\nSecond, in both HSS configurations, Harmonia 0 1 2 3 CDE K-SVM RNN-HSS Sibyl Sibyl K-SVM Sibyl RNN-HSS CDE RL-Migr SAPM Harmonia Oracle 0 1 2 3 SYSTOR17 RocksDB YCSB MLPerf AVG Write Amplification (b) Cost-Optimized HSS (a) Performance-Optimized HSS Figure 15: Write amplification of Harmonia and baselines on performance-optimized (top) and cost-optimized (bottom) HSS. Lower is better. has lower WA (1.24) in write-intensive workloads (i.e., SYSTOR17) compared to its WA (1.8) in read-intensive workloads (i.e., RocksDB, MLPerf). In write-intensive workloads, Harmonia learns to move data during update operations and performs fewer migrations. Write Traffic Distribution in Harmonia. Fig. 16 shows the write distribution of Harmonia s migration policy across devices in performance- and cost-optimized HSS. We make two observations. 0 25 50 75 100 SYSTOR17 RocksDB YCSB MLPerf AVG Fast Device Writes Slow Device Writes SYSTOR17 RocksDB YCSB MLPerf AVG (a) Performance- Optimized HSS (b) Cost-Optimized HSS Figure 16: Write traffic due to Harmonia s migrations on performance-optimized (left) and cost-optimized (right) HSS. First, Harmonia migrates more data to slow device as it effectively utilizes fast device for frequently-accessed data. Second, Harmonia has more writes to fast device in read-intensive workloads (i.e., RocksDB, MLPerf) as it proactively prefetches frequently-read data to the fast device even when there are no application updates. We conclude that Harmonia s adaptive policies provide higher performance at slightly higher average WA compared to Sibyl. 7 Discussion Alternate Objectives and Reward Structures. Since Harmonia s focus is to improve HSS performance, we design the RL agents reward structure based on I O latencies. We can design Harmonia for other objectives such as device lifetimes, fairness, quality of service (QoS) and energy efficiency.\n\n--- Segment 40 ---\nSince Harmonia s focus is to improve HSS performance, we design the RL agents reward structure based on I O latencies. We can design Harmonia for other objectives such as device lifetimes, fairness, quality of service (QoS) and energy efficiency. For example, to improve device lifetimes, the state features and reward structure can include (1) writes performed on each device, (2) device endurance in terms of program erase cycles, (3) write amplification (WA), and (4) de- vice reliability properties (e.g., NAND flash type (SLC TLC QLC), ECC capability). To manage write traffic to each device, the reward should be appropriate to the device where the data is placed or 11 migrated. Harmonia can accommodate fairness and QoS objectives by including factors such as bandwidth utilization, device queue depths and application requirements. We leave the exploration of these objectives and reward structures to future work. Extensibility of Harmonia. Harmonia is easily extensible to a large number of storage devices with minimal designer effort. When a new device is added to the HSS, we only add (1) capacity utilization of the new device as a state feature (see Table 1), and (2) an output node in each RL agent s neural network to represent the action of placing or migrating a page to the new device. However, scaling Harmonia to more devices can lead to two challenges: (1) the large state and action spaces can delay the convergence to an optimal policy, and (2) the metadata overhead increases. Applicability to Other Systems. Harmonia can be adapted to other heterogeneous systems such as tiered storage, hybrid memory, disaggregated storage and networked storage, which have diverse constraints and requirements. In a tiered storage system, placement is always performed on the top-tier device, and hence Harmonia s placement agent can be replaced by a heuristic approach. The mi- gration agent gains significance due to data migration across tiers. We can add multiple migration agents for each pair of adjacent tiers. Hybrid memory systems have strict latency constraints, and require Harmonia to be redesigned for low-latency decision-making. Dis- aggregated storage systems that use CXL [117 119] for large-scale memory pooling can leverage Harmonia s benefits when deployed on CXL fabric controller. We hope Harmonia can assist in designing adaptive policies for other heterogeneous systems.\n\n--- Segment 41 ---\nDis- aggregated storage systems that use CXL [117 119] for large-scale memory pooling can leverage Harmonia s benefits when deployed on CXL fabric controller. We hope Harmonia can assist in designing adaptive policies for other heterogeneous systems. 8 Related Work In this section, we review other related work in HSS data manage- ment, and RL in storage memory systems. HSS Data Management. A large body of prior work proposes heuristic- and machine-learning-based data-placement (e.g., [5 7, 9, 13 33, 52 64]) and data-migration (e.g., [7, 11, 33, 69 73]) techniques. Hot Random Off-loading (HRO) [7] uses a 0-1 knap- sack model to allocate or migrate files between the storage devices. Vengerov [72] uses a fuzzy rule base architecture to perform data migration in a hierarchical storage system. These works do not perform (1) real system evaluation or (2) combined optimization of placement and migration policies using multiple RL agents. RL in Storage Systems. Several works (e.g., [120 134]) use RL to improve different aspects of storage systems. Prior works [120, 122] use RL to improve cloud storage utilization. RLAlloc [125] im- proves SSD throughput and Quality of Service (QoS). Other works [121, 123, 124, 126] use RL to optimize SSD maintenance activities (e.g., garbage collection) and error mitigation. Harmonia is orthogonal to all these works and can be used in conjunction with these techniques on an HSS. Multi-Agent RL Systems. Several works propose using multi- agent RL (MARL) for various system optimizations (e.g., [84 93]). Qiu et al. [85 87, 89] use MARL to improve tail latency of Server- less Function-as-a-Service (FaaS), targeting different systems and objectives than Harmonia. Jain et al. [84, 91] use MARL for dy- namic partitioning of last level cache and multi-level caches. Shen et al. [93] propose MARL-assisted cache cleaning for shingled man- aged recording drives.\n\n--- Segment 42 ---\nShen et al. [93] propose MARL-assisted cache cleaning for shingled man- aged recording drives. These techniques have four key differences from Harmonia: (1) their objectives are different from HSS data placement and migration, (2) they use identical RL agents unlike Harmonia s heterogeneous agents, (3) they use joint actions and shared rewards for all agents, and (4) they use table-based RL unlike Harmonia s neural network based RL. RLRP [90] proposes a deep RL-based replica placement and migration technique for distributed storage systems. RLRP has five key differences from Harmonia: (1) RLRP does not handle typical I O requests but focuses on replica placement, (2) RLRP s placement and migration occur at sparse in- tervals, making it unsuitable for HSS, (3) RLRP retrains its policies only when storage nodes are added or removed, while Harmonia continuously adapts its policies, (4) RLRP considers only a few state features (e.g., capacity utilization), and (5) RLRP s design has higher storage overhead than Harmonia. We conclude that these reasons make RLRP unsuitable for data placement and migration in HSS. 9 Conclusion We propose Harmonia, the first multi-agent reinforcement learning based holistic data-management technique for HSS that optimizes data placement and data migration together. We show that Har- monia s design enables coordination between the data-placement and data-migration agents to improve HSS performance. Our real- system evaluation demonstrates that Harmonia significantly out- performs prior HSS data-management techniques on four HSS con- figurations across a wide range of workloads. Harmonia is easily extensible to different HSS configurations. The performance bene- fits of Harmonia come with low latency and storage overheads. References [1] Wenjian Xiao, Huanqing Dong, Liuying Ma, Zhenjun Liu, and Qiang Zhang. HS-BAS: A Hybrid Storage System Based on Band Awareness of Shingled Write Disk. In ICCD, 2016. [2] Chunling Wang, Dandan Wang, Yupeng Chai, Chuanwen Wang, and Diansen Sun. Larger, Cheaper, but Faster: SSD-SMR Hybrid Storage Boosted by a New SMR-Oriented Cache Framework. In MSST, 2017.\n\n--- Segment 43 ---\nLarger, Cheaper, but Faster: SSD-SMR Hybrid Storage Boosted by a New SMR-Oriented Cache Framework. In MSST, 2017. [3] Seongjin Lee, Youjip Won, and Sungwoo Hong. Mining-Based File Caching in a Hybrid Storage System. In JISE, 2014. [4] Wes Felter, Anthony Hylick, and John Carter. Reliability-Aware Energy Man- agement for Hybrid Storage Systems. In MSST, 2011. [5] Kai Bu, Meng Wang, Hongshan Nie, Wei Huang, and Bo Li. The Optimization of the Hierarchical Storage System Based on the Hybrid SSD Technology. In ISDEA, 2012. [6] KR Krish, Bharti Wadhwa, M Safdar Iqbal, M Mustafa Rafique, and Ali R Butt. On Efficient Hierarchical Storage for Big Data Processing. In CCGrid, 2016. [7] Lin Lin, Yifeng Zhu, Jianhui Yue, Zhao Cai, and Bruce Segee. Hot Random Off- Loading: A Hybrid Storage System with Dynamic Data Migration. In MASCOTS, 2011. [8] Junpeng Niu, Jun Xu, and Lihua Xie. Hybrid Storage Systems: A Survey of Architectures and Algorithms. In IEEE Access, 2018. [9] Jianzhe Tai, Bo Sheng, Yi Yao, and Ningfang Mi. SLA-Aware Data Migration in a Shared Hybrid Storage Cluster. In CC, 2015. [10] Jiaxin Ou, Jiwu Shu, Youyou Lu, Letian Yi, and Wei Wang. EDM: An Endurance- Aware Data Migration Scheme for Load Balancing in SSD Storage Clusters. In IPDPS, 2014. [11] Yuxia Cheng, Wenzhi Chen, Zonghui Wang, Xinjie Yu, and Yang Xiang. AMC: An Adaptive Multi-Level Cache Algorithm in Hybrid Storage Systems. In CCPE, 2015. [12] Ziliang Zong, Ribel Fares, Brian Romoser, and Joal Wood. FastStor: Data-Mining- Based Multilayer Prefetching for Hybrid Storage Systems. In CC, 2014. [13] Chihiro Matsui, Chao Sun, and Ken Takeuchi.\n\n--- Segment 44 ---\nIn CC, 2014. [13] Chihiro Matsui, Chao Sun, and Ken Takeuchi. Design of Hybrid SSDs With Storage Class Memory and NAND Flash Memory. In Proc. IEEE, 2017. [14] Feng Chen, David A Koufaty, and Xiaodong Zhang. Hystor: Making the Best Use of Solid State Drives in High Performance Storage Systems. In SC, 2011. [15] Yanfei Lv, Xuexuan Chen, Guangyu Sun, and Bin Cui. A Probabilistic Data Replacement Strategy for Flash-Based Hybrid Storage System. In APWeb, 2013. 12 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS [16] Jorge Guerra, Himabindu Pucha, Joseph S Glider, Wendy Belluomini, and Raju Rangaswami. Cost Effective Storage Using Extent Based Dynamic Tiering. In FAST, 2011. [17] Ahmed Elnably, Hui Wang, Ajay Gulati, and Peter J Varman. Efficient QoS for Multi-Tiered Storage Systems. In HotStorage, 2012. [18] Hui Wang and Peter Varman. Balancing Fairness and Efficiency in Tiered Storage Systems with Bottleneck-Aware Allocation. In FAST, 2014. [19] Gong Zhang, Lawrence Chiu, Clem Dickey, Ling Liu, Paul Muench, and Sangeetha Seshadri. Automated Lookahead Data Migration in SSD-enabled Multi-tiered Storage Systems. In MSST, 2010. [20] Xiaojian Wu and AL Narasimha Reddy. Data Organization in a Hybrid Storage System. In ICNC, 2012. [21] Ilias Iliadis, Jens Jelitto, Yusik Kim, Slavisa Sarafijanovic, and Vinodh Venkatesan. ExaPlan: Queueing-Based Data Placement and Provisioning for Large Tiered Storage Systems. In MASCOTS, 2015. [22] Yanfei Lv, Bin Cui, Xuexuan Chen, and Jing Li. Hotness-Aware Buffer Manage- ment For Flash-Based Hybrid Storage Systems. In CIKM, 2013.\n\n--- Segment 45 ---\nHotness-Aware Buffer Manage- ment For Flash-Based Hybrid Storage Systems. In CIKM, 2013. [23] Chihiro Matsui, Tomoaki Yamada, Yusuke Sugiyama, Yusuke Yamaga, and Ken Takeuchi. Tri-Hybrid SSD with Storage Class Memory (SCM) and MLC TLC NAND Flash Memories. Proc. IEEE, 2017. [24] Zhijie Feng, Zhiyong Feng, Xin Wang, Guozheng Rao, Yazhou Wei, and Zhiyuan Li. HDStore: An SSD HDD Hybrid Distributed Storage Scheme for Large-Scale Data. In WAIM, 2014. [25] Zhengyu Yang, Morteza Hoseinzadeh, Allen Andrews, Clay Mayers, David Thomas Evans, Rory Thomas Bolt, Janki Bhimani, Ningfang Mi, and Steven Swanson. AutoTiering: Automatic Data Placement Manager in Multi- Tier All-Flash Datacenter. In IPCCC, 2017. [26] Youngjae Kim, Aayush Gupta, Bhuvan Urgaonkar, Piotr Berman, and Anand Sivasubramaniam. HybridStore: A Cost-Efficient, High-Performance Storage System Combining SSDs and HDDs. In MASCOTS, 2011. [27] Lei Liu, Shengjie Yang, Lu Peng, and Xinyu Li. Hierarchical Hybrid Memory Management in OS for Tiered Memory Systems. In TPDS, 2019. [28] Yongping Luo, Peiquan Jin, and Shouhong Wan. Optimal Data Placement for Data-Centric Algorithms on NVM-Based Hybrid Memory. In DSAA, 2020. [29] Gagandeep Singh, Rakesh Nadig, Jisung Park, Rahul Bera, Nastaran Hajinazar, David Novo, Juan G칩mez-Luna, Sander Stuijk, Henk Corporaal, and Onur Mutlu. Sibyl: Adaptive and Extensible Data Placement in Hybrid Storage Systems Using Online Reinforcement Learning. In ISCA, 2022.\n\n--- Segment 46 ---\nSibyl: Adaptive and Extensible Data Placement in Hybrid Storage Systems Using Online Reinforcement Learning. In ISCA, 2022. [30] Thaleia Dimitra Doudali, Sergey Blagodurov, Abhinav Vishnu, Sudhanva Guru- murthi, and Ada Gavrilovska. Kleio: A Hybrid Memory Page Scheduler with Machine Intelligence. In HPDC, 2019. [31] Jinting Ren, Xianzhang Chen, Yujuan Tan, Duo Liu, Moming Duan, Liang Liang, and Lei Qiao. Archivist: A Machine Learning Assisted Data Placement Mechanism for Hybrid Storage Systems. In ICCD, 2019. [32] Peng Cheng, Yutong Lu, Yunfei Du, Zhiguang Chen, and Yang Liu. Optimizing Data Placement on Hierarchical Storage Architecture via Machine Learning. In NPC, 2019. [33] Milan Shetti, Bingzhe Li, and David Du. Machine Learning-based Adaptive Migration Algorithm for Hybrid Storage Systems. In TOS, 2019. [34] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole- Jean Wu, Alisson G Azzolini, et al. Deep learning recommendation model for personalization and recommendation systems. arXiv preprint arXiv:1906.00091, 2019. [35] C.Szegedy et al. Inception-v4, inception-resnet and the impact of residual connections on learning. In CVPR, 2016. [36] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840 6851, 2020. [37] Hugo Touvron et al. Llama: Open and efficient foundation language models. CoRR, abs 2302.13971, 2023. [38] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.\n\n--- Segment 47 ---\nCoRR, abs 2302.13971, 2023. [38] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [39] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek- v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [40] Chee-Yong Chan and Yannis E. Ioannidis. Bitmap Index Design and Evaluation. In SIGMOD, 1998. [41] FastBit: An Efficient Compressed Bitmap Index Technology. gov fastbit . [42] Redis. Redis bitmaps. [43] Scott Beamer, Krste Asanovic, and David Patterson. Direction-Optimizing Breadth-First Search. In SC, 2012. [44] Eric S Lander, Lauren M Linton, Bruce Birren, Chad Nusbaum, Michael C Zody, Jennifer Baldwin, Keri Devon, Ken Dewar, Michael Doyle, William Fitzhugh, et al. Initial Sequencing and Analysis of the Human Genome. Nature, 2001. [45] Stephen F Altschul, Warren Gish, Webb Miller, Eugene W Myers, and David J Lipman. Basic Local Alignment Search Tool. JMB, 1990. [46] Gene Myers. A Fast Bit-Vector Algorithm for Approximate String Matching Based on Dynamic Programming. JACM, 1999. [47] Intel. Intel Optane SSD DC P4801X Series, us en ark products 149365 intel-optane-ssd-dc-p4801x-series-100gb-2-5in- pcie-x4-3d-xpoint.html. [48] Intel.\n\n--- Segment 48 ---\nIntel Optane SSD DC P4801X Series, us en ark products 149365 intel-optane-ssd-dc-p4801x-series-100gb-2-5in- pcie-x4-3d-xpoint.html. [48] Intel. Intel SSD D3-S4510 Series, products memory-storage solid-state-drives data-center-ssds d3-series d3- s4510-series d3-s4510-1-92tb-2-5inch-3d2.html. [49] Samsung. Ultra-Low Latency with Samsung Z-NAND SSD, Low_Latency_with_Samsung_Z-NAND_SSD-0.pdf. [50] Samsung. Samsung SSD 980 PRO, " minisite ssd product consumer 980pro ", 2020. [51] Samsung. Samsung SSD 960 PRO NVMe M.2 512GB, "https: www.samsung.com us computing memory-storage solid-state-drives ssd- 960-pro-m-2-512gb-mz-v6p512bw". [52] Chao Sun, Kousuke Miyaji, Koh Johguchi, and Ken Takeuchi. A High Perfor- mance and Energy-Efficient Cold Data Eviction Algorithm for 3D-TSV Hybrid ReRAM MLC NAND SSD. In CAS, 2013. [53] Yang Li, Saugata Ghose, Jongmoo Choi, Jin Sun, Hui Wang, and Onur Mutlu. Utility-Based Hybrid Memory Management. In CLUSTER, 2017. [54] Neha Agarwal, David Nellans, Mark Stephenson, Mike O Connor, and Stephen W Keckler. Page Placement Strategies for GPUs Within Heterogeneous Memory Systems. In ASPLOS, 2015. [55] Neha Agarwal and Thomas F Wenisch. Thermostat: Application-Transparent Page Management for Two-Tiered Main Memory. In ASPLOS, 2017. [56] Tae Jun Ham, Bharath K Chelepalli, Neng Xue, and Benjamin C Lee. Disinte- grated Control for Energy-Efficient and Heterogeneous Memory Systems. In HPCA, 2013.\n\n--- Segment 49 ---\nDisinte- grated Control for Energy-Efficient and Heterogeneous Memory Systems. In HPCA, 2013. [57] Reza Salkhordeh, Hossein Asadi, and Shahriar Ebrahimi. Operating System Level Data Tiering Using Online Workload Characterization. In JSC, 2015. [58] Milan Pavlovic, Nikola Puzovic, and Alex Ramirez. Data Placement in HPC Architectures With Heterogeneous Off-Chip Memory. In ICCD, 2013. [59] Justin Meza, Yixin Luo, Samira Khan, Jishen Zhao, Yuan Xie, and Onur Mutlu. A Case for Efficient Hardware Software Cooperative Management of Storage and Memory. In WEED, 2013. [60] Chiachen Chou, Aamer Jaleel, and Moinuddin K Qureshi. BATMAN: Maximiz- ing Bandwidth Utilization of Hybrid Memory Systems. MEMSYS, 2017. [61] Chenxi Wang, Huimin Cui, Ting Cao, John Zigman, Haris Volos, Onur Mutlu, Fang Lv, Xiaobing Feng, and Guoqing Harry Xu. Panthera: Holistic Memory Management for Big Data Processing over Hybrid Memories. In PLDI, 2019. [62] Luiz E Ramos, Eugene Gorbatov, and Ricardo Bianchini. Page Placement in Hybrid Memory Systems. In ICS, 2011. [63] Thaleia Dimitra Doudali, Daniel Zahka, and Ada Gavrilovska. Cori: Dancing to the Right Beat of Periodic Data Movements over Hybrid Memory Systems. In IPDPS, 2021. [64] Satyabrata Sen and Neena Imam. Machine Learning Based Design Space Explo- ration for Hybrid Main-Memory Design. In MEMSYS, 2019. [65] Chin-Hsien Wu, Cheng-Wei Huang, and Chen-Yu Chang. A Data Management Method for Databases Using Hybrid Storage Systems. ACM SIGAPP Applied Computing Review, 2019. [66] Yuan Hua Yang, Xian Bin Xu, Shui Bing He, and Yu Hua Wen. A Statistics-Based Data Placement Strategy for Hybrid Storage. Applied Mechanics and Materials, 2014.\n\n--- Segment 50 ---\nA Statistics-Based Data Placement Strategy for Hybrid Storage. Applied Mechanics and Materials, 2014. [67] Agil Yolchuyev and Janos Levendovszky. Data Chunks Placement Optimization for Hybrid Storage Systems. Future Internet, 2021. [68] John D Strunk. Hybrid Aggregates: Combining SSDs and HDDs in a Single Storage Pool. ACM SIGOPS Operating Systems Review, 2012. [69] Mingwei Lin, Riqing Chen, Jinbo Xiong, Xuan Li, and Zhiqiang Yao. Efficient Sequential Data Migration Scheme Considering Dying Data for HDD SSD Hybrid Storage Systems. IEEE Access, 2017. [70] Mingwei Lin, Riqing Chen, Li Lin, Xuan Li, and Jingchang Huang. Buffer-Aware Data Migration Scheme for Hybrid Storage Systems. IEEE Access, 2018. [71] Gong Zhang, Lawrence Chiu, and Ling Liu. Adaptive Data Migration in Multi- tiered Storage Based Cloud Environment. In IEEE CLOUD, 2010. [72] David Vengerov. A Reinforcement Learning Framework for Online Data Migra- tion in Hierarchical Storage Systems. Journal of Supercomputing, 2008. [73] Evangelos Vasilakis, Vassilis Papaefstathiou, Pedro Trancoso, and Ioannis Sour- dis. Hybrid2: Combining Caching and Migration in Hybrid Memory Systems. In HPCA, 2020. [74] Wikipedia. Harmonia, [75] Ruihan Yang, Huazhe Xu, Yi Wu, and Xiaolong Wang. Multi-Task Reinforcement Learning with Soft Modularization. NeurIPS, 2020. [76] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-World: A Benchmark and Evaluation for Multi- Task and Meta Reinforcement Learning. 2020. 13 [77] Yee Teh, Victor Bapst, Wojciech M Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: Robust Multitask Reinforcement Learning. NeurIPS, 2017.\n\n--- Segment 51 ---\nDistral: Robust Multitask Reinforcement Learning. NeurIPS, 2017. [78] Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning. NeurIPS, 2020. [79] NVM Express. Everything You Need to Know About the NVMe 2.0 Specifications and New Technical Proposals. [80] Serial ATA International Organization. Serial ATA Revision 3.1. io.org system files specifications SerialATA_Revision_3_1_Gold.pdf. [81] Linux. Multiple Device Driver, [82] Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. 2018. [83] Lorenzo Canese, Gian Carlo Cardarilli, Luca Di Nunzio, Rocco Fazzolari, Daniele Giardino, Marco Re, and Sergio Span. Multi-Agent Reinforcement Learning: A Review of Challenges and Applications. Applied Sciences, 2021. [84] Rahul Jain, Preeti Ranjan Panda, and Sreenivas Subramoney. Cooperative Multi- Agent Reinforcement Learning-Based Co-optimization of Cores, Caches, and On-chip Network. In TACO, 2017. [85] Haoran Qiu, Weichao Mao, Archit Patke, Chen Wang, Hubertus Franke, Zbig- niew T Kalbarczyk, Tamer Ba르r, and Ravishankar K Iyer. Reinforcement Learning for Resource Management in Multi-tenant Serverless Platforms. In EuroMLSys, 2022. [86] Haoran Qiu, Weichao Mao, Archit Patke, Chen Wang, Hubertus Franke, Zbig- niew T Kalbarczyk, Tamer Ba르r, and Ravishankar K Iyer. SIMPPO: A Scalable and Incremental Online Learning Framework for Serverless Resource Manage- ment. In SoCC, 2022. [87] Haoran Qiu, Subho S Banerjee, Saurabh Jha, Zbigniew T Kalbarczyk, and Rav- ishankar K Iyer. FIRM: An Intelligent Fine-grained Resource Management Framework for SLO-Oriented Microservices.\n\n--- Segment 52 ---\n[87] Haoran Qiu, Subho S Banerjee, Saurabh Jha, Zbigniew T Kalbarczyk, and Rav- ishankar K Iyer. FIRM: An Intelligent Fine-grained Resource Management Framework for SLO-Oriented Microservices. In OSDI, 2020. [88] Weichao Mao, Haoran Qiu, Chen Wang, Hubertus Franke, Zbigniew Kalbarczyk, Ravi Iyer, and Tamer Basar. Multi-Agent Meta-Reinforcement Learning: Sharper Convergence Rates with Task Similarity. In NeurIPS, 2023. [89] Haoran Qiu, Weichao Mao, Chen Wang, Hubertus Franke, Zbigniew T. Kalbar- czyk, Tamer Ba르r, and Ravishankar K. Iyer. On the Promise and Challenges of Foundation Models for Learning-based Cloud Systems Management. In Workshop on ML for Systems at NeurIPS, 2023. [90] Kai Lu, Nannan Zhao, Jiguang Wan, Changhong Fei, Wei Zhao, and Tongliang Deng. RLRP: High-Efficient Data Placement with Reinforcement Learning for Modern Distributed Storage Systems. In IPDPS, 2022. [91] Rahul Jain, Preeti Ranjan Panda, and Sreenivas Subramoney. Cooperative multi-agent reinforcement learning-based co-optimization of cores, caches, and on-chip network. TACO, 2017. [92] Rahul Jain, Preeti Ranjan Panda, and Sreenivas Subramoney. A Coordinated Multi-Agent Reinforcement Learning Approach to Multi-Level Cache Co- Partitioning. In DATE, 2017. [93] Zhaoyan Shen, Yuhan Yang, Yungang Pan, Yuhao Zhang, Zhiping Jia, Xiaojun Cai, Bingzhe Li, and Zili Shao. A Multiagent Reinforcement Learning-Assisted Cache Cleaning Scheme for DM-SMR. IEEE TCAD, 2022. [94] Haitham Bou Ammar, Eric Eaton, Matthew E Taylor, Decebal Constantin Mo- canu, Kurt Driessens, Gerhard Weiss, and Karl T칲yls.\n\n--- Segment 53 ---\nIEEE TCAD, 2022. [94] Haitham Bou Ammar, Eric Eaton, Matthew E Taylor, Decebal Constantin Mo- canu, Kurt Driessens, Gerhard Weiss, and Karl T칲yls. An Automated Measure of MDP Similarity for Transfer in Reinforcement Learning. In AAAI, 2014. [95] James L Carroll and Kevin Seppi. Task Similarity Measures for Transfer in Reinforcement Learning Task Libraries. In IJCNN, 2005. [96] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy Gradient Methods for Reinforcement Learning With Function Approximation. In NIPS, 1999. [97] Nan Rosemary Ke, Amanpreet Singh, Ahmed Touati, Anirudh Goyal, Yoshua Bengio, Devi Parikh, and Dhruv Batra. Modeling the Long Term Future in Model-Based Reinforcement Learning. In ICLR, 2018. [98] Mitesh R Meswani, Sergey Blagodurov, David Roberts, John Slice, Mike Igna- towski, and Gabriel H Loh. Heterogeneous Memory Architectures: A HW SW Approach for Mixing Die-stacked and Off-package Memories. In HPCA, 2015. [99] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, He- len King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-Level Control Through Deep Reinforcement Learning. In Nature, 2015. [100] Jacques De Villiers and Etienne Barnard. Backpropagation Neural Nets with One and Two Hidden Layers. In IEEE Trans. Neural Netw. Learn. Sys, 1993. [101] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for Activation Functions. In arXiv, 2017. [102] Marc G Bellemare, Will Dabney, and R칠mi Munos. A Distributional Perspective on Reinforcement Learning.\n\n--- Segment 54 ---\n[102] Marc G Bellemare, Will Dabney, and R칠mi Munos. A Distributional Perspective on Reinforcement Learning. In arXiv, 2017. [103] Daniel JB Harrold, Jun Cao, and Zhong Fan. Data-Driven Battery Operation For Energy Arbitrage Using Rainbow Deep Reinforcement Learning. In Energy, 2021. [104] Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter Selec- tion For Offline Reinforcement Learning. In arXiv, 2020. [105] Sylvain Arlot and Alain Celisse. A Survey of Cross-Validation Procedures for Model Selection. SS, 2010. [106] Michel Tokic and G칲nther Palm. Value-Difference Based Exploration: Adaptive Control between Epsilon-Greedy and Softmax. In AAAI, 2011. [107] Shunsuke Tsukada, Hikaru Takayashiki, Masayuki Sato, Kazuhiko Komatsu, and Hiroaki Kobayashi. A Metadata Prefetching Mechanism for Hybrid Memory Architectures. In COOL CHIPS, 2021. [108] Seagate. Seagate Barracuda Datasheet, content datasheets pdfs 3-5-barracuda-3tbDS1900-10-1710US-en_US.pdf. [109] Sergio Guadarrama, Anoop Korattikara, Oscar Ramirez, Pablo Castro, Ethan Holly, Sam Fishman, Ke Wang, Ekaterina Gonina, Neal Wu, Efi Kokiopoulou, Luciano Sbaiz, Jamie Smith, G치bor Bart칩k, Jesse Berent, Chris Harris, Vincent Vanhoucke, and Eugene Brevdo. TF-Agents: A Library for Reinforcement Learning in TensorFlow, 2018. [110] AMD. AMD Ryzen 7 PRO 2700 Processor, cpu amd-ryzen-7-2700. [111] ADATA. ADATA Ultimate Series: SU630, ultimate-series-su630-960gb-sata-iii-internal-2-5-solid-state-drive . [112] Intel Corporation. Intel Optane Persistent Memory 200 Series Brief.\n\n--- Segment 55 ---\n[112] Intel Corporation. Intel Optane Persistent Memory 200 Series Brief. optane-persistent-memory optane-persistent-memory-200-series-brief.html, 2020. [113] Chunghan Lee, Tatsuo Kumano, Tatsuma Matsuki, Hiroshi Endo, Naoto Fuku- moto, and Mariko Sugawara. Understanding Storage Traffic Characteristics on Enterprise Virtual Desktop Infrastructure. In SYSTOR, 2017. [114] Gala Yadgar, Moshe Gabel, Shehbaz Jaffer, and Bianca Schroeder. SSD-Based Workload Characteristics and their Performance Implications. TOS, 2021. [115] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell Sears. Benchmarking Cloud Serving Systems With YCSB. In SOCC, 2010. [116] MLCommons. MLPerf Storage Benchmarks. benchmarks storage , 2024. [117] Hasan Al Maruf, Hao Wang, Abhishek Dhanotia, Johannes Weiner, Niket Agar- wal, Pallab Bhattacharya, Chris Petersen, Mosharaf Chowdhury, Shobhit Kanau- jia, and Prakash Chauhan. TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory. In ASPLOS, 2023. [118] Donghyun Gouk, Miryeong Kwon, Hanyeoreum Bae, Sangwon Lee, and My- oungsoo Jung. Memory Pooling With CXL. IEEE Micro, 2023. [119] Huaicheng Li, Daniel S Berger, Lisa Hsu, Daniel Ernst, Pantea Zardoshti, Stanko Novakovic, Monish Shah, Samir Rajadnya, Scott Lee, Ishwar Agarwal, et al. Pond: CXL-Based Memory Pooling Systems for Cloud Platforms. In ASPLOS, 2023. [120] Kaiyang Liu, Jun Peng, Jingrong Wang, Boyang Yu, Zhuofan Liao, Zhiwu Huang, and Jianping Pan. A Learning-Based Data Placement Framework for Low Latency in Data Center Networks. In TCC, 2019. [121] Sangjin Yoo and Dongkun Shin.\n\n--- Segment 56 ---\nIn TCC, 2019. [121] Sangjin Yoo and Dongkun Shin. Reinforcement Learning-Based SLC Cache Technique for Enhancing SSD Write Performance. In HotStorage, 2020. [122] Haoyu Wang, Haiying Shen, Qi Liu, Kevin Zheng, and Jie Xu. A Reinforcement Learning Based System for Minimizing Cloud Storage Service Cost. In ICPP, 2020. [123] Wonkyung Kang, Dongkun Shin, and Sungjoo Yoo. Reinforcement Learning- Assisted Garbage Collection to Mitigate Long-Tail Latency in SSD. In TECS, 2017. [124] Wonkyung Kang and Sungjoo Yoo. Dynamic Management of Key States for Re- inforcement Learning-assisted Garbage Collection to Reduce Long Tail Latency in SSD. In DAC, 2018. [125] Mengquan Li, Chao Wu, Congming Gao, Cheng Ji, and Kenli Li. RLAlloc: A Deep Reinforcement Learning-Assisted Resource Allocation Framework for Enhanced Both I O Throughput and QoS Performance of Multi-Streamed SSDs. In DAC, 2023. [126] Jun Li, Bowen Huang, Zhibing Sha, Zhigang Cai, Jianwei Liao, Balazs Gerofi, and Yutaka Ishikawa. Mitigating Negative Impacts of Read Disturb in SSDs. TODAES, 2020. [127] Ibrahim Umit Akgun, Ali Selman Aydin, Aadil Shaikh, Lukas Velikov, and Erez Zadok. A Machine Learning Framework to Improve Storage System Performance. In HotStorage, 2021. [128] Wonkyung Kang, Dongkun Shin, and Sungjoo Yoo. Reinforcement Learning- Assisted Garbage Collection to Mitigate Long-Tail Latency in SSD. TECS, 2017. [129] Wonkyung Kang and Sungjoo Yoo. Q-Value Prediction for Reinforcement Learning Assisted Garbage Collection to Reduce Long Tail Latency in SSD. IEEE TCAD, 2019. [130] Qian Wei, Yi Li, Zhiping Jia, Mengying Zhao, Zhaoyan Shen, and Bingzhe Li. Reinforcement Learning-Assisted Management for Convertible SSDs. In DAC, 2023.\n\n--- Segment 57 ---\nReinforcement Learning-Assisted Management for Convertible SSDs. In DAC, 2023. [131] Jun Li, Zhigang Cai, Balazs Gerofi, Yutaka Ishikawa, and Jianwei Liao. Page Type-Aware Full-Sequence Program Scheduling via Reinforcement Learning in 14 Harmonia: A Multi-Agent RL Approach to Data Placement and Migration in HSS High Density SSDs. IEEE TCAD, 2024. [132] Yong-Cheng Liaw, Shuo-Han Chen, and Yu-Pei Liang. Reinforcement Learning- Based Read Performance Throttling to Enhance Lifetime of 3D NAND SSD. In NVMSA, 2024. [133] Chao Wu, Cheng Ji, Qiao Li, Chenchen Fu, and Chun Jason Xue. Maximizing I O Throughput and Minimizing Performance Variation via Reinforcement Learning Based I O Merging for SSDs. In IEEE TC, 2019. [134] Jin Yong Ha, Sangjin Lee, Heon Young Yeom, and Yongseok Son. RL-Watchdog: A Fast and Predictable SSD Liveness Watchdog on Storage Systems. In USENIX ATC, 2024. 15\n\n