=== ORIGINAL PDF: ParticipationGamesICML25.pdf ===\n\nRaw text length: 52104 characters\nCleaned text length: 51670 characters\nNumber of segments: 29\n\n=== CLEANED TEXT ===\n\n000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 Participation Games: Collaborative Strategies for Robust Inference in Energy Harvesting Wireless Sensor Networks Anonymous Authors1 Abstract Energy-harvesting wireless sensor networks (EH- WSNs) offer sustainable solutions for large-scale IoT deployments but face challenges due to the unreliability and intermittent availability of in- dividual sensors. We propose a comprehensive framework that integrates a game-theoretic partici- pation strategy with a federated learning approach tailored for EH-WSNs. Our game-theoretic model enables sensors to make optimal participation de- cisions based on energy levels, data quality, and collective inference impact, fostering cooperative behavior while managing individual energy con- straints. The federated learning framework ac- commodates intermittent participation and vari- able data quality, ensuring robust model training despite sensor unreliability. Simulation results demonstrate that our integrated approach signif- icantly enhances inference accuracy and energy efficiency compared to traditional participation strategies. 1. Introduction The rapid proliferation of the Internet of Things (IoT) has sparked a tremendous growth in the scale and diversity of sensor deployments, from smart homes to expansive in- dustrial and environmental monitoring systems. As these networks continue to expand, sustaining continuous opera- tion in the face of finite power sources becomes a paramount concern. To address this, energy harvesting (EH) technolo- gies have emerged as a viable solution, enabling sensors to convert ambient energy (e.g., solar, thermal, or vibration) into electrical power. This approach promises perpetual, maintenance-free operation, significantly reducing environ- mental impact and long-term operational costs. 1Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute. Despite these advantages, large-scale EH Wireless Sensor Networks (EH-WSNs) remain inherently uncertain. Ambi- ent energy availability varies over time and space, leading to fluctuating sensor activity levels and intermittent participa- tion in both training and inference tasks. Some sensors may frequently become inactive or produce low-quality data due to energy scarcity or environmental noise. Consequently, the mere presence of numerous EH sensors does not guaran- tee robust and reliable performance for complex tasks such as image recognition, acoustic surveillance, or precision agriculture monitoring. Achieving accurate inference in these complex scenarios de- pends on effective coordination. Multiple sensors observing the same phenomenon from different angles can collectively provide more comprehensive and reliable insights than any single sensor could. However, requiring all sensors to partic- ipate at all times is impractical, as it drains energy reserves too quickly. Conversely, simplistic policies such as se- lecting only the highest-energy sensors ignore factors like data relevance, sensor quality, and the strategic implications of current participation on future network states. This challenge motivates the need for intelligent, context- aware participation strategies that dynamically determine which sensors should engage during both the training phase where global model parameters are periodically fine-tuned or updated and the inference phase where newly observed data are aggregated to produce predictions. Sensors must carefully balance immediate accuracy gains against conserving energy for future tasks, while also antici- pating the behavior of other sensors that may be collaborat- ing or competing. To address these interdependent decisions, we employ a game-theoretic framework. Unlike simple heuristic meth- ods that ignore future resource allocation or complex ap- proaches like reinforcement learning that may be too costly to implement, game theory provides equilibrium guaran- tees. By modeling each sensor as a rational player aiming to optimize its own long-term utility, we achieve stable, coop- erative equilibria where no sensor can improve its outcome through unilateral deviation. This strategic equilibrium un- derpins both training and inference participation decisions, ensuring that the sensors most likely to improve the global 1 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 model given their energy, data quality, and network condi- tions are the ones that engage. To refine the global model parameters without incurring continuous on-edge training costs, we adopt a federated learning paradigm adapted to EH-WSNs. Rather than rely- ing on persistent, centralized updates or continuous feder- ated aggregation, we perform periodic or equilibrium-driven fine-tuning rounds. These updates occur only when sensors have sufficient energy to participate meaningfully, guided by the game-theoretic equilibrium strategy. By integrating the game-theoretic approach with federated learning principles, we reduce communication overhead and ensure that contri- butions to model updates come from sensors best positioned to improve accuracy under energy constraints and uncertain availability. Our key contributions are as follows: Game-Theoretic Participation Strategy: We develop a novel game-theoretic model for EH-WSNs that applies to both training and inference phases. This model balances anticipated energy availability, local data quality, and global benefit to establish stable and cooperative equilibria, opti- mizing the energy-accuracy trade-offs. Federated Learning Integration: We introduce a fed- erated learning-based framework tailored for intermittent participation and heterogeneous data quality. Unlike con- tinuous on-edge training, we employ periodic or triggered fine-tuning sessions aligned with equilibrium strategies, en- suring robust and progressively improving global models. Joint Optimization of Training and Inference: Our unified solution aligns training participation decisions with inference needs. Sensors strategically decide when to ex- pend energy on local model updates and when to engage in inference tasks, ultimately maximizing their long-term contribution to the network s performance. Demonstrated Performance Gains: Through simula- tions (add simulation details later), we show that our inte- grated framework outperforms baseline approaches such as always-on participation or simplistic energy-based selec- tion by achieving higher inference accuracy, lower energy consumption, and more sustainable long-term operation in EH-WSNs. By tackling the dual challenges of sensor unreliability and energy scarcity through a rigorous game-theoretic and fed- erated learning lens, our work addresses a critical gap in the design of sustainable, intelligent EH-WSNs. This inte- grated framework is theoretically grounded, yet practical for a wide range of IoT applications, from remote wildlife mon- itoring to large-scale industrial status tracking and precision agriculture. The remainder of this paper is organized as follows. In Sec- tion 3, we present the system model, detailing the EH-WSN setup and data capture process. In Section 4, we introduce the game-theoretic model of sensor participation, motivat- ing our approach against simpler heuristics and discussing why equilibrium solutions are desirable. Section 5 outlines the training and fine-tuning framework that integrates the equilibrium strategies into a federated learning paradigm. Fi- nally, Section ?? presents simulation results and Section ?? concludes with a discussion of limitations and future work. 2. Background and Related Work Very basic, just the outline, compact and make robust 2.1. Energy Harvesting Wireless Sensor Networks Energy harvesting wireless sensor networks (EH-WSNs) have emerged as a sustainable solution for long-term envi- ronmental monitoring, infrastructure surveillance, and IoT applications (?). By harnessing ambient energy sources such as solar, thermal, or kinetic energy, EH sensors can operate indefinitely without the need for battery replace- ment or external power supplies. However, the intermittent and unpredictable nature of harvested energy introduces significant challenges in maintaining reliable and consistent network performance (?). The unreliability of individual EH sensors, due to fluctua- tions in energy availability, necessitates the deployment of a large number of inexpensive and potentially unreliable de- vices to ensure network robustness. This redundancy allows for continuous operation despite individual sensor failures or downtime. However, it also introduces complexities in coordinating sensor activities, managing energy resources, and ensuring efficient data collection and processing (?). 2.2. Participation Strategies in EH-WSNs Efficient participation strategies are critical in EH-WSNs to optimize network performance while conserving limited energy resources. Traditional approaches often assume con- tinuous participation of all sensors, which is impractical in energy-constrained environments (?). Some methods pro- pose selecting a subset of sensors based on energy levels or predefined schedules (?), but these can lead to suboptimal performance by not considering the sensors data quality or potential future contributions. Several works have explored adaptive participation strate- gies that consider energy harvesting rates, energy consump- tion patterns, and application-specific requirements (?). These strategies aim to balance energy expenditure with the need for timely and accurate data, often using heuristic or optimization-based approaches. However, they may not fully exploit the potential for collaboration among sensors or account for the strategic interactions inherent in decen- tralized networks. 2 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 2.3. Game-Theoretic Models in Sensor Networks Game theory provides a powerful framework for modeling and analyzing strategic interactions in distributed systems, including sensor networks (?). In the context of EH-WSNs, game-theoretic models have been employed to design dis- tributed algorithms for resource allocation, power control, and cooperative communication (?). Cooperative game theory has been used to encourage collab- oration among sensors to enhance network performance (?). Non-cooperative game models allow sensors to make au- tonomous decisions while considering the potential ac- tions of others, leading to equilibria that balance individ- ual utility with collective goals (?). However, integrating game-theoretic participation strategies with machine learn- ing tasks, particularly in energy-harvesting environments, remains an area with limited exploration. 2.4. Federated Learning in Resource-Constrained Environments Federated learning enables multiple devices to collabora- tively train a global model without sharing raw data, pre- serving privacy and reducing communication overhead (?). While federated learning has gained significant attention in mobile and IoT devices, applying it to EH-WSNs presents unique challenges due to intermittent participation, limited computational capabilities, and variable data quality (?). Recent studies have begun to address federated learning in resource-constrained and unreliable networks. Strategies in- clude adaptive aggregation methods, energy-aware training schedules, and robustness to device dropouts (?). However, these approaches often assume some level of reliability or do not fully integrate energy harvesting dynamics into the learning process. 2.5. Multi-View Learning and Collaborative Inference Multi-view learning leverages multiple sources or perspec- tives to improve learning performance (?). In EH-WSNs, sensors providing different views of the same scene can en- hance inference accuracy through collaborative processing. Techniques such as co-training, consensus learning, and en- semble methods have been explored to combine information from multiple sensors (?). Collaborative inference in sensor networks involves com- bining local inferences to achieve a global understanding of the environment (?). Challenges include aligning hetero- geneous data, managing communication costs, and dealing with unreliable or missing inputs. Existing methods may not account for the energy constraints and participation vari- ability inherent in EH-WSNs. 3. System Model We consider a network of N EH sensors S {s1, s2, . . . , sN} deployed to monitor a common scene. Each sensor observes the environment from a distinct van- tage point. Time is slotted and indexed by t N. In each time slot, the network may perform an inference event, dur- ing which sensors have the opportunity to contribute data that enhances the accuracy of a global inference task, such as object detection or environmental classification. Each sensor si harvests energy from ambient sources, such as solar or vibrational energy, resulting in a stochastically varying energy supply. We denote by Ei(t) the energy harvested by sensor si during slot t. The sensor maintains an energy buffer whose state evolves as Bi(t 1) Bi(t) Ei(t) ei(t), where Bi(t) is the energy available at the beginning of slot t, and ei(t) is the energy expended during that slot. Predicting future energy intake is challenging, so each sensor employs an estimator ˆEi(t 1) to anticipate its upcoming energy resources. Incorporating uncertainty-aware models or robust estimation techniques is beyond the scope of this paper. Prior to deployment, a global inference model fθ is trained offline on representative data and distributed to each sensor. This model maps sensor observations to inference outputs. Although parameters θ can theoretically be updated through on-edge training, we assume that frequent retraining in situ is prohibitively expensive given energy constraints. Thus, θ remains largely static post-deployment. Sensors focus on inference using their local copies of fθ. However, the sub- sequent training framework, discussed in Section 5, allows for occasional fine-tuning of θ based on equilibrium-driven participation, thereby refining the model to better suit the operational dynamics of the network. In each inference event, sensors decide whether to partic- ipate. If sensor si participates at time t, it must capture data at a chosen Signal-to-Noise Ratio (SNR), process the data using fθ, and transmit the result to a designated lead sensor. High-SNR data capture improves the sensor s con- tribution to global accuracy but consumes more energy. Let ecap(SNR) denote the energy required for capture at a given SNR level. We assume a monotonic relationship: higher SNR increases both the capture cost and the expected ac- curacy contribution. This assumption simplifies the model by ensuring that better data quality unequivocally enhances inference performance, while also making the energy ex- penditure predictable. In addition to capture costs, partici- pation incurs inference computation cost einf and communi- cation cost ecomm. Thus, if sensor si participates with SNR SNRi(t), its total energy expenditure is ei(t) ecap(SNRi(t)) einf ecomm 3 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 The improvement in global inference accuracy due to sen- sor si is denoted by Ai(t). This quantity depends on SNRi(t) and on the data contributed by other participating sensors, as their combined perspectives shape the overall result. While Ai(t) may not be known precisely, we as- sume that each sensor can estimate its expected contribution based on historical observations and current conditions. Ev- ery inference event presents a binary decision for sensor si: ai(t) {Participate (P), Not Participate (NP)}. Choosing P involves selecting an SNR level, incurring energy costs, and aiming to improve global accuracy. Choosing NP con- serves energy but forfeits any contribution or associated reward. Because sensors have limited energy and the net- work may operate for extended periods, each sensor must consider the future implications of its current actions. The interplay of multiple sensors making similar decisions un- der uncertainty and energy constraints naturally suggests a game-theoretic framework for modeling their interactions. 4. Game-Theoretic Modeling 4.1. Motivation for Game Theory over Simpler Methods While heuristic methods such as always selecting the top- k sensors based on current energy levels or greedy algo- rithms that maximize immediate utility might offer straight- forward solutions, they fall short in addressing the strate- gic and long-term dynamics inherent in EH-WSNs. These simplistic approaches ignore the interdependencies among sensor decisions and fail to account for future resource allo- cation, potentially leading to suboptimal performance over time. For instance, always selecting the highest-energy sen- sors can rapidly deplete their energy reserves, reducing the network s resilience during critical future events. Alternatively, reinforcement learning or Markov Decision Process-based approaches could adaptively learn partici- pation policies that consider both immediate rewards and future states. However, these methods often require exten- sive training data, significant computational resources, and complex communication protocols, which may be impracti- cal for resource-constrained sensor nodes. In contrast, a game-theoretic framework provides equilib- rium guarantees, ensuring stable and cooperative partici- pation strategies. By modeling each sensor as a rational player optimizing its own utility, we can derive participation patterns that are robust against unilateral deviations. This stability is crucial for maintaining long-term network per- formance without necessitating continuous recalibration or extensive communication overhead. 4.2. Utility Function Definition We define a utility function Ui(t) for each sensor si that en- capsulates the trade-off between accuracy gains and energy expenditures, as well as future opportunities. The utility function is designed to reflect both immediate rewards and long-term sustainability. Immediate Rewards and Penalties: Let γ 0 be a scal- ing factor that translates accuracy gains into utility rewards. When sensor si participates (ai(t) P) and contributes cor- rectly to the inference task, it receives a reward proportional to the improvement in global accuracy, denoted by Ai(t): Ri(t) γ Ai(t), if ai(t) P and correct inference, δ, if ai(t) P and incorrect inference, η, if ai(t) NP. Here, δ 0 penalizes incorrect participation, discourag- ing sensors from submitting low-quality data, while η 0 penalizes non-participation to prevent perpetual abstention. Importantly, we set η δ, ensuring that consistently opting out is more detrimental than occasionally providing inaccu- rate data. Energy Costs and Future Utility: Participation incurs energy costs, reducing the sensor s capacity for future tasks. Additionally, sensors must consider the discounted value of future utility. Let Ci(t) represent the cost component: Ci(t) ei(t) βVi(t 1), where ei(t) is the total energy expenditure for participa- tion, encompassing data capture, inference computation, and communication: ei(t) ecap(SNRi(t)) einf ecomm. The discount factor β [0, 1) captures how sensors value future utility, with Vi(t 1) representing the expected fu- ture utility given current decisions and predicted energy availability ˆEi(t 1). Overall Utility Function: Combining immediate rewards and costs, the overall utility function for sensor si at time t is: Ui(t) Ri(t) Ci(t). This utility function effectively balances the benefits of participation against the associated costs and future oppor- tunities, guiding sensors to make strategic decisions that optimize their long-term contributions to the network. Nash Equilibrium and Stability: A Nash equilibrium (NE) represents a stable action profile a (t) where no sensor can unilaterally improve its utility by deviating from its current strategy: Ui(a i (t), a i(t)) Ui(ai(t), a i(t)) ai(t), i. 4 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 Achieving an NE ensures that sensor participation patterns are stable; once equilibrium is reached, no single sensor ben- efits from changing its participation decision independently. This stability is critical for maintaining consistent network performance and energy sustainability over time. Distributed Best-Response Algorithm: To realize the NE, we propose a distributed best-response algorithm where each sensor iteratively adjusts its action based on the current state and the expected actions of others. The algorithm operates as follows: Algorithm 1 Distributed Best-Response Participation Algo- rithm 1: Input: Current energies Bi(t), predicted harvest ˆEi(t 1), parameters γ, δ, η, β, and energy costs ecap( ), einf, ecomm. 2: At each inference event: 3: Each sensor si receives a solicitation from the lead sensor and forms an estimate of Ai(t) given potential SNR choices and expected actions of others. 4: For each action candidate ai(t) {P, NP}, the sensor computes the expected utility: U ai(t) i E[Ri(t)] E[ei(t)] βE[Vi(t 1)], where the expectations are taken over uncertainties in correctness, SNR impact, and future energy. 5: If U P i U NP i and Bi(t) ecap(SNRi(t)) einf ecomm, then si chooses P. Otherwise, it chooses NP. 6: After all sensors decide, the action profile a(t) is real- ized, and energies are updated: Bi(t 1) Bi(t) Ei(t) ei(t). 7: Sensors iterate this process at each inference event, re- fining their estimates and converging to stable action patterns. Sensors employ this best-response mechanism, continuously updating their participation decisions based on the evolving network state and the actions of other sensors. Over repeated iterations, under suitable conditions, this process converges to a Nash equilibrium where participation strategies are mutually optimal. Existence and Convergence of Equilibrium: Theorem 4.1. Suppose that each utility function Ui(t) is non-decreasing in Ai(t), that energy constraints and dis- counting ensure diminishing marginal returns for repeated deviations, and that sensors have consistent estimation of Ai(t) and ˆEi(t 1). Then, the iterative best-response updates described in Algorithm 1 converge to a Nash equi- librium action profile a (t). Proof of Theorem 4.1: The proof constructs a potential function Φ(a(t)) PN i 1 Ui(ai(t), a i(t)) that strictly in- creases whenever a sensor makes a profitable unilateral deviation. Since utilities are bounded (due to finite en- ergy and limited accuracy gains) and returns diminish over time, no infinite sequence of profitable deviations is possi- ble. Hence, the best-response dynamics must terminate at a profile where no sensor can improve its utility alone, i.e., a Nash equilibrium. A complete formal proof, including all technical conditions, is provided in Appendix B. Guidelines for Hyperparameter Selection: The param- eters γ, δ, and η critically influence sensor behavior by dictating the trade-offs between participation rewards, penal- ties for incorrect submissions, and deterrents against non- participation. Detailed guidelines for selecting these hy- perparameters are provided in Appendix A. Briefly, these parameters should be chosen to ensure that: (1) γ suffi- ciently incentivizes correct participation without leading to excessive energy expenditure. (2) δ appropriately penalizes incorrect inferences, discouraging low-quality data contri- butions. (3) η δ to prevent sensors from consistently abstaining, thereby promoting overall network engagement. These guidelines help in balancing immediate utility gains with long-term energy sustainability, ensuring that the game- theoretic model drives desirable participation behaviors. 5. Training and Aggregation Framework Having established the equilibrium participation strategies and the underlying reward-based utility functions, we now consider the training process that fine-tunes the global in- ference model θ Rd within this EH, multi-sensor envi- ronment. Initially, θ is pre-trained offline and deployed to all sensors, enabling them to perform basic inference tasks. However, this initial model may not be optimally adapted to the complex operational reality of the network, where sensors strategically choose SNR levels, participate inter- mittently according to equilibrium strategies, and generate data distributions that deviate from the original training set. The goal of the training process is to adjust θ to these condi- tions, effectively fine-tuning the model to the nonstationary data distribution D induced by the sensors equilibrium be- haviors. At equilibrium, sensors strike a balance between accurate data contribution and energy conservation, result- ing in a stable pattern of participation and SNR choices. Over time, this induces a stationary, albeit non-trivial, effec- tive data distribution D. Learning Approach: Our approach diverges from classi- cal Learning paradigms. We adopt a hybrid strategy where periodic or event-triggered updates refine the model parame- ters based on equilibrium-driven data collection. This hybrid 5 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 approach mitigates the high communication overhead and energy consumption, making it more suitable for resource- constrained EH-WSNs. While traditional methods require persistent communication between sensors and the aggre- gator, our framework leverages the established equilibrium strategies to determine optimal times for model updates. By aligning update events with periods when sensors are most likely to participate meaningfully, we ensure that the global model is refined efficiently without imposing excessive en- ergy demands on the sensors. Training Objective and Regularization: To enhance ro- bustness and efficiency, we incorporate regularizers that pe- nalize undesirable model properties. Specifically, we intro- duce two regularizers: (1) ΩSNR(θ): Encourages the model to maintain performance across varying SNR levels, pre- venting over-reliance on high-SNR data. (2) Ωcomplexity(θ): Controls model complexity, reducing computational and communication overheads by discouraging overly intricate models. The full training objective is formulated as: J(θ) L(θ) λ1ΩSNR(θ) λ2Ωcomplexity(θ), where L(θ) E(x,y) D[ℓ(fθ(x), y)], and λ1, λ2 0 are hyperparameters that balance accuracy, robustness, and efficiency. Gradient Computation and Backpropagation: Integrat- ing regularizers into the training process is straightforward due to their known closed-form gradients. During back- propagation, each sensor computes the gradient of the loss function ℓ(fθ(x), y) with respect to θ based on locally available samples from D. Additionally, the gradients of the regularizers, ΩSNR(θ) and Ωcomplexity(θ), are analyt- ically derived and added to the local gradient estimates. Since both regularizers are convex and smooth, their in- clusion ensures that the overall objective J(θ) maintains desirable convexity and smoothness properties, facilitating the convergence of stochastic gradient descent (SGD). Periodic Equilibrium-Aware Training: Model updates are performed periodically at an aggregator node that col- lects gradient estimates from participating sensors. Partici- pation during training follows the same equilibrium model: sensors decide whether to compute and send gradients based on their current energy states, predicted future utilities, and the established reward structure. By aggregating these gradi- ent updates over multiple training rounds, the aggregator ap- proximates the gradient J(θ) and performs an SGD step. Our training framework is encapsulated in Algorithm 2, which outlines the periodic equilibrium-aware training pro- cess. This training framework is intrinsically linked to the game-theoretic participation strategies. Sensors participate in training rounds based on their equilibrium-driven de- cisions, ensuring that gradient updates are contributed by those sensors most capable and willing to improve the global model. This alignment minimizes unnecessary energy ex- penditure and maximizes the efficacy of each training round. Algorithm 2 Periodic Equilibrium-Aware Training Algo- rithm 1: Initialization: Initialize θ0. Broadcast θ0 to all sensors. Set a diminishing step-size schedule {αk}k 0. 2: for each training round k 0, 1, 2, . . . do 3: The aggregator signals that a training update round is imminent. 4: Sensors decide on participation. Participation in- volves: 1. Determining if they have enough energy and incentive (based on the established equilibrium strategy and reward parameters γ, δ, η). 2. If participating: capturing data at their cho- sen SNR, performing inference, and computing local gradients ℓ(fθk(x), y) on their locally available samples drawn from D. 3. Adding regularizer gradients λ1 ΩSNR(θk) and λ2 Ωcomplexity(θk). 5: A subset of sensors, determined by the equilibrium, send their gradient estimates to the aggregator. 6: The aggregator forms an unbiased estimate of the full gradient: b J(θk) b L(θk) λ1 ΩSNR(θk) λ2 Ωcomplexity(θk). 7: Update model parameters: θk 1 θk αk b J(θk). 8: Broadcast θk 1 to all sensors. 9: end for Regularizers and SGD Convergence: The chosen reg- ularizers ΩSNR(θ) and Ωcomplexity(θ) are both convex and smooth, with known closed-form gradients. This property guarantees that the inclusion of regularizers does not com- promise the convexity or smoothness of the overall objective J(θ). Consequently, the stochastic gradient descent (SGD) updates retain their convergence properties, ensuring that the training process reliably optimizes J(θ). 6 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 6. Implementation and Evaluation 6.1. Discussions and Limitations 7. Conclusion This should finish at 8 pages. Impact Statement Authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. This statement should be in an unnumbered section at the end of the paper (co- located with Acknowledgments the two may appear in either order, but both must be before References), and does not count toward the paper page limit. In many cases, where the ethical impacts and expected societal implications are those that are well established when advancing the field of Machine Learning, substantial discussion is not required, and a simple statement such as the following will suffice: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. The above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review. References A. Guidelines for Hyperparameter Selection and Bounds on Reward Parameters The parameters γ, δ, and η govern the reward structure of the proposed framework, influencing whether sensors participate consistently, over-participate and waste energy, or abstain altogether. This appendix provides a systematic approach to selecting these parameters, including formal bounds, practical heuristics, and an algorithmic procedure to explore suitable values. Conceptual Role of Parameters The scalar γ 0 represents the reward scaling for correct participation. If γ is too low, sensors will not have suffi- cient incentive to expend energy on high-SNR captures. If γ is too high, sensors may waste energy attempting difficult inferences. The parameter δ 0 penalizes incorrect in- ferences, discouraging reckless submissions of low-quality data. The parameter η 0 penalizes non-participation, ensuring that sensors do not remain idle indefinitely. As discussed, maintaining η δ encourages sensors to at least attempt participation rather than always remain offline. Formal Bounds and Conditions To ensure balanced behavior, it is helpful to relate γ, δ, and η to typical values of accuracy improvement and energy costs. Accuracy Gains and Costs: Let Amin and Amax de- note the minimum and maximum expected accuracy im- provements from any sensor s participation. Let emax total emax cap einf ecomm represent the maximum energy cost (for a chosen SNR mode). A baseline condition that ensures correct participation can overcome occasional penalties is: γ Amin δ emax total . This inequality implies that even in a worst-case scenario for accuracy gain, the net expected benefit of correct par- ticipation surpasses the sum of potential incorrect penalties and energy costs. Without this condition, sensors might find participation systematically unprofitable. Non-Participation and Equilibrium: Since η δ, we ensure that sensors prefer risking occasional incorrect infer- ences over consistently abstaining. A suitable gap might be chosen so that: δ η δ c, for some small c 0. Choosing c relative to typical gains, say c 0.1 γ Amax, helps maintain a moderate deterrent against non-participation without forcing sensors to always participate. 7 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 Energy Preservation: If γ is too large, sensors might not value future energy at all. To prevent myopic strategies, one can limit γ such that continuously investing in high-SNR captures does not dominate long-term considerations. For example: γ Amax η margin, where margin accounts for future opportunities and energy savings. A small margin ensures sensors do not always expend maximal energy for short-term gains. Practical Hyperparameter Tuning Strategies 1. Baseline Ratios: Start with ratios that link γ to typical accuracy gains and set δ, η based on fractions or multiples of γ Amin or γ Amax. 2. Iterative Refinement: Use simulation or small-scale ex- perimental runs to refine parameters. If sensors rarely par- ticipate, increase γ or decrease η. If sensors over-exert themselves, reduce γ or increase δ, η. 3. Adaptive Tuning: If conditions change over time, adjust γ, δ, and η dynamically based on observed participation rates, accuracy levels, and energy depletion patterns. Exploration Algorithm Algorithm 3 outlines a systematic approach to exploring suit- able hyperparameter values. It combines theoretical bounds with empirical evaluation, guiding the search toward stable and efficient equilibria. The above guidelines and the explo- ration algorithm provide a structured approach to selecting and refining γ, δ, and η. By starting from theoretically in- formed baseline conditions and iteratively refining through simulation-based feedback, it is possible to reach a stable set of parameters that promotes balanced participation, discour- ages perpetual abstention, and prevents excessive energy expenditure. Regular re-tuning may be warranted as oper- ating conditions, energy harvesting patterns, or accuracy requirements evolve over the network s lifetime. B. Equilibrium Existence and Convergence with Reward-Based Utility In this appendix, we provide a detailed and formal proof that the best-response dynamics, incorporating the newly defined reward-based utility functions, converge to a Nash equilibrium (NE). We first restate the key assumptions and the utility model. We then show that the iterative best- response updates cannot lead to infinite improvement cycles, implying the existence of an NE. Finally, we prove that the equilibrium is reached under the given assumptions. Algorithm 3 Hyperparameter Exploration for Reward Pa- rameters 1: Inputs: Estimates Amin, Amax, energy costs emax cap , einf, ecomm, initial guesses γ0, δ0, η0, and tuning increments γ, δ, η. 2: Compute emax total emax cap einf ecomm. 3: Ensure baseline feasibility: If γ0 Amin δ0 emax total , increase γ0 until this condition is met. 4: Set η0 δ0. Start with η0 δ0 c, where c is a small positive number. If preliminary tests show insufficient participation, slightly increase η0. If participation is overly aggressive, reduce γ0 or increase δ0. 5: Simulation-Refinement Loop: 6: for k 1, 2, . . . , K (number of refinement iterations) do 7: Run a simulation or small-scale test deployment us- ing the current γk, δk, ηk. 8: Measure key indicators: participation rate, average energy depletion rate, frequency of incorrect infer- ences, and overall inference accuracy. 9: if participation is too low (e.g., pmin) or sensors remain idle too often then 10: Increase γk γk γ or decrease ηk ηk η. 11: else if participation is too high, leading to frequent energy depletion then 12: Decrease γk γk γ or increase δk δk δ to discourage high-risk attempts. 13: else if incorrect inferences are prevalent then 14: Increase δk δk δ to penalize low-quality submissions more strongly. 15: end if 16: Check feasibility conditions again to ensure no viola- tion of baseline inequalities. 17: If performance metrics (accuracy, sustainability) are satisfactory, terminate. Otherwise, continue refine- ment. 18: end for Restatement of the Utility Function and Assumptions Recall that at each inference event t, each sensor si chooses an action ai(t) {P, NP}. The chosen action profile is a(t) (a1(t), . . . , aN(t)). The immediate reward for sensor si is defined as: Ri(t) γ Ai(t), if ai(t) P and inference is correct, δ, if ai(t) P and inference is incorrect, η, if ai(t) NP. Here, γ 0 scales the reward for correct participation, δ 0 penalizes incorrect inference, and η 0 penalizes non-participation, with η δ ensuring that remaining idle 8 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 is more penalizing than at least attempting participation. The cost incorporates energy consumption and future op- portunities. Let ei(t) be the energy expenditure for sensor si if it participates at time t, accounting for capture, infer- ence, and communication costs. Introduce a discount factor β [0, 1), and let Vi(t 1) represent the expected future utility of sensor si given its current decisions and predicted energy availability. The cost is: Ci(t) ei(t) βVi(t 1). The overall utility is: Ui(t) Ri(t) Ci(t). We assume that Ai(t) is non-decreasing in the quality of sensor si s data (e.g., higher SNR yields higher Ai(t)). We also assume that energy resources, accuracy gains, and reward penalty parameters are finite and bounded, and that sensors have consistent estimation mechanisms for Ai(t) and ˆEi(t 1). Potential Function Construction To prove convergence, we define a potential function that reflects the collective utility of the sensor network: Φ(a(t)) N X i 1 Ui(ai(t), a i(t)). Since Ui(t) Ri(t) Ci(t), we have: Φ(a(t)) N X i 1 [Ri(t) Ci(t)]. The terms Ri(t) depend on the chosen actions and correct- ness of inferences. Due to bounded γ, δ, and η, and the fact that Ai(t) and energy costs are bounded, each Ui(t) is finite. Thus, Φ(a(t)) is also finite for all feasible action profiles. Monotonicity of the Potential Function Consider a unilateral deviation by a single sensor sj from an action aj(t) to a different action a j(t). Such a deviation affects only Uj(t), not the utilities of other sensors directly in a one-step change. If this deviation is profitable for sensor sj, we have: Uj(a j(t), a j(t)) Uj(aj(t), a j(t)). Because the other sensors utilities do not change instan- taneously by sj s unilateral action, the increment in Uj(t) results in: Φ(a j(t), a j(t)) Φ(a(t)) Uj(a j(t), a j(t)) Uj(aj(t), a j(t)) 0. Thus, any unilateral profitable deviation increases Φ(a(t)). Boundedness and Impossibility of Infinite Improvement Sequences Since all utilities are bounded (due to finite γ, δ, η, bounded Ai(t), and bounded energy resources), there exists a finite upper bound Φmax such that: Φ(a(t)) Φmax a(t). Suppose, for contradiction, that there exists an infinite se- quence of unilateral profitable deviations. Each such de- viation strictly increases Φ(a(t)). Because Φ is bounded above by Φmax, only a finite number of increments can occur before no further improvements are possible. This contradiction shows that no infinite improvement sequence can occur. Existence of a Nash Equilibrium Since no infinite sequence of profitable unilateral deviations can occur, the best-response dynamics must terminate in a state where no sensor can unilaterally improve its utility. By definition, this state is a Nash equilibrium a (t): Ui(a i (t), a i(t)) Ui(ai(t), a i(t)) ai(t), i. Thus, the existence of a Nash equilibrium follows directly from the finiteness of utilities, the monotonicity of Φ, and the impossibility of infinite improvement sequences. Convergence to the Nash Equilibrium The final step is to show that the iterative best-response dy- namics indeed converge to the NE identified above. Since each sensor s best-response update seeks to maximize its own utility, sensors will continue to deviate as long as prof- itable deviations exist. Our argument shows that profitable deviations must terminate. Under the assumptions that Ai(t) is non-decreasing and that sensors have consistent energy and accuracy estimates, no cyclical behavior can persist. A cycle would imply an infinite sequence of im- provements or a return to a previously visited state without improvement, which cannot occur since profitable devia- tions strictly increase Φ(a(t)). The presence of the discount factor β further stabilizes the process. With β [0, 1), sensors value future utility less than immediate utility. This discounting ensures diminish- ing returns for postponing beneficial participation or indefi- nitely waiting for ideal conditions. As a result, sensors do not continually defer improvements, preventing complex long-term cycles. 9 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 Because the best-response process eliminates profitable de- viations step by step and cannot cycle indefinitely, the action profile sequence generated by iterative best responses con- verges to the NE. We have shown that, with the reward-based utility function that includes correct participation rewards (γ Ai(t)), penalties for incorrect inferences (δ), and penalties for non- participation (η), the best-response dynamics lead to a Nash equilibrium. The proof relies on constructing a potential function Φ that is strictly increased by unilateral profitable deviations and bounded above. The impossibility of infinite improvement sequences guarantees the existence of an NE, and the assumptions on monotonicity, boundedness, and discounting ensure that the iterative best-response process converges to this equilibrium. C. Proof of Convergence for the Equilibrium-Aware Training Process In this appendix, we provide a comprehensive and detailed proof of the convergence theorem stated in the main text. We also elaborate on how the new loss function, the introduced regularizers, and their gradients integrate into the backprop- agation and stochastic gradient descent (SGD) steps. Addi- tionally, we discuss bounds on the newly introduced hyper- parameters and provide guidelines for selecting them. Problem Setting and Notation We consider a global inference model fθ : X Y parame- terized by θ Rd. The model s performance is measured by a loss function ℓ: Y Y R 0 that is convex in θ for any fixed input-label pair (x, y). The model operates in an energy-harvesting wireless sensor network (EH-WSN) envi- ronment where sensors participate strategically in inference tasks based on a game-theoretic equilibrium. Let D denote the effective data distribution induced by the equilibrium strategies of the sensors. Under equilibrium conditions, the distribution D is stationary or at least sta- tionary over sufficiently large timescales. The expected loss is L(θ) E(x,y) D[ℓ(fθ(x), y)]. To enhance robustness and efficiency, we introduce two regularizers: ΩSNR(θ) and Ωcomplexity(θ). ΩSNR(θ) encourages the model to perform reasonably well across varying SNR levels, while Ωcomplexity(θ) penalizes overly complex models that might demand excessive energy or communication costs. Both are assumed convex and have bounded gradients. The final training objective is: J(θ) L(θ) λ1ΩSNR(θ) λ2Ωcomplexity(θ), where λ1, λ2 0 are hyperparameters controlling the influ- ence of the regularizers. Our goal is to show that by running a diminishing step-size SGD on J(θ), using unbiased gradient estimates from the equilibrium distribution D, the parameters {θk} converge in expectation to a stationary point θ of J(θ). Key Assumptions and Conditions 1. Convexity of ℓ. The loss ℓ(fθ(x), y) is convex in θ. Consequently, the expected loss L(θ) is also convex. 2. L-smoothness of ℓ. There exists a constant L 0 such that for all θ, θ , L(θ) L(θ ) L θ θ . This ensures that L(θ) is Lipschitz-smooth. 3. Convexity and boundedness of regularizers. The regularizers ΩSNR and Ωcomplexity are convex in θ, and their gradients are bounded. Let ΩSNR(θ) G1, Ωcomplexity(θ) G2 θ. 4. Stationary distribution D. The equilibrium participa- tion strategies induce a stationary effective distribution D. Over sufficiently large timescales, the system does not drift away from this equilibrium, and samples (x, y) can be considered drawn i.i.d. from D. 5. Unbiased gradient estimates. When the aggregator requests a training update, a subset of sensors, deter- mined by equilibrium conditions, provide local gradi- ents. Although not all sensors participate every time, the equilibrium ensures a stable pattern of participation. Averaged over multiple rounds, the collected gradients form an unbiased estimator b L(θ) of L(θ): E[b L(θ)] L(θ). Since the regularizers are deterministic, their gradients ΩSNR(θ) and Ωcomplexity(θ) do not introduce bias. Integration of Regularizers in Backpropagation and SGD During the training iteration k: 1. Forward pass: Each participating sensor collects data (x, y) and evaluates ℓ(fθk(x), y). 10 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 2. Backward pass: The sensor computes θℓ(fθk(x), y) via standard backpropagation. To incorporate regu- larizers, the sensor (or the aggregator after collecting updates) adds λ1 ΩSNR(θk) and λ2 Ωcomplexity(θk). These gradients are computed analytically since the regularizers are explicit, differentiable functions of θ. 3. Aggregation: The aggregator averages the received gradients: b J(θk) b L(θk) λ1 ΩSNR(θk) λ2 Ωcomplexity(θk). Since E[b L(θk)] L(θk), we also have E[b J(θk)] J(θk). 4. Update step: With a chosen step size αk, θk 1 θk αk b J(θk). Diminishing Step-Size and Convergence Results Classical convex optimization theory (see Bottou et al. (2018) or Nemirovski et al. (2009)) states that for con- vex, Lipschitz-smooth objectives and unbiased gradient oracles, SGD converges to a stationary point if the step sizes {αk} decrease at an appropriate rate. A common choice is αk 1 k, but any diminishing sequence with P k αk and P k α2 k works. Under these conditions, we have: lim k E[J(θk)] J(θ ) and lim k E[ J(θk) ] 0. This implies θk converges in expectation to a stationary point θ of J(θ). Equilibrium Stability and Impact on Stationarity The key subtlety is that D depends on equilibrium strategies. However, the equilibrium ensures a stable operating regime where sensor behaviors and thus D do not change dras- tically over time. This stability allows us to treat D as effectively fixed for the purpose of the asymptotic analy- sis. If D were to drift significantly, standard SGD results would not directly apply. The equilibrium prevents such non-stationary behavior in the long run. Furthermore, since the regularizers are deterministic and have bounded gradients, they do not add pathological con- ditions to the optimization landscape. They may alter the shape of J(θ), encouraging certain regions of parameter space, but they do not prevent convergence. On the con- trary, they may help by smoothing out undesirable minima or limiting model complexity. Bounding and Selecting Hyperparameters λ1, λ2 The choice of λ1 and λ2 affects the curvature of J(θ) and can influence convergence speed and the location of θ . Some guidelines include: 1. Start with small values of λ1 and λ2 to avoid over- whelming the primary loss L(θ). Gradually increase them if the model relies too heavily on high-SNR data or becomes too complex. 2. Ensure λ1 c1 G1 and λ2 c2 G2 for some constants c1, c2 0, to prevent excessively large gradients due to the regularizers. 3. Tune λ1, λ2 based on validation performance. If the model overfits high-SNR data, increase λ1. If it be- comes too large and slow to run, increase λ2. By keeping λ1, λ2 within reasonable bounds, we ensure that the modified gradient b J(θ) remains well-behaved, preserving the conditions for SGD convergence. We have shown that under the stated assump- tions convexity and smoothness of ℓ, convexity and bounded gradients of ΩSNR and Ωcomplexity, stationarity of D induced by equilibrium strategies, and unbiased gradient estimates the diminishing step-size SGD applied to J(θ) converges in expectation to a stationary point θ . The equilibrium ensures D remains stable, allowing classi- cal stochastic optimization theory to hold. The regularizers, being convex and with bounded gradients, integrate seam- lessly into the backpropagation and SGD updates, shaping the optimization landscape but not invalidating convergence properties. Proper selection and tuning of λ1, λ2 help main- tain stable and robust training dynamics. Thus, the proposed training process achieves a harmonious balance: it respects the strategic, energy-constrained envi- ronment (through equilibrium and game-theoretic consider- ations), while leveraging well-established convex optimiza- tion guarantees to ensure convergence of the global model parameters. 11\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\n000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053 054 Participation Games: Collaborative Strategies for Robust Inference in Energy Harvesting Wireless Sensor Networks Anonymous Authors1 Abstract Energy-harvesting wireless sensor networks (EH- WSNs) offer sustainable solutions for large-scale IoT deployments but face challenges due to the unreliability and intermittent availability of in- dividual sensors. We propose a comprehensive framework that integrates a game-theoretic partici- pation strategy with a federated learning approach tailored for EH-WSNs. Our game-theoretic model enables sensors to make optimal participation de- cisions based on energy levels, data quality, and collective inference impact, fostering cooperative behavior while managing individual energy con- straints. The federated learning framework ac- commodates intermittent participation and vari- able data quality, ensuring robust model training despite sensor unreliability. Simulation results demonstrate that our integrated approach signif- icantly enhances inference accuracy and energy efficiency compared to traditional participation strategies. 1. Introduction The rapid proliferation of the Internet of Things (IoT) has sparked a tremendous growth in the scale and diversity of sensor deployments, from smart homes to expansive in- dustrial and environmental monitoring systems. As these networks continue to expand, sustaining continuous opera- tion in the face of finite power sources becomes a paramount concern. To address this, energy harvesting (EH) technolo- gies have emerged as a viable solution, enabling sensors to convert ambient energy (e.g., solar, thermal, or vibration) into electrical power. This approach promises perpetual, maintenance-free operation, significantly reducing environ- mental impact and long-term operational costs. 1Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute. Despite these advantages, large-scale EH Wireless Sensor Networks (EH-WSNs) remain inherently uncertain.\n\n--- Segment 2 ---\nDo not distribute. Despite these advantages, large-scale EH Wireless Sensor Networks (EH-WSNs) remain inherently uncertain. Ambi- ent energy availability varies over time and space, leading to fluctuating sensor activity levels and intermittent participa- tion in both training and inference tasks. Some sensors may frequently become inactive or produce low-quality data due to energy scarcity or environmental noise. Consequently, the mere presence of numerous EH sensors does not guaran- tee robust and reliable performance for complex tasks such as image recognition, acoustic surveillance, or precision agriculture monitoring. Achieving accurate inference in these complex scenarios de- pends on effective coordination. Multiple sensors observing the same phenomenon from different angles can collectively provide more comprehensive and reliable insights than any single sensor could. However, requiring all sensors to partic- ipate at all times is impractical, as it drains energy reserves too quickly. Conversely, simplistic policies such as se- lecting only the highest-energy sensors ignore factors like data relevance, sensor quality, and the strategic implications of current participation on future network states. This challenge motivates the need for intelligent, context- aware participation strategies that dynamically determine which sensors should engage during both the training phase where global model parameters are periodically fine-tuned or updated and the inference phase where newly observed data are aggregated to produce predictions. Sensors must carefully balance immediate accuracy gains against conserving energy for future tasks, while also antici- pating the behavior of other sensors that may be collaborat- ing or competing. To address these interdependent decisions, we employ a game-theoretic framework. Unlike simple heuristic meth- ods that ignore future resource allocation or complex ap- proaches like reinforcement learning that may be too costly to implement, game theory provides equilibrium guaran- tees. By modeling each sensor as a rational player aiming to optimize its own long-term utility, we achieve stable, coop- erative equilibria where no sensor can improve its outcome through unilateral deviation.\n\n--- Segment 3 ---\nUnlike simple heuristic meth- ods that ignore future resource allocation or complex ap- proaches like reinforcement learning that may be too costly to implement, game theory provides equilibrium guaran- tees. By modeling each sensor as a rational player aiming to optimize its own long-term utility, we achieve stable, coop- erative equilibria where no sensor can improve its outcome through unilateral deviation. This strategic equilibrium un- derpins both training and inference participation decisions, ensuring that the sensors most likely to improve the global 1 055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 model given their energy, data quality, and network condi- tions are the ones that engage. To refine the global model parameters without incurring continuous on-edge training costs, we adopt a federated learning paradigm adapted to EH-WSNs. Rather than rely- ing on persistent, centralized updates or continuous feder- ated aggregation, we perform periodic or equilibrium-driven fine-tuning rounds. These updates occur only when sensors have sufficient energy to participate meaningfully, guided by the game-theoretic equilibrium strategy. By integrating the game-theoretic approach with federated learning principles, we reduce communication overhead and ensure that contri- butions to model updates come from sensors best positioned to improve accuracy under energy constraints and uncertain availability. Our key contributions are as follows: Game-Theoretic Participation Strategy: We develop a novel game-theoretic model for EH-WSNs that applies to both training and inference phases. This model balances anticipated energy availability, local data quality, and global benefit to establish stable and cooperative equilibria, opti- mizing the energy-accuracy trade-offs. Federated Learning Integration: We introduce a fed- erated learning-based framework tailored for intermittent participation and heterogeneous data quality. Unlike con- tinuous on-edge training, we employ periodic or triggered fine-tuning sessions aligned with equilibrium strategies, en- suring robust and progressively improving global models.\n\n--- Segment 4 ---\nFederated Learning Integration: We introduce a fed- erated learning-based framework tailored for intermittent participation and heterogeneous data quality. Unlike con- tinuous on-edge training, we employ periodic or triggered fine-tuning sessions aligned with equilibrium strategies, en- suring robust and progressively improving global models. Joint Optimization of Training and Inference: Our unified solution aligns training participation decisions with inference needs. Sensors strategically decide when to ex- pend energy on local model updates and when to engage in inference tasks, ultimately maximizing their long-term contribution to the network s performance. Demonstrated Performance Gains: Through simula- tions (add simulation details later), we show that our inte- grated framework outperforms baseline approaches such as always-on participation or simplistic energy-based selec- tion by achieving higher inference accuracy, lower energy consumption, and more sustainable long-term operation in EH-WSNs. By tackling the dual challenges of sensor unreliability and energy scarcity through a rigorous game-theoretic and fed- erated learning lens, our work addresses a critical gap in the design of sustainable, intelligent EH-WSNs. This inte- grated framework is theoretically grounded, yet practical for a wide range of IoT applications, from remote wildlife mon- itoring to large-scale industrial status tracking and precision agriculture. The remainder of this paper is organized as follows. In Sec- tion 3, we present the system model, detailing the EH-WSN setup and data capture process. In Section 4, we introduce the game-theoretic model of sensor participation, motivat- ing our approach against simpler heuristics and discussing why equilibrium solutions are desirable. Section 5 outlines the training and fine-tuning framework that integrates the equilibrium strategies into a federated learning paradigm. Fi- nally, Section ? ? presents simulation results and Section ? ? concludes with a discussion of limitations and future work. 2. Background and Related Work Very basic, just the outline, compact and make robust 2.1. Energy Harvesting Wireless Sensor Networks Energy harvesting wireless sensor networks (EH-WSNs) have emerged as a sustainable solution for long-term envi- ronmental monitoring, infrastructure surveillance, and IoT applications (?).\n\n--- Segment 5 ---\nBackground and Related Work Very basic, just the outline, compact and make robust 2.1. Energy Harvesting Wireless Sensor Networks Energy harvesting wireless sensor networks (EH-WSNs) have emerged as a sustainable solution for long-term envi- ronmental monitoring, infrastructure surveillance, and IoT applications (?). By harnessing ambient energy sources such as solar, thermal, or kinetic energy, EH sensors can operate indefinitely without the need for battery replace- ment or external power supplies. However, the intermittent and unpredictable nature of harvested energy introduces significant challenges in maintaining reliable and consistent network performance (?). The unreliability of individual EH sensors, due to fluctua- tions in energy availability, necessitates the deployment of a large number of inexpensive and potentially unreliable de- vices to ensure network robustness. This redundancy allows for continuous operation despite individual sensor failures or downtime. However, it also introduces complexities in coordinating sensor activities, managing energy resources, and ensuring efficient data collection and processing (?). 2.2. Participation Strategies in EH-WSNs Efficient participation strategies are critical in EH-WSNs to optimize network performance while conserving limited energy resources. Traditional approaches often assume con- tinuous participation of all sensors, which is impractical in energy-constrained environments (?). Some methods pro- pose selecting a subset of sensors based on energy levels or predefined schedules (? ), but these can lead to suboptimal performance by not considering the sensors data quality or potential future contributions. Several works have explored adaptive participation strate- gies that consider energy harvesting rates, energy consump- tion patterns, and application-specific requirements (?). These strategies aim to balance energy expenditure with the need for timely and accurate data, often using heuristic or optimization-based approaches. However, they may not fully exploit the potential for collaboration among sensors or account for the strategic interactions inherent in decen- tralized networks. 2 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 2.3. Game-Theoretic Models in Sensor Networks Game theory provides a powerful framework for modeling and analyzing strategic interactions in distributed systems, including sensor networks (?).\n\n--- Segment 6 ---\n2 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 2.3. Game-Theoretic Models in Sensor Networks Game theory provides a powerful framework for modeling and analyzing strategic interactions in distributed systems, including sensor networks (?). In the context of EH-WSNs, game-theoretic models have been employed to design dis- tributed algorithms for resource allocation, power control, and cooperative communication (?). Cooperative game theory has been used to encourage collab- oration among sensors to enhance network performance (?). Non-cooperative game models allow sensors to make au- tonomous decisions while considering the potential ac- tions of others, leading to equilibria that balance individ- ual utility with collective goals (?). However, integrating game-theoretic participation strategies with machine learn- ing tasks, particularly in energy-harvesting environments, remains an area with limited exploration. 2.4. Federated Learning in Resource-Constrained Environments Federated learning enables multiple devices to collabora- tively train a global model without sharing raw data, pre- serving privacy and reducing communication overhead (?). While federated learning has gained significant attention in mobile and IoT devices, applying it to EH-WSNs presents unique challenges due to intermittent participation, limited computational capabilities, and variable data quality (?). Recent studies have begun to address federated learning in resource-constrained and unreliable networks. Strategies in- clude adaptive aggregation methods, energy-aware training schedules, and robustness to device dropouts (?). However, these approaches often assume some level of reliability or do not fully integrate energy harvesting dynamics into the learning process. 2.5. Multi-View Learning and Collaborative Inference Multi-view learning leverages multiple sources or perspec- tives to improve learning performance (?). In EH-WSNs, sensors providing different views of the same scene can en- hance inference accuracy through collaborative processing. Techniques such as co-training, consensus learning, and en- semble methods have been explored to combine information from multiple sensors (?). Collaborative inference in sensor networks involves com- bining local inferences to achieve a global understanding of the environment (?).\n\n--- Segment 7 ---\nTechniques such as co-training, consensus learning, and en- semble methods have been explored to combine information from multiple sensors (?). Collaborative inference in sensor networks involves com- bining local inferences to achieve a global understanding of the environment (?). Challenges include aligning hetero- geneous data, managing communication costs, and dealing with unreliable or missing inputs. Existing methods may not account for the energy constraints and participation vari- ability inherent in EH-WSNs. 3. System Model We consider a network of N EH sensors S {s1, s2, . . . , sN} deployed to monitor a common scene. Each sensor observes the environment from a distinct van- tage point. Time is slotted and indexed by t N. In each time slot, the network may perform an inference event, dur- ing which sensors have the opportunity to contribute data that enhances the accuracy of a global inference task, such as object detection or environmental classification. Each sensor si harvests energy from ambient sources, such as solar or vibrational energy, resulting in a stochastically varying energy supply. We denote by Ei(t) the energy harvested by sensor si during slot t. The sensor maintains an energy buffer whose state evolves as Bi(t 1) Bi(t) Ei(t) ei(t), where Bi(t) is the energy available at the beginning of slot t, and ei(t) is the energy expended during that slot. Predicting future energy intake is challenging, so each sensor employs an estimator ˆEi(t 1) to anticipate its upcoming energy resources. Incorporating uncertainty-aware models or robust estimation techniques is beyond the scope of this paper. Prior to deployment, a global inference model fθ is trained offline on representative data and distributed to each sensor. This model maps sensor observations to inference outputs. Although parameters θ can theoretically be updated through on-edge training, we assume that frequent retraining in situ is prohibitively expensive given energy constraints. Thus, θ remains largely static post-deployment. Sensors focus on inference using their local copies of fθ. However, the sub- sequent training framework, discussed in Section 5, allows for occasional fine-tuning of θ based on equilibrium-driven participation, thereby refining the model to better suit the operational dynamics of the network.\n\n--- Segment 8 ---\nSensors focus on inference using their local copies of fθ. However, the sub- sequent training framework, discussed in Section 5, allows for occasional fine-tuning of θ based on equilibrium-driven participation, thereby refining the model to better suit the operational dynamics of the network. In each inference event, sensors decide whether to partic- ipate. If sensor si participates at time t, it must capture data at a chosen Signal-to-Noise Ratio (SNR), process the data using fθ, and transmit the result to a designated lead sensor. High-SNR data capture improves the sensor s con- tribution to global accuracy but consumes more energy. Let ecap(SNR) denote the energy required for capture at a given SNR level. We assume a monotonic relationship: higher SNR increases both the capture cost and the expected ac- curacy contribution. This assumption simplifies the model by ensuring that better data quality unequivocally enhances inference performance, while also making the energy ex- penditure predictable. In addition to capture costs, partici- pation incurs inference computation cost einf and communi- cation cost ecomm. Thus, if sensor si participates with SNR SNRi(t), its total energy expenditure is ei(t) ecap(SNRi(t)) einf ecomm 3 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 The improvement in global inference accuracy due to sen- sor si is denoted by Ai(t). This quantity depends on SNRi(t) and on the data contributed by other participating sensors, as their combined perspectives shape the overall result. While Ai(t) may not be known precisely, we as- sume that each sensor can estimate its expected contribution based on historical observations and current conditions. Ev- ery inference event presents a binary decision for sensor si: ai(t) {Participate (P), Not Participate (NP)}. Choosing P involves selecting an SNR level, incurring energy costs, and aiming to improve global accuracy. Choosing NP con- serves energy but forfeits any contribution or associated reward.\n\n--- Segment 9 ---\nChoosing P involves selecting an SNR level, incurring energy costs, and aiming to improve global accuracy. Choosing NP con- serves energy but forfeits any contribution or associated reward. Because sensors have limited energy and the net- work may operate for extended periods, each sensor must consider the future implications of its current actions. The interplay of multiple sensors making similar decisions un- der uncertainty and energy constraints naturally suggests a game-theoretic framework for modeling their interactions. 4. Game-Theoretic Modeling 4.1. Motivation for Game Theory over Simpler Methods While heuristic methods such as always selecting the top- k sensors based on current energy levels or greedy algo- rithms that maximize immediate utility might offer straight- forward solutions, they fall short in addressing the strate- gic and long-term dynamics inherent in EH-WSNs. These simplistic approaches ignore the interdependencies among sensor decisions and fail to account for future resource allo- cation, potentially leading to suboptimal performance over time. For instance, always selecting the highest-energy sen- sors can rapidly deplete their energy reserves, reducing the network s resilience during critical future events. Alternatively, reinforcement learning or Markov Decision Process-based approaches could adaptively learn partici- pation policies that consider both immediate rewards and future states. However, these methods often require exten- sive training data, significant computational resources, and complex communication protocols, which may be impracti- cal for resource-constrained sensor nodes. In contrast, a game-theoretic framework provides equilib- rium guarantees, ensuring stable and cooperative partici- pation strategies. By modeling each sensor as a rational player optimizing its own utility, we can derive participation patterns that are robust against unilateral deviations. This stability is crucial for maintaining long-term network per- formance without necessitating continuous recalibration or extensive communication overhead. 4.2. Utility Function Definition We define a utility function Ui(t) for each sensor si that en- capsulates the trade-off between accuracy gains and energy expenditures, as well as future opportunities. The utility function is designed to reflect both immediate rewards and long-term sustainability. Immediate Rewards and Penalties: Let γ 0 be a scal- ing factor that translates accuracy gains into utility rewards.\n\n--- Segment 10 ---\nThe utility function is designed to reflect both immediate rewards and long-term sustainability. Immediate Rewards and Penalties: Let γ 0 be a scal- ing factor that translates accuracy gains into utility rewards. When sensor si participates (ai(t) P) and contributes cor- rectly to the inference task, it receives a reward proportional to the improvement in global accuracy, denoted by Ai(t): Ri(t) γ Ai(t), if ai(t) P and correct inference, δ, if ai(t) P and incorrect inference, η, if ai(t) NP. Here, δ 0 penalizes incorrect participation, discourag- ing sensors from submitting low-quality data, while η 0 penalizes non-participation to prevent perpetual abstention. Importantly, we set η δ, ensuring that consistently opting out is more detrimental than occasionally providing inaccu- rate data. Energy Costs and Future Utility: Participation incurs energy costs, reducing the sensor s capacity for future tasks. Additionally, sensors must consider the discounted value of future utility. Let Ci(t) represent the cost component: Ci(t) ei(t) βVi(t 1), where ei(t) is the total energy expenditure for participa- tion, encompassing data capture, inference computation, and communication: ei(t) ecap(SNRi(t)) einf ecomm. The discount factor β [0, 1) captures how sensors value future utility, with Vi(t 1) representing the expected fu- ture utility given current decisions and predicted energy availability ˆEi(t 1). Overall Utility Function: Combining immediate rewards and costs, the overall utility function for sensor si at time t is: Ui(t) Ri(t) Ci(t). This utility function effectively balances the benefits of participation against the associated costs and future oppor- tunities, guiding sensors to make strategic decisions that optimize their long-term contributions to the network. Nash Equilibrium and Stability: A Nash equilibrium (NE) represents a stable action profile a (t) where no sensor can unilaterally improve its utility by deviating from its current strategy: Ui(a i (t), a i(t)) Ui(ai(t), a i(t)) ai(t), i.\n\n--- Segment 11 ---\nThis utility function effectively balances the benefits of participation against the associated costs and future oppor- tunities, guiding sensors to make strategic decisions that optimize their long-term contributions to the network. Nash Equilibrium and Stability: A Nash equilibrium (NE) represents a stable action profile a (t) where no sensor can unilaterally improve its utility by deviating from its current strategy: Ui(a i (t), a i(t)) Ui(ai(t), a i(t)) ai(t), i. 4 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 Achieving an NE ensures that sensor participation patterns are stable; once equilibrium is reached, no single sensor ben- efits from changing its participation decision independently. This stability is critical for maintaining consistent network performance and energy sustainability over time. Distributed Best-Response Algorithm: To realize the NE, we propose a distributed best-response algorithm where each sensor iteratively adjusts its action based on the current state and the expected actions of others. The algorithm operates as follows: Algorithm 1 Distributed Best-Response Participation Algo- rithm 1: Input: Current energies Bi(t), predicted harvest ˆEi(t 1), parameters γ, δ, η, β, and energy costs ecap( ), einf, ecomm. 2: At each inference event: 3: Each sensor si receives a solicitation from the lead sensor and forms an estimate of Ai(t) given potential SNR choices and expected actions of others. 4: For each action candidate ai(t) {P, NP}, the sensor computes the expected utility: U ai(t) i E[Ri(t)] E[ei(t)] βE[Vi(t 1)], where the expectations are taken over uncertainties in correctness, SNR impact, and future energy. 5: If U P i U NP i and Bi(t) ecap(SNRi(t)) einf ecomm, then si chooses P. Otherwise, it chooses NP.\n\n--- Segment 12 ---\n4: For each action candidate ai(t) {P, NP}, the sensor computes the expected utility: U ai(t) i E[Ri(t)] E[ei(t)] βE[Vi(t 1)], where the expectations are taken over uncertainties in correctness, SNR impact, and future energy. 5: If U P i U NP i and Bi(t) ecap(SNRi(t)) einf ecomm, then si chooses P. Otherwise, it chooses NP. 6: After all sensors decide, the action profile a(t) is real- ized, and energies are updated: Bi(t 1) Bi(t) Ei(t) ei(t). 7: Sensors iterate this process at each inference event, re- fining their estimates and converging to stable action patterns. Sensors employ this best-response mechanism, continuously updating their participation decisions based on the evolving network state and the actions of other sensors. Over repeated iterations, under suitable conditions, this process converges to a Nash equilibrium where participation strategies are mutually optimal. Existence and Convergence of Equilibrium: Theorem 4.1. Suppose that each utility function Ui(t) is non-decreasing in Ai(t), that energy constraints and dis- counting ensure diminishing marginal returns for repeated deviations, and that sensors have consistent estimation of Ai(t) and ˆEi(t 1). Then, the iterative best-response updates described in Algorithm 1 converge to a Nash equi- librium action profile a (t). Proof of Theorem 4.1: The proof constructs a potential function Φ(a(t)) PN i 1 Ui(ai(t), a i(t)) that strictly in- creases whenever a sensor makes a profitable unilateral deviation. Since utilities are bounded (due to finite en- ergy and limited accuracy gains) and returns diminish over time, no infinite sequence of profitable deviations is possi- ble. Hence, the best-response dynamics must terminate at a profile where no sensor can improve its utility alone, i.e., a Nash equilibrium. A complete formal proof, including all technical conditions, is provided in Appendix B.\n\n--- Segment 13 ---\nHence, the best-response dynamics must terminate at a profile where no sensor can improve its utility alone, i.e., a Nash equilibrium. A complete formal proof, including all technical conditions, is provided in Appendix B. Guidelines for Hyperparameter Selection: The param- eters γ, δ, and η critically influence sensor behavior by dictating the trade-offs between participation rewards, penal- ties for incorrect submissions, and deterrents against non- participation. Detailed guidelines for selecting these hy- perparameters are provided in Appendix A. Briefly, these parameters should be chosen to ensure that: (1) γ suffi- ciently incentivizes correct participation without leading to excessive energy expenditure. (2) δ appropriately penalizes incorrect inferences, discouraging low-quality data contri- butions. (3) η δ to prevent sensors from consistently abstaining, thereby promoting overall network engagement. These guidelines help in balancing immediate utility gains with long-term energy sustainability, ensuring that the game- theoretic model drives desirable participation behaviors. 5. Training and Aggregation Framework Having established the equilibrium participation strategies and the underlying reward-based utility functions, we now consider the training process that fine-tunes the global in- ference model θ Rd within this EH, multi-sensor envi- ronment. Initially, θ is pre-trained offline and deployed to all sensors, enabling them to perform basic inference tasks. However, this initial model may not be optimally adapted to the complex operational reality of the network, where sensors strategically choose SNR levels, participate inter- mittently according to equilibrium strategies, and generate data distributions that deviate from the original training set. The goal of the training process is to adjust θ to these condi- tions, effectively fine-tuning the model to the nonstationary data distribution D induced by the sensors equilibrium be- haviors. At equilibrium, sensors strike a balance between accurate data contribution and energy conservation, result- ing in a stable pattern of participation and SNR choices. Over time, this induces a stationary, albeit non-trivial, effec- tive data distribution D. Learning Approach: Our approach diverges from classi- cal Learning paradigms. We adopt a hybrid strategy where periodic or event-triggered updates refine the model parame- ters based on equilibrium-driven data collection.\n\n--- Segment 14 ---\nOver time, this induces a stationary, albeit non-trivial, effec- tive data distribution D. Learning Approach: Our approach diverges from classi- cal Learning paradigms. We adopt a hybrid strategy where periodic or event-triggered updates refine the model parame- ters based on equilibrium-driven data collection. This hybrid 5 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 approach mitigates the high communication overhead and energy consumption, making it more suitable for resource- constrained EH-WSNs. While traditional methods require persistent communication between sensors and the aggre- gator, our framework leverages the established equilibrium strategies to determine optimal times for model updates. By aligning update events with periods when sensors are most likely to participate meaningfully, we ensure that the global model is refined efficiently without imposing excessive en- ergy demands on the sensors. Training Objective and Regularization: To enhance ro- bustness and efficiency, we incorporate regularizers that pe- nalize undesirable model properties. Specifically, we intro- duce two regularizers: (1) ΩSNR(θ): Encourages the model to maintain performance across varying SNR levels, pre- venting over-reliance on high-SNR data. (2) Ωcomplexity(θ): Controls model complexity, reducing computational and communication overheads by discouraging overly intricate models. The full training objective is formulated as: J(θ) L(θ) λ1ΩSNR(θ) λ2Ωcomplexity(θ), where L(θ) E(x,y) D[ℓ(fθ(x), y)], and λ1, λ2 0 are hyperparameters that balance accuracy, robustness, and efficiency. Gradient Computation and Backpropagation: Integrat- ing regularizers into the training process is straightforward due to their known closed-form gradients.\n\n--- Segment 15 ---\nThe full training objective is formulated as: J(θ) L(θ) λ1ΩSNR(θ) λ2Ωcomplexity(θ), where L(θ) E(x,y) D[ℓ(fθ(x), y)], and λ1, λ2 0 are hyperparameters that balance accuracy, robustness, and efficiency. Gradient Computation and Backpropagation: Integrat- ing regularizers into the training process is straightforward due to their known closed-form gradients. During back- propagation, each sensor computes the gradient of the loss function ℓ(fθ(x), y) with respect to θ based on locally available samples from D. Additionally, the gradients of the regularizers, ΩSNR(θ) and Ωcomplexity(θ), are analyt- ically derived and added to the local gradient estimates. Since both regularizers are convex and smooth, their in- clusion ensures that the overall objective J(θ) maintains desirable convexity and smoothness properties, facilitating the convergence of stochastic gradient descent (SGD). Periodic Equilibrium-Aware Training: Model updates are performed periodically at an aggregator node that col- lects gradient estimates from participating sensors. Partici- pation during training follows the same equilibrium model: sensors decide whether to compute and send gradients based on their current energy states, predicted future utilities, and the established reward structure. By aggregating these gradi- ent updates over multiple training rounds, the aggregator ap- proximates the gradient J(θ) and performs an SGD step. Our training framework is encapsulated in Algorithm 2, which outlines the periodic equilibrium-aware training pro- cess. This training framework is intrinsically linked to the game-theoretic participation strategies. Sensors participate in training rounds based on their equilibrium-driven de- cisions, ensuring that gradient updates are contributed by those sensors most capable and willing to improve the global model. This alignment minimizes unnecessary energy ex- penditure and maximizes the efficacy of each training round. Algorithm 2 Periodic Equilibrium-Aware Training Algo- rithm 1: Initialization: Initialize θ0. Broadcast θ0 to all sensors.\n\n--- Segment 16 ---\nAlgorithm 2 Periodic Equilibrium-Aware Training Algo- rithm 1: Initialization: Initialize θ0. Broadcast θ0 to all sensors. Set a diminishing step-size schedule {αk}k 0. 2: for each training round k 0, 1, 2, . . . do 3: The aggregator signals that a training update round is imminent. 4: Sensors decide on participation. Participation in- volves: 1. Determining if they have enough energy and incentive (based on the established equilibrium strategy and reward parameters γ, δ, η). 2. If participating: capturing data at their cho- sen SNR, performing inference, and computing local gradients ℓ(fθk(x), y) on their locally available samples drawn from D. 3. Adding regularizer gradients λ1 ΩSNR(θk) and λ2 Ωcomplexity(θk). 5: A subset of sensors, determined by the equilibrium, send their gradient estimates to the aggregator. 6: The aggregator forms an unbiased estimate of the full gradient: b J(θk) b L(θk) λ1 ΩSNR(θk) λ2 Ωcomplexity(θk). 7: Update model parameters: θk 1 θk αk b J(θk). 8: Broadcast θk 1 to all sensors. 9: end for Regularizers and SGD Convergence: The chosen reg- ularizers ΩSNR(θ) and Ωcomplexity(θ) are both convex and smooth, with known closed-form gradients. This property guarantees that the inclusion of regularizers does not com- promise the convexity or smoothness of the overall objective J(θ). Consequently, the stochastic gradient descent (SGD) updates retain their convergence properties, ensuring that the training process reliably optimizes J(θ). 6 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 6. Implementation and Evaluation 6.1.\n\n--- Segment 17 ---\n6 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 6. Implementation and Evaluation 6.1. Discussions and Limitations 7. Conclusion This should finish at 8 pages. Impact Statement Authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. This statement should be in an unnumbered section at the end of the paper (co- located with Acknowledgments the two may appear in either order, but both must be before References), and does not count toward the paper page limit. In many cases, where the ethical impacts and expected societal implications are those that are well established when advancing the field of Machine Learning, substantial discussion is not required, and a simple statement such as the following will suffice: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. The above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review. References A. Guidelines for Hyperparameter Selection and Bounds on Reward Parameters The parameters γ, δ, and η govern the reward structure of the proposed framework, influencing whether sensors participate consistently, over-participate and waste energy, or abstain altogether. This appendix provides a systematic approach to selecting these parameters, including formal bounds, practical heuristics, and an algorithmic procedure to explore suitable values. Conceptual Role of Parameters The scalar γ 0 represents the reward scaling for correct participation. If γ is too low, sensors will not have suffi- cient incentive to expend energy on high-SNR captures. If γ is too high, sensors may waste energy attempting difficult inferences. The parameter δ 0 penalizes incorrect in- ferences, discouraging reckless submissions of low-quality data. The parameter η 0 penalizes non-participation, ensuring that sensors do not remain idle indefinitely. As discussed, maintaining η δ encourages sensors to at least attempt participation rather than always remain offline.\n\n--- Segment 18 ---\nThe parameter η 0 penalizes non-participation, ensuring that sensors do not remain idle indefinitely. As discussed, maintaining η δ encourages sensors to at least attempt participation rather than always remain offline. Formal Bounds and Conditions To ensure balanced behavior, it is helpful to relate γ, δ, and η to typical values of accuracy improvement and energy costs. Accuracy Gains and Costs: Let Amin and Amax de- note the minimum and maximum expected accuracy im- provements from any sensor s participation. Let emax total emax cap einf ecomm represent the maximum energy cost (for a chosen SNR mode). A baseline condition that ensures correct participation can overcome occasional penalties is: γ Amin δ emax total . This inequality implies that even in a worst-case scenario for accuracy gain, the net expected benefit of correct par- ticipation surpasses the sum of potential incorrect penalties and energy costs. Without this condition, sensors might find participation systematically unprofitable. Non-Participation and Equilibrium: Since η δ, we ensure that sensors prefer risking occasional incorrect infer- ences over consistently abstaining. A suitable gap might be chosen so that: δ η δ c, for some small c 0. Choosing c relative to typical gains, say c 0.1 γ Amax, helps maintain a moderate deterrent against non-participation without forcing sensors to always participate. 7 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 Energy Preservation: If γ is too large, sensors might not value future energy at all. To prevent myopic strategies, one can limit γ such that continuously investing in high-SNR captures does not dominate long-term considerations. For example: γ Amax η margin, where margin accounts for future opportunities and energy savings. A small margin ensures sensors do not always expend maximal energy for short-term gains. Practical Hyperparameter Tuning Strategies 1. Baseline Ratios: Start with ratios that link γ to typical accuracy gains and set δ, η based on fractions or multiples of γ Amin or γ Amax. 2.\n\n--- Segment 19 ---\nBaseline Ratios: Start with ratios that link γ to typical accuracy gains and set δ, η based on fractions or multiples of γ Amin or γ Amax. 2. Iterative Refinement: Use simulation or small-scale ex- perimental runs to refine parameters. If sensors rarely par- ticipate, increase γ or decrease η. If sensors over-exert themselves, reduce γ or increase δ, η. 3. Adaptive Tuning: If conditions change over time, adjust γ, δ, and η dynamically based on observed participation rates, accuracy levels, and energy depletion patterns. Exploration Algorithm Algorithm 3 outlines a systematic approach to exploring suit- able hyperparameter values. It combines theoretical bounds with empirical evaluation, guiding the search toward stable and efficient equilibria. The above guidelines and the explo- ration algorithm provide a structured approach to selecting and refining γ, δ, and η. By starting from theoretically in- formed baseline conditions and iteratively refining through simulation-based feedback, it is possible to reach a stable set of parameters that promotes balanced participation, discour- ages perpetual abstention, and prevents excessive energy expenditure. Regular re-tuning may be warranted as oper- ating conditions, energy harvesting patterns, or accuracy requirements evolve over the network s lifetime. B. Equilibrium Existence and Convergence with Reward-Based Utility In this appendix, we provide a detailed and formal proof that the best-response dynamics, incorporating the newly defined reward-based utility functions, converge to a Nash equilibrium (NE). We first restate the key assumptions and the utility model. We then show that the iterative best- response updates cannot lead to infinite improvement cycles, implying the existence of an NE. Finally, we prove that the equilibrium is reached under the given assumptions. Algorithm 3 Hyperparameter Exploration for Reward Pa- rameters 1: Inputs: Estimates Amin, Amax, energy costs emax cap , einf, ecomm, initial guesses γ0, δ0, η0, and tuning increments γ, δ, η. 2: Compute emax total emax cap einf ecomm. 3: Ensure baseline feasibility: If γ0 Amin δ0 emax total , increase γ0 until this condition is met. 4: Set η0 δ0.\n\n--- Segment 20 ---\n3: Ensure baseline feasibility: If γ0 Amin δ0 emax total , increase γ0 until this condition is met. 4: Set η0 δ0. Start with η0 δ0 c, where c is a small positive number. If preliminary tests show insufficient participation, slightly increase η0. If participation is overly aggressive, reduce γ0 or increase δ0. 5: Simulation-Refinement Loop: 6: for k 1, 2, . . . , K (number of refinement iterations) do 7: Run a simulation or small-scale test deployment us- ing the current γk, δk, ηk. 8: Measure key indicators: participation rate, average energy depletion rate, frequency of incorrect infer- ences, and overall inference accuracy. 9: if participation is too low (e.g., pmin) or sensors remain idle too often then 10: Increase γk γk γ or decrease ηk ηk η. 11: else if participation is too high, leading to frequent energy depletion then 12: Decrease γk γk γ or increase δk δk δ to discourage high-risk attempts. 13: else if incorrect inferences are prevalent then 14: Increase δk δk δ to penalize low-quality submissions more strongly. 15: end if 16: Check feasibility conditions again to ensure no viola- tion of baseline inequalities. 17: If performance metrics (accuracy, sustainability) are satisfactory, terminate. Otherwise, continue refine- ment. 18: end for Restatement of the Utility Function and Assumptions Recall that at each inference event t, each sensor si chooses an action ai(t) {P, NP}. The chosen action profile is a(t) (a1(t), . . . , aN(t)). The immediate reward for sensor si is defined as: Ri(t) γ Ai(t), if ai(t) P and inference is correct, δ, if ai(t) P and inference is incorrect, η, if ai(t) NP.\n\n--- Segment 21 ---\n, aN(t)). The immediate reward for sensor si is defined as: Ri(t) γ Ai(t), if ai(t) P and inference is correct, δ, if ai(t) P and inference is incorrect, η, if ai(t) NP. Here, γ 0 scales the reward for correct participation, δ 0 penalizes incorrect inference, and η 0 penalizes non-participation, with η δ ensuring that remaining idle 8 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 is more penalizing than at least attempting participation. The cost incorporates energy consumption and future op- portunities. Let ei(t) be the energy expenditure for sensor si if it participates at time t, accounting for capture, infer- ence, and communication costs. Introduce a discount factor β [0, 1), and let Vi(t 1) represent the expected future utility of sensor si given its current decisions and predicted energy availability. The cost is: Ci(t) ei(t) βVi(t 1). The overall utility is: Ui(t) Ri(t) Ci(t). We assume that Ai(t) is non-decreasing in the quality of sensor si s data (e.g., higher SNR yields higher Ai(t)). We also assume that energy resources, accuracy gains, and reward penalty parameters are finite and bounded, and that sensors have consistent estimation mechanisms for Ai(t) and ˆEi(t 1). Potential Function Construction To prove convergence, we define a potential function that reflects the collective utility of the sensor network: Φ(a(t)) N X i 1 Ui(ai(t), a i(t)). Since Ui(t) Ri(t) Ci(t), we have: Φ(a(t)) N X i 1 [Ri(t) Ci(t)].\n\n--- Segment 22 ---\nPotential Function Construction To prove convergence, we define a potential function that reflects the collective utility of the sensor network: Φ(a(t)) N X i 1 Ui(ai(t), a i(t)). Since Ui(t) Ri(t) Ci(t), we have: Φ(a(t)) N X i 1 [Ri(t) Ci(t)]. The terms Ri(t) depend on the chosen actions and correct- ness of inferences. Due to bounded γ, δ, and η, and the fact that Ai(t) and energy costs are bounded, each Ui(t) is finite. Thus, Φ(a(t)) is also finite for all feasible action profiles. Monotonicity of the Potential Function Consider a unilateral deviation by a single sensor sj from an action aj(t) to a different action a j(t). Such a deviation affects only Uj(t), not the utilities of other sensors directly in a one-step change. If this deviation is profitable for sensor sj, we have: Uj(a j(t), a j(t)) Uj(aj(t), a j(t)). Because the other sensors utilities do not change instan- taneously by sj s unilateral action, the increment in Uj(t) results in: Φ(a j(t), a j(t)) Φ(a(t)) Uj(a j(t), a j(t)) Uj(aj(t), a j(t)) 0. Thus, any unilateral profitable deviation increases Φ(a(t)). Boundedness and Impossibility of Infinite Improvement Sequences Since all utilities are bounded (due to finite γ, δ, η, bounded Ai(t), and bounded energy resources), there exists a finite upper bound Φmax such that: Φ(a(t)) Φmax a(t). Suppose, for contradiction, that there exists an infinite se- quence of unilateral profitable deviations. Each such de- viation strictly increases Φ(a(t)). Because Φ is bounded above by Φmax, only a finite number of increments can occur before no further improvements are possible. This contradiction shows that no infinite improvement sequence can occur.\n\n--- Segment 23 ---\nBecause Φ is bounded above by Φmax, only a finite number of increments can occur before no further improvements are possible. This contradiction shows that no infinite improvement sequence can occur. Existence of a Nash Equilibrium Since no infinite sequence of profitable unilateral deviations can occur, the best-response dynamics must terminate in a state where no sensor can unilaterally improve its utility. By definition, this state is a Nash equilibrium a (t): Ui(a i (t), a i(t)) Ui(ai(t), a i(t)) ai(t), i. Thus, the existence of a Nash equilibrium follows directly from the finiteness of utilities, the monotonicity of Φ, and the impossibility of infinite improvement sequences. Convergence to the Nash Equilibrium The final step is to show that the iterative best-response dy- namics indeed converge to the NE identified above. Since each sensor s best-response update seeks to maximize its own utility, sensors will continue to deviate as long as prof- itable deviations exist. Our argument shows that profitable deviations must terminate. Under the assumptions that Ai(t) is non-decreasing and that sensors have consistent energy and accuracy estimates, no cyclical behavior can persist. A cycle would imply an infinite sequence of im- provements or a return to a previously visited state without improvement, which cannot occur since profitable devia- tions strictly increase Φ(a(t)). The presence of the discount factor β further stabilizes the process. With β [0, 1), sensors value future utility less than immediate utility. This discounting ensures diminish- ing returns for postponing beneficial participation or indefi- nitely waiting for ideal conditions. As a result, sensors do not continually defer improvements, preventing complex long-term cycles.\n\n--- Segment 24 ---\nThis discounting ensures diminish- ing returns for postponing beneficial participation or indefi- nitely waiting for ideal conditions. As a result, sensors do not continually defer improvements, preventing complex long-term cycles. 9 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 Because the best-response process eliminates profitable de- viations step by step and cannot cycle indefinitely, the action profile sequence generated by iterative best responses con- verges to the NE. We have shown that, with the reward-based utility function that includes correct participation rewards (γ Ai(t)), penalties for incorrect inferences (δ), and penalties for non- participation (η), the best-response dynamics lead to a Nash equilibrium. The proof relies on constructing a potential function Φ that is strictly increased by unilateral profitable deviations and bounded above. The impossibility of infinite improvement sequences guarantees the existence of an NE, and the assumptions on monotonicity, boundedness, and discounting ensure that the iterative best-response process converges to this equilibrium. C. Proof of Convergence for the Equilibrium-Aware Training Process In this appendix, we provide a comprehensive and detailed proof of the convergence theorem stated in the main text. We also elaborate on how the new loss function, the introduced regularizers, and their gradients integrate into the backprop- agation and stochastic gradient descent (SGD) steps. Addi- tionally, we discuss bounds on the newly introduced hyper- parameters and provide guidelines for selecting them. Problem Setting and Notation We consider a global inference model fθ : X Y parame- terized by θ Rd. The model s performance is measured by a loss function ℓ: Y Y R 0 that is convex in θ for any fixed input-label pair (x, y). The model operates in an energy-harvesting wireless sensor network (EH-WSN) envi- ronment where sensors participate strategically in inference tasks based on a game-theoretic equilibrium.\n\n--- Segment 25 ---\nThe model s performance is measured by a loss function ℓ: Y Y R 0 that is convex in θ for any fixed input-label pair (x, y). The model operates in an energy-harvesting wireless sensor network (EH-WSN) envi- ronment where sensors participate strategically in inference tasks based on a game-theoretic equilibrium. Let D denote the effective data distribution induced by the equilibrium strategies of the sensors. Under equilibrium conditions, the distribution D is stationary or at least sta- tionary over sufficiently large timescales. The expected loss is L(θ) E(x,y) D[ℓ(fθ(x), y)]. To enhance robustness and efficiency, we introduce two regularizers: ΩSNR(θ) and Ωcomplexity(θ). ΩSNR(θ) encourages the model to perform reasonably well across varying SNR levels, while Ωcomplexity(θ) penalizes overly complex models that might demand excessive energy or communication costs. Both are assumed convex and have bounded gradients. The final training objective is: J(θ) L(θ) λ1ΩSNR(θ) λ2Ωcomplexity(θ), where λ1, λ2 0 are hyperparameters controlling the influ- ence of the regularizers. Our goal is to show that by running a diminishing step-size SGD on J(θ), using unbiased gradient estimates from the equilibrium distribution D, the parameters {θk} converge in expectation to a stationary point θ of J(θ). Key Assumptions and Conditions 1. Convexity of ℓ. The loss ℓ(fθ(x), y) is convex in θ. Consequently, the expected loss L(θ) is also convex. 2. L-smoothness of ℓ. There exists a constant L 0 such that for all θ, θ , L(θ) L(θ ) L θ θ . This ensures that L(θ) is Lipschitz-smooth. 3. Convexity and boundedness of regularizers.\n\n--- Segment 26 ---\n3. Convexity and boundedness of regularizers. The regularizers ΩSNR and Ωcomplexity are convex in θ, and their gradients are bounded. Let ΩSNR(θ) G1, Ωcomplexity(θ) G2 θ. 4. Stationary distribution D. The equilibrium participa- tion strategies induce a stationary effective distribution D. Over sufficiently large timescales, the system does not drift away from this equilibrium, and samples (x, y) can be considered drawn i.i.d. from D. 5. Unbiased gradient estimates. When the aggregator requests a training update, a subset of sensors, deter- mined by equilibrium conditions, provide local gradi- ents. Although not all sensors participate every time, the equilibrium ensures a stable pattern of participation. Averaged over multiple rounds, the collected gradients form an unbiased estimator b L(θ) of L(θ): E[b L(θ)] L(θ). Since the regularizers are deterministic, their gradients ΩSNR(θ) and Ωcomplexity(θ) do not introduce bias. Integration of Regularizers in Backpropagation and SGD During the training iteration k: 1. Forward pass: Each participating sensor collects data (x, y) and evaluates ℓ(fθk(x), y). 10 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 2. Backward pass: The sensor computes θℓ(fθk(x), y) via standard backpropagation. To incorporate regu- larizers, the sensor (or the aggregator after collecting updates) adds λ1 ΩSNR(θk) and λ2 Ωcomplexity(θk).\n\n--- Segment 27 ---\nBackward pass: The sensor computes θℓ(fθk(x), y) via standard backpropagation. To incorporate regu- larizers, the sensor (or the aggregator after collecting updates) adds λ1 ΩSNR(θk) and λ2 Ωcomplexity(θk). These gradients are computed analytically since the regularizers are explicit, differentiable functions of θ. 3. Aggregation: The aggregator averages the received gradients: b J(θk) b L(θk) λ1 ΩSNR(θk) λ2 Ωcomplexity(θk). Since E[b L(θk)] L(θk), we also have E[b J(θk)] J(θk). 4. Update step: With a chosen step size αk, θk 1 θk αk b J(θk). Diminishing Step-Size and Convergence Results Classical convex optimization theory (see Bottou et al. (2018) or Nemirovski et al. (2009)) states that for con- vex, Lipschitz-smooth objectives and unbiased gradient oracles, SGD converges to a stationary point if the step sizes {αk} decrease at an appropriate rate. A common choice is αk 1 k, but any diminishing sequence with P k αk and P k α2 k works. Under these conditions, we have: lim k E[J(θk)] J(θ ) and lim k E[ J(θk) ] 0. This implies θk converges in expectation to a stationary point θ of J(θ). Equilibrium Stability and Impact on Stationarity The key subtlety is that D depends on equilibrium strategies. However, the equilibrium ensures a stable operating regime where sensor behaviors and thus D do not change dras- tically over time. This stability allows us to treat D as effectively fixed for the purpose of the asymptotic analy- sis. If D were to drift significantly, standard SGD results would not directly apply. The equilibrium prevents such non-stationary behavior in the long run.\n\n--- Segment 28 ---\nIf D were to drift significantly, standard SGD results would not directly apply. The equilibrium prevents such non-stationary behavior in the long run. Furthermore, since the regularizers are deterministic and have bounded gradients, they do not add pathological con- ditions to the optimization landscape. They may alter the shape of J(θ), encouraging certain regions of parameter space, but they do not prevent convergence. On the con- trary, they may help by smoothing out undesirable minima or limiting model complexity. Bounding and Selecting Hyperparameters λ1, λ2 The choice of λ1 and λ2 affects the curvature of J(θ) and can influence convergence speed and the location of θ . Some guidelines include: 1. Start with small values of λ1 and λ2 to avoid over- whelming the primary loss L(θ). Gradually increase them if the model relies too heavily on high-SNR data or becomes too complex. 2. Ensure λ1 c1 G1 and λ2 c2 G2 for some constants c1, c2 0, to prevent excessively large gradients due to the regularizers. 3. Tune λ1, λ2 based on validation performance. If the model overfits high-SNR data, increase λ1. If it be- comes too large and slow to run, increase λ2. By keeping λ1, λ2 within reasonable bounds, we ensure that the modified gradient b J(θ) remains well-behaved, preserving the conditions for SGD convergence. We have shown that under the stated assump- tions convexity and smoothness of ℓ, convexity and bounded gradients of ΩSNR and Ωcomplexity, stationarity of D induced by equilibrium strategies, and unbiased gradient estimates the diminishing step-size SGD applied to J(θ) converges in expectation to a stationary point θ . The equilibrium ensures D remains stable, allowing classi- cal stochastic optimization theory to hold. The regularizers, being convex and with bounded gradients, integrate seam- lessly into the backpropagation and SGD updates, shaping the optimization landscape but not invalidating convergence properties. Proper selection and tuning of λ1, λ2 help main- tain stable and robust training dynamics.\n\n--- Segment 29 ---\nThe regularizers, being convex and with bounded gradients, integrate seam- lessly into the backpropagation and SGD updates, shaping the optimization landscape but not invalidating convergence properties. Proper selection and tuning of λ1, λ2 help main- tain stable and robust training dynamics. Thus, the proposed training process achieves a harmonious balance: it respects the strategic, energy-constrained envi- ronment (through equilibrium and game-theoretic consider- ations), while leveraging well-established convex optimiza- tion guarantees to ensure convergence of the global model parameters. 11\n\n