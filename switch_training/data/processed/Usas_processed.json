{
  "source": "Usas.pdf",
  "raw_length": 106711,
  "cleaned_length": 105710,
  "base_segments": 433,
  "augmented_segments": 866,
  "segments": [
    {
      "text": "Abstract —Edge servers have recently become very popular for performing localized analytics, especially on video, as they reduce data trafﬁc and protect privacy. However, due to their resource constraints, these servers often employ compressed models, which are typically prone to data drift. Consequently, for edge servers to provide cloud-comparable quality, they must also perform continuous learning to mitigate this drift.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "Consequently, for edge servers to provide cloud-comparable quality, they must also perform continuous learning to mitigate this drift. However, at expected deployment scales, performing continuous training on every edge server is not sustainable due to their aggregate power demands on grid supply and associated sustainability footprints. To address these challenges, we propose Us.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "To address these challenges, we propose Us. ´as, an ap- proach combining algorithmic adjustments, hardware-software co-design, and morphable acceleration hardware to enable the training of workloads on these edge servers to be powered by re- newable, but intermittent, solar power that can sustainably scale alongside data sources. Our evaluation of Us.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "Our evaluation of Us. ´as on a real-world trafﬁc dataset indicates that our continuous learning approach simultaneously improves both accuracy and efﬁciency: Us. ´as offers a 4.96% greater mean accuracy than prior approaches while our morphable accelerator that adapts to solar variance can save up to  { 234.95kWH, 2.63MWH } /year/edge-server compared to a  { DNN accelerator, data center scale GPU } , respectively.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "´as offers a 4.96% greater mean accuracy than prior approaches while our morphable accelerator that adapts to solar variance can save up to  { 234.95kWH, 2.63MWH } /year/edge-server compared to a  { DNN accelerator, data center scale GPU } , respectively. I. I NTRODUCTION \nThe rampant growth, and anticipated sustained expansion of data collection and consumption are currently driving data-driven analytics using trained inference models, with signiﬁcant economic impact. Amidst the myriad of data-driven domains, urban mobility, smart cities, autonomous driving, and the Internet of Things (IoT) emerge as some of the most rapidly expanding ﬁelds contributing to the global economy, amounting to more than 4 trillion US dollars [ 1 ], [ 54 ], [ 76 ], [ 98 ].",
      "type": "sliding_window",
      "tokens": 188
    },
    {
      "text": "Amidst the myriad of data-driven domains, urban mobility, smart cities, autonomous driving, and the Internet of Things (IoT) emerge as some of the most rapidly expanding ﬁelds contributing to the global economy, amounting to more than 4 trillion US dollars [ 1 ], [ 54 ], [ 76 ], [ 98 ]. These statistics underscore the profound signiﬁcance and transformative potential of these data-driven realms, delineat- ing their pivotal role in shaping the landscape of computing technology, from algorithms to architecture. What distinguishes these data is their diverse origin, span- ning from IoT devices to wearables, and their acquisition from challenging environments, including autonomous driving and urban mobility scenarios.",
      "type": "sliding_window",
      "tokens": 156
    },
    {
      "text": "What distinguishes these data is their diverse origin, span- ning from IoT devices to wearables, and their acquisition from challenging environments, including autonomous driving and urban mobility scenarios. Consequently, they frequently ex- hibit a phenomenon known as “data drift”, where the incoming data deviates from the distribution of the originally trained model, leading to degradation in inference accuracy. Mitigating Data Drift:  Dealing with data drift in edge com- pute nodes presents a signiﬁcant challenge.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "Mitigating Data Drift:  Dealing with data drift in edge com- pute nodes presents a signiﬁcant challenge. While larger models with more parameters may exhibit limited data drift due to their increased capacity to generalize, deploying such large models on edge compute nodes can be difﬁcult due to \ninherent limitations in form factor, energy efﬁciency, thermal constraints, and compute resources. To accommodate these constraints, it is a common practice to employ compressed Deep Neural Network (DNN) models, that are quantized, distilled, or otherwise reduced in size.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "To accommodate these constraints, it is a common practice to employ compressed Deep Neural Network (DNN) models, that are quantized, distilled, or otherwise reduced in size. However, while com- pressed models are essential for meeting resource limitations, they are more sensitive to data drift because they may not generalize as effectively. Traditionally, data drift has been handled by cloud-based periodic re-training using continuous learning algorithms [ 20 ], [ 74 ].",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "Traditionally, data drift has been handled by cloud-based periodic re-training using continuous learning algorithms [ 20 ], [ 74 ]. However, there are challenges in resources, privacy, and sustainability to utilize existing techniques at envisioned scales. As these applications become more ubiquitous, partic- ularly in urban deployments for tasks like trafﬁc surveillance, autonomous driving, and health analytics [ 18 ], [ 77 ], [ 90 ], demands on communication bandwidth and network reliability limit the direct streaming of diverse data (e.g., video, 3D point cloud, sensor, voice) from numerous sensor-compute nodes to the cloud.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "As these applications become more ubiquitous, partic- ularly in urban deployments for tasks like trafﬁc surveillance, autonomous driving, and health analytics [ 18 ], [ 77 ], [ 90 ], demands on communication bandwidth and network reliability limit the direct streaming of diverse data (e.g., video, 3D point cloud, sensor, voice) from numerous sensor-compute nodes to the cloud. Moreover, recent changes in privacy regulations across multiple countries [ 2 ], [ 100 ] call for preserving the privacy of citizens [ 12 ] and may preclude streaming personal data to third-party cloud services. As a result, “on-premise” edge servers  [ 7 ], [ 8 ] have become prime choices for local inference and prediction [ 6 ], [ 39 ], [ 85 ], [ 86 ], necessitating the handling of both learning and inference tasks to meet application needs, including privacy preservation, reduced data communication, and disaggregated computing.",
      "type": "sliding_window",
      "tokens": 213
    },
    {
      "text": "As a result, “on-premise” edge servers  [ 7 ], [ 8 ] have become prime choices for local inference and prediction [ 6 ], [ 39 ], [ 85 ], [ 86 ], necessitating the handling of both learning and inference tasks to meet application needs, including privacy preservation, reduced data communication, and disaggregated computing. Finally, although recent studies have suggested co-locating training and inference [ 12 ] to tackle privacy concerns without signiﬁcantly affecting the inference service, the power demand associated with equipping multiple commercial edge servers [ 7 ], [ 8 ] for both tasks hinders sustainable scaling. The Problem Space:  To address the multi-faceted challenges of sustainable, scalable and privacy-preserving continuous learning at edge servers, several crucial problem spaces must be explored.",
      "type": "sliding_window",
      "tokens": 175
    },
    {
      "text": "The Problem Space:  To address the multi-faceted challenges of sustainable, scalable and privacy-preserving continuous learning at edge servers, several crucial problem spaces must be explored. Firstly, the issue of  (non-)supervision  arises, demanding the ability to label data without human interven- tion to preserve privacy during the learning process. While recent works [ 12 ], [ 46 ] have attempted to tackle this concern through student-teacher paradigms, efﬁciently deploying such approaches in complex data modalities (e.g., multi-class video, 3D point cloud) remains a formidable challenge.",
      "type": "sliding_window",
      "tokens": 128
    },
    {
      "text": "While recent works [ 12 ], [ 46 ] have attempted to tackle this concern through student-teacher paradigms, efﬁciently deploying such approaches in complex data modalities (e.g., multi-class video, 3D point cloud) remains a formidable challenge. Ensuring adherence to Service Level Agreements (SLAs), where in- ference typically utilizes a lower-resource model [ 51 ], [ 75 ] and labeling is performed using a larger teacher model [ 44 ], \n891 \n2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA) \n2378-203X/24/$31.00 ©2024 IEEE DOI 10.1109/HPCA57654.2024.00073 \n2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA) | 979-8-3503-9313-2/24/$31.00 ©2024 IEEE | DOI: 10.1109/HPCA57654.2024.00073 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 249
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. [ 50 ], [ 73 ] at a much lower rate, necessitates informed decisions regarding deployment placement and sampling rates.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "[ 50 ], [ 73 ] at a much lower rate, necessitates informed decisions regarding deployment placement and sampling rates. Secondly, the issue of  functionality  comes to the fore, requiring effective continuous learning from often non- Independently and Identically Distributed (non-IID) data. Such non-IID data distributions, evident in tasks like standard trafﬁc monitoring with varied class observations (e.g., more cars than buses, all frames having  STOP  signs), may introduce sampling bias  [ 70 ], [ 74 ] in the network.",
      "type": "sliding_window",
      "tokens": 126
    },
    {
      "text": "Such non-IID data distributions, evident in tasks like standard trafﬁc monitoring with varied class observations (e.g., more cars than buses, all frames having  STOP  signs), may introduce sampling bias  [ 70 ], [ 74 ] in the network. This challenge can be addressed through proper  exemplar selection  algorithms employing representation learning techniques [ 31 ], [ 74 ], ca- pable of learning new classes in real-time. However, these compute-intensive algorithms can be optimized further through dedicated hardware acceleration.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "However, these compute-intensive algorithms can be optimized further through dedicated hardware acceleration. Thirdly, the aspect of  sustainability  poses a critical question of deploying such systems, ideally with minimal reliance on the power grid for learning tasks. Designing a learning platform that can adapt to intermittent renewable energy sources (e.g., solar power) and maintain a minimal operational carbon footprint [ 29 ] is paramount.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "Designing a learning platform that can adapt to intermittent renewable energy sources (e.g., solar power) and maintain a minimal operational carbon footprint [ 29 ] is paramount. Such a platform should continuously make progress on unsupervised labeling, exem- plar building, and continuous learning, and maximize  drift mitigation  while minimizing power consumption. Moreover, the system must accommodate  support for intermittency inherent in sustainable power sources like solar and wind.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "Moreover, the system must accommodate  support for intermittency inherent in sustainable power sources like solar and wind. While incorporating conventional battery storage can mitigate intermittency, it introduces environmental and sustainability challenges associated with resource extraction, production, and replacement [ 3 ], [ 5 ], [ 10 ], [ 13 ], [ 53 ], [ 66 ], [ 69 ]. An ideal solution would entail a battery-free system ( not  energy storage- free, i.e., still with some capacitive storage), circumventing these concerns and aligning with the objectives of sustainable and reliable continuous learning at the edge.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "An ideal solution would entail a battery-free system ( not  energy storage- free, i.e., still with some capacitive storage), circumventing these concerns and aligning with the objectives of sustainable and reliable continuous learning at the edge. To these ends, we propose Us.´as,   1   a HW-SW co-design approach to building sustainable, scalable, drift-mitigating edge analytics platforms using harvested power to support continuous learning. Us.´as, unlike prior edge-focused analytics approaches (e.g., Ekya [ 12 ]), detaches the inference and train- ing hardware, as the training task is the major source of the compute, power, and time consumption.",
      "type": "sliding_window",
      "tokens": 162
    },
    {
      "text": "Us.´as, unlike prior edge-focused analytics approaches (e.g., Ekya [ 12 ]), detaches the inference and train- ing hardware, as the training task is the major source of the compute, power, and time consumption. Us.´as introduces an al- gorithmic framework for data labeling using a teacher-student model, designing the exemplar selection using representation learning and determining the right set of hyperparameters using micro proﬁling to energy-efﬁciently continuously train the DNNs with the selected exemplar sets. Us.´as also employs a dynamically morphable systolic array for enabling energy- efﬁcient computing within the harvested power envelope.",
      "type": "sliding_window",
      "tokens": 165
    },
    {
      "text": "Us.´as also employs a dynamically morphable systolic array for enabling energy- efﬁcient computing within the harvested power envelope. Key contributions  of the work include: •  We propose algorithmic enhancements of  continuous learn- \ning  for mitigating data drift and design a  student-teacher based automated data labelling algorithm , to prepare train- ing exemplars from input data. We use a two-level data annotation mechanism: exemplar identiﬁcation based on the \n1 Vedic goddess of dawn in Hinduism [ 36 ]; emphasizing the dawn of sustainable continuous learning and signiﬁcance of solar power in our design.",
      "type": "sliding_window",
      "tokens": 143
    },
    {
      "text": "We use a two-level data annotation mechanism: exemplar identiﬁcation based on the \n1 Vedic goddess of dawn in Hinduism [ 36 ]; emphasizing the dawn of sustainable continuous learning and signiﬁcance of solar power in our design. conﬁdence matrix of the student model, followed by a repre- sentation learning based exemplar selection by ensembling multiple teacher models. Our policy updates  both  the teacher and student models for robust unsupervised learning.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Our policy updates  both  the teacher and student models for robust unsupervised learning. •  We implement a  micro-proﬁler , which predicts the right set \nof hyper-parameters to efﬁciently perform the training tasks on an energy-harvesting edge server while operating within its power budget and minimizing data drift. •  We design a  morphable hardware accelerator  that efﬁ- \nciently maps training tasks, is suitable for intermittent computing, and can adapt its capabilities to reduce power emergencies without devolving to grid operation.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "•  We design a  morphable hardware accelerator  that efﬁ- \nciently maps training tasks, is suitable for intermittent computing, and can adapt its capabilities to reduce power emergencies without devolving to grid operation. We dis- cuss how the proposed hardware techniques can be adapted by many of the current DNN training accelerators to add similar dynamism in sustainability-sensitive environments. •  Finally, we evaluate Us.´as in depth on a  real-world trafﬁc \ndata set  [ 97 ] and perform sensitivity studies on other classes (audio, IMU) of data.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "•  Finally, we evaluate Us.´as in depth on a  real-world trafﬁc \ndata set  [ 97 ] and perform sensitivity studies on other classes (audio, IMU) of data. Our algorithmic framework for performing continuous learning has a 4.96% greater mean accuracy than a na¨ıve continuous learner. Power estimations of our hardware design, modeled by Design Compiler [ 93 ], indicate that the proposed morphable accelerator approach can save up to 234.95kWH/year/edge-server, compared to running continuous learning on a state of the art DNN accelerator and 2.63MWH/year/edge-server, compared to utilizing a datacenter-scale GPU for learning on the edge.",
      "type": "sliding_window",
      "tokens": 166
    },
    {
      "text": "Power estimations of our hardware design, modeled by Design Compiler [ 93 ], indicate that the proposed morphable accelerator approach can save up to 234.95kWH/year/edge-server, compared to running continuous learning on a state of the art DNN accelerator and 2.63MWH/year/edge-server, compared to utilizing a datacenter-scale GPU for learning on the edge. II. B ACKGROUND AND  M OTIVATION \nEdge servers often leverage the convenience and ﬂexibil- ity of cloud interfaces, granting access to the same APIs, tools, and functionalities [ 60 ].",
      "type": "sliding_window",
      "tokens": 140
    },
    {
      "text": "B ACKGROUND AND  M OTIVATION \nEdge servers often leverage the convenience and ﬂexibil- ity of cloud interfaces, granting access to the same APIs, tools, and functionalities [ 60 ]. However, due to their inherent limitations in resources, such as weak GPUs and smaller memory capacities [ 83 ], these servers often resort to “cus- tomized” analytics services to maximize throughput and meet SLAs, including specialized DNN models tailored for edge deployments [ 51 ], [ 75 ], which are compressed, quantized, and optimized for the targeted hardware [ 30 ], [ 103 ], [ 109 ]. These tailored models enable accurate inference with high throughput and reduced resource footprint, with some compressed models having approximately 50 ×  fewer parameters [ 30 ], but with a greater susceptibility to data drift [ 42 ], [ 55 ].",
      "type": "sliding_window",
      "tokens": 193
    },
    {
      "text": "These tailored models enable accurate inference with high throughput and reduced resource footprint, with some compressed models having approximately 50 ×  fewer parameters [ 30 ], but with a greater susceptibility to data drift [ 42 ], [ 55 ]. Data drift emerges as a signiﬁcant concern in real-world systems as the live data diverges from the original training data, and the environment undergoes rapid changes [ 12 ]. Fig.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Fig. 1  depicts our experimental investigations on data drift, encompassing training and testing multiple DNNs on diverse datasets such as Urban Trafﬁc [ 12 ], [ 97 ], 3D Point Cloud [ 14 ], [ 24 ], and audio [ 78 ]. The similar trends across these results highlight the impact of varying time windows and encounter- ing diverse scene changes, leading to degradation in network accuracy by up to 30%.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "The similar trends across these results highlight the impact of varying time windows and encounter- ing diverse scene changes, leading to degradation in network accuracy by up to 30%. These ﬁndings underscore the critical challenge posed by data drift and the need for continuous learning on edge servers. Continuous Learning at the Edge: Continuous learning, wherein the model continually learns from new samples over time, adapting to seen and previously unseen classes, has \n892 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Continuous Learning at the Edge: Continuous learning, wherein the model continually learns from new samples over time, adapting to seen and previously unseen classes, has \n892 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Restrictions apply. 30 35 40 45 50 55 60 65 70 75 80 \nBaseline \nWin-1 \nWin-2 \nWin-3 \nWin-4 \nWin-5 \nBaseline \nWin-1 \nWin-2 \nWin-3 \nWin-4 \nWin-5 \nBaseline \nWin-1 \nWin-2 \nWin-3 \nWin-4 \nWin-5 \nVideo Audio 3DPC \nAccuracy (%) \nSM SMR LM \nFig. 1: Data drift on different data modalities.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "1: Data drift on different data modalities. Sampling window size: 4hours for video, 20 minutes for audio for urban trafﬁc video and audio data. 1hour for 3D Point Cloud simulated data.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "1hour for 3D Point Cloud simulated data. [SM:Small Model (smaller model or larger model pruned and quantized using energy aware pruning [ 102 ] and NetAdapat [ 103 ]), LM:Large Model (no pruning or quan- tization), SMR:Small Model with Retraining]. emerged as a preferred approach to mitigate data drift [ 20 ], [ 44 ], [ 50 ], [ 74 ].",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "emerged as a preferred approach to mitigate data drift [ 20 ], [ 44 ], [ 50 ], [ 74 ]. The temporal locality of (video like) data has shown models to effectively learn from recent data. Although, multiple task-dedicated models are typically deployed to enhance accuracy and reduce sampling bias [ 70 ], particularly in scenarios like trafﬁc monitoring, where different time periods exhibit distinct trafﬁc patterns, they are not immune to data drift.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Although, multiple task-dedicated models are typically deployed to enhance accuracy and reduce sampling bias [ 70 ], particularly in scenarios like trafﬁc monitoring, where different time periods exhibit distinct trafﬁc patterns, they are not immune to data drift. As depicted in Fig. 1 , our experiments, on different modalities, shows the accuracy degradation due to data drift.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "1 , our experiments, on different modalities, shows the accuracy degradation due to data drift. Speciﬁcally focuing on video data, we observe that: using quantized MobileNet-v2 (14M paramters, 71.3% accuracy) as the small model and ResNet-101 (171M parameters, 76.4% accuracy) as the large model, the accuracy of the smaller model has degraded  >  20% over 5 sampling windows (of 4 hours each), where as the effect is minimal in the larger model. However, with a proper retraining, the smaller model could keep up with the original accuracy.",
      "type": "sliding_window",
      "tokens": 135
    },
    {
      "text": "However, with a proper retraining, the smaller model could keep up with the original accuracy. We also observe a similar trend over other modalities, making the importance of continuous learning clear for multiple domains. However, in a continuous learning paradigm, training be- comes an essential, repeatedly scheduled task whose computa- tional and time costs cannot be considered a one-time overhead freely delegated to the cloud.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "However, in a continuous learning paradigm, training be- comes an essential, repeatedly scheduled task whose computa- tional and time costs cannot be considered a one-time overhead freely delegated to the cloud. A recent work, Ekya [ 12 ], has demonstrated that edge servers equipped with GPUs are capable of performing the necessary tasks for continuous learning within their form-factor-imposed resource constraints, provided that those resources are intelligently managed. Sustainable  Continuous Learning at the Edge:  Even given such advancements in continuous learning on edge servers, provisioning training resources at the edge for every sensing- to-analytics application entails sustainability questions.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "Sustainable  Continuous Learning at the Edge:  Even given such advancements in continuous learning on edge servers, provisioning training resources at the edge for every sensing- to-analytics application entails sustainability questions. For ex- ample, a popular AWS outpost, a g4dn.12xlarge instance [ 83 ], consists of a 24 core Intel Xeon CPU (150W TDP) [ 71 ] with 192GB of memory and 4 NVIDIA T4 (with tensor cores, 70W TDP) [ 65 ] with 64GB GPU memory. A standard offering with 2 × g4dn.12xlarge instances need 4kW power [ 7 ] (the compute units have a TDP of  ≈ 1 kW  [ 65 ], [ 71 ]) for \nperforming analytics.",
      "type": "sliding_window",
      "tokens": 191
    },
    {
      "text": "A standard offering with 2 × g4dn.12xlarge instances need 4kW power [ 7 ] (the compute units have a TDP of  ≈ 1 kW  [ 65 ], [ 71 ]) for \nperforming analytics. With state-of-the-art learning APIs [ 60 ] and intelligent co-location and scheduling of inference and continuous learning [ 12 ], these edge servers can support about 8 videos streams [ 12 ], resulting in  ≈ 120 W  (just for compute) per video stream. Scaling this to crowded cities with 30- 50+kilo-cameras like Beverly Hills ( >  35 k  [ 11 ]), Los Angeles ( ≈ 35 k  [ 92 ]), New York ( ≈ 56k [ 92 ]), or Chicago ( ≈ 30k) will need a lot of power.",
      "type": "sliding_window",
      "tokens": 190
    },
    {
      "text": "Scaling this to crowded cities with 30- 50+kilo-cameras like Beverly Hills ( >  35 k  [ 11 ]), Los Angeles ( ≈ 35 k  [ 92 ]), New York ( ≈ 56k [ 92 ]), or Chicago ( ≈ 30k) will need a lot of power. In fact, it will take  ≥ 3Million cameras (assuming  ≈ 9 cameras/1000 people, similar to LA, and scaled to US population) to just enable autonomous urban mobility in the USA, which may consume 360 MW  power (1296 GWh  energy, 0.03% of US power) for video analytics alone. Clearly, the current solution is  not  sustainable, neither in terms of the load on the power grid, nor in terms of the  CO 2  footprint (1.1 × 10 9 lbs); reducing the power budget for continuous learning is essential, as the carbon footprint of DNN training has emerged as a prominent concern [ 21 ], [ 57 ], [ 67 ], [ 89 ], demanding careful consideration as a primary design metric.",
      "type": "sliding_window",
      "tokens": 238
    },
    {
      "text": "Clearly, the current solution is  not  sustainable, neither in terms of the load on the power grid, nor in terms of the  CO 2  footprint (1.1 × 10 9 lbs); reducing the power budget for continuous learning is essential, as the carbon footprint of DNN training has emerged as a prominent concern [ 21 ], [ 57 ], [ 67 ], [ 89 ], demanding careful consideration as a primary design metric. Although green data centers [ 58 ], [ 59 ] provide partial mit- igation, they fail to address data privacy and communication bandwidth challenges in the current context. Similarly, other applications with diverse data modalities, such as LiDAR and Camera for autonomous driving, IMU, bio-sensors, and Speech for IoT, face similar issues.",
      "type": "sliding_window",
      "tokens": 175
    },
    {
      "text": "Similarly, other applications with diverse data modalities, such as LiDAR and Camera for autonomous driving, IMU, bio-sensors, and Speech for IoT, face similar issues. Thus,  attaining a sustain- able solution for privacy-preserving, distributed continuous learning remains an ongoing pursuit. Exploiting Intermittent Computing:  An obvious solution to the power problem is to run the training in a self-sustained way, i.e., without depending on the power grid and by relying on a renewable energy source like solar power; opportunities for harvesting renewables naturally scale alongside a greater number of deployment locations and solar power, even though not always available, is in abundance.",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "Exploiting Intermittent Computing:  An obvious solution to the power problem is to run the training in a self-sustained way, i.e., without depending on the power grid and by relying on a renewable energy source like solar power; opportunities for harvesting renewables naturally scale alongside a greater number of deployment locations and solar power, even though not always available, is in abundance. In the United States, a typical 12% efﬁcient solar panel [ 91 ], can provide an annual average of 50 W / m 2   − 150 W / m 2   of power [ 64 ]. Furthermore, solar power has reasonably predictability characteristics.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "Furthermore, solar power has reasonably predictability characteristics. Typ- ically, inference tasks have signiﬁcantly less compute time and power requirement, and commercial off the shelf devices, like edgeTPU [ 19 ] can perform object detection using the aforementioned compressed models at a reasonable frame rate (at times  ≥ 71  f ps ). Therefore,  designing a training platform to perform continuous learning with the intermittent solar power and within the typical harvested budget  would be the best solution.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Therefore,  designing a training platform to perform continuous learning with the intermittent solar power and within the typical harvested budget  would be the best solution. The power sustainability consequently reduces the cost of deployment as the publicly available edge server, like AWS outpost offering (one of the cheaper and lower power consuming ones) for performing edge inference costs $5,134.92/month [ 83 ]. Our Approach (and its Novelty):  Us.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "Our Approach (and its Novelty):  Us. ´as  introduces several novel contributions in the domain of  sustainable  continuous learning at edge servers using harvested energy, setting it apart from prior works examining on-edge learning. Battery-Free Operation:  A key highlight of  Us.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "Battery-Free Operation:  A key highlight of  Us. ´as  lies in its battery-free operation, which aligns with the current global push for sustainable computing. The scaling up via mil- \n893 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "The scaling up via mil- \n893 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "Restrictions apply. lions of additional battery-supported analytics platforms would introduce severe environmental challenges due to resource extraction, production, and replacement of batteries [ 3 ], [ 5 ], [ 10 ], [ 13 ], [ 53 ], [ 66 ], [ 69 ]. By demonstrating the viability of a battery-less edge server for video analytics, Us.´as spearheads the adoption of similarly sustainable systems for other do- mains.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "By demonstrating the viability of a battery-less edge server for video analytics, Us.´as spearheads the adoption of similarly sustainable systems for other do- mains. While the initial scope is limited to urban mobility applications, the concept’s adaptability extends to various domains, including autonomous driving, smart industries, and remote sensing: Section  V-D  performs an initial exploration of how techniques from Us.´as will apply to other domains. Algorithmic Advancements:  Us.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Algorithmic Advancements:  Us. ´as  extends the frontier of rep- resentation learning for continuous learning by implementing it at a large scale and addressing related challenges. Prior works relied on supervised learning or K-means clustering, un- suitable for  Us.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "Prior works relied on supervised learning or K-means clustering, un- suitable for  Us. ´as  due to its need for unsupervised data annota- tion and the inability to handle large-scale datasets with numer- ous classes. To overcome these limitations,  Us.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "To overcome these limitations,  Us. ´as  employs an ensembled teacher-student method, wherein multiple teachers annotate student data. A hierarchical K-means+ (or DBSCAN) clustering approach learns representations for exemplar se- lection.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "A hierarchical K-means+ (or DBSCAN) clustering approach learns representations for exemplar se- lection. Additionally, a novel power-aware micro-proﬁling policy is adapted to determine optimal hyper-parameters for a variable-power environment. The robust exemplar selection and micro-proﬁling mechanisms are discussed and evaluated in § III-B  and § III-C , respectively.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "The robust exemplar selection and micro-proﬁling mechanisms are discussed and evaluated in § III-B  and § III-C , respectively. Hardware Innovation:  Us.´as embraces the intermittency en- tailed by harvesting and advocates for hardware adaptation (resizing) to efﬁciently manage variable power income and avoid power emergencies. While previous works have de- signed energy-efﬁcient training hardware with support for variable precision training, none have adapted to variable energy income.",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "While previous works have de- signed energy-efﬁcient training hardware with support for variable precision training, none have adapted to variable energy income. Us. ´as  optimizes the entire solution space, maximizing hardware reuse for exemplar selection and micro- proﬁling while addressing the training task.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "´as  optimizes the entire solution space, maximizing hardware reuse for exemplar selection and micro- proﬁling while addressing the training task. The system can turn off individual compute-tiles to accommodate runtime power variability (see § IV-B ) and enable seamless operation during power reductions. Overall, Us.´as demonstrates the viability of sustainable con- tinuous learning at edge servers, encompassing advancements in energy harvesting, algorithmic techniques, and hardware adaptation.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Overall, Us.´as demonstrates the viability of sustainable con- tinuous learning at edge servers, encompassing advancements in energy harvesting, algorithmic techniques, and hardware adaptation. III. C ONTINUOUS  L EARNING \nThe ﬁrst step to any data-driven learning algorithm is data collection and annotation.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "C ONTINUOUS  L EARNING \nThe ﬁrst step to any data-driven learning algorithm is data collection and annotation. Since  Us. ´as  is a continuous learning framework and learns from the live data that the camera(s) capture, data collection is simply storing the live video feed.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "´as  is a continuous learning framework and learns from the live data that the camera(s) capture, data collection is simply storing the live video feed. However, data annotation or labeling is more challenging. Classically, once data is collected, it is classiﬁed, labeled, and bounded by borders (bounding box) mostly using manual labor (at times with software assistance) or crowd sourcing [ 33 ], [ 82 ], [ 88 ].",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "Classically, once data is collected, it is classiﬁed, labeled, and bounded by borders (bounding box) mostly using manual labor (at times with software assistance) or crowd sourcing [ 33 ], [ 82 ], [ 88 ]. This requires the data to be present at a central location for manual inspection, both of which are not possible because of communication and privacy constraints. Therefore, we adapt a “student-teacher paradigm” [ 46 ], where a more \nClassify \nLow Conf \nFrame?",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "Therefore, we adapt a “student-teacher paradigm” [ 46 ], where a more \nClassify \nLow Conf \nFrame? Discard \nFrame No \nExemplar Yes \nYes \nEdge Model \nConfidence Matrix \nMajority Voting Labeling \nM1 \nM2 \nM3 \nFig. 2: Auto-labeling in  Us.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "2: Auto-labeling in  Us. ´as : Select frames only with low conﬁdence as they might contain potentially new information, and use ensemble learning to improve the labeling. general, robust and larger model (typically with hundreds of millions of parameters [ 43 ], [ 99 ]) helps in annotating the data.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "general, robust and larger model (typically with hundreds of millions of parameters [ 43 ], [ 99 ]) helps in annotating the data. However, because of the heavy compute requirements, the teacher model runs with a much slower frame rate and annotates only some (important) frames. There has been a signiﬁcant body of work on frame similarity and saliency [ 45 ], [ 84 ], [ 101 ], [ 105 ], [ 107 ], [ 108 ], and those details remain beyond the scope of this work.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "There has been a signiﬁcant body of work on frame similarity and saliency [ 45 ], [ 84 ], [ 101 ], [ 105 ], [ 107 ], [ 108 ], and those details remain beyond the scope of this work. A. Data Annotation \nPicking the Important Ones:  Typically, edge models are ca- pable of inferring at the frame rate of the camera (at times, 30fps to 60fps) [ 19 ].",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "Data Annotation \nPicking the Important Ones:  Typically, edge models are ca- pable of inferring at the frame rate of the camera (at times, 30fps to 60fps) [ 19 ]. However, the teacher model used to label the incoming data cannot match this in a resource- constrained environment where performing training is going to be even more resource consuming. Therefore, we employ an intelligent “data sampling mechanism” to select the frames that might contain new information and a potential candidate for learning.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "Therefore, we employ an intelligent “data sampling mechanism” to select the frames that might contain new information and a potential candidate for learning. Fig. 2  shows the different components of the student-teacher data annotation model adapted in  Us.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "2  shows the different components of the student-teacher data annotation model adapted in  Us. ´as , where the edge model is the “student” (continuously retrained), and larger models are the “teachers” (the ones teaching the student about what-is-what). The students models are typically optimized for edge, i.e.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "The students models are typically optimized for edge, i.e. with optimizations like quantization, pruning etc. or by developing an application speciﬁc model from scratch along with the said optimizations.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "or by developing an application speciﬁc model from scratch along with the said optimizations. These student models, thanks to their lack of robustness (which is often, but not always, related to the smaller footprint they have, and thereby lacking the parameter space to generalize better), are susceptible to data drift and hence are continuously retrained. However, the teacher models are typically large, and with a wide parameter space can generalize the learning process better than the students.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "However, the teacher models are typically large, and with a wide parameter space can generalize the learning process better than the students. These teacher models are often factory trained. As they are less prone to drift, they need occasional updates.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "As they are less prone to drift, they need occasional updates. For each sampled frame, the classiﬁcation results and the conﬁdence matrix (output of the last layer) are sent for annotation. If the student (or the edge model) is conﬁdent about the classiﬁcation (e.g.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "If the student (or the edge model) is conﬁdent about the classiﬁcation (e.g. a clear frame with no new objects, or a frame similar to one of the training samples), then that frame is discarded as it potentially contains little to no new information. However, if the student is not conﬁdent on the classiﬁcation, the frame is then saved as a potential exemplar (we will further reﬁne this in § III-B ).",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "However, if the student is not conﬁdent on the classiﬁcation, the frame is then saved as a potential exemplar (we will further reﬁne this in § III-B ). The potential exemplars \n894 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. 0 20 40 60 80 100 \n0 20 40 60 80 100 \nClass Distribution \nAccuracy (%) \nBaseline Train-Win-1 Train-Win-2 Train-Win-4 Appeared \nFig.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "0 20 40 60 80 100 \n0 20 40 60 80 100 \nClass Distribution \nAccuracy (%) \nBaseline Train-Win-1 Train-Win-2 Train-Win-4 Appeared \nFig. 3: Distribution of different classes on a typical trafﬁc pattern and the impact of training on the sampling bias. The ”appeared“ line represents the percentage of the frames in which the corresponding class is present, e.g.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "The ”appeared“ line represents the percentage of the frames in which the corresponding class is present, e.g. Fire hydrant, in the taken scene, is present in 100% of the frames. Incorrect exemplar selection might lead to non-IID training data distri- bution, leading to catastrophic forgetting or over-ﬁtting.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "Incorrect exemplar selection might lead to non-IID training data distri- bution, leading to catastrophic forgetting or over-ﬁtting. are then further reﬁned and classiﬁed by the teacher models. To improve the conﬁdence of the teacher models, we employ an ensemble learning based weighted majority voting policy [ 28 ].",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "To improve the conﬁdence of the teacher models, we employ an ensemble learning based weighted majority voting policy [ 28 ]. Each of the teacher models infers on the exemplar frame. Furthermore, each teacher model has its private conﬁdence matrix on different object classes.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Furthermore, each teacher model has its private conﬁdence matrix on different object classes. This conﬁdence matrix serves as a weight for performing the ensemble of multiple teachers, and helps exploiting the expertise of each of the teacher models for each of the individual classes, signiﬁcantly boosting the accuracy and robustness of the data annotation. This maximizes the accuracy of the teacher, and consequently minimizes the chance of the student model learning wrong labels.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "This maximizes the accuracy of the teacher, and consequently minimizes the chance of the student model learning wrong labels. Note that the limited parameters of the student make it more sensitive to data ﬁdelity and hence ensuring an accurate data labeling is very important for end to end classiﬁcation accuracy. The impact of wrong labeling is discussed in § V .",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "The impact of wrong labeling is discussed in § V . The Problem:  However, this exemplar section mechanism has an inherent ﬂaw. Consider a trafﬁc camera looking at a busy street with a trafﬁc signal.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Consider a trafﬁc camera looking at a busy street with a trafﬁc signal. Due to the trafﬁc distribution (e.g., more cars than buses), the camera typically sees a varied distribution of different classes, which might reﬂect in the exemplar set. Moreover, some static objects (trafﬁc light, stop sign, etc.)",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "Moreover, some static objects (trafﬁc light, stop sign, etc.) might be present in all frames. This creates a sampling “bias” [ 70 ] while performing the training, and often leads to catastrophic forgetting.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "This creates a sampling “bias” [ 70 ] while performing the training, and often leads to catastrophic forgetting. Fig. 3  shows a typical trafﬁc distribution from Urban Trafﬁc data [ 97 ]) and the impact of sampling bias on class distribution.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "3  shows a typical trafﬁc distribution from Urban Trafﬁc data [ 97 ]) and the impact of sampling bias on class distribution. Note that, as some of the classes (e.g., bicycles) are barely present in the exemplar, the model tend to lose accuracy (because of catastrophic forgetting) on them, whereas the model rapidly over-ﬁts for the classes with more examples (e.g., trafﬁc light). B.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "B. Proper Exemplar Selection \nTo tackle the sampling bias [ 70 ], we adapt a representation learning [ 74 ] framework for designing the proper exemplar selection. The fundamental issue with the previous approach is the inability to select correct numbers of IID data for training.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "The fundamental issue with the previous approach is the inability to select correct numbers of IID data for training. In addition to that, just DNN training cannot learn \new classes if there is no way to annotate and label new classes. Representation learning solves both these issues.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Representation learning solves both these issues. The learner (here the teacher models) need to properly classify the data, learn if the data is a new type of one of the older classes, and identify if it encounters a new class. We achieve this by clustering the feature vector of the Large DNN model.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "We achieve this by clustering the feature vector of the Large DNN model. Fundamentally, we use the larger DNN models as feature extractors which turn the data into a feature vector. In the original training phase, these feature vectors are separated using K-means [ 74 ] or other clustering.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "In the original training phase, these feature vectors are separated using K-means [ 74 ] or other clustering. The cluster centers for each data ( μ y  for class  y ) are calculated as  μ y  =   1 \nP y   ∑ p ∈ P y   Φ ( p ) , where  P y  is the number of samples belonging to class (or cluster  y ), and  Φ  is the feature extraction function working on the data  p . These clusters represent the classes in the high dimensional feature space.",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "These clusters represent the classes in the high dimensional feature space. When the classiﬁer sees new data ( x ), it calculates its distance from all the cluster centers as y ∗ =  min y = 1 ... t  || Φ ( x ) − μ y || . There are three cases: Case-1:  If the data is close to one of the cluster centers and belongs to its cluster boundary, then it falls into the bucket of that particular class.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "There are three cases: Case-1:  If the data is close to one of the cluster centers and belongs to its cluster boundary, then it falls into the bucket of that particular class. This typically happens if the data are very similar to the training samples. Case-2:  If the data belongs to a known class, but is signif- icantly different from the training samples, it falls not too far from one of the clusters.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Case-2:  If the data belongs to a known class, but is signif- icantly different from the training samples, it falls not too far from one of the clusters. This distance of the new data from the cluster center is called the “distillation loss” [ 74 ]. An encounter of a new example of the existing class is followed by an update to the clustering by minimizing the classiﬁcation loss of the newly-seen data.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "An encounter of a new example of the existing class is followed by an update to the clustering by minimizing the classiﬁcation loss of the newly-seen data. Case-3:  Finally, if the classiﬁer sees an example of a new class then the feature vector of the data sits far from all the cluster centers indicating an unknown class. The distance of this feature vector from the other cluster center is called “classiﬁcation loss” [ 74 ], and this re-triggers clustering with an updated number of clusters.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "The distance of this feature vector from the other cluster center is called “classiﬁcation loss” [ 74 ], and this re-triggers clustering with an updated number of clusters. Over multiple time windows, the representation learner goes through all the possible exemplars selected by using the conﬁdence matrix and creates an exemplar set with same number of examples from each possible class. Since we have multiple teacher models, each of them contributes to the exemplar set, making it robust and removing bias.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Since we have multiple teacher models, each of them contributes to the exemplar set, making it robust and removing bias. To efﬁciently implement the exemplar selection algorithm,  Us. ´as implements the major portions using “custom hardware” (dis- cussed in § IV-A ).",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "´as implements the major portions using “custom hardware” (dis- cussed in § IV-A ). The annotations on the new exemplar set created by the representation learner is compared against the conﬁdence matrix of the edge model to calculate the “drift”. Consequently, this exemplar set becomes the training data for the continuous learning, which consequently minimizes the drift.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "Consequently, this exemplar set becomes the training data for the continuous learning, which consequently minimizes the drift. Once the student model is trained with the exemplar set, the data is discarded and the feature space for the teacher models is updated. By doing this,  Us.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "By doing this,  Us. ´as  keeps  both  the student and the teacher models “updated.” Since the feature space of the teacher model is updated using K-means+, the major computation is the training of the student model using the exemplar data. Although efﬁcient hardware accelerators [ 16 ], [ 27 ], [ 80 ] have been developed to do the same, these ac- celerators are typically designed with a “throughput-ﬁrst” \n895 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "Although efﬁcient hardware accelerators [ 16 ], [ 27 ], [ 80 ] have been developed to do the same, these ac- celerators are typically designed with a “throughput-ﬁrst” \n895 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "Restrictions apply. approach and are neither conﬁgured nor capable of operating with an intermittent power source. Deploying sufﬁcient battery resources to allow intermittency-unaware designs to operate on solar power is neither efﬁcient nor sustainable.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "Deploying sufﬁcient battery resources to allow intermittency-unaware designs to operate on solar power is neither efﬁcient nor sustainable. C. Hyperparameters: The Right Way to Learn \nAfter ﬁnalizing the training set for continuous learning, the next challenge is to learn within the power and time budget. Given enough time even na¨ıve low power hardware can ﬁnish training, but will have longer periods where the drift is exposed.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Given enough time even na¨ıve low power hardware can ﬁnish training, but will have longer periods where the drift is exposed. A more preferable solution is to get rid of drift as quickly as possible, i.e. ﬁnish training on the exemplars (described in § III-B ) as soon as possible and also reach the desired accuracy – but to do this within the harvested budget.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "ﬁnish training on the exemplars (described in § III-B ) as soon as possible and also reach the desired accuracy – but to do this within the harvested budget. Prior works [ 34 ], [ 48 ], [ 68 ] suggest that selecting the right hyper-parameters (like batch size, learning rate, number of layers to train etc.) have a huge impact on the convergence and accuracy of the models.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "have a huge impact on the convergence and accuracy of the models. For each edge servers to handle multiple steams with multiple drifts, we need to jointly optimize the hyper-parameters for maximizing accuracy with minimum power and resource budget. To achieve this, we design a “micro-proﬁler” that can look into the drift of the models as well as the power availability and decide the right hyperparameters to train the models.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "To achieve this, we design a “micro-proﬁler” that can look into the drift of the models as well as the power availability and decide the right hyperparameters to train the models. Prior works [ 12 ], [ 38 ], [ 68 ] have designed hyperparameter micro-proﬁlers. However, they never considered an intermit- tent power source, nor explored jointly optimizing multiple models with power, accuracy and latency constraints.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "However, they never considered an intermit- tent power source, nor explored jointly optimizing multiple models with power, accuracy and latency constraints. Further- more, each model might contribute differently to the overall accuracy. Observing this, we propose a “weighted accuracy metric”, where the weight of each of the model is a function of the accuracy, time needed and power availability.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Observing this, we propose a “weighted accuracy metric”, where the weight of each of the model is a function of the accuracy, time needed and power availability. Furthermore, we allow some slack to the weighted accuracy so that the optimizer can choose a better set of hyperparameters if we can reach  close to  the weighted accuracy with much lower resource (power or compute) consumption. Typically, there is an inverse correlation of the convergence of the stochastic gradient descent (SGD) algorithm, the most popular training algorithm for DNNs, over the number of iterations ( n i ) [ 68 ]: l  ∝ O ( 1 / n i )  and  l  = 1 β 0 .",
      "type": "sliding_window",
      "tokens": 175
    },
    {
      "text": "Typically, there is an inverse correlation of the convergence of the stochastic gradient descent (SGD) algorithm, the most popular training algorithm for DNNs, over the number of iterations ( n i ) [ 68 ]: l  ∝ O ( 1 / n i )  and  l  = 1 β 0 . n i + β 1   +  β 2 , where  l  is the loss of the SGD and  β i  is an non-negative real number. Therefore, by running a few iterations of the SGD algorithms with various other hyperparameters, we can easily  predict  the con- vergence of the models.",
      "type": "sliding_window",
      "tokens": 156
    },
    {
      "text": "Therefore, by running a few iterations of the SGD algorithms with various other hyperparameters, we can easily  predict  the con- vergence of the models. Note that this needs to be done every time one of the constraints (accuracy, power etc.) changes.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "changes. The micro-proﬁler optimizes the weighted accuracy ( A w  = \nW i \nA i   / ∑ W i ; ∀ i  ≤ # models ; W i  =  f ( time , dri ft , compute )  with a user-deﬁned slack value of  δ ), with respect to available power ( P av ): max A w ;  s . t .",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "t . P  ≤ P av . Energy Buffering and Power-Predictor:  To regulate, manage and ensure a stable power supply to the circuitry,  Us.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "Energy Buffering and Power-Predictor:  To regulate, manage and ensure a stable power supply to the circuitry,  Us. ´as  uses a super-capacitor assisted voltage regulation circuit. To properly model the energy harvesting, losses during conversion, and leakage, we built a rectiﬁcation circuit with 4  ×  5 .",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "To properly model the energy harvesting, losses during conversion, and leakage, we built a rectiﬁcation circuit with 4  ×  5 . 5 V ,  2 . 2 F super-capacitors connected in parallel to a voltage regulator \ncircuit.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "2 F super-capacitors connected in parallel to a voltage regulator \ncircuit. The harvested power is given as an input to a mov- ing average power predictor [ 61 ], [ 72 ] to predict the future available power. Note that the power predictors used in prior works are meant for ﬁckle energy harvesting scenarios like piezoelectric (movement), or RF (WiFi).",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Note that the power predictors used in prior works are meant for ﬁckle energy harvesting scenarios like piezoelectric (movement), or RF (WiFi). We have adjusted the time window size. We took a history (years 2019 and 2020; from Seattle, WA; Sterling, VA; and Oak Ridge, TN) of solar energy traces from SOLRAD [ 25 ], [ 91 ] and built a weight matrix which looks into a window of 1 hour at 1 minute (average power) intervals to predict the power for next 10 minutes (1 minute granularity).",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "We took a history (years 2019 and 2020; from Seattle, WA; Sterling, VA; and Oak Ridge, TN) of solar energy traces from SOLRAD [ 25 ], [ 91 ] and built a weight matrix which looks into a window of 1 hour at 1 minute (average power) intervals to predict the power for next 10 minutes (1 minute granularity). We use regression to ﬁnd the weights (exponents and coefﬁcients) to the prediction curve followed by exponential smoothing to decay the weights. The rate of exponential smoothing depends on the scheduler used - while for the conservative scheduler the predictor always underestimated the power (shallow smoothing), the eager scheduling uses the direct output of the predictor (steeper smoothing).",
      "type": "sliding_window",
      "tokens": 170
    },
    {
      "text": "The rate of exponential smoothing depends on the scheduler used - while for the conservative scheduler the predictor always underestimated the power (shallow smoothing), the eager scheduling uses the direct output of the predictor (steeper smoothing). In either case, the predictor predicts the power with  ≈ 95% (peak of 98 . 72 (with real solar power trace) and minimum of 89 .",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "72 (with real solar power trace) and minimum of 89 . 14 (with synthetic power trace) accuracy. The micro-proﬁler, having run multiple sweeps, returns a set of hyper-parameters ( Ψ i ) for each model which is then stored in a history table.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "The micro-proﬁler, having run multiple sweeps, returns a set of hyper-parameters ( Ψ i ) for each model which is then stored in a history table. This helps us avoid unnecessary proﬁling (up to 41%). When introduced to a new set of constraints (change of power availability, accuracy etc.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "When introduced to a new set of constraints (change of power availability, accuracy etc. ), the micro-proﬁler ﬁrst looks in the history table to ﬁnd a conﬁguration and runs proﬁling if and only if it could not ﬁnd one. IV.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "IV. T HE  MORPHABLE H ARDWARE \nWhy Not Commercial GP-GPUs? DNN training is mas- sively parallel, fairly compute intensive, time consuming, and needs a lot of (albeit structured) data movements [ 16 ], [ 37 ].",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "DNN training is mas- sively parallel, fairly compute intensive, time consuming, and needs a lot of (albeit structured) data movements [ 16 ], [ 37 ]. Therefore, GP-GPUs have classically been used to train DNN models. However, as mentioned in § I , the commercial GPUs used for DNN training are typically power hungry ( typically in 100s of Watts TDP; We exprimented with multiple GPUs, server class A6000: 300W TDP, server class A100: 250W – 400W TDP, client class TRX3090: 350W TDP, and client class T4: 70W TDP), and are  not  equipped to handle intermittent power emergencies.",
      "type": "sliding_window",
      "tokens": 161
    },
    {
      "text": "However, as mentioned in § I , the commercial GPUs used for DNN training are typically power hungry ( typically in 100s of Watts TDP; We exprimented with multiple GPUs, server class A6000: 300W TDP, server class A100: 250W – 400W TDP, client class TRX3090: 350W TDP, and client class T4: 70W TDP), and are  not  equipped to handle intermittent power emergencies. However, these GPUs are often equipped with dynamic voltage and frequency scaling (DVFS). 2   To un- derstand the impact of DVFS on energy savings and dynamic compute scaling, we implemented a simple multi-arm bandit algorithm to select the right bucket of compute frequencies (SM frequency for NVIDIA GPUs), and memory frequencies to match the power-demands of the intermittent solar source.",
      "type": "sliding_window",
      "tokens": 185
    },
    {
      "text": "2   To un- derstand the impact of DVFS on energy savings and dynamic compute scaling, we implemented a simple multi-arm bandit algorithm to select the right bucket of compute frequencies (SM frequency for NVIDIA GPUs), and memory frequencies to match the power-demands of the intermittent solar source. As shown in Fig. 4  even with DVFS, commercial off the shelf GPUs could only ﬁnish  <  50% of the scheduled training task.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "4  even with DVFS, commercial off the shelf GPUs could only ﬁnish  <  50% of the scheduled training task. However, hardware is not the only limitation, as even with custom hardware [ 16 ] enabled with the state-of-the-art continuous learning algorithm [ 12 ] could only ﬁnish  ≈ 75% \n2 NVIDIA provides the list of supported clocks through the API “ nvidia--smi --q --d SUPPORTED_CLOCKS ”; We did not creport the results from A100 for this, as it does not offer multiple memory clocks, signiﬁcantly impacting its DVFS capabilities. T4, thanks to its limited compute capabilities, could not ﬁnish training tasks on time.",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "T4, thanks to its limited compute capabilities, could not ﬁnish training tasks on time. 896 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. of the scheduled training without any intermittency support.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "of the scheduled training without any intermittency support. It is clear that we  can neither  use the commercial GPUs  nor rely on the standard software and algorithmic approach for intermittent training purpose as they  cannot  ﬁnish the compute given the intermittent power budget. 0 0.2 0.4 0.6 0.8 \n1 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nA6000 w/DVFS RTX3090 w/DVFS Non-intermittent Custom HW w/Ekya \nOur Custom HW \nCompleted/Scheduled \nC/S Win-1 C/S Win-2 C/S Win-3 C/S Win-4 C/S Win-5 C/S mean \nFig.",
      "type": "sliding_window",
      "tokens": 197
    },
    {
      "text": "0 0.2 0.4 0.6 0.8 \n1 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nA6000 w/DVFS RTX3090 w/DVFS Non-intermittent Custom HW w/Ekya \nOur Custom HW \nCompleted/Scheduled \nC/S Win-1 C/S Win-2 C/S Win-3 C/S Win-4 C/S Win-5 C/S mean \nFig. 4: Impact of DVFS on completion (average power budget 70W). Note that, even with DVFS, most scheduled compute could not be ﬁnished.",
      "type": "sliding_window",
      "tokens": 179
    },
    {
      "text": "Note that, even with DVFS, most scheduled compute could not be ﬁnished. This includes the intermittent failures ( ≤ 20W where no compute could be done); we included check- pointing to ensure that progress is saved in power-failures. C/S is the ratio of  C ompleted over the  S cheduled training tasks over multiple time windows of 4hours.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "C/S is the ratio of  C ompleted over the  S cheduled training tasks over multiple time windows of 4hours. Our custom HW runs with intermittent support both by hardware and software. There have also been signiﬁcant efforts in designing and optimizing specialized DNN training accelerators [ 16 ], [ 27 ], [ 81 ], and many commercial organizations have already devel- oped their own accelerators [ 37 ], [ 95 ] as well.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "There have also been signiﬁcant efforts in designing and optimizing specialized DNN training accelerators [ 16 ], [ 27 ], [ 81 ], and many commercial organizations have already devel- oped their own accelerators [ 37 ], [ 95 ] as well. Considering the compute mapping of the DNN training, almost all of these de- signs are based on a “systolic architecture”, performing chains of multiplication and accumulations (MACs). However, these devices take a “throughput-ﬁrst” approach, to minimize the time consumption and seldom optimize power consumption ﬁrst.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "However, these devices take a “throughput-ﬁrst” approach, to minimize the time consumption and seldom optimize power consumption ﬁrst. This has lead to a global concern of the energy and consequently carbon-footprint of the DNN training [ 21 ], [ 57 ], [ 67 ], [ 89 ]. Furthermore, these accelerators have been designed to operate under constantly available power.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "Furthermore, these accelerators have been designed to operate under constantly available power. Although our pro- posed representation learning (§ III ) and micro-proﬁler (§ III-C ) help us ﬁnd a better training conﬁguration that can minimize the compute if deployed in the aforementioned accelerators, it does not solve sustainability: That is, with variable solar power, can we scale compute alongside power to continue to make “forward progress”, even when minimum amount of power is available. The systolic array structure of the DNN accelerators is well suited for this as we can change the com- pute size, as well as the number of memory channels feeding to those compute units as per the power availability.",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "The systolic array structure of the DNN accelerators is well suited for this as we can change the com- pute size, as well as the number of memory channels feeding to those compute units as per the power availability. However, we need to be innovative in terms of designing and placing the compute hierarchy to ensure minimum data movement and re-computations when compute scaling. The hardware design of  Us.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "The hardware design of  Us. ´as  (Fig. 5a ) incorporates all the aforementioned points.",
      "type": "sliding_window",
      "tokens": 27
    },
    {
      "text": "5a ) incorporates all the aforementioned points. Note that,  Us. ´as  introduces a design philosophy for building a morphable hardware, and it can easily be adapted by any of the systolic array based commercial off the shelf (or research prototype) DNN training accelerators.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "´as  introduces a design philosophy for building a morphable hardware, and it can easily be adapted by any of the systolic array based commercial off the shelf (or research prototype) DNN training accelerators. DNN Compute Mapping:  Typically there are three ways of mapping DNN compute into a systolic array, namely, 1. output \nDRAM \nRepresntation Learning \nMicro-profiler \nActivation \nNormalize & Pooling \nHost \nPower Predictor  Examplar \nGlobal Scratch-pad \nDouble Buffered IF Map \nDouble Buffered Filter \nSTT-RAM S \nSTT-RAM N \nNVSB-SE \nNVSB-NW \nDouble Buffered \nOF Map \nP-SUM \nW \nArbiter \n2xNVSB \nP- MUX \nTile-Q \nPE-Block \nSouth Control \nMaster \nNorth \nControl \n(a) High-level arch \no-Path De-Mux \n4x4 STiles w/ 8x8 SA each 128 cycles \n(64 x16)x 16bits Results \nPower off Warning from \nPredictor \nNext \nTile \nTo arbiter \n& Fabric \nclk \nw-pdown \nstate on pwr warning (>256 cycles) off \nbackup \ndat Compute \npath tile arbiter & fabric \nfab-state \n!pdown \n4kB NVSB \nNext STile \n128 Cycles \nFilter \ni-Path mux \nFrom IF or NVSB of failed PE  \ni-Path mux \nPrev STile Next STile \nWork-Q \nLogic \nOutput \nFilter \n8x8 Priority \nMux \n512B NVSB if FULL \nWr-Q \n(b) Power-down/ Failure handling \nFig. 5: Overall architecture with the components and the power failure handle sequence of  Us.",
      "type": "sliding_window",
      "tokens": 403
    },
    {
      "text": "5: Overall architecture with the components and the power failure handle sequence of  Us. ´as . stationary; 2. input stationary; and 3. weight stationary [ 79 ].",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "stationary; 2. input stationary; and 3. weight stationary [ 79 ]. Most large-scale accelerators use the output stationary imple- mentations to minimize the output feature map movement [ 81 ], and some of available hardware even supports multiple types of mappings [ 15 ], [ 37 ]. However, our design objective is to minimize  data movements in the case of compute reconﬁgura- tion.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "However, our design objective is to minimize  data movements in the case of compute reconﬁgura- tion. In an output stationary mapping,  both  input and weights are dynamic and any power failure or reconﬁguration will need to save and restore a lot of current context (partial sums, indices of weights and inputs etc.) to resume and remap the compute.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "to resume and remap the compute. This problem reduces in both input stationary and weight stationary, but at the cost of throughput [ 80 ]. Typically, the input feature maps are larger than the (individual) weights, and more importantly large weights can easily be represented \n897 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "Typically, the input feature maps are larger than the (individual) weights, and more importantly large weights can easily be represented \n897 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "6: Weight stationary compute mapping. The PE-level shows how the input ﬂows and the convolutions are computed with a 3x3 convolution toy example. The tile-level shows how each tile consists of multiple such PEs and will be working on one kernel at a time.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "The tile-level shows how each tile consists of multiple such PEs and will be working on one kernel at a time. The accelerator-level shows that the entire accelerator is made of multiple such tiles (4x4 in the toy example). Inputs are broadcast into each tile so that each tile can work on a kernel.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "Inputs are broadcast into each tile so that each tile can work on a kernel. Computation is  redistributed  when there is a change in the power availability, and multiple tiles are shutdown (redacted) without impacting the data ﬂow. or decomposed as multiple units called “kernels” (or “ﬁlters”).",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "or decomposed as multiple units called “kernels” (or “ﬁlters”). In a typical convolutional neural network (CNN), each kernel is convoluted over the entire input feature map, and hence there is an “inter-kernel parallelism” (all kernels of a single layer can be executed in parallel) and “intra-kernel parallelism” (multiple computes in a convolution can happen in parallel). This property is true both for the forward pass and the backward pass of the standard CNN training.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "This property is true both for the forward pass and the backward pass of the standard CNN training. The modular nature of the weight stationary mapping makes it a strong candidate for use in a re-conﬁgurable or morphable systolic structure as turning off some compute is the same as not computing a kernel and scheduling it for later. Therefore,  Us.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "Therefore,  Us. ´as employs a weight stationary compute mapping for executing the training tasks on the morphable hardware. A.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "A. Design Description of the DNN Hardware Augmentations \nCompute Mapping:  Fig. 5a  shows the high level design, architecture and different components present in our proposed accelerator.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "5a  shows the high level design, architecture and different components present in our proposed accelerator. The accelerator encompasses 256 tiles, structured in a 4 × 4 conﬁguration of 16 super-tiles, each harboring 4 × 4 tiles. These super-tiles Each tile, individually switchable ON or OFF based on power availability, houses 64 16-bit ﬂoating point MAC units conﬁgured in an 8  ×  8 systolic array for convolution operations.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "These super-tiles Each tile, individually switchable ON or OFF based on power availability, houses 64 16-bit ﬂoating point MAC units conﬁgured in an 8  ×  8 systolic array for convolution operations. A modular computational approach is adopted where each tile is accountable for one CNN kernel, necessitating  ⌈ [ C  × H  × W ] / 64 ⌉ iterations for a kernel of size [ C  × H  × W ] . Data streaming and partial compute storage are facilitated by four double buffered SRAM structures, with the \nweights residing in a double buffered multi-banked SRAM.",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "Data streaming and partial compute storage are facilitated by four double buffered SRAM structures, with the \nweights residing in a double buffered multi-banked SRAM. The ﬁlter SRAM has 256 banks (one per tile), each with a size of 1kB (double buffered, 512B per buffer per bank). Input data broadcast to all tiles is managed by a 64kB double- buffered input feature map SRAM (32kB each), requiring ⌈ [ X  × Y  × Z ] / 1024 ⌉ iterations for full input loading, with each buffer loaded  [ X  × Y  ×  Z ] / 2048 times.",
      "type": "sliding_window",
      "tokens": 160
    },
    {
      "text": "Input data broadcast to all tiles is managed by a 64kB double- buffered input feature map SRAM (32kB each), requiring ⌈ [ X  × Y  × Z ] / 1024 ⌉ iterations for full input loading, with each buffer loaded  [ X  × Y  ×  Z ] / 2048 times. The convolution map transforms  [ X  × Y  ×  Z ]   M × C × W × H −−−−−−−→ [ M  × U  × V ]  to yield an output tensor of dimensions  [ M  ×  U  ×  V ] , supported by a 256 banked double buffered output feature map SRAM, each bank of size 8kB (4kB/buffer). A 128kB SRAM serves as a scratchpad for storing activations, transposes, and intermediate differentials during the backward pass.",
      "type": "sliding_window",
      "tokens": 213
    },
    {
      "text": "A 128kB SRAM serves as a scratchpad for storing activations, transposes, and intermediate differentials during the backward pass. The accelerator also houses 256  ×  256 compactor-mux combinational logic units (256 units per tile) for ReLU activation (forward pass) and inverse activation (backward pass). For smaller DNNs without 256 kernels in any layer, a batching mode is operational with a batch size of  B  =  ⌊ A / L ⌋ images, where  L  denotes the layer with the fewest channels, and  A  the number of active tiles.",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "For smaller DNNs without 256 kernels in any layer, a batching mode is operational with a batch size of  B  =  ⌊ A / L ⌋ images, where  L  denotes the layer with the fewest channels, and  A  the number of active tiles. This generic design is adaptable for various workloads. In DNN training, meticulous compute mapping, mem- ory access strategies, and operational formulas are instru- mental for the forward and backward passes.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "In DNN training, meticulous compute mapping, mem- ory access strategies, and operational formulas are instru- mental for the forward and backward passes. The for- ward pass computes activations via the formula  A muv  = ∑ C − 1 \nc = 0   ∑ H − 1 \ni = 0   ∑ W − 1 \nj = 0   X ( u + i )( v +  j ) c   · K mijc , with results stored in the double-buffered output feature map SRAM. The backward pass emphasizes gradient computation through backpropaga- tion, which is crucial for weight updates.",
      "type": "sliding_window",
      "tokens": 152
    },
    {
      "text": "The backward pass emphasizes gradient computation through backpropaga- tion, which is crucial for weight updates. The gradient of the loss function concerning the weights is computed through the formula ∂ L ∂ K mi jc   =  ∑ U − 1 \nu = 0   ∑ V − 1 \nv = 0 \n∂ L ∂ A muv   ·  X ( u + i )( v +  j ) c . This gra- dient computation, fundamental for learning, is meticulously mapped across the systolic array, ensuring precise and efﬁcient backpropagation.",
      "type": "sliding_window",
      "tokens": 141
    },
    {
      "text": "This gra- dient computation, fundamental for learning, is meticulously mapped across the systolic array, ensuring precise and efﬁcient backpropagation. Memory accesses are optimally managed via the double-buffered SRAM structures, providing timely data availability for the MAC units. The 8 × 8 systolic array in each tile executes multiply-accumulate operations in a pipelined and parallel fashion, abiding by the Weight Stationary approach, thereby optimizing the throughput and efﬁciency of the train- ing operations within this hardware architecture.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "The 8 × 8 systolic array in each tile executes multiply-accumulate operations in a pipelined and parallel fashion, abiding by the Weight Stationary approach, thereby optimizing the throughput and efﬁciency of the train- ing operations within this hardware architecture. Power Control Logic:  Power emergency prediction in  Us. ´as is always conservative, and the solar power predictor has a mean accuracy of 92%, limiting false positives and helping the control unit select appropriate tile counts.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "´as is always conservative, and the solar power predictor has a mean accuracy of 92%, limiting false positives and helping the control unit select appropriate tile counts. The system needs at least 512 cycles of advanced notice to ﬂush compute and enable a compute migration. Fig.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Fig. 5a  shows the block diagram of the mesh interconnect, and Fig. 5b  shows the power-down sequence and signal states.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "5b  shows the power-down sequence and signal states. The network works at a super-tile (STile) granularity and each arbiter node uses an 8x8 priority- mux. The network only gets activated when it gets a  w-pdown warning signal from the predictor.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "The network only gets activated when it gets a  w-pdown warning signal from the predictor. This signal starts a graceful power-down sequence for the required number of tiles. The w-pdown  triggers the  backup  signal and the system goes into pwr-warning  state (other states being on, off, invalid and X).",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "The w-pdown  triggers the  backup  signal and the system goes into pwr-warning  state (other states being on, off, invalid and X). In the  pwr-warning  phase the system ﬁnishes the remaining compute of the systolic arrays (which can take up to 64 cycles), \n898 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. and starts ﬂushing the results for backup.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "and starts ﬂushing the results for backup. Buffer Management:  Us. ´as  uses non-volatile state buffers (NVSBs, 18 count, 1 per 4x4 tiles, each of 1kB, and 2 of 4kB each) for state saving and data backup.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "´as  uses non-volatile state buffers (NVSBs, 18 count, 1 per 4x4 tiles, each of 1kB, and 2 of 4kB each) for state saving and data backup. The control logic prioritizes writing data into the local NVSB for the arbiter (each arbiter caters to 2 STiles). If those get full because of continuous power failures, the control directs the data to the global NVSBS (NVSB-NW and NVSB-SE in Fig.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "If those get full because of continuous power failures, the control directs the data to the global NVSBS (NVSB-NW and NVSB-SE in Fig. 5a ). The NVSB stores the global/asynchronous work queue and the shufﬂing conﬁguration (mini-batch arrangement).",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "The NVSB stores the global/asynchronous work queue and the shufﬂing conﬁguration (mini-batch arrangement). B. Power Failure and Compute Scheduling \nCentral to the  Us.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "Power Failure and Compute Scheduling \nCentral to the  Us. ´as  accelerator’s operational efﬁciency is the work queue—an intricately designed, hierarchical structure that meticulously catalogues pending computational tasks. Each task, represented in the queue, corresponds to the execu- tion of speciﬁc CNN kernels, feature tiles and operations.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "Each task, represented in the queue, corresponds to the execu- tion of speciﬁc CNN kernels, feature tiles and operations. As deep neural network models often have a complex interplay of layers, each with distinct computational needs, the work queue ensures a systematic, prioritized approach to handle these oper- ations. Two distinct scheduling strategies, each complemented by its own type of work queue, govern the computational ﬂow: Conservative Scheduling and Eager Scheduling.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Two distinct scheduling strategies, each complemented by its own type of work queue, govern the computational ﬂow: Conservative Scheduling and Eager Scheduling. The dual-scheduling mechanism, bolstered by the work queue’s ﬂexible architecture, not only optimizes compute performance but also offers resilience against power uncertainties. The work queue schedule, intermediate result, network and layer information are saved on predicted power failure, and data from the DRAM (the working set of IF/OF/ﬁlter and model state) are moved to an NV-RAM using STT-RAM based buffers in the memory hierarchy (parallel to the IF/OF/ﬁlter).",
      "type": "sliding_window",
      "tokens": 145
    },
    {
      "text": "The work queue schedule, intermediate result, network and layer information are saved on predicted power failure, and data from the DRAM (the working set of IF/OF/ﬁlter and model state) are moved to an NV-RAM using STT-RAM based buffers in the memory hierarchy (parallel to the IF/OF/ﬁlter). We do  not  replace the DRAM buffers with NVM because of limited lifetimes [ 17 ]. The host writes the latest copy of the completed iteration (in epoch granularity) into the STT- RAMs (STT-RAM-N for the upper 128 SAs, and STT-RAM- S for the lower 128SAs, Fig.",
      "type": "sliding_window",
      "tokens": 161
    },
    {
      "text": "The host writes the latest copy of the completed iteration (in epoch granularity) into the STT- RAMs (STT-RAM-N for the upper 128 SAs, and STT-RAM- S for the lower 128SAs, Fig. 5a ). In case of a complete power failure, the compute in ﬂight are rejected and, once the system starts working, the work queues get invalidated and the host starts the compute again from the last checkpoint.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "In case of a complete power failure, the compute in ﬂight are rejected and, once the system starts working, the work queues get invalidated and the host starts the compute again from the last checkpoint. Along with that, the most common intermittent software libraries and software designs [ 26 ], [ 52 ] (and most DNN training libraries like PyTorch, TensorFlow) also offer periodic checkpoints. Note that the power-up sequence for a tile runs in the exact opposite order of the  powerdown  sequence (a tile becomes computationally active 512 cycles after it gets the power up signal).",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "Note that the power-up sequence for a tile runs in the exact opposite order of the  powerdown  sequence (a tile becomes computationally active 512 cycles after it gets the power up signal). Us. ´as  uses two kinds of scheduling policies to handle the graceful  powerdown  and work queue rearrangement.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "´as  uses two kinds of scheduling policies to handle the graceful  powerdown  and work queue rearrangement. Conservative Scheduling:  The most important part of the Us. ´as  accelerator design is to ensure proper “compute place- ment” even under a power emergency or power scaling.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "´as  accelerator design is to ensure proper “compute place- ment” even under a power emergency or power scaling. Fig. 6 : Accelerator level  provides a high-level overview of the compute scheduling (where the redacted part of the hard- ware is turned off because of the lack of power).",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "6 : Accelerator level  provides a high-level overview of the compute scheduling (where the redacted part of the hard- ware is turned off because of the lack of power). The key components of the scheduler are the “moving average power predictor” and the “micro-proﬁler”. In the  i th   kernel scheduling iteration, given the power budget and power prediction, the \nmicro-proﬁler decides the required training conﬁguration, and the control logic (conservatively) enables suitable number of tiles (say  t i  tiles of the 256 tiles).",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "In the  i th   kernel scheduling iteration, given the power budget and power prediction, the \nmicro-proﬁler decides the required training conﬁguration, and the control logic (conservatively) enables suitable number of tiles (say  t i  tiles of the 256 tiles). Those  t i  tiles fetch  t i unique kernels from the 1Byte wide, 256 deep global kernel dispatch queue (GKDQ,  t i  kernels scheduled in parallel ). Note that the power requirement of each tile is known in advance (please refer to § V , TABLE  I  for details).",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "Note that the power requirement of each tile is known in advance (please refer to § V , TABLE  I  for details). Once the scheduled ( t i ) tiles are completed, the micro-proﬁler again ﬁnds the right conﬁguration for the  i  +  1 th   iteration and the scheduler again conservatively enables  t i + 1  number of tiles suitable for the power budget. The  t i + 1  tiles fetch the next  t i + 1  kernels from the GKDQ and the process continues.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "The  t i + 1  tiles fetch the next  t i + 1  kernels from the GKDQ and the process continues. This conservative compute and power estimation ensures that none of the kernel computes (the lowest decomposed level of compute unit for the hardware) ever fails and hence there is no need for any partial data movement. The GKDQ always points to the next available kernel location.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "The GKDQ always points to the next available kernel location. The control fetches the right number of kernels and all of them are synchronously executed in the active tiles. Eager Scheduling:  A weight stationary implementation with a conservative scheduling will always run synchronously.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Eager Scheduling:  A weight stationary implementation with a conservative scheduling will always run synchronously. However, in the middle of an kernel execution iteration, if the hardware gains access to more power which in turn can enable more tiles, it cannot do so without breaking synchrony (i.e. when some of the tiles are half way through the compute, some other tiles can just start execution).",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "when some of the tiles are half way through the compute, some other tiles can just start execution). Facilitating such schedul- ing will provide us less idle time, more forward progress and more efﬁcient use of the incoming power but at the expense of more control overheads. We call this  Eager Scheduling .",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "We call this  Eager Scheduling . To enable eager scheduling, we decentralized the global kernel dispatch queue and equipped each tile with a local kernel dispatch queue (1Byte wide 16 deep). At the beginning of each kernel scheduling iteration, the micro-proﬁler decides the right conﬁguration, and the control distributes equal number of kernels to each active tile (given  A  active kernel, and  K total kernels, each tile gets  ⌊ K / A ⌋ kernels to execute).",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "At the beginning of each kernel scheduling iteration, the micro-proﬁler decides the right conﬁguration, and the control distributes equal number of kernels to each active tile (given  A  active kernel, and  K total kernels, each tile gets  ⌊ K / A ⌋ kernels to execute). The conservative scheduler ensures that no tile loses power before ﬁnishing the current scheduled kernel. However, in the middle of the execution if any new tiles becomes alive (because of an increase in harvested power), the scheduler immediately marks it ready to start working and the tile fetches a kernel (currently not scheduled in any of the tiles) and starts working on it.",
      "type": "sliding_window",
      "tokens": 145
    },
    {
      "text": "However, in the middle of the execution if any new tiles becomes alive (because of an increase in harvested power), the scheduler immediately marks it ready to start working and the tile fetches a kernel (currently not scheduled in any of the tiles) and starts working on it. We face three issues here: 1. How does the new tile get any kernel to work on?",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "How does the new tile get any kernel to work on? 2. Over multiple iterations of such asynchronous scheduling, the kernel queue for each tile will be of different size creating a load imbalance; how to tackle this?",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Over multiple iterations of such asynchronous scheduling, the kernel queue for each tile will be of different size creating a load imbalance; how to tackle this? 3. How do we know when to stop executing?",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "How do we know when to stop executing? To address the ﬁrst two issues, we developed a work-stealing mechanism for each tile. When any of the active tiles are marked ready by the scheduler, the tile employees a state machine to decide where to get work from.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "When any of the active tiles are marked ready by the scheduler, the tile employees a state machine to decide where to get work from. Each time the tile ﬁnishes some work, if its remaining work queue (the local work queue size) is less than the average of all other active tiles, it seeks a new kernel to work on. Considering the global control always enqueues any idle tile with work, whenever the tile has no work left, it steals a kernel from the most \n899 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Considering the global control always enqueues any idle tile with work, whenever the tile has no work left, it steals a kernel from the most \n899 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Restrictions apply. heavily loaded tiles. We implemented a counter (local kernel counter) to keep track of the size of the remaining local work queue of each of the tile.",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "We implemented a counter (local kernel counter) to keep track of the size of the remaining local work queue of each of the tile. We also implemented a counter (layer kernel counter) which keeps track of the total kernels to be scheduled for each layer. Whenever all the local work queue counter hits zero along with the layer kernel counter, the control moves to schedule the next layer (or previous layer in backward propagation) for computation.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Whenever all the local work queue counter hits zero along with the layer kernel counter, the control moves to schedule the next layer (or previous layer in backward propagation) for computation. Note that we do not delve into the details of the computa- tional primitives involved in training the DNN as several prior works [ 15 ], [ 16 ], [ 80 ] provide a very detailed accounting of it (for both forward and backward pass) along with the hardware and control requirements. We treat the convolution scheduling (using input stationary and at a kernel level) in a morphable systolic hardware to be the main challenge and explain it.",
      "type": "sliding_window",
      "tokens": 147
    },
    {
      "text": "We treat the convolution scheduling (using input stationary and at a kernel level) in a morphable systolic hardware to be the main challenge and explain it. V. I MPLEMENTATION AND  E VALUATION \nWe focus our evaluation on urban mobility, i.e. performing single shot  object detection for trafﬁc monitoring using the MobileNetV2  [ 51 ] model on the urban trafﬁc data set [ 97 ].",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "performing single shot  object detection for trafﬁc monitoring using the MobileNetV2  [ 51 ] model on the urban trafﬁc data set [ 97 ]. This is a trafﬁc video dataset containing 62GB of videos recorded from ﬁve pole-mounted ﬁsh-eye cameras in the city of Bellevue, WA, USA. Each video stream is recorded with a resolution of 1280  ×  720 at 30fps.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Each video stream is recorded with a resolution of 1280  ×  720 at 30fps. This contains a total of 101 hour of video across all cameras of which 30 hours of video is used to ﬁne-tune the teacher models and the rest 71 hours of data is used to evaluate our continuous learning so- lution. For annotating the incoming video stream we use three teachers models, namely, ResNet101 [ 32 ], YOLOV2 [ 22 ], and VGG16 [ 87 ].",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "For annotating the incoming video stream we use three teachers models, namely, ResNet101 [ 32 ], YOLOV2 [ 22 ], and VGG16 [ 87 ]. For sustainability, we use solar power to perform our compute. Since our dataset is from Bellevue, WA, we took the SOLRAD solar radiation data [ 25 ] (managed and published by National Oceanic and Atmospheric Administra- tion, NOAA) of Seattle, WA (the SOLRAD center closest to Bellevue and hence we believe is a good approximation).",
      "type": "sliding_window",
      "tokens": 136
    },
    {
      "text": "Since our dataset is from Bellevue, WA, we took the SOLRAD solar radiation data [ 25 ] (managed and published by National Oceanic and Atmospheric Administra- tion, NOAA) of Seattle, WA (the SOLRAD center closest to Bellevue and hence we believe is a good approximation). In our experiments we assume the hardware to be powered by a solar panel of one square-meter, and the powers are scaled accordingly (data is available as  W / m 2 ). Finally, we assume the exact same setup of the Urban trafﬁc dataset and hence have 5 different MobileNetV2 models trying to classify the trafﬁc they are facing, and learning from the streaming data.",
      "type": "sliding_window",
      "tokens": 162
    },
    {
      "text": "Finally, we assume the exact same setup of the Urban trafﬁc dataset and hence have 5 different MobileNetV2 models trying to classify the trafﬁc they are facing, and learning from the streaming data. We vary the training intervals to see the effect of frequency of retraining. Existing Approaches:  Although there has been signiﬁcant re- search [ 40 ], [ 41 ], [ 47 ], [ 52 ], [ 56 ], [ 61 ], [ 72 ], [ 104 ] on enabling machine learning in intermittently powered devices, a major- ity of it focuses on performing inference.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "Existing Approaches:  Although there has been signiﬁcant re- search [ 40 ], [ 41 ], [ 47 ], [ 52 ], [ 56 ], [ 61 ], [ 72 ], [ 104 ] on enabling machine learning in intermittently powered devices, a major- ity of it focuses on performing inference. Only intermittent learning [ 47 ] focuses on performing on-device training, but with very small workloads and models. Considering the scale, scope and workload of our problem, limits direct comparisons, except for comparing their exemplar selection method (refer Fig.",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "Considering the scale, scope and workload of our problem, limits direct comparisons, except for comparing their exemplar selection method (refer Fig. 9 ). Similarly, Ekya [ 12 ] only focuses on co-location of computation, and it’s efﬁciency on ﬁnishing compute even on custom hardware is shown in Fig.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "Similarly, Ekya [ 12 ] only focuses on co-location of computation, and it’s efﬁciency on ﬁnishing compute even on custom hardware is shown in Fig. 4 . 0 25 50 75 100 \nMN-BL \nTeacher \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-BL \nTeacher \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nNaïve w/Exemplar \nAccuracy in % \nHour-0 Hour-2 Hour-4 Hour-6 Hour-8 \nFig.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "0 25 50 75 100 \nMN-BL \nTeacher \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-BL \nTeacher \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nNaïve w/Exemplar \nAccuracy in % \nHour-0 Hour-2 Hour-4 Hour-6 Hour-8 \nFig. 7: Accuracy boost due to proper exemplar selection over 8 hours of time window. Labels: MN – MobileNet-V2, BL – Baseline, Teacher – the ensemble of teacher models, MN–#: targeted MobileNet-V2 model for the particular time of day.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "Labels: MN – MobileNet-V2, BL – Baseline, Teacher – the ensemble of teacher models, MN–#: targeted MobileNet-V2 model for the particular time of day. A. Continuous Learning: Accuracy \nFig.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "Continuous Learning: Accuracy \nFig. 7  shows the accuracy improvement over a time window of 8 hours by using the continuous learning algorithm. We compare against a baseline using na¨ıve continuous learning algorithm with no representation learning.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "We compare against a baseline using na¨ıve continuous learning algorithm with no representation learning. In contrast,  Us. ´as uses a 2 level exemplar selection algorithm (one using the conﬁdence matrix, and then further reﬁned by the representa- tion learning).",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "´as uses a 2 level exemplar selection algorithm (one using the conﬁdence matrix, and then further reﬁned by the representa- tion learning). We observe that, with representation learning, Us. ´as  is  ≈ 4 .",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "´as  is  ≈ 4 . 94% (maximum  ≈ 8 . 03%, and minimum  ≈ 2 .",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "03%, and minimum  ≈ 2 . 62%) more accurate than the na¨ıve learner. Further,  Us.",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "Further,  Us. ´as  converges closer to the accuracy of the teacher model. This was possible by restricting the training space and by using the superior exemplar set construction by using representation learning.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "This was possible by restricting the training space and by using the superior exemplar set construction by using representation learning. Fig. 8  shows the impact of micro-proﬁling on the hyper parameter selection.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "8  shows the impact of micro-proﬁling on the hyper parameter selection. Due to the drift- and weighted accuracy- aware micro-proﬁler, the suggested conﬁguration is almost every time the same as an oracular selection. Fig.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "Fig. 8a  shows the number of layers trained for a DNN, in contrast to the ideal number of layers to achieve maximum accuracy. Over 10 training iterations, we observed the micro-proﬁler to be con- sistent with the oracle (except for one case of iteration 7).",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Over 10 training iterations, we observed the micro-proﬁler to be con- sistent with the oracle (except for one case of iteration 7). Note that the hyperparameter selected in iteration 7 by the micro- proﬁler performs as good as the the oracle model in terms of achieving accuracy, albeit by performing more computation. A deep dive into 7 th   iteration reveals that the micro-proﬁler chose a higher learning rate (compared to the oracle), which biased the convergence curve ﬁtting and extrapolation (as discussed in § III-C ) and hence suggested a larger number of layers to be trained to achieve the required convergence.",
      "type": "sliding_window",
      "tokens": 153
    },
    {
      "text": "A deep dive into 7 th   iteration reveals that the micro-proﬁler chose a higher learning rate (compared to the oracle), which biased the convergence curve ﬁtting and extrapolation (as discussed in § III-C ) and hence suggested a larger number of layers to be trained to achieve the required convergence. Similarly, the micro-proﬁler shows consistent behaviour while choosing the right number of batches. Fig.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "Fig. 8b  also shows the error rate of retraining performed by choosing the hyperparameters given by the micro-proﬁler vs an oracle selection. Observation over 40 hours of continuous learning on the dataset suggest that the micro-proﬁler has, on average, an accuracy deviation of 2 .",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Observation over 40 hours of continuous learning on the dataset suggest that the micro-proﬁler has, on average, an accuracy deviation of 2 . 46%, compared to an oracle parameter selection. Along with that, the micro-proﬁler selects correct batch size 82 .",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Along with that, the micro-proﬁler selects correct batch size 82 . 64% of the time and the correct number of layers for 87 . 06% of the time.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "06% of the time. B. Impact on Exemplar Selection \nUs.",
      "type": "sliding_window",
      "tokens": 15
    },
    {
      "text": "Impact on Exemplar Selection \nUs. ´as  beneﬁts from the use of  multiple  teacher models for data annotation and exemplar selection. Prior works on intermittent learning have either chosen one teacher model \n900 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "Prior works on intermittent learning have either chosen one teacher model \n900 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "Restrictions apply. 0 \n20 \n40 \n60 \n1 2 3 4 5 6 7 8 9 10 \n# Layers Tranined \nTraining Iterations \n# Layers Trained-Actual # Layers Trained-Oracle \n(a) Number of layers trained. 0 \n2 \n4 \n6 \n8 \n0 \n10 \n20 \n30 \n40 \n1 2 3 4 5 6 7 8 9 10 \nConvergence Error(%) \nBatch Size \nTranining Iterations Batch Size-Actual Batch Size-Oracle Convergence Error (%)-Actual Convergence Error (%)-Oracle \n(b) Batch-size and convergence.",
      "type": "sliding_window",
      "tokens": 142
    },
    {
      "text": "0 \n2 \n4 \n6 \n8 \n0 \n10 \n20 \n30 \n40 \n1 2 3 4 5 6 7 8 9 10 \nConvergence Error(%) \nBatch Size \nTranining Iterations Batch Size-Actual Batch Size-Oracle Convergence Error (%)-Actual Convergence Error (%)-Oracle \n(b) Batch-size and convergence. 0 \n5 \n10 \n15 \n0% \n20% \n40% \n60% \n80% \n100% \n1 2 3 4 5 6 7 8 9 10 \n# Exemplar/100 Frames \nExemplar vs Traning  \nTime \nTranining Iterations Exemplar Selection Tranining #Exemplar/100 frames \n(c) Exemplar selection w.r.t. training.",
      "type": "sliding_window",
      "tokens": 159
    },
    {
      "text": "8: Algorithmic performance of  Us. ´as : the beniﬁts of the exemplar selection and  μ -proﬁler. Component Spec Power Area(mm 2 ) SRAM Buffers 1kB*256+8kB*256+64kB+16*256kB 10.372W 117.164 MAC Unit (8*8)*256 8.46W 32.72 Adder Tree and Comparator 16*16bit + 256 2.4W 21.556 Control – 0.96W 12.2 Host ∼ Cortex A78 series 11W – Design at 592MHz with Synopsys AED 32nm library Total 256 tiles 33.192W 183.64 \nTABLE I: Area and power estimation of our design.",
      "type": "sliding_window",
      "tokens": 175
    },
    {
      "text": "Component Spec Power Area(mm 2 ) SRAM Buffers 1kB*256+8kB*256+64kB+16*256kB 10.372W 117.164 MAC Unit (8*8)*256 8.46W 32.72 Adder Tree and Comparator 16*16bit + 256 2.4W 21.556 Control – 0.96W 12.2 Host ∼ Cortex A78 series 11W – Design at 592MHz with Synopsys AED 32nm library Total 256 tiles 33.192W 183.64 \nTABLE I: Area and power estimation of our design. (e.g. Ekya [ 12 ] using ResNeXt101) to annotate the data or used a heuristic on top of the teacher model (e.g.",
      "type": "sliding_window",
      "tokens": 186
    },
    {
      "text": "Ekya [ 12 ] using ResNeXt101) to annotate the data or used a heuristic on top of the teacher model (e.g. intermittent learning [ 47 ] using randomized selection, K–Last List, or round-robin policy). As shown in Fig.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "As shown in Fig. 9 , a single teacher, even with the augmented heuristics, typically fails to select the right exemplar set. The exemplar set signiﬁcantly impacts the accuracy in two ways: 1. missing valid exemplars will result in the student model missing out in learning vital information, increasing its drift, and 2. a wrong annotation by the teacher can also result in the student learning wrong labels, resulting in increased mis-predictions.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "The exemplar set signiﬁcantly impacts the accuracy in two ways: 1. missing valid exemplars will result in the student model missing out in learning vital information, increasing its drift, and 2. a wrong annotation by the teacher can also result in the student learning wrong labels, resulting in increased mis-predictions. To avoid this, in  Us. ´as , the teacher models perform majority voting to decide the right exemplar, which signiﬁcantly reduces false positives and true negatives (refer to the top bar in Fig.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "´as , the teacher models perform majority voting to decide the right exemplar, which signiﬁcantly reduces false positives and true negatives (refer to the top bar in Fig. 9 : with the ensemble, the best case annotation is the ideal one with only 2 false positives). Furthermore, the feature extraction for each of the potential \nexemplars for the teacher model is hardware-assisted (§ V-C ), and hence poses no overhead to the inference task.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Furthermore, the feature extraction for each of the potential \nexemplars for the teacher model is hardware-assisted (§ V-C ), and hence poses no overhead to the inference task. 0 \n9 \n18 \n27 \nR Y V R+Y R+V Y+V R+Y+V IL-RR IL-K-Last List \n# Exemplars \n#Exemplars Selected (Best Case) #Exemplars Selected (Worst Case) Average Exemplars \nFig. 9: Impact of multiple teachers on exemplar selection.",
      "type": "sliding_window",
      "tokens": 128
    },
    {
      "text": "9: Impact of multiple teachers on exemplar selection. X- axis shows #exemplars/100 inferred frames over a 2hr window. Having an ensemble provides robust exemplar selection and improves accuracy over a single teacher.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Having an ensemble provides robust exemplar selection and improves accuracy over a single teacher. The X-Axis has different DNNs , R: ResNeXt101, T: YOLO-V3, V: VGG- 16, IL: Intermittent Learning, RR: Round Robin. C. Hardware Implementation and Evaluation \nThe proposed morphable hardware was simulated using an in-house simulator based on ScaleSim [ 79 ].",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "C. Hardware Implementation and Evaluation \nThe proposed morphable hardware was simulated using an in-house simulator based on ScaleSim [ 79 ]. We included a wrapper around ScaleSim to  dynamically  change the con- ﬁguration of the systolic array. Further, the simulator was integrated with CACTI [ 62 ] and DRAMSIM3 [ 49 ] to estimate access latency, power, and simulate the memory access pattern.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Further, the simulator was integrated with CACTI [ 62 ] and DRAMSIM3 [ 49 ] to estimate access latency, power, and simulate the memory access pattern. Rather than including a cycle accurate CPU (host) simulator to orchestrate the compute, we used a simple program to act as proxy for the host CPU and send control signals to schedule and orchestrate the compute on the systolic array. To correctly estimate accelerator power and area, we implemented a register-transfer level model using System Verilog and syn- thesized using Synopsys Design Compiler [ 93 ] with a 32nm library [ 94 ].",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "To correctly estimate accelerator power and area, we implemented a register-transfer level model using System Verilog and syn- thesized using Synopsys Design Compiler [ 93 ] with a 32nm library [ 94 ]. Table  I  lists the estimated power consumption and area of the major components. Instead of simulating the CPU, we tested the K-means clustering and cluster optimization on a mobile SoC with 8 ×  ARM Cortex A78 series CPU.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Instead of simulating the CPU, we tested the K-means clustering and cluster optimization on a mobile SoC with 8 ×  ARM Cortex A78 series CPU. Table  II gives the key attributes of the implemented hardware against some of the prior accelerators. Note that  Us.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Note that  Us. ´as  hardware is not outperforming any of them as the goal was greater  scheduling ﬂexibility  for power tracking rather than performance or area. Platform Freq.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "Platform Freq. (MHz) \nArea (mm 2 ) \nPower \n(W) \nPeak Thpt. (GOps) \nEnergy Eff.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "(GOps) \nEnergy Eff. (GOps/W) DaDianNao [ 16 ] 606 67.3 16.3 4964 304.54 CNVLUTIN [ 4 ] 606 70.1 17.4 4964 285.29 Activation Sparse [ 80 ] 667 292 19.2 5466 284.69 EyerissV2 [ 15 ] 200MHz N/A N/A 153.6G 8b ﬁxed pt/s 193.7 FlexBlock [ 63 ] 333MHz 160.3 (65nm) 34.4 (when same #PEs) 4504 131.03 \nUs. ´as \n592 168.2 22.7 (17.2 if only train) 4016 159.42 Fully powered, DNN Compute only 287.44 Fully powered, DNN  μ − proﬁler 255.39 EH +  μ − proﬁle + NV-mems + resizing RAM + Host 159.40 \nTABLE II: Comparison with prior accelerator-based platforms.",
      "type": "sliding_window",
      "tokens": 238
    },
    {
      "text": "´as \n592 168.2 22.7 (17.2 if only train) 4016 159.42 Fully powered, DNN Compute only 287.44 Fully powered, DNN  μ − proﬁler 255.39 EH +  μ − proﬁle + NV-mems + resizing RAM + Host 159.40 \nTABLE II: Comparison with prior accelerator-based platforms. The systolic array accelerator time multiplexes between per- forming feature extraction for exemplar selection and running the training. Fig.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "Fig. 8c  shows the time distribution of the accel- erator between performing exemplar selection and training. It also shows the number of exemplar frames per 100 frame, i.e., of any 100 frame encountered, how many of those will contain a relatively new data.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "It also shows the number of exemplar frames per 100 frame, i.e., of any 100 frame encountered, how many of those will contain a relatively new data. Over 10 iterations of retraining, \n901 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. -18.87 \n-9.14 \n-10.2 -11.35 \n-20 \n-15 \n-10 \n-5 \n0 \n0% 20% 40% 60% 80% 100% \nRapidly Varrying \nModerately \nstable \nRelatively \nStable \nFully Powered \nLoss in accuracy in % \nContribution of Policy \nExemplar Profiler Morphable No Optimization \n(a) Contribution of components on video data \n0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \nBest Average Best Average \nUsas iCARL Usas Optimus \n# Relative Exemplars # Relative Epoch \nRelative Error wrt Oracle  \n(Lower  is better) \n(Audio) Audio MNIST (Audio) CHiME Home (3D PC) KITTI Vision \n(3D PC) nuScenes (IMU) Bearing Fault (IMU) MHEALTH \n(b)  Us.",
      "type": "sliding_window",
      "tokens": 225
    },
    {
      "text": "-18.87 \n-9.14 \n-10.2 -11.35 \n-20 \n-15 \n-10 \n-5 \n0 \n0% 20% 40% 60% 80% 100% \nRapidly Varrying \nModerately \nstable \nRelatively \nStable \nFully Powered \nLoss in accuracy in % \nContribution of Policy \nExemplar Profiler Morphable No Optimization \n(a) Contribution of components on video data \n0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \nBest Average Best Average \nUsas iCARL Usas Optimus \n# Relative Exemplars # Relative Epoch \nRelative Error wrt Oracle  \n(Lower  is better) \n(Audio) Audio MNIST (Audio) CHiME Home (3D PC) KITTI Vision \n(3D PC) nuScenes (IMU) Bearing Fault (IMU) MHEALTH \n(b)  Us. ´as  beyond video data \n0 \n10 \n20 \n30 \n40 \n50 \nPredictable Sporadic Predictable Sporadic Predictable Sporadic \nLarge - Video (SA size: 256; AP: 16W) \nMedium - Audio (SA size: 128; AP: 8W) \nSmall - IMU (SA size: 128; AP: 2W) \n% of Unfinished  Compute  \ndue to Power Failure \nAverage Power Baseline SW Backup SW + NVM HW + NVM \n(c)  Us .´as  hardware - different data and energy \nFig. 10: Contribution of different components of  Us.",
      "type": "sliding_window",
      "tokens": 336
    },
    {
      "text": "10: Contribution of different components of  Us. ´as  on other applications, data modalities, and power environments. the learner classiﬁed  ≈ 4.5 frames/100-frames (on an average) as exemplar data.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "the learner classiﬁed  ≈ 4.5 frames/100-frames (on an average) as exemplar data. And, over 40 hours of continuous learning, we get  ≈ 5.02 frames/100-frames as exemplar data (resulting in  ≈ 17.4% of total accelerator time). Performance-Power Trade-offs:  As Table  II  suggest,  Us.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Performance-Power Trade-offs:  As Table  II  suggest,  Us. ´as does not deliver the highest throughput and also consumes more power compared to the other accelerators. Us.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "Us. ´as  was designed on a intermittency friendly approach, and was never designed to hit the best throughput. The unit compute (only a 3  ×  3 convolution per tile) that  Us.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "The unit compute (only a 3  ×  3 convolution per tile) that  Us. ´as  can perform is much smaller than the other accelerators, limiting its throughput but increasing its modularity of handling intermittent power failures (or power changes). Us.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "Us. ´as  also consumes more power than the other accelerators since it also performs the exemplar selection along with the DNN training, and also houses NV-SRAM buffers for hardware check-pointing. While fully powered,  Us.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "While fully powered,  Us. ´as  is competitive in terms of energy efﬁciency for training-only tasks. We include details on the energy efﬁciency of  Us.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "We include details on the energy efﬁciency of  Us. ´as  under different of operation conﬁgurations in Table  II . Note that the energy inefﬁciency arises primarily from i) multiple saves and restores, ii) use of NV memories \nand iii) reconﬁguring the DRAM (along with a commercial ARM based host CPU).",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "Note that the energy inefﬁciency arises primarily from i) multiple saves and restores, ii) use of NV memories \nand iii) reconﬁguring the DRAM (along with a commercial ARM based host CPU). Note that most prior works ignore memory and host overheads while reporting the throughput, efﬁciency and power numbers. Moreover,  Us.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Moreover,  Us. ´as  needs more I/O operations to store the streaming data to compute the exemplars. We believe it will not be fair to compare the energy efﬁciency and throughput of a system like ours, which in- herently has more memory, I/O and reconﬁguration operation with a pure compute based systems mentioned in the hardware baseline.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "We believe it will not be fair to compare the energy efﬁciency and throughput of a system like ours, which in- herently has more memory, I/O and reconﬁguration operation with a pure compute based systems mentioned in the hardware baseline. Further, we are the ﬁrst of a kind system to imagine sustainability ﬁrst and design a morphable hardware which can facilitate multiple functionality. We also compare our work against two reconﬁgurable platforms [ 15 ], [ 63 ].",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "We also compare our work against two reconﬁgurable platforms [ 15 ], [ 63 ]. Power Aware Scaling: The  Us. ´as  hardware’s most important feature is its ability to  morph  according to power availability.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "´as  hardware’s most important feature is its ability to  morph  according to power availability. Fig. 11  shows its ability to maximize the instantaneous power utilization and scale the number of tiles.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "11  shows its ability to maximize the instantaneous power utilization and scale the number of tiles. This allows  Us. ´as to effectively perform more computation with an intermittent power source.",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "´as to effectively perform more computation with an intermittent power source. As shown in Fig. 11b ,  Us.",
      "type": "sliding_window",
      "tokens": 28
    },
    {
      "text": "11b ,  Us. ´as  maintains a high duty cycle across power variance, whereas DaDianNao [ 16 ] could not be active for all the power cycles. Considering the power proﬁle of Fig.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "Considering the power proﬁle of Fig. 11b ,  Us. ´as  can ﬁnish about 50 cycles of retraining (50 complete training cycles) and DaDianNao can only ﬁnish 22 training cycles, even assuming a zero overhead, seamless save-restore of the partial computes of DaDianNao during a power failure/emergency.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "´as  can ﬁnish about 50 cycles of retraining (50 complete training cycles) and DaDianNao can only ﬁnish 22 training cycles, even assuming a zero overhead, seamless save-restore of the partial computes of DaDianNao during a power failure/emergency. Setup Effective Training Accuracy Degradation Replacement Cycle* Battery Backed Custom HW[5000mAH] 93.17 2.48 2 - 3 years Battery Backed Mobile GPU 78.55 7.43 18 - 24 months Fixed Power [15W] 67.54 12.6 NA Fixed Power [35W] 100 1.87 Us. ´as 95.3 1.92 7 - 10 years \nTABLE III: Comparing  Us.",
      "type": "sliding_window",
      "tokens": 166
    },
    {
      "text": "´as 95.3 1.92 7 - 10 years \nTABLE III: Comparing  Us. ´as  with other possible solutions. Sustainability:  To ensure sustainable and continuous learning at the edge,  Us.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "Sustainability:  To ensure sustainable and continuous learning at the edge,  Us. ´as  operates independently of the power grid or cloud dependency. We evaluated  Us.",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "We evaluated  Us. ´as  against DaDian- Nao, a power-efﬁcient DNN training accelerator, with some modiﬁcations for comparison. Using the Seattle SOLRAD power trace for January 1, 2022, we simulated 40 hours of continuous learning with 5 different models on Urban Trafﬁc data [ 97 ] and  Us.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "Using the Seattle SOLRAD power trace for January 1, 2022, we simulated 40 hours of continuous learning with 5 different models on Urban Trafﬁc data [ 97 ] and  Us. ´as  hardware. The results, summarized in Table  IV , demonstrate the effectiveness of  Us.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "The results, summarized in Table  IV , demonstrate the effectiveness of  Us. ´as  in achieving continuous forward progress compared to other approaches. It completed more training tasks while consuming less power and minimizing wastage.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "It completed more training tasks while consuming less power and minimizing wastage. In contrast, cloud-based solutions ex- hibited poor sustainability, relying on high-power-consuming GP-GPUs, and edge servers without power availability strug- gled to perform any compute. Us.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Us. ´as  emerges as a promising solution, effectively achieving sustainable and carbon-neutral continuous learning at the edge, addressing critical challenges related to power constraints and environmental impact. Alternate Solutions:  Although  Us.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "Alternate Solutions:  Although  Us. ´as  works completely using intermittent power, it is imperative to compare and contrast it with other possible solutions. TABLE  III  depicts some of such possible comparison points.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "TABLE  III  depicts some of such possible comparison points. The possible alternate solu- tions being battery-backed custom HW [ 16 ], battery-backed commercial GPU and ﬁxed power budget with store and \n902 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. 0 \n100 \n200 \n300 \n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 \n# TIles Utilized \nTraning Iteration \n#Tiles-Oracle ηdŝůĞƐͲhƔĄƐ DadianNao ŵĞĂŶͲhƔĄƐ Mean-DaDianNao \n(a) Monotonically increasing \n0 \n50 \n100 \n1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 \n#Tiles Utilized \nTraining Iteration \n#Tiles-Oracle dŝůĞƐͲhƔĄƐ DaDianNao DĞĂŶͲhƔĄƐ Mean-DaDianNao \n(b) Rapidly varying \nFig.",
      "type": "sliding_window",
      "tokens": 189
    },
    {
      "text": "0 \n100 \n200 \n300 \n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 \n# TIles Utilized \nTraning Iteration \n#Tiles-Oracle ηdŝůĞƐͲhƔĄƐ DadianNao ŵĞĂŶͲhƔĄƐ Mean-DaDianNao \n(a) Monotonically increasing \n0 \n50 \n100 \n1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 \n#Tiles Utilized \nTraining Iteration \n#Tiles-Oracle dŝůĞƐͲhƔĄƐ DaDianNao DĞĂŶͲhƔĄƐ Mean-DaDianNao \n(b) Rapidly varying \nFig. 11: Tile utilization against available power:  Us. ´as  with eager scheduling vs an oracle scheduler.",
      "type": "sliding_window",
      "tokens": 187
    },
    {
      "text": "´as  with eager scheduling vs an oracle scheduler. Us. ´as  closely tracks oracle, where as DaDianNao [ 16 ] falls short.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "´as  closely tracks oracle, where as DaDianNao [ 16 ] falls short. execute (using a capacitor/ battery). We quantitatively compare the effective training, i.e.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "We quantitatively compare the effective training, i.e. the ratio of number of scheduled training to the number of completed training, loss of accuracy compared to the baseline. It is clear that even with intermittent power availability  Us.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "It is clear that even with intermittent power availability  Us. ´as  is objectively ﬁnishing more tasks (except compared to a system with a consistently high power availability). Furthermore, we also present a qualitative com- parison on the maintenance cycle needed for these solutions.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "Furthermore, we also present a qualitative com- parison on the maintenance cycle needed for these solutions. While a completely grid based solution is best in terms of reliability, it is not feasible because of the power demands. Any battery backed system will be limited to the charging cycle of the batteries ( ≈ 500 cycles for Li-ion batteries) which leads to a typical 18 to 24 months of life for such devices (compared to this, a super capacitor have a life of more than 100 years).",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "Any battery backed system will be limited to the charging cycle of the batteries ( ≈ 500 cycles for Li-ion batteries) which leads to a typical 18 to 24 months of life for such devices (compared to this, a super capacitor have a life of more than 100 years). We believe that the lifetime of  Us. ´as  will be limited either by the live of the harvesting source ( ≈ 20 – 30 years for solar, ≈ 10 – 12 years for portable wind turbines), or the training hardware (typical life cycle of embedded devices are of range of 7 – 10 years).",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "´as  will be limited either by the live of the harvesting source ( ≈ 20 – 30 years for solar, ≈ 10 – 12 years for portable wind turbines), or the training hardware (typical life cycle of embedded devices are of range of 7 – 10 years). We agree that a limitation of our work comes from the choice of solar energy: unavailability during night and bad weather makes the deployment harder. However, there has been signiﬁcant recent development in portable wind turbines [ 96 ], which can be deployed on rooftops, can work with  ≥ 5 mph  wind speed, and can provide power equivalent of 15 solar cells.",
      "type": "sliding_window",
      "tokens": 140
    },
    {
      "text": "However, there has been signiﬁcant recent development in portable wind turbines [ 96 ], which can be deployed on rooftops, can work with  ≥ 5 mph  wind speed, and can provide power equivalent of 15 solar cells. Therefore, similar technologies can be used to augment the harvesting mechanism. Deployment Training Completed \nMean Power Consumed (W) \nMean Power \nWasted (W) \nCarbon Footprint (lbs/yr) Us.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "Deployment Training Completed \nMean Power Consumed (W) \nMean Power \nWasted (W) \nCarbon Footprint (lbs/yr) Us. ´as 11 17.2 3.54 8.33 DaDianNao (persistent) 6 6.4 10.8 128.0712 DaDianNao (Software Only) 4 5.8 12.46 135.964 DaDianNao (actual) 2 2.09 24.73 199.70 Edge Cloud 0 0 – Cloud 1 200 0 2233.8 Max Power = 32W; Min Power = 12W; Training Scheduled = 12 \nTABLE IV: Comparing  Us. ´as  hardware with other state of the art offerings for both performance and sustainability.",
      "type": "sliding_window",
      "tokens": 162
    },
    {
      "text": "´as  hardware with other state of the art offerings for both performance and sustainability. D. Towards Other Applications and Domains \nThe morphable hardware design of  Us. ´as  plays a crucial role in efﬁciently handling varying energy income and workloads.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "´as  plays a crucial role in efﬁciently handling varying energy income and workloads. As the energy income becomes more sporadic, hardware- assisted scheduling seamlessly transfers work to active pro- cessing elements (PEs), maximizing the completion of tasks that could have otherwise been lost. This hardware-driven adaptive scheduling signiﬁcantly impacts different data modal- ities, from large-scale to small-scale, and various magnitudes of energy income, as depicted in Fig.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "This hardware-driven adaptive scheduling signiﬁcantly impacts different data modal- ities, from large-scale to small-scale, and various magnitudes of energy income, as depicted in Fig. 10c . For scenarios with larger and predictable energy income, software-based backup and restore mechanisms can offer signiﬁcant beneﬁts, as the energy consumed for such operations is typically a small fraction of the overall energy income.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "For scenarios with larger and predictable energy income, software-based backup and restore mechanisms can offer signiﬁcant beneﬁts, as the energy consumed for such operations is typically a small fraction of the overall energy income. Predictive actions for saving the system state can be easily taken. However, in situations with sporadic energy income, the hardware-assisted scheduling becomes paramount.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "However, in situations with sporadic energy income, the hardware-assisted scheduling becomes paramount. It ensures that active PEs efﬁciently utilize available power to complete work, preventing potential losses and eliminating the need to restart tasks from the beginning. Us.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Us. ´as  excels as a candidate for continuous learning at all scales due to the hardware’s adaptability to varying data and model sizes. As data and model dimensions decrease, the hardware assistance’s impact becomes more pronounced, making  Us.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "As data and model dimensions decrease, the hardware assistance’s impact becomes more pronounced, making  Us. ´as  an excellent solution for continuous learning across diverse application sizes. Along with morphable hardware, the exemplar selection and the micro-proﬁler play an important role for the success of  Us.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "Along with morphable hardware, the exemplar selection and the micro-proﬁler play an important role for the success of  Us. ´as . When power is highly uncertain, the morphable hardware also strongly contributes, however, as the power proﬁle becomes stable, the algorithmic contributions dominate.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "When power is highly uncertain, the morphable hardware also strongly contributes, however, as the power proﬁle becomes stable, the algorithmic contributions dominate. Fig. 10a  shows the contribution of the different components of  Us.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "10a  shows the contribution of the different components of  Us. ´as  under different power proﬁles. Moreover, the algo- rithmic contributions can be extended into any classiﬁcation based application or data modality.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "Moreover, the algo- rithmic contributions can be extended into any classiﬁcation based application or data modality. If the learning has to be unsupervised, one needs to experiment with known clustering techniques to decide the right classiﬁcation approach. We demonstrate this by testing the exemplar selection and the μ − proﬁler with different modalities of data.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "We demonstrate this by testing the exemplar selection and the μ − proﬁler with different modalities of data. Our workloads included Audio [ 23 ], [ 35 ](speech classiﬁcation), 3D Point Clouds [ 14 ], [ 24 ](object classiﬁcation) and Inertial Measure- ment Unit sensor data [ 9 ], [ 106 ](fault and activity detection). Observe that, as  Us.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "Observe that, as  Us. ´as  is designed to handle dense and noisy data, it outperforms the respective state-of-the-arts (which were tuned for small, clean benchmark data). 903 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "903 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "Restrictions apply. VI. D ISCUSSION \nKey insights:  Compared to other systems, the ratio of energy requirement of task vs the harvested energy is much higher here.",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "D ISCUSSION \nKey insights:  Compared to other systems, the ratio of energy requirement of task vs the harvested energy is much higher here. Added with time constraints, designing such a system becomes tricky. While many of the prior works have designed their systems around inference using intermittent systems, we are one of the few works which focuses on learning, and the only work which does it on a large scale of data.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "While many of the prior works have designed their systems around inference using intermittent systems, we are one of the few works which focuses on learning, and the only work which does it on a large scale of data. The proposed system is not only energy intermittent, but also memory intermittent, interconnect intermittent and most importantly data intermittent (we don’t know how much data and what data). This gives us a unique platform to think of intermittency beyond embedded systems and energy.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "This gives us a unique platform to think of intermittency beyond embedded systems and energy. Related Works:  Although there has been signiﬁcant re- search [ 40 ], [ 41 ], [ 47 ], [ 52 ], [ 56 ], [ 61 ], [ 72 ], [ 104 ] on enabling machine learning in intermittently powered devices, a majority of it focuses on performing inference. Only intermittent learn- ing [ 47 ] focuses on performing on-device training, but with very small workloads and models.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "Only intermittent learn- ing [ 47 ] focuses on performing on-device training, but with very small workloads and models. Considering the scale, scope and workload of our problem, limits direct comparisons, ex- cept for comparing their exemplar selection method. Similarly, Ekya [ 12 ] only focuses on co-location of computation, and it’s efﬁciency on ﬁnishing compute even on a custom hardware is shown in Fig.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Similarly, Ekya [ 12 ] only focuses on co-location of computation, and it’s efﬁciency on ﬁnishing compute even on a custom hardware is shown in Fig. 4 . Green Data Centers:  As sustainability gains traction, industry has worked towards building green data centers [ 58 ], [ 59 ].",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "Green Data Centers:  As sustainability gains traction, industry has worked towards building green data centers [ 58 ], [ 59 ]. Although using these data centers for computation can be an alternative, it will not solve the bandwidth and the privacy issues mentioned in § I . Moreover, communicating and storing such high volume data will also require energy.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "Moreover, communicating and storing such high volume data will also require energy. Our solu- tion decentralizes this massive compute using a sustainable approach and hence has its own merits. Further, this can help build future solutions using these decentralised nodes for other applications.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Further, this can help build future solutions using these decentralised nodes for other applications. We do encourage the use of green data centers for other centralized compute applications. VII.",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "VII. C ONCLUDING  R EMARKS \nThe growth of smart cities and urban mobility applications, along with reformations in privacy laws, have produced a need for pervasive, DNN based continuous learning at the edge. Although current commercial devices are capable of handling inference at the edge, the power and resource requirements of training make it impractical and unsustainable for all edge nodes to also perform continuous training off of grid power.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Although current commercial devices are capable of handling inference at the edge, the power and resource requirements of training make it impractical and unsustainable for all edge nodes to also perform continuous training off of grid power. In this work, we design  Us. ´as , a sustainable continuous learning platform, which can perform video analytics by using an inter- mittent power source like solar power.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "´as , a sustainable continuous learning platform, which can perform video analytics by using an inter- mittent power source like solar power. The learning algorithm of  Us. ´as  delivers 4.96% more accurate classiﬁcation compared to a na¨ıve learner, and the morphable hardware design uses intermittent computing to maintain forward progress even while running on lower power budget.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "´as  delivers 4.96% more accurate classiﬁcation compared to a na¨ıve learner, and the morphable hardware design uses intermittent computing to maintain forward progress even while running on lower power budget. Together,  Us. ´as  can save up to 200lbs of  CO 2  per year compared to a state of the art accelerator running on the grid.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "´as  can save up to 200lbs of  CO 2  per year compared to a state of the art accelerator running on the grid. VIII. A CKNOWLEDGMENTS \nWe would like to offer our thanks to the anonymous reviewers for their detailed feedback, which has greatly helped to improve and reﬁne this paper.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "A CKNOWLEDGMENTS \nWe would like to offer our thanks to the anonymous reviewers for their detailed feedback, which has greatly helped to improve and reﬁne this paper. This work was supported in part by Semiconductor Research Corporation (SRC), Cen- ter for Brain-inspired Computing (C-BRIC) and NSF Grant #1822923 (SPX: SOPHIA). We acknowledge that all product names used are for identiﬁcation purposes only and may be trademarks of their respective companies.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "We acknowledge that all product names used are for identiﬁcation purposes only and may be trademarks of their respective companies. R EFERENCES \n[1] S. M. Abhay S, “Autonomous vehicle market by level of au- \ntomation,”  https://www.alliedmarketresearch.com/autonomous-vehicle- market , Feb 2022, (Accessed on 08/04/2023). [2] “Achieving Compliant Data Residency and Security with Azure,” \nhttps://azure.microsoft.com/mediahandler/ﬁles/resourceﬁles/achieving- compliant-data-residency-and-security-with-azure/Achieving Compliant Data Residency and Security with Azure.pdf .",
      "type": "sliding_window",
      "tokens": 168
    },
    {
      "text": "[2] “Achieving Compliant Data Residency and Security with Azure,” \nhttps://azure.microsoft.com/mediahandler/ﬁles/resourceﬁles/achieving- compliant-data-residency-and-security-with-azure/Achieving Compliant Data Residency and Security with Azure.pdf . [3] D. B. Agusdinata, W. Liu, H. Eakin, and H. Romero, “Socio- \nenvironmental impacts of lithium mineral extraction: towards a research agenda,”  Environmental Research Letters , vol. 13, no.",
      "type": "sliding_window",
      "tokens": 140
    },
    {
      "text": "13, no. 12, p. 123001, 2018. [4] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and \nA. Moshovos, “Cnvlutin: Ineffectual-neuron-free deep neural network computing,”  ACM SIGARCH Computer Architecture News , vol.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "[4] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and \nA. Moshovos, “Cnvlutin: Ineffectual-neuron-free deep neural network computing,”  ACM SIGARCH Computer Architecture News , vol. 44, no. 3, pp.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "3, pp. 1–13, 2016. [5] W. Amit Katwala, “The spiralling environmental cost of our lithium \nbattery addiction,” https://www.wired.co.uk/article/lithium-batteries- environment-impact , May 2018, (Accessed on 07/08/2023).",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "[5] W. Amit Katwala, “The spiralling environmental cost of our lithium \nbattery addiction,” https://www.wired.co.uk/article/lithium-batteries- environment-impact , May 2018, (Accessed on 07/08/2023). [6] G. Ananthanarayanan, V. Bahl, P. Bod´ık, K. Chintalapudi, M. Philipose, \nL. R. Sivalingam, and S. Sinha, “Real-time Video Analytics – the killer app for edge computing,”  IEEE Computer , 2017. [7] AWS Outposts, “https://aws.amazon.com/outposts/rack/hardware- specs/?nc=sn&loc=4.” [8] Azure Stack Edge, “https://azure.microsoft.com/en- us/services/databox/edge/.” [9] O. Banos, C. Villalonga, R. Garc´ıa, A. Saez, M. Damas, J. Holgado- \nTerriza, S. Lee, H. Pomares, and I. Rojas, “Design, implementation and validation of a novel open framework for agile development of mobile health applications,”  BioMedical Engineering OnLine , 2015.",
      "type": "sliding_window",
      "tokens": 308
    },
    {
      "text": "[7] AWS Outposts, “https://aws.amazon.com/outposts/rack/hardware- specs/?nc=sn&loc=4.” [8] Azure Stack Edge, “https://azure.microsoft.com/en- us/services/databox/edge/.” [9] O. Banos, C. Villalonga, R. Garc´ıa, A. Saez, M. Damas, J. Holgado- \nTerriza, S. Lee, H. Pomares, and I. Rojas, “Design, implementation and validation of a novel open framework for agile development of mobile health applications,”  BioMedical Engineering OnLine , 2015. [10] S. Bauer, “Explainer: The opportunities and challenges of the lithium \nindustry,”  Di´alogo Chino , 2020. [11] Beverly Hills has thousands of surveillance cameras, “https://bit.ly/BeverlyHillsCamera.” [12] R. Bhardwaj, Z. Xia, G. Ananthanarayanan, J. Jiang, Y. Shu, N. Kar- \nianakis, K. Hsieh, P. Bahl, and I. Stoica, “Ekya: Continuous learning of video analytics models on edge compute servers,” in  19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , 2022, pp.",
      "type": "sliding_window",
      "tokens": 346
    },
    {
      "text": "[11] Beverly Hills has thousands of surveillance cameras, “https://bit.ly/BeverlyHillsCamera.” [12] R. Bhardwaj, Z. Xia, G. Ananthanarayanan, J. Jiang, Y. Shu, N. Kar- \nianakis, K. Hsieh, P. Bahl, and I. Stoica, “Ekya: Continuous learning of video analytics models on edge compute servers,” in  19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , 2022, pp. 119–135. [13] R. Bird, Z. J. Baum, X. Yu, and J. Ma, “The regulatory environment \nfor lithium-ion battery recycling,” 2022.",
      "type": "sliding_window",
      "tokens": 185
    },
    {
      "text": "[13] R. Bird, Z. J. Baum, X. Yu, and J. Ma, “The regulatory environment \nfor lithium-ion battery recycling,” 2022. [14] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, \nA. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous driving,” in  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2020, pp. 11 621–11 631.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "11 621–11 631. [15] Y.-H. Chen, T.-J. Yang, J. Emer, and V. Sze, “Eyeriss v2: A ﬂexible \naccelerator for emerging deep neural networks on mobile devices,” IEEE Journal on Emerging and Selected Topics in Circuits and Systems , vol.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Yang, J. Emer, and V. Sze, “Eyeriss v2: A ﬂexible \naccelerator for emerging deep neural networks on mobile devices,” IEEE Journal on Emerging and Selected Topics in Circuits and Systems , vol. 9, no. 2, pp.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "2, pp. 292–308, 2019. [16] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, \nZ. Xu, N. Sun  et al.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "[16] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, \nZ. Xu, N. Sun  et al. , “Dadiannao: A machine-learning supercomputer,” in  2014 47th Annual IEEE/ACM International Symposium on Microar- chitecture . IEEE, 2014, pp.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "IEEE, 2014, pp. 609–622. [17] P. Chi, S. Li, Y. Cheng, Y. Lu, S. H. Kang, and Y. Xie, “Architecture \ndesign with stt-ram: Opportunities and challenges,” in  2016 21st Asia and South Paciﬁc design automation conference (ASP-DAC) .",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "[17] P. Chi, S. Li, Y. Cheng, Y. Lu, S. H. Kang, and Y. Xie, “Architecture \ndesign with stt-ram: Opportunities and challenges,” in  2016 21st Asia and South Paciﬁc design automation conference (ASP-DAC) . IEEE, 2016, pp. 109–114.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "109–114. 904 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. [18] A. F. CNBC, “How trafﬁc sensors and cameras are trans- forming city streets,”  https://www.cnbc.com/2021/02/22/how-trafﬁc- sensors-and-cameras-are-transforming-city-streets.html , (Accessed on 04/28/2023).",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "[18] A. F. CNBC, “How trafﬁc sensors and cameras are trans- forming city streets,”  https://www.cnbc.com/2021/02/22/how-trafﬁc- sensors-and-cameras-are-transforming-city-streets.html , (Accessed on 04/28/2023). [19] Coral.ai, “Edge tpu performance benchmarks =  https://coral.ai/docs/ \nedgetpu/benchmarks/ ,” (Accessed on 11/21/2022). [20] D Maltoni, V Lomonaco, “Continuous learning in single-incremental- \ntask scenarios,” in  Neural Networks , 2019.",
      "type": "sliding_window",
      "tokens": 161
    },
    {
      "text": "[20] D Maltoni, V Lomonaco, “Continuous learning in single-incremental- \ntask scenarios,” in  Neural Networks , 2019. [21] J. Dodge, T. Prewitt, R. Tachet des Combes, E. Odmark, R. Schwartz, \nE. Strubell, A. S. Luccioni, N. A. Smith, N. DeCario, and W. Buchanan, “Measuring the carbon intensity of ai in cloud instances,” in  2022 ACM Conference on Fairness, Accountability, and Transparency , 2022, pp.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "Dodge, T. Prewitt, R. Tachet des Combes, E. Odmark, R. Schwartz, \nE. Strubell, A. S. Luccioni, N. A. Smith, N. DeCario, and W. Buchanan, “Measuring the carbon intensity of ai in cloud instances,” in  2022 ACM Conference on Fairness, Accountability, and Transparency , 2022, pp. 1877–1894. [22] E. Dong, Y. Zhu, Y. Ji, and S. Du, “An improved convolution neural \network for object detection using yolov2,” in  2018 IEEE International Conference on Mechatronics and Automation (ICMA) .",
      "type": "sliding_window",
      "tokens": 176
    },
    {
      "text": "[22] E. Dong, Y. Zhu, Y. Ji, and S. Du, “An improved convolution neural \network for object detection using yolov2,” in  2018 IEEE International Conference on Mechatronics and Automation (ICMA) . IEEE, 2018, pp. 1184–1188.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "1184–1188. [23] P. Foster, S. Sigtia, S. Krstulovic, J. Barker, and M. D. Plumbley, \n“Chime-home: A dataset for sound source recognition in a domestic environment,” in  2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) . IEEE, 2015, pp.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "IEEE, 2015, pp. 1–5. [24] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous \ndriving?",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "[24] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous \ndriving? the kitti vision benchmark suite,” in  2012 IEEE conference on computer vision and pattern recognition . IEEE, 2012, pp.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "IEEE, 2012, pp. 3354– 3361. [25] N. Global Monitoring Laboratory, “Solrad network,”  https://gml.noaa.",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "[25] N. Global Monitoring Laboratory, “Solrad network,”  https://gml.noaa. gov/grad/solrad/index.html , (Accessed on 11/21/2022). [26] G. Gobieski, B. Lucia, and N. Beckmann, “Intelligence beyond the \nedge: Inference on intermittent embedded systems,” in  ASPLOS .",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "[26] G. Gobieski, B. Lucia, and N. Beckmann, “Intelligence beyond the \nedge: Inference on intermittent embedded systems,” in  ASPLOS . ACM, 2019. [27] Z. Gong, H. Ji, C. W. Fletcher, C. J. Hughes, and J. Torrellas, \n“Sparsetrain: Leveraging dynamic sparsity in software for training dnns on general-purpose simd processors,” in  Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques , 2020, pp.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "[27] Z. Gong, H. Ji, C. W. Fletcher, C. J. Hughes, and J. Torrellas, \n“Sparsetrain: Leveraging dynamic sparsity in software for training dnns on general-purpose simd processors,” in  Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques , 2020, pp. 279–292. [28] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, B. Sharma, M. T. \nKandemir, and C. R. Das, “Cocktail: A multidimensional optimization for model serving in cloud,” in  19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , 2022, pp.",
      "type": "sliding_window",
      "tokens": 192
    },
    {
      "text": "[28] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, B. Sharma, M. T. \nKandemir, and C. R. Das, “Cocktail: A multidimensional optimization for model serving in cloud,” in  19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , 2022, pp. 1041–1057. [29] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y.",
      "type": "sliding_window",
      "tokens": 143
    },
    {
      "text": "[29] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y. Wei, \nD. Brooks, and C.-J. Wu, “Chasing carbon: The elusive environmental footprint of computing,”  IEEE Micro , vol.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "Wu, “Chasing carbon: The elusive environmental footprint of computing,”  IEEE Micro , vol. 42, no. 4, pp.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "4, pp. 37–47, 2022. [30] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing \ndeep neural networks with pruning, trained quantization and huffman coding,”  arXiv preprint arXiv:1510.00149 , 2015.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "[30] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing \ndeep neural networks with pruning, trained quantization and huffman coding,”  arXiv preprint arXiv:1510.00149 , 2015. [31] J. He and F. Zhu, “Online continual learning for visual food classiﬁ- \ncation,” in  Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "He and F. Zhu, “Online continual learning for visual food classiﬁ- \ncation,” in  Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 2337–2346. [32] K. He, X. Zhang, S. Ren, and J.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "[32] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep resid- \nual networks,” in  European conference on computer vision . Springer, 2016, pp.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "Springer, 2016, pp. 630–645. [33] IBM, “Data labeling,”  https://www.ibm.com/cloud/learn/data-labeling , \n(Accessed on 11/21/2022).",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "[33] IBM, “Data labeling,”  https://www.ibm.com/cloud/learn/data-labeling , \n(Accessed on 11/21/2022). [34] I. Ilievski, T. Akhtar, J. Feng, and C. Shoemaker, “Efﬁcient hyperpa- \nrameter optimization for deep learning algorithms using deterministic rbf surrogates,” in  Proceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 31, no.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "31, no. 1, 2017. [35] Z. Jackson, “Free spoken digit dataset (fsdd),” https://github.",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "[35] Z. Jackson, “Free spoken digit dataset (fsdd),” https://github. com/Jakobovski/free-spoken-digit-dataset , July 2022, (Accessed on 07/08/2023). [36] C. Jones and J. D. Ryan,  Encyclopedia of hinduism .",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "[36] C. Jones and J. D. Ryan,  Encyclopedia of hinduism . Infobase publishing, 2006. [37] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, \nS. Bates, S. Bhatia, N. Boden, A. Borchers  et al.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "[37] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, \nS. Bates, S. Bhatia, N. Boden, A. Borchers  et al. , “In-datacenter performance analysis of a tensor processing unit,” in  Proceedings of the 44th annual international symposium on computer architecture , 2017, pp. 1–12.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "1–12. [38] Junchen Jiang, Ganesh Ananthanarayanan, Peter Bod´ık, Siddhartha \nSen, Ion Stoica, “Chameleon: Scalable adaptation of video analytics,” in  ACM SIGCOMM , 2018. [39] Junjue Wang, Ziqiang Feng, Shilpa George, Roger Iyengar, Pillai Pad- \nmanabhan, Mahadev Satyanarayanan, “Towards scalable edge-native applications,” in  ACM/IEEE Symposium on Edge Computing , 2019.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "[39] Junjue Wang, Ziqiang Feng, Shilpa George, Roger Iyengar, Pillai Pad- \nmanabhan, Mahadev Satyanarayanan, “Towards scalable edge-native applications,” in  ACM/IEEE Symposium on Edge Computing , 2019. [40] C.-K. Kang, H. R. Mendis, C.-H. Lin, M.-S. Chen, and P.-C. Hsiu, \n“Everything leaves footprints: Hardware accelerated intermittent deep inference,”  IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , vol. 39, no.",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "3479–3491, 2020. [41] C.-K. Kang, H. R. Mendis, C.-H. Lin, M.-S. Chen, and P.-C. Hsiu, \n“More is less: Model augmentation for intermittent deep inference,” ACM Transactions on Embedded Computing Systems (TECS) , vol. 21, no.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "1–26, 2022. [42] D. Kang, J. Emmons, F. Abuzaid, P. Bailis, and M. Zaharia, “Noscope: \nOptimizing deep cnn-based queries over video streams at scale.”  Proc. VLDB Endow.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "1586–1597, 2017. [43] Keras, “Keras applications,”  https://keras.io/api/applications/ , (Ac- \ncessed on 11/21/2022). [44] Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari , “Incremental \nlearning of object detectors without catastrophic forgetting,” in  ICCV , 2017.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "[44] Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari , “Incremental \nlearning of object detectors without catastrophic forgetting,” in  ICCV , 2017. [45] G. Kordopatis-Zilos, S. Papadopoulos, I. Patras, and I. Kompatsiaris, \n“Visil: Fine-grained spatio-temporal video similarity learning,” in  Pro- ceedings of the IEEE/CVF international conference on computer vision , 2019, pp. 6351–6360.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "6351–6360. [46] S. Lee, S. Goldt, and A. Saxe, “Continual learning in the teacher- \nstudent setup: Impact of task similarity,” in  International Conference on Machine Learning . PMLR, 2021, pp.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "PMLR, 2021, pp. 6109–6119. [47] S. Lee, B. Islam, Y. Luo, and S. Nirjon, “Intermittent learning: On- \ndevice machine learning on intermittently powered system,”  Proceed- ings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies , vol.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "[47] S. Lee, B. Islam, Y. Luo, and S. Nirjon, “Intermittent learning: On- \ndevice machine learning on intermittently powered system,”  Proceed- ings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies , vol. 3, no. 4, pp.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "4, pp. 1–30, 2019. [48] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar, \n“Hyperband: A novel bandit-based approach to hyperparameter opti- mization,”  The Journal of Machine Learning Research , vol.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "[48] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar, \n“Hyperband: A novel bandit-based approach to hyperparameter opti- mization,”  The Journal of Machine Learning Research , vol. 18, no. 1, pp.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "1, pp. 6765–6816, 2017. [49] S. Li, Z. Yang, D. Reddy, A. Srivastava, and B. Jacob, “Dramsim3: \na cycle-accurate, thermal-capable dram simulator,”  IEEE Computer Architecture Letters , vol.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "[49] S. Li, Z. Yang, D. Reddy, A. Srivastava, and B. Jacob, “Dramsim3: \na cycle-accurate, thermal-capable dram simulator,”  IEEE Computer Architecture Letters , vol. 19, no. 2, pp.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "2, pp. 106–109, 2020. [50] Z. Li and D. Hoiem, “Learning without forgetting,”  IEEE transactions \non pattern analysis and machine intelligence , vol.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "[50] Z. Li and D. Hoiem, “Learning without forgetting,”  IEEE transactions \non pattern analysis and machine intelligence , vol. 40, no. 12, pp.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "12, pp. 2935– 2947, 2017. [51] M Sandler, A Howard, Menglong Zhu, Andrey Zhmoginov, Liang- \nChieh Chen , “Mobilenetv2: Inverted residuals and linear bottlenecks,” in  CVPR , 2018.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "[51] M Sandler, A Howard, Menglong Zhu, Andrey Zhmoginov, Liang- \nChieh Chen , “Mobilenetv2: Inverted residuals and linear bottlenecks,” in  CVPR , 2018. [52] K. Maeng and B. Lucia, “Adaptive dynamic checkpointing for safe \nefﬁcient intermittent computing,” in  OSDI . USENIX Association, 2018.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "USENIX Association, 2018. [53] B. Makuza, Q. Tian, X. Guo, K. Chattopadhyay, and D. Yu, “Py- \nrometallurgical options for recycling spent lithium-ion batteries: A comprehensive review,”  Journal of Power Sources , vol. 491, p. 229622, 2021.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "491, p. 229622, 2021. [54] Markets and Markets, “Smart cities market analysis, industry size and \nforecast,”  https://www.marketsandmarkets.com/Market-Reports/smart- cities-market-542.html#: ∼ :text=The%20global%20Smart%20Cities% 20Market,drive%20the%20smart%20cities%20market , 2022, (Accessed on 08/04/2023). [55] M. McCloskey and N. J. Cohen, “Catastrophic interference in connec- \ntionist networks: The sequential learning problem,” in  Psychology of learning and motivation .",
      "type": "sliding_window",
      "tokens": 162
    },
    {
      "text": "[55] M. McCloskey and N. J. Cohen, “Catastrophic interference in connec- \ntionist networks: The sequential learning problem,” in  Psychology of learning and motivation . Elsevier, 1989, vol. 24, pp.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "24, pp. 109–165. [56] H. R. Mendis, C.-K. Kang, and P.-c. Hsiu, “Intermittent-aware neu- \nral architecture search,”  ACM Transactions on Embedded Computing Systems (TECS) , vol.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "[56] H. R. Mendis, C.-K. Kang, and P.-c. Hsiu, “Intermittent-aware neu- \nral architecture search,”  ACM Transactions on Embedded Computing Systems (TECS) , vol. 20, no. 5s, pp.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "5s, pp. 1–27, 2021. [57] Meta, “Sharing our progress on combating climate change = https://about.fb.com/news/2022/11/metas-progress-on-combating- climate-change/ ,” (Accessed on 11/21/2022).",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "[57] Meta, “Sharing our progress on combating climate change = https://about.fb.com/news/2022/11/metas-progress-on-combating- climate-change/ ,” (Accessed on 11/21/2022). [58] Meta, “Sustainability: Data centers,”  https://sustainability.fb.com/data- \ncenters/ , (Accessed on 04/28/2023). [59] Microsoft, “Building world-class sustainable datacenters and investing \nin solar power in arizona,” https://blogs.microsoft.com/on-the- issues/2019/07/30/building-world-class-sustainable-datacenters-and- investing-in-solar-power-in-arizona/ , (Accessed on 04/28/2023).",
      "type": "sliding_window",
      "tokens": 186
    },
    {
      "text": "[59] Microsoft, “Building world-class sustainable datacenters and investing \nin solar power in arizona,” https://blogs.microsoft.com/on-the- issues/2019/07/30/building-world-class-sustainable-datacenters-and- investing-in-solar-power-in-arizona/ , (Accessed on 04/28/2023). [60] Microsoft-Rocket-Video-Analytics-Platform, \n“https://github.com/microsoft/Microsoft-Rocket-Video-Analytics- Platform.” [61] C. S. Mishra, J. Sampson, M. T. Kandemir, and V. Narayanan, “Origin: \nEnabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks,” in  DATE , 2021. 905 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window",
      "tokens": 217
    },
    {
      "text": "905 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "Restrictions apply. [62] N. Muralimanohar, R. Balasubramonian, and N. P. Jouppi, “Cacti 6.0: \nA tool to model large caches,”  HP laboratories , vol. 27, p. 28, 2009.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "27, p. 28, 2009. [63] S.-H. Noh, J. Koo, S. Lee, J. Park, and J. Kung, “Flexblock: A ﬂexible \ndnn training accelerator with multi-mode block ﬂoating point support,” IEEE Transactions on Computers , 2023.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "Park, and J. Kung, “Flexblock: A ﬂexible \ndnn training accelerator with multi-mode block ﬂoating point support,” IEEE Transactions on Computers , 2023. [64] T. N. R. E. L. (NREL), “Solar resource maps and data,”  https://www. rel.gov/gis/solar-resource-maps.html , (Accessed on 11/21/2022).",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "rel.gov/gis/solar-resource-maps.html , (Accessed on 11/21/2022). [65] NVIDIA T4 for Virtualization, “http://bit.ly/3EﬁuWg.” [66] I. of Energy Research, “The environmental impact of lithium \nbatteries,” https://www.instituteforenergyresearch.org/renewable/the- environmental-impact-of-lithium-batteries/ , November 2020, (Accessed on 07/08/2023). [67] D. Patterson, J. Gonzalez, Q.",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "[67] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, \nD. Rothchild, D. So, M. Texier, and J. Dean, “Carbon emissions and large neural network training,”  arXiv preprint arXiv:2104.10350 , 2021.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "Dean, “Carbon emissions and large neural network training,”  arXiv preprint arXiv:2104.10350 , 2021. [68] Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo, “Optimus: an efﬁcient \ndynamic resource scheduler for deep learning clusters,” in  Proceedings of the Thirteenth EuroSys Conference , 2018, pp. 1–14.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "1–14. [69] J. F. Peters, M. Baumann, B. Zimmermann, J. Braun, and M. Weil, “The \nenvironmental impact of li-ion batteries and the role of key parameters– a review,”  Renewable and Sustainable Energy Reviews , vol. 67, pp.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "67, pp. 491–506, 2017. [70] A. Prabhu, C. Dognin, and M. Singh, “Sampling bias in deep active \nclassiﬁcation: An empirical study,”  arXiv preprint arXiv:1909.09389 , 2019.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "[70] A. Prabhu, C. Dognin, and M. Singh, “Sampling bias in deep active \nclassiﬁcation: An empirical study,”  arXiv preprint arXiv:1909.09389 , 2019. [71] I. X. G. Processors, “Rankings about energy in the world,” https://www.intel.com/content/www/us/en/products/details/processors/ xeon/scalable/gold/products.html , (Accessed on 11/21/2022). [72] K. Qiu, N. Jao, M. Zhao, C. S. Mishra, G. Gudukbay, S. Jose, \nJ. Sampson, M. T. Kandemir, and V. Narayanan, “Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent em- bedded processors,” in  2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) .",
      "type": "sliding_window",
      "tokens": 238
    },
    {
      "text": "[72] K. Qiu, N. Jao, M. Zhao, C. S. Mishra, G. Gudukbay, S. Jose, \nJ. Sampson, M. T. Kandemir, and V. Narayanan, “Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent em- bedded processors,” in  2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) . IEEE, 2020, pp. 315– 327.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "315– 327. [73] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, \nKayvon Fatahalian, “Online model distillation for efﬁcient video in- ference,” in  ICCV , 2019. [74] S.-A.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "[74] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl: \nIncremental classiﬁer and representation learning,” in  Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , 2017, pp. 2001–2010.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "2001–2010. [75] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” \narXiv preprint arXiv:1804.02767 , 2018. [76] G. V. Research, “Consumer iot market size, share and trends \nanalysis report by component (hardware, services), by connectivity technology (wired, wireless), by application (healthcare, wearable devices), and segment forecasts, 2023 - 2030,” https://www.grandviewresearch.com/industry-analysis/consumer- iot-market-report#: ∼ :text=The%20global%20consumer%20IoT% 20market,advanced%20devices%20and%20home%20appliances.",
      "type": "sliding_window",
      "tokens": 189
    },
    {
      "text": "[76] G. V. Research, “Consumer iot market size, share and trends \nanalysis report by component (hardware, services), by connectivity technology (wired, wireless), by application (healthcare, wearable devices), and segment forecasts, 2023 - 2030,” https://www.grandviewresearch.com/industry-analysis/consumer- iot-market-report#: ∼ :text=The%20global%20consumer%20IoT% 20market,advanced%20devices%20and%20home%20appliances. , July 2022, (Accessed on 08/04/2023). [77] M. S. rutgers.edu, “Support for trafﬁc cameras increases if used as a tool to limit interactions with police,” https://www.rutgers.edu/news/support-trafﬁc-cameras-increases-if- used-tool-limit-interactions-police , (Accessed on 04/28/2023).",
      "type": "sliding_window",
      "tokens": 245
    },
    {
      "text": "[77] M. S. rutgers.edu, “Support for trafﬁc cameras increases if used as a tool to limit interactions with police,” https://www.rutgers.edu/news/support-trafﬁc-cameras-increases-if- used-tool-limit-interactions-police , (Accessed on 04/28/2023). [78] J. Salamon, C. Jacoby, and J. P. Bello, “A dataset and taxonomy \nfor urban sound research,” in  22nd ACM International Conference on Multimedia (ACM-MM’14) , Orlando, FL, USA, Nov. 2014, pp. 1041– 1044.",
      "type": "sliding_window",
      "tokens": 161
    },
    {
      "text": "1041– 1044. [79] A. Samajdar, Y. Zhu, P. Whatmough, M. Mattina, and T. Kr- \nishna, “Scale-sim: Systolic cnn accelerator simulator,”  arXiv preprint arXiv:1811.02883 , 2018. [80] A. Sarma, S. Singh, H. Jiang, A. Pattnaik, A. K. Mishra, V. Narayanan, \nM. T. Kandemir, and C. R. Das, “Exploiting activation based gradient output sparsity to accelerate backpropagation in cnns,”  arXiv preprint arXiv:2109.07710 , 2021.",
      "type": "sliding_window",
      "tokens": 189
    },
    {
      "text": "[80] A. Sarma, S. Singh, H. Jiang, A. Pattnaik, A. K. Mishra, V. Narayanan, \nM. T. Kandemir, and C. R. Das, “Exploiting activation based gradient output sparsity to accelerate backpropagation in cnns,”  arXiv preprint arXiv:2109.07710 , 2021. [81] A. Sarma, S. Singh, H. Jiang, A. Pattnaik, A. K. Mishra, V. Narayanan, \nM. T. Kandemir, and C. R. Das, “Exploiting activation based gradient output sparsity to accelerate backpropagation in cnns,”  arXiv preprint arXiv:2109.07710 , 2021. [82] scale.com, “Data labeling: The authoritative guide,” https://scale.com/guides/data-labeling-annotation-guide#data-labeling- for-computer-vision , (Accessed on 11/21/2022).",
      "type": "sliding_window",
      "tokens": 269
    },
    {
      "text": "[82] scale.com, “Data labeling: The authoritative guide,” https://scale.com/guides/data-labeling-annotation-guide#data-labeling- for-computer-vision , (Accessed on 11/21/2022). [83] A. W. Services, “Aws outposts rack pricing,”  https://aws.amazon.com/ \noutposts/rack/pricing/ , (Accessed on 11/21/2022). [84] K. Seyerlehner, G. Widmer, and P. Knees, “Frame level audio \nsimilarity-a codebook approach,” in  Proc.",
      "type": "sliding_window",
      "tokens": 153
    },
    {
      "text": "[84] K. Seyerlehner, G. Widmer, and P. Knees, “Frame level audio \nsimilarity-a codebook approach,” in  Proc. of the 11th Int. Conf.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "Conf. on Digital Audio Effects (DAFx-08) , 2008, p. 31. [85] Shadi Noghabi, Landon Cox, Sharad Agarwal, Ganesh Anantha- \narayanan, “The emerging landscape of edge-computing,” in  ACM SIGMOBILE GetMobile , 2020.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "[85] Shadi Noghabi, Landon Cox, Sharad Agarwal, Ganesh Anantha- \narayanan, “The emerging landscape of edge-computing,” in  ACM SIGMOBILE GetMobile , 2020. [86] Si Young Jang, Yoonhyung Lee, Byoungheon Shin, Dongman Lee, \nDionisio Vendrell Jacinto , “Application-aware iot camera virtualization for video analytics edge computing,” in  ACM/IEEE SEC , 2018. [87] K. Simonyan and A. Zisserman, “Very deep convolutional networks for \nlarge-scale image recognition,”  arXiv preprint arXiv:1409.1556 , 2014.",
      "type": "sliding_window",
      "tokens": 180
    },
    {
      "text": "[87] K. Simonyan and A. Zisserman, “Very deep convolutional networks for \nlarge-scale image recognition,”  arXiv preprint arXiv:1409.1556 , 2014. [88] snorkel.ai, “Making automated data labeling a reality in modern ai,” \nhttps://snorkel.ai/automated-data-labeling/ , (Accessed on 11/21/2022). [89] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy con- \nsiderations for deep learning in nlp,”  arXiv preprint arXiv:1906.02243 , 2019.",
      "type": "sliding_window",
      "tokens": 166
    },
    {
      "text": "[89] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy con- \nsiderations for deep learning in nlp,”  arXiv preprint arXiv:1906.02243 , 2019. [90] J. S.-M. studyﬁnds.org, “Trafﬁc cameras become more popular when \nthey cut down on interactions with police,”  https://studyﬁnds.org/ trafﬁc-cameras-interactions-police/ , (Accessed on 04/28/2023). [91] N. A. W. .",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "[91] N. A. W. . Sun, “Us solar insolation maps,”  https://www.solar-electric. com/learning-center/solar-insolation-maps.html/#Map1 , (Accessed on 11/21/2022).",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "com/learning-center/solar-insolation-maps.html/#Map1 , (Accessed on 11/21/2022). [92] “Surveillance camera statistics: which cities have the most cctv \ncameras?” https://www.comparitech.com/vpn-privacy/the-worlds- most-surveilled-cities/ , (Accessed on 11/21/2022). [93] Synopsys, “Design compiler,” https://www.synopsys.com/ implementation-and-signoff/rtl-synthesis-test/dc-ultra.html , (Accessed on 04/28/2023).",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "[93] Synopsys, “Design compiler,” https://www.synopsys.com/ implementation-and-signoff/rtl-synthesis-test/dc-ultra.html , (Accessed on 04/28/2023). [94] Synopsys, “Standard cell libraries,”  https://www.synopsys.com/dw/ \nipdir.php?ds=dwc standard cell , (Accessed on 04/28/2023). [95] E. Talpes, D. Williams, and D. D. Sarma, “Dojo: The microarchitecture \nof tesla’s exa-scale computer,” in  2022 IEEE Hot Chips 34 Symposium (HCS) .",
      "type": "sliding_window",
      "tokens": 172
    },
    {
      "text": "[95] E. Talpes, D. Williams, and D. D. Sarma, “Dojo: The microarchitecture \nof tesla’s exa-scale computer,” in  2022 IEEE Hot Chips 34 Symposium (HCS) . IEEE Computer Society, 2022, pp. 1–28.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "1–28. [96] A. Technologies, “Aeromine technologies,” https://www.",
      "type": "sliding_window",
      "tokens": 23
    },
    {
      "text": "Technologies, “Aeromine technologies,” https://www. aerominetechnologies.com/ , (Accessed on 04/28/2023). [97] Urban Trafﬁc Dataset, “https://github.com/edge-video- services/ekya#urban-trafﬁc-dataset.” [98] O. Wayman, “How urban mobility will change by 2030,” https://www.oliverwyman.com/our-expertise/insights/2022/jun/how- urban-mobility-will-change-by-2030.html , 2022, (Accessed on 08/04/2023).",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "[97] Urban Trafﬁc Dataset, “https://github.com/edge-video- services/ekya#urban-trafﬁc-dataset.” [98] O. Wayman, “How urban mobility will change by 2030,” https://www.oliverwyman.com/our-expertise/insights/2022/jun/how- urban-mobility-will-change-by-2030.html , 2022, (Accessed on 08/04/2023). [99] P. with Code, “Object detection on coco test-dev,” https://paperswithcode.com/sota/object-detection-on-coco , (Accessed on 11/21/2022). [100] www.dlapiperdataprotection.com, “Sweden data collection & process- \ning,”  https://www.dlapiperdataprotection.com/index.html?t=collection- and-processing&c=SE , (Accessed on 11/21/2022).",
      "type": "sliding_window",
      "tokens": 233
    },
    {
      "text": "[100] www.dlapiperdataprotection.com, “Sweden data collection & process- \ning,”  https://www.dlapiperdataprotection.com/index.html?t=collection- and-processing&c=SE , (Accessed on 11/21/2022). [101] J. Xu and X. Wang, “Rethinking self-supervised correspondence learn- \ning: A video frame-level similarity perspective,” in  Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. 10 075–10 085.",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "10 075–10 085. [102] T.-J. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convo- \nlutional neural networks using energy-aware pruning,” in  Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, pp.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convo- \nlutional neural networks using energy-aware pruning,” in  Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, pp. 5687–5695. [103] T.-J.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "[103] T.-J. Yang, A. Howard, B. Chen, X. Zhang, A. Go, M. Sandler, V. Sze, \nand H. Adam, “Netadapt: Platform-aware neural network adaptation for mobile applications,” in  Proceedings of the European Conference on Computer Vision (ECCV) , 2018, pp.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Go, M. Sandler, V. Sze, \nand H. Adam, “Netadapt: Platform-aware neural network adaptation for mobile applications,” in  Proceedings of the European Conference on Computer Vision (ECCV) , 2018, pp. 285–300. [104] C.-H.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "[104] C.-H. Yen, H. R. Mendis, T.-W. Kuo, and P.-C. Hsiu, “Stateful neural \networks for intermittent systems,”  IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems , vol. 41, no.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "4229–4240, 2022. [105] Z. Ying, S. Zhao, H. Zhang, C. S. Mishra, S. Bhuyan, M. T. Kandemir, \nA. Sivasubramaniam, and C. R. Das, “Exploiting frame similarity for efﬁcient inference on edge devices,” in  2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS) . IEEE, 2022, pp.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "IEEE, 2022, pp. 1073–1084. [106] R. Zhang, H. Tao, L. Wu, and Y. Guan, “Transfer learning with neural \networks for bearing fault diagnosis in changing working conditions,” Ieee Access , vol.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "[106] R. Zhang, H. Tao, L. Wu, and Y. Guan, “Transfer learning with neural \networks for bearing fault diagnosis in changing working conditions,” Ieee Access , vol. 5, pp. 14 347–14 357, 2017.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "14 347–14 357, 2017. 906 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. [107] S. Zhao, H. Zhang, S. Bhuyan, C. S. Mishra, Z. Ying, M. T. Kandemir, \nA. Sivasubramaniam, and C. R. Das, “D´eja view: Spatio-temporal compute reuse for ‘energy-efﬁcient 360 vr video streaming,” in  2020 ACM/IEEE 47th Annual International Symposium on Computer Archi- tecture (ISCA) .",
      "type": "sliding_window",
      "tokens": 141
    },
    {
      "text": "[107] S. Zhao, H. Zhang, S. Bhuyan, C. S. Mishra, Z. Ying, M. T. Kandemir, \nA. Sivasubramaniam, and C. R. Das, “D´eja view: Spatio-temporal compute reuse for ‘energy-efﬁcient 360 vr video streaming,” in  2020 ACM/IEEE 47th Annual International Symposium on Computer Archi- tecture (ISCA) . IEEE, 2020, pp. 241–253.",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "241–253. [108] S. Zhao, H. Zhang, C. S. Mishra, S. Bhuyan, Z. Ying, M. T. Kandemir, \nA. Sivasubramaniam, and C. Das, “Holoar: On-the-ﬂy optimization of \n3d holographic processing for augmented reality,” in  MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture , 2021, pp. 494–506.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "494–506. [109] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable \narchitectures for scalable image recognition,” in  Proceedings of the IEEE conference on computer vision and pattern recognition , 2018, pp. 8697–8710.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "8697–8710. 907 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "Consequently, for edge servers to provide cloud-comparable quality, they must also perform continuous learning to mitigate this drift. However, due to their resource constraints, these servers often employ compressed models, which are typically prone to data drift. Abstract —Edge servers have recently become very popular for performing localized analytics, especially on video, as they reduce data trafﬁc and protect privacy.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "To address these challenges, we propose Us. Consequently, for edge servers to provide cloud-comparable quality, they must also perform continuous learning to mitigate this drift. However, at expected deployment scales, performing continuous training on every edge server is not sustainable due to their aggregate power demands on grid supply and associated sustainability footprints.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "Our evaluation of Us. To address these challenges, we propose Us. ´as, an ap- proach combining algorithmic adjustments, hardware-software co-design, and morphable acceleration hardware to enable the training of workloads on these edge servers to be powered by re- newable, but intermittent, solar power that can sustainably scale alongside data sources.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "´as on a real-world trafﬁc dataset indicates that our continuous learning approach simultaneously improves both accuracy and efﬁciency: Us. ´as offers a 4.96% greater mean accuracy than prior approaches while our morphable accelerator that adapts to solar variance can save up to  { 234.95kWH, 2.63MWH } /year/edge-server compared to a  { DNN accelerator, data center scale GPU } , respectively. Our evaluation of Us.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "´as offers a 4.96% greater mean accuracy than prior approaches while our morphable accelerator that adapts to solar variance can save up to  { 234.95kWH, 2.63MWH } /year/edge-server compared to a  { DNN accelerator, data center scale GPU } , respectively. I. I NTRODUCTION \nThe rampant growth, and anticipated sustained expansion of data collection and consumption are currently driving data-driven analytics using trained inference models, with signiﬁcant economic impact. Amidst the myriad of data-driven domains, urban mobility, smart cities, autonomous driving, and the Internet of Things (IoT) emerge as some of the most rapidly expanding ﬁelds contributing to the global economy, amounting to more than 4 trillion US dollars [ 1 ], [ 54 ], [ 76 ], [ 98 ].",
      "type": "sliding_window_shuffled",
      "tokens": 188,
      "augmented": true
    },
    {
      "text": "These statistics underscore the profound signiﬁcance and transformative potential of these data-driven realms, delineat- ing their pivotal role in shaping the landscape of computing technology, from algorithms to architecture. What distinguishes these data is their diverse origin, span- ning from IoT devices to wearables, and their acquisition from challenging environments, including autonomous driving and urban mobility scenarios. Amidst the myriad of data-driven domains, urban mobility, smart cities, autonomous driving, and the Internet of Things (IoT) emerge as some of the most rapidly expanding ﬁelds contributing to the global economy, amounting to more than 4 trillion US dollars [ 1 ], [ 54 ], [ 76 ], [ 98 ].",
      "type": "sliding_window_shuffled",
      "tokens": 156,
      "augmented": true
    },
    {
      "text": "Consequently, they frequently ex- hibit a phenomenon known as “data drift”, where the incoming data deviates from the distribution of the originally trained model, leading to degradation in inference accuracy. What distinguishes these data is their diverse origin, span- ning from IoT devices to wearables, and their acquisition from challenging environments, including autonomous driving and urban mobility scenarios. Mitigating Data Drift:  Dealing with data drift in edge com- pute nodes presents a signiﬁcant challenge.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "While larger models with more parameters may exhibit limited data drift due to their increased capacity to generalize, deploying such large models on edge compute nodes can be difﬁcult due to \ninherent limitations in form factor, energy efﬁciency, thermal constraints, and compute resources. To accommodate these constraints, it is a common practice to employ compressed Deep Neural Network (DNN) models, that are quantized, distilled, or otherwise reduced in size. Mitigating Data Drift:  Dealing with data drift in edge com- pute nodes presents a signiﬁcant challenge.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "To accommodate these constraints, it is a common practice to employ compressed Deep Neural Network (DNN) models, that are quantized, distilled, or otherwise reduced in size. Traditionally, data drift has been handled by cloud-based periodic re-training using continuous learning algorithms [ 20 ], [ 74 ]. However, while com- pressed models are essential for meeting resource limitations, they are more sensitive to data drift because they may not generalize as effectively.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "Traditionally, data drift has been handled by cloud-based periodic re-training using continuous learning algorithms [ 20 ], [ 74 ]. As these applications become more ubiquitous, partic- ularly in urban deployments for tasks like trafﬁc surveillance, autonomous driving, and health analytics [ 18 ], [ 77 ], [ 90 ], demands on communication bandwidth and network reliability limit the direct streaming of diverse data (e.g., video, 3D point cloud, sensor, voice) from numerous sensor-compute nodes to the cloud. However, there are challenges in resources, privacy, and sustainability to utilize existing techniques at envisioned scales.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "As a result, “on-premise” edge servers  [ 7 ], [ 8 ] have become prime choices for local inference and prediction [ 6 ], [ 39 ], [ 85 ], [ 86 ], necessitating the handling of both learning and inference tasks to meet application needs, including privacy preservation, reduced data communication, and disaggregated computing. As these applications become more ubiquitous, partic- ularly in urban deployments for tasks like trafﬁc surveillance, autonomous driving, and health analytics [ 18 ], [ 77 ], [ 90 ], demands on communication bandwidth and network reliability limit the direct streaming of diverse data (e.g., video, 3D point cloud, sensor, voice) from numerous sensor-compute nodes to the cloud. Moreover, recent changes in privacy regulations across multiple countries [ 2 ], [ 100 ] call for preserving the privacy of citizens [ 12 ] and may preclude streaming personal data to third-party cloud services.",
      "type": "sliding_window_shuffled",
      "tokens": 213,
      "augmented": true
    },
    {
      "text": "The Problem Space:  To address the multi-faceted challenges of sustainable, scalable and privacy-preserving continuous learning at edge servers, several crucial problem spaces must be explored. Finally, although recent studies have suggested co-locating training and inference [ 12 ] to tackle privacy concerns without signiﬁcantly affecting the inference service, the power demand associated with equipping multiple commercial edge servers [ 7 ], [ 8 ] for both tasks hinders sustainable scaling. As a result, “on-premise” edge servers  [ 7 ], [ 8 ] have become prime choices for local inference and prediction [ 6 ], [ 39 ], [ 85 ], [ 86 ], necessitating the handling of both learning and inference tasks to meet application needs, including privacy preservation, reduced data communication, and disaggregated computing.",
      "type": "sliding_window_shuffled",
      "tokens": 175,
      "augmented": true
    },
    {
      "text": "Firstly, the issue of  (non-)supervision  arises, demanding the ability to label data without human interven- tion to preserve privacy during the learning process. The Problem Space:  To address the multi-faceted challenges of sustainable, scalable and privacy-preserving continuous learning at edge servers, several crucial problem spaces must be explored. While recent works [ 12 ], [ 46 ] have attempted to tackle this concern through student-teacher paradigms, efﬁciently deploying such approaches in complex data modalities (e.g., multi-class video, 3D point cloud) remains a formidable challenge.",
      "type": "sliding_window_shuffled",
      "tokens": 128,
      "augmented": true
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Ensuring adherence to Service Level Agreements (SLAs), where in- ference typically utilizes a lower-resource model [ 51 ], [ 75 ] and labeling is performed using a larger teacher model [ 44 ], \n891 \n2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA) \n2378-203X/24/$31.00 ©2024 IEEE DOI 10.1109/HPCA57654.2024.00073 \n2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA) | 979-8-3503-9313-2/24/$31.00 ©2024 IEEE | DOI: 10.1109/HPCA57654.2024.00073 \nAuthorized licensed use limited to: Penn State University. While recent works [ 12 ], [ 46 ] have attempted to tackle this concern through student-teacher paradigms, efﬁciently deploying such approaches in complex data modalities (e.g., multi-class video, 3D point cloud) remains a formidable challenge.",
      "type": "sliding_window_shuffled",
      "tokens": 249,
      "augmented": true
    },
    {
      "text": "[ 50 ], [ 73 ] at a much lower rate, necessitates informed decisions regarding deployment placement and sampling rates. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "Secondly, the issue of  functionality  comes to the fore, requiring effective continuous learning from often non- Independently and Identically Distributed (non-IID) data. [ 50 ], [ 73 ] at a much lower rate, necessitates informed decisions regarding deployment placement and sampling rates. Such non-IID data distributions, evident in tasks like standard trafﬁc monitoring with varied class observations (e.g., more cars than buses, all frames having  STOP  signs), may introduce sampling bias  [ 70 ], [ 74 ] in the network.",
      "type": "sliding_window_shuffled",
      "tokens": 126,
      "augmented": true
    },
    {
      "text": "This challenge can be addressed through proper  exemplar selection  algorithms employing representation learning techniques [ 31 ], [ 74 ], ca- pable of learning new classes in real-time. However, these compute-intensive algorithms can be optimized further through dedicated hardware acceleration. Such non-IID data distributions, evident in tasks like standard trafﬁc monitoring with varied class observations (e.g., more cars than buses, all frames having  STOP  signs), may introduce sampling bias  [ 70 ], [ 74 ] in the network.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "However, these compute-intensive algorithms can be optimized further through dedicated hardware acceleration. Designing a learning platform that can adapt to intermittent renewable energy sources (e.g., solar power) and maintain a minimal operational carbon footprint [ 29 ] is paramount. Thirdly, the aspect of  sustainability  poses a critical question of deploying such systems, ideally with minimal reliance on the power grid for learning tasks.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "Designing a learning platform that can adapt to intermittent renewable energy sources (e.g., solar power) and maintain a minimal operational carbon footprint [ 29 ] is paramount. Such a platform should continuously make progress on unsupervised labeling, exem- plar building, and continuous learning, and maximize  drift mitigation  while minimizing power consumption. Moreover, the system must accommodate  support for intermittency inherent in sustainable power sources like solar and wind.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "Moreover, the system must accommodate  support for intermittency inherent in sustainable power sources like solar and wind. While incorporating conventional battery storage can mitigate intermittency, it introduces environmental and sustainability challenges associated with resource extraction, production, and replacement [ 3 ], [ 5 ], [ 10 ], [ 13 ], [ 53 ], [ 66 ], [ 69 ]. An ideal solution would entail a battery-free system ( not  energy storage- free, i.e., still with some capacitive storage), circumventing these concerns and aligning with the objectives of sustainable and reliable continuous learning at the edge.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "To these ends, we propose Us.´as,   1   a HW-SW co-design approach to building sustainable, scalable, drift-mitigating edge analytics platforms using harvested power to support continuous learning. Us.´as, unlike prior edge-focused analytics approaches (e.g., Ekya [ 12 ]), detaches the inference and train- ing hardware, as the training task is the major source of the compute, power, and time consumption. An ideal solution would entail a battery-free system ( not  energy storage- free, i.e., still with some capacitive storage), circumventing these concerns and aligning with the objectives of sustainable and reliable continuous learning at the edge.",
      "type": "sliding_window_shuffled",
      "tokens": 162,
      "augmented": true
    },
    {
      "text": "Us.´as also employs a dynamically morphable systolic array for enabling energy- efﬁcient computing within the harvested power envelope. Us.´as, unlike prior edge-focused analytics approaches (e.g., Ekya [ 12 ]), detaches the inference and train- ing hardware, as the training task is the major source of the compute, power, and time consumption. Us.´as introduces an al- gorithmic framework for data labeling using a teacher-student model, designing the exemplar selection using representation learning and determining the right set of hyperparameters using micro proﬁling to energy-efﬁciently continuously train the DNNs with the selected exemplar sets.",
      "type": "sliding_window_shuffled",
      "tokens": 165,
      "augmented": true
    },
    {
      "text": "Key contributions  of the work include: •  We propose algorithmic enhancements of  continuous learn- \ning  for mitigating data drift and design a  student-teacher based automated data labelling algorithm , to prepare train- ing exemplars from input data. We use a two-level data annotation mechanism: exemplar identiﬁcation based on the \n1 Vedic goddess of dawn in Hinduism [ 36 ]; emphasizing the dawn of sustainable continuous learning and signiﬁcance of solar power in our design. Us.´as also employs a dynamically morphable systolic array for enabling energy- efﬁcient computing within the harvested power envelope.",
      "type": "sliding_window_shuffled",
      "tokens": 143,
      "augmented": true
    },
    {
      "text": "We use a two-level data annotation mechanism: exemplar identiﬁcation based on the \n1 Vedic goddess of dawn in Hinduism [ 36 ]; emphasizing the dawn of sustainable continuous learning and signiﬁcance of solar power in our design. Our policy updates  both  the teacher and student models for robust unsupervised learning. conﬁdence matrix of the student model, followed by a repre- sentation learning based exemplar selection by ensembling multiple teacher models.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Our policy updates  both  the teacher and student models for robust unsupervised learning. •  We implement a  micro-proﬁler , which predicts the right set \nof hyper-parameters to efﬁciently perform the training tasks on an energy-harvesting edge server while operating within its power budget and minimizing data drift. •  We design a  morphable hardware accelerator  that efﬁ- \nciently maps training tasks, is suitable for intermittent computing, and can adapt its capabilities to reduce power emergencies without devolving to grid operation.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "We dis- cuss how the proposed hardware techniques can be adapted by many of the current DNN training accelerators to add similar dynamism in sustainability-sensitive environments. •  Finally, we evaluate Us.´as in depth on a  real-world trafﬁc \ndata set  [ 97 ] and perform sensitivity studies on other classes (audio, IMU) of data. •  We design a  morphable hardware accelerator  that efﬁ- \nciently maps training tasks, is suitable for intermittent computing, and can adapt its capabilities to reduce power emergencies without devolving to grid operation.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "•  Finally, we evaluate Us.´as in depth on a  real-world trafﬁc \ndata set  [ 97 ] and perform sensitivity studies on other classes (audio, IMU) of data. Power estimations of our hardware design, modeled by Design Compiler [ 93 ], indicate that the proposed morphable accelerator approach can save up to 234.95kWH/year/edge-server, compared to running continuous learning on a state of the art DNN accelerator and 2.63MWH/year/edge-server, compared to utilizing a datacenter-scale GPU for learning on the edge. Our algorithmic framework for performing continuous learning has a 4.96% greater mean accuracy than a na¨ıve continuous learner.",
      "type": "sliding_window_shuffled",
      "tokens": 166,
      "augmented": true
    },
    {
      "text": "Power estimations of our hardware design, modeled by Design Compiler [ 93 ], indicate that the proposed morphable accelerator approach can save up to 234.95kWH/year/edge-server, compared to running continuous learning on a state of the art DNN accelerator and 2.63MWH/year/edge-server, compared to utilizing a datacenter-scale GPU for learning on the edge. B ACKGROUND AND  M OTIVATION \nEdge servers often leverage the convenience and ﬂexibil- ity of cloud interfaces, granting access to the same APIs, tools, and functionalities [ 60 ]. II.",
      "type": "sliding_window_shuffled",
      "tokens": 140,
      "augmented": true
    },
    {
      "text": "These tailored models enable accurate inference with high throughput and reduced resource footprint, with some compressed models having approximately 50 ×  fewer parameters [ 30 ], but with a greater susceptibility to data drift [ 42 ], [ 55 ]. However, due to their inherent limitations in resources, such as weak GPUs and smaller memory capacities [ 83 ], these servers often resort to “cus- tomized” analytics services to maximize throughput and meet SLAs, including specialized DNN models tailored for edge deployments [ 51 ], [ 75 ], which are compressed, quantized, and optimized for the targeted hardware [ 30 ], [ 103 ], [ 109 ]. B ACKGROUND AND  M OTIVATION \nEdge servers often leverage the convenience and ﬂexibil- ity of cloud interfaces, granting access to the same APIs, tools, and functionalities [ 60 ].",
      "type": "sliding_window_shuffled",
      "tokens": 193,
      "augmented": true
    },
    {
      "text": "These tailored models enable accurate inference with high throughput and reduced resource footprint, with some compressed models having approximately 50 ×  fewer parameters [ 30 ], but with a greater susceptibility to data drift [ 42 ], [ 55 ]. Data drift emerges as a signiﬁcant concern in real-world systems as the live data diverges from the original training data, and the environment undergoes rapid changes [ 12 ]. Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "1  depicts our experimental investigations on data drift, encompassing training and testing multiple DNNs on diverse datasets such as Urban Trafﬁc [ 12 ], [ 97 ], 3D Point Cloud [ 14 ], [ 24 ], and audio [ 78 ]. The similar trends across these results highlight the impact of varying time windows and encounter- ing diverse scene changes, leading to degradation in network accuracy by up to 30%. Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Continuous Learning at the Edge: Continuous learning, wherein the model continually learns from new samples over time, adapting to seen and previously unseen classes, has \n892 \nAuthorized licensed use limited to: Penn State University. These ﬁndings underscore the critical challenge posed by data drift and the need for continuous learning on edge servers. The similar trends across these results highlight the impact of varying time windows and encounter- ing diverse scene changes, leading to degradation in network accuracy by up to 30%.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Continuous Learning at the Edge: Continuous learning, wherein the model continually learns from new samples over time, adapting to seen and previously unseen classes, has \n892 \nAuthorized licensed use limited to: Penn State University. Restrictions apply. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "30 35 40 45 50 55 60 65 70 75 80 \nBaseline \nWin-1 \nWin-2 \nWin-3 \nWin-4 \nWin-5 \nBaseline \nWin-1 \nWin-2 \nWin-3 \nWin-4 \nWin-5 \nBaseline \nWin-1 \nWin-2 \nWin-3 \nWin-4 \nWin-5 \nVideo Audio 3DPC \nAccuracy (%) \nSM SMR LM \nFig. Restrictions apply. 1: Data drift on different data modalities.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "1: Data drift on different data modalities. Sampling window size: 4hours for video, 20 minutes for audio for urban trafﬁc video and audio data. 1hour for 3D Point Cloud simulated data.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "[SM:Small Model (smaller model or larger model pruned and quantized using energy aware pruning [ 102 ] and NetAdapat [ 103 ]), LM:Large Model (no pruning or quan- tization), SMR:Small Model with Retraining]. 1hour for 3D Point Cloud simulated data. emerged as a preferred approach to mitigate data drift [ 20 ], [ 44 ], [ 50 ], [ 74 ].",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "Although, multiple task-dedicated models are typically deployed to enhance accuracy and reduce sampling bias [ 70 ], particularly in scenarios like trafﬁc monitoring, where different time periods exhibit distinct trafﬁc patterns, they are not immune to data drift. The temporal locality of (video like) data has shown models to effectively learn from recent data. emerged as a preferred approach to mitigate data drift [ 20 ], [ 44 ], [ 50 ], [ 74 ].",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "1 , our experiments, on different modalities, shows the accuracy degradation due to data drift. As depicted in Fig. Although, multiple task-dedicated models are typically deployed to enhance accuracy and reduce sampling bias [ 70 ], particularly in scenarios like trafﬁc monitoring, where different time periods exhibit distinct trafﬁc patterns, they are not immune to data drift.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "1 , our experiments, on different modalities, shows the accuracy degradation due to data drift. However, with a proper retraining, the smaller model could keep up with the original accuracy. Speciﬁcally focuing on video data, we observe that: using quantized MobileNet-v2 (14M paramters, 71.3% accuracy) as the small model and ResNet-101 (171M parameters, 76.4% accuracy) as the large model, the accuracy of the smaller model has degraded  >  20% over 5 sampling windows (of 4 hours each), where as the effect is minimal in the larger model.",
      "type": "sliding_window_shuffled",
      "tokens": 135,
      "augmented": true
    },
    {
      "text": "We also observe a similar trend over other modalities, making the importance of continuous learning clear for multiple domains. However, in a continuous learning paradigm, training be- comes an essential, repeatedly scheduled task whose computa- tional and time costs cannot be considered a one-time overhead freely delegated to the cloud. However, with a proper retraining, the smaller model could keep up with the original accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "However, in a continuous learning paradigm, training be- comes an essential, repeatedly scheduled task whose computa- tional and time costs cannot be considered a one-time overhead freely delegated to the cloud. A recent work, Ekya [ 12 ], has demonstrated that edge servers equipped with GPUs are capable of performing the necessary tasks for continuous learning within their form-factor-imposed resource constraints, provided that those resources are intelligently managed. Sustainable  Continuous Learning at the Edge:  Even given such advancements in continuous learning on edge servers, provisioning training resources at the edge for every sensing- to-analytics application entails sustainability questions.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "A standard offering with 2 × g4dn.12xlarge instances need 4kW power [ 7 ] (the compute units have a TDP of  ≈ 1 kW  [ 65 ], [ 71 ]) for \nperforming analytics. Sustainable  Continuous Learning at the Edge:  Even given such advancements in continuous learning on edge servers, provisioning training resources at the edge for every sensing- to-analytics application entails sustainability questions. For ex- ample, a popular AWS outpost, a g4dn.12xlarge instance [ 83 ], consists of a 24 core Intel Xeon CPU (150W TDP) [ 71 ] with 192GB of memory and 4 NVIDIA T4 (with tensor cores, 70W TDP) [ 65 ] with 64GB GPU memory.",
      "type": "sliding_window_shuffled",
      "tokens": 191,
      "augmented": true
    },
    {
      "text": "A standard offering with 2 × g4dn.12xlarge instances need 4kW power [ 7 ] (the compute units have a TDP of  ≈ 1 kW  [ 65 ], [ 71 ]) for \nperforming analytics. Scaling this to crowded cities with 30- 50+kilo-cameras like Beverly Hills ( >  35 k  [ 11 ]), Los Angeles ( ≈ 35 k  [ 92 ]), New York ( ≈ 56k [ 92 ]), or Chicago ( ≈ 30k) will need a lot of power. With state-of-the-art learning APIs [ 60 ] and intelligent co-location and scheduling of inference and continuous learning [ 12 ], these edge servers can support about 8 videos streams [ 12 ], resulting in  ≈ 120 W  (just for compute) per video stream.",
      "type": "sliding_window_shuffled",
      "tokens": 190,
      "augmented": true
    },
    {
      "text": "Clearly, the current solution is  not  sustainable, neither in terms of the load on the power grid, nor in terms of the  CO 2  footprint (1.1 × 10 9 lbs); reducing the power budget for continuous learning is essential, as the carbon footprint of DNN training has emerged as a prominent concern [ 21 ], [ 57 ], [ 67 ], [ 89 ], demanding careful consideration as a primary design metric. In fact, it will take  ≥ 3Million cameras (assuming  ≈ 9 cameras/1000 people, similar to LA, and scaled to US population) to just enable autonomous urban mobility in the USA, which may consume 360 MW  power (1296 GWh  energy, 0.03% of US power) for video analytics alone. Scaling this to crowded cities with 30- 50+kilo-cameras like Beverly Hills ( >  35 k  [ 11 ]), Los Angeles ( ≈ 35 k  [ 92 ]), New York ( ≈ 56k [ 92 ]), or Chicago ( ≈ 30k) will need a lot of power.",
      "type": "sliding_window_shuffled",
      "tokens": 238,
      "augmented": true
    },
    {
      "text": "Clearly, the current solution is  not  sustainable, neither in terms of the load on the power grid, nor in terms of the  CO 2  footprint (1.1 × 10 9 lbs); reducing the power budget for continuous learning is essential, as the carbon footprint of DNN training has emerged as a prominent concern [ 21 ], [ 57 ], [ 67 ], [ 89 ], demanding careful consideration as a primary design metric. Similarly, other applications with diverse data modalities, such as LiDAR and Camera for autonomous driving, IMU, bio-sensors, and Speech for IoT, face similar issues. Although green data centers [ 58 ], [ 59 ] provide partial mit- igation, they fail to address data privacy and communication bandwidth challenges in the current context.",
      "type": "sliding_window_shuffled",
      "tokens": 175,
      "augmented": true
    },
    {
      "text": "Exploiting Intermittent Computing:  An obvious solution to the power problem is to run the training in a self-sustained way, i.e., without depending on the power grid and by relying on a renewable energy source like solar power; opportunities for harvesting renewables naturally scale alongside a greater number of deployment locations and solar power, even though not always available, is in abundance. Thus,  attaining a sustain- able solution for privacy-preserving, distributed continuous learning remains an ongoing pursuit. Similarly, other applications with diverse data modalities, such as LiDAR and Camera for autonomous driving, IMU, bio-sensors, and Speech for IoT, face similar issues.",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "Exploiting Intermittent Computing:  An obvious solution to the power problem is to run the training in a self-sustained way, i.e., without depending on the power grid and by relying on a renewable energy source like solar power; opportunities for harvesting renewables naturally scale alongside a greater number of deployment locations and solar power, even though not always available, is in abundance. Furthermore, solar power has reasonably predictability characteristics. In the United States, a typical 12% efﬁcient solar panel [ 91 ], can provide an annual average of 50 W / m 2   − 150 W / m 2   of power [ 64 ].",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "Therefore,  designing a training platform to perform continuous learning with the intermittent solar power and within the typical harvested budget  would be the best solution. Typ- ically, inference tasks have signiﬁcantly less compute time and power requirement, and commercial off the shelf devices, like edgeTPU [ 19 ] can perform object detection using the aforementioned compressed models at a reasonable frame rate (at times  ≥ 71  f ps ). Furthermore, solar power has reasonably predictability characteristics.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Our Approach (and its Novelty):  Us. The power sustainability consequently reduces the cost of deployment as the publicly available edge server, like AWS outpost offering (one of the cheaper and lower power consuming ones) for performing edge inference costs $5,134.92/month [ 83 ]. Therefore,  designing a training platform to perform continuous learning with the intermittent solar power and within the typical harvested budget  would be the best solution.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "Battery-Free Operation:  A key highlight of  Us. ´as  introduces several novel contributions in the domain of  sustainable  continuous learning at edge servers using harvested energy, setting it apart from prior works examining on-edge learning. Our Approach (and its Novelty):  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "The scaling up via mil- \n893 \nAuthorized licensed use limited to: Penn State University. ´as  lies in its battery-free operation, which aligns with the current global push for sustainable computing. Battery-Free Operation:  A key highlight of  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Restrictions apply. The scaling up via mil- \n893 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "lions of additional battery-supported analytics platforms would introduce severe environmental challenges due to resource extraction, production, and replacement of batteries [ 3 ], [ 5 ], [ 10 ], [ 13 ], [ 53 ], [ 66 ], [ 69 ]. By demonstrating the viability of a battery-less edge server for video analytics, Us.´as spearheads the adoption of similarly sustainable systems for other do- mains. Restrictions apply.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Algorithmic Advancements:  Us. By demonstrating the viability of a battery-less edge server for video analytics, Us.´as spearheads the adoption of similarly sustainable systems for other do- mains. While the initial scope is limited to urban mobility applications, the concept’s adaptability extends to various domains, including autonomous driving, smart industries, and remote sensing: Section  V-D  performs an initial exploration of how techniques from Us.´as will apply to other domains.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "Prior works relied on supervised learning or K-means clustering, un- suitable for  Us. Algorithmic Advancements:  Us. ´as  extends the frontier of rep- resentation learning for continuous learning by implementing it at a large scale and addressing related challenges.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "To overcome these limitations,  Us. Prior works relied on supervised learning or K-means clustering, un- suitable for  Us. ´as  due to its need for unsupervised data annota- tion and the inability to handle large-scale datasets with numer- ous classes.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "A hierarchical K-means+ (or DBSCAN) clustering approach learns representations for exemplar se- lection. ´as  employs an ensembled teacher-student method, wherein multiple teachers annotate student data. To overcome these limitations,  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "A hierarchical K-means+ (or DBSCAN) clustering approach learns representations for exemplar se- lection. Additionally, a novel power-aware micro-proﬁling policy is adapted to determine optimal hyper-parameters for a variable-power environment. The robust exemplar selection and micro-proﬁling mechanisms are discussed and evaluated in § III-B  and § III-C , respectively.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "While previous works have de- signed energy-efﬁcient training hardware with support for variable precision training, none have adapted to variable energy income. Hardware Innovation:  Us.´as embraces the intermittency en- tailed by harvesting and advocates for hardware adaptation (resizing) to efﬁciently manage variable power income and avoid power emergencies. The robust exemplar selection and micro-proﬁling mechanisms are discussed and evaluated in § III-B  and § III-C , respectively.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "While previous works have de- signed energy-efﬁcient training hardware with support for variable precision training, none have adapted to variable energy income. Us. ´as  optimizes the entire solution space, maximizing hardware reuse for exemplar selection and micro- proﬁling while addressing the training task.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "The system can turn off individual compute-tiles to accommodate runtime power variability (see § IV-B ) and enable seamless operation during power reductions. Overall, Us.´as demonstrates the viability of sustainable con- tinuous learning at edge servers, encompassing advancements in energy harvesting, algorithmic techniques, and hardware adaptation. ´as  optimizes the entire solution space, maximizing hardware reuse for exemplar selection and micro- proﬁling while addressing the training task.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "C ONTINUOUS  L EARNING \nThe ﬁrst step to any data-driven learning algorithm is data collection and annotation. III. Overall, Us.´as demonstrates the viability of sustainable con- tinuous learning at edge servers, encompassing advancements in energy harvesting, algorithmic techniques, and hardware adaptation.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "Since  Us. ´as  is a continuous learning framework and learns from the live data that the camera(s) capture, data collection is simply storing the live video feed. C ONTINUOUS  L EARNING \nThe ﬁrst step to any data-driven learning algorithm is data collection and annotation.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "However, data annotation or labeling is more challenging. Classically, once data is collected, it is classiﬁed, labeled, and bounded by borders (bounding box) mostly using manual labor (at times with software assistance) or crowd sourcing [ 33 ], [ 82 ], [ 88 ]. ´as  is a continuous learning framework and learns from the live data that the camera(s) capture, data collection is simply storing the live video feed.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "Therefore, we adapt a “student-teacher paradigm” [ 46 ], where a more \nClassify \nLow Conf \nFrame? This requires the data to be present at a central location for manual inspection, both of which are not possible because of communication and privacy constraints. Classically, once data is collected, it is classiﬁed, labeled, and bounded by borders (bounding box) mostly using manual labor (at times with software assistance) or crowd sourcing [ 33 ], [ 82 ], [ 88 ].",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "Therefore, we adapt a “student-teacher paradigm” [ 46 ], where a more \nClassify \nLow Conf \nFrame? Discard \nFrame No \nExemplar Yes \nYes \nEdge Model \nConfidence Matrix \nMajority Voting Labeling \nM1 \nM2 \nM3 \nFig. 2: Auto-labeling in  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "2: Auto-labeling in  Us. general, robust and larger model (typically with hundreds of millions of parameters [ 43 ], [ 99 ]) helps in annotating the data. ´as : Select frames only with low conﬁdence as they might contain potentially new information, and use ensemble learning to improve the labeling.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "general, robust and larger model (typically with hundreds of millions of parameters [ 43 ], [ 99 ]) helps in annotating the data. There has been a signiﬁcant body of work on frame similarity and saliency [ 45 ], [ 84 ], [ 101 ], [ 105 ], [ 107 ], [ 108 ], and those details remain beyond the scope of this work. However, because of the heavy compute requirements, the teacher model runs with a much slower frame rate and annotates only some (important) frames.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Data Annotation \nPicking the Important Ones:  Typically, edge models are ca- pable of inferring at the frame rate of the camera (at times, 30fps to 60fps) [ 19 ]. A. There has been a signiﬁcant body of work on frame similarity and saliency [ 45 ], [ 84 ], [ 101 ], [ 105 ], [ 107 ], [ 108 ], and those details remain beyond the scope of this work.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "Data Annotation \nPicking the Important Ones:  Typically, edge models are ca- pable of inferring at the frame rate of the camera (at times, 30fps to 60fps) [ 19 ]. However, the teacher model used to label the incoming data cannot match this in a resource- constrained environment where performing training is going to be even more resource consuming. Therefore, we employ an intelligent “data sampling mechanism” to select the frames that might contain new information and a potential candidate for learning.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "Fig. Therefore, we employ an intelligent “data sampling mechanism” to select the frames that might contain new information and a potential candidate for learning. 2  shows the different components of the student-teacher data annotation model adapted in  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "2  shows the different components of the student-teacher data annotation model adapted in  Us. ´as , where the edge model is the “student” (continuously retrained), and larger models are the “teachers” (the ones teaching the student about what-is-what). The students models are typically optimized for edge, i.e.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "or by developing an application speciﬁc model from scratch along with the said optimizations. with optimizations like quantization, pruning etc. The students models are typically optimized for edge, i.e.",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "These student models, thanks to their lack of robustness (which is often, but not always, related to the smaller footprint they have, and thereby lacking the parameter space to generalize better), are susceptible to data drift and hence are continuously retrained. or by developing an application speciﬁc model from scratch along with the said optimizations. However, the teacher models are typically large, and with a wide parameter space can generalize the learning process better than the students.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "As they are less prone to drift, they need occasional updates. These teacher models are often factory trained. However, the teacher models are typically large, and with a wide parameter space can generalize the learning process better than the students.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "If the student (or the edge model) is conﬁdent about the classiﬁcation (e.g. For each sampled frame, the classiﬁcation results and the conﬁdence matrix (output of the last layer) are sent for annotation. As they are less prone to drift, they need occasional updates.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "If the student (or the edge model) is conﬁdent about the classiﬁcation (e.g. However, if the student is not conﬁdent on the classiﬁcation, the frame is then saved as a potential exemplar (we will further reﬁne this in § III-B ). a clear frame with no new objects, or a frame similar to one of the training samples), then that frame is discarded as it potentially contains little to no new information.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "The potential exemplars \n894 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. However, if the student is not conﬁdent on the classiﬁcation, the frame is then saved as a potential exemplar (we will further reﬁne this in § III-B ).",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "0 20 40 60 80 100 \n0 20 40 60 80 100 \nClass Distribution \nAccuracy (%) \nBaseline Train-Win-1 Train-Win-2 Train-Win-4 Appeared \nFig. Restrictions apply. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "The ”appeared“ line represents the percentage of the frames in which the corresponding class is present, e.g. 3: Distribution of different classes on a typical trafﬁc pattern and the impact of training on the sampling bias. 0 20 40 60 80 100 \n0 20 40 60 80 100 \nClass Distribution \nAccuracy (%) \nBaseline Train-Win-1 Train-Win-2 Train-Win-4 Appeared \nFig.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "The ”appeared“ line represents the percentage of the frames in which the corresponding class is present, e.g. Incorrect exemplar selection might lead to non-IID training data distri- bution, leading to catastrophic forgetting or over-ﬁtting. Fire hydrant, in the taken scene, is present in 100% of the frames.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "Incorrect exemplar selection might lead to non-IID training data distri- bution, leading to catastrophic forgetting or over-ﬁtting. are then further reﬁned and classiﬁed by the teacher models. To improve the conﬁdence of the teacher models, we employ an ensemble learning based weighted majority voting policy [ 28 ].",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "To improve the conﬁdence of the teacher models, we employ an ensemble learning based weighted majority voting policy [ 28 ]. Furthermore, each teacher model has its private conﬁdence matrix on different object classes. Each of the teacher models infers on the exemplar frame.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Furthermore, each teacher model has its private conﬁdence matrix on different object classes. This conﬁdence matrix serves as a weight for performing the ensemble of multiple teachers, and helps exploiting the expertise of each of the teacher models for each of the individual classes, signiﬁcantly boosting the accuracy and robustness of the data annotation. This maximizes the accuracy of the teacher, and consequently minimizes the chance of the student model learning wrong labels.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "The impact of wrong labeling is discussed in § V . Note that the limited parameters of the student make it more sensitive to data ﬁdelity and hence ensuring an accurate data labeling is very important for end to end classiﬁcation accuracy. This maximizes the accuracy of the teacher, and consequently minimizes the chance of the student model learning wrong labels.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "The impact of wrong labeling is discussed in § V . The Problem:  However, this exemplar section mechanism has an inherent ﬂaw. Consider a trafﬁc camera looking at a busy street with a trafﬁc signal.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "Due to the trafﬁc distribution (e.g., more cars than buses), the camera typically sees a varied distribution of different classes, which might reﬂect in the exemplar set. Moreover, some static objects (trafﬁc light, stop sign, etc.) Consider a trafﬁc camera looking at a busy street with a trafﬁc signal.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "Moreover, some static objects (trafﬁc light, stop sign, etc.) This creates a sampling “bias” [ 70 ] while performing the training, and often leads to catastrophic forgetting. might be present in all frames.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "3  shows a typical trafﬁc distribution from Urban Trafﬁc data [ 97 ]) and the impact of sampling bias on class distribution. This creates a sampling “bias” [ 70 ] while performing the training, and often leads to catastrophic forgetting. Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "3  shows a typical trafﬁc distribution from Urban Trafﬁc data [ 97 ]) and the impact of sampling bias on class distribution. Note that, as some of the classes (e.g., bicycles) are barely present in the exemplar, the model tend to lose accuracy (because of catastrophic forgetting) on them, whereas the model rapidly over-ﬁts for the classes with more examples (e.g., trafﬁc light). B.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "Proper Exemplar Selection \nTo tackle the sampling bias [ 70 ], we adapt a representation learning [ 74 ] framework for designing the proper exemplar selection. B. The fundamental issue with the previous approach is the inability to select correct numbers of IID data for training.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "In addition to that, just DNN training cannot learn \new classes if there is no way to annotate and label new classes. The fundamental issue with the previous approach is the inability to select correct numbers of IID data for training. Representation learning solves both these issues.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Representation learning solves both these issues. We achieve this by clustering the feature vector of the Large DNN model. The learner (here the teacher models) need to properly classify the data, learn if the data is a new type of one of the older classes, and identify if it encounters a new class.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "In the original training phase, these feature vectors are separated using K-means [ 74 ] or other clustering. We achieve this by clustering the feature vector of the Large DNN model. Fundamentally, we use the larger DNN models as feature extractors which turn the data into a feature vector.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "The cluster centers for each data ( μ y  for class  y ) are calculated as  μ y  =   1 \nP y   ∑ p ∈ P y   Φ ( p ) , where  P y  is the number of samples belonging to class (or cluster  y ), and  Φ  is the feature extraction function working on the data  p . In the original training phase, these feature vectors are separated using K-means [ 74 ] or other clustering. These clusters represent the classes in the high dimensional feature space.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "These clusters represent the classes in the high dimensional feature space. There are three cases: Case-1:  If the data is close to one of the cluster centers and belongs to its cluster boundary, then it falls into the bucket of that particular class. When the classiﬁer sees new data ( x ), it calculates its distance from all the cluster centers as y ∗ =  min y = 1 ... t  || Φ ( x ) − μ y || .",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "There are three cases: Case-1:  If the data is close to one of the cluster centers and belongs to its cluster boundary, then it falls into the bucket of that particular class. This typically happens if the data are very similar to the training samples. Case-2:  If the data belongs to a known class, but is signif- icantly different from the training samples, it falls not too far from one of the clusters.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Case-2:  If the data belongs to a known class, but is signif- icantly different from the training samples, it falls not too far from one of the clusters. This distance of the new data from the cluster center is called the “distillation loss” [ 74 ]. An encounter of a new example of the existing class is followed by an update to the clustering by minimizing the classiﬁcation loss of the newly-seen data.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "The distance of this feature vector from the other cluster center is called “classiﬁcation loss” [ 74 ], and this re-triggers clustering with an updated number of clusters. An encounter of a new example of the existing class is followed by an update to the clustering by minimizing the classiﬁcation loss of the newly-seen data. Case-3:  Finally, if the classiﬁer sees an example of a new class then the feature vector of the data sits far from all the cluster centers indicating an unknown class.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "The distance of this feature vector from the other cluster center is called “classiﬁcation loss” [ 74 ], and this re-triggers clustering with an updated number of clusters. Since we have multiple teacher models, each of them contributes to the exemplar set, making it robust and removing bias. Over multiple time windows, the representation learner goes through all the possible exemplars selected by using the conﬁdence matrix and creates an exemplar set with same number of examples from each possible class.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "´as implements the major portions using “custom hardware” (dis- cussed in § IV-A ). To efﬁciently implement the exemplar selection algorithm,  Us. Since we have multiple teacher models, each of them contributes to the exemplar set, making it robust and removing bias.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "The annotations on the new exemplar set created by the representation learner is compared against the conﬁdence matrix of the edge model to calculate the “drift”. ´as implements the major portions using “custom hardware” (dis- cussed in § IV-A ). Consequently, this exemplar set becomes the training data for the continuous learning, which consequently minimizes the drift.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "By doing this,  Us. Consequently, this exemplar set becomes the training data for the continuous learning, which consequently minimizes the drift. Once the student model is trained with the exemplar set, the data is discarded and the feature space for the teacher models is updated.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "Although efﬁcient hardware accelerators [ 16 ], [ 27 ], [ 80 ] have been developed to do the same, these ac- celerators are typically designed with a “throughput-ﬁrst” \n895 \nAuthorized licensed use limited to: Penn State University. By doing this,  Us. ´as  keeps  both  the student and the teacher models “updated.” Since the feature space of the teacher model is updated using K-means+, the major computation is the training of the student model using the exemplar data.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Although efﬁcient hardware accelerators [ 16 ], [ 27 ], [ 80 ] have been developed to do the same, these ac- celerators are typically designed with a “throughput-ﬁrst” \n895 \nAuthorized licensed use limited to: Penn State University. Restrictions apply. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "Restrictions apply. Deploying sufﬁcient battery resources to allow intermittency-unaware designs to operate on solar power is neither efﬁcient nor sustainable. approach and are neither conﬁgured nor capable of operating with an intermittent power source.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "Deploying sufﬁcient battery resources to allow intermittency-unaware designs to operate on solar power is neither efﬁcient nor sustainable. Given enough time even na¨ıve low power hardware can ﬁnish training, but will have longer periods where the drift is exposed. C. Hyperparameters: The Right Way to Learn \nAfter ﬁnalizing the training set for continuous learning, the next challenge is to learn within the power and time budget.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "ﬁnish training on the exemplars (described in § III-B ) as soon as possible and also reach the desired accuracy – but to do this within the harvested budget. A more preferable solution is to get rid of drift as quickly as possible, i.e. Given enough time even na¨ıve low power hardware can ﬁnish training, but will have longer periods where the drift is exposed.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "ﬁnish training on the exemplars (described in § III-B ) as soon as possible and also reach the desired accuracy – but to do this within the harvested budget. Prior works [ 34 ], [ 48 ], [ 68 ] suggest that selecting the right hyper-parameters (like batch size, learning rate, number of layers to train etc.) have a huge impact on the convergence and accuracy of the models.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "have a huge impact on the convergence and accuracy of the models. To achieve this, we design a “micro-proﬁler” that can look into the drift of the models as well as the power availability and decide the right hyperparameters to train the models. For each edge servers to handle multiple steams with multiple drifts, we need to jointly optimize the hyper-parameters for maximizing accuracy with minimum power and resource budget.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "However, they never considered an intermit- tent power source, nor explored jointly optimizing multiple models with power, accuracy and latency constraints. To achieve this, we design a “micro-proﬁler” that can look into the drift of the models as well as the power availability and decide the right hyperparameters to train the models. Prior works [ 12 ], [ 38 ], [ 68 ] have designed hyperparameter micro-proﬁlers.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "Observing this, we propose a “weighted accuracy metric”, where the weight of each of the model is a function of the accuracy, time needed and power availability. Further- more, each model might contribute differently to the overall accuracy. However, they never considered an intermit- tent power source, nor explored jointly optimizing multiple models with power, accuracy and latency constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "Typically, there is an inverse correlation of the convergence of the stochastic gradient descent (SGD) algorithm, the most popular training algorithm for DNNs, over the number of iterations ( n i ) [ 68 ]: l  ∝ O ( 1 / n i )  and  l  = 1 β 0 . Furthermore, we allow some slack to the weighted accuracy so that the optimizer can choose a better set of hyperparameters if we can reach  close to  the weighted accuracy with much lower resource (power or compute) consumption. Observing this, we propose a “weighted accuracy metric”, where the weight of each of the model is a function of the accuracy, time needed and power availability.",
      "type": "sliding_window_shuffled",
      "tokens": 175,
      "augmented": true
    },
    {
      "text": "Typically, there is an inverse correlation of the convergence of the stochastic gradient descent (SGD) algorithm, the most popular training algorithm for DNNs, over the number of iterations ( n i ) [ 68 ]: l  ∝ O ( 1 / n i )  and  l  = 1 β 0 . n i + β 1   +  β 2 , where  l  is the loss of the SGD and  β i  is an non-negative real number. Therefore, by running a few iterations of the SGD algorithms with various other hyperparameters, we can easily  predict  the con- vergence of the models.",
      "type": "sliding_window_shuffled",
      "tokens": 156,
      "augmented": true
    },
    {
      "text": "changes. Therefore, by running a few iterations of the SGD algorithms with various other hyperparameters, we can easily  predict  the con- vergence of the models. Note that this needs to be done every time one of the constraints (accuracy, power etc.)",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "t . The micro-proﬁler optimizes the weighted accuracy ( A w  = \nW i \nA i   / ∑ W i ; ∀ i  ≤ # models ; W i  =  f ( time , dri ft , compute )  with a user-deﬁned slack value of  δ ), with respect to available power ( P av ): max A w ;  s . changes.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "t . P  ≤ P av . Energy Buffering and Power-Predictor:  To regulate, manage and ensure a stable power supply to the circuitry,  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "´as  uses a super-capacitor assisted voltage regulation circuit. To properly model the energy harvesting, losses during conversion, and leakage, we built a rectiﬁcation circuit with 4  ×  5 . Energy Buffering and Power-Predictor:  To regulate, manage and ensure a stable power supply to the circuitry,  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "2 F super-capacitors connected in parallel to a voltage regulator \ncircuit. 5 V ,  2 . To properly model the energy harvesting, losses during conversion, and leakage, we built a rectiﬁcation circuit with 4  ×  5 .",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Note that the power predictors used in prior works are meant for ﬁckle energy harvesting scenarios like piezoelectric (movement), or RF (WiFi). 2 F super-capacitors connected in parallel to a voltage regulator \ncircuit. The harvested power is given as an input to a mov- ing average power predictor [ 61 ], [ 72 ] to predict the future available power.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "We took a history (years 2019 and 2020; from Seattle, WA; Sterling, VA; and Oak Ridge, TN) of solar energy traces from SOLRAD [ 25 ], [ 91 ] and built a weight matrix which looks into a window of 1 hour at 1 minute (average power) intervals to predict the power for next 10 minutes (1 minute granularity). We have adjusted the time window size. Note that the power predictors used in prior works are meant for ﬁckle energy harvesting scenarios like piezoelectric (movement), or RF (WiFi).",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "We use regression to ﬁnd the weights (exponents and coefﬁcients) to the prediction curve followed by exponential smoothing to decay the weights. The rate of exponential smoothing depends on the scheduler used - while for the conservative scheduler the predictor always underestimated the power (shallow smoothing), the eager scheduling uses the direct output of the predictor (steeper smoothing). We took a history (years 2019 and 2020; from Seattle, WA; Sterling, VA; and Oak Ridge, TN) of solar energy traces from SOLRAD [ 25 ], [ 91 ] and built a weight matrix which looks into a window of 1 hour at 1 minute (average power) intervals to predict the power for next 10 minutes (1 minute granularity).",
      "type": "sliding_window_shuffled",
      "tokens": 170,
      "augmented": true
    },
    {
      "text": "The rate of exponential smoothing depends on the scheduler used - while for the conservative scheduler the predictor always underestimated the power (shallow smoothing), the eager scheduling uses the direct output of the predictor (steeper smoothing). In either case, the predictor predicts the power with  ≈ 95% (peak of 98 . 72 (with real solar power trace) and minimum of 89 .",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "72 (with real solar power trace) and minimum of 89 . 14 (with synthetic power trace) accuracy. The micro-proﬁler, having run multiple sweeps, returns a set of hyper-parameters ( Ψ i ) for each model which is then stored in a history table.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "This helps us avoid unnecessary proﬁling (up to 41%). The micro-proﬁler, having run multiple sweeps, returns a set of hyper-parameters ( Ψ i ) for each model which is then stored in a history table. When introduced to a new set of constraints (change of power availability, accuracy etc.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "When introduced to a new set of constraints (change of power availability, accuracy etc. IV. ), the micro-proﬁler ﬁrst looks in the history table to ﬁnd a conﬁguration and runs proﬁling if and only if it could not ﬁnd one.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "IV. DNN training is mas- sively parallel, fairly compute intensive, time consuming, and needs a lot of (albeit structured) data movements [ 16 ], [ 37 ]. T HE  MORPHABLE H ARDWARE \nWhy Not Commercial GP-GPUs?",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "However, as mentioned in § I , the commercial GPUs used for DNN training are typically power hungry ( typically in 100s of Watts TDP; We exprimented with multiple GPUs, server class A6000: 300W TDP, server class A100: 250W – 400W TDP, client class TRX3090: 350W TDP, and client class T4: 70W TDP), and are  not  equipped to handle intermittent power emergencies. Therefore, GP-GPUs have classically been used to train DNN models. DNN training is mas- sively parallel, fairly compute intensive, time consuming, and needs a lot of (albeit structured) data movements [ 16 ], [ 37 ].",
      "type": "sliding_window_shuffled",
      "tokens": 161,
      "augmented": true
    },
    {
      "text": "2   To un- derstand the impact of DVFS on energy savings and dynamic compute scaling, we implemented a simple multi-arm bandit algorithm to select the right bucket of compute frequencies (SM frequency for NVIDIA GPUs), and memory frequencies to match the power-demands of the intermittent solar source. However, these GPUs are often equipped with dynamic voltage and frequency scaling (DVFS). However, as mentioned in § I , the commercial GPUs used for DNN training are typically power hungry ( typically in 100s of Watts TDP; We exprimented with multiple GPUs, server class A6000: 300W TDP, server class A100: 250W – 400W TDP, client class TRX3090: 350W TDP, and client class T4: 70W TDP), and are  not  equipped to handle intermittent power emergencies.",
      "type": "sliding_window_shuffled",
      "tokens": 185,
      "augmented": true
    },
    {
      "text": "2   To un- derstand the impact of DVFS on energy savings and dynamic compute scaling, we implemented a simple multi-arm bandit algorithm to select the right bucket of compute frequencies (SM frequency for NVIDIA GPUs), and memory frequencies to match the power-demands of the intermittent solar source. 4  even with DVFS, commercial off the shelf GPUs could only ﬁnish  <  50% of the scheduled training task. As shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "4  even with DVFS, commercial off the shelf GPUs could only ﬁnish  <  50% of the scheduled training task. T4, thanks to its limited compute capabilities, could not ﬁnish training tasks on time. However, hardware is not the only limitation, as even with custom hardware [ 16 ] enabled with the state-of-the-art continuous learning algorithm [ 12 ] could only ﬁnish  ≈ 75% \n2 NVIDIA provides the list of supported clocks through the API “ nvidia--smi --q --d SUPPORTED_CLOCKS ”; We did not creport the results from A100 for this, as it does not offer multiple memory clocks, signiﬁcantly impacting its DVFS capabilities.",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. 896 \nAuthorized licensed use limited to: Penn State University. T4, thanks to its limited compute capabilities, could not ﬁnish training tasks on time.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "of the scheduled training without any intermittency support. Restrictions apply. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "It is clear that we  can neither  use the commercial GPUs  nor rely on the standard software and algorithmic approach for intermittent training purpose as they  cannot  ﬁnish the compute given the intermittent power budget. of the scheduled training without any intermittency support. 0 0.2 0.4 0.6 0.8 \n1 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nA6000 w/DVFS RTX3090 w/DVFS Non-intermittent Custom HW w/Ekya \nOur Custom HW \nCompleted/Scheduled \nC/S Win-1 C/S Win-2 C/S Win-3 C/S Win-4 C/S Win-5 C/S mean \nFig.",
      "type": "sliding_window_shuffled",
      "tokens": 197,
      "augmented": true
    },
    {
      "text": "Note that, even with DVFS, most scheduled compute could not be ﬁnished. 0 0.2 0.4 0.6 0.8 \n1 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nA6000 w/DVFS RTX3090 w/DVFS Non-intermittent Custom HW w/Ekya \nOur Custom HW \nCompleted/Scheduled \nC/S Win-1 C/S Win-2 C/S Win-3 C/S Win-4 C/S Win-5 C/S mean \nFig. 4: Impact of DVFS on completion (average power budget 70W).",
      "type": "sliding_window_shuffled",
      "tokens": 179,
      "augmented": true
    },
    {
      "text": "This includes the intermittent failures ( ≤ 20W where no compute could be done); we included check- pointing to ensure that progress is saved in power-failures. Note that, even with DVFS, most scheduled compute could not be ﬁnished. C/S is the ratio of  C ompleted over the  S cheduled training tasks over multiple time windows of 4hours.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "There have also been signiﬁcant efforts in designing and optimizing specialized DNN training accelerators [ 16 ], [ 27 ], [ 81 ], and many commercial organizations have already devel- oped their own accelerators [ 37 ], [ 95 ] as well. C/S is the ratio of  C ompleted over the  S cheduled training tasks over multiple time windows of 4hours. Our custom HW runs with intermittent support both by hardware and software.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Considering the compute mapping of the DNN training, almost all of these de- signs are based on a “systolic architecture”, performing chains of multiplication and accumulations (MACs). However, these devices take a “throughput-ﬁrst” approach, to minimize the time consumption and seldom optimize power consumption ﬁrst. There have also been signiﬁcant efforts in designing and optimizing specialized DNN training accelerators [ 16 ], [ 27 ], [ 81 ], and many commercial organizations have already devel- oped their own accelerators [ 37 ], [ 95 ] as well.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "Furthermore, these accelerators have been designed to operate under constantly available power. This has lead to a global concern of the energy and consequently carbon-footprint of the DNN training [ 21 ], [ 57 ], [ 67 ], [ 89 ]. However, these devices take a “throughput-ﬁrst” approach, to minimize the time consumption and seldom optimize power consumption ﬁrst.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Furthermore, these accelerators have been designed to operate under constantly available power. Although our pro- posed representation learning (§ III ) and micro-proﬁler (§ III-C ) help us ﬁnd a better training conﬁguration that can minimize the compute if deployed in the aforementioned accelerators, it does not solve sustainability: That is, with variable solar power, can we scale compute alongside power to continue to make “forward progress”, even when minimum amount of power is available. The systolic array structure of the DNN accelerators is well suited for this as we can change the com- pute size, as well as the number of memory channels feeding to those compute units as per the power availability.",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "The hardware design of  Us. The systolic array structure of the DNN accelerators is well suited for this as we can change the com- pute size, as well as the number of memory channels feeding to those compute units as per the power availability. However, we need to be innovative in terms of designing and placing the compute hierarchy to ensure minimum data movement and re-computations when compute scaling.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "5a ) incorporates all the aforementioned points. The hardware design of  Us. ´as  (Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 27,
      "augmented": true
    },
    {
      "text": "5a ) incorporates all the aforementioned points. Note that,  Us. ´as  introduces a design philosophy for building a morphable hardware, and it can easily be adapted by any of the systolic array based commercial off the shelf (or research prototype) DNN training accelerators.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "5: Overall architecture with the components and the power failure handle sequence of  Us. ´as  introduces a design philosophy for building a morphable hardware, and it can easily be adapted by any of the systolic array based commercial off the shelf (or research prototype) DNN training accelerators. DNN Compute Mapping:  Typically there are three ways of mapping DNN compute into a systolic array, namely, 1. output \nDRAM \nRepresntation Learning \nMicro-profiler \nActivation \nNormalize & Pooling \nHost \nPower Predictor  Examplar \nGlobal Scratch-pad \nDouble Buffered IF Map \nDouble Buffered Filter \nSTT-RAM S \nSTT-RAM N \nNVSB-SE \nNVSB-NW \nDouble Buffered \nOF Map \nP-SUM \nW \nArbiter \n2xNVSB \nP- MUX \nTile-Q \nPE-Block \nSouth Control \nMaster \nNorth \nControl \n(a) High-level arch \no-Path De-Mux \n4x4 STiles w/ 8x8 SA each 128 cycles \n(64 x16)x 16bits Results \nPower off Warning from \nPredictor \nNext \nTile \nTo arbiter \n& Fabric \nclk \nw-pdown \nstate on pwr warning (>256 cycles) off \nbackup \ndat Compute \npath tile arbiter & fabric \nfab-state \n!pdown \n4kB NVSB \nNext STile \n128 Cycles \nFilter \ni-Path mux \nFrom IF or NVSB of failed PE  \ni-Path mux \nPrev STile Next STile \nWork-Q \nLogic \nOutput \nFilter \n8x8 Priority \nMux \n512B NVSB if FULL \nWr-Q \n(b) Power-down/ Failure handling \nFig.",
      "type": "sliding_window_shuffled",
      "tokens": 403,
      "augmented": true
    },
    {
      "text": "´as . 5: Overall architecture with the components and the power failure handle sequence of  Us. stationary; 2. input stationary; and 3. weight stationary [ 79 ].",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "However, our design objective is to minimize  data movements in the case of compute reconﬁgura- tion. Most large-scale accelerators use the output stationary imple- mentations to minimize the output feature map movement [ 81 ], and some of available hardware even supports multiple types of mappings [ 15 ], [ 37 ]. stationary; 2. input stationary; and 3. weight stationary [ 79 ].",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "to resume and remap the compute. However, our design objective is to minimize  data movements in the case of compute reconﬁgura- tion. In an output stationary mapping,  both  input and weights are dynamic and any power failure or reconﬁguration will need to save and restore a lot of current context (partial sums, indices of weights and inputs etc.)",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "This problem reduces in both input stationary and weight stationary, but at the cost of throughput [ 80 ]. Typically, the input feature maps are larger than the (individual) weights, and more importantly large weights can easily be represented \n897 \nAuthorized licensed use limited to: Penn State University. to resume and remap the compute.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "Typically, the input feature maps are larger than the (individual) weights, and more importantly large weights can easily be represented \n897 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "The tile-level shows how each tile consists of multiple such PEs and will be working on one kernel at a time. 6: Weight stationary compute mapping. The PE-level shows how the input ﬂows and the convolutions are computed with a 3x3 convolution toy example.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "The accelerator-level shows that the entire accelerator is made of multiple such tiles (4x4 in the toy example). The tile-level shows how each tile consists of multiple such PEs and will be working on one kernel at a time. Inputs are broadcast into each tile so that each tile can work on a kernel.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "Computation is  redistributed  when there is a change in the power availability, and multiple tiles are shutdown (redacted) without impacting the data ﬂow. Inputs are broadcast into each tile so that each tile can work on a kernel. or decomposed as multiple units called “kernels” (or “ﬁlters”).",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "In a typical convolutional neural network (CNN), each kernel is convoluted over the entire input feature map, and hence there is an “inter-kernel parallelism” (all kernels of a single layer can be executed in parallel) and “intra-kernel parallelism” (multiple computes in a convolution can happen in parallel). This property is true both for the forward pass and the backward pass of the standard CNN training. or decomposed as multiple units called “kernels” (or “ﬁlters”).",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "The modular nature of the weight stationary mapping makes it a strong candidate for use in a re-conﬁgurable or morphable systolic structure as turning off some compute is the same as not computing a kernel and scheduling it for later. Therefore,  Us. This property is true both for the forward pass and the backward pass of the standard CNN training.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "Therefore,  Us. A. ´as employs a weight stationary compute mapping for executing the training tasks on the morphable hardware.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "Design Description of the DNN Hardware Augmentations \nCompute Mapping:  Fig. 5a  shows the high level design, architecture and different components present in our proposed accelerator. A.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "The accelerator encompasses 256 tiles, structured in a 4 × 4 conﬁguration of 16 super-tiles, each harboring 4 × 4 tiles. These super-tiles Each tile, individually switchable ON or OFF based on power availability, houses 64 16-bit ﬂoating point MAC units conﬁgured in an 8  ×  8 systolic array for convolution operations. 5a  shows the high level design, architecture and different components present in our proposed accelerator.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Data streaming and partial compute storage are facilitated by four double buffered SRAM structures, with the \nweights residing in a double buffered multi-banked SRAM. These super-tiles Each tile, individually switchable ON or OFF based on power availability, houses 64 16-bit ﬂoating point MAC units conﬁgured in an 8  ×  8 systolic array for convolution operations. A modular computational approach is adopted where each tile is accountable for one CNN kernel, necessitating  ⌈ [ C  × H  × W ] / 64 ⌉ iterations for a kernel of size [ C  × H  × W ] .",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "The ﬁlter SRAM has 256 banks (one per tile), each with a size of 1kB (double buffered, 512B per buffer per bank). Input data broadcast to all tiles is managed by a 64kB double- buffered input feature map SRAM (32kB each), requiring ⌈ [ X  × Y  × Z ] / 1024 ⌉ iterations for full input loading, with each buffer loaded  [ X  × Y  ×  Z ] / 2048 times. Data streaming and partial compute storage are facilitated by four double buffered SRAM structures, with the \nweights residing in a double buffered multi-banked SRAM.",
      "type": "sliding_window_shuffled",
      "tokens": 160,
      "augmented": true
    },
    {
      "text": "A 128kB SRAM serves as a scratchpad for storing activations, transposes, and intermediate differentials during the backward pass. The convolution map transforms  [ X  × Y  ×  Z ]   M × C × W × H −−−−−−−→ [ M  × U  × V ]  to yield an output tensor of dimensions  [ M  ×  U  ×  V ] , supported by a 256 banked double buffered output feature map SRAM, each bank of size 8kB (4kB/buffer). Input data broadcast to all tiles is managed by a 64kB double- buffered input feature map SRAM (32kB each), requiring ⌈ [ X  × Y  × Z ] / 1024 ⌉ iterations for full input loading, with each buffer loaded  [ X  × Y  ×  Z ] / 2048 times.",
      "type": "sliding_window_shuffled",
      "tokens": 213,
      "augmented": true
    },
    {
      "text": "A 128kB SRAM serves as a scratchpad for storing activations, transposes, and intermediate differentials during the backward pass. For smaller DNNs without 256 kernels in any layer, a batching mode is operational with a batch size of  B  =  ⌊ A / L ⌋ images, where  L  denotes the layer with the fewest channels, and  A  the number of active tiles. The accelerator also houses 256  ×  256 compactor-mux combinational logic units (256 units per tile) for ReLU activation (forward pass) and inverse activation (backward pass).",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "In DNN training, meticulous compute mapping, mem- ory access strategies, and operational formulas are instru- mental for the forward and backward passes. This generic design is adaptable for various workloads. For smaller DNNs without 256 kernels in any layer, a batching mode is operational with a batch size of  B  =  ⌊ A / L ⌋ images, where  L  denotes the layer with the fewest channels, and  A  the number of active tiles.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "The for- ward pass computes activations via the formula  A muv  = ∑ C − 1 \nc = 0   ∑ H − 1 \ni = 0   ∑ W − 1 \nj = 0   X ( u + i )( v +  j ) c   · K mijc , with results stored in the double-buffered output feature map SRAM. In DNN training, meticulous compute mapping, mem- ory access strategies, and operational formulas are instru- mental for the forward and backward passes. The backward pass emphasizes gradient computation through backpropaga- tion, which is crucial for weight updates.",
      "type": "sliding_window_shuffled",
      "tokens": 152,
      "augmented": true
    },
    {
      "text": "The backward pass emphasizes gradient computation through backpropaga- tion, which is crucial for weight updates. This gra- dient computation, fundamental for learning, is meticulously mapped across the systolic array, ensuring precise and efﬁcient backpropagation. The gradient of the loss function concerning the weights is computed through the formula ∂ L ∂ K mi jc   =  ∑ U − 1 \nu = 0   ∑ V − 1 \nv = 0 \n∂ L ∂ A muv   ·  X ( u + i )( v +  j ) c .",
      "type": "sliding_window_shuffled",
      "tokens": 141,
      "augmented": true
    },
    {
      "text": "The 8 × 8 systolic array in each tile executes multiply-accumulate operations in a pipelined and parallel fashion, abiding by the Weight Stationary approach, thereby optimizing the throughput and efﬁciency of the train- ing operations within this hardware architecture. Memory accesses are optimally managed via the double-buffered SRAM structures, providing timely data availability for the MAC units. This gra- dient computation, fundamental for learning, is meticulously mapped across the systolic array, ensuring precise and efﬁcient backpropagation.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "The 8 × 8 systolic array in each tile executes multiply-accumulate operations in a pipelined and parallel fashion, abiding by the Weight Stationary approach, thereby optimizing the throughput and efﬁciency of the train- ing operations within this hardware architecture. Power Control Logic:  Power emergency prediction in  Us. ´as is always conservative, and the solar power predictor has a mean accuracy of 92%, limiting false positives and helping the control unit select appropriate tile counts.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "The system needs at least 512 cycles of advanced notice to ﬂush compute and enable a compute migration. ´as is always conservative, and the solar power predictor has a mean accuracy of 92%, limiting false positives and helping the control unit select appropriate tile counts. Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Fig. 5a  shows the block diagram of the mesh interconnect, and Fig. 5b  shows the power-down sequence and signal states.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "The network works at a super-tile (STile) granularity and each arbiter node uses an 8x8 priority- mux. The network only gets activated when it gets a  w-pdown warning signal from the predictor. 5b  shows the power-down sequence and signal states.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "The w-pdown  triggers the  backup  signal and the system goes into pwr-warning  state (other states being on, off, invalid and X). The network only gets activated when it gets a  w-pdown warning signal from the predictor. This signal starts a graceful power-down sequence for the required number of tiles.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "In the  pwr-warning  phase the system ﬁnishes the remaining compute of the systolic arrays (which can take up to 64 cycles), \n898 \nAuthorized licensed use limited to: Penn State University. The w-pdown  triggers the  backup  signal and the system goes into pwr-warning  state (other states being on, off, invalid and X). Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "and starts ﬂushing the results for backup. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "Buffer Management:  Us. ´as  uses non-volatile state buffers (NVSBs, 18 count, 1 per 4x4 tiles, each of 1kB, and 2 of 4kB each) for state saving and data backup. and starts ﬂushing the results for backup.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "´as  uses non-volatile state buffers (NVSBs, 18 count, 1 per 4x4 tiles, each of 1kB, and 2 of 4kB each) for state saving and data backup. The control logic prioritizes writing data into the local NVSB for the arbiter (each arbiter caters to 2 STiles). If those get full because of continuous power failures, the control directs the data to the global NVSBS (NVSB-NW and NVSB-SE in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "If those get full because of continuous power failures, the control directs the data to the global NVSBS (NVSB-NW and NVSB-SE in Fig. 5a ). The NVSB stores the global/asynchronous work queue and the shufﬂing conﬁguration (mini-batch arrangement).",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "The NVSB stores the global/asynchronous work queue and the shufﬂing conﬁguration (mini-batch arrangement). Power Failure and Compute Scheduling \nCentral to the  Us. B.",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "Power Failure and Compute Scheduling \nCentral to the  Us. ´as  accelerator’s operational efﬁciency is the work queue—an intricately designed, hierarchical structure that meticulously catalogues pending computational tasks. Each task, represented in the queue, corresponds to the execu- tion of speciﬁc CNN kernels, feature tiles and operations.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "As deep neural network models often have a complex interplay of layers, each with distinct computational needs, the work queue ensures a systematic, prioritized approach to handle these oper- ations. Each task, represented in the queue, corresponds to the execu- tion of speciﬁc CNN kernels, feature tiles and operations. Two distinct scheduling strategies, each complemented by its own type of work queue, govern the computational ﬂow: Conservative Scheduling and Eager Scheduling.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "The work queue schedule, intermediate result, network and layer information are saved on predicted power failure, and data from the DRAM (the working set of IF/OF/ﬁlter and model state) are moved to an NV-RAM using STT-RAM based buffers in the memory hierarchy (parallel to the IF/OF/ﬁlter). The dual-scheduling mechanism, bolstered by the work queue’s ﬂexible architecture, not only optimizes compute performance but also offers resilience against power uncertainties. Two distinct scheduling strategies, each complemented by its own type of work queue, govern the computational ﬂow: Conservative Scheduling and Eager Scheduling.",
      "type": "sliding_window_shuffled",
      "tokens": 145,
      "augmented": true
    },
    {
      "text": "The host writes the latest copy of the completed iteration (in epoch granularity) into the STT- RAMs (STT-RAM-N for the upper 128 SAs, and STT-RAM- S for the lower 128SAs, Fig. We do  not  replace the DRAM buffers with NVM because of limited lifetimes [ 17 ]. The work queue schedule, intermediate result, network and layer information are saved on predicted power failure, and data from the DRAM (the working set of IF/OF/ﬁlter and model state) are moved to an NV-RAM using STT-RAM based buffers in the memory hierarchy (parallel to the IF/OF/ﬁlter).",
      "type": "sliding_window_shuffled",
      "tokens": 161,
      "augmented": true
    },
    {
      "text": "5a ). The host writes the latest copy of the completed iteration (in epoch granularity) into the STT- RAMs (STT-RAM-N for the upper 128 SAs, and STT-RAM- S for the lower 128SAs, Fig. In case of a complete power failure, the compute in ﬂight are rejected and, once the system starts working, the work queues get invalidated and the host starts the compute again from the last checkpoint.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "In case of a complete power failure, the compute in ﬂight are rejected and, once the system starts working, the work queues get invalidated and the host starts the compute again from the last checkpoint. Along with that, the most common intermittent software libraries and software designs [ 26 ], [ 52 ] (and most DNN training libraries like PyTorch, TensorFlow) also offer periodic checkpoints. Note that the power-up sequence for a tile runs in the exact opposite order of the  powerdown  sequence (a tile becomes computationally active 512 cycles after it gets the power up signal).",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "´as  uses two kinds of scheduling policies to handle the graceful  powerdown  and work queue rearrangement. Note that the power-up sequence for a tile runs in the exact opposite order of the  powerdown  sequence (a tile becomes computationally active 512 cycles after it gets the power up signal). Us.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "´as  accelerator design is to ensure proper “compute place- ment” even under a power emergency or power scaling. Conservative Scheduling:  The most important part of the Us. ´as  uses two kinds of scheduling policies to handle the graceful  powerdown  and work queue rearrangement.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "Fig. ´as  accelerator design is to ensure proper “compute place- ment” even under a power emergency or power scaling. 6 : Accelerator level  provides a high-level overview of the compute scheduling (where the redacted part of the hard- ware is turned off because of the lack of power).",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "The key components of the scheduler are the “moving average power predictor” and the “micro-proﬁler”. 6 : Accelerator level  provides a high-level overview of the compute scheduling (where the redacted part of the hard- ware is turned off because of the lack of power). In the  i th   kernel scheduling iteration, given the power budget and power prediction, the \nmicro-proﬁler decides the required training conﬁguration, and the control logic (conservatively) enables suitable number of tiles (say  t i  tiles of the 256 tiles).",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "In the  i th   kernel scheduling iteration, given the power budget and power prediction, the \nmicro-proﬁler decides the required training conﬁguration, and the control logic (conservatively) enables suitable number of tiles (say  t i  tiles of the 256 tiles). Those  t i  tiles fetch  t i unique kernels from the 1Byte wide, 256 deep global kernel dispatch queue (GKDQ,  t i  kernels scheduled in parallel ). Note that the power requirement of each tile is known in advance (please refer to § V , TABLE  I  for details).",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "Once the scheduled ( t i ) tiles are completed, the micro-proﬁler again ﬁnds the right conﬁguration for the  i  +  1 th   iteration and the scheduler again conservatively enables  t i + 1  number of tiles suitable for the power budget. Note that the power requirement of each tile is known in advance (please refer to § V , TABLE  I  for details). The  t i + 1  tiles fetch the next  t i + 1  kernels from the GKDQ and the process continues.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "The  t i + 1  tiles fetch the next  t i + 1  kernels from the GKDQ and the process continues. The GKDQ always points to the next available kernel location. This conservative compute and power estimation ensures that none of the kernel computes (the lowest decomposed level of compute unit for the hardware) ever fails and hence there is no need for any partial data movement.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "The GKDQ always points to the next available kernel location. The control fetches the right number of kernels and all of them are synchronously executed in the active tiles. Eager Scheduling:  A weight stationary implementation with a conservative scheduling will always run synchronously.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "when some of the tiles are half way through the compute, some other tiles can just start execution). However, in the middle of an kernel execution iteration, if the hardware gains access to more power which in turn can enable more tiles, it cannot do so without breaking synchrony (i.e. Eager Scheduling:  A weight stationary implementation with a conservative scheduling will always run synchronously.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "We call this  Eager Scheduling . when some of the tiles are half way through the compute, some other tiles can just start execution). Facilitating such schedul- ing will provide us less idle time, more forward progress and more efﬁcient use of the incoming power but at the expense of more control overheads.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "To enable eager scheduling, we decentralized the global kernel dispatch queue and equipped each tile with a local kernel dispatch queue (1Byte wide 16 deep). At the beginning of each kernel scheduling iteration, the micro-proﬁler decides the right conﬁguration, and the control distributes equal number of kernels to each active tile (given  A  active kernel, and  K total kernels, each tile gets  ⌊ K / A ⌋ kernels to execute). We call this  Eager Scheduling .",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "At the beginning of each kernel scheduling iteration, the micro-proﬁler decides the right conﬁguration, and the control distributes equal number of kernels to each active tile (given  A  active kernel, and  K total kernels, each tile gets  ⌊ K / A ⌋ kernels to execute). However, in the middle of the execution if any new tiles becomes alive (because of an increase in harvested power), the scheduler immediately marks it ready to start working and the tile fetches a kernel (currently not scheduled in any of the tiles) and starts working on it. The conservative scheduler ensures that no tile loses power before ﬁnishing the current scheduled kernel.",
      "type": "sliding_window_shuffled",
      "tokens": 145,
      "augmented": true
    },
    {
      "text": "We face three issues here: 1. How does the new tile get any kernel to work on? However, in the middle of the execution if any new tiles becomes alive (because of an increase in harvested power), the scheduler immediately marks it ready to start working and the tile fetches a kernel (currently not scheduled in any of the tiles) and starts working on it.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "How does the new tile get any kernel to work on? Over multiple iterations of such asynchronous scheduling, the kernel queue for each tile will be of different size creating a load imbalance; how to tackle this? 2.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "3. Over multiple iterations of such asynchronous scheduling, the kernel queue for each tile will be of different size creating a load imbalance; how to tackle this? How do we know when to stop executing?",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "When any of the active tiles are marked ready by the scheduler, the tile employees a state machine to decide where to get work from. To address the ﬁrst two issues, we developed a work-stealing mechanism for each tile. How do we know when to stop executing?",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "Considering the global control always enqueues any idle tile with work, whenever the tile has no work left, it steals a kernel from the most \n899 \nAuthorized licensed use limited to: Penn State University. When any of the active tiles are marked ready by the scheduler, the tile employees a state machine to decide where to get work from. Each time the tile ﬁnishes some work, if its remaining work queue (the local work queue size) is less than the average of all other active tiles, it seeks a new kernel to work on.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply. Considering the global control always enqueues any idle tile with work, whenever the tile has no work left, it steals a kernel from the most \n899 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "heavily loaded tiles. We implemented a counter (local kernel counter) to keep track of the size of the remaining local work queue of each of the tile. Restrictions apply.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "We implemented a counter (local kernel counter) to keep track of the size of the remaining local work queue of each of the tile. Whenever all the local work queue counter hits zero along with the layer kernel counter, the control moves to schedule the next layer (or previous layer in backward propagation) for computation. We also implemented a counter (layer kernel counter) which keeps track of the total kernels to be scheduled for each layer.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Note that we do not delve into the details of the computa- tional primitives involved in training the DNN as several prior works [ 15 ], [ 16 ], [ 80 ] provide a very detailed accounting of it (for both forward and backward pass) along with the hardware and control requirements. Whenever all the local work queue counter hits zero along with the layer kernel counter, the control moves to schedule the next layer (or previous layer in backward propagation) for computation. We treat the convolution scheduling (using input stationary and at a kernel level) in a morphable systolic hardware to be the main challenge and explain it.",
      "type": "sliding_window_shuffled",
      "tokens": 147,
      "augmented": true
    },
    {
      "text": "V. I MPLEMENTATION AND  E VALUATION \nWe focus our evaluation on urban mobility, i.e. We treat the convolution scheduling (using input stationary and at a kernel level) in a morphable systolic hardware to be the main challenge and explain it. performing single shot  object detection for trafﬁc monitoring using the MobileNetV2  [ 51 ] model on the urban trafﬁc data set [ 97 ].",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "performing single shot  object detection for trafﬁc monitoring using the MobileNetV2  [ 51 ] model on the urban trafﬁc data set [ 97 ]. This is a trafﬁc video dataset containing 62GB of videos recorded from ﬁve pole-mounted ﬁsh-eye cameras in the city of Bellevue, WA, USA. Each video stream is recorded with a resolution of 1280  ×  720 at 30fps.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "For annotating the incoming video stream we use three teachers models, namely, ResNet101 [ 32 ], YOLOV2 [ 22 ], and VGG16 [ 87 ]. This contains a total of 101 hour of video across all cameras of which 30 hours of video is used to ﬁne-tune the teacher models and the rest 71 hours of data is used to evaluate our continuous learning so- lution. Each video stream is recorded with a resolution of 1280  ×  720 at 30fps.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "For sustainability, we use solar power to perform our compute. For annotating the incoming video stream we use three teachers models, namely, ResNet101 [ 32 ], YOLOV2 [ 22 ], and VGG16 [ 87 ]. Since our dataset is from Bellevue, WA, we took the SOLRAD solar radiation data [ 25 ] (managed and published by National Oceanic and Atmospheric Administra- tion, NOAA) of Seattle, WA (the SOLRAD center closest to Bellevue and hence we believe is a good approximation).",
      "type": "sliding_window_shuffled",
      "tokens": 136,
      "augmented": true
    },
    {
      "text": "In our experiments we assume the hardware to be powered by a solar panel of one square-meter, and the powers are scaled accordingly (data is available as  W / m 2 ). Since our dataset is from Bellevue, WA, we took the SOLRAD solar radiation data [ 25 ] (managed and published by National Oceanic and Atmospheric Administra- tion, NOAA) of Seattle, WA (the SOLRAD center closest to Bellevue and hence we believe is a good approximation). Finally, we assume the exact same setup of the Urban trafﬁc dataset and hence have 5 different MobileNetV2 models trying to classify the trafﬁc they are facing, and learning from the streaming data.",
      "type": "sliding_window_shuffled",
      "tokens": 162,
      "augmented": true
    },
    {
      "text": "Existing Approaches:  Although there has been signiﬁcant re- search [ 40 ], [ 41 ], [ 47 ], [ 52 ], [ 56 ], [ 61 ], [ 72 ], [ 104 ] on enabling machine learning in intermittently powered devices, a major- ity of it focuses on performing inference. Finally, we assume the exact same setup of the Urban trafﬁc dataset and hence have 5 different MobileNetV2 models trying to classify the trafﬁc they are facing, and learning from the streaming data. We vary the training intervals to see the effect of frequency of retraining.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "Only intermittent learning [ 47 ] focuses on performing on-device training, but with very small workloads and models. Existing Approaches:  Although there has been signiﬁcant re- search [ 40 ], [ 41 ], [ 47 ], [ 52 ], [ 56 ], [ 61 ], [ 72 ], [ 104 ] on enabling machine learning in intermittently powered devices, a major- ity of it focuses on performing inference. Considering the scale, scope and workload of our problem, limits direct comparisons, except for comparing their exemplar selection method (refer Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "Considering the scale, scope and workload of our problem, limits direct comparisons, except for comparing their exemplar selection method (refer Fig. 9 ). Similarly, Ekya [ 12 ] only focuses on co-location of computation, and it’s efﬁciency on ﬁnishing compute even on custom hardware is shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "0 25 50 75 100 \nMN-BL \nTeacher \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-BL \nTeacher \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nNaïve w/Exemplar \nAccuracy in % \nHour-0 Hour-2 Hour-4 Hour-6 Hour-8 \nFig. Similarly, Ekya [ 12 ] only focuses on co-location of computation, and it’s efﬁciency on ﬁnishing compute even on custom hardware is shown in Fig. 4 .",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "7: Accuracy boost due to proper exemplar selection over 8 hours of time window. Labels: MN – MobileNet-V2, BL – Baseline, Teacher – the ensemble of teacher models, MN–#: targeted MobileNet-V2 model for the particular time of day. 0 25 50 75 100 \nMN-BL \nTeacher \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nMN-BL \nTeacher \nMN-1 \nMN-2 \nMN-3 \nMN-4 \nMn-5 \nNaïve w/Exemplar \nAccuracy in % \nHour-0 Hour-2 Hour-4 Hour-6 Hour-8 \nFig.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "Continuous Learning: Accuracy \nFig. Labels: MN – MobileNet-V2, BL – Baseline, Teacher – the ensemble of teacher models, MN–#: targeted MobileNet-V2 model for the particular time of day. A.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "Continuous Learning: Accuracy \nFig. We compare against a baseline using na¨ıve continuous learning algorithm with no representation learning. 7  shows the accuracy improvement over a time window of 8 hours by using the continuous learning algorithm.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "We compare against a baseline using na¨ıve continuous learning algorithm with no representation learning. In contrast,  Us. ´as uses a 2 level exemplar selection algorithm (one using the conﬁdence matrix, and then further reﬁned by the representa- tion learning).",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "We observe that, with representation learning, Us. ´as  is  ≈ 4 . ´as uses a 2 level exemplar selection algorithm (one using the conﬁdence matrix, and then further reﬁned by the representa- tion learning).",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "´as  is  ≈ 4 . 03%, and minimum  ≈ 2 . 94% (maximum  ≈ 8 .",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "Further,  Us. 62%) more accurate than the na¨ıve learner. 03%, and minimum  ≈ 2 .",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "´as  converges closer to the accuracy of the teacher model. Further,  Us. This was possible by restricting the training space and by using the superior exemplar set construction by using representation learning.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Fig. 8  shows the impact of micro-proﬁling on the hyper parameter selection. This was possible by restricting the training space and by using the superior exemplar set construction by using representation learning.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "8  shows the impact of micro-proﬁling on the hyper parameter selection. Fig. Due to the drift- and weighted accuracy- aware micro-proﬁler, the suggested conﬁguration is almost every time the same as an oracular selection.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "8a  shows the number of layers trained for a DNN, in contrast to the ideal number of layers to achieve maximum accuracy. Fig. Over 10 training iterations, we observed the micro-proﬁler to be con- sistent with the oracle (except for one case of iteration 7).",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "A deep dive into 7 th   iteration reveals that the micro-proﬁler chose a higher learning rate (compared to the oracle), which biased the convergence curve ﬁtting and extrapolation (as discussed in § III-C ) and hence suggested a larger number of layers to be trained to achieve the required convergence. Note that the hyperparameter selected in iteration 7 by the micro- proﬁler performs as good as the the oracle model in terms of achieving accuracy, albeit by performing more computation. Over 10 training iterations, we observed the micro-proﬁler to be con- sistent with the oracle (except for one case of iteration 7).",
      "type": "sliding_window_shuffled",
      "tokens": 153,
      "augmented": true
    },
    {
      "text": "A deep dive into 7 th   iteration reveals that the micro-proﬁler chose a higher learning rate (compared to the oracle), which biased the convergence curve ﬁtting and extrapolation (as discussed in § III-C ) and hence suggested a larger number of layers to be trained to achieve the required convergence. Similarly, the micro-proﬁler shows consistent behaviour while choosing the right number of batches. Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "Fig. Observation over 40 hours of continuous learning on the dataset suggest that the micro-proﬁler has, on average, an accuracy deviation of 2 . 8b  also shows the error rate of retraining performed by choosing the hyperparameters given by the micro-proﬁler vs an oracle selection.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "46%, compared to an oracle parameter selection. Along with that, the micro-proﬁler selects correct batch size 82 . Observation over 40 hours of continuous learning on the dataset suggest that the micro-proﬁler has, on average, an accuracy deviation of 2 .",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "64% of the time and the correct number of layers for 87 . 06% of the time. Along with that, the micro-proﬁler selects correct batch size 82 .",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "B. 06% of the time. Impact on Exemplar Selection \nUs.",
      "type": "sliding_window_shuffled",
      "tokens": 15,
      "augmented": true
    },
    {
      "text": "Prior works on intermittent learning have either chosen one teacher model \n900 \nAuthorized licensed use limited to: Penn State University. Impact on Exemplar Selection \nUs. ´as  beneﬁts from the use of  multiple  teacher models for data annotation and exemplar selection.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "Prior works on intermittent learning have either chosen one teacher model \n900 \nAuthorized licensed use limited to: Penn State University. Restrictions apply. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "Restrictions apply. 0 \n20 \n40 \n60 \n1 2 3 4 5 6 7 8 9 10 \n# Layers Tranined \nTraining Iterations \n# Layers Trained-Actual # Layers Trained-Oracle \n(a) Number of layers trained. 0 \n2 \n4 \n6 \n8 \n0 \n10 \n20 \n30 \n40 \n1 2 3 4 5 6 7 8 9 10 \nConvergence Error(%) \nBatch Size \nTranining Iterations Batch Size-Actual Batch Size-Oracle Convergence Error (%)-Actual Convergence Error (%)-Oracle \n(b) Batch-size and convergence.",
      "type": "sliding_window_shuffled",
      "tokens": 142,
      "augmented": true
    },
    {
      "text": "training. 0 \n2 \n4 \n6 \n8 \n0 \n10 \n20 \n30 \n40 \n1 2 3 4 5 6 7 8 9 10 \nConvergence Error(%) \nBatch Size \nTranining Iterations Batch Size-Actual Batch Size-Oracle Convergence Error (%)-Actual Convergence Error (%)-Oracle \n(b) Batch-size and convergence. 0 \n5 \n10 \n15 \n0% \n20% \n40% \n60% \n80% \n100% \n1 2 3 4 5 6 7 8 9 10 \n# Exemplar/100 Frames \nExemplar vs Traning  \nTime \nTranining Iterations Exemplar Selection Tranining #Exemplar/100 frames \n(c) Exemplar selection w.r.t.",
      "type": "sliding_window_shuffled",
      "tokens": 159,
      "augmented": true
    },
    {
      "text": "8: Algorithmic performance of  Us. Component Spec Power Area(mm 2 ) SRAM Buffers 1kB*256+8kB*256+64kB+16*256kB 10.372W 117.164 MAC Unit (8*8)*256 8.46W 32.72 Adder Tree and Comparator 16*16bit + 256 2.4W 21.556 Control – 0.96W 12.2 Host ∼ Cortex A78 series 11W – Design at 592MHz with Synopsys AED 32nm library Total 256 tiles 33.192W 183.64 \nTABLE I: Area and power estimation of our design. ´as : the beniﬁts of the exemplar selection and  μ -proﬁler.",
      "type": "sliding_window_shuffled",
      "tokens": 175,
      "augmented": true
    },
    {
      "text": "Component Spec Power Area(mm 2 ) SRAM Buffers 1kB*256+8kB*256+64kB+16*256kB 10.372W 117.164 MAC Unit (8*8)*256 8.46W 32.72 Adder Tree and Comparator 16*16bit + 256 2.4W 21.556 Control – 0.96W 12.2 Host ∼ Cortex A78 series 11W – Design at 592MHz with Synopsys AED 32nm library Total 256 tiles 33.192W 183.64 \nTABLE I: Area and power estimation of our design. (e.g. Ekya [ 12 ] using ResNeXt101) to annotate the data or used a heuristic on top of the teacher model (e.g.",
      "type": "sliding_window_shuffled",
      "tokens": 186,
      "augmented": true
    },
    {
      "text": "As shown in Fig. Ekya [ 12 ] using ResNeXt101) to annotate the data or used a heuristic on top of the teacher model (e.g. intermittent learning [ 47 ] using randomized selection, K–Last List, or round-robin policy).",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "As shown in Fig. 9 , a single teacher, even with the augmented heuristics, typically fails to select the right exemplar set. The exemplar set signiﬁcantly impacts the accuracy in two ways: 1. missing valid exemplars will result in the student model missing out in learning vital information, increasing its drift, and 2. a wrong annotation by the teacher can also result in the student learning wrong labels, resulting in increased mis-predictions.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "To avoid this, in  Us. ´as , the teacher models perform majority voting to decide the right exemplar, which signiﬁcantly reduces false positives and true negatives (refer to the top bar in Fig. The exemplar set signiﬁcantly impacts the accuracy in two ways: 1. missing valid exemplars will result in the student model missing out in learning vital information, increasing its drift, and 2. a wrong annotation by the teacher can also result in the student learning wrong labels, resulting in increased mis-predictions.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "9 : with the ensemble, the best case annotation is the ideal one with only 2 false positives). Furthermore, the feature extraction for each of the potential \nexemplars for the teacher model is hardware-assisted (§ V-C ), and hence poses no overhead to the inference task. ´as , the teacher models perform majority voting to decide the right exemplar, which signiﬁcantly reduces false positives and true negatives (refer to the top bar in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Furthermore, the feature extraction for each of the potential \nexemplars for the teacher model is hardware-assisted (§ V-C ), and hence poses no overhead to the inference task. 0 \n9 \n18 \n27 \nR Y V R+Y R+V Y+V R+Y+V IL-RR IL-K-Last List \n# Exemplars \n#Exemplars Selected (Best Case) #Exemplars Selected (Worst Case) Average Exemplars \nFig. 9: Impact of multiple teachers on exemplar selection.",
      "type": "sliding_window_shuffled",
      "tokens": 128,
      "augmented": true
    },
    {
      "text": "9: Impact of multiple teachers on exemplar selection. Having an ensemble provides robust exemplar selection and improves accuracy over a single teacher. X- axis shows #exemplars/100 inferred frames over a 2hr window.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "C. Hardware Implementation and Evaluation \nThe proposed morphable hardware was simulated using an in-house simulator based on ScaleSim [ 79 ]. Having an ensemble provides robust exemplar selection and improves accuracy over a single teacher. The X-Axis has different DNNs , R: ResNeXt101, T: YOLO-V3, V: VGG- 16, IL: Intermittent Learning, RR: Round Robin.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "Further, the simulator was integrated with CACTI [ 62 ] and DRAMSIM3 [ 49 ] to estimate access latency, power, and simulate the memory access pattern. C. Hardware Implementation and Evaluation \nThe proposed morphable hardware was simulated using an in-house simulator based on ScaleSim [ 79 ]. We included a wrapper around ScaleSim to  dynamically  change the con- ﬁguration of the systolic array.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "To correctly estimate accelerator power and area, we implemented a register-transfer level model using System Verilog and syn- thesized using Synopsys Design Compiler [ 93 ] with a 32nm library [ 94 ]. Further, the simulator was integrated with CACTI [ 62 ] and DRAMSIM3 [ 49 ] to estimate access latency, power, and simulate the memory access pattern. Rather than including a cycle accurate CPU (host) simulator to orchestrate the compute, we used a simple program to act as proxy for the host CPU and send control signals to schedule and orchestrate the compute on the systolic array.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "Instead of simulating the CPU, we tested the K-means clustering and cluster optimization on a mobile SoC with 8 ×  ARM Cortex A78 series CPU. Table  I  lists the estimated power consumption and area of the major components. To correctly estimate accelerator power and area, we implemented a register-transfer level model using System Verilog and syn- thesized using Synopsys Design Compiler [ 93 ] with a 32nm library [ 94 ].",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Note that  Us. Instead of simulating the CPU, we tested the K-means clustering and cluster optimization on a mobile SoC with 8 ×  ARM Cortex A78 series CPU. Table  II gives the key attributes of the implemented hardware against some of the prior accelerators.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "Platform Freq. Note that  Us. ´as  hardware is not outperforming any of them as the goal was greater  scheduling ﬂexibility  for power tracking rather than performance or area.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "Platform Freq. (MHz) \nArea (mm 2 ) \nPower \n(W) \nPeak Thpt. (GOps) \nEnergy Eff.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "´as \n592 168.2 22.7 (17.2 if only train) 4016 159.42 Fully powered, DNN Compute only 287.44 Fully powered, DNN  μ − proﬁler 255.39 EH +  μ − proﬁle + NV-mems + resizing RAM + Host 159.40 \nTABLE II: Comparison with prior accelerator-based platforms. (GOps) \nEnergy Eff. (GOps/W) DaDianNao [ 16 ] 606 67.3 16.3 4964 304.54 CNVLUTIN [ 4 ] 606 70.1 17.4 4964 285.29 Activation Sparse [ 80 ] 667 292 19.2 5466 284.69 EyerissV2 [ 15 ] 200MHz N/A N/A 153.6G 8b ﬁxed pt/s 193.7 FlexBlock [ 63 ] 333MHz 160.3 (65nm) 34.4 (when same #PEs) 4504 131.03 \nUs.",
      "type": "sliding_window_shuffled",
      "tokens": 238,
      "augmented": true
    },
    {
      "text": "´as \n592 168.2 22.7 (17.2 if only train) 4016 159.42 Fully powered, DNN Compute only 287.44 Fully powered, DNN  μ − proﬁler 255.39 EH +  μ − proﬁle + NV-mems + resizing RAM + Host 159.40 \nTABLE II: Comparison with prior accelerator-based platforms. The systolic array accelerator time multiplexes between per- forming feature extraction for exemplar selection and running the training. Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "Fig. 8c  shows the time distribution of the accel- erator between performing exemplar selection and training. It also shows the number of exemplar frames per 100 frame, i.e., of any 100 frame encountered, how many of those will contain a relatively new data.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "It also shows the number of exemplar frames per 100 frame, i.e., of any 100 frame encountered, how many of those will contain a relatively new data. Over 10 iterations of retraining, \n901 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. -18.87 \n-9.14 \n-10.2 -11.35 \n-20 \n-15 \n-10 \n-5 \n0 \n0% 20% 40% 60% 80% 100% \nRapidly Varrying \nModerately \nstable \nRelatively \nStable \nFully Powered \nLoss in accuracy in % \nContribution of Policy \nExemplar Profiler Morphable No Optimization \n(a) Contribution of components on video data \n0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \nBest Average Best Average \nUsas iCARL Usas Optimus \n# Relative Exemplars # Relative Epoch \nRelative Error wrt Oracle  \n(Lower  is better) \n(Audio) Audio MNIST (Audio) CHiME Home (3D PC) KITTI Vision \n(3D PC) nuScenes (IMU) Bearing Fault (IMU) MHEALTH \n(b)  Us. Restrictions apply.",
      "type": "sliding_window_shuffled",
      "tokens": 225,
      "augmented": true
    },
    {
      "text": "-18.87 \n-9.14 \n-10.2 -11.35 \n-20 \n-15 \n-10 \n-5 \n0 \n0% 20% 40% 60% 80% 100% \nRapidly Varrying \nModerately \nstable \nRelatively \nStable \nFully Powered \nLoss in accuracy in % \nContribution of Policy \nExemplar Profiler Morphable No Optimization \n(a) Contribution of components on video data \n0 \n0.1 \n0.2 \n0.3 \n0.4 \n0.5 \n0.6 \nBest Average Best Average \nUsas iCARL Usas Optimus \n# Relative Exemplars # Relative Epoch \nRelative Error wrt Oracle  \n(Lower  is better) \n(Audio) Audio MNIST (Audio) CHiME Home (3D PC) KITTI Vision \n(3D PC) nuScenes (IMU) Bearing Fault (IMU) MHEALTH \n(b)  Us. ´as  beyond video data \n0 \n10 \n20 \n30 \n40 \n50 \nPredictable Sporadic Predictable Sporadic Predictable Sporadic \nLarge - Video (SA size: 256; AP: 16W) \nMedium - Audio (SA size: 128; AP: 8W) \nSmall - IMU (SA size: 128; AP: 2W) \n% of Unfinished  Compute  \ndue to Power Failure \nAverage Power Baseline SW Backup SW + NVM HW + NVM \n(c)  Us .´as  hardware - different data and energy \nFig. 10: Contribution of different components of  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 336,
      "augmented": true
    },
    {
      "text": "the learner classiﬁed  ≈ 4.5 frames/100-frames (on an average) as exemplar data. ´as  on other applications, data modalities, and power environments. 10: Contribution of different components of  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "And, over 40 hours of continuous learning, we get  ≈ 5.02 frames/100-frames as exemplar data (resulting in  ≈ 17.4% of total accelerator time). Performance-Power Trade-offs:  As Table  II  suggest,  Us. the learner classiﬁed  ≈ 4.5 frames/100-frames (on an average) as exemplar data.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "Us. ´as does not deliver the highest throughput and also consumes more power compared to the other accelerators. Performance-Power Trade-offs:  As Table  II  suggest,  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "The unit compute (only a 3  ×  3 convolution per tile) that  Us. Us. ´as  was designed on a intermittency friendly approach, and was never designed to hit the best throughput.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "The unit compute (only a 3  ×  3 convolution per tile) that  Us. ´as  can perform is much smaller than the other accelerators, limiting its throughput but increasing its modularity of handling intermittent power failures (or power changes). Us.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "Us. While fully powered,  Us. ´as  also consumes more power than the other accelerators since it also performs the exemplar selection along with the DNN training, and also houses NV-SRAM buffers for hardware check-pointing.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "´as  is competitive in terms of energy efﬁciency for training-only tasks. We include details on the energy efﬁciency of  Us. While fully powered,  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "We include details on the energy efﬁciency of  Us. Note that the energy inefﬁciency arises primarily from i) multiple saves and restores, ii) use of NV memories \nand iii) reconﬁguring the DRAM (along with a commercial ARM based host CPU). ´as  under different of operation conﬁgurations in Table  II .",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "Note that most prior works ignore memory and host overheads while reporting the throughput, efﬁciency and power numbers. Note that the energy inefﬁciency arises primarily from i) multiple saves and restores, ii) use of NV memories \nand iii) reconﬁguring the DRAM (along with a commercial ARM based host CPU). Moreover,  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "Moreover,  Us. We believe it will not be fair to compare the energy efﬁciency and throughput of a system like ours, which in- herently has more memory, I/O and reconﬁguration operation with a pure compute based systems mentioned in the hardware baseline. ´as  needs more I/O operations to store the streaming data to compute the exemplars.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "Further, we are the ﬁrst of a kind system to imagine sustainability ﬁrst and design a morphable hardware which can facilitate multiple functionality. We also compare our work against two reconﬁgurable platforms [ 15 ], [ 63 ]. We believe it will not be fair to compare the energy efﬁciency and throughput of a system like ours, which in- herently has more memory, I/O and reconﬁguration operation with a pure compute based systems mentioned in the hardware baseline.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "´as  hardware’s most important feature is its ability to  morph  according to power availability. We also compare our work against two reconﬁgurable platforms [ 15 ], [ 63 ]. Power Aware Scaling: The  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "11  shows its ability to maximize the instantaneous power utilization and scale the number of tiles. Fig. ´as  hardware’s most important feature is its ability to  morph  according to power availability.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "´as to effectively perform more computation with an intermittent power source. This allows  Us. 11  shows its ability to maximize the instantaneous power utilization and scale the number of tiles.",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "As shown in Fig. ´as to effectively perform more computation with an intermittent power source. 11b ,  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 28,
      "augmented": true
    },
    {
      "text": "Considering the power proﬁle of Fig. 11b ,  Us. ´as  maintains a high duty cycle across power variance, whereas DaDianNao [ 16 ] could not be active for all the power cycles.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "11b ,  Us. Considering the power proﬁle of Fig. ´as  can ﬁnish about 50 cycles of retraining (50 complete training cycles) and DaDianNao can only ﬁnish 22 training cycles, even assuming a zero overhead, seamless save-restore of the partial computes of DaDianNao during a power failure/emergency.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Setup Effective Training Accuracy Degradation Replacement Cycle* Battery Backed Custom HW[5000mAH] 93.17 2.48 2 - 3 years Battery Backed Mobile GPU 78.55 7.43 18 - 24 months Fixed Power [15W] 67.54 12.6 NA Fixed Power [35W] 100 1.87 Us. ´as 95.3 1.92 7 - 10 years \nTABLE III: Comparing  Us. ´as  can ﬁnish about 50 cycles of retraining (50 complete training cycles) and DaDianNao can only ﬁnish 22 training cycles, even assuming a zero overhead, seamless save-restore of the partial computes of DaDianNao during a power failure/emergency.",
      "type": "sliding_window_shuffled",
      "tokens": 166,
      "augmented": true
    },
    {
      "text": "´as  with other possible solutions. Sustainability:  To ensure sustainable and continuous learning at the edge,  Us. ´as 95.3 1.92 7 - 10 years \nTABLE III: Comparing  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "´as  operates independently of the power grid or cloud dependency. We evaluated  Us. Sustainability:  To ensure sustainable and continuous learning at the edge,  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "Using the Seattle SOLRAD power trace for January 1, 2022, we simulated 40 hours of continuous learning with 5 different models on Urban Trafﬁc data [ 97 ] and  Us. We evaluated  Us. ´as  against DaDian- Nao, a power-efﬁcient DNN training accelerator, with some modiﬁcations for comparison.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "The results, summarized in Table  IV , demonstrate the effectiveness of  Us. Using the Seattle SOLRAD power trace for January 1, 2022, we simulated 40 hours of continuous learning with 5 different models on Urban Trafﬁc data [ 97 ] and  Us. ´as  hardware.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "´as  in achieving continuous forward progress compared to other approaches. It completed more training tasks while consuming less power and minimizing wastage. The results, summarized in Table  IV , demonstrate the effectiveness of  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "Us. In contrast, cloud-based solutions ex- hibited poor sustainability, relying on high-power-consuming GP-GPUs, and edge servers without power availability strug- gled to perform any compute. It completed more training tasks while consuming less power and minimizing wastage.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "´as  emerges as a promising solution, effectively achieving sustainable and carbon-neutral continuous learning at the edge, addressing critical challenges related to power constraints and environmental impact. Alternate Solutions:  Although  Us. Us.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "´as  works completely using intermittent power, it is imperative to compare and contrast it with other possible solutions. TABLE  III  depicts some of such possible comparison points. Alternate Solutions:  Although  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "The possible alternate solu- tions being battery-backed custom HW [ 16 ], battery-backed commercial GPU and ﬁxed power budget with store and \n902 \nAuthorized licensed use limited to: Penn State University. TABLE  III  depicts some of such possible comparison points. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "0 \n100 \n200 \n300 \n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 \n# TIles Utilized \nTraning Iteration \n#Tiles-Oracle ηdŝůĞƐͲhƔĄƐ DadianNao ŵĞĂŶͲhƔĄƐ Mean-DaDianNao \n(a) Monotonically increasing \n0 \n50 \n100 \n1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 \n#Tiles Utilized \nTraining Iteration \n#Tiles-Oracle dŝůĞƐͲhƔĄƐ DaDianNao DĞĂŶͲhƔĄƐ Mean-DaDianNao \n(b) Rapidly varying \nFig. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. Restrictions apply.",
      "type": "sliding_window_shuffled",
      "tokens": 189,
      "augmented": true
    },
    {
      "text": "11: Tile utilization against available power:  Us. ´as  with eager scheduling vs an oracle scheduler. 0 \n100 \n200 \n300 \n1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 \n# TIles Utilized \nTraning Iteration \n#Tiles-Oracle ηdŝůĞƐͲhƔĄƐ DadianNao ŵĞĂŶͲhƔĄƐ Mean-DaDianNao \n(a) Monotonically increasing \n0 \n50 \n100 \n1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 \n#Tiles Utilized \nTraining Iteration \n#Tiles-Oracle dŝůĞƐͲhƔĄƐ DaDianNao DĞĂŶͲhƔĄƐ Mean-DaDianNao \n(b) Rapidly varying \nFig.",
      "type": "sliding_window_shuffled",
      "tokens": 187,
      "augmented": true
    },
    {
      "text": "Us. ´as  closely tracks oracle, where as DaDianNao [ 16 ] falls short. ´as  with eager scheduling vs an oracle scheduler.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "We quantitatively compare the effective training, i.e. ´as  closely tracks oracle, where as DaDianNao [ 16 ] falls short. execute (using a capacitor/ battery).",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "the ratio of number of scheduled training to the number of completed training, loss of accuracy compared to the baseline. We quantitatively compare the effective training, i.e. It is clear that even with intermittent power availability  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "Furthermore, we also present a qualitative com- parison on the maintenance cycle needed for these solutions. ´as  is objectively ﬁnishing more tasks (except compared to a system with a consistently high power availability). It is clear that even with intermittent power availability  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "While a completely grid based solution is best in terms of reliability, it is not feasible because of the power demands. Any battery backed system will be limited to the charging cycle of the batteries ( ≈ 500 cycles for Li-ion batteries) which leads to a typical 18 to 24 months of life for such devices (compared to this, a super capacitor have a life of more than 100 years). Furthermore, we also present a qualitative com- parison on the maintenance cycle needed for these solutions.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "´as  will be limited either by the live of the harvesting source ( ≈ 20 – 30 years for solar, ≈ 10 – 12 years for portable wind turbines), or the training hardware (typical life cycle of embedded devices are of range of 7 – 10 years). We believe that the lifetime of  Us. Any battery backed system will be limited to the charging cycle of the batteries ( ≈ 500 cycles for Li-ion batteries) which leads to a typical 18 to 24 months of life for such devices (compared to this, a super capacitor have a life of more than 100 years).",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "´as  will be limited either by the live of the harvesting source ( ≈ 20 – 30 years for solar, ≈ 10 – 12 years for portable wind turbines), or the training hardware (typical life cycle of embedded devices are of range of 7 – 10 years). We agree that a limitation of our work comes from the choice of solar energy: unavailability during night and bad weather makes the deployment harder. However, there has been signiﬁcant recent development in portable wind turbines [ 96 ], which can be deployed on rooftops, can work with  ≥ 5 mph  wind speed, and can provide power equivalent of 15 solar cells.",
      "type": "sliding_window_shuffled",
      "tokens": 140,
      "augmented": true
    },
    {
      "text": "However, there has been signiﬁcant recent development in portable wind turbines [ 96 ], which can be deployed on rooftops, can work with  ≥ 5 mph  wind speed, and can provide power equivalent of 15 solar cells. Deployment Training Completed \nMean Power Consumed (W) \nMean Power \nWasted (W) \nCarbon Footprint (lbs/yr) Us. Therefore, similar technologies can be used to augment the harvesting mechanism.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "´as 11 17.2 3.54 8.33 DaDianNao (persistent) 6 6.4 10.8 128.0712 DaDianNao (Software Only) 4 5.8 12.46 135.964 DaDianNao (actual) 2 2.09 24.73 199.70 Edge Cloud 0 0 – Cloud 1 200 0 2233.8 Max Power = 32W; Min Power = 12W; Training Scheduled = 12 \nTABLE IV: Comparing  Us. ´as  hardware with other state of the art offerings for both performance and sustainability. Deployment Training Completed \nMean Power Consumed (W) \nMean Power \nWasted (W) \nCarbon Footprint (lbs/yr) Us.",
      "type": "sliding_window_shuffled",
      "tokens": 162,
      "augmented": true
    },
    {
      "text": "´as  plays a crucial role in efﬁciently handling varying energy income and workloads. D. Towards Other Applications and Domains \nThe morphable hardware design of  Us. ´as  hardware with other state of the art offerings for both performance and sustainability.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "As the energy income becomes more sporadic, hardware- assisted scheduling seamlessly transfers work to active pro- cessing elements (PEs), maximizing the completion of tasks that could have otherwise been lost. This hardware-driven adaptive scheduling signiﬁcantly impacts different data modal- ities, from large-scale to small-scale, and various magnitudes of energy income, as depicted in Fig. ´as  plays a crucial role in efﬁciently handling varying energy income and workloads.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "This hardware-driven adaptive scheduling signiﬁcantly impacts different data modal- ities, from large-scale to small-scale, and various magnitudes of energy income, as depicted in Fig. For scenarios with larger and predictable energy income, software-based backup and restore mechanisms can offer signiﬁcant beneﬁts, as the energy consumed for such operations is typically a small fraction of the overall energy income. 10c .",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "However, in situations with sporadic energy income, the hardware-assisted scheduling becomes paramount. For scenarios with larger and predictable energy income, software-based backup and restore mechanisms can offer signiﬁcant beneﬁts, as the energy consumed for such operations is typically a small fraction of the overall energy income. Predictive actions for saving the system state can be easily taken.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "It ensures that active PEs efﬁciently utilize available power to complete work, preventing potential losses and eliminating the need to restart tasks from the beginning. However, in situations with sporadic energy income, the hardware-assisted scheduling becomes paramount. Us.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "As data and model dimensions decrease, the hardware assistance’s impact becomes more pronounced, making  Us. Us. ´as  excels as a candidate for continuous learning at all scales due to the hardware’s adaptability to varying data and model sizes.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "´as  an excellent solution for continuous learning across diverse application sizes. As data and model dimensions decrease, the hardware assistance’s impact becomes more pronounced, making  Us. Along with morphable hardware, the exemplar selection and the micro-proﬁler play an important role for the success of  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "´as . When power is highly uncertain, the morphable hardware also strongly contributes, however, as the power proﬁle becomes stable, the algorithmic contributions dominate. Along with morphable hardware, the exemplar selection and the micro-proﬁler play an important role for the success of  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "Fig. 10a  shows the contribution of the different components of  Us. When power is highly uncertain, the morphable hardware also strongly contributes, however, as the power proﬁle becomes stable, the algorithmic contributions dominate.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "Moreover, the algo- rithmic contributions can be extended into any classiﬁcation based application or data modality. 10a  shows the contribution of the different components of  Us. ´as  under different power proﬁles.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "If the learning has to be unsupervised, one needs to experiment with known clustering techniques to decide the right classiﬁcation approach. Moreover, the algo- rithmic contributions can be extended into any classiﬁcation based application or data modality. We demonstrate this by testing the exemplar selection and the μ − proﬁler with different modalities of data.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "Observe that, as  Us. We demonstrate this by testing the exemplar selection and the μ − proﬁler with different modalities of data. Our workloads included Audio [ 23 ], [ 35 ](speech classiﬁcation), 3D Point Clouds [ 14 ], [ 24 ](object classiﬁcation) and Inertial Measure- ment Unit sensor data [ 9 ], [ 106 ](fault and activity detection).",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "´as  is designed to handle dense and noisy data, it outperforms the respective state-of-the-arts (which were tuned for small, clean benchmark data). Observe that, as  Us. 903 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "Restrictions apply. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. 903 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "Restrictions apply. VI. D ISCUSSION \nKey insights:  Compared to other systems, the ratio of energy requirement of task vs the harvested energy is much higher here.",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "D ISCUSSION \nKey insights:  Compared to other systems, the ratio of energy requirement of task vs the harvested energy is much higher here. While many of the prior works have designed their systems around inference using intermittent systems, we are one of the few works which focuses on learning, and the only work which does it on a large scale of data. Added with time constraints, designing such a system becomes tricky.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "This gives us a unique platform to think of intermittency beyond embedded systems and energy. While many of the prior works have designed their systems around inference using intermittent systems, we are one of the few works which focuses on learning, and the only work which does it on a large scale of data. The proposed system is not only energy intermittent, but also memory intermittent, interconnect intermittent and most importantly data intermittent (we don’t know how much data and what data).",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "Only intermittent learn- ing [ 47 ] focuses on performing on-device training, but with very small workloads and models. This gives us a unique platform to think of intermittency beyond embedded systems and energy. Related Works:  Although there has been signiﬁcant re- search [ 40 ], [ 41 ], [ 47 ], [ 52 ], [ 56 ], [ 61 ], [ 72 ], [ 104 ] on enabling machine learning in intermittently powered devices, a majority of it focuses on performing inference.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "Only intermittent learn- ing [ 47 ] focuses on performing on-device training, but with very small workloads and models. Similarly, Ekya [ 12 ] only focuses on co-location of computation, and it’s efﬁciency on ﬁnishing compute even on a custom hardware is shown in Fig. Considering the scale, scope and workload of our problem, limits direct comparisons, ex- cept for comparing their exemplar selection method.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Green Data Centers:  As sustainability gains traction, industry has worked towards building green data centers [ 58 ], [ 59 ]. Similarly, Ekya [ 12 ] only focuses on co-location of computation, and it’s efﬁciency on ﬁnishing compute even on a custom hardware is shown in Fig. 4 .",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "Moreover, communicating and storing such high volume data will also require energy. Green Data Centers:  As sustainability gains traction, industry has worked towards building green data centers [ 58 ], [ 59 ]. Although using these data centers for computation can be an alternative, it will not solve the bandwidth and the privacy issues mentioned in § I .",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "Our solu- tion decentralizes this massive compute using a sustainable approach and hence has its own merits. Further, this can help build future solutions using these decentralised nodes for other applications. Moreover, communicating and storing such high volume data will also require energy.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "We do encourage the use of green data centers for other centralized compute applications. Further, this can help build future solutions using these decentralised nodes for other applications. VII.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "C ONCLUDING  R EMARKS \nThe growth of smart cities and urban mobility applications, along with reformations in privacy laws, have produced a need for pervasive, DNN based continuous learning at the edge. VII. Although current commercial devices are capable of handling inference at the edge, the power and resource requirements of training make it impractical and unsustainable for all edge nodes to also perform continuous training off of grid power.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "Although current commercial devices are capable of handling inference at the edge, the power and resource requirements of training make it impractical and unsustainable for all edge nodes to also perform continuous training off of grid power. In this work, we design  Us. ´as , a sustainable continuous learning platform, which can perform video analytics by using an inter- mittent power source like solar power.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "´as  delivers 4.96% more accurate classiﬁcation compared to a na¨ıve learner, and the morphable hardware design uses intermittent computing to maintain forward progress even while running on lower power budget. ´as , a sustainable continuous learning platform, which can perform video analytics by using an inter- mittent power source like solar power. The learning algorithm of  Us.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "´as  delivers 4.96% more accurate classiﬁcation compared to a na¨ıve learner, and the morphable hardware design uses intermittent computing to maintain forward progress even while running on lower power budget. Together,  Us. ´as  can save up to 200lbs of  CO 2  per year compared to a state of the art accelerator running on the grid.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "A CKNOWLEDGMENTS \nWe would like to offer our thanks to the anonymous reviewers for their detailed feedback, which has greatly helped to improve and reﬁne this paper. VIII. ´as  can save up to 200lbs of  CO 2  per year compared to a state of the art accelerator running on the grid.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "A CKNOWLEDGMENTS \nWe would like to offer our thanks to the anonymous reviewers for their detailed feedback, which has greatly helped to improve and reﬁne this paper. This work was supported in part by Semiconductor Research Corporation (SRC), Cen- ter for Brain-inspired Computing (C-BRIC) and NSF Grant #1822923 (SPX: SOPHIA). We acknowledge that all product names used are for identiﬁcation purposes only and may be trademarks of their respective companies.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "We acknowledge that all product names used are for identiﬁcation purposes only and may be trademarks of their respective companies. R EFERENCES \n[1] S. M. Abhay S, “Autonomous vehicle market by level of au- \ntomation,”  https://www.alliedmarketresearch.com/autonomous-vehicle- market , Feb 2022, (Accessed on 08/04/2023). [2] “Achieving Compliant Data Residency and Security with Azure,” \nhttps://azure.microsoft.com/mediahandler/ﬁles/resourceﬁles/achieving- compliant-data-residency-and-security-with-azure/Achieving Compliant Data Residency and Security with Azure.pdf .",
      "type": "sliding_window_shuffled",
      "tokens": 168,
      "augmented": true
    },
    {
      "text": "[2] “Achieving Compliant Data Residency and Security with Azure,” \nhttps://azure.microsoft.com/mediahandler/ﬁles/resourceﬁles/achieving- compliant-data-residency-and-security-with-azure/Achieving Compliant Data Residency and Security with Azure.pdf . 13, no. [3] D. B. Agusdinata, W. Liu, H. Eakin, and H. Romero, “Socio- \nenvironmental impacts of lithium mineral extraction: towards a research agenda,”  Environmental Research Letters , vol.",
      "type": "sliding_window_shuffled",
      "tokens": 140,
      "augmented": true
    },
    {
      "text": "[4] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and \nA. Moshovos, “Cnvlutin: Ineffectual-neuron-free deep neural network computing,”  ACM SIGARCH Computer Architecture News , vol. 12, p. 123001, 2018. 13, no.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "[4] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and \nA. Moshovos, “Cnvlutin: Ineffectual-neuron-free deep neural network computing,”  ACM SIGARCH Computer Architecture News , vol. 44, no. 3, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "3, pp. [5] W. Amit Katwala, “The spiralling environmental cost of our lithium \nbattery addiction,” https://www.wired.co.uk/article/lithium-batteries- environment-impact , May 2018, (Accessed on 07/08/2023). 1–13, 2016.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "[5] W. Amit Katwala, “The spiralling environmental cost of our lithium \nbattery addiction,” https://www.wired.co.uk/article/lithium-batteries- environment-impact , May 2018, (Accessed on 07/08/2023). [7] AWS Outposts, “https://aws.amazon.com/outposts/rack/hardware- specs/?nc=sn&loc=4.” [8] Azure Stack Edge, “https://azure.microsoft.com/en- us/services/databox/edge/.” [9] O. Banos, C. Villalonga, R. Garc´ıa, A. Saez, M. Damas, J. Holgado- \nTerriza, S. Lee, H. Pomares, and I. Rojas, “Design, implementation and validation of a novel open framework for agile development of mobile health applications,”  BioMedical Engineering OnLine , 2015. [6] G. Ananthanarayanan, V. Bahl, P. Bod´ık, K. Chintalapudi, M. Philipose, \nL. R. Sivalingam, and S. Sinha, “Real-time Video Analytics – the killer app for edge computing,”  IEEE Computer , 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 308,
      "augmented": true
    },
    {
      "text": "[11] Beverly Hills has thousands of surveillance cameras, “https://bit.ly/BeverlyHillsCamera.” [12] R. Bhardwaj, Z. Xia, G. Ananthanarayanan, J. Jiang, Y. Shu, N. Kar- \nianakis, K. Hsieh, P. Bahl, and I. Stoica, “Ekya: Continuous learning of video analytics models on edge compute servers,” in  19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , 2022, pp. [7] AWS Outposts, “https://aws.amazon.com/outposts/rack/hardware- specs/?nc=sn&loc=4.” [8] Azure Stack Edge, “https://azure.microsoft.com/en- us/services/databox/edge/.” [9] O. Banos, C. Villalonga, R. Garc´ıa, A. Saez, M. Damas, J. Holgado- \nTerriza, S. Lee, H. Pomares, and I. Rojas, “Design, implementation and validation of a novel open framework for agile development of mobile health applications,”  BioMedical Engineering OnLine , 2015. [10] S. Bauer, “Explainer: The opportunities and challenges of the lithium \nindustry,”  Di´alogo Chino , 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 346,
      "augmented": true
    },
    {
      "text": "[13] R. Bird, Z. J. Baum, X. Yu, and J. Ma, “The regulatory environment \nfor lithium-ion battery recycling,” 2022. [11] Beverly Hills has thousands of surveillance cameras, “https://bit.ly/BeverlyHillsCamera.” [12] R. Bhardwaj, Z. Xia, G. Ananthanarayanan, J. Jiang, Y. Shu, N. Kar- \nianakis, K. Hsieh, P. Bahl, and I. Stoica, “Ekya: Continuous learning of video analytics models on edge compute servers,” in  19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , 2022, pp. 119–135.",
      "type": "sliding_window_shuffled",
      "tokens": 185,
      "augmented": true
    },
    {
      "text": "[14] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, \nA. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous driving,” in  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , 2020, pp. [13] R. Bird, Z. J. Baum, X. Yu, and J. Ma, “The regulatory environment \nfor lithium-ion battery recycling,” 2022. 11 621–11 631.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "Yang, J. Emer, and V. Sze, “Eyeriss v2: A ﬂexible \naccelerator for emerging deep neural networks on mobile devices,” IEEE Journal on Emerging and Selected Topics in Circuits and Systems , vol. [15] Y.-H. Chen, T.-J. 11 621–11 631.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Yang, J. Emer, and V. Sze, “Eyeriss v2: A ﬂexible \naccelerator for emerging deep neural networks on mobile devices,” IEEE Journal on Emerging and Selected Topics in Circuits and Systems , vol. 2, pp. 9, no.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "292–308, 2019. [16] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, \nZ. Xu, N. Sun  et al. 2, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "[16] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen, \nZ. Xu, N. Sun  et al. , “Dadiannao: A machine-learning supercomputer,” in  2014 47th Annual IEEE/ACM International Symposium on Microar- chitecture . IEEE, 2014, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "IEEE, 2014, pp. [17] P. Chi, S. Li, Y. Cheng, Y. Lu, S. H. Kang, and Y. Xie, “Architecture \ndesign with stt-ram: Opportunities and challenges,” in  2016 21st Asia and South Paciﬁc design automation conference (ASP-DAC) . 609–622.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "[17] P. Chi, S. Li, Y. Cheng, Y. Lu, S. H. Kang, and Y. Xie, “Architecture \ndesign with stt-ram: Opportunities and challenges,” in  2016 21st Asia and South Paciﬁc design automation conference (ASP-DAC) . IEEE, 2016, pp. 109–114.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "904 \nAuthorized licensed use limited to: Penn State University. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. 109–114.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "Restrictions apply. [18] A. F. CNBC, “How trafﬁc sensors and cameras are trans- forming city streets,”  https://www.cnbc.com/2021/02/22/how-trafﬁc- sensors-and-cameras-are-transforming-city-streets.html , (Accessed on 04/28/2023). Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "[19] Coral.ai, “Edge tpu performance benchmarks =  https://coral.ai/docs/ \nedgetpu/benchmarks/ ,” (Accessed on 11/21/2022). [20] D Maltoni, V Lomonaco, “Continuous learning in single-incremental- \ntask scenarios,” in  Neural Networks , 2019. [18] A. F. CNBC, “How trafﬁc sensors and cameras are trans- forming city streets,”  https://www.cnbc.com/2021/02/22/how-trafﬁc- sensors-and-cameras-are-transforming-city-streets.html , (Accessed on 04/28/2023).",
      "type": "sliding_window_shuffled",
      "tokens": 161,
      "augmented": true
    },
    {
      "text": "Dodge, T. Prewitt, R. Tachet des Combes, E. Odmark, R. Schwartz, \nE. Strubell, A. S. Luccioni, N. A. Smith, N. DeCario, and W. Buchanan, “Measuring the carbon intensity of ai in cloud instances,” in  2022 ACM Conference on Fairness, Accountability, and Transparency , 2022, pp. [21] J. [20] D Maltoni, V Lomonaco, “Continuous learning in single-incremental- \ntask scenarios,” in  Neural Networks , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "Dodge, T. Prewitt, R. Tachet des Combes, E. Odmark, R. Schwartz, \nE. Strubell, A. S. Luccioni, N. A. Smith, N. DeCario, and W. Buchanan, “Measuring the carbon intensity of ai in cloud instances,” in  2022 ACM Conference on Fairness, Accountability, and Transparency , 2022, pp. [22] E. Dong, Y. Zhu, Y. Ji, and S. Du, “An improved convolution neural \network for object detection using yolov2,” in  2018 IEEE International Conference on Mechatronics and Automation (ICMA) . 1877–1894.",
      "type": "sliding_window_shuffled",
      "tokens": 176,
      "augmented": true
    },
    {
      "text": "1184–1188. [22] E. Dong, Y. Zhu, Y. Ji, and S. Du, “An improved convolution neural \network for object detection using yolov2,” in  2018 IEEE International Conference on Mechatronics and Automation (ICMA) . IEEE, 2018, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "IEEE, 2015, pp. [23] P. Foster, S. Sigtia, S. Krstulovic, J. Barker, and M. D. Plumbley, \n“Chime-home: A dataset for sound source recognition in a domestic environment,” in  2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) . 1184–1188.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "IEEE, 2015, pp. [24] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous \ndriving? 1–5.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "the kitti vision benchmark suite,” in  2012 IEEE conference on computer vision and pattern recognition . [24] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous \ndriving? IEEE, 2012, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "[25] N. Global Monitoring Laboratory, “Solrad network,”  https://gml.noaa. IEEE, 2012, pp. 3354– 3361.",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "[25] N. Global Monitoring Laboratory, “Solrad network,”  https://gml.noaa. [26] G. Gobieski, B. Lucia, and N. Beckmann, “Intelligence beyond the \nedge: Inference on intermittent embedded systems,” in  ASPLOS . gov/grad/solrad/index.html , (Accessed on 11/21/2022).",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "ACM, 2019. [27] Z. Gong, H. Ji, C. W. Fletcher, C. J. Hughes, and J. Torrellas, \n“Sparsetrain: Leveraging dynamic sparsity in software for training dnns on general-purpose simd processors,” in  Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques , 2020, pp. [26] G. Gobieski, B. Lucia, and N. Beckmann, “Intelligence beyond the \nedge: Inference on intermittent embedded systems,” in  ASPLOS .",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "279–292. [28] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, B. Sharma, M. T. \nKandemir, and C. R. Das, “Cocktail: A multidimensional optimization for model serving in cloud,” in  19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , 2022, pp. [27] Z. Gong, H. Ji, C. W. Fletcher, C. J. Hughes, and J. Torrellas, \n“Sparsetrain: Leveraging dynamic sparsity in software for training dnns on general-purpose simd processors,” in  Proceedings of the ACM International Conference on Parallel Architectures and Compilation Techniques , 2020, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 192,
      "augmented": true
    },
    {
      "text": "1041–1057. [29] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y. [28] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, B. Sharma, M. T. \nKandemir, and C. R. Das, “Cocktail: A multidimensional optimization for model serving in cloud,” in  19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22) , 2022, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 143,
      "augmented": true
    },
    {
      "text": "Wu, “Chasing carbon: The elusive environmental footprint of computing,”  IEEE Micro , vol. Wei, \nD. Brooks, and C.-J. [29] U. Gupta, Y. G. Kim, S. Lee, J. Tse, H.-H. S. Lee, G.-Y.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "4, pp. 42, no. Wu, “Chasing carbon: The elusive environmental footprint of computing,”  IEEE Micro , vol.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "37–47, 2022. [30] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing \ndeep neural networks with pruning, trained quantization and huffman coding,”  arXiv preprint arXiv:1510.00149 , 2015. 4, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "[31] J. He and F. Zhu, “Online continual learning for visual food classiﬁ- \ncation,” in  Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. [30] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing \ndeep neural networks with pruning, trained quantization and huffman coding,”  arXiv preprint arXiv:1510.00149 , 2015.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "2337–2346. He and F. Zhu, “Online continual learning for visual food classiﬁ- \ncation,” in  Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. [32] K. He, X. Zhang, S. Ren, and J.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "Sun, “Identity mappings in deep resid- \nual networks,” in  European conference on computer vision . [32] K. He, X. Zhang, S. Ren, and J. Springer, 2016, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "[33] IBM, “Data labeling,”  https://www.ibm.com/cloud/learn/data-labeling , \n(Accessed on 11/21/2022). 630–645. Springer, 2016, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "[34] I. Ilievski, T. Akhtar, J. Feng, and C. Shoemaker, “Efﬁcient hyperpa- \nrameter optimization for deep learning algorithms using deterministic rbf surrogates,” in  Proceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 31, no. [33] IBM, “Data labeling,”  https://www.ibm.com/cloud/learn/data-labeling , \n(Accessed on 11/21/2022).",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "31, no. 1, 2017. [35] Z. Jackson, “Free spoken digit dataset (fsdd),” https://github.",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "[36] C. Jones and J. D. Ryan,  Encyclopedia of hinduism . [35] Z. Jackson, “Free spoken digit dataset (fsdd),” https://github. com/Jakobovski/free-spoken-digit-dataset , July 2022, (Accessed on 07/08/2023).",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "[37] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, \nS. Bates, S. Bhatia, N. Boden, A. Borchers  et al. Infobase publishing, 2006. [36] C. Jones and J. D. Ryan,  Encyclopedia of hinduism .",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": ", “In-datacenter performance analysis of a tensor processing unit,” in  Proceedings of the 44th annual international symposium on computer architecture , 2017, pp. [37] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa, \nS. Bates, S. Bhatia, N. Boden, A. Borchers  et al. 1–12.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "[38] Junchen Jiang, Ganesh Ananthanarayanan, Peter Bod´ık, Siddhartha \nSen, Ion Stoica, “Chameleon: Scalable adaptation of video analytics,” in  ACM SIGCOMM , 2018. [39] Junjue Wang, Ziqiang Feng, Shilpa George, Roger Iyengar, Pillai Pad- \nmanabhan, Mahadev Satyanarayanan, “Towards scalable edge-native applications,” in  ACM/IEEE Symposium on Edge Computing , 2019. 1–12.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "[39] Junjue Wang, Ziqiang Feng, Shilpa George, Roger Iyengar, Pillai Pad- \nmanabhan, Mahadev Satyanarayanan, “Towards scalable edge-native applications,” in  ACM/IEEE Symposium on Edge Computing , 2019. [40] C.-K. Kang, H. R. Mendis, C.-H. Lin, M.-S. Chen, and P.-C. Hsiu, \n“Everything leaves footprints: Hardware accelerated intermittent deep inference,”  IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , vol. 39, no.",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "21, no. [41] C.-K. Kang, H. R. Mendis, C.-H. Lin, M.-S. Chen, and P.-C. Hsiu, \n“More is less: Model augmentation for intermittent deep inference,” ACM Transactions on Embedded Computing Systems (TECS) , vol. 3479–3491, 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "1–26, 2022. VLDB Endow. [42] D. Kang, J. Emmons, F. Abuzaid, P. Bailis, and M. Zaharia, “Noscope: \nOptimizing deep cnn-based queries over video streams at scale.”  Proc.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "1586–1597, 2017. [43] Keras, “Keras applications,”  https://keras.io/api/applications/ , (Ac- \ncessed on 11/21/2022). [44] Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari , “Incremental \nlearning of object detectors without catastrophic forgetting,” in  ICCV , 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "6351–6360. [45] G. Kordopatis-Zilos, S. Papadopoulos, I. Patras, and I. Kompatsiaris, \n“Visil: Fine-grained spatio-temporal video similarity learning,” in  Pro- ceedings of the IEEE/CVF international conference on computer vision , 2019, pp. [44] Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari , “Incremental \nlearning of object detectors without catastrophic forgetting,” in  ICCV , 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "[46] S. Lee, S. Goldt, and A. Saxe, “Continual learning in the teacher- \nstudent setup: Impact of task similarity,” in  International Conference on Machine Learning . PMLR, 2021, pp. 6351–6360.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "PMLR, 2021, pp. 6109–6119. [47] S. Lee, B. Islam, Y. Luo, and S. Nirjon, “Intermittent learning: On- \ndevice machine learning on intermittently powered system,”  Proceed- ings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies , vol.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "4, pp. 3, no. [47] S. Lee, B. Islam, Y. Luo, and S. Nirjon, “Intermittent learning: On- \ndevice machine learning on intermittently powered system,”  Proceed- ings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies , vol.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "4, pp. [48] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar, \n“Hyperband: A novel bandit-based approach to hyperparameter opti- mization,”  The Journal of Machine Learning Research , vol. 1–30, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "[48] L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar, \n“Hyperband: A novel bandit-based approach to hyperparameter opti- mization,”  The Journal of Machine Learning Research , vol. 1, pp. 18, no.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "[49] S. Li, Z. Yang, D. Reddy, A. Srivastava, and B. Jacob, “Dramsim3: \na cycle-accurate, thermal-capable dram simulator,”  IEEE Computer Architecture Letters , vol. 6765–6816, 2017. 1, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "19, no. [49] S. Li, Z. Yang, D. Reddy, A. Srivastava, and B. Jacob, “Dramsim3: \na cycle-accurate, thermal-capable dram simulator,”  IEEE Computer Architecture Letters , vol. 2, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "[50] Z. Li and D. Hoiem, “Learning without forgetting,”  IEEE transactions \non pattern analysis and machine intelligence , vol. 106–109, 2020. 2, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "12, pp. 40, no. [50] Z. Li and D. Hoiem, “Learning without forgetting,”  IEEE transactions \non pattern analysis and machine intelligence , vol.",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "2935– 2947, 2017. [51] M Sandler, A Howard, Menglong Zhu, Andrey Zhmoginov, Liang- \nChieh Chen , “Mobilenetv2: Inverted residuals and linear bottlenecks,” in  CVPR , 2018. 12, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "[51] M Sandler, A Howard, Menglong Zhu, Andrey Zhmoginov, Liang- \nChieh Chen , “Mobilenetv2: Inverted residuals and linear bottlenecks,” in  CVPR , 2018. USENIX Association, 2018. [52] K. Maeng and B. Lucia, “Adaptive dynamic checkpointing for safe \nefﬁcient intermittent computing,” in  OSDI .",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "[53] B. Makuza, Q. Tian, X. Guo, K. Chattopadhyay, and D. Yu, “Py- \nrometallurgical options for recycling spent lithium-ion batteries: A comprehensive review,”  Journal of Power Sources , vol. 491, p. 229622, 2021. USENIX Association, 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "[55] M. McCloskey and N. J. Cohen, “Catastrophic interference in connec- \ntionist networks: The sequential learning problem,” in  Psychology of learning and motivation . [54] Markets and Markets, “Smart cities market analysis, industry size and \nforecast,”  https://www.marketsandmarkets.com/Market-Reports/smart- cities-market-542.html#: ∼ :text=The%20global%20Smart%20Cities% 20Market,drive%20the%20smart%20cities%20market , 2022, (Accessed on 08/04/2023). 491, p. 229622, 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 162,
      "augmented": true
    },
    {
      "text": "[55] M. McCloskey and N. J. Cohen, “Catastrophic interference in connec- \ntionist networks: The sequential learning problem,” in  Psychology of learning and motivation . 24, pp. Elsevier, 1989, vol.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "109–165. [56] H. R. Mendis, C.-K. Kang, and P.-c. Hsiu, “Intermittent-aware neu- \nral architecture search,”  ACM Transactions on Embedded Computing Systems (TECS) , vol. 24, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "5s, pp. 20, no. [56] H. R. Mendis, C.-K. Kang, and P.-c. Hsiu, “Intermittent-aware neu- \nral architecture search,”  ACM Transactions on Embedded Computing Systems (TECS) , vol.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "5s, pp. 1–27, 2021. [57] Meta, “Sharing our progress on combating climate change = https://about.fb.com/news/2022/11/metas-progress-on-combating- climate-change/ ,” (Accessed on 11/21/2022).",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "[57] Meta, “Sharing our progress on combating climate change = https://about.fb.com/news/2022/11/metas-progress-on-combating- climate-change/ ,” (Accessed on 11/21/2022). [59] Microsoft, “Building world-class sustainable datacenters and investing \nin solar power in arizona,” https://blogs.microsoft.com/on-the- issues/2019/07/30/building-world-class-sustainable-datacenters-and- investing-in-solar-power-in-arizona/ , (Accessed on 04/28/2023). [58] Meta, “Sustainability: Data centers,”  https://sustainability.fb.com/data- \ncenters/ , (Accessed on 04/28/2023).",
      "type": "sliding_window_shuffled",
      "tokens": 186,
      "augmented": true
    },
    {
      "text": "[59] Microsoft, “Building world-class sustainable datacenters and investing \nin solar power in arizona,” https://blogs.microsoft.com/on-the- issues/2019/07/30/building-world-class-sustainable-datacenters-and- investing-in-solar-power-in-arizona/ , (Accessed on 04/28/2023). 905 \nAuthorized licensed use limited to: Penn State University. [60] Microsoft-Rocket-Video-Analytics-Platform, \n“https://github.com/microsoft/Microsoft-Rocket-Video-Analytics- Platform.” [61] C. S. Mishra, J. Sampson, M. T. Kandemir, and V. Narayanan, “Origin: \nEnabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks,” in  DATE , 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 217,
      "augmented": true
    },
    {
      "text": "905 \nAuthorized licensed use limited to: Penn State University. Restrictions apply. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "Restrictions apply. 27, p. 28, 2009. [62] N. Muralimanohar, R. Balasubramonian, and N. P. Jouppi, “Cacti 6.0: \nA tool to model large caches,”  HP laboratories , vol.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "[63] S.-H. Noh, J. Koo, S. Lee, J. Park, and J. Kung, “Flexblock: A ﬂexible \ndnn training accelerator with multi-mode block ﬂoating point support,” IEEE Transactions on Computers , 2023. 27, p. 28, 2009.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "[64] T. N. R. E. L. (NREL), “Solar resource maps and data,”  https://www. Park, and J. Kung, “Flexblock: A ﬂexible \ndnn training accelerator with multi-mode block ﬂoating point support,” IEEE Transactions on Computers , 2023. rel.gov/gis/solar-resource-maps.html , (Accessed on 11/21/2022).",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "rel.gov/gis/solar-resource-maps.html , (Accessed on 11/21/2022). [67] D. Patterson, J. Gonzalez, Q. [65] NVIDIA T4 for Virtualization, “http://bit.ly/3EﬁuWg.” [66] I. of Energy Research, “The environmental impact of lithium \nbatteries,” https://www.instituteforenergyresearch.org/renewable/the- environmental-impact-of-lithium-batteries/ , November 2020, (Accessed on 07/08/2023).",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "[67] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, \nD. Rothchild, D. So, M. Texier, and J. Dean, “Carbon emissions and large neural network training,”  arXiv preprint arXiv:2104.10350 , 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "[68] Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo, “Optimus: an efﬁcient \ndynamic resource scheduler for deep learning clusters,” in  Proceedings of the Thirteenth EuroSys Conference , 2018, pp. Dean, “Carbon emissions and large neural network training,”  arXiv preprint arXiv:2104.10350 , 2021. 1–14.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "[69] J. F. Peters, M. Baumann, B. Zimmermann, J. Braun, and M. Weil, “The \nenvironmental impact of li-ion batteries and the role of key parameters– a review,”  Renewable and Sustainable Energy Reviews , vol. 1–14. 67, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "[70] A. Prabhu, C. Dognin, and M. Singh, “Sampling bias in deep active \nclassiﬁcation: An empirical study,”  arXiv preprint arXiv:1909.09389 , 2019. 491–506, 2017. 67, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "[72] K. Qiu, N. Jao, M. Zhao, C. S. Mishra, G. Gudukbay, S. Jose, \nJ. Sampson, M. T. Kandemir, and V. Narayanan, “Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent em- bedded processors,” in  2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) . [71] I. X. G. Processors, “Rankings about energy in the world,” https://www.intel.com/content/www/us/en/products/details/processors/ xeon/scalable/gold/products.html , (Accessed on 11/21/2022). [70] A. Prabhu, C. Dognin, and M. Singh, “Sampling bias in deep active \nclassiﬁcation: An empirical study,”  arXiv preprint arXiv:1909.09389 , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 238,
      "augmented": true
    },
    {
      "text": "315– 327. IEEE, 2020, pp. [72] K. Qiu, N. Jao, M. Zhao, C. S. Mishra, G. Gudukbay, S. Jose, \nJ. Sampson, M. T. Kandemir, and V. Narayanan, “Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent em- bedded processors,” in  2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) .",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "315– 327. [73] Ravi Teja Mullapudi, Steven Chen, Keyi Zhang, Deva Ramanan, \nKayvon Fatahalian, “Online model distillation for efﬁcient video in- ference,” in  ICCV , 2019. [74] S.-A.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "[74] S.-A. Rebufﬁ, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl: \nIncremental classiﬁer and representation learning,” in  Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , 2017, pp. 2001–2010.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "[75] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” \narXiv preprint arXiv:1804.02767 , 2018. [76] G. V. Research, “Consumer iot market size, share and trends \nanalysis report by component (hardware, services), by connectivity technology (wired, wireless), by application (healthcare, wearable devices), and segment forecasts, 2023 - 2030,” https://www.grandviewresearch.com/industry-analysis/consumer- iot-market-report#: ∼ :text=The%20global%20consumer%20IoT% 20market,advanced%20devices%20and%20home%20appliances. 2001–2010.",
      "type": "sliding_window_shuffled",
      "tokens": 189,
      "augmented": true
    },
    {
      "text": ", July 2022, (Accessed on 08/04/2023). [76] G. V. Research, “Consumer iot market size, share and trends \nanalysis report by component (hardware, services), by connectivity technology (wired, wireless), by application (healthcare, wearable devices), and segment forecasts, 2023 - 2030,” https://www.grandviewresearch.com/industry-analysis/consumer- iot-market-report#: ∼ :text=The%20global%20consumer%20IoT% 20market,advanced%20devices%20and%20home%20appliances. [77] M. S. rutgers.edu, “Support for trafﬁc cameras increases if used as a tool to limit interactions with police,” https://www.rutgers.edu/news/support-trafﬁc-cameras-increases-if- used-tool-limit-interactions-police , (Accessed on 04/28/2023).",
      "type": "sliding_window_shuffled",
      "tokens": 245,
      "augmented": true
    },
    {
      "text": "[78] J. Salamon, C. Jacoby, and J. P. Bello, “A dataset and taxonomy \nfor urban sound research,” in  22nd ACM International Conference on Multimedia (ACM-MM’14) , Orlando, FL, USA, Nov. 2014, pp. [77] M. S. rutgers.edu, “Support for trafﬁc cameras increases if used as a tool to limit interactions with police,” https://www.rutgers.edu/news/support-trafﬁc-cameras-increases-if- used-tool-limit-interactions-police , (Accessed on 04/28/2023). 1041– 1044.",
      "type": "sliding_window_shuffled",
      "tokens": 161,
      "augmented": true
    },
    {
      "text": "[80] A. Sarma, S. Singh, H. Jiang, A. Pattnaik, A. K. Mishra, V. Narayanan, \nM. T. Kandemir, and C. R. Das, “Exploiting activation based gradient output sparsity to accelerate backpropagation in cnns,”  arXiv preprint arXiv:2109.07710 , 2021. [79] A. Samajdar, Y. Zhu, P. Whatmough, M. Mattina, and T. Kr- \nishna, “Scale-sim: Systolic cnn accelerator simulator,”  arXiv preprint arXiv:1811.02883 , 2018. 1041– 1044.",
      "type": "sliding_window_shuffled",
      "tokens": 189,
      "augmented": true
    },
    {
      "text": "[81] A. Sarma, S. Singh, H. Jiang, A. Pattnaik, A. K. Mishra, V. Narayanan, \nM. T. Kandemir, and C. R. Das, “Exploiting activation based gradient output sparsity to accelerate backpropagation in cnns,”  arXiv preprint arXiv:2109.07710 , 2021. [82] scale.com, “Data labeling: The authoritative guide,” https://scale.com/guides/data-labeling-annotation-guide#data-labeling- for-computer-vision , (Accessed on 11/21/2022). [80] A. Sarma, S. Singh, H. Jiang, A. Pattnaik, A. K. Mishra, V. Narayanan, \nM. T. Kandemir, and C. R. Das, “Exploiting activation based gradient output sparsity to accelerate backpropagation in cnns,”  arXiv preprint arXiv:2109.07710 , 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 269,
      "augmented": true
    },
    {
      "text": "[82] scale.com, “Data labeling: The authoritative guide,” https://scale.com/guides/data-labeling-annotation-guide#data-labeling- for-computer-vision , (Accessed on 11/21/2022). [83] A. W. Services, “Aws outposts rack pricing,”  https://aws.amazon.com/ \noutposts/rack/pricing/ , (Accessed on 11/21/2022). [84] K. Seyerlehner, G. Widmer, and P. Knees, “Frame level audio \nsimilarity-a codebook approach,” in  Proc.",
      "type": "sliding_window_shuffled",
      "tokens": 153,
      "augmented": true
    },
    {
      "text": "of the 11th Int. [84] K. Seyerlehner, G. Widmer, and P. Knees, “Frame level audio \nsimilarity-a codebook approach,” in  Proc. Conf.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "on Digital Audio Effects (DAFx-08) , 2008, p. 31. Conf. [85] Shadi Noghabi, Landon Cox, Sharad Agarwal, Ganesh Anantha- \narayanan, “The emerging landscape of edge-computing,” in  ACM SIGMOBILE GetMobile , 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "[85] Shadi Noghabi, Landon Cox, Sharad Agarwal, Ganesh Anantha- \narayanan, “The emerging landscape of edge-computing,” in  ACM SIGMOBILE GetMobile , 2020. [87] K. Simonyan and A. Zisserman, “Very deep convolutional networks for \nlarge-scale image recognition,”  arXiv preprint arXiv:1409.1556 , 2014. [86] Si Young Jang, Yoonhyung Lee, Byoungheon Shin, Dongman Lee, \nDionisio Vendrell Jacinto , “Application-aware iot camera virtualization for video analytics edge computing,” in  ACM/IEEE SEC , 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 180,
      "augmented": true
    },
    {
      "text": "[88] snorkel.ai, “Making automated data labeling a reality in modern ai,” \nhttps://snorkel.ai/automated-data-labeling/ , (Accessed on 11/21/2022). [89] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy con- \nsiderations for deep learning in nlp,”  arXiv preprint arXiv:1906.02243 , 2019. [87] K. Simonyan and A. Zisserman, “Very deep convolutional networks for \nlarge-scale image recognition,”  arXiv preprint arXiv:1409.1556 , 2014.",
      "type": "sliding_window_shuffled",
      "tokens": 166,
      "augmented": true
    },
    {
      "text": "[89] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy con- \nsiderations for deep learning in nlp,”  arXiv preprint arXiv:1906.02243 , 2019. [90] J. S.-M. studyﬁnds.org, “Trafﬁc cameras become more popular when \nthey cut down on interactions with police,”  https://studyﬁnds.org/ trafﬁc-cameras-interactions-police/ , (Accessed on 04/28/2023). [91] N. A. W. .",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "com/learning-center/solar-insolation-maps.html/#Map1 , (Accessed on 11/21/2022). [91] N. A. W. . Sun, “Us solar insolation maps,”  https://www.solar-electric.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "[92] “Surveillance camera statistics: which cities have the most cctv \ncameras?” https://www.comparitech.com/vpn-privacy/the-worlds- most-surveilled-cities/ , (Accessed on 11/21/2022). com/learning-center/solar-insolation-maps.html/#Map1 , (Accessed on 11/21/2022). [93] Synopsys, “Design compiler,” https://www.synopsys.com/ implementation-and-signoff/rtl-synthesis-test/dc-ultra.html , (Accessed on 04/28/2023).",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "[94] Synopsys, “Standard cell libraries,”  https://www.synopsys.com/dw/ \nipdir.php?ds=dwc standard cell , (Accessed on 04/28/2023). [95] E. Talpes, D. Williams, and D. D. Sarma, “Dojo: The microarchitecture \nof tesla’s exa-scale computer,” in  2022 IEEE Hot Chips 34 Symposium (HCS) . [93] Synopsys, “Design compiler,” https://www.synopsys.com/ implementation-and-signoff/rtl-synthesis-test/dc-ultra.html , (Accessed on 04/28/2023).",
      "type": "sliding_window_shuffled",
      "tokens": 172,
      "augmented": true
    },
    {
      "text": "1–28. IEEE Computer Society, 2022, pp. [95] E. Talpes, D. Williams, and D. D. Sarma, “Dojo: The microarchitecture \nof tesla’s exa-scale computer,” in  2022 IEEE Hot Chips 34 Symposium (HCS) .",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Technologies, “Aeromine technologies,” https://www. 1–28. [96] A.",
      "type": "sliding_window_shuffled",
      "tokens": 23,
      "augmented": true
    },
    {
      "text": "[97] Urban Trafﬁc Dataset, “https://github.com/edge-video- services/ekya#urban-trafﬁc-dataset.” [98] O. Wayman, “How urban mobility will change by 2030,” https://www.oliverwyman.com/our-expertise/insights/2022/jun/how- urban-mobility-will-change-by-2030.html , 2022, (Accessed on 08/04/2023). Technologies, “Aeromine technologies,” https://www. aerominetechnologies.com/ , (Accessed on 04/28/2023).",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "[99] P. with Code, “Object detection on coco test-dev,” https://paperswithcode.com/sota/object-detection-on-coco , (Accessed on 11/21/2022). [97] Urban Trafﬁc Dataset, “https://github.com/edge-video- services/ekya#urban-trafﬁc-dataset.” [98] O. Wayman, “How urban mobility will change by 2030,” https://www.oliverwyman.com/our-expertise/insights/2022/jun/how- urban-mobility-will-change-by-2030.html , 2022, (Accessed on 08/04/2023). [100] www.dlapiperdataprotection.com, “Sweden data collection & process- \ning,”  https://www.dlapiperdataprotection.com/index.html?t=collection- and-processing&c=SE , (Accessed on 11/21/2022).",
      "type": "sliding_window_shuffled",
      "tokens": 233,
      "augmented": true
    },
    {
      "text": "[101] J. Xu and X. Wang, “Rethinking self-supervised correspondence learn- \ning: A video frame-level similarity perspective,” in  Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021, pp. [100] www.dlapiperdataprotection.com, “Sweden data collection & process- \ning,”  https://www.dlapiperdataprotection.com/index.html?t=collection- and-processing&c=SE , (Accessed on 11/21/2022). 10 075–10 085.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "[102] T.-J. 10 075–10 085. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convo- \nlutional neural networks using energy-aware pruning,” in  Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "[103] T.-J. 5687–5695. Yang, Y.-H. Chen, and V. Sze, “Designing energy-efﬁcient convo- \nlutional neural networks using energy-aware pruning,” in  Proceedings of the IEEE conference on computer vision and pattern recognition , 2017, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "Yang, A. Howard, B. Chen, X. Zhang, A. [103] T.-J. Go, M. Sandler, V. Sze, \nand H. Adam, “Netadapt: Platform-aware neural network adaptation for mobile applications,” in  Proceedings of the European Conference on Computer Vision (ECCV) , 2018, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "[104] C.-H. Go, M. Sandler, V. Sze, \nand H. Adam, “Netadapt: Platform-aware neural network adaptation for mobile applications,” in  Proceedings of the European Conference on Computer Vision (ECCV) , 2018, pp. 285–300.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "Yen, H. R. Mendis, T.-W. Kuo, and P.-C. Hsiu, “Stateful neural \networks for intermittent systems,”  IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems , vol. [104] C.-H. 41, no.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "[105] Z. Ying, S. Zhao, H. Zhang, C. S. Mishra, S. Bhuyan, M. T. Kandemir, \nA. Sivasubramaniam, and C. R. Das, “Exploiting frame similarity for efﬁcient inference on edge devices,” in  2022 IEEE 42nd International Conference on Distributed Computing Systems (ICDCS) . 4229–4240, 2022. IEEE, 2022, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "IEEE, 2022, pp. 1073–1084. [106] R. Zhang, H. Tao, L. Wu, and Y. Guan, “Transfer learning with neural \networks for bearing fault diagnosis in changing working conditions,” Ieee Access , vol.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "[106] R. Zhang, H. Tao, L. Wu, and Y. Guan, “Transfer learning with neural \networks for bearing fault diagnosis in changing working conditions,” Ieee Access , vol. 14 347–14 357, 2017. 5, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "14 347–14 357, 2017. Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. 906 \nAuthorized licensed use limited to: Penn State University.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. [107] S. Zhao, H. Zhang, S. Bhuyan, C. S. Mishra, Z. Ying, M. T. Kandemir, \nA. Sivasubramaniam, and C. R. Das, “D´eja view: Spatio-temporal compute reuse for ‘energy-efﬁcient 360 vr video streaming,” in  2020 ACM/IEEE 47th Annual International Symposium on Computer Archi- tecture (ISCA) . Restrictions apply.",
      "type": "sliding_window_shuffled",
      "tokens": 141,
      "augmented": true
    },
    {
      "text": "IEEE, 2020, pp. [107] S. Zhao, H. Zhang, S. Bhuyan, C. S. Mishra, Z. Ying, M. T. Kandemir, \nA. Sivasubramaniam, and C. R. Das, “D´eja view: Spatio-temporal compute reuse for ‘energy-efﬁcient 360 vr video streaming,” in  2020 ACM/IEEE 47th Annual International Symposium on Computer Archi- tecture (ISCA) . 241–253.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "494–506. 241–253. [108] S. Zhao, H. Zhang, C. S. Mishra, S. Bhuyan, Z. Ying, M. T. Kandemir, \nA. Sivasubramaniam, and C. Das, “Holoar: On-the-ﬂy optimization of \n3d holographic processing for augmented reality,” in  MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture , 2021, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "8697–8710. [109] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable \narchitectures for scalable image recognition,” in  Proceedings of the IEEE conference on computer vision and pattern recognition , 2018, pp. 494–506.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Downloaded on April 02,2025 at 23:57:52 UTC from IEEE Xplore. 907 \nAuthorized licensed use limited to: Penn State University. 8697–8710.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    }
  ]
}