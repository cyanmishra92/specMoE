=== ORIGINAL PDF: 2503.22900v1_Learning_Library_Cell_Representations_in_Vector_Sp.pdf ===\n\nRaw text length: 34070 characters\nCleaned text length: 33817 characters\nNumber of segments: 19\n\n=== CLEANED TEXT ===\n\nLearning Library Cell Representations in Vector Space Rongjian Liang NVIDIA Yi-Chen Lu NVIDIA Wen-Hao Liu NVIDIA Haoxing Ren NVIDIA Abstract We propose Lib2Vec, a novel self-supervised framework to efficiently learn meaningful vector representations of library cells, enabling ML models to capture essential cell semantics. The framework comprises three key components: (1) an automated method for generating regularity tests to quantitatively evaluate how well cell representations reflect inter-cell relationships; (2) a self-supervised learning scheme that systematically extracts training data from Liberty files, removing the need for costly labeling; and (3) an attention-based model architecture that accommodates various pin counts and enables the creation of property- specific cell and arc embeddings. Experimental results demonstrate that Lib2Vec effectively captures functional and electrical similarities. Moreover, linear algebraic operations on cell vectors reveal meaningful relationships, such as vector(BUF) - vector(INV) vector(NAND) ap- proximating the vector of AND, showcasing the framework s nuanced representation capabilities. Lib2Vec also enhances downstream circuit learning applications, especially when labeled data is scarce. Index Terms representation learning, library cell, netlist optimization I. INTRODUCTION Library cell representations are vital for effective machine learning (ML)-based circuit analysis and optimization, as library cells are the fundamental building blocks of circuit netlists. Traditional methods often rely on manually defined features [1] [4], requiring extensive expertise and feature engineering. Alternatively, one-hot encoding [5] demands large amounts of domain-specific training data, which may not always be available. To address these limitations, this work ex- plores a data-efficient, self-supervised learning approach to generate cell representations. Our method maps library cells into a continuous vector space, capturing semantic relationships and enabling ML models to operate in a simpler, structured space instead of the original high-dimensional, discrete cell space. Our work also aligns with the broader AI shift toward training foundation models [6] on self-supervised data and adapting them to diverse downstream tasks. By providing a unified, self-supervised method for learning cell representations, this work has the potential to serve as a groundwork for developing circuit foundation models [7]. While related efforts, such as DeepGate [8], [9] and FCNN [10], [11], have introduced pre-training methods that achieve notable results in circuit representation, they primarily focus on structural and functional aspects of AND-Inverter graphs, overlooking other cell types and electrical properties. Moreover, they embed circuit knowledge within the weights of graph neural networks, restricting the transferability of this knowledge to other ML models. Another related research direction focuses on ML-based library cell char- acterization [12], [13]. While these methods have shown promise, they primarily aim to improve arc-based timing characterization accuracy rather than enabling ML models to capture and understand semantic relationships among cells. To our knowledge, efficient learning of functional and electrical representations of library cells in a vector space compatible with diverse ML architectures (including transformer, the most popular architecture for foundation models) remains unexplored. This work addresses that gap by enabling more general and versatile cell representation learning. A. Challenges We identify three key challenges in learning meaningful vector representations of library cells: 1) Defining and Evaluating Cell Representations: Defining the semantics of library cells, and devising effective test sets and metrics to assess representation quality. 2) Efficient Training Data Generation: Creating comprehensive training data that captures the diverse functional and electrical properties of cells. 3) Flexible Model Architecture: Developing an architecture that accommodates varying pin counts and enables straightforward generation of property-specific (e.g., cell delay-focused) and arc- specific representations from a cell s base representation. This is critical for downstream circuit learning tasks. B. Contributions Our key insight is that a cell s semantics are defined by its re- sponses to input conditions, allowing us to capture semantic relation- ships among cells based on their behaviors under the same inputs. By systematically collecting (input conditions, output responses) pairs, we can create data that encapsulates both the functional and electrical properties of cells. Building on this, we introduce Lib2Vec, a novel self-supervised learning framework for library cell representation. Our main contributions are summarized as follows: 1) First systematic exploration of learning library cell representa- tion in vector space, adaptable to various ML architectures and circuit applications; 2) An automated method for generating regularity tests to quanti- tatively evaluate the quality of cell representations; 3) A self-supervised learning scheme that extracts training data from Liberty files, removing the need for costly labeling; 4) An attention-based architecture that accommodates various pin counts and enables property- and arc-specific embeddings; 5) Experimental results show that Lib2Vec effectively captures functional and electrical properties of cells. Lib2Vec also en- hances downstream circuit learning applications, particularly in scenarios with limited labeled data. The remainder of this paper is organized as follows. Section II introduces key properties of library cells, providing essential back- ground for this study. Section III gives an overview of the Lib2Vec framework, with detailed descriptions of its three components in Sec- tion IV, Section V and Section VI. Section VII discusses the usage model of Lib2Vec. Experiment results are shown in Section VIII, and Section IX gives some concluding remarks. II. LIBRARY CELL PROPERTIES In VLSI design, library cells have three categories of properties: functional, electrical, and physical. Functional properties define a 1 arXiv:2503.22900v1 [cs.LG] 28 Mar 2025 Fig. 1. Overview of the lib2vec framework. cell s logical behavior. Electrical properties capture timing, power, and signal integrity, including parameters like propagation delay, transition time, capacitance, leakage and internal power, and noise margins. Physical properties describe a cell s layout and geometry, such as cell dimensions and pin locations. These properties can be further classified as static or dynamic. Static properties, like physical characteristics, remain constant, while dynamic properties, such as most functional and electrical behaviors, vary with input conditions. This work focuses on dynamic properties, specifically functional and electrical characteristics, as static properties are typically straightfor- ward to model. Specifically, Lib2Vec models functional properties and rise fall propagation delay, transition time, and internal power. The framework is extensible to incorporate additional properties. Liberty files [14] are a standard format to describe the functional and electrical properties of library cells. In this work, we utilize Liberty files from the ASAP7 cell library [15] as our test case. A cell s function is described by its functional expression; for instance, the function expression for AND2x2 ASAP7 75t R is A B. In the ASAP7 library, cell propagation delay, transition time, and internal power are characterized as functions of input transition time and total output capacitance, represented through lookup tables. Importantly, the Lib2Vec framework is adaptable to more advanced delay and power models, as long as the output responses of a cell can be efficiently sampled. III. LIB2VEC OVERVIEW Fig. 1 depicts the Lib2Vec framework, comprising three key components: (a) an automated method for generating regularity tests, (b) a self-supervised training scheme, and (c) a flexible attention- based model architecture. Self-supervised training data and regularity tests, both derived from Liberty files automatically, remove the need for costly labeling and can be easily adapted to new cell libraries. The attention-based neural networks leverage the generated data to learn cell representations, which are subsequently validated using the regularity tests. Detailed descriptions of each component are provided in the following sections. IV. DEFINITION AND TEST SETS FOR CELL SEMANTICS This section tackles the challenge of defining and evaluating cell representations by formally framing the cell representation problem and introducing multiple sets of regularity tests to efficiently assess representation quality. We propose that a cell s semantics are fully characterized by its responses to specific inputs. Functional and electrical similarities between cells can thus be defined by differences in output responses under identical input conditions. Such similarities are crucial for ML models to analyze and optimize circuit netlist performance while enabling effective cross-cell knowledge transfer. Beyond similarity, functional inversion is another key relationship for tasks like logic propagation and netlist rewriting. Based on these observations, we define three sets of regularity tests that can be automatically derived from Liberty files. The cell representation learning problem can then be formulated as learning vector space representations that maximize accuracy on these regularity tests. Our approach assumes well-documented Liberty files with consistent pin naming, as seen in the ASAP library used in this study. Consequently, input pin reordering is not considered in the regularity tests. Details of the regularity tests are elaborated as follows. A. Inverting Functionality Tests This test set evaluates inverting functionality relationships among cell types. A cell type refers to a group of standard cells with the same functionality but differing in driving strengths, voltage thresholds, or layout implementations. Two cell types with identical input pin names are considered to have an inverting functionality relationship if their outputs always complement each other, such as BUF (buffer) and INV (inverter). After identifying all inverting functionality pairs, we design tests to evaluate these relationships. For instance, as shown in Fig. 1(a), given two pairs, (BUF, INV) and (AND2, NAND2), two tests are created: (BUF vs. INV) (AND2 vs. ?) (AND2 vs. NAND2) (BUF vs. ?). More examples can be found in Table I. Using linear algebraic operations on cell vectors, we assess whether the target vector 2 (e.g., vector(NAND2)) ranks among the top-K closest vectors to the inferred vector (e.g., vector(INV) - vector(BUF) vector(AND2)). The resulting top-K accuracy indicates how well the learned cell representations capture inverting functionality relationships. B. Functional Similarity Tests This test set evaluates functional similarity among cell types with identical input pins. To simplify the analysis, we focus on single- output cells, which constitute the majority in the ASAP7 library used in our experiments. Future work will extend functional similarity evaluation to individual output pins. Functional similarity between two cells is computed by comparing their truth tables, as shown in Fig. 2. It is defined as the ratio of matching output values to the total number of input combinations. For example, the functional similarity between NAND2 and NOR2 is FunSim(NAND2, NOR2) 2 4, while FunSim(XOR2,NOR2) 1 4. A functional similarity test is created as Which is closer to NOR2: NAND2 or XOR2? And the answer is NAND2 as FunSim(NAND2, NOR2) FunSim(XOR2,NOR2). Functional similarity tests (e.g., which is closer to C: A or B?) are further categorized based on the similarity difference: Easy test if 0.5 FunSim(B, C) FunSim(A, C) , the difference is substantial, making the test easier; Hard test if 0 FunSim(B, C) FunSim(A, C) 0.5, the similarity scores are closer, making the test more challenging. These tests are answered by comparing the Euclidean distances between functional cell vectors. As binary classification tasks, random guessing yields an accuracy of 50 . Higher accuracy indicates that the learned representations effectively capture functional similarity. Fig. 2. Functional similarity calculation for cells with identical input pins. C. Electrical Similarity Tests This test set evaluates electrical similarity among cell arcs, encom- passing rise fall delay, transition time and internal power. We detail the process of deriving electrical similarity tests using rise delay as an example, which consists of the following 4 steps: (1) Input condition sampling: The Non-Linear Delay Model in ASAP7 represents delays using lookup tables parameterized by input slew and output load. To construct input condition combinations, we first determine the maximal ranges of input slew and output load across all cells. After applying a logarithmic transformation to these ranges, we uniformly sample 150 points from each range. This results in 150 150 22, 500 input condition combinations, denoted as conditions [(slew1, load1) , (slew2, load2) , ... ]. (2) Output value calculation: We apply the identical input condi- tions conditions to all cell arcs and compute the rise delay values. These delay values are then logarithmically transformed to ensure that the distribution approximates a Gaussian distribution, producing a set of transformed delays log-delay [log(delay)1, log(delay)2, ... ]. (3) Similarity measurement. We use Euclidean distance to measure the similarity between log-delay vectors of two arcs. (4) Similarity test creation and evaluation metrics. One example test is: Which NOR2 arc is closest to arc(INVx1,Y,A) in terms of rise cell delay? To answer this, we calculate the distance between the rise delay-specific representation of arc(INVx1,Y,A) and all arcs in NOR2 cells, then report the NOR2 arc with the smallest distance. The top- K accuracy measures how effectively the learned cell representations capture delay-specific similarity relationships. It is important to note that the test sets described above are not exhaustive in defining what Lib2Vec can capture. They are designed for fast evaluation of the quality of cell representations, but they do not constrain the scope of what Lib2Vec can learn. For instance, as illustrated in Fig. 4 (b), Lib2Vec identifies functional similarities between cells such as AO222, AO322, and AO332, despite their differing input configurations. V. LIBRARY CELL SELF-SUPERVISED LEARNING SCHEME This section addresses the challenge of training data generation for cell representations. An automatic method is proposed to create comprehensive functional and electrical data from Liberty files. In natural language processing, masked prediction predicting missing words based on context [16] has proven effective for generating word representations, as a word s semantics are defined by its context. Inspired by this, we propose novel self-supervised learning methods tailored to capture the semantics of cells. Since a cell s semantics are determined by its response to input conditions, we introduce four self-supervised tasks, where training data is derived from the functional and electrical responses of cells, as shown in Fig. 1 (b): (1) Functional output prediction: It predicts the output logic value of a cell given its input logic values. Example: For an AND2 cell, with inputs A 1 and B 0, the output Y is ? (answer: 0). (2) Functional difference prediction: It predicts the output logic value difference between two cells given the same input logic values. Example: For cells AND2 and XOR2, with inputs A 1 and B 0, the difference Y(AND2) - Y(XOR2) is ? (answer: -1). (3) Electrical output prediction: it predicts the electrical prop- erty values (e.g., delay, power, transition) of a specific cell arc under the input conditions conditions introduced in Section IV- C. Example: For the arc(AND2x1,Y,A), the rise cell delay is ? (answer:[0.8, 1.3, 1.4, ... ]). (4) Electrical difference prediction: It predicts the difference in electrical property values between two cell arcs under the same conditions. Example: For arc(AND2x1,Y,A) and arc(XOR2x2,Y,A), the rise cell delay difference is ? (answer:[ 0.1, 0.3, 0.2, ... ]). Difference prediction data emphasizes how cells differ in func- tionality or electrical properties, complementing to the absolute output value prediction. These tests ensure the model captures subtle relationships between cells, improving robustness and aligning with real-world design tasks that rely on comparing cell behaviors. VI. MODEL ARCHITECTURE This section presents an attention-based model architecture de- signed to efficiently process functional and electrical datasets intro- duced in Section V. The architecture ensures consistent-length vector representations for cells with different input output configurations, while also supporting property-specific representations for both entire cells and individual timing arcs. Since a cell s functional properties are independent of its electrical properties, two separate models are developed to learn functional and electrical representations. Despite being distinct, the two models share a similar architecture, as shown in Fig. 1(c). The proposed ar- chitecture includes learnable embeddings for cells, pin names (shared across cells), and properties (e.g., rise delay). For functional output prediction, the attention layer generates the functional embedding 3 for an output pin by attending to the cell s functional embedding and the embeddings of all corresponding pins. This attention mechanism allows the model to accommodate cells with varying pin counts. Multiple fully connected layers, referred to as Func-Out-FCL in Fig. 1(c), then transform the functional embedding of the output pin into a logic value prediction. For electrical output prediction, an electrical property-specific (e.g., rise delay) cell representation is created by concatenating the base electrical embedding of the cell with the property token embedding and passing them through the fully connected layer Property-FCL. Since the same input conditions are applied to all arcs, we do not take the input conditions as input. An attention layer then combines the property-specific cell embedding with the input and output pin embeddings to create the timing arc embedding. The Elec-Out-FCL further maps this arc embedding to the electrical output prediction. For functional and electrical difference prediction tasks, the model includes an additional branch to compute the embeddings and dif- ferences between two cells, as depicted by the optional modules in Fig. 1(c). This architecture offers flexibility to adapt to various learning tasks. To encourage the models to encode cell knowledge within the cell embeddings rather than the weights of the attention and fully connected layers, we limit the number of learnable parameters in these layers. Specifically, we use a single-head attention operator in the Attention Layer module and two-layer fully connected operators in the FCL modules. VII. UTILIZING LIB2VEC FOR DOWNSTREAM APPLICATIONS We propose two strategies for integrating Lib2Vec into ML models for downstream applications: (1) Representation-based integration: Directly use pre-trained cell embeddings or property-specific cell arc representations as input fea- tures for downstream tasks. This approach is simple and compatible with a wide range of ML models. (2) Model-based integration: Incorporate the proposed attention- based architecture into downstream ML models for circuit applica- tions. By using Lib2Vec s self-supervised training as a pretraining step, both cell embeddings and model weights are initialized ef- fectively. This tightly integrates Lib2Vec with the downstream task, potentially yielding greater performance benefits compared to the first approach. VIII. EXPERIMENTAL RESULTS A. Lib2Vec Implementation Details The Lib2Vec framework was implemented in Python. A custom Liberty parser for the ASAP7 cell library was developed based on [17]. Regularity tests and self-supervised training datasets were generated using tailored Python scripts. The ASAP7 library contains 190 standard cells, which can be grouped into 86 cell types according to their functional expressions. The attention-based models were implemented in PyTorch and trained on a Linux machine equipped with an AMD EPYC 7742 64-Core Processor and Nvidia A100 GPUs. We explored various embedding sizes to assess their impact on capturing cell relationships. The runtime for training data generation is approximately 10 minutes. Learning functional representations takes around 20 minutes, while learning electrical representations requires about 4 hours on one GPU. B. Regularity Test Results Table I presents examples of regularity tests and their evaluation metrics, encompassing various functional and electrical relationships among cells. In total, 930 inverting functionality tests are created. Regarding functional similarity, 116 easy and 166 hard tests are generated. In terms of electrical similarity, 635, 467, 975, 858, 722 and 722 tests are created for rise delay, fall delay, rise transition, fall transition, rise internal power and fall internal power, respectively. Answering these tests requires both functional and electrical cell representations, as well as property-specific (delay transition power) arc representations. To the best of our knowledge, no existing meth- ods offer this level of flexibility. Therefore, we compare Lib2Vec s performance against random guessing. Fig. 3(a), (b), and (c) show the results on the inverting functionality, functional similarity, and electrical similarity test sets, respectively. Lib2Vec consistently outperforms random guessing. For inverting functionality (Fig. 3(a)), Lib2Vec with larger embedding size leads to higher accuracy. And Lib2Vec with an embedding size of 64 achieves a top-10 accuracy of 61 , compared to 11 for random guess. For functional similarity (Fig. 3(b)), different embedding sizes result in similar performance. Lib2Vec achieves near-perfect accuracy on easy tests and over 80 on hard tests, far surpassing the 50 baseline. For electrical similarity (Fig. 3(c)), Lib2Vec excels across all metrics. On average, Lib2Vec with embedding size 32 achieves 52 top- 1 accuracy and 89 top-3 accuracy in electrical similarity tests, significantly outperforming 7 top-1 and 22 top-3 accuracies of random guessing. These results validate Lib2Vec s ability to capture both functional and electrical properties. Additional ablation studies were conducted to evaluate the impact of functional electrical difference prediction tasks and of key model architecture decisions. We find that excluding the difference predic- tion datasets destabilizes Lib2Vec training and reduces accuracy in the regularity tests. Furthermore, replacing the single-layer, single-head attention operator in our model with a more complex two-layer, two- head attention mechanism lowers training loss but degrades regularity test accuracy. This suggests that limiting the number of trainable parameters in the attention operator encourages the model to encode cell knowledge effectively into the representations. C. Visualization of the Cell Representations We visualize cell representations learned through masked predic- tion and Lib2Vec using the t-SNE technique [18]. For masked pre- diction, we collect approximately 30, 000 timing paths from 10 post- route designs in the IWLS 2005 benchmark suite [19], synthesized with commercial EDA tools at the ASAP7 technology node. Cell representations are learned using a TransSizer [20]-style transformer model with an embedding size of 32, trained to predict masked cells within timing paths. As shown in Fig. 4, the cell representations learned by Lib2Vec reveal clear, intuitive relationships, while the representations learned through masked prediction do not exhibit discernible patterns. The functional embedding space produced by Lib2Vec effectively captures the complex functional relationships among cells. Notably, the space is naturally divided into a X space (X BUF,AND,OR,...) and an inverting-X space (inverting-X INV,NAND,NOR,...). Linear algebraic operations on the representations reveal interesting relationships, such as vector(BUF) - vector(INV) vector(NAND3) - vector(AND3) vector(NOR4) - vector(OR4), depicted in Fig. 4(b). Fig. 4(d) visual- izes the rise delay-specific cell representations learned by Lib2Vec. Using INV cells as an example, the cell representations space clearly captures the driving strengths ordering of INV cells. D. Results on Integrating Lib2Vec for Downstream Tasks We integrate Lib2Vec into a graph neural model for three netlist logic prediction tasks. Given a netlist represented as a directed acyclic 4 TABLE I EXAMPLES OF REGULARITY TESTS AND EVALUATION METRICS Relationship Question Answer Evaluation metrics Inverting functionality (BUF vs. INV) (AND2 vs. ?) NAND2 Use linear algebraic operations on cell vectors to determine the answer. E.g., assess whether vector(NAND2) falls within the top-K closest vectors to vector(INV)-vector(BUF) vector(AND2), and report the resulting top-K accuracy (BUF vs. INV) (XNOR2 vs. ?) XOR2 (AO211 vs. AOI211) (OR2 vs. ?) NOR2 (OR5 vs. NOR5) (OA333 vs. ?) OAI333 (MAJ vs. MAJI) (AND5 vs. ?) NAND5 Functional similarity Easy Which is closer to AO21: OA21 or AOI21? OA21 Determine the answer by evaluating the Euclidean distance between functional cell vectors, and report the accuracy of the binary classification Which is closer to NAND5: OR5 or NOR5? OR5 Which is closer to NOR4: AND4 or NAND4? AND4 Hard Which is closer to A2O1A1I: O2A1O1I or AO211? O2A1O1I Which is closer to A2O1A1I: OAI211 or AOI211? OAI211 Which is closer to NOR2: NAND2 or XOR2? NAND2 Electrical similarity Rise delay Which NOR2 arc is closest to arc(INVx1, Y, A) arc(NOR2x1,Y,B) Determine the answer by evaluating the Euclidean distance between delay transition power-specific cell arc vectors and report top-K accuracy Which NAND2 arc is closest to arc(INVxp33,Y,A) arc(NAND2xp33,Y,B) Fall delay Which NOR2 arc is closest to arc(A2O1A1Ixp33,Y,A1) arc(NOR2xp67,Y,A) Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx8,Y,A) Rise transition Which NAND2 arc is closest to arc(INVx1,Y,A) arc(NAND2x1,Y,B) Which NAND2 arc is closest to arc(INVx2,Y,A) arc(NAND2x2,Y,B) Fall transition Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx2,Y,A) Which BUF ar is closest to arc(AO211x2,Y,A2) arc(BUFx4,Y,A) Rise internal power Which NOR2 arc is closest to arc(INVx1,Y,A) arc(NOR2x1,Y,A) Which NOR2 arc is closest to arc(INVx2,Y,A) arc(NOR2x2,Y,A) Fall internal power Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx2,Y,A) Which BUF arc is closest to arc(AO211x2,Y,A2) arc(BUFx2,Y,A) Fig. 3. Accuracy comparison in (a) inverting functionality, (b) functional similarity and (c) electrical similarity test sets between random guess and Lib2Vec with various embedding sizes. graph and input vectors for its input ports, the model predicts (1) the output vector at each cell s output pin, (2) the logic probability (probability of logic 1 ) and (3) switching activity. Achieving high accuracy on these tasks requires the model to effectively approximate logic functions of various cell types, manage propagation across the graph, and mitigate error accumulation. To ensure diverse datasets and control netlist dimensions, we develop an artificial netlist generator to create 1, 351 netlists using cells from the ASAP library. The statistics of these netlists are detailed in Table II. TABLE II STATISTICS OF GENERATED NETLISTS cells input ports edges topological levels range [16, 235] [1, 16] [34, 875] [7, 111] mean 117 10 421 51 A custom graph attention network is developed to perform attention-based message passing in topological order, approximating logic propagation within a netlist. Node features are cell representa- tions, while edge features combine the driver and sink pin representa- tions. Both cell and pin representations are learnable vectors of length 32. In the baseline random initialization, these representations and network weights are initialized randomly. It essentially employs one- hot encoding for cell pin representations. For representation-based Lib2Vec integration, functional embeddings from Lib2Vec serve as cell pin representations, with other parameters initialized randomly. In model-based integration, cell pin features and model weights (especially the attention operator parameters for message passing) are pretrained using Lib2Vec s self-supervised framework. The primary goal is to assess whether Lib2Vec enhances learning in scenarios with limited labeled data. To ensure a fair comparison, consistent training and testing data splits, as well as identical hyperparameters, are used across all methods, with results averaged over three runs. Each training run requires about one day on a GPU, while the Lib2Vec pretraining process completes in about 15 minutes. The proportion of training data varies from as low as 0.3 (4 samples) to 10 (135 samples) and performance is evaluated on the remaining dataset. As shown in Fig. 5, both integration methods outperform random initialization, particularly in low-data regimes, demonstrating that Lib2Vec effectively transfers cell knowledge to aid circuit learning. Notably, the model-based integration achieves approximately 80 accuracy in logic output prediction and 0.236 RMSE (Root Mean Square Error) with only 4 training samples, comparing to the 65 accuracy and 0.378 RMSE of random initialization. It highlights Lib2Vec s potential for enabling few-shot learning. Further investi- gation is needed to fully understand and expand this capability. 5 Fig. 4. Visualization of cell representations. Fig. 5. Impacts of integrating Lib2Vec into ML models for different prediction tasks. 6 IX. CONCLUSION In this paper, we introduce Lib2Vec, a novel self-supervised frame- work for learning meaningful vector representations of library cells, enabling ML models to capture functional and electrical relationships. Future work includes exploring Lib2Vec for circuit foundation models and systematically evaluating its impact on downstream tasks. REFERENCES [1] E. C. Barboza et al., Machine learning-based pre-routing timing prediction with reduced pessimism, in Proceedings of Design Automation Conference (DAC), 2019, pp. 1 6. [2] Z. Xie et al., Preplacement net length and timing estimation by customized graph neural network, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 41, no. 11, pp. 4667 4680, 2022. [3] Z. Guo et al., A timing engine inspired graph neural network model for pre-routing slack prediction, in Proceedings of Design Automation Conference (DAC), 2022, pp. 1207 1212. [4] Y. Zhang, H. Ren, and B. Khailany, GRANNITE: Graph neural network inference for transferable power estimation, in Proceedings of Design Automation Conference (DAC), 2020, pp. 1 6. [5] A. Fayyazi et al., Deep learning-based circuit recognition using sparse mapping and level-dependent decaying sum circuit representations, in Proceedings of Design, Automation Test in Europe Conference Exhibition (DATE), 2019, pp. 638 641. [6] R. Bommasani et al., On the opportunities and risks of foundation models, arXiv preprint arXiv:2108.07258, 2021. [7] L. Chen et al., Large circuit models: opportunities and challenges, Science China Information Sciences (Sci. China Inf. Sci.), vol. 67, no. 10, pp. 1 42, 2024. [8] M. Li et al., Deepgate: Learning neural representations of logic gates, in Proceedings of Design Automation Conference (DAC), 2022, pp. 667 672. [9] Z. Shi et al., Deepgate2: Functionality-aware circuit representation learning, in Proceedings of International Conference on Computer Aided Design (ICCAD), 2023, pp. 1 9. [10] Z. Wang et al., Functionality matters in netlist representation learning, in Proceedings of Design Automation Conference (DAC), 2022, pp. 61 66. [11] Z. Wang, C. Bai, and Z. He, FGNN2: A powerful pre-training framework for learning the logic functionality of circuits, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), pp. 1 1, 2024. [12] W. Raslan and Y. Ismail, Deep-learning cell-delay modeling for static timing analysis, Ain Shams Engineering Journal (ASEJ), vol. 14, no. 1, p. 101828, 2023. [13] D. Hyun, Y. Jung, and Y. Shin, Accurate interpolation of library timing parameters through recurrent convolutional neural network, IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems (TCAD), vol. 43, no. 1, pp. 244 248, 2023. [14] Liberty User Guides and Reference Manual Suite Version 2013.03, eecs.berkeley.edu alanmi publications other liberty13 03.pdf. [15] L. T. Clark et al., Asap7: A 7-nm finfet predictive process design kit, Microelectronics Journal (Microelectron. J.), vol. 53, pp. 105 115, 2016. [16] T. Mikolov, Efficient estimation of word representations in vector space, arXiv preprint arXiv:1301.3781, vol. 3781, 2013. [17] Liberty Parser, [18] L. Van der Maaten and G. Hinton, Visualizing data using t-sne. Journal of machine learning research (JMLR), vol. 9, no. 11, 2008. [19] C. Albrecht, Iwls 2005 benchmarks, in Proceedings of International Workshop for Logic Synthesis (IWLS), vol. 9, 2005. [20] S. Nath et al., Transsizer: A novel transformer-based fast gate sizer, in Proceedings of International Conference on Computer-Aided Design (ICCAD), 2022, pp. 1 9. 7\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nLearning Library Cell Representations in Vector Space Rongjian Liang NVIDIA Yi-Chen Lu NVIDIA Wen-Hao Liu NVIDIA Haoxing Ren NVIDIA Abstract We propose Lib2Vec, a novel self-supervised framework to efficiently learn meaningful vector representations of library cells, enabling ML models to capture essential cell semantics. The framework comprises three key components: (1) an automated method for generating regularity tests to quantitatively evaluate how well cell representations reflect inter-cell relationships; (2) a self-supervised learning scheme that systematically extracts training data from Liberty files, removing the need for costly labeling; and (3) an attention-based model architecture that accommodates various pin counts and enables the creation of property- specific cell and arc embeddings. Experimental results demonstrate that Lib2Vec effectively captures functional and electrical similarities. Moreover, linear algebraic operations on cell vectors reveal meaningful relationships, such as vector(BUF) - vector(INV) vector(NAND) ap- proximating the vector of AND, showcasing the framework s nuanced representation capabilities. Lib2Vec also enhances downstream circuit learning applications, especially when labeled data is scarce. Index Terms representation learning, library cell, netlist optimization I. INTRODUCTION Library cell representations are vital for effective machine learning (ML)-based circuit analysis and optimization, as library cells are the fundamental building blocks of circuit netlists. Traditional methods often rely on manually defined features [1] [4], requiring extensive expertise and feature engineering. Alternatively, one-hot encoding [5] demands large amounts of domain-specific training data, which may not always be available. To address these limitations, this work ex- plores a data-efficient, self-supervised learning approach to generate cell representations. Our method maps library cells into a continuous vector space, capturing semantic relationships and enabling ML models to operate in a simpler, structured space instead of the original high-dimensional, discrete cell space. Our work also aligns with the broader AI shift toward training foundation models [6] on self-supervised data and adapting them to diverse downstream tasks. By providing a unified, self-supervised method for learning cell representations, this work has the potential to serve as a groundwork for developing circuit foundation models [7].\n\n--- Segment 2 ---\nOur work also aligns with the broader AI shift toward training foundation models [6] on self-supervised data and adapting them to diverse downstream tasks. By providing a unified, self-supervised method for learning cell representations, this work has the potential to serve as a groundwork for developing circuit foundation models [7]. While related efforts, such as DeepGate [8], [9] and FCNN [10], [11], have introduced pre-training methods that achieve notable results in circuit representation, they primarily focus on structural and functional aspects of AND-Inverter graphs, overlooking other cell types and electrical properties. Moreover, they embed circuit knowledge within the weights of graph neural networks, restricting the transferability of this knowledge to other ML models. Another related research direction focuses on ML-based library cell char- acterization [12], [13]. While these methods have shown promise, they primarily aim to improve arc-based timing characterization accuracy rather than enabling ML models to capture and understand semantic relationships among cells. To our knowledge, efficient learning of functional and electrical representations of library cells in a vector space compatible with diverse ML architectures (including transformer, the most popular architecture for foundation models) remains unexplored. This work addresses that gap by enabling more general and versatile cell representation learning. A. Challenges We identify three key challenges in learning meaningful vector representations of library cells: 1) Defining and Evaluating Cell Representations: Defining the semantics of library cells, and devising effective test sets and metrics to assess representation quality. 2) Efficient Training Data Generation: Creating comprehensive training data that captures the diverse functional and electrical properties of cells. 3) Flexible Model Architecture: Developing an architecture that accommodates varying pin counts and enables straightforward generation of property-specific (e.g., cell delay-focused) and arc- specific representations from a cell s base representation. This is critical for downstream circuit learning tasks. B. Contributions Our key insight is that a cell s semantics are defined by its re- sponses to input conditions, allowing us to capture semantic relation- ships among cells based on their behaviors under the same inputs. By systematically collecting (input conditions, output responses) pairs, we can create data that encapsulates both the functional and electrical properties of cells. Building on this, we introduce Lib2Vec, a novel self-supervised learning framework for library cell representation.\n\n--- Segment 3 ---\nBy systematically collecting (input conditions, output responses) pairs, we can create data that encapsulates both the functional and electrical properties of cells. Building on this, we introduce Lib2Vec, a novel self-supervised learning framework for library cell representation. Our main contributions are summarized as follows: 1) First systematic exploration of learning library cell representa- tion in vector space, adaptable to various ML architectures and circuit applications; 2) An automated method for generating regularity tests to quanti- tatively evaluate the quality of cell representations; 3) A self-supervised learning scheme that extracts training data from Liberty files, removing the need for costly labeling; 4) An attention-based architecture that accommodates various pin counts and enables property- and arc-specific embeddings; 5) Experimental results show that Lib2Vec effectively captures functional and electrical properties of cells. Lib2Vec also en- hances downstream circuit learning applications, particularly in scenarios with limited labeled data. The remainder of this paper is organized as follows. Section II introduces key properties of library cells, providing essential back- ground for this study. Section III gives an overview of the Lib2Vec framework, with detailed descriptions of its three components in Sec- tion IV, Section V and Section VI. Section VII discusses the usage model of Lib2Vec. Experiment results are shown in Section VIII, and Section IX gives some concluding remarks. II. LIBRARY CELL PROPERTIES In VLSI design, library cells have three categories of properties: functional, electrical, and physical. Functional properties define a 1 arXiv:2503.22900v1 [cs.LG] 28 Mar 2025 Fig. 1. Overview of the lib2vec framework. cell s logical behavior. Electrical properties capture timing, power, and signal integrity, including parameters like propagation delay, transition time, capacitance, leakage and internal power, and noise margins. Physical properties describe a cell s layout and geometry, such as cell dimensions and pin locations. These properties can be further classified as static or dynamic. Static properties, like physical characteristics, remain constant, while dynamic properties, such as most functional and electrical behaviors, vary with input conditions. This work focuses on dynamic properties, specifically functional and electrical characteristics, as static properties are typically straightfor- ward to model. Specifically, Lib2Vec models functional properties and rise fall propagation delay, transition time, and internal power.\n\n--- Segment 4 ---\nThis work focuses on dynamic properties, specifically functional and electrical characteristics, as static properties are typically straightfor- ward to model. Specifically, Lib2Vec models functional properties and rise fall propagation delay, transition time, and internal power. The framework is extensible to incorporate additional properties. Liberty files [14] are a standard format to describe the functional and electrical properties of library cells. In this work, we utilize Liberty files from the ASAP7 cell library [15] as our test case. A cell s function is described by its functional expression; for instance, the function expression for AND2x2 ASAP7 75t R is A B. In the ASAP7 library, cell propagation delay, transition time, and internal power are characterized as functions of input transition time and total output capacitance, represented through lookup tables. Importantly, the Lib2Vec framework is adaptable to more advanced delay and power models, as long as the output responses of a cell can be efficiently sampled. III. LIB2VEC OVERVIEW Fig. 1 depicts the Lib2Vec framework, comprising three key components: (a) an automated method for generating regularity tests, (b) a self-supervised training scheme, and (c) a flexible attention- based model architecture. Self-supervised training data and regularity tests, both derived from Liberty files automatically, remove the need for costly labeling and can be easily adapted to new cell libraries. The attention-based neural networks leverage the generated data to learn cell representations, which are subsequently validated using the regularity tests. Detailed descriptions of each component are provided in the following sections. IV. DEFINITION AND TEST SETS FOR CELL SEMANTICS This section tackles the challenge of defining and evaluating cell representations by formally framing the cell representation problem and introducing multiple sets of regularity tests to efficiently assess representation quality. We propose that a cell s semantics are fully characterized by its responses to specific inputs. Functional and electrical similarities between cells can thus be defined by differences in output responses under identical input conditions. Such similarities are crucial for ML models to analyze and optimize circuit netlist performance while enabling effective cross-cell knowledge transfer. Beyond similarity, functional inversion is another key relationship for tasks like logic propagation and netlist rewriting. Based on these observations, we define three sets of regularity tests that can be automatically derived from Liberty files.\n\n--- Segment 5 ---\nBeyond similarity, functional inversion is another key relationship for tasks like logic propagation and netlist rewriting. Based on these observations, we define three sets of regularity tests that can be automatically derived from Liberty files. The cell representation learning problem can then be formulated as learning vector space representations that maximize accuracy on these regularity tests. Our approach assumes well-documented Liberty files with consistent pin naming, as seen in the ASAP library used in this study. Consequently, input pin reordering is not considered in the regularity tests. Details of the regularity tests are elaborated as follows. A. Inverting Functionality Tests This test set evaluates inverting functionality relationships among cell types. A cell type refers to a group of standard cells with the same functionality but differing in driving strengths, voltage thresholds, or layout implementations. Two cell types with identical input pin names are considered to have an inverting functionality relationship if their outputs always complement each other, such as BUF (buffer) and INV (inverter). After identifying all inverting functionality pairs, we design tests to evaluate these relationships. For instance, as shown in Fig. 1(a), given two pairs, (BUF, INV) and (AND2, NAND2), two tests are created: (BUF vs. INV) (AND2 vs. ?) (AND2 vs. NAND2) (BUF vs. ? ). More examples can be found in Table I. Using linear algebraic operations on cell vectors, we assess whether the target vector 2 (e.g., vector(NAND2)) ranks among the top-K closest vectors to the inferred vector (e.g., vector(INV) - vector(BUF) vector(AND2)). The resulting top-K accuracy indicates how well the learned cell representations capture inverting functionality relationships. B. Functional Similarity Tests This test set evaluates functional similarity among cell types with identical input pins. To simplify the analysis, we focus on single- output cells, which constitute the majority in the ASAP7 library used in our experiments. Future work will extend functional similarity evaluation to individual output pins. Functional similarity between two cells is computed by comparing their truth tables, as shown in Fig. 2. It is defined as the ratio of matching output values to the total number of input combinations.\n\n--- Segment 6 ---\n2. It is defined as the ratio of matching output values to the total number of input combinations. For example, the functional similarity between NAND2 and NOR2 is FunSim(NAND2, NOR2) 2 4, while FunSim(XOR2,NOR2) 1 4. A functional similarity test is created as Which is closer to NOR2: NAND2 or XOR2? And the answer is NAND2 as FunSim(NAND2, NOR2) FunSim(XOR2,NOR2). Functional similarity tests (e.g., which is closer to C: A or B?) are further categorized based on the similarity difference: Easy test if 0.5 FunSim(B, C) FunSim(A, C) , the difference is substantial, making the test easier; Hard test if 0 FunSim(B, C) FunSim(A, C) 0.5, the similarity scores are closer, making the test more challenging. These tests are answered by comparing the Euclidean distances between functional cell vectors. As binary classification tasks, random guessing yields an accuracy of 50 . Higher accuracy indicates that the learned representations effectively capture functional similarity. Fig. 2. Functional similarity calculation for cells with identical input pins. C. Electrical Similarity Tests This test set evaluates electrical similarity among cell arcs, encom- passing rise fall delay, transition time and internal power. We detail the process of deriving electrical similarity tests using rise delay as an example, which consists of the following 4 steps: (1) Input condition sampling: The Non-Linear Delay Model in ASAP7 represents delays using lookup tables parameterized by input slew and output load. To construct input condition combinations, we first determine the maximal ranges of input slew and output load across all cells. After applying a logarithmic transformation to these ranges, we uniformly sample 150 points from each range. This results in 150 150 22, 500 input condition combinations, denoted as conditions [(slew1, load1) , (slew2, load2) , ... ]. (2) Output value calculation: We apply the identical input condi- tions conditions to all cell arcs and compute the rise delay values. These delay values are then logarithmically transformed to ensure that the distribution approximates a Gaussian distribution, producing a set of transformed delays log-delay [log(delay)1, log(delay)2, ... ].\n\n--- Segment 7 ---\n(2) Output value calculation: We apply the identical input condi- tions conditions to all cell arcs and compute the rise delay values. These delay values are then logarithmically transformed to ensure that the distribution approximates a Gaussian distribution, producing a set of transformed delays log-delay [log(delay)1, log(delay)2, ... ]. (3) Similarity measurement. We use Euclidean distance to measure the similarity between log-delay vectors of two arcs. (4) Similarity test creation and evaluation metrics. One example test is: Which NOR2 arc is closest to arc(INVx1,Y,A) in terms of rise cell delay? To answer this, we calculate the distance between the rise delay-specific representation of arc(INVx1,Y,A) and all arcs in NOR2 cells, then report the NOR2 arc with the smallest distance. The top- K accuracy measures how effectively the learned cell representations capture delay-specific similarity relationships. It is important to note that the test sets described above are not exhaustive in defining what Lib2Vec can capture. They are designed for fast evaluation of the quality of cell representations, but they do not constrain the scope of what Lib2Vec can learn. For instance, as illustrated in Fig. 4 (b), Lib2Vec identifies functional similarities between cells such as AO222, AO322, and AO332, despite their differing input configurations. V. LIBRARY CELL SELF-SUPERVISED LEARNING SCHEME This section addresses the challenge of training data generation for cell representations. An automatic method is proposed to create comprehensive functional and electrical data from Liberty files. In natural language processing, masked prediction predicting missing words based on context [16] has proven effective for generating word representations, as a word s semantics are defined by its context. Inspired by this, we propose novel self-supervised learning methods tailored to capture the semantics of cells. Since a cell s semantics are determined by its response to input conditions, we introduce four self-supervised tasks, where training data is derived from the functional and electrical responses of cells, as shown in Fig. 1 (b): (1) Functional output prediction: It predicts the output logic value of a cell given its input logic values. Example: For an AND2 cell, with inputs A 1 and B 0, the output Y is ?\n\n--- Segment 8 ---\n1 (b): (1) Functional output prediction: It predicts the output logic value of a cell given its input logic values. Example: For an AND2 cell, with inputs A 1 and B 0, the output Y is ? (answer: 0). (2) Functional difference prediction: It predicts the output logic value difference between two cells given the same input logic values. Example: For cells AND2 and XOR2, with inputs A 1 and B 0, the difference Y(AND2) - Y(XOR2) is ? (answer: -1). (3) Electrical output prediction: it predicts the electrical prop- erty values (e.g., delay, power, transition) of a specific cell arc under the input conditions conditions introduced in Section IV- C. Example: For the arc(AND2x1,Y,A), the rise cell delay is ? (answer:[0.8, 1.3, 1.4, ... ]). (4) Electrical difference prediction: It predicts the difference in electrical property values between two cell arcs under the same conditions. Example: For arc(AND2x1,Y,A) and arc(XOR2x2,Y,A), the rise cell delay difference is ? (answer:[ 0.1, 0.3, 0.2, ... ]). Difference prediction data emphasizes how cells differ in func- tionality or electrical properties, complementing to the absolute output value prediction. These tests ensure the model captures subtle relationships between cells, improving robustness and aligning with real-world design tasks that rely on comparing cell behaviors. VI. MODEL ARCHITECTURE This section presents an attention-based model architecture de- signed to efficiently process functional and electrical datasets intro- duced in Section V. The architecture ensures consistent-length vector representations for cells with different input output configurations, while also supporting property-specific representations for both entire cells and individual timing arcs. Since a cell s functional properties are independent of its electrical properties, two separate models are developed to learn functional and electrical representations. Despite being distinct, the two models share a similar architecture, as shown in Fig. 1(c). The proposed ar- chitecture includes learnable embeddings for cells, pin names (shared across cells), and properties (e.g., rise delay).\n\n--- Segment 9 ---\n1(c). The proposed ar- chitecture includes learnable embeddings for cells, pin names (shared across cells), and properties (e.g., rise delay). For functional output prediction, the attention layer generates the functional embedding 3 for an output pin by attending to the cell s functional embedding and the embeddings of all corresponding pins. This attention mechanism allows the model to accommodate cells with varying pin counts. Multiple fully connected layers, referred to as Func-Out-FCL in Fig. 1(c), then transform the functional embedding of the output pin into a logic value prediction. For electrical output prediction, an electrical property-specific (e.g., rise delay) cell representation is created by concatenating the base electrical embedding of the cell with the property token embedding and passing them through the fully connected layer Property-FCL. Since the same input conditions are applied to all arcs, we do not take the input conditions as input. An attention layer then combines the property-specific cell embedding with the input and output pin embeddings to create the timing arc embedding. The Elec-Out-FCL further maps this arc embedding to the electrical output prediction. For functional and electrical difference prediction tasks, the model includes an additional branch to compute the embeddings and dif- ferences between two cells, as depicted by the optional modules in Fig. 1(c). This architecture offers flexibility to adapt to various learning tasks. To encourage the models to encode cell knowledge within the cell embeddings rather than the weights of the attention and fully connected layers, we limit the number of learnable parameters in these layers. Specifically, we use a single-head attention operator in the Attention Layer module and two-layer fully connected operators in the FCL modules. VII. UTILIZING LIB2VEC FOR DOWNSTREAM APPLICATIONS We propose two strategies for integrating Lib2Vec into ML models for downstream applications: (1) Representation-based integration: Directly use pre-trained cell embeddings or property-specific cell arc representations as input fea- tures for downstream tasks. This approach is simple and compatible with a wide range of ML models. (2) Model-based integration: Incorporate the proposed attention- based architecture into downstream ML models for circuit applica- tions.\n\n--- Segment 10 ---\nThis approach is simple and compatible with a wide range of ML models. (2) Model-based integration: Incorporate the proposed attention- based architecture into downstream ML models for circuit applica- tions. By using Lib2Vec s self-supervised training as a pretraining step, both cell embeddings and model weights are initialized ef- fectively. This tightly integrates Lib2Vec with the downstream task, potentially yielding greater performance benefits compared to the first approach. VIII. EXPERIMENTAL RESULTS A. Lib2Vec Implementation Details The Lib2Vec framework was implemented in Python. A custom Liberty parser for the ASAP7 cell library was developed based on [17]. Regularity tests and self-supervised training datasets were generated using tailored Python scripts. The ASAP7 library contains 190 standard cells, which can be grouped into 86 cell types according to their functional expressions. The attention-based models were implemented in PyTorch and trained on a Linux machine equipped with an AMD EPYC 7742 64-Core Processor and Nvidia A100 GPUs. We explored various embedding sizes to assess their impact on capturing cell relationships. The runtime for training data generation is approximately 10 minutes. Learning functional representations takes around 20 minutes, while learning electrical representations requires about 4 hours on one GPU. B. Regularity Test Results Table I presents examples of regularity tests and their evaluation metrics, encompassing various functional and electrical relationships among cells. In total, 930 inverting functionality tests are created. Regarding functional similarity, 116 easy and 166 hard tests are generated. In terms of electrical similarity, 635, 467, 975, 858, 722 and 722 tests are created for rise delay, fall delay, rise transition, fall transition, rise internal power and fall internal power, respectively. Answering these tests requires both functional and electrical cell representations, as well as property-specific (delay transition power) arc representations. To the best of our knowledge, no existing meth- ods offer this level of flexibility. Therefore, we compare Lib2Vec s performance against random guessing. Fig. 3(a), (b), and (c) show the results on the inverting functionality, functional similarity, and electrical similarity test sets, respectively. Lib2Vec consistently outperforms random guessing. For inverting functionality (Fig. 3(a)), Lib2Vec with larger embedding size leads to higher accuracy.\n\n--- Segment 11 ---\nFor inverting functionality (Fig. 3(a)), Lib2Vec with larger embedding size leads to higher accuracy. And Lib2Vec with an embedding size of 64 achieves a top-10 accuracy of 61 , compared to 11 for random guess. For functional similarity (Fig. 3(b)), different embedding sizes result in similar performance. Lib2Vec achieves near-perfect accuracy on easy tests and over 80 on hard tests, far surpassing the 50 baseline. For electrical similarity (Fig. 3(c)), Lib2Vec excels across all metrics. On average, Lib2Vec with embedding size 32 achieves 52 top- 1 accuracy and 89 top-3 accuracy in electrical similarity tests, significantly outperforming 7 top-1 and 22 top-3 accuracies of random guessing. These results validate Lib2Vec s ability to capture both functional and electrical properties. Additional ablation studies were conducted to evaluate the impact of functional electrical difference prediction tasks and of key model architecture decisions. We find that excluding the difference predic- tion datasets destabilizes Lib2Vec training and reduces accuracy in the regularity tests. Furthermore, replacing the single-layer, single-head attention operator in our model with a more complex two-layer, two- head attention mechanism lowers training loss but degrades regularity test accuracy. This suggests that limiting the number of trainable parameters in the attention operator encourages the model to encode cell knowledge effectively into the representations. C. Visualization of the Cell Representations We visualize cell representations learned through masked predic- tion and Lib2Vec using the t-SNE technique [18]. For masked pre- diction, we collect approximately 30, 000 timing paths from 10 post- route designs in the IWLS 2005 benchmark suite [19], synthesized with commercial EDA tools at the ASAP7 technology node. Cell representations are learned using a TransSizer [20]-style transformer model with an embedding size of 32, trained to predict masked cells within timing paths. As shown in Fig. 4, the cell representations learned by Lib2Vec reveal clear, intuitive relationships, while the representations learned through masked prediction do not exhibit discernible patterns. The functional embedding space produced by Lib2Vec effectively captures the complex functional relationships among cells.\n\n--- Segment 12 ---\n4, the cell representations learned by Lib2Vec reveal clear, intuitive relationships, while the representations learned through masked prediction do not exhibit discernible patterns. The functional embedding space produced by Lib2Vec effectively captures the complex functional relationships among cells. Notably, the space is naturally divided into a X space (X BUF,AND,OR,...) and an inverting-X space (inverting-X INV,NAND,NOR,...). Linear algebraic operations on the representations reveal interesting relationships, such as vector(BUF) - vector(INV) vector(NAND3) - vector(AND3) vector(NOR4) - vector(OR4), depicted in Fig. 4(b). Fig. 4(d) visual- izes the rise delay-specific cell representations learned by Lib2Vec. Using INV cells as an example, the cell representations space clearly captures the driving strengths ordering of INV cells. D. Results on Integrating Lib2Vec for Downstream Tasks We integrate Lib2Vec into a graph neural model for three netlist logic prediction tasks. Given a netlist represented as a directed acyclic 4 TABLE I EXAMPLES OF REGULARITY TESTS AND EVALUATION METRICS Relationship Question Answer Evaluation metrics Inverting functionality (BUF vs. INV) (AND2 vs. ?) NAND2 Use linear algebraic operations on cell vectors to determine the answer. E.g., assess whether vector(NAND2) falls within the top-K closest vectors to vector(INV)-vector(BUF) vector(AND2), and report the resulting top-K accuracy (BUF vs. INV) (XNOR2 vs. ?) XOR2 (AO211 vs. AOI211) (OR2 vs. ?) NOR2 (OR5 vs. NOR5) (OA333 vs. ?) OAI333 (MAJ vs. MAJI) (AND5 vs. ?) NAND5 Functional similarity Easy Which is closer to AO21: OA21 or AOI21? OA21 Determine the answer by evaluating the Euclidean distance between functional cell vectors, and report the accuracy of the binary classification Which is closer to NAND5: OR5 or NOR5? OR5 Which is closer to NOR4: AND4 or NAND4?\n\n--- Segment 13 ---\nOA21 Determine the answer by evaluating the Euclidean distance between functional cell vectors, and report the accuracy of the binary classification Which is closer to NAND5: OR5 or NOR5? OR5 Which is closer to NOR4: AND4 or NAND4? AND4 Hard Which is closer to A2O1A1I: O2A1O1I or AO211? O2A1O1I Which is closer to A2O1A1I: OAI211 or AOI211? OAI211 Which is closer to NOR2: NAND2 or XOR2?\n\n--- Segment 14 ---\nO2A1O1I Which is closer to A2O1A1I: OAI211 or AOI211? OAI211 Which is closer to NOR2: NAND2 or XOR2? NAND2 Electrical similarity Rise delay Which NOR2 arc is closest to arc(INVx1, Y, A) arc(NOR2x1,Y,B) Determine the answer by evaluating the Euclidean distance between delay transition power-specific cell arc vectors and report top-K accuracy Which NAND2 arc is closest to arc(INVxp33,Y,A) arc(NAND2xp33,Y,B) Fall delay Which NOR2 arc is closest to arc(A2O1A1Ixp33,Y,A1) arc(NOR2xp67,Y,A) Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx8,Y,A) Rise transition Which NAND2 arc is closest to arc(INVx1,Y,A) arc(NAND2x1,Y,B) Which NAND2 arc is closest to arc(INVx2,Y,A) arc(NAND2x2,Y,B) Fall transition Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx2,Y,A) Which BUF ar is closest to arc(AO211x2,Y,A2) arc(BUFx4,Y,A) Rise internal power Which NOR2 arc is closest to arc(INVx1,Y,A) arc(NOR2x1,Y,A) Which NOR2 arc is closest to arc(INVx2,Y,A) arc(NOR2x2,Y,A) Fall internal power Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx2,Y,A) Which BUF arc is closest to arc(AO211x2,Y,A2) arc(BUFx2,Y,A) Fig. 3.\n\n--- Segment 15 ---\nNAND2 Electrical similarity Rise delay Which NOR2 arc is closest to arc(INVx1, Y, A) arc(NOR2x1,Y,B) Determine the answer by evaluating the Euclidean distance between delay transition power-specific cell arc vectors and report top-K accuracy Which NAND2 arc is closest to arc(INVxp33,Y,A) arc(NAND2xp33,Y,B) Fall delay Which NOR2 arc is closest to arc(A2O1A1Ixp33,Y,A1) arc(NOR2xp67,Y,A) Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx8,Y,A) Rise transition Which NAND2 arc is closest to arc(INVx1,Y,A) arc(NAND2x1,Y,B) Which NAND2 arc is closest to arc(INVx2,Y,A) arc(NAND2x2,Y,B) Fall transition Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx2,Y,A) Which BUF ar is closest to arc(AO211x2,Y,A2) arc(BUFx4,Y,A) Rise internal power Which NOR2 arc is closest to arc(INVx1,Y,A) arc(NOR2x1,Y,A) Which NOR2 arc is closest to arc(INVx2,Y,A) arc(NOR2x2,Y,A) Fall internal power Which BUF arc is closest to arc(AO211x2,Y,A1) arc(BUFx2,Y,A) Which BUF arc is closest to arc(AO211x2,Y,A2) arc(BUFx2,Y,A) Fig. 3. Accuracy comparison in (a) inverting functionality, (b) functional similarity and (c) electrical similarity test sets between random guess and Lib2Vec with various embedding sizes.\n\n--- Segment 16 ---\n3. Accuracy comparison in (a) inverting functionality, (b) functional similarity and (c) electrical similarity test sets between random guess and Lib2Vec with various embedding sizes. graph and input vectors for its input ports, the model predicts (1) the output vector at each cell s output pin, (2) the logic probability (probability of logic 1 ) and (3) switching activity. Achieving high accuracy on these tasks requires the model to effectively approximate logic functions of various cell types, manage propagation across the graph, and mitigate error accumulation. To ensure diverse datasets and control netlist dimensions, we develop an artificial netlist generator to create 1, 351 netlists using cells from the ASAP library. The statistics of these netlists are detailed in Table II. TABLE II STATISTICS OF GENERATED NETLISTS cells input ports edges topological levels range [16, 235] [1, 16] [34, 875] [7, 111] mean 117 10 421 51 A custom graph attention network is developed to perform attention-based message passing in topological order, approximating logic propagation within a netlist. Node features are cell representa- tions, while edge features combine the driver and sink pin representa- tions. Both cell and pin representations are learnable vectors of length 32. In the baseline random initialization, these representations and network weights are initialized randomly. It essentially employs one- hot encoding for cell pin representations. For representation-based Lib2Vec integration, functional embeddings from Lib2Vec serve as cell pin representations, with other parameters initialized randomly. In model-based integration, cell pin features and model weights (especially the attention operator parameters for message passing) are pretrained using Lib2Vec s self-supervised framework. The primary goal is to assess whether Lib2Vec enhances learning in scenarios with limited labeled data. To ensure a fair comparison, consistent training and testing data splits, as well as identical hyperparameters, are used across all methods, with results averaged over three runs. Each training run requires about one day on a GPU, while the Lib2Vec pretraining process completes in about 15 minutes. The proportion of training data varies from as low as 0.3 (4 samples) to 10 (135 samples) and performance is evaluated on the remaining dataset. As shown in Fig.\n\n--- Segment 17 ---\nThe proportion of training data varies from as low as 0.3 (4 samples) to 10 (135 samples) and performance is evaluated on the remaining dataset. As shown in Fig. 5, both integration methods outperform random initialization, particularly in low-data regimes, demonstrating that Lib2Vec effectively transfers cell knowledge to aid circuit learning. Notably, the model-based integration achieves approximately 80 accuracy in logic output prediction and 0.236 RMSE (Root Mean Square Error) with only 4 training samples, comparing to the 65 accuracy and 0.378 RMSE of random initialization. It highlights Lib2Vec s potential for enabling few-shot learning. Further investi- gation is needed to fully understand and expand this capability. 5 Fig. 4. Visualization of cell representations. Fig. 5. Impacts of integrating Lib2Vec into ML models for different prediction tasks. 6 IX. CONCLUSION In this paper, we introduce Lib2Vec, a novel self-supervised frame- work for learning meaningful vector representations of library cells, enabling ML models to capture functional and electrical relationships. Future work includes exploring Lib2Vec for circuit foundation models and systematically evaluating its impact on downstream tasks. REFERENCES [1] E. C. Barboza et al., Machine learning-based pre-routing timing prediction with reduced pessimism, in Proceedings of Design Automation Conference (DAC), 2019, pp. 1 6. [2] Z. Xie et al., Preplacement net length and timing estimation by customized graph neural network, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 41, no. 11, pp. 4667 4680, 2022. [3] Z. Guo et al., A timing engine inspired graph neural network model for pre-routing slack prediction, in Proceedings of Design Automation Conference (DAC), 2022, pp. 1207 1212. [4] Y. Zhang, H. Ren, and B. Khailany, GRANNITE: Graph neural network inference for transferable power estimation, in Proceedings of Design Automation Conference (DAC), 2020, pp. 1 6. [5] A. Fayyazi et al., Deep learning-based circuit recognition using sparse mapping and level-dependent decaying sum circuit representations, in Proceedings of Design, Automation Test in Europe Conference Exhibition (DATE), 2019, pp.\n\n--- Segment 18 ---\n1 6. [5] A. Fayyazi et al., Deep learning-based circuit recognition using sparse mapping and level-dependent decaying sum circuit representations, in Proceedings of Design, Automation Test in Europe Conference Exhibition (DATE), 2019, pp. 638 641. [6] R. Bommasani et al., On the opportunities and risks of foundation models, arXiv preprint arXiv:2108.07258, 2021. [7] L. Chen et al., Large circuit models: opportunities and challenges, Science China Information Sciences (Sci. China Inf. Sci. ), vol. 67, no. 10, pp. 1 42, 2024. [8] M. Li et al., Deepgate: Learning neural representations of logic gates, in Proceedings of Design Automation Conference (DAC), 2022, pp. 667 672. [9] Z. Shi et al., Deepgate2: Functionality-aware circuit representation learning, in Proceedings of International Conference on Computer Aided Design (ICCAD), 2023, pp. 1 9. [10] Z. Wang et al., Functionality matters in netlist representation learning, in Proceedings of Design Automation Conference (DAC), 2022, pp. 61 66. [11] Z. Wang, C. Bai, and Z. He, FGNN2: A powerful pre-training framework for learning the logic functionality of circuits, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (TCAD), pp. 1 1, 2024. [12] W. Raslan and Y. Ismail, Deep-learning cell-delay modeling for static timing analysis, Ain Shams Engineering Journal (ASEJ), vol. 14, no. 1, p. 101828, 2023. [13] D. Hyun, Y. Jung, and Y. Shin, Accurate interpolation of library timing parameters through recurrent convolutional neural network, IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems (TCAD), vol. 43, no. 1, pp. 244 248, 2023. [14] Liberty User Guides and Reference Manual Suite Version 2013.03, eecs.berkeley.edu alanmi publications other liberty13 03.pdf.\n\n--- Segment 19 ---\n244 248, 2023. [14] Liberty User Guides and Reference Manual Suite Version 2013.03, eecs.berkeley.edu alanmi publications other liberty13 03.pdf. [15] L. T. Clark et al., Asap7: A 7-nm finfet predictive process design kit, Microelectronics Journal (Microelectron. J. ), vol. 53, pp. 105 115, 2016. [16] T. Mikolov, Efficient estimation of word representations in vector space, arXiv preprint arXiv:1301.3781, vol. 3781, 2013. [17] Liberty Parser, [18] L. Van der Maaten and G. Hinton, Visualizing data using t-sne. Journal of machine learning research (JMLR), vol. 9, no. 11, 2008. [19] C. Albrecht, Iwls 2005 benchmarks, in Proceedings of International Workshop for Logic Synthesis (IWLS), vol. 9, 2005. [20] S. Nath et al., Transsizer: A novel transformer-based fast gate sizer, in Proceedings of International Conference on Computer-Aided Design (ICCAD), 2022, pp. 1 9. 7\n\n