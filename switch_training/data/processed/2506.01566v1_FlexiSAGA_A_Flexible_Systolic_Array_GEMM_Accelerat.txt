=== ORIGINAL PDF: 2506.01566v1_FlexiSAGA_A_Flexible_Systolic_Array_GEMM_Accelerat.pdf ===\n\nRaw text length: 48253 characters\nCleaned text length: 48006 characters\nNumber of segments: 34\n\n=== CLEANED TEXT ===\n\narXiv:2506.01566v1 [cs.PF] 2 Jun 2025 FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing Mika Markus Müller , Konstantin Lübeck , Alexander Louis-Ferdinand Jung , Jannik Steinmetz , and Oliver Bringmann Embedded Systems, University of Tübingen, Tübingen, Germany These authors contributed equally to this work. Abstract. Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs), have become an important tool for a wide range of applications, from computer vision to natural language processing. How- ever, the computational complexity of DNN inference poses a significant challenge, particularly for processing on resource-constrained edge de- vices. One promising approach to address this challenge is the exploita- tion of sparsity in DNN operator weights. In this work, we present FlexiSAGA, an architecturally configurable and dataflow-flexible AI hardware accelerator for the sparse and dense pro- cessing of general matrix multiplications (GEMMs). FlexiSAGA sup- ports seven different sparse and dense dataflows, enabling efficient pro- cessing of resource intensive DNN operators. Additionally, we propose a DNN pruning method specifically tailored towards the FlexiSAGA architecture, allowing for near-optimal processing of dense and sparse convolution and fully-connected operators, facilitating a DNN HW co- design flow. Our results show a whole DNN sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming commercial and literature-reported accelerator platforms. 1 Introduction In recent years, the deployment of AI workloads, such as Deep Neural Networks (DNNs), has shifted away from datacenters to resource-constrained edge de- vices due to privacy concerns, real-time requirements and costs. To fulfil these non-functional requirements, specialized AI accelerators are often necessary, as they can process data locally and faster than conventional microcontrollers while maintaining a small area and energy footprint. When using an AI accelerator, the goal is often to efficiently process the most computationally intensive DNN operators, like convolution (CONV) and fully-connected (FC). While the FC operator implements a general matrix mul- tiplication (GEMM), the CONV operator can be converted into a GEMM by applying an im2col transformation [3]. This enables the usage of the same ar- chitecture and mapping approach to process both operators. The GEMM can be further optimized by splitting it into smaller tiles, which allows for improved cache utilization, increased parallelism, and memory bandwidth optimization. 2 M. M. Müller and K. Lübeck et al. State-of-the-art GEMM accelerators [4,2,7] use systolic arrays (SAs) to ef- ficiently compute tiled GEMMs as they do not have the drawbacks of Von Neumann architectures [6]. SAs enable parallel processing where some data is streamed, while the other data is kept stationary to reach maximum reuse of the stationary data which minimizes costly memory accesses. The dataflow deter- mines which type of data remains stationary. There are three common dataflows used in SAs: output stationary (OS), weight stationary (WS) and input sta- tionary (IS). For different matrix and tile sizes used for the tiled GEMM these dataflows can reach vastly different runtimes depending on the architecture. To further improve the processing and memory footprint of DNNs, pruning has become a popular technique [5]. Pruning involves selectively removing less important DNN weights by replacing them with zeros, and thereby reducing the overall model complexity without significantly impacting its accuracy, which results in sparse weight matrices and therefore in sparse GEMMs. Several AI accelerators have been proposed which employ sparsity-centric optimizations to efficiently process sparse GEMMs [4,14,8,17]. However, a common limitation of many AI accelerators is that they tend to only process the sparse GEMMs using a single dataflow, which may not be the most efficient one for all DNN operators. In this paper, we present FlexiSAGA, a flexible and configurable systolic ar- ray for GEMM acceleration, which can process both dense and sparse GEMMs using seven different dataflows to achieve a higher flexibility and higher sparse- over-dense speedups than many other accelerators when processing AI work- loads. To reach this goal, sparsity is exploited solely within the weight matrix. This can be done at the time of deployment, i.e. without affecting the on-device DNN processing time, compared to the dynamic identification of zeros within the input matrices at runtime. We use the two-stage bitmap format [17] and we present a custom sparse format to efficiently store and decode weight matrix tiles. Furthermore, we introduce a structured pruning technique based on [19]. The custom sparse format and the pruning technique are tailored specifically to the FlexiSAGA architecture, enabling a DNN HW co-design flow. We evaluate FlexiSAGA with a number of representative DNNs (AlexNet [11], VGG16 [16], GoogLeNet [18], and ResNet50 [9]) pruned and deployed on differently sized FlexiSAGA instances and compare the achieved sparse-over- dense speedups to an Intel Xeon CPU, the Nvidia Orin ARM CPU and GPU, and the SCNN [14] and SparTen [8] accelerators. These results show a whole DNN sparse-over-dense speedup of 1.41 up to 4.28. Additionally, we conduct a design space exploration (DSE) to find the near-optimal combination of dataflow, pruning and architectural parameters to minimize the DNN operator runtimes. 2 Related Work Several AI accelerators and pruning frameworks for processing sparse matrices have been proposed. The SPOTS accelerator [17] has an integrated im2col unit which transforms CONV input and weight tensors on-the-fly to process them as tiled GEMM. Additionally, the authors introduce a two-stage bitmap format FlexiSAGA 3 for storing sparse matrices pruned using structured sparsity learning [19]. Their accelerator shows a speedup of up to 20 for whole DNNs compared to the dense execution on a CPU and a speedup of up to 1.86 compared to other accelerator architectures presented in [4,7], supporting dense and sparse processing, using a reconfigurable systolic array composed of 512 processing elements. SCNN [14] focuses on the processing of CONV operators. Exploiting input and weight sparsity, the authors present a whole DNN speedup of up to 3.52 for VGG16 compared to the dense processing on the same architecture utilizing 64 processing elements and using a novel input stationary dataflow. SparTen [8] uses bitmap encoding to store sparse input and weight matrices. The results show a sparse-over-dense speedup of up to 15.5 for CONV operators using up to 64 compute clusters. DeepSparse [12] is a Sparsity-aware deep learning inference runtime for CPUs 1 which provides an automatic deployment for pruned PyTorch [15] mod- els. The authors report a whole DNN sparse-over-dense speedup of up to 8.2 for 32bit floating-point models running on an Intel Xeon CPU. Nvidia s 2:4 sparsity [13] is specifically tailored towards the Nvidia Ampere microarchitecture. 2:4 sparsity pruning splits the weight matrix into row vec- tors containing four elements. In each vector, the two smallest values are set to zero. After pruning, the DNNs are fine-tuned. They report a sparse-over-dense speedup of up to 2 for an Nvidia A100 GPU using 16bit floating-point models. We compare our FlexiSAGA architecture together with the proposed DNN pruning method to the CONV operator sparse-over-dense speedup of the one- sided SCNN and SparTen accelerators presented in [8] as they provide detailed per DNN operator results. Additionally, we compare our whole DNN sparse- over-dense speedup to DeepSparse and TensorRT, run on an Intel Xeon CPU, Nvidia Orin ARM CPU and GPU respectively. 3 Sparse Matrix Formats An important factor for processing sparse GEMM operations is in which format sparse matrices are stored. A sparse matrix format should have a good compres- sion ratio to minimize the memory footprint and allow for efficient decompres- sion such that it does not slow down the processing of a GEMM operation. In the following section, different sparse matrix formats are briefly introduced and compared to each other. Additionally, we present the compressed sparse block (CSB) format, which is tailored towards the sparse GEMM processing of Flexi- SAGA. Fig. 1(a) shows a comparison of the different sparse matrix formats for a 128 512 matrix with varying sparsities and uniformly distributed zeros. The compressed sparse row (CSR) format stores the non-zero elements of a matrix in an array, along with two additional arrays that store the column in- dices and the row pointers. The row pointers array indicates the starting index of each row in the non-zero elements array. Similarly to CSR is the compressed 1 (accessed March 13, 2025). 4 M. M. Müller and K. Lübeck et al. 0.5 0.6 0.7 0.8 0.9 Sparsity 0 100 200 300 400 Memory Footprint (kB) No Compression RLE-4 COO Bitmap CSR Two-Stage-Bitmap CSC CSB (a) 1 3 2 4 6 5 7 1 3 2 4 6 5 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 non-zero values column bit array element bit array 0 1 2 3 4 5 (b) 1 6 4 7 5 2 3 8 0 0 0 0 0 0 0 0 0 0 1 5 6 2 4 7 3 8 0 2 0 3 1 1 4 -1 5 3 column index array number of col. after merge non-zero values 0 1 2 3 4 5 (c) Fig. 1. (a) Memory footprint comparison of different sparse matrix formats and no compression for a 128 512 matrix of 32bit values with varying sparsities and uniformly distributed zero elements. (b) Two-stage bitmap format example. (c) Compressed sparse block (CSB) format example. sparse column (CSC) format, but it stores a matrix by columns instead of rows. The non-zero elements are also stored in an array, along with the row indices and column pointers stored in two additional arrays. The coordinate format (COO) stores the non-zero elements of a matrix as a list of (row, column, value) tuples, which is easy to implement but comes with significant storage overhead. The Run-Length Encoded 4bit (RLE-4) format represents a matrix as a sequence of 4bit codes, where each code indicates the length of a run of zeros followed by a non-zero element. This format can be very compact for matrices with long runs of zeros, but it may be less efficient for matrices with more uniform sparse dis- tributions. The bitmap format also contains an array for the non-zero elements, but additionally defines a bit array, where for every element is stored whether it is non-zero or zero indicated by a 1 and 0 respectively. The two-stage bitmap format introduced in [17] uses an array containing all non-zero elements of a matrix and two bit arrays which encode the coordinates of non-zero elements. The first array, called the column bit array, contains as many bits as columns in a matrix. A 0 in the column bit array encodes that the corresponding matrix column contains only zero elements, a 1 encodes that the corresponding column contains non-zero elements. The second bit array, called the element bit array, encodes for each non-zero column which elements are non-zero. This allows for a high compression ratio and efficient decompression, where loading of entire zero columns can be skipped. Fig. 1(b) shows an example encoding for a 3 6 sparse matrix. The two-stage bitmap format demonstrates a good compression ratio, even for low sparsities (see Fig. 1(a)). However, the occurrence of entire zero columns of height n with uniformly distributed zero elements and sparsity s is described by the Bernoulli process n n sn(1 s)n n sn and is consequently very low for large n. Therefore, we introduce the compressed sparse block (CSB) format, which allows merging multiple non-zero columns with high sparsity into one. Similarly to the two-stage bitmap format, the CSB format uses one array to store all non-zero elements of a matrix, additionally for each non-zero element FlexiSAGA 5 LU 0,0 SU 0,n LU 0,1 Controller PE 0,0 PE 0,1 Main Memory SU 1,n SU m,n SU m,0 SU m,1 LU 0,n PE 0,n LU 1,0 PE 1,0 PE 1,1 LU m,0 PE 1,n PE m,0 PE m,1 PE m,n DecU (a) GEMM tiles dense weight stationary (dWS) dense output stationary (dOS) dense input stationary (dIS) movement of the streamed matrix tile movement of the the partial sums a d g h b e i c f A D G E H B I C F weight matrix input matrix c f i h b e g a d A D G H B E I C F A D G E H B I C F a d g h b e i c f c f i h b e g a d A B C E F D I G H (b) Fig. 2. (a) Block diagram of the FlexiSAGA architecture. (b) Visualization of the three dense tiled GEMM dataflows supported by the FlexiSAGA architecture. the column index is stored. The row index is implicitly encoded in the order of the column index array. For each column starting from the first, we use greedy search to find matching columns to merge with. Columns match if the position of the non-zero elements of one column matches the position of the zero elements in the other column, while it is allowed that zero elements of one column can match zero elements of the other column. Columns only containing zeros are not merged and are skipped entirely. The resulting number of columns after the merge is stored together with the non-zero elements and the column index array. Fig. 1(c) shows an example of the CSB format. Because of the column index array, the memory footprint for the CSB format is higher than for the bitmap formats. However, it allows for skipping more than just zero columns because combined columns are loaded as a single column. FlexiSAGA utilizes the two-stage bitmap format and the CSB format to efficiently store and process sparse matrices, which is described in detail in the following section. 4 FlexiSAGA Architecture Fig. 2 shows the block diagram of the proposed FlexiSAGA architecture. Flexi- SAGA is designed as a systolic array (SA) whose main components are Process- ing Elements (PEs) organized in a 2D grid, configurable in its height and width. Each PE has a register file containing nine registers of configurable data word width to store stationary data and partial sums. An arithmetic logic unit (ALU) supporting floating-point and integer operations inside each PE can access the register file to execute move, multiply, addition, and multiply-accumulate opera- tions. Furthermore, the ALU of each PE can also write into the register files of its neighboring PEs to its right and below, which facilitates data transfer through the SA. To load data from the main memory into the SA, each PE in the left column and in the top row is connected to a load unit (LU) which performs mem- ory read transactions and writes the read data words into the register file of the 6 M. M. Müller and K. Lübeck et al. connected PE. All LUs are connected to the decompression unit (DecU) which acts as an arbiter for main memory accesses. When a dense GEMM is processed, the DecU forwards all read transactions to the main memory. In case of a sparse GEMM the DecU checks if the accessed value is declared as a zero in the sparse representation of the weight matrix tile and emits a zero instead of forwarding the read transaction to the main memory. The data width and the number of ports connecting the main memory to the DecU is configurable. The PEs on the bottom and the right column are connected to store units (SUs) which read data from the PE register files and write them into the memory, again through the DecU. The behavior of all components is globally set by a programmable controller, which contains a schedule for the dense or sparse GEMM. In case of a sparse GEMM the controller is programmed to skip the processing of entire zero columns or rows of the weight matrix. The following sections detail how dense and sparse GEMMs are processed by the FlexiSAGA architecture. 4.1 Dense Tiled GEMM Processing FlexiSAGA supports the three common dense GEMM dataflows: input station- ary (dIS), weight stationary (dWS), and output stationary (dOS). Before pro- cessing a dense GEMM on the FlexiSAGA architecture, the weight and input matrices are split into tiles of the size of the SA. Fig. 2(b) presents the different dense tiled GEMM dataflows. When using the dIS dataflow, the elements of the weight matrix tile are streamed into the SA, while the elements of the input ma- trix tile are held stationary in the PEs. Each PE performs a multiply-accumulate operation between weight and input elements, accumulating the partial sums, which are then propagated through the array to compute the output tile. The dWS dataflow is similar to the dIS dataflow, but the roles of weight and input matrix tiles are reversed. In the dOS dataflow, the elements of the weight and in- put tiles are streamed into the SA, and each PE performs a multiply-accumulate operation, accumulating the partial sums in a local register. The output tile is then read from the PE registers after all the partial sums have been accumulated. 4.2 Sparse Tiled GEMM Processing In addition to the dense tiled GEMM processing, FlexiSAGA supports output stationary (sOS), weight stationary (sWS), and input stationary (sIS) dataflows for sparse tiled GEMMs. All three sparse dataflows use the two-stage bitmap format to compress the weight matrix tiles. Additionally, for the output station- ary dataflow we propose a variant using the CSB format to compress the weight matrix tiles (csOS). The following sections detail all four sparse dataflows. The tiles of the weight matrices are stored in a compressed format in the main mem- ory before the inference of a DNN. The input matrices are stored in row-major order in the main memory, and the addresses for accessing the corresponding input tile elements for a weight tile are generated by the FlexiSAGA controller. The elements of the output matrix are stored in row-major order in the main memory and serve as input matrix for the succeeding DNN operator. FlexiSAGA 7 a b c 0 0 0 0 0 d e 0 0 A B C D E F G H 0 c B a A Step 0 0 c A a B a A a A Step 1 0 A c B c A c A a B a B a A a A Step 2 0 B 0 A 0 A c B c B c A c A a B a B a A a A Step 3 0 B 0 B 0 A 0 A c B c B c A c A a B a B a A a A Step 4 0 B 0 B e A 0 A c B c B d A c A a H a B b G a A Step 5 0 B 0 B e A 0 A c B c B d G c A b H a B b G a A b G Step 6 0 B 0 B e G 0 A d H c B d G c A d G b H a B b H b G a A b G Step 7 e H 0 B e G 0 A e G d H c B d H d G c A d G b H a B b H b G a A b G Step 8 e H 0 B e H e G 0 A e G d H c B d H d G c A d G b H a B b H b G a A b G Step 9 weight input partial sum 1 0 0 1 a c b d e (d) Step by step processing of sparse tiled GEMM for the output stationary dataflow (sOS) using the two-stage bitmap format. (a) GEMM tiles weights inputs non-zero values column bit array 1 1 0 1 1 1 element bit array (b) Two-stage bitmap format representation (c) PE register contents Fig. 3. Example for processing single tile using the sparse tiled GEMM output sta- tionary dataflow (sOS) on a FlexiSAGA architecture of size 2 3. Sparse Output Stationary Dataflow (sOS) Fig. 3 presents an example of processing of a single GEMM tile using the sOS dataflow. All weight matrix tiles are compressed using the two-stage bitmap format (see Fig. 3(b)). While the weight matrix tile consists of 12 elements in its uncompressed form, storing it in the two-stage bitmap format allows us to only read seven data words to access the whole tile. Fig. 3(d) shows the step by step processing for the sOS dataflow. Each step shows the register contents of all six PEs, while Fig. 3(c) provides a legend for the PE register contents. In step 0, the first column of the weight tile (a,c,0) is loaded into the registers of the PEs in the left column and the first row of the input tile (A,B) is loaded into PEs in the top row. In step 1, the top left PE calculates the first partial sum and stores it in its register, additionally this PE copies the input element A into the PE below it and the weight element a into the PE to the right of it. In step 2, the top right PE and the middle left PE calculate their partial sums and copy the input and weight elements to their neighbor PEs below and to the right (if they exist). This is repeated until step 4, in which all PEs contain a partial sum and each element of the first weight tile column has been multiplied with each element of the first input tile row. In steps 5 to 9, the last column of the weight tile (b,d,e) and the last row of the input tile (G,H) are loaded into the PEs and the partial sums of the multiplications are accumulated. The PE registers now contain all non-zero elements of the output tile, which are then forwarded to the main memory. Sparse Weight Stationary Dataflow (sWS) Fig. 4 presents an example of processing of a single GEMM tile using the sWS dataflow. Fig. 4(d) shows the PE register contents for each step of the sWS dataflow processing. In step 0 the whole weight tile is loaded into the SA and stays there until all elements of the output tile have been calculated. In steps 1 to 4, the first column of the input tile is propagated vertically through the SA and the first column of the output tile is accumulated in the right PE column and then forwarded to the main memory. In 8 M. M. Müller and K. Lübeck et al. a b c d e 0 0 0 0 0 A B C D e 0 d c b C a A Step 0 e 0 d c A b C a A a A a A Step 1 e 0 A d C c A c A c A b C a A b C a A a A Step 2 e C 0 A 0 A 0 A d C c A d C c A c A b C a A b C a A a A Step 3 e C 0 A e C 0 A 0 A d C c A d C c A c A b C a A b C a A a A Step 4 e C 0 A d C c A b D a B Step 5 e C 0 A d C c B b D a B a B a B Step 6 e C 0 B d D c B c B c B b D a B b D a B a B Step 7 e D 0 B 0 B 0 B d D c B d D c B c B b D a B b D a B a B Step 8 e D 0 B e D 0 B 0 B d D c B d D c B c B b D a B b D a B a B Step 9 weight input partial sum 1 0 0 1 1 a b c d e (d) Step by step processing of sparse tiled GEMM for the weight stationary dataflow (sWS) using the two-stage bitmap format. (a) GEMM tiles weights inputs non-zero values row bit array 1 1 1 1 0 1 element bit array (b) Two-stage bitmap format representation (c) PE register contents Fig. 4. Example for processing single tile using the sparse tiled GEMM weight station- ary dataflow (sWS) on a FlexiSAGA architecture of size 2 3. a d e c b 0 0 0 0 0 0 0 A B C D E F F c E D b C B a A Step 0 c F e E b D 0 C a B d A Step 5 c F e E b D 0 C d A d B a B d A d A Step 6 c F e E d A 0 C 0 D d B 0 C d A 0 C d B d B d A d A Step 7 e F d B 0 D e E d A 0 C e E 0 D d B 0 D 0 C d A 0 C d B d B d A d A Step 8 e F d B 0 D e F e E d A 0 C e E 0 D d B 0 D 0 C d A 0 C d B d B d A d A Step 9 F c E D b C a A a B a A a A Step 1 F c E a A b C b D a B b C a A b C a B a B a A a A Step 2 c F a B b D c E a A b C c E b D a B b D b C a A b C a B a B a A a A Step 3 c F a B b D c F c E a A b C c E b D a B b D b C a A b C a B a B a A a A Step 4 weight input partial sum 1 0 0 1 a b c d e (d) Step by step processing of sparse tiled GEMM for the input stationary dataflow (sIS) using the two-stage bitmap format. (a) GEMM data weights inputs non-zero values row bit array 1 1 1 1 0 1 element bit array (b) Two-stage bitmap format representation (c) PE register contents Fig. 5. Example for processing single tile using the sparse tiled GEMM input stationary dataflow (sIS) on a FlexiSAGA architecture of size 2 3. steps 5 to 9, the partial sums are cleared and the second column of the input tile is propagated through the SA in the same manner as the first column. Similarly to the sOS dataflow in this example only seven data words need to be read from the main memory to process a weight tile containing ten elements. Sparse Input Stationary Dataflow (sIS) Fig. 5 presents an example of processing of a single GEMM tile using the sIS dataflow. Fig. 5(d) shows the PE register contents for each step of the sIS dataflow processing. In step 0 the whole input tile is loaded into the SA and stays there until all output tile elements have been calculated. Additionally, the first row of the weight tile is loaded into the registers of the left PE column. In steps 1 to 4, the weight tile row is horizontally propagated through the SA and the first output tile row is accumulated in the bottom PE row and then forwarded to the main memory. FlexiSAGA 9 a d c b e 0 0 0 0 0 0 0 e a b d c 0 A B C D E F G H A B C D E F G H a A a A 0 a B a B c A c A 0 0 c B c B -1 Step 2 a A a A 0 a B a B c A c A 0 0 c B c B A -1 0 B Step 3 b A a A 3 a B a B d A c A 1 c B c B e A 3 B Step 4 b G a A b G 3 b H a B b H d A c A 1 d B c B e A 3 e B Step 5 b G a A b G 3 b H a B b H d G c A 1 3 d H c B e A 3 e B Step 6 b G a A b G 3 b H a B b H d G c A 1 3 d H c B e G e G 3 3 e H e H Step 7 b C a A b G 3 b D a B b H d C c A 1 d D c B e G e G 3 e H e H Step 8 b C a A b G 3 b D a B b H d C c A d C 1 d D c B d D e C e G 3 1 e D e H Step 9 a A a A 0 a B a B c 0 c -1 Step 1 a 0 c 0 -1 Step 0 col idx weight input tmp idx partial sum 0 0 -1 3 1 3 2 a c b d e (d) Step by step processing of sparse tiled GEMM for the output stationary dataflow (csOS) using the CSB format. (a) GEMM tiles CSB comp. weight tile input tile weights inputs non-zero values num. of columns column index array (b) CSB format representation (c) controller and PE register contents PE finished Fig. 6. Example for processing single tile using the sparse tiled GEMM output station- ary dataflow (csOS) using the CSB format on a FlexiSAGA architecture of size 2 3. In steps 5 to 9, the partial sums are cleared and the next non-zero row of the weight tile is propagated through the SA in the same manner as the first row. As seen before, the two-stage bitmap format drastically reduces the amount of data words which have to be read from the main memory for the weight tile. Sparse Output Stationary Dataflow (csOS) using the CSB format In contrast to the sOS, sWS, and sIS dataflows which utilize the two-stage bitmap format to compress the weight tiles, the csOS dataflow uses the CSB format. Since the CSB format merges multiple weight tile columns, the input tile rows and weight tile columns cannot simply be propagated through the SA as input and weight elements might not match anymore. Therefore, the FlexiSAGA con- troller tracks the column index of the weights the PE row currently stores to identify if a multiply-accumulate operation is needed. Fig. 6(d) shows the step by step processing for the csOS dataflow. Each step shows the register contents of all six PEs together with the column index and a temporary index for each PE row stored in the controller, while Fig. 6(c) provides a legend for the controller and PE register contents. In step 0, the first column of the weight tile is loaded into the registers of the left PEs while the column index for each PE row is stored in the controller. In step 1 the first row of the input tile is loaded into the first PE row because the column index for the first PE row is set to 0, which corresponds to the first row of the input tile. Additionally, weights of the left PE column are copied to the right PE column and PEs in the first row calculate their partial sums. Because the PEs in the first row have finished their processing for the current weights, the PEs are marked with a blue outline. In step 2, the input elements are propagated vertically to the next PE row and the column index of the first row is copied to the temporary index of the second row and the PEs in the second row compute their partial sums. In step 3, the inputs are further propagated, however, the PEs in the last row do not have to execute a multiply-accumulate operation because the column index 10 M. M. Müller and K. Lübeck et al. is set to -1 which indicates a zero weight. Now all rows are marked as finished, which implies that in step 4 the next weight column is loaded into the left PE column. Step 5 is analogous to step 1. In step 6, inputs are propagated vertically from the first to the second PE row. Now the column index and the temporary index in the second PE row do not match, which means no computation has to be done. Step 7 is analogous to step 2. After step 7, the second PE row remains unmarked, which means the PEs contain weights for a different input row. In step 8, the inputs corresponding to the column index stored for the second PE row are loaded into the SA and propagated to the second PE row. In step 9 the PE register files now contain all non-zero elements of the output tile, which are then forwarded to the main memory. If the two-stage bitmap format had been applied to the weight tile presented in Fig. 6(a), an additional weight column would be loaded when using the sOS dataflow resulting in more data movement. 5 Deep Neural Network Pruning To exploit sparsity during the inference of DNNs, they have to be pruned before deployment. The simplest technique is global unstructured pruning, in which a global sparsity s for a whole pre-trained DNN is set. Then a proportion s of all weights wi which have the smallest l1-norm wi are set to zero. Even though this technique can lead to runtime improvements, there is often a considerable accuracy loss and an imbalanced sparsity distribution across all weights. Addi- tionally, the unstructured sparsity may lead to a complex control flow, degrading the runtime improvements. To alleviate those drawbacks, we implemented a pruning technique based on structured sparsity learning [19] in PyTorch [15] which allows us to introduce zero rows and columns in the DNN weight matrix tiles tailored towards the two- stage bitmap and CSB format. Firstly, the DNNs are trained using a standard training loop until they reach the desired accuracy a. Secondly, the prunable DNN operators, in our case CONV and FC, are grouped by operator type. Each group j gets assigned a sparsity sj. Afterward, each weight tensor of the CONV group is transformed into a matrix using an im2col transformation and split into tiles of the desired size. The FC weights can be split into tiles directly. Those tiles are further split into row or column vectors wi Wj of length n corresponding to the tile width or height. For each group j, the proportion sj of wi Wj with the smallest l2-norm wi 2 pPn k 0 w2 k are set to zero. Then the DNN is trained again until it reaches accuracy a ϵ with ϵ 0 while the pruned vectors stay zero. When the accuracy a ϵ is reached, sj is increased by δj and the weights in each group are pruned again. This is repeated until the DNN training cannot reach accuracy a ϵ anymore after a fixed amount of training epochs. FlexiSAGA 11 Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 (FC) Layer 7 (FC) Overall 0.0 0.2 0.4 0.6 0.8 1.0 Sparsity AlexNet Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Layer 13 (FC) Layer 14 (FC) Layer 15 (FC) Overall 0.0 0.2 0.4 0.6 0.8 1.0 Sparsity VGG16 Block 1 Block 2 Block 3 Block 3a Block 3b Block 4a Block 4b Block 4c Block 4d Block 4e Block 5a Block 5b FC Overall 0.0 0.2 0.4 0.6 0.8 1.0 Sparsity GoogLeNet n 1 Row n 4 Col. n 4 Row n 8 Col. n 8 Row n 16 Col. n 16 Block 0 Block 1.0 Block 1.1 Block 1.2 Block 2.0 Block 2.1 Block 2.2 Block 2.3 Block 3.0 Block 3.1 Block 3.2 Block 3.3 Block 3.4 Block 3.5 Block 4.0 Block 4.1 Block 4.2 FC Overall 0.0 0.2 0.4 0.6 0.8 1.0 Sparsity ResNet50 Fig. 7. Operator and operator block sparsities for different DNNs and vector lengths n and vector orientations. 6 Results 6.1 Experimental Setup To evaluate the FlexiSAGA architecture, we implemented a configurable virtual prototype (VP) using the Amaranth HDL [1] which allows for translation into a register-transfer level (RTL) representation for the integrated RTL simulator or into synthesizable Verilog. For the runtime results presented in this work, we used the Amaranth HDL RTL simulator together with a cycle-approximate SRAM memory model with unit read and write latencies and eight ports spread across multiple banks and a port width of 32bit adopted from the UltraTrail [2] memory architecture. The PE s register files are set to a width of 32bit, and the PE ALUs support 32bit floating-point operations. We use the PyTorch reference implementations of AlexNet [11], VGG16 [16], ResNet50 [9], and GoogLeNet [18] and trained them for classification of the CIFAR-10 dataset with the following top-1 accuracies a: AlexNet 0.86, VGG16 0.89, ResNet50 0.89, and GoogLeNet 0.91 using 32bit floating-point weights. Those DNNs were chosen to compare against the results presented in [8]. After training, the DNNs were pruned using the technique introduced in section 5 with an initial group sparsity sj 0.7, a sparsity progression δj 0.01 for all groups j, an ϵ a 0.02, and a stochastic gradient descent optimizer which allows for a maximum top-1 accuracy drop of 2 which is in accordance with the accuracies for pruned DNNs reported in [17]. Fig. 7 shows the operator and overall sparsities after pruning, for each DNN using different vector lengths n (1, 4, 8, and 16) and row or column orientation. Since ResNet50 consists of 109 CONV and FC operators and GoogLeNet consists of 115 operators, their sparsity results are presented in operator blocks. Pruning with vector length n 1 shows the highest sparsities across all DNNs, however, n 1 does not introduce any structured sparsity. With an increasing n, the first couple of operators for all DNNs and the last FC operator of AlexNet and VGG16 drastically decrease in sparsity. 12 M. M. Müller and K. Lübeck et al. 4x4 8x8 16x16 4x4 8x8 16x16 4x4 8x8 16x16 4x4 8x8 16x16 FlexiSAGA Systolic Array Size 109 1010 Cycles (log) AlexNet VGG16 GoogLeNet ResNet50 dense sparse (a) csOS dWS dOS sWS sOS dataflow 0 50 100 150 200 selected 40.2 24.3 16.9 15.2 3.4 (b) Fig. 8. (a) Comparison of whole DNN runtimes in clock cycles for different FlexiSAGA systolic array sizes. (b) Distribution of selected dataflows with minimal runtime per DNN operator across all DNNs and all FlexiSAGA sizes. The overall sparsity shows only slight variations for different vector lengths and orientations and remains above 0.75 for AlexNet, VGG16, and GoogLeNet which is comparable to the sparsities reported for the same DNNs in [17]. To compare the sparse-over-dense inference speedup to other architectures, we used the DeepSparse [10] inference engine and deployed the four DNNs un- pruned (dense) and pruned (sparse) onto an Intel Xeon Platinum 8168 CPU with 24 cores and 48 threads and the Nvidia Jetson AGX Orin ARM Cortex- A78AE CPU with 12 cores. Each DNN was run using a single core and multiple cores threads. The sparse variant of the DNNs were pruned with ϵ a 0.02 and n 1, as it provides the highest sparsity and the highest speedup when using DeepSparse and limiting the top-1 accuracy drop to 2 . Additionally, we used Nvidia s 2:4 sparsity [13] to prune all four DNNs with a maximum top-1 accuracy drop of 2 and deployed the dense and sparse 16bit floating-point vari- ants onto the Nvidia Jetson AGX Orin Ampere GPU using TensorRT. For the DeepSparse and TensorRT runtime measurements, we collected multiple sam- ples to calculate the mean to accommodate for interferences of the operating system. Moreover, we compared the operator-wise sparse-over-dense speedup to the SCNN and SparTen architectures using the results reported in [8]. 6.2 Whole DNN Inference Evaluation Fig. 8(a) presents the runtime in clock cycles for the four DNNs deployed onto the FlexiSAGA architecture of different sizes (4 4, 8 8, and 16 16). Dense represents the clock cycle sum of the unpruned CONV and FC operators, while sparse represents the clock cycle sum of the pruned operators. The DNNs were pruned with vector length n set to the systolic array size in row and column ori- entation. For each operator, the dataflow with the minimal runtime (dOS, dWS, dIS, sOS, sWS, sIS, or csOS) was chosen by measuring all different variants. For the pruned DNNs the vector orientation is the same for all operators. For all DNNs, the runtime decreases when quadrupling the SA size. The mean dense and sparse speedup across all DNNs when increasing the SA size is 2.1 and 2.07 respectively. Even though the number of PEs scales quadratically, the memory interface scales only linearly because only the outer PEs have access to the main memory. This linear scaling is reflected in the mean dense and sparse speedup. FlexiSAGA 13 AlexNet VGG16 GoogLeNet ResNet50 0 1 2 3 4 Speedup Nvidia Orin GPU Nvidia Orin ARM CPU, single-core Nvidia Orin ARM CPU, 12 cores Intel Xeon CPU, single-core Intel Xeon CPU, 48 threads FlexiSAGA 4x4 FlexiSAGA 8x8 FlexiSAGA 16x16 Fig. 9. Whole DNN sparse-over-dense speedup comparison between an Nvidia Orin GPU (2:4 sparsity), Nvidia Orin ARM CPU and Intel Xeon CPU (DeepSparse), and FlexiSAGA with different systolic array sizes. Fig. 9 shows the sparse-over-dense speedup comparison for the Nvidia Orin GPU, Nvidia Orin ARM CPU, Intel Xeon CPU, and FlexiSAGA of different sizes. While FlexiSAGA only supports the CONV and FC operators, the other platforms support all operators of the deployed DNNs. However, the runtime of activation, pooling and element-wise operators is very small compared to the CONV and FC operators. AlexNet generally shows the best sparse-over-dense speedup for all platforms, this can be attributed to the high sparsity that can be achieved across all operators when pruning AlexNet (see Fig. 7). ResNet50 shows the lowest speedup across all architectures because ResNet50 has the lowest overall sparsity and consists of 109 CONV and FC operators, which have smaller inputs and weights than other DNNs, which leads to more processing overhead. FlexiSAGA shows better sparse-over-dense speedups, ranging from 1.41 for ResNet50 to 4.28 for AlexNet, compared to CPU and GPU architectures using highly optimized inference runtimes. A key factor for achieving better sparse-over-dense speedups is that for each operator, the dataflow with minimal runtime was selected. Fig. 8(b) presents the distribution of selected dataflows per DNN operator across all DNNs and all FlexiSAGA sizes. This indicates that a significant amount of sparse-over-dense speedup can be attributed to the csOS dataflow which specifically exploits the proposed CSB sparse matrix format. 6.3 DNN Operator-wise Inference Evaluation To provide a more fine-granular analysis, this section presents an operator-wise sparse-over-dense speedup comparison of FlexiSAGA with the one-sided SCNN and SparTen architectures for the CONV operators of AlexNet, VGG16, and GoogLeNet (3a and 5a blocks) for which the speedups are reported in [8]. Fig. 10 shows the sparse-over-dense speedup comparison for all three architectures. Since the SCNN and SparTen architectures for which the results are reported only sup- port CONV operators and are comprised of 64 processing elements, we compare them to FlexiSAGA with an SA size of 8 8 and mapped only the CONV op- erators. While FlexiSAGA shows lower speedups for the first couple of CONV operators, the sparse-over-dense speedup is often higher for the second half. This can be explained by the sparsity distribution across all operators, as presented in Fig. 7. This is especially apparent for the GoogLeNet operators in block 5a, 14 M. M. Müller and K. Lübeck et al. Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Mean 0.0 2.5 5.0 7.5 Speedup AlexNet one-sided SCNN one-sided SparTen FlexiSAGA 8x8 Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Mean 0.0 2.5 5.0 7.5 Speedup VGG16 Layer 3a.0 Layer 3a.1 Layer 3a.2 Layer 3a.3 Layer 3a.4 Layer 3a.5 Layer 5a.0 Layer 5a.1 Layer 5a.2 Layer 5a.3 Layer 5a.4 Layer 5a.5 Mean 100 101 102 Speedup (log) GoogLeNet Fig. 10. Operator-wise sparse-over-dense speedup comparison between one-sided SCNN, one-sided SparTen, and FlexiSAGA 8 8 for AlexNet, VGG16, and GoogLeNet convolution operators. where some operators have such a high sparsity that they can almost be skipped entirely, leading to a sparse-over-dense speedup of up to 100. For the mean sparse-over-dense speedup FlexiSAGA shows better results for all three DNNs. 6.4 Design Space Exploration Lastly, we present a design space exploration (DSE) in Fig. 11 for two AlexNet operators processed on different FlexiSAGA instances, all comprised of 72 PEs coupled with different dataflows and pruning parameters. The DSE heatmap shows the runtime in clock cycles. For the CONV operator on the left-hand side, the best runtime (local optimum) is achieved for a FlexiSAGA instance of size 6 12 using the sOS dataflow with pruned column vectors of length n 6. For the FC operator on the right-hand side, the best runtime (local optimum) is reached with a SA of size 12 6 using the csOS dataflow and pruned column vectors of length n 12. We conducted this DSE for all AlexNet CONV and FC operators and selected the architecture and pruning method which has the lowest whole DNN inference runtime. This resulted in a SA of size 4 18 for which the DNN is pruned using column vectors with n 4. For the presented operators in Fig. 11 the dataflow with the best global runtime is csOS (global optimum). The unbalanced row to column ratio of the best SA instance can be FlexiSAGA 15 4x18 6x12 8x9 9x8 12x6 18x4 FlexiSAGA Systolic Array Size n 1 Row Col. n 1 Row Col. n 1 Row Col. n 1 Row Col. Dataflow Pruning Parameter 4.36e 08 3.09e 08 2.46e 08 2.38e 08 3.01e 08 4.30e 08 4.45e 08 3.39e 08 2.96e 08 2.85e 08 2.71e 08 3.05e 08 4.85e 08 3.73e 08 3.24e 08 3.11e 08 3.65e 08 4.84e 08 1.55e 08 1.43e 08 2.16e 08 2.51e 08 3.66e 08 6.13e 08 2.08e 08 1.58e 08 2.12e 08 2.33e 08 3.32e 08 5.18e 08 8.49e 07 7.67e 07 9.96e 07 1.16e 08 1.53e 08 2.36e 08 2.97e 08 2.64e 08 8.53e 08 8.20e 08 7.38e 08 6.46e 08 3.84e 08 3.27e 08 2.89e 08 2.74e 08 2.77e 08 2.68e 08 2.70e 08 2.26e 08 2.12e 08 2.20e 08 2.14e 08 2.31e 08 2.52e 08 1.89e 08 1.81e 08 1.97e 08 2.86e 08 4.99e 08 1.86e 08 1.43e 08 1.70e 08 1.25e 08 1.31e 08 2.52e 08 2.05e 08 1.64e 08 1.92e 08 2.25e 08 3.29e 08 5.21e 08 1.97e 08 1.81e 08 1.70e 08 1.75e 08 1.97e 08 4.03e 08 2.70e 08 1.93e 08 1.81e 08 1.94e 08 2.21e 08 3.92e 08 8.27e 07 8.68e 07 9.42e 07 1.15e 08 1.52e 08 2.88e 08 dOS dWS dIS sOS sWS sIS csOS AlexNet Layer 1 (Convolution) 4x18 6x12 8x9 9x8 12x6 18x4 FlexiSAGA Systolic Array Size 4.01e 08 3.61e 08 3.41e 08 3.35e 08 3.21e 08 3.08e 08 3.59e 08 3.45e 08 3.45e 08 3.46e 08 3.51e 08 3.69e 08 4.58e 08 3.90e 08 3.68e 08 3.52e 08 3.36e 08 3.14e 08 2.28e 08 2.18e 08 2.28e 08 2.36e 08 2.63e 08 3.16e 08 2.11e 08 1.90e 08 1.91e 08 1.91e 08 2.16e 08 2.57e 08 1.59e 08 1.15e 08 9.03e 07 8.30e 07 6.59e 07 4.85e 07 2.61e 08 2.16e 08 1.87e 08 1.79e 08 1.85e 08 2.67e 08 7.49e 07 9.65e 07 1.20e 08 1.29e 08 1.59e 08 2.20e 08 2.45e 08 2.51e 08 3.70e 08 2.90e 08 2.89e 08 3.02e 08 2.28e 08 1.87e 08 1.69e 08 1.81e 08 2.08e 08 2.56e 08 1.97e 08 1.36e 08 1.21e 08 1.02e 08 8.80e 07 8.02e 07 2.37e 08 2.17e 08 2.36e 08 2.06e 08 1.97e 08 2.35e 08 8.04e 07 9.47e 07 7.73e 07 9.53e 07 9.89e 07 1.62e 08 6.69e 07 8.17e 07 6.80e 07 7.63e 07 8.79e 07 1.37e 08 2.80e 07 3.05e 07 2.63e 07 2.89e 07 2.48e 07 2.65e 07 AlexNet Layer 5 (Fully-connected) Local Optimum Global Optimum Fig. 11. Design space exploration comparing the runtime in clock cycles of two AlexNet operators mapped onto a FlexiSAGA architecture of different shapes comprised of 72 PEs for all dataflows and different pruning parameters. explained by the linear scaling of the memory interface. The more PEs at the border are connected to the main memory, the more data can be streamed into the SA. These results show that finding the optimal combination of architecture configuration, pruning parameters, and dataflow is not trivial and motivates the use of a DNN HW co-design flow and an extensive performance evaluation. Generating the 1440 measurements using the VP took 421.4 h, with a mean runtime per measurement of 17.6 min and a standard deviation of 13.2 min. 7 Conclusion This paper presented FlexiSAGA a flexible systolic array GEMM accelerator for sparse and dense processing. Our results show a better sparse-over-dense whole DNN inference speedup compared to other architectures, reaching a speedup of up to 4.28. This is enabled by a near-optimal processing of DNN CONV and FC operators using seven different dense and sparse dataflows and a specifically tailored pruning technique, which constitutes a DNN HW co-design flow. FlexiSAGA is broadly applicable to DNNs comprised of mostly CONV and FC operators. Going forward, we plan to extend the range of supported operators to multi-head attention, which can be found in transformer networks such as large language and generative AI models. Acknowledgments. This work has been funded by the German Federal Ministry of Research, Technology and Space (BMFTR) under grant numbers 16ME0129 (Scale4Edge) and 01IS22086H (MANNHEIM-FlexKI). References 1. Amaranth HDL. (Mar 2025) 16 M. M. Müller and K. Lübeck et al. 2. Bernardo P.P. et al.: UltraTrail: A Configurable Ultralow-Power TC-ResNet AI Ac- celerator for Efficient Keyword Spotting. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 39(11), 4240 4251 (2020) 3. Chellapilla, K., Puri, S., Simard, P.: High Performance Convolutional Neural Net- works for Document Processing. In: Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft, La Baule, France (Oct 2006) 4. Chen, Y.H. et al.: Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE Journal of Solid-State Circuits 52(1), 127 138 (Jan 2017) 5. Cheng, H., Zhang, M., Shi, J.Q.: A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence 46(12), 10558 10578 (2024) 6. Ganguly, A., Muralidhar, R., Singh, V.: Towards Energy Efficient non-von Neu- mann Architectures for Deep Learning. In: 20th International Symposium on Qual- ity Electronic Design (ISQED). pp. 335 342 (2019) 7. Genc, H. et al.: Gemmini: Enabling Systematic Deep-Learning Architecture Eval- uation via Full-Stack Integration. In: Proceedings of the 58th Annual Design Au- tomation Conference (DAC) (2021) 8. Gonidmalla A. et al.: SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks. In: Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture. p. 151 165. MICRO 52, New York, NY, USA (2019) 9. He, K. et al.: Deep Residual Learning for Image Recognition (2015) 10. Iofinova, E. et al.: How Well Do Sparse ImageNet Models Transfer? In: 2022 IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE (Jun 2022) 11. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet Classification with Deep Convolutional Neural Networks. Communications of the ACM 60(6), 84 90 (May 2017) 12. Kurtic, E. et al.: Sparse Fine-tuning for Inference Acceleration of Large Language Models (2023) 13. Mishra, A. et al.: Accelerating Sparse Deep Neural Networks (2021) 14. Parashar, A. et al.: SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks. In: Proceedings of the 44th Annual International Symposium on Computer Architecture. p. 27 40. ISCA 17, Association for Computing Machinery, New York, NY, USA (2017) 15. Paszke, A. et al.: PyTorch: An Imperative Style, High-Performance Deep Learning Library. In: Advances in Neural Information Processing Systems. vol. 32. Curran Associates, Inc. (2019) 16. Simonyan, K., Zisserman, A.: Very Deep Convolutional Networks for Large-Scale Image Recognition (2015) 17. Soltaniyeh, M., Martin, R.P., Nagarakatte, S.: An Accelerator for Sparse Convolu- tional Neural Networks Leveraging Systolic General Matrix-matrix Multiplication. ACM Transactions on Architecture and Code Optimization 19(3), 1 26 (May 2022) 18. Szegedy, C. et al.: Going Deeper with Convolutions. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). p. 1 9 (Jun 2015) 19. Wen, W. et al.: Learning Structured Sparsity in Deep Neural Networks. In: Ad- vances in Neural Information Processing Systems. vol. 29. Curran Associates, Inc. (2016)\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.01566v1 [cs.PF] 2 Jun 2025 FlexiSAGA: A Flexible Systolic Array GEMM Accelerator for Sparse and Dense Processing Mika Markus Müller , Konstantin Lübeck , Alexander Louis-Ferdinand Jung , Jannik Steinmetz , and Oliver Bringmann Embedded Systems, University of Tübingen, Tübingen, Germany These authors contributed equally to this work. Abstract. Artificial Intelligence (AI) algorithms, such as Deep Neural Networks (DNNs), have become an important tool for a wide range of applications, from computer vision to natural language processing. How- ever, the computational complexity of DNN inference poses a significant challenge, particularly for processing on resource-constrained edge de- vices. One promising approach to address this challenge is the exploita- tion of sparsity in DNN operator weights. In this work, we present FlexiSAGA, an architecturally configurable and dataflow-flexible AI hardware accelerator for the sparse and dense pro- cessing of general matrix multiplications (GEMMs). FlexiSAGA sup- ports seven different sparse and dense dataflows, enabling efficient pro- cessing of resource intensive DNN operators. Additionally, we propose a DNN pruning method specifically tailored towards the FlexiSAGA architecture, allowing for near-optimal processing of dense and sparse convolution and fully-connected operators, facilitating a DNN HW co- design flow. Our results show a whole DNN sparse-over-dense inference speedup ranging from 1.41 up to 4.28, outperforming commercial and literature-reported accelerator platforms. 1 Introduction In recent years, the deployment of AI workloads, such as Deep Neural Networks (DNNs), has shifted away from datacenters to resource-constrained edge de- vices due to privacy concerns, real-time requirements and costs. To fulfil these non-functional requirements, specialized AI accelerators are often necessary, as they can process data locally and faster than conventional microcontrollers while maintaining a small area and energy footprint. When using an AI accelerator, the goal is often to efficiently process the most computationally intensive DNN operators, like convolution (CONV) and fully-connected (FC).\n\n--- Segment 2 ---\nTo fulfil these non-functional requirements, specialized AI accelerators are often necessary, as they can process data locally and faster than conventional microcontrollers while maintaining a small area and energy footprint. When using an AI accelerator, the goal is often to efficiently process the most computationally intensive DNN operators, like convolution (CONV) and fully-connected (FC). While the FC operator implements a general matrix mul- tiplication (GEMM), the CONV operator can be converted into a GEMM by applying an im2col transformation [3]. This enables the usage of the same ar- chitecture and mapping approach to process both operators. The GEMM can be further optimized by splitting it into smaller tiles, which allows for improved cache utilization, increased parallelism, and memory bandwidth optimization. 2 M. M. Müller and K. Lübeck et al. State-of-the-art GEMM accelerators [4,2,7] use systolic arrays (SAs) to ef- ficiently compute tiled GEMMs as they do not have the drawbacks of Von Neumann architectures [6]. SAs enable parallel processing where some data is streamed, while the other data is kept stationary to reach maximum reuse of the stationary data which minimizes costly memory accesses. The dataflow deter- mines which type of data remains stationary. There are three common dataflows used in SAs: output stationary (OS), weight stationary (WS) and input sta- tionary (IS). For different matrix and tile sizes used for the tiled GEMM these dataflows can reach vastly different runtimes depending on the architecture. To further improve the processing and memory footprint of DNNs, pruning has become a popular technique [5]. Pruning involves selectively removing less important DNN weights by replacing them with zeros, and thereby reducing the overall model complexity without significantly impacting its accuracy, which results in sparse weight matrices and therefore in sparse GEMMs. Several AI accelerators have been proposed which employ sparsity-centric optimizations to efficiently process sparse GEMMs [4,14,8,17]. However, a common limitation of many AI accelerators is that they tend to only process the sparse GEMMs using a single dataflow, which may not be the most efficient one for all DNN operators.\n\n--- Segment 3 ---\nSeveral AI accelerators have been proposed which employ sparsity-centric optimizations to efficiently process sparse GEMMs [4,14,8,17]. However, a common limitation of many AI accelerators is that they tend to only process the sparse GEMMs using a single dataflow, which may not be the most efficient one for all DNN operators. In this paper, we present FlexiSAGA, a flexible and configurable systolic ar- ray for GEMM acceleration, which can process both dense and sparse GEMMs using seven different dataflows to achieve a higher flexibility and higher sparse- over-dense speedups than many other accelerators when processing AI work- loads. To reach this goal, sparsity is exploited solely within the weight matrix. This can be done at the time of deployment, i.e. without affecting the on-device DNN processing time, compared to the dynamic identification of zeros within the input matrices at runtime. We use the two-stage bitmap format [17] and we present a custom sparse format to efficiently store and decode weight matrix tiles. Furthermore, we introduce a structured pruning technique based on [19]. The custom sparse format and the pruning technique are tailored specifically to the FlexiSAGA architecture, enabling a DNN HW co-design flow. We evaluate FlexiSAGA with a number of representative DNNs (AlexNet [11], VGG16 [16], GoogLeNet [18], and ResNet50 [9]) pruned and deployed on differently sized FlexiSAGA instances and compare the achieved sparse-over- dense speedups to an Intel Xeon CPU, the Nvidia Orin ARM CPU and GPU, and the SCNN [14] and SparTen [8] accelerators. These results show a whole DNN sparse-over-dense speedup of 1.41 up to 4.28. Additionally, we conduct a design space exploration (DSE) to find the near-optimal combination of dataflow, pruning and architectural parameters to minimize the DNN operator runtimes. 2 Related Work Several AI accelerators and pruning frameworks for processing sparse matrices have been proposed. The SPOTS accelerator [17] has an integrated im2col unit which transforms CONV input and weight tensors on-the-fly to process them as tiled GEMM.\n\n--- Segment 4 ---\n2 Related Work Several AI accelerators and pruning frameworks for processing sparse matrices have been proposed. The SPOTS accelerator [17] has an integrated im2col unit which transforms CONV input and weight tensors on-the-fly to process them as tiled GEMM. Additionally, the authors introduce a two-stage bitmap format FlexiSAGA 3 for storing sparse matrices pruned using structured sparsity learning [19]. Their accelerator shows a speedup of up to 20 for whole DNNs compared to the dense execution on a CPU and a speedup of up to 1.86 compared to other accelerator architectures presented in [4,7], supporting dense and sparse processing, using a reconfigurable systolic array composed of 512 processing elements. SCNN [14] focuses on the processing of CONV operators. Exploiting input and weight sparsity, the authors present a whole DNN speedup of up to 3.52 for VGG16 compared to the dense processing on the same architecture utilizing 64 processing elements and using a novel input stationary dataflow. SparTen [8] uses bitmap encoding to store sparse input and weight matrices. The results show a sparse-over-dense speedup of up to 15.5 for CONV operators using up to 64 compute clusters. DeepSparse [12] is a Sparsity-aware deep learning inference runtime for CPUs 1 which provides an automatic deployment for pruned PyTorch [15] mod- els. The authors report a whole DNN sparse-over-dense speedup of up to 8.2 for 32bit floating-point models running on an Intel Xeon CPU. Nvidia s 2:4 sparsity [13] is specifically tailored towards the Nvidia Ampere microarchitecture. 2:4 sparsity pruning splits the weight matrix into row vec- tors containing four elements. In each vector, the two smallest values are set to zero. After pruning, the DNNs are fine-tuned. They report a sparse-over-dense speedup of up to 2 for an Nvidia A100 GPU using 16bit floating-point models. We compare our FlexiSAGA architecture together with the proposed DNN pruning method to the CONV operator sparse-over-dense speedup of the one- sided SCNN and SparTen accelerators presented in [8] as they provide detailed per DNN operator results.\n\n--- Segment 5 ---\nThey report a sparse-over-dense speedup of up to 2 for an Nvidia A100 GPU using 16bit floating-point models. We compare our FlexiSAGA architecture together with the proposed DNN pruning method to the CONV operator sparse-over-dense speedup of the one- sided SCNN and SparTen accelerators presented in [8] as they provide detailed per DNN operator results. Additionally, we compare our whole DNN sparse- over-dense speedup to DeepSparse and TensorRT, run on an Intel Xeon CPU, Nvidia Orin ARM CPU and GPU respectively. 3 Sparse Matrix Formats An important factor for processing sparse GEMM operations is in which format sparse matrices are stored. A sparse matrix format should have a good compres- sion ratio to minimize the memory footprint and allow for efficient decompres- sion such that it does not slow down the processing of a GEMM operation. In the following section, different sparse matrix formats are briefly introduced and compared to each other. Additionally, we present the compressed sparse block (CSB) format, which is tailored towards the sparse GEMM processing of Flexi- SAGA. Fig. 1(a) shows a comparison of the different sparse matrix formats for a 128 512 matrix with varying sparsities and uniformly distributed zeros. The compressed sparse row (CSR) format stores the non-zero elements of a matrix in an array, along with two additional arrays that store the column in- dices and the row pointers. The row pointers array indicates the starting index of each row in the non-zero elements array. Similarly to CSR is the compressed 1 (accessed March 13, 2025). 4 M. M. Müller and K. Lübeck et al.\n\n--- Segment 6 ---\nSimilarly to CSR is the compressed 1 (accessed March 13, 2025). 4 M. M. Müller and K. Lübeck et al. 0.5 0.6 0.7 0.8 0.9 Sparsity 0 100 200 300 400 Memory Footprint (kB) No Compression RLE-4 COO Bitmap CSR Two-Stage-Bitmap CSC CSB (a) 1 3 2 4 6 5 7 1 3 2 4 6 5 7 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1 1 non-zero values column bit array element bit array 0 1 2 3 4 5 (b) 1 6 4 7 5 2 3 8 0 0 0 0 0 0 0 0 0 0 1 5 6 2 4 7 3 8 0 2 0 3 1 1 4 -1 5 3 column index array number of col. after merge non-zero values 0 1 2 3 4 5 (c) Fig. 1. (a) Memory footprint comparison of different sparse matrix formats and no compression for a 128 512 matrix of 32bit values with varying sparsities and uniformly distributed zero elements. (b) Two-stage bitmap format example. (c) Compressed sparse block (CSB) format example. sparse column (CSC) format, but it stores a matrix by columns instead of rows. The non-zero elements are also stored in an array, along with the row indices and column pointers stored in two additional arrays. The coordinate format (COO) stores the non-zero elements of a matrix as a list of (row, column, value) tuples, which is easy to implement but comes with significant storage overhead. The Run-Length Encoded 4bit (RLE-4) format represents a matrix as a sequence of 4bit codes, where each code indicates the length of a run of zeros followed by a non-zero element. This format can be very compact for matrices with long runs of zeros, but it may be less efficient for matrices with more uniform sparse dis- tributions. The bitmap format also contains an array for the non-zero elements, but additionally defines a bit array, where for every element is stored whether it is non-zero or zero indicated by a 1 and 0 respectively.\n\n--- Segment 7 ---\nThis format can be very compact for matrices with long runs of zeros, but it may be less efficient for matrices with more uniform sparse dis- tributions. The bitmap format also contains an array for the non-zero elements, but additionally defines a bit array, where for every element is stored whether it is non-zero or zero indicated by a 1 and 0 respectively. The two-stage bitmap format introduced in [17] uses an array containing all non-zero elements of a matrix and two bit arrays which encode the coordinates of non-zero elements. The first array, called the column bit array, contains as many bits as columns in a matrix. A 0 in the column bit array encodes that the corresponding matrix column contains only zero elements, a 1 encodes that the corresponding column contains non-zero elements. The second bit array, called the element bit array, encodes for each non-zero column which elements are non-zero. This allows for a high compression ratio and efficient decompression, where loading of entire zero columns can be skipped. Fig. 1(b) shows an example encoding for a 3 6 sparse matrix. The two-stage bitmap format demonstrates a good compression ratio, even for low sparsities (see Fig. 1(a)). However, the occurrence of entire zero columns of height n with uniformly distributed zero elements and sparsity s is described by the Bernoulli process n n sn(1 s)n n sn and is consequently very low for large n. Therefore, we introduce the compressed sparse block (CSB) format, which allows merging multiple non-zero columns with high sparsity into one.\n\n--- Segment 8 ---\n1(a)). However, the occurrence of entire zero columns of height n with uniformly distributed zero elements and sparsity s is described by the Bernoulli process n n sn(1 s)n n sn and is consequently very low for large n. Therefore, we introduce the compressed sparse block (CSB) format, which allows merging multiple non-zero columns with high sparsity into one. Similarly to the two-stage bitmap format, the CSB format uses one array to store all non-zero elements of a matrix, additionally for each non-zero element FlexiSAGA 5 LU 0,0 SU 0,n LU 0,1 Controller PE 0,0 PE 0,1 Main Memory SU 1,n SU m,n SU m,0 SU m,1 LU 0,n PE 0,n LU 1,0 PE 1,0 PE 1,1 LU m,0 PE 1,n PE m,0 PE m,1 PE m,n DecU (a) GEMM tiles dense weight stationary (dWS) dense output stationary (dOS) dense input stationary (dIS) movement of the streamed matrix tile movement of the the partial sums a d g h b e i c f A D G E H B I C F weight matrix input matrix c f i h b e g a d A D G H B E I C F A D G E H B I C F a d g h b e i c f c f i h b e g a d A B C E F D I G H (b) Fig. 2. (a) Block diagram of the FlexiSAGA architecture. (b) Visualization of the three dense tiled GEMM dataflows supported by the FlexiSAGA architecture. the column index is stored. The row index is implicitly encoded in the order of the column index array. For each column starting from the first, we use greedy search to find matching columns to merge with. Columns match if the position of the non-zero elements of one column matches the position of the zero elements in the other column, while it is allowed that zero elements of one column can match zero elements of the other column. Columns only containing zeros are not merged and are skipped entirely. The resulting number of columns after the merge is stored together with the non-zero elements and the column index array. Fig. 1(c) shows an example of the CSB format.\n\n--- Segment 9 ---\nFig. 1(c) shows an example of the CSB format. Because of the column index array, the memory footprint for the CSB format is higher than for the bitmap formats. However, it allows for skipping more than just zero columns because combined columns are loaded as a single column. FlexiSAGA utilizes the two-stage bitmap format and the CSB format to efficiently store and process sparse matrices, which is described in detail in the following section. 4 FlexiSAGA Architecture Fig. 2 shows the block diagram of the proposed FlexiSAGA architecture. Flexi- SAGA is designed as a systolic array (SA) whose main components are Process- ing Elements (PEs) organized in a 2D grid, configurable in its height and width. Each PE has a register file containing nine registers of configurable data word width to store stationary data and partial sums. An arithmetic logic unit (ALU) supporting floating-point and integer operations inside each PE can access the register file to execute move, multiply, addition, and multiply-accumulate opera- tions. Furthermore, the ALU of each PE can also write into the register files of its neighboring PEs to its right and below, which facilitates data transfer through the SA. To load data from the main memory into the SA, each PE in the left column and in the top row is connected to a load unit (LU) which performs mem- ory read transactions and writes the read data words into the register file of the 6 M. M. Müller and K. Lübeck et al. connected PE. All LUs are connected to the decompression unit (DecU) which acts as an arbiter for main memory accesses. When a dense GEMM is processed, the DecU forwards all read transactions to the main memory. In case of a sparse GEMM the DecU checks if the accessed value is declared as a zero in the sparse representation of the weight matrix tile and emits a zero instead of forwarding the read transaction to the main memory. The data width and the number of ports connecting the main memory to the DecU is configurable. The PEs on the bottom and the right column are connected to store units (SUs) which read data from the PE register files and write them into the memory, again through the DecU.\n\n--- Segment 10 ---\nThe data width and the number of ports connecting the main memory to the DecU is configurable. The PEs on the bottom and the right column are connected to store units (SUs) which read data from the PE register files and write them into the memory, again through the DecU. The behavior of all components is globally set by a programmable controller, which contains a schedule for the dense or sparse GEMM. In case of a sparse GEMM the controller is programmed to skip the processing of entire zero columns or rows of the weight matrix. The following sections detail how dense and sparse GEMMs are processed by the FlexiSAGA architecture. 4.1 Dense Tiled GEMM Processing FlexiSAGA supports the three common dense GEMM dataflows: input station- ary (dIS), weight stationary (dWS), and output stationary (dOS). Before pro- cessing a dense GEMM on the FlexiSAGA architecture, the weight and input matrices are split into tiles of the size of the SA. Fig. 2(b) presents the different dense tiled GEMM dataflows. When using the dIS dataflow, the elements of the weight matrix tile are streamed into the SA, while the elements of the input ma- trix tile are held stationary in the PEs. Each PE performs a multiply-accumulate operation between weight and input elements, accumulating the partial sums, which are then propagated through the array to compute the output tile. The dWS dataflow is similar to the dIS dataflow, but the roles of weight and input matrix tiles are reversed. In the dOS dataflow, the elements of the weight and in- put tiles are streamed into the SA, and each PE performs a multiply-accumulate operation, accumulating the partial sums in a local register. The output tile is then read from the PE registers after all the partial sums have been accumulated. 4.2 Sparse Tiled GEMM Processing In addition to the dense tiled GEMM processing, FlexiSAGA supports output stationary (sOS), weight stationary (sWS), and input stationary (sIS) dataflows for sparse tiled GEMMs. All three sparse dataflows use the two-stage bitmap format to compress the weight matrix tiles.\n\n--- Segment 11 ---\n4.2 Sparse Tiled GEMM Processing In addition to the dense tiled GEMM processing, FlexiSAGA supports output stationary (sOS), weight stationary (sWS), and input stationary (sIS) dataflows for sparse tiled GEMMs. All three sparse dataflows use the two-stage bitmap format to compress the weight matrix tiles. Additionally, for the output station- ary dataflow we propose a variant using the CSB format to compress the weight matrix tiles (csOS). The following sections detail all four sparse dataflows. The tiles of the weight matrices are stored in a compressed format in the main mem- ory before the inference of a DNN. The input matrices are stored in row-major order in the main memory, and the addresses for accessing the corresponding input tile elements for a weight tile are generated by the FlexiSAGA controller. The elements of the output matrix are stored in row-major order in the main memory and serve as input matrix for the succeeding DNN operator.\n\n--- Segment 12 ---\nThe input matrices are stored in row-major order in the main memory, and the addresses for accessing the corresponding input tile elements for a weight tile are generated by the FlexiSAGA controller. The elements of the output matrix are stored in row-major order in the main memory and serve as input matrix for the succeeding DNN operator. FlexiSAGA 7 a b c 0 0 0 0 0 d e 0 0 A B C D E F G H 0 c B a A Step 0 0 c A a B a A a A Step 1 0 A c B c A c A a B a B a A a A Step 2 0 B 0 A 0 A c B c B c A c A a B a B a A a A Step 3 0 B 0 B 0 A 0 A c B c B c A c A a B a B a A a A Step 4 0 B 0 B e A 0 A c B c B d A c A a H a B b G a A Step 5 0 B 0 B e A 0 A c B c B d G c A b H a B b G a A b G Step 6 0 B 0 B e G 0 A d H c B d G c A d G b H a B b H b G a A b G Step 7 e H 0 B e G 0 A e G d H c B d H d G c A d G b H a B b H b G a A b G Step 8 e H 0 B e H e G 0 A e G d H c B d H d G c A d G b H a B b H b G a A b G Step 9 weight input partial sum 1 0 0 1 a c b d e (d) Step by step processing of sparse tiled GEMM for the output stationary dataflow (sOS) using the two-stage bitmap format. (a) GEMM tiles weights inputs non-zero values column bit array 1 1 0 1 1 1 element bit array (b) Two-stage bitmap format representation (c) PE register contents Fig. 3. Example for processing single tile using the sparse tiled GEMM output sta- tionary dataflow (sOS) on a FlexiSAGA architecture of size 2 3. Sparse Output Stationary Dataflow (sOS) Fig. 3 presents an example of processing of a single GEMM tile using the sOS dataflow.\n\n--- Segment 13 ---\nSparse Output Stationary Dataflow (sOS) Fig. 3 presents an example of processing of a single GEMM tile using the sOS dataflow. All weight matrix tiles are compressed using the two-stage bitmap format (see Fig. 3(b)). While the weight matrix tile consists of 12 elements in its uncompressed form, storing it in the two-stage bitmap format allows us to only read seven data words to access the whole tile. Fig. 3(d) shows the step by step processing for the sOS dataflow. Each step shows the register contents of all six PEs, while Fig. 3(c) provides a legend for the PE register contents. In step 0, the first column of the weight tile (a,c,0) is loaded into the registers of the PEs in the left column and the first row of the input tile (A,B) is loaded into PEs in the top row. In step 1, the top left PE calculates the first partial sum and stores it in its register, additionally this PE copies the input element A into the PE below it and the weight element a into the PE to the right of it. In step 2, the top right PE and the middle left PE calculate their partial sums and copy the input and weight elements to their neighbor PEs below and to the right (if they exist). This is repeated until step 4, in which all PEs contain a partial sum and each element of the first weight tile column has been multiplied with each element of the first input tile row. In steps 5 to 9, the last column of the weight tile (b,d,e) and the last row of the input tile (G,H) are loaded into the PEs and the partial sums of the multiplications are accumulated. The PE registers now contain all non-zero elements of the output tile, which are then forwarded to the main memory. Sparse Weight Stationary Dataflow (sWS) Fig. 4 presents an example of processing of a single GEMM tile using the sWS dataflow. Fig. 4(d) shows the PE register contents for each step of the sWS dataflow processing. In step 0 the whole weight tile is loaded into the SA and stays there until all elements of the output tile have been calculated.\n\n--- Segment 14 ---\n4(d) shows the PE register contents for each step of the sWS dataflow processing. In step 0 the whole weight tile is loaded into the SA and stays there until all elements of the output tile have been calculated. In steps 1 to 4, the first column of the input tile is propagated vertically through the SA and the first column of the output tile is accumulated in the right PE column and then forwarded to the main memory. In 8 M. M. Müller and K. Lübeck et al. a b c d e 0 0 0 0 0 A B C D e 0 d c b C a A Step 0 e 0 d c A b C a A a A a A Step 1 e 0 A d C c A c A c A b C a A b C a A a A Step 2 e C 0 A 0 A 0 A d C c A d C c A c A b C a A b C a A a A Step 3 e C 0 A e C 0 A 0 A d C c A d C c A c A b C a A b C a A a A Step 4 e C 0 A d C c A b D a B Step 5 e C 0 A d C c B b D a B a B a B Step 6 e C 0 B d D c B c B c B b D a B b D a B a B Step 7 e D 0 B 0 B 0 B d D c B d D c B c B b D a B b D a B a B Step 8 e D 0 B e D 0 B 0 B d D c B d D c B c B b D a B b D a B a B Step 9 weight input partial sum 1 0 0 1 1 a b c d e (d) Step by step processing of sparse tiled GEMM for the weight stationary dataflow (sWS) using the two-stage bitmap format. (a) GEMM tiles weights inputs non-zero values row bit array 1 1 1 1 0 1 element bit array (b) Two-stage bitmap format representation (c) PE register contents Fig. 4.\n\n--- Segment 15 ---\n(a) GEMM tiles weights inputs non-zero values row bit array 1 1 1 1 0 1 element bit array (b) Two-stage bitmap format representation (c) PE register contents Fig. 4. Example for processing single tile using the sparse tiled GEMM weight station- ary dataflow (sWS) on a FlexiSAGA architecture of size 2 3. a d e c b 0 0 0 0 0 0 0 A B C D E F F c E D b C B a A Step 0 c F e E b D 0 C a B d A Step 5 c F e E b D 0 C d A d B a B d A d A Step 6 c F e E d A 0 C 0 D d B 0 C d A 0 C d B d B d A d A Step 7 e F d B 0 D e E d A 0 C e E 0 D d B 0 D 0 C d A 0 C d B d B d A d A Step 8 e F d B 0 D e F e E d A 0 C e E 0 D d B 0 D 0 C d A 0 C d B d B d A d A Step 9 F c E D b C a A a B a A a A Step 1 F c E a A b C b D a B b C a A b C a B a B a A a A Step 2 c F a B b D c E a A b C c E b D a B b D b C a A b C a B a B a A a A Step 3 c F a B b D c F c E a A b C c E b D a B b D b C a A b C a B a B a A a A Step 4 weight input partial sum 1 0 0 1 a b c d e (d) Step by step processing of sparse tiled GEMM for the input stationary dataflow (sIS) using the two-stage bitmap format. (a) GEMM data weights inputs non-zero values row bit array 1 1 1 1 0 1 element bit array (b) Two-stage bitmap format representation (c) PE register contents Fig. 5.\n\n--- Segment 16 ---\n(a) GEMM data weights inputs non-zero values row bit array 1 1 1 1 0 1 element bit array (b) Two-stage bitmap format representation (c) PE register contents Fig. 5. Example for processing single tile using the sparse tiled GEMM input stationary dataflow (sIS) on a FlexiSAGA architecture of size 2 3. steps 5 to 9, the partial sums are cleared and the second column of the input tile is propagated through the SA in the same manner as the first column. Similarly to the sOS dataflow in this example only seven data words need to be read from the main memory to process a weight tile containing ten elements. Sparse Input Stationary Dataflow (sIS) Fig. 5 presents an example of processing of a single GEMM tile using the sIS dataflow. Fig. 5(d) shows the PE register contents for each step of the sIS dataflow processing. In step 0 the whole input tile is loaded into the SA and stays there until all output tile elements have been calculated. Additionally, the first row of the weight tile is loaded into the registers of the left PE column. In steps 1 to 4, the weight tile row is horizontally propagated through the SA and the first output tile row is accumulated in the bottom PE row and then forwarded to the main memory.\n\n--- Segment 17 ---\nAdditionally, the first row of the weight tile is loaded into the registers of the left PE column. In steps 1 to 4, the weight tile row is horizontally propagated through the SA and the first output tile row is accumulated in the bottom PE row and then forwarded to the main memory. FlexiSAGA 9 a d c b e 0 0 0 0 0 0 0 e a b d c 0 A B C D E F G H A B C D E F G H a A a A 0 a B a B c A c A 0 0 c B c B -1 Step 2 a A a A 0 a B a B c A c A 0 0 c B c B A -1 0 B Step 3 b A a A 3 a B a B d A c A 1 c B c B e A 3 B Step 4 b G a A b G 3 b H a B b H d A c A 1 d B c B e A 3 e B Step 5 b G a A b G 3 b H a B b H d G c A 1 3 d H c B e A 3 e B Step 6 b G a A b G 3 b H a B b H d G c A 1 3 d H c B e G e G 3 3 e H e H Step 7 b C a A b G 3 b D a B b H d C c A 1 d D c B e G e G 3 e H e H Step 8 b C a A b G 3 b D a B b H d C c A d C 1 d D c B d D e C e G 3 1 e D e H Step 9 a A a A 0 a B a B c 0 c -1 Step 1 a 0 c 0 -1 Step 0 col idx weight input tmp idx partial sum 0 0 -1 3 1 3 2 a c b d e (d) Step by step processing of sparse tiled GEMM for the output stationary dataflow (csOS) using the CSB format. (a) GEMM tiles CSB comp. weight tile input tile weights inputs non-zero values num. of columns column index array (b) CSB format representation (c) controller and PE register contents PE finished Fig. 6.\n\n--- Segment 18 ---\nof columns column index array (b) CSB format representation (c) controller and PE register contents PE finished Fig. 6. Example for processing single tile using the sparse tiled GEMM output station- ary dataflow (csOS) using the CSB format on a FlexiSAGA architecture of size 2 3. In steps 5 to 9, the partial sums are cleared and the next non-zero row of the weight tile is propagated through the SA in the same manner as the first row. As seen before, the two-stage bitmap format drastically reduces the amount of data words which have to be read from the main memory for the weight tile. Sparse Output Stationary Dataflow (csOS) using the CSB format In contrast to the sOS, sWS, and sIS dataflows which utilize the two-stage bitmap format to compress the weight tiles, the csOS dataflow uses the CSB format. Since the CSB format merges multiple weight tile columns, the input tile rows and weight tile columns cannot simply be propagated through the SA as input and weight elements might not match anymore. Therefore, the FlexiSAGA con- troller tracks the column index of the weights the PE row currently stores to identify if a multiply-accumulate operation is needed. Fig. 6(d) shows the step by step processing for the csOS dataflow. Each step shows the register contents of all six PEs together with the column index and a temporary index for each PE row stored in the controller, while Fig. 6(c) provides a legend for the controller and PE register contents. In step 0, the first column of the weight tile is loaded into the registers of the left PEs while the column index for each PE row is stored in the controller. In step 1 the first row of the input tile is loaded into the first PE row because the column index for the first PE row is set to 0, which corresponds to the first row of the input tile. Additionally, weights of the left PE column are copied to the right PE column and PEs in the first row calculate their partial sums. Because the PEs in the first row have finished their processing for the current weights, the PEs are marked with a blue outline.\n\n--- Segment 19 ---\nAdditionally, weights of the left PE column are copied to the right PE column and PEs in the first row calculate their partial sums. Because the PEs in the first row have finished their processing for the current weights, the PEs are marked with a blue outline. In step 2, the input elements are propagated vertically to the next PE row and the column index of the first row is copied to the temporary index of the second row and the PEs in the second row compute their partial sums. In step 3, the inputs are further propagated, however, the PEs in the last row do not have to execute a multiply-accumulate operation because the column index 10 M. M. Müller and K. Lübeck et al. is set to -1 which indicates a zero weight. Now all rows are marked as finished, which implies that in step 4 the next weight column is loaded into the left PE column. Step 5 is analogous to step 1. In step 6, inputs are propagated vertically from the first to the second PE row. Now the column index and the temporary index in the second PE row do not match, which means no computation has to be done. Step 7 is analogous to step 2. After step 7, the second PE row remains unmarked, which means the PEs contain weights for a different input row. In step 8, the inputs corresponding to the column index stored for the second PE row are loaded into the SA and propagated to the second PE row. In step 9 the PE register files now contain all non-zero elements of the output tile, which are then forwarded to the main memory. If the two-stage bitmap format had been applied to the weight tile presented in Fig. 6(a), an additional weight column would be loaded when using the sOS dataflow resulting in more data movement. 5 Deep Neural Network Pruning To exploit sparsity during the inference of DNNs, they have to be pruned before deployment. The simplest technique is global unstructured pruning, in which a global sparsity s for a whole pre-trained DNN is set. Then a proportion s of all weights wi which have the smallest l1-norm wi are set to zero. Even though this technique can lead to runtime improvements, there is often a considerable accuracy loss and an imbalanced sparsity distribution across all weights.\n\n--- Segment 20 ---\nThen a proportion s of all weights wi which have the smallest l1-norm wi are set to zero. Even though this technique can lead to runtime improvements, there is often a considerable accuracy loss and an imbalanced sparsity distribution across all weights. Addi- tionally, the unstructured sparsity may lead to a complex control flow, degrading the runtime improvements. To alleviate those drawbacks, we implemented a pruning technique based on structured sparsity learning [19] in PyTorch [15] which allows us to introduce zero rows and columns in the DNN weight matrix tiles tailored towards the two- stage bitmap and CSB format. Firstly, the DNNs are trained using a standard training loop until they reach the desired accuracy a. Secondly, the prunable DNN operators, in our case CONV and FC, are grouped by operator type. Each group j gets assigned a sparsity sj. Afterward, each weight tensor of the CONV group is transformed into a matrix using an im2col transformation and split into tiles of the desired size. The FC weights can be split into tiles directly. Those tiles are further split into row or column vectors wi Wj of length n corresponding to the tile width or height. For each group j, the proportion sj of wi Wj with the smallest l2-norm wi 2 pPn k 0 w2 k are set to zero. Then the DNN is trained again until it reaches accuracy a ϵ with ϵ 0 while the pruned vectors stay zero. When the accuracy a ϵ is reached, sj is increased by δj and the weights in each group are pruned again. This is repeated until the DNN training cannot reach accuracy a ϵ anymore after a fixed amount of training epochs.\n\n--- Segment 21 ---\nWhen the accuracy a ϵ is reached, sj is increased by δj and the weights in each group are pruned again. This is repeated until the DNN training cannot reach accuracy a ϵ anymore after a fixed amount of training epochs. FlexiSAGA 11 Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 (FC) Layer 7 (FC) Overall 0.0 0.2 0.4 0.6 0.8 1.0 Sparsity AlexNet Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Layer 13 (FC) Layer 14 (FC) Layer 15 (FC) Overall 0.0 0.2 0.4 0.6 0.8 1.0 Sparsity VGG16 Block 1 Block 2 Block 3 Block 3a Block 3b Block 4a Block 4b Block 4c Block 4d Block 4e Block 5a Block 5b FC Overall 0.0 0.2 0.4 0.6 0.8 1.0 Sparsity GoogLeNet n 1 Row n 4 Col. n 4 Row n 8 Col. n 8 Row n 16 Col. n 16 Block 0 Block 1.0 Block 1.1 Block 1.2 Block 2.0 Block 2.1 Block 2.2 Block 2.3 Block 3.0 Block 3.1 Block 3.2 Block 3.3 Block 3.4 Block 3.5 Block 4.0 Block 4.1 Block 4.2 FC Overall 0.0 0.2 0.4 0.6 0.8 1.0 Sparsity ResNet50 Fig. 7. Operator and operator block sparsities for different DNNs and vector lengths n and vector orientations. 6 Results 6.1 Experimental Setup To evaluate the FlexiSAGA architecture, we implemented a configurable virtual prototype (VP) using the Amaranth HDL [1] which allows for translation into a register-transfer level (RTL) representation for the integrated RTL simulator or into synthesizable Verilog. For the runtime results presented in this work, we used the Amaranth HDL RTL simulator together with a cycle-approximate SRAM memory model with unit read and write latencies and eight ports spread across multiple banks and a port width of 32bit adopted from the UltraTrail [2] memory architecture.\n\n--- Segment 22 ---\n6 Results 6.1 Experimental Setup To evaluate the FlexiSAGA architecture, we implemented a configurable virtual prototype (VP) using the Amaranth HDL [1] which allows for translation into a register-transfer level (RTL) representation for the integrated RTL simulator or into synthesizable Verilog. For the runtime results presented in this work, we used the Amaranth HDL RTL simulator together with a cycle-approximate SRAM memory model with unit read and write latencies and eight ports spread across multiple banks and a port width of 32bit adopted from the UltraTrail [2] memory architecture. The PE s register files are set to a width of 32bit, and the PE ALUs support 32bit floating-point operations. We use the PyTorch reference implementations of AlexNet [11], VGG16 [16], ResNet50 [9], and GoogLeNet [18] and trained them for classification of the CIFAR-10 dataset with the following top-1 accuracies a: AlexNet 0.86, VGG16 0.89, ResNet50 0.89, and GoogLeNet 0.91 using 32bit floating-point weights. Those DNNs were chosen to compare against the results presented in [8]. After training, the DNNs were pruned using the technique introduced in section 5 with an initial group sparsity sj 0.7, a sparsity progression δj 0.01 for all groups j, an ϵ a 0.02, and a stochastic gradient descent optimizer which allows for a maximum top-1 accuracy drop of 2 which is in accordance with the accuracies for pruned DNNs reported in [17]. Fig. 7 shows the operator and overall sparsities after pruning, for each DNN using different vector lengths n (1, 4, 8, and 16) and row or column orientation. Since ResNet50 consists of 109 CONV and FC operators and GoogLeNet consists of 115 operators, their sparsity results are presented in operator blocks. Pruning with vector length n 1 shows the highest sparsities across all DNNs, however, n 1 does not introduce any structured sparsity. With an increasing n, the first couple of operators for all DNNs and the last FC operator of AlexNet and VGG16 drastically decrease in sparsity.\n\n--- Segment 23 ---\nPruning with vector length n 1 shows the highest sparsities across all DNNs, however, n 1 does not introduce any structured sparsity. With an increasing n, the first couple of operators for all DNNs and the last FC operator of AlexNet and VGG16 drastically decrease in sparsity. 12 M. M. Müller and K. Lübeck et al. 4x4 8x8 16x16 4x4 8x8 16x16 4x4 8x8 16x16 4x4 8x8 16x16 FlexiSAGA Systolic Array Size 109 1010 Cycles (log) AlexNet VGG16 GoogLeNet ResNet50 dense sparse (a) csOS dWS dOS sWS sOS dataflow 0 50 100 150 200 selected 40.2 24.3 16.9 15.2 3.4 (b) Fig. 8. (a) Comparison of whole DNN runtimes in clock cycles for different FlexiSAGA systolic array sizes. (b) Distribution of selected dataflows with minimal runtime per DNN operator across all DNNs and all FlexiSAGA sizes. The overall sparsity shows only slight variations for different vector lengths and orientations and remains above 0.75 for AlexNet, VGG16, and GoogLeNet which is comparable to the sparsities reported for the same DNNs in [17]. To compare the sparse-over-dense inference speedup to other architectures, we used the DeepSparse [10] inference engine and deployed the four DNNs un- pruned (dense) and pruned (sparse) onto an Intel Xeon Platinum 8168 CPU with 24 cores and 48 threads and the Nvidia Jetson AGX Orin ARM Cortex- A78AE CPU with 12 cores. Each DNN was run using a single core and multiple cores threads. The sparse variant of the DNNs were pruned with ϵ a 0.02 and n 1, as it provides the highest sparsity and the highest speedup when using DeepSparse and limiting the top-1 accuracy drop to 2 .\n\n--- Segment 24 ---\nEach DNN was run using a single core and multiple cores threads. The sparse variant of the DNNs were pruned with ϵ a 0.02 and n 1, as it provides the highest sparsity and the highest speedup when using DeepSparse and limiting the top-1 accuracy drop to 2 . Additionally, we used Nvidia s 2:4 sparsity [13] to prune all four DNNs with a maximum top-1 accuracy drop of 2 and deployed the dense and sparse 16bit floating-point vari- ants onto the Nvidia Jetson AGX Orin Ampere GPU using TensorRT. For the DeepSparse and TensorRT runtime measurements, we collected multiple sam- ples to calculate the mean to accommodate for interferences of the operating system. Moreover, we compared the operator-wise sparse-over-dense speedup to the SCNN and SparTen architectures using the results reported in [8]. 6.2 Whole DNN Inference Evaluation Fig. 8(a) presents the runtime in clock cycles for the four DNNs deployed onto the FlexiSAGA architecture of different sizes (4 4, 8 8, and 16 16). Dense represents the clock cycle sum of the unpruned CONV and FC operators, while sparse represents the clock cycle sum of the pruned operators. The DNNs were pruned with vector length n set to the systolic array size in row and column ori- entation. For each operator, the dataflow with the minimal runtime (dOS, dWS, dIS, sOS, sWS, sIS, or csOS) was chosen by measuring all different variants. For the pruned DNNs the vector orientation is the same for all operators. For all DNNs, the runtime decreases when quadrupling the SA size. The mean dense and sparse speedup across all DNNs when increasing the SA size is 2.1 and 2.07 respectively. Even though the number of PEs scales quadratically, the memory interface scales only linearly because only the outer PEs have access to the main memory. This linear scaling is reflected in the mean dense and sparse speedup.\n\n--- Segment 25 ---\nEven though the number of PEs scales quadratically, the memory interface scales only linearly because only the outer PEs have access to the main memory. This linear scaling is reflected in the mean dense and sparse speedup. FlexiSAGA 13 AlexNet VGG16 GoogLeNet ResNet50 0 1 2 3 4 Speedup Nvidia Orin GPU Nvidia Orin ARM CPU, single-core Nvidia Orin ARM CPU, 12 cores Intel Xeon CPU, single-core Intel Xeon CPU, 48 threads FlexiSAGA 4x4 FlexiSAGA 8x8 FlexiSAGA 16x16 Fig. 9. Whole DNN sparse-over-dense speedup comparison between an Nvidia Orin GPU (2:4 sparsity), Nvidia Orin ARM CPU and Intel Xeon CPU (DeepSparse), and FlexiSAGA with different systolic array sizes. Fig. 9 shows the sparse-over-dense speedup comparison for the Nvidia Orin GPU, Nvidia Orin ARM CPU, Intel Xeon CPU, and FlexiSAGA of different sizes. While FlexiSAGA only supports the CONV and FC operators, the other platforms support all operators of the deployed DNNs. However, the runtime of activation, pooling and element-wise operators is very small compared to the CONV and FC operators. AlexNet generally shows the best sparse-over-dense speedup for all platforms, this can be attributed to the high sparsity that can be achieved across all operators when pruning AlexNet (see Fig. 7). ResNet50 shows the lowest speedup across all architectures because ResNet50 has the lowest overall sparsity and consists of 109 CONV and FC operators, which have smaller inputs and weights than other DNNs, which leads to more processing overhead. FlexiSAGA shows better sparse-over-dense speedups, ranging from 1.41 for ResNet50 to 4.28 for AlexNet, compared to CPU and GPU architectures using highly optimized inference runtimes. A key factor for achieving better sparse-over-dense speedups is that for each operator, the dataflow with minimal runtime was selected. Fig. 8(b) presents the distribution of selected dataflows per DNN operator across all DNNs and all FlexiSAGA sizes.\n\n--- Segment 26 ---\nFig. 8(b) presents the distribution of selected dataflows per DNN operator across all DNNs and all FlexiSAGA sizes. This indicates that a significant amount of sparse-over-dense speedup can be attributed to the csOS dataflow which specifically exploits the proposed CSB sparse matrix format. 6.3 DNN Operator-wise Inference Evaluation To provide a more fine-granular analysis, this section presents an operator-wise sparse-over-dense speedup comparison of FlexiSAGA with the one-sided SCNN and SparTen architectures for the CONV operators of AlexNet, VGG16, and GoogLeNet (3a and 5a blocks) for which the speedups are reported in [8]. Fig. 10 shows the sparse-over-dense speedup comparison for all three architectures. Since the SCNN and SparTen architectures for which the results are reported only sup- port CONV operators and are comprised of 64 processing elements, we compare them to FlexiSAGA with an SA size of 8 8 and mapped only the CONV op- erators. While FlexiSAGA shows lower speedups for the first couple of CONV operators, the sparse-over-dense speedup is often higher for the second half. This can be explained by the sparsity distribution across all operators, as presented in Fig. 7. This is especially apparent for the GoogLeNet operators in block 5a, 14 M. M. Müller and K. Lübeck et al. Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Mean 0.0 2.5 5.0 7.5 Speedup AlexNet one-sided SCNN one-sided SparTen FlexiSAGA 8x8 Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Mean 0.0 2.5 5.0 7.5 Speedup VGG16 Layer 3a.0 Layer 3a.1 Layer 3a.2 Layer 3a.3 Layer 3a.4 Layer 3a.5 Layer 5a.0 Layer 5a.1 Layer 5a.2 Layer 5a.3 Layer 5a.4 Layer 5a.5 Mean 100 101 102 Speedup (log) GoogLeNet Fig. 10.\n\n--- Segment 27 ---\nLayer 0 Layer 1 Layer 2 Layer 3 Layer 4 Mean 0.0 2.5 5.0 7.5 Speedup AlexNet one-sided SCNN one-sided SparTen FlexiSAGA 8x8 Layer 0 Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6 Layer 7 Layer 8 Layer 9 Layer 10 Layer 11 Layer 12 Mean 0.0 2.5 5.0 7.5 Speedup VGG16 Layer 3a.0 Layer 3a.1 Layer 3a.2 Layer 3a.3 Layer 3a.4 Layer 3a.5 Layer 5a.0 Layer 5a.1 Layer 5a.2 Layer 5a.3 Layer 5a.4 Layer 5a.5 Mean 100 101 102 Speedup (log) GoogLeNet Fig. 10. Operator-wise sparse-over-dense speedup comparison between one-sided SCNN, one-sided SparTen, and FlexiSAGA 8 8 for AlexNet, VGG16, and GoogLeNet convolution operators. where some operators have such a high sparsity that they can almost be skipped entirely, leading to a sparse-over-dense speedup of up to 100. For the mean sparse-over-dense speedup FlexiSAGA shows better results for all three DNNs. 6.4 Design Space Exploration Lastly, we present a design space exploration (DSE) in Fig. 11 for two AlexNet operators processed on different FlexiSAGA instances, all comprised of 72 PEs coupled with different dataflows and pruning parameters. The DSE heatmap shows the runtime in clock cycles. For the CONV operator on the left-hand side, the best runtime (local optimum) is achieved for a FlexiSAGA instance of size 6 12 using the sOS dataflow with pruned column vectors of length n 6. For the FC operator on the right-hand side, the best runtime (local optimum) is reached with a SA of size 12 6 using the csOS dataflow and pruned column vectors of length n 12. We conducted this DSE for all AlexNet CONV and FC operators and selected the architecture and pruning method which has the lowest whole DNN inference runtime. This resulted in a SA of size 4 18 for which the DNN is pruned using column vectors with n 4. For the presented operators in Fig.\n\n--- Segment 28 ---\nThis resulted in a SA of size 4 18 for which the DNN is pruned using column vectors with n 4. For the presented operators in Fig. 11 the dataflow with the best global runtime is csOS (global optimum).\n\n--- Segment 29 ---\nFor the presented operators in Fig. 11 the dataflow with the best global runtime is csOS (global optimum). The unbalanced row to column ratio of the best SA instance can be FlexiSAGA 15 4x18 6x12 8x9 9x8 12x6 18x4 FlexiSAGA Systolic Array Size n 1 Row Col. n 1 Row Col. n 1 Row Col. n 1 Row Col. Dataflow Pruning Parameter 4.36e 08 3.09e 08 2.46e 08 2.38e 08 3.01e 08 4.30e 08 4.45e 08 3.39e 08 2.96e 08 2.85e 08 2.71e 08 3.05e 08 4.85e 08 3.73e 08 3.24e 08 3.11e 08 3.65e 08 4.84e 08 1.55e 08 1.43e 08 2.16e 08 2.51e 08 3.66e 08 6.13e 08 2.08e 08 1.58e 08 2.12e 08 2.33e 08 3.32e 08 5.18e 08 8.49e 07 7.67e 07 9.96e 07 1.16e 08 1.53e 08 2.36e 08 2.97e 08 2.64e 08 8.53e 08 8.20e 08 7.38e 08 6.46e 08 3.84e 08 3.27e 08 2.89e 08 2.74e 08 2.77e 08 2.68e 08 2.70e 08 2.26e 08 2.12e 08 2.20e 08 2.14e 08 2.31e 08 2.52e 08 1.89e 08 1.81e 08 1.97e 08 2.86e 08 4.99e 08 1.86e 08 1.43e 08 1.70e 08 1.25e 08 1.31e 08 2.52e 08 2.05e 08 1.64e 08 1.92e 08 2.25e 08 3.29e 08 5.21e 08 1.97e 08 1.81e 08 1.70e 08 1.75e 08 1.97e 08 4.03e 08 2.70e 08 1.93e 08 1.81e 08 1.94e 08 2.21e 08 3.92e 08 8.27e 07 8.68e 07 9.42e 07 1.15e 08 1.52e 08 2.88e 08 dOS dWS dIS sOS sWS sIS csOS AlexNet Layer 1 (Convolution) 4x18 6x12 8x9 9x8 12x6 18x4 FlexiSAGA Systolic Array Size 4.01e 08 3.61e 08 3.41e 08 3.35e 08 3.21e 08 3.08e 08 3.59e 08 3.45e 08 3.45e 08 3.46e 08 3.51e 08 3.69e 08 4.58e 08 3.90e 08 3.68e 08 3.52e 08 3.36e 08 3.14e 08 2.28e 08 2.18e 08 2.28e 08 2.36e 08 2.63e 08 3.16e 08 2.11e 08 1.90e 08 1.91e 08 1.91e 08 2.16e 08 2.57e 08 1.59e 08 1.15e 08 9.03e 07 8.30e 07 6.59e 07 4.85e 07 2.61e 08 2.16e 08 1.87e 08 1.79e 08 1.85e 08 2.67e 08 7.49e 07 9.65e 07 1.20e 08 1.29e 08 1.59e 08 2.20e 08 2.45e 08 2.51e 08 3.70e 08 2.90e 08 2.89e 08 3.02e 08 2.28e 08 1.87e 08 1.69e 08 1.81e 08 2.08e 08 2.56e 08 1.97e 08 1.36e 08 1.21e 08 1.02e 08 8.80e 07 8.02e 07 2.37e 08 2.17e 08 2.36e 08 2.06e 08 1.97e 08 2.35e 08 8.04e 07 9.47e 07 7.73e 07 9.53e 07 9.89e 07 1.62e 08 6.69e 07 8.17e 07 6.80e 07 7.63e 07 8.79e 07 1.37e 08 2.80e 07 3.05e 07 2.63e 07 2.89e 07 2.48e 07 2.65e 07 AlexNet Layer 5 (Fully-connected) Local Optimum Global Optimum Fig.\n\n--- Segment 30 ---\n11 the dataflow with the best global runtime is csOS (global optimum). The unbalanced row to column ratio of the best SA instance can be FlexiSAGA 15 4x18 6x12 8x9 9x8 12x6 18x4 FlexiSAGA Systolic Array Size n 1 Row Col. n 1 Row Col. n 1 Row Col. n 1 Row Col. Dataflow Pruning Parameter 4.36e 08 3.09e 08 2.46e 08 2.38e 08 3.01e 08 4.30e 08 4.45e 08 3.39e 08 2.96e 08 2.85e 08 2.71e 08 3.05e 08 4.85e 08 3.73e 08 3.24e 08 3.11e 08 3.65e 08 4.84e 08 1.55e 08 1.43e 08 2.16e 08 2.51e 08 3.66e 08 6.13e 08 2.08e 08 1.58e 08 2.12e 08 2.33e 08 3.32e 08 5.18e 08 8.49e 07 7.67e 07 9.96e 07 1.16e 08 1.53e 08 2.36e 08 2.97e 08 2.64e 08 8.53e 08 8.20e 08 7.38e 08 6.46e 08 3.84e 08 3.27e 08 2.89e 08 2.74e 08 2.77e 08 2.68e 08 2.70e 08 2.26e 08 2.12e 08 2.20e 08 2.14e 08 2.31e 08 2.52e 08 1.89e 08 1.81e 08 1.97e 08 2.86e 08 4.99e 08 1.86e 08 1.43e 08 1.70e 08 1.25e 08 1.31e 08 2.52e 08 2.05e 08 1.64e 08 1.92e 08 2.25e 08 3.29e 08 5.21e 08 1.97e 08 1.81e 08 1.70e 08 1.75e 08 1.97e 08 4.03e 08 2.70e 08 1.93e 08 1.81e 08 1.94e 08 2.21e 08 3.92e 08 8.27e 07 8.68e 07 9.42e 07 1.15e 08 1.52e 08 2.88e 08 dOS dWS dIS sOS sWS sIS csOS AlexNet Layer 1 (Convolution) 4x18 6x12 8x9 9x8 12x6 18x4 FlexiSAGA Systolic Array Size 4.01e 08 3.61e 08 3.41e 08 3.35e 08 3.21e 08 3.08e 08 3.59e 08 3.45e 08 3.45e 08 3.46e 08 3.51e 08 3.69e 08 4.58e 08 3.90e 08 3.68e 08 3.52e 08 3.36e 08 3.14e 08 2.28e 08 2.18e 08 2.28e 08 2.36e 08 2.63e 08 3.16e 08 2.11e 08 1.90e 08 1.91e 08 1.91e 08 2.16e 08 2.57e 08 1.59e 08 1.15e 08 9.03e 07 8.30e 07 6.59e 07 4.85e 07 2.61e 08 2.16e 08 1.87e 08 1.79e 08 1.85e 08 2.67e 08 7.49e 07 9.65e 07 1.20e 08 1.29e 08 1.59e 08 2.20e 08 2.45e 08 2.51e 08 3.70e 08 2.90e 08 2.89e 08 3.02e 08 2.28e 08 1.87e 08 1.69e 08 1.81e 08 2.08e 08 2.56e 08 1.97e 08 1.36e 08 1.21e 08 1.02e 08 8.80e 07 8.02e 07 2.37e 08 2.17e 08 2.36e 08 2.06e 08 1.97e 08 2.35e 08 8.04e 07 9.47e 07 7.73e 07 9.53e 07 9.89e 07 1.62e 08 6.69e 07 8.17e 07 6.80e 07 7.63e 07 8.79e 07 1.37e 08 2.80e 07 3.05e 07 2.63e 07 2.89e 07 2.48e 07 2.65e 07 AlexNet Layer 5 (Fully-connected) Local Optimum Global Optimum Fig. 11.\n\n--- Segment 31 ---\nThe unbalanced row to column ratio of the best SA instance can be FlexiSAGA 15 4x18 6x12 8x9 9x8 12x6 18x4 FlexiSAGA Systolic Array Size n 1 Row Col. n 1 Row Col. n 1 Row Col. n 1 Row Col. Dataflow Pruning Parameter 4.36e 08 3.09e 08 2.46e 08 2.38e 08 3.01e 08 4.30e 08 4.45e 08 3.39e 08 2.96e 08 2.85e 08 2.71e 08 3.05e 08 4.85e 08 3.73e 08 3.24e 08 3.11e 08 3.65e 08 4.84e 08 1.55e 08 1.43e 08 2.16e 08 2.51e 08 3.66e 08 6.13e 08 2.08e 08 1.58e 08 2.12e 08 2.33e 08 3.32e 08 5.18e 08 8.49e 07 7.67e 07 9.96e 07 1.16e 08 1.53e 08 2.36e 08 2.97e 08 2.64e 08 8.53e 08 8.20e 08 7.38e 08 6.46e 08 3.84e 08 3.27e 08 2.89e 08 2.74e 08 2.77e 08 2.68e 08 2.70e 08 2.26e 08 2.12e 08 2.20e 08 2.14e 08 2.31e 08 2.52e 08 1.89e 08 1.81e 08 1.97e 08 2.86e 08 4.99e 08 1.86e 08 1.43e 08 1.70e 08 1.25e 08 1.31e 08 2.52e 08 2.05e 08 1.64e 08 1.92e 08 2.25e 08 3.29e 08 5.21e 08 1.97e 08 1.81e 08 1.70e 08 1.75e 08 1.97e 08 4.03e 08 2.70e 08 1.93e 08 1.81e 08 1.94e 08 2.21e 08 3.92e 08 8.27e 07 8.68e 07 9.42e 07 1.15e 08 1.52e 08 2.88e 08 dOS dWS dIS sOS sWS sIS csOS AlexNet Layer 1 (Convolution) 4x18 6x12 8x9 9x8 12x6 18x4 FlexiSAGA Systolic Array Size 4.01e 08 3.61e 08 3.41e 08 3.35e 08 3.21e 08 3.08e 08 3.59e 08 3.45e 08 3.45e 08 3.46e 08 3.51e 08 3.69e 08 4.58e 08 3.90e 08 3.68e 08 3.52e 08 3.36e 08 3.14e 08 2.28e 08 2.18e 08 2.28e 08 2.36e 08 2.63e 08 3.16e 08 2.11e 08 1.90e 08 1.91e 08 1.91e 08 2.16e 08 2.57e 08 1.59e 08 1.15e 08 9.03e 07 8.30e 07 6.59e 07 4.85e 07 2.61e 08 2.16e 08 1.87e 08 1.79e 08 1.85e 08 2.67e 08 7.49e 07 9.65e 07 1.20e 08 1.29e 08 1.59e 08 2.20e 08 2.45e 08 2.51e 08 3.70e 08 2.90e 08 2.89e 08 3.02e 08 2.28e 08 1.87e 08 1.69e 08 1.81e 08 2.08e 08 2.56e 08 1.97e 08 1.36e 08 1.21e 08 1.02e 08 8.80e 07 8.02e 07 2.37e 08 2.17e 08 2.36e 08 2.06e 08 1.97e 08 2.35e 08 8.04e 07 9.47e 07 7.73e 07 9.53e 07 9.89e 07 1.62e 08 6.69e 07 8.17e 07 6.80e 07 7.63e 07 8.79e 07 1.37e 08 2.80e 07 3.05e 07 2.63e 07 2.89e 07 2.48e 07 2.65e 07 AlexNet Layer 5 (Fully-connected) Local Optimum Global Optimum Fig. 11. Design space exploration comparing the runtime in clock cycles of two AlexNet operators mapped onto a FlexiSAGA architecture of different shapes comprised of 72 PEs for all dataflows and different pruning parameters.\n\n--- Segment 32 ---\n11. Design space exploration comparing the runtime in clock cycles of two AlexNet operators mapped onto a FlexiSAGA architecture of different shapes comprised of 72 PEs for all dataflows and different pruning parameters. explained by the linear scaling of the memory interface. The more PEs at the border are connected to the main memory, the more data can be streamed into the SA. These results show that finding the optimal combination of architecture configuration, pruning parameters, and dataflow is not trivial and motivates the use of a DNN HW co-design flow and an extensive performance evaluation. Generating the 1440 measurements using the VP took 421.4 h, with a mean runtime per measurement of 17.6 min and a standard deviation of 13.2 min. 7 Conclusion This paper presented FlexiSAGA a flexible systolic array GEMM accelerator for sparse and dense processing. Our results show a better sparse-over-dense whole DNN inference speedup compared to other architectures, reaching a speedup of up to 4.28. This is enabled by a near-optimal processing of DNN CONV and FC operators using seven different dense and sparse dataflows and a specifically tailored pruning technique, which constitutes a DNN HW co-design flow. FlexiSAGA is broadly applicable to DNNs comprised of mostly CONV and FC operators. Going forward, we plan to extend the range of supported operators to multi-head attention, which can be found in transformer networks such as large language and generative AI models. Acknowledgments. This work has been funded by the German Federal Ministry of Research, Technology and Space (BMFTR) under grant numbers 16ME0129 (Scale4Edge) and 01IS22086H (MANNHEIM-FlexKI). References 1. Amaranth HDL. (Mar 2025) 16 M. M. Müller and K. Lübeck et al. 2. Bernardo P.P. et al. : UltraTrail: A Configurable Ultralow-Power TC-ResNet AI Ac- celerator for Efficient Keyword Spotting. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 39(11), 4240 4251 (2020) 3. Chellapilla, K., Puri, S., Simard, P.: High Performance Convolutional Neural Net- works for Document Processing.\n\n--- Segment 33 ---\nIEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 39(11), 4240 4251 (2020) 3. Chellapilla, K., Puri, S., Simard, P.: High Performance Convolutional Neural Net- works for Document Processing. In: Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft, La Baule, France (Oct 2006) 4. Chen, Y.H. et al. : Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks. IEEE Journal of Solid-State Circuits 52(1), 127 138 (Jan 2017) 5. Cheng, H., Zhang, M., Shi, J.Q. : A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations. IEEE Transactions on Pattern Analysis and Machine Intelligence 46(12), 10558 10578 (2024) 6. Ganguly, A., Muralidhar, R., Singh, V.: Towards Energy Efficient non-von Neu- mann Architectures for Deep Learning. In: 20th International Symposium on Qual- ity Electronic Design (ISQED). pp. 335 342 (2019) 7. Genc, H. et al. : Gemmini: Enabling Systematic Deep-Learning Architecture Eval- uation via Full-Stack Integration. In: Proceedings of the 58th Annual Design Au- tomation Conference (DAC) (2021) 8. Gonidmalla A. et al. : SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks. In: Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture. p. 151 165. MICRO 52, New York, NY, USA (2019) 9. He, K. et al. : Deep Residual Learning for Image Recognition (2015) 10. Iofinova, E. et al. : How Well Do Sparse ImageNet Models Transfer? In: 2022 IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE (Jun 2022) 11. Krizhevsky, A., Sutskever, I., Hinton, G.E. : ImageNet Classification with Deep Convolutional Neural Networks.\n\n--- Segment 34 ---\nKrizhevsky, A., Sutskever, I., Hinton, G.E. : ImageNet Classification with Deep Convolutional Neural Networks. Communications of the ACM 60(6), 84 90 (May 2017) 12. Kurtic, E. et al. : Sparse Fine-tuning for Inference Acceleration of Large Language Models (2023) 13. Mishra, A. et al. : Accelerating Sparse Deep Neural Networks (2021) 14. Parashar, A. et al. : SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks. In: Proceedings of the 44th Annual International Symposium on Computer Architecture. p. 27 40. ISCA 17, Association for Computing Machinery, New York, NY, USA (2017) 15. Paszke, A. et al. : PyTorch: An Imperative Style, High-Performance Deep Learning Library. In: Advances in Neural Information Processing Systems. vol. 32. Curran Associates, Inc. (2019) 16. Simonyan, K., Zisserman, A.: Very Deep Convolutional Networks for Large-Scale Image Recognition (2015) 17. Soltaniyeh, M., Martin, R.P., Nagarakatte, S.: An Accelerator for Sparse Convolu- tional Neural Networks Leveraging Systolic General Matrix-matrix Multiplication. ACM Transactions on Architecture and Code Optimization 19(3), 1 26 (May 2022) 18. Szegedy, C. et al. : Going Deeper with Convolutions. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). p. 1 9 (Jun 2015) 19. Wen, W. et al. : Learning Structured Sparsity in Deep Neural Networks. In: Ad- vances in Neural Information Processing Systems. vol. 29. Curran Associates, Inc. (2016)\n\n