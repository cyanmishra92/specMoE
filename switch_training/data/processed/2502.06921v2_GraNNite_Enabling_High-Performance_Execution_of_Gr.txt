=== ORIGINAL PDF: 2502.06921v2_GraNNite_Enabling_High-Performance_Execution_of_Gr.pdf ===\n\nRaw text length: 55638 characters\nCleaned text length: 54981 characters\nNumber of segments: 30\n\n=== CLEANED TEXT ===\n\nGraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty and Vijay Raghunathan Abstract Graph Neural Networks (GNNs) are crucial for learning and reasoning over graph-structured data, with appli- cations in network analysis, recommendation systems, and speech analytics. Deploying them on edge devices, such as client PCs and laptops, enables real-time processing, enhances privacy, and reduces cloud dependency. For instance, GNNs can augment Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and enable event-based vision tasks. However, irregular memory access, sparse graphs, and dynamic struc- tures lead to high latency and energy consumption on resource- constrained devices. Modern edge processors combine CPUs, GPUs, and NPUs, where NPUs excel at data-parallel tasks but face challenges with irregular GNN computations. To address these gaps, we present GraNNite, the first hardware-aware framework tailored to optimize GNN deployment on commercial-off-the- shelf (COTS) state-of-the-art (SOTA) DNN accelerators using a systematic three-step methodology: (1) enabling GNN execution on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance and energy efficiency gains. Towards that end, the first category includes techniques such as GraphSplit for workload distribution and StaGr for static graph aggregation, while GrAd and NodePad handle real-time updates for dynamic graphs. Next, performance improvement is acquired through techniques such as EffOp for control-heavy operations and GraSp for sparsity exploitation. For Graph Convolution layers, PreG, SymG, and CacheG reduce redundancy and memory transfers. The final class of techniques deals with quality vs efficiency tradeoffs QuantGr applies INT8 quantization to lower memory usage and computation time, while GrAx1, GrAx2, and GrAx3 optimize graph attention, broadcast-add, and sample-and-aggregate (SAGE)-max aggregation for higher throughput with minimal quality loss. Experimental evaluations on Intel Core Ultra Series 1 and 2 AI PCs demonstrate that GraNNite achieves speedups of 2.6 to 7.6 over default NPU mappings, with energy efficiency improvements up to 8.6 compared to CPUs and GPUs. Across various GNN models, GraNNite delivers up to 10.8 and 6.7 higher performance than CPUs and GPUs, respectively. Our code implementation is available at this link. I. INTRODUCTION Graph Neural Networks (GNNs) have become essential for learning and reasoning over graph-structured data, with appli- This work was supported in part by the Center for the Co-Design of Cog- nitive Systems (CoCoSYS) and the Center on Cognitive Multispectral Sensors (CogniSense), two research centers under the Joint University Microelectronics Program (JUMP) 2.0, a Semiconductor Research Corporation (SRC) initiative sponsored by DARPA. Arghadip Das (corresponding author, e-mail: and Vijay Raghunathan are with the Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA. Shamik Kundu (e-mail: Arnab Raha (e-mail: Soumendu Ghosh (e-mail: and Deepak Mathaikutty (e-mail: are with the Advanced Architecture Research Team, NPU IP, CGAI (CCG), Intel Corporation, Santa Clara, CA, USA. Personal Assistant Question-answering Enhanced search Recommendations Event-driven vision tasks Theft detection Automatic lock Proximity alarm Speech- to-text AI- backend Model library Storage LLMs GNNs RAG Knowledge Graph (KG) Event Stream Subsampling Graph Generation GNNs Prediction GNN tasks run frequently in the background Map on NPU for faster response with lower power Link prediction KG alignment Node classification KG reasoning Image classification Object detection Image segmentation Pose estimation Face recognition Fig. 1. Applications of GNNs on Client PCs: showcasing GNN-driven tasks like recommendations and event-driven vision, mapped onto Intel Core Ultra processors for faster response and lower power. cations in areas like network analysis, recommendation sys- tems [1], and speech analytics [2]. Their ability to capture com- plex relationships through graph topology distinguishes them from traditional neural networks such as CNNs and LLMs. Recently, the inclusion of R-GAT, a prominent GNN model, in MLPerf s inference benchmarks emphasizes their growing importance in real-world applications. GNNs are particularly important compared to LLMs and newer architectures like State Space Models (SSMs) due to their ability to explicitly model relational and structural information, which is critical for tasks involving interconnected data such as social networks, molecular structures, and knowledge graphs [3]. While LLMs excel in sequential data processing and SSMs offer efficiency in modeling long-range dependencies [4], GNNs uniquely capture complex relationships through graph topology, making them indispensable for tasks where data is inherently non-Euclidean. Running GNNs on edge devices, including laptops and client PCs, has significant advantages. Edge-based inference ensures real-time processing, enhances data privacy, and reduces depen- dency on cloud infrastructure. For instance, GNNs can enhance Retrieval-Augmented Generation (RAG) for LLMs [5], [6], enabling efficient personal assistant applications. In addition, they are integral to event-based vision tasks [7], [8], allowing rapid processing of irregular data streams (as shown in Fig. 1). The rising popularity of sensors in mobile devices further drives the deployment of GNNs to the wireless network edge for tasks such as sensing and interaction, including collision prediction in self-driving vehicles [9] and speech analytics [2]. These benefits make edge deployment crucial for achieving low-latency and energy-efficient solutions while addressing the growing demand for intelligent local inference. Despite their potential, deploying GNNs on resource-constrained edge devices presents challenges. Irregular memory access patterns, arXiv:2502.06921v2 [cs.LG] 13 Feb 2025 dynamic graph structures, and limited parallelism hinder com- putational efficiency. Sparse graphs exacerbate memory latency and lead to underutilized resources [1]. For example, deploying the DGCNN model on a Raspberry Pi 3B achieves less than 0.3 frames per second (fps), far below practical requirements [10]. Additionally, edge devices often rely on slower DRAM due to limited SRAM, resulting in high inference latency, increased energy consumption, and reduced battery life. These limitations highlight the need for optimized techniques to efficiently map GNN workloads onto edge platforms. Modern edge processors, such as AI PCs from Intel, Qual- comm, and AMD, integrate heterogeneous computing units, including CPUs, GPUs, and NPUs, to efficiently support di- verse AI workloads. Among these, NPUs or Neural Processing Units are specialized processors optimized for data-parallel op- erations, particularly matrix multiplication, which is the foun- dation of most neural network computations. NPUs typically include Data Processing Units (DPUs) [11] for parallelized matrix operations and Digital Signal Processors (DSPs) for sequential tasks like non-linear activation functions. NPUs are well-suited for AI workloads due to their high throughput and energy efficiency, outperforming traditional CPUs and GPUs. These advantages make NPUs ideal for continuous and resource-intensive GNN workloads on edge devices. However, the aforementioned challenges hinder their efficient utilization for GNN processing. Although prior research has proposed methods to optimize GNN processing [12], these efforts re- main insufficient for real-time edge deployments [13], [14]. To address these gaps, we introduce GraNNite, a framework specifically designed to optimize the deployment of GNNs on NPUs, enhancing performance and efficiency. GraNNite leverages hardware-aware techniques to mitigate the challenges, ensuring scalable and efficient GNN execution on edge plat- forms. Modern GNNs primarily rely on three foundational layer types: Graph Convolution (GraphConv), Graph Atten- tion (GraphAttn), and Sample and Aggregate (SAGE), which form the basis of architectures such as Graph Convolution Network (GCN) [15], Graph Attention Network (GAT) [16], and GraphSAGE [17] (Fig. 2). These layers were selected for our study as they address distinct challenges: GCNs capture local structure through neighbor averaging, GATs improve representation quality by assigning importance weights via attention mechanisms, and GraphSAGE enhances scalability by sampling neighbors for efficient large-graph processing. GraNNite optimizes these layers to achieve efficient execution by introducing a systematic 3-step methodology: (1) enabling GNNs on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance gains. While we evaluate GraNNite on GNNs using NPUs, the methodology is generic and can be extended to other models and hardware platforms without loss of generality. Our key contributions are: Step 1: Enabling GNNs on NPUs. GraNNite introduces GraphSplit to optimize sequential and irregular compute tasks by assigning graph preprocessing to the CPU and parallelizable tasks to the NPU using an offline cost model, minimizing communication overhead. For static graphs, StaGr transforms node aggregation into matrix multipli- Graph Neural Networks (GNNs) Graph Convolution Network (GCN) Convolutional aggregation of neighbor features based on node links Simple and fundamental Graph Attention Network (GAT) Attention mechanism assigns weights to neighbors' importance Better quality than GCN Compute Quality GraphSAGE (SAmple and aggregate) Samples and aggregates neighbors for scalability Used for large graphs Compute Quality Aggregation Combination ‚Ñéùëñ ùëô ùúé ùëó {ùëñ} ùí©(ùëñ) 1 ùëëùëñùëëùëó ‚Ñéùëó ùëô 1ùëäùëô ùëíùëñùëó ùêøùëíùëéùëòùë¶ùëÖùëíùêøùëà(ùëéùëá [‚Ñéùëñ ùëô 1ùëäùëô·âõ‚Ñéùëó ùëô 1ùëäùëô] ‚Ñéùëñ ùëô ùúé œÉùëó {ùëñ} ùí©(ùëñ) exp(ùëíùëñùëó)‚Ñéùëó ùëô 1ùëäùëô œÉùëó {ùëñ} ùí©(ùëñ) exp(ùëíùëñùëó) ‚Ñéùëñ ùëô ùúéùëéùëò‚Ñéùëó ùëô 1ùëäùëô, ùëó {ùëñ} ùëÜùí©(ùëñ) Fig. 2. Three fundamental GNNs: GCN, GAT, and GraphSAGE, emphasizing their unique approaches convolutional aggregation, attention-based weighting, and neighbor sampling for scalability. cation using a precomputed mask, while for dynamic graphs, GrAd and NodePad enable real-time updates with preconfigured node capacities. Step 2: Optimizing GNN performance. GraNNite en- hances efficiency with EffOp, which substitutes control- heavy DSP operations (e.g., select, gather) with equiva- lent data-parallel operations for DPU execution, reducing latency and improving energy efficiency. Additionally, GraSp exploits sparsity bitmaps to skip zero values, re- ducing memory usage and improving energy efficiency. For GNNs with GraphConv layers, PreG, SymG, and CacheG reduce redundancy: PreG offloads normalization factor computation to the CPU, SymG stores only half the normalization matrix, and CacheG reuses precomputed matrices to minimize memory transfers. Step 3: Trading accuracy for performance gains. GraNNite introduces QuantGr, which shifts computations from FP16 to INT8, achieving performance gains with reduced memory and computation time for negligible quality loss. For further throughput improvements, three approximation techniques are employed: GrAx1 simplifies attention score computation, GrAx2 optimizes broadcast- add operations, and GrAx3 accelerates SAGE-max aggre- gation using parallelized DPU operations. Experiments on Intel AI PCs show that GraNNite achieves speedups of 2.6 7.6 over out-of-the-box NPU mappings, with energy efficiency up to 8.6 higher than CPUs and GPUs. Across GNN models, it outperforms CPUs by 3.3 10.8 and GPUs by 2.3 6.7 . Intel Core Ultra Series 2 NPUs deliver up to 1.7 higher throughput than Intel Core Ultra Series 1 NPUs. The paper is organized as follows: Section II reviews prior work on GNN optimization for specialized hardware. Sec- tion III covers GNN execution and computational challenges. Section IV describes GraNNite methodology for optimizing GNNs on NPUs. Section V explains the experimental setup, including datasets, models, and hardware. Section VI presents performance and energy efficiency results. Finally, Section VII concludes the paper and outlines future directions. II. RELATED WORK GNNs excel at structural tasks due to their ability to extract features from graph topology [1], yet they require substantial computational power [18]. As detailed in Section I, deploying Aggregation 1 Combination 1 Input graph Dataloader Node embeddings Edge indices 0 1 2 0 1 2 3 Feature dim. d Number of nodes N 4 (1,2)(1,3)(1,4)(3,4) Generate adjacency mat. (A) Add self loops for aggregation (Adj) ùëõùëúùëüùëöùëñùëó ‡µû 1 ùëëùëñùëëùëó , ùëñùëì ùëéùëëùëóùëñùëó 1 0, ùëñùëì ùëéùëëùëóùëñùëó 0 Node Degree 1 4 2 2 3 3 4 3 0 1 2 3 0 1 1 1 0 1 0 0 0 1 1 0 0 1 2 1 0 1 0 3 0 1 2 3 1 1 1 1 0 1 1 0 0 1 1 0 1 1 2 1 0 1 1 3 Node degree Normalization factor formula Aggregation 0 Combination 0 N GCN Execution Flow Fig. 3. Execution flow of a GCN: graph preprocessing followed by iterative aggregation and combination phases for GNN computation [23]. Fig. 4. Execution Latency Breakdown of GraphConv and GraphAttn Layers (1433 input features and 64 output features) on Intel Core Ultra Se- ries 2 NPU across graph preprocessing (DPU DSP) and GNN computation (DPU DSP) for a graph with 1354 nodes and 5429 edges. GNNs in resource-constrained edge environments presents seri- ous difficulties. To tackle this, strategies to optimize GNNs for edge devices include simplifying model architectures [12] and employing hardware-aware neural architecture search (NAS) techniques like HGNAS [14] among others [13]. Nonetheless, these approaches still fall short; for instance, HGNAS boosts point cloud processing speed to merely 2 fps on a Raspberry Pi [14]. On the other hand, previous optimization approaches for DNN accelerators focused on techniques such as model fine- tuning, memory optimization, and standard quantization [19] [21]. Although they improved efficiency, they often required extensive retraining or hardware-specific code modifications, limiting portability. Furthermore, existing GNN mapping meth- ods do not fully leverage NPU-specific features like efficient sparsity handling, static data shapes, and optimized memory access, leading to suboptimal performance [22]. These meth- ods also struggle with the irregular computation patterns and memory intensity of GNNs, limiting their deployment on real- time edge devices. GraNNite addresses these challenges by introducing NPU-tailored optimizations that enable efficient, high-performance GNN execution on resource-constrained ac- celerators for real-time deployment. III. BACKGROUND MOTIVATION Understanding the execution of GNNs involves analyzing their core computational stages: Node Embedding, Aggregation, Combination, and Decode [23]. Fig. 3 demonstrates this process using a GCN [15] as an example. The process begins with loading the graph structure and node embeddings via a data loader. Graph edges are typically represented as tuples of connected node indices. To enhance computational efficiency, the graph can be preprocessed into a structured format, such as an adjacency matrix. This binary matrix indicates edge connections and includes self-loops to incorporate node-specific features. Additionally, a normalization matrix is derived from node degrees to ensure a balanced computation. During the Fig. 5. Execution latency breakdown of GNN computation of a single GraphConv and GraphAttn layer (1433 input features and 64 output features) on Intel Core Ultra Series 2 NPU across operations [24] for a graph with 1354 nodes and 5429 edges. Node Embedding stage, raw graph data is converted into feature vectors that serve as inputs to subsequent stages. The Aggre- gation phase then collects features from neighboring nodes, leveraging operations such as pooling or reduction to capture relationships within the graph structure. However, this phase often incurs irregular memory access due to the variable num- ber of neighbors. Next, the Combination phase applies neural transformations, such as fully connected layers or attention mechanisms, to the aggregated features, producing higher-level representations. Finally, in the Decode phase, these refined features are processed through layers like MLPs or SoftMax to generate predictions. The Aggregation and Combination phases (main GNN compute) are the most computationally intensive, as they are performed repeatedly throughout the model, emphasizing their critical role in GNN execution. This iterative nature underscores the need for efficient preprocessing and computational strategies to optimize performance. Fig. 4 presents the latency breakdown for a single GraphConv and GraphAttn layer mapped out-of-the-box on the Intel Core Ultra Series 2 NPU. The breakdown highlights two major components: graph preprocessing and GNN compute (illustrated in Fig. 3), which includes operations such as combination and aggregation. Additionally, the figure provides a detailed view of how these components are distributed across the NPU s DPU and DSP units. It is evident from this breakdown that preprocessing plays a dominant role, contributing approximately 55 of the execution time in GraphAttn and nearly 99 in GraphConv. The preprocessing tasks, being control-flow heavy, are primarily executed on the DSP (relatively slower than DPU), further exacerbating the latency issue. Addressing this control-flow challenge is critical for improving GNN performance. In particular, GraphSplit, which is introduced in Section IV, is designed to mitigate this issue, optimizing preprocessing and enhancing overall execution efficiency. Fig. 5 further highlights the breakdown of GNN compute operations across different units of the NPU, with GraphConv benefiting from efficient matrix multiplication (MatMul) on the DPU. While this operation suits NPUs well due to their strength in data-parallel tasks, GraphAttn still presents opportunities for improvement. In particular, around 30 of the GNN compute execution time in GraphAttn is spent on operations such as Select, Greater, Softmax, and Elu, which are control-heavy and executed on the DSP. These control-flow- intensive sections are prime targets for optimization, which GraNNite addresses through EffOp, as discussed in Section IV. Additionally, GNNs benefit from sparse input graphs and do not require full precision (FP32) for compute. This opens up further Operator Latency Gather Select MatMul Pre-trained Graph Neural Network (GNN) model GNN partitioning Offline Profile Table Control-flow dominant graph preprocessing at CPU Data-parallel GNN computation on NPU Processed node embeddings and connectivity information Apply model optimizations Convert to intermediate representation (IR) model.xml model.bin Intermediate representation (IR) NPU compiler Hardware Optimizations GraSp QuantGr model.blob Compiled binary Main memory (DRAM) Neural Processing Unit (NPU) Data Processing Unit (DPU) Digital Signal Processor (DSP) Local SRAM memory Direct Memory Access (DMA) Other logic Software Optimizations EffOp GrAx1, GrAx2, GrAx3 PreG, SymG, CacheG GraphSplit StaGr, GrAd, NodePad Fig. 6. End-to-end GraNNite methodology to efficiently enable GNNs on NPUs through model partitioning and optimizations. opportunities for optimization, where approximate methods can be deployed to reduce computation at the cost of minimal quality loss. GraNNite leverages these characteristics to enable high-speed GNN execution on NPUs, pushing the boundaries of real-time performance in edge environments. IV. GRANNITE DESIGN METHODOLOGY GraNNite provides an end-to-end framework (as shown in Fig. 6) for deploying pre-trained GNNs on NPUs without retraining. We consider an output-stationary NPU architecture inspired by Ref. [11]. The core component is the DPU, an M M grid of Versatile Processing Elements, each comprising an N N array of MAC Processing Elements (MPEs) designed for efficient Multiply-and-Accumulate (MAC) operations. This DPU is well-suited for operations like matrix multiplication, which are fundamental to many neural network computations. The architecture includes a local SRAM for storing activations and weights, a tensor distribution network for data flow to and from the DPU, and control logic for managing computation, accumulation, and output extraction. MAC operations, integral to DNNs, calculate dot products of weights and activations to produce output feature maps. Each MPE leverages a local data path with register files, multipliers, and accumulators to perform these tasks. Additionally, a DSP handles non-linear activation functions and control-flow operations, complement- ing the data-parallel DPU. Although our case study considers an output-stationary NPU architecture, the proposed techniques are generic and can be applied to other NPUs without loss of generality. GraNNite proposes a generic step-by-step methodol- ogy (Fig. 7) to optimize emerging neural networks on existing AI accelerators. While demonstrated on GNNs using FlexNN- like [11] NPUs, the methodology is generalizable to other models and hardware platforms. It consists of three key steps: CPU-NPU Partitioned GNN inference GraphSplit Efficient GNN inference for static input graphs StaGr Enabling GNNs with dynamic input graphs on NPUs GrAd NodePad Optimization of control-flow heavy model sections EffOp Performance enhancement through activation sparsity GraSp Accelerating graph convolution PreG SymG CacheG Enhancing compute efficiency through quantization QuantGr Approximate GNN computation GrAx1 GrAx2 GrAx3 GraNNite GraNNite techniques Expected Vector of Improvement GraphSplit Improved workload distribution and parallelism. StaGr Reduced latency for static graph partitioning through precomputation. GrAd NodePad Efficient handling of dynamic graphs. EffOp Faster execution and reduced memory bandwidth usage. GraSp Reduced memory usage and computation by exploiting sparsity. PreG SymG CacheG Reduced redundancy and latency with optimized memory management. QuantGr Accelerated computation through reduced precision and memory access. GrAx1 GrAx2 GrAx3 Improved throughput with approximation techniques. Fig. 7. Suite of GraNNite Optimization Techniques for Efficient GNN Inference on NPUs. (1) Enabling the Model on the NPU. This step ensures the model runs efficiently on the NPU while maintaining flexibility. For GNNs, GraNNite introduces workload partitioning (Graph- Split), precomputed static graph processing (StaGr), and dy- namic graph handling (GrAd and NodePad) to support real-time updates and adaptive memory management. These techniques enable execution with minimal overhead. (2) Optimizing GNN Performance. Once enabled, the model undergoes further op- timizations to maximize efficiency without degrading accuracy. EffOp accelerates execution and reduces memory bandwidth usage, while PreG, SymG, and CacheG optimize memory access for Graph Convolution layers. GraSp exploits sparsity to lower memory and compute costs, improving throughput and energy efficiency. (3) Trading Accuracy for Performance and Energy Gains. For applications prioritizing speed and efficiency over quality, GraNNite offers QuantGr for INT8 quantization and approximation techniques (GrAx1, GrAx2, GrAx3) to further enhance throughput with minimal quality loss. These steps provide a systematic framework for deploying GNNs efficiently on NPUs, addressing resource constraints while ensuring scalability, performance, and energy efficiency. A. Step-1: Enabling GNNs on the NPU GraphSplit: To enable efficient execution of GNNs on NPUs, the first challenge is to address the mismatch between the hardware s strengths and the computational demands of graph- based workloads. NPUs excel at data-parallel tasks like matrix multiplications in neural networks, but are less efficient for control-heavy tasks involving frequent decision-making. CPUs, on the other hand, excel at these control-intensive tasks, using techniques such as predictive execution and out-of-order pro- cessing to maximize instruction-level parallelism. Given these contrasting strengths, one might assume it s best to offload all control-heavy tasks during GNN inference, such as computing initial masks (i.e., preprocessing in Fig. 4) for aggregation or calculating intermediate attention scores, to the CPU. However, a challenge arises when control-flow tasks exhibit a Read- after-Write (RAW) dependency on previous data-parallel tasks, necessitating the transfer of data back to the CPU. This results Fig. 8. GraphSplit, partitioned GNN inference using CPU and NPU: CPU handles graph preprocessing; NPU accelerates data-parallel GNN computation. in considerable communication overhead. To overcome this, GraNNite introduces an offline profiling phase during model calibration. In this phase, we build a cost model that measures real-time latencies of various operations on both the CPU and NPU. This cost model also factors in the overhead from data transfer and communication between the CPU and NPU. Using this information, GraphSplit identifies the most effec- tive partition points to minimize communication and latency. GraphSplit s partitioning strategy is designed to play to the strengths of each processing unit. Control-flow tasks, which require complex decision-making, are assigned to the CPU. Computationally heavy, data-parallel tasks, such as matrix multiplications, are sent to the NPU. This careful distribution improves graph processing performance by reducing the need for frequent data exchanges. For example, offloading initial in- put preprocessing to the CPU requires minimal communication with the NPU, resulting in better performance. As shown in Fig. 8, this partitioned inference setup for models such as GCN, GAT, and GraphSAGE effectively balances workload between CPU and NPU. StaGr: For applications involving static graph structures, GraNNite proposes an efficient methodology (StaGr) for im- plementing GNNs on hardware accelerators. Using a precom- puted mask tailored to a fixed input graph, StaGr transforms the aggregation of node features in Graph Convolution into a streamlined matrix multiplication operation (refer to GCN in Fig. 9), fully utilizing the capabilities of the NPU. This precomputed mask establishes node connections beforehand, significantly reducing irregular memory accesses and improving memory latency and energy efficiency, all without requiring extensive hardware modifications. For Graph Attention and GraphSAGE, GraNNite leverages precomputed masks an at- tention mask for efficient attention score calculation and a sampled adjacency matrix for reuse during inference (see Fig. 9. StaGr: execution of GNNs on a static graph structure with dynamic node features. Fig. 10. One of the challenges to efficiently enable GNNs on NPUs: Dynamic input graph (An example of on-device knowledge graph). Fig. 9). This methodology achieves highly efficient inference, minimizing computational overhead and latency while optimiz- ing NPU performance under fixed-structure conditions. GrAd NodePad: To handle dynamic input graphs (refer Fig. 10), GraNNite proposes a new approach (GrAd) that uses a mask as input rather than a precomputed weight, allowing dynamic updates to edges without the need to recompile the model. Real-time graphs often undergo structural changes with nodes and edges dynamically added or removed (Fig. 10). However, NPUs typically support static input shapes, as DNN models are precompiled for fixed input shapes, with optimiza- tions such as tiling based on corresponding input configuration. This limitation requires recompilation when the input graph shape changes. Compiling the model for a static input shape and using mini-batches for inference may seem viable, but risks information loss by excluding edges connecting nodes outside the subgraph. Additionally, selecting an optimal batch size is challenging and may lead to underutilized NPU resources. Our approach introduces a node-padding technique (NodePad) that compiles the entire model with a higher node capacity than immediately needed for the whole input graph. For smaller graphs, embeddings for unused nodes are zero-padded, while absent edges are represented by zeroes in the adjacency matrix, following the conventional interpretation of 0 as no edge and 1 as an active connection. This node padding strategy min- imizes the need for frequent recompilation and eliminates the need to store multiple precompiled model versions for different graph sizes. Fig. 11 illustrates how a GNN with GraphConv Fig. 11. GrAd NodePad: Dynamic input graph support for GNNs via node padding: Eliminates multiple precompiled blobs, saving memory and removing the need for frequent recompilation with varying input node counts. layers can handle a time-varying input graph on an NPU. This approach applies zero padding to the input features and utilizes a norm matrix (mask), precomputed on the CPU, which is then fed into the main GNN computation on the NPU. By dynamically updating the mask at runtime, GrAd and NodePad allow the GNN to efficiently adapt to evolving graph structures. These techniques significantly improve performance and energy efficiency of GNN inference by reducing the overhead tied to model recompilation. B. Step-2: Optimizing GNN Performance on NPU EffOp: After enabling GNNs on the NPU, the next challenge lies in optimizing their performance without compromising application quality. A significant bottleneck arises from the control-heavy operations, such as conditional logic, Select, or Gather, residing deep in the GNNs being executed on the DSP within the NPU (as shown in Fig. 5). The DSP is designed for these operations, but runs at a lower frequency than the DPU. This difference often causes bottlenecks and increases latency in deep, sequential GNN sections. To address this limitation, GraNNite proposes a novel approach, EffOp, that converts these control-heavy operations into equivalent data- parallel tasks, allowing them to be executed on the faster DPU rather than the DSP. The core idea is to restructure sequential tasks, such as Select and Gather, to be processed as simple, elementwise reduction operations on the DPU. By redefining these tasks using operations like multiplication and addition, combined with precomputed masks, we transform inherently sequential processes into parallel-friendly ones. This allows the DPU to handle tasks that would traditionally rely on the slower DSP, reducing the need for sequential processing and, consequently, lowering overall execution time. As shown in Fig. 12, this method is particularly beneficial for operations in Graph Attention Networks, specifically in sections where intermediate attention scores are computed. EffOp demonstrates 0 1 2 3 0 1 2 3 Intermediate Attention Scores 0 1 2 3 0 1 2 3 Adjacency matrix 0 1 2 3 0 1 2 3 Connectivity mask Masked Intermediate Attention Scores 0 1 2 3 0 1 2 3 Mask 0 1 2 3 0 1 2 3 Element 0? Replace element from connectivity mask Neural Processing Unit (NPU) Data Processing Unit (DPU) Digital Signal Processor (DSP) Local SRAM memory Direct Memory Access (DMA) Other logic Softmax Element-wise Addition DSP DPU CPU Baseline GAT implementation NPU components Optimized GAT implementation DSP Ops DPU Ops 0 1 Legend Element-wise Multiplication Fig. 12. Effop, efficient GNN computation by substituting control-intensive DSP operations with equivalent DPU operations: Utilizing the DPU s higher frequency and increased parallel compute units reduces end-to-end latency. Fig. 13. GraSp, exploiting input graph sparsity for faster execution: zero elements in node embeddings and adjacency matrices are compressed (ZVC), and a sparsity bitmap is used to bypass computation. how this computation can be achieved using elementwise multiplication, followed by elementwise addition with a slightly modified connectivity mask. In EffOp, tasks that typically involve complex control logic are optimized to utilize the DPU s strengths, transforming them into matrix and elemen- twise operations that can be efficiently parallelized. GraSp: In the context of GNN optimization on NPUs, activation sparsity offers a powerful mechanism to significantly boost performance by skipping unnecessary calculations. Given that input graphs are often highly sparse, with up to 99 of values being zero, GraNNite leverages this sparsity to optimize both memory usage and computational efficiency. The adjacency matrix in real-world graphs typically exhibits this extreme sparsity, containing many zero-valued entries where no direct connection exists between nodes. By capitalizing on this inherent sparsity, NPUs [25], [26] can streamline computations by skipping zero values, reducing the workload without af- fecting the accuracy of model inference. To efficiently manage these sparse values, GraNNite proposes GraSp, which utilizes a storage format known as Zero Value Compression (ZVC) [27]. In this approach, only the non-zero values in the input graphs Fig. 14. PreG: GraphConv normalization factors rely only on the graph struc- ture, enabling precomputation and bypassing costly square-root and division operation on the NPU s slower DSP units. Fig. 15. SymG CacheG: Symmetric norm matrix in GraphConv layers is reused across all layers, allowing partial storage for memory savings and increased reuse. are stored explicitly, while the zero values are omitted, allowing the system to allocate memory and computational resources effectively. For GraSp implementation, sparsity bitmaps are used alongside compressed data to denote the locations of non- zero values within the matrix. This bitmap directs the NPU to focus solely on meaningful data while bypassing zero entries. Fig. 13 illustrates how sparsity bitmaps are integrated within the NPU s processing pipeline to skip zero-valued computations leading to latency speedup. PreG, SymG CacheG: GraNNite presents a streamlined approach tailored for GNNs that use GraphConv layers as core components. PreG leverages a precomputed, constant normalization matrix to accelerate processing. Since Graph- Conv is foundational and commonly used in more advanced GNN architectures, this enhancement offers broad applicability and efficiency gains. In GraphConv, the normalization factors required after neighborhood aggregation depend solely on the degrees of neighboring nodes, not on their specific features. Here, a node s degree represents the count of its neighboring nodes, including itself. By exploiting this feature independence, PreG precomputes these normalization factors once, storing them in a constant matrix (refer Fig. 14). By precomputing the normalization matrix on the CPU, the aggregation and normal- ization steps are combined into a single matrix multiplication. This approach leverages the NPU s strength in matrix multi- plication while avoiding the slower DSP unit, which typically handles division, thus streamlining execution and optimizing performance. In addition, GraNNite introduces a memory- efficient technique (SymG) that capitalizes on the symmetry of the normalization matrix (see Fig. 15) in GraphConv layers, allowing only half of the adjacency matrix and its diagonal ele- ments to be stored. This optimization effectively reduces mem- ory complexity, translating into substantial savings in memory usage. SymG also minimizes memory traffic, especially during Direct Memory Access (DMA) transfers from DRAM to NPU s local memory, which can be a bottleneck. Finally, GraNNite Fig. 16. GrAx1, GraphAttn approximation 1: Removing compute-intensive multiplications boosts performance while preserving accuracy. Fig. 17. GrAx2, GraphAttn approximation 2: Removing a transpose and a broadcast operation reduces execution latency with negligible quality loss. introduces CacheG that caches the constant normalization matrix within the NPU s local memory and reuses it across all GraphConv layers in the model, significantly reducing memory access overhead. This caching strategy not only boosts runtime efficiency, but also lowers inference latency, making the overall execution of GNNs on the NPU more streamlined and resource- efficient. Fig. 15 demonstrates the portion of the normalization matrix stored in memory and illustrates how it is cached within the NPU s local memory. C. Step-3: Trading Accuracy for Performance Energy Gains QuantGr: In the pursuit of optimizing GNN performance and energy efficiency, we first explore traditional methods of reducing model precision before introducing GraNNite s novel approach. QuantGr, a quantization technique for GNNs is integrated in GraNNite that reduces numerical precision to achieve significant performance gains while preserving model accuracy. NPUs, typically designed with low-precision ca- pabilities, support both INT8 and FP16 datapaths, allowing notable performance gains over traditional FP32 computation. 0 1 2 3 0 1 2 3 Sampled adjacency matrix Aggregated neighborhood features Element[j] 0? Feature[j] x[j] Unsqueeze adjacency matrix CPU Baseline SAGE-max implementation 0 1 2 0 1 2 3 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 3 0 1 2 0 1 2 3 Yes i j j Feature[j] No Reduce Max Elementwise Multiplication 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 3 0 1 2 0 1 2 3 Maxpool1D Number of neighbors 2 Input node features Optimized SAGE-max (GrAx3) DSP Ops DPU Ops 0 1 Legend Fig. 18. GrAx3, SAGE-max approximation: Replaces sequential DSP op- erations in SAGE-max aggregation with parallel element-wise multiplication and max pooling on the DPU, improving efficiency while maintaining correct feature aggregation for non-negative values. Specifically, INT8 precision provides a 2 boost in perfor- mance (TOPs) and a 4 improvement in performance per watt (TOPs Watt) compared to FP16. By carefully configuring the quantization parameters, such as setting an optimal zero point and scale, QuantGr can achieve competitive accuracy at lower precision. QuantGr uses symmetric, static quanti- zation, meaning both weights and activations are quantized around a zero point, with equal scaling factors for positive and negative values. Static quantization, which precomputes scaling and zero-point parameters during model calibration, enables consistent and faster inference, as these values remain fixed throughout execution. Symmetric quantization simplifies processing by ensuring consistent scaling and compatibility across all hardware layers, minimizing conversion overhead. Leveraging the NPU s support for static quantization of acti- vations and weights, this approach unlocks higher efficiency for GNNs, making it well-suited for performance-sensitive, resource-limited environments. GrAx1: Application of all previously discussed techniques in GraNNite can significantly improve GNN performance and energy efficiency on NPUs compared to the initial out-of-the- box implementation. However, further improvements can be achieved through approximate computing. This approach trades off minimal DNN accuracy for better computational efficiency, enabling faster processing and reduced resource usage [28], [29]. GNNs with Graph Attention (such as GAT) are well- regarded for their ability to generate attention maps that assign varying importance to nodes within a graph. However, these networks face significant computational challenges, particu- larly in managing non-existent edges. To prevent these edges from influencing the final attention values, they are typically masked by assigning them a large negative number before being processed through a SoftMax function. This masking step effectively ensures that attention coefficients for non-existent edges are rendered negligible during the aggregation phase. In GAT implementation with EffOp, an element-wise multiplica- GNN Graph Labelled Graph GCN GAT SAGE Intel AI PCs Hyperparameter Value Learning rate 0.01 Weight decay 5e-4 Batch size 64 Epochs 100 Training Recipe CoreTM Ultra Series 2 CoreTM Ultra Series 1 Fig. 19. Experimental setup for GNN evaluation on Intel AI PCs. tion is performed between the attention map and the mask to eliminate the influence of non-existent edges. However, this multiplication is computationally intensive and not well-suited for the DPU. To mitigate this inefficiency, GraNNite proposes a novel approximation technique (GrAx1). Instead of multiplying the attention map by the mask, it suggests directly adding a large negative value to the positions in the attention map that correspond to non-existent edges. This modification effectively bypasses the multiplication step (as shown in Fig. 16), leading to a substantial reduction in computational burden on the DPU. As a result, throughput is increased without sacrificing the final attention map quality. GrAx2: Another significant bottleneck in GAT arises during the broadcast-add operation (refer Fig. 5), which is essential for calculating the intermediate attention map. The traditional im- plementation of the broadcast-add operation requires adding the same value to multiple nodes, a process that involves broad- casting and transposing the data (refer left of Fig. 17). This step can lead to inefficiencies when executed on the DPU, hindering overall performance. To address this inefficiency, GraNNite proposes another novel approximate solution (GrAx2) that replaces the conventional broadcast-add operation with just an addition followed by broadcasting. As shown in Fig. 17, this approach eliminates one transpose and one broadcast operation, significantly reducing the computational load for addition and minimizing memory copy reference operations, which enables the DPU to execute it more efficiently. GrAx3: For GNNs using the Sample and Aggregate (SAGE) layers with a max aggregation strategy, the feature selection for each neighborhood is traditionally processed se- quentially on the DSP, leading to inefficiency. GrAx3 replaces this sequential operation with parallel element-wise multiplica- tion using a mask (sampled adjacency matrix), followed by max pooling on the DPU. As shown in Fig. 18, GrAx3 simplifies the aggregation process, ensuring the correct aggregation of neighborhood features for most cases where feature values are greater than 0. V. EXPERIMENTAL METHODOLOGY As illustrated in Fig. 19, we evaluated GNNs on Intel NPUs using the Cora dataset (2,708 nodes, 5,429 edges, 7 classes, 1,433 features) and Citeseer dataset (3,327 nodes, 4,732 edges, 6 classes, 3,703 features) for node classification. The bench- Fig. 20. Progressive performance improvement of GNN through different GraNNite optimizations. marked models included Graph Convolutional Networks, Graph Attention Networks, and GraphSAGE, achieving baseline Top- 1 classification accuracies of 80.80 (GCN), 81.30 (GAT), 79.30 (SAGE-max), and 75.50 (SAGE-mean). We trained these models using PyTorch and PyTorch Geometric (PyG) with a learning rate of 0.01, weight decay of 5 10 4, batch size of 64 for 100 epochs. After training, the models were converted to an OpenVINO [30] compatible format for execution on the NPU. Experiments were conducted on two systems: Intel Core Ultra Series 2 [25] (ASUS Zenbook S 14 with 16GB RAM, 256V NPU) and Intel Core Ultra Series 1 [26] (ASUS NUC 14 Pro with 16GB RAM, 165H NPU). We mea- sured inference latency, throughput, and energy efficiency using OpenVINO s benchmark_app tool, configuring performance hints and input output precision. For the NodePad technique, we augmented the Cora dataset by adding 292 nodes, making the static input size 3,000 nodes. In the GraphSAGE model, we limited the aggregation to a maximum of 10 randomly selected neighbor nodes. Energy consumption analysis was conducted using the HWINFO tool to assess the efficiency of the NPU in comparison to CPU and GPU implementations. All results were collected using public frameworks (OpenVINO, HWINFO, PyTorch) and can be replicated with the optimized models provided at the link (given in abstract). VI. RESULTS This section highlights the benefits of GraNNite optimization techniques, compares performance between Intel Core Ul- tra Series 1 2 NPUs, and demonstrates the superior energy efficiency of NPUs over CPUs and GPUs for GNN execution. Since GraNNite is the first hardware-aware framework tailored for optimizing GNN deployment on COTS SOTA NPUs, no existing works exist for direct comparison. Benefits of GraNNite Optimizations: Fig. 20 illustrates the performance progression of GNN models on the Intel Core Ultra Series 2 NPU, highlighting the impact of various optimizations proposed by GraNNite. Each optimization builds upon the preceding set unless otherwise specified. For example, the performance of QuantGr in GCN reflects a model in which GrAd, NodePad, GraphSplit, and QuantGr are cumulatively applied. However, in SAGE-max, EffOp and GrAx3 target the same model, and their performance gains are not cumulative. For GCN, the initial optimization, StaGr combined with Graph- Split, achieves a 1.51 speedup over the baseline by efficiently 0 0.2 0.4 0.6 0.8 1 Baseline StaGr GraphSplit GrAd NodePad QuantGr Baseline StaGr GraphSplit GrAd NodePad QuantGr Cora Citeseer Normalized Throughput GraNNite techniques Dataset Meteor Lake Lunar Lake 1.7X 1.6X GCN on Intel NPUs Baseline GrAd Cora StaGr GraphSplit NodePad QuantGr Baseline GrAd Citeseer StaGr GraphSplit NodePad QuantGr Intel Core Ultra Series 1 Intel Core Ultra Series 2 Fig. 21. Performance of GCN on different Intel NPUs: Intel Core Ultra Series 2 and Intel Core Ultra Series 1. 0 0.2 0.4 0.6 0.8 1 GCN GAT SAGE-mean Normalized Throughput GNN Models CPU GPU NPU GNNs on Intel Core Ultra Series 2 2.9X 3.3X 2.3X 3.8X 6.7X 10.8X Fig. 22. Performance of GNN models on different devices of an Intel AI PC: NPU outperforms CPU and GPU by a large margin. partitioning workloads between the CPU and NPU. Adding GrAd and NodePad introduces support for time-varying graphs and enhances parallelism but reduces performance to 1.4 due to CPU preprocessing overhead and an increased node count on the NPU. GraSp further boosts throughput by 1.1 . The most significant improvement, 2.7 , is achieved by combining GrAd, NodePad, GraphSplit, and QuantGr, leveraging low- precision arithmetic to minimize computational overhead. For GAT, EffOp alone provides a 3 speedup, while incorporating approximations (GrAx2) boosts performance to 7.6 with negligible impact on model quality. Similarly, for SAGE-max, EffOp yields a 2 speedup, which increases to 3.2 with GrAx3, again with no quality degradation. We note that the effects of SymG and CacheG could not be demonstrated as they require modifications to the (proprietary) NPU compiler. Performance Comparison on Intel Core Ultra Series 1 vs. Intel Core Ultra Series 2 NPUs: Fig. 21 compares GCN performance across GraNNite optimizations on Intel Core Ultra Series 1 and Intel Core Ultra Series 2 NPUs. Series 2 consistently outperforms series 1 due to its higher tile count (4 vs. 2). For the most optimized configuration (GrAd NodePad QuantGr), Intel Core Ultra Series 2 delivers 1.7 and 1.6 higher throughput than Intel Core Ultra Series 1 for the Cora and Citeseer datasets, respectively. This advantage arises from the higher number of MAC units in Series 2, enabling greater data parallelism. However, the observed gains fall short of the theoretical 2 maximum due to limited parallelism inherent in the GCN. Performance and Energy Efficiency of CPU, GPU, and NPU with GraNNite Optimizations: Fig. 22 compares the 0 0.2 0.4 0.6 0.8 1 Baseline StaGr GraphSplit GrAd NodePad QuantGr Baseline StaGr GraphSplit GrAd NodePad QuantGr Cora Citeseer Normalized Energy GraNNite techniques Dataset CPU GPU NPU 4.4X Baseline GrAd Cora StaGr GraphSplit NodePad QuantGr Baseline GrAd Citeseer StaGr GraphSplit NodePad QuantGr 8.6X 8.5X 4.1X GNNs on Intel Core Ultra Series 2 Fig. 23. Normalized Energy Consumption of GCN on Intel Core Ultra Series 2 Devices (CPU, GPU, and NPU), highlighting significant energy savings achieved with GraNNite optimizations. performance of CPU, GPU, and NPU across three GNN layers: GraphConv (GCN), GraphAttn (GAT), and SAGE (Graph- SAGE). For GCN, the NPU achieves a 2.9 speedup over the GPU and 3.3 over the CPU. For GAT layers, the NPU provides 2.3 and 3.8 improvements over the GPU and CPU, respectively. Similarly, for GraphSAGE with mean aggregation, the NPU achieves 6.7 and 10.8 speedups over the GPU and CPU. These results highlight the computational efficiency of NPUs and the effectiveness of GraNNite optimizations in delivering high-performance GNN execution without hardware modifications. Fig. 23 demonstrates the energy efficiency of NPUs compared to CPUs and GPUs for GNN execution. For the Cora dataset, the NPU is 4.1 and 8.5 more energy- efficient than the most efficient GPU and CPU implementa- tions, respectively. Similarly, for the Citeseer dataset, the NPU achieves 4.4 and 8.6 greater energy efficiency. VII. CONCLUSION This work presents GraNNite, a framework that optimizes GNN execution on NPUs using a three-step methodology. It addresses challenges like irregular memory access, dynamic graph updates, and control-heavy operations through hardware- aware optimizations. By improving parallelism, memory ef- ficiency, and low-precision computation, GraNNite reduces overhead, latency, and energy consumption while preserving accuracy. These enhancements enable real-time GNN execution for applications such as knowledge graph queries and event- driven analytics. Experimental evaluations on Intel Core Ultra Series 1 and 2 AI PCs show that GraNNite outperforms out-of-the-box NPU mappings and achieves significant energy efficiency gains over CPUs and GPUs. Its optimizations require no hardware modifications, ensuring scalability across diverse edge and accelerator platforms. REFERENCES [1] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, A compre- hensive survey on graph neural networks, TNNLS, vol. 32, no. 1, 2021. [2] F. Chen, J. Shao, S. Zhu, and H. T. Shen, Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recogni- tion in conversation, in CVPR, 2023. [3] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, Geometric deep learning: Going beyond euclidean data, IEEE Signal Processing Magazine, vol. 34, no. 4, 2017. [4] A. Gu, K. Goel, and C. Re, Efficiently modeling long sequences with structured state spaces, in ICLR, 2022. [5] C. Mavromatis and G. Karypis, GNN-RAG: Graph neural retrieval for large language model reasoning, in Submitted to ICLR, 2025, under review. [6] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun, X. Bresson, and B. Hooi, G-retriever: Retrieval-augmented generation for textual graph understanding and question answering, in NeurIPS, 2024. [7] S. Schaefer, D. Gehrig, and D. Scaramuzza, AEGNN: Asynchronous event-based graph neural networks, in CVPR, 2022. [8] Y. Yang, A. Kneip, and C. Frenkel, EvGNN: An event-driven graph neural network accelerator for edge vision, in arXiv, 2024. [9] S.-Y. Yu, A. V. Malawade, D. Muthirayan, P. P. Khargonekar, and M. A. A. Faruque, Scene-graph augmented data-driven risk assessment of autonomous vehicle decisions, T-ITS, vol. 23, no. 7, 2022. [10] A. Zhou, J. Yang, T. Qiao, Y. Qi, Z. Yang, W. Zhao, and C. Hu, Graph neural networks automated design and deployment on device-edge co- inference systems, in DAC, 2024. [11] A. Raha, D. A. Mathaikutty, S. K. Ghosh, and S. Kundu, FlexNN: A dataflow-aware flexible deep learning accelerator for energy-efficient edge devices, in arXiv, 2024. [12] Y. Li, H. Chen, Z. Cui, R. Timofte, M. Pollefeys, G. Chirikjian, and L. Van Gool, Towards efficient graph convolutional networks for point cloud handling, in ICCV, 2021. [13] A. Zhou, J. Yang, Y. Qi, Y. Shi, T. Qiao, W. Zhao, and C. Hu, Hardware- aware graph neural network automated design for edge computing plat- forms, in DAC, 2023. [14] Q. Lu, W. Jiang, M. Jiang, J. Hu, and Y. Shi, Hardware software co- exploration for graph neural architectures on fpgas, in ISVLSI, 2022. [15] T. N. Kipf and M. Welling, Semi-supervised classification with graph convolutional networks, in ICLR, 2017. [16] P. VeliÀáckovi c, G. Cucurull, A. Casanova, A. Romero, P. Li o, and Y. Ben- gio, Graph attention networks, in ICLR, 2018. [17] W. L. Hamilton, R. Ying, and J. Leskovec, Inductive representation learning on large graphs, in NeurIPS, 2017. [18] Y. Zhang, H. You, Y. Fu, T. Geng, A. Li, and Y. Lin, G-CoS: GNN- accelerator Co-Search towards both better accuracy and efficiency, in ICCAD, 2021. [19] F. Teichteil-K onigsbuch, G. Pov eda, G. Gonz alez de Garibay Barba, T. Luchterhand, and S. Thi ebaux, Fast and robust resource-constrained scheduling with graph neural networks, in ICAPS, 2023. [20] J. Nunez-Yanez, Accelerating graph neural networks in pytorch with hls and deep dataflows, in ARC, 2023. [21] C. Savard, N. Manganelli, B. Holzman, L. Gray, A. Perloff, K. Pedro, K. Stenson, and K. Ulmer, Optimizing high-throughput inference on graph neural networks at shared computing facilities with the nvidia triton inference server, in Computing and Software for Big Science, 2024. [22] S. Liang, Y. Wang, C. Liu, L. He, H. LI, D. Xu, and X. Li, EnGN: A high-throughput and energy-efficient accelerator for large graph neural networks, in TC, 2021. [23] A. Raha, R. Sung, S. Ghosh, P. K. Gupta, D. A. Mathaikutty, U. I. Cheema, K. Hyland, C. Brick, and V. Raghunathan, Efficient Hardware Acceleration of Emerging Neural Networks for Embedded Machine Learning: An Industry Perspective. Springer, 2024, pp. 121 172. [24] OpenVINO Documentation, Openvino IR format: Operation sets and specifications, r-format operation-sets operation-specs.html, 2024, accessed: Jan. 30, 2025. [Online]. Available: openvino-ir-format operation-sets operation-specs.html [25] Intel, Intel core ultra series mobile processors product brief, https: www.intel.com content www us en products docs processors core-ultra core-ultra-series-2-mobile-product-brief.html, 2024, accessed: January 30, 2025. [26] Intel, Intel core ultra series 1 product brief, content www us en products docs processors core-ultra core-ultra-serie s-1-product-brief.html, 2024, accessed: Jan. 30, 2025. [27] M. Rhu, M. O Connor, N. Chatterjee, J. Pool, Y. Kwon, and S. W. Keckler, Compressing dma engine: Leveraging activation sparsity for training deep neural networks, in HPCA, 2018. [28] S. K. Ghosh, A. Raha, and V. Raghunathan, Energy-efficient approxi- mate edge inference systems, ACM TECS, vol. 22, no. 4, Jul. 2023. [29] A. Das, S. K. Ghosh, A. Raha, and V. Raghunathan, Toward energy- efficient collaborative inference using multisystem approximations, IOTJ, vol. 11, no. 10, 2024. [30] Intel, Intel Distribution of OpenVINO Toolkit, accessed: Jan. 30, 2025. [Online]. Available: veloper tools openvino-toolkit overview.html\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nGraNNite: Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty and Vijay Raghunathan Abstract Graph Neural Networks (GNNs) are crucial for learning and reasoning over graph-structured data, with appli- cations in network analysis, recommendation systems, and speech analytics. Deploying them on edge devices, such as client PCs and laptops, enables real-time processing, enhances privacy, and reduces cloud dependency. For instance, GNNs can augment Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and enable event-based vision tasks. However, irregular memory access, sparse graphs, and dynamic struc- tures lead to high latency and energy consumption on resource- constrained devices. Modern edge processors combine CPUs, GPUs, and NPUs, where NPUs excel at data-parallel tasks but face challenges with irregular GNN computations. To address these gaps, we present GraNNite, the first hardware-aware framework tailored to optimize GNN deployment on commercial-off-the- shelf (COTS) state-of-the-art (SOTA) DNN accelerators using a systematic three-step methodology: (1) enabling GNN execution on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance and energy efficiency gains. Towards that end, the first category includes techniques such as GraphSplit for workload distribution and StaGr for static graph aggregation, while GrAd and NodePad handle real-time updates for dynamic graphs. Next, performance improvement is acquired through techniques such as EffOp for control-heavy operations and GraSp for sparsity exploitation. For Graph Convolution layers, PreG, SymG, and CacheG reduce redundancy and memory transfers. The final class of techniques deals with quality vs efficiency tradeoffs QuantGr applies INT8 quantization to lower memory usage and computation time, while GrAx1, GrAx2, and GrAx3 optimize graph attention, broadcast-add, and sample-and-aggregate (SAGE)-max aggregation for higher throughput with minimal quality loss.\n\n--- Segment 2 ---\nFor Graph Convolution layers, PreG, SymG, and CacheG reduce redundancy and memory transfers. The final class of techniques deals with quality vs efficiency tradeoffs QuantGr applies INT8 quantization to lower memory usage and computation time, while GrAx1, GrAx2, and GrAx3 optimize graph attention, broadcast-add, and sample-and-aggregate (SAGE)-max aggregation for higher throughput with minimal quality loss. Experimental evaluations on Intel Core Ultra Series 1 and 2 AI PCs demonstrate that GraNNite achieves speedups of 2.6 to 7.6 over default NPU mappings, with energy efficiency improvements up to 8.6 compared to CPUs and GPUs. Across various GNN models, GraNNite delivers up to 10.8 and 6.7 higher performance than CPUs and GPUs, respectively. Our code implementation is available at this link. I. INTRODUCTION Graph Neural Networks (GNNs) have become essential for learning and reasoning over graph-structured data, with appli- This work was supported in part by the Center for the Co-Design of Cog- nitive Systems (CoCoSYS) and the Center on Cognitive Multispectral Sensors (CogniSense), two research centers under the Joint University Microelectronics Program (JUMP) 2.0, a Semiconductor Research Corporation (SRC) initiative sponsored by DARPA. Arghadip Das (corresponding author, e-mail: and Vijay Raghunathan are with the Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA. Shamik Kundu (e-mail: Arnab Raha (e-mail: Soumendu Ghosh (e-mail: and Deepak Mathaikutty (e-mail: are with the Advanced Architecture Research Team, NPU IP, CGAI (CCG), Intel Corporation, Santa Clara, CA, USA.\n\n--- Segment 3 ---\nArghadip Das (corresponding author, e-mail: and Vijay Raghunathan are with the Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA. Shamik Kundu (e-mail: Arnab Raha (e-mail: Soumendu Ghosh (e-mail: and Deepak Mathaikutty (e-mail: are with the Advanced Architecture Research Team, NPU IP, CGAI (CCG), Intel Corporation, Santa Clara, CA, USA. Personal Assistant Question-answering Enhanced search Recommendations Event-driven vision tasks Theft detection Automatic lock Proximity alarm Speech- to-text AI- backend Model library Storage LLMs GNNs RAG Knowledge Graph (KG) Event Stream Subsampling Graph Generation GNNs Prediction GNN tasks run frequently in the background Map on NPU for faster response with lower power Link prediction KG alignment Node classification KG reasoning Image classification Object detection Image segmentation Pose estimation Face recognition Fig. 1. Applications of GNNs on Client PCs: showcasing GNN-driven tasks like recommendations and event-driven vision, mapped onto Intel Core Ultra processors for faster response and lower power. cations in areas like network analysis, recommendation sys- tems [1], and speech analytics [2]. Their ability to capture com- plex relationships through graph topology distinguishes them from traditional neural networks such as CNNs and LLMs. Recently, the inclusion of R-GAT, a prominent GNN model, in MLPerf s inference benchmarks emphasizes their growing importance in real-world applications. GNNs are particularly important compared to LLMs and newer architectures like State Space Models (SSMs) due to their ability to explicitly model relational and structural information, which is critical for tasks involving interconnected data such as social networks, molecular structures, and knowledge graphs [3]. While LLMs excel in sequential data processing and SSMs offer efficiency in modeling long-range dependencies [4], GNNs uniquely capture complex relationships through graph topology, making them indispensable for tasks where data is inherently non-Euclidean. Running GNNs on edge devices, including laptops and client PCs, has significant advantages. Edge-based inference ensures real-time processing, enhances data privacy, and reduces depen- dency on cloud infrastructure.\n\n--- Segment 4 ---\nRunning GNNs on edge devices, including laptops and client PCs, has significant advantages. Edge-based inference ensures real-time processing, enhances data privacy, and reduces depen- dency on cloud infrastructure. For instance, GNNs can enhance Retrieval-Augmented Generation (RAG) for LLMs [5], [6], enabling efficient personal assistant applications. In addition, they are integral to event-based vision tasks [7], [8], allowing rapid processing of irregular data streams (as shown in Fig. 1). The rising popularity of sensors in mobile devices further drives the deployment of GNNs to the wireless network edge for tasks such as sensing and interaction, including collision prediction in self-driving vehicles [9] and speech analytics [2]. These benefits make edge deployment crucial for achieving low-latency and energy-efficient solutions while addressing the growing demand for intelligent local inference. Despite their potential, deploying GNNs on resource-constrained edge devices presents challenges. Irregular memory access patterns, arXiv:2502.06921v2 [cs.LG] 13 Feb 2025 dynamic graph structures, and limited parallelism hinder com- putational efficiency. Sparse graphs exacerbate memory latency and lead to underutilized resources [1]. For example, deploying the DGCNN model on a Raspberry Pi 3B achieves less than 0.3 frames per second (fps), far below practical requirements [10]. Additionally, edge devices often rely on slower DRAM due to limited SRAM, resulting in high inference latency, increased energy consumption, and reduced battery life. These limitations highlight the need for optimized techniques to efficiently map GNN workloads onto edge platforms. Modern edge processors, such as AI PCs from Intel, Qual- comm, and AMD, integrate heterogeneous computing units, including CPUs, GPUs, and NPUs, to efficiently support di- verse AI workloads. Among these, NPUs or Neural Processing Units are specialized processors optimized for data-parallel op- erations, particularly matrix multiplication, which is the foun- dation of most neural network computations. NPUs typically include Data Processing Units (DPUs) [11] for parallelized matrix operations and Digital Signal Processors (DSPs) for sequential tasks like non-linear activation functions. NPUs are well-suited for AI workloads due to their high throughput and energy efficiency, outperforming traditional CPUs and GPUs.\n\n--- Segment 5 ---\nNPUs typically include Data Processing Units (DPUs) [11] for parallelized matrix operations and Digital Signal Processors (DSPs) for sequential tasks like non-linear activation functions. NPUs are well-suited for AI workloads due to their high throughput and energy efficiency, outperforming traditional CPUs and GPUs. These advantages make NPUs ideal for continuous and resource-intensive GNN workloads on edge devices. However, the aforementioned challenges hinder their efficient utilization for GNN processing. Although prior research has proposed methods to optimize GNN processing [12], these efforts re- main insufficient for real-time edge deployments [13], [14]. To address these gaps, we introduce GraNNite, a framework specifically designed to optimize the deployment of GNNs on NPUs, enhancing performance and efficiency. GraNNite leverages hardware-aware techniques to mitigate the challenges, ensuring scalable and efficient GNN execution on edge plat- forms. Modern GNNs primarily rely on three foundational layer types: Graph Convolution (GraphConv), Graph Atten- tion (GraphAttn), and Sample and Aggregate (SAGE), which form the basis of architectures such as Graph Convolution Network (GCN) [15], Graph Attention Network (GAT) [16], and GraphSAGE [17] (Fig. 2). These layers were selected for our study as they address distinct challenges: GCNs capture local structure through neighbor averaging, GATs improve representation quality by assigning importance weights via attention mechanisms, and GraphSAGE enhances scalability by sampling neighbors for efficient large-graph processing. GraNNite optimizes these layers to achieve efficient execution by introducing a systematic 3-step methodology: (1) enabling GNNs on NPUs, (2) optimizing performance, and (3) trading accuracy for further performance gains. While we evaluate GraNNite on GNNs using NPUs, the methodology is generic and can be extended to other models and hardware platforms without loss of generality. Our key contributions are: Step 1: Enabling GNNs on NPUs. GraNNite introduces GraphSplit to optimize sequential and irregular compute tasks by assigning graph preprocessing to the CPU and parallelizable tasks to the NPU using an offline cost model, minimizing communication overhead.\n\n--- Segment 6 ---\nOur key contributions are: Step 1: Enabling GNNs on NPUs. GraNNite introduces GraphSplit to optimize sequential and irregular compute tasks by assigning graph preprocessing to the CPU and parallelizable tasks to the NPU using an offline cost model, minimizing communication overhead. For static graphs, StaGr transforms node aggregation into matrix multipli- Graph Neural Networks (GNNs) Graph Convolution Network (GCN) Convolutional aggregation of neighbor features based on node links Simple and fundamental Graph Attention Network (GAT) Attention mechanism assigns weights to neighbors' importance Better quality than GCN Compute Quality GraphSAGE (SAmple and aggregate) Samples and aggregates neighbors for scalability Used for large graphs Compute Quality Aggregation Combination ‚Ñéùëñ ùëô ùúé ùëó {ùëñ} ùí©(ùëñ) 1 ùëëùëñùëëùëó ‚Ñéùëó ùëô 1ùëäùëô ùëíùëñùëó ùêøùëíùëéùëòùë¶ùëÖùëíùêøùëà(ùëéùëá [‚Ñéùëñ ùëô 1ùëäùëô·âõ‚Ñéùëó ùëô 1ùëäùëô] ‚Ñéùëñ ùëô ùúé œÉùëó {ùëñ} ùí©(ùëñ) exp(ùëíùëñùëó)‚Ñéùëó ùëô 1ùëäùëô œÉùëó {ùëñ} ùí©(ùëñ) exp(ùëíùëñùëó) ‚Ñéùëñ ùëô ùúéùëéùëò‚Ñéùëó ùëô 1ùëäùëô, ùëó {ùëñ} ùëÜùí©(ùëñ) Fig. 2. Three fundamental GNNs: GCN, GAT, and GraphSAGE, emphasizing their unique approaches convolutional aggregation, attention-based weighting, and neighbor sampling for scalability.\n\n--- Segment 7 ---\n2. Three fundamental GNNs: GCN, GAT, and GraphSAGE, emphasizing their unique approaches convolutional aggregation, attention-based weighting, and neighbor sampling for scalability. cation using a precomputed mask, while for dynamic graphs, GrAd and NodePad enable real-time updates with preconfigured node capacities. Step 2: Optimizing GNN performance. GraNNite en- hances efficiency with EffOp, which substitutes control- heavy DSP operations (e.g., select, gather) with equiva- lent data-parallel operations for DPU execution, reducing latency and improving energy efficiency. Additionally, GraSp exploits sparsity bitmaps to skip zero values, re- ducing memory usage and improving energy efficiency. For GNNs with GraphConv layers, PreG, SymG, and CacheG reduce redundancy: PreG offloads normalization factor computation to the CPU, SymG stores only half the normalization matrix, and CacheG reuses precomputed matrices to minimize memory transfers. Step 3: Trading accuracy for performance gains. GraNNite introduces QuantGr, which shifts computations from FP16 to INT8, achieving performance gains with reduced memory and computation time for negligible quality loss. For further throughput improvements, three approximation techniques are employed: GrAx1 simplifies attention score computation, GrAx2 optimizes broadcast- add operations, and GrAx3 accelerates SAGE-max aggre- gation using parallelized DPU operations. Experiments on Intel AI PCs show that GraNNite achieves speedups of 2.6 7.6 over out-of-the-box NPU mappings, with energy efficiency up to 8.6 higher than CPUs and GPUs. Across GNN models, it outperforms CPUs by 3.3 10.8 and GPUs by 2.3 6.7 . Intel Core Ultra Series 2 NPUs deliver up to 1.7 higher throughput than Intel Core Ultra Series 1 NPUs. The paper is organized as follows: Section II reviews prior work on GNN optimization for specialized hardware. Sec- tion III covers GNN execution and computational challenges. Section IV describes GraNNite methodology for optimizing GNNs on NPUs. Section V explains the experimental setup, including datasets, models, and hardware. Section VI presents performance and energy efficiency results. Finally, Section VII concludes the paper and outlines future directions. II.\n\n--- Segment 8 ---\nFinally, Section VII concludes the paper and outlines future directions. II. RELATED WORK GNNs excel at structural tasks due to their ability to extract features from graph topology [1], yet they require substantial computational power [18]. As detailed in Section I, deploying Aggregation 1 Combination 1 Input graph Dataloader Node embeddings Edge indices 0 1 2 0 1 2 3 Feature dim. d Number of nodes N 4 (1,2)(1,3)(1,4)(3,4) Generate adjacency mat. (A) Add self loops for aggregation (Adj) ùëõùëúùëüùëöùëñùëó ‡µû 1 ùëëùëñùëëùëó , ùëñùëì ùëéùëëùëóùëñùëó 1 0, ùëñùëì ùëéùëëùëóùëñùëó 0 Node Degree 1 4 2 2 3 3 4 3 0 1 2 3 0 1 1 1 0 1 0 0 0 1 1 0 0 1 2 1 0 1 0 3 0 1 2 3 1 1 1 1 0 1 1 0 0 1 1 0 1 1 2 1 0 1 1 3 Node degree Normalization factor formula Aggregation 0 Combination 0 N GCN Execution Flow Fig. 3. Execution flow of a GCN: graph preprocessing followed by iterative aggregation and combination phases for GNN computation [23]. Fig. 4. Execution Latency Breakdown of GraphConv and GraphAttn Layers (1433 input features and 64 output features) on Intel Core Ultra Se- ries 2 NPU across graph preprocessing (DPU DSP) and GNN computation (DPU DSP) for a graph with 1354 nodes and 5429 edges. GNNs in resource-constrained edge environments presents seri- ous difficulties. To tackle this, strategies to optimize GNNs for edge devices include simplifying model architectures [12] and employing hardware-aware neural architecture search (NAS) techniques like HGNAS [14] among others [13]. Nonetheless, these approaches still fall short; for instance, HGNAS boosts point cloud processing speed to merely 2 fps on a Raspberry Pi [14].\n\n--- Segment 9 ---\nTo tackle this, strategies to optimize GNNs for edge devices include simplifying model architectures [12] and employing hardware-aware neural architecture search (NAS) techniques like HGNAS [14] among others [13]. Nonetheless, these approaches still fall short; for instance, HGNAS boosts point cloud processing speed to merely 2 fps on a Raspberry Pi [14]. On the other hand, previous optimization approaches for DNN accelerators focused on techniques such as model fine- tuning, memory optimization, and standard quantization [19] [21]. Although they improved efficiency, they often required extensive retraining or hardware-specific code modifications, limiting portability. Furthermore, existing GNN mapping meth- ods do not fully leverage NPU-specific features like efficient sparsity handling, static data shapes, and optimized memory access, leading to suboptimal performance [22]. These meth- ods also struggle with the irregular computation patterns and memory intensity of GNNs, limiting their deployment on real- time edge devices. GraNNite addresses these challenges by introducing NPU-tailored optimizations that enable efficient, high-performance GNN execution on resource-constrained ac- celerators for real-time deployment. III. BACKGROUND MOTIVATION Understanding the execution of GNNs involves analyzing their core computational stages: Node Embedding, Aggregation, Combination, and Decode [23]. Fig. 3 demonstrates this process using a GCN [15] as an example. The process begins with loading the graph structure and node embeddings via a data loader. Graph edges are typically represented as tuples of connected node indices. To enhance computational efficiency, the graph can be preprocessed into a structured format, such as an adjacency matrix. This binary matrix indicates edge connections and includes self-loops to incorporate node-specific features. Additionally, a normalization matrix is derived from node degrees to ensure a balanced computation. During the Fig. 5. Execution latency breakdown of GNN computation of a single GraphConv and GraphAttn layer (1433 input features and 64 output features) on Intel Core Ultra Series 2 NPU across operations [24] for a graph with 1354 nodes and 5429 edges. Node Embedding stage, raw graph data is converted into feature vectors that serve as inputs to subsequent stages.\n\n--- Segment 10 ---\nExecution latency breakdown of GNN computation of a single GraphConv and GraphAttn layer (1433 input features and 64 output features) on Intel Core Ultra Series 2 NPU across operations [24] for a graph with 1354 nodes and 5429 edges. Node Embedding stage, raw graph data is converted into feature vectors that serve as inputs to subsequent stages. The Aggre- gation phase then collects features from neighboring nodes, leveraging operations such as pooling or reduction to capture relationships within the graph structure. However, this phase often incurs irregular memory access due to the variable num- ber of neighbors. Next, the Combination phase applies neural transformations, such as fully connected layers or attention mechanisms, to the aggregated features, producing higher-level representations. Finally, in the Decode phase, these refined features are processed through layers like MLPs or SoftMax to generate predictions. The Aggregation and Combination phases (main GNN compute) are the most computationally intensive, as they are performed repeatedly throughout the model, emphasizing their critical role in GNN execution. This iterative nature underscores the need for efficient preprocessing and computational strategies to optimize performance. Fig. 4 presents the latency breakdown for a single GraphConv and GraphAttn layer mapped out-of-the-box on the Intel Core Ultra Series 2 NPU. The breakdown highlights two major components: graph preprocessing and GNN compute (illustrated in Fig. 3), which includes operations such as combination and aggregation. Additionally, the figure provides a detailed view of how these components are distributed across the NPU s DPU and DSP units. It is evident from this breakdown that preprocessing plays a dominant role, contributing approximately 55 of the execution time in GraphAttn and nearly 99 in GraphConv. The preprocessing tasks, being control-flow heavy, are primarily executed on the DSP (relatively slower than DPU), further exacerbating the latency issue. Addressing this control-flow challenge is critical for improving GNN performance. In particular, GraphSplit, which is introduced in Section IV, is designed to mitigate this issue, optimizing preprocessing and enhancing overall execution efficiency. Fig. 5 further highlights the breakdown of GNN compute operations across different units of the NPU, with GraphConv benefiting from efficient matrix multiplication (MatMul) on the DPU.\n\n--- Segment 11 ---\nFig. 5 further highlights the breakdown of GNN compute operations across different units of the NPU, with GraphConv benefiting from efficient matrix multiplication (MatMul) on the DPU. While this operation suits NPUs well due to their strength in data-parallel tasks, GraphAttn still presents opportunities for improvement. In particular, around 30 of the GNN compute execution time in GraphAttn is spent on operations such as Select, Greater, Softmax, and Elu, which are control-heavy and executed on the DSP. These control-flow- intensive sections are prime targets for optimization, which GraNNite addresses through EffOp, as discussed in Section IV. Additionally, GNNs benefit from sparse input graphs and do not require full precision (FP32) for compute. This opens up further Operator Latency Gather Select MatMul Pre-trained Graph Neural Network (GNN) model GNN partitioning Offline Profile Table Control-flow dominant graph preprocessing at CPU Data-parallel GNN computation on NPU Processed node embeddings and connectivity information Apply model optimizations Convert to intermediate representation (IR) model.xml model.bin Intermediate representation (IR) NPU compiler Hardware Optimizations GraSp QuantGr model.blob Compiled binary Main memory (DRAM) Neural Processing Unit (NPU) Data Processing Unit (DPU) Digital Signal Processor (DSP) Local SRAM memory Direct Memory Access (DMA) Other logic Software Optimizations EffOp GrAx1, GrAx2, GrAx3 PreG, SymG, CacheG GraphSplit StaGr, GrAd, NodePad Fig. 6. End-to-end GraNNite methodology to efficiently enable GNNs on NPUs through model partitioning and optimizations. opportunities for optimization, where approximate methods can be deployed to reduce computation at the cost of minimal quality loss. GraNNite leverages these characteristics to enable high-speed GNN execution on NPUs, pushing the boundaries of real-time performance in edge environments. IV. GRANNITE DESIGN METHODOLOGY GraNNite provides an end-to-end framework (as shown in Fig. 6) for deploying pre-trained GNNs on NPUs without retraining. We consider an output-stationary NPU architecture inspired by Ref. [11].\n\n--- Segment 12 ---\nWe consider an output-stationary NPU architecture inspired by Ref. [11]. The core component is the DPU, an M M grid of Versatile Processing Elements, each comprising an N N array of MAC Processing Elements (MPEs) designed for efficient Multiply-and-Accumulate (MAC) operations. This DPU is well-suited for operations like matrix multiplication, which are fundamental to many neural network computations. The architecture includes a local SRAM for storing activations and weights, a tensor distribution network for data flow to and from the DPU, and control logic for managing computation, accumulation, and output extraction. MAC operations, integral to DNNs, calculate dot products of weights and activations to produce output feature maps. Each MPE leverages a local data path with register files, multipliers, and accumulators to perform these tasks. Additionally, a DSP handles non-linear activation functions and control-flow operations, complement- ing the data-parallel DPU. Although our case study considers an output-stationary NPU architecture, the proposed techniques are generic and can be applied to other NPUs without loss of generality. GraNNite proposes a generic step-by-step methodol- ogy (Fig. 7) to optimize emerging neural networks on existing AI accelerators. While demonstrated on GNNs using FlexNN- like [11] NPUs, the methodology is generalizable to other models and hardware platforms. It consists of three key steps: CPU-NPU Partitioned GNN inference GraphSplit Efficient GNN inference for static input graphs StaGr Enabling GNNs with dynamic input graphs on NPUs GrAd NodePad Optimization of control-flow heavy model sections EffOp Performance enhancement through activation sparsity GraSp Accelerating graph convolution PreG SymG CacheG Enhancing compute efficiency through quantization QuantGr Approximate GNN computation GrAx1 GrAx2 GrAx3 GraNNite GraNNite techniques Expected Vector of Improvement GraphSplit Improved workload distribution and parallelism. StaGr Reduced latency for static graph partitioning through precomputation. GrAd NodePad Efficient handling of dynamic graphs. EffOp Faster execution and reduced memory bandwidth usage. GraSp Reduced memory usage and computation by exploiting sparsity. PreG SymG CacheG Reduced redundancy and latency with optimized memory management. QuantGr Accelerated computation through reduced precision and memory access.\n\n--- Segment 13 ---\nPreG SymG CacheG Reduced redundancy and latency with optimized memory management. QuantGr Accelerated computation through reduced precision and memory access. GrAx1 GrAx2 GrAx3 Improved throughput with approximation techniques. Fig. 7. Suite of GraNNite Optimization Techniques for Efficient GNN Inference on NPUs. (1) Enabling the Model on the NPU. This step ensures the model runs efficiently on the NPU while maintaining flexibility. For GNNs, GraNNite introduces workload partitioning (Graph- Split), precomputed static graph processing (StaGr), and dy- namic graph handling (GrAd and NodePad) to support real-time updates and adaptive memory management. These techniques enable execution with minimal overhead. (2) Optimizing GNN Performance. Once enabled, the model undergoes further op- timizations to maximize efficiency without degrading accuracy. EffOp accelerates execution and reduces memory bandwidth usage, while PreG, SymG, and CacheG optimize memory access for Graph Convolution layers. GraSp exploits sparsity to lower memory and compute costs, improving throughput and energy efficiency. (3) Trading Accuracy for Performance and Energy Gains. For applications prioritizing speed and efficiency over quality, GraNNite offers QuantGr for INT8 quantization and approximation techniques (GrAx1, GrAx2, GrAx3) to further enhance throughput with minimal quality loss. These steps provide a systematic framework for deploying GNNs efficiently on NPUs, addressing resource constraints while ensuring scalability, performance, and energy efficiency. A. Step-1: Enabling GNNs on the NPU GraphSplit: To enable efficient execution of GNNs on NPUs, the first challenge is to address the mismatch between the hardware s strengths and the computational demands of graph- based workloads. NPUs excel at data-parallel tasks like matrix multiplications in neural networks, but are less efficient for control-heavy tasks involving frequent decision-making. CPUs, on the other hand, excel at these control-intensive tasks, using techniques such as predictive execution and out-of-order pro- cessing to maximize instruction-level parallelism. Given these contrasting strengths, one might assume it s best to offload all control-heavy tasks during GNN inference, such as computing initial masks (i.e., preprocessing in Fig. 4) for aggregation or calculating intermediate attention scores, to the CPU.\n\n--- Segment 14 ---\nGiven these contrasting strengths, one might assume it s best to offload all control-heavy tasks during GNN inference, such as computing initial masks (i.e., preprocessing in Fig. 4) for aggregation or calculating intermediate attention scores, to the CPU. However, a challenge arises when control-flow tasks exhibit a Read- after-Write (RAW) dependency on previous data-parallel tasks, necessitating the transfer of data back to the CPU. This results Fig. 8. GraphSplit, partitioned GNN inference using CPU and NPU: CPU handles graph preprocessing; NPU accelerates data-parallel GNN computation. in considerable communication overhead. To overcome this, GraNNite introduces an offline profiling phase during model calibration. In this phase, we build a cost model that measures real-time latencies of various operations on both the CPU and NPU. This cost model also factors in the overhead from data transfer and communication between the CPU and NPU. Using this information, GraphSplit identifies the most effec- tive partition points to minimize communication and latency. GraphSplit s partitioning strategy is designed to play to the strengths of each processing unit. Control-flow tasks, which require complex decision-making, are assigned to the CPU. Computationally heavy, data-parallel tasks, such as matrix multiplications, are sent to the NPU. This careful distribution improves graph processing performance by reducing the need for frequent data exchanges. For example, offloading initial in- put preprocessing to the CPU requires minimal communication with the NPU, resulting in better performance. As shown in Fig. 8, this partitioned inference setup for models such as GCN, GAT, and GraphSAGE effectively balances workload between CPU and NPU. StaGr: For applications involving static graph structures, GraNNite proposes an efficient methodology (StaGr) for im- plementing GNNs on hardware accelerators. Using a precom- puted mask tailored to a fixed input graph, StaGr transforms the aggregation of node features in Graph Convolution into a streamlined matrix multiplication operation (refer to GCN in Fig. 9), fully utilizing the capabilities of the NPU. This precomputed mask establishes node connections beforehand, significantly reducing irregular memory accesses and improving memory latency and energy efficiency, all without requiring extensive hardware modifications.\n\n--- Segment 15 ---\n9), fully utilizing the capabilities of the NPU. This precomputed mask establishes node connections beforehand, significantly reducing irregular memory accesses and improving memory latency and energy efficiency, all without requiring extensive hardware modifications. For Graph Attention and GraphSAGE, GraNNite leverages precomputed masks an at- tention mask for efficient attention score calculation and a sampled adjacency matrix for reuse during inference (see Fig. 9. StaGr: execution of GNNs on a static graph structure with dynamic node features. Fig. 10. One of the challenges to efficiently enable GNNs on NPUs: Dynamic input graph (An example of on-device knowledge graph). Fig. 9). This methodology achieves highly efficient inference, minimizing computational overhead and latency while optimiz- ing NPU performance under fixed-structure conditions. GrAd NodePad: To handle dynamic input graphs (refer Fig. 10), GraNNite proposes a new approach (GrAd) that uses a mask as input rather than a precomputed weight, allowing dynamic updates to edges without the need to recompile the model. Real-time graphs often undergo structural changes with nodes and edges dynamically added or removed (Fig. 10). However, NPUs typically support static input shapes, as DNN models are precompiled for fixed input shapes, with optimiza- tions such as tiling based on corresponding input configuration. This limitation requires recompilation when the input graph shape changes. Compiling the model for a static input shape and using mini-batches for inference may seem viable, but risks information loss by excluding edges connecting nodes outside the subgraph. Additionally, selecting an optimal batch size is challenging and may lead to underutilized NPU resources. Our approach introduces a node-padding technique (NodePad) that compiles the entire model with a higher node capacity than immediately needed for the whole input graph. For smaller graphs, embeddings for unused nodes are zero-padded, while absent edges are represented by zeroes in the adjacency matrix, following the conventional interpretation of 0 as no edge and 1 as an active connection. This node padding strategy min- imizes the need for frequent recompilation and eliminates the need to store multiple precompiled model versions for different graph sizes. Fig. 11 illustrates how a GNN with GraphConv Fig. 11.\n\n--- Segment 16 ---\n11 illustrates how a GNN with GraphConv Fig. 11. GrAd NodePad: Dynamic input graph support for GNNs via node padding: Eliminates multiple precompiled blobs, saving memory and removing the need for frequent recompilation with varying input node counts. layers can handle a time-varying input graph on an NPU. This approach applies zero padding to the input features and utilizes a norm matrix (mask), precomputed on the CPU, which is then fed into the main GNN computation on the NPU. By dynamically updating the mask at runtime, GrAd and NodePad allow the GNN to efficiently adapt to evolving graph structures. These techniques significantly improve performance and energy efficiency of GNN inference by reducing the overhead tied to model recompilation. B. Step-2: Optimizing GNN Performance on NPU EffOp: After enabling GNNs on the NPU, the next challenge lies in optimizing their performance without compromising application quality. A significant bottleneck arises from the control-heavy operations, such as conditional logic, Select, or Gather, residing deep in the GNNs being executed on the DSP within the NPU (as shown in Fig. 5). The DSP is designed for these operations, but runs at a lower frequency than the DPU. This difference often causes bottlenecks and increases latency in deep, sequential GNN sections. To address this limitation, GraNNite proposes a novel approach, EffOp, that converts these control-heavy operations into equivalent data- parallel tasks, allowing them to be executed on the faster DPU rather than the DSP. The core idea is to restructure sequential tasks, such as Select and Gather, to be processed as simple, elementwise reduction operations on the DPU. By redefining these tasks using operations like multiplication and addition, combined with precomputed masks, we transform inherently sequential processes into parallel-friendly ones. This allows the DPU to handle tasks that would traditionally rely on the slower DSP, reducing the need for sequential processing and, consequently, lowering overall execution time. As shown in Fig. 12, this method is particularly beneficial for operations in Graph Attention Networks, specifically in sections where intermediate attention scores are computed.\n\n--- Segment 17 ---\nAs shown in Fig. 12, this method is particularly beneficial for operations in Graph Attention Networks, specifically in sections where intermediate attention scores are computed. EffOp demonstrates 0 1 2 3 0 1 2 3 Intermediate Attention Scores 0 1 2 3 0 1 2 3 Adjacency matrix 0 1 2 3 0 1 2 3 Connectivity mask Masked Intermediate Attention Scores 0 1 2 3 0 1 2 3 Mask 0 1 2 3 0 1 2 3 Element 0? Replace element from connectivity mask Neural Processing Unit (NPU) Data Processing Unit (DPU) Digital Signal Processor (DSP) Local SRAM memory Direct Memory Access (DMA) Other logic Softmax Element-wise Addition DSP DPU CPU Baseline GAT implementation NPU components Optimized GAT implementation DSP Ops DPU Ops 0 1 Legend Element-wise Multiplication Fig. 12. Effop, efficient GNN computation by substituting control-intensive DSP operations with equivalent DPU operations: Utilizing the DPU s higher frequency and increased parallel compute units reduces end-to-end latency. Fig. 13. GraSp, exploiting input graph sparsity for faster execution: zero elements in node embeddings and adjacency matrices are compressed (ZVC), and a sparsity bitmap is used to bypass computation. how this computation can be achieved using elementwise multiplication, followed by elementwise addition with a slightly modified connectivity mask. In EffOp, tasks that typically involve complex control logic are optimized to utilize the DPU s strengths, transforming them into matrix and elemen- twise operations that can be efficiently parallelized. GraSp: In the context of GNN optimization on NPUs, activation sparsity offers a powerful mechanism to significantly boost performance by skipping unnecessary calculations. Given that input graphs are often highly sparse, with up to 99 of values being zero, GraNNite leverages this sparsity to optimize both memory usage and computational efficiency. The adjacency matrix in real-world graphs typically exhibits this extreme sparsity, containing many zero-valued entries where no direct connection exists between nodes. By capitalizing on this inherent sparsity, NPUs [25], [26] can streamline computations by skipping zero values, reducing the workload without af- fecting the accuracy of model inference. To efficiently manage these sparse values, GraNNite proposes GraSp, which utilizes a storage format known as Zero Value Compression (ZVC) [27].\n\n--- Segment 18 ---\nBy capitalizing on this inherent sparsity, NPUs [25], [26] can streamline computations by skipping zero values, reducing the workload without af- fecting the accuracy of model inference. To efficiently manage these sparse values, GraNNite proposes GraSp, which utilizes a storage format known as Zero Value Compression (ZVC) [27]. In this approach, only the non-zero values in the input graphs Fig. 14. PreG: GraphConv normalization factors rely only on the graph struc- ture, enabling precomputation and bypassing costly square-root and division operation on the NPU s slower DSP units. Fig. 15. SymG CacheG: Symmetric norm matrix in GraphConv layers is reused across all layers, allowing partial storage for memory savings and increased reuse. are stored explicitly, while the zero values are omitted, allowing the system to allocate memory and computational resources effectively. For GraSp implementation, sparsity bitmaps are used alongside compressed data to denote the locations of non- zero values within the matrix. This bitmap directs the NPU to focus solely on meaningful data while bypassing zero entries. Fig. 13 illustrates how sparsity bitmaps are integrated within the NPU s processing pipeline to skip zero-valued computations leading to latency speedup. PreG, SymG CacheG: GraNNite presents a streamlined approach tailored for GNNs that use GraphConv layers as core components. PreG leverages a precomputed, constant normalization matrix to accelerate processing. Since Graph- Conv is foundational and commonly used in more advanced GNN architectures, this enhancement offers broad applicability and efficiency gains. In GraphConv, the normalization factors required after neighborhood aggregation depend solely on the degrees of neighboring nodes, not on their specific features. Here, a node s degree represents the count of its neighboring nodes, including itself. By exploiting this feature independence, PreG precomputes these normalization factors once, storing them in a constant matrix (refer Fig. 14). By precomputing the normalization matrix on the CPU, the aggregation and normal- ization steps are combined into a single matrix multiplication. This approach leverages the NPU s strength in matrix multi- plication while avoiding the slower DSP unit, which typically handles division, thus streamlining execution and optimizing performance.\n\n--- Segment 19 ---\nBy precomputing the normalization matrix on the CPU, the aggregation and normal- ization steps are combined into a single matrix multiplication. This approach leverages the NPU s strength in matrix multi- plication while avoiding the slower DSP unit, which typically handles division, thus streamlining execution and optimizing performance. In addition, GraNNite introduces a memory- efficient technique (SymG) that capitalizes on the symmetry of the normalization matrix (see Fig. 15) in GraphConv layers, allowing only half of the adjacency matrix and its diagonal ele- ments to be stored. This optimization effectively reduces mem- ory complexity, translating into substantial savings in memory usage. SymG also minimizes memory traffic, especially during Direct Memory Access (DMA) transfers from DRAM to NPU s local memory, which can be a bottleneck. Finally, GraNNite Fig. 16. GrAx1, GraphAttn approximation 1: Removing compute-intensive multiplications boosts performance while preserving accuracy. Fig. 17. GrAx2, GraphAttn approximation 2: Removing a transpose and a broadcast operation reduces execution latency with negligible quality loss. introduces CacheG that caches the constant normalization matrix within the NPU s local memory and reuses it across all GraphConv layers in the model, significantly reducing memory access overhead. This caching strategy not only boosts runtime efficiency, but also lowers inference latency, making the overall execution of GNNs on the NPU more streamlined and resource- efficient. Fig. 15 demonstrates the portion of the normalization matrix stored in memory and illustrates how it is cached within the NPU s local memory. C. Step-3: Trading Accuracy for Performance Energy Gains QuantGr: In the pursuit of optimizing GNN performance and energy efficiency, we first explore traditional methods of reducing model precision before introducing GraNNite s novel approach. QuantGr, a quantization technique for GNNs is integrated in GraNNite that reduces numerical precision to achieve significant performance gains while preserving model accuracy. NPUs, typically designed with low-precision ca- pabilities, support both INT8 and FP16 datapaths, allowing notable performance gains over traditional FP32 computation. 0 1 2 3 0 1 2 3 Sampled adjacency matrix Aggregated neighborhood features Element[j] 0?\n\n--- Segment 20 ---\nNPUs, typically designed with low-precision ca- pabilities, support both INT8 and FP16 datapaths, allowing notable performance gains over traditional FP32 computation. 0 1 2 3 0 1 2 3 Sampled adjacency matrix Aggregated neighborhood features Element[j] 0? Feature[j] x[j] Unsqueeze adjacency matrix CPU Baseline SAGE-max implementation 0 1 2 0 1 2 3 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 3 0 1 2 0 1 2 3 Yes i j j Feature[j] No Reduce Max Elementwise Multiplication 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 3 0 1 2 0 1 2 3 Maxpool1D Number of neighbors 2 Input node features Optimized SAGE-max (GrAx3) DSP Ops DPU Ops 0 1 Legend Fig. 18. GrAx3, SAGE-max approximation: Replaces sequential DSP op- erations in SAGE-max aggregation with parallel element-wise multiplication and max pooling on the DPU, improving efficiency while maintaining correct feature aggregation for non-negative values. Specifically, INT8 precision provides a 2 boost in perfor- mance (TOPs) and a 4 improvement in performance per watt (TOPs Watt) compared to FP16. By carefully configuring the quantization parameters, such as setting an optimal zero point and scale, QuantGr can achieve competitive accuracy at lower precision. QuantGr uses symmetric, static quanti- zation, meaning both weights and activations are quantized around a zero point, with equal scaling factors for positive and negative values. Static quantization, which precomputes scaling and zero-point parameters during model calibration, enables consistent and faster inference, as these values remain fixed throughout execution. Symmetric quantization simplifies processing by ensuring consistent scaling and compatibility across all hardware layers, minimizing conversion overhead. Leveraging the NPU s support for static quantization of acti- vations and weights, this approach unlocks higher efficiency for GNNs, making it well-suited for performance-sensitive, resource-limited environments. GrAx1: Application of all previously discussed techniques in GraNNite can significantly improve GNN performance and energy efficiency on NPUs compared to the initial out-of-the- box implementation. However, further improvements can be achieved through approximate computing.\n\n--- Segment 21 ---\nGrAx1: Application of all previously discussed techniques in GraNNite can significantly improve GNN performance and energy efficiency on NPUs compared to the initial out-of-the- box implementation. However, further improvements can be achieved through approximate computing. This approach trades off minimal DNN accuracy for better computational efficiency, enabling faster processing and reduced resource usage [28], [29]. GNNs with Graph Attention (such as GAT) are well- regarded for their ability to generate attention maps that assign varying importance to nodes within a graph. However, these networks face significant computational challenges, particu- larly in managing non-existent edges. To prevent these edges from influencing the final attention values, they are typically masked by assigning them a large negative number before being processed through a SoftMax function. This masking step effectively ensures that attention coefficients for non-existent edges are rendered negligible during the aggregation phase. In GAT implementation with EffOp, an element-wise multiplica- GNN Graph Labelled Graph GCN GAT SAGE Intel AI PCs Hyperparameter Value Learning rate 0.01 Weight decay 5e-4 Batch size 64 Epochs 100 Training Recipe CoreTM Ultra Series 2 CoreTM Ultra Series 1 Fig. 19. Experimental setup for GNN evaluation on Intel AI PCs. tion is performed between the attention map and the mask to eliminate the influence of non-existent edges. However, this multiplication is computationally intensive and not well-suited for the DPU. To mitigate this inefficiency, GraNNite proposes a novel approximation technique (GrAx1). Instead of multiplying the attention map by the mask, it suggests directly adding a large negative value to the positions in the attention map that correspond to non-existent edges. This modification effectively bypasses the multiplication step (as shown in Fig. 16), leading to a substantial reduction in computational burden on the DPU. As a result, throughput is increased without sacrificing the final attention map quality. GrAx2: Another significant bottleneck in GAT arises during the broadcast-add operation (refer Fig. 5), which is essential for calculating the intermediate attention map. The traditional im- plementation of the broadcast-add operation requires adding the same value to multiple nodes, a process that involves broad- casting and transposing the data (refer left of Fig. 17). This step can lead to inefficiencies when executed on the DPU, hindering overall performance.\n\n--- Segment 22 ---\n17). This step can lead to inefficiencies when executed on the DPU, hindering overall performance. To address this inefficiency, GraNNite proposes another novel approximate solution (GrAx2) that replaces the conventional broadcast-add operation with just an addition followed by broadcasting. As shown in Fig. 17, this approach eliminates one transpose and one broadcast operation, significantly reducing the computational load for addition and minimizing memory copy reference operations, which enables the DPU to execute it more efficiently. GrAx3: For GNNs using the Sample and Aggregate (SAGE) layers with a max aggregation strategy, the feature selection for each neighborhood is traditionally processed se- quentially on the DSP, leading to inefficiency. GrAx3 replaces this sequential operation with parallel element-wise multiplica- tion using a mask (sampled adjacency matrix), followed by max pooling on the DPU. As shown in Fig. 18, GrAx3 simplifies the aggregation process, ensuring the correct aggregation of neighborhood features for most cases where feature values are greater than 0. V. EXPERIMENTAL METHODOLOGY As illustrated in Fig. 19, we evaluated GNNs on Intel NPUs using the Cora dataset (2,708 nodes, 5,429 edges, 7 classes, 1,433 features) and Citeseer dataset (3,327 nodes, 4,732 edges, 6 classes, 3,703 features) for node classification. The bench- Fig. 20. Progressive performance improvement of GNN through different GraNNite optimizations. marked models included Graph Convolutional Networks, Graph Attention Networks, and GraphSAGE, achieving baseline Top- 1 classification accuracies of 80.80 (GCN), 81.30 (GAT), 79.30 (SAGE-max), and 75.50 (SAGE-mean). We trained these models using PyTorch and PyTorch Geometric (PyG) with a learning rate of 0.01, weight decay of 5 10 4, batch size of 64 for 100 epochs. After training, the models were converted to an OpenVINO [30] compatible format for execution on the NPU.\n\n--- Segment 23 ---\nWe trained these models using PyTorch and PyTorch Geometric (PyG) with a learning rate of 0.01, weight decay of 5 10 4, batch size of 64 for 100 epochs. After training, the models were converted to an OpenVINO [30] compatible format for execution on the NPU. Experiments were conducted on two systems: Intel Core Ultra Series 2 [25] (ASUS Zenbook S 14 with 16GB RAM, 256V NPU) and Intel Core Ultra Series 1 [26] (ASUS NUC 14 Pro with 16GB RAM, 165H NPU). We mea- sured inference latency, throughput, and energy efficiency using OpenVINO s benchmark_app tool, configuring performance hints and input output precision. For the NodePad technique, we augmented the Cora dataset by adding 292 nodes, making the static input size 3,000 nodes. In the GraphSAGE model, we limited the aggregation to a maximum of 10 randomly selected neighbor nodes. Energy consumption analysis was conducted using the HWINFO tool to assess the efficiency of the NPU in comparison to CPU and GPU implementations. All results were collected using public frameworks (OpenVINO, HWINFO, PyTorch) and can be replicated with the optimized models provided at the link (given in abstract). VI. RESULTS This section highlights the benefits of GraNNite optimization techniques, compares performance between Intel Core Ul- tra Series 1 2 NPUs, and demonstrates the superior energy efficiency of NPUs over CPUs and GPUs for GNN execution. Since GraNNite is the first hardware-aware framework tailored for optimizing GNN deployment on COTS SOTA NPUs, no existing works exist for direct comparison. Benefits of GraNNite Optimizations: Fig. 20 illustrates the performance progression of GNN models on the Intel Core Ultra Series 2 NPU, highlighting the impact of various optimizations proposed by GraNNite. Each optimization builds upon the preceding set unless otherwise specified. For example, the performance of QuantGr in GCN reflects a model in which GrAd, NodePad, GraphSplit, and QuantGr are cumulatively applied. However, in SAGE-max, EffOp and GrAx3 target the same model, and their performance gains are not cumulative.\n\n--- Segment 24 ---\nFor example, the performance of QuantGr in GCN reflects a model in which GrAd, NodePad, GraphSplit, and QuantGr are cumulatively applied. However, in SAGE-max, EffOp and GrAx3 target the same model, and their performance gains are not cumulative. For GCN, the initial optimization, StaGr combined with Graph- Split, achieves a 1.51 speedup over the baseline by efficiently 0 0.2 0.4 0.6 0.8 1 Baseline StaGr GraphSplit GrAd NodePad QuantGr Baseline StaGr GraphSplit GrAd NodePad QuantGr Cora Citeseer Normalized Throughput GraNNite techniques Dataset Meteor Lake Lunar Lake 1.7X 1.6X GCN on Intel NPUs Baseline GrAd Cora StaGr GraphSplit NodePad QuantGr Baseline GrAd Citeseer StaGr GraphSplit NodePad QuantGr Intel Core Ultra Series 1 Intel Core Ultra Series 2 Fig. 21. Performance of GCN on different Intel NPUs: Intel Core Ultra Series 2 and Intel Core Ultra Series 1. 0 0.2 0.4 0.6 0.8 1 GCN GAT SAGE-mean Normalized Throughput GNN Models CPU GPU NPU GNNs on Intel Core Ultra Series 2 2.9X 3.3X 2.3X 3.8X 6.7X 10.8X Fig. 22. Performance of GNN models on different devices of an Intel AI PC: NPU outperforms CPU and GPU by a large margin. partitioning workloads between the CPU and NPU. Adding GrAd and NodePad introduces support for time-varying graphs and enhances parallelism but reduces performance to 1.4 due to CPU preprocessing overhead and an increased node count on the NPU. GraSp further boosts throughput by 1.1 . The most significant improvement, 2.7 , is achieved by combining GrAd, NodePad, GraphSplit, and QuantGr, leveraging low- precision arithmetic to minimize computational overhead. For GAT, EffOp alone provides a 3 speedup, while incorporating approximations (GrAx2) boosts performance to 7.6 with negligible impact on model quality. Similarly, for SAGE-max, EffOp yields a 2 speedup, which increases to 3.2 with GrAx3, again with no quality degradation.\n\n--- Segment 25 ---\nFor GAT, EffOp alone provides a 3 speedup, while incorporating approximations (GrAx2) boosts performance to 7.6 with negligible impact on model quality. Similarly, for SAGE-max, EffOp yields a 2 speedup, which increases to 3.2 with GrAx3, again with no quality degradation. We note that the effects of SymG and CacheG could not be demonstrated as they require modifications to the (proprietary) NPU compiler. Performance Comparison on Intel Core Ultra Series 1 vs. Intel Core Ultra Series 2 NPUs: Fig. 21 compares GCN performance across GraNNite optimizations on Intel Core Ultra Series 1 and Intel Core Ultra Series 2 NPUs. Series 2 consistently outperforms series 1 due to its higher tile count (4 vs. 2). For the most optimized configuration (GrAd NodePad QuantGr), Intel Core Ultra Series 2 delivers 1.7 and 1.6 higher throughput than Intel Core Ultra Series 1 for the Cora and Citeseer datasets, respectively. This advantage arises from the higher number of MAC units in Series 2, enabling greater data parallelism. However, the observed gains fall short of the theoretical 2 maximum due to limited parallelism inherent in the GCN. Performance and Energy Efficiency of CPU, GPU, and NPU with GraNNite Optimizations: Fig. 22 compares the 0 0.2 0.4 0.6 0.8 1 Baseline StaGr GraphSplit GrAd NodePad QuantGr Baseline StaGr GraphSplit GrAd NodePad QuantGr Cora Citeseer Normalized Energy GraNNite techniques Dataset CPU GPU NPU 4.4X Baseline GrAd Cora StaGr GraphSplit NodePad QuantGr Baseline GrAd Citeseer StaGr GraphSplit NodePad QuantGr 8.6X 8.5X 4.1X GNNs on Intel Core Ultra Series 2 Fig. 23. Normalized Energy Consumption of GCN on Intel Core Ultra Series 2 Devices (CPU, GPU, and NPU), highlighting significant energy savings achieved with GraNNite optimizations. performance of CPU, GPU, and NPU across three GNN layers: GraphConv (GCN), GraphAttn (GAT), and SAGE (Graph- SAGE). For GCN, the NPU achieves a 2.9 speedup over the GPU and 3.3 over the CPU.\n\n--- Segment 26 ---\nperformance of CPU, GPU, and NPU across three GNN layers: GraphConv (GCN), GraphAttn (GAT), and SAGE (Graph- SAGE). For GCN, the NPU achieves a 2.9 speedup over the GPU and 3.3 over the CPU. For GAT layers, the NPU provides 2.3 and 3.8 improvements over the GPU and CPU, respectively. Similarly, for GraphSAGE with mean aggregation, the NPU achieves 6.7 and 10.8 speedups over the GPU and CPU. These results highlight the computational efficiency of NPUs and the effectiveness of GraNNite optimizations in delivering high-performance GNN execution without hardware modifications. Fig. 23 demonstrates the energy efficiency of NPUs compared to CPUs and GPUs for GNN execution. For the Cora dataset, the NPU is 4.1 and 8.5 more energy- efficient than the most efficient GPU and CPU implementa- tions, respectively. Similarly, for the Citeseer dataset, the NPU achieves 4.4 and 8.6 greater energy efficiency. VII. CONCLUSION This work presents GraNNite, a framework that optimizes GNN execution on NPUs using a three-step methodology. It addresses challenges like irregular memory access, dynamic graph updates, and control-heavy operations through hardware- aware optimizations. By improving parallelism, memory ef- ficiency, and low-precision computation, GraNNite reduces overhead, latency, and energy consumption while preserving accuracy. These enhancements enable real-time GNN execution for applications such as knowledge graph queries and event- driven analytics. Experimental evaluations on Intel Core Ultra Series 1 and 2 AI PCs show that GraNNite outperforms out-of-the-box NPU mappings and achieves significant energy efficiency gains over CPUs and GPUs. Its optimizations require no hardware modifications, ensuring scalability across diverse edge and accelerator platforms. REFERENCES [1] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, A compre- hensive survey on graph neural networks, TNNLS, vol. 32, no. 1, 2021.\n\n--- Segment 27 ---\n32, no. 1, 2021. [2] F. Chen, J. Shao, S. Zhu, and H. T. Shen, Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recogni- tion in conversation, in CVPR, 2023. [3] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst, Geometric deep learning: Going beyond euclidean data, IEEE Signal Processing Magazine, vol. 34, no. 4, 2017. [4] A. Gu, K. Goel, and C. Re, Efficiently modeling long sequences with structured state spaces, in ICLR, 2022. [5] C. Mavromatis and G. Karypis, GNN-RAG: Graph neural retrieval for large language model reasoning, in Submitted to ICLR, 2025, under review. [6] X. He, Y. Tian, Y. Sun, N. V. Chawla, T. Laurent, Y. LeCun, X. Bresson, and B. Hooi, G-retriever: Retrieval-augmented generation for textual graph understanding and question answering, in NeurIPS, 2024. [7] S. Schaefer, D. Gehrig, and D. Scaramuzza, AEGNN: Asynchronous event-based graph neural networks, in CVPR, 2022. [8] Y. Yang, A. Kneip, and C. Frenkel, EvGNN: An event-driven graph neural network accelerator for edge vision, in arXiv, 2024. [9] S.-Y. Yu, A. V. Malawade, D. Muthirayan, P. P. Khargonekar, and M. A. A. Faruque, Scene-graph augmented data-driven risk assessment of autonomous vehicle decisions, T-ITS, vol. 23, no. 7, 2022. [10] A. Zhou, J. Yang, T. Qiao, Y. Qi, Z. Yang, W. Zhao, and C. Hu, Graph neural networks automated design and deployment on device-edge co- inference systems, in DAC, 2024.\n\n--- Segment 28 ---\n7, 2022. [10] A. Zhou, J. Yang, T. Qiao, Y. Qi, Z. Yang, W. Zhao, and C. Hu, Graph neural networks automated design and deployment on device-edge co- inference systems, in DAC, 2024. [11] A. Raha, D. A. Mathaikutty, S. K. Ghosh, and S. Kundu, FlexNN: A dataflow-aware flexible deep learning accelerator for energy-efficient edge devices, in arXiv, 2024. [12] Y. Li, H. Chen, Z. Cui, R. Timofte, M. Pollefeys, G. Chirikjian, and L. Van Gool, Towards efficient graph convolutional networks for point cloud handling, in ICCV, 2021. [13] A. Zhou, J. Yang, Y. Qi, Y. Shi, T. Qiao, W. Zhao, and C. Hu, Hardware- aware graph neural network automated design for edge computing plat- forms, in DAC, 2023. [14] Q. Lu, W. Jiang, M. Jiang, J. Hu, and Y. Shi, Hardware software co- exploration for graph neural architectures on fpgas, in ISVLSI, 2022. [15] T. N. Kipf and M. Welling, Semi-supervised classification with graph convolutional networks, in ICLR, 2017. [16] P. VeliÀáckovi c, G. Cucurull, A. Casanova, A. Romero, P. Li o, and Y. Ben- gio, Graph attention networks, in ICLR, 2018. [17] W. L. Hamilton, R. Ying, and J. Leskovec, Inductive representation learning on large graphs, in NeurIPS, 2017. [18] Y. Zhang, H. You, Y. Fu, T. Geng, A. Li, and Y. Lin, G-CoS: GNN- accelerator Co-Search towards both better accuracy and efficiency, in ICCAD, 2021.\n\n--- Segment 29 ---\n[17] W. L. Hamilton, R. Ying, and J. Leskovec, Inductive representation learning on large graphs, in NeurIPS, 2017. [18] Y. Zhang, H. You, Y. Fu, T. Geng, A. Li, and Y. Lin, G-CoS: GNN- accelerator Co-Search towards both better accuracy and efficiency, in ICCAD, 2021. [19] F. Teichteil-K onigsbuch, G. Pov eda, G. Gonz alez de Garibay Barba, T. Luchterhand, and S. Thi ebaux, Fast and robust resource-constrained scheduling with graph neural networks, in ICAPS, 2023. [20] J. Nunez-Yanez, Accelerating graph neural networks in pytorch with hls and deep dataflows, in ARC, 2023. [21] C. Savard, N. Manganelli, B. Holzman, L. Gray, A. Perloff, K. Pedro, K. Stenson, and K. Ulmer, Optimizing high-throughput inference on graph neural networks at shared computing facilities with the nvidia triton inference server, in Computing and Software for Big Science, 2024. [22] S. Liang, Y. Wang, C. Liu, L. He, H. LI, D. Xu, and X. Li, EnGN: A high-throughput and energy-efficient accelerator for large graph neural networks, in TC, 2021. [23] A. Raha, R. Sung, S. Ghosh, P. K. Gupta, D. A. Mathaikutty, U. I. Cheema, K. Hyland, C. Brick, and V. Raghunathan, Efficient Hardware Acceleration of Emerging Neural Networks for Embedded Machine Learning: An Industry Perspective. Springer, 2024, pp. 121 172. [24] OpenVINO Documentation, Openvino IR format: Operation sets and specifications, r-format operation-sets operation-specs.html, 2024, accessed: Jan. 30, 2025. [Online].\n\n--- Segment 30 ---\n[24] OpenVINO Documentation, Openvino IR format: Operation sets and specifications, r-format operation-sets operation-specs.html, 2024, accessed: Jan. 30, 2025. [Online]. Available: openvino-ir-format operation-sets operation-specs.html [25] Intel, Intel core ultra series mobile processors product brief, https: www.intel.com content www us en products docs processors core-ultra core-ultra-series-2-mobile-product-brief.html, 2024, accessed: January 30, 2025. [26] Intel, Intel core ultra series 1 product brief, content www us en products docs processors core-ultra core-ultra-serie s-1-product-brief.html, 2024, accessed: Jan. 30, 2025. [27] M. Rhu, M. O Connor, N. Chatterjee, J. Pool, Y. Kwon, and S. W. Keckler, Compressing dma engine: Leveraging activation sparsity for training deep neural networks, in HPCA, 2018. [28] S. K. Ghosh, A. Raha, and V. Raghunathan, Energy-efficient approxi- mate edge inference systems, ACM TECS, vol. 22, no. 4, Jul. 2023. [29] A. Das, S. K. Ghosh, A. Raha, and V. Raghunathan, Toward energy- efficient collaborative inference using multisystem approximations, IOTJ, vol. 11, no. 10, 2024. [30] Intel, Intel Distribution of OpenVINO Toolkit, accessed: Jan. 30, 2025. [Online]. Available: veloper tools openvino-toolkit overview.html\n\n