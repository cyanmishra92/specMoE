=== ORIGINAL PDF: 2507.04535v1_da4ml_Distributed_Arithmetic_for_Real-time_Neural_.pdf ===\n\nRaw text length: 83766 characters\nCleaned text length: 81632 characters\nNumber of segments: 59\n\n=== CLEANED TEXT ===\n\narXiv:2507.04535v1 [cs.AR] 6 Jul 2025 da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs CHANG SUN, California Institute of Technology, USA ZHIQIANG QUE, Imperial College London, UK VLADIMIR LONCAR , CERN, Switzerland WAYNE LUK, Imperial College London, UK MARIA SPIROPULU, California Institute of Technology, USA Neural networks with a latency requirement on the order of microseconds, like the ones used at the CERN Large Hadron Collider, are typically deployed on FPGAs fully unrolled and pipelined. A bottleneck for the deployment of such neural networks is area utilization, which is directly related to the required constant matrix-vector multiplication (CMVM) operations. In this work, we propose an efficient algorithm for implementing CMVM operations with distributed arithmetic (DA) on FPGAs that simultaneously optimizes for area consumption and latency. The algorithm achieves resource reduction similar to state-of-the-art algorithms while being significantly faster to compute. The proposed algorithm is open-sourced and integrated into the hls4ml library, a free and open-source library for running real-time neural network inference on FPGAs. We show that the proposed algorithm can reduce on-chip resources by up to a third for realistic, highly quantized neural networks while simultaneously reducing latency, enabling the implementation of previously infeasible networks. CCS Concepts: Hardware Reconfigurable logic applications; Computing methodologies Neural networks. Additional Key Words and Phrases: Quantized Neural Network, Real-time inference, High level synthesis, FPGA ACM Reference Format: Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu. 2025. da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym XX). ACM, New York, NY, USA, 24 pages. 1 Introduction Due to the need for low latency and high throughput in applications, edge computing has significantly increased its importance for real-time neural network inference [35]. While typical real-time inference applications have latency constraints of a few milliseconds [17, 28, 43], certain specific applications require sub-microsecond inference latency. For instance, at the CERN Large Hadron Collider (LHC) [40], hundreds of terabytes of data are generated by the detectors every second from proton-proton collisions at a rate of 4 MHz. This enormous data throughput is reduced by a hardware system, ùë°ùëüùëñùëîùëîùëíùëü, which filters the data in real-time at the same rate. It determines which event should be kept for offline processing or discarded, and the final decision must be made within a few microseconds [38, 39]. The Also at Institute of Physics Belgrade, Serbia. Authors Contact Information: Chang Sun, California Institute of Technology, Pasadena, CA, USA; Zhiqiang Que, Imperial College London, London, UK; Vladimir Loncar, CERN, Geneva, Switzerland; Wayne Luk, Imperial College London, London, UK; Maria Spiropulu, California Institute of Technology, Pasadena, CA, USA. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from 2025 Copyright held by the owner author(s). Publication rights licensed to ACM. Manuscript submitted to ACM Manuscript submitted to ACM 1 2 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu trigger s accuracy is vital to keep only the relevant events for physics studies, effectively managing the downstream data rate by reducing it by two orders of magnitude. O(1000) field-programmable gate arrays (FPGAs) are currently used in the trigger system, where several algorithms run in parallel on each FPGA. As a result, resources are scarce, and the footprint of each algorithm must be minimal. In anticipation of the LHC s upgrade to the High-Luminosity LHC (HL-LHC) [45], Machine Learning (ML) techniques are being actively explored to enhance the current trigger system [1, 38, 39]. However, integrating demanding models under such strict resource and latency constraints remains challenging. To meet the latency requirements, the neural networks used in such applications are typically deployed on FPGAs fully unrolled and pipelined (usually with an initiation interval of one). hls4ml, a free and open-source library for synthesizing deployable firmware for running neural network inferences on FPGAs, is widely used for implementing such neural networks on FPGAs [1, 12 14, 36, 38, 39]. The library parses the architectures of the neural networks and generates the corresponding high-level synthesis (HLS) projects for FPGAs with a set of fixed templates for common neural network layers or operations. The generated HLS projects are then synthesized into hardware description language (HDL) code by the corresponding HLS backends. Frequently, the most resource-consuming part of the designs are the constant matrix-vector multiplication (CMVM) operations in the neural networks, such as those in the dense or convolutional layers. As the final design is fully unrolled, the CMVM operations are typically implemented primarily with distributed arithmetic (DA) [27] by the HLS backends, as suggested by the high lookup-table (LUT) utilization with moderate to low digital signal processor (DSP) utilization in the synthesized designs. In this work, we present da4ml, a fast, scalable, and accurate optimization framework for CMVM targeting ultra- low-latency neural networks on FPGAs. Most existing exact multiplierless CMVM algorithms are either too slow (e.g., ùêªùëêùëöùë£ùëö: O(ùëÅ3)1 time [3], hours for moderate-size matrices), or miss significant opportunities for optimization (e.g., SCMVM: O(ùëÅ2) time [44], but cannot capture differently scaled subexpressions). The proposed da4ml combines a novel graph-based decomposition with cost-aware Common Subexpression Elimination (CSE), which retains full numerical precision (not approximate), captures subexpression reuse even across differently scaled terms and signed digits, and has a much better asymptotic complexity O(ùëÅ2) and a five-orders-of-magnitude faster runtime (Table 2). This makes it practical for realistic, large-scale networks in ultra-low-latency environments such as the CERN LHC. To the best of our knowledge, this is the first open-source, end-to-end DA-based neural network compiler with optimized CMVM operations. By tightly integrating our framework with hls4ml, it provides a drop-in complement for the default CMVM implementation in hls4ml. This tight integration into an established toolchain (the hls4ml workflow) lowers the barrier to adoption for the broader HEP and FPGA communities. The framework also supports direct RTL generation without going through HLS for fast prototyping and easier integration into existing RTL workflows. The practical impact of the proposed framework has been demonstrated by enabling the production deployment of the AXOL1TL [1] anomaly detection trigger at the CMS experiment, where it significantly improves the resource utilization and timing closure of the synthesized design. The contributions of this work are as follows: We propose da4ml, a novel, performant, and scalable CMVM optimization framework with a hybrid algorithm that combines graph-based decomposition with cost-aware CSE. It offers orders of magnitude faster runtime than the prior state of the art while achieving comparable resource efficiency. 1ùëÅis the number of non-zero signed digits needed to represent the constant matrix. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 3 We implement the proposed da4ml framework as an open-source library and tightly integrate it into the widely used hls4ml tool as a drop-in solution for FPGA-based ML designs. da4ml can also generate synthesizable RTL directly, allowing users to bypass HLS where needed. In addition, da4ml has enabled production deployment of the AXOL1TL [1] anomaly detection trigger at CMS with improved resource efficiency and timing closure. We evaluate the proposed framework on both synthetic CMVM benchmarks and realistic neural networks for the CERN LHC trigger system. The results show significant improvements in hardware resource utilization, design latency, and compilation speed, demonstrating the framework s practicality for ultra-low-latency FPGA applications. 1.1 Background and Related Work 1.1.1 Constant Matrix-Vector Multiplication. Constant matrix-vector multiplication (CMVM) is a common problem in digital signal processing (DSP) applications. The problem involves computing an operation of the form ùë¶T ùë•TùëÄ, where ùëÄis a constant integer matrix and ùë•is a vector of input values. The problem is well studied in the literature, and there are multiple algorithms proposed for solving it. Distributed arithmetic (DA) is a multiplierless method for implementing the multiplication-accumulation (MAC) operations in hardware by replacing them by shift-and-add (or subtract) operations, which are usually mapped to LUTs on FPGAs. The method is particularly useful for applications where extremely low latency and high throughput are required. The DA algorithm has been studied in the literature for a long time, and multiple algorithms have been proposed for solving it. Many of the existing works focus on the multiple constant multiplication (MCM) problem, which is a special case of CMVM where the variable vector ùë•is of size one, commonly used in finite impulse response (FIR) filters [3], such as [24 27, 30, 41]. The works [3, 8, 9, 15, 16, 19, 44] propose algorithms for the CMVM problem, which is a more general case of the MCM problem. [15] applies CSE to the CMVM problem by recursively removing the most common two-term subexpression from the problem. [9] uses a similar approach but also takes into account the conflict between the subexpressions. In contrast, [16] uses a graph based approach, which transforms subgraphs in the constructed adder tree without increasing the maximum adder depth. Though dated more than a decade, to the best of our knowledge, the state-of-the-art algorithm for implementing a CMVM operation with an adder tree without precision loss is still the Hcmvmalgorithm proposed in [3]. The algorithm aggressively searches for possible transformations of the CMVM problem into potentially simpler subproblems and evaluates the cost of each using a heuristic similar to [9]. The algorithm also supports specifying the maximum allowed adder depth for the generated adder tree by limiting the search phase space. However, this algorithm is computationally expensive, which requires O(1000) CPU-seconds to optimize a random 16 16, 8-bit matrix. Moreover, the runtime scaling with respect to ,ùëÅ, the number of non-zero signed digits required to represent the constant matrix, lies between O(ùëÅ3) and O(ùëÅ3.5). This makes it impractical for optimizing any moderately large matrices used in neural networks. In contrast, we propose a new algorithm for optimizing the CMVM problem that achieves resource efficiency comparable to [3] while significantly reducing computational cost, making it practical for larger matrices. [44] proposes a scalable algorithm for the CMVM problem, which is demonstrated to be runtime-efficient and practically useful for up to 100 100 constant matrices. Unlike previously mentioned CSE-based approaches using two-term subexpressions, the algorithm considers multi-term common subexpressions with greedy CSE for efficient processing. However, this algorithm fails to capture common subexpressions with different power-of-two scaling Manuscript submitted to ACM 4 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu factors, and it does not account for possible negative values in the weights. [8] adopts a similar multi-term common subexpression approach but uses a simulated annealing to optimize the cost of the adder tree for better resource utilization at the expense of algorithm runtime. While shifted common subexpressions are supported, the algorithm can only identify them in a coarse-grained fashion (i.e., a uniform shift across the whole row or column of the constant matrix). Signed digit representation is stated to be supported, but no discrete method is given, and its impact on identifying multi-term common subexpressions is not discussed. To avoid these drawbacks, we design our algorithm to effectively capture common subexpressions with different shifts and signs while preserving runtime efficiency, enabling more efficient optimization for a broader class of constant matrices. For neural networks, implementing CMVM operations without precision loss is not always required. [23] uses computation coding for approximate CMVM operations by decomposing the constant matrix into the product of multiple matrices containing only powers-of-two. By applying this method to embedded neural networks, the method demonstrates minimal accuracy loss while improving resource utilization by 3 to 6-fold. Similarly, [4] proposes another method based on 0-1 linear programming to optimize the CMVM problem approximately. However, for already highly quantized neural networks, such as those trained with HGQ or QKeras [10, 11] with low bitwidths, the weights usually carry non-negotiable gradients on the losses, and such additional approximations in the implementation stage are not preferred. Hence, in this work, we focus on the implementation of CMVM operations without any approximations, preserving full numerical precision throughout. 1.2 Ultra-Low Latency Neural Networks Neural networks with a latency requirement on the order of O(1) ùúás are required for the trigger systems of the Large Hadron Collider. Simultaneously, the networks are required to produce one inference at least every 25 ns. To satisfy these constraints, these neural networks are typically heavily quantized and pruned and deployed on FPGAs fully unrolled with initiation interval (II) of one [1, 12 14, 36, 38, 39]. As the networks are fully unrolled, on-chip resource consumption is of major concern. In our experience, the most resource-consuming part of the designs are usually the matrix-vector multiplications in the fully connected or convolutional layers. The standard approach for optimizing resource utilization is to quantize and prune the neural networks to reduce the resulting firmware size. In this work, we go further by introducing an efficient algorithm for optimizing CMVM operations based on distributed arithmetic, which reduces resource utilization by up to one-third and improves timing closure, enabling previously infeasible designs to meet the stringent LHC trigger system constraints. 1.3 hls4ml hls4ml [12] is a template library for translating quantized neural networks into HLS projects for FPGAs. hls4ml provides a high-level interface for users to interface popular machine learning frameworks with HLS backends, and enables users to deploy neural networks on FPGAs with minimal effort. This library is widely used for real-time inference applications. Notably, it has been deployed at the Compact Muon Solenoid (CMS) experiment in its L1 trigger for anomaly detection [1] and is being studied for the upgrade of the trigger system at the ATLAS experiment [36]. The library has a large built-in template library for common neural network layers, each customized and optimized for different HLS backends. In this work, we use hls4ml with the Vivado and Vitis HLS backends to implement the proposed algorithm, and we use its implementation as a baseline for comparison. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 5 2 Problem Formulation The objective is to implement the CMVM operation, ùë¶T ùë•TùëÄ, as an adder tree with minimum LUTs on FPGAs under a delay constraint. The delay constraint (DC) is defined by the maximum of additional adder depth with respect to the minimal adder depth possible. The target application of this work is the real-time inference of quantized neural networks on FPGAs targeting the LHC trigger system, with possible extension to other general DSP applications. For the target application, the neural networks are required to have an initial interval 25 ns. For this purpose, when implementing the CMVM operations with adder trees, we expect multiple adder steps within one pipeline stage. Hence, though the final design is expected to be pipelined, we expect the number of LUTs used for implementing the adders to dominate the resource utilization with moderate to low register usage. In the adder tree, the dominant operation is in the form of ùëé (ùëè ùë†), where ùëéand ùëèare the variable inputs to the adder. The bit-shift ùë†and the sign for the input ùëèare known at compilation time. The expected cost of this operation is formulated as the number of full and half adders required to implement the operation (i.e., the number of output bits conditioned on more than one input). We denote the bitwidths for ùëéand ùëèas ùëèùë§ùëéand ùëèùë§ùëè, respectively. When there is at least one bit overlapping between the two operands (i.e., max(ùëèùë§ùëé,ùëèùë§ùëè) ùë†), the simplified expected cost is given by cost(ùëèùë§ùëé,ùëèùë§ùëè,ùë†, sign) max(ùëèùë§ùëé,ùëèùë§ùëè ùë†) min(0,ùë†) 1. (1) In practice, the logic delay of each adder with different input bitwidths will be different. However, as we notice that the majority of the delay in the adder trees is due to routing, we assume each adder to have the same delay and model the overall delay approximately by the adder depth for simplicity, following [3, 4, 16]. 3 da4ml Algorithm 3.1 Notations For a number of fixed-point type, it takes the general form of fixed ùëÜ, ùëä, ùêº , where ùëÜis the sign bit, ùëäis the total bitwidth, and ùêºis the number of integer bits including the sign bit. In the algorithm, we denote the fixed-point numbers by quantized interval, namely, by their low value, high value, and the step size, [ùëô,‚Ñé,ùõø]. For a generic fixed-point number in the form of fixed ùëÜ, ùëä, ùêº , we have ùëô ùëÜ 2(ùêº ùëÜ), ‚Ñé 2ùêº ùëÜ 2 ùëä ùêº, and ùõø 2 ùëä ùêº. This notation is useful to track the required bitwidths when accumulating a large number of fixed-point numbers, as otherwise one carry bit will always need to be added to prevent overflow. The notation we use in the algorithm is summarized in Table 1. In particular, ùëÄ[ùëëùëñùëõ,ùëëùëúùë¢ùë°], ùëûùëñùëõùë°ùëñùëõ[ùëëùëñùëõ], ùëëùëíùëùùë°‚Ñéùëñùëõùë°[ùëëùëñùëõ], and ùëëùëêare the input parameters to the algorithm for a single CMVM operation. 3.2 Overview The da4ml is a CSE-based hybrid algorithm. Like other CSE-based algorithms, the algorithm operates on a discrete representation of the constant matrix. In this work, following [3, 4, 9, 15], we adopt the canonical signed digit (CSD) [6] representation. CSD is a signed digit representation of a number that never has two consecutive non-zero digits, and the number of non-zero digits is guaranteed to be minimal. Hence, for a number with ùë•digits, the CSD representation has at most ùë• 2 1 non-zero digits, which is 1 3 of the total number of digits on average. As ùëÄcontains only fixed-point integers, the algorithm first normalizes it by applying bit-shifts across the rows and columns such that no row column has all entries even except for zeros. The resultant scaling factors are recorded and will be applied to the input output vectors. Manuscript submitted to ACM 6 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Table 1. List of parameters used in the proposed da4ml algorithm Parameter Description ùëëùëñùëõ The number of input elements. ùëëùëúùë¢ùë° The number of output elements. ùëÄ[ùëëùëñùëõ,ùëëùëúùë¢ùë°] The input constant matrix containing fixed-point numbers. ùëëùëê Delay constraint, maximum number of extra adder depth permitted. ùëûùëñùëõùë°ùëñùëõ[ùëëùëñùëõ] An array representing the quantized intervals of the input vector. ùëô,‚Ñé,ùõø The low, high, and step size that uniquely defines a quantized interval. ùëëùëíùëùùë°‚Ñéùëñùëõùë°[ùëëùëñùëõ] The adder depth associated with each element of the input vector. ùëÄ, ùêº,ùëÜ The width, integer bits (include sign bit if presents), and sign bit of a fixed-point numbers. ùë† The bit-shift applied to an operand in a two-term subexpression. ùëèùë§ùëÄ The bitwidth of the constant matrix ùëÄ. First Stage ùë£ùëñ A vector corresponding to the i-th column of the constant matrix ùëÄ. ùë£ùëñ A vertex corresponds to the column vector ùë£ùëñ. ùë£0 The root vertex in the graph, associated with the zero vector ( 0). ùëÄ1, ùëÄ2 The two submatrices such that ùëÄ ùëÄ1ùëÄ2. Second Stage ùëÄ Overloaded to the input matrix for the second stage, either ùëÄ1 or ùëÄ2. ÀÜùëÄ The normalized version of matrix ùëÄ. ùëÄùëíùë•ùëùùëü[ùëëùëñùëõ,ùëëùëúùë¢ùë°, ùêµ] The CSD representation of the matrix ÀÜùëÄ, with values in { 1, 0, 1}. ùêµ The span of powers of the CSD digits. ùêøùëñùëöùëùùëô A list containing the implemented values. ùëé,ùëè The two input values for a two-term subexpression. Fig. 1. Overview of the proposed da4ml automatic optimization flow for CMVM on FPGAs. The algorithm first decomposes the constant matrix ùëÄinto two submatrices ùëÄ1 and ùëÄ2 using a graph-based approach that captures shared structure across columns. It then applies CSE on both submatrices to minimize redundant computations. The resulting optimized adder tree significantly reduces LUT usage and improve the latency. The proposed da4ml algorithm first decomposes the constant matrix ùëÄinto two submatrices ùëÄ1 and ùëÄ2 using a graph-based approach that captures high-level common pattern across rows. It then applies CSE on both submatrices to minimize redundant computations, as shown in Figure 1. The resulting optimized adder tree significantly reduces LUT usage and improve the latency. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 7 3.3 First stage: Graph-based Decomposition In the first stage, the algorithm decomposes the constant matrix into two submatrices, ùëÄ1 and ùëÄ2 such that ùëÄ ùëÄ1ùëÄ2. Inspired by Hcmvm, we propose this stage to exploit the high-level similarity between the different columns of the constant matrix. For this purpose, we first let each column of ùëÄ, ùëÄ[:,ùëñ] ùë£ùëñ(ùëñ [1, . . . ,ùëëùëúùë¢ùë°]) be a vertex ùë£ùëñin a graph. Then, we add the root vertex ùë£0 associated with the zero vector 0 to the graph. The distance between two vertices ùë£ùëñ and ùë£ùëóis defined as the sum of the number of non-zero digits in the vector ùë£ùëñ ùë£ùëóor ùë£ùëñ ùë£ùëó, whichever is lower. Starting from ùë£0 associated with 0, we use Prim s algorithm [31] to find an approximate Minimum Spanning Tree (MST) of the graph, subject to a maximum depth smaller than or equal to 2ùëëùëêfrom the root vertex. Each edge in the approximate MST is then translated back into a column vector in ùëÄ1 of shape [ùëëùëñùëõ,ùëëùëúùë¢ùë°], with its contributions to the original outputs (-1, 0, or 1) recorded in the second submatrix ùëÄ2 of shape [ùëëùëúùë¢ùë°,ùëëùëúùë¢ùë°]. The contribution can be determined by tracing from the root vertex to the corresponding vertex, where each edge traversed has a non-zero contribution. ùëÄ2 constructed this way is usually significantly more sparse compared to ùëÄ1. Both ùëÄ1 and ùëÄ2 are then passed to the next stage. This stage is useful for matrices with correlated columns and a high digit count. For matrices where the columns are rarely correlated, the algorithm would usually produce trivial decomposition, where ùëÄ1 is a shuffled form of ùëÄand ùëÄ2 is a shuffled identity matrix. As a realistic example would be too large for display, we show a minimal example of decomposing an matrix ùëÄof size 3 3: ùëÄ ùë£1 ùë£2 ùë£3 ¬™ 0 1 3 1 2 4 2 3 5 ¬™ (2) The graph constructed form this matrix is shown in Figure 2. In this particular example, the MST constructed is a chain of ùë£0 ùë£1 ùë£2 ùë£3, shown by the colored edges. The corresponding column vectors to these edges are recorded in ùëÄ1, and their contributions to ùë£1, ùë£2, and ùë£3 are recorded in ùëÄ2, where ùë£1 is obtained by adding the first edge, ùë£2 is obtained by adding the first two edges, and ùë£3 is obtained by adding all three edges. 3.4 Second stage: Common Subexpression Elimination In the second stage, CSE is applied independently on ùëÄ1 and ùëÄ2, and the solutions are concatenated to form the final solution. For simplicity, we denote the input matrix for the second stage as just ùëÄ. In this stage, ùëÄis again normalized into ÀÜùëÄ, which has no row or column containing all even numbers except for zeros. The algorithm then converts the input matrix to the CSD representation ùëÄùëíùë•ùëùùëü ( 1, 0, 1)ùëëùëñùëõ,ùëëùëúùë¢ùë°,ùêµ, where ùêµis the span of the powers of the CSD digits (i.e., the difference between the minimal and maximal bit-shifts associated with the CSD digits plus one). The CSE algorithm starts with the ùëÄùëíùë•ùëùùëümatrix, and a list of implemented values ùêøùëñùëöùëùùëôinitialized with the elements of the input vector: ùêøùëñùëöùëùùëô [ùë£1, ùë£2, . . . , ùë£ùëëùëñùëõ]. These two objects defines the state of the algorithm, and the algorithm updates the state iteratively until exhausting all common subexpressions. For each update step, the algorithm selects a two-term subexpression and implements it. A two-term subexpression is defined as an operation with the general form ùëé ùëè ùë†, characterized as a four-tuple: the two inputs ùëéand ùëè, the sign, and the bit-shift ùë†for the second operand2. 2The order of ùëéand ùëèis fixed by their location in the matrix in practice for the uniqueness of the four-tuple. Manuscript submitted to ACM 8 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Fig. 2. An example of the graph constructed from the constant matrix ùëÄin the first stage of the da4ml algorithm without a delay constraint. The graph is constructed with Prim s algorithm, and the MST is shown in colored edges. The root vertex ùë£0 is on the bottom left. The column vectors on the edges are the corresponding vectors that need to be added subtracted to transform between the two connected vertices. The number after the column vectors is the number of non-zero digits needed to represent the vector, and the smaller of the two is used as the edge weight. The column vectors corresponding to the edges in the MST are stored in ùëÄ1, and their contributions to ùë£1, ùë£2, and ùë£3 are recorded in ùëÄ2. Implementing a two-term subexpression consists of two steps. The algorithm first appends the result of the two-term subexpression to the list of implemented values ùêøùëñùëöùëùùëô. Then, it appends an empty row to ùëÄùëíùë•ùëùùëü. For every occurrence of the subexpression in ùëÄùëíùë•ùëùùëü, the algorithm sets the corresponding two digits to zero and sets -1 or 1 on the new row at the corresponding power. At all times, the value of √ç ùëóùë£ùëóÀÜùëÄùëóùëñcan be recovered with √ç ùëó,ùë†2ùë† ùêøùëñùëöùëùùëô[ùëó] ÀÜùëÄùëíùë•ùëùùëü[ùëó,ùëñ,ùë†]. In the implementation, we keep a hash table that caches the frequency of all two-term subexpressions to speed up the optimization. While [3, 9] show that selecting the two-term subexpression that minimally conflicts with the other subexpressions could result in lower final adder usage, this approach requires a one-step look-ahead operation when selecting the two-term common subexpression in each update step, which involves a complexity of O( ùêøùëñùëöùëùùëô 2). Due to practical considerations for larger matrices, we instead select the most common subexpression to implement for each update step, which can be performed with a complexity of O( ùêøùëñùëöùëùùëô ). As ùêøùëñùëöùëùùëô is of the same order of magnitude as the total number of non-zero digits in ùëÄùëíùë•ùëùùëü, the additional time complexity for performing the look-ahead operation would be substantial. As suggested in [3], because this only improves the final resource utilization by less than 2 as measured in adder count, we decided that the additional complexity is not worth the runtime overhead. In contrast to [15], we also take into account the operands quantized intervals (i.e., the bitwidths and shift) when choosing the subexpression for each update step. As implied by the cost function in (1), it is preferred to have the two operands with similar bitwidths and shifts. However, directly weighting the frequency of the two-term subexpressions by the total cost is not ideal: it would also count the half-adders used, which are "overheads" as they may unnecessarily increase the accumulator width downstream. Instead, we weight the frequency by the number of overlapping bits between the two operands. This weighting reduces to a constant factor when the input bitwidths are uniform and significantly larger than the constant matrix s bitwidths. We show a minimal example of the second stage in Figure 3 with a constant matrix from the H.264 integer trans- form [42]. For presentation purposes, the matrix shown is a transposed matrix (i.e., ùë¶ ùëÄ ùë•instead of ùë¶T ùë•TùëÄ). In this minimal example, we do not weight the frequency of the subexpressions by the operand bitwidths for simplicity. Also, Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 9 please note that although the example matrix contains only power-of-two numbers, the algorithm accepts matrices with arbitrary fixed-point elements. The algorithm first identifies three two-term subexpressions, shown in the bounding boxes with the same color. In the first step, the expression ùë•0 ùë•3 has the highest frequency and is implemented, and all of its occurrences are replaced. The algorithm then proceeds to implement the other two subexpressions. The adder graphs before and after optimization are shown in Figure 4, where the original adder graph requiring 12 adders on the left is reduced to 8 adders on the right. 1 1 1 1 2 1 -1 1 1 -1 -1 1 1 -2 2 1 1 1 -1 -1 -1 -2 2 1 0 0 2 -2 0 0 0 0 1 0 1 1 x0 x1 x2 x3 y0 y1 y2 y3 x0 x1 x2 x3 x0 x3 0 0 1 -1 0 0 -2 2 0 0 2 -2 0 0 0 0 1 0 1 1 x0 x1 x2 x3 x0 x3 x1 x2 1 0 -1 0 0 0 2 -2 0 0 0 0 1 0 1 1 x0 x3 x0 x3 x1 x2 x1-x2 1 0 -1 0 0 1 0 -2 Fig. 3. An example of the second stage of the da4ml algorithm. The algorithm identifies three two-term subexpressions, shown in the bounding boxes with the same color. For representation purposes, the matrix shown is a transposed matrix (i.e., ùë¶ ùëÄ ùë• instead of ùë¶T ùë•TùëÄ). The first subexpression ùë•0 ùë•3 has the highest frequency and is implemented first, followed by the other two subexpressions. Frequency weighting by operand bitwidths is not applied in this example for simplicity. In each step, the subexpressions with a colored shade are eliminated. In the last step, the all-zero columns and corresponding elements in the implemented values are omitted. -2 2 -1 1 Fig. 4. An example of the adder graphs implementing the H.264 constant matrix before and after the optimization. The original adder graph on the left requires 12 adders, while the optimized adder graph on the right only requires 8 adders. Each square node represents an adder subtractor, and the edges represent the inputs to the adder subtractor. The color of the edges indicates the sign and the power-pf-two coefficients of the inputs. The circle nodes represent the inputs to the adder graph. Manuscript submitted to ACM 10 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu 1 configuration 2 hls_config {'Model ': { 3 'Precision ': INSERT_MODEL_PRECISION , 4 'ReuseFactor ': 1, 5 'Strategy ': 'distributed_arithmetic ' 6 }} 7 8 convert , may also be from other frameworks 9 model_hls convert_from_keras_model(model , hls_config hls_config , ...) Listing 1. Enabling the DA strategy on all supported layers in hls4ml using our framework. 3.5 Complexity Analysis and Performance For complexity analysis, we denote the bitwidth of the constant matrix ùëÄas ùëèùë§ùëÄ, and the number of inputs and outputs as ùëëùëñùëõand ùëëùëúùë¢ùë°, respectively. Let ùëÅ ùëëùëñùëõ ùëëùëúùë¢ùë° ùëèùë§ùëÄ. The complexity of the algorithm is dominated by the second stage. As the first stage is a constrained variation of the Prim s algorithm, the complexity of the first stage is bounded O(ùëë2 ùëúùë¢ùë°). For the second stage, we found that the complexity is dominated by optimizing ùëÄ1, for which it is similar to optimizing ùëÄdirectly. Initializing the hash table requires a complexity of O ùëëùëúùë¢ùë° (ùëëùëñùëõ ùëèùë§ùëÄ)2 for double iteration over the input dimensions and single iteration over the output expressions. O( ùêøùëñùëöùëùùëô ) O(ùëÅ) update steps are required to reach the final solution. For each step, the complexity is O(ùëÅ) to find the most common two-term subexpression over a cached dictionary. Each frequency update also takes O(ùëÅ) time, as only single iteration over the input dimensions is required for differential updates. Hence, the overall asymptotic complexity of the algorithm is expected to be O(ùëÅ2), dominated by the iterative update steps. In practice, we found the execution time of the algorithm to be asymptotically close to O(ùëÅ2 log(ùëÅ)2) up to ùëÅ 105 (128 128 8-bit). We postulate that the logarithmic factor is due to the overhead of the hash table or memory allocation deallocation when appending rows to the ùëÄùëíùë•ùëùùëümatrix. 4 Implementation and Integration da4ml is implemented as a high-performance Python library and is open-sourced at At its core, da4ml leverages Numba [20], a just-in-time compiler for Python that compiles code to MLIR and then to a binary with LLVM [21] for performance considerations. Designed for seamless deployment, the library is tightly integrated with the widely adopted hls4ml framework [12]. Users can easily enable da4ml by setting the strategy to distributed_arithmetic for any layer requiring CMVM operations, as shown in Listing 1. No further user configuration or intervention is required. The overall workflow is shown in Figure 5. In this workflow, da4ml generates an optimized adder tree for the CMVM operation in HLS C code, which is then used as a drop-in replacement for the default CMVM implementation within the hls4ml. This drop-in integration enables significant improvements in resource efficiency and latency without sacrificing usability. Currently, the Dense, EinsumDense, and Conv1D Conv2D layers are supported for the Vivado Vitis backends, with plans for broader support underway. By bridging algorithmic innovation with production-ready toolchains, da4ml provides a scalable and practical solution for deploying high-throughput, low-latency neural networks on FPGAs. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 11 FPGA x0 x1 x2 x3 y0 y1 y2 y3 1 1da 1 1 1 1 2 1 -1 -2 1 -1 -1 1 1 -2 2 1 x0 x1 x2 x3 ml 4 CMVM Problems (or other frontend) Optimized adder trees 1 1 1 1 2 1 -1 1 1 -1-1 1 1 -2 2 1 x0x1x2x3 C bitstream Fig. 5. The workflow of using the da4ml library with the hls4ml library. The da4ml library generates the optimized adder tree for the CMVM operation in HLS C code, which is send to the hls4ml library for drop-in replacement of the default CMVM implementation. 5 Experiments In this section, we first evaluate the proposed da4ml algorithm on random matrices and compare the high-level metrics with the state-of-the-art Hcmvmalgorithm. We then evaluate the performance of the proposed algorithm on realistic neural networks consisting CMVM operations. We perform both the HLS version and hardware-description-language (HDL) version of the algorithm. The HLS version is implemented with the hls4ml library [12], and the HDL version is implemented standalone with the da4ml library. 5.1 Random Matrices We evaluate the performance of the proposed algorithm on random ùëö ùëömatrices. In this subsection, to facilitate comparison with the Hcmvmalgorithm, we adopt a convention from [3] where a ùëèùë§-bit random matrix is generated by sampling integers uniformly from [2ùëèùë§ 1 1, 2ùëèùë§ 1]. ùëëùëê 1 (no delay constraint) ùëëùëê 0 ùëëùëê 2 da4ml Hcmvm [3] da4ml Hcmvm [3] da4ml Hcmvm [3] N step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder 2 3.3 8.7 0.1 4.4 8.2 1.0e1 3.1 9.9 0.1 3.1 8.8 1.0e1 3.3 8.7 0.1 3.7 8.2 4 6.1 29.3 0.3 7.8 27.6 4.8e2 4.1 37. 0.3 4.1 32.1 4.7e2 5.9 30. 0.3 5.7 28.1 6 8.4 59. 0.6 10. 57.3 3.3e3 5. 77.8 0.8 5. 66.8 3.8e3 6.7 62.6 0.6 7. 58.2 8 9.4 98. 1.3 11.9 96.3 1.5e4 5.1 130.9 2. 5.1 117.2 1.7e4 7. 102.3 1.4 7.1 99.5 1 10.8 146.6 2.7 13.2 143.5 5.4e4 6. 195.6 4.2 6. 157.7 8.2e4 7.8 152.8 2.8 8. 146.9 12 11.6 203.6 4.8 14.6 200.4 1.7e5 6. 271.8 7.9 6. 241.6 1.7e5 8. 214.9 5.2 8. 206.8 14 12.3 269.3 8.3 15.5 264.3 4.8e5 6. 358.5 14.1 6. 324. 4.2e5 8. 279.2 8.9 8. 274.8 16 13. 343.4 13.3 16.3 338.3 1.2e6 6. 456. 22.5 6. 423.2 9.9e5 8. 358.7 14.9 8. 353.3 Table 2. Comparison of the da4ml algorithm with the Hcmvm algorithm on random matrices. In Table 2, we show the comparison of the da4ml algorithm with the Hcmvm algorithm on random matrices with 8-bits under different delay constraints. For each configuration, we report the adder depth (denoted as step ), total number of adders, and runtime on CPU. Results are shown for three delay constraint settings: no constraint, strict delay constraint (ùëëùëê 0), and moderate delay constraint (ùëëùëê 2). The CPU time is measured on a single thread of an Intel i7-13700 CPU for da4ml, and on a single thread of an Intel Xeon at 2.33 GHz for Hcmvm, as reported by the authors. The table shows the number of adders used in the optimized design of both algorithms under the same delay constraints, Manuscript submitted to ACM 12 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu and the adder depth (step) of both. When delay constraint is not exactly 0, the proposed algorithm has 2 overhead in adder counts compared to the Hcmvm algorithm. When strict delay constraint is required, the proposed algorithm has a larger overhead of 8 in adder counts. Please note that this slight trade-off in resource usage yields dramatic improvements in compilation speed. With respect to runtime, the da4ml algorithm is significantly more efficient, being O(100000) times faster than the Hcmvm algorithm with a moderate matrix size of 16 16. The difference would be more significant for larger matrices given the difference in asymptotic complexities. Although there is a generation gap between the two CPU processors, the magnitude of the observed speedup far exceeds what can be attributed solely to CPU differences, highlighting the efficiency and scalability of our framework. We further show extended runtime results for larger matrices in Figure 6 the computation time of the da4ml algorithm on random matrices with up to 128 128 elements with 8 bits, as well as the time expected from the asymptotic complexity of the algorithm. These results demostrate that the da4ml algorithm may handle reasonably complex CMVM problems that may be implemented on FPGAs unrolled in reasonable time. 0 20 40 60 80 100 120 Size of constant matrix (m) 10 3 10 1 101 103 105 Average CPU time (ms) Actual time O(m4 log2 m) Fig. 6. Computation time of the da4ml algorithm on random matrices with different sizes. The asymptotic complexity is O(ùëÅ2 log(ùëÅ)2), where ùëÅ ùëëùëñùëõ ùëëùëúùë¢ùë° ùëèùë§ùëÄ. The log(ùëÅ)2 factor was found empirically. We show the post-synthesis results of the proposed da4ml algorithm on random matrices with 8 bits and 4 bits, shown in Table 3 and Table 4, respectively. The results shown in this section are obtained after HLS synthesis with Vitis 2023.2 and out-of-context synthesis and place-and-route with the Vivado 2023.2 backend. The target FPGA is xcvu13p-flga2577-2-e. To quantify the logic delay, the designs are synthesized with a latency of one clock cycle, where the CMVM logic is a combinational logic block sandwiched between two layers of registers. For each matrix size, we use the same random constant matrix for the three delay constraints: 0, 2, and -1 (no constraint), marked as DC in the tables. The baseline for comparison is the latency-optimized implementation in hls4ml, where an unrolled double for-loop is used to implement the CMVM operation with a fixed-point multiplication-accumulation operation for HLS. The inputs for all designs are 8-bit signed integers. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 13 As shown in Table 3, the da4ml approach completely avoids using DSP blocks. In cases where the baseline also relies on LUTs for multiplication (e.g., Table 4 or the 8 8 matrix in Table 3), da4ml can consistently reduce LUT usage by approximately half. When the baseline utilizes DSPs, the LUT usage is comparable, though it can be significantly reduced by relaxing the delay constraint. For latency, the critical path of the da4ml designs can be either shorter or longer than the baseline, depending on the chosen bitwidth and delay constraint. Strategy DC Matrix Size LUT DSP FF Latency [ns] latency - 8 8 3193 0 645 2.21 DA 0 8 8 1570 0 654 1.97 DA 2 8 8 1214 0 420 2.62 DA -1 8 8 1200 0 412 3.14 latency - 16 16 4319 212 2301 3.05 DA 0 16 16 5281 0 1947 2.53 DA 2 16 16 4545 0 1618 2.66 DA -1 16 16 4321 0 1283 4.11 latency - 32 32 17666 807 4886 4.43 DA 0 32 32 18807 0 6408 3.14 DA 2 32 32 15427 0 4494 3.52 DA -1 32 32 14866 0 4643 5.71 latency - 64 64 70821 2897 18969 5.63 DA 0 64 64 66864 0 21503 4.65 DA 2 64 64 63852 0 20509 4.91 DA -1 64 64 52198 0 16214 6.49 Table 3. Resource utilization and latency of the da4ml algorithm on random matrices with different sizes and delay constraints. Strategy latency is the latency-optimized algorithm in hls4ml, and DA is the optimized implementation with the da4ml algorithm. The delay constraint is set to 0, 2, or -1, where -1 means no delay constraint. The matrix size is ùëëùëñùëõ ùëëùëúùë¢ùë°, and the bitwidth is 8. The input bitwidth is 8 in all cases. 5.2 Realistic Neural Networks We show the performance of the da4ml algorithm on realistic neural networks with CMVM operations. The networks are trained with the HGQ [10] library. It is worth noting these neural networks trained with HGQ have heterogeneous bitwidths within one CMVM operation, i.e., there is a high bit-wise sparsity in the constant matrix and the inputs. Due to the extremely heterogeneous bitwidths, da4ml s benefit is less significant than the random matrix case. Based on the results from the previous section, we found delay constraint of 2 is a good trade-off between resource utilization and latency, and we use it for all evaluations in this section. 5.2.1 High-level Feature Jet Tagging Network. We show the out-of-context results after place-and-route for the high-level feature jet tagging network [12] on the VU13P FPGA. The network is a fully-connected neural network with 4 dense layers of sizes 16 64 32 16 16 5. In Table 5 and Table 6, we show the resource utilization and latency of the network synthesized against target clock frequencies of 200 MHz and 1 GHz, respectively. Vitis 2023.2 is used for HLS synthesis, and the total latency is not specified but left to the HLS tool. All designs are fully pipelined with an II of 1 clock cycle. When the target clock frequency is 200 MHz, the designs optimized with the da4ml algorithm always meet the timing constraints, whereas hls4ml s latency-optimized designs failed to meet timing for the model with 76.9 accuracy. In all Manuscript submitted to ACM 14 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Strategy DC Matrix Size LUT DSP FF Latency [ns] latency - 8 8 1241 0 459 1.77 DA 0 8 8 885 0 375 1.70 DA 2 8 8 721 0 278 1.86 DA -1 8 8 644 0 275 2.48 latency - 16 16 4538 0 1585 1.88 DA 0 16 16 2922 0 1084 2.30 DA 2 16 16 2268 0 745 2.34 DA -1 16 16 2126 0 699 3.27 latency - 32 32 13550 0 3761 2.44 DA 0 32 32 10658 0 3721 2.78 DA 2 32 32 7452 0 2465 2.93 DA -1 32 32 7339 0 2442 3.62 latency - 64 64 47274 0 14652 3.10 DA 0 64 64 38205 0 11711 3.37 DA 2 64 64 26715 0 6026 4.18 DA -1 64 64 25493 0 8279 5.09 Table 4. Resource utilization and latency of the da4ml algorithm on random matrices with different sizes and delay constraints. Strategy latency is the latency-optimized algorithm in hls4ml, and DA is the optimized implementation with the da4ml algorithm. The delay constraint is set to 0, 2, or -1, where -1 means no delay constraint. The matrix size is ùëëùëñùëõ ùëëùëúùë¢ùë°, and the bitwidth is 4. The input bitwidth is 8 in all cases. cases, da4ml reduces LUT consumption by 10 for this network. To obtain the maximum frequency, we resynthesized the designs with a target clock frequency of 1 GHz. Under this setting, the designs optimized with the da4ml algorithm are pipelined with fewer stages and achieve Fmax values similar to or higher than the baseline hls4ml designs. As more registers are inserted, LUT fusion and sharing are less effective, and the LUT utilization is higher than for the designs with a 200 MHz target clock frequency. The DSP utilization is reduced to 0 in all cases with da4ml. Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] Latency 76.9 4 (21.6 ns) 13,258 55 1,497 185.5 DA 4 (18.9 ns) 12,250 0 1,502 211.5 Latency 76.6 3 (14.7 ns) 7,502 27 900 203.6 DA 3 (13.3 ns) 6,869 0 966 226.0 Latency 76.5 3 (14.3 ns) 6,690 30 845 210.2 DA 3 (13.1 ns) 6,005 0 871 229.6 Latency 76.3 3 (14.0 ns) 5,209 15 799 215.0 DA 3 (12.8 ns) 4,728 0 786 234.1 Latency 76.0 3 (13.4 ns) 3,498 22 639 223.2 DA 3 (12.3 ns) 3,308 0 661 244.8 Latency 75.9 3 (13.5 ns) 3,043 15 621 221.5 DA 3 (12.8 ns) 2,878 0 601 234.0 Table 5. Resource utilization and latency of the high-level feature jet tagging network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 15 Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] Latency 76.9 42 (57.6 ns) 16,081 57 26,484 729.4 DA 31 (44.1 ns) 12,682 0 19,056 702.2 Latency 76.6 32 (46.1 ns) 9,347 28 15,216 694.0 DA 26 (37.4 ns) 7,392 0 11,462 695.9 Latency 76.5 35 (67.2 ns) 8,548 30 14,418 520.8 DA 25 (35.4 ns) 6,448 0 10,109 707.2 Latency 76.3 33 (51.8 ns) 6,667 15 11,184 637.3 DA 22 (30.2 ns) 5,019 0 7,682 729.4 Latency 76.0 33 (48.3 ns) 4,471 22 7,279 683.5 DA 23 (32.6 ns) 3,602 0 5,693 706.2 Latency 75.9 31 (42.8 ns) 4,005 17 6,305 723.6 DA 21 (24.9 ns) 2,908 0 4,720 844.6 Table 6. Resource utilization and latency of the high-level feature jet tagging network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 1 GHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown. 5.2.2 SVHN Classification Network. We show the out-of-context results after place route for the SVHN classification network [12] on the VU9P FPGA. The network is a LeNet-like [22] convolutional network with dense classification head from [2], and the architecture is shown in Figure 7. The trained networks are taken from [10]. In contrast to other networks shown in this work, resource takes place in these networks for the convolution filters applying to different inputs The designs are synthesized with Vivado HLS 2020.1 due to inconsistent behaviors of the DATAFLOW pragma used in hls4ml. More details for these networks can be found in [10]. The target clock frequency is set to 200 MHz, and the delay constraint is set to 2 for each CMVM operation. The results are shown in Table 7. From the table, it can be seen that the designs optimized with da4ml can reduce the LUT utilization by 1 4, while removing all DSPs. No timing violations are observed in all cases. Conv2D 16 filter: 3,3,3 Pixels 32,32,3 30,30,16 ReLU 30,30,16 MaxPool [2,2] 15,15,16 Dense kernel: 42,96 bias: 42 ReLU 42 42 Conv2D 16 filter: 3,3,16 13,13,16 ReLU 13,13,16 MaxPool [2,2] 6,6,16 Conv2D 24 filter: 3,3,16 4,4,24 ReLU 4,4,24 MaxPool [2,2] 2,2,24 Flatten 96 Dense kernel: 64,42 bias: 64 ReLU 64 64 Dense kernel: 10,64 bias: 10 Probability Logits 10 Fig. 7. The architecture of the SVHN classification network [12]. The network is a LeNet-like [22] network from [2]. The trained networks are taken from [10]. Manuscript submitted to ACM 16 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] Latency 93.9 1,050 (5,058.9 ns) 69,407 58 27,853 207.6 1,029 DA 1,045 (5,159.2 ns) 53,425 0 20,048 202.6 1,029 Latency 93.1 1,061 (5,026.0 ns) 47,314 30 20,582 211.1 1,029 DA 1,045 (4,952.3 ns) 36,739 0 15,491 211.0 1,029 Latency 91.9 1,058 (5,211.7 ns) 40,032 15 18,087 203.0 1,029 DA 1,045 (5,020.2 ns) 31,267 0 14,293 208.2 1,029 Latency 90.8 1,059 (4,868.2 ns) 34,435 13 17,261 217.5 1,029 DA 1,045 (5,041.1 ns) 28,078 0 13,038 207.3 1,029 Latency 89.9 1,056 (4,819.6 ns) 30,766 10 15,205 219.1 1,029 DA 1,045 (4,993.0 ns) 24,947 0 12,261 209.3 1,029 Latency 88.8 1,056 (5,120.5 ns) 27,982 6 14,736 206.2 1,029 DA 1,045 (5,134.1 ns) 23,609 0 12,021 203.5 1,029 Table 7. Resource utilization and latency of the SVHN classification network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu9p-flga2104-2L-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. 5.2.3 Muon Tracking Network. The results after place-and-route for the Muon Tracking network [12] on the VU13P FPGA are shown in Table 8. The network is a multi-stage network with mainly dense layers, and the architecture is shown in Figure 8. The trained networks are taken from [10], and all designs are synthesized with Vitis HLS 2023.2 and Vivado 2023.2. The target clock frequency is set to 160 MHz to match the previous work [36], and the delay constraint is set to 2 for each CMVM operation optimized. As the inputs of this network are all 1-bit, we do not apply da4ml to the initial convolutional layers, as they can be efficiently implemented using conditional accumulation logic. All implementations shown have an II of 1 clock cycle. Compared to the latency-optimized baseline from hls4ml, the da4ml-optimized designs not only reduce LUT utilization by approximately 10 but also entirely eliminate DSP usage. All designs met timing, while the latency- optimized designs in hls4ml are pipelined with more stages and have higher latency. Masked Dense kernel: 50,50 tanh Add Conv1D 1 filter: 3,3 Conv1D 1 filter: 3,2 Conv1D 1 filter: 3,2 tanh Masked Dense kernel: 50,50 Masked Dense kernel: 50,50 ReLU Add tanh Dense kernel: 28,50 bias: 28 ReLU Dense kernel: 14,28 bias: 14 ReLU Dense kernel: 8,14 bias: 8 Dense kernel: 1,8 bias: 1 Feature Extraction Dense Net M1 Input 50,3 M2 Input 50,2 M3 Input 50,2 28 28 14 14 8 8 Œ∏ Output 1 Fig. 8. The architecture of the Muon Tracking network [36]. The network is a multi-stage network with mainly dense layers. The masked dense layers refers to dense layers with specific sparsity pattern enforced on the weights, and please refer to [36] for the detailed implementation. The trained networks are taken from [10]. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 17 Strategy Resolution Latency [cycles] LUT DSP FF Fmax [MHz] Latency 1.95 11 (67.5 ns) 39,413 522 6,043 162.9 DA 9 (55.4 ns) 37,125 0 5,547 162.5 Latency 2.00 11 (66.0 ns) 34,460 154 5,263 166.6 DA 9 (56.0 ns) 27,832 0 3,147 160.8 Latency 2.09 12 (71.4 ns) 24,941 68 4,677 168.2 DA 8 (47.2 ns) 21,778 0 3,697 169.5 Latency 2.20 13 (75.9 ns) 21,557 41 4,699 171.3 DA 8 (46.6 ns) 18,895 0 3,105 171.6 Latency 2.39 10 (57.3 ns) 16,918 27 2,484 174.5 DA 8 (47.0 ns) 14,735 0 2,215 170.2 Latency 2.63 12 (60.6 ns) 13,306 10 3,429 197.9 DA 8 (44.0 ns) 12,318 0 2,166 181.9 Table 8. Resource utilization and latency of the Muon tracking network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 160 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown. 5.2.4 Particle-based Jet Tagging Network. The particle-based jet tagging network [37] is an MLP Mixer based neural network for jet tagging. We adopt the architecture from [37] with the 64-particle input, and 16-features per particle. The network is trained with the HGQ library [10]. The architecture is shown in Figure 9. For this neural network, Vitis HLS 2023.2 cannot produce designs with II of 1 clock cycle when using the original hls4ml implementation. We suspect the issue is related to the multidimensional linear operation implemented with Einstein Summation templates, and some pragma usage used may be inappropriate. However, we do not investigate this issue further as it is beyond the scope of this work. The results are shown in Table 9. The designs are synthesized with Vitis HLS 2023.2 and Vivado 2023.2, with a target clock frequency set to 200 MHz and a target II of 1. The delay constraint is set to 2 for each optimized CMVM operation. Marginal resource savings are observed for these designs for LUTs. However, the designs without da4ml failed to be fully pipelined with an II of 1, and the achieved Fmax is significantly lower than the target 200 MHz clock frequency. Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] Latency 81.4 35 (319.6 ns) 137,893 79 24,785 109.5 12 DA 13 (62.6 ns) 125,694 0 26,097 207.6 1 Latency 81.0 29 (201.0 ns) 72,299 81 13,510 144.3 5 DA 13 (63.8 ns) 68,157 0 13,147 203.9 1 Latency 80.3 29 (182.3 ns) 56,253 80 11,771 159.1 5 DA 12 (56.9 ns) 50,657 0 11,411 210.9 1 Latency 77.3 12 (69.6 ns) 25,828 61 5,662 172.5 2 DA 11 (53.4 ns) 25,002 0 5,554 205.8 1 Table 9. Resource utilization and latency of the particle-based jet tagging network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. Manuscript submitted to ACM 18 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu MLP1 MLP1 MLP1 MLP1 MLP2 MLP2 MLP2 MLP2 N particles MLP1 (MLP3) DenseBn n 16 ReLU DenseBn 16 n ReLU DenseBn N 1 ReLU DenseBn N N ReLU MLP4 MLP2 MLP4 MLP4 MLP4 MLP4 MLP3 MLP3 MLP3 MLP3 HEAD 5 Classes DenseBn 16 16 ReLU DenseBn 16 16 ReLU HEAD DenseBn 16 16 ReLU DenseBn 16 5 n feature Fig. 9. The architecture of the particle-based jet tagging network [37]. The network is an MLP Mixer based neural network for jet tagging. The architecture is adopted from [37] with the 64-particle input, and 16-features per particle (ùëÅ 64 and ùëõ 16). The model consists of four MLP blocks with a single skip-connection. The implementation of each MLP and the classification head is shown in the corresponding dashed blocks. MLP1 and MLP3 act on the feature dimension; MLP2 and MLP4 act on the particle dimension. DenseBn represents a dense layer followed by a batch normalization layer during training, which are fused into a single layer during inference. FPGA x0 x1 x2 x3 y0 y1 y2 y3 1 1da 1 1 1 1 2 1 -1 -2 1 -1 -1 1 1 -2 2 1 x0 x1 x2 x3 ml 4 Manual deÔ¨Ånition Verilog out Einsum(inp, w0) out ReLU(out) out Quantize(out) out Einsum(inp, w1) ... bitstream Fig. 10. The workflow of the standalone workflow with da4ml. The user defines the network in a functional style, and the code generation is performed with da4ml to generate synthesizable, pipelined Verilog code. 5.3 Standalone Code Generation Other than the main workflow introduced in Section 4, da4ml can also work standalone to generate Verilog designs for a subset of neural networks, allowing for easier integration into existing RTL workflows. Currently, the generated designs must be either fully pipelined with an II of 1 or combinational. In addition, da4ml only supports networks with generalized CMVM operations (i.e., including convolution and Einstein Summation between constants and inputs), arithmetic operations excluding division, and rectified linear unit (ReLU) activations for standalone code generation. In the current implementation, manually defining the network in a functional style is required, and the workflow is shown in Figure 10. Pipelining of the generated design is performed by layers of adders, in which each adder is assumed to have a constant latency. While this approximation is not accurate, we use it to simplify the pipelining process, as we notice that routing delay dominates the latency in most cases in our experiments and more complex modeling of the adders latency does not yield significant improvements. We also reserve an API for the user to specify the latency of each adder depending on the bitwidths of the operands. We show the performance of the high-level feature jet tagging network with 200 MHz and 1 GHz target clock frequencies in Table 10 and Table 11, and the results for the particle-based jet tagging network with MLP Mixer in Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 19 Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] hls4ml DA 76.9 4 (18.9 ns) 12,250 0 1,502 211.5 da4ml 4 (20.0 ns) 12,032 0 3,113 200.1 hls4ml DA 76.6 3 (13.3 ns) 6,869 0 966 226.0 da4ml 4 (19.5 ns) 6,153 0 1,775 205.4 hls4ml DA 76.5 3 (13.1 ns) 6,005 0 871 229.6 da4ml 4 (19.8 ns) 5,613 0 1,539 202.0 hls4ml DA 76.3 3 (12.8 ns) 4,728 0 786 234.1 da4ml 3 (14.8 ns) 4,129 0 903 202.4 hls4ml DA 76.0 3 (12.3 ns) 3,308 0 661 244.8 da4ml 3 (14.9 ns) 2,814 0 628 201.0 hls4ml DA 75.9 3 (12.8 ns) 2,878 0 601 234.0 da4ml 3 (14.7 ns) 2,485 0 683 204.2 Table 10. Resource utilization and latency of the high-level feature jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The Verilog designs are pipelined every 5 adders. II is 1 for all designs shown. Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] hls4ml DA 76.9 31 (44.1 ns) 12,682 0 19,056 702.2 da4ml 20 (34.0 ns) 12,298 0 13,664 588.6 hls4ml DA 76.6 26 (37.4 ns) 7,392 0 11,462 695.9 da4ml 16 (27.5 ns) 6,944 0 7,419 582.4 hls4ml DA 76.5 25 (35.4 ns) 6,448 0 10,109 707.2 da4ml 17 (23.1 ns) 6,165 0 7,207 735.8 hls4ml DA 76.3 22 (30.2 ns) 5,019 0 7,682 729.4 da4ml 15 (20.8 ns) 4,655 0 5,258 722.0 hls4ml DA 76.0 23 (32.6 ns) 3,602 0 5,693 706.2 da4ml 14 (20.6 ns) 3,127 0 3,587 681.2 hls4ml DA 75.9 21 (24.9 ns) 2,908 0 4,720 844.6 da4ml 13 (19.0 ns) 2,734 0 3,139 685.4 Table 11. Resource utilization and latency of the high-level feature jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively. The FPGA part is xcvu13p-flga2577-2-e with 1 GHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The Verilog designs are pipelined every adder. II is 1 for all designs shown. Table 12, respectively. The designs generated with hls4ml are marked with hls4ml DA , and the designs generated directly by da4ml with Verilog are marked with da4ml . All designs are pipelined with an II of 1, and the delay constraint is set to 2 for each CMVM operation. For the standalone generated Verilog designs, we pipeline every 5 adders for the 200 MHz target clock frequency and every adder for the 1 GHz target clock frequency. Global retiming is enabled in Vivado for the Verilog designs. The results show that the designs generated with da4ml can achieve lower LUT consumption by 5 20 compared to the designs generated in conjunction with hls4ml and synthesized with Vitis HLS. However, the reached Fmax is generally slightly lower. In particular, it can be noticed that the designs generated in conjunction with hls4ml are Manuscript submitted to ACM 20 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] hls4ml DA 81.4 13 (62.6 ns) 125,694 0 26,097 207.6 da4ml 11 (54.8 ns) 120,512 0 28,284 200.8 hls4ml DA 81.0 13 (63.8 ns) 68,157 0 13,147 203.9 da4ml 11 (54.8 ns) 61,771 0 16,031 200.6 hls4ml DA 80.3 12 (56.9 ns) 50,657 0 11,411 210.9 da4ml 10 (48.4 ns) 47,778 0 12,515 206.8 hls4ml DA 77.3 11 (53.4 ns) 25,002 0 5,554 205.8 da4ml 9 (41.9 ns) 24,285 0 6,262 214.8 Table 12. Resource utilization and latency of the particle-based jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml for each CMVM operation. The Verilog designs are pipelined every 5 adders. II is 1 for all designs shown. pipelined with more stages than the adder graph depth, which suggests that the HLS compiler is further breaking down the graph for timing optimization, a process not performed in the standalone code generation. We suspect this is the reason for the lower Fmax in the designs generated with da4ml. It is worth noting that designs generated with da4ml directly can significantly reduce compilation time: for designs with a large amount of code, HLS can take a very long time. For instance, the synthesis time for the particle-based jet tagging networks3 with Vitis HLS and Vivado 2023.2 is around 17 hours without memory bottleneck when optimized with da4ml. However, the designs of the same networks generated directly in Verilog with da4ml only take around 26 minutes to synthesize with Vivado 2023.2. The peak memory usage also significantly reduced from 70 GB to 10 GB. These results suggest that da4ml s standalone code generation can facilitate fast prototyping and allowing for faster iterations in software-hardware co-design. 5.4 Comparison to Other Methods While this work focuses on distributed arithmetic optimization for CMVM operations in FPGA-based neural networks, alternative methods exist for implementing machine learning-based models on FPGAs. In this section, we briefly compare neural networks optimized with da4ml to other methods. We show the comparasion in Table 13, where we compare the performance of the networks shown in earlier sections against other state-of-the-art methods. It is important to differentiate our approach from methods such as TreeLUT [18], NeuraLUT-Assemble [5], and DWN [7]. While da4ml optimizes the implementation of quantized traditional neural networks, these methods re- formulates the entire model into inherently hardware-native structures. In particular, TreeLUT defines the model as collection of quantized decision trees, while NeuraLUT-Assemble and DWN construct models directly mapped into LUTs with particular fan-in. These methods are highly specialized and may achieve exceptional efficiency in terms of resource utilization and throughput, while they often come at the cost of reduced generalizability and slight accuracy degradation. In contrast, da4ml focuses on optimizing the core CMVM operations of quantized, conventional neural networks, which would enable more straightforward integration with existing machine learning workflows and is applicable on more complex tasks. In particular, we show that when the network is properly quantized with HGQ [10], 3All-inclusive time, until routing finishes Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 21 High-level feature jet tagging network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 76.9 31 (44.1 ns) 12,682 0 19,056 702.2 1 This Work: HGQ da4ml (RTL) 76.5 17 (23.1 ns) 6,165 0 7,207 735.8 1 This Work: HGQ hls4ml 76.9 42 (57.6 ns) 16,081 57 26,484 729.4 1 This Work: HGQ hls4ml 76.5 35 (67.2 ns) 8,548 30 14,418 520.8 1 QKeras hls4ml [34] 76.3 15 (105 ns) 5,504 175 3,036 142.9 2 DWN [7] 76.3 10 (14.4 ns) 6,302 0 4,128 695. 1 MetaML-Pro [32] 76.1 10 (50 ns) 13,042 70 N A 200 1 NeuraLUT-Assemble [5] 76.0 2 (2.1 ns) 1,780 0 540 940. 1 TreeLUT [18] 75.6 2 (2.7 ns) 2,234 0 347 735. 1 Muon tracking network Implementation Resolution [mrad] Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 1.95 9 (55.4 ns) 37,125 0 5,547 162.5 1 HGQ hls4ml [10] 1.95 11 (67.5 ns) 39,413 522 6,043 162.9 1 QKeras hls4ml [36] 1.95 17 (106.3 ns) 37,867 1,762 8,443 160 1 SVHN classification network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 93.9 1,045 (5,159.2 ns) 53,425 0 20,048 202.6 1,029 HGQ hls4ml [10] 93.9 1,050 (5,058.9 ns) 69,407 58 27,853 207.6 1,029 QKeras hls4ml [2] 94. 1,035 (5,175 ns) 111,152 174 32,554 200 1,030 QKeras hls4ml [34] 92.4 5,447 (43,576 ns) 59,279 1,215 46,584 125 N A Particle-based jet tagging network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (RTL) 81.4 11 (54.8 ns) 120,512 0 28,284 200.8 1 LL-GNN [33] 81.2 181 (905 ns) 815k 8,986 189k 200 150 QKeras hls4ml, DS [29] 75.9 26 (130 ns) 903,284 434 358,754 200 2 QKeras hls4ml, GNN [29] 75.8 32 (160 ns) 1,162,104 2,120 761,061 200 3 Table 13. Resource and performance of various neural network tasks with different implementations. When the neural network is quantized with HGQ [10], we show that traditional neural networks may achieve competitive resource utilization, throughput, and superior accuracy compared with the methods designed specifically for FPGAs. All works are using either xcvu13p-flga2577-2-e or xcvu9p-flgb2104-2-i, where the results are comparable. da4ml (HLS) refers to using da4ml with the hls4ml workflow, while da4ml (RTL) refers to the standalone workflow. Works marked with report the results after logic synthesis without place and route. We put the target clock frequency for Fmax and used it to compute the latency for these works. DWN results are cited from [5] instead, as the original paper omits preprocessing steps for the firmware implementation, which are added in [5] for a fair comparison. Works marked with reports resource utilization after place and routes, but did not report Fmax. We use the achieved target clock as Fmax for these works. traditional neural networks can achieve competitive resource utilization throughput - accuracy tradeoff compared to those methods highly co-designed for FPGAs. Comparing to other conventional neural network based methods shown in Table 13, our proposed da4ml framework demonstrates significant advantage across diverse workloads, offering a favorable balance between accuracy, latency, and hardware efficiency. Notably, our designs consistently eliminate DSP usage while maintaining competitive or superior accuracy, making them suited for resource-constrained FPGA designs. Similarly, in the muon tracking and SVHN classification tasks, our method matches the accuracy of existing approaches while reducing LUT and flip-flop utilization. Furthermore, the particle-based jet tagging design achieves the highest accuracy (81.4 ) while consuming more than 10 fewer LUTs and FFs than state-of-the-art GNN-based implementations. Across all tasks, our designs Manuscript submitted to ACM 22 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu maintain a consistent II of 1 cycle, ensuring high-throughput operation. These results underscore the generalizability and practicality of our approach for low-latency, resource-efficient FPGA deployment in both traditional and physics-inspired neural networks. 6 Conclusion In this work, we present an efficient algorithm for optimizing CMVM operations with distributed arithmetic for FPGA- based neural networks. We further present the da4ml library, which implements the algorithm efficiently and tightly integrates it with the hls4ml library for easy adoption. We then demonstrate the performance of the algorithm on a set of neural networks, including dense, convolutional, and MLP-Mixer-based networks. The results show that designs optimized with da4ml can simultaneously reduce resource utilization and timing on realistic, heavily quantized, and sparse neural networks for physics applications. We also show that for a limited set of networks, da4ml can generate Verilog designs directly for fast prototyping and software-hardware co-design. The results show that designs generated directly with da4ml could further reduce resource utilization and synthesis time significantly at the expense of a slightly lower Fmax. We hope this work will facilitate the design of efficient neural networks for FPGAs, especially in the context of physics applications, and provide a solid foundation for future work on hardware-aware neural network design. 7 Acknowledgments Chang Sun and Maria Spiropulu are partially supported by the U.S. Department of Energy (DOE), Office of Science, Office of High Energy Physics grant DE-SC0011925. Chang Sun is partially supported for computing resources by the NSF ACCESS Grant number PHY240298. Chang Sun and Maria Spiropulu acknowledge partial support by the DOE AI4HEP award. Zhiqiang Que and Wayne Luk are supported by the United Kingdom EPSRC (grant numbers UKRI256, EP V028251 1, EP N031768 1, EP S030069 1, and EP X036006 1). Vladimir Loncar is supported by the Eric Wendy Schmidt Fund for Strategic Innovation through the CERN Next Generation Triggers project under grant agreement number SIF-2023-004. References [1] 2024. 2024 Data Collected with AXOL1TL Anomaly Detection at the CMS Level-1 Trigger. (2024). [2] Thea Aarrestad, Vladimir Loncar, Nicol√≤ Ghielmetti, Maurizio Pierini, Sioni Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus Linander, Yutaro Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris, Dylan Rankin, Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward Kreinar, Zhenbin Wu, and Duc Hoang. 2021. Fast convolutional neural networks on FPGAs with hls4ml. Machine Learning: Science and Technology 2, 4 (jul 2021), 045015. [3] Levent Aksoy, Eduardo da Costa, Paulo Flores, and Jos√© Monteiro. 2012. Multiplierless Design of Linear DSP Transforms. In VLSI-SoC: Advanced Research for Systems on Chip, Salvador Mir, Chi-Ying Tsui, Ricardo Reis, and Oliver C. S. Choy (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 73 93. [4] Levent Aksoy, Paulo Flores, and Jos√© Monteiro. 2015. A Novel Method for the Approximation of Multiplierless Constant Matrix Vector Multiplication. In 2015 IEEE 13th International Conference on Embedded and Ubiquitous Computing. 98 105. [5] Marta Andronic and George A. Constantinides. 2025. NeuraLUT-Assemble: Hardware-Aware Assembling of Sub-Neural Networks for Efficient LUT Inference. In 2025 IEEE 33rd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). 208 216. https: doi.org 10.1109 FCCM62733.2025.00077 [6] Algirdas Avizienis. 1961. Signed-Digit Numbe Representations for Fast Parallel Arithmetic. IRE Transactions on Electronic Computers EC-10, 3 (1961), 389 400. [7] Alan Tendler Leibel Bacellar, Zachary Susskind, Mauricio Breternitz Jr, Eugene John, Lizy Kurian John, Priscila Machado Vieira Lima, and Felipe M.G. Fran√ßa. 2024. Differentiable Weightless Neural Networks. In Proceedings of the 41st International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR, 2277 2295. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 23 [8] D. Benyamin, W. Luk, and J. Villasenor. 1999. Optimizing FPGA-based vector product designs. In Seventh Annual IEEE Symposium on Field- Programmable Custom Computing Machines (Cat. No.PR00375). 188 197. [9] N. Boullis and A. Tisserand. 2005. Some optimizations of hardware multiplication by constant matrices. IEEE Trans. Comput. 54, 10 (2005), 1271 1282. [10] Sun Chang, Thea √Örrestad, Vladimir Lonƒçar, Jennifer Ngadiuba, and Maria Spiropulu. 2024. Gradient-based Automatic Per-Weight Mixed Precision Quantization for Neural Networks On-Chip. [11] Claudionor N. Coelho, Aki Kuusela, Shan Li, Hao Zhuang, Jennifer Ngadiuba, Thea Klaeboe Aarrestad, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, and Sioni Summers. 2021. Automatic heterogeneous quantization of deep neural networks for low-latency inference on the edge for particle detectors. Nature Machine Intelligence 3, 8 (jun 2021), 675 686. [12] Farah Fahim, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, Giuseppe Di Guglielmo, Philip C. Harris, Jeffrey D. Krupa, Dylan Rankin, Manuel Blanco Valentin, Josiah Hester, Yingyi Luo, John Mamish, Seda Orgrenci-Memik, Thea Aarrestad, Hamza Javed, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, Sioni Summers, Javier M. Duarte, Scott Hauck, Shih-Chieh Hsu, Jennifer Ngadiuba, Mia Liu, Duc Hoang, Edward Kreinar, and Zhenbin Wu. 2021. hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices. CoRR abs 2103.05579 (2021). arXiv:2103.05579 [13] Ekaterina Govorkova, Ema Puljak, Thea Aarrestad, Thomas James, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, Nicol√≤ Ghielmetti, Maksymilian Graczyk, Sioni Summers, Jennifer Ngadiuba, Thong Q. Nguyen, Javier Duarte, and Zhenbin Wu. 2021. Autoencoders on FPGAs for real-time, unsupervised new physics detection at 40 MHz at the Large Hadron Collider. [14] Ekaterina Govorkova, Ema Puljak, Thea Aarrestad, Thomas James, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, Nicol√≤ Ghielmetti, Maksymilian Graczyk, Sioni Summers, Jennifer Ngadiuba, Thong Q. Nguyen, Javier Duarte, and Zhenbin Wu. 2022. Autoencoders on field-programmable gate arrays for real-time, unsupervised new physics detection at 40 MHz at the Large Hadron Collider. Nature Machine Intelligence 4, 2 (feb 2022), 154 161. [15] Anup Hosangadi, Farzan Fallah, and Ryan Kastner. 2005. Reducing hardware complexity of linear DSP systems by iteratively eliminating two-term common subexpressions. In Proceedings of the 2005 Asia and South Pacific Design Automation Conference (Shanghai, China) (ASP-DAC 05). Association for Computing Machinery, New York, NY, USA, 523 528. [16] Anup Hosangadi, Farzan Fallah, and Ryan Kastner. 2005. Simultaneous Optimization of Delay and Number of Operations in Multiplierless Implementation of Linear Systems. International Workshop on Logic and Synthesis (IWLS) (2005). [17] Kai Huang and Wei Gao. 2022. Real-time neural network inference on extremely weak devices: agile offloading with explainable AI. In Proceedings of the 28th Annual International Conference on Mobile Computing And Networking (Sydney, NSW, Australia) (MobiCom 22). Association for Computing Machinery, New York, NY, USA, 200 213. [18] Alireza Khataei and Kia Bazargan. 2025. TreeLUT: An Efficient Alternative to Deep Neural Networks for Inference Acceleration Using Gradient Boosted Decision Trees. In Proceedings of the 2025 ACM SIGDA International Symposium on Field Programmable Gate Arrays (FPGA 25). ACM, 14 24. [19] Martin Kumm, Martin Hardieck, and Peter Zipf. 2017. Optimization of Constant Matrix Multiplication with Low Power and High Throughput. IEEE Trans. Comput. 66, 12 (2017), 2072 2080. [20] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. 2015. Numba: A llvm-based python jit compiler. In Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC. 1 6. [21] Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for Lifelong Program Analysis Transformation. In Proceedings of the International Symposium on Code Generation and Optimization: Feedback-Directed and Runtime Optimization (Palo Alto, California) (CGO 04). IEEE Computer Society, USA, 75. [22] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. 1989. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation 1 (1989), 541 551. 41312633 [23] Alexander Lehnert, Philipp Holzinger, Simon Pfenning, Ralf M√ºller, and Marc Reichenbach. 2023. Most Resource Efficient Matrix Vector Multiplication on FPGAs. IEEE Access 11 (2023), 3881 3898. [24] Ying Li, Chungan Peng, Dunshan Yu, and Xing Zhang. 2008. The implementation methods of high speed FIR filter on FPGA. In 2008 9th International Conference on Solid-State and Integrated-Circuit Technology. 2216 2219. [25] Ying Li, Chungan Peng, Dunshan Yu, and Xing Zhang. 2008. The implementation methods of high speed FIR filter on FPGA. In 2008 9th International Conference on Solid-State and Integrated-Circuit Technology. 2216 2219. [26] Songlin Lyu, Jiawen Cheng, Yun Shao, Yong Xiao, and Wenjian Yu. 2022. Multi-Constant Multiplication Optimization Based on Common Sub-Expression Elimination. In 2022 IEEE 16th International Conference on Solid-State Integrated Circuit Technology (ICSICT). 1 3. https: doi.org 10.1109 ICSICT55466.2022.9963464 [27] Shahnam Mirzaei, Anup Hosangadi, and Ryan Kastner. 2006. FPGA Implementation of High Speed FIR Filters Using Add and Shift Method. In 2006 International Conference on Computer Design. 308 313. [28] Wei Niu, Zhengang Li, Xiaolong Ma, Peiyan Dong, Gang Zhou, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin Ren. 2022. GRIM: A General, Real-Time Deep Learning Inference Framework for Mobile Devices Based on Fine-Grained Structured Weight Sparsity. IEEE Trans. Pattern Anal. Mach. Intell. 44, 10_Part_1 (oct 2022), 6224 6239. Manuscript submitted to ACM 24 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu [29] Patrick Odagiu, Zhiqiang Que, Javier Duarte, Johannes Haller, Gregor Kasieczka, Artur Lobanov, Vladimir Loncar, Wayne Luk, Jennifer Ngadiuba, Maurizio Pierini, Philipp Rincke, Arpita Seksaria, Sioni Summers, Andre Sznajder, Alexander Tapper, and Thea K √Örrestad. 2024. Ultrafast jet classification at the HL-LHC. Machine Learning: Science and Technology 5, 3 (July 2024), 035017. [30] M. Potkonjak, M.B. Srivastava, and A.P. Chandrakasan. 1996. Multiple constant multiplications: efficient and versatile framework and algorithms for exploring common subexpression elimination. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 15, 2 (1996), 151 165. [31] Robert Clay Prim. 1957. Shortest connection networks and some generalizations. The Bell System Technical Journal 36, 6 (1957), 1389 1401. [32] Zhiqiang Que, Jose G. F. Coutinho, Ce Guo, Hongxiang Fan, and Wayne Luk. 2025. MetaML-Pro: Cross-Stage Design Flow Automation for Efficient Deep Learning Acceleration. arXiv:2502.05850 [cs.AR] [33] Zhiqiang Que, Hongxiang Fan, Marcus Loo, He Li, Michaela Blott, Maurizio Pierini, Alexander Tapper, and Wayne Luk. 2024. LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics. ACM Transactions on Embedded Computing Systems 23, 2 (March 2024), 1 28. [34] Benjamin Ramhorst, George A. Constantinides, and Vladimir Loncar. 2023. FPGA Resource-aware Structured Pruning for Real-Time Neural Networks. arXiv:2308.05170v1 [cs.AR] [35] Raghubir Singh and Sukhpal Singh Gill. 2023. Edge AI: A survey. Internet of Things and Cyber-Physical Systems 3 (2023), 71 92. 1016 j.iotcps.2023.02.004 [36] Chang Sun, Takumi Nakajima, Yuki Mitsumori, Yasuyuki Horii, and Makoto Tomoto. 2023. Fast muon tracking with machine learning implemented in FPGA. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 1045 (Jan. 2023), 167546. [37] Chang Sun, Jennifer Ngadiuba, Maurizio Pierini, and Maria Spiropulu. 2025. Fast Jet Tagging with MLP-Mixers on FPGAs. (2025). arXiv:2503.03103 [physics.ins-det] [38] The ATLAS Collaboration. 2017. Technical Design Report for the Phase-II Upgrade of the ATLAS TDAQ System. Technical Report. CERN, Geneva. [39] The CMS Collaboration. 2020. The Phase-2 Upgrade of the CMS Level-1 Trigger. Technical Report. CERN, Geneva. Final version. [40] The LHC Study Group. 1995. The Large Hadron Collider, Conceptual Design. Technical Report. CERN AC 95-05 (LHC) Geneva. [41] Yevgen Voronenko and Markus P√ºschel. 2007. Multiplierless multiple constant multiplication. ACM Trans. Algorithms 3, 2 (May 2007), 11 es. [42] T. Wiegand, G.J. Sullivan, G. Bjontegaard, and A. Luthra. 2003. Overview of the H.264 AVC video coding standard. IEEE Transactions on Circuits and Systems for Video Technology 13, 7 (2003), 560 576. [43] Yang Yang, Yury Kartynnik, Pen Li, Jiuqiang Tang, Xing Li, George Sung, and Matthias Grundmann. 2024. StreamVC: Real-Time Low-Latency Voice Conversion. [44] Aymen-Alaeddine Zeghaida, Dinesh Daultani, J.M. Pierre Langlois, and Jean Pierre David. 2024. Scalable Low-Complexity Implementation of Constant Matrix Multiplication Circuits. In 2024 IEEE 67th International Midwest Symposium on Circuits and Systems (MWSCAS). 357 361. [45] I. Zurbano Fernandez et al. 2020. High-Luminosity Large Hadron Collider (HL-LHC): Technical design report. CERN Yellow Reports: Monographs 10 2020 (12 2020). Manuscript submitted to ACM\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2507.04535v1 [cs.AR] 6 Jul 2025 da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs CHANG SUN, California Institute of Technology, USA ZHIQIANG QUE, Imperial College London, UK VLADIMIR LONCAR , CERN, Switzerland WAYNE LUK, Imperial College London, UK MARIA SPIROPULU, California Institute of Technology, USA Neural networks with a latency requirement on the order of microseconds, like the ones used at the CERN Large Hadron Collider, are typically deployed on FPGAs fully unrolled and pipelined. A bottleneck for the deployment of such neural networks is area utilization, which is directly related to the required constant matrix-vector multiplication (CMVM) operations. In this work, we propose an efficient algorithm for implementing CMVM operations with distributed arithmetic (DA) on FPGAs that simultaneously optimizes for area consumption and latency. The algorithm achieves resource reduction similar to state-of-the-art algorithms while being significantly faster to compute. The proposed algorithm is open-sourced and integrated into the hls4ml library, a free and open-source library for running real-time neural network inference on FPGAs. We show that the proposed algorithm can reduce on-chip resources by up to a third for realistic, highly quantized neural networks while simultaneously reducing latency, enabling the implementation of previously infeasible networks. CCS Concepts: Hardware Reconfigurable logic applications; Computing methodologies Neural networks. Additional Key Words and Phrases: Quantized Neural Network, Real-time inference, High level synthesis, FPGA ACM Reference Format: Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu. 2025. da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs. In Proceedings of Make sure to enter the correct conference title from your rights confirmation emai (Conference acronym XX). ACM, New York, NY, USA, 24 pages. 1 Introduction Due to the need for low latency and high throughput in applications, edge computing has significantly increased its importance for real-time neural network inference [35]. While typical real-time inference applications have latency constraints of a few milliseconds [17, 28, 43], certain specific applications require sub-microsecond inference latency.\n\n--- Segment 2 ---\n1 Introduction Due to the need for low latency and high throughput in applications, edge computing has significantly increased its importance for real-time neural network inference [35]. While typical real-time inference applications have latency constraints of a few milliseconds [17, 28, 43], certain specific applications require sub-microsecond inference latency. For instance, at the CERN Large Hadron Collider (LHC) [40], hundreds of terabytes of data are generated by the detectors every second from proton-proton collisions at a rate of 4 MHz. This enormous data throughput is reduced by a hardware system, ùë°ùëüùëñùëîùëîùëíùëü, which filters the data in real-time at the same rate. It determines which event should be kept for offline processing or discarded, and the final decision must be made within a few microseconds [38, 39]. The Also at Institute of Physics Belgrade, Serbia. Authors Contact Information: Chang Sun, California Institute of Technology, Pasadena, CA, USA; Zhiqiang Que, Imperial College London, London, UK; Vladimir Loncar, CERN, Geneva, Switzerland; Wayne Luk, Imperial College London, London, UK; Maria Spiropulu, California Institute of Technology, Pasadena, CA, USA. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from 2025 Copyright held by the owner author(s). Publication rights licensed to ACM. Manuscript submitted to ACM Manuscript submitted to ACM 1 2 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu trigger s accuracy is vital to keep only the relevant events for physics studies, effectively managing the downstream data rate by reducing it by two orders of magnitude. O(1000) field-programmable gate arrays (FPGAs) are currently used in the trigger system, where several algorithms run in parallel on each FPGA.\n\n--- Segment 3 ---\nManuscript submitted to ACM Manuscript submitted to ACM 1 2 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu trigger s accuracy is vital to keep only the relevant events for physics studies, effectively managing the downstream data rate by reducing it by two orders of magnitude. O(1000) field-programmable gate arrays (FPGAs) are currently used in the trigger system, where several algorithms run in parallel on each FPGA. As a result, resources are scarce, and the footprint of each algorithm must be minimal. In anticipation of the LHC s upgrade to the High-Luminosity LHC (HL-LHC) [45], Machine Learning (ML) techniques are being actively explored to enhance the current trigger system [1, 38, 39]. However, integrating demanding models under such strict resource and latency constraints remains challenging. To meet the latency requirements, the neural networks used in such applications are typically deployed on FPGAs fully unrolled and pipelined (usually with an initiation interval of one). hls4ml, a free and open-source library for synthesizing deployable firmware for running neural network inferences on FPGAs, is widely used for implementing such neural networks on FPGAs [1, 12 14, 36, 38, 39]. The library parses the architectures of the neural networks and generates the corresponding high-level synthesis (HLS) projects for FPGAs with a set of fixed templates for common neural network layers or operations. The generated HLS projects are then synthesized into hardware description language (HDL) code by the corresponding HLS backends. Frequently, the most resource-consuming part of the designs are the constant matrix-vector multiplication (CMVM) operations in the neural networks, such as those in the dense or convolutional layers. As the final design is fully unrolled, the CMVM operations are typically implemented primarily with distributed arithmetic (DA) [27] by the HLS backends, as suggested by the high lookup-table (LUT) utilization with moderate to low digital signal processor (DSP) utilization in the synthesized designs. In this work, we present da4ml, a fast, scalable, and accurate optimization framework for CMVM targeting ultra- low-latency neural networks on FPGAs.\n\n--- Segment 4 ---\nAs the final design is fully unrolled, the CMVM operations are typically implemented primarily with distributed arithmetic (DA) [27] by the HLS backends, as suggested by the high lookup-table (LUT) utilization with moderate to low digital signal processor (DSP) utilization in the synthesized designs. In this work, we present da4ml, a fast, scalable, and accurate optimization framework for CMVM targeting ultra- low-latency neural networks on FPGAs. Most existing exact multiplierless CMVM algorithms are either too slow (e.g., ùêªùëêùëöùë£ùëö: O(ùëÅ3)1 time [3], hours for moderate-size matrices), or miss significant opportunities for optimization (e.g., SCMVM: O(ùëÅ2) time [44], but cannot capture differently scaled subexpressions). The proposed da4ml combines a novel graph-based decomposition with cost-aware Common Subexpression Elimination (CSE), which retains full numerical precision (not approximate), captures subexpression reuse even across differently scaled terms and signed digits, and has a much better asymptotic complexity O(ùëÅ2) and a five-orders-of-magnitude faster runtime (Table 2). This makes it practical for realistic, large-scale networks in ultra-low-latency environments such as the CERN LHC. To the best of our knowledge, this is the first open-source, end-to-end DA-based neural network compiler with optimized CMVM operations. By tightly integrating our framework with hls4ml, it provides a drop-in complement for the default CMVM implementation in hls4ml. This tight integration into an established toolchain (the hls4ml workflow) lowers the barrier to adoption for the broader HEP and FPGA communities. The framework also supports direct RTL generation without going through HLS for fast prototyping and easier integration into existing RTL workflows. The practical impact of the proposed framework has been demonstrated by enabling the production deployment of the AXOL1TL [1] anomaly detection trigger at the CMS experiment, where it significantly improves the resource utilization and timing closure of the synthesized design.\n\n--- Segment 5 ---\nThe framework also supports direct RTL generation without going through HLS for fast prototyping and easier integration into existing RTL workflows. The practical impact of the proposed framework has been demonstrated by enabling the production deployment of the AXOL1TL [1] anomaly detection trigger at the CMS experiment, where it significantly improves the resource utilization and timing closure of the synthesized design. The contributions of this work are as follows: We propose da4ml, a novel, performant, and scalable CMVM optimization framework with a hybrid algorithm that combines graph-based decomposition with cost-aware CSE. It offers orders of magnitude faster runtime than the prior state of the art while achieving comparable resource efficiency. 1ùëÅis the number of non-zero signed digits needed to represent the constant matrix. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 3 We implement the proposed da4ml framework as an open-source library and tightly integrate it into the widely used hls4ml tool as a drop-in solution for FPGA-based ML designs. da4ml can also generate synthesizable RTL directly, allowing users to bypass HLS where needed. In addition, da4ml has enabled production deployment of the AXOL1TL [1] anomaly detection trigger at CMS with improved resource efficiency and timing closure. We evaluate the proposed framework on both synthetic CMVM benchmarks and realistic neural networks for the CERN LHC trigger system. The results show significant improvements in hardware resource utilization, design latency, and compilation speed, demonstrating the framework s practicality for ultra-low-latency FPGA applications. 1.1 Background and Related Work 1.1.1 Constant Matrix-Vector Multiplication. Constant matrix-vector multiplication (CMVM) is a common problem in digital signal processing (DSP) applications. The problem involves computing an operation of the form ùë¶T ùë•TùëÄ, where ùëÄis a constant integer matrix and ùë•is a vector of input values. The problem is well studied in the literature, and there are multiple algorithms proposed for solving it. Distributed arithmetic (DA) is a multiplierless method for implementing the multiplication-accumulation (MAC) operations in hardware by replacing them by shift-and-add (or subtract) operations, which are usually mapped to LUTs on FPGAs.\n\n--- Segment 6 ---\nThe problem is well studied in the literature, and there are multiple algorithms proposed for solving it. Distributed arithmetic (DA) is a multiplierless method for implementing the multiplication-accumulation (MAC) operations in hardware by replacing them by shift-and-add (or subtract) operations, which are usually mapped to LUTs on FPGAs. The method is particularly useful for applications where extremely low latency and high throughput are required. The DA algorithm has been studied in the literature for a long time, and multiple algorithms have been proposed for solving it. Many of the existing works focus on the multiple constant multiplication (MCM) problem, which is a special case of CMVM where the variable vector ùë•is of size one, commonly used in finite impulse response (FIR) filters [3], such as [24 27, 30, 41]. The works [3, 8, 9, 15, 16, 19, 44] propose algorithms for the CMVM problem, which is a more general case of the MCM problem. [15] applies CSE to the CMVM problem by recursively removing the most common two-term subexpression from the problem. [9] uses a similar approach but also takes into account the conflict between the subexpressions. In contrast, [16] uses a graph based approach, which transforms subgraphs in the constructed adder tree without increasing the maximum adder depth. Though dated more than a decade, to the best of our knowledge, the state-of-the-art algorithm for implementing a CMVM operation with an adder tree without precision loss is still the Hcmvmalgorithm proposed in [3]. The algorithm aggressively searches for possible transformations of the CMVM problem into potentially simpler subproblems and evaluates the cost of each using a heuristic similar to [9]. The algorithm also supports specifying the maximum allowed adder depth for the generated adder tree by limiting the search phase space. However, this algorithm is computationally expensive, which requires O(1000) CPU-seconds to optimize a random 16 16, 8-bit matrix. Moreover, the runtime scaling with respect to ,ùëÅ, the number of non-zero signed digits required to represent the constant matrix, lies between O(ùëÅ3) and O(ùëÅ3.5). This makes it impractical for optimizing any moderately large matrices used in neural networks.\n\n--- Segment 7 ---\nMoreover, the runtime scaling with respect to ,ùëÅ, the number of non-zero signed digits required to represent the constant matrix, lies between O(ùëÅ3) and O(ùëÅ3.5). This makes it impractical for optimizing any moderately large matrices used in neural networks. In contrast, we propose a new algorithm for optimizing the CMVM problem that achieves resource efficiency comparable to [3] while significantly reducing computational cost, making it practical for larger matrices. [44] proposes a scalable algorithm for the CMVM problem, which is demonstrated to be runtime-efficient and practically useful for up to 100 100 constant matrices. Unlike previously mentioned CSE-based approaches using two-term subexpressions, the algorithm considers multi-term common subexpressions with greedy CSE for efficient processing. However, this algorithm fails to capture common subexpressions with different power-of-two scaling Manuscript submitted to ACM 4 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu factors, and it does not account for possible negative values in the weights. [8] adopts a similar multi-term common subexpression approach but uses a simulated annealing to optimize the cost of the adder tree for better resource utilization at the expense of algorithm runtime. While shifted common subexpressions are supported, the algorithm can only identify them in a coarse-grained fashion (i.e., a uniform shift across the whole row or column of the constant matrix). Signed digit representation is stated to be supported, but no discrete method is given, and its impact on identifying multi-term common subexpressions is not discussed. To avoid these drawbacks, we design our algorithm to effectively capture common subexpressions with different shifts and signs while preserving runtime efficiency, enabling more efficient optimization for a broader class of constant matrices. For neural networks, implementing CMVM operations without precision loss is not always required. [23] uses computation coding for approximate CMVM operations by decomposing the constant matrix into the product of multiple matrices containing only powers-of-two. By applying this method to embedded neural networks, the method demonstrates minimal accuracy loss while improving resource utilization by 3 to 6-fold. Similarly, [4] proposes another method based on 0-1 linear programming to optimize the CMVM problem approximately.\n\n--- Segment 8 ---\nBy applying this method to embedded neural networks, the method demonstrates minimal accuracy loss while improving resource utilization by 3 to 6-fold. Similarly, [4] proposes another method based on 0-1 linear programming to optimize the CMVM problem approximately. However, for already highly quantized neural networks, such as those trained with HGQ or QKeras [10, 11] with low bitwidths, the weights usually carry non-negotiable gradients on the losses, and such additional approximations in the implementation stage are not preferred. Hence, in this work, we focus on the implementation of CMVM operations without any approximations, preserving full numerical precision throughout. 1.2 Ultra-Low Latency Neural Networks Neural networks with a latency requirement on the order of O(1) ùúás are required for the trigger systems of the Large Hadron Collider. Simultaneously, the networks are required to produce one inference at least every 25 ns. To satisfy these constraints, these neural networks are typically heavily quantized and pruned and deployed on FPGAs fully unrolled with initiation interval (II) of one [1, 12 14, 36, 38, 39]. As the networks are fully unrolled, on-chip resource consumption is of major concern. In our experience, the most resource-consuming part of the designs are usually the matrix-vector multiplications in the fully connected or convolutional layers. The standard approach for optimizing resource utilization is to quantize and prune the neural networks to reduce the resulting firmware size. In this work, we go further by introducing an efficient algorithm for optimizing CMVM operations based on distributed arithmetic, which reduces resource utilization by up to one-third and improves timing closure, enabling previously infeasible designs to meet the stringent LHC trigger system constraints. 1.3 hls4ml hls4ml [12] is a template library for translating quantized neural networks into HLS projects for FPGAs. hls4ml provides a high-level interface for users to interface popular machine learning frameworks with HLS backends, and enables users to deploy neural networks on FPGAs with minimal effort. This library is widely used for real-time inference applications.\n\n--- Segment 9 ---\nhls4ml provides a high-level interface for users to interface popular machine learning frameworks with HLS backends, and enables users to deploy neural networks on FPGAs with minimal effort. This library is widely used for real-time inference applications. Notably, it has been deployed at the Compact Muon Solenoid (CMS) experiment in its L1 trigger for anomaly detection [1] and is being studied for the upgrade of the trigger system at the ATLAS experiment [36]. The library has a large built-in template library for common neural network layers, each customized and optimized for different HLS backends. In this work, we use hls4ml with the Vivado and Vitis HLS backends to implement the proposed algorithm, and we use its implementation as a baseline for comparison. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 5 2 Problem Formulation The objective is to implement the CMVM operation, ùë¶T ùë•TùëÄ, as an adder tree with minimum LUTs on FPGAs under a delay constraint. The delay constraint (DC) is defined by the maximum of additional adder depth with respect to the minimal adder depth possible. The target application of this work is the real-time inference of quantized neural networks on FPGAs targeting the LHC trigger system, with possible extension to other general DSP applications. For the target application, the neural networks are required to have an initial interval 25 ns. For this purpose, when implementing the CMVM operations with adder trees, we expect multiple adder steps within one pipeline stage. Hence, though the final design is expected to be pipelined, we expect the number of LUTs used for implementing the adders to dominate the resource utilization with moderate to low register usage. In the adder tree, the dominant operation is in the form of ùëé (ùëè ùë†), where ùëéand ùëèare the variable inputs to the adder. The bit-shift ùë†and the sign for the input ùëèare known at compilation time. The expected cost of this operation is formulated as the number of full and half adders required to implement the operation (i.e., the number of output bits conditioned on more than one input).\n\n--- Segment 10 ---\nThe bit-shift ùë†and the sign for the input ùëèare known at compilation time. The expected cost of this operation is formulated as the number of full and half adders required to implement the operation (i.e., the number of output bits conditioned on more than one input). We denote the bitwidths for ùëéand ùëèas ùëèùë§ùëéand ùëèùë§ùëè, respectively. When there is at least one bit overlapping between the two operands (i.e., max(ùëèùë§ùëé,ùëèùë§ùëè) ùë†), the simplified expected cost is given by cost(ùëèùë§ùëé,ùëèùë§ùëè,ùë†, sign) max(ùëèùë§ùëé,ùëèùë§ùëè ùë†) min(0,ùë†) 1. (1) In practice, the logic delay of each adder with different input bitwidths will be different. However, as we notice that the majority of the delay in the adder trees is due to routing, we assume each adder to have the same delay and model the overall delay approximately by the adder depth for simplicity, following [3, 4, 16]. 3 da4ml Algorithm 3.1 Notations For a number of fixed-point type, it takes the general form of fixed ùëÜ, ùëä, ùêº , where ùëÜis the sign bit, ùëäis the total bitwidth, and ùêºis the number of integer bits including the sign bit. In the algorithm, we denote the fixed-point numbers by quantized interval, namely, by their low value, high value, and the step size, [ùëô,‚Ñé,ùõø]. For a generic fixed-point number in the form of fixed ùëÜ, ùëä, ùêº , we have ùëô ùëÜ 2(ùêº ùëÜ), ‚Ñé 2ùêº ùëÜ 2 ùëä ùêº, and ùõø 2 ùëä ùêº.\n\n--- Segment 11 ---\nIn the algorithm, we denote the fixed-point numbers by quantized interval, namely, by their low value, high value, and the step size, [ùëô,‚Ñé,ùõø]. For a generic fixed-point number in the form of fixed ùëÜ, ùëä, ùêº , we have ùëô ùëÜ 2(ùêº ùëÜ), ‚Ñé 2ùêº ùëÜ 2 ùëä ùêº, and ùõø 2 ùëä ùêº. This notation is useful to track the required bitwidths when accumulating a large number of fixed-point numbers, as otherwise one carry bit will always need to be added to prevent overflow. The notation we use in the algorithm is summarized in Table 1. In particular, ùëÄ[ùëëùëñùëõ,ùëëùëúùë¢ùë°], ùëûùëñùëõùë°ùëñùëõ[ùëëùëñùëõ], ùëëùëíùëùùë°‚Ñéùëñùëõùë°[ùëëùëñùëõ], and ùëëùëêare the input parameters to the algorithm for a single CMVM operation. 3.2 Overview The da4ml is a CSE-based hybrid algorithm. Like other CSE-based algorithms, the algorithm operates on a discrete representation of the constant matrix. In this work, following [3, 4, 9, 15], we adopt the canonical signed digit (CSD) [6] representation. CSD is a signed digit representation of a number that never has two consecutive non-zero digits, and the number of non-zero digits is guaranteed to be minimal. Hence, for a number with ùë•digits, the CSD representation has at most ùë• 2 1 non-zero digits, which is 1 3 of the total number of digits on average. As ùëÄcontains only fixed-point integers, the algorithm first normalizes it by applying bit-shifts across the rows and columns such that no row column has all entries even except for zeros. The resultant scaling factors are recorded and will be applied to the input output vectors.\n\n--- Segment 12 ---\nAs ùëÄcontains only fixed-point integers, the algorithm first normalizes it by applying bit-shifts across the rows and columns such that no row column has all entries even except for zeros. The resultant scaling factors are recorded and will be applied to the input output vectors. Manuscript submitted to ACM 6 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Table 1. List of parameters used in the proposed da4ml algorithm Parameter Description ùëëùëñùëõ The number of input elements. ùëëùëúùë¢ùë° The number of output elements. ùëÄ[ùëëùëñùëõ,ùëëùëúùë¢ùë°] The input constant matrix containing fixed-point numbers. ùëëùëê Delay constraint, maximum number of extra adder depth permitted. ùëûùëñùëõùë°ùëñùëõ[ùëëùëñùëõ] An array representing the quantized intervals of the input vector. ùëô,‚Ñé,ùõø The low, high, and step size that uniquely defines a quantized interval. ùëëùëíùëùùë°‚Ñéùëñùëõùë°[ùëëùëñùëõ] The adder depth associated with each element of the input vector. ùëÄ, ùêº,ùëÜ The width, integer bits (include sign bit if presents), and sign bit of a fixed-point numbers. ùë† The bit-shift applied to an operand in a two-term subexpression. ùëèùë§ùëÄ The bitwidth of the constant matrix ùëÄ. First Stage ùë£ùëñ A vector corresponding to the i-th column of the constant matrix ùëÄ. ùë£ùëñ A vertex corresponds to the column vector ùë£ùëñ. ùë£0 The root vertex in the graph, associated with the zero vector ( 0). ùëÄ1, ùëÄ2 The two submatrices such that ùëÄ ùëÄ1ùëÄ2.\n\n--- Segment 13 ---\nùë£0 The root vertex in the graph, associated with the zero vector ( 0). ùëÄ1, ùëÄ2 The two submatrices such that ùëÄ ùëÄ1ùëÄ2. Second Stage ùëÄ Overloaded to the input matrix for the second stage, either ùëÄ1 or ùëÄ2. ÀÜùëÄ The normalized version of matrix ùëÄ. ùëÄùëíùë•ùëùùëü[ùëëùëñùëõ,ùëëùëúùë¢ùë°, ùêµ] The CSD representation of the matrix ÀÜùëÄ, with values in { 1, 0, 1}. ùêµ The span of powers of the CSD digits. ùêøùëñùëöùëùùëô A list containing the implemented values. ùëé,ùëè The two input values for a two-term subexpression. Fig. 1. Overview of the proposed da4ml automatic optimization flow for CMVM on FPGAs. The algorithm first decomposes the constant matrix ùëÄinto two submatrices ùëÄ1 and ùëÄ2 using a graph-based approach that captures shared structure across columns. It then applies CSE on both submatrices to minimize redundant computations. The resulting optimized adder tree significantly reduces LUT usage and improve the latency. The proposed da4ml algorithm first decomposes the constant matrix ùëÄinto two submatrices ùëÄ1 and ùëÄ2 using a graph-based approach that captures high-level common pattern across rows. It then applies CSE on both submatrices to minimize redundant computations, as shown in Figure 1. The resulting optimized adder tree significantly reduces LUT usage and improve the latency. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 7 3.3 First stage: Graph-based Decomposition In the first stage, the algorithm decomposes the constant matrix into two submatrices, ùëÄ1 and ùëÄ2 such that ùëÄ ùëÄ1ùëÄ2.\n\n--- Segment 14 ---\nThe resulting optimized adder tree significantly reduces LUT usage and improve the latency. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 7 3.3 First stage: Graph-based Decomposition In the first stage, the algorithm decomposes the constant matrix into two submatrices, ùëÄ1 and ùëÄ2 such that ùëÄ ùëÄ1ùëÄ2. Inspired by Hcmvm, we propose this stage to exploit the high-level similarity between the different columns of the constant matrix. For this purpose, we first let each column of ùëÄ, ùëÄ[:,ùëñ] ùë£ùëñ(ùëñ [1, . . . ,ùëëùëúùë¢ùë°]) be a vertex ùë£ùëñin a graph. Then, we add the root vertex ùë£0 associated with the zero vector 0 to the graph. The distance between two vertices ùë£ùëñ and ùë£ùëóis defined as the sum of the number of non-zero digits in the vector ùë£ùëñ ùë£ùëóor ùë£ùëñ ùë£ùëó, whichever is lower. Starting from ùë£0 associated with 0, we use Prim s algorithm [31] to find an approximate Minimum Spanning Tree (MST) of the graph, subject to a maximum depth smaller than or equal to 2ùëëùëêfrom the root vertex. Each edge in the approximate MST is then translated back into a column vector in ùëÄ1 of shape [ùëëùëñùëõ,ùëëùëúùë¢ùë°], with its contributions to the original outputs (-1, 0, or 1) recorded in the second submatrix ùëÄ2 of shape [ùëëùëúùë¢ùë°,ùëëùëúùë¢ùë°]. The contribution can be determined by tracing from the root vertex to the corresponding vertex, where each edge traversed has a non-zero contribution. ùëÄ2 constructed this way is usually significantly more sparse compared to ùëÄ1.\n\n--- Segment 15 ---\nThe contribution can be determined by tracing from the root vertex to the corresponding vertex, where each edge traversed has a non-zero contribution. ùëÄ2 constructed this way is usually significantly more sparse compared to ùëÄ1. Both ùëÄ1 and ùëÄ2 are then passed to the next stage. This stage is useful for matrices with correlated columns and a high digit count. For matrices where the columns are rarely correlated, the algorithm would usually produce trivial decomposition, where ùëÄ1 is a shuffled form of ùëÄand ùëÄ2 is a shuffled identity matrix. As a realistic example would be too large for display, we show a minimal example of decomposing an matrix ùëÄof size 3 3: ùëÄ ùë£1 ùë£2 ùë£3 ¬™ 0 1 3 1 2 4 2 3 5 ¬™ (2) The graph constructed form this matrix is shown in Figure 2. In this particular example, the MST constructed is a chain of ùë£0 ùë£1 ùë£2 ùë£3, shown by the colored edges. The corresponding column vectors to these edges are recorded in ùëÄ1, and their contributions to ùë£1, ùë£2, and ùë£3 are recorded in ùëÄ2, where ùë£1 is obtained by adding the first edge, ùë£2 is obtained by adding the first two edges, and ùë£3 is obtained by adding all three edges. 3.4 Second stage: Common Subexpression Elimination In the second stage, CSE is applied independently on ùëÄ1 and ùëÄ2, and the solutions are concatenated to form the final solution. For simplicity, we denote the input matrix for the second stage as just ùëÄ. In this stage, ùëÄis again normalized into ÀÜùëÄ, which has no row or column containing all even numbers except for zeros.\n\n--- Segment 16 ---\nFor simplicity, we denote the input matrix for the second stage as just ùëÄ. In this stage, ùëÄis again normalized into ÀÜùëÄ, which has no row or column containing all even numbers except for zeros. The algorithm then converts the input matrix to the CSD representation ùëÄùëíùë•ùëùùëü ( 1, 0, 1)ùëëùëñùëõ,ùëëùëúùë¢ùë°,ùêµ, where ùêµis the span of the powers of the CSD digits (i.e., the difference between the minimal and maximal bit-shifts associated with the CSD digits plus one). The CSE algorithm starts with the ùëÄùëíùë•ùëùùëümatrix, and a list of implemented values ùêøùëñùëöùëùùëôinitialized with the elements of the input vector: ùêøùëñùëöùëùùëô [ùë£1, ùë£2, . . . , ùë£ùëëùëñùëõ]. These two objects defines the state of the algorithm, and the algorithm updates the state iteratively until exhausting all common subexpressions. For each update step, the algorithm selects a two-term subexpression and implements it. A two-term subexpression is defined as an operation with the general form ùëé ùëè ùë†, characterized as a four-tuple: the two inputs ùëéand ùëè, the sign, and the bit-shift ùë†for the second operand2. 2The order of ùëéand ùëèis fixed by their location in the matrix in practice for the uniqueness of the four-tuple. Manuscript submitted to ACM 8 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Fig. 2. An example of the graph constructed from the constant matrix ùëÄin the first stage of the da4ml algorithm without a delay constraint. The graph is constructed with Prim s algorithm, and the MST is shown in colored edges. The root vertex ùë£0 is on the bottom left.\n\n--- Segment 17 ---\nThe graph is constructed with Prim s algorithm, and the MST is shown in colored edges. The root vertex ùë£0 is on the bottom left. The column vectors on the edges are the corresponding vectors that need to be added subtracted to transform between the two connected vertices. The number after the column vectors is the number of non-zero digits needed to represent the vector, and the smaller of the two is used as the edge weight. The column vectors corresponding to the edges in the MST are stored in ùëÄ1, and their contributions to ùë£1, ùë£2, and ùë£3 are recorded in ùëÄ2. Implementing a two-term subexpression consists of two steps. The algorithm first appends the result of the two-term subexpression to the list of implemented values ùêøùëñùëöùëùùëô. Then, it appends an empty row to ùëÄùëíùë•ùëùùëü. For every occurrence of the subexpression in ùëÄùëíùë•ùëùùëü, the algorithm sets the corresponding two digits to zero and sets -1 or 1 on the new row at the corresponding power. At all times, the value of √ç ùëóùë£ùëóÀÜùëÄùëóùëñcan be recovered with √ç ùëó,ùë†2ùë† ùêøùëñùëöùëùùëô[ùëó] ÀÜùëÄùëíùë•ùëùùëü[ùëó,ùëñ,ùë†]. In the implementation, we keep a hash table that caches the frequency of all two-term subexpressions to speed up the optimization. While [3, 9] show that selecting the two-term subexpression that minimally conflicts with the other subexpressions could result in lower final adder usage, this approach requires a one-step look-ahead operation when selecting the two-term common subexpression in each update step, which involves a complexity of O( ùêøùëñùëöùëùùëô 2).\n\n--- Segment 18 ---\nIn the implementation, we keep a hash table that caches the frequency of all two-term subexpressions to speed up the optimization. While [3, 9] show that selecting the two-term subexpression that minimally conflicts with the other subexpressions could result in lower final adder usage, this approach requires a one-step look-ahead operation when selecting the two-term common subexpression in each update step, which involves a complexity of O( ùêøùëñùëöùëùùëô 2). Due to practical considerations for larger matrices, we instead select the most common subexpression to implement for each update step, which can be performed with a complexity of O( ùêøùëñùëöùëùùëô ). As ùêøùëñùëöùëùùëô is of the same order of magnitude as the total number of non-zero digits in ùëÄùëíùë•ùëùùëü, the additional time complexity for performing the look-ahead operation would be substantial. As suggested in [3], because this only improves the final resource utilization by less than 2 as measured in adder count, we decided that the additional complexity is not worth the runtime overhead. In contrast to [15], we also take into account the operands quantized intervals (i.e., the bitwidths and shift) when choosing the subexpression for each update step. As implied by the cost function in (1), it is preferred to have the two operands with similar bitwidths and shifts. However, directly weighting the frequency of the two-term subexpressions by the total cost is not ideal: it would also count the half-adders used, which are "overheads" as they may unnecessarily increase the accumulator width downstream. Instead, we weight the frequency by the number of overlapping bits between the two operands. This weighting reduces to a constant factor when the input bitwidths are uniform and significantly larger than the constant matrix s bitwidths. We show a minimal example of the second stage in Figure 3 with a constant matrix from the H.264 integer trans- form [42]. For presentation purposes, the matrix shown is a transposed matrix (i.e., ùë¶ ùëÄ ùë•instead of ùë¶T ùë•TùëÄ).\n\n--- Segment 19 ---\nWe show a minimal example of the second stage in Figure 3 with a constant matrix from the H.264 integer trans- form [42]. For presentation purposes, the matrix shown is a transposed matrix (i.e., ùë¶ ùëÄ ùë•instead of ùë¶T ùë•TùëÄ). In this minimal example, we do not weight the frequency of the subexpressions by the operand bitwidths for simplicity. Also, Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 9 please note that although the example matrix contains only power-of-two numbers, the algorithm accepts matrices with arbitrary fixed-point elements. The algorithm first identifies three two-term subexpressions, shown in the bounding boxes with the same color. In the first step, the expression ùë•0 ùë•3 has the highest frequency and is implemented, and all of its occurrences are replaced. The algorithm then proceeds to implement the other two subexpressions. The adder graphs before and after optimization are shown in Figure 4, where the original adder graph requiring 12 adders on the left is reduced to 8 adders on the right. 1 1 1 1 2 1 -1 1 1 -1 -1 1 1 -2 2 1 1 1 -1 -1 -1 -2 2 1 0 0 2 -2 0 0 0 0 1 0 1 1 x0 x1 x2 x3 y0 y1 y2 y3 x0 x1 x2 x3 x0 x3 0 0 1 -1 0 0 -2 2 0 0 2 -2 0 0 0 0 1 0 1 1 x0 x1 x2 x3 x0 x3 x1 x2 1 0 -1 0 0 0 2 -2 0 0 0 0 1 0 1 1 x0 x3 x0 x3 x1 x2 x1-x2 1 0 -1 0 0 1 0 -2 Fig. 3. An example of the second stage of the da4ml algorithm. The algorithm identifies three two-term subexpressions, shown in the bounding boxes with the same color. For representation purposes, the matrix shown is a transposed matrix (i.e., ùë¶ ùëÄ ùë• instead of ùë¶T ùë•TùëÄ).\n\n--- Segment 20 ---\nThe algorithm identifies three two-term subexpressions, shown in the bounding boxes with the same color. For representation purposes, the matrix shown is a transposed matrix (i.e., ùë¶ ùëÄ ùë• instead of ùë¶T ùë•TùëÄ). The first subexpression ùë•0 ùë•3 has the highest frequency and is implemented first, followed by the other two subexpressions. Frequency weighting by operand bitwidths is not applied in this example for simplicity. In each step, the subexpressions with a colored shade are eliminated. In the last step, the all-zero columns and corresponding elements in the implemented values are omitted. -2 2 -1 1 Fig. 4. An example of the adder graphs implementing the H.264 constant matrix before and after the optimization. The original adder graph on the left requires 12 adders, while the optimized adder graph on the right only requires 8 adders. Each square node represents an adder subtractor, and the edges represent the inputs to the adder subtractor. The color of the edges indicates the sign and the power-pf-two coefficients of the inputs. The circle nodes represent the inputs to the adder graph. Manuscript submitted to ACM 10 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu 1 configuration 2 hls_config {'Model ': { 3 'Precision ': INSERT_MODEL_PRECISION , 4 'ReuseFactor ': 1, 5 'Strategy ': 'distributed_arithmetic ' 6 }} 7 8 convert , may also be from other frameworks 9 model_hls convert_from_keras_model(model , hls_config hls_config , ...) Listing 1. Enabling the DA strategy on all supported layers in hls4ml using our framework. 3.5 Complexity Analysis and Performance For complexity analysis, we denote the bitwidth of the constant matrix ùëÄas ùëèùë§ùëÄ, and the number of inputs and outputs as ùëëùëñùëõand ùëëùëúùë¢ùë°, respectively.\n\n--- Segment 21 ---\nEnabling the DA strategy on all supported layers in hls4ml using our framework. 3.5 Complexity Analysis and Performance For complexity analysis, we denote the bitwidth of the constant matrix ùëÄas ùëèùë§ùëÄ, and the number of inputs and outputs as ùëëùëñùëõand ùëëùëúùë¢ùë°, respectively. Let ùëÅ ùëëùëñùëõ ùëëùëúùë¢ùë° ùëèùë§ùëÄ. The complexity of the algorithm is dominated by the second stage. As the first stage is a constrained variation of the Prim s algorithm, the complexity of the first stage is bounded O(ùëë2 ùëúùë¢ùë°). For the second stage, we found that the complexity is dominated by optimizing ùëÄ1, for which it is similar to optimizing ùëÄdirectly. Initializing the hash table requires a complexity of O ùëëùëúùë¢ùë° (ùëëùëñùëõ ùëèùë§ùëÄ)2 for double iteration over the input dimensions and single iteration over the output expressions. O( ùêøùëñùëöùëùùëô ) O(ùëÅ) update steps are required to reach the final solution. For each step, the complexity is O(ùëÅ) to find the most common two-term subexpression over a cached dictionary. Each frequency update also takes O(ùëÅ) time, as only single iteration over the input dimensions is required for differential updates. Hence, the overall asymptotic complexity of the algorithm is expected to be O(ùëÅ2), dominated by the iterative update steps. In practice, we found the execution time of the algorithm to be asymptotically close to O(ùëÅ2 log(ùëÅ)2) up to ùëÅ 105 (128 128 8-bit). We postulate that the logarithmic factor is due to the overhead of the hash table or memory allocation deallocation when appending rows to the ùëÄùëíùë•ùëùùëümatrix.\n\n--- Segment 22 ---\nIn practice, we found the execution time of the algorithm to be asymptotically close to O(ùëÅ2 log(ùëÅ)2) up to ùëÅ 105 (128 128 8-bit). We postulate that the logarithmic factor is due to the overhead of the hash table or memory allocation deallocation when appending rows to the ùëÄùëíùë•ùëùùëümatrix. 4 Implementation and Integration da4ml is implemented as a high-performance Python library and is open-sourced at At its core, da4ml leverages Numba [20], a just-in-time compiler for Python that compiles code to MLIR and then to a binary with LLVM [21] for performance considerations. Designed for seamless deployment, the library is tightly integrated with the widely adopted hls4ml framework [12]. Users can easily enable da4ml by setting the strategy to distributed_arithmetic for any layer requiring CMVM operations, as shown in Listing 1. No further user configuration or intervention is required. The overall workflow is shown in Figure 5. In this workflow, da4ml generates an optimized adder tree for the CMVM operation in HLS C code, which is then used as a drop-in replacement for the default CMVM implementation within the hls4ml. This drop-in integration enables significant improvements in resource efficiency and latency without sacrificing usability. Currently, the Dense, EinsumDense, and Conv1D Conv2D layers are supported for the Vivado Vitis backends, with plans for broader support underway. By bridging algorithmic innovation with production-ready toolchains, da4ml provides a scalable and practical solution for deploying high-throughput, low-latency neural networks on FPGAs.\n\n--- Segment 23 ---\nCurrently, the Dense, EinsumDense, and Conv1D Conv2D layers are supported for the Vivado Vitis backends, with plans for broader support underway. By bridging algorithmic innovation with production-ready toolchains, da4ml provides a scalable and practical solution for deploying high-throughput, low-latency neural networks on FPGAs. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 11 FPGA x0 x1 x2 x3 y0 y1 y2 y3 1 1da 1 1 1 1 2 1 -1 -2 1 -1 -1 1 1 -2 2 1 x0 x1 x2 x3 ml 4 CMVM Problems (or other frontend) Optimized adder trees 1 1 1 1 2 1 -1 1 1 -1-1 1 1 -2 2 1 x0x1x2x3 C bitstream Fig. 5. The workflow of using the da4ml library with the hls4ml library. The da4ml library generates the optimized adder tree for the CMVM operation in HLS C code, which is send to the hls4ml library for drop-in replacement of the default CMVM implementation. 5 Experiments In this section, we first evaluate the proposed da4ml algorithm on random matrices and compare the high-level metrics with the state-of-the-art Hcmvmalgorithm. We then evaluate the performance of the proposed algorithm on realistic neural networks consisting CMVM operations. We perform both the HLS version and hardware-description-language (HDL) version of the algorithm. The HLS version is implemented with the hls4ml library [12], and the HDL version is implemented standalone with the da4ml library. 5.1 Random Matrices We evaluate the performance of the proposed algorithm on random ùëö ùëömatrices. In this subsection, to facilitate comparison with the Hcmvmalgorithm, we adopt a convention from [3] where a ùëèùë§-bit random matrix is generated by sampling integers uniformly from [2ùëèùë§ 1 1, 2ùëèùë§ 1].\n\n--- Segment 24 ---\n5.1 Random Matrices We evaluate the performance of the proposed algorithm on random ùëö ùëömatrices. In this subsection, to facilitate comparison with the Hcmvmalgorithm, we adopt a convention from [3] where a ùëèùë§-bit random matrix is generated by sampling integers uniformly from [2ùëèùë§ 1 1, 2ùëèùë§ 1]. ùëëùëê 1 (no delay constraint) ùëëùëê 0 ùëëùëê 2 da4ml Hcmvm [3] da4ml Hcmvm [3] da4ml Hcmvm [3] N step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder cpu [ms] step adder 2 3.3 8.7 0.1 4.4 8.2 1.0e1 3.1 9.9 0.1 3.1 8.8 1.0e1 3.3 8.7 0.1 3.7 8.2 4 6.1 29.3 0.3 7.8 27.6 4.8e2 4.1 37. 0.3 4.1 32.1 4.7e2 5.9 30. 0.3 5.7 28.1 6 8.4 59. 0.6 10. 57.3 3.3e3 5. 77.8 0.8 5. 66.8 3.8e3 6.7 62.6 0.6 7. 58.2 8 9.4 98. 1.3 11.9 96.3 1.5e4 5.1 130.9 2. 5.1 117.2 1.7e4 7. 102.3 1.4 7.1 99.5 1 10.8 146.6 2.7 13.2 143.5 5.4e4 6. 195.6 4.2 6. 157.7 8.2e4 7.8 152.8 2.8 8. 146.9 12 11.6 203.6 4.8 14.6 200.4 1.7e5 6. 271.8 7.9 6. 241.6 1.7e5 8. 214.9 5.2 8.\n\n--- Segment 25 ---\n241.6 1.7e5 8. 214.9 5.2 8. 206.8 14 12.3 269.3 8.3 15.5 264.3 4.8e5 6. 358.5 14.1 6. 324. 4.2e5 8. 279.2 8.9 8. 274.8 16 13. 343.4 13.3 16.3 338.3 1.2e6 6. 456. 22.5 6. 423.2 9.9e5 8. 358.7 14.9 8. 353.3 Table 2. Comparison of the da4ml algorithm with the Hcmvm algorithm on random matrices. In Table 2, we show the comparison of the da4ml algorithm with the Hcmvm algorithm on random matrices with 8-bits under different delay constraints. For each configuration, we report the adder depth (denoted as step ), total number of adders, and runtime on CPU. Results are shown for three delay constraint settings: no constraint, strict delay constraint (ùëëùëê 0), and moderate delay constraint (ùëëùëê 2). The CPU time is measured on a single thread of an Intel i7-13700 CPU for da4ml, and on a single thread of an Intel Xeon at 2.33 GHz for Hcmvm, as reported by the authors. The table shows the number of adders used in the optimized design of both algorithms under the same delay constraints, Manuscript submitted to ACM 12 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu and the adder depth (step) of both. When delay constraint is not exactly 0, the proposed algorithm has 2 overhead in adder counts compared to the Hcmvm algorithm. When strict delay constraint is required, the proposed algorithm has a larger overhead of 8 in adder counts. Please note that this slight trade-off in resource usage yields dramatic improvements in compilation speed. With respect to runtime, the da4ml algorithm is significantly more efficient, being O(100000) times faster than the Hcmvm algorithm with a moderate matrix size of 16 16. The difference would be more significant for larger matrices given the difference in asymptotic complexities.\n\n--- Segment 26 ---\nWith respect to runtime, the da4ml algorithm is significantly more efficient, being O(100000) times faster than the Hcmvm algorithm with a moderate matrix size of 16 16. The difference would be more significant for larger matrices given the difference in asymptotic complexities. Although there is a generation gap between the two CPU processors, the magnitude of the observed speedup far exceeds what can be attributed solely to CPU differences, highlighting the efficiency and scalability of our framework. We further show extended runtime results for larger matrices in Figure 6 the computation time of the da4ml algorithm on random matrices with up to 128 128 elements with 8 bits, as well as the time expected from the asymptotic complexity of the algorithm. These results demostrate that the da4ml algorithm may handle reasonably complex CMVM problems that may be implemented on FPGAs unrolled in reasonable time. 0 20 40 60 80 100 120 Size of constant matrix (m) 10 3 10 1 101 103 105 Average CPU time (ms) Actual time O(m4 log2 m) Fig. 6. Computation time of the da4ml algorithm on random matrices with different sizes. The asymptotic complexity is O(ùëÅ2 log(ùëÅ)2), where ùëÅ ùëëùëñùëõ ùëëùëúùë¢ùë° ùëèùë§ùëÄ. The log(ùëÅ)2 factor was found empirically. We show the post-synthesis results of the proposed da4ml algorithm on random matrices with 8 bits and 4 bits, shown in Table 3 and Table 4, respectively. The results shown in this section are obtained after HLS synthesis with Vitis 2023.2 and out-of-context synthesis and place-and-route with the Vivado 2023.2 backend. The target FPGA is xcvu13p-flga2577-2-e. To quantify the logic delay, the designs are synthesized with a latency of one clock cycle, where the CMVM logic is a combinational logic block sandwiched between two layers of registers. For each matrix size, we use the same random constant matrix for the three delay constraints: 0, 2, and -1 (no constraint), marked as DC in the tables.\n\n--- Segment 27 ---\nThe target FPGA is xcvu13p-flga2577-2-e. To quantify the logic delay, the designs are synthesized with a latency of one clock cycle, where the CMVM logic is a combinational logic block sandwiched between two layers of registers. For each matrix size, we use the same random constant matrix for the three delay constraints: 0, 2, and -1 (no constraint), marked as DC in the tables. The baseline for comparison is the latency-optimized implementation in hls4ml, where an unrolled double for-loop is used to implement the CMVM operation with a fixed-point multiplication-accumulation operation for HLS. The inputs for all designs are 8-bit signed integers. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 13 As shown in Table 3, the da4ml approach completely avoids using DSP blocks. In cases where the baseline also relies on LUTs for multiplication (e.g., Table 4 or the 8 8 matrix in Table 3), da4ml can consistently reduce LUT usage by approximately half. When the baseline utilizes DSPs, the LUT usage is comparable, though it can be significantly reduced by relaxing the delay constraint. For latency, the critical path of the da4ml designs can be either shorter or longer than the baseline, depending on the chosen bitwidth and delay constraint.\n\n--- Segment 28 ---\nWhen the baseline utilizes DSPs, the LUT usage is comparable, though it can be significantly reduced by relaxing the delay constraint. For latency, the critical path of the da4ml designs can be either shorter or longer than the baseline, depending on the chosen bitwidth and delay constraint. Strategy DC Matrix Size LUT DSP FF Latency [ns] latency - 8 8 3193 0 645 2.21 DA 0 8 8 1570 0 654 1.97 DA 2 8 8 1214 0 420 2.62 DA -1 8 8 1200 0 412 3.14 latency - 16 16 4319 212 2301 3.05 DA 0 16 16 5281 0 1947 2.53 DA 2 16 16 4545 0 1618 2.66 DA -1 16 16 4321 0 1283 4.11 latency - 32 32 17666 807 4886 4.43 DA 0 32 32 18807 0 6408 3.14 DA 2 32 32 15427 0 4494 3.52 DA -1 32 32 14866 0 4643 5.71 latency - 64 64 70821 2897 18969 5.63 DA 0 64 64 66864 0 21503 4.65 DA 2 64 64 63852 0 20509 4.91 DA -1 64 64 52198 0 16214 6.49 Table 3. Resource utilization and latency of the da4ml algorithm on random matrices with different sizes and delay constraints. Strategy latency is the latency-optimized algorithm in hls4ml, and DA is the optimized implementation with the da4ml algorithm. The delay constraint is set to 0, 2, or -1, where -1 means no delay constraint. The matrix size is ùëëùëñùëõ ùëëùëúùë¢ùë°, and the bitwidth is 8. The input bitwidth is 8 in all cases. 5.2 Realistic Neural Networks We show the performance of the da4ml algorithm on realistic neural networks with CMVM operations. The networks are trained with the HGQ [10] library. It is worth noting these neural networks trained with HGQ have heterogeneous bitwidths within one CMVM operation, i.e., there is a high bit-wise sparsity in the constant matrix and the inputs. Due to the extremely heterogeneous bitwidths, da4ml s benefit is less significant than the random matrix case.\n\n--- Segment 29 ---\nIt is worth noting these neural networks trained with HGQ have heterogeneous bitwidths within one CMVM operation, i.e., there is a high bit-wise sparsity in the constant matrix and the inputs. Due to the extremely heterogeneous bitwidths, da4ml s benefit is less significant than the random matrix case. Based on the results from the previous section, we found delay constraint of 2 is a good trade-off between resource utilization and latency, and we use it for all evaluations in this section. 5.2.1 High-level Feature Jet Tagging Network. We show the out-of-context results after place-and-route for the high-level feature jet tagging network [12] on the VU13P FPGA. The network is a fully-connected neural network with 4 dense layers of sizes 16 64 32 16 16 5. In Table 5 and Table 6, we show the resource utilization and latency of the network synthesized against target clock frequencies of 200 MHz and 1 GHz, respectively. Vitis 2023.2 is used for HLS synthesis, and the total latency is not specified but left to the HLS tool. All designs are fully pipelined with an II of 1 clock cycle. When the target clock frequency is 200 MHz, the designs optimized with the da4ml algorithm always meet the timing constraints, whereas hls4ml s latency-optimized designs failed to meet timing for the model with 76.9 accuracy.\n\n--- Segment 30 ---\nAll designs are fully pipelined with an II of 1 clock cycle. When the target clock frequency is 200 MHz, the designs optimized with the da4ml algorithm always meet the timing constraints, whereas hls4ml s latency-optimized designs failed to meet timing for the model with 76.9 accuracy. In all Manuscript submitted to ACM 14 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Strategy DC Matrix Size LUT DSP FF Latency [ns] latency - 8 8 1241 0 459 1.77 DA 0 8 8 885 0 375 1.70 DA 2 8 8 721 0 278 1.86 DA -1 8 8 644 0 275 2.48 latency - 16 16 4538 0 1585 1.88 DA 0 16 16 2922 0 1084 2.30 DA 2 16 16 2268 0 745 2.34 DA -1 16 16 2126 0 699 3.27 latency - 32 32 13550 0 3761 2.44 DA 0 32 32 10658 0 3721 2.78 DA 2 32 32 7452 0 2465 2.93 DA -1 32 32 7339 0 2442 3.62 latency - 64 64 47274 0 14652 3.10 DA 0 64 64 38205 0 11711 3.37 DA 2 64 64 26715 0 6026 4.18 DA -1 64 64 25493 0 8279 5.09 Table 4. Resource utilization and latency of the da4ml algorithm on random matrices with different sizes and delay constraints. Strategy latency is the latency-optimized algorithm in hls4ml, and DA is the optimized implementation with the da4ml algorithm. The delay constraint is set to 0, 2, or -1, where -1 means no delay constraint. The matrix size is ùëëùëñùëõ ùëëùëúùë¢ùë°, and the bitwidth is 4. The input bitwidth is 8 in all cases. cases, da4ml reduces LUT consumption by 10 for this network. To obtain the maximum frequency, we resynthesized the designs with a target clock frequency of 1 GHz. Under this setting, the designs optimized with the da4ml algorithm are pipelined with fewer stages and achieve Fmax values similar to or higher than the baseline hls4ml designs.\n\n--- Segment 31 ---\nTo obtain the maximum frequency, we resynthesized the designs with a target clock frequency of 1 GHz. Under this setting, the designs optimized with the da4ml algorithm are pipelined with fewer stages and achieve Fmax values similar to or higher than the baseline hls4ml designs. As more registers are inserted, LUT fusion and sharing are less effective, and the LUT utilization is higher than for the designs with a 200 MHz target clock frequency. The DSP utilization is reduced to 0 in all cases with da4ml. Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] Latency 76.9 4 (21.6 ns) 13,258 55 1,497 185.5 DA 4 (18.9 ns) 12,250 0 1,502 211.5 Latency 76.6 3 (14.7 ns) 7,502 27 900 203.6 DA 3 (13.3 ns) 6,869 0 966 226.0 Latency 76.5 3 (14.3 ns) 6,690 30 845 210.2 DA 3 (13.1 ns) 6,005 0 871 229.6 Latency 76.3 3 (14.0 ns) 5,209 15 799 215.0 DA 3 (12.8 ns) 4,728 0 786 234.1 Latency 76.0 3 (13.4 ns) 3,498 22 639 223.2 DA 3 (12.3 ns) 3,308 0 661 244.8 Latency 75.9 3 (13.5 ns) 3,043 15 621 221.5 DA 3 (12.8 ns) 2,878 0 601 234.0 Table 5. Resource utilization and latency of the high-level feature jet tagging network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown.\n\n--- Segment 32 ---\nThe accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 15 Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] Latency 76.9 42 (57.6 ns) 16,081 57 26,484 729.4 DA 31 (44.1 ns) 12,682 0 19,056 702.2 Latency 76.6 32 (46.1 ns) 9,347 28 15,216 694.0 DA 26 (37.4 ns) 7,392 0 11,462 695.9 Latency 76.5 35 (67.2 ns) 8,548 30 14,418 520.8 DA 25 (35.4 ns) 6,448 0 10,109 707.2 Latency 76.3 33 (51.8 ns) 6,667 15 11,184 637.3 DA 22 (30.2 ns) 5,019 0 7,682 729.4 Latency 76.0 33 (48.3 ns) 4,471 22 7,279 683.5 DA 23 (32.6 ns) 3,602 0 5,693 706.2 Latency 75.9 31 (42.8 ns) 4,005 17 6,305 723.6 DA 21 (24.9 ns) 2,908 0 4,720 844.6 Table 6. Resource utilization and latency of the high-level feature jet tagging network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 1 GHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown. 5.2.2 SVHN Classification Network. We show the out-of-context results after place route for the SVHN classification network [12] on the VU9P FPGA. The network is a LeNet-like [22] convolutional network with dense classification head from [2], and the architecture is shown in Figure 7.\n\n--- Segment 33 ---\nWe show the out-of-context results after place route for the SVHN classification network [12] on the VU9P FPGA. The network is a LeNet-like [22] convolutional network with dense classification head from [2], and the architecture is shown in Figure 7. The trained networks are taken from [10]. In contrast to other networks shown in this work, resource takes place in these networks for the convolution filters applying to different inputs The designs are synthesized with Vivado HLS 2020.1 due to inconsistent behaviors of the DATAFLOW pragma used in hls4ml. More details for these networks can be found in [10]. The target clock frequency is set to 200 MHz, and the delay constraint is set to 2 for each CMVM operation. The results are shown in Table 7. From the table, it can be seen that the designs optimized with da4ml can reduce the LUT utilization by 1 4, while removing all DSPs. No timing violations are observed in all cases. Conv2D 16 filter: 3,3,3 Pixels 32,32,3 30,30,16 ReLU 30,30,16 MaxPool [2,2] 15,15,16 Dense kernel: 42,96 bias: 42 ReLU 42 42 Conv2D 16 filter: 3,3,16 13,13,16 ReLU 13,13,16 MaxPool [2,2] 6,6,16 Conv2D 24 filter: 3,3,16 4,4,24 ReLU 4,4,24 MaxPool [2,2] 2,2,24 Flatten 96 Dense kernel: 64,42 bias: 64 ReLU 64 64 Dense kernel: 10,64 bias: 10 Probability Logits 10 Fig. 7. The architecture of the SVHN classification network [12]. The network is a LeNet-like [22] network from [2]. The trained networks are taken from [10].\n\n--- Segment 34 ---\nThe network is a LeNet-like [22] network from [2]. The trained networks are taken from [10]. Manuscript submitted to ACM 16 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] Latency 93.9 1,050 (5,058.9 ns) 69,407 58 27,853 207.6 1,029 DA 1,045 (5,159.2 ns) 53,425 0 20,048 202.6 1,029 Latency 93.1 1,061 (5,026.0 ns) 47,314 30 20,582 211.1 1,029 DA 1,045 (4,952.3 ns) 36,739 0 15,491 211.0 1,029 Latency 91.9 1,058 (5,211.7 ns) 40,032 15 18,087 203.0 1,029 DA 1,045 (5,020.2 ns) 31,267 0 14,293 208.2 1,029 Latency 90.8 1,059 (4,868.2 ns) 34,435 13 17,261 217.5 1,029 DA 1,045 (5,041.1 ns) 28,078 0 13,038 207.3 1,029 Latency 89.9 1,056 (4,819.6 ns) 30,766 10 15,205 219.1 1,029 DA 1,045 (4,993.0 ns) 24,947 0 12,261 209.3 1,029 Latency 88.8 1,056 (5,120.5 ns) 27,982 6 14,736 206.2 1,029 DA 1,045 (5,134.1 ns) 23,609 0 12,021 203.5 1,029 Table 7. Resource utilization and latency of the SVHN classification network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu9p-flga2104-2L-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation.\n\n--- Segment 35 ---\nThe FPGA part is xcvu9p-flga2104-2L-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. 5.2.3 Muon Tracking Network. The results after place-and-route for the Muon Tracking network [12] on the VU13P FPGA are shown in Table 8. The network is a multi-stage network with mainly dense layers, and the architecture is shown in Figure 8. The trained networks are taken from [10], and all designs are synthesized with Vitis HLS 2023.2 and Vivado 2023.2. The target clock frequency is set to 160 MHz to match the previous work [36], and the delay constraint is set to 2 for each CMVM operation optimized. As the inputs of this network are all 1-bit, we do not apply da4ml to the initial convolutional layers, as they can be efficiently implemented using conditional accumulation logic. All implementations shown have an II of 1 clock cycle. Compared to the latency-optimized baseline from hls4ml, the da4ml-optimized designs not only reduce LUT utilization by approximately 10 but also entirely eliminate DSP usage. All designs met timing, while the latency- optimized designs in hls4ml are pipelined with more stages and have higher latency. Masked Dense kernel: 50,50 tanh Add Conv1D 1 filter: 3,3 Conv1D 1 filter: 3,2 Conv1D 1 filter: 3,2 tanh Masked Dense kernel: 50,50 Masked Dense kernel: 50,50 ReLU Add tanh Dense kernel: 28,50 bias: 28 ReLU Dense kernel: 14,28 bias: 14 ReLU Dense kernel: 8,14 bias: 8 Dense kernel: 1,8 bias: 1 Feature Extraction Dense Net M1 Input 50,3 M2 Input 50,2 M3 Input 50,2 28 28 14 14 8 8 Œ∏ Output 1 Fig. 8. The architecture of the Muon Tracking network [36]. The network is a multi-stage network with mainly dense layers.\n\n--- Segment 36 ---\nThe architecture of the Muon Tracking network [36]. The network is a multi-stage network with mainly dense layers. The masked dense layers refers to dense layers with specific sparsity pattern enforced on the weights, and please refer to [36] for the detailed implementation. The trained networks are taken from [10]. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 17 Strategy Resolution Latency [cycles] LUT DSP FF Fmax [MHz] Latency 1.95 11 (67.5 ns) 39,413 522 6,043 162.9 DA 9 (55.4 ns) 37,125 0 5,547 162.5 Latency 2.00 11 (66.0 ns) 34,460 154 5,263 166.6 DA 9 (56.0 ns) 27,832 0 3,147 160.8 Latency 2.09 12 (71.4 ns) 24,941 68 4,677 168.2 DA 8 (47.2 ns) 21,778 0 3,697 169.5 Latency 2.20 13 (75.9 ns) 21,557 41 4,699 171.3 DA 8 (46.6 ns) 18,895 0 3,105 171.6 Latency 2.39 10 (57.3 ns) 16,918 27 2,484 174.5 DA 8 (47.0 ns) 14,735 0 2,215 170.2 Latency 2.63 12 (60.6 ns) 13,306 10 3,429 197.9 DA 8 (44.0 ns) 12,318 0 2,166 181.9 Table 8. Resource utilization and latency of the Muon tracking network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 160 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. II is 1 for all designs shown. 5.2.4 Particle-based Jet Tagging Network. The particle-based jet tagging network [37] is an MLP Mixer based neural network for jet tagging.\n\n--- Segment 37 ---\n5.2.4 Particle-based Jet Tagging Network. The particle-based jet tagging network [37] is an MLP Mixer based neural network for jet tagging. We adopt the architecture from [37] with the 64-particle input, and 16-features per particle. The network is trained with the HGQ library [10]. The architecture is shown in Figure 9. For this neural network, Vitis HLS 2023.2 cannot produce designs with II of 1 clock cycle when using the original hls4ml implementation. We suspect the issue is related to the multidimensional linear operation implemented with Einstein Summation templates, and some pragma usage used may be inappropriate. However, we do not investigate this issue further as it is beyond the scope of this work. The results are shown in Table 9. The designs are synthesized with Vitis HLS 2023.2 and Vivado 2023.2, with a target clock frequency set to 200 MHz and a target II of 1. The delay constraint is set to 2 for each optimized CMVM operation. Marginal resource savings are observed for these designs for LUTs. However, the designs without da4ml failed to be fully pipelined with an II of 1, and the achieved Fmax is significantly lower than the target 200 MHz clock frequency. Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] Latency 81.4 35 (319.6 ns) 137,893 79 24,785 109.5 12 DA 13 (62.6 ns) 125,694 0 26,097 207.6 1 Latency 81.0 29 (201.0 ns) 72,299 81 13,510 144.3 5 DA 13 (63.8 ns) 68,157 0 13,147 203.9 1 Latency 80.3 29 (182.3 ns) 56,253 80 11,771 159.1 5 DA 12 (56.9 ns) 50,657 0 11,411 210.9 1 Latency 77.3 12 (69.6 ns) 25,828 61 5,662 172.5 2 DA 11 (53.4 ns) 25,002 0 5,554 205.8 1 Table 9.\n\n--- Segment 38 ---\nHowever, the designs without da4ml failed to be fully pipelined with an II of 1, and the achieved Fmax is significantly lower than the target 200 MHz clock frequency. Strategy Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] Latency 81.4 35 (319.6 ns) 137,893 79 24,785 109.5 12 DA 13 (62.6 ns) 125,694 0 26,097 207.6 1 Latency 81.0 29 (201.0 ns) 72,299 81 13,510 144.3 5 DA 13 (63.8 ns) 68,157 0 13,147 203.9 1 Latency 80.3 29 (182.3 ns) 56,253 80 11,771 159.1 5 DA 12 (56.9 ns) 50,657 0 11,411 210.9 1 Latency 77.3 12 (69.6 ns) 25,828 61 5,662 172.5 2 DA 11 (53.4 ns) 25,002 0 5,554 205.8 1 Table 9. Resource utilization and latency of the particle-based jet tagging network with and without da4ml generated with hls4ml, marked with DA and Latency for strategy, respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The accuracy is evaluated on the test set of the dataset. Manuscript submitted to ACM 18 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu MLP1 MLP1 MLP1 MLP1 MLP2 MLP2 MLP2 MLP2 N particles MLP1 (MLP3) DenseBn n 16 ReLU DenseBn 16 n ReLU DenseBn N 1 ReLU DenseBn N N ReLU MLP4 MLP2 MLP4 MLP4 MLP4 MLP4 MLP3 MLP3 MLP3 MLP3 HEAD 5 Classes DenseBn 16 16 ReLU DenseBn 16 16 ReLU HEAD DenseBn 16 16 ReLU DenseBn 16 5 n feature Fig. 9.\n\n--- Segment 39 ---\nManuscript submitted to ACM 18 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu MLP1 MLP1 MLP1 MLP1 MLP2 MLP2 MLP2 MLP2 N particles MLP1 (MLP3) DenseBn n 16 ReLU DenseBn 16 n ReLU DenseBn N 1 ReLU DenseBn N N ReLU MLP4 MLP2 MLP4 MLP4 MLP4 MLP4 MLP3 MLP3 MLP3 MLP3 HEAD 5 Classes DenseBn 16 16 ReLU DenseBn 16 16 ReLU HEAD DenseBn 16 16 ReLU DenseBn 16 5 n feature Fig. 9. The architecture of the particle-based jet tagging network [37]. The network is an MLP Mixer based neural network for jet tagging. The architecture is adopted from [37] with the 64-particle input, and 16-features per particle (ùëÅ 64 and ùëõ 16). The model consists of four MLP blocks with a single skip-connection. The implementation of each MLP and the classification head is shown in the corresponding dashed blocks. MLP1 and MLP3 act on the feature dimension; MLP2 and MLP4 act on the particle dimension. DenseBn represents a dense layer followed by a batch normalization layer during training, which are fused into a single layer during inference. FPGA x0 x1 x2 x3 y0 y1 y2 y3 1 1da 1 1 1 1 2 1 -1 -2 1 -1 -1 1 1 -2 2 1 x0 x1 x2 x3 ml 4 Manual deÔ¨Ånition Verilog out Einsum(inp, w0) out ReLU(out) out Quantize(out) out Einsum(inp, w1) ... bitstream Fig. 10. The workflow of the standalone workflow with da4ml. The user defines the network in a functional style, and the code generation is performed with da4ml to generate synthesizable, pipelined Verilog code.\n\n--- Segment 40 ---\nThe workflow of the standalone workflow with da4ml. The user defines the network in a functional style, and the code generation is performed with da4ml to generate synthesizable, pipelined Verilog code. 5.3 Standalone Code Generation Other than the main workflow introduced in Section 4, da4ml can also work standalone to generate Verilog designs for a subset of neural networks, allowing for easier integration into existing RTL workflows. Currently, the generated designs must be either fully pipelined with an II of 1 or combinational. In addition, da4ml only supports networks with generalized CMVM operations (i.e., including convolution and Einstein Summation between constants and inputs), arithmetic operations excluding division, and rectified linear unit (ReLU) activations for standalone code generation. In the current implementation, manually defining the network in a functional style is required, and the workflow is shown in Figure 10. Pipelining of the generated design is performed by layers of adders, in which each adder is assumed to have a constant latency. While this approximation is not accurate, we use it to simplify the pipelining process, as we notice that routing delay dominates the latency in most cases in our experiments and more complex modeling of the adders latency does not yield significant improvements. We also reserve an API for the user to specify the latency of each adder depending on the bitwidths of the operands.\n\n--- Segment 41 ---\nWhile this approximation is not accurate, we use it to simplify the pipelining process, as we notice that routing delay dominates the latency in most cases in our experiments and more complex modeling of the adders latency does not yield significant improvements. We also reserve an API for the user to specify the latency of each adder depending on the bitwidths of the operands. We show the performance of the high-level feature jet tagging network with 200 MHz and 1 GHz target clock frequencies in Table 10 and Table 11, and the results for the particle-based jet tagging network with MLP Mixer in Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 19 Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] hls4ml DA 76.9 4 (18.9 ns) 12,250 0 1,502 211.5 da4ml 4 (20.0 ns) 12,032 0 3,113 200.1 hls4ml DA 76.6 3 (13.3 ns) 6,869 0 966 226.0 da4ml 4 (19.5 ns) 6,153 0 1,775 205.4 hls4ml DA 76.5 3 (13.1 ns) 6,005 0 871 229.6 da4ml 4 (19.8 ns) 5,613 0 1,539 202.0 hls4ml DA 76.3 3 (12.8 ns) 4,728 0 786 234.1 da4ml 3 (14.8 ns) 4,129 0 903 202.4 hls4ml DA 76.0 3 (12.3 ns) 3,308 0 661 244.8 da4ml 3 (14.9 ns) 2,814 0 628 201.0 hls4ml DA 75.9 3 (12.8 ns) 2,878 0 601 234.0 da4ml 3 (14.7 ns) 2,485 0 683 204.2 Table 10. Resource utilization and latency of the high-level feature jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively.\n\n--- Segment 42 ---\nWe show the performance of the high-level feature jet tagging network with 200 MHz and 1 GHz target clock frequencies in Table 10 and Table 11, and the results for the particle-based jet tagging network with MLP Mixer in Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 19 Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] hls4ml DA 76.9 4 (18.9 ns) 12,250 0 1,502 211.5 da4ml 4 (20.0 ns) 12,032 0 3,113 200.1 hls4ml DA 76.6 3 (13.3 ns) 6,869 0 966 226.0 da4ml 4 (19.5 ns) 6,153 0 1,775 205.4 hls4ml DA 76.5 3 (13.1 ns) 6,005 0 871 229.6 da4ml 4 (19.8 ns) 5,613 0 1,539 202.0 hls4ml DA 76.3 3 (12.8 ns) 4,728 0 786 234.1 da4ml 3 (14.8 ns) 4,129 0 903 202.4 hls4ml DA 76.0 3 (12.3 ns) 3,308 0 661 244.8 da4ml 3 (14.9 ns) 2,814 0 628 201.0 hls4ml DA 75.9 3 (12.8 ns) 2,878 0 601 234.0 da4ml 3 (14.7 ns) 2,485 0 683 204.2 Table 10. Resource utilization and latency of the high-level feature jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The Verilog designs are pipelined every 5 adders. II is 1 for all designs shown.\n\n--- Segment 43 ---\nThe Verilog designs are pipelined every 5 adders. II is 1 for all designs shown. Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] hls4ml DA 76.9 31 (44.1 ns) 12,682 0 19,056 702.2 da4ml 20 (34.0 ns) 12,298 0 13,664 588.6 hls4ml DA 76.6 26 (37.4 ns) 7,392 0 11,462 695.9 da4ml 16 (27.5 ns) 6,944 0 7,419 582.4 hls4ml DA 76.5 25 (35.4 ns) 6,448 0 10,109 707.2 da4ml 17 (23.1 ns) 6,165 0 7,207 735.8 hls4ml DA 76.3 22 (30.2 ns) 5,019 0 7,682 729.4 da4ml 15 (20.8 ns) 4,655 0 5,258 722.0 hls4ml DA 76.0 23 (32.6 ns) 3,602 0 5,693 706.2 da4ml 14 (20.6 ns) 3,127 0 3,587 681.2 hls4ml DA 75.9 21 (24.9 ns) 2,908 0 4,720 844.6 da4ml 13 (19.0 ns) 2,734 0 3,139 685.4 Table 11. Resource utilization and latency of the high-level feature jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively. The FPGA part is xcvu13p-flga2577-2-e with 1 GHz target clock frequency. Delay constraint is set to 2 for the da4ml optimized designs for each CMVM operation. The Verilog designs are pipelined every adder. II is 1 for all designs shown. Table 12, respectively. The designs generated with hls4ml are marked with hls4ml DA , and the designs generated directly by da4ml with Verilog are marked with da4ml .\n\n--- Segment 44 ---\nTable 12, respectively. The designs generated with hls4ml are marked with hls4ml DA , and the designs generated directly by da4ml with Verilog are marked with da4ml . All designs are pipelined with an II of 1, and the delay constraint is set to 2 for each CMVM operation. For the standalone generated Verilog designs, we pipeline every 5 adders for the 200 MHz target clock frequency and every adder for the 1 GHz target clock frequency. Global retiming is enabled in Vivado for the Verilog designs. The results show that the designs generated with da4ml can achieve lower LUT consumption by 5 20 compared to the designs generated in conjunction with hls4ml and synthesized with Vitis HLS. However, the reached Fmax is generally slightly lower. In particular, it can be noticed that the designs generated in conjunction with hls4ml are Manuscript submitted to ACM 20 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] hls4ml DA 81.4 13 (62.6 ns) 125,694 0 26,097 207.6 da4ml 11 (54.8 ns) 120,512 0 28,284 200.8 hls4ml DA 81.0 13 (63.8 ns) 68,157 0 13,147 203.9 da4ml 11 (54.8 ns) 61,771 0 16,031 200.6 hls4ml DA 80.3 12 (56.9 ns) 50,657 0 11,411 210.9 da4ml 10 (48.4 ns) 47,778 0 12,515 206.8 hls4ml DA 77.3 11 (53.4 ns) 25,002 0 5,554 205.8 da4ml 9 (41.9 ns) 24,285 0 6,262 214.8 Table 12. Resource utilization and latency of the particle-based jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency.\n\n--- Segment 45 ---\nResource utilization and latency of the particle-based jet tagging network generated with hls4ml with da4ml, and with da4ml directly via Verilog, marked with hls4ml DA and da4ml , respectively. The FPGA part is xcvu13p-flga2577-2-e with 200 MHz target clock frequency. Delay constraint is set to 2 for the da4ml for each CMVM operation. The Verilog designs are pipelined every 5 adders. II is 1 for all designs shown. pipelined with more stages than the adder graph depth, which suggests that the HLS compiler is further breaking down the graph for timing optimization, a process not performed in the standalone code generation. We suspect this is the reason for the lower Fmax in the designs generated with da4ml. It is worth noting that designs generated with da4ml directly can significantly reduce compilation time: for designs with a large amount of code, HLS can take a very long time. For instance, the synthesis time for the particle-based jet tagging networks3 with Vitis HLS and Vivado 2023.2 is around 17 hours without memory bottleneck when optimized with da4ml. However, the designs of the same networks generated directly in Verilog with da4ml only take around 26 minutes to synthesize with Vivado 2023.2. The peak memory usage also significantly reduced from 70 GB to 10 GB. These results suggest that da4ml s standalone code generation can facilitate fast prototyping and allowing for faster iterations in software-hardware co-design. 5.4 Comparison to Other Methods While this work focuses on distributed arithmetic optimization for CMVM operations in FPGA-based neural networks, alternative methods exist for implementing machine learning-based models on FPGAs. In this section, we briefly compare neural networks optimized with da4ml to other methods. We show the comparasion in Table 13, where we compare the performance of the networks shown in earlier sections against other state-of-the-art methods. It is important to differentiate our approach from methods such as TreeLUT [18], NeuraLUT-Assemble [5], and DWN [7]. While da4ml optimizes the implementation of quantized traditional neural networks, these methods re- formulates the entire model into inherently hardware-native structures.\n\n--- Segment 46 ---\nIt is important to differentiate our approach from methods such as TreeLUT [18], NeuraLUT-Assemble [5], and DWN [7]. While da4ml optimizes the implementation of quantized traditional neural networks, these methods re- formulates the entire model into inherently hardware-native structures. In particular, TreeLUT defines the model as collection of quantized decision trees, while NeuraLUT-Assemble and DWN construct models directly mapped into LUTs with particular fan-in. These methods are highly specialized and may achieve exceptional efficiency in terms of resource utilization and throughput, while they often come at the cost of reduced generalizability and slight accuracy degradation. In contrast, da4ml focuses on optimizing the core CMVM operations of quantized, conventional neural networks, which would enable more straightforward integration with existing machine learning workflows and is applicable on more complex tasks. In particular, we show that when the network is properly quantized with HGQ [10], 3All-inclusive time, until routing finishes Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 21 High-level feature jet tagging network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 76.9 31 (44.1 ns) 12,682 0 19,056 702.2 1 This Work: HGQ da4ml (RTL) 76.5 17 (23.1 ns) 6,165 0 7,207 735.8 1 This Work: HGQ hls4ml 76.9 42 (57.6 ns) 16,081 57 26,484 729.4 1 This Work: HGQ hls4ml 76.5 35 (67.2 ns) 8,548 30 14,418 520.8 1 QKeras hls4ml [34] 76.3 15 (105 ns) 5,504 175 3,036 142.9 2 DWN [7] 76.3 10 (14.4 ns) 6,302 0 4,128 695.\n\n--- Segment 47 ---\nIn contrast, da4ml focuses on optimizing the core CMVM operations of quantized, conventional neural networks, which would enable more straightforward integration with existing machine learning workflows and is applicable on more complex tasks. In particular, we show that when the network is properly quantized with HGQ [10], 3All-inclusive time, until routing finishes Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 21 High-level feature jet tagging network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 76.9 31 (44.1 ns) 12,682 0 19,056 702.2 1 This Work: HGQ da4ml (RTL) 76.5 17 (23.1 ns) 6,165 0 7,207 735.8 1 This Work: HGQ hls4ml 76.9 42 (57.6 ns) 16,081 57 26,484 729.4 1 This Work: HGQ hls4ml 76.5 35 (67.2 ns) 8,548 30 14,418 520.8 1 QKeras hls4ml [34] 76.3 15 (105 ns) 5,504 175 3,036 142.9 2 DWN [7] 76.3 10 (14.4 ns) 6,302 0 4,128 695. 1 MetaML-Pro [32] 76.1 10 (50 ns) 13,042 70 N A 200 1 NeuraLUT-Assemble [5] 76.0 2 (2.1 ns) 1,780 0 540 940. 1 TreeLUT [18] 75.6 2 (2.7 ns) 2,234 0 347 735.\n\n--- Segment 48 ---\n1 MetaML-Pro [32] 76.1 10 (50 ns) 13,042 70 N A 200 1 NeuraLUT-Assemble [5] 76.0 2 (2.1 ns) 1,780 0 540 940. 1 TreeLUT [18] 75.6 2 (2.7 ns) 2,234 0 347 735. 1 Muon tracking network Implementation Resolution [mrad] Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 1.95 9 (55.4 ns) 37,125 0 5,547 162.5 1 HGQ hls4ml [10] 1.95 11 (67.5 ns) 39,413 522 6,043 162.9 1 QKeras hls4ml [36] 1.95 17 (106.3 ns) 37,867 1,762 8,443 160 1 SVHN classification network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 93.9 1,045 (5,159.2 ns) 53,425 0 20,048 202.6 1,029 HGQ hls4ml [10] 93.9 1,050 (5,058.9 ns) 69,407 58 27,853 207.6 1,029 QKeras hls4ml [2] 94.\n\n--- Segment 49 ---\n1 TreeLUT [18] 75.6 2 (2.7 ns) 2,234 0 347 735. 1 Muon tracking network Implementation Resolution [mrad] Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 1.95 9 (55.4 ns) 37,125 0 5,547 162.5 1 HGQ hls4ml [10] 1.95 11 (67.5 ns) 39,413 522 6,043 162.9 1 QKeras hls4ml [36] 1.95 17 (106.3 ns) 37,867 1,762 8,443 160 1 SVHN classification network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (HLS) 93.9 1,045 (5,159.2 ns) 53,425 0 20,048 202.6 1,029 HGQ hls4ml [10] 93.9 1,050 (5,058.9 ns) 69,407 58 27,853 207.6 1,029 QKeras hls4ml [2] 94. 1,035 (5,175 ns) 111,152 174 32,554 200 1,030 QKeras hls4ml [34] 92.4 5,447 (43,576 ns) 59,279 1,215 46,584 125 N A Particle-based jet tagging network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (RTL) 81.4 11 (54.8 ns) 120,512 0 28,284 200.8 1 LL-GNN [33] 81.2 181 (905 ns) 815k 8,986 189k 200 150 QKeras hls4ml, DS [29] 75.9 26 (130 ns) 903,284 434 358,754 200 2 QKeras hls4ml, GNN [29] 75.8 32 (160 ns) 1,162,104 2,120 761,061 200 3 Table 13. Resource and performance of various neural network tasks with different implementations.\n\n--- Segment 50 ---\n1,035 (5,175 ns) 111,152 174 32,554 200 1,030 QKeras hls4ml [34] 92.4 5,447 (43,576 ns) 59,279 1,215 46,584 125 N A Particle-based jet tagging network Implementation Accuracy Latency [cycles] LUT DSP FF Fmax [MHz] II [cycles] This Work: HGQ da4ml (RTL) 81.4 11 (54.8 ns) 120,512 0 28,284 200.8 1 LL-GNN [33] 81.2 181 (905 ns) 815k 8,986 189k 200 150 QKeras hls4ml, DS [29] 75.9 26 (130 ns) 903,284 434 358,754 200 2 QKeras hls4ml, GNN [29] 75.8 32 (160 ns) 1,162,104 2,120 761,061 200 3 Table 13. Resource and performance of various neural network tasks with different implementations. When the neural network is quantized with HGQ [10], we show that traditional neural networks may achieve competitive resource utilization, throughput, and superior accuracy compared with the methods designed specifically for FPGAs. All works are using either xcvu13p-flga2577-2-e or xcvu9p-flgb2104-2-i, where the results are comparable. da4ml (HLS) refers to using da4ml with the hls4ml workflow, while da4ml (RTL) refers to the standalone workflow. Works marked with report the results after logic synthesis without place and route. We put the target clock frequency for Fmax and used it to compute the latency for these works. DWN results are cited from [5] instead, as the original paper omits preprocessing steps for the firmware implementation, which are added in [5] for a fair comparison. Works marked with reports resource utilization after place and routes, but did not report Fmax. We use the achieved target clock as Fmax for these works. traditional neural networks can achieve competitive resource utilization throughput - accuracy tradeoff compared to those methods highly co-designed for FPGAs.\n\n--- Segment 51 ---\nWe use the achieved target clock as Fmax for these works. traditional neural networks can achieve competitive resource utilization throughput - accuracy tradeoff compared to those methods highly co-designed for FPGAs. Comparing to other conventional neural network based methods shown in Table 13, our proposed da4ml framework demonstrates significant advantage across diverse workloads, offering a favorable balance between accuracy, latency, and hardware efficiency. Notably, our designs consistently eliminate DSP usage while maintaining competitive or superior accuracy, making them suited for resource-constrained FPGA designs. Similarly, in the muon tracking and SVHN classification tasks, our method matches the accuracy of existing approaches while reducing LUT and flip-flop utilization. Furthermore, the particle-based jet tagging design achieves the highest accuracy (81.4 ) while consuming more than 10 fewer LUTs and FFs than state-of-the-art GNN-based implementations. Across all tasks, our designs Manuscript submitted to ACM 22 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu maintain a consistent II of 1 cycle, ensuring high-throughput operation. These results underscore the generalizability and practicality of our approach for low-latency, resource-efficient FPGA deployment in both traditional and physics-inspired neural networks. 6 Conclusion In this work, we present an efficient algorithm for optimizing CMVM operations with distributed arithmetic for FPGA- based neural networks. We further present the da4ml library, which implements the algorithm efficiently and tightly integrates it with the hls4ml library for easy adoption. We then demonstrate the performance of the algorithm on a set of neural networks, including dense, convolutional, and MLP-Mixer-based networks. The results show that designs optimized with da4ml can simultaneously reduce resource utilization and timing on realistic, heavily quantized, and sparse neural networks for physics applications. We also show that for a limited set of networks, da4ml can generate Verilog designs directly for fast prototyping and software-hardware co-design. The results show that designs generated directly with da4ml could further reduce resource utilization and synthesis time significantly at the expense of a slightly lower Fmax. We hope this work will facilitate the design of efficient neural networks for FPGAs, especially in the context of physics applications, and provide a solid foundation for future work on hardware-aware neural network design.\n\n--- Segment 52 ---\nThe results show that designs generated directly with da4ml could further reduce resource utilization and synthesis time significantly at the expense of a slightly lower Fmax. We hope this work will facilitate the design of efficient neural networks for FPGAs, especially in the context of physics applications, and provide a solid foundation for future work on hardware-aware neural network design. 7 Acknowledgments Chang Sun and Maria Spiropulu are partially supported by the U.S. Department of Energy (DOE), Office of Science, Office of High Energy Physics grant DE-SC0011925. Chang Sun is partially supported for computing resources by the NSF ACCESS Grant number PHY240298. Chang Sun and Maria Spiropulu acknowledge partial support by the DOE AI4HEP award. Zhiqiang Que and Wayne Luk are supported by the United Kingdom EPSRC (grant numbers UKRI256, EP V028251 1, EP N031768 1, EP S030069 1, and EP X036006 1). Vladimir Loncar is supported by the Eric Wendy Schmidt Fund for Strategic Innovation through the CERN Next Generation Triggers project under grant agreement number SIF-2023-004. References [1] 2024. 2024 Data Collected with AXOL1TL Anomaly Detection at the CMS Level-1 Trigger. (2024). [2] Thea Aarrestad, Vladimir Loncar, Nicol√≤ Ghielmetti, Maurizio Pierini, Sioni Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus Linander, Yutaro Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris, Dylan Rankin, Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward Kreinar, Zhenbin Wu, and Duc Hoang. 2021. Fast convolutional neural networks on FPGAs with hls4ml. Machine Learning: Science and Technology 2, 4 (jul 2021), 045015. [3] Levent Aksoy, Eduardo da Costa, Paulo Flores, and Jos√© Monteiro. 2012. Multiplierless Design of Linear DSP Transforms. In VLSI-SoC: Advanced Research for Systems on Chip, Salvador Mir, Chi-Ying Tsui, Ricardo Reis, and Oliver C. S. Choy (Eds.).\n\n--- Segment 53 ---\nMultiplierless Design of Linear DSP Transforms. In VLSI-SoC: Advanced Research for Systems on Chip, Salvador Mir, Chi-Ying Tsui, Ricardo Reis, and Oliver C. S. Choy (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 73 93. [4] Levent Aksoy, Paulo Flores, and Jos√© Monteiro. 2015. A Novel Method for the Approximation of Multiplierless Constant Matrix Vector Multiplication. In 2015 IEEE 13th International Conference on Embedded and Ubiquitous Computing. 98 105. [5] Marta Andronic and George A. Constantinides. 2025. NeuraLUT-Assemble: Hardware-Aware Assembling of Sub-Neural Networks for Efficient LUT Inference. In 2025 IEEE 33rd Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). 208 216. https: doi.org 10.1109 FCCM62733.2025.00077 [6] Algirdas Avizienis. 1961. Signed-Digit Numbe Representations for Fast Parallel Arithmetic. IRE Transactions on Electronic Computers EC-10, 3 (1961), 389 400. [7] Alan Tendler Leibel Bacellar, Zachary Susskind, Mauricio Breternitz Jr, Eugene John, Lizy Kurian John, Priscila Machado Vieira Lima, and Felipe M.G. Fran√ßa. 2024. Differentiable Weightless Neural Networks. In Proceedings of the 41st International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 235), Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR, 2277 2295. Manuscript submitted to ACM da4ml: Distributed Arithmetic for Real-time Neural Networks on FPGAs 23 [8] D. Benyamin, W. Luk, and J. Villasenor. 1999. Optimizing FPGA-based vector product designs. In Seventh Annual IEEE Symposium on Field- Programmable Custom Computing Machines (Cat. No.PR00375). 188 197. [9] N. Boullis and A. Tisserand. 2005.\n\n--- Segment 54 ---\n[9] N. Boullis and A. Tisserand. 2005. Some optimizations of hardware multiplication by constant matrices. IEEE Trans. Comput. 54, 10 (2005), 1271 1282. [10] Sun Chang, Thea √Örrestad, Vladimir Lonƒçar, Jennifer Ngadiuba, and Maria Spiropulu. 2024. Gradient-based Automatic Per-Weight Mixed Precision Quantization for Neural Networks On-Chip. [11] Claudionor N. Coelho, Aki Kuusela, Shan Li, Hao Zhuang, Jennifer Ngadiuba, Thea Klaeboe Aarrestad, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, and Sioni Summers. 2021. Automatic heterogeneous quantization of deep neural networks for low-latency inference on the edge for particle detectors. Nature Machine Intelligence 3, 8 (jun 2021), 675 686. [12] Farah Fahim, Benjamin Hawks, Christian Herwig, James Hirschauer, Sergo Jindariani, Nhan Tran, Luca P. Carloni, Giuseppe Di Guglielmo, Philip C. Harris, Jeffrey D. Krupa, Dylan Rankin, Manuel Blanco Valentin, Josiah Hester, Yingyi Luo, John Mamish, Seda Orgrenci-Memik, Thea Aarrestad, Hamza Javed, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, Sioni Summers, Javier M. Duarte, Scott Hauck, Shih-Chieh Hsu, Jennifer Ngadiuba, Mia Liu, Duc Hoang, Edward Kreinar, and Zhenbin Wu. 2021. hls4ml: An Open-Source Codesign Workflow to Empower Scientific Low-Power Machine Learning Devices. CoRR abs 2103.05579 (2021). arXiv:2103.05579 [13] Ekaterina Govorkova, Ema Puljak, Thea Aarrestad, Thomas James, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, Nicol√≤ Ghielmetti, Maksymilian Graczyk, Sioni Summers, Jennifer Ngadiuba, Thong Q. Nguyen, Javier Duarte, and Zhenbin Wu. 2021.\n\n--- Segment 55 ---\narXiv:2103.05579 [13] Ekaterina Govorkova, Ema Puljak, Thea Aarrestad, Thomas James, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, Nicol√≤ Ghielmetti, Maksymilian Graczyk, Sioni Summers, Jennifer Ngadiuba, Thong Q. Nguyen, Javier Duarte, and Zhenbin Wu. 2021. Autoencoders on FPGAs for real-time, unsupervised new physics detection at 40 MHz at the Large Hadron Collider. [14] Ekaterina Govorkova, Ema Puljak, Thea Aarrestad, Thomas James, Vladimir Loncar, Maurizio Pierini, Adrian Alan Pol, Nicol√≤ Ghielmetti, Maksymilian Graczyk, Sioni Summers, Jennifer Ngadiuba, Thong Q. Nguyen, Javier Duarte, and Zhenbin Wu. 2022. Autoencoders on field-programmable gate arrays for real-time, unsupervised new physics detection at 40 MHz at the Large Hadron Collider. Nature Machine Intelligence 4, 2 (feb 2022), 154 161. [15] Anup Hosangadi, Farzan Fallah, and Ryan Kastner. 2005. Reducing hardware complexity of linear DSP systems by iteratively eliminating two-term common subexpressions. In Proceedings of the 2005 Asia and South Pacific Design Automation Conference (Shanghai, China) (ASP-DAC 05). Association for Computing Machinery, New York, NY, USA, 523 528. [16] Anup Hosangadi, Farzan Fallah, and Ryan Kastner. 2005. Simultaneous Optimization of Delay and Number of Operations in Multiplierless Implementation of Linear Systems. International Workshop on Logic and Synthesis (IWLS) (2005). [17] Kai Huang and Wei Gao. 2022. Real-time neural network inference on extremely weak devices: agile offloading with explainable AI. In Proceedings of the 28th Annual International Conference on Mobile Computing And Networking (Sydney, NSW, Australia) (MobiCom 22). Association for Computing Machinery, New York, NY, USA, 200 213. [18] Alireza Khataei and Kia Bazargan. 2025.\n\n--- Segment 56 ---\n[18] Alireza Khataei and Kia Bazargan. 2025. TreeLUT: An Efficient Alternative to Deep Neural Networks for Inference Acceleration Using Gradient Boosted Decision Trees. In Proceedings of the 2025 ACM SIGDA International Symposium on Field Programmable Gate Arrays (FPGA 25). ACM, 14 24. [19] Martin Kumm, Martin Hardieck, and Peter Zipf. 2017. Optimization of Constant Matrix Multiplication with Low Power and High Throughput. IEEE Trans. Comput. 66, 12 (2017), 2072 2080. [20] Siu Kwan Lam, Antoine Pitrou, and Stanley Seibert. 2015. Numba: A llvm-based python jit compiler. In Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC. 1 6. [21] Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for Lifelong Program Analysis Transformation. In Proceedings of the International Symposium on Code Generation and Optimization: Feedback-Directed and Runtime Optimization (Palo Alto, California) (CGO 04). IEEE Computer Society, USA, 75. [22] Yann LeCun, Bernhard E. Boser, John S. Denker, Donnie Henderson, Richard E. Howard, Wayne E. Hubbard, and Lawrence D. Jackel. 1989. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation 1 (1989), 541 551. 41312633 [23] Alexander Lehnert, Philipp Holzinger, Simon Pfenning, Ralf M√ºller, and Marc Reichenbach. 2023. Most Resource Efficient Matrix Vector Multiplication on FPGAs. IEEE Access 11 (2023), 3881 3898. [24] Ying Li, Chungan Peng, Dunshan Yu, and Xing Zhang. 2008. The implementation methods of high speed FIR filter on FPGA. In 2008 9th International Conference on Solid-State and Integrated-Circuit Technology. 2216 2219. [25] Ying Li, Chungan Peng, Dunshan Yu, and Xing Zhang. 2008. The implementation methods of high speed FIR filter on FPGA. In 2008 9th International Conference on Solid-State and Integrated-Circuit Technology.\n\n--- Segment 57 ---\nThe implementation methods of high speed FIR filter on FPGA. In 2008 9th International Conference on Solid-State and Integrated-Circuit Technology. 2216 2219. [26] Songlin Lyu, Jiawen Cheng, Yun Shao, Yong Xiao, and Wenjian Yu. 2022. Multi-Constant Multiplication Optimization Based on Common Sub-Expression Elimination. In 2022 IEEE 16th International Conference on Solid-State Integrated Circuit Technology (ICSICT). 1 3. https: doi.org 10.1109 ICSICT55466.2022.9963464 [27] Shahnam Mirzaei, Anup Hosangadi, and Ryan Kastner. 2006. FPGA Implementation of High Speed FIR Filters Using Add and Shift Method. In 2006 International Conference on Computer Design. 308 313. [28] Wei Niu, Zhengang Li, Xiaolong Ma, Peiyan Dong, Gang Zhou, Xuehai Qian, Xue Lin, Yanzhi Wang, and Bin Ren. 2022. GRIM: A General, Real-Time Deep Learning Inference Framework for Mobile Devices Based on Fine-Grained Structured Weight Sparsity. IEEE Trans. Pattern Anal. Mach. Intell. 44, 10_Part_1 (oct 2022), 6224 6239. Manuscript submitted to ACM 24 Chang Sun, Zhiqiang Que, Vladimir Loncar, Wayne Luk, and Maria Spiropulu [29] Patrick Odagiu, Zhiqiang Que, Javier Duarte, Johannes Haller, Gregor Kasieczka, Artur Lobanov, Vladimir Loncar, Wayne Luk, Jennifer Ngadiuba, Maurizio Pierini, Philipp Rincke, Arpita Seksaria, Sioni Summers, Andre Sznajder, Alexander Tapper, and Thea K √Örrestad. 2024. Ultrafast jet classification at the HL-LHC. Machine Learning: Science and Technology 5, 3 (July 2024), 035017. [30] M. Potkonjak, M.B. Srivastava, and A.P. Chandrakasan. 1996. Multiple constant multiplications: efficient and versatile framework and algorithms for exploring common subexpression elimination. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 15, 2 (1996), 151 165.\n\n--- Segment 58 ---\nMultiple constant multiplications: efficient and versatile framework and algorithms for exploring common subexpression elimination. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 15, 2 (1996), 151 165. [31] Robert Clay Prim. 1957. Shortest connection networks and some generalizations. The Bell System Technical Journal 36, 6 (1957), 1389 1401. [32] Zhiqiang Que, Jose G. F. Coutinho, Ce Guo, Hongxiang Fan, and Wayne Luk. 2025. MetaML-Pro: Cross-Stage Design Flow Automation for Efficient Deep Learning Acceleration. arXiv:2502.05850 [cs.AR] [33] Zhiqiang Que, Hongxiang Fan, Marcus Loo, He Li, Michaela Blott, Maurizio Pierini, Alexander Tapper, and Wayne Luk. 2024. LL-GNN: Low Latency Graph Neural Networks on FPGAs for High Energy Physics. ACM Transactions on Embedded Computing Systems 23, 2 (March 2024), 1 28. [34] Benjamin Ramhorst, George A. Constantinides, and Vladimir Loncar. 2023. FPGA Resource-aware Structured Pruning for Real-Time Neural Networks. arXiv:2308.05170v1 [cs.AR] [35] Raghubir Singh and Sukhpal Singh Gill. 2023. Edge AI: A survey. Internet of Things and Cyber-Physical Systems 3 (2023), 71 92. 1016 j.iotcps.2023.02.004 [36] Chang Sun, Takumi Nakajima, Yuki Mitsumori, Yasuyuki Horii, and Makoto Tomoto. 2023. Fast muon tracking with machine learning implemented in FPGA. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment 1045 (Jan. 2023), 167546. [37] Chang Sun, Jennifer Ngadiuba, Maurizio Pierini, and Maria Spiropulu. 2025. Fast Jet Tagging with MLP-Mixers on FPGAs. (2025). arXiv:2503.03103 [physics.ins-det] [38] The ATLAS Collaboration. 2017.\n\n--- Segment 59 ---\narXiv:2503.03103 [physics.ins-det] [38] The ATLAS Collaboration. 2017. Technical Design Report for the Phase-II Upgrade of the ATLAS TDAQ System. Technical Report. CERN, Geneva. [39] The CMS Collaboration. 2020. The Phase-2 Upgrade of the CMS Level-1 Trigger. Technical Report. CERN, Geneva. Final version. [40] The LHC Study Group. 1995. The Large Hadron Collider, Conceptual Design. Technical Report. CERN AC 95-05 (LHC) Geneva. [41] Yevgen Voronenko and Markus P√ºschel. 2007. Multiplierless multiple constant multiplication. ACM Trans. Algorithms 3, 2 (May 2007), 11 es. [42] T. Wiegand, G.J. Sullivan, G. Bjontegaard, and A. Luthra. 2003. Overview of the H.264 AVC video coding standard. IEEE Transactions on Circuits and Systems for Video Technology 13, 7 (2003), 560 576. [43] Yang Yang, Yury Kartynnik, Pen Li, Jiuqiang Tang, Xing Li, George Sung, and Matthias Grundmann. 2024. StreamVC: Real-Time Low-Latency Voice Conversion. [44] Aymen-Alaeddine Zeghaida, Dinesh Daultani, J.M. Pierre Langlois, and Jean Pierre David. 2024. Scalable Low-Complexity Implementation of Constant Matrix Multiplication Circuits. In 2024 IEEE 67th International Midwest Symposium on Circuits and Systems (MWSCAS). 357 361. [45] I. Zurbano Fernandez et al. 2020. High-Luminosity Large Hadron Collider (HL-LHC): Technical design report. CERN Yellow Reports: Monographs 10 2020 (12 2020). Manuscript submitted to ACM\n\n