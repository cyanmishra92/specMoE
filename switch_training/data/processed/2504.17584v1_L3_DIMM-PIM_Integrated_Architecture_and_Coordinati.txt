=== ORIGINAL PDF: 2504.17584v1_L3_DIMM-PIM_Integrated_Architecture_and_Coordinati.pdf ===\n\nRaw text length: 112498 characters\nCleaned text length: 109407 characters\nNumber of segments: 71\n\n=== CLEANED TEXT ===\n\nL3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Qingyuan Liu , Liyan Chen , Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Shanghai Jiao Tong University Shanghai, China Abstract Large Language Models (LLMs) increasingly require processing long text sequences, but GPU memory limitations force difficult trade-offs between memory capacity and bandwidth. While HBM- based acceleration offers high bandwidth, its capacity remains con- strained. Offloading data to host-side DIMMs improves capacity but introduces costly data swapping overhead. We identify that the critical memory bottleneck lies in the decoding phase of multi-head attention (MHA) exclusively, which demands substantial capacity for storing KV caches and high bandwidth for attention computa- tion. Our key insight reveals this operation uniquely aligns with modern DIMM-based processing-in-memory (PIM) architectures, which offers scalability of both capacity and bandwidth. Based on this observation and insight, we propose L3, a hardware- software co-designed system integrating DIMM-PIM and GPU de- vices. L3 introduces three innovations: First, hardware redesigns resolve data layout mismatches and computational element mis- matches in DIMM-PIM, enhancing LLM inference utilization. Sec- ond, communication optimization enables hiding the data transfer overhead with the computation. Third, an adaptive scheduler coordi- nates GPU-DIMM-PIM operations to maximize parallelism between devices. Evaluations using real-world traces show L3 achieves up to 6.1 speedup over state-of-the-art HBM-PIM solutions while significantly improving batch sizes. 1 Introduction The growing adoption of Large Language Models (LLMs) has cre- ated an imperative for long-context processing capabilities, driven by the increasingly complex and rich user prompts [27, 88, 91]. Today s reasoning models [19, 34, 75] exacerbate this demand by employing multi-round thinking to enhance the quality of output content, thereby increasing the demand for long outputs. Real- world traces from OpenR1 [5] show an average output length of more than 6k tokens. When supporting long-context LLM inference, current GPU systems face memory constraints in two aspects: Equal contribution. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from Conference 17, July 2017, Washington, DC, USA 2025 Copyright held by the owner author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x YY MM First is the unscalable memory capacity constraint. Since inference requests with long context have large KV cache whose memory capacity demand scales linearly with the number of context tokens, the spare and expensive GPU HBM (High-Bandwidth-Memory [41]) constrains the batch sizes of requests. For example, 2.28 requests of GPT-175B with 8k token length could consume the entire memory of one A100 [3] (80GB). The constrained batch sizes of long-context requests further limit the GPU utilization and throughput. Second is the unscalable memory bandwidth constraint. HBM falls short in providing scalable bandwidth due to its high integration overhead on either TSV [59] (Through-Silicon-Via) densities (single stack) or interposer dimensions [35, 52] (multiple stacks). Moreover, it is challenging to add additional HBM stacks for already packaged chips. Since the increasing KV cache needs to be repeatedly fetched from off-chip HBMs in each iteration, it would result in increasing decoding latencies. Moreover, as the performance is bounded by off-chip bandwidth, GPUs are underutilized during the memory- intensive KV cache related computations [72]. Prior works fail to achieve simultaneously scalability of capacity and bandwidth and suffer from trade-offs between them [65]. HBM- based optimizations such as integrating HBM with Processing-in- Memory (PIM) [32, 33, 72] efficiently improve bandwidth. How- ever, they inherit or even exacerbate the capacity limitations of HBMs. Some works resort to CPU-side DIMM-based memory for KV cache storage [25, 85] for higher capacity, whose data swapping overhead can easily dominate the overall performance even with the prefetching mechanism. Works that further offload operations (e.g., Feed-forward or multi-head-attention) to the CPU side re- duce data transfer between devices but suffer from low computing efficiency [30, 40, 70], especially in the long-context scenario. To address the scalability issue, our insight is that, the dual- dimensional memory requirements for capacity and bandwidth scal- ability are essentially driven by the linearly increasing storage and computation demands of the KV cache, which are exclusive to the de- coding MHA operation. Therefore, we propose fully decoupling and offloading the decoding MHA along with the entire KV cache to PIM- enhanced CPU host memory (DIMMs). Decoding MHA could take advantage of DIMM-PIM in two aspects. First, DIMM-PIM devices inherit the scalability of DIMM with its standard modular hardware interface and plug-and-play feature [1]. Second, DIMM-PIM s capac- ity and bandwidth scales with the number of DIMMs [11, 57], which suits the feature of decoding MHA whose capacity and bandwidth requirements scale with the number of context tokens. Achiev- ing both types of scalability helps increase the batch size, thus optimizing GPU utilization and throughput without sacrificing time-between-tokens (TBT). 1 arXiv:2504.17584v1 [cs.AR] 24 Apr 2025 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen The integration with DIMM-PIM introduces new challenges on both hardware and system sides. Hardware challenges. The DIMM-based communication layout mismatches with the PIM computation layout for decoding MHA, resulting in additional re-layout overhead and computation inef- ficiency. First, for each offloaded KV, the bits of a single element are stored across multiple DRAM chips [11, 20], while PIM com- putation requires them to reside in the same chip, which we refer to as bit-level mismatch. Second, the co-processed elements of K cache or V cache are interleaved on chips or addresses, while PIM efficiency needs element mapping with both locality and regularity, which we refer to as element-level mismatch. System challenges. The heterogeneous inference systems intro- duce new challenges to fully utilize the resources on the two devices simultaneously. First, host memory integration introduces cross- device data transfer. This could incur non-trivial overhead to the inference process and interfere DIMM-PIM computation. Second, it is challenging to parallelize the execution on the two devices with- out idle bubbles for two reasons. To begin with, the parallelizable operations on the two devices within a batch is constrained by the computation dependency. Besides, due to the autoregressive nature of LLM inference, the execution latencies of parallelizable parts on the two devices could be varying and imbalanced, causing idle bubbles on the device where the workload is finished earlier. To address those challenges, we propose L3, a heterogeneous LLM inference system that integrates GPUs with scalable DIMM- PIM. On the hardware side, L3 proposes the fine-grained re-layout and DIMM-tailored KV cache mapping methods to eliminate the two mismatches. Specifically, we propose a zero-latency in-flight re-layout method, which is seamlessly integrated in conventional DIMM-based burst transfer without DDR protocol violations [1, 2]. Moreover, two distinct mapping methods for KV cache are proposed to achieve both communication and computation efficiency, while co-designed configurable processing units (PU) at both rank level and bank level are leveraged to enable MHA kernel fusion with bubble-free pipelined execution. L3 system maximizes the resource utilizations on both GPU and DIMM-PIM devices. First, L3 applies dependency-aware communication- computation overlap, a software-hardware co-design to hide the cross-device communication overheads. It involves three techniques, which enable concurrent communication and computation on the DIMM-PIM side, achieve load balancing of data transfer and com- putation across ranks, and remove most data communication from the critical path of the inference process. Second, L3 scheduler ap- plies adaptive chunk-partitioned batch interleaving to maximize parallel execution on both GPUs and DIMM-PIM with minimum idle bubbles. Specifically, to eliminate the idle bubbles caused by the computation dependency of inference operations, L3 sched- uler coordinates requests with two sub-batches. It enables two key features, fine-grained tunability and performance predictability, to align the parallel execution of the two sub-batches. This fur- ther minimizes the idle bubbles caused by unbalanced cross-device parallel execution. Comprehensive evaluations on three LLM models and four real- world traces [4 6, 12] show that L3 achieves up to 6.1 speedup over state-of-the-art HBM-PIM solutions [33, 72] with significantly is Layer 0 Layer 1 Layer N ... great Layer 0 Layer 1 Layer N ... QKV Generation Multi Head Attention (MHA) Q K K mat. V V mat. Projection Feed Forward FC MHA Score Q 1 Dh Kt Dh (Lt 1) Softmax Os 1 (Lt 1) Context V (Lt 1) Dh S 1 (Lt 1) think Layer 0 Layer 1 Layer N ... is I this New Prefilling Reqs great Decoding Reqs Finished Reqs as Figure 1: LLM inference process. The FC operations for each request can be batched and are compute-intensive, whereas MHA cannot be batched and is memory bandwidth-intensive. improved batch sizes (up to 14.3 on DGX-A100 [3]). L3 is the first host memory offloading system for higher throughput without sacrificing latencies (time-between-tokens) compared with server- grade GPUs such as A100, and will be open source. 2 Motivation 2.1 Background: LLM Inference Fig. 1 illustrates the LLM inference process [9, 22, 27, 43], which in- cludes two stages. For a new request prompt (e.g., I think this ), the model undergoes a prefilling stage, encoding the input to generate an output token for the subsequent decoding stages. In the decod- ing stage, the model produces one token at each iteration in an autoregressive manner until detecting the EOS (End of Sequence). The LLM model constitutes a sequence of layers, each compris- ing two major kinds of operations: (1) multi-head attention (MHA) operations [81] and (2) fully-connected (FC) operations, which encompass QKVGen, projection, Feed-forward Network, etc. Nor- malization and activation operations are not depicted in the figure, which are jointly scheduled with FC operations in this paper. The computations of MHA consist of 𝑁ℎindependent heads. For each head, the prefilling stage computes the initial Key-Value (KV) ma- trices (cache) from the input prompt, while each decoding stage updates them by concatenating, with size of head dimension 𝐷ℎ and total token length of 𝐿𝑡. The MHA operation follows score (𝑄 𝐾𝑡), softmax, and context computations (𝑆 𝑉). Existing inference frameworks [7, 55, 84] typically batch multiple requests to improve system throughput and computing resource utilization, since FC weights can be shared by different token vectors of prefilling requests (e.g., I think this ) and token vectors from batched decoding requests. Increasing batch size can improve the data reuse and decrease off-chip memory access, while the large matrix dimension (total token length in a batch) can also enable better tiling and workload partition, which improves computing resources (e.g., GPU SM) utilization. On the contrary, since KV cache is distinct and specific for each request, the computation of MHA does not benefit from batching. 2.2 Background: Processing-In-Memory (PIM) PIM is a promising solution to memory-intensive operations, which can be built upon a variety of memory devices, such as HBM [31, 50, 51], GDDR [48, 56, 58, 62], and DIMM [20, 45, 46, 61]. By integrating PUs into memory devices, PIM offers higher aggregated bandwidth. For example, with two DIMMs (e.g., two ranks of 16 banks per DIMM) in one channel, placing PUs near ranks (rank PU [23, 2 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA 45, 57]) can achieve about 4 the bandwidth of host CPU, while integrating PUs near banks (bank PU [20, 68, 73]) can improve bandwidth by more than 30 . Since the memory buses are shared between DIMMs in a channel, PIM also enables the aggregated internal bandwidth scaling with the number of DIMMs. However, leveraging the high aggregated bandwidth of PIM architectures, particularly at the bank level, necessitates careful consideration of data mapping methods to maintain data locality and regularity. PIM typically operates in a SIMD-like execution manner [31, 50, 58], where PUs execute broadcast commands re- ceived via the command address (C A) bus. During operation, each PU accesses and processes data from its associated memory device (e.g., bank-level PUs access their respective banks) using identical row and column addresses across all PUs. In this case, PIM efficiency requires co-processed data to be mapped within the same device using contiguous addressing schemes, which is a constraint that diverges from conventional host-side memory mapping approaches that distribute data across multiple memory devices. 2.3 GPU Memory Bottleneck With the development and prevalence of reasoning models, the out- put text length of LLMs is becoming increasingly long. For example, real-world traces such as OpenThoughts [6] show an average to- ken length of more than 6k. However, existing systems face two memory limitations when supporting such long-text scenarios: Capacity constraint. First, KV cache size grows linearly with the number of request tokens. The increased memory capacity demand per request means that the expensive and scarce GPU memory can only accommodate the KV cache of fewer requests. This limits the batch size for decoding, which could further result in GPU under- utilization and throughput degradation. For example, even when deploying a 7B Llama model on an A100 with 80GB GPU memory, only a batch size of about 62 requests can be accommodated when the average token length of each request is 2K, and this would result in a batched token number of 62 during FC operations. As shown in Fig. 2-a, it would suffer severe GPU Streaming Multiprocessor (SM) under-utilization. Moreover, the continuous increase in model size further exacerbates the memory capacity pressure. Bandwidth constraint. Second, the demand for memory band- width grows in tandem with the demand for memory capacity. This is because decoding requires reading the whole KV cache from memory, whose overhead increases linearly with the number of tokens. Moreover, there is no data reuse for the KV cache, which benefits little from GPU cache and shows bandwidth-bound pat- tern [33, 72]. With the increasingly long context, the latency of decoding could also grow and could become the main bottleneck for the end-to-end latency of LLM inference. As shown in Fig. 2-b, when the context length increases to 16K, decoding MHA becomes the main bottleneck for decoding, i.e., accounting for more than 61.3 latency of the iteration. 2.4 Insight and Proposal of L3 Our insight is that, the dual dimensional memory requirements for ca- pacity and bandwidth scalability are essentially driven by the linearly increasing storage and computation demands of the KV cache, which 0 20 40 60 80 100 64 128 256 512 1024 2048 4096 8192 Utilization ( ) Batched Token Number Active SM Memory Bandwidth (a) Utilizations of A100. 0 1 2 1K 2K 4K 8K 16K Batch Size 4 27.7ms Norm. Latency Decoding Token Length MHA FC 23.0 27.0 33.6 47.5 61.3 (b) Decoding latency breakdown. Figure 2: GPU memory bottlenecks with Llama-7B on A100. Capacity constraints the batch size while bandwidth constraints the time-between-tokens. (a) profiles the GPU utilizations during the Feed-forward operation. (b) normalizes the decoding latency of 1K token length to 1. The inference system is S-LoRA [77]. (a) Hardware challenges. (b) System challenges. Chips Data Bus 8 8 8 8 Token Vector ... ... ... ... beat of burst transfer ... New Token K V Cache Vec. Q Vec. S head dimension token length Proj, FF QKV Gen Dec. MHA Pre. MHA offloading GPU DIMM-PIM Time BUBBLE BUBBLE BUBBLE costly PCIe communication bubble caused by imbalanced parallelism FP16 ... ... on- loading Figure 3: Challenges of DIMM-PIM integration. are exclusive to decoding MHA . On the contrary, for the FC oper- ations in decoding, their computation efficiency is solely related to the batch size, and an increase in token length does not incur additional computational load or memory capacity requirements. This insight motivates our architectural proposal: decoupling and fully offloading decoding MHA along with the entire KV caches to PIM enhanced host memory (DIMM). DIMM-PIM can provide benefits that match the requirements of decoding MHA: First, DIMM-PIM can leverage the modular hard- ware interface of DIMMs to conveniently achieve plug-and-play scalability. Second, the memory bandwidth and capacity require- ments of decoding MHA scale in proportion to the number of tokens. DIMM-PIM aligns perfectly with the characteristics of decoding MHA, as it enables memory bandwidth and capacity to scale pro- portionally with the number of DIMMs. Besides, FC operations are executed on the GPU side, with the batch size of these batchable parts maximized to fully utilize the GPU s computational capacity. 2.5 Challenges of DIMM-PIM Integration Integrating DIMM-PIM in GPU systems for LLM inference can be challenging for the following reasons: Hardware challenges. As shown in Fig. 3-a, DIMM leverages multi- ple co-operated DRAM chips as a rank to form the burst data on the memory bus, with each chip contributing a portion of the total data 3 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Table 1: Related works fall short in the scenario with long contexts and large batch sizes. CPU offloading introduces a bottleneck in terms of computational efficiency and communication overhead with long contexts. PIM can effectively provide bandwidth expansion, but may be limited by its memory scalability, inappropriate kernel mapping, or inefficient cross-device scheduling. Low latency means no sacrifice in TBT compared with the GPU-only baseline in our scenario. Method Work Year Platform Capacity Scalability Bandwidth Scalability Low Latency Comm-comp Overlap Bubble Reduction Software optimization FlexGen [78] 2023 GPU CPU - CachedAttention [25] 2024 GPU CPU - NEO [40] 2025 GPU CPU Hardware-software co-optimization NeuPIMs [33] 2024 GPU HBM-PIM AttAcc [72] 2024 GPU HBM-PIM IANUS [76] 2024 GPU GDDR-PIM Hermes [70] 2025 GPU DIMM-PIM PAPI [32] 2025 GPU HBM-PIM L3 - GPU DIMM-PIM width. In this case, a KV element s different bits can be distributed to different chips after offloading (e.g., FP16 and 8 chips), which completely restricts PIM computation performed in each chip. To solve such a bit-level mismatch, prior works typically apply offline data shuffling via CPU-assisted transposition [11, 20, 29, 49, 60], which can incur unacceptable latency during LLM inference. For instance, the CPU transposition requires additional repeated off- chip access after KV offloading, which could account for more than 2 base transfer time. Meanwhile, PIM efficiency highly relies on data locality and regular distribution, in which the bank PU is hard to access data from banks other than its corresponding one, let alone data from other chips [38, 80, 90]. Moreover, accessing non-contiguous stored data may significantly reduce bandwidth utilization due to the row switch overhead. However, as shown in the figure, the co- processed K elements stored on continuous addresses are actually distributed in physical banks across multiple DRAM chips, while the co-processed V elements would be stored discontinuously since they are generated in different iterations. Existing uniform and transposed mapping methods for KV cache incur fine-grained mem- ory access [33, 48, 56] (e.g., masked write) and frequent inter-PU communication [72], which also cannot be applied to distributed physical banks in DIMM-PIM. System challenges. Heterogeneous inference comprising GPUs and DIMM-PIMs presents new challenges for achieving high resource utilization on both devices in two aspects. First is the communi- cation challenge, since the LLM inference is executed across the GPU and host memory and requires data transfer between them. The communication involves offloading the Q, K, V generated on the GPU side to host memory, and onloading the computed atten- tion from the host memory to GPU, as shown in Fig. 3-b. The data transfer would incur additional latencies with the constraint of PCIe bandwidth. Moreover, with the constraint of shared internal memory buses, the data transfer of communication would interfere or even stick the computation on DIMM-PIM. Second, it is challenging to parallelize the execution on the two devices without idle bubbles for two reasons. To begin with, opera- tions of LLM inference have computational dependencies. Within the same batch, only the MHA operation of prefilling decoding requests can be executed in parallel on both the GPU and DIMM- PIM. Other operations need to be batched together and executed exclusively on the GPU, which could result in idle bubbles on the DIMM-PIM, as shown in Fig. 3-b. Next, the parallelizable operations on the two devices have varying execution latencies due to the au- toregressive nature of LLM inference. The latencies are affected by the batch size, various input token lengths, and unpredictable output token lengths, etc. This implies that logically parallelizable parts on the two device sides could have mismatched execution latencies, causing idle bubbles on the PU where the workload is finished earlier, for example, the bubble on the GPU side in Fig. 3-b. 2.6 Related Works As shown in Table 1, prior works fail to achieve our goal for the following reasons: Host memory integration. Prior works such as CachedAtten- tion [25] and FlexGen [78] have explored expanding storage capac- ity by utilizing host memory. However, the inference requires fetch- ing data from host memory to the GPU which introduces significant latency overhead limited by the PCIe bandwidth. In long-context scenarios, the overhead could be prohibitively large and cannot be effectively hidden by computation, even with techniques such as prefetching [25]. Besides, FastDecode [30] and NEO [33] further offload corresponding computations to the CPU to reduce communi- cation costs, as the GPU does not need to fetch the entire offloaded data from host memory, but rather the computed results, which are significantly smaller in size. However, the limited TFLOPS and memory bandwidth of CPU brings new bottlenecks. For example in DGX-A100 system, the aggregated HBM bandwidth can reach about 16.3TB s, which is 40 higher than 16-channel DDR4-3200 mem- ory (406GB s). Consequently, when executing bandwidth-intensive workloads such as decoding MHA, the performance may experience degradation by more than 10 . 4 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA GPU GPU GPU ... ... HBMs HBMs HBMs Batch Scheduler DIMM-PIM Lin0 Lin1 Host CPU MC Rank PU ... Bank PUs Banks Decoding Queue Prefilling Queue Request CPUs Scheduler QKV Attn. KV PCIe Dec. MHA FC, Pre. MHA Figure 4: L3 overview. Hardware-Software co-optimization. Prior research has inves- tigated GPU-PIM integration for inference systems to capitalize on PIM s bandwidth advantages. Systems such as NeuPIMs [33], AttAcc [72], and PAPI [32] enhance GPUs with HBM-PIM, achiev- ing considerable performance improvements through increased bandwidth. However, their capacity constraints may impact batch size scalability and further limit GPU utilization. While IANUS [76] demonstrates effectiveness for non-batched inference, its operation mapping approach differs from what might be optimal for large- batch scenarios. Hermes [70] targets LLM inference on consumer- grade GPUs, leveraging DIMM-PIM for both FC and MHA oper- ations. It fails to exploit bank-level PIM bandwidth, resulting in its PIM device s bandwidth being unable to match that of cloud- based GPUs (about only one-fourth of the GPU s bandwidth with DGX-A100 s configuration). Moreover, they do not consider how to exploit cross-device parallelism under large batch sizes, leading to idle bubbles on both devices. 3 Overview of L3 We propose L3, a scalable heterogeneous LLM inference system with computation decoupling and coordination between GPU and DIMM-PIM. Fig. 4 illustrates LLM inference with L3 s heteroge- neous architecture. L3 maintains model parameters within GPU memory while exclusively storing the KV cache in host memory. High-performance GPUs provide massive computational resources, and compute-intensive GeMM kernels are mapped to GPUs. DIMM- PIM-integrated host memory provides scalable capacity and band- width, and the KV cache related decoding MHA phase is offloaded to DIMM-PIMs. Cross-device inference process. For each iteration, L3 scheduler selects prefilling and decoding requests to form a batch. Initially, QKV Generation of all requests is batched and executed on the GPU. Then, prefilling and decoding MHA operations are processed concurrently on the GPU and PIM-enabled host memory respec- tively. Following decoding MHA, the computed attention results are transmitted back to the GPU. L3 aggregates attention outputs from all prefilling and decoding requests in the batch, and then performs batched execution of subsequent operations (projection, Feed-forward, etc). Throughout the inference process, PCIe inter- connects facilitate data transfer between devices. Design overview. The design of L3 mainly involves two primary components. First, to address the challenges posed by the two mis- matches, L3 implements an in-flight fine-grained relayout ( 4.1), which seamlessly integrates with conventional DIMM-based burst transfer without DDR protocol violations. In addition, cross-level PUs are configured to cooperate with two distinct mapping meth- ods of KV cache ( 4.2), which further enables kernel fusion with a bubble-free pipelined execution manner ( 4.3). Second, L3 maxi- mizes resource utilization across both GPU and DIMM-PIM sides with optimizations on two dimensions: dependency-aware communication- computation overlap ( 5.1), that reduces the cross-device data trans- fer overhead, and adaptive chunk-partitioned batch interleaving ( 5.2), that optimizes parallel execution across both sides. The fol- lowing sections elaborate on these two designs (DIMM-PIM hard- ware and PIM-powered inference system) in greater detail. 4 L3 DIMM-PIM Our architecture implements a hierarchical processing framework with strategically integrated PUs that cooperate across different levels: rank-level PUs positioned on the buffer chip and bank-level PUs embedded within each DRAM chip, as illustrated in Fig. 5-a. This cross-level coordination forms the foundation for three key innovations: (1) in-flight data re-layout mechanisms that resolve the bit-level mismatch while overcoming the challenges introduced by DDR burst [1] and timing constraints [2]; (2) distinct KV map- ping methods that organize co-processed elements contiguously at burst-transfer granularity, effectively addressing the element-level mismatch through the cross-level PU collaboration; and (3) com- prehensive kernel fusion that seamlessly integrates score computa- tion, softmax operations, and context computation in a bubble-free pipelined execution flow. Together, they enable L3 to overcome the fundamental mismatch challenges in DIMM-PIM while maximizing computational throughput during attention computation. 4.1 Zero-latency In-flight Re-layout We implement a re-layout unit in the rank PU to perform data trans- formation during GPU-to-CPU offloading (KV for prefilling, QKV for decoding). This unit addresses scenarios where element preci- sion mismatches with DRAM chip width, such as FP16 precision and 8 DRAM chips. As shown in Fig. 5-b, our approach strategically exchanges specific bits between data elements to map complete elements to one DRAM chip. Unlike conventional mapping where the upper and lower 8 bits of an element reside in separate chips, the re-layout technique ensures each element is stored within a single DRAM chip. Challenges. However, our re-layout approach encounters two sig- nificant challenges. First, it conflicts with burst transfer operations, as data issued by the host memory controller (MC) arrives on the data bus in a continuous stream, providing no opportunity to collec- tively process and re-layout multiple elements after transfer begins. Second, performing in-flight re-layout operations risks introduc- ing additional processing cycles during data transfer, potentially violating critical DDR timing constraints and resulting in DRAM physical errors or bus contention issues. To address these limita- tions effectively, we introduce two techniques described below. Double buffering. To seamlessly integrate re-layout operations during DDR burst transfer, we implement an efficient double buffer- ing mechanism on the buffer chip, replacing the conventional single- buffer approach. This technique enables simultaneous buffering of two consecutive burst data beats, providing the necessary temporal overlap for performing bit exchange operations without disrupting 5 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen ... Bank PU Bank PU Bank PU ... Shared Buffer DDR CMD PIM CMD Rank PU Buffer Data PIM CMD Rank PU Side Result Buffer Shared Buffer Bank I O Sense Amplifiers Result Buffer Shared Buffer Bank I O Sense Amplifiers Config. State Re-layout Unit Softmax Unit Adder Unit On-chip Bufer Memory Ctrl. Host Data Rank PU Data Disabled during comp. Host Side Bank PU Side V Max Adders Exponentials Adder Tree Divider Multipliers Local Global Softmax Result On-chip Buffer DRAM Chip Element Head Dimension: Dh New Token: Bank N Tokens: Lt 1 Bank 3: Token 3 Matrix K Vector Q C0 C1 C2 C3 C4 C5 C6 C7 Head Dimension: Dh Bank 0: Token 0 Bank 1: Token 1 Bank 2: Token 2 New Token: Bank N-N 3 Adder Tree Mode Accumulator Mode Vec. psum DDR CMD DDR PIM CMD exchange Bus Width Chip Width Chip Num Data 0 Data 1 Data 0 Data 1 (d) Score computation and mapping method. (c) Pipelined softmax unit. (b) Fine-grained re-layout. (e) Context computation and mapping method. (a) DIMM-PIM hardware for one rank. Rank PU Buffer Chip Chip Chip Chip Rank Host Banks Mat. Chip Burst Transfer Bank 0 - Bank 3: Token 0 C0 C1 C2 C3 C4 C5 C6 C7 Bank 8 - Bank 11: Token 2 Matrix V Bank 0 - Bank 3: Token 4 Bank 4 - Bank 7: Token 1 Bank 12 - Bank 15: Token 3 Broadcast to DRAM Chips Vector S Figure 5: DIMM-PIM architecure for LLM attention. the continuous data flow. Consequently, the double buffer creates a processing window where re-layout can occur. Spoofed timing constraints. To address potential DDR timing violations caused by re-layout processing, we implement a strategic timing constraint modification between the host MC and DIMM- PIM devices. Specifically, during the booting stage when the host MC reads Serial Presence Detect (SPD) on the DIMM, its provided recorded parameter values are intentionally spoofed to mislead the host MC. For example, for the timing parameter tWL, which is the latency between issuing a WR (write) command and starting to transfer burst data, the provided parameter value is smaller (e.g., one cycle) than the actual parameter value. Consequently, host MC will launch data transfer beforehand, while the data after re-layout is stored in DRAM banks with no introduced latency. Conversely, for the latency of subsequent commands, such as tWR for latency between writing completion and issuing a PRE command, the pro- vided value should be increased to prevent early issuing. 4.2 Attention Mapping and Computation The attention computation strategy consists of two complemen- tary components: KV mapping methods and PU design. The KV mapping methods distribute co-processed elements to satisfy PIM s locality and regularity requirements, while the cross-level PUs are specifically engineered to efficiently execute these distributed computations. Softmax computation optimizations are addressed separately in the subsequent section. Challenges. A fundamental challenge emerges from the dimen- sional asymmetry in attention operations: score and context per- form matrix-vector multiplications (GEMV) along orthogonal di- mensions of the KV cache, necessitating distinct mapping methods to resolve element-level mismatch. Simultaneously, the PUs must accommodate these divergent computational patterns without sac- rificing efficiency. For clarity in our analysis, we present mapping methods based on matrices and vectors after they have undergone bit-level re-layout and chip-level distribution, rather than their original representations. Score K Cache. As shown in Fig. 5-d, for each newly gener- ated vector K, whose elements are co-processed together, they are partitioned to different DRAM chips (vertical) and mapped to the same logic bank (horizontal). Within individual DRAM chips, co- processed K vector elements are stored contiguously in the same physical bank. To maximize computational throughput through all-bank parallelism, we distribute vectors from different tokens (K cache) across separate banks while maintaining consistent row and column indices. The V vector follows an analogous mapping strategy, ensuring elements with matching indices reside in the same DRAM chip, facilitating broadcasting across banks during computation. This mapping method enables efficient inner product for score computation, with both bank-level PUs and the rank PU s adder unit configured as adder trees. Bank PUs perform multipli- cations using elements from their dedicated bank row buffers and the shared buffer, while the rank PU aggregates partial products from multiple DRAM chips to complete the computation. Context V Cache. As shown in Fig. 5-e, we distribute elements of each newly generated V vector, which operate independently, across multiple banks (four in our example) using burst transfer as the minimum granularity. This arrangement places co-processed elements in sequential burst transfers at identical offsets within their respective banks, exemplified by the first burst transfer (or- ange) of tokens 0 and 4. We also distribute tokens across different banks, enabling full exploitation of all-bank parallelism. The vector S is broadcast to all DRAM chips, where the element is shared in the same bank but may be different across banks (e.g., banks 0-3 and banks 4-7). Based on this mapping, we implement an outer product approach where both bank PUs and rank PUs function as accumulators. While the shared buffer broadcasts vector elements, multipliers in each bank PU utilize identical vector elements that may differ from those in other bank PUs. The rank PU performs a final aggregation of partial results from bank PUs within the same DRAM chip upon completion of context computation. 4.3 Kernel Fusion with Bubble-free Pipelining Considering the complexity of softmax, the softmax unit is typi- cally implemented by logic process on the buffer chip [70] (or logic die in HBM-PIM [72]), as shown in Fig. 5-c. To mitigate poten- tial resource underutilization during computation, we introduce a pipelined execution framework that strategically partitions the DIMM-PIM hierarchy into three non-interfering domains: during 6 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA computation: (1) the host CPU, which monitors execution status and configures PIM transactions for the rank PU; (2) the rank PU, which manages data transfer with bank PU buffers; and (3) the bank PUs, which operate concurrently with their respective mem- ory banks. We leverage this partitioning method to ensure kernel fusion with pipelined execution. Chunk softmax. We implement chunk softmax [18] to enable pipelining with score computation rather than awaiting the comple- tion of score computation. During score computation, the rank PU simultaneously retrieves partial results from bank PU result buffers. We define a "chunk" as the collective partial outputs generated syn- chronously across all logic bank PUs. Chunks undergo independent softmax operations and are pipelined through the softmax unit, culminating in a final normalization once all chunks are processed. The normalization process is executed on a per-chunk basis to facil- itate further pipelining with context computation, wherein outputs are directly broadcast to DRAM chips and their respective bank PUs without intermediate bank storage. Quantitative analysis of the pipeline execution proves it is bubble-free. 4.4 Supplementary Details Dynamic refresh. We offload the management of DRAM refresh to the rank PU, which is responsible for the refresh of all DRAM chips in the rank. The need for refresh is checked with each head computation completion, while the rank PU can still receive fol- lowing transactions. According to the protocol [1], the interval between REF commands can also support dynamically adjusting, with up to 8 REF commands can be delayed, which provides flex- ibility for the rank PU to manage the refresh when token length varies dynamically. Furthermore, since computation time is nearly linearly correlated with token length, the rank PU can dynamically determine the optimal timing for issuing the next REF command. Parameter settings. The number of arithmetic units (e.g., mul- tipliers, adders, exponential units) in bank PUs and rank PU is primarily determined by the bandwidth. For instance, since the bank row buffer provides 64-bit data for one RD (read) command, we implement four multipliers supporting two 16-bit inputs in each physical bank PU, while the shared buffer also broadcasts 64-bit data to all bank PUs in each computation phase. The size of on-chip buffer in the rank PU determines the number of tokens that can be processed in parallel. Following work [70], a 256KB buffer is lever- aged, which can support about a token size of 128K. For requests with larger lengths, repeated data fetching is required. 5 L3 System This section describes L3 system, a PIM-powered LLM inference system with hardware-software co-design to achieve scalability. The L3 system primarily addresses two major challenges: 1) mini- mizing the additional overhead caused by communication between the GPU and host memory, and 2) balancing the computation be- tween GPU and DIMM-PIM with scheduling techniques to reduce idle bubbles on both sides for high resource utilization. H0 H1 H3 H2 Channel 0 Channel 1 Channel N Req 0 H0 Layer 0 H0 Layer 1 Layer 0 Req 1 Rankset 0 Rankset 1 H0 H0 H0 H0 H0 H0 Layer 1 Req 2 Req 3 H0 H0 H0 H0 H0 H0 H0 H0 H0 Ch0 H1 Ch1 H2 Ch2 H3 Ch3 (a) Communication and computation with the rankset. (b) The data mapping on ranksets for load balancing. offloaded data PCIe CPU comm ... rank rank rank rank rank rank computation ... ... Rankset M Mapping Figure 6: Communication-computation overlapping with load balanced ranksets. 5.1 Optimizing Cross-device Communication L3 introduces dependency-aware communication-computation over- lap, a hardware-software co-design involving three techniques: 1) overlapping the communication and computation on the DIMM- PIM to hide the cross-device communication latencies, 2) achieving data layout load balancing among ranks, and 3) removing most data transfer out of the inference critical path. Concurrent communication and computation. Conventional scheduling treats all DIMM-PIM as a monolithic entity, alternat- ing between communication and computation modes. However, we observe that due to the shared memory buses between ranks, channel bandwidth does not scale with the number of ranks for communication. A single rank can nearly saturate the available channel bandwidth, rendering the simultaneous activation of all ranks for communication inefficient. L3 enables concurrent communication with computation on the DIMM-PIM side. The key insight is that, KV offloading does not require all ranks to operate in the communication mode concurrently. Therefore, we introduce a new hardware extension to use rankset as the basic unit for data transfer, with one rank on each channel forming a rankset, as shown in Fig. 6-a. During a single data transfer, only one rankset (e.g., rankset 0 in Fig. 6-a) is engaged to receive the data, while other ranksets can perform independent computations without being stuck. This hardware extension enables L3 to achieve parallel PIM computation and data transfer at the DIMM-PIM level. Taking the DGX-A100 with four ranks per channel as an example, this design preserves 75 of the DIMM-PIM s computational power during communication. Load balancing among ranksets. Randomly mapping the KV cache of requests to ranksets could cause load imbalance among ranks. The rankset receiving the most data could have longer data transfer and executing latency than other ranksets and thus slowing down the overall progress. To achieve load balance, we utilize the feature of LLM that for each request, the KV cache of different layers has the identical size. Therefore, we store the request s KV cache at the granularity of layers, as shown in Fig. 6-b. During the inference for a prefilling request, its KV cache for different layers is alternately offloaded to different ranksets. As a result, the data transfer time is eventually balanced across each rankset. A request s KV cache is also evenly distributed across ranksets. KV cache of different heads in the same 7 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen S0 Gen GPU S0 MHA S1 Gen S1 MHA S0 Proj, FF S0 Gen S0 MHA S1 Proj, FF S1 Gen S1 MHA S0 Proj, FF S0 Gen Batch Prefill Decode Sub-batch 0 Sub-batch 1 Offload Onload DIMM-PIM QKV0 KV0 A0 S1 MHA S0 MHA QKV0 KV0 QKV1 KV1 A1 S0 MHA S1 MHA QKV0 KV0 Rankset 0 Rankset 1 S0 MHA S0 MHA S0 KV S1 MHA S1 MHA S1 KV S0 MHA S0 MHA S0 KV S1 MHA S1 MHA S1 KV Align Layer 0 Align Layer 1 Repeat (L - 1) Layers QKV0 S0 MHA A0 QKV0 S1 MHA QKV0 S0 MHA A0 QKV0 S1 MHA QKV1 S0 MHA A0 Figure 7: Computation graph in L3. Data transfer between devices can be overlapped with computation. Gen denotes QKV Generation and Proj, FF denotes the projection and feed-forward operations. layer is assigned to different channels within a rankset. In this man- ner, L3 ensures load balance at the rankset-level for computation and communication simultaneously. Remove dependency-independent communication from the critical path. According to data dependencies of the inference process, L3 removes most of the data communication from the in- ference critical path to further reduce the overhead. Specifically, two parts of data are needed to be transferred between devices through PCIe: Q, K and V values from the GPU to host memory after the QKV generation phase, and the computed attention re- sults of decoding requests from host memory to the GPU. Only the vector-sized Q, K and V generated for decoding requests are computational-dependent, which will be immediately used by the attention computation on DIMM-PIM in the same iteration. Simi- larly, the computed attention results of each decoding request are also of the vector size. The remaining part of offloading (K and V val- ues generated by prefilling requests) is transferred asynchronously, i.e., delayed to the projection or Feed-forward stages. In addition, the asynchronous communication will not affect the GPU side since the GPU is sufficient in memory bandwidth, as shown in Fig. 2. Ac- cording to our evaluation, the transfer of the prefilling KV cache can always be hidden by projection and Feed-forward (typically 16 of the Feed-forward latency). This is because the latency of the projection Feed-forward and the latency of prefilling KV cache data offloading are both related to the input size, and the former increases more significantly as the input size increases. 5.2 Optimizing Resource Utilization with L3 Scheduler The goal of L3 scheduler is to maximize parallel execution on both the GPU and DIMM-PIM and minimize potential bubbles. The key design is adaptive chunk-partitioned cross-device batch interleaving. First, to overcome the limitations imposed by the computational dependencies within a single batch, instead of parallelizing the prefilling and decoding MHA of the same batch, the L3 scheduler divides requests into two sub-batches, overlapping the prefilling of one sub-batch with the decoding of the other, as illustrated in Fig. 7. Moreover, we further design a novel scheduling policy to align the parallel parts of the two sub-batches to minimize idle bubbles on two devices. L3 achieves this with two key features: fine-grained tunability with adaptive chunk-partitioning and performance pre- dictability. Features of L3 scheduler. First, fine-grained tunability refers to L3 s ability to finely adjust the execution time on the GPU side, better aligning with the parallel execution on the DIMM-PIM side to mini- mize idle bubbles. In the long-context scenario, the FC operations of decoding-only batches may not overlap with the latency of decod- ing MHA (e.g., 16K tokens in Fig. 2), leading to idle bubbles on the GPU. At this point, L3 will add prefilling requests to the sub-batch. To avoid excessive execution latency on the GPU side after adding a prefilling request with long inputs, L3 dynamically chunks the overlong prefilling request. This enables fine-grained adjustment of the batch-execution phases on the GPU side, thereby enabling pre- cise calibration of execution latencies on the GPU to synchronize with those on the DIMM-PIM. Unlike original chunked-prefill design [7], L3 s scheduling policy ensures that only one request is chunked in a batch, and in most cases, this request will be chunked only once (details in 5.3). This further minimizes the additional overhead brought by chunking. The second feature is performance predictability. L3 s separa- tion of prefilling and decoding requests enables homogeneous and interference-free operations on both GPU and DIMM-PIM, thereby providing predictability of the computation time on both devices the factors that affect the batch performances (e.g., batch size, num- ber of finished tokens, the chunk size, etc) are known in advance. Therefore, the scheduler can predict the performances before the execution of a batch and make scheduling decisions accordingly. Predicting the performance of various workloads on GPUs as a reference for scheduling is a common practice [15, 17]. 5.3 Details of L3 Scheduler Problem formulation. The following arguments can describe a sub-batch 𝑖: 𝑐𝑝𝑖, the list of chunk sizes of each prefilling request; 𝑓𝑝𝑖, the list of finished token numbers of each prefilling request; 𝑓𝑑𝑖, the list of finished token numbers of each decoding request. The goal of the scheduling is to prioritize the computation time of 8 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA decoding and add prefilling requests to hide the execution bubble. It requires modeling the following latencies of both sub-batches: 𝑇GPU0 𝑡𝑝(𝑐𝑝0, 𝑓𝑝0) 𝑡batch(𝑐𝑝0, 𝑓𝑑1) (1) 𝑇PIM0 𝑡𝑑(𝑓𝑑0) 𝑡comm(𝑓𝑑0) 𝑡overlap(𝑐𝑝1) (2) 𝑇GPU1 𝑡𝑝(𝑐𝑝1, 𝑓𝑝1) 𝑡batch(𝑐𝑝1, 𝑓𝑑0) (3) 𝑇PIM1 𝑡𝑑(𝑓𝑑1) 𝑡comm(𝑓𝑑1) 𝑡overlap(𝑐𝑝0) (4) Equation 1,3 denotes the latency on the GPU side, which contains the latency of prefilling MHA and batched operations (FC opera- tions). Equation 2,4 denotes the latency on the DIMM-PIM, which contains the latency of decoding MHA, QKV transferring for de- coding, and the overlapped transferring of prefilling requests KV. Scheduling algorithm. The scheduling algorithm constructs sub- batches for each iteration with the following steps: (1) As long as there is space in the host memory, decoding re- quests are fetched from the decoding queue (Fig. 4), which will be processed in this iteration. (2) These decoding requests are divided into two sub-batches, en- suring that the total number of tokens in the two sub-batches is as even as possible, thereby making the computation time for decoding MHA as even as possible. For example, four requests with 2k, 3k, 4k and 5k tokens can be divided to (2k, 5k) and (3k, 4k). (3) Prefilling requests are incrementally added to sub-batch 1 to gradually increase𝑇GPU1 until𝑇GPU1 𝑇PIM0, at which point the process stops. Similarly, prefilling requests are added to sub-batch 0 until 𝑇GPU0 𝑇PIM1. (4) The last prefilling request of each sub-batch is chunked to make 𝑇GPU as close as possible to the 𝑇PIM of the other sub- batch. The chunk size is a multiple of 16, better aligning with the requirements of GPU tiling. After this iteration ends, the chunked prefilling request is placed at the head of the prefilling queue. Runtime profiling and modeling. Now we describe how L3 mod- els the latencies of various inference operations. First, to model 𝑇PIM, since the latencies of 𝑡𝑑, 𝑡comm and 𝑡overlap are almost linearly related to the number of computed transferred tokens, we can use a linear model for prediction, which is simple and fast. Second, to model 𝑇GPU, we leverage Random Forest Regression (RFR) for its three advantages: capability of incremental learning, low latency, and high accuracy. The models can be described with the following formula (take sub-batch 0 as an example): 𝑇PIM0 Linear( 𝑓𝑑0, len(𝑓𝑑0), 𝑐𝑝1) (5) 𝑇GPU0 RFR𝑝(𝑐𝑝0, 𝑓𝑝0) RFRbatch( 𝑐𝑝0, len(𝑓𝑑0)) (6) L3 uses a runtime profiling framework to collect the execution and communication latencies during the inference process. When a sufficient amount of data has been collected, L3 uses the col- lected data to train the model. Moreover, the dataset maintained by L3 dynamically evolves with the collected data, and the model is capable of being dynamically updated in response to the actual situation, thereby adapting to a complex and changing execution environment. Table 2: Simulator details. GPU Configuration CPU Configuration Processor 8 A100 (5 HBM2e) Memory 16 Channels 2 DIMM Capacity 640GB Capacity 2TB (Scalable) Bandwidth 16.3TB s (GPU) 260.8TB s (HBM-PIM) Bandwidth 406GB s (CPU) 13.0TB s (DIMM-PIM) DIMM DIMM-PIM: DDR4-3200 Hierarchy 2 Ranks 4 Bank Groups 4 Banks DRAM Timing BL 4:CCD 4:RRD 4 8:RCD 22:RAS 52:RP 22:RC 74: CL 22:WL 16:CDLR 4 12:WR 24:CCDL 8:RTP 12 Table 3: The evaluated LLM models. Model Layers Nl Heads Nh Embedding De Tensor Para. Data Para. OPT-66B 64 72 9216 2 4 GPT-89B 48 96 12288 4 2 GPT-175B 96 96 12288 8 1 Precision: 𝑁𝑝𝑟𝑒 2 (16-bit floating point); Head Embedding: 𝐷ℎ 𝐷𝑒 𝑁ℎ 128. Table 4: Real-world traces. Trace Avg. Lin Std. Lin Avg. Lout Std. Lout OpenR1-Math-220k [5] 96.0 75.1 12684.1 8464.6 Dolphin-r1 [4] 201.9 563.0 3926.2 4216.0 OpenThoughts-114k-math [6] 89.4 66.7 6366.7 4662.9 LongBench [12] 7703.9 4285.5 89.8 213.7 6 Evaluation 6.1 Experimental Setup L3 implementation. L3 is built upon the DGX-A100 system [3]. On the GPU side, 8 NVIDIA A100 GPUs, each with 5 HBM2e of 80 GB capacity, are integrated, providing a total of 156 TFLOPs on FP16. The GPUs are connected via NVLink [82]. On the CPU side, there are 16 channels with 2 DIMMs of total 2TB capacity, which are further equipped with the proposed PIM capability. We develop a simulator based on DRAMSim3 [66] and AttAcc [72] to evaluate the performance of L3, including both GPU and DIMM- PIM components. The hardware specifications are listed in Table 2. For hardware validation, we implement the logic of the bank PU and the rank PU with SystemVerilog. The prototype is synthesized with Synopsys Design Compiler under the TSMC 28nm CMOS technology to estimate the area and power consumption. Baseline systems. We compare L3 with four baseline systems: 1) GPU-only, which executes LLM inference exclusively on GPUs. 2) GPU with HBM-PIM, which represents NeuPIMs [33] and At- tAcc [72]. We assume double buffers in each bank for concurrent GPU and HBM-PIM execution. It adopts prefilling-prioritized sched- uling, and utilizes sub-batches for decoding to reduce GPU-side idle bubbles. 3) GPU with rank-level DIMM-PIM (R-PIM), which represents Hermes [70]. It leverages prefilling-prioritized schedul- ing and single-batch methods. CPU-GPU communication (e.g., KV transfer) is hidden by concurrent computation. 4) GPU with CPU offloading without PIM acceleration, which represents NEO [40] and FastDecode [30]. We re-implement NEO s scheduling policy for CPU-GPU coordination. LLM models. We evaluate L3 on three LLM models with varying model sizes: OPT-66B, GPT-89B, and GPT-175B. Considering both the GPU memory capacity and inter-GPU communication overhead, 9 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen 0 1 2 3 GPT-175BGPT-89B OPT-66B \ \ OOM Normalized Throughput GPU HBM-PIM CPU R-PIM L3 (a) OpenR1. 0 1 3 5 GPT-175B GPT-89B OPT-66B (b) Dolphin. 0 1 2 3 GPT-175B GPT-89B OPT-66B \ \ OOM (c) OpenThoughts. 0 1 2 3 GPT-175B GPT-89B OPT-66B (d) LongBench. Figure 8: End-to-end inference throughput on real-world traces. The throughput of GPU-only baseline is normalized to 1. For the OPT-66B model, GPU out-of-memory (OOM) errors occurred in two traces, so the throughput of L3 is normalized to 1. we apply various methods of parallelism to these models. Details are listed in Table 3. Workloads. We use four real-world LLM inference datasets: OpenR1- Math-220K [5], Dolphin-r1 [4], OpenThoughts-114k-math [6] and LongBench [12]. All of these datasets are used for evaluating the performance of LLM inference on long sequences. The details about the traces are shown in Table 4. 6.2 End-to-end Performance We evaluate the end-to-end throughput for each LLM model and workload using various baselines. For each evaluation, we randomly select and execute 1,000 requests from the traces. We measure the overall throughput by dividing the total number of output tokens by the total execution time. Throughput with real-world traces. Fig. 8 presents the normal- ized throughput of L3 compared with four baselines. The results show that L3 achieves the highest throughput over all baselines with various settings. Specifically, L3 achieves up to 6.1 higher throughput than the HBM-PIM baseline, 5.0 than the GPU-only baseline, and 9.2 higher than R-PIM baseline. These improve- ments are attributed to the following reasons. First, L3 achieves much larger batch sizes compared to the HBM-based baselines. For example, for GPT-175B, L3 has 2 TB of memory on the host memory for KV cache storage, while GPUs have only about 310 GB of memory in total after deploying the model. Second, L3 enables parallel execution of prefilling and decoding. In contrast, the HBM- PIM baseline suffers from PIM bubbles if prefilling requests are added to a batch. Third, the CPU offloading baseline suffers from a performance bottleneck due to the limited channel bandwidth. Finally, compared with R-PIM, L3 achieves fewer bubbles on both devices and offers about 8 bandwidth with bank-level PIM. We observe that HBM-PIM s sub-batch strategy demonstrates promising efficiency in bubble reduction, though our analysis re- veals certain limitations when processing Dolphin trace on OPT- 66B. It faces challenges in efficiently utilizing GPU resources when handling inherently small batch sizes, potentially leading to GPU- side execution bottlenecks that may result in performance be- low conventional GPU-only implementations. On the contrary, L3 achieves considerable merit with much larger and more balanced sub-batches, resulting in significant throughput improvements. 0 1 2 3 4 5 Bw-only Cap-only Bw-Cap Normalized Throughput 1 2 4 8 (a) GPT-175B. 0 1 2 3 4 5 6 Bw-only Cap-only Bw-Cap Normalized Throughput 1 2 4 8 (b) GPT-89B. Figure 9: Scalability analysis. The results are evaluated using the OpenR1 trace. Scalability analysis. We further evaluate L3 performance with various host memory configurations, showing that performance scalability requires simultaneous scaling in memory capacity and bandwidth. We first establish the base memory configuration as (512 GB, 2 ranksets). Based on this, we scale up the memory in different ways for different baselines: Bw-only indicates that we only scale up the rankset from 2 to 16, while Cap-only means we only scale up the capacity from 512GB to 4TB. Bw-Cap implies that we scale up both the rankset and capacity to (4TB, 16 ranksets). Similarly, we execute 1,000 requests from the trace and calculate the average throughput. We normalize the performance of the smallest memory configuration to 1. Fig. 9 shows the results of GPT-89B and GPT-175B with the OpenR1 trace. Results for other models and traces are similar. It shows that expanding either bandwidth or capacity alone does not effectively improve throughput. For example, when exclusively scaling memory capacity or bandwidth by 8 on the GPT-175B model, the throughput only increases by 1.6 and 1.1 , respectively. In contrast, when both bandwidth and capacity are enlarged, the throughput increases by 5.1 . This demonstrates that L3 effectively leverages the value of both scalability aspects. Ablation study. With L3 as the baseline, we remove optimizations for communication ( 5.1) and scheduling ( 5.2) and then succes- sively restore them. The results of end-to-end throughput with the GPT-175B model and the GPT-89B model on the OpenR1 trace are shown in Fig. 10. For example, with GPT-175B, after apply- ing adaptive chunk-partitioned cross-device batch interleaving, the throughput increases by 1.3 . Applying dependency-aware 10 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA 1 1.4 1.8 GPT-175B GPT-89B Normalized Throughput Naive Comm Comm Sched (a) Ablation study. 0.5 1 Pre-MHA Dec-MHA FC Error Rate ( ) (b) Prediction errors. Figure 10: Ablation study and prediction errors. The throughput with the most naive implementation in (a) is normalized to 1. 0 1 2 3 GPU RS-2 RS-4 RS-8 RS-16 GPU RS-2 RS-4 RS-8 RS-16 GPU RS-2 RS-4 RS-8 RS-16 GPU RS-2 RS-4 RS-8 RS-16 Normalized TBT MHA QKVGen Proj,FF Batch Size 64 Batch Size 48 Batch Size 32 Batch Size 16 Figure 11: Latency analysis. RS-N denotes to L3 with N ranksets. The GPU baseline is assumed infinite memory capacity. communication-computation overlap further improves the through- put by 1.7 . These results demonstrate that our system designs effectively enhance throughput in a heterogeneous architecture. 6.3 Latency Analysis Latency scalability. We evaluate L3 s time-between-tokens (TBT) compared with GPU-only inference. This evaluation is done using the GPT-89B model and different batch sizes of decoding requests, each with a token length of 6K. To ensure that the GPU baseline and L3 have the same batch size in the evaluation, we assume that the GPU baseline has infinite HBM capacity. The normalized TBT is shown in Fig.11, where the TBT of the GPU baseline with a batch size of 16 is normalized to 1. First, L3 s TBT is comparable to the GPU-only baseline even with the smallest number of ranksets. This demonstrates the effectiveness of using DIMM-PIM for decoding MHA and also shows that host-memory integration does not sacri- fice latency while improving throughput. Second, L3 s TBT can be further reduced by increasing the number of ranksets, indicating that we can further optimize performance with the expansion of host memory. When scaled up to 16 ranksets, L3 s TBT is only 29 53 of the GPU-only baseline. Performance predictability. We evaluate the performance models of L3 scheduler. We profile the performance data corresponding to 1,000 different batches, partitioning 80 of them into a training set to train our model ( 5.3). We subsequently utilize the remaining 20 of the data as a test set for validation. We use relative error to represent the prediction errors, i.e., RE 1 𝑛 Í𝑛 𝑖 1 𝑌𝑖 ˆ𝑌𝑖 𝑌𝑖 , where 𝑌𝑖 Table 5: Area and power overhead. Re-layout Unit Adder Unit Softmax Unit Bank PU Area (𝑢𝑚2) 948 3446 18506 3323 Power (𝑚𝑊) 0.245 1.812 4.345 1.129 Implementation with DRAM process would lead to 10 area overhead and ˆ𝑌𝑖represent the real latency and the predicted latency, respec- tively. Fig 10-b shows the results, representing the prediction model errors for prefilling MHA, decoding MHA, and FC, respectively. It exhibits the performance predictability of the operations, which facilitates effective balancing of parallel execution on the GPU and DIMM-PIM ( 5.2). 6.4 Hardware Overhead Table 5 presents the results of each component of the rank PU and the bank PU. The number of arithmetic units is basically deter- mined by the DDR4 parameters. In the rank PU, the AU is composed with eight adders for FP16 precision, whose interconnections are configurable to support adder trees and self-accumulators. The SU performs softmax computation in a pipelined manner, with chunk size of sixteen FP16 elements (outputs from sixteen logic banks). The multipliers and dividers are also used for final normalization between chunks. The RU performs fine-grained re-layout of two burst data of 64 bits, which equals the bus width. Considering that the DIMM can have a buffer chip of 100𝑚𝑚2 and an entire power of 10W, we identify that the rank PU incurs negligible extra power and area consumption. On the other hand, the physical bank PU is integrated near the physical bank, designed with four multipli- ers and four adders with configurable interconections. Compared with PIM based on other memory devices, we identify that DIMM- PIM s distributed physical bank PUs are more area-beneficial and introduce less overhead in each memory chip. 7 Related Work Besides works discussed in 2.6, we present other related works in this section. KV cache pruning or quantization. Prior works leverage al- gorithmic optimizations to reduce the memory requirements of LLM inference. For example, some works [26, 63, 85, 86] leverage the sparsity of attention and prune the KV cache for long context. Other works [24, 54, 67, 83] reduce the KV cache size by quanti- zation. These methods could potentially affect the accuracy of the model. Moreover, we consider our approach orthogonal to these methods since they can naturally apply L3 for further enhancement. PIM-only architecture for LLM inference. The PIM computing paradigm addresses the data movement bottleneck between mem- ory and processors by placing computation near or inside memory devices, which provides promising methods for memory-intensive kernels [8, 10, 13, 14, 16, 21, 23, 36, 37, 39, 42, 45, 47, 49, 57, 64, 68, 69, 71, 73, 79, 87, 89]. In addition to the GPU-PIM collaborative inference work listed in Table 1, there are also several works, e.g., CENT [28], LoL-PIM [53], and CXL-PNM [74], that have imple- mented LLM inference entirely on PIM, addressing the scalability in the granularity of the entire model. They fall short on their ker- nel decoupling methods. Though they provide a collective path or 11 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen matrix unit for FC kernels, their efficiency is still restricted by low data reuse and computing resources. 8 Discussions Economic advantages. While both HBM and PIM provide high bandwidth, PIM has significant economic advantages [62]. To pro- vide external high bandwidth for general purpose, HBM relies on expensive manufacturing processes involving TSVs [59] and sili- con interposers [35, 52], substantially increasing production costs. Additionally, DIMM-PIM offers greater deployment flexibility, as it can be incrementally adopted within existing server infrastruc- tures, allowing for gradual investment rather than complete system replacement. Utilizing spare GPU memory capacity. Since the entire KV cache is offloaded to the host memory, GPU memory is now utilized for storing model weights and intermediate data for prefilling requests, e.g., the activation, the partial KV cache of unfinished chunked- prefill requests, etc. Besides, in cases where there is spare GPU memory, decoding requests with their KV cache can be added to saturate the GPU. In most cases, the batch size of GPU decoding requests is much smaller than that of DIMM-PIM decoding requests, so they cause little interference to tasks on the GPU. Prior tech- niques [44] can be utilized for further optimizing the colocation of GPU decoding and prefilling requests. 9 Conclusion This paper proposes L3, a hardware-software co-designed system that provides scalable memory capacity and bandwidth for LLM inference. The primary innovation lies in utilizing DIMM-PIM to decouple and offload both KV cache storage and decoding MHA computations from GPU resources, through three key techniques: DIMM-PIM redesigns, communication optimizations, and adap- tive scheduling. Evaluations demonstrate L3 achieves significant speedup while enabling larger batch sizes, establishing a new para- digm for efficient long-context LLM deployment. References [1] 2015. DDR4 SDRAM LRDIMM. [Online]. Avalable: content dam micron global secure spectek data-sheet dram ddr4 spectek- 8gb-ddr4-sdram.pdf. [2] 2015. JEDEC Standard: DDR4 SDRAM Load Reduced DIMM (LRDIMM) Design Specification. [Online]. Avalable: docs module4_20_27. [3] 2023. NVIDIA DGX A100. [Online]. Avalable: dgx-systems dgx-ai. [4] 2025. cognitivecomputations dolphin-r1 Datasets at Hugging Face. https: huggingface.co datasets cognitivecomputations dolphin-r1. Referenced April 2025. [5] 2025. open-r1 OpenR1-Math-220k Datasets at Hugging Face. https: huggingface.co datasets open-r1 OpenR1-Math-220k. Referenced April 2025. [6] 2025. open-r1 OpenThoughts-114k-math Datasets at Hugging Face. https: huggingface.co datasets open-r1 OpenThoughts-114k-math. Referenced April 2025. [7] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming throughput-latency tradeoff in LLM inference with sarathi-serve. In Proceedings of the 18th USENIX Conference on Operating Systems Design and Implementation (Santa Clara, CA, USA) (OSDI 24). USENIX Association, USA, Article 7, 18 pages. [8] Mohammad Alian, Seung Won Min, Hadi Asgharimoghaddam, Ashutosh Dhar, Dong Kai Wang, Thomas Roewer, Adam McPadden, Oliver O Halloran, Deming Chen, Jinjun Xiong, Daehoon Kim, Wen-mei Hwu, and Nam Sung Kim. 2018. Application-Transparent Near-Memory Processing Architecture with Memory Channel Network. In 2018 51st Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 802 814. [9] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier- Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebas- tian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhong- tao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wi- eting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2 Technical Report. arXiv:2305.10403 [cs.CL] [10] Bahar Asgari, Ramyad Hadidi, Jiashen Cao, Da Eun Shim, Sung-Kyu Lim, and Hyesoon Kim. 2021. FAFNIR: Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent Reduction. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 908 920. 1109 HPCA51647.2021.00080 [11] Hadi Asghari-Moghaddam, Young Hoon Son, Jung Ho Ahn, and Nam Sung Kim. 2016. Chameleon: versatile and practical near-DRAM acceleration architecture for large memory systems. In The 49th Annual IEEE ACM International Symposium on Microarchitecture (Taipei, Taiwan) (MICRO-49). IEEE Press, Article 50, 13 pages. [12] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding. arXiv:2308.14508 [cs.CL] [13] Amirali Boroumand, Saugata Ghose, Minesh Patel, Hasan Hassan, Brandon Lu- cia, Rachata Ausavarungnirun, Kevin Hsieh, Nastaran Hajinazar, Krishna T. Malladi, Hongzhong Zheng, and Onur Mutlu. 2019. CoNDA: efficient cache coherence support for near-data accelerators. In Proceedings of the 46th Inter- national Symposium on Computer Architecture (Phoenix, Arizona) (ISCA 19). Association for Computing Machinery, New York, NY, USA, 629 642. https: doi.org 10.1145 3307650.3322266 [14] Dan Chen, Haiheng He, Hai Jin, Long Zheng, Yu Huang, Xinyang Shen, and Xiaofei Liao. 2023. MetaNMP: Leveraging Cartesian-Like Product to Acceler- ate HGNNs with Near-Memory Processing. In Proceedings of the 50th Annual International Symposium on Computer Architecture (Orlando, FL, USA) (ISCA 23). Association for Computing Machinery, New York, NY, USA, Article 56, 13 pages. [15] Quan Chen, Hailong Yang, Minyi Guo, Ram Srivatsa Kannan, Jason Mars, and Lingjia Tang. 2017. Prophet: Precise QoS Prediction on Non-Preemptive Acceler- ators to Improve Utilization in Warehouse-Scale Computers. SIGARCH Comput. Archit. News 45, 1 (April 2017), 17 32. [16] Benjamin Y. Cho, Yongkee Kwon, Sangkug Lym, and Mattan Erez. 2020. Near data acceleration with concurrent host access. In Proceedings of the ACM IEEE 47th Annual International Symposium on Computer Architecture (Virtual Event) (ISCA 20). IEEE Press, 818 831. [17] Weihao Cui, Han Zhao, Quan Chen, Ningxin Zheng, Jingwen Leng, Jieru Zhao, Zhuo Song, Tao Ma, Yong Yang, Chao Li, and Minyi Guo. 2021. Enable Simultane- ous DNN Services Based on Deterministic Operator Overlap and Precise Latency Prediction. In SC21: International Conference for High Performance Computing, Networking, Storage and Analysis. 1 15. [18] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FLASHATTENTION: fast and memory-efficient exact attention with IO- awareness. In Proceedings of the 36th International Conference on Neural Informa- tion Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 1189, 16 pages. [19] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo 12 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] [20] Fabrice Devaux. 2019. The true Processing In Memory accelerator. In 2019 IEEE Hot Chips 31 Symposium (HCS). 1 24. 8875680 [21] Alexandar Devic, Siddhartha Balakrishna Rai, Anand Sivasubramaniam, Ameen Akel, Sean Eilert, and Justin Eno. 2022. To PIM or not for emerging general purpose processing in DDR memory systems. In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 231 244. https: doi.org 10.1145 3470496.3527431 [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL] [23] Siying Feng, Xin He, Kuan-Yu Chen, Liu Ke, Xuan Zhang, David Blaauw, Trevor Mudge, and Ronald Dreslinski. 2022. MeNDA: a near-memory multi-way merge solution for sparse transposition and dataflows. In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 245 258. https: doi.org 10.1145 3470496.3527432 [24] Elias Frantar and Dan Alistarh. 2023. SparseGPT: massive language models can be accurately pruned in one-shot. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML 23). JMLR.org, Article 414, 15 pages. [25] Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. 2024. Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention. In 2024 USENIX Annual Technical Conference (USENIX ATC 24). USENIX Asso- ciation, Santa Clara, CA, 111 126. presentation gao-bin-cost [26] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. 2025. SeerAtten- tion: Learning Intrinsic Sparse Attention in Your LLMs. arXiv:2410.13276 [cs.CL] [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That- tai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasu- den Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsim- poukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Va- sic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Ro- han Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold- schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ram- chandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowl- ing, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Cag- gioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Han- nah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Has- son, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navy- ata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Niko- lay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyag- ina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Ran- gaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, 13 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robin- son, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] [28] Yufeng Gu, Alireza Khadem, Sumanth Umesh, Ning Liang, Xavier Servot, Onur Mutlu, Ravi Iyer, and Reetuparna Das. 2025. PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. ACM, 862 881. 1145 3676641.3716267 [29] Juan Gómez-Luna, Izzat El Hajj, Ivan Fernandez, Christina Giannoula, Geraldo F. Oliveira, and Onur Mutlu. 2022. Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System. IEEE Access 10 (2022), 52565 52608. [30] Jiaao He and Jidong Zhai. 2024. FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines. arXiv:2403.11421 [cs.DC] https: arxiv.org abs 2403.11421 [31] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N. Vijaykumar. 2020. Newton: A DRAM-maker s Accelerator-in-Memory (AiM) Architecture for Machine Learning. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 372 385. [32] Yintao He, Haiyu Mao, Christina Giannoula, Mohammad Sadrosadati, Juan Gómez-Luna, Huawei Li, Xiaowei Li, Ying Wang, and Onur Mutlu. 2025. PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Lan- guages and Operating Systems, Volume 2 (Rotterdam, Netherlands) (ASPLOS 25). Association for Computing Machinery, New York, NY, USA, 766 782. [33] Guseul Heo, Sangyeop Lee, Jaehong Cho, Hyunmin Choi, Sanghyeon Lee, Hyungkyu Ham, Gwangsun Kim, Divya Mahajan, and Jongse Park. 2024. Ne- uPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (AS- PLOS 24). Association for Computing Machinery, New York, NY, USA, 722 737. [34] Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. arXiv:2212.10403 [cs.CL] 2212.10403 [35] P. K. Huang, C. Y. Lu, W. H. Wei, Christine Chiu, K. C. Ting, Clark Hu, C.H. Tsai, S. Y. Hou, W. C. Chiou, C. T. Wang, and Douglas Yu. 2021. Wafer Level System Integration of the Fifth Generation CoWoS -S with High Performance Si Interposer at 2500 mm2. In 2021 IEEE 71st Electronic Components and Technology Conference (ECTC). 101 104. [36] Wenqin Huangfu, Xueqi Li, Shuangchen Li, Xing Hu, Peng Gu, and Yuan Xie. 2019. MEDAL: Scalable DIMM based Near Data Processing Accelerator for DNA Seeding Algorithm. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture (Columbus, OH, USA) (MICRO 52). Association for Computing Machinery, New York, NY, USA, 587 599. 1145 3352460.3358329 [37] Wenqin Huangfu, Krishna T. Malladi, Andrew Chang, and Yuan Xie. 2023. BEA- CON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support. In Proceedings of the 55th Annual IEEE ACM International Symposium on Microarchitecture (Chicago, Illinois, USA) (MICRO 22). IEEE Press, 727 743. [38] Son Hyojun, Jonatan Gilbert, Xiangyu Wu, Cho Haeyoon, Shivdikar Kaustubh, Abellán José L., Joshi Ajay, Kaeli David, and Kim John. 2025. PIMnet: A Domain- Specific Network for Efficient Collective Communication in Scalable PIM. [39] Bongjoon Hyun, Taehun Kim, Dongjae Lee, and Minsoo Rhu. 2024. Pathfinding Future PIM Architectures by Demystifying a Commercial PIM Technology. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 263 279. [40] Xuanlin Jiang, Yang Zhou, Shiyi Cao, Ion Stoica, and Minlan Yu. 2024. NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference. arXiv:2411.01142 [cs.DC] [41] Hongshin Jun, Jinhee Cho, Kangseol Lee, Ho-Young Son, Kwiwook Kim, Hanho Jin, and Keith Kim. 2017. HBM (High Bandwidth Memory) DRAM Technology and Architecture. In 2017 IEEE International Memory Workshop (IMW). 1 4. https: doi.org 10.1109 IMW.2017.7939084 [42] Hongju Kal, Chanyoung Yoo, and Won Woo Ro. 2023. AESPA: Asynchronous Execution Scheme to Exploit Bank-Level Parallelism of Processing-in-Memory. In Proceedings of the 56th Annual IEEE ACM International Symposium on Microar- chitecture (Toronto, ON, Canada) (MICRO 23). Association for Computing Ma- chinery, New York, NY, USA, 815 827. [43] Katikapalli Subramanyam Kalyan. 2023. A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4. arXiv:2310.12321 [cs.CL] [44] Aditya K. Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachan- dran Ramjee, and Ashish Panwar. 2025. POD-Attention: Unlocking Full Prefill- Decode Overlap for Faster LLM Inference. In Proceedings of the 30th ACM In- ternational Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (Rotterdam, Netherlands) (ASPLOS 25). As- sociation for Computing Machinery, New York, NY, USA, 897 912. https: doi.org 10.1145 3676641.3715996 [45] Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David Brooks, Vikas Chandra, Utku Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng Li, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail Smelyanskiy, Xiaodong Wang, Brandon Reagen, Carole-Jean Wu, Mark Hempstead, and Xuan Zhang. 2020. RecNMP: Accelerating Personalized Rec- ommendation with Near-Memory Processing. In 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). 790 803. https: doi.org 10.1109 ISCA45697.2020.00070 [46] Liu Ke, Xuan Zhang, Jinin So, Jong-Geon Lee, Shin-Haeng Kang, Sukhan Lee, Songyi Han, YeonGon Cho, Jin Hyun Kim, Yongsuk Kwon, KyungSoo Kim, Jin Jung, Ilkwon Yun, Sung Joo Park, Hyunsun Park, Joonho Song, Jeonghyeon Cho, Kyomin Sohn, Nam Sung Kim, and Hsien-Hsin S. Lee. 2022. Near-Memory Processing in Action: Accelerating Personalized Recommendation With AxDIMM. IEEE Micro 42, 1 (2022), 116 127. [47] Liu Ke, Xuan Zhang, Jinin So, Jong-Geon Lee, Shin-Haeng Kang, Sukhan Lee, Songyi Han, YeonGon Cho, Jin Hyun Kim, Yongsuk Kwon, KyungSoo Kim, Jin Jung, Ilkwon Yun, Sung Joo Park, Hyunsun Park, Joonho Song, Jeonghyeon Cho, Kyomin Sohn, Nam Sung Kim, and Hsien-Hsin S. Lee. 2022. Near-Memory Processing in Action: Accelerating Personalized Recommendation With AxDIMM. IEEE Micro 42, 1 (2022), 116 127. [48] Guhyun Kim, Jinkwon Kim, Nahsung Kim, Woojae Shin, Jongsoon Won, Hyunha Joo, Haerang Choi, Byeongju An, Gyeongcheol Shin, Dayeon Yun, Jeongbin Kim, Changhyun Kim, Ilkon Kim, Jaehan Park, Yosub Song, Byeongsu Yang, Hyeongdeok Lee, Seungyeong Park, Wonjun Lee, Seonghun Kim, Yonghoon Park, Yousub Jung, Gi-Ho Park, and Euicheol Lim. 2024. SK Hynix AI-Specific Computing Memory Solution: From AiM Device to Heterogeneous AiMX-xPU System for Comprehensive LLM Inference. In 2024 IEEE Hot Chips 36 Symposium (HCS). 1 26. [49] Heesu Kim, Hanmin Park, Taehyun Kim, Kwanheum Cho, Eojin Lee, Soojung Ryu, Hyuk-Jae Lee, Kiyoung Choi, and Jinho Lee. 2021. GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent. In 2021 IEEE Interna- tional Symposium on High-Performance Computer Architecture (HPCA). 249 262. [50] Jin Hyun Kim, Shin-haeng Kang, Sukhan Lee, Hyeonsu Kim, Woongjae Song, Yuhwan Ro, Seungwon Lee, David Wang, Hyunsung Shin, Bengseng Phuah, Jihyun Choi, Jinin So, YeonGon Cho, JoonHo Song, Jangseok Choi, Jeonghyeon Cho, Kyomin Sohn, Youngsoo Sohn, Kwangil Park, and Nam Sung Kim. 2021. Aquabolt-XL: Samsung HBM2-PIM with in-memory processing for ML ac- celerators and beyond. In 2021 IEEE Hot Chips 33 Symposium (HCS). 1 26. [51] Jin Hyun Kim, Shin-haeng Kang, Sukhan Lee, Hyeonsu Kim, Woongjae Song, Yuhwan Ro, Seungwon Lee, David Wang, Hyunsung Shin, Bengseng Phuah, Jihyun Choi, Jinin So, YeonGon Cho, JoonHo Song, Jangseok Choi, Jeonghyeon Cho, Kyomin Sohn, Youngsoo Sohn, Kwangil Park, and Nam Sung Kim. 2021. Aquabolt-XL: Samsung HBM2-PIM with in-memory processing for ML ac- celerators and beyond. In 2021 IEEE Hot Chips 33 Symposium (HCS). 1 26. [52] Kwiwook Kim and Myeong-jae Park. 2024. Present and Future, Challenges of High Bandwith Memory (HBM). In 2024 IEEE International Memory Workshop (IMW). 1 4. [53] Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, and Jungwook Choi. 2025. LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System. arXiv:2412.20166 [cs.AR] [54] Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Bae- seong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee. 2022. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale 14 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA Pre-Trained Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 3288 3305. [55] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (Koblenz, Germany) (SOSP 23). Association for Computing Machinery, New York, NY, USA, 611 626. [56] Yongkee Kwon, Guhyun Kim, Nahsung Kim, Woojae Shin, Jongsoon Won, Hyunha Joo, Haerang Choi, Byeongju An, Gyeongcheol Shin, Dayeon Yun, Jeongbin Kim, Changhyun Kim, Ilkon Kim, Jaehan Park, Chanwook Park, Yosub Song, Byeongsu Yang, Hyeongdeok Lee, Seungyeong Park, Wonjun Lee, Seongju Lee, Kyuyoung Kim, Daehan Kwon, Chunseok Jeong, John Kim, Euicheol Lim, and Junhyun Chun. 2023. Memory-Centric Computing with SK Hynix s Domain-Specific Memory. In 2023 IEEE Hot Chips 35 Symposium (HCS). 1 26. [57] Youngeun Kwon, Yunjae Lee, and Minsoo Rhu. 2019. TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture (Columbus, OH, USA) (MICRO 52). Association for Computing Machinery, New York, NY, USA, 740 753. 1145 3352460.3358284 [58] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim, Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim, Byeongju An, Jeong- bin Kim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook Park, Yosub Song, Byeongsu Yang, Hyungdeok Lee, Seho Kim, Daehan Kwon, Seongju Lee, Kyuy- oung Kim, Sanghoon Oh, Joonhong Park, Gimoon Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil Kang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee, Minyoung Shin, Minhwan Shin, Jaekyung Cha, Changson Jung, Kijoon Chang, Chunseok Jeong, Euicheol Lim, Il Park, Junhyun Chun, and Sk Hynix. 2022. System Architecture and Software Stack for GDDR6-AiM. In 2022 IEEE Hot Chips 34 Symposium (HCS). 1 25. [59] John H. Lau. 2022. Recent Advances and Trends in Multiple System and Heterogeneous Integration With TSV-Less Interposers. IEEE Transactions on Components, Packaging and Manufacturing Technology 12, 8 (2022), 1271 1281. [60] Dongjae Lee, Bongjoon Hyun, Taehun Kim, and Minsoo Rhu. 2024. PIM-MMU: A Memory Management Unit for Accelerating Data Transfers in Commercial PIM Systems. In 2024 57th IEEE ACM International Symposium on Microarchitecture (MICRO). 627 642. [61] Donghun Lee, Jinin So, MINSEON AHN, Jong-Geon Lee, Jungmin Kim, Jeonghyeon Cho, Rebholz Oliver, Vishnu Charan Thummala, Ravi shankar JV, Sachin Suresh Upadhya, Mohammed Ibrahim Khan, and Jin Hyun Kim. 2022. Im- proving In-Memory Database Operations with Acceleration DIMM (AxDIMM). In Proceedings of the 18th International Workshop on Data Management on New Hard- ware (Philadelphia, PA, USA) (DaMoN 22). Association for Computing Machinery, New York, NY, USA, Article 2, 9 pages. [62] Hyungdeok Lee, Guhyun Kim, Dayeon Yun, Ilkon Kim, Yongkee Kwon, and Euicheol Lim. 2024. Cost-Effective LLM Accelerator Using Processing in Memory Technology. In 2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Tech- nology and Circuits). 1 2. 2024.10631397 [63] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. 2024. InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). USENIX Association, Santa Clara, CA, 155 172. [64] Cong Li, Zhe Zhou, Size Zheng, Jiaxi Zhang, Yun Liang, and Guangyu Sun. 2024. SpecPIM: Accelerating Speculative Inference on PIM-Enabled System via Architecture-Dataflow Co-Exploration. In Proceedings of the 29th ACM Inter- national Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 950 965. 3620666.3651352 [65] Jinhao Li, Jiaming Xu, Shan Huang, Yonghua Chen, Wen Li, Jun Liu, Yaoxiu Lian, Jiayi Pan, Li Ding, Hao Zhou, Yu Wang, and Guohao Dai. 2025. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective. arXiv:2410.04466 [cs.AR] [66] Shang Li, Zhiyuan Yang, Dhiraj Reddy, Ankur Srivastava, and Bruce Jacob. 2020. DRAMsim3: A Cycle-Accurate, Thermal-Capable DRAM Simulator. IEEE Comput. Archit. Lett. 19, 2 (July 2020), 106 109. [67] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Guangxuan Xiao, and Song Han. 2025. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. GetMobile: Mobile Comp. and Comm. 28, 4 (Jan. 2025), 12 17. [68] Haifeng Liu, Long Zheng, Yu Huang, Chaoqiang Liu, Xiangyu Ye, Jingrui Yuan, Xiaofei Liao, Hai Jin, and Jingling Xue. 2023. Accelerating Personalized Recom- mendation with Cross-level Near-Memory Processing. In Proceedings of the 50th Annual International Symposium on Computer Architecture (Orlando, FL, USA) (ISCA 23). Association for Computing Machinery, New York, NY, USA, Article 66, 13 pages. [69] Liu Liu, Jilan Lin, Zheng Qu, Yufei Ding, and Yuan Xie. 2021. ENMC: Extreme Near-Memory Classification via Approximate Screening. In MICRO-54: 54th Annual IEEE ACM International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO 21). Association for Computing Machinery, New York, NY, USA, 1309 1322. [70] Lian Liu, Shixin Zhao, Bing Li, Haimeng Ren, Zhaohui Xu, Mengdi Wang, Xiaowei Li, Yinhe Han, and Ying Wang. 2025. Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM. [71] Anirban Nag and Rajeev Balasubramonian. 2021. OrderLight: Lightweight Memory-Ordering Primitive for Efficient Fine-Grained PIM Computations. In MICRO-54: 54th Annual IEEE ACM International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO 21). Association for Computing Machinery, New York, NY, USA, 298 310. [72] Jaehyun Park, Jaewan Choi, Kwanhee Kyung, Michael Jaemin Kim, Yongsuk Kwon, Nam Sung Kim, and Jung Ho Ahn. 2024. AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 103 119. https: doi.org 10.1145 3620665.3640422 [73] Jaehyun Park, Byeongho Kim, Sungmin Yun, Eojin Lee, Minsoo Rhu, and Jung Ho Ahn. 2021. TRiM: Enhancing Processor-Memory Interfaces with Scal- able Tensor Reduction in Memory. In MICRO-54: 54th Annual IEEE ACM In- ternational Symposium on Microarchitecture (Virtual Event, Greece) (MICRO 21). Association for Computing Machinery, New York, NY, USA, 268 281. [74] Sang-Soo Park, KyungSoo Kim, Jinin So, Jin Jung, Jonggeon Lee, Kyoungwan Woo, Nayeon Kim, Younghyun Lee, Hyungyo Kim, Yongsuk Kwon, Jinhyun Kim, Jieun Lee, YeonGon Cho, Yongmin Tai, Jeonghyeon Cho, Hoyoung Song, Jung Ho Ahn, and Nam Sung Kim. 2024. An LPDDR-based CXL-PNM Platform for TCO- efficient Inference of Transformer-based Large Language Models. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 970 982. [75] Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. 2024. Reasoning with Large Language Models, a Survey. arXiv:2407.11511 [cs.AI] [76] Minseok Seo, Xuan Truong Nguyen, Seok Joong Hwang, Yongkee Kwon, Guhyun Kim, Chanwook Park, Ilkon Kim, Jaehan Park, Jeongbin Kim, Woojae Shin, Jongsoon Won, Haerang Choi, Kyuyoung Kim, Daehan Kwon, Chunseok Jeong, Sangheon Lee, Yongseok Choi, Wooseok Byun, Seungcheol Baek, Hyuk-Jae Lee, and John Kim. 2024. IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 545 560. [77] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gon- zalez, and Ion Stoica. 2023. S-LoRA: Serving Thousands of Concurrent LoRA Adapters. arXiv preprint arXiv:2311.03285 (2023). [78] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. FlexGen: high-throughput generative inference of large language models with a single GPU. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML 23). JMLR.org, Article 1288, 23 pages. [79] Weiyi Sun, Zhaoshi Li, Shouyi Yin, Shaojun Wei, and Leibo Liu. 2021. ABC-DIMM: alleviating the bottleneck of communication in DIMM-based near-memory pro- cessing with inter-DIMM broadcast. In Proceedings of the 48th Annual International Symposium on Computer Architecture (Virtual Event, Spain) (ISCA 21). IEEE Press, 237 250. [80] Boyu Tian, Yiwei Li, Li Jiang, Shuangyu Cai, and Mingyu Gao. 2024. NDP- Bridge: Enabling Cross-Bank Coordination in Near-DRAM-Bank Processing Ar- chitectures. In 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA). 628 643. [81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS 17). Curran Associates Inc., Red Hook, NY, USA, 6000 6010. [82] Ying Wei, Yi Chieh Huang, Haiming Tang, Nithya Sankaran, Ish Chadha, Dai Dai, Olakanmi Oluwole, Vishnu Balan, and Edward Lee. 2023. 9.3 NVLink-C2C: A Coherent Off Package Chip-to-Chip Interconnect with 40Gbps pin Single- ended Signaling. In 2023 IEEE International Solid-State Circuits Conference (ISSCC). 15 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen 160 162. [83] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. ZeroQuant: efficient and affordable post-training quantization for large-scale transformers. In Proceedings of the 36th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 1970, 16 pages. [84] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung- Gon Chun. 2022. Orca: A Distributed Serving System for Transformer-Based Generative Models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). USENIX Association, Carlsbad, CA, 521 538. https: www.usenix.org conference osdi22 presentation yu [85] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention. arXiv:2502.11089 [cs.CL] [86] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. 2023. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 1506, 50 pages. [87] Yilong Zhao, Mingyu Gao, Fangxin Liu, Yiwei Hu, Zongwu Wang, Han Lin, Ji Li, He Xian, Hanlin Dong, Tao Yang, Naifeng Jing, Xiaoyao Liang, and Li Jiang. 2024. UM-PIM: DRAM-based PIM with Uniform Shared Memory Space. In 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA). 644 659. [88] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yihen Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Lichao Sun, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Ninghao Liu, Bei Jiang, Linglong Kong, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, and Tianming Liu. 2024. Evaluation of OpenAI o1: Opportunities and Challenges of AGI. arXiv:2409.18486 [cs.CL] [89] Zhe Zhou, Cong Li, Xuechao Wei, Xiaoyang Wang, and Guangyu Sun. 2023. GNNear: Accelerating Full-Batch Training of Graph Neural Networks with near-Memory Processing. In Proceedings of the International Conference on Par- allel Architectures and Compilation Techniques (Chicago, Illinois) (PACT 22). Association for Computing Machinery, New York, NY, USA, 54 68. https: doi.org 10.1145 3559009.3569670 [90] Zhe Zhou, Cong Li, Fan Yang, and Guangyu Sun. 2023. DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 302 316. [91] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao- Ping Zhang, Yuhan Dong, and Yu Wang. 2024. A Survey on Efficient Inference for Large Language Models. arXiv:2404.14294 [cs.CL] 14294 16\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nL3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Qingyuan Liu , Liyan Chen , Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Shanghai Jiao Tong University Shanghai, China Abstract Large Language Models (LLMs) increasingly require processing long text sequences, but GPU memory limitations force difficult trade-offs between memory capacity and bandwidth. While HBM- based acceleration offers high bandwidth, its capacity remains con- strained. Offloading data to host-side DIMMs improves capacity but introduces costly data swapping overhead. We identify that the critical memory bottleneck lies in the decoding phase of multi-head attention (MHA) exclusively, which demands substantial capacity for storing KV caches and high bandwidth for attention computa- tion. Our key insight reveals this operation uniquely aligns with modern DIMM-based processing-in-memory (PIM) architectures, which offers scalability of both capacity and bandwidth. Based on this observation and insight, we propose L3, a hardware- software co-designed system integrating DIMM-PIM and GPU de- vices. L3 introduces three innovations: First, hardware redesigns resolve data layout mismatches and computational element mis- matches in DIMM-PIM, enhancing LLM inference utilization. Sec- ond, communication optimization enables hiding the data transfer overhead with the computation. Third, an adaptive scheduler coordi- nates GPU-DIMM-PIM operations to maximize parallelism between devices. Evaluations using real-world traces show L3 achieves up to 6.1 speedup over state-of-the-art HBM-PIM solutions while significantly improving batch sizes. 1 Introduction The growing adoption of Large Language Models (LLMs) has cre- ated an imperative for long-context processing capabilities, driven by the increasingly complex and rich user prompts [27, 88, 91]. Today s reasoning models [19, 34, 75] exacerbate this demand by employing multi-round thinking to enhance the quality of output content, thereby increasing the demand for long outputs. Real- world traces from OpenR1 [5] show an average output length of more than 6k tokens. When supporting long-context LLM inference, current GPU systems face memory constraints in two aspects: Equal contribution.\n\n--- Segment 2 ---\nReal- world traces from OpenR1 [5] show an average output length of more than 6k tokens. When supporting long-context LLM inference, current GPU systems face memory constraints in two aspects: Equal contribution. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from Conference 17, July 2017, Washington, DC, USA 2025 Copyright held by the owner author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x YY MM First is the unscalable memory capacity constraint. Since inference requests with long context have large KV cache whose memory capacity demand scales linearly with the number of context tokens, the spare and expensive GPU HBM (High-Bandwidth-Memory [41]) constrains the batch sizes of requests. For example, 2.28 requests of GPT-175B with 8k token length could consume the entire memory of one A100 [3] (80GB). The constrained batch sizes of long-context requests further limit the GPU utilization and throughput. Second is the unscalable memory bandwidth constraint. HBM falls short in providing scalable bandwidth due to its high integration overhead on either TSV [59] (Through-Silicon-Via) densities (single stack) or interposer dimensions [35, 52] (multiple stacks). Moreover, it is challenging to add additional HBM stacks for already packaged chips. Since the increasing KV cache needs to be repeatedly fetched from off-chip HBMs in each iteration, it would result in increasing decoding latencies. Moreover, as the performance is bounded by off-chip bandwidth, GPUs are underutilized during the memory- intensive KV cache related computations [72]. Prior works fail to achieve simultaneously scalability of capacity and bandwidth and suffer from trade-offs between them [65].\n\n--- Segment 3 ---\nMoreover, as the performance is bounded by off-chip bandwidth, GPUs are underutilized during the memory- intensive KV cache related computations [72]. Prior works fail to achieve simultaneously scalability of capacity and bandwidth and suffer from trade-offs between them [65]. HBM- based optimizations such as integrating HBM with Processing-in- Memory (PIM) [32, 33, 72] efficiently improve bandwidth. How- ever, they inherit or even exacerbate the capacity limitations of HBMs. Some works resort to CPU-side DIMM-based memory for KV cache storage [25, 85] for higher capacity, whose data swapping overhead can easily dominate the overall performance even with the prefetching mechanism. Works that further offload operations (e.g., Feed-forward or multi-head-attention) to the CPU side re- duce data transfer between devices but suffer from low computing efficiency [30, 40, 70], especially in the long-context scenario. To address the scalability issue, our insight is that, the dual- dimensional memory requirements for capacity and bandwidth scal- ability are essentially driven by the linearly increasing storage and computation demands of the KV cache, which are exclusive to the de- coding MHA operation. Therefore, we propose fully decoupling and offloading the decoding MHA along with the entire KV cache to PIM- enhanced CPU host memory (DIMMs). Decoding MHA could take advantage of DIMM-PIM in two aspects. First, DIMM-PIM devices inherit the scalability of DIMM with its standard modular hardware interface and plug-and-play feature [1]. Second, DIMM-PIM s capac- ity and bandwidth scales with the number of DIMMs [11, 57], which suits the feature of decoding MHA whose capacity and bandwidth requirements scale with the number of context tokens. Achiev- ing both types of scalability helps increase the batch size, thus optimizing GPU utilization and throughput without sacrificing time-between-tokens (TBT).\n\n--- Segment 4 ---\nSecond, DIMM-PIM s capac- ity and bandwidth scales with the number of DIMMs [11, 57], which suits the feature of decoding MHA whose capacity and bandwidth requirements scale with the number of context tokens. Achiev- ing both types of scalability helps increase the batch size, thus optimizing GPU utilization and throughput without sacrificing time-between-tokens (TBT). 1 arXiv:2504.17584v1 [cs.AR] 24 Apr 2025 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen The integration with DIMM-PIM introduces new challenges on both hardware and system sides. Hardware challenges. The DIMM-based communication layout mismatches with the PIM computation layout for decoding MHA, resulting in additional re-layout overhead and computation inef- ficiency. First, for each offloaded KV, the bits of a single element are stored across multiple DRAM chips [11, 20], while PIM com- putation requires them to reside in the same chip, which we refer to as bit-level mismatch. Second, the co-processed elements of K cache or V cache are interleaved on chips or addresses, while PIM efficiency needs element mapping with both locality and regularity, which we refer to as element-level mismatch. System challenges. The heterogeneous inference systems intro- duce new challenges to fully utilize the resources on the two devices simultaneously. First, host memory integration introduces cross- device data transfer. This could incur non-trivial overhead to the inference process and interfere DIMM-PIM computation. Second, it is challenging to parallelize the execution on the two devices with- out idle bubbles for two reasons. To begin with, the parallelizable operations on the two devices within a batch is constrained by the computation dependency. Besides, due to the autoregressive nature of LLM inference, the execution latencies of parallelizable parts on the two devices could be varying and imbalanced, causing idle bubbles on the device where the workload is finished earlier. To address those challenges, we propose L3, a heterogeneous LLM inference system that integrates GPUs with scalable DIMM- PIM.\n\n--- Segment 5 ---\nBesides, due to the autoregressive nature of LLM inference, the execution latencies of parallelizable parts on the two devices could be varying and imbalanced, causing idle bubbles on the device where the workload is finished earlier. To address those challenges, we propose L3, a heterogeneous LLM inference system that integrates GPUs with scalable DIMM- PIM. On the hardware side, L3 proposes the fine-grained re-layout and DIMM-tailored KV cache mapping methods to eliminate the two mismatches. Specifically, we propose a zero-latency in-flight re-layout method, which is seamlessly integrated in conventional DIMM-based burst transfer without DDR protocol violations [1, 2]. Moreover, two distinct mapping methods for KV cache are proposed to achieve both communication and computation efficiency, while co-designed configurable processing units (PU) at both rank level and bank level are leveraged to enable MHA kernel fusion with bubble-free pipelined execution. L3 system maximizes the resource utilizations on both GPU and DIMM-PIM devices. First, L3 applies dependency-aware communication- computation overlap, a software-hardware co-design to hide the cross-device communication overheads. It involves three techniques, which enable concurrent communication and computation on the DIMM-PIM side, achieve load balancing of data transfer and com- putation across ranks, and remove most data communication from the critical path of the inference process. Second, L3 scheduler ap- plies adaptive chunk-partitioned batch interleaving to maximize parallel execution on both GPUs and DIMM-PIM with minimum idle bubbles. Specifically, to eliminate the idle bubbles caused by the computation dependency of inference operations, L3 sched- uler coordinates requests with two sub-batches. It enables two key features, fine-grained tunability and performance predictability, to align the parallel execution of the two sub-batches. This fur- ther minimizes the idle bubbles caused by unbalanced cross-device parallel execution.\n\n--- Segment 6 ---\nIt enables two key features, fine-grained tunability and performance predictability, to align the parallel execution of the two sub-batches. This fur- ther minimizes the idle bubbles caused by unbalanced cross-device parallel execution. Comprehensive evaluations on three LLM models and four real- world traces [4 6, 12] show that L3 achieves up to 6.1 speedup over state-of-the-art HBM-PIM solutions [33, 72] with significantly is Layer 0 Layer 1 Layer N ... great Layer 0 Layer 1 Layer N ... QKV Generation Multi Head Attention (MHA) Q K K mat. V V mat. Projection Feed Forward FC MHA Score Q 1 Dh Kt Dh (Lt 1) Softmax Os 1 (Lt 1) Context V (Lt 1) Dh S 1 (Lt 1) think Layer 0 Layer 1 Layer N ... is I this New Prefilling Reqs great Decoding Reqs Finished Reqs as Figure 1: LLM inference process. The FC operations for each request can be batched and are compute-intensive, whereas MHA cannot be batched and is memory bandwidth-intensive. improved batch sizes (up to 14.3 on DGX-A100 [3]). L3 is the first host memory offloading system for higher throughput without sacrificing latencies (time-between-tokens) compared with server- grade GPUs such as A100, and will be open source. 2 Motivation 2.1 Background: LLM Inference Fig. 1 illustrates the LLM inference process [9, 22, 27, 43], which in- cludes two stages. For a new request prompt (e.g., I think this ), the model undergoes a prefilling stage, encoding the input to generate an output token for the subsequent decoding stages. In the decod- ing stage, the model produces one token at each iteration in an autoregressive manner until detecting the EOS (End of Sequence). The LLM model constitutes a sequence of layers, each compris- ing two major kinds of operations: (1) multi-head attention (MHA) operations [81] and (2) fully-connected (FC) operations, which encompass QKVGen, projection, Feed-forward Network, etc. Nor- malization and activation operations are not depicted in the figure, which are jointly scheduled with FC operations in this paper.\n\n--- Segment 7 ---\nThe LLM model constitutes a sequence of layers, each compris- ing two major kinds of operations: (1) multi-head attention (MHA) operations [81] and (2) fully-connected (FC) operations, which encompass QKVGen, projection, Feed-forward Network, etc. Nor- malization and activation operations are not depicted in the figure, which are jointly scheduled with FC operations in this paper. The computations of MHA consist of 𝑁ℎindependent heads. For each head, the prefilling stage computes the initial Key-Value (KV) ma- trices (cache) from the input prompt, while each decoding stage updates them by concatenating, with size of head dimension 𝐷ℎ and total token length of 𝐿𝑡. The MHA operation follows score (𝑄 𝐾𝑡), softmax, and context computations (𝑆 𝑉). Existing inference frameworks [7, 55, 84] typically batch multiple requests to improve system throughput and computing resource utilization, since FC weights can be shared by different token vectors of prefilling requests (e.g., I think this ) and token vectors from batched decoding requests. Increasing batch size can improve the data reuse and decrease off-chip memory access, while the large matrix dimension (total token length in a batch) can also enable better tiling and workload partition, which improves computing resources (e.g., GPU SM) utilization. On the contrary, since KV cache is distinct and specific for each request, the computation of MHA does not benefit from batching. 2.2 Background: Processing-In-Memory (PIM) PIM is a promising solution to memory-intensive operations, which can be built upon a variety of memory devices, such as HBM [31, 50, 51], GDDR [48, 56, 58, 62], and DIMM [20, 45, 46, 61]. By integrating PUs into memory devices, PIM offers higher aggregated bandwidth.\n\n--- Segment 8 ---\n2.2 Background: Processing-In-Memory (PIM) PIM is a promising solution to memory-intensive operations, which can be built upon a variety of memory devices, such as HBM [31, 50, 51], GDDR [48, 56, 58, 62], and DIMM [20, 45, 46, 61]. By integrating PUs into memory devices, PIM offers higher aggregated bandwidth. For example, with two DIMMs (e.g., two ranks of 16 banks per DIMM) in one channel, placing PUs near ranks (rank PU [23, 2 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA 45, 57]) can achieve about 4 the bandwidth of host CPU, while integrating PUs near banks (bank PU [20, 68, 73]) can improve bandwidth by more than 30 . Since the memory buses are shared between DIMMs in a channel, PIM also enables the aggregated internal bandwidth scaling with the number of DIMMs. However, leveraging the high aggregated bandwidth of PIM architectures, particularly at the bank level, necessitates careful consideration of data mapping methods to maintain data locality and regularity. PIM typically operates in a SIMD-like execution manner [31, 50, 58], where PUs execute broadcast commands re- ceived via the command address (C A) bus. During operation, each PU accesses and processes data from its associated memory device (e.g., bank-level PUs access their respective banks) using identical row and column addresses across all PUs. In this case, PIM efficiency requires co-processed data to be mapped within the same device using contiguous addressing schemes, which is a constraint that diverges from conventional host-side memory mapping approaches that distribute data across multiple memory devices. 2.3 GPU Memory Bottleneck With the development and prevalence of reasoning models, the out- put text length of LLMs is becoming increasingly long. For example, real-world traces such as OpenThoughts [6] show an average to- ken length of more than 6k. However, existing systems face two memory limitations when supporting such long-text scenarios: Capacity constraint. First, KV cache size grows linearly with the number of request tokens.\n\n--- Segment 9 ---\nHowever, existing systems face two memory limitations when supporting such long-text scenarios: Capacity constraint. First, KV cache size grows linearly with the number of request tokens. The increased memory capacity demand per request means that the expensive and scarce GPU memory can only accommodate the KV cache of fewer requests. This limits the batch size for decoding, which could further result in GPU under- utilization and throughput degradation. For example, even when deploying a 7B Llama model on an A100 with 80GB GPU memory, only a batch size of about 62 requests can be accommodated when the average token length of each request is 2K, and this would result in a batched token number of 62 during FC operations. As shown in Fig. 2-a, it would suffer severe GPU Streaming Multiprocessor (SM) under-utilization. Moreover, the continuous increase in model size further exacerbates the memory capacity pressure. Bandwidth constraint. Second, the demand for memory band- width grows in tandem with the demand for memory capacity. This is because decoding requires reading the whole KV cache from memory, whose overhead increases linearly with the number of tokens. Moreover, there is no data reuse for the KV cache, which benefits little from GPU cache and shows bandwidth-bound pat- tern [33, 72]. With the increasingly long context, the latency of decoding could also grow and could become the main bottleneck for the end-to-end latency of LLM inference. As shown in Fig. 2-b, when the context length increases to 16K, decoding MHA becomes the main bottleneck for decoding, i.e., accounting for more than 61.3 latency of the iteration. 2.4 Insight and Proposal of L3 Our insight is that, the dual dimensional memory requirements for ca- pacity and bandwidth scalability are essentially driven by the linearly increasing storage and computation demands of the KV cache, which 0 20 40 60 80 100 64 128 256 512 1024 2048 4096 8192 Utilization ( ) Batched Token Number Active SM Memory Bandwidth (a) Utilizations of A100. 0 1 2 1K 2K 4K 8K 16K Batch Size 4 27.7ms Norm. Latency Decoding Token Length MHA FC 23.0 27.0 33.6 47.5 61.3 (b) Decoding latency breakdown.\n\n--- Segment 10 ---\n0 1 2 1K 2K 4K 8K 16K Batch Size 4 27.7ms Norm. Latency Decoding Token Length MHA FC 23.0 27.0 33.6 47.5 61.3 (b) Decoding latency breakdown. Figure 2: GPU memory bottlenecks with Llama-7B on A100. Capacity constraints the batch size while bandwidth constraints the time-between-tokens. (a) profiles the GPU utilizations during the Feed-forward operation. (b) normalizes the decoding latency of 1K token length to 1. The inference system is S-LoRA [77]. (a) Hardware challenges. (b) System challenges. Chips Data Bus 8 8 8 8 Token Vector ... ... ... ... beat of burst transfer ... New Token K V Cache Vec. Q Vec. S head dimension token length Proj, FF QKV Gen Dec. MHA Pre. MHA offloading GPU DIMM-PIM Time BUBBLE BUBBLE BUBBLE costly PCIe communication bubble caused by imbalanced parallelism FP16 ... ... on- loading Figure 3: Challenges of DIMM-PIM integration. are exclusive to decoding MHA . On the contrary, for the FC oper- ations in decoding, their computation efficiency is solely related to the batch size, and an increase in token length does not incur additional computational load or memory capacity requirements. This insight motivates our architectural proposal: decoupling and fully offloading decoding MHA along with the entire KV caches to PIM enhanced host memory (DIMM). DIMM-PIM can provide benefits that match the requirements of decoding MHA: First, DIMM-PIM can leverage the modular hard- ware interface of DIMMs to conveniently achieve plug-and-play scalability. Second, the memory bandwidth and capacity require- ments of decoding MHA scale in proportion to the number of tokens. DIMM-PIM aligns perfectly with the characteristics of decoding MHA, as it enables memory bandwidth and capacity to scale pro- portionally with the number of DIMMs. Besides, FC operations are executed on the GPU side, with the batch size of these batchable parts maximized to fully utilize the GPU s computational capacity.\n\n--- Segment 11 ---\nDIMM-PIM aligns perfectly with the characteristics of decoding MHA, as it enables memory bandwidth and capacity to scale pro- portionally with the number of DIMMs. Besides, FC operations are executed on the GPU side, with the batch size of these batchable parts maximized to fully utilize the GPU s computational capacity. 2.5 Challenges of DIMM-PIM Integration Integrating DIMM-PIM in GPU systems for LLM inference can be challenging for the following reasons: Hardware challenges. As shown in Fig. 3-a, DIMM leverages multi- ple co-operated DRAM chips as a rank to form the burst data on the memory bus, with each chip contributing a portion of the total data 3 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Table 1: Related works fall short in the scenario with long contexts and large batch sizes. CPU offloading introduces a bottleneck in terms of computational efficiency and communication overhead with long contexts. PIM can effectively provide bandwidth expansion, but may be limited by its memory scalability, inappropriate kernel mapping, or inefficient cross-device scheduling. Low latency means no sacrifice in TBT compared with the GPU-only baseline in our scenario. Method Work Year Platform Capacity Scalability Bandwidth Scalability Low Latency Comm-comp Overlap Bubble Reduction Software optimization FlexGen [78] 2023 GPU CPU - CachedAttention [25] 2024 GPU CPU - NEO [40] 2025 GPU CPU Hardware-software co-optimization NeuPIMs [33] 2024 GPU HBM-PIM AttAcc [72] 2024 GPU HBM-PIM IANUS [76] 2024 GPU GDDR-PIM Hermes [70] 2025 GPU DIMM-PIM PAPI [32] 2025 GPU HBM-PIM L3 - GPU DIMM-PIM width. In this case, a KV element s different bits can be distributed to different chips after offloading (e.g., FP16 and 8 chips), which completely restricts PIM computation performed in each chip.\n\n--- Segment 12 ---\nMethod Work Year Platform Capacity Scalability Bandwidth Scalability Low Latency Comm-comp Overlap Bubble Reduction Software optimization FlexGen [78] 2023 GPU CPU - CachedAttention [25] 2024 GPU CPU - NEO [40] 2025 GPU CPU Hardware-software co-optimization NeuPIMs [33] 2024 GPU HBM-PIM AttAcc [72] 2024 GPU HBM-PIM IANUS [76] 2024 GPU GDDR-PIM Hermes [70] 2025 GPU DIMM-PIM PAPI [32] 2025 GPU HBM-PIM L3 - GPU DIMM-PIM width. In this case, a KV element s different bits can be distributed to different chips after offloading (e.g., FP16 and 8 chips), which completely restricts PIM computation performed in each chip. To solve such a bit-level mismatch, prior works typically apply offline data shuffling via CPU-assisted transposition [11, 20, 29, 49, 60], which can incur unacceptable latency during LLM inference. For instance, the CPU transposition requires additional repeated off- chip access after KV offloading, which could account for more than 2 base transfer time. Meanwhile, PIM efficiency highly relies on data locality and regular distribution, in which the bank PU is hard to access data from banks other than its corresponding one, let alone data from other chips [38, 80, 90]. Moreover, accessing non-contiguous stored data may significantly reduce bandwidth utilization due to the row switch overhead. However, as shown in the figure, the co- processed K elements stored on continuous addresses are actually distributed in physical banks across multiple DRAM chips, while the co-processed V elements would be stored discontinuously since they are generated in different iterations. Existing uniform and transposed mapping methods for KV cache incur fine-grained mem- ory access [33, 48, 56] (e.g., masked write) and frequent inter-PU communication [72], which also cannot be applied to distributed physical banks in DIMM-PIM. System challenges. Heterogeneous inference comprising GPUs and DIMM-PIMs presents new challenges for achieving high resource utilization on both devices in two aspects. First is the communi- cation challenge, since the LLM inference is executed across the GPU and host memory and requires data transfer between them.\n\n--- Segment 13 ---\nHeterogeneous inference comprising GPUs and DIMM-PIMs presents new challenges for achieving high resource utilization on both devices in two aspects. First is the communi- cation challenge, since the LLM inference is executed across the GPU and host memory and requires data transfer between them. The communication involves offloading the Q, K, V generated on the GPU side to host memory, and onloading the computed atten- tion from the host memory to GPU, as shown in Fig. 3-b. The data transfer would incur additional latencies with the constraint of PCIe bandwidth. Moreover, with the constraint of shared internal memory buses, the data transfer of communication would interfere or even stick the computation on DIMM-PIM. Second, it is challenging to parallelize the execution on the two devices without idle bubbles for two reasons. To begin with, opera- tions of LLM inference have computational dependencies. Within the same batch, only the MHA operation of prefilling decoding requests can be executed in parallel on both the GPU and DIMM- PIM. Other operations need to be batched together and executed exclusively on the GPU, which could result in idle bubbles on the DIMM-PIM, as shown in Fig. 3-b. Next, the parallelizable operations on the two devices have varying execution latencies due to the au- toregressive nature of LLM inference. The latencies are affected by the batch size, various input token lengths, and unpredictable output token lengths, etc. This implies that logically parallelizable parts on the two device sides could have mismatched execution latencies, causing idle bubbles on the PU where the workload is finished earlier, for example, the bubble on the GPU side in Fig. 3-b. 2.6 Related Works As shown in Table 1, prior works fail to achieve our goal for the following reasons: Host memory integration. Prior works such as CachedAtten- tion [25] and FlexGen [78] have explored expanding storage capac- ity by utilizing host memory. However, the inference requires fetch- ing data from host memory to the GPU which introduces significant latency overhead limited by the PCIe bandwidth. In long-context scenarios, the overhead could be prohibitively large and cannot be effectively hidden by computation, even with techniques such as prefetching [25].\n\n--- Segment 14 ---\nHowever, the inference requires fetch- ing data from host memory to the GPU which introduces significant latency overhead limited by the PCIe bandwidth. In long-context scenarios, the overhead could be prohibitively large and cannot be effectively hidden by computation, even with techniques such as prefetching [25]. Besides, FastDecode [30] and NEO [33] further offload corresponding computations to the CPU to reduce communi- cation costs, as the GPU does not need to fetch the entire offloaded data from host memory, but rather the computed results, which are significantly smaller in size. However, the limited TFLOPS and memory bandwidth of CPU brings new bottlenecks. For example in DGX-A100 system, the aggregated HBM bandwidth can reach about 16.3TB s, which is 40 higher than 16-channel DDR4-3200 mem- ory (406GB s). Consequently, when executing bandwidth-intensive workloads such as decoding MHA, the performance may experience degradation by more than 10 . 4 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA GPU GPU GPU ... ... HBMs HBMs HBMs Batch Scheduler DIMM-PIM Lin0 Lin1 Host CPU MC Rank PU ... Bank PUs Banks Decoding Queue Prefilling Queue Request CPUs Scheduler QKV Attn. KV PCIe Dec. MHA FC, Pre. MHA Figure 4: L3 overview. Hardware-Software co-optimization. Prior research has inves- tigated GPU-PIM integration for inference systems to capitalize on PIM s bandwidth advantages. Systems such as NeuPIMs [33], AttAcc [72], and PAPI [32] enhance GPUs with HBM-PIM, achiev- ing considerable performance improvements through increased bandwidth. However, their capacity constraints may impact batch size scalability and further limit GPU utilization. While IANUS [76] demonstrates effectiveness for non-batched inference, its operation mapping approach differs from what might be optimal for large- batch scenarios. Hermes [70] targets LLM inference on consumer- grade GPUs, leveraging DIMM-PIM for both FC and MHA oper- ations.\n\n--- Segment 15 ---\nWhile IANUS [76] demonstrates effectiveness for non-batched inference, its operation mapping approach differs from what might be optimal for large- batch scenarios. Hermes [70] targets LLM inference on consumer- grade GPUs, leveraging DIMM-PIM for both FC and MHA oper- ations. It fails to exploit bank-level PIM bandwidth, resulting in its PIM device s bandwidth being unable to match that of cloud- based GPUs (about only one-fourth of the GPU s bandwidth with DGX-A100 s configuration). Moreover, they do not consider how to exploit cross-device parallelism under large batch sizes, leading to idle bubbles on both devices. 3 Overview of L3 We propose L3, a scalable heterogeneous LLM inference system with computation decoupling and coordination between GPU and DIMM-PIM. Fig. 4 illustrates LLM inference with L3 s heteroge- neous architecture. L3 maintains model parameters within GPU memory while exclusively storing the KV cache in host memory. High-performance GPUs provide massive computational resources, and compute-intensive GeMM kernels are mapped to GPUs. DIMM- PIM-integrated host memory provides scalable capacity and band- width, and the KV cache related decoding MHA phase is offloaded to DIMM-PIMs. Cross-device inference process. For each iteration, L3 scheduler selects prefilling and decoding requests to form a batch. Initially, QKV Generation of all requests is batched and executed on the GPU. Then, prefilling and decoding MHA operations are processed concurrently on the GPU and PIM-enabled host memory respec- tively. Following decoding MHA, the computed attention results are transmitted back to the GPU. L3 aggregates attention outputs from all prefilling and decoding requests in the batch, and then performs batched execution of subsequent operations (projection, Feed-forward, etc). Throughout the inference process, PCIe inter- connects facilitate data transfer between devices. Design overview. The design of L3 mainly involves two primary components. First, to address the challenges posed by the two mis- matches, L3 implements an in-flight fine-grained relayout ( 4.1), which seamlessly integrates with conventional DIMM-based burst transfer without DDR protocol violations.\n\n--- Segment 16 ---\nThe design of L3 mainly involves two primary components. First, to address the challenges posed by the two mis- matches, L3 implements an in-flight fine-grained relayout ( 4.1), which seamlessly integrates with conventional DIMM-based burst transfer without DDR protocol violations. In addition, cross-level PUs are configured to cooperate with two distinct mapping meth- ods of KV cache ( 4.2), which further enables kernel fusion with a bubble-free pipelined execution manner ( 4.3). Second, L3 maxi- mizes resource utilization across both GPU and DIMM-PIM sides with optimizations on two dimensions: dependency-aware communication- computation overlap ( 5.1), that reduces the cross-device data trans- fer overhead, and adaptive chunk-partitioned batch interleaving ( 5.2), that optimizes parallel execution across both sides. The fol- lowing sections elaborate on these two designs (DIMM-PIM hard- ware and PIM-powered inference system) in greater detail. 4 L3 DIMM-PIM Our architecture implements a hierarchical processing framework with strategically integrated PUs that cooperate across different levels: rank-level PUs positioned on the buffer chip and bank-level PUs embedded within each DRAM chip, as illustrated in Fig. 5-a. This cross-level coordination forms the foundation for three key innovations: (1) in-flight data re-layout mechanisms that resolve the bit-level mismatch while overcoming the challenges introduced by DDR burst [1] and timing constraints [2]; (2) distinct KV map- ping methods that organize co-processed elements contiguously at burst-transfer granularity, effectively addressing the element-level mismatch through the cross-level PU collaboration; and (3) com- prehensive kernel fusion that seamlessly integrates score computa- tion, softmax operations, and context computation in a bubble-free pipelined execution flow. Together, they enable L3 to overcome the fundamental mismatch challenges in DIMM-PIM while maximizing computational throughput during attention computation. 4.1 Zero-latency In-flight Re-layout We implement a re-layout unit in the rank PU to perform data trans- formation during GPU-to-CPU offloading (KV for prefilling, QKV for decoding).\n\n--- Segment 17 ---\nTogether, they enable L3 to overcome the fundamental mismatch challenges in DIMM-PIM while maximizing computational throughput during attention computation. 4.1 Zero-latency In-flight Re-layout We implement a re-layout unit in the rank PU to perform data trans- formation during GPU-to-CPU offloading (KV for prefilling, QKV for decoding). This unit addresses scenarios where element preci- sion mismatches with DRAM chip width, such as FP16 precision and 8 DRAM chips. As shown in Fig. 5-b, our approach strategically exchanges specific bits between data elements to map complete elements to one DRAM chip. Unlike conventional mapping where the upper and lower 8 bits of an element reside in separate chips, the re-layout technique ensures each element is stored within a single DRAM chip. Challenges. However, our re-layout approach encounters two sig- nificant challenges. First, it conflicts with burst transfer operations, as data issued by the host memory controller (MC) arrives on the data bus in a continuous stream, providing no opportunity to collec- tively process and re-layout multiple elements after transfer begins. Second, performing in-flight re-layout operations risks introduc- ing additional processing cycles during data transfer, potentially violating critical DDR timing constraints and resulting in DRAM physical errors or bus contention issues. To address these limita- tions effectively, we introduce two techniques described below. Double buffering. To seamlessly integrate re-layout operations during DDR burst transfer, we implement an efficient double buffer- ing mechanism on the buffer chip, replacing the conventional single- buffer approach. This technique enables simultaneous buffering of two consecutive burst data beats, providing the necessary temporal overlap for performing bit exchange operations without disrupting 5 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen ... Bank PU Bank PU Bank PU ... Shared Buffer DDR CMD PIM CMD Rank PU Buffer Data PIM CMD Rank PU Side Result Buffer Shared Buffer Bank I O Sense Amplifiers Result Buffer Shared Buffer Bank I O Sense Amplifiers Config. State Re-layout Unit Softmax Unit Adder Unit On-chip Bufer Memory Ctrl. Host Data Rank PU Data Disabled during comp.\n\n--- Segment 18 ---\nState Re-layout Unit Softmax Unit Adder Unit On-chip Bufer Memory Ctrl. Host Data Rank PU Data Disabled during comp. Host Side Bank PU Side V Max Adders Exponentials Adder Tree Divider Multipliers Local Global Softmax Result On-chip Buffer DRAM Chip Element Head Dimension: Dh New Token: Bank N Tokens: Lt 1 Bank 3: Token 3 Matrix K Vector Q C0 C1 C2 C3 C4 C5 C6 C7 Head Dimension: Dh Bank 0: Token 0 Bank 1: Token 1 Bank 2: Token 2 New Token: Bank N-N 3 Adder Tree Mode Accumulator Mode Vec. psum DDR CMD DDR PIM CMD exchange Bus Width Chip Width Chip Num Data 0 Data 1 Data 0 Data 1 (d) Score computation and mapping method. (c) Pipelined softmax unit. (b) Fine-grained re-layout. (e) Context computation and mapping method. (a) DIMM-PIM hardware for one rank. Rank PU Buffer Chip Chip Chip Chip Rank Host Banks Mat. Chip Burst Transfer Bank 0 - Bank 3: Token 0 C0 C1 C2 C3 C4 C5 C6 C7 Bank 8 - Bank 11: Token 2 Matrix V Bank 0 - Bank 3: Token 4 Bank 4 - Bank 7: Token 1 Bank 12 - Bank 15: Token 3 Broadcast to DRAM Chips Vector S Figure 5: DIMM-PIM architecure for LLM attention. the continuous data flow. Consequently, the double buffer creates a processing window where re-layout can occur. Spoofed timing constraints. To address potential DDR timing violations caused by re-layout processing, we implement a strategic timing constraint modification between the host MC and DIMM- PIM devices. Specifically, during the booting stage when the host MC reads Serial Presence Detect (SPD) on the DIMM, its provided recorded parameter values are intentionally spoofed to mislead the host MC. For example, for the timing parameter tWL, which is the latency between issuing a WR (write) command and starting to transfer burst data, the provided parameter value is smaller (e.g., one cycle) than the actual parameter value. Consequently, host MC will launch data transfer beforehand, while the data after re-layout is stored in DRAM banks with no introduced latency.\n\n--- Segment 19 ---\nFor example, for the timing parameter tWL, which is the latency between issuing a WR (write) command and starting to transfer burst data, the provided parameter value is smaller (e.g., one cycle) than the actual parameter value. Consequently, host MC will launch data transfer beforehand, while the data after re-layout is stored in DRAM banks with no introduced latency. Conversely, for the latency of subsequent commands, such as tWR for latency between writing completion and issuing a PRE command, the pro- vided value should be increased to prevent early issuing. 4.2 Attention Mapping and Computation The attention computation strategy consists of two complemen- tary components: KV mapping methods and PU design. The KV mapping methods distribute co-processed elements to satisfy PIM s locality and regularity requirements, while the cross-level PUs are specifically engineered to efficiently execute these distributed computations. Softmax computation optimizations are addressed separately in the subsequent section. Challenges. A fundamental challenge emerges from the dimen- sional asymmetry in attention operations: score and context per- form matrix-vector multiplications (GEMV) along orthogonal di- mensions of the KV cache, necessitating distinct mapping methods to resolve element-level mismatch. Simultaneously, the PUs must accommodate these divergent computational patterns without sac- rificing efficiency. For clarity in our analysis, we present mapping methods based on matrices and vectors after they have undergone bit-level re-layout and chip-level distribution, rather than their original representations. Score K Cache. As shown in Fig. 5-d, for each newly gener- ated vector K, whose elements are co-processed together, they are partitioned to different DRAM chips (vertical) and mapped to the same logic bank (horizontal). Within individual DRAM chips, co- processed K vector elements are stored contiguously in the same physical bank. To maximize computational throughput through all-bank parallelism, we distribute vectors from different tokens (K cache) across separate banks while maintaining consistent row and column indices. The V vector follows an analogous mapping strategy, ensuring elements with matching indices reside in the same DRAM chip, facilitating broadcasting across banks during computation. This mapping method enables efficient inner product for score computation, with both bank-level PUs and the rank PU s adder unit configured as adder trees.\n\n--- Segment 20 ---\nThe V vector follows an analogous mapping strategy, ensuring elements with matching indices reside in the same DRAM chip, facilitating broadcasting across banks during computation. This mapping method enables efficient inner product for score computation, with both bank-level PUs and the rank PU s adder unit configured as adder trees. Bank PUs perform multipli- cations using elements from their dedicated bank row buffers and the shared buffer, while the rank PU aggregates partial products from multiple DRAM chips to complete the computation. Context V Cache. As shown in Fig. 5-e, we distribute elements of each newly generated V vector, which operate independently, across multiple banks (four in our example) using burst transfer as the minimum granularity. This arrangement places co-processed elements in sequential burst transfers at identical offsets within their respective banks, exemplified by the first burst transfer (or- ange) of tokens 0 and 4. We also distribute tokens across different banks, enabling full exploitation of all-bank parallelism. The vector S is broadcast to all DRAM chips, where the element is shared in the same bank but may be different across banks (e.g., banks 0-3 and banks 4-7). Based on this mapping, we implement an outer product approach where both bank PUs and rank PUs function as accumulators. While the shared buffer broadcasts vector elements, multipliers in each bank PU utilize identical vector elements that may differ from those in other bank PUs. The rank PU performs a final aggregation of partial results from bank PUs within the same DRAM chip upon completion of context computation. 4.3 Kernel Fusion with Bubble-free Pipelining Considering the complexity of softmax, the softmax unit is typi- cally implemented by logic process on the buffer chip [70] (or logic die in HBM-PIM [72]), as shown in Fig.\n\n--- Segment 21 ---\nThe rank PU performs a final aggregation of partial results from bank PUs within the same DRAM chip upon completion of context computation. 4.3 Kernel Fusion with Bubble-free Pipelining Considering the complexity of softmax, the softmax unit is typi- cally implemented by logic process on the buffer chip [70] (or logic die in HBM-PIM [72]), as shown in Fig. 5-c. To mitigate poten- tial resource underutilization during computation, we introduce a pipelined execution framework that strategically partitions the DIMM-PIM hierarchy into three non-interfering domains: during 6 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA computation: (1) the host CPU, which monitors execution status and configures PIM transactions for the rank PU; (2) the rank PU, which manages data transfer with bank PU buffers; and (3) the bank PUs, which operate concurrently with their respective mem- ory banks. We leverage this partitioning method to ensure kernel fusion with pipelined execution. Chunk softmax. We implement chunk softmax [18] to enable pipelining with score computation rather than awaiting the comple- tion of score computation. During score computation, the rank PU simultaneously retrieves partial results from bank PU result buffers. We define a "chunk" as the collective partial outputs generated syn- chronously across all logic bank PUs. Chunks undergo independent softmax operations and are pipelined through the softmax unit, culminating in a final normalization once all chunks are processed. The normalization process is executed on a per-chunk basis to facil- itate further pipelining with context computation, wherein outputs are directly broadcast to DRAM chips and their respective bank PUs without intermediate bank storage. Quantitative analysis of the pipeline execution proves it is bubble-free. 4.4 Supplementary Details Dynamic refresh. We offload the management of DRAM refresh to the rank PU, which is responsible for the refresh of all DRAM chips in the rank. The need for refresh is checked with each head computation completion, while the rank PU can still receive fol- lowing transactions.\n\n--- Segment 22 ---\nWe offload the management of DRAM refresh to the rank PU, which is responsible for the refresh of all DRAM chips in the rank. The need for refresh is checked with each head computation completion, while the rank PU can still receive fol- lowing transactions. According to the protocol [1], the interval between REF commands can also support dynamically adjusting, with up to 8 REF commands can be delayed, which provides flex- ibility for the rank PU to manage the refresh when token length varies dynamically. Furthermore, since computation time is nearly linearly correlated with token length, the rank PU can dynamically determine the optimal timing for issuing the next REF command. Parameter settings. The number of arithmetic units (e.g., mul- tipliers, adders, exponential units) in bank PUs and rank PU is primarily determined by the bandwidth. For instance, since the bank row buffer provides 64-bit data for one RD (read) command, we implement four multipliers supporting two 16-bit inputs in each physical bank PU, while the shared buffer also broadcasts 64-bit data to all bank PUs in each computation phase. The size of on-chip buffer in the rank PU determines the number of tokens that can be processed in parallel. Following work [70], a 256KB buffer is lever- aged, which can support about a token size of 128K. For requests with larger lengths, repeated data fetching is required. 5 L3 System This section describes L3 system, a PIM-powered LLM inference system with hardware-software co-design to achieve scalability. The L3 system primarily addresses two major challenges: 1) mini- mizing the additional overhead caused by communication between the GPU and host memory, and 2) balancing the computation be- tween GPU and DIMM-PIM with scheduling techniques to reduce idle bubbles on both sides for high resource utilization. H0 H1 H3 H2 Channel 0 Channel 1 Channel N Req 0 H0 Layer 0 H0 Layer 1 Layer 0 Req 1 Rankset 0 Rankset 1 H0 H0 H0 H0 H0 H0 Layer 1 Req 2 Req 3 H0 H0 H0 H0 H0 H0 H0 H0 H0 Ch0 H1 Ch1 H2 Ch2 H3 Ch3 (a) Communication and computation with the rankset. (b) The data mapping on ranksets for load balancing.\n\n--- Segment 23 ---\nH0 H1 H3 H2 Channel 0 Channel 1 Channel N Req 0 H0 Layer 0 H0 Layer 1 Layer 0 Req 1 Rankset 0 Rankset 1 H0 H0 H0 H0 H0 H0 Layer 1 Req 2 Req 3 H0 H0 H0 H0 H0 H0 H0 H0 H0 Ch0 H1 Ch1 H2 Ch2 H3 Ch3 (a) Communication and computation with the rankset. (b) The data mapping on ranksets for load balancing. offloaded data PCIe CPU comm ... rank rank rank rank rank rank computation ... ... Rankset M Mapping Figure 6: Communication-computation overlapping with load balanced ranksets. 5.1 Optimizing Cross-device Communication L3 introduces dependency-aware communication-computation over- lap, a hardware-software co-design involving three techniques: 1) overlapping the communication and computation on the DIMM- PIM to hide the cross-device communication latencies, 2) achieving data layout load balancing among ranks, and 3) removing most data transfer out of the inference critical path. Concurrent communication and computation. Conventional scheduling treats all DIMM-PIM as a monolithic entity, alternat- ing between communication and computation modes. However, we observe that due to the shared memory buses between ranks, channel bandwidth does not scale with the number of ranks for communication. A single rank can nearly saturate the available channel bandwidth, rendering the simultaneous activation of all ranks for communication inefficient. L3 enables concurrent communication with computation on the DIMM-PIM side. The key insight is that, KV offloading does not require all ranks to operate in the communication mode concurrently. Therefore, we introduce a new hardware extension to use rankset as the basic unit for data transfer, with one rank on each channel forming a rankset, as shown in Fig. 6-a. During a single data transfer, only one rankset (e.g., rankset 0 in Fig. 6-a) is engaged to receive the data, while other ranksets can perform independent computations without being stuck. This hardware extension enables L3 to achieve parallel PIM computation and data transfer at the DIMM-PIM level. Taking the DGX-A100 with four ranks per channel as an example, this design preserves 75 of the DIMM-PIM s computational power during communication.\n\n--- Segment 24 ---\nThis hardware extension enables L3 to achieve parallel PIM computation and data transfer at the DIMM-PIM level. Taking the DGX-A100 with four ranks per channel as an example, this design preserves 75 of the DIMM-PIM s computational power during communication. Load balancing among ranksets. Randomly mapping the KV cache of requests to ranksets could cause load imbalance among ranks. The rankset receiving the most data could have longer data transfer and executing latency than other ranksets and thus slowing down the overall progress. To achieve load balance, we utilize the feature of LLM that for each request, the KV cache of different layers has the identical size. Therefore, we store the request s KV cache at the granularity of layers, as shown in Fig. 6-b. During the inference for a prefilling request, its KV cache for different layers is alternately offloaded to different ranksets. As a result, the data transfer time is eventually balanced across each rankset. A request s KV cache is also evenly distributed across ranksets.\n\n--- Segment 25 ---\nAs a result, the data transfer time is eventually balanced across each rankset. A request s KV cache is also evenly distributed across ranksets. KV cache of different heads in the same 7 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen S0 Gen GPU S0 MHA S1 Gen S1 MHA S0 Proj, FF S0 Gen S0 MHA S1 Proj, FF S1 Gen S1 MHA S0 Proj, FF S0 Gen Batch Prefill Decode Sub-batch 0 Sub-batch 1 Offload Onload DIMM-PIM QKV0 KV0 A0 S1 MHA S0 MHA QKV0 KV0 QKV1 KV1 A1 S0 MHA S1 MHA QKV0 KV0 Rankset 0 Rankset 1 S0 MHA S0 MHA S0 KV S1 MHA S1 MHA S1 KV S0 MHA S0 MHA S0 KV S1 MHA S1 MHA S1 KV Align Layer 0 Align Layer 1 Repeat (L - 1) Layers QKV0 S0 MHA A0 QKV0 S1 MHA QKV0 S0 MHA A0 QKV0 S1 MHA QKV1 S0 MHA A0 Figure 7: Computation graph in L3. Data transfer between devices can be overlapped with computation. Gen denotes QKV Generation and Proj, FF denotes the projection and feed-forward operations. layer is assigned to different channels within a rankset. In this man- ner, L3 ensures load balance at the rankset-level for computation and communication simultaneously. Remove dependency-independent communication from the critical path. According to data dependencies of the inference process, L3 removes most of the data communication from the in- ference critical path to further reduce the overhead. Specifically, two parts of data are needed to be transferred between devices through PCIe: Q, K and V values from the GPU to host memory after the QKV generation phase, and the computed attention re- sults of decoding requests from host memory to the GPU.\n\n--- Segment 26 ---\nAccording to data dependencies of the inference process, L3 removes most of the data communication from the in- ference critical path to further reduce the overhead. Specifically, two parts of data are needed to be transferred between devices through PCIe: Q, K and V values from the GPU to host memory after the QKV generation phase, and the computed attention re- sults of decoding requests from host memory to the GPU. Only the vector-sized Q, K and V generated for decoding requests are computational-dependent, which will be immediately used by the attention computation on DIMM-PIM in the same iteration. Simi- larly, the computed attention results of each decoding request are also of the vector size. The remaining part of offloading (K and V val- ues generated by prefilling requests) is transferred asynchronously, i.e., delayed to the projection or Feed-forward stages. In addition, the asynchronous communication will not affect the GPU side since the GPU is sufficient in memory bandwidth, as shown in Fig. 2. Ac- cording to our evaluation, the transfer of the prefilling KV cache can always be hidden by projection and Feed-forward (typically 16 of the Feed-forward latency). This is because the latency of the projection Feed-forward and the latency of prefilling KV cache data offloading are both related to the input size, and the former increases more significantly as the input size increases. 5.2 Optimizing Resource Utilization with L3 Scheduler The goal of L3 scheduler is to maximize parallel execution on both the GPU and DIMM-PIM and minimize potential bubbles. The key design is adaptive chunk-partitioned cross-device batch interleaving. First, to overcome the limitations imposed by the computational dependencies within a single batch, instead of parallelizing the prefilling and decoding MHA of the same batch, the L3 scheduler divides requests into two sub-batches, overlapping the prefilling of one sub-batch with the decoding of the other, as illustrated in Fig. 7. Moreover, we further design a novel scheduling policy to align the parallel parts of the two sub-batches to minimize idle bubbles on two devices. L3 achieves this with two key features: fine-grained tunability with adaptive chunk-partitioning and performance pre- dictability. Features of L3 scheduler.\n\n--- Segment 27 ---\nL3 achieves this with two key features: fine-grained tunability with adaptive chunk-partitioning and performance pre- dictability. Features of L3 scheduler. First, fine-grained tunability refers to L3 s ability to finely adjust the execution time on the GPU side, better aligning with the parallel execution on the DIMM-PIM side to mini- mize idle bubbles. In the long-context scenario, the FC operations of decoding-only batches may not overlap with the latency of decod- ing MHA (e.g., 16K tokens in Fig. 2), leading to idle bubbles on the GPU. At this point, L3 will add prefilling requests to the sub-batch. To avoid excessive execution latency on the GPU side after adding a prefilling request with long inputs, L3 dynamically chunks the overlong prefilling request. This enables fine-grained adjustment of the batch-execution phases on the GPU side, thereby enabling pre- cise calibration of execution latencies on the GPU to synchronize with those on the DIMM-PIM. Unlike original chunked-prefill design [7], L3 s scheduling policy ensures that only one request is chunked in a batch, and in most cases, this request will be chunked only once (details in 5.3). This further minimizes the additional overhead brought by chunking. The second feature is performance predictability. L3 s separa- tion of prefilling and decoding requests enables homogeneous and interference-free operations on both GPU and DIMM-PIM, thereby providing predictability of the computation time on both devices the factors that affect the batch performances (e.g., batch size, num- ber of finished tokens, the chunk size, etc) are known in advance. Therefore, the scheduler can predict the performances before the execution of a batch and make scheduling decisions accordingly. Predicting the performance of various workloads on GPUs as a reference for scheduling is a common practice [15, 17]. 5.3 Details of L3 Scheduler Problem formulation.\n\n--- Segment 28 ---\nPredicting the performance of various workloads on GPUs as a reference for scheduling is a common practice [15, 17]. 5.3 Details of L3 Scheduler Problem formulation. The following arguments can describe a sub-batch 𝑖: 𝑐𝑝𝑖, the list of chunk sizes of each prefilling request; 𝑓𝑝𝑖, the list of finished token numbers of each prefilling request; 𝑓𝑑𝑖, the list of finished token numbers of each decoding request. The goal of the scheduling is to prioritize the computation time of 8 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA decoding and add prefilling requests to hide the execution bubble. It requires modeling the following latencies of both sub-batches: 𝑇GPU0 𝑡𝑝(𝑐𝑝0, 𝑓𝑝0) 𝑡batch(𝑐𝑝0, 𝑓𝑑1) (1) 𝑇PIM0 𝑡𝑑(𝑓𝑑0) 𝑡comm(𝑓𝑑0) 𝑡overlap(𝑐𝑝1) (2) 𝑇GPU1 𝑡𝑝(𝑐𝑝1, 𝑓𝑝1) 𝑡batch(𝑐𝑝1, 𝑓𝑑0) (3) 𝑇PIM1 𝑡𝑑(𝑓𝑑1) 𝑡comm(𝑓𝑑1) 𝑡overlap(𝑐𝑝0) (4) Equation 1,3 denotes the latency on the GPU side, which contains the latency of prefilling MHA and batched operations (FC opera- tions). Equation 2,4 denotes the latency on the DIMM-PIM, which contains the latency of decoding MHA, QKV transferring for de- coding, and the overlapped transferring of prefilling requests KV. Scheduling algorithm.\n\n--- Segment 29 ---\nEquation 2,4 denotes the latency on the DIMM-PIM, which contains the latency of decoding MHA, QKV transferring for de- coding, and the overlapped transferring of prefilling requests KV. Scheduling algorithm. The scheduling algorithm constructs sub- batches for each iteration with the following steps: (1) As long as there is space in the host memory, decoding re- quests are fetched from the decoding queue (Fig. 4), which will be processed in this iteration. (2) These decoding requests are divided into two sub-batches, en- suring that the total number of tokens in the two sub-batches is as even as possible, thereby making the computation time for decoding MHA as even as possible. For example, four requests with 2k, 3k, 4k and 5k tokens can be divided to (2k, 5k) and (3k, 4k). (3) Prefilling requests are incrementally added to sub-batch 1 to gradually increase𝑇GPU1 until𝑇GPU1 𝑇PIM0, at which point the process stops. Similarly, prefilling requests are added to sub-batch 0 until 𝑇GPU0 𝑇PIM1. (4) The last prefilling request of each sub-batch is chunked to make 𝑇GPU as close as possible to the 𝑇PIM of the other sub- batch. The chunk size is a multiple of 16, better aligning with the requirements of GPU tiling. After this iteration ends, the chunked prefilling request is placed at the head of the prefilling queue. Runtime profiling and modeling. Now we describe how L3 mod- els the latencies of various inference operations. First, to model 𝑇PIM, since the latencies of 𝑡𝑑, 𝑡comm and 𝑡overlap are almost linearly related to the number of computed transferred tokens, we can use a linear model for prediction, which is simple and fast. Second, to model 𝑇GPU, we leverage Random Forest Regression (RFR) for its three advantages: capability of incremental learning, low latency, and high accuracy.\n\n--- Segment 30 ---\nFirst, to model 𝑇PIM, since the latencies of 𝑡𝑑, 𝑡comm and 𝑡overlap are almost linearly related to the number of computed transferred tokens, we can use a linear model for prediction, which is simple and fast. Second, to model 𝑇GPU, we leverage Random Forest Regression (RFR) for its three advantages: capability of incremental learning, low latency, and high accuracy. The models can be described with the following formula (take sub-batch 0 as an example): 𝑇PIM0 Linear( 𝑓𝑑0, len(𝑓𝑑0), 𝑐𝑝1) (5) 𝑇GPU0 RFR𝑝(𝑐𝑝0, 𝑓𝑝0) RFRbatch( 𝑐𝑝0, len(𝑓𝑑0)) (6) L3 uses a runtime profiling framework to collect the execution and communication latencies during the inference process. When a sufficient amount of data has been collected, L3 uses the col- lected data to train the model. Moreover, the dataset maintained by L3 dynamically evolves with the collected data, and the model is capable of being dynamically updated in response to the actual situation, thereby adapting to a complex and changing execution environment. Table 2: Simulator details. GPU Configuration CPU Configuration Processor 8 A100 (5 HBM2e) Memory 16 Channels 2 DIMM Capacity 640GB Capacity 2TB (Scalable) Bandwidth 16.3TB s (GPU) 260.8TB s (HBM-PIM) Bandwidth 406GB s (CPU) 13.0TB s (DIMM-PIM) DIMM DIMM-PIM: DDR4-3200 Hierarchy 2 Ranks 4 Bank Groups 4 Banks DRAM Timing BL 4:CCD 4:RRD 4 8:RCD 22:RAS 52:RP 22:RC 74: CL 22:WL 16:CDLR 4 12:WR 24:CCDL 8:RTP 12 Table 3: The evaluated LLM models. Model Layers Nl Heads Nh Embedding De Tensor Para. Data Para.\n\n--- Segment 31 ---\nModel Layers Nl Heads Nh Embedding De Tensor Para. Data Para. OPT-66B 64 72 9216 2 4 GPT-89B 48 96 12288 4 2 GPT-175B 96 96 12288 8 1 Precision: 𝑁𝑝𝑟𝑒 2 (16-bit floating point); Head Embedding: 𝐷ℎ 𝐷𝑒 𝑁ℎ 128. Table 4: Real-world traces. Trace Avg. Lin Std. Lin Avg. Lout Std. Lout OpenR1-Math-220k [5] 96.0 75.1 12684.1 8464.6 Dolphin-r1 [4] 201.9 563.0 3926.2 4216.0 OpenThoughts-114k-math [6] 89.4 66.7 6366.7 4662.9 LongBench [12] 7703.9 4285.5 89.8 213.7 6 Evaluation 6.1 Experimental Setup L3 implementation. L3 is built upon the DGX-A100 system [3]. On the GPU side, 8 NVIDIA A100 GPUs, each with 5 HBM2e of 80 GB capacity, are integrated, providing a total of 156 TFLOPs on FP16. The GPUs are connected via NVLink [82]. On the CPU side, there are 16 channels with 2 DIMMs of total 2TB capacity, which are further equipped with the proposed PIM capability. We develop a simulator based on DRAMSim3 [66] and AttAcc [72] to evaluate the performance of L3, including both GPU and DIMM- PIM components. The hardware specifications are listed in Table 2. For hardware validation, we implement the logic of the bank PU and the rank PU with SystemVerilog. The prototype is synthesized with Synopsys Design Compiler under the TSMC 28nm CMOS technology to estimate the area and power consumption. Baseline systems. We compare L3 with four baseline systems: 1) GPU-only, which executes LLM inference exclusively on GPUs. 2) GPU with HBM-PIM, which represents NeuPIMs [33] and At- tAcc [72]. We assume double buffers in each bank for concurrent GPU and HBM-PIM execution.\n\n--- Segment 32 ---\n2) GPU with HBM-PIM, which represents NeuPIMs [33] and At- tAcc [72]. We assume double buffers in each bank for concurrent GPU and HBM-PIM execution. It adopts prefilling-prioritized sched- uling, and utilizes sub-batches for decoding to reduce GPU-side idle bubbles. 3) GPU with rank-level DIMM-PIM (R-PIM), which represents Hermes [70]. It leverages prefilling-prioritized schedul- ing and single-batch methods. CPU-GPU communication (e.g., KV transfer) is hidden by concurrent computation. 4) GPU with CPU offloading without PIM acceleration, which represents NEO [40] and FastDecode [30]. We re-implement NEO s scheduling policy for CPU-GPU coordination. LLM models. We evaluate L3 on three LLM models with varying model sizes: OPT-66B, GPT-89B, and GPT-175B. Considering both the GPU memory capacity and inter-GPU communication overhead, 9 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen 0 1 2 3 GPT-175BGPT-89B OPT-66B \ \ OOM Normalized Throughput GPU HBM-PIM CPU R-PIM L3 (a) OpenR1. 0 1 3 5 GPT-175B GPT-89B OPT-66B (b) Dolphin. 0 1 2 3 GPT-175B GPT-89B OPT-66B \ \ OOM (c) OpenThoughts. 0 1 2 3 GPT-175B GPT-89B OPT-66B (d) LongBench. Figure 8: End-to-end inference throughput on real-world traces. The throughput of GPU-only baseline is normalized to 1. For the OPT-66B model, GPU out-of-memory (OOM) errors occurred in two traces, so the throughput of L3 is normalized to 1. we apply various methods of parallelism to these models. Details are listed in Table 3. Workloads.\n\n--- Segment 33 ---\nDetails are listed in Table 3. Workloads. We use four real-world LLM inference datasets: OpenR1- Math-220K [5], Dolphin-r1 [4], OpenThoughts-114k-math [6] and LongBench [12]. All of these datasets are used for evaluating the performance of LLM inference on long sequences. The details about the traces are shown in Table 4. 6.2 End-to-end Performance We evaluate the end-to-end throughput for each LLM model and workload using various baselines. For each evaluation, we randomly select and execute 1,000 requests from the traces. We measure the overall throughput by dividing the total number of output tokens by the total execution time. Throughput with real-world traces. Fig. 8 presents the normal- ized throughput of L3 compared with four baselines. The results show that L3 achieves the highest throughput over all baselines with various settings. Specifically, L3 achieves up to 6.1 higher throughput than the HBM-PIM baseline, 5.0 than the GPU-only baseline, and 9.2 higher than R-PIM baseline. These improve- ments are attributed to the following reasons. First, L3 achieves much larger batch sizes compared to the HBM-based baselines. For example, for GPT-175B, L3 has 2 TB of memory on the host memory for KV cache storage, while GPUs have only about 310 GB of memory in total after deploying the model. Second, L3 enables parallel execution of prefilling and decoding. In contrast, the HBM- PIM baseline suffers from PIM bubbles if prefilling requests are added to a batch. Third, the CPU offloading baseline suffers from a performance bottleneck due to the limited channel bandwidth. Finally, compared with R-PIM, L3 achieves fewer bubbles on both devices and offers about 8 bandwidth with bank-level PIM. We observe that HBM-PIM s sub-batch strategy demonstrates promising efficiency in bubble reduction, though our analysis re- veals certain limitations when processing Dolphin trace on OPT- 66B. It faces challenges in efficiently utilizing GPU resources when handling inherently small batch sizes, potentially leading to GPU- side execution bottlenecks that may result in performance be- low conventional GPU-only implementations.\n\n--- Segment 34 ---\nWe observe that HBM-PIM s sub-batch strategy demonstrates promising efficiency in bubble reduction, though our analysis re- veals certain limitations when processing Dolphin trace on OPT- 66B. It faces challenges in efficiently utilizing GPU resources when handling inherently small batch sizes, potentially leading to GPU- side execution bottlenecks that may result in performance be- low conventional GPU-only implementations. On the contrary, L3 achieves considerable merit with much larger and more balanced sub-batches, resulting in significant throughput improvements. 0 1 2 3 4 5 Bw-only Cap-only Bw-Cap Normalized Throughput 1 2 4 8 (a) GPT-175B. 0 1 2 3 4 5 6 Bw-only Cap-only Bw-Cap Normalized Throughput 1 2 4 8 (b) GPT-89B. Figure 9: Scalability analysis. The results are evaluated using the OpenR1 trace. Scalability analysis. We further evaluate L3 performance with various host memory configurations, showing that performance scalability requires simultaneous scaling in memory capacity and bandwidth. We first establish the base memory configuration as (512 GB, 2 ranksets). Based on this, we scale up the memory in different ways for different baselines: Bw-only indicates that we only scale up the rankset from 2 to 16, while Cap-only means we only scale up the capacity from 512GB to 4TB. Bw-Cap implies that we scale up both the rankset and capacity to (4TB, 16 ranksets). Similarly, we execute 1,000 requests from the trace and calculate the average throughput. We normalize the performance of the smallest memory configuration to 1. Fig. 9 shows the results of GPT-89B and GPT-175B with the OpenR1 trace. Results for other models and traces are similar. It shows that expanding either bandwidth or capacity alone does not effectively improve throughput. For example, when exclusively scaling memory capacity or bandwidth by 8 on the GPT-175B model, the throughput only increases by 1.6 and 1.1 , respectively. In contrast, when both bandwidth and capacity are enlarged, the throughput increases by 5.1 . This demonstrates that L3 effectively leverages the value of both scalability aspects. Ablation study.\n\n--- Segment 35 ---\nThis demonstrates that L3 effectively leverages the value of both scalability aspects. Ablation study. With L3 as the baseline, we remove optimizations for communication ( 5.1) and scheduling ( 5.2) and then succes- sively restore them. The results of end-to-end throughput with the GPT-175B model and the GPT-89B model on the OpenR1 trace are shown in Fig. 10. For example, with GPT-175B, after apply- ing adaptive chunk-partitioned cross-device batch interleaving, the throughput increases by 1.3 . Applying dependency-aware 10 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA 1 1.4 1.8 GPT-175B GPT-89B Normalized Throughput Naive Comm Comm Sched (a) Ablation study. 0.5 1 Pre-MHA Dec-MHA FC Error Rate ( ) (b) Prediction errors. Figure 10: Ablation study and prediction errors. The throughput with the most naive implementation in (a) is normalized to 1. 0 1 2 3 GPU RS-2 RS-4 RS-8 RS-16 GPU RS-2 RS-4 RS-8 RS-16 GPU RS-2 RS-4 RS-8 RS-16 GPU RS-2 RS-4 RS-8 RS-16 Normalized TBT MHA QKVGen Proj,FF Batch Size 64 Batch Size 48 Batch Size 32 Batch Size 16 Figure 11: Latency analysis. RS-N denotes to L3 with N ranksets. The GPU baseline is assumed infinite memory capacity. communication-computation overlap further improves the through- put by 1.7 . These results demonstrate that our system designs effectively enhance throughput in a heterogeneous architecture. 6.3 Latency Analysis Latency scalability. We evaluate L3 s time-between-tokens (TBT) compared with GPU-only inference. This evaluation is done using the GPT-89B model and different batch sizes of decoding requests, each with a token length of 6K. To ensure that the GPU baseline and L3 have the same batch size in the evaluation, we assume that the GPU baseline has infinite HBM capacity.\n\n--- Segment 36 ---\nThis evaluation is done using the GPT-89B model and different batch sizes of decoding requests, each with a token length of 6K. To ensure that the GPU baseline and L3 have the same batch size in the evaluation, we assume that the GPU baseline has infinite HBM capacity. The normalized TBT is shown in Fig.11, where the TBT of the GPU baseline with a batch size of 16 is normalized to 1. First, L3 s TBT is comparable to the GPU-only baseline even with the smallest number of ranksets. This demonstrates the effectiveness of using DIMM-PIM for decoding MHA and also shows that host-memory integration does not sacri- fice latency while improving throughput. Second, L3 s TBT can be further reduced by increasing the number of ranksets, indicating that we can further optimize performance with the expansion of host memory. When scaled up to 16 ranksets, L3 s TBT is only 29 53 of the GPU-only baseline. Performance predictability. We evaluate the performance models of L3 scheduler. We profile the performance data corresponding to 1,000 different batches, partitioning 80 of them into a training set to train our model ( 5.3). We subsequently utilize the remaining 20 of the data as a test set for validation. We use relative error to represent the prediction errors, i.e., RE 1 𝑛 Í𝑛 𝑖 1 𝑌𝑖 ˆ𝑌𝑖 𝑌𝑖 , where 𝑌𝑖 Table 5: Area and power overhead. Re-layout Unit Adder Unit Softmax Unit Bank PU Area (𝑢𝑚2) 948 3446 18506 3323 Power (𝑚𝑊) 0.245 1.812 4.345 1.129 Implementation with DRAM process would lead to 10 area overhead and ˆ𝑌𝑖represent the real latency and the predicted latency, respec- tively. Fig 10-b shows the results, representing the prediction model errors for prefilling MHA, decoding MHA, and FC, respectively. It exhibits the performance predictability of the operations, which facilitates effective balancing of parallel execution on the GPU and DIMM-PIM ( 5.2).\n\n--- Segment 37 ---\nFig 10-b shows the results, representing the prediction model errors for prefilling MHA, decoding MHA, and FC, respectively. It exhibits the performance predictability of the operations, which facilitates effective balancing of parallel execution on the GPU and DIMM-PIM ( 5.2). 6.4 Hardware Overhead Table 5 presents the results of each component of the rank PU and the bank PU. The number of arithmetic units is basically deter- mined by the DDR4 parameters. In the rank PU, the AU is composed with eight adders for FP16 precision, whose interconnections are configurable to support adder trees and self-accumulators. The SU performs softmax computation in a pipelined manner, with chunk size of sixteen FP16 elements (outputs from sixteen logic banks). The multipliers and dividers are also used for final normalization between chunks. The RU performs fine-grained re-layout of two burst data of 64 bits, which equals the bus width. Considering that the DIMM can have a buffer chip of 100𝑚𝑚2 and an entire power of 10W, we identify that the rank PU incurs negligible extra power and area consumption. On the other hand, the physical bank PU is integrated near the physical bank, designed with four multipli- ers and four adders with configurable interconections. Compared with PIM based on other memory devices, we identify that DIMM- PIM s distributed physical bank PUs are more area-beneficial and introduce less overhead in each memory chip. 7 Related Work Besides works discussed in 2.6, we present other related works in this section. KV cache pruning or quantization. Prior works leverage al- gorithmic optimizations to reduce the memory requirements of LLM inference. For example, some works [26, 63, 85, 86] leverage the sparsity of attention and prune the KV cache for long context. Other works [24, 54, 67, 83] reduce the KV cache size by quanti- zation. These methods could potentially affect the accuracy of the model. Moreover, we consider our approach orthogonal to these methods since they can naturally apply L3 for further enhancement. PIM-only architecture for LLM inference.\n\n--- Segment 38 ---\nMoreover, we consider our approach orthogonal to these methods since they can naturally apply L3 for further enhancement. PIM-only architecture for LLM inference. The PIM computing paradigm addresses the data movement bottleneck between mem- ory and processors by placing computation near or inside memory devices, which provides promising methods for memory-intensive kernels [8, 10, 13, 14, 16, 21, 23, 36, 37, 39, 42, 45, 47, 49, 57, 64, 68, 69, 71, 73, 79, 87, 89]. In addition to the GPU-PIM collaborative inference work listed in Table 1, there are also several works, e.g., CENT [28], LoL-PIM [53], and CXL-PNM [74], that have imple- mented LLM inference entirely on PIM, addressing the scalability in the granularity of the entire model. They fall short on their ker- nel decoupling methods. Though they provide a collective path or 11 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen matrix unit for FC kernels, their efficiency is still restricted by low data reuse and computing resources. 8 Discussions Economic advantages. While both HBM and PIM provide high bandwidth, PIM has significant economic advantages [62]. To pro- vide external high bandwidth for general purpose, HBM relies on expensive manufacturing processes involving TSVs [59] and sili- con interposers [35, 52], substantially increasing production costs. Additionally, DIMM-PIM offers greater deployment flexibility, as it can be incrementally adopted within existing server infrastruc- tures, allowing for gradual investment rather than complete system replacement. Utilizing spare GPU memory capacity. Since the entire KV cache is offloaded to the host memory, GPU memory is now utilized for storing model weights and intermediate data for prefilling requests, e.g., the activation, the partial KV cache of unfinished chunked- prefill requests, etc. Besides, in cases where there is spare GPU memory, decoding requests with their KV cache can be added to saturate the GPU.\n\n--- Segment 39 ---\nSince the entire KV cache is offloaded to the host memory, GPU memory is now utilized for storing model weights and intermediate data for prefilling requests, e.g., the activation, the partial KV cache of unfinished chunked- prefill requests, etc. Besides, in cases where there is spare GPU memory, decoding requests with their KV cache can be added to saturate the GPU. In most cases, the batch size of GPU decoding requests is much smaller than that of DIMM-PIM decoding requests, so they cause little interference to tasks on the GPU. Prior tech- niques [44] can be utilized for further optimizing the colocation of GPU decoding and prefilling requests. 9 Conclusion This paper proposes L3, a hardware-software co-designed system that provides scalable memory capacity and bandwidth for LLM inference. The primary innovation lies in utilizing DIMM-PIM to decouple and offload both KV cache storage and decoding MHA computations from GPU resources, through three key techniques: DIMM-PIM redesigns, communication optimizations, and adap- tive scheduling. Evaluations demonstrate L3 achieves significant speedup while enabling larger batch sizes, establishing a new para- digm for efficient long-context LLM deployment. References [1] 2015. DDR4 SDRAM LRDIMM. [Online]. Avalable: content dam micron global secure spectek data-sheet dram ddr4 spectek- 8gb-ddr4-sdram.pdf. [2] 2015. JEDEC Standard: DDR4 SDRAM Load Reduced DIMM (LRDIMM) Design Specification. [Online]. Avalable: docs module4_20_27. [3] 2023. NVIDIA DGX A100. [Online]. Avalable: dgx-systems dgx-ai. [4] 2025. cognitivecomputations dolphin-r1 Datasets at Hugging Face. https: huggingface.co datasets cognitivecomputations dolphin-r1. Referenced April 2025. [5] 2025. open-r1 OpenR1-Math-220k Datasets at Hugging Face. https: huggingface.co datasets open-r1 OpenR1-Math-220k. Referenced April 2025.\n\n--- Segment 40 ---\nhttps: huggingface.co datasets open-r1 OpenR1-Math-220k. Referenced April 2025. [6] 2025. open-r1 OpenThoughts-114k-math Datasets at Hugging Face. https: huggingface.co datasets open-r1 OpenThoughts-114k-math. Referenced April 2025. [7] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming throughput-latency tradeoff in LLM inference with sarathi-serve. In Proceedings of the 18th USENIX Conference on Operating Systems Design and Implementation (Santa Clara, CA, USA) (OSDI 24). USENIX Association, USA, Article 7, 18 pages. [8] Mohammad Alian, Seung Won Min, Hadi Asgharimoghaddam, Ashutosh Dhar, Dong Kai Wang, Thomas Roewer, Adam McPadden, Oliver O Halloran, Deming Chen, Jinjun Xiong, Daehoon Kim, Wen-mei Hwu, and Nam Sung Kim. 2018. Application-Transparent Near-Memory Processing Architecture with Memory Channel Network. In 2018 51st Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 802 814.\n\n--- Segment 41 ---\nIn 2018 51st Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 802 814. [9] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier- Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebas- tian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhong- tao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wi- eting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.\n\n--- Segment 42 ---\n802 814. [9] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier- Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebas- tian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhong- tao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wi- eting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023.\n\n--- Segment 43 ---\n[9] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier- Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebas- tian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhong- tao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wi- eting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. PaLM 2 Technical Report.\n\n--- Segment 44 ---\n2023. PaLM 2 Technical Report. arXiv:2305.10403 [cs.CL] [10] Bahar Asgari, Ramyad Hadidi, Jiashen Cao, Da Eun Shim, Sung-Kyu Lim, and Hyesoon Kim. 2021. FAFNIR: Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent Reduction. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 908 920. 1109 HPCA51647.2021.00080 [11] Hadi Asghari-Moghaddam, Young Hoon Son, Jung Ho Ahn, and Nam Sung Kim. 2016. Chameleon: versatile and practical near-DRAM acceleration architecture for large memory systems. In The 49th Annual IEEE ACM International Symposium on Microarchitecture (Taipei, Taiwan) (MICRO-49). IEEE Press, Article 50, 13 pages. [12] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding. arXiv:2308.14508 [cs.CL] [13] Amirali Boroumand, Saugata Ghose, Minesh Patel, Hasan Hassan, Brandon Lu- cia, Rachata Ausavarungnirun, Kevin Hsieh, Nastaran Hajinazar, Krishna T. Malladi, Hongzhong Zheng, and Onur Mutlu. 2019. CoNDA: efficient cache coherence support for near-data accelerators. In Proceedings of the 46th Inter- national Symposium on Computer Architecture (Phoenix, Arizona) (ISCA 19). Association for Computing Machinery, New York, NY, USA, 629 642. https: doi.org 10.1145 3307650.3322266 [14] Dan Chen, Haiheng He, Hai Jin, Long Zheng, Yu Huang, Xinyang Shen, and Xiaofei Liao. 2023. MetaNMP: Leveraging Cartesian-Like Product to Acceler- ate HGNNs with Near-Memory Processing.\n\n--- Segment 45 ---\n2023. MetaNMP: Leveraging Cartesian-Like Product to Acceler- ate HGNNs with Near-Memory Processing. In Proceedings of the 50th Annual International Symposium on Computer Architecture (Orlando, FL, USA) (ISCA 23). Association for Computing Machinery, New York, NY, USA, Article 56, 13 pages. [15] Quan Chen, Hailong Yang, Minyi Guo, Ram Srivatsa Kannan, Jason Mars, and Lingjia Tang. 2017. Prophet: Precise QoS Prediction on Non-Preemptive Acceler- ators to Improve Utilization in Warehouse-Scale Computers. SIGARCH Comput. Archit. News 45, 1 (April 2017), 17 32. [16] Benjamin Y. Cho, Yongkee Kwon, Sangkug Lym, and Mattan Erez. 2020. Near data acceleration with concurrent host access. In Proceedings of the ACM IEEE 47th Annual International Symposium on Computer Architecture (Virtual Event) (ISCA 20). IEEE Press, 818 831. [17] Weihao Cui, Han Zhao, Quan Chen, Ningxin Zheng, Jingwen Leng, Jieru Zhao, Zhuo Song, Tao Ma, Yong Yang, Chao Li, and Minyi Guo. 2021. Enable Simultane- ous DNN Services Based on Deterministic Operator Overlap and Precise Latency Prediction. In SC21: International Conference for High Performance Computing, Networking, Storage and Analysis. 1 15. [18] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. FLASHATTENTION: fast and memory-efficient exact attention with IO- awareness. In Proceedings of the 36th International Conference on Neural Informa- tion Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 1189, 16 pages.\n\n--- Segment 46 ---\nIn Proceedings of the 36th International Conference on Neural Informa- tion Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 1189, 16 pages. [19] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo 12 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z.\n\n--- Segment 47 ---\nCurran Associates Inc., Red Hook, NY, USA, Article 1189, 16 pages. [19] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo 12 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.\n\n--- Segment 48 ---\n[19] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo 12 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025.\n\n--- Segment 49 ---\nZ. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] [20] Fabrice Devaux. 2019. The true Processing In Memory accelerator. In 2019 IEEE Hot Chips 31 Symposium (HCS). 1 24. 8875680 [21] Alexandar Devic, Siddhartha Balakrishna Rai, Anand Sivasubramaniam, Ameen Akel, Sean Eilert, and Justin Eno. 2022. To PIM or not for emerging general purpose processing in DDR memory systems. In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 231 244. https: doi.org 10.1145 3470496.3527431 [22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL] [23] Siying Feng, Xin He, Kuan-Yu Chen, Liu Ke, Xuan Zhang, David Blaauw, Trevor Mudge, and Ronald Dreslinski. 2022. MeNDA: a near-memory multi-way merge solution for sparse transposition and dataflows. In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 245 258. https: doi.org 10.1145 3470496.3527432 [24] Elias Frantar and Dan Alistarh. 2023.\n\n--- Segment 50 ---\nAssociation for Computing Machinery, New York, NY, USA, 245 258. https: doi.org 10.1145 3470496.3527432 [24] Elias Frantar and Dan Alistarh. 2023. SparseGPT: massive language models can be accurately pruned in one-shot. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML 23). JMLR.org, Article 414, 15 pages. [25] Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. 2024. Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention. In 2024 USENIX Annual Technical Conference (USENIX ATC 24). USENIX Asso- ciation, Santa Clara, CA, 111 126. presentation gao-bin-cost [26] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. 2025. SeerAtten- tion: Learning Intrinsic Sparse Attention in Your LLMs.\n\n--- Segment 51 ---\n2025. SeerAtten- tion: Learning Intrinsic Sparse Attention in Your LLMs. arXiv:2410.13276 [cs.CL] [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That- tai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasu- den Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsim- poukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Va- sic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Ro- han Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold- schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ram- chandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowl- ing, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Cag- gioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Han- nah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Has- son, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navy- ata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Niko- lay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyag- ina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Ran- gaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, 13 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robin- son, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma.\n\n--- Segment 52 ---\nSeerAtten- tion: Learning Intrinsic Sparse Attention in Your LLMs. arXiv:2410.13276 [cs.CL] [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That- tai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasu- den Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsim- poukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Va- sic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Ro- han Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold- schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ram- chandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowl- ing, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Cag- gioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Han- nah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Has- son, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navy- ata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Niko- lay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyag- ina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Ran- gaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, 13 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robin- son, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024.\n\n--- Segment 53 ---\narXiv:2410.13276 [cs.CL] [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Ab- hishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind That- tai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasu- den Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsim- poukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Va- sic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Ro- han Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold- schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ram- chandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowl- ing, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Cag- gioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Han- nah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Has- son, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navy- ata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Niko- lay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyag- ina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Ran- gaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, 13 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robin- son, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The Llama 3 Herd of Models.\n\n--- Segment 54 ---\n2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] [28] Yufeng Gu, Alireza Khadem, Sumanth Umesh, Ning Liang, Xavier Servot, Onur Mutlu, Ravi Iyer, and Reetuparna Das. 2025. PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. ACM, 862 881. 1145 3676641.3716267 [29] Juan Gómez-Luna, Izzat El Hajj, Ivan Fernandez, Christina Giannoula, Geraldo F. Oliveira, and Onur Mutlu. 2022. Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing-in-Memory System. IEEE Access 10 (2022), 52565 52608. [30] Jiaao He and Jidong Zhai. 2024. FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines. arXiv:2403.11421 [cs.DC] https: arxiv.org abs 2403.11421 [31] Mingxuan He, Choungki Song, Ilkon Kim, Chunseok Jeong, Seho Kim, Il Park, Mithuna Thottethodi, and T. N. Vijaykumar. 2020. Newton: A DRAM-maker s Accelerator-in-Memory (AiM) Architecture for Machine Learning. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 372 385. [32] Yintao He, Haiyu Mao, Christina Giannoula, Mohammad Sadrosadati, Juan Gómez-Luna, Huawei Li, Xiaowei Li, Ying Wang, and Onur Mutlu. 2025. PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Lan- guages and Operating Systems, Volume 2 (Rotterdam, Netherlands) (ASPLOS 25).\n\n--- Segment 55 ---\nPAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Lan- guages and Operating Systems, Volume 2 (Rotterdam, Netherlands) (ASPLOS 25). Association for Computing Machinery, New York, NY, USA, 766 782. [33] Guseul Heo, Sangyeop Lee, Jaehong Cho, Hyunmin Choi, Sanghyeon Lee, Hyungkyu Ham, Gwangsun Kim, Divya Mahajan, and Jongse Park. 2024. Ne- uPIMs: NPU-PIM Heterogeneous Acceleration for Batched LLM Inferencing. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (AS- PLOS 24). Association for Computing Machinery, New York, NY, USA, 722 737. [34] Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards Reasoning in Large Language Models: A Survey. arXiv:2212.10403 [cs.CL] 2212.10403 [35] P. K. Huang, C. Y. Lu, W. H. Wei, Christine Chiu, K. C. Ting, Clark Hu, C.H. Tsai, S. Y. Hou, W. C. Chiou, C. T. Wang, and Douglas Yu. 2021. Wafer Level System Integration of the Fifth Generation CoWoS -S with High Performance Si Interposer at 2500 mm2. In 2021 IEEE 71st Electronic Components and Technology Conference (ECTC). 101 104. [36] Wenqin Huangfu, Xueqi Li, Shuangchen Li, Xing Hu, Peng Gu, and Yuan Xie. 2019. MEDAL: Scalable DIMM based Near Data Processing Accelerator for DNA Seeding Algorithm. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture (Columbus, OH, USA) (MICRO 52). Association for Computing Machinery, New York, NY, USA, 587 599.\n\n--- Segment 56 ---\nIn Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture (Columbus, OH, USA) (MICRO 52). Association for Computing Machinery, New York, NY, USA, 587 599. 1145 3352460.3358329 [37] Wenqin Huangfu, Krishna T. Malladi, Andrew Chang, and Yuan Xie. 2023. BEA- CON: Scalable Near-Data-Processing Accelerators for Genome Analysis near Memory Pool with the CXL Support. In Proceedings of the 55th Annual IEEE ACM International Symposium on Microarchitecture (Chicago, Illinois, USA) (MICRO 22). IEEE Press, 727 743. [38] Son Hyojun, Jonatan Gilbert, Xiangyu Wu, Cho Haeyoon, Shivdikar Kaustubh, Abellán José L., Joshi Ajay, Kaeli David, and Kim John. 2025. PIMnet: A Domain- Specific Network for Efficient Collective Communication in Scalable PIM. [39] Bongjoon Hyun, Taehun Kim, Dongjae Lee, and Minsoo Rhu. 2024. Pathfinding Future PIM Architectures by Demystifying a Commercial PIM Technology. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 263 279. [40] Xuanlin Jiang, Yang Zhou, Shiyi Cao, Ion Stoica, and Minlan Yu. 2024. NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM Inference. arXiv:2411.01142 [cs.DC] [41] Hongshin Jun, Jinhee Cho, Kangseol Lee, Ho-Young Son, Kwiwook Kim, Hanho Jin, and Keith Kim. 2017. HBM (High Bandwidth Memory) DRAM Technology and Architecture. In 2017 IEEE International Memory Workshop (IMW). 1 4. https: doi.org 10.1109 IMW.2017.7939084 [42] Hongju Kal, Chanyoung Yoo, and Won Woo Ro. 2023. AESPA: Asynchronous Execution Scheme to Exploit Bank-Level Parallelism of Processing-in-Memory.\n\n--- Segment 57 ---\n2023. AESPA: Asynchronous Execution Scheme to Exploit Bank-Level Parallelism of Processing-in-Memory. In Proceedings of the 56th Annual IEEE ACM International Symposium on Microar- chitecture (Toronto, ON, Canada) (MICRO 23). Association for Computing Ma- chinery, New York, NY, USA, 815 827. [43] Katikapalli Subramanyam Kalyan. 2023. A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4. arXiv:2310.12321 [cs.CL] [44] Aditya K. Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachan- dran Ramjee, and Ashish Panwar. 2025. POD-Attention: Unlocking Full Prefill- Decode Overlap for Faster LLM Inference. In Proceedings of the 30th ACM In- ternational Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (Rotterdam, Netherlands) (ASPLOS 25). As- sociation for Computing Machinery, New York, NY, USA, 897 912. https: doi.org 10.1145 3676641.3715996 [45] Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David Brooks, Vikas Chandra, Utku Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng Li, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail Smelyanskiy, Xiaodong Wang, Brandon Reagen, Carole-Jean Wu, Mark Hempstead, and Xuan Zhang. 2020. RecNMP: Accelerating Personalized Rec- ommendation with Near-Memory Processing. In 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA).\n\n--- Segment 58 ---\nRecNMP: Accelerating Personalized Rec- ommendation with Near-Memory Processing. In 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). 790 803. https: doi.org 10.1109 ISCA45697.2020.00070 [46] Liu Ke, Xuan Zhang, Jinin So, Jong-Geon Lee, Shin-Haeng Kang, Sukhan Lee, Songyi Han, YeonGon Cho, Jin Hyun Kim, Yongsuk Kwon, KyungSoo Kim, Jin Jung, Ilkwon Yun, Sung Joo Park, Hyunsun Park, Joonho Song, Jeonghyeon Cho, Kyomin Sohn, Nam Sung Kim, and Hsien-Hsin S. Lee. 2022. Near-Memory Processing in Action: Accelerating Personalized Recommendation With AxDIMM. IEEE Micro 42, 1 (2022), 116 127. [47] Liu Ke, Xuan Zhang, Jinin So, Jong-Geon Lee, Shin-Haeng Kang, Sukhan Lee, Songyi Han, YeonGon Cho, Jin Hyun Kim, Yongsuk Kwon, KyungSoo Kim, Jin Jung, Ilkwon Yun, Sung Joo Park, Hyunsun Park, Joonho Song, Jeonghyeon Cho, Kyomin Sohn, Nam Sung Kim, and Hsien-Hsin S. Lee. 2022. Near-Memory Processing in Action: Accelerating Personalized Recommendation With AxDIMM. IEEE Micro 42, 1 (2022), 116 127. [48] Guhyun Kim, Jinkwon Kim, Nahsung Kim, Woojae Shin, Jongsoon Won, Hyunha Joo, Haerang Choi, Byeongju An, Gyeongcheol Shin, Dayeon Yun, Jeongbin Kim, Changhyun Kim, Ilkon Kim, Jaehan Park, Yosub Song, Byeongsu Yang, Hyeongdeok Lee, Seungyeong Park, Wonjun Lee, Seonghun Kim, Yonghoon Park, Yousub Jung, Gi-Ho Park, and Euicheol Lim. 2024.\n\n--- Segment 59 ---\n[48] Guhyun Kim, Jinkwon Kim, Nahsung Kim, Woojae Shin, Jongsoon Won, Hyunha Joo, Haerang Choi, Byeongju An, Gyeongcheol Shin, Dayeon Yun, Jeongbin Kim, Changhyun Kim, Ilkon Kim, Jaehan Park, Yosub Song, Byeongsu Yang, Hyeongdeok Lee, Seungyeong Park, Wonjun Lee, Seonghun Kim, Yonghoon Park, Yousub Jung, Gi-Ho Park, and Euicheol Lim. 2024. SK Hynix AI-Specific Computing Memory Solution: From AiM Device to Heterogeneous AiMX-xPU System for Comprehensive LLM Inference. In 2024 IEEE Hot Chips 36 Symposium (HCS). 1 26. [49] Heesu Kim, Hanmin Park, Taehyun Kim, Kwanheum Cho, Eojin Lee, Soojung Ryu, Hyuk-Jae Lee, Kiyoung Choi, and Jinho Lee. 2021. GradPIM: A Practical Processing-in-DRAM Architecture for Gradient Descent. In 2021 IEEE Interna- tional Symposium on High-Performance Computer Architecture (HPCA). 249 262. [50] Jin Hyun Kim, Shin-haeng Kang, Sukhan Lee, Hyeonsu Kim, Woongjae Song, Yuhwan Ro, Seungwon Lee, David Wang, Hyunsung Shin, Bengseng Phuah, Jihyun Choi, Jinin So, YeonGon Cho, JoonHo Song, Jangseok Choi, Jeonghyeon Cho, Kyomin Sohn, Youngsoo Sohn, Kwangil Park, and Nam Sung Kim. 2021. Aquabolt-XL: Samsung HBM2-PIM with in-memory processing for ML ac- celerators and beyond. In 2021 IEEE Hot Chips 33 Symposium (HCS). 1 26.\n\n--- Segment 60 ---\nIn 2021 IEEE Hot Chips 33 Symposium (HCS). 1 26. [51] Jin Hyun Kim, Shin-haeng Kang, Sukhan Lee, Hyeonsu Kim, Woongjae Song, Yuhwan Ro, Seungwon Lee, David Wang, Hyunsung Shin, Bengseng Phuah, Jihyun Choi, Jinin So, YeonGon Cho, JoonHo Song, Jangseok Choi, Jeonghyeon Cho, Kyomin Sohn, Youngsoo Sohn, Kwangil Park, and Nam Sung Kim. 2021. Aquabolt-XL: Samsung HBM2-PIM with in-memory processing for ML ac- celerators and beyond. In 2021 IEEE Hot Chips 33 Symposium (HCS). 1 26. [52] Kwiwook Kim and Myeong-jae Park. 2024. Present and Future, Challenges of High Bandwith Memory (HBM). In 2024 IEEE International Memory Workshop (IMW). 1 4. [53] Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, and Jungwook Choi. 2025. LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System. arXiv:2412.20166 [cs.AR] [54] Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Bae- seong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, and Dongsoo Lee. 2022. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale 14 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA Pre-Trained Language Models.\n\n--- Segment 61 ---\n2022. AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale 14 L3: DIMM-PIM Integrated Architecture and Coordination for Scalable Long-Context LLM Inference Conference 17, July 2017, Washington, DC, USA Pre-Trained Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 3288 3305. [55] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles (Koblenz, Germany) (SOSP 23). Association for Computing Machinery, New York, NY, USA, 611 626. [56] Yongkee Kwon, Guhyun Kim, Nahsung Kim, Woojae Shin, Jongsoon Won, Hyunha Joo, Haerang Choi, Byeongju An, Gyeongcheol Shin, Dayeon Yun, Jeongbin Kim, Changhyun Kim, Ilkon Kim, Jaehan Park, Chanwook Park, Yosub Song, Byeongsu Yang, Hyeongdeok Lee, Seungyeong Park, Wonjun Lee, Seongju Lee, Kyuyoung Kim, Daehan Kwon, Chunseok Jeong, John Kim, Euicheol Lim, and Junhyun Chun. 2023. Memory-Centric Computing with SK Hynix s Domain-Specific Memory. In 2023 IEEE Hot Chips 35 Symposium (HCS). 1 26. [57] Youngeun Kwon, Yunjae Lee, and Minsoo Rhu. 2019. TensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture (Columbus, OH, USA) (MICRO 52).\n\n--- Segment 62 ---\nTensorDIMM: A Practical Near-Memory Processing Architecture for Embeddings and Tensor Operations in Deep Learning. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture (Columbus, OH, USA) (MICRO 52). Association for Computing Machinery, New York, NY, USA, 740 753. 1145 3352460.3358284 [58] Yongkee Kwon, Kornijcuk Vladimir, Nahsung Kim, Woojae Shin, Jongsoon Won, Minkyu Lee, Hyunha Joo, Haerang Choi, Guhyun Kim, Byeongju An, Jeong- bin Kim, Jaewook Lee, Ilkon Kim, Jaehan Park, Chanwook Park, Yosub Song, Byeongsu Yang, Hyungdeok Lee, Seho Kim, Daehan Kwon, Seongju Lee, Kyuy- oung Kim, Sanghoon Oh, Joonhong Park, Gimoon Hong, Dongyoon Ka, Kyudong Hwang, Jeongje Park, Kyeongpil Kang, Jungyeon Kim, Junyeol Jeon, Myeongjun Lee, Minyoung Shin, Minhwan Shin, Jaekyung Cha, Changson Jung, Kijoon Chang, Chunseok Jeong, Euicheol Lim, Il Park, Junhyun Chun, and Sk Hynix. 2022. System Architecture and Software Stack for GDDR6-AiM. In 2022 IEEE Hot Chips 34 Symposium (HCS). 1 25. [59] John H. Lau. 2022. Recent Advances and Trends in Multiple System and Heterogeneous Integration With TSV-Less Interposers. IEEE Transactions on Components, Packaging and Manufacturing Technology 12, 8 (2022), 1271 1281. [60] Dongjae Lee, Bongjoon Hyun, Taehun Kim, and Minsoo Rhu. 2024. PIM-MMU: A Memory Management Unit for Accelerating Data Transfers in Commercial PIM Systems. In 2024 57th IEEE ACM International Symposium on Microarchitecture (MICRO). 627 642.\n\n--- Segment 63 ---\nIn 2024 57th IEEE ACM International Symposium on Microarchitecture (MICRO). 627 642. [61] Donghun Lee, Jinin So, MINSEON AHN, Jong-Geon Lee, Jungmin Kim, Jeonghyeon Cho, Rebholz Oliver, Vishnu Charan Thummala, Ravi shankar JV, Sachin Suresh Upadhya, Mohammed Ibrahim Khan, and Jin Hyun Kim. 2022. Im- proving In-Memory Database Operations with Acceleration DIMM (AxDIMM). In Proceedings of the 18th International Workshop on Data Management on New Hard- ware (Philadelphia, PA, USA) (DaMoN 22). Association for Computing Machinery, New York, NY, USA, Article 2, 9 pages. [62] Hyungdeok Lee, Guhyun Kim, Dayeon Yun, Ilkon Kim, Yongkee Kwon, and Euicheol Lim. 2024. Cost-Effective LLM Accelerator Using Processing in Memory Technology. In 2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Tech- nology and Circuits). 1 2. 2024.10631397 [63] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. 2024. InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). USENIX Association, Santa Clara, CA, 155 172. [64] Cong Li, Zhe Zhou, Size Zheng, Jiaxi Zhang, Yun Liang, and Guangyu Sun. 2024. SpecPIM: Accelerating Speculative Inference on PIM-Enabled System via Architecture-Dataflow Co-Exploration. In Proceedings of the 29th ACM Inter- national Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 950 965.\n\n--- Segment 64 ---\nIn Proceedings of the 29th ACM Inter- national Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 950 965. 3620666.3651352 [65] Jinhao Li, Jiaming Xu, Shan Huang, Yonghua Chen, Wen Li, Jun Liu, Yaoxiu Lian, Jiayi Pan, Li Ding, Hao Zhou, Yu Wang, and Guohao Dai. 2025. Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective. arXiv:2410.04466 [cs.AR] [66] Shang Li, Zhiyuan Yang, Dhiraj Reddy, Ankur Srivastava, and Bruce Jacob. 2020. DRAMsim3: A Cycle-Accurate, Thermal-Capable DRAM Simulator. IEEE Comput. Archit. Lett. 19, 2 (July 2020), 106 109. [67] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Guangxuan Xiao, and Song Han. 2025. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. GetMobile: Mobile Comp. and Comm. 28, 4 (Jan. 2025), 12 17. [68] Haifeng Liu, Long Zheng, Yu Huang, Chaoqiang Liu, Xiangyu Ye, Jingrui Yuan, Xiaofei Liao, Hai Jin, and Jingling Xue. 2023. Accelerating Personalized Recom- mendation with Cross-level Near-Memory Processing. In Proceedings of the 50th Annual International Symposium on Computer Architecture (Orlando, FL, USA) (ISCA 23). Association for Computing Machinery, New York, NY, USA, Article 66, 13 pages. [69] Liu Liu, Jilan Lin, Zheng Qu, Yufei Ding, and Yuan Xie. 2021. ENMC: Extreme Near-Memory Classification via Approximate Screening. In MICRO-54: 54th Annual IEEE ACM International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO 21). Association for Computing Machinery, New York, NY, USA, 1309 1322.\n\n--- Segment 65 ---\nIn MICRO-54: 54th Annual IEEE ACM International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO 21). Association for Computing Machinery, New York, NY, USA, 1309 1322. [70] Lian Liu, Shixin Zhao, Bing Li, Haimeng Ren, Zhaohui Xu, Mengdi Wang, Xiaowei Li, Yinhe Han, and Ying Wang. 2025. Make LLM Inference Affordable to Everyone: Augmenting GPU Memory with NDP-DIMM. [71] Anirban Nag and Rajeev Balasubramonian. 2021. OrderLight: Lightweight Memory-Ordering Primitive for Efficient Fine-Grained PIM Computations. In MICRO-54: 54th Annual IEEE ACM International Symposium on Microarchitecture (Virtual Event, Greece) (MICRO 21). Association for Computing Machinery, New York, NY, USA, 298 310. [72] Jaehyun Park, Jaewan Choi, Kwanhee Kyung, Michael Jaemin Kim, Yongsuk Kwon, Nam Sung Kim, and Jung Ho Ahn. 2024. AttAcc! Unleashing the Power of PIM for Batched Transformer-based Generative Model Inference. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 103 119. https: doi.org 10.1145 3620665.3640422 [73] Jaehyun Park, Byeongho Kim, Sungmin Yun, Eojin Lee, Minsoo Rhu, and Jung Ho Ahn. 2021. TRiM: Enhancing Processor-Memory Interfaces with Scal- able Tensor Reduction in Memory. In MICRO-54: 54th Annual IEEE ACM In- ternational Symposium on Microarchitecture (Virtual Event, Greece) (MICRO 21). Association for Computing Machinery, New York, NY, USA, 268 281.\n\n--- Segment 66 ---\nIn MICRO-54: 54th Annual IEEE ACM In- ternational Symposium on Microarchitecture (Virtual Event, Greece) (MICRO 21). Association for Computing Machinery, New York, NY, USA, 268 281. [74] Sang-Soo Park, KyungSoo Kim, Jinin So, Jin Jung, Jonggeon Lee, Kyoungwan Woo, Nayeon Kim, Younghyun Lee, Hyungyo Kim, Yongsuk Kwon, Jinhyun Kim, Jieun Lee, YeonGon Cho, Yongmin Tai, Jeonghyeon Cho, Hoyoung Song, Jung Ho Ahn, and Nam Sung Kim. 2024. An LPDDR-based CXL-PNM Platform for TCO- efficient Inference of Transformer-based Large Language Models. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 970 982. [75] Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. 2024. Reasoning with Large Language Models, a Survey. arXiv:2407.11511 [cs.AI] [76] Minseok Seo, Xuan Truong Nguyen, Seok Joong Hwang, Yongkee Kwon, Guhyun Kim, Chanwook Park, Ilkon Kim, Jaehan Park, Jeongbin Kim, Woojae Shin, Jongsoon Won, Haerang Choi, Kyuyoung Kim, Daehan Kwon, Chunseok Jeong, Sangheon Lee, Yongseok Choi, Wooseok Byun, Seungcheol Baek, Hyuk-Jae Lee, and John Kim. 2024. IANUS: Integrated Accelerator based on NPU-PIM Unified Memory System. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 545 560.\n\n--- Segment 67 ---\nIn Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (La Jolla, CA, USA) (ASPLOS 24). Association for Computing Machinery, New York, NY, USA, 545 560. [77] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gon- zalez, and Ion Stoica. 2023. S-LoRA: Serving Thousands of Concurrent LoRA Adapters. arXiv preprint arXiv:2311.03285 (2023). [78] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. FlexGen: high-throughput generative inference of large language models with a single GPU. In Proceedings of the 40th International Conference on Machine Learning (Honolulu, Hawaii, USA) (ICML 23). JMLR.org, Article 1288, 23 pages. [79] Weiyi Sun, Zhaoshi Li, Shouyi Yin, Shaojun Wei, and Leibo Liu. 2021. ABC-DIMM: alleviating the bottleneck of communication in DIMM-based near-memory pro- cessing with inter-DIMM broadcast. In Proceedings of the 48th Annual International Symposium on Computer Architecture (Virtual Event, Spain) (ISCA 21). IEEE Press, 237 250. [80] Boyu Tian, Yiwei Li, Li Jiang, Shuangyu Cai, and Mingyu Gao. 2024. NDP- Bridge: Enabling Cross-Bank Coordination in Near-DRAM-Bank Processing Ar- chitectures. In 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA). 628 643. [81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.\n\n--- Segment 68 ---\n2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS 17). Curran Associates Inc., Red Hook, NY, USA, 6000 6010. [82] Ying Wei, Yi Chieh Huang, Haiming Tang, Nithya Sankaran, Ish Chadha, Dai Dai, Olakanmi Oluwole, Vishnu Balan, and Edward Lee. 2023. 9.3 NVLink-C2C: A Coherent Off Package Chip-to-Chip Interconnect with 40Gbps pin Single- ended Signaling. In 2023 IEEE International Solid-State Circuits Conference (ISSCC). 15 Conference 17, July 2017, Washington, DC, USA Qingyuan Liu1, Liyan Chen1, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen 160 162. [83] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022. ZeroQuant: efficient and affordable post-training quantization for large-scale transformers. In Proceedings of the 36th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 1970, 16 pages. [84] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung- Gon Chun. 2022. Orca: A Distributed Serving System for Transformer-Based Generative Models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). USENIX Association, Carlsbad, CA, 521 538. https: www.usenix.org conference osdi22 presentation yu [85] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025.\n\n--- Segment 69 ---\nUSENIX Association, Carlsbad, CA, 521 538. https: www.usenix.org conference osdi22 presentation yu [85] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. 2025. Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention. arXiv:2502.11089 [cs.CL] [86] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. 2023. H2O: heavy-hitter oracle for efficient generative inference of large language models. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 1506, 50 pages. [87] Yilong Zhao, Mingyu Gao, Fangxin Liu, Yiwei Hu, Zongwu Wang, Han Lin, Ji Li, He Xian, Hanlin Dong, Tao Yang, Naifeng Jing, Xiaoyao Liang, and Li Jiang. 2024. UM-PIM: DRAM-based PIM with Uniform Shared Memory Space. In 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA). 644 659.\n\n--- Segment 70 ---\nIn 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA). 644 659. [88] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yihen Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Lichao Sun, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Ninghao Liu, Bei Jiang, Linglong Kong, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, and Tianming Liu. 2024. Evaluation of OpenAI o1: Opportunities and Challenges of AGI. arXiv:2409.18486 [cs.CL] [89] Zhe Zhou, Cong Li, Xuechao Wei, Xiaoyang Wang, and Guangyu Sun. 2023. GNNear: Accelerating Full-Batch Training of Graph Neural Networks with near-Memory Processing. In Proceedings of the International Conference on Par- allel Architectures and Compilation Techniques (Chicago, Illinois) (PACT 22).\n\n--- Segment 71 ---\nGNNear: Accelerating Full-Batch Training of Graph Neural Networks with near-Memory Processing. In Proceedings of the International Conference on Par- allel Architectures and Compilation Techniques (Chicago, Illinois) (PACT 22). Association for Computing Machinery, New York, NY, USA, 54 68. https: doi.org 10.1145 3559009.3569670 [90] Zhe Zhou, Cong Li, Fan Yang, and Guangyu Sun. 2023. DIMM-Link: Enabling Efficient Inter-DIMM Communication for Near-Memory Processing. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 302 316. [91] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao- Ping Zhang, Yuhan Dong, and Yu Wang. 2024. A Survey on Efficient Inference for Large Language Models. arXiv:2404.14294 [cs.CL] 14294 16\n\n