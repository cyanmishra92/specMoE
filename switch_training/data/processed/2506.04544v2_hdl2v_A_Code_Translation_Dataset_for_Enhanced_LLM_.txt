=== ORIGINAL PDF: 2506.04544v2_hdl2v_A_Code_Translation_Dataset_for_Enhanced_LLM_.pdf ===\n\nRaw text length: 35908 characters\nCleaned text length: 34553 characters\nNumber of segments: 23\n\n=== CLEANED TEXT ===\n\nhdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation Charles Hong , Brendan Roberts, Huijae An, Alex Um, Advay Ratan, Yakun Sophia Shao UC Berkeley Abstract Large language models (LLMs) are playing an increasingly large role in domains such as code generation, including hardware code generation, where Verilog is the key language. However, the amount of publicly available Verilog code pales in comparison to the amount of code available for software languages like Python. In this work, we present hdl2v ( HDL- to-Verilog ), a dataset which seeks to increase the amount of available human-written Verilog data by translating or compiling three other hardware description languages VHDL, Chisel, and PyMTL3 to Verilog. Furthermore, we demonstrate the value of hdl2v in enhancing LLM Verilog generation by improving performance of a 32 billion-parameter open-weight model by up to 23 in VerilogEvalV2, without utilizing any data augmentation or knowledge distillation from larger models. We also show hdl2v s ability to boost the performance of a data augmentation-based fine-tuning approach by 63 . Finally, we characterize and analyze our dataset to better understand which characteristics of HDL-to-Verilog datasets can be expanded upon in future work for even better performance. I. INTRODUCTION Large language models (LLMs) have demonstrated impres- sive performance in a wide range of tasks, ranging from general reasoning ability and instruction-following [6], [25] to code generation [9], [18], [24], [27]. LLMs have potential to automate a wide range of tasks in hardware design, ranging from design to verification to optimization [5], [8], [14], [15]. A number of studies have attempted to evaluate and improve LLMs potential in generating Verilog code [13], [31], [36]. Compared to popular software programming languages such as Python or C, there is not as much publicly available Verilog. In fact, as of April 2025 and according to RepositoryStats, there are 132,264 popular GitHub repositories with Python as the primary language, compared to just 848 for Verilog or SystemVerilog [1]. As a result, a number of prior works have attempted to fine-tune LLMs on novel Verilog datasets, and successfully improved Verilog generation performance. These works utilize techniques such as data augmentation [7], [10], [28], [33], [35] and synthetic Verilog generation [21]. However, Verilog is not the only hardware description language (HDL). While Verilog is often used as the common interface between hardware code and tools such as RTL simulators or synthesis software, design can be done in higher- level languages such as Chisel. VHDL is another popular HDL with its own ecosystem of supported hardware. Nonetheless, generating Verilog with LLMs is still an important task as it is the most commonly supported and written. In this work, we investigate how the wealth of HDL code in languages other than Verilog can be used to improve LLMs ability to generate Verilog. Specifically, we present hdl2v, a dataset consisting of 46,549 pairs of VHDL, Chisel, and PyMTL3 translated compiled to Verilog. We use this data in supervised fine-tuning of LLMs. Our findings from these experiments are as follows: Fine-tuning with this data yields significant improvements in Verilog generation performance. We find that Verilo- gEvalV2 performance of a state-of-the-art open-weight LLM improves by up to 13 for and 23 for upon being fine-tuned on a combination of our datasets. hdl2v works in tandem with other fine-tuning approaches. We demonstrate that by adding data from hdl2v to existing Verilog training data, we boost performance of a data augmentation approach by 63 . Language matters; fine-tuning with VHDL-Verilog pairs yields better results than C-Verilog pairs, when the Ver- ilog is held constant. Fine-tuned models learn from the code in prompt- response pairs, not just natural language. However, utiliz- ing meaningful module and variable names is important in helping LLMs learn from this data. hdl2v is fully open-source and available for others to expand on this research. 1 2 3 II. BACKGROUND A. Verilog Code Generation Prior work has seeked to improve LLMs ability to generate correct Verilog code, using techniques such as fine tuning on textbooks and Verilog from Github [31]. Other prior works augment existing Verilog datasets [7], [28], [33], [35] or generate novel synthetic Verilog [21]. Multi-agent systems utilize feedback from RTL simulation tools and modify test code in order to debug generated code [13], [36]. Benchmarks such as VerilogEval [20] and RTLLM [22] have been de- veloped to standardize evaluation of LLM Verilog generation performance. This work does not seek to supercede such prior work. Instead, we provide new data that can complement other 1VHDL dataset: 2Chisel dataset: 3PyMTL3 dataset: 1 arXiv:2506.04544v2 [cs.AR] 8 Jul 2025 Verilog V2C VHDL Verilog vhd2vl Prompt: module logic_1 (output A); Response: module logic_1 (output A); assign A 1; endmodule Prompt: void logic_1 (bool A) { A 1; } Response: module logic_1 (output A); assign A 1; endmodule C Dataset Prompt: entity logic_0 is port ( A : out std_logic ); end logic_0; architecture Behavioral of logic_0 is begin A '0'; end Behavioral; Response: module logic_0 (output A); assign A 0; endmodule VHDL Dataset Verilog Completion Dataset Fig. 1. How the Verilog Completion, C, and VHDL datasets are collected. Note that Verilog is translated to C and VHDL is translated to Verilog, but during fine-tuning Verilog is always used as the response. approaches, such as data augmentation and agentic systems, to further improve LLM Verilog generation. We show a concrete example of this synergy in Section V-C. B. Hardware Description Languages (HDLs) Verilog is a key language in hardware design due to its use as a common representation for representing register-transfer level (RTL) designs. A wide range of design automation tools, such as logic synthesis software and FPGA compilers, use Verilog as a common input to represent digital hardware. However, while digital hardware design is commonly done in Verilog (and its more feature-rich descendant, SystemVerilog), many other hardware description languages (HDLs) exist and have been used to tape out real-world chips. For example, VHDL is an alternative to Verilog that fills largely the same role. On the other hand, high-level HDLs like Chisel [3], Spinal- HDL [26], MyHDL [11], PyMTL3 [19], and many others seek to provide user-friendly features such as type-checking and greater parameterizability. Often these languages are em- bedded in high-level software languages, such as Scala in the case of Chisel and SpinalHDL, and Python in the case of MyHDL and PyMTL3. Nonetheless, designs written in these languages still compile to Verilog so that they can be used with industry standard design automation tools. As a result, the Chisel or PyMTL3 code is often a higher-level (and as a result, closer to natural language) representation of the Verilog it compiles to. We use this fact to our advantage in hdl2v specifically, we show that fine-tuning LLMs with pairs of Chisel PyMTL3 code and their corresponding compiled Verilog can successfully improve LLMs ability to generate Verilog from a natural language spec. We select VHDL for this work as it is the most popular HDL outside of Verilog and SytemVerilog. Additionally, we include Chisel and PyMTL3 as they are two of the most popular high- level HDLs, embedded in two different high-level software languages, with a large amount of diverse and high-quality code written in each language. Of course, future work could extend our approach to include other HDLs. C. Verilog Translation for LLM Fine-Tuning BetterV [28] introduces the idea of translating Verilog to C in order to improve correctness of generated Verilog. However, we demonstrate in Section V-A that when training with a Verilog dataset and the corresponding data translated to C, the benefit from training with C is minimal compared to training with only Verilog. Furthermore, the Verilog in this dataset is also likely to be present in LLMs pre-training data, as it originates from open-source Github repositories. To our knowledge, this work is the first to translate other HDLs to Verilog to generate novel Verilog data for LLM fine- tuning. III. DATASET CONSTRUCTION A. Datasets From Prior Work As a baseline, we fine-tune with datasets from prior work that are based on existing Verilog from public sources. 1) Verilog Completion: As in BetterV [28], this dataset consists of a filtered set of Verilog modules from public sources. In this case, the prompt for fine-tuning is the header of the Verilog module, and the response is the rest of the module. Figure 1 shows an example of what an entry in this dataset might look like. This dataset contains 147,138 entries with total size 84.4 MB. 2) C: As in BetterV [28] and as depicted in Figure 1, we use v2c [23] to translate the above Verilog to C. In this case, the prompt for fine-tuning is actually the translated C code, and the original Verilog is the response. The intent is to improve the model s understanding of the Verilog code by attempting to correlate it with C code, which has greater presence in pretraining data. As not all Verilog modules in our dataset can 2 CONFIG "SmallBoomV3Conﬁg" ALU .scala Chisel Dataset , , , Chisel Verilog Collect Compile DCache .scala HellaCache .scala TLB .scala ALU .v DCache .v TLB .v TLB .scala TLB .scala Fig. 2. How the Chisel dataset is collected. One pair is collected for each generated Verilog module. Note that in cases where multiple Verilog modules are generated from the same Chisel file, that Chisel file will be included in multiple pairs in the dataset. be successfully translated to C, this results in 26,803 entries with size 36.5 MB. B. hdl2v Datasets Each hdl2v dataset is fully open-source and available on HuggingFace, as described in Section I. 1) VHDL: As shown in Figure 1, we use Google BigQuery to collect VHDL files from Github. Specifically, we collect every file with a .vhd or .vhdl extension. This results in 53,698 VHDL entities. We attempt to translate each VHDL entity to Verilog using the open-source tool vhd2vl [12]. vhd2vl is able to successfully translate 8974 entities to Verilog, and we filter out entries that do not contain the strings module and later endmodule. The remaining 8626 entries have a total size of 48.4 MB. Each prompt is a VHDL entity, and its response is the corresponding translated Verilog module. 2) Chisel: Chisel is a high-level HDL embedded in Scala that can be compiled into Verilog or SystemVerilog. Therefore, it intrinsically provides matching pairs between itself and Verilog. To gather this data, we used Chipyard [2], which contains a variety of generators that can be combined to create a wide range of SoC configurations. We compile a large number of Chipyard SoC configurations to Verilog, aiming to collect Verilog files generated from as many Chisel source files as possible in the Chipyard repository. Our dataset includes 55 of the .scala files in Chipyard and its subrepositories; of those not covered, most do not contain synthesizable Chisel. The generated Verilog contains annotations that indicate which Chisel file and line each Verilog line is generated from, allowing us to collect a set of relevant Chisel files for each generated Verilog file. Each Verilog file contains one module. We show an example of how Chisel-Verilog pairs are collected for one SoC configuration in Figure 2. Duplicates PyMTL3 VerilogTranslation Pass Verilog PyHDL-Eval Prompt: class AlwaysOne( Component ): def construct( s ): s.o OutPort() update def logic_block(): s.o b1(1) Response: module always_one (output o); assign o 1; endmodule PyMTL3 Dataset Fig. 3. How the PyMTL3 dataset is collected. are removed, but for cases where the same Chisel file with different parameters generates differing Verilog output, all the data is kept. This results in 18,939 Chisel Verilog pairs (with a total size of 1.69 GB). We additionally collect the corresponding FIRRTL. We do not use it for any further experiments in this work, but include it in our dataset for future users. In our prompt response pairs for LLM fine-tuning, the response consists of one generated Verilog file, which contain a single Verilog module. The prompt contains one or multiple Chisel source files (including one primary class and all of its dependencies), and a request to translate the code into Verilog. While the Chisel source code provided in the prompt does not directly compile to the response Verilog, we seek to improve the LLM s ability to correlate high-level HDL code in the Scala-embedded Chisel, which may contain more information about design semantics, to lower-level compiled Verilog. 3) PyMTL3: PyHDL-Eval [4] evaluates the ability of LLMs to correctly generate the Python-embedded HDLs PyMTL3, PyRTL, MyHDL, Migen, and Amaranth. The authors provide as an artifact the PyHDL code generated by LLMs during these experiments. As shown in Figure 3, we use LLM-generated PyMTL3 code from PyHDL-Eval to construct a dataset similar to the Chisel dataset above. Like Chisel, PyMTL3 is a high-level HDL which can be compiled to Verilog. We compile each PyMTL3 example from PyHDL-Eval s artifact, numbering about 50,000, to Verilog using PyMTL3 s VerilogTranslation- Pass. 18,636 examples (with a total size of 28 MB) compile to Verilog successfully, as many of the PyMTL3 examples in PyHDL-Eval contain syntax errors. In this dataset, each prompt is one PyMTL3 class, and each response is the corresponding translated Verilog module. IV. EXPERIMENT SETUP A. LLM Fine-Tuning Setup We use Qwen2.5-Coder-32B-Instruct [17] as our base model. To train the model, we use DeepSpeed-Chat s Super- vised Fine-Tuning pipeline [34] and enable ZeRO Stage 3 [30] and LoRA [16] for efficient training. We maintain consistent hyperparameter settings across all experiments, including the 3 use of FusedAdam optimizer, cosine learning decay, a learning rate of 1e-5, a single training epoch, and a batch size of 8. All experiments are conducted on a server with four NVIDIA L40S GPUs. A summary of the training hyperparameters is provided in the following table: Hyperparameter Value ZeRO Stage 3 LoRA Dimension 32 Data Type bfloat16 Batch Size 8 Learning Rate 1e-5 Number of Epochs 1 TABLE I HYPERPARAMETERS USED FOR FINE-TUNING B. Evaluation Setup VerilogEvalV2 [29] is a benchmark that consists of 156 Verilog design problems, intended to test LLMs abil- ity to generate functionally correct Verilog according to a (mostly) natural language specification. We use Verilo- gEvalV2 s spec-to-rtl benchmark to evaluate our model, with the following settings: Parameter Value Samples 20 Temperature 0.85 top p 0.95 ICL examples 0 ICL rules no TABLE II PARAMETERS USED FOR EVALUATION WITH VERILOGEVALV2 We use both and to evaluate model performance. effectively measures the total percentage of functionally correct responses, whereas estimates the model s ability to generate at least one correct response when multiple samples (in this case 10) are taken. V. FINE-TUNING EXPERIMENTS A. Fine-Tuning with Individual Datasets As shown in Figure 4, our datasets have varying effective- ness in improving our models performance in VerilogEvalV2. Of the five datasets tested, PyMTL3 and VHDL perform the best, both providing about 18 increase in over the base Qwen2.5-Coder-32B-Instruct model. As we will discuss further in Section VI, the VHDL dataset has the highest perplexity in the dataset and provides a diverse dataset that has not been seen (as Verilog) during pretraining. On the other hand, while the PyMTL3 dataset is relatively less diverse, the set of designs it targets is highly relevant to VerilogEval, as PyHDL-Eval also generated code for a benchmark set of designs similar to VerilogEval. The entries in the PyMTL3 dataset also tend to be shorter than the entries in the other two hdl2v datasets (see Table III), making it easier for the model to learn relationships between the high-level PyMTL3 and the translated Verilog. Base Model Verilog Completion C VHDL Chisel PyMTL3 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 44.6 42.1 39.5 48.6 44.3 49.4 58.8 64.6 64.1 69.3 62.7 69.3 Fig. 4. VerilogEvalV2 performance for Qwen2.5-Coder-32B-Instruct, after being fine-tuned with each individual dataset. Base Model C, Verilog ("BetterV-base") Chisel, VHDL Chisel, VHDL, Verilog Chisel, VHDL, Verilog, PyMTL3 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 44.6 42.2 50.2 47.6 45.4 58.8 65.5 68.7 72.2 66.4 Fig. 5. VerilogEvalV2 performance for Qwen2.5-Coder-32B-Instruct, after being fine-tuned with combined datasets. The Verilog, C, and Chisel datasets provide relatively smaller improvements. In fact, the C and Verilog Comple- tion datasets decrease but increase which indicates that fine-tuning on these datasets has increased the diversity of generated code. B. Combining Datasets We also explore the effects of fine-tuning with multiple datasets combined. First, we fine-tune with our equivalent of BetterV s fine- tuning dataset. This includes the C dataset with both directions (C in the prompt and Verilog in the response, and vice versa), as well as our Verilog Completion dataset. Note that this data is not exactly the same as the dataset used in BetterV, and we do not include the discriminative guidance component. Combining the C and Verilog datasets did not yield a significant improvement (within about one percentage point) compared to just Verilog. This is likely because these datasets originate from the same Verilog data, and as we show in Section VI-B, there are other languages that might perform better than C when translated to. Next, we fine-tune data with other combinations of datasets. Specifically, we interleave datasets one entry at a time such that the distribution in the beginning of fine-tuning (when 4 Base Model Augmented Verilog Augmented Verilog, VHDL Augmented Verilog, VHDL, Chisel 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 33.6 33.9 34.1 31.6 54.4 58.6 60.3 61.0 Fig. 6. VerilogEvalV2 performance of Qwen2.5-Coder-7B-Instruct, after being fine-tuned with gpt-4o-augmented versions of our datasets. learning rate is highest) is equal for each dataset used. Com- bining datasets seems to yield limited but positive results. In particular combining Chisel and VHDL datasets yields our highest of 50.2 , and combining Chisel, VHDL, and Verilog yields our highest of 72.2 , as shown in Figure 5. However, adding PyMTL3 on top of this combination reduces both and Overall, we find that fine-tuning with hdl2v data tends to increase both and but especially increases (by 23 , compared to up to 13 increase in Since hdl2v data originates from a variety of non- Verilog sources, it makes sense that fine-tuning with this data tends to improve the diversity of generated code, thereby increasing the likelihood of sampling at least one correct response among multiple. C. Data Augmentation We further explore the potential benefits of hdl2v data via a data augmentation case study. Several prior works have shown that augmenting existing Verilog data using methods such as LLM-based summarization can be used to improve LLM Verilog generation performance via fine-tuning [7], [28], [33], [35]. In this work, we apply a generic data augmentation approach as a case study to demonstrate the usefulness of hdl2v data. Specifically, we prompt OpenAI s gpt-4o to generate natural language descriptions of Verilog modules, using OriGen s [10] code description prompt. We apply this prompt to the Verilog modules in the Verilog dataset, as well as the translated Verilog modules in the VHDL and Chisel datasets. We compare the effectiveness of fine-tuning with description-Verilog pairs from just the Verilog dataset to the effectiveness of fine-tuning with description-Verilog pairs from both the Verilog dataset and hdl2v datasets. Note that from this section onward, we use Qwen2.5-Coder-7B-Instruct (the 7B variant rather than 32B) as our base model, due to the high computational and runtime costs of fine-tuning. We begin by fine-tuning with the augmented Verilog data, which yields a few percentage points improvement in from 54.4 to 58.6 , as shown in Figure 6. Then, we add gpt-4o-augmented Verilog data from hdl2v s VHDL dataset, which further boosts from 58.6 to 60.3 . Combin- ing augmented Verilog from the Verilog, VHDL, and Chisel datasets (the combination which yielded the highest in Section V-B) boosts even further to 61 , but drops to 31.6 . Compared to the baseline of 54.8 , including hdl2v data increases the delta caused by fine-tuning by up to 63 , from 3.8 percentage points to 6.6 percentage points. This case study points to hdl2v being useful not just in isolation, but also in tandem with other approaches. VI. ANALYSIS Our datasets differ along several axes. In addition to lan- guage, they also vary in factors such as distribution of designs and human readability. In this section, we characterize our datasets and perform two ablation studies to better understand what makes a dataset helpful in improving Verilog generation. A. Dataset Statistics We characterize the Verilog code of each dataset to elim- inate effects of language syntax. We use Qwen2Tokenizer to compute token counts and diversity. Table III includes statistics such as: Type-Token Ratio (TTR), the ratio of unique tokens to total tokens. N-gram diversity, the ratio of unique token sequences of length N to the total number of such sequences. Higher values indicate greater token variety for both metrics. Perplexity, which measures how well a model makes predictions on a dataset, with lower values indicating bet- ter performance. The model s prediction accuracy can be estimated using the formula: 1 perplexity 100. For example, a perplexity of 1.81 corresponds to a prediction accuracy of about 55.2 . In our case, we use Qwen2.5-Coder-7B as our model and randomly sample 1000 entries from each dataset to compute perplexity. The Verilog code from the VHDL dataset has the highest perplexity of any of our datasets, and the highest vocabulary size and token diversity of the hdl2v datasets. This makes sense as the Verilog from the VHDL dataset is both unseen in pre-training data and is sourced from a wide range of repositories on GitHub. As a result, it is unsurprising that the VHDL dataset is one of the better-performing individual datasets (along with the PyMTL3 dataset) in Section V-A. B. C vs VHDL In Section III-A2, we created our C-Verilog fine-tuning dataset by translating Verilog to C. In order to isolate the effects of using different languages from other variables, we create a dataset of Verilog to VHDL translations using this same dataset. We translate Verilog to VHDL using Icarus Ver- ilog [32]. Both C and VHDL datasets are machine-translated, and they sample the same distribution of designs. We train models using the subset of 12,612 pairs that were able to be translated to both C and VHDL. Like in Section V-C, we use Qwen2.5-Coder-7B-Instruct as our base 5 Metric Verilog Completion C VHDL Chisel PyMTL3 Total Tokens 27,818,433 5,274,385 7,039,588 128,662,957 6,607,407 Vocabulary Size 23,247 15,075 22,279 4,441 1,731 Type-Token Ratio (TTR) 0.0008 0.0029 0.0032 0.0000 0.0003 N-gram Diversity 2-gram 0.0195 0.0393 0.0407 0.0003 0.0017 3-gram 0.0767 0.1217 0.1032 0.0010 0.0046 Average Entry Length (no. tokens) 188.06 195.78 783.71 7394.64 353.55 Standard Deviation of Entry Length 279.89 438.75 1796.34 10804.84 284.06 Perplexity 1.81 2.15 2.34 1.55 1.84 TABLE III STATISTICS FOR INDIVIDUAL DATASETS Base Model C VHDL (from C-translatable Verilog) 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 33.6 25.1 32.2 54.4 52.0 58.6 Fig. 7. VerilogEvalV2 performance for Qwen2.5-Coder-7B-Instruct, after being fine-tuned with a subset of the Verilog dataset translated to C and VHDL, respectively. Original VHDL Comments Removed Obfuscated Variable Names 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 34.5 33.1 21.0 59.8 57.0 39.1 Fig. 8. VerilogEvalV2 performance for Qwen2.5-Coder-7B-Instruct, after being fine-tuned with modified versions of our VHDL dataset. model. Figure 7 shows that the VHDL-translated dataset performs noticeably better than the C dataset. This indicates that the syntactic closeness of VHDL to Verilog, and the fact that VHDL is an HDL (as opposed to C, which is a software language) plays some role in our VHDL dataset outperforming our C dataset. C. Modifying the VHDL Dataset In this section, we explore another axis of difference between datasets: human-readability. As a first step, we re- move any comments from the VHDL dataset, then fine-tune Qwen2.5-Coder-7B-Instruct with this modified dataset. Next, in addition to removing comments, we obfuscate variable names across both VHDL and Verilog by replacing variable names with generic placeholders. VHDL Verilog keywords are preserved and common variable names across a VHDL-Verilog pair remain identical, but obfuscated. As shown in Figure 8, we find that removing comments has only a small effect, whereas obfuscating variable names significantly degrades model performance. This shows that our model learns mostly from code, and the effect of natural language descriptions in the code (in the form of comments) is minimal. VII. CONCLUSION In this work, we present hdl2v, which contains three new datasets for LLM Verilog generation fine-tuning. We utilize existing VHDL, Chisel, and PyMTL3 code to construct these datasets, and show that fine-tuning on HDL-Verilog translation pairs yields up to 13 improvement in and 23 improvement in on VerilogEvalV2. Furthermore, we find that some languages are inherently better than others for this process; specifically, we find that VHDL-Verilog pairs perform better than C-Verilog pairs for the same set of designs. We also find that the model does indeed learn from code rather than from the natural language comments in the code. While hdl2v succeeds in improving LLM Verilog generation via fine-tuning, its real strength is the novel Verilog data it provides. Unlike prior work which focuses on augmentation of existing Verilog, we create multiple datasets of entirely new Verilog, which are both 1) unseen in LLM pre-training corpora and 2) not generated by LLMs themselves. We demonstrate the value of this approach by combining hdl2v VHDL and Chisel data with the existing Verilog corpus to boost the performance of data augmentation-based fine-tuning by 63 . In future work, we would like to combine our dataset with other data augmentation methods, reasoning models, and agentic flows to push Verilog generation performance of open-weight models to even higher levels. REFERENCES [1] [Online]. Available: [2] A. Amid, D. Biancolin, A. Gonzalez, D. Grubb, S. Karandikar, H. Liew, A. Magyar, H. Mao, A. Ou, N. Pemberton, P. Rigge, C. Schmidt, J. Wright, J. Zhao, Y. S. Shao, K. Asanovi c, and B. Nikoli c, Chipyard: Integrated design, simulation, and implementation framework for custom socs, IEEE Micro, vol. 40, no. 4, pp. 10 21, 2020. [3] J. Bachrach, H. Vo, B. Richards, Y. Lee, A. Waterman, R. Aviˇzienis, J. Wawrzynek, and K. Asanovi c, Chisel: constructing hardware in a scala embedded language, in Proceedings of the 49th Annual Design Automation Conference, ser. DAC 12. New York, NY, USA: Association for Computing Machinery, 2012, p. 1216 1225. [Online]. Available: 6 [4] C. Batten, N. Pinckney, M. Liu, H. Ren, and B. Khailany, Pyhdl-eval: An llm evaluation framework for hardware design using python- embedded dsls, in Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD, ser. MLCAD 24. New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: [5] J. Blocklove, S. Garg, R. Karri, and H. Pearce, Chip-chat: Chal- lenges and opportunities in conversational hardware design, in 2023 ACM IEEE 5th Workshop on Machine Learning for CAD (MLCAD), 2023, pp. 1 6. [6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert- Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, Language models are few-shot learners, in Pro- ceedings of the 34th International Conference on Neural Information Processing Systems, ser. NeurIPS 20. Red Hook, NY, USA: Curran Associates Inc., 2020. [7] K. Chang, K. Wang, N. Yang, Y. Wang, D. Jin, W. Zhu, Z. Chen, C. Li, H. Yan, Y. Zhou, Z. Zhao, Y. Cheng, Y. Pan, Y. Liu, M. Wang, S. Liang, Y. Han, H. Li, and X. Li, Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework, in Proceedings of the 61st ACM IEEE Design Automation Conference, ser. DAC 24. New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: [8] K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, and X. Li, Chipgpt: How far are we from natural language hardware design, 2023. [Online]. Available: [9] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., Evaluating large language models trained on code, arXiv preprint arXiv:2107.03374, 2021. [10] F. Cui, C. Yin, K. Zhou, Y. Xiao, G. Sun, Q. Xu, Q. Guo, Y. Liang, X. Zhang, D. Song, and D. Lin, Origen: Enhancing rtl code generation with code-to-code augmentation and self-reflection, in Proceedings of the 43rd IEEE ACM International Conference on Computer-Aided Design, ser. ICCAD 24. New York, NY, USA: Association for Computing Machinery, 2025. [Online]. Available: [11] J. Decaluwe, Myhdl: a python-based hardware description language, Linux J., vol. 2004, no. 127, p. 5, Nov. 2004. [12] L. Doolittle, Sep 2015. [Online]. Available: larry vhd2vl [13] C.-T. Ho, H. Ren, and B. Khailany, Verilogcoder: Autonomous verilog coding agents with graph-based planning and abstract syntax tree (ast)-based waveform tracing tool, 2025. [Online]. Available: [14] C. Hong, S. Bhatia, A. Cheung, and Y. S. Shao, Autocomp: Llm-driven code optimization for tensor accelerators, 2025. [Online]. Available: [15] C. Hong, S. Bhatia, A. Haan, S. K. Dong, D. Nikiforov, A. Cheung, and Y. S. Shao, Llm-aided compilation for tensor accelerators, in 2024 IEEE LLM Aided Design Workshop (LAD), 2024, pp. 1 14. [16] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, 2021. [Online]. Available: [17] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu, K. Dang, Y. Fan, Y. Zhang, A. Yang, R. Men, F. Huang, B. Zheng, Y. Miao, S. Quan, Y. Feng, X. Ren, X. Ren, J. Zhou, and J. Lin, Qwen2.5-coder technical report, 2024. [Online]. Available: [18] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica, Livecodebench: Holistic and contamination free evaluation of large language models for code, arXiv preprint arXiv:2403.07974, 2024. [19] S. Jiang, P. Pan, Y. Ou, and C. Batten, Pymtl3: A python framework for open-source hardware modeling, generation, simulation, and verifi- cation, IEEE Micro, vol. 40, no. 4, pp. 58 66, 2020. [20] M. Liu, N. Pinckney, B. Khailany, and H. Ren, Verilogeval: Evaluating large language models for verilog code generation, 2023. [Online]. Available: [21] M. Liu, Y.-D. Tsai, W. Zhou, and H. Ren, Craftrtl: High-quality synthetic data generation for verilog code models with correct-by- construction non-textual representations and targeted code repair, 2025. [Online]. Available: [22] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, Rtllm: An open-source benchmark for design rtl generation with large language model, in Proceedings of the 29th Asia and South Pacific Design Automation Conference, ser. ASPDAC 24. IEEE Press, 2024, p. 722 727. [Online]. Available: [23] R. Mukherjee, M. Tautschnig, and D. Kroening, V2c a verilog to c translator, vol. 9636. Springer, 2016, pp. 580 586. [24] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [25] OpenAI et al., Gpt-4 technical report, 2024. [26] C. Papon and Y. Xiao, Spinalhdl. [Online]. Available: https: github.com SpinalHDL SpinalHDL [27] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, Gorilla: Large language model connected with massive apis, arXiv preprint arXiv:2305.15334, 2023. [28] Z. Pei, H.-L. Zhen, M. Yuan, Y. Huang, and B. Yu, Betterv: controlled verilog generation with discriminative guidance, in Proceedings of the 41st International Conference on Machine Learning, ser. ICML 24. JMLR.org, 2024. [29] N. Pinckney, C. Batten, M. Liu, H. Ren, and B. Khailany, Revisiting verilogeval: A year of improvements in large-language models for hardware code generation, 2025. [Online]. Available: [30] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, Zero: Memory optimizations toward training trillion parameter models, 2020. [Online]. Available: [31] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan- Gavitt, and S. Garg, Benchmarking large language models for auto- mated verilog rtl code generation, in 2023 Design, Automation Test in Europe Conference Exhibition (DATE), 2023, pp. 1 6. [32] S. Williams and M. Baxter, Icarus verilog: open-source verilog more than a year later, Linux J., vol. 2002, no. 99, p. 3, Jul. 2002. [33] Y. Yang, F. Teng, P. Liu, M. Qi, C. Lv, J. Li, X. Zhang, and Z. He, Haven: Hallucination-mitigated llm for verilog code generation aligned with hdl engineers, 2025. [Online]. Available: [34] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhandari, X. Wu, A. A. Awan, J. Rasley, M. Zhang, C. Li, C. Holmes, Z. Zhou, M. Wyatt, M. Smith, L. Kurilenko, H. Qin, M. Tanaka, S. Che, S. L. Song, and Y. He, Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales, 2023. [Online]. Available: [35] Y. Zhang, Z. Yu, Y. Fu, C. Wan, and Y. C. Lin, Mg-verilog: Multi- grained dataset towards enhanced llm-assisted verilog generation, 2024. [Online]. Available: [36] Y. Zhao, H. Zhang, H. Huang, Z. Yu, and J. Zhao, Mage: A multi-agent engine for automated rtl code generation, 2024. [Online]. Available: 7\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nhdl2v: A Code Translation Dataset for Enhanced LLM Verilog Generation Charles Hong , Brendan Roberts, Huijae An, Alex Um, Advay Ratan, Yakun Sophia Shao UC Berkeley Abstract Large language models (LLMs) are playing an increasingly large role in domains such as code generation, including hardware code generation, where Verilog is the key language. However, the amount of publicly available Verilog code pales in comparison to the amount of code available for software languages like Python. In this work, we present hdl2v ( HDL- to-Verilog ), a dataset which seeks to increase the amount of available human-written Verilog data by translating or compiling three other hardware description languages VHDL, Chisel, and PyMTL3 to Verilog. Furthermore, we demonstrate the value of hdl2v in enhancing LLM Verilog generation by improving performance of a 32 billion-parameter open-weight model by up to 23 in VerilogEvalV2, without utilizing any data augmentation or knowledge distillation from larger models. We also show hdl2v s ability to boost the performance of a data augmentation-based fine-tuning approach by 63 . Finally, we characterize and analyze our dataset to better understand which characteristics of HDL-to-Verilog datasets can be expanded upon in future work for even better performance. I. INTRODUCTION Large language models (LLMs) have demonstrated impres- sive performance in a wide range of tasks, ranging from general reasoning ability and instruction-following [6], [25] to code generation [9], [18], [24], [27]. LLMs have potential to automate a wide range of tasks in hardware design, ranging from design to verification to optimization [5], [8], [14], [15]. A number of studies have attempted to evaluate and improve LLMs potential in generating Verilog code [13], [31], [36]. Compared to popular software programming languages such as Python or C, there is not as much publicly available Verilog. In fact, as of April 2025 and according to RepositoryStats, there are 132,264 popular GitHub repositories with Python as the primary language, compared to just 848 for Verilog or SystemVerilog [1].\n\n--- Segment 2 ---\nCompared to popular software programming languages such as Python or C, there is not as much publicly available Verilog. In fact, as of April 2025 and according to RepositoryStats, there are 132,264 popular GitHub repositories with Python as the primary language, compared to just 848 for Verilog or SystemVerilog [1]. As a result, a number of prior works have attempted to fine-tune LLMs on novel Verilog datasets, and successfully improved Verilog generation performance. These works utilize techniques such as data augmentation [7], [10], [28], [33], [35] and synthetic Verilog generation [21]. However, Verilog is not the only hardware description language (HDL). While Verilog is often used as the common interface between hardware code and tools such as RTL simulators or synthesis software, design can be done in higher- level languages such as Chisel. VHDL is another popular HDL with its own ecosystem of supported hardware. Nonetheless, generating Verilog with LLMs is still an important task as it is the most commonly supported and written. In this work, we investigate how the wealth of HDL code in languages other than Verilog can be used to improve LLMs ability to generate Verilog. Specifically, we present hdl2v, a dataset consisting of 46,549 pairs of VHDL, Chisel, and PyMTL3 translated compiled to Verilog. We use this data in supervised fine-tuning of LLMs. Our findings from these experiments are as follows: Fine-tuning with this data yields significant improvements in Verilog generation performance. We find that Verilo- gEvalV2 performance of a state-of-the-art open-weight LLM improves by up to 13 for and 23 for upon being fine-tuned on a combination of our datasets. hdl2v works in tandem with other fine-tuning approaches. We demonstrate that by adding data from hdl2v to existing Verilog training data, we boost performance of a data augmentation approach by 63 . Language matters; fine-tuning with VHDL-Verilog pairs yields better results than C-Verilog pairs, when the Ver- ilog is held constant. Fine-tuned models learn from the code in prompt- response pairs, not just natural language.\n\n--- Segment 3 ---\nLanguage matters; fine-tuning with VHDL-Verilog pairs yields better results than C-Verilog pairs, when the Ver- ilog is held constant. Fine-tuned models learn from the code in prompt- response pairs, not just natural language. However, utiliz- ing meaningful module and variable names is important in helping LLMs learn from this data. hdl2v is fully open-source and available for others to expand on this research. 1 2 3 II. BACKGROUND A. Verilog Code Generation Prior work has seeked to improve LLMs ability to generate correct Verilog code, using techniques such as fine tuning on textbooks and Verilog from Github [31]. Other prior works augment existing Verilog datasets [7], [28], [33], [35] or generate novel synthetic Verilog [21]. Multi-agent systems utilize feedback from RTL simulation tools and modify test code in order to debug generated code [13], [36]. Benchmarks such as VerilogEval [20] and RTLLM [22] have been de- veloped to standardize evaluation of LLM Verilog generation performance. This work does not seek to supercede such prior work. Instead, we provide new data that can complement other 1VHDL dataset: 2Chisel dataset: 3PyMTL3 dataset: 1 arXiv:2506.04544v2 [cs.AR] 8 Jul 2025 Verilog V2C VHDL Verilog vhd2vl Prompt: module logic_1 (output A); Response: module logic_1 (output A); assign A 1; endmodule Prompt: void logic_1 (bool A) { A 1; } Response: module logic_1 (output A); assign A 1; endmodule C Dataset Prompt: entity logic_0 is port ( A : out std_logic ); end logic_0; architecture Behavioral of logic_0 is begin A '0'; end Behavioral; Response: module logic_0 (output A); assign A 0; endmodule VHDL Dataset Verilog Completion Dataset Fig. 1. How the Verilog Completion, C, and VHDL datasets are collected.\n\n--- Segment 4 ---\n1. How the Verilog Completion, C, and VHDL datasets are collected. Note that Verilog is translated to C and VHDL is translated to Verilog, but during fine-tuning Verilog is always used as the response. approaches, such as data augmentation and agentic systems, to further improve LLM Verilog generation. We show a concrete example of this synergy in Section V-C. B. Hardware Description Languages (HDLs) Verilog is a key language in hardware design due to its use as a common representation for representing register-transfer level (RTL) designs. A wide range of design automation tools, such as logic synthesis software and FPGA compilers, use Verilog as a common input to represent digital hardware. However, while digital hardware design is commonly done in Verilog (and its more feature-rich descendant, SystemVerilog), many other hardware description languages (HDLs) exist and have been used to tape out real-world chips. For example, VHDL is an alternative to Verilog that fills largely the same role. On the other hand, high-level HDLs like Chisel [3], Spinal- HDL [26], MyHDL [11], PyMTL3 [19], and many others seek to provide user-friendly features such as type-checking and greater parameterizability. Often these languages are em- bedded in high-level software languages, such as Scala in the case of Chisel and SpinalHDL, and Python in the case of MyHDL and PyMTL3. Nonetheless, designs written in these languages still compile to Verilog so that they can be used with industry standard design automation tools. As a result, the Chisel or PyMTL3 code is often a higher-level (and as a result, closer to natural language) representation of the Verilog it compiles to. We use this fact to our advantage in hdl2v specifically, we show that fine-tuning LLMs with pairs of Chisel PyMTL3 code and their corresponding compiled Verilog can successfully improve LLMs ability to generate Verilog from a natural language spec. We select VHDL for this work as it is the most popular HDL outside of Verilog and SytemVerilog.\n\n--- Segment 5 ---\nWe use this fact to our advantage in hdl2v specifically, we show that fine-tuning LLMs with pairs of Chisel PyMTL3 code and their corresponding compiled Verilog can successfully improve LLMs ability to generate Verilog from a natural language spec. We select VHDL for this work as it is the most popular HDL outside of Verilog and SytemVerilog. Additionally, we include Chisel and PyMTL3 as they are two of the most popular high- level HDLs, embedded in two different high-level software languages, with a large amount of diverse and high-quality code written in each language. Of course, future work could extend our approach to include other HDLs. C. Verilog Translation for LLM Fine-Tuning BetterV [28] introduces the idea of translating Verilog to C in order to improve correctness of generated Verilog. However, we demonstrate in Section V-A that when training with a Verilog dataset and the corresponding data translated to C, the benefit from training with C is minimal compared to training with only Verilog. Furthermore, the Verilog in this dataset is also likely to be present in LLMs pre-training data, as it originates from open-source Github repositories. To our knowledge, this work is the first to translate other HDLs to Verilog to generate novel Verilog data for LLM fine- tuning. III. DATASET CONSTRUCTION A. Datasets From Prior Work As a baseline, we fine-tune with datasets from prior work that are based on existing Verilog from public sources. 1) Verilog Completion: As in BetterV [28], this dataset consists of a filtered set of Verilog modules from public sources. In this case, the prompt for fine-tuning is the header of the Verilog module, and the response is the rest of the module. Figure 1 shows an example of what an entry in this dataset might look like. This dataset contains 147,138 entries with total size 84.4 MB. 2) C: As in BetterV [28] and as depicted in Figure 1, we use v2c [23] to translate the above Verilog to C. In this case, the prompt for fine-tuning is actually the translated C code, and the original Verilog is the response.\n\n--- Segment 6 ---\nThis dataset contains 147,138 entries with total size 84.4 MB. 2) C: As in BetterV [28] and as depicted in Figure 1, we use v2c [23] to translate the above Verilog to C. In this case, the prompt for fine-tuning is actually the translated C code, and the original Verilog is the response. The intent is to improve the model s understanding of the Verilog code by attempting to correlate it with C code, which has greater presence in pretraining data. As not all Verilog modules in our dataset can 2 CONFIG "SmallBoomV3Conﬁg" ALU .scala Chisel Dataset , , , Chisel Verilog Collect Compile DCache .scala HellaCache .scala TLB .scala ALU .v DCache .v TLB .v TLB .scala TLB .scala Fig. 2. How the Chisel dataset is collected. One pair is collected for each generated Verilog module. Note that in cases where multiple Verilog modules are generated from the same Chisel file, that Chisel file will be included in multiple pairs in the dataset. be successfully translated to C, this results in 26,803 entries with size 36.5 MB. B. hdl2v Datasets Each hdl2v dataset is fully open-source and available on HuggingFace, as described in Section I. 1) VHDL: As shown in Figure 1, we use Google BigQuery to collect VHDL files from Github. Specifically, we collect every file with a .vhd or .vhdl extension. This results in 53,698 VHDL entities. We attempt to translate each VHDL entity to Verilog using the open-source tool vhd2vl [12]. vhd2vl is able to successfully translate 8974 entities to Verilog, and we filter out entries that do not contain the strings module and later endmodule. The remaining 8626 entries have a total size of 48.4 MB. Each prompt is a VHDL entity, and its response is the corresponding translated Verilog module. 2) Chisel: Chisel is a high-level HDL embedded in Scala that can be compiled into Verilog or SystemVerilog. Therefore, it intrinsically provides matching pairs between itself and Verilog.\n\n--- Segment 7 ---\n2) Chisel: Chisel is a high-level HDL embedded in Scala that can be compiled into Verilog or SystemVerilog. Therefore, it intrinsically provides matching pairs between itself and Verilog. To gather this data, we used Chipyard [2], which contains a variety of generators that can be combined to create a wide range of SoC configurations. We compile a large number of Chipyard SoC configurations to Verilog, aiming to collect Verilog files generated from as many Chisel source files as possible in the Chipyard repository. Our dataset includes 55 of the .scala files in Chipyard and its subrepositories; of those not covered, most do not contain synthesizable Chisel. The generated Verilog contains annotations that indicate which Chisel file and line each Verilog line is generated from, allowing us to collect a set of relevant Chisel files for each generated Verilog file. Each Verilog file contains one module. We show an example of how Chisel-Verilog pairs are collected for one SoC configuration in Figure 2. Duplicates PyMTL3 VerilogTranslation Pass Verilog PyHDL-Eval Prompt: class AlwaysOne( Component ): def construct( s ): s.o OutPort() update def logic_block(): s.o b1(1) Response: module always_one (output o); assign o 1; endmodule PyMTL3 Dataset Fig. 3. How the PyMTL3 dataset is collected. are removed, but for cases where the same Chisel file with different parameters generates differing Verilog output, all the data is kept. This results in 18,939 Chisel Verilog pairs (with a total size of 1.69 GB). We additionally collect the corresponding FIRRTL. We do not use it for any further experiments in this work, but include it in our dataset for future users. In our prompt response pairs for LLM fine-tuning, the response consists of one generated Verilog file, which contain a single Verilog module. The prompt contains one or multiple Chisel source files (including one primary class and all of its dependencies), and a request to translate the code into Verilog.\n\n--- Segment 8 ---\nIn our prompt response pairs for LLM fine-tuning, the response consists of one generated Verilog file, which contain a single Verilog module. The prompt contains one or multiple Chisel source files (including one primary class and all of its dependencies), and a request to translate the code into Verilog. While the Chisel source code provided in the prompt does not directly compile to the response Verilog, we seek to improve the LLM s ability to correlate high-level HDL code in the Scala-embedded Chisel, which may contain more information about design semantics, to lower-level compiled Verilog. 3) PyMTL3: PyHDL-Eval [4] evaluates the ability of LLMs to correctly generate the Python-embedded HDLs PyMTL3, PyRTL, MyHDL, Migen, and Amaranth. The authors provide as an artifact the PyHDL code generated by LLMs during these experiments. As shown in Figure 3, we use LLM-generated PyMTL3 code from PyHDL-Eval to construct a dataset similar to the Chisel dataset above. Like Chisel, PyMTL3 is a high-level HDL which can be compiled to Verilog. We compile each PyMTL3 example from PyHDL-Eval s artifact, numbering about 50,000, to Verilog using PyMTL3 s VerilogTranslation- Pass. 18,636 examples (with a total size of 28 MB) compile to Verilog successfully, as many of the PyMTL3 examples in PyHDL-Eval contain syntax errors. In this dataset, each prompt is one PyMTL3 class, and each response is the corresponding translated Verilog module. IV. EXPERIMENT SETUP A. LLM Fine-Tuning Setup We use Qwen2.5-Coder-32B-Instruct [17] as our base model. To train the model, we use DeepSpeed-Chat s Super- vised Fine-Tuning pipeline [34] and enable ZeRO Stage 3 [30] and LoRA [16] for efficient training. We maintain consistent hyperparameter settings across all experiments, including the 3 use of FusedAdam optimizer, cosine learning decay, a learning rate of 1e-5, a single training epoch, and a batch size of 8.\n\n--- Segment 9 ---\nTo train the model, we use DeepSpeed-Chat s Super- vised Fine-Tuning pipeline [34] and enable ZeRO Stage 3 [30] and LoRA [16] for efficient training. We maintain consistent hyperparameter settings across all experiments, including the 3 use of FusedAdam optimizer, cosine learning decay, a learning rate of 1e-5, a single training epoch, and a batch size of 8. All experiments are conducted on a server with four NVIDIA L40S GPUs. A summary of the training hyperparameters is provided in the following table: Hyperparameter Value ZeRO Stage 3 LoRA Dimension 32 Data Type bfloat16 Batch Size 8 Learning Rate 1e-5 Number of Epochs 1 TABLE I HYPERPARAMETERS USED FOR FINE-TUNING B. Evaluation Setup VerilogEvalV2 [29] is a benchmark that consists of 156 Verilog design problems, intended to test LLMs abil- ity to generate functionally correct Verilog according to a (mostly) natural language specification. We use Verilo- gEvalV2 s spec-to-rtl benchmark to evaluate our model, with the following settings: Parameter Value Samples 20 Temperature 0.85 top p 0.95 ICL examples 0 ICL rules no TABLE II PARAMETERS USED FOR EVALUATION WITH VERILOGEVALV2 We use both and to evaluate model performance. effectively measures the total percentage of functionally correct responses, whereas estimates the model s ability to generate at least one correct response when multiple samples (in this case 10) are taken. V. FINE-TUNING EXPERIMENTS A. Fine-Tuning with Individual Datasets As shown in Figure 4, our datasets have varying effective- ness in improving our models performance in VerilogEvalV2. Of the five datasets tested, PyMTL3 and VHDL perform the best, both providing about 18 increase in over the base Qwen2.5-Coder-32B-Instruct model. As we will discuss further in Section VI, the VHDL dataset has the highest perplexity in the dataset and provides a diverse dataset that has not been seen (as Verilog) during pretraining.\n\n--- Segment 10 ---\nOf the five datasets tested, PyMTL3 and VHDL perform the best, both providing about 18 increase in over the base Qwen2.5-Coder-32B-Instruct model. As we will discuss further in Section VI, the VHDL dataset has the highest perplexity in the dataset and provides a diverse dataset that has not been seen (as Verilog) during pretraining. On the other hand, while the PyMTL3 dataset is relatively less diverse, the set of designs it targets is highly relevant to VerilogEval, as PyHDL-Eval also generated code for a benchmark set of designs similar to VerilogEval. The entries in the PyMTL3 dataset also tend to be shorter than the entries in the other two hdl2v datasets (see Table III), making it easier for the model to learn relationships between the high-level PyMTL3 and the translated Verilog. Base Model Verilog Completion C VHDL Chisel PyMTL3 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 44.6 42.1 39.5 48.6 44.3 49.4 58.8 64.6 64.1 69.3 62.7 69.3 Fig. 4. VerilogEvalV2 performance for Qwen2.5-Coder-32B-Instruct, after being fine-tuned with each individual dataset. Base Model C, Verilog ("BetterV-base") Chisel, VHDL Chisel, VHDL, Verilog Chisel, VHDL, Verilog, PyMTL3 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 44.6 42.2 50.2 47.6 45.4 58.8 65.5 68.7 72.2 66.4 Fig. 5. VerilogEvalV2 performance for Qwen2.5-Coder-32B-Instruct, after being fine-tuned with combined datasets. The Verilog, C, and Chisel datasets provide relatively smaller improvements. In fact, the C and Verilog Comple- tion datasets decrease but increase which indicates that fine-tuning on these datasets has increased the diversity of generated code. B.\n\n--- Segment 11 ---\nIn fact, the C and Verilog Comple- tion datasets decrease but increase which indicates that fine-tuning on these datasets has increased the diversity of generated code. B. Combining Datasets We also explore the effects of fine-tuning with multiple datasets combined. First, we fine-tune with our equivalent of BetterV s fine- tuning dataset. This includes the C dataset with both directions (C in the prompt and Verilog in the response, and vice versa), as well as our Verilog Completion dataset. Note that this data is not exactly the same as the dataset used in BetterV, and we do not include the discriminative guidance component. Combining the C and Verilog datasets did not yield a significant improvement (within about one percentage point) compared to just Verilog. This is likely because these datasets originate from the same Verilog data, and as we show in Section VI-B, there are other languages that might perform better than C when translated to. Next, we fine-tune data with other combinations of datasets. Specifically, we interleave datasets one entry at a time such that the distribution in the beginning of fine-tuning (when 4 Base Model Augmented Verilog Augmented Verilog, VHDL Augmented Verilog, VHDL, Chisel 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 33.6 33.9 34.1 31.6 54.4 58.6 60.3 61.0 Fig. 6. VerilogEvalV2 performance of Qwen2.5-Coder-7B-Instruct, after being fine-tuned with gpt-4o-augmented versions of our datasets. learning rate is highest) is equal for each dataset used. Com- bining datasets seems to yield limited but positive results. In particular combining Chisel and VHDL datasets yields our highest of 50.2 , and combining Chisel, VHDL, and Verilog yields our highest of 72.2 , as shown in Figure 5.\n\n--- Segment 12 ---\nCom- bining datasets seems to yield limited but positive results. In particular combining Chisel and VHDL datasets yields our highest of 50.2 , and combining Chisel, VHDL, and Verilog yields our highest of 72.2 , as shown in Figure 5. However, adding PyMTL3 on top of this combination reduces both and Overall, we find that fine-tuning with hdl2v data tends to increase both and but especially increases (by 23 , compared to up to 13 increase in Since hdl2v data originates from a variety of non- Verilog sources, it makes sense that fine-tuning with this data tends to improve the diversity of generated code, thereby increasing the likelihood of sampling at least one correct response among multiple. C. Data Augmentation We further explore the potential benefits of hdl2v data via a data augmentation case study. Several prior works have shown that augmenting existing Verilog data using methods such as LLM-based summarization can be used to improve LLM Verilog generation performance via fine-tuning [7], [28], [33], [35]. In this work, we apply a generic data augmentation approach as a case study to demonstrate the usefulness of hdl2v data. Specifically, we prompt OpenAI s gpt-4o to generate natural language descriptions of Verilog modules, using OriGen s [10] code description prompt. We apply this prompt to the Verilog modules in the Verilog dataset, as well as the translated Verilog modules in the VHDL and Chisel datasets. We compare the effectiveness of fine-tuning with description-Verilog pairs from just the Verilog dataset to the effectiveness of fine-tuning with description-Verilog pairs from both the Verilog dataset and hdl2v datasets. Note that from this section onward, we use Qwen2.5-Coder-7B-Instruct (the 7B variant rather than 32B) as our base model, due to the high computational and runtime costs of fine-tuning. We begin by fine-tuning with the augmented Verilog data, which yields a few percentage points improvement in from 54.4 to 58.6 , as shown in Figure 6.\n\n--- Segment 13 ---\nNote that from this section onward, we use Qwen2.5-Coder-7B-Instruct (the 7B variant rather than 32B) as our base model, due to the high computational and runtime costs of fine-tuning. We begin by fine-tuning with the augmented Verilog data, which yields a few percentage points improvement in from 54.4 to 58.6 , as shown in Figure 6. Then, we add gpt-4o-augmented Verilog data from hdl2v s VHDL dataset, which further boosts from 58.6 to 60.3 . Combin- ing augmented Verilog from the Verilog, VHDL, and Chisel datasets (the combination which yielded the highest in Section V-B) boosts even further to 61 , but drops to 31.6 . Compared to the baseline of 54.8 , including hdl2v data increases the delta caused by fine-tuning by up to 63 , from 3.8 percentage points to 6.6 percentage points. This case study points to hdl2v being useful not just in isolation, but also in tandem with other approaches. VI. ANALYSIS Our datasets differ along several axes. In addition to lan- guage, they also vary in factors such as distribution of designs and human readability. In this section, we characterize our datasets and perform two ablation studies to better understand what makes a dataset helpful in improving Verilog generation. A. Dataset Statistics We characterize the Verilog code of each dataset to elim- inate effects of language syntax. We use Qwen2Tokenizer to compute token counts and diversity. Table III includes statistics such as: Type-Token Ratio (TTR), the ratio of unique tokens to total tokens. N-gram diversity, the ratio of unique token sequences of length N to the total number of such sequences. Higher values indicate greater token variety for both metrics. Perplexity, which measures how well a model makes predictions on a dataset, with lower values indicating bet- ter performance. The model s prediction accuracy can be estimated using the formula: 1 perplexity 100. For example, a perplexity of 1.81 corresponds to a prediction accuracy of about 55.2 . In our case, we use Qwen2.5-Coder-7B as our model and randomly sample 1000 entries from each dataset to compute perplexity.\n\n--- Segment 14 ---\nFor example, a perplexity of 1.81 corresponds to a prediction accuracy of about 55.2 . In our case, we use Qwen2.5-Coder-7B as our model and randomly sample 1000 entries from each dataset to compute perplexity. The Verilog code from the VHDL dataset has the highest perplexity of any of our datasets, and the highest vocabulary size and token diversity of the hdl2v datasets. This makes sense as the Verilog from the VHDL dataset is both unseen in pre-training data and is sourced from a wide range of repositories on GitHub. As a result, it is unsurprising that the VHDL dataset is one of the better-performing individual datasets (along with the PyMTL3 dataset) in Section V-A. B. C vs VHDL In Section III-A2, we created our C-Verilog fine-tuning dataset by translating Verilog to C. In order to isolate the effects of using different languages from other variables, we create a dataset of Verilog to VHDL translations using this same dataset. We translate Verilog to VHDL using Icarus Ver- ilog [32]. Both C and VHDL datasets are machine-translated, and they sample the same distribution of designs. We train models using the subset of 12,612 pairs that were able to be translated to both C and VHDL. Like in Section V-C, we use Qwen2.5-Coder-7B-Instruct as our base 5 Metric Verilog Completion C VHDL Chisel PyMTL3 Total Tokens 27,818,433 5,274,385 7,039,588 128,662,957 6,607,407 Vocabulary Size 23,247 15,075 22,279 4,441 1,731 Type-Token Ratio (TTR) 0.0008 0.0029 0.0032 0.0000 0.0003 N-gram Diversity 2-gram 0.0195 0.0393 0.0407 0.0003 0.0017 3-gram 0.0767 0.1217 0.1032 0.0010 0.0046 Average Entry Length (no.\n\n--- Segment 15 ---\nWe train models using the subset of 12,612 pairs that were able to be translated to both C and VHDL. Like in Section V-C, we use Qwen2.5-Coder-7B-Instruct as our base 5 Metric Verilog Completion C VHDL Chisel PyMTL3 Total Tokens 27,818,433 5,274,385 7,039,588 128,662,957 6,607,407 Vocabulary Size 23,247 15,075 22,279 4,441 1,731 Type-Token Ratio (TTR) 0.0008 0.0029 0.0032 0.0000 0.0003 N-gram Diversity 2-gram 0.0195 0.0393 0.0407 0.0003 0.0017 3-gram 0.0767 0.1217 0.1032 0.0010 0.0046 Average Entry Length (no. tokens) 188.06 195.78 783.71 7394.64 353.55 Standard Deviation of Entry Length 279.89 438.75 1796.34 10804.84 284.06 Perplexity 1.81 2.15 2.34 1.55 1.84 TABLE III STATISTICS FOR INDIVIDUAL DATASETS Base Model C VHDL (from C-translatable Verilog) 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 33.6 25.1 32.2 54.4 52.0 58.6 Fig. 7. VerilogEvalV2 performance for Qwen2.5-Coder-7B-Instruct, after being fine-tuned with a subset of the Verilog dataset translated to C and VHDL, respectively. Original VHDL Comments Removed Obfuscated Variable Names 0 10 20 30 40 50 60 70 80 VerilogEvalV2 Pass Rate ( ) 34.5 33.1 21.0 59.8 57.0 39.1 Fig. 8. VerilogEvalV2 performance for Qwen2.5-Coder-7B-Instruct, after being fine-tuned with modified versions of our VHDL dataset. model. Figure 7 shows that the VHDL-translated dataset performs noticeably better than the C dataset.\n\n--- Segment 16 ---\nmodel. Figure 7 shows that the VHDL-translated dataset performs noticeably better than the C dataset. This indicates that the syntactic closeness of VHDL to Verilog, and the fact that VHDL is an HDL (as opposed to C, which is a software language) plays some role in our VHDL dataset outperforming our C dataset. C. Modifying the VHDL Dataset In this section, we explore another axis of difference between datasets: human-readability. As a first step, we re- move any comments from the VHDL dataset, then fine-tune Qwen2.5-Coder-7B-Instruct with this modified dataset. Next, in addition to removing comments, we obfuscate variable names across both VHDL and Verilog by replacing variable names with generic placeholders. VHDL Verilog keywords are preserved and common variable names across a VHDL-Verilog pair remain identical, but obfuscated. As shown in Figure 8, we find that removing comments has only a small effect, whereas obfuscating variable names significantly degrades model performance. This shows that our model learns mostly from code, and the effect of natural language descriptions in the code (in the form of comments) is minimal. VII. CONCLUSION In this work, we present hdl2v, which contains three new datasets for LLM Verilog generation fine-tuning. We utilize existing VHDL, Chisel, and PyMTL3 code to construct these datasets, and show that fine-tuning on HDL-Verilog translation pairs yields up to 13 improvement in and 23 improvement in on VerilogEvalV2. Furthermore, we find that some languages are inherently better than others for this process; specifically, we find that VHDL-Verilog pairs perform better than C-Verilog pairs for the same set of designs. We also find that the model does indeed learn from code rather than from the natural language comments in the code. While hdl2v succeeds in improving LLM Verilog generation via fine-tuning, its real strength is the novel Verilog data it provides. Unlike prior work which focuses on augmentation of existing Verilog, we create multiple datasets of entirely new Verilog, which are both 1) unseen in LLM pre-training corpora and 2) not generated by LLMs themselves.\n\n--- Segment 17 ---\nWhile hdl2v succeeds in improving LLM Verilog generation via fine-tuning, its real strength is the novel Verilog data it provides. Unlike prior work which focuses on augmentation of existing Verilog, we create multiple datasets of entirely new Verilog, which are both 1) unseen in LLM pre-training corpora and 2) not generated by LLMs themselves. We demonstrate the value of this approach by combining hdl2v VHDL and Chisel data with the existing Verilog corpus to boost the performance of data augmentation-based fine-tuning by 63 . In future work, we would like to combine our dataset with other data augmentation methods, reasoning models, and agentic flows to push Verilog generation performance of open-weight models to even higher levels. REFERENCES [1] [Online]. Available: [2] A. Amid, D. Biancolin, A. Gonzalez, D. Grubb, S. Karandikar, H. Liew, A. Magyar, H. Mao, A. Ou, N. Pemberton, P. Rigge, C. Schmidt, J. Wright, J. Zhao, Y. S. Shao, K. Asanovi c, and B. Nikoli c, Chipyard: Integrated design, simulation, and implementation framework for custom socs, IEEE Micro, vol. 40, no. 4, pp. 10 21, 2020. [3] J. Bachrach, H. Vo, B. Richards, Y. Lee, A. Waterman, R. Aviˇzienis, J. Wawrzynek, and K. Asanovi c, Chisel: constructing hardware in a scala embedded language, in Proceedings of the 49th Annual Design Automation Conference, ser. DAC 12. New York, NY, USA: Association for Computing Machinery, 2012, p. 1216 1225. [Online]. Available: 6 [4] C. Batten, N. Pinckney, M. Liu, H. Ren, and B. Khailany, Pyhdl-eval: An llm evaluation framework for hardware design using python- embedded dsls, in Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD, ser. MLCAD 24.\n\n--- Segment 18 ---\nAvailable: 6 [4] C. Batten, N. Pinckney, M. Liu, H. Ren, and B. Khailany, Pyhdl-eval: An llm evaluation framework for hardware design using python- embedded dsls, in Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD, ser. MLCAD 24. New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: [5] J. Blocklove, S. Garg, R. Karri, and H. Pearce, Chip-chat: Chal- lenges and opportunities in conversational hardware design, in 2023 ACM IEEE 5th Workshop on Machine Learning for CAD (MLCAD), 2023, pp. 1 6. [6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert- Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, Language models are few-shot learners, in Pro- ceedings of the 34th International Conference on Neural Information Processing Systems, ser. NeurIPS 20. Red Hook, NY, USA: Curran Associates Inc., 2020. [7] K. Chang, K. Wang, N. Yang, Y. Wang, D. Jin, W. Zhu, Z. Chen, C. Li, H. Yan, Y. Zhou, Z. Zhao, Y. Cheng, Y. Pan, Y. Liu, M. Wang, S. Liang, Y. Han, H. Li, and X. Li, Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework, in Proceedings of the 61st ACM IEEE Design Automation Conference, ser.\n\n--- Segment 19 ---\nRed Hook, NY, USA: Curran Associates Inc., 2020. [7] K. Chang, K. Wang, N. Yang, Y. Wang, D. Jin, W. Zhu, Z. Chen, C. Li, H. Yan, Y. Zhou, Z. Zhao, Y. Cheng, Y. Pan, Y. Liu, M. Wang, S. Liang, Y. Han, H. Li, and X. Li, Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework, in Proceedings of the 61st ACM IEEE Design Automation Conference, ser. DAC 24. New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: [8] K. Chang, Y. Wang, H. Ren, M. Wang, S. Liang, Y. Han, H. Li, and X. Li, Chipgpt: How far are we from natural language hardware design, 2023. [Online]. Available: [9] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman et al., Evaluating large language models trained on code, arXiv preprint arXiv:2107.03374, 2021. [10] F. Cui, C. Yin, K. Zhou, Y. Xiao, G. Sun, Q. Xu, Q. Guo, Y. Liang, X. Zhang, D. Song, and D. Lin, Origen: Enhancing rtl code generation with code-to-code augmentation and self-reflection, in Proceedings of the 43rd IEEE ACM International Conference on Computer-Aided Design, ser. ICCAD 24. New York, NY, USA: Association for Computing Machinery, 2025. [Online]. Available: [11] J. Decaluwe, Myhdl: a python-based hardware description language, Linux J., vol. 2004, no. 127, p. 5, Nov. 2004. [12] L. Doolittle, Sep 2015. [Online].\n\n--- Segment 20 ---\n[12] L. Doolittle, Sep 2015. [Online]. Available: larry vhd2vl [13] C.-T. Ho, H. Ren, and B. Khailany, Verilogcoder: Autonomous verilog coding agents with graph-based planning and abstract syntax tree (ast)-based waveform tracing tool, 2025. [Online]. Available: [14] C. Hong, S. Bhatia, A. Cheung, and Y. S. Shao, Autocomp: Llm-driven code optimization for tensor accelerators, 2025. [Online]. Available: [15] C. Hong, S. Bhatia, A. Haan, S. K. Dong, D. Nikiforov, A. Cheung, and Y. S. Shao, Llm-aided compilation for tensor accelerators, in 2024 IEEE LLM Aided Design Workshop (LAD), 2024, pp. 1 14. [16] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, 2021. [Online]. Available: [17] B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Lu, K. Dang, Y. Fan, Y. Zhang, A. Yang, R. Men, F. Huang, B. Zheng, Y. Miao, S. Quan, Y. Feng, X. Ren, X. Ren, J. Zhou, and J. Lin, Qwen2.5-coder technical report, 2024. [Online]. Available: [18] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica, Livecodebench: Holistic and contamination free evaluation of large language models for code, arXiv preprint arXiv:2403.07974, 2024.\n\n--- Segment 21 ---\n[Online]. Available: [18] N. Jain, K. Han, A. Gu, W.-D. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica, Livecodebench: Holistic and contamination free evaluation of large language models for code, arXiv preprint arXiv:2403.07974, 2024. [19] S. Jiang, P. Pan, Y. Ou, and C. Batten, Pymtl3: A python framework for open-source hardware modeling, generation, simulation, and verifi- cation, IEEE Micro, vol. 40, no. 4, pp. 58 66, 2020. [20] M. Liu, N. Pinckney, B. Khailany, and H. Ren, Verilogeval: Evaluating large language models for verilog code generation, 2023. [Online]. Available: [21] M. Liu, Y.-D. Tsai, W. Zhou, and H. Ren, Craftrtl: High-quality synthetic data generation for verilog code models with correct-by- construction non-textual representations and targeted code repair, 2025. [Online]. Available: [22] Y. Lu, S. Liu, Q. Zhang, and Z. Xie, Rtllm: An open-source benchmark for design rtl generation with large language model, in Proceedings of the 29th Asia and South Pacific Design Automation Conference, ser. ASPDAC 24. IEEE Press, 2024, p. 722 727. [Online]. Available: [23] R. Mukherjee, M. Tautschnig, and D. Kroening, V2c a verilog to c translator, vol. 9636. Springer, 2016, pp. 580 586. [24] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [25] OpenAI et al., Gpt-4 technical report, 2024.\n\n--- Segment 22 ---\n[24] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [25] OpenAI et al., Gpt-4 technical report, 2024. [26] C. Papon and Y. Xiao, Spinalhdl. [Online]. Available: https: github.com SpinalHDL SpinalHDL [27] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, Gorilla: Large language model connected with massive apis, arXiv preprint arXiv:2305.15334, 2023. [28] Z. Pei, H.-L. Zhen, M. Yuan, Y. Huang, and B. Yu, Betterv: controlled verilog generation with discriminative guidance, in Proceedings of the 41st International Conference on Machine Learning, ser. ICML 24. JMLR.org, 2024. [29] N. Pinckney, C. Batten, M. Liu, H. Ren, and B. Khailany, Revisiting verilogeval: A year of improvements in large-language models for hardware code generation, 2025. [Online]. Available: [30] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, Zero: Memory optimizations toward training trillion parameter models, 2020. [Online]. Available: [31] S. Thakur, B. Ahmad, Z. Fan, H. Pearce, B. Tan, R. Karri, B. Dolan- Gavitt, and S. Garg, Benchmarking large language models for auto- mated verilog rtl code generation, in 2023 Design, Automation Test in Europe Conference Exhibition (DATE), 2023, pp. 1 6. [32] S. Williams and M. Baxter, Icarus verilog: open-source verilog more than a year later, Linux J., vol. 2002, no. 99, p. 3, Jul. 2002.\n\n--- Segment 23 ---\n99, p. 3, Jul. 2002. [33] Y. Yang, F. Teng, P. Liu, M. Qi, C. Lv, J. Li, X. Zhang, and Z. He, Haven: Hallucination-mitigated llm for verilog code generation aligned with hdl engineers, 2025. [Online]. Available: [34] Z. Yao, R. Y. Aminabadi, O. Ruwase, S. Rajbhandari, X. Wu, A. A. Awan, J. Rasley, M. Zhang, C. Li, C. Holmes, Z. Zhou, M. Wyatt, M. Smith, L. Kurilenko, H. Qin, M. Tanaka, S. Che, S. L. Song, and Y. He, Deepspeed-chat: Easy, fast and affordable rlhf training of chatgpt-like models at all scales, 2023. [Online]. Available: [35] Y. Zhang, Z. Yu, Y. Fu, C. Wan, and Y. C. Lin, Mg-verilog: Multi- grained dataset towards enhanced llm-assisted verilog generation, 2024. [Online]. Available: [36] Y. Zhao, H. Zhang, H. Huang, Z. Yu, and J. Zhao, Mage: A multi-agent engine for automated rtl code generation, 2024. [Online]. Available: 7\n\n