=== ORIGINAL PDF: 2505.13461v1_FPGA-based_Acceleration_for_Convolutional_Neural_N.pdf ===\n\nRaw text length: 106347 characters\nCleaned text length: 105446 characters\nNumber of segments: 55\n\n=== CLEANED TEXT ===\n\n1 FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review Junye Jiang , Yaan Zhou , Yuanhao Gong , Haoxuan Yuan, and Shuanglong Liu Abstract Convolutional Neural Networks (CNNs) are fun- damental to deep learning, driving applications across various domains. However, their growing complexity has significantly increased computational demands, necessitating efficient hard- ware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a leading solution, offering reconfigurability, parallelism, and energy efficiency. This paper provides a compre- hensive review of FPGA-based hardware accelerators specifically designed for CNNs. It presents and summarizes the performance evaluation framework grounded in existing studies and explores key optimization strategies, such as parallel computing, dataflow optimization, and hardware-software co-design. It also compares various FPGA architectures in terms of latency, throughput, compute efficiency, power consumption, and resource utilization. Finally, the paper highlights future challenges and opportunities, emphasizing the potential for continued innovation in this field. Index Terms Convolutional Neural Networks (CNNs), Field Programmable Gate Arrays (FPGAs), Hardware Accelerator, Compute Efficiency, Parallel Computing, Design Space Explo- ration (DSE) I. INTRODUCTION In recent years, Deep Learning (DL) has gained significant attention in computer science [1], with Convolutional Neural Networks (CNNs) serving as a cornerstone of this progress. CNNs have achieved remarkable success across various appli- cations, including image classification [2], object detection [3], and semantic segmentation [4]. Their transformative impact has led to widespread adoption by industry leaders, recog- nizing CNNs as a vital tool for innovation and competitive advantage. The strength of CNNs lies in their ability to process high- dimensional data through convolutional operations, pooling, and non-linear activations, enabling automated multi-level feature extraction. However, this capability comes at the cost of increased computational complexity and resource demands. With their adoption in mobile and edge devices like drones and smartphones [5], CNNs face critical challenges in achieving high performance under resource constraints. To address these challenges, extensive research has been conducted on hardware acceleration for CNNs [6] [9]. While CPUs, GPUs and ASICs are commonly employed, their limita- tions in handling the complexity of modern CNN architectures This work was supported in part by the Scientific Research Foundation of Hunan Provincial Education Department (Key project 23A0087) and Changsha Municipal Natural Science Foundation (No. kq2502001). Junye Jiang, Yaan Zhou, Yuanhao Gong, Haoxuan Yuan and Shuanglong Liu are with the Key Laboratory of Low-Dimensional Quantum Structures and Quantum Control of Ministry of Education, School of Physics and Electronics, Hunan Normal University, Changsha, 410081 China. Equal Contribution. Corresponding author: Shuanglong Liu and real-time processing needs have prompted growing inter- est in alternative platforms. Field-Programmable Gate Arrays (FPGAs) have emerged as a promising solution, offering reconfigurability, parallel processing, and energy efficiency. Their ability to customize hardware for specific tasks enables the design of high-performance, resource-efficient accelerators tailored for deep learning applications. This review article explores recent advances in FPGA- based CNN accelerators, focusing on acceleration methods, architectural innovations, hardware optimization techniques, and software-hardware co-design frameworks. It highlights key trends, addresses current challenges, and outlines future directions in this rapidly evolving domain. Additionally, the article examines performance evaluation metrics and provides insights into diverse architectures, offering practical guidance for researchers and practitioners. The main contributions of this work are summarized as follows: Evaluation Metrics: We review FPGA-based CNN ac- celeration research, consolidating key performance met- rics, particularly compute and overall efficiency. It ex- plores the relationships between these metrics, aiding researchers in identifying design limitations and guiding future advancements (Section III); Acceleration Methods: We present a quantitative com- parison of existing CNN acceleration methods, providing practitioners and researchers with valuable insights to select approaches that best align with their specific system requirements (Section IV); Parallel Computing Analysis: This study reviews paral- lel computing methods and their hardware architectures in existing designs, analyzing their strengths and weak- nesses using the introduced evaluation metrics. It high- lights the importance of dynamic parallelism in enhancing hardware performance, including compute efficiency and resource utilization (Section V); Future Directions: Building on the comprehensive re- view in this work, we propose and discuss future research directions to help researchers and designers identify key design considerations and potential optimizations in this field (Section VII). The rest part of this paper is organized as follows: Section II compares the acceleration platforms for CNNs, emphasizing the advantages of FPGAs. In Section III, we analyze and outline design performance metrics for FPGA-based CNN accelerators, guiding the comparison of existing designs in subsequent sections. This section also highlights commonly overlooked metrics in the literature and suggests directions arXiv:2505.13461v1 [cs.LG] 4 May 2025 2 for future design and optimization. Section IV presents ac- celeration methods for CNNs at both the algorithmic and hardware levels, with qualitative and quantitative comparisons. Section V reviews existing FPGA-based CNN accelerators, focusing on parallel computing techniques in hardware design. Section VI discusses hardware-software co-design method- ologies, including toolflows, design space exploration, and performance and resource modeling. Section VII concludes this paper with a discussion on future research directions. II. CNN ACCELERATION PLATFORMS This section compares the architectural and performance characteristics of common hardware acceleration platforms, including central processing units (CPUs), graphics processing units (GPUs), application-specific integrated circuits (ASICs), and field-programmable gate arrays (FPGAs). It focuses on their performance in CNN applications, emphasizing that FPGA-based accelerators deliver superior performance for deep learning tasks. A. Hardware Platforms Central Processing Units (CPUs): CPUs are fundamental components in modern computing systems, adhering to the von Neumann architecture. In the context of CNN models, CPUs are well-suited for handling complex logical operations, such as model initialization, data preprocessing, and managing communication between hardware components. They excel in processing diverse data types and executing intricate com- putations independently. However, their limited capacity for parallel processing makes them less efficient for the high- volume matrix operations and convolutions characteristic of CNNs. Additionally, CPUs often face challenges like high power consumption and restricted memory bandwidth, which can become bottlenecks when processing the large-scale data and computations required by CNN models [10]. Graphics Processing Units (GPUs): GPUs have been rapidly developed to handle large-scale computations, particu- larly in applications like image processing and CNN models. Unlike CPUs, GPUs are optimized for parallel computing, capable of processing massive amounts of similar data si- multaneously, making them well-suited for tasks like CNN training, where multiple convolutions and matrix operations can be executed in parallel. The architecture of GPUs, with larger arithmetic logic units, enables faster processing speeds and enhances their parallel computing capabilities. For example, in CNN-based applications, GPUs signifi- cantly speed up tasks such as training deep learning models by accelerating the computation of convolutional layers and weight updates [11]. However, GPUs have limitations in single-threaded performance, which makes them less ideal for tasks requiring complex logic or single-core execution. Additionally, GPUs face challenges such as high power con- sumption, substantial cooling needs, and limited memory capacity when dealing with extremely large datasets, which can hinder performance in ultra-large-scale CNN models. Application-Specific Integrated Circuits (ASICs): ASICs are specialized integrated circuits designed to meet specific product requirements. The main advantage of ASICs in deep learning applications lies in their customization, which allows for the design of high-performance circuits tailored to the spe- cific operations of CNNs, such as convolution, activation, and pooling layers. This customization allows for the elimination of redundant components, significantly reducing chip area and power consumption compared to CPUs and GPUs [12]. However, the highly specialized nature of ASICs comes with trade-offs. The design process is time-consuming, and ASICs are highly dependent on the particular CNN models they are designed for, making them less adaptable to evolv- ing architectures or new tasks. Furthermore, ASICs lack the flexibility to rapidly adjust to the dynamic nature of CNN development, which limits their applicability as deep learning models evolve. Unlike FPGAs, which can be reprogrammed or dynamically configured, ASICs are optimized for specific tasks, making them less versatile and unsuitable for handling diverse workloads outside their narrow focus. Field Programmable Gate Array (FPGAs): To enhance the programmability of hardware modules, FPGAs are increas- ingly utilized in various applications, thanks to their flexibility and reconfigurability. In the context of CNNs, FPGAs serve as a low-latency hardware platform that allows for customized data path designs and optimized memory hierarchies, making them a valuable complement to CPUs and GPUs. FPGAs excel in maximizing performance through dynamic task partitioning and efficient software-hardware co-design, which are critical for optimizing CNN operations. These features make FPGAs particularly advantageous in AI inference tasks, especially in resource-constrained environments like edge computing, where power efficiency and flexibility are paramount [13]. For this reason, this article provides a detailed literature review of FPGA-based CNN accelerators, which will be discussed in the following sections. B. Further Comparison of FPGAs and GPUs The acceleration of CNNs consists of two main stages: training and inference. CNN training involves constructing and optimizing the model through iterative processes across multiple epochs, refining the model parameters with each cy- cle. CNN inference uses a trained model to make predictions, such as classification or detection, without the need for weight updates or gradient calculations. In the training phase, GPUs excel in leveraging their high parallelism, achieving substantial throughput by processing large batches of data. However, their efficiency depends heav- ily on batch size, which works well in offline scenarios but is less effective in real-time processing, particularly for CNN models requiring continuous data transfer. In such cases, GPU performance tends to degrade [14]. Additionally, GPUs often introduce significant latency, especially during convolution operations, which can impact overall system performance. On the other hand, dedicated architectures for CNNs can be designed to parallelize computations within a frame. The flexibility of FPGAs makes them a promising candidate for 3 TABLE I COMPARISON OF CNN ACCELERATION PLATFORMS Metrics CPUs GPUs ASICs FPGAs Compute Performance Limited computing capability with general-purpose Highly parallel computing capability Highly parallelized for specific applications Highly parallelized with customizable hardware Energy Efficiency Moderate power Large power Lowest Power Low power Flexibility Highly flexible Limited flexibility Fully customized Programmable Development Time Shortest development time Moderate development time Time-consuming development Acceptable development time Cost High cost Largest cost High initial cost but cost-effective for large-scale production Relatively low Model Scalability Reconfigurable Reconfigurable Worst scalability Scaled with reconfiguration CNN acceleration. With a scalable design, CNN accelerators can also be implemented on embedded FPGAs. While GPUs offer inherent parallelism, FPGAs allow for finer-grained par- allelism by customizing the hardware architecture to optimize specific CNN layers. This customization enables faster execu- tion of operations, such as convolution and activation, thereby improving overall CNN performance [15]. C. Discussion Table I summarizes the performance comparison of CPUs, GPUs, ASICs, and FPGAs across various metrics such as computing performance, energy efficiency, flexibility, devel- opment time, cost, and scalability. Based on these factors, a comparative analysis of the platforms is provided. FPGAs stand out in accelerating CNN models due to their customization and flexibility. Unlike ASICs, which are fixed to specific tasks, FPGAs can be reprogrammed to optimize performance for different CNN architectures, making them ideal for applications requiring a balance between accuracy and throughput. Additionally, FPGAs can leverage sparsity techniques to significantly enhance throughput. For exam- ple, the sparsity-aware CNN accelerator by Wen et al. [16] achieved up to 89.7 higher throughput compared to baseline designs. FPGAs also offer advantages in power efficiency, particu- larly for edge devices with strict power constraints, providing a good balance between flexibility and low power consumption. While ASICs may offer lower power usage, FPGAs are more adaptable and power-efficient than GPUs and CPUs. Further- more, FPGAs excel in reducing latency, making them ideal for real-time applications that demand quick responses [17]. III. EVALUATION FRAMEWORK In CNN acceleration, evaluation metrics are crucial for assessing design performance. These metrics fall into two categories: network model performance and hardware accel- erator evaluation. This section focuses on the key metrics used to evaluate FPGA-based CNN accelerators, highlighting throughput, compute efficiency, and energy efficiency as crit- ical factors for hardware optimization. A. Model Performance Evaluation Accuracy: Algorithm adaptation and optimization tech- niques, such as model compression and data quantization, are often employed to reduce complexity. As a result, evaluating the accuracy of optimized models becomes crucial. Accuracy is a widely used metric in CNN model design to assess performance, representing the proportion of correctly classified samples in a dataset. It is calculated using the formula: Accuracy TP TN TP TN FP FN (1) where TP (True Positive), TN (True Negative), FP (False Positive), and FN (False Negative) indicate the model s correct and incorrect predictions. Accuracy provides an overall measure of classification performance, especially when the dataset has a balanced distribution of positive and negative samples. In such cases, a high accuracy rate typically indi- cates better performance. However, accuracy has limitations, particularly in the presence of class imbalance. In these cases, a high accuracy rate may mask poor performance on minority classes. Precision and Recall: To address the limitations of ac- curacy, it is often used in conjunction with precision and recall [18]. Precision measures the model s accuracy in pre- dicting positive classes, indicating the proportion of predicted positive samples that are correctly identified as positive. It is calculated as: Precision TP TP FP (2) However, precision does not consider false negatives and focuses only on the accuracy of positive predictions, ignoring cases where the model fails to identify actual positive samples. In scenarios where missing positive instances has a high cost, relying solely on precision may not be enough. To address this, Recall is used to evaluate the model s ability to correctly identify all positive samples. Recall is calculated using the formula: Recall TP TP FN (3) F1 Score: Precision and recall often exhibit an inverse trade-off. Increasing recall typically leads to a decrease in precision, and vice versa. This occurs because when the 4 model prioritizes predicting positive classes to improve recall, the number of false positives may rise, resulting in lower precision. On the other hand, if the model overly emphasizes precision, it might miss some true positive samples, leading to a reduction in recall. To balance this trade-off, the F1 score is commonly used [19]. It is the harmonic mean of precision and recall, offering a comprehensive evaluation of model performance across both metrics. It is calculated as follows: F1 2 Precision Recall Precision Recall (4) B. Hardware Performance Evaluation Various hardware performance metrics, including through- put, latency, and power consumption, are widely used to eval- uate and compare FPGA-based CNN accelerators. However, achieving a fair comparison requires careful consideration of the operations or workloads of the CNN model, the available FPGA resources, and the underlying chip technology. Here, we provide a detailed discussion of the commonly employed evaluation metrics in the literature, highlighting their signifi- cance and impact. Latency: Latency is a key consideration for CNN hardware accelerators, representing the runtime taken from data input to the completion of processing and output generation. It serves as a critical measure of the system s responsiveness when processing a single image. Low latency is especially important for real-time applications such as autonomous driving [20] and speech recognition [21]. The overall latency of a design can be divided into three primary components: (1) Off-chip data transfer latency, determined by the model parameters and the memory bandwidth; (2) On-chip data transfer latency, influenced by the efficiency of internal data routing within the hardware; (3) Computation latency, dependent on the number of operations required by the CNN model and the level of parallelism implemented in the hardware design. To minimize latency in CNN accelerators, the following optimization strategies are commonly employed: Increasing memory bandwidth: Utilize high-bandwidth, low-latency memory technologies such as High Band- width Memory (HBM) to accelerate data read and write processes [22]; Reducing data transfer bottlenecks: Employ efficient data transfer protocols or advanced bus architectures, such as PCIe [23] or NVLink [24], to lower latency between accelerators and external devices; Optimizing data storage patterns: Enhance data reuse and reduce the frequency of data transfers by employing optimized storage patterns [25], thereby decreasing data transfer latency; Enhancing computational parallelism: Design hardware architectures to maximize computational parallelism, en- abling faster processing within the accelerator and reduc- ing overall computational latency [7]. Throughput: Throughput, often referred to as hardware performance, measures the number of computational opera- tions a system can process within a specific time frame. Unlike latency, throughput accounts for the computational workloads of different CNN models. As such, it serves as a key metric for evaluating hardware efficiency and performance [26]. The relationship between CNN workloads, throughput, and latency can be expressed as: Throughput Workloads Latency (5) Here, workloads refer to the volume of computational opera- tions in the CNN model. It is important to differentiate system throughput from the theoretical peak performance of the design, which is defined as: Peak throughput design 2 MACs f (6) where MACs represents the number of multiply-accumulate (MAC) units in the hardware accelerator, and f denotes the operating clock frequency of the design. This formula rep- resents the theoretical maximum performance of the realized hardware architecture. While the theoretical peak throughput indicates the maxi- mum achievable performance of the hardware design, actual throughput reflects the real-world performance. According to Eq. (6), the peak throughput is influenced by the number of MAC units, which is directly related to the parallelism of the hardware framework. Larger parallelism leads to more MAC units and increased computational capacity. Compute Efficiency: The theoretical peak throughput rep- resents the ideal maximum performance of a hardware system, acting as the upper limit of an accelerator s capacity. However, in practical CNN acceleration, the network parameters may not align with the computational parallelism of the design. This misalignment can cause unbalanced workloads among the processing elements (PEs) in the computation engine, leading to suboptimal utilization of available resources. Additionally, factors such as memory bandwidth and data transfer rates significantly impact throughput, often creating bottlenecks. As a result, actual throughput is lower than the theoretical peak performance due to these practical constraints. To evaluate the efficiency of a hardware design, compute ef- ficiency, also referred to as MAC efficiency, is employed [27]. This metric is defined as the ratio of the actual throughput to the peak theoretical throughput. It serves as a critical measure of hardware accelerators effectiveness by quantifying the proportion of useful MAC cycles relative to the total MAC units in the design. Compute efficiency is calculated using the formula: Compute EFF. Throughput 2 MACs f (7) Unlike other evaluation metrics mentioned above, compute efficiency is independent of the specific FPGA device or the number of MAC units utilized. Instead, it reflects the intrinsic efficiency of the accelerator design, making it a more reliable measure for assessing and comparing different hardware architectures for CNN acceleration. 5 Resource Efficiency: In CNN accelerators, resource ef- ficiency measures the system s throughput relative to the number of DSP units used [28]. This metric evaluates the contribution of each DSP unit to the computational task and is defined as: Res. EFF. Throughput DSP used (8) Higher resource efficiency reflects better DSP utilization, with each unit delivering significant computational throughput. This enhances hardware resource usage while maintaining high performance, reducing power consumption and idle re- sources, and improving overall system efficiency and stability. However, in designs where logic resources are used alongside DSPs to implement MACs, this metric may not provide a fair basis for comparison across different designs. Clock Efficiency: Clock efficiency measures how effec- tively a hardware accelerator operates relative to its maximum clock frequency [29]. It is defined as: Clock EFF. Working Clock Maximum Clock (9) This ratio indicates the utilization of the hardware s com- putational potential. Higher clock efficiency means the ac- celerator operates near its peak frequency, while lower ef- ficiency suggests underutilization. Practical constraints like power consumption, heat dissipation, and system stability often limit clock efficiency. Improving heat dissipation, power management, and task scheduling can boost clock efficiency, leading to better performance in high-demand applications such as real-time processing and data centers. Overall Efficiency: Corresponds to the peak throughput of the design mentioned above, the theoretical peak performance of an FPGA device is calculated by multiplying the total number of multipliers and adders in its DSP blocks by the maximum clock rate [30]. This value represents the theoretical computational limit, which is unattainable in practice because real-world algorithms cannot continuously utilize all compu- tational units. However, it serves as a valuable benchmark for comparison. The overall efficiency is defined as the ratio of achieved performance to the peak performance of the device, offering a comprehensive measure of the hardware s effectiveness [6]. It can be expressed as: OVERALL EFF. CLOCK EFF. RES. UTI. COMPUTE EFF. (10) where clock efficiency is the ratio of the working clock rate to the maximum clock rate, resource utilization measures the fraction of multipliers and adders utilized in the design relative to those available in DSP blocks, and compute efficiency reflects the fraction of useful MAC cycles achieved. Optimizing clock rates and improving resource utiliza- tion often represent competing strategies for achieving high- performance FPGA accelerators. Note that logic resources are typically excluded from peak performance calculations, as their usage complicates computation, requires significant resources for auxiliary functions, and negatively impacts the working clock rate [30]. Overall efficiency provides a unified view of hardware performance, helping identify imbalances across components. By incorporating previously discussed metrics, this evaluation framework facilitates a quantitative comparison of existing research and highlights emerging trends in CNN hardware accelerators. Energy Efficiency: Energy efficiency [6], [28] measures how efficiently the CNN hardware accelerators perform com- putations relative to its power consumption. It is typically expressed as the amount of work completed per watt of power consumed. The formula for energy efficiency is: Energy EFF. Throughput Power (11) As computational demands grow, especially in resource- intensive tasks like large-scale data processing and CNN model training, managing power consumption has become increasingly critical. Optimizing hardware accelerators to min- imize power consumption while maintaining high throughput is a key challenge in the field. Improving energy efficiency not only reduces operating costs but also addresses thermal management issues, ensuring better overall system stability and sustainability. C. Discussion on Evaluation Framework This section introduces various evaluation metrics for CNN hardware accelerators, enabling a quantitative comparison of performance across different designs in subsequent sections. Additionally, the relationships between these metrics are ex- amined, providing valuable insights for the design of FPGA- based CNN hardware accelerators. A significant observation from the literature review is that some researchers evaluate hardware performance using only a limited subset of metrics. To address this, Section V compare hardware designs using the comprehensive metrics of com- putational efficiency and overall efficiency introduced in this section. This approach aims to provide readers with a clearer understanding of the trade-offs and performance characteristics of different designs, offering practical guidance for future research and development in CNN hardware accelerators. IV. CNN ACCELERATION METHODS CNN models that achieve high accuracy tend to be large and resource-intensive. However, the larger the model, the more memory and storage it requires, making deployment on resource-constrained devices a significant challenge. Fur- thermore, larger models typically result in higher inference times and increased energy consumption, which limits their practicality for real-world applications. While these models perform well in controlled environments, they are not always feasible for deployment on edge devices. The solution lies in reducing the size of the model through compression techniques or optimizing the computations involved in CNNs. In this section, we explore existing methods for accelerating CNNs. We offer both qualitative and quantitative analyses of each approach, helping readers select the techniques that best suit their specific system requirements. The common methods 6 for CNN acceleration can be grouped into the following categories: 1) Pruning and Quantization: This approach reduces the computational complexity and memory demands of a model by either decreasing the number of model param- eters or reducing the bit-width of data. Techniques such as sparsity, pruning, and quantization are commonly employed in this category. 2) Model Structure Compression: This method optimizes the model s structure to enhance performance. It in- cludes techniques like lightweight networks, knowledge distillation, layer fusion, and the use of smaller convo- lution kernels. 3) Computation Reduction: This approach replaces com- plex operations in the original network with more ef- ficient computational techniques, such as FFT-based convolutions, fully spectral methods, and Winograd con- volutions. A. Pruning and Quantization CNN models often contain a large number of parameters, which are adjusted during training to minimize the loss function. These parameters help the network capture intricate data patterns by connecting multiple layers of neurons and applying non-linear activation functions. While this enables the model to achieve high accuracy, it also introduces significant computational costs. Therefore, controlling the quantity and quality of parameters is crucial to balancing performance and computation workloads. 1) Pruning: Pruning reduces the size and complexity of CNN models by eliminating unimportant parameters or struc- tures. Similar to pruning branches in biology, this technique removes redundant components to make the model more effi- cient. The process typically involves training a full-scale deep neural network, evaluating the importance of each parameter based on criteria such as weight magnitude or activation levels, and then removing unimportant weights, neurons, channels, or layers. The remaining network is usually fine-tuned to restore performance, with pruning and fine-tuning often repeated for further compression. For example, Han et al. [31] combined pruning, quantiza- tion, and Huffman coding to efficiently compress deep neural networks, reducing memory usage and improving computa- tional speed and energy efficiency. Liu et al. [32] introduced L1 regularization for automatic identification and pruning of unimportant channels, applicable to modern CNN architectures with minimal computational overhead. Lin et al. [33] proposed a pruning criterion based on the rank of the output feature map to rank and prune filters efficiently. Yeom et al. [34] used Layer-wise Relevance Propagation (LRP) to prune net- works, calculating the importance of each unit through back- propagation for better compression and accuracy, especially without fine-tuning. Liu et al. [35] proposed AutoCompress, an automatic structured pruning framework that optimizes the pruning rate for each layer. More recently, Tan et al. [36] developed the Moving-one-Sample-out (MoSo) method, which measures sample importance based on the change in empiri- cal risk when a sample is excluded, reducing computational overhead while maintaining high performance, even at high pruning ratios. 2) Quantization: Quantization is a technique used to re- duce the computational overhead and memory demands of neural networks while preserving accuracy. By converting model weights and activation values from floating-point num- bers (typically 32-bit) to lower-precision integers (e.g., 8-bit or 16-bit), quantization reduces memory usage and acceler- ates inference speed. This technique is particularly valuable for deploying models on edge devices like FPGAs, where integer representations are more efficient than floating-point operations. For FPGA-based accelerations, fixed-point quantization is commonly used. Nguyen et al. [37] proposed a high- throughput, low-power FPGA implementation of the YOLO CNN for object detection, using binary weights and flexible low-bit activations, reducing model size and hardware costs. Kim et al. [38] developed a fixed-point quantization method for YOLO, adjusting weight distributions by subtracting the mean value before quantization and applying iterative retrain- ing to minimize accuracy loss. For hardware implementation, Jain et al. [39] introduced a uniform symmetric quantizer with power-of-two scaling factors to achieve better optimization between range and precision. Jin et al. [40] proposed F8Net, a fixed-point 8-bit multiplication network quantization method that achieves superior performance and efficiency compared to existing methods. Lin et al. [41] introduced a fixed- point quantization method for deep convolutional networks (DCNs) based on optimizing the signal-to-quantization-noise ratio (SQNR) to determine the optimal bit width for each layer. To maximize acceleration, some researchers compress net- works to even lower bit widths. Li et al. [42] introduced FQN (Fully Quantized Network), which uses fully quantized training at low bit widths, ensuring equal distances between adjacent quantization points. Cao et al. [43] developed Seer- Net, which uses highly quantized (e.g., 4-bit or 1-bit) versions of CNN networks to predict binary sparse masks for output feature maps, guiding full-precision convolutions to exploit sparsity and accelerate inference. Similarly, Rastegar et al. [44] proposed XNOR-Net, where both filters and inputs to convo- lutional layers are binary, achieving 58 faster convolutions and 32 memory savings. B. Model Structure Compression The complexity of neural network architectures stems from the design and interaction of key components such as depth, width, and branching structures. Network depth refers to the number of layers in a model, and deeper networks can capture more abstract, complex features. However, very deep models can suffer from gradient vanishing or exploding problems, making training more challenging. Network width is the number of neurons or channels per layer, which influences the model s ability to process and capture input data. While wider networks can increase the capacity to learn, they can also introduce parameter redundancy and higher memory usage. Additionally, branching architectures where a model has mul- tiple branches at different stages can improve performance by 7 enabling the model to capture multi-scale features. However, these designs also increase computational complexity and memory demands. In general, more complex network architectures can yield better performance but at the cost of higher computational overhead and memory requirements. When deploying these models on resource-constrained edge devices, there is a need to balance complexity with efficiency. Model structure com- pression aims to reduce the computational burden and memory usage by optimizing the network s design or eliminating redundant components. These techniques focus on the archi- tectural level of the network and are essential for creating more efficient, deployable models. 1) Lightweight Networks: Lightweight networks are neural network models designed to operate efficiently in resource- constrained environments, aiming to reduce model size, com- putational complexity, and memory usage while preserving high performance. Szegedy et al. [45] introduced GoogleNet, which featured the Inception module a multi-scale convolutional layer design that employs 1 1, 3 3, and 5 5 convolutional kernels in parallel. This approach captures multi-scale features while keeping computational cost low. By repeating the Inception module across the network and combining it with pooling layers, they successfully increased both the depth and width of the model without significantly increasing the computational load. Similarly, Iandola et al. [46] proposed SqueezeNet, which uses a novel Fire module to build a lightweight model. The Fire module consists of a squeeze layer (using 1 1 convolutions to reduce feature map dimensions) and an expand layer (combining 1 1 and 3 3 convolutions), making it efficient and compact. The MobileNet series has also gained significant attention for its lightweight design. Howard et al. [47] introduced MobileNet v1, which uses depthwise separable convolutions, splitting traditional convolution into two parts: Depthwise Convolution and Pointwise Convolution. This reduces the number of parameters and computational complexity, while allowing flexibility in latency-accuracy trade-offs through two simple hyperparameters. MobileNet v2 [48] further optimized the network by introducing linear bottlenecks and inverted residuals, enabling deeper yet smaller and faster networks. MobileNet v3 [49] refined MobileNet v2 by removing certain layers, reducing computational overhead without sacrificing accuracy. Han et al. [50] proposed GhostNet, which uses the Ghost module for model compression. This technique splits a convo- lutional layer into two parts: one performing standard convolu- tion and the other generating additional feature maps through simple linear operations, significantly reducing both computa- tion and parameter count while maintaining accuracy. Chen et al. [51] introduced Partial Convolution (PConv), which effectively extracts spatial features by reducing redundant computations and memory accesses. Based on PConv, Faster- Net was also developed in [51], a new family of networks that achieves higher running speeds on various devices without sacrificing accuracy in visual tasks. 2) Knowledge Distillation: Knowledge Distillation (KD) is a model compression technique that enhances the efficiency of models without significant loss in performance. In this approach, a large, pre-trained, complex model (the teacher) transfers its knowledge to a simpler, smaller model (the student). The goal is to enable the student model to achieve performance close to that of the teacher model despite having fewer parameters. KD has been widely applied in areas such as mobile deployment, edge computing, real-time applications, resource conservation, and fields like medical imaging, natural language processing, and autonomous systems. The concept of KD was first introduced by Hinton et al. [52], who demonstrated how a teacher model could transfer knowledge to a smaller model by using both hard targets (the original labels) and soft targets (the teacher s outputs). This combination improves the accuracy of the student model. However, when there is a large size discrepancy between the teacher and student, the student model may struggle to learn effectively. To address this, Son et al. [53] proposed the use of multiple progressively smaller Teacher Assistants (TAs) to guide the student, making the knowledge transfer smoother. They also introduced the Random DGKD method, which randomly drops knowledge connections to avoid overfitting. Hao et al. [54] advanced KD with Self-Mutual Knowledge Distillation (SMKD), where the model distills knowledge from itself, and student networks learn from each other in a mutual distillation process. This approach improves generalization and enhances both the visual and contextual capabilities of the network. Later, Sun et al. [55] proposed a method where the temperature (scaling factor) of both the teacher and student models, as well as the sample logits, is varied. They introduced a Z-score preprocessing method that standardizes logits, en- abling the student model to better learn the relationships from the teacher model. 3) Layer Fusion: In CNNs, layers such as convolution, batch normalization, activation, and pooling are typically ex- ecuted in sequence. However, researchers have shown that re- ordering or merging certain layers can significantly reduce data transfer times and computational complexity when executing the CNN models in hardware. Alwani et al. [56] introduced a pyramid-shaped multi-layer sliding window that processes input feature maps, allowing multiple layer results to be computed in advance. Abtahi et al. [57] proposed integrating Batch Normalization (BN) layer parameters directly into the preceding convolutional layer during inference, as BN is a lin- ear operation with fixed parameters. This integration reduces the number of operations. Syafeeza et al. [58] demonstrated that performing max pooling before ReLU activation helps reduce the computational workload on the ReLU layer. Liu et al. [59] applied layer fusion to the Fast Fourier Transform (FFT) approach, combining pooling and convolutional layers to eliminate unnecessary operations and improve efficiency. C. Computation Reduction Computation reduction involves simplifying complex op- erations in neural networks, typically leveraging mathemat- ical theorems. Techniques like the Fast Fourier Transform 8 (FFT) and the Winograd algorithm can significantly reduce the computational complexity of convolutional layers. Further advancements, such as fully spectral CNNs, boost performance by transforming each layer into the frequency domain. 1) FFT Approach: The FFT approach accelerates convo- lution operations by using the convolution theorem, which states that spatial domain convolutions are equivalent to point- wise multiplications in the frequency domain. By transforming both the input data and weights into the frequency domain with FFT, performing element-wise multiplication, and then converting the result back to the spatial domain through Inverse FFT (IFFT), convolution operations can be executed more efficiently. Mathieu et al. [60] applied FFT to speed up both training and inference in convolutional neural networks, achieving notable improvements by reusing transformed feature maps. Rippe et al. [61] introduced frequency domain pooling, ob- serving that low-frequency components carry most of the critical information. By truncating feature representations in the frequency domain rather than using traditional spatial pool- ing, they preserved more information and allowed for flexible adjustments in the output size. Zeng et al. [62] implemented FFT on FPGA and developed a frequency domain loop tiling technique called Overlap-and-Add (OaA) to enhance through- put through better data reuse. Abtahi et al. [57] compared three methods on embedded hardware: Direct Convolution (Direct- Conv), FFT-based Convolution (FFT-Conv), and Overlap-and- Add based FFT Convolution (FFT-OVA-Conv), providing a detailed comparison of computational complexity and memory requirements. While traditional FFT methods reduce computational com- plexity, they have a significant drawback: the lack of frequency domain activation functions. This limitation necessitates re- peated FFT-IFFT transformations, diminishing the acceleration benefits. To address this, fully spectral CNNs were introduced. Liu et al. [63] overcame this limitation by constructing acti- vation functions in the frequency domain, enabling all CNN layers to operate in the frequency domain and eliminating the need for repeated FFT-IFFT transformations. Similarly, Ayat et al. [64] utilized the convolution theorem to create a frequency-domain SReLU activation function, further enhanc- ing computation through layer fusion techniques. Watanabe et al. [65] explored the ReLU operation in the frequency domain and proposed the 2SReLU (Second Harmonics Superposition ReLU) activation function, performing all CNN operations in the frequency domain. 2) Winograd: While FFT is effective for large convolution kernel operations, CNNs increasingly use smaller kernels, where the Winograd algorithm excels. The Winograd algo- rithm minimizes the number of multiplication operations in convolution by substituting multiplications with additions, thus reducing computational complexity. Although originally proposed by Shmuel Winograd in 1980, the algorithm was first applied to accelerate convolution operations in neural networks by Lavin et al. [66]. They divided large input data into smaller tiles and performed low-complexity convolutions on each tile, significantly re- ducing computational load. Their experiments demonstrated the algorithm s efficiency, particularly for smaller convolution kernels. Subsequently, Liang et al. [67] developed a hardware framework to implement the Winograd algorithm on FPGAs, utilizing a line-buffer structure for efficient feature map reuse across different blocks, and pipelining Winograd Processing Element (PE) units in parallel. Yepez et al. [68] introduced a novel Winograd-based method that supports strides of 2 and is adaptable across one, two, or three dimensions. D. Discussion and Comparison We have reviewed the current methods for accelerating CNNs as described above. Here, we also provide a com- prehensive summary of their key research efforts, as shown in Table II. This summary highlights each method s core contributions and results, aiming to offer an overview of their respective acceleration effects. However, each method has its own set of advantages and limitations, making them more or less suitable depending on the specific scenario. For example, both pruning and quantization can help reduce computational overhead. However, pruning is a complex pro- cess that often depends on the characteristics of the task and hardware architecture. Moreover, it may require careful fine- tuning. Quantization, while effective at reducing model size and computation, may require additional calibration steps to maintain performance. Knowledge distillation allows smaller models to learn from larger, more complex models, leveraging their output distribu- tions or intermediate features. While this can result in compact models with good performance, the distillation process itself is relatively complex and heavily reliant on the teacher model and the specific task at hand. Lightweight models, on the other hand, are designed to be compact and efficient, often using specialized architectures to reduce memory and computation. However, they may not match the performance of larger, general-purpose models in terms of accuracy. The FFT approach can significantly reduce the computa- tional complexity of convolution layers by leveraging fre- quency domain operations. However, it requires repeated FFT and IFFT transformations, which can hinder its effectiveness in hardware implementations. Fully spectral CNNs, which aim to overcome this limitation, have been optimized for hardware, though they may not be ideal for scenarios with large inputs. The Winograd algorithm reduces convolution complexity by minimizing the number of multiplications required, improving computational efficiency. However, it introduces additional intermediate transformation matrices and cache requirements, leading to higher memory usage. To determine the most optimal acceleration method for a given scenario, we conduct both qualitative and quantitative analyses of these methods, as shown in Table III. The speedups for each method are estimated based on the results presented in the literature. This table provides valuable insights into the strengths and weaknesses of each approach, helping us identify the most suitable method for maximizing acceleration in various use cases. 9 TABLE II COMPARISON OF CNN ACCELERATION METHODS Type Ref. Methods Results Pruning [31] Combined pruning, quantization, and huffman coding. AlexNet parameters reduced: 97.12 VGG-16 parameters reduced: 97.95 [33] Used the rank of the output feature map to determine the ranking of filters. ResNet-110 parameters reduced: 59.2 ResNet-50 parameters reduced: 36.7 [32] Used L1 regularization to the batch normalization layers. VGGNet parameters reduced: 95 [35] Proposed an automatic structured pruning framework. VGG-16 and ResNet-18, parameters reduced: 99.16 . Quantization [38] Centered the weight distribution to zero. YOLOv3 and YOLOv4, parameters reduced: 80 . [41] Used SQNR to find the optimal bit width for each layer. DCNs parameters reduced: 20 [44] Both the filters and the input to convolutional layers are binary. ResNet18 parameters reduced:: 98.5 [43] Proposed a binary sparsity by inference on the original network. ResNet18 speedup: 2.45 Lightweight Networks [45] Introduced the Inception module . Top-1 accuracy on ImageNet: 43.9 Parameters: 5 million [46] Introduced the fire module. Top-1 accuracy on ImageNet: 57.5 Parameters: 4.8 million [47] Introduced depthwise separable convolutions. Top-1 accuracy on ImageNet: 70.6 Parameters: 4.2 million [50] Introduced the ghost module for model compression. Top-1 accuracy on ImageNet: 73.9 Parameters: 5.2 million [51] Proposed a new partial convolution (PConv). Top-1 accuracy on ImageNet: 76.2 Parameters: 7.6 million FFT [62] Developed overlap-and-add technique. AlexNet speedup: 10.6 VGG16 speedup: 7.4 [63] Introduced spectral activation functions. Speedup compared to spatial methods: 4 6.6 Speedup compared totraditional FFT: 3 4.4 [64] Introduced SReLU activation function. Speedup compared to spatial methods: 3.45 Winograd [66] Used winograd to accelerate convolution operations. Speedup: 1.63 7.42 [67] Developed a hardware framework for w inograd algorithm. AlexNet speedup: 11.8 [68] Introduced the winograd algorithm to more dimensions. Speedup: 1.44 2.42 TABLE III SUMMARY AND QUANTITATIVE ANALYSIS OF THE REVIEWED METHODS Methods Cheaper Arithmetic Operations Memory Reduction Parameters Reduction Hardware Friendly Speedup Pruning 2 10 Quantization 2 50 Lightweight Networks 8 9 Knowledge Distillation 1.5 5 Layer Fusion 2 FFT 5 10 Winograd 3 5 V. HARDWARE APPROACHES This section introduces the key techniques commonly used for CNN hardware acceleration, followed by an overview of optimization strategies that build upon these techniques to enhance performance. A. Data Blocking and Parallel Computing 1) Loop Tiling: Loop tiling, also known as data blocking, is a critical optimization technique in hardware acceleration [69], [70], especially for CNN models that involve large-scale data processing. Its main objective is to minimize memory access overhead and maximize hardware resource utilization by enhancing cache efficiency. In convolutional nested loops, large input feature maps are typically processed element by element. While this method is straightforward, it often leads to frequent cache misses, as large portions of data do not remain in the processor s cache. This results in excessive memory accesses, which degrade overall performance. Loop tiling addresses this issue by dividing large loops into smaller, more manageable tiles, as shown in Code 1, where Ti and Tj denote tiling coefficients for different loop levels. By processing only a subset of data at a time, loop tiling ensures that each tile remains in the cache, 10 Code 1 Tiling Convolutional Layer Algorithm 1: for (f 0 ; f F ; f ) 2: for (c 0 ; c C ; c ) 3: for (h 0 ; h Ho ; h ) 4: for (w 0 ; w Wo ; w ) 5: for (ii 0 ; ii K ; i Ti) 6: for (jj 0 ; jj K ; j Tj) 7: for(i ii ; i ii Ti ; i ) 8: for(j j ; j jj Tj ; j ) 9: O[f][h][w] W[f][c][i][j] I[c][h S i][w S j] significantly improving data locality and reducing memory bandwidth usage. The technique introduces additional outer loops that partition the dataset into smaller chunks. Each chunk is processed independently, allowing it to be efficiently stored in cache and reducing memory access overhead. This segmentation enhances execution speed by minimizing data movement between main memory and processing units. In hardware-accelerated environments, loop tiling is often combined with parallelism and pipelining to further opti- mize performance. In parallel computing architectures such as GPUs, multiple processing units can operate on different tiles concurrently, enabling efficient parallel execution. In FPGAs and custom accelerators, loop tiling can be integrated with hardware-specific architectural features to mitigate memory bottlenecks and enhance overall data throughput. 2) Loop Unrolling: Loop unrolling is a widely used op- timization technique aimed at improving the execution effi- ciency of loops in CNN hardware accelerators, particularly on FPGAs. The key idea is to merge multiple iterations of a loop into a single, larger operation, reducing loop control overhead and increasing parallelism to accelerate execution. In CNN computations, convolution operations often involve deeply nested loops, leading to a high number of iterations when processing large-scale input data. Loop unrolling ad- dresses this inefficiency by transforming a single iteration into multiple parallel operations. By increasing the workload of each iteration, it reduces the need for control logic, such as loop counters and branching decisions, thereby improving instruction execution efficiency and maximizing hardware par- allelism. For instance, in convolution operations, a standard loop structure processes input data elements sequentially. With loop unrolling, multiple elements can be processed in a single iteration, or different convolution tasks can be executed simul- taneously, significantly boosting computational throughput. However, the degree of unrolling must be carefully tuned to match the available hardware resources, such as logic units and memory bandwidth. Excessive unrolling can lead to resource congestion, negatively affecting overall performance. There- fore, in CNN hardware acceleration, loop unrolling must be strategically applied to strike a balance between performance gains and efficient hardware utilization. 3) Parallel Computing: Parallel computing is a fundamen- tal technique in CNN hardware acceleration, particularly for computationally intensive tasks like convolution operations. ... ... F Pf ... ... ... ... ... ... Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Pc Pc K Pv Pk Fig. 1. An illustration depicting the four levels of parallelism in convolution computation across the filter (Pf), channel (Pc), pixel (Pv), and kernel (Pk) dimensions. Convolutions involve numerous multiplications and additions, which can be executed concurrently across multiple processing units, significantly boosting computational speed. In hardware acceleration platforms such as FPGAs and GPUs, convolution operations are naturally suited for parallel execution. By distributing data and tasks across multiple computing units, processing throughput is greatly enhanced. For example, FPGAs can be configured with numerous par- allel computing units to process multiple input data points and convolution kernel elements simultaneously, alleviating the bottleneck of sequential execution. In such architectures, multiplication and summation of input data and convolution kernel elements occur in parallel, fully utilizing the available hardware resources. Additionally, the reconfigurability of FP- GAs allows for the customization of parallel computing archi- tectures tailored to specific CNN models. Techniques such as pipelined data flow processing and hierarchical parallel task scheduling can further enhance CNN inference speed. More- over, loop unrolling, which increases computational workload per iteration while reducing loop control overhead, creates more opportunities for parallel execution, further improving efficiency. Fig. 1 illustrates several parallel computing strategies com- monly used to accelerate convolution operations. Convolution layers consist of multiple nested loops that slide over the kernel and feature maps, offering a vast design space for parallel computing strategies and data caching optimization. These loop transformations have been widely explored in previous research. Expanding different loops directly impacts parallel computing methods, memory access patterns, and convolution engine architectures. To simplify the discussion, we define parallelism in different dimensions as follows: (1) Filter Parallelism (Pf): In CNNs, each input feature map is convolved with multiple filters to generate correspond- ing output feature maps. Traditionally, this process is per- formed sequentially, which underutilizes hardware resources. With filter parallelism, multiple filters operate simultaneously on the same input feature map, generating multiple output 11 channels in parallel. This approach enables efficient reuse of feature data and significantly enhances computational through- put. (2) Channel Parallelism (Pc): CNNs typically process multi-channel input feature maps, such as RGB images (three channels) or deeper feature maps generated by previous con- volution layers. Conventionally, each convolution kernel com- putes results for each channel independently before summing them to produce the final output. Channel parallelism accel- erates this process by simultaneously computing convolutions across all input channels, reducing computational dependen- cies and improving efficiency. This method also facilitates effective reuse of weight data. (3) Pixel Parallelism (Pv): In convolution operations, kernels slide across input feature maps to compute convolution results at different spatial positions. Since these computations at different positions are largely independent, they can be pro- cessed in parallel. Pixel parallelism exploits this independence by computing multiple spatial locations simultaneously. While this approach simplifies data storage, it requires multiple levels of buffering for data shifting in on-chip cache design due to the sliding window nature of convolutions. (4) Kernel Parallelism (Pk): Kernel parallelism extends pixel parallelism to the convolution kernel dimension. It pro- cesses multiple weight values for a single feature pixel in parallel. However, this method is often constrained by kernel size and presents challenges in data storage design, making it less flexible for large-scale CNN architectures. B. CNN Hardware Accelerators based on Parallel Computing This section explores and analyzes the integration of data- level parallel computing strategies in the design of CNN hard- ware accelerators, as discussed in existing literature. Various parallelization techniques are employed in different combina- tions to optimize accelerator architectures [6], [7], [14], [28], [71] [84]. This work presents a detailed comparison of various parallelization strategies, evaluating their computational effi- ciency and summarizing key performance metrics. The goal is to provide insights into the latest advancements in CNN hardware acceleration, enabling researchers, developers, and engineers to better understand the trade-offs among different approaches. By examining the characteristics and capabilities of diverse parallel strategies, readers can design optimized ac- celeration solutions tailored to specific tasks. A comprehensive summary of performance comparisons is provided in Table IV. For example, Zhao et al. [72] and Liu et al. [71] imple- mented a combination of kernel, pixel, and filter parallelism (Pk, Pv, Pf) to accelerate CNN computations. Liu et al. achieved a processing performance of 107 Gop s on a Zynq ZC7045 FPGA with 16-bit data quantization, reaching an effi- ciency of 0.12 Gop s per DSP. While this approach delivered high computational throughput, it suffered from inefficient resource utilization, ultimately limiting overall accelerator efficiency. Similarly, Rahman et al. [73] and Ma et al. [74] adopted a hybrid strategy combining pixel parallelism (Pv) and filter parallelism (Pf). In their implementation of ResNet-50 on Intel FPGA Stratix V and Arria 10, Ma et al. achieved a throughput of 315.5 Gop s. This method maximized data reuse and improved computational efficiency. However, it introduced challenges in hardware resource utilization, particularly when convolution layer dimensions were not divisible by Pv, leading to underutilized computational units in certain CNN layers. Unlike Pk and Pv, which are constrained by kernel size and feature characteristics, the combined approach of Pc and Pf offers greater flexibility [6], [28], [75] [78]. For example, [28] achieves energy efficiency 3.8 5.6 higher than GPUs and up to 1.4 2.2 better resource efficiency than FPGA-based accelerators for 2D and 3D CNNs, with an overall efficiency of 50.47 . Some studies integrate pixel, channel, and filter parallelism (Pv, Pc, Pf) [79], achieving high throughput. For example, [79] reported 2870 Gop s peak performance for convolution operations. Channel parallelism enables large-scale parallelism more efficiently than pixel parallelism, improving throughput. Other works employ (Pk, Pc, Pf) [14], [80] [82], extending parallelism to additional dimensions for better performance. However, Pc and Pf parallelism face constraints; for example, the first convolution layer typically processes RGB channels, but Pc often scales exponentially by 2, leading to computa- tional imbalances and reduced efficiency. Despite various parallel strategies, architectures exhibit sig- nificant differences in compute efficiency due to fixed paral- lelism in each data dimension, resulting in resource under- utilization and limited adaptability. To address this, some works propose dynamic parallelism [7], [83], [84]. For exam- ple, DCP-CNN [7] dynamically adjusts parallelism based on input size, kernel size, and network configuration, achieving over 800 Gop s throughput and 72 98 compute effi- ciency on Intel Stratix 10 GX650 FPGA, outperforming exist- ing accelerators. Dynamic parallelism optimizes FPGA-based CNN acceleration by tailoring parallel strategies to each layer, preventing load imbalance and enhancing efficiency. However, it introduces design complexity, requiring adaptive data storage and on-chip buffering to handle transitions between parallel strategies. Additionally, varying network parameters pose chal- lenges in identifying optimal configurations. C. Other Optimizations 1) Input Reshaping: Input reshaping is a key optimiza- tion in CNN hardware accelerators that enhances compute efficiency by reorganizing data storage and access patterns to better align with hardware architectures [85] [88]. It ensures efficient data alignment for convolution operations, facilitating optimized memory access and computation. A common approach, Im2Col [89], converts feature maps into matrix form, enabling efficient matrix multiplications instead of direct convolution operations. Additionally, input re- shaping can partition data into cache-friendly blocks, reducing memory accesses and improving processing efficiency. Format conversions, such as between NCHW and NHWC, further optimize performance across different hardware architectures. For sparse inputs, compressed storage formats enhance sparse matrix computation efficiency. While input reshaping improves 12 TABLE IV COMPARISON OF HARDWARE PERFORMANCE OF EXISTING CNN ACCELERATORS USING DIFFERENT PARALLELISM Ref. Employed Parallelism Latency (ms) Throughput (Gop s) Compute EFF. Resource EFF. (Gop s DSP) Clock EFF. Overall EFF. Power (W) Energy EFF. (Gop s W) [71] Pk, Pv, Pf - 107 41.7 0.12 40 16.6 9.6 11.14 [72] 3.7 391.6 - - 40 - 178 2.19 [73] Pv, Pf - 147.82 17.14 0.11 26.7 4.24 - - [74] 71.71 315.48 69.28 0.21 33.3 20.6 - - [28] Pc, Pf 4.62 1667 92.05 1.24 44 36.1 45 37 [6] 5.07 1519 90 1.0 44 54 19.1 79.5 [75] 21.61 61.62 69.1 0.03 20 11.04 18.61 3.31 [76] 47.97 645.25 68.58 0.43 30 20.57 - - [77] - 1218 95 0.76 40 23.18 17.7 68.81 [78] - 1240.8 - 0.32 28.6 - 13.38 92.4 [79] Pv, Pc, Pf - 2870 92.2 0.79 38 24.88 39.5 72.7 [14] Pk, Pc, Pf 364 84.3 80.25 0.17 26.7 18.58 3.5 24 [80] 2.56 565.94 84.6 0.26 31.2 15.84 30.2 18.74 [81] 20.1 136.5 - 0.56 24 - 24.2 5.64 [82] 0.62 2423.63 - 0.89 48.2 - 18.04 143.32 [83] Dynamic - 129.7 41.8 0.13 30 4.63 18 7.2 [84] 178 684 49.47 0.2 40 19.39 26 26.3 [7] 38.3 807 98.5 0.79 40 35.03 6.3 128.1 memory utilization and supports parallelism, it may introduce computational and storage overhead. Effective integration with hardware design is essential to balance reshaping costs with performance gains. 2) DSP Optimization: CNN accelerators benefit from re- duced precision due to their tolerance to small numerical errors, making low-precision computing a key strategy for improving performance. However, CNN operations rely on DSP units, which support limited precision levels, posing challenges for precision reduction. To address this, researchers have explored executing multiple low-precision MAC opera- tions within a single DSP unit to enhance efficiency. Lee et al. [90] introduced a 2-way SIMD MAC design for CNN convolutions, where SIMD multiplications share a common operand, enabling efficient dual MAC operations in a single DSP block. Similarly, Liu et al. [6] proposed an INT8 optimization technique for Intel DSPs, utilizing an 18 19 multiplier to achieve a 1:2 DSP-to-INT8 MAC ratio. By partitioning INT8 computations into smaller multiplications, they optimized DSP usage while leveraging logic resources for additional operations. D. Discussion based on Our Evaluation Framework In this section, we provide a comparison of various works based on different parallel combinations, as shown in Table IV. A key observation from existing research is that many studies evaluate their designs using a limited set of metrics, such as throughput, power consumption, and latency. However, these metrics are not solely influenced by the design itself; they are also highly dependent on the FPGA device used, as per- formance can vary across different CNN models. To address this, we emphasize the importance of compute efficiency and overall efficiency. These metrics, included in Table IV based on reported design parameters, offer a more comprehensive understanding of the evaluated works. Some entries are left blank because the results were not provided in the respective papers. Compute efficiency is closely tied to the number of MAC operations in the design and is relatively unaffected by FPGA device choice. We calculate and summarize compute efficiency from various hardware architectures, such as [73] (17.14 ) and [7] (98.5 ). A key conclusion from our comparison is that parallelization strategy plays a significant role in determining compute efficiency. For example, [73] used a fixed paral- 13 Fig. 2. Energy efficiency comparison across CNN accelerator architectures. The left vertical axis represents energy efficiency, while the right vertical axis shows power consumption. lelization strategy with constant parallelism degrees (Pv, Pf), which can lead to load imbalances and resource inefficiency. In contrast, [7] dynamically adjusted the parallelization strategy based on the layer being executed, optimizing resource usage and achieving higher compute efficiency. Given these findings, future research should focus on leveraging dynamic parallelism to maximize performance. However, implementing dynamic parallelism introduces de- sign complexities, such as the need for flexible data storage schemes and on-chip buffers to adapt to different parallel strategies. Failure to properly manage these transitions may degrade data transfer efficiency and overall performance. Overall efficiency builds upon compute efficiency but also considers resource utilization and clock efficiency. One key method to improve resource utilization is DSP optimization, which maximizes the use of limited DSP resources. For example, [6] demonstrated high overall efficiency largely due to effective DSP optimization. Improving clock efficiency requires careful attention to pipelining, timing optimization, and resource sharing. We also compare the energy efficiency across designs, shown in Fig. 2. Few designs achieve high energy efficiency with low power consumption; for most, energy efficiency is directly proportional to power consumption, meaning higher power usage leads to higher throughput. Thus, research efforts should focus on balancing low power consumption with high throughput. Additionally, we conduct a comprehensive comparison of latency, throughput, and power consumption for architectures based on different parallelism combinations. As shown in Fig. 3, high throughput and low latency are generally associ- ated with higher power consumption, while low-power designs tend to have lower throughput and higher latency. Future work should focus on optimizing architectures to enhance throughput while maintaining low power consumption. VI. ALGORITHM-HARDWARE CO-DESIGN To meet the growing complexity and accuracy demands of neural architectures, the number of parameters in CNN models has increased significantly. While this boosts performance, it also raises computational and memory requirements, leading to higher time and energy consumption during training and inference. Edge computing platforms, such as mobile and embedded devices, face challenges due to limited processing power, storage, and energy efficiency, making it difficult to deploy these resource-intensive models, especially in real-time applications. Optimizing only from the hardware or software perspective is not enough to address these challenges. Algorithm-hardware co-design integrates both aspects, tailoring algorithms to lever- age hardware features while adapting hardware architectures to meet algorithmic needs. This approach maximizes perfor- mance, resource utilization, and energy efficiency by optimiz- ing the synergy between software and hardware. A. CNN Acceleration Toolflows Toolflows represent a system-level approach to automating the mapping of CNNs onto FPGAs. These toolflows streamline the design process, ensuring rapid deployment and high energy efficiency. As application needs and hardware capabilities evolve, CNN toolflows have adapted to drive continuous innovation in both algorithms and hardware. They encompass a wide range of technologies, from deep learning frame- works to hardware acceleration, automated optimization, and explainability analysis. These tools enable users to generate customized CNN hardware implementations without requiring deep hardware expertise, making FPGA integration more ac- cessible within the deep learning ecosystem. Table V presents various CNN-to-FPGA toolflows that leverage the algorithm- hardware co-design framework to generate optimized FPGA accelerators tailored to specific CNN-FPGA pairs. fpgaConvNet [91] utilized specialized hardware blocks to efficiently map irregular data flows like Inception, Residual, and Dense blocks. It explores the design space based on the Synchronous Data Flow (SDF) model, accounting for platform resource constraints. Several optimizations have been proposed for fpgaConvNet, including a latency-driven design [92] and a 14 Throughput (Gop s) Power (W) Fig. 3. An illustrative comparison of the hardware performance of existing CNN accelerators in terms of power consumption, latency, and throughput. The size of the bubbles corresponds to latency, with larger bubbles indicating higher latency and smaller ones indicating lower latency. The horizontal axis represents power consumption, increasing from left to right, while the vertical axis represents throughput, increasing from bottom to top. heuristic method for pruning the architecture search space. Ve- nieris et al. [93] later extended this framework to handle CNNs with irregular connectivity, treating design space exploration (DSE) as a Multi-Objective Optimization (MOO) problem to address throughput and latency requirements. In contrast, Angel-Eye [14] and Snowflake [94] adopted a flexible computing engine design, allowing the reuse of hard- ware across different CNNs without recompilation. Snowflake further enhanced performance with double buffering to overlap computation and communication, hiding external memory latency. f-CNNx [95] enabled mapping multiple CNNs onto a single FPGA, optimizing resource allocation and memory bandwidth to reduce contention and improve performance. It used a multi-objective cost function for efficient map- ping. FMM-X3D [96] optimized 3D-CNNs for human action recognition (HAR) on FPGAs, while HARFLOW3D [97] introduced runtime parameter reconfiguration to avoid full bitstream reconfiguration, improving latency. fpgaHART [98] extended the SDF model to manage irregular blocks with branching structures in 3D-CNN HAR models. Pflow [23] de- coupled hardware details with an Overlay library and employs adaptive scheduling and operator fusion to maximize hardware utilization, enabling end-to-end acceleration from application to hardware. B. Design Space Exploration Design Space Exploration (DSE) involves systematically optimizing design parameters within set constraints. To maxi- mize FPGA capabilities, factors such as network architecture, hardware resources, memory bandwidth, and latency must be considered. Optimizing these parameters enhances CNN inference while making full use of FPGA hardware features. Table VI compares various DSE methods, including brute- force and heuristic approaches. 1) Brute-Force Method: The Brute-Force method [99] exhaustively searches all possible solutions to find the global optimum. While it guarantees the best result, it is computa- tionally expensive and time-consuming, making it impractical for large-scale optimizations. 2) Simulated Annealing: Simulated Annealing (SA) [93], [99] uses probabilistic jumping to explore the solution space, gradually lowering the search temperature to converge to the global optimum. While it prevents local optima, its conver- gence is slow, making it less efficient for large-scale searches. 3) Rule-Based Method: The Rule-Based method [99] uses a deterministic approach, optimizing variables starting from a minimum resource state. It is faster than SA but may be limited by memory bandwidth constraints, impacting performance. 4) Genetic Algorithm: The Genetic Algorithm (GA) [100], [101] simulates natural selection through selection, crossover, and mutation. It iteratively improves solutions based on fitness functions. GA is highly parallelizable and effective for global search but suffers from slow convergence and the risk of premature local optima. 5) Other Methods: Biggs et al. [102] used probability profiles to optimize network phases based on throughput and area trade-offs. [71] applied Fmincon for optimizing CNN accelerators, although its results can vary with initial condi- tions. [103] introduced a Genetic Simulated Annealing (GSA) method, combining the strengths of GA and SA to explore a broader solution space with reduced computational time and fewer parameter dependencies. C. Performance and Resource Modeling The DSE process typically uses two approximate models: resource and performance models. The resource model esti- mates the hardware requirements for a given architecture on the target FPGA, while the performance model predicts system 15 TABLE V SUMMARY AND COMPARISON OF EXISTING CNN ACCELERATION TOOLFLOWS Ref. Toolflow Name Type Device Throughput (Gop s) Energy EFF. (Gop s W) Resource EFF. (GOp s DSP) [91] fpgaConvNet Streaming Structure Xilinx XC7Z020 12.73 7.27 - [14] Angel-Eye Single Engine Xilinx XC7Z045 XC7Z020 XC7Z045: 137 XC7Z020: 84.3 XC7Z045: 14.2 XC7Z020: 24.1 - [94] Snowflake Single Engine Xilinx XC7Z045 GoogLeNet: 116.5 GoogLeNet: 12.3 - [92] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 134.10 VGG16: 123.12 - AlexNet: 0.18 VGG16: 0.14 [93] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 197.40 VGG16: 155.81 GoogLeNet: 165.30 ResNet-152: 188.18 DenseNet-161: 155.57 AlexNet: 49.35 VGG16: 38.95 GoogLeNet: 41.32 ResNet-152: 47.04 DenseNet-161: 38.9 AlexNet: 0.22 VGG16: 0.17 GoogLeNet: 0.184 ResNet-152: 0.209 DenseNet-161: 0.173 [95] f-CNNx Streaming Structure Xilinx XC7Z045 VGG16: 75.00 VGG16 : 18.75 - [96] FMM-X3D Streaming Structure Xilinx ZCU102 X3D-M: 119.83 - X3D-M: 0.055 [97] HARFLOW3D Streaming Structure Xilinx ZCU102 VC709 ZCU102: C3D: 393.37 Slowonly: 177.05 R(2 1)D-18: 173.91 (R(2 1)D-34): 184.29 X3D-M: 43.78 VC709: C3D: 424.14 Slowonly: 229.01 R(2 1)D-18: 185.13 (R(2 1)D-34: 206.39 X3D-M: 56.14 - ZCU102: C3D: 0.156 Slowonly: 0.07 R(2 1)D-18: 0.069 (R(2 1)D-34: 0.073 X3D-M: 0.017 VC709: C3D: 0.117 Slowonly: 0.063 R(2 1)D-18: 0.051 (R(2 1)D-34: 0.057 X3D-M: 0.015 [98] fpgaHART Streaming Structure Xilinx ZCU102 C3D: 130.84 Slowonly: 144.44 R(2 1)D-18: 39.59 (R(2 1)D-34: 34.26 X3D-M: 85.96 - C3D: 0.052 Slowonly: 0.057 R(2 1)D-18: 0.015 (R(2 1)D-34: 0.013 X3D-M: 0.034 [23] Pflow Streaming Structure Xilinx XCZU3EG XCVU13P XCZU3EG: 272.64 XCVU13P: 3686.4 XCZU3EG: 46.5 XCVU13P: 59.4 XCZU3EG: 0.71 XCVU13P: 0.6 TABLE VI COMPARISON OF EXISTING DSE METHODS DSE Method Local Optimum Large Parameter Space Real-Time Low Complexity Low Parameter Dependency Memory Bandwidth Limitations BruteForce Simulated Annealing Rule-Based Method Genetic Algorithm performance (e.g., latency) based on hardware configuration and algorithm characteristics. These models replace the time- consuming task of running CNNs on actual hardware by providing estimates based on hardware parameters. Analytical prediction formulas are often used for simplicity and integra- tion into DSE optimization loops, compared to more time- intensive full simulations like ModelSim [15]. 1) Performance Modeling: Liu et al. [6] combined Gaus- sian Process Regression (GPR) [104] with analytical formu- las [71] to estimate CNN latency on accelerators. Makrani et al. [105] introduced the Pyramid framework, using the Minerva tool and ensemble learning to improve throughput prediction accuracy, achieving over 95 accuracy. The In- tel FPGA Power and Thermal Tool [106] estimated power consumption based on FPGA model, frequency, and resource usage, though discrepancies may occur. PowerGear [107] used a Heterogeneous Edge-Centered Graph Neural Network (HEC- GNN) to predict power consumption efficiently. 2) Resource Modeling: Dai et al. [108] developed a dataset with over 1,300 samples to predict resource usage and timing during HLS with machine learning models. Koeplinger et al. [109] introduced DHDL, a hardware representation method that used parameterized templates to capture locality and parallelism in designs. Lee et al. [110] presented a framework 16 for fast, accurate resource estimation by combining high-level hardware descriptions with low-level performance models, offering cycle-level timing and power estimates with high simulation speed. VII. DISCUSSION AND CONCLUSION A. Discussion Insights and optimization: Based on the analysis presented in previous sections, several key challenges in CNN accel- erator design have been identified, along with corresponding optimization strategies to address these challenges. These insights can guide future research and development efforts in enhancing the performance and efficiency of CNN hardware accelerators. 1) Clock Efficiency Optimization. One of the primary concerns in FPGA-based CNN accelerators is maximizing clock efficiency. FPGAs inherently offer fine-grained control over clock frequencies, which can be leveraged to optimize resource utilization. Dynamic frequency scaling, where high clock frequencies are assigned to computation-heavy tasks (e.g., convolution layers) and lower frequencies to less inten- sive operations, helps minimize energy consumption without sacrificing performance. Additionally, optimizing clock net- work design and clock gating reduces unnecessary switching activity by deactivating idle modules, contributing further to power savings. Pipelining techniques are also employed to reduce critical path delays, ensuring that the FPGA operates efficiently at higher frequencies while maintaining stable syn- chronization. In multi-clock domain systems, asynchronous FIFOs or synchronous bridges are utilized to prevent metasta- bility, enhancing stability and performance across different clock domains. 2) Power Efficiency Optimization. Power consumption is a key challenge, particularly in edge applications where energy efficiency is critical. FPGA-based CNN accelerators employ several techniques to reduce power usage. Weight pruning and feature map sparsification lower the computational load, cutting dynamic power. Low-bit-width quantization reduces both memory usage and computation time, further conserving power. On-chip storage and computation fusion minimize power-hungry off-chip memory accesses. Power management methods like dynamic voltage and frequency scaling (DVFS) adjust power and frequency based on workload, while clock gating shuts down idle modules. Architectural strategies such as deep pipelining, parallelism, and reconfigurable units im- prove both computation and energy efficiency. Choosing low- power FPGA devices and optimizing architecture to remove non-essential resources helps minimize static power, making FPGAs ideal for low-power, real-time applications. Future directions: Several promising directions for future research in CNN hardware acceleration can significantly en- hance the performance, efficiency, and versatility of hardware accelerators, especially in the context of emerging applications in AI, computer vision, and edge computing. The following are key areas for continued exploration: 1) Heterogeneous Computing Architectures. Integrating multiple computing units (CPU, GPU, FPGA, ASIC) on a single chip could significantly enhance resource utilization and energy efficiency for CNN tasks. Key technologies like opti- mized Network-on-Chip (NoC) and intelligent task scheduling will drive this development, though challenges in hardware complexity and software support remain. 2) AI-Driven Design Space Exploration (DSE). As CNN models continue to grow in complexity, the need for efficient design space exploration (DSE) methods becomes increasingly important. Traditional DSE methods often require exhaustive searching of vast design spaces, which is time-consuming and computationally expensive. Future research should focus on integrating machine learning techniques, such as reinforcement learning or neural architecture search, into the DSE process. AI-driven DSE can dynamically explore design spaces based on performance metrics, such as throughput, latency, and power consumption, enabling the rapid discovery of opti- mal hardware configurations. By automating this process, researchers can significantly reduce design time and enhance the ability to tailor hardware accelerators to specific CNN models and tasks. 3) Scalable and Adaptive CNN Hardware Accelerators. As CNN models continue to grow in size and complexity, ac- celerators must be able to scale efficiently. Scalable hardware accelerators capable of dynamically adapting to different CNN architectures and input sizes are essential. Future research could focus on developing adaptive FPGA architectures that allow for dynamic reconfiguration based on the model being run. These accelerators would dynamically adjust resources (e.g., memory, processing units) and parallelism levels to accommodate the requirements of different layers in the CNN. Such adaptive systems could support a broader range of CNN applications, from small, lightweight models to large, deep networks, without requiring a complete redesign of the hardware. B. Conclusion CNNs are integral to AI-driven applications, but the chal- lenge remains to achieve low-cost, low-latency, and high- performance solutions. This paper reviewed various hardware acceleration platforms, comparing their strengths and weak- nesses. We also discussed key evaluation metrics for both software and hardware, such as accuracy, latency, and through- put. By examining popular CNN acceleration techniques, we identified the advantages and limitations of each approach. Our evaluation emphasized the importance of computational efficiency and dynamic parallel computing in optimizing CNN hardware accelerators. As the field continues to evolve, future research should focus on dynamic parallel computing and heterogeneous on-chip computing for CNN acceleration. With increasing design complexity, effective design space explo- ration will also become crucial. ACKNOWLEDGEMENT The support of Scientific Research Foundation of Hu- nan Provincial Education Department (Key project 23A0087) and Changsha Municipal Natural Science Foundation (No. kq2502001) is gratefully acknowledged. 17 REFERENCES [1] V. K. Chauhan, J. Zhou, P. Lu, S. Molaei, and D. A. Clifton, A brief review of hypernetworks in deep learning, Artificial Intelligence Review, vol. 57, no. 9, p. 250, 2024. [2] L. Chen, S. Li, Q. Bai, J. Yang, S. Jiang, and Y. Miao, Review of im- age classification algorithms based on convolutional neural networks, Remote Sensing, vol. 13, no. 22, p. 4712, 2021. [3] F. Ashiq et al., Cnn-based object recognition and tracking system to assist visually impaired people, IEEE Access, vol. 10, pp. 14 819 14 834, 2022. [4] S. Gidaris and N. Komodakis, Object detection via a multi-region and semantic segmentation-aware cnn model, in Proc. ICCV, 2015, pp. 1134 1142. [5] A. Wang, H. Chen, Z. Lin, J. Han, and G. Ding, Repvit: Revisiting mobile cnn from vit perspective, in Proc. CVPR, 2024, pp. 15 909 15 920. [6] S. Liu, H. Fan, M. Ferianc, X. Niu, H. Shi, and W. Luk, Toward full- stack acceleration of deep convolutional neural networks on fpgas, IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 8, pp. 3974 3987, 2021. [7] K. Dai, Z. Xie, and S. Liu, DCP-CNN: Efficient Acceleration of CNNs With Dynamic Computing Parallelism on FPGA, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [8] K. Hegde, R. Agrawal, Y. Yao, and C. W. Fletcher, Morph: Flexible acceleration for 3d cnn-based video understanding, in Proc. MICRO, 2018, pp. 933 946. [9] D. Moolchandani, A. Kumar, and S. R. Sarangi, Accelerating CNN inference on ASICs: A survey, Journal of Systems Architecture, vol. 113, p. 101887, 2021. [10] D. Gyawali, Comparative analysis of cpu and gpu profiling for deep learning models, arXiv preprint arXiv:2309.02521, 2023. [11] Q. Zhang, M. Zhang, T. Chen, Z. Sun, Y. Ma, and B. Yu, Recent ad- vances in convolutional neural network acceleration, Neurocomputing, vol. 323, pp. 37 51, 2019. [12] S. F. Beldianu and S. G. Ziavras, Asic design of shared vector accelerators for multicore processors, in 2014 IEEE 26th International Symposium on Computer Architecture and High Performance Comput- ing, 2014, pp. 182 189. [13] A. Jose, K. T. Alense, L. Gijo, and J. Jacob, FPGA Implementation of CNN Accelerator with Pruning for ADAS Applications, in 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), 2024, pp. 1 6. [14] K. Guo et al., Angel-eye: A complete design flow for mapping cnn onto embedded fpga, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 37, no. 1, pp. 35 47, 2017. [15] S. Liu and W. Luk, Towards an efficient accelerator for dnn-based remote sensing image segmentation on fpgas, in Proc. FPL, 2019, pp. 187 193. [16] J. Wen, Y. Ma, and Z. Wang, An efficient fpga accelerator optimized for high throughput sparse cnn inference, in 2020 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS), 2020, pp. 165 168. [17] E. Nurvitadhi et al., Accelerating Binarized Neural Networks: Com- parison of FPGA, CPU, GPU, and ASIC, in Proc. FPT, 2016, pp. 77 84. [18] M. Buckland and F. Gey, The relationship between recall and preci- sion, Journal of the American society for information science, vol. 45, no. 1, pp. 12 19, 1994. [19] R. Yacouby and D. Axman, Probabilistic extension of precision, recall, and f1 score for more thorough evaluation of classification models, in Proceedings of the first workshop on evaluation and comparison of NLP systems, 2020, pp. 79 91. [20] X. Li, Z. Xie, X. Deng, Y. Wu, and Y. Pi, Traffic sign detection based on improved faster r-cnn for autonomous driving, The Journal of Supercomputing, pp. 1 21, 2022. [21] C. Hema and F. P. G. Marquez, Emotional speech recognition using cnn and deep learning techniques, Applied Acoustics, vol. 211, p. 109492, 2023. [22] C. Jiang, D. Ojika, B. Patel, and H. Lam, Optimized fpga-based deep learning accelerator for sparse cnn using high bandwidth memory, in Proc. FCCM, 2021, pp. 157 164. [23] Y. Wan, X. Xie, L. Yi, B. Jiang, J. Chen, and Y. Jiang, Pflow: An end-to-end heterogeneous acceleration framework for cnn inference on fpgas, Journal of Systems Architecture, vol. 150, p. 103113, 2024. [24] S. M. Nabavinejad et al., An overview of efficient interconnection networks for deep neural network accelerators, IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 10, no. 3, pp. 268 282, 2020. [25] S. Li et al., An efficient cnn accelerator using inter-frame data reuse of videos on fpgas, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 30, no. 11, pp. 1587 1600, 2022. [26] W. Huang et al., Fpga-based high-throughput cnn hardware accelerator with high computing resource utilization ratio, IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 8, pp. 4069 4083, 2022. [27] E. Wu, X. Zhang, D. Berman, I. Cho, and J. Thendean, Compute- efficient neural-network acceleration, in Proc. FPGA, 2019, pp. 191 200. [28] H. Fan, S. Liu, Z. Que, X. Niu, and W. Luk, High-performance acceleration of 2-d and 3-d cnns on fpgas using static block floating point, IEEE Transactions on Neural Networks and Learning Systems, vol. 34, no. 8, pp. 4473 4487, 2021. [29] W. Jiang, H. Yu, F. Chen, and Y. Ha, Aos: An automated overclocking system for high-performance cnn accelerator through timing delay measurement on fpga, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 42, no. 9, pp. 2952 2965, 2023. [30] M. Parker, Understanding peak floating-point performance claims, Technical White Paper WP-012220-1.0, 2014. [31] S. Han, H. Mao, and W. J. Dally, Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, arXiv preprint arXiv:1510.00149, 2015. [32] Z. Liu et al., Learning efficient convolutional networks through network slimming, in Proc. ICCV, 2017, pp. 2736 2744. [33] M. Lin et al., Hrank: Filter pruning using high-rank feature map, in Proc. CVPR, 2020, pp. 1529 1538. [34] S.-K. Yeom et al., Pruning by explaining: A novel criterion for deep neural network pruning, Pattern Recognition, vol. 115, p. 107899, 2021. [35] N. Liu et al., Autocompress: An automatic dnn structured pruning framework for ultra-high compression rates, in Proc. AAAI, vol. 34, no. 04, 2020, pp. 4876 4883. [36] H. Tan, S. Wu, F. Du, Y. Chen, Z. Wang, F. Wang, and X. Qi, Data pruning via moving-one-sample-out, Advances in Neural Information Processing Systems, vol. 36, 2024. [37] D. T. Nguyen, T. N. Nguyen, H. Kim, and H.-J. Lee, A high- throughput and power-efficient fpga implementation of yolo cnn for object detection, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 27, no. 8, pp. 1861 1873, 2019. [38] S. Kim and H. Kim, Zero-centered fixed-point quantization with iterative retraining for deep convolutional neural network-based object detectors, IEEE Access, vol. 9, pp. 20 828 20 839, 2021. [39] S. Jain, A. Gural, M. Wu, and C. Dick, Trained quantization thresh- olds for accurate and efficient fixed-point inference of deep neural networks, Proceedings of Machine Learning and Systems, vol. 2, pp. 112 128, 2020. [40] Q. Jin et al., F8net: Fixed-point 8-bit only multiplication for network quantization, arXiv preprint arXiv:2202.05239, 2022. [41] D. Lin, S. Talathi, and S. Annapureddy, Fixed point quantization of deep convolutional networks, in Proc. PMLR, 2016, pp. 2849 2858. [42] R. Li et al., Fully quantized network for object detection, in Proc. CVPR, 2019, pp. 2810 2819. [43] S. Cao et al., Seernet: Predicting convolutional neural network feature- map sparsity through low-bit quantization, in Proc. CVPR, 2019, pp. 11 216 11 225. [44] M. Rastegari et al., Xnor-net: Imagenet classification using binary convolutional neural networks, in Proc. ECCV, 2016, pp. 525 542. [45] C. Szegedy et al., Going deeper with convolutions, in Proc. CVPR, 2015, pp. 1 9. [46] F. N. Iandola, SqueezeNet: AlexNet-level accuracy with 50x fewer pa- rameters and 0.5 MB model size, arXiv preprint arXiv:1602.07360, 2016. [47] A. G. Howard, Mobilenets: Efficient convolutional neural networks for mobile vision applications, arXiv preprint arXiv:1704.04861, 2017. [48] M. Sandler et al., Mobilenetv2: Inverted residuals and linear bottle- necks, in Proc. CVPR, 2018, pp. 4510 4520. [49] A. Howard et al., Searching for mobilenetv3, in Proc. ICCV, 2019, pp. 1314 1324. [50] K. Han et al., Ghostnet: More features from cheap operations, in Proc. CVPR, 2020, pp. 1580 1589. [51] J. Chen et al., Run, don t walk: chasing higher flops for faster neural networks, in Proc. CVPR, 2023, pp. 12 021 12 031. [52] G. Hinton, Distilling the knowledge in a neural network, arXiv preprint arXiv:1503.02531, 2015. 18 [53] W. Son, J. Na, J. Choi, and W. Hwang, Densely guided knowledge distillation using multiple teacher assistants, in Proc. ICCV, 2021, pp. 9395 9404. [54] A. Hao, Y. Min, and X. Chen, Self-mutual distillation learning for continuous sign language recognition, in Proc. ICCV, 2021, pp. 11 303 11 312. [55] S. Sun et al., Logit standardization in knowledge distillation, in Proc. CVPR, 2024, pp. 15 731 15 740. [56] M. Alwani, H. Chen, M. Ferdman, and P. Milder, Fused-layer cnn accelerators, in Proc. MICRO, 2016, pp. 1 12. [57] T. Abtahi, C. Shea, A. Kulkarni, and T. Mohsenin, Accelerating convolutional neural network with fft on embedded hardware, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 26, no. 9, pp. 1737 1749, 2018. [58] A. Syafeeza et al., Convolutional neural networks with fused layers applied to face recognition, International Journal of Computational Intelligence and Applications, vol. 14, no. 03, p. 1550014, 2015. [59] S. Liu, H. Fan, and W. Luk, Accelerating fully spectral cnns with adaptive activation functions on fpga, in Proc. DATE, 2021, pp. 1530 1535. [60] M. Mathieu, M. Henaff, and Y. LeCun, Fast training of convolutional networks through ffts, arXiv preprint arXiv:1312.5851, 2013. [61] O. Rippel, J. Snoek, and R. P. Adams, Spectral representations for convolutional neural networks, Advances in neural information processing systems, vol. 28, 2015. [62] H. Zeng et al., A framework for generating high throughput cnn implementations on fpgas, in Proc. FPGA, 2018, pp. 117 126. [63] S. Liu, H. Fan, and W. Luk, Design of fully spectral cnns for efficient fpga-based acceleration, IEEE Transactions on Neural Networks and Learning Systems, 2022. [64] S. O. Ayat, M. Khalil-Hani, A. A.-H. Ab Rahman, and H. Abdellatef, Spectral-based convolutional neural network without multiple spatial- frequency domain switchings, Neurocomputing, vol. 364, pp. 152 167, 2019. [65] T. Watanabe and D. F. Wolf, Image classification in frequency domain with 2srelu: a second harmonics superposition activation function, Applied Soft Computing, vol. 112, p. 107851, 2021. [66] A. Lavin and S. Gray, Fast algorithms for convolutional neural networks, in Proc. CVPR, 2016, pp. 4013 4021. [67] Y. Liang, L. Lu, Q. Xiao, and S. Yan, Evaluating fast algorithms for convolutional neural networks on fpgas, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, no. 4, pp. 857 870, 2019. [68] J. Yepez and S.-B. Ko, Stride 2 1-D, 2-D, and 3-D Winograd for convolutional neural networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 4, pp. 853 863, 2020. [69] M. Yang, S. Cao, W. Zhang, Y. Li, and Z. Jiang, Loop-tiling based compiling optimization for cnn accelerators, in 2023 IEEE 15th International Conference on ASIC (ASICON), 2023, pp. 1 4. [70] H. Huang, X. Hu, X. Li, and X. Xiong, An efficient loop tiling framework for convolutional neural network inference accelerators, IET circuits, devices systems, vol. 16, no. 1, pp. 116 123, 2022. [71] S. Liu et al., Optimizing cnn-based segmentation with deeply cus- tomized convolutional and deconvolutional architectures on fpga, ACM Trans. Reconfigurable Technol. Syst., vol. 11, no. 22, Dec. 2018. [72] R. Zhao et al., Optimizing cnn-based object detection algorithms on embedded fpga platforms, in Proc. ARC, 2017, pp. 255 267. [73] A. Rahman, J. Lee, and K. Choi, Efficient fpga acceleration of convolutional neural networks using logical-3d compute array, in Proc. DATE, 2016, pp. 1393 1398. [74] Y. Ma et al., End-to-end scalable fpga accelerator for deep residual networks, in Proc. ISCAS, 2017, pp. 1 4. [75] C. Zhang et al., Optimizing fpga-based accelerator design for deep convolutional neural networks, in Proc. FPGA, 2015, pp. 161 170. [76] Y. Ma et al., Optimizing loop operation and dataflow in fpga accel- eration of deep convolutional neural networks, in Proc. FPGA, 2017, pp. 45 54. [77] Y. Yu et al., OPU: An FPGA-based overlay processor for convo- lutional neural networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 1, pp. 35 47, 2019. [78] Z. Zhou, X. Duan, and J. Han, A design framework for generating energy-efficient accelerator on fpga toward low-level vision, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2024. [79] T.-H. Wu, C. Shu, and T.-T. Liu, An efficient fpga-based dilated and transposed convolutional neural network accelerator, IEEE Transac- tions on Circuits and Systems I: Regular Papers, 2024. [80] H. Li et al., A high performance fpga-based accelerator for large-scale convolutional neural networks, in Proc. FPL, 2016, pp. 1 9. [81] N. Suda et al., Throughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks, in Proc. FPGA, 2016, pp. 16 25. [82] Z. Li et al., A high-performance pixel-level fully pipelined hardware accelerator for neural networks, IEEE Transactions on Neural Net- works and Learning Systems, 2024. [83] N. Shah, P. Chaudhari, and K. Varghese, Runtime programmable and memory bandwidth optimized fpga-based coprocessor for deep convolutional neural network, IEEE Transactions on Neural Networks and Learning Systems, vol. 29, no. 12, pp. 5922 5934, 2018. [84] F. H. Khan, M. A. Pasha, and S. Masud, Towards designing a hardware accelerator for 3d convolutional neural networks, Computers and Electrical Engineering, vol. 105, p. 108489, 2023. [85] Q. Zhang and X. Zhang, Design of a low-latency general-purpose cnn hardware accelerator based on pulsed arrays on fpgas, in Proc. ICNC-FSKD, 2024, pp. 1 8. [86] J. Li et al., An fpga-based high-throughput keypoint detection acceler- ator using convolutional neural network for mobile robot applications, in 2022 IEEE Asia Pacific Conference on Postgraduate Research in Microelectronics and Electronics (PrimeAsia), 2022, pp. 81 84. [87] H.-Y. Ting et al., System services for reconfigurable hardware accel- eration in mobile devices, in Proc. ReConFig, 2018, pp. 1 6. [88] X. Chen, J. Li, and Y. Zhao, Hardware resource and computational density efficient cnn accelerator design based on fpga, in 2021 IEEE International Conference on Integrated Circuits, Technologies and Applications (ICTA), 2021, pp. 204 205. [89] H. Wang and C. Ma, An optimization of im2col, an important method of cnns, based on continuous address access, in 2021 IEEE Interna- tional Conference on Consumer Electronics and Computer Engineering (ICCECE), 2021, pp. 314 320. [90] S. Lee, D. Kim, D. Nguyen, and J. Lee, Double MAC on a DSP: Boosting the performance of convolutional neural networks on FP- GAs, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 38, no. 5, pp. 888 897, 2018. [91] S. I. Venieris and C.-S. Bouganis, fpgaConvNet: A framework for mapping convolutional neural networks on FPGAs, in Proc. FCCM, 2016, pp. 40 47. [92] , Latency-driven design for fpga-based convolutional neural net- works, in Proc. FPL, 2017, pp. 1 8. [93] , fpgaConvNet: Mapping regular and irregular convolutional neural networks on FPGAs, IEEE transactions on neural networks and learning systems, vol. 30, no. 2, pp. 326 342, 2018. [94] V. Gokhale et al., Snowflake: An efficient hardware accelerator for convolutional neural networks, in Proc. ISCAS, 2017, pp. 1 4. [95] S. I. Venieris and C.-S. Bouganis, f-CNNx: A toolflow for mapping multiple convolutional neural networks on FPGAs, in Proc. FPL, 2018, pp. 381 3817. [96] P. Toupas, C.-S. Bouganis, and D. Tzovaras, FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition, in Proc. ASAP, 2023, pp. 119 126. [97] P. Toupas, A. Montgomerie-Corcoran, C.-S. Bouganis, and D. Tzo- varas, Harflow3d: A latency-oriented 3d-cnn accelerator toolflow for har on fpga devices, in Proc. FCCM, 2023, pp. 144 154. [98] P. Toupas, C.-S. Bouganis, and D. Tzovaras, fpgaHART: A toolflow for throughput-oriented acceleration of 3D CNNs for HAR onto FPGAs, in Proc. FPL, 2023, pp. 86 92. [99] A. Montgomerie-Corcoran, Z. Yu, and C.-S. Bouganis, Samo: Opti- mised mapping of convolutional neural networks to streaming archi- tectures, in Proc. FPL, 2022, pp. 418 424. [100] V. A. Shim, K. C. Tan, and H. Tang, Adaptive memetic computing for evolutionary multiobjective optimization, IEEE transactions on cybernetics, vol. 45, no. 4, pp. 610 621, 2014. [101] Y. Yu, Y. Li, S. Che, N. K. Jha, and W. Zhang, Software-defined design space exploration for an efficient dnn accelerator architecture, IEEE Transactions on Computers, vol. 70, no. 1, pp. 45 56, 2020. [102] B. Biggs, C.-S. Bouganis, and G. Constantinides, ATHEENA: A Toolflow for Hardware Early-Exit Network Automation, in Proc. FCCM, 2023, pp. 121 132. [103] Z. Xie et al., Design space exploration of cnn accelerators based on gsa algorithm, in Proc. ICSIP, 2024, pp. 319 323. [104] C. Williams and C. Rasmussen, Gaussian processes for regression, Advances in neural information processing systems, vol. 8, 1995. [105] H. M. Makrani et al., Pyramid: Machine learning framework to estimate the optimal timing and resource usage of a high-level synthesis design, in Proc. FPL, 2019, pp. 397 403. 19 [106] Intel , Intel FPGA Power and Thermal Calculator User Guide , 2024. [Online]. Available: docs programmable 683445 22-2 overview-of-the.html [107] Z. Lin et al., Powergear: Early-stage power estimation in fpga hls via heterogeneous edge-centric gnns, in Proc. DATE, 2022, pp. 1341 1346. [108] S. Dai et al., Fast and accurate estimation of quality of results in high-level synthesis with machine learning, in Proc. FCCM, 2018, pp. 129 132. [109] D. Koeplinger et al., Automatic generation of efficient accelerators for reconfigurable hardware, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 115 127, 2016. [110] D. Lee, L. K. John, and A. Gerstlauer, Dynamic power and per- formance back-annotation for fast and accurate functional hardware simulation, in Proc. DATE, 2015, pp. 1126 1131. Junye Jiang received the B.Sc. degree from the School of Electrical and Information Engineering, Wuhan Institute of Technology, Wuhan, China, in 2023. He is currently a M.Sc. student in the Computer Vision and High-Performance Computing group at Hunan Normal University. His research interests include the hardware acceleration of con- volutional neural networks (CNNs) and neural ar- chitecture design optimizations applied to computer vision tasks. Yaan Zhou received the B.Sc. degree in Electronic Information Engineering of Central South Univer- sity of Forestry and Technology, Changsha, China, in 2021. He is currently a M.Sc. student in the Computer Vision and High-Performance Computing group at Hunan Normal University. His current research includes the hardware acceleration of con- volutional neural networks (CNNs). Yuanhao Gong received the B.S. degree in Elec- tronic Information Science and Technology from Hunan Normal University, Changsha, China, in 2023. He is currently pursuing the M.S. degree in Electronic Science and Technology with Hunan Nor- mal University. His research interests mainly focus on particle filter methods and efficient hardware architectures for particle filters. Haoxuan Yuan received the B.S. degree in Elec- tronic Information Science and Technology from Hunan Normal University, Changsha, China, in 2024. He is currently a M.Sc. student in the Computer Vision and High-Performance Computing group at Hunan Normal University. His research interests include the reconfigurable acceleration of artificial intelligence algorithms. Shuanglong Liu received the B.Sc. and M.Sc. de- grees from the Department of Electronic Engineer- ing, Tsinghua University, Beijing, China, in 2010 and 2013 respectively, and Ph.D. degree in Electric Engineering from Imperial College London, London, U.K, in 2017. From 2017 to 2020, he was a Research Associate with the Department of Computing, Impe- rial College London. He is currently a Distinguished Professor in the School of Physics and Electronics, Hunan Normal University, Changsha, China. He has published over 40 research papers in peer-referred journals and international conferences. His current research interests include reconfigurable and high performance computing for Convolutional Neural Networks (CNNs) and statistical inference problems.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\n1 FPGA-based Acceleration for Convolutional Neural Networks: A Comprehensive Review Junye Jiang , Yaan Zhou , Yuanhao Gong , Haoxuan Yuan, and Shuanglong Liu Abstract Convolutional Neural Networks (CNNs) are fun- damental to deep learning, driving applications across various domains. However, their growing complexity has significantly increased computational demands, necessitating efficient hard- ware accelerators. Field-Programmable Gate Arrays (FPGAs) have emerged as a leading solution, offering reconfigurability, parallelism, and energy efficiency. This paper provides a compre- hensive review of FPGA-based hardware accelerators specifically designed for CNNs. It presents and summarizes the performance evaluation framework grounded in existing studies and explores key optimization strategies, such as parallel computing, dataflow optimization, and hardware-software co-design. It also compares various FPGA architectures in terms of latency, throughput, compute efficiency, power consumption, and resource utilization. Finally, the paper highlights future challenges and opportunities, emphasizing the potential for continued innovation in this field. Index Terms Convolutional Neural Networks (CNNs), Field Programmable Gate Arrays (FPGAs), Hardware Accelerator, Compute Efficiency, Parallel Computing, Design Space Explo- ration (DSE) I. INTRODUCTION In recent years, Deep Learning (DL) has gained significant attention in computer science [1], with Convolutional Neural Networks (CNNs) serving as a cornerstone of this progress. CNNs have achieved remarkable success across various appli- cations, including image classification [2], object detection [3], and semantic segmentation [4]. Their transformative impact has led to widespread adoption by industry leaders, recog- nizing CNNs as a vital tool for innovation and competitive advantage. The strength of CNNs lies in their ability to process high- dimensional data through convolutional operations, pooling, and non-linear activations, enabling automated multi-level feature extraction. However, this capability comes at the cost of increased computational complexity and resource demands. With their adoption in mobile and edge devices like drones and smartphones [5], CNNs face critical challenges in achieving high performance under resource constraints. To address these challenges, extensive research has been conducted on hardware acceleration for CNNs [6] [9].\n\n--- Segment 2 ---\nWith their adoption in mobile and edge devices like drones and smartphones [5], CNNs face critical challenges in achieving high performance under resource constraints. To address these challenges, extensive research has been conducted on hardware acceleration for CNNs [6] [9]. While CPUs, GPUs and ASICs are commonly employed, their limita- tions in handling the complexity of modern CNN architectures This work was supported in part by the Scientific Research Foundation of Hunan Provincial Education Department (Key project 23A0087) and Changsha Municipal Natural Science Foundation (No. kq2502001). Junye Jiang, Yaan Zhou, Yuanhao Gong, Haoxuan Yuan and Shuanglong Liu are with the Key Laboratory of Low-Dimensional Quantum Structures and Quantum Control of Ministry of Education, School of Physics and Electronics, Hunan Normal University, Changsha, 410081 China. Equal Contribution. Corresponding author: Shuanglong Liu and real-time processing needs have prompted growing inter- est in alternative platforms. Field-Programmable Gate Arrays (FPGAs) have emerged as a promising solution, offering reconfigurability, parallel processing, and energy efficiency. Their ability to customize hardware for specific tasks enables the design of high-performance, resource-efficient accelerators tailored for deep learning applications. This review article explores recent advances in FPGA- based CNN accelerators, focusing on acceleration methods, architectural innovations, hardware optimization techniques, and software-hardware co-design frameworks. It highlights key trends, addresses current challenges, and outlines future directions in this rapidly evolving domain. Additionally, the article examines performance evaluation metrics and provides insights into diverse architectures, offering practical guidance for researchers and practitioners. The main contributions of this work are summarized as follows: Evaluation Metrics: We review FPGA-based CNN ac- celeration research, consolidating key performance met- rics, particularly compute and overall efficiency. It ex- plores the relationships between these metrics, aiding researchers in identifying design limitations and guiding future advancements (Section III); Acceleration Methods: We present a quantitative com- parison of existing CNN acceleration methods, providing practitioners and researchers with valuable insights to select approaches that best align with their specific system requirements (Section IV); Parallel Computing Analysis: This study reviews paral- lel computing methods and their hardware architectures in existing designs, analyzing their strengths and weak- nesses using the introduced evaluation metrics.\n\n--- Segment 3 ---\nThe main contributions of this work are summarized as follows: Evaluation Metrics: We review FPGA-based CNN ac- celeration research, consolidating key performance met- rics, particularly compute and overall efficiency. It ex- plores the relationships between these metrics, aiding researchers in identifying design limitations and guiding future advancements (Section III); Acceleration Methods: We present a quantitative com- parison of existing CNN acceleration methods, providing practitioners and researchers with valuable insights to select approaches that best align with their specific system requirements (Section IV); Parallel Computing Analysis: This study reviews paral- lel computing methods and their hardware architectures in existing designs, analyzing their strengths and weak- nesses using the introduced evaluation metrics. It high- lights the importance of dynamic parallelism in enhancing hardware performance, including compute efficiency and resource utilization (Section V); Future Directions: Building on the comprehensive re- view in this work, we propose and discuss future research directions to help researchers and designers identify key design considerations and potential optimizations in this field (Section VII). The rest part of this paper is organized as follows: Section II compares the acceleration platforms for CNNs, emphasizing the advantages of FPGAs. In Section III, we analyze and outline design performance metrics for FPGA-based CNN accelerators, guiding the comparison of existing designs in subsequent sections. This section also highlights commonly overlooked metrics in the literature and suggests directions arXiv:2505.13461v1 [cs.LG] 4 May 2025 2 for future design and optimization. Section IV presents ac- celeration methods for CNNs at both the algorithmic and hardware levels, with qualitative and quantitative comparisons. Section V reviews existing FPGA-based CNN accelerators, focusing on parallel computing techniques in hardware design. Section VI discusses hardware-software co-design method- ologies, including toolflows, design space exploration, and performance and resource modeling. Section VII concludes this paper with a discussion on future research directions. II. CNN ACCELERATION PLATFORMS This section compares the architectural and performance characteristics of common hardware acceleration platforms, including central processing units (CPUs), graphics processing units (GPUs), application-specific integrated circuits (ASICs), and field-programmable gate arrays (FPGAs). It focuses on their performance in CNN applications, emphasizing that FPGA-based accelerators deliver superior performance for deep learning tasks. A.\n\n--- Segment 4 ---\nIt focuses on their performance in CNN applications, emphasizing that FPGA-based accelerators deliver superior performance for deep learning tasks. A. Hardware Platforms Central Processing Units (CPUs): CPUs are fundamental components in modern computing systems, adhering to the von Neumann architecture. In the context of CNN models, CPUs are well-suited for handling complex logical operations, such as model initialization, data preprocessing, and managing communication between hardware components. They excel in processing diverse data types and executing intricate com- putations independently. However, their limited capacity for parallel processing makes them less efficient for the high- volume matrix operations and convolutions characteristic of CNNs. Additionally, CPUs often face challenges like high power consumption and restricted memory bandwidth, which can become bottlenecks when processing the large-scale data and computations required by CNN models [10]. Graphics Processing Units (GPUs): GPUs have been rapidly developed to handle large-scale computations, particu- larly in applications like image processing and CNN models. Unlike CPUs, GPUs are optimized for parallel computing, capable of processing massive amounts of similar data si- multaneously, making them well-suited for tasks like CNN training, where multiple convolutions and matrix operations can be executed in parallel. The architecture of GPUs, with larger arithmetic logic units, enables faster processing speeds and enhances their parallel computing capabilities. For example, in CNN-based applications, GPUs signifi- cantly speed up tasks such as training deep learning models by accelerating the computation of convolutional layers and weight updates [11]. However, GPUs have limitations in single-threaded performance, which makes them less ideal for tasks requiring complex logic or single-core execution. Additionally, GPUs face challenges such as high power con- sumption, substantial cooling needs, and limited memory capacity when dealing with extremely large datasets, which can hinder performance in ultra-large-scale CNN models. Application-Specific Integrated Circuits (ASICs): ASICs are specialized integrated circuits designed to meet specific product requirements. The main advantage of ASICs in deep learning applications lies in their customization, which allows for the design of high-performance circuits tailored to the spe- cific operations of CNNs, such as convolution, activation, and pooling layers. This customization allows for the elimination of redundant components, significantly reducing chip area and power consumption compared to CPUs and GPUs [12]. However, the highly specialized nature of ASICs comes with trade-offs.\n\n--- Segment 5 ---\nThis customization allows for the elimination of redundant components, significantly reducing chip area and power consumption compared to CPUs and GPUs [12]. However, the highly specialized nature of ASICs comes with trade-offs. The design process is time-consuming, and ASICs are highly dependent on the particular CNN models they are designed for, making them less adaptable to evolv- ing architectures or new tasks. Furthermore, ASICs lack the flexibility to rapidly adjust to the dynamic nature of CNN development, which limits their applicability as deep learning models evolve. Unlike FPGAs, which can be reprogrammed or dynamically configured, ASICs are optimized for specific tasks, making them less versatile and unsuitable for handling diverse workloads outside their narrow focus. Field Programmable Gate Array (FPGAs): To enhance the programmability of hardware modules, FPGAs are increas- ingly utilized in various applications, thanks to their flexibility and reconfigurability. In the context of CNNs, FPGAs serve as a low-latency hardware platform that allows for customized data path designs and optimized memory hierarchies, making them a valuable complement to CPUs and GPUs. FPGAs excel in maximizing performance through dynamic task partitioning and efficient software-hardware co-design, which are critical for optimizing CNN operations. These features make FPGAs particularly advantageous in AI inference tasks, especially in resource-constrained environments like edge computing, where power efficiency and flexibility are paramount [13]. For this reason, this article provides a detailed literature review of FPGA-based CNN accelerators, which will be discussed in the following sections. B. Further Comparison of FPGAs and GPUs The acceleration of CNNs consists of two main stages: training and inference. CNN training involves constructing and optimizing the model through iterative processes across multiple epochs, refining the model parameters with each cy- cle. CNN inference uses a trained model to make predictions, such as classification or detection, without the need for weight updates or gradient calculations. In the training phase, GPUs excel in leveraging their high parallelism, achieving substantial throughput by processing large batches of data. However, their efficiency depends heav- ily on batch size, which works well in offline scenarios but is less effective in real-time processing, particularly for CNN models requiring continuous data transfer. In such cases, GPU performance tends to degrade [14].\n\n--- Segment 6 ---\nHowever, their efficiency depends heav- ily on batch size, which works well in offline scenarios but is less effective in real-time processing, particularly for CNN models requiring continuous data transfer. In such cases, GPU performance tends to degrade [14]. Additionally, GPUs often introduce significant latency, especially during convolution operations, which can impact overall system performance. On the other hand, dedicated architectures for CNNs can be designed to parallelize computations within a frame. The flexibility of FPGAs makes them a promising candidate for 3 TABLE I COMPARISON OF CNN ACCELERATION PLATFORMS Metrics CPUs GPUs ASICs FPGAs Compute Performance Limited computing capability with general-purpose Highly parallel computing capability Highly parallelized for specific applications Highly parallelized with customizable hardware Energy Efficiency Moderate power Large power Lowest Power Low power Flexibility Highly flexible Limited flexibility Fully customized Programmable Development Time Shortest development time Moderate development time Time-consuming development Acceptable development time Cost High cost Largest cost High initial cost but cost-effective for large-scale production Relatively low Model Scalability Reconfigurable Reconfigurable Worst scalability Scaled with reconfiguration CNN acceleration. With a scalable design, CNN accelerators can also be implemented on embedded FPGAs. While GPUs offer inherent parallelism, FPGAs allow for finer-grained par- allelism by customizing the hardware architecture to optimize specific CNN layers. This customization enables faster execu- tion of operations, such as convolution and activation, thereby improving overall CNN performance [15]. C. Discussion Table I summarizes the performance comparison of CPUs, GPUs, ASICs, and FPGAs across various metrics such as computing performance, energy efficiency, flexibility, devel- opment time, cost, and scalability. Based on these factors, a comparative analysis of the platforms is provided. FPGAs stand out in accelerating CNN models due to their customization and flexibility. Unlike ASICs, which are fixed to specific tasks, FPGAs can be reprogrammed to optimize performance for different CNN architectures, making them ideal for applications requiring a balance between accuracy and throughput. Additionally, FPGAs can leverage sparsity techniques to significantly enhance throughput. For exam- ple, the sparsity-aware CNN accelerator by Wen et al. [16] achieved up to 89.7 higher throughput compared to baseline designs.\n\n--- Segment 7 ---\nFor exam- ple, the sparsity-aware CNN accelerator by Wen et al. [16] achieved up to 89.7 higher throughput compared to baseline designs. FPGAs also offer advantages in power efficiency, particu- larly for edge devices with strict power constraints, providing a good balance between flexibility and low power consumption. While ASICs may offer lower power usage, FPGAs are more adaptable and power-efficient than GPUs and CPUs. Further- more, FPGAs excel in reducing latency, making them ideal for real-time applications that demand quick responses [17]. III. EVALUATION FRAMEWORK In CNN acceleration, evaluation metrics are crucial for assessing design performance. These metrics fall into two categories: network model performance and hardware accel- erator evaluation. This section focuses on the key metrics used to evaluate FPGA-based CNN accelerators, highlighting throughput, compute efficiency, and energy efficiency as crit- ical factors for hardware optimization. A. Model Performance Evaluation Accuracy: Algorithm adaptation and optimization tech- niques, such as model compression and data quantization, are often employed to reduce complexity. As a result, evaluating the accuracy of optimized models becomes crucial. Accuracy is a widely used metric in CNN model design to assess performance, representing the proportion of correctly classified samples in a dataset. It is calculated using the formula: Accuracy TP TN TP TN FP FN (1) where TP (True Positive), TN (True Negative), FP (False Positive), and FN (False Negative) indicate the model s correct and incorrect predictions. Accuracy provides an overall measure of classification performance, especially when the dataset has a balanced distribution of positive and negative samples. In such cases, a high accuracy rate typically indi- cates better performance. However, accuracy has limitations, particularly in the presence of class imbalance. In these cases, a high accuracy rate may mask poor performance on minority classes. Precision and Recall: To address the limitations of ac- curacy, it is often used in conjunction with precision and recall [18]. Precision measures the model s accuracy in pre- dicting positive classes, indicating the proportion of predicted positive samples that are correctly identified as positive. It is calculated as: Precision TP TP FP (2) However, precision does not consider false negatives and focuses only on the accuracy of positive predictions, ignoring cases where the model fails to identify actual positive samples.\n\n--- Segment 8 ---\nPrecision measures the model s accuracy in pre- dicting positive classes, indicating the proportion of predicted positive samples that are correctly identified as positive. It is calculated as: Precision TP TP FP (2) However, precision does not consider false negatives and focuses only on the accuracy of positive predictions, ignoring cases where the model fails to identify actual positive samples. In scenarios where missing positive instances has a high cost, relying solely on precision may not be enough. To address this, Recall is used to evaluate the model s ability to correctly identify all positive samples. Recall is calculated using the formula: Recall TP TP FN (3) F1 Score: Precision and recall often exhibit an inverse trade-off. Increasing recall typically leads to a decrease in precision, and vice versa. This occurs because when the 4 model prioritizes predicting positive classes to improve recall, the number of false positives may rise, resulting in lower precision. On the other hand, if the model overly emphasizes precision, it might miss some true positive samples, leading to a reduction in recall. To balance this trade-off, the F1 score is commonly used [19]. It is the harmonic mean of precision and recall, offering a comprehensive evaluation of model performance across both metrics. It is calculated as follows: F1 2 Precision Recall Precision Recall (4) B. Hardware Performance Evaluation Various hardware performance metrics, including through- put, latency, and power consumption, are widely used to eval- uate and compare FPGA-based CNN accelerators. However, achieving a fair comparison requires careful consideration of the operations or workloads of the CNN model, the available FPGA resources, and the underlying chip technology. Here, we provide a detailed discussion of the commonly employed evaluation metrics in the literature, highlighting their signifi- cance and impact. Latency: Latency is a key consideration for CNN hardware accelerators, representing the runtime taken from data input to the completion of processing and output generation. It serves as a critical measure of the system s responsiveness when processing a single image. Low latency is especially important for real-time applications such as autonomous driving [20] and speech recognition [21].\n\n--- Segment 9 ---\nIt serves as a critical measure of the system s responsiveness when processing a single image. Low latency is especially important for real-time applications such as autonomous driving [20] and speech recognition [21]. The overall latency of a design can be divided into three primary components: (1) Off-chip data transfer latency, determined by the model parameters and the memory bandwidth; (2) On-chip data transfer latency, influenced by the efficiency of internal data routing within the hardware; (3) Computation latency, dependent on the number of operations required by the CNN model and the level of parallelism implemented in the hardware design. To minimize latency in CNN accelerators, the following optimization strategies are commonly employed: Increasing memory bandwidth: Utilize high-bandwidth, low-latency memory technologies such as High Band- width Memory (HBM) to accelerate data read and write processes [22]; Reducing data transfer bottlenecks: Employ efficient data transfer protocols or advanced bus architectures, such as PCIe [23] or NVLink [24], to lower latency between accelerators and external devices; Optimizing data storage patterns: Enhance data reuse and reduce the frequency of data transfers by employing optimized storage patterns [25], thereby decreasing data transfer latency; Enhancing computational parallelism: Design hardware architectures to maximize computational parallelism, en- abling faster processing within the accelerator and reduc- ing overall computational latency [7]. Throughput: Throughput, often referred to as hardware performance, measures the number of computational opera- tions a system can process within a specific time frame. Unlike latency, throughput accounts for the computational workloads of different CNN models. As such, it serves as a key metric for evaluating hardware efficiency and performance [26]. The relationship between CNN workloads, throughput, and latency can be expressed as: Throughput Workloads Latency (5) Here, workloads refer to the volume of computational opera- tions in the CNN model. It is important to differentiate system throughput from the theoretical peak performance of the design, which is defined as: Peak throughput design 2 MACs f (6) where MACs represents the number of multiply-accumulate (MAC) units in the hardware accelerator, and f denotes the operating clock frequency of the design. This formula rep- resents the theoretical maximum performance of the realized hardware architecture. While the theoretical peak throughput indicates the maxi- mum achievable performance of the hardware design, actual throughput reflects the real-world performance.\n\n--- Segment 10 ---\nThis formula rep- resents the theoretical maximum performance of the realized hardware architecture. While the theoretical peak throughput indicates the maxi- mum achievable performance of the hardware design, actual throughput reflects the real-world performance. According to Eq. (6), the peak throughput is influenced by the number of MAC units, which is directly related to the parallelism of the hardware framework. Larger parallelism leads to more MAC units and increased computational capacity. Compute Efficiency: The theoretical peak throughput rep- resents the ideal maximum performance of a hardware system, acting as the upper limit of an accelerator s capacity. However, in practical CNN acceleration, the network parameters may not align with the computational parallelism of the design. This misalignment can cause unbalanced workloads among the processing elements (PEs) in the computation engine, leading to suboptimal utilization of available resources. Additionally, factors such as memory bandwidth and data transfer rates significantly impact throughput, often creating bottlenecks. As a result, actual throughput is lower than the theoretical peak performance due to these practical constraints. To evaluate the efficiency of a hardware design, compute ef- ficiency, also referred to as MAC efficiency, is employed [27]. This metric is defined as the ratio of the actual throughput to the peak theoretical throughput. It serves as a critical measure of hardware accelerators effectiveness by quantifying the proportion of useful MAC cycles relative to the total MAC units in the design. Compute efficiency is calculated using the formula: Compute EFF. Throughput 2 MACs f (7) Unlike other evaluation metrics mentioned above, compute efficiency is independent of the specific FPGA device or the number of MAC units utilized. Instead, it reflects the intrinsic efficiency of the accelerator design, making it a more reliable measure for assessing and comparing different hardware architectures for CNN acceleration. 5 Resource Efficiency: In CNN accelerators, resource ef- ficiency measures the system s throughput relative to the number of DSP units used [28]. This metric evaluates the contribution of each DSP unit to the computational task and is defined as: Res. EFF. Throughput DSP used (8) Higher resource efficiency reflects better DSP utilization, with each unit delivering significant computational throughput. This enhances hardware resource usage while maintaining high performance, reducing power consumption and idle re- sources, and improving overall system efficiency and stability.\n\n--- Segment 11 ---\nThroughput DSP used (8) Higher resource efficiency reflects better DSP utilization, with each unit delivering significant computational throughput. This enhances hardware resource usage while maintaining high performance, reducing power consumption and idle re- sources, and improving overall system efficiency and stability. However, in designs where logic resources are used alongside DSPs to implement MACs, this metric may not provide a fair basis for comparison across different designs. Clock Efficiency: Clock efficiency measures how effec- tively a hardware accelerator operates relative to its maximum clock frequency [29]. It is defined as: Clock EFF. Working Clock Maximum Clock (9) This ratio indicates the utilization of the hardware s com- putational potential. Higher clock efficiency means the ac- celerator operates near its peak frequency, while lower ef- ficiency suggests underutilization. Practical constraints like power consumption, heat dissipation, and system stability often limit clock efficiency. Improving heat dissipation, power management, and task scheduling can boost clock efficiency, leading to better performance in high-demand applications such as real-time processing and data centers. Overall Efficiency: Corresponds to the peak throughput of the design mentioned above, the theoretical peak performance of an FPGA device is calculated by multiplying the total number of multipliers and adders in its DSP blocks by the maximum clock rate [30]. This value represents the theoretical computational limit, which is unattainable in practice because real-world algorithms cannot continuously utilize all compu- tational units. However, it serves as a valuable benchmark for comparison. The overall efficiency is defined as the ratio of achieved performance to the peak performance of the device, offering a comprehensive measure of the hardware s effectiveness [6]. It can be expressed as: OVERALL EFF. CLOCK EFF. RES. UTI. COMPUTE EFF. (10) where clock efficiency is the ratio of the working clock rate to the maximum clock rate, resource utilization measures the fraction of multipliers and adders utilized in the design relative to those available in DSP blocks, and compute efficiency reflects the fraction of useful MAC cycles achieved. Optimizing clock rates and improving resource utiliza- tion often represent competing strategies for achieving high- performance FPGA accelerators. Note that logic resources are typically excluded from peak performance calculations, as their usage complicates computation, requires significant resources for auxiliary functions, and negatively impacts the working clock rate [30].\n\n--- Segment 12 ---\nOptimizing clock rates and improving resource utiliza- tion often represent competing strategies for achieving high- performance FPGA accelerators. Note that logic resources are typically excluded from peak performance calculations, as their usage complicates computation, requires significant resources for auxiliary functions, and negatively impacts the working clock rate [30]. Overall efficiency provides a unified view of hardware performance, helping identify imbalances across components. By incorporating previously discussed metrics, this evaluation framework facilitates a quantitative comparison of existing research and highlights emerging trends in CNN hardware accelerators. Energy Efficiency: Energy efficiency [6], [28] measures how efficiently the CNN hardware accelerators perform com- putations relative to its power consumption. It is typically expressed as the amount of work completed per watt of power consumed. The formula for energy efficiency is: Energy EFF. Throughput Power (11) As computational demands grow, especially in resource- intensive tasks like large-scale data processing and CNN model training, managing power consumption has become increasingly critical. Optimizing hardware accelerators to min- imize power consumption while maintaining high throughput is a key challenge in the field. Improving energy efficiency not only reduces operating costs but also addresses thermal management issues, ensuring better overall system stability and sustainability. C. Discussion on Evaluation Framework This section introduces various evaluation metrics for CNN hardware accelerators, enabling a quantitative comparison of performance across different designs in subsequent sections. Additionally, the relationships between these metrics are ex- amined, providing valuable insights for the design of FPGA- based CNN hardware accelerators. A significant observation from the literature review is that some researchers evaluate hardware performance using only a limited subset of metrics. To address this, Section V compare hardware designs using the comprehensive metrics of com- putational efficiency and overall efficiency introduced in this section. This approach aims to provide readers with a clearer understanding of the trade-offs and performance characteristics of different designs, offering practical guidance for future research and development in CNN hardware accelerators. IV. CNN ACCELERATION METHODS CNN models that achieve high accuracy tend to be large and resource-intensive. However, the larger the model, the more memory and storage it requires, making deployment on resource-constrained devices a significant challenge. Fur- thermore, larger models typically result in higher inference times and increased energy consumption, which limits their practicality for real-world applications. While these models perform well in controlled environments, they are not always feasible for deployment on edge devices.\n\n--- Segment 13 ---\nFur- thermore, larger models typically result in higher inference times and increased energy consumption, which limits their practicality for real-world applications. While these models perform well in controlled environments, they are not always feasible for deployment on edge devices. The solution lies in reducing the size of the model through compression techniques or optimizing the computations involved in CNNs. In this section, we explore existing methods for accelerating CNNs. We offer both qualitative and quantitative analyses of each approach, helping readers select the techniques that best suit their specific system requirements. The common methods 6 for CNN acceleration can be grouped into the following categories: 1) Pruning and Quantization: This approach reduces the computational complexity and memory demands of a model by either decreasing the number of model param- eters or reducing the bit-width of data. Techniques such as sparsity, pruning, and quantization are commonly employed in this category. 2) Model Structure Compression: This method optimizes the model s structure to enhance performance. It in- cludes techniques like lightweight networks, knowledge distillation, layer fusion, and the use of smaller convo- lution kernels. 3) Computation Reduction: This approach replaces com- plex operations in the original network with more ef- ficient computational techniques, such as FFT-based convolutions, fully spectral methods, and Winograd con- volutions. A. Pruning and Quantization CNN models often contain a large number of parameters, which are adjusted during training to minimize the loss function. These parameters help the network capture intricate data patterns by connecting multiple layers of neurons and applying non-linear activation functions. While this enables the model to achieve high accuracy, it also introduces significant computational costs. Therefore, controlling the quantity and quality of parameters is crucial to balancing performance and computation workloads. 1) Pruning: Pruning reduces the size and complexity of CNN models by eliminating unimportant parameters or struc- tures. Similar to pruning branches in biology, this technique removes redundant components to make the model more effi- cient. The process typically involves training a full-scale deep neural network, evaluating the importance of each parameter based on criteria such as weight magnitude or activation levels, and then removing unimportant weights, neurons, channels, or layers. The remaining network is usually fine-tuned to restore performance, with pruning and fine-tuning often repeated for further compression. For example, Han et al.\n\n--- Segment 14 ---\nThe remaining network is usually fine-tuned to restore performance, with pruning and fine-tuning often repeated for further compression. For example, Han et al. [31] combined pruning, quantiza- tion, and Huffman coding to efficiently compress deep neural networks, reducing memory usage and improving computa- tional speed and energy efficiency. Liu et al. [32] introduced L1 regularization for automatic identification and pruning of unimportant channels, applicable to modern CNN architectures with minimal computational overhead. Lin et al. [33] proposed a pruning criterion based on the rank of the output feature map to rank and prune filters efficiently. Yeom et al. [34] used Layer-wise Relevance Propagation (LRP) to prune net- works, calculating the importance of each unit through back- propagation for better compression and accuracy, especially without fine-tuning. Liu et al. [35] proposed AutoCompress, an automatic structured pruning framework that optimizes the pruning rate for each layer. More recently, Tan et al. [36] developed the Moving-one-Sample-out (MoSo) method, which measures sample importance based on the change in empiri- cal risk when a sample is excluded, reducing computational overhead while maintaining high performance, even at high pruning ratios. 2) Quantization: Quantization is a technique used to re- duce the computational overhead and memory demands of neural networks while preserving accuracy. By converting model weights and activation values from floating-point num- bers (typically 32-bit) to lower-precision integers (e.g., 8-bit or 16-bit), quantization reduces memory usage and acceler- ates inference speed. This technique is particularly valuable for deploying models on edge devices like FPGAs, where integer representations are more efficient than floating-point operations. For FPGA-based accelerations, fixed-point quantization is commonly used. Nguyen et al. [37] proposed a high- throughput, low-power FPGA implementation of the YOLO CNN for object detection, using binary weights and flexible low-bit activations, reducing model size and hardware costs. Kim et al. [38] developed a fixed-point quantization method for YOLO, adjusting weight distributions by subtracting the mean value before quantization and applying iterative retrain- ing to minimize accuracy loss.\n\n--- Segment 15 ---\nKim et al. [38] developed a fixed-point quantization method for YOLO, adjusting weight distributions by subtracting the mean value before quantization and applying iterative retrain- ing to minimize accuracy loss. For hardware implementation, Jain et al. [39] introduced a uniform symmetric quantizer with power-of-two scaling factors to achieve better optimization between range and precision. Jin et al. [40] proposed F8Net, a fixed-point 8-bit multiplication network quantization method that achieves superior performance and efficiency compared to existing methods. Lin et al. [41] introduced a fixed- point quantization method for deep convolutional networks (DCNs) based on optimizing the signal-to-quantization-noise ratio (SQNR) to determine the optimal bit width for each layer. To maximize acceleration, some researchers compress net- works to even lower bit widths. Li et al. [42] introduced FQN (Fully Quantized Network), which uses fully quantized training at low bit widths, ensuring equal distances between adjacent quantization points. Cao et al. [43] developed Seer- Net, which uses highly quantized (e.g., 4-bit or 1-bit) versions of CNN networks to predict binary sparse masks for output feature maps, guiding full-precision convolutions to exploit sparsity and accelerate inference. Similarly, Rastegar et al. [44] proposed XNOR-Net, where both filters and inputs to convo- lutional layers are binary, achieving 58 faster convolutions and 32 memory savings. B. Model Structure Compression The complexity of neural network architectures stems from the design and interaction of key components such as depth, width, and branching structures. Network depth refers to the number of layers in a model, and deeper networks can capture more abstract, complex features. However, very deep models can suffer from gradient vanishing or exploding problems, making training more challenging. Network width is the number of neurons or channels per layer, which influences the model s ability to process and capture input data. While wider networks can increase the capacity to learn, they can also introduce parameter redundancy and higher memory usage. Additionally, branching architectures where a model has mul- tiple branches at different stages can improve performance by 7 enabling the model to capture multi-scale features. However, these designs also increase computational complexity and memory demands.\n\n--- Segment 16 ---\nAdditionally, branching architectures where a model has mul- tiple branches at different stages can improve performance by 7 enabling the model to capture multi-scale features. However, these designs also increase computational complexity and memory demands. In general, more complex network architectures can yield better performance but at the cost of higher computational overhead and memory requirements. When deploying these models on resource-constrained edge devices, there is a need to balance complexity with efficiency. Model structure com- pression aims to reduce the computational burden and memory usage by optimizing the network s design or eliminating redundant components. These techniques focus on the archi- tectural level of the network and are essential for creating more efficient, deployable models. 1) Lightweight Networks: Lightweight networks are neural network models designed to operate efficiently in resource- constrained environments, aiming to reduce model size, com- putational complexity, and memory usage while preserving high performance. Szegedy et al. [45] introduced GoogleNet, which featured the Inception module a multi-scale convolutional layer design that employs 1 1, 3 3, and 5 5 convolutional kernels in parallel. This approach captures multi-scale features while keeping computational cost low. By repeating the Inception module across the network and combining it with pooling layers, they successfully increased both the depth and width of the model without significantly increasing the computational load. Similarly, Iandola et al. [46] proposed SqueezeNet, which uses a novel Fire module to build a lightweight model. The Fire module consists of a squeeze layer (using 1 1 convolutions to reduce feature map dimensions) and an expand layer (combining 1 1 and 3 3 convolutions), making it efficient and compact. The MobileNet series has also gained significant attention for its lightweight design. Howard et al. [47] introduced MobileNet v1, which uses depthwise separable convolutions, splitting traditional convolution into two parts: Depthwise Convolution and Pointwise Convolution. This reduces the number of parameters and computational complexity, while allowing flexibility in latency-accuracy trade-offs through two simple hyperparameters. MobileNet v2 [48] further optimized the network by introducing linear bottlenecks and inverted residuals, enabling deeper yet smaller and faster networks. MobileNet v3 [49] refined MobileNet v2 by removing certain layers, reducing computational overhead without sacrificing accuracy. Han et al.\n\n--- Segment 17 ---\nMobileNet v3 [49] refined MobileNet v2 by removing certain layers, reducing computational overhead without sacrificing accuracy. Han et al. [50] proposed GhostNet, which uses the Ghost module for model compression. This technique splits a convo- lutional layer into two parts: one performing standard convolu- tion and the other generating additional feature maps through simple linear operations, significantly reducing both computa- tion and parameter count while maintaining accuracy. Chen et al. [51] introduced Partial Convolution (PConv), which effectively extracts spatial features by reducing redundant computations and memory accesses. Based on PConv, Faster- Net was also developed in [51], a new family of networks that achieves higher running speeds on various devices without sacrificing accuracy in visual tasks. 2) Knowledge Distillation: Knowledge Distillation (KD) is a model compression technique that enhances the efficiency of models without significant loss in performance. In this approach, a large, pre-trained, complex model (the teacher) transfers its knowledge to a simpler, smaller model (the student). The goal is to enable the student model to achieve performance close to that of the teacher model despite having fewer parameters. KD has been widely applied in areas such as mobile deployment, edge computing, real-time applications, resource conservation, and fields like medical imaging, natural language processing, and autonomous systems. The concept of KD was first introduced by Hinton et al. [52], who demonstrated how a teacher model could transfer knowledge to a smaller model by using both hard targets (the original labels) and soft targets (the teacher s outputs). This combination improves the accuracy of the student model. However, when there is a large size discrepancy between the teacher and student, the student model may struggle to learn effectively. To address this, Son et al. [53] proposed the use of multiple progressively smaller Teacher Assistants (TAs) to guide the student, making the knowledge transfer smoother. They also introduced the Random DGKD method, which randomly drops knowledge connections to avoid overfitting. Hao et al. [54] advanced KD with Self-Mutual Knowledge Distillation (SMKD), where the model distills knowledge from itself, and student networks learn from each other in a mutual distillation process. This approach improves generalization and enhances both the visual and contextual capabilities of the network. Later, Sun et al.\n\n--- Segment 18 ---\nThis approach improves generalization and enhances both the visual and contextual capabilities of the network. Later, Sun et al. [55] proposed a method where the temperature (scaling factor) of both the teacher and student models, as well as the sample logits, is varied. They introduced a Z-score preprocessing method that standardizes logits, en- abling the student model to better learn the relationships from the teacher model. 3) Layer Fusion: In CNNs, layers such as convolution, batch normalization, activation, and pooling are typically ex- ecuted in sequence. However, researchers have shown that re- ordering or merging certain layers can significantly reduce data transfer times and computational complexity when executing the CNN models in hardware. Alwani et al. [56] introduced a pyramid-shaped multi-layer sliding window that processes input feature maps, allowing multiple layer results to be computed in advance. Abtahi et al. [57] proposed integrating Batch Normalization (BN) layer parameters directly into the preceding convolutional layer during inference, as BN is a lin- ear operation with fixed parameters. This integration reduces the number of operations. Syafeeza et al. [58] demonstrated that performing max pooling before ReLU activation helps reduce the computational workload on the ReLU layer. Liu et al. [59] applied layer fusion to the Fast Fourier Transform (FFT) approach, combining pooling and convolutional layers to eliminate unnecessary operations and improve efficiency. C. Computation Reduction Computation reduction involves simplifying complex op- erations in neural networks, typically leveraging mathemat- ical theorems. Techniques like the Fast Fourier Transform 8 (FFT) and the Winograd algorithm can significantly reduce the computational complexity of convolutional layers. Further advancements, such as fully spectral CNNs, boost performance by transforming each layer into the frequency domain. 1) FFT Approach: The FFT approach accelerates convo- lution operations by using the convolution theorem, which states that spatial domain convolutions are equivalent to point- wise multiplications in the frequency domain. By transforming both the input data and weights into the frequency domain with FFT, performing element-wise multiplication, and then converting the result back to the spatial domain through Inverse FFT (IFFT), convolution operations can be executed more efficiently. Mathieu et al.\n\n--- Segment 19 ---\nBy transforming both the input data and weights into the frequency domain with FFT, performing element-wise multiplication, and then converting the result back to the spatial domain through Inverse FFT (IFFT), convolution operations can be executed more efficiently. Mathieu et al. [60] applied FFT to speed up both training and inference in convolutional neural networks, achieving notable improvements by reusing transformed feature maps. Rippe et al. [61] introduced frequency domain pooling, ob- serving that low-frequency components carry most of the critical information. By truncating feature representations in the frequency domain rather than using traditional spatial pool- ing, they preserved more information and allowed for flexible adjustments in the output size. Zeng et al. [62] implemented FFT on FPGA and developed a frequency domain loop tiling technique called Overlap-and-Add (OaA) to enhance through- put through better data reuse. Abtahi et al. [57] compared three methods on embedded hardware: Direct Convolution (Direct- Conv), FFT-based Convolution (FFT-Conv), and Overlap-and- Add based FFT Convolution (FFT-OVA-Conv), providing a detailed comparison of computational complexity and memory requirements. While traditional FFT methods reduce computational com- plexity, they have a significant drawback: the lack of frequency domain activation functions. This limitation necessitates re- peated FFT-IFFT transformations, diminishing the acceleration benefits. To address this, fully spectral CNNs were introduced. Liu et al. [63] overcame this limitation by constructing acti- vation functions in the frequency domain, enabling all CNN layers to operate in the frequency domain and eliminating the need for repeated FFT-IFFT transformations. Similarly, Ayat et al. [64] utilized the convolution theorem to create a frequency-domain SReLU activation function, further enhanc- ing computation through layer fusion techniques. Watanabe et al. [65] explored the ReLU operation in the frequency domain and proposed the 2SReLU (Second Harmonics Superposition ReLU) activation function, performing all CNN operations in the frequency domain. 2) Winograd: While FFT is effective for large convolution kernel operations, CNNs increasingly use smaller kernels, where the Winograd algorithm excels.\n\n--- Segment 20 ---\n[65] explored the ReLU operation in the frequency domain and proposed the 2SReLU (Second Harmonics Superposition ReLU) activation function, performing all CNN operations in the frequency domain. 2) Winograd: While FFT is effective for large convolution kernel operations, CNNs increasingly use smaller kernels, where the Winograd algorithm excels. The Winograd algo- rithm minimizes the number of multiplication operations in convolution by substituting multiplications with additions, thus reducing computational complexity. Although originally proposed by Shmuel Winograd in 1980, the algorithm was first applied to accelerate convolution operations in neural networks by Lavin et al. [66]. They divided large input data into smaller tiles and performed low-complexity convolutions on each tile, significantly re- ducing computational load. Their experiments demonstrated the algorithm s efficiency, particularly for smaller convolution kernels. Subsequently, Liang et al. [67] developed a hardware framework to implement the Winograd algorithm on FPGAs, utilizing a line-buffer structure for efficient feature map reuse across different blocks, and pipelining Winograd Processing Element (PE) units in parallel. Yepez et al. [68] introduced a novel Winograd-based method that supports strides of 2 and is adaptable across one, two, or three dimensions. D. Discussion and Comparison We have reviewed the current methods for accelerating CNNs as described above. Here, we also provide a com- prehensive summary of their key research efforts, as shown in Table II. This summary highlights each method s core contributions and results, aiming to offer an overview of their respective acceleration effects. However, each method has its own set of advantages and limitations, making them more or less suitable depending on the specific scenario. For example, both pruning and quantization can help reduce computational overhead. However, pruning is a complex pro- cess that often depends on the characteristics of the task and hardware architecture. Moreover, it may require careful fine- tuning. Quantization, while effective at reducing model size and computation, may require additional calibration steps to maintain performance. Knowledge distillation allows smaller models to learn from larger, more complex models, leveraging their output distribu- tions or intermediate features. While this can result in compact models with good performance, the distillation process itself is relatively complex and heavily reliant on the teacher model and the specific task at hand.\n\n--- Segment 21 ---\nKnowledge distillation allows smaller models to learn from larger, more complex models, leveraging their output distribu- tions or intermediate features. While this can result in compact models with good performance, the distillation process itself is relatively complex and heavily reliant on the teacher model and the specific task at hand. Lightweight models, on the other hand, are designed to be compact and efficient, often using specialized architectures to reduce memory and computation. However, they may not match the performance of larger, general-purpose models in terms of accuracy. The FFT approach can significantly reduce the computa- tional complexity of convolution layers by leveraging fre- quency domain operations. However, it requires repeated FFT and IFFT transformations, which can hinder its effectiveness in hardware implementations. Fully spectral CNNs, which aim to overcome this limitation, have been optimized for hardware, though they may not be ideal for scenarios with large inputs. The Winograd algorithm reduces convolution complexity by minimizing the number of multiplications required, improving computational efficiency. However, it introduces additional intermediate transformation matrices and cache requirements, leading to higher memory usage. To determine the most optimal acceleration method for a given scenario, we conduct both qualitative and quantitative analyses of these methods, as shown in Table III. The speedups for each method are estimated based on the results presented in the literature. This table provides valuable insights into the strengths and weaknesses of each approach, helping us identify the most suitable method for maximizing acceleration in various use cases. 9 TABLE II COMPARISON OF CNN ACCELERATION METHODS Type Ref. Methods Results Pruning [31] Combined pruning, quantization, and huffman coding. AlexNet parameters reduced: 97.12 VGG-16 parameters reduced: 97.95 [33] Used the rank of the output feature map to determine the ranking of filters. ResNet-110 parameters reduced: 59.2 ResNet-50 parameters reduced: 36.7 [32] Used L1 regularization to the batch normalization layers. VGGNet parameters reduced: 95 [35] Proposed an automatic structured pruning framework. VGG-16 and ResNet-18, parameters reduced: 99.16 . Quantization [38] Centered the weight distribution to zero. YOLOv3 and YOLOv4, parameters reduced: 80 . [41] Used SQNR to find the optimal bit width for each layer.\n\n--- Segment 22 ---\nYOLOv3 and YOLOv4, parameters reduced: 80 . [41] Used SQNR to find the optimal bit width for each layer. DCNs parameters reduced: 20 [44] Both the filters and the input to convolutional layers are binary. ResNet18 parameters reduced:: 98.5 [43] Proposed a binary sparsity by inference on the original network. ResNet18 speedup: 2.45 Lightweight Networks [45] Introduced the Inception module . Top-1 accuracy on ImageNet: 43.9 Parameters: 5 million [46] Introduced the fire module. Top-1 accuracy on ImageNet: 57.5 Parameters: 4.8 million [47] Introduced depthwise separable convolutions. Top-1 accuracy on ImageNet: 70.6 Parameters: 4.2 million [50] Introduced the ghost module for model compression. Top-1 accuracy on ImageNet: 73.9 Parameters: 5.2 million [51] Proposed a new partial convolution (PConv). Top-1 accuracy on ImageNet: 76.2 Parameters: 7.6 million FFT [62] Developed overlap-and-add technique. AlexNet speedup: 10.6 VGG16 speedup: 7.4 [63] Introduced spectral activation functions. Speedup compared to spatial methods: 4 6.6 Speedup compared totraditional FFT: 3 4.4 [64] Introduced SReLU activation function. Speedup compared to spatial methods: 3.45 Winograd [66] Used winograd to accelerate convolution operations. Speedup: 1.63 7.42 [67] Developed a hardware framework for w inograd algorithm. AlexNet speedup: 11.8 [68] Introduced the winograd algorithm to more dimensions. Speedup: 1.44 2.42 TABLE III SUMMARY AND QUANTITATIVE ANALYSIS OF THE REVIEWED METHODS Methods Cheaper Arithmetic Operations Memory Reduction Parameters Reduction Hardware Friendly Speedup Pruning 2 10 Quantization 2 50 Lightweight Networks 8 9 Knowledge Distillation 1.5 5 Layer Fusion 2 FFT 5 10 Winograd 3 5 V. HARDWARE APPROACHES This section introduces the key techniques commonly used for CNN hardware acceleration, followed by an overview of optimization strategies that build upon these techniques to enhance performance. A.\n\n--- Segment 23 ---\nSpeedup: 1.44 2.42 TABLE III SUMMARY AND QUANTITATIVE ANALYSIS OF THE REVIEWED METHODS Methods Cheaper Arithmetic Operations Memory Reduction Parameters Reduction Hardware Friendly Speedup Pruning 2 10 Quantization 2 50 Lightweight Networks 8 9 Knowledge Distillation 1.5 5 Layer Fusion 2 FFT 5 10 Winograd 3 5 V. HARDWARE APPROACHES This section introduces the key techniques commonly used for CNN hardware acceleration, followed by an overview of optimization strategies that build upon these techniques to enhance performance. A. Data Blocking and Parallel Computing 1) Loop Tiling: Loop tiling, also known as data blocking, is a critical optimization technique in hardware acceleration [69], [70], especially for CNN models that involve large-scale data processing. Its main objective is to minimize memory access overhead and maximize hardware resource utilization by enhancing cache efficiency. In convolutional nested loops, large input feature maps are typically processed element by element. While this method is straightforward, it often leads to frequent cache misses, as large portions of data do not remain in the processor s cache. This results in excessive memory accesses, which degrade overall performance. Loop tiling addresses this issue by dividing large loops into smaller, more manageable tiles, as shown in Code 1, where Ti and Tj denote tiling coefficients for different loop levels. By processing only a subset of data at a time, loop tiling ensures that each tile remains in the cache, 10 Code 1 Tiling Convolutional Layer Algorithm 1: for (f 0 ; f F ; f ) 2: for (c 0 ; c C ; c ) 3: for (h 0 ; h Ho ; h ) 4: for (w 0 ; w Wo ; w ) 5: for (ii 0 ; ii K ; i Ti) 6: for (jj 0 ; jj K ; j Tj) 7: for(i ii ; i ii Ti ; i ) 8: for(j j ; j jj Tj ; j ) 9: O[f][h][w] W[f][c][i][j] I[c][h S i][w S j] significantly improving data locality and reducing memory bandwidth usage. The technique introduces additional outer loops that partition the dataset into smaller chunks. Each chunk is processed independently, allowing it to be efficiently stored in cache and reducing memory access overhead.\n\n--- Segment 24 ---\nThe technique introduces additional outer loops that partition the dataset into smaller chunks. Each chunk is processed independently, allowing it to be efficiently stored in cache and reducing memory access overhead. This segmentation enhances execution speed by minimizing data movement between main memory and processing units. In hardware-accelerated environments, loop tiling is often combined with parallelism and pipelining to further opti- mize performance. In parallel computing architectures such as GPUs, multiple processing units can operate on different tiles concurrently, enabling efficient parallel execution. In FPGAs and custom accelerators, loop tiling can be integrated with hardware-specific architectural features to mitigate memory bottlenecks and enhance overall data throughput. 2) Loop Unrolling: Loop unrolling is a widely used op- timization technique aimed at improving the execution effi- ciency of loops in CNN hardware accelerators, particularly on FPGAs. The key idea is to merge multiple iterations of a loop into a single, larger operation, reducing loop control overhead and increasing parallelism to accelerate execution. In CNN computations, convolution operations often involve deeply nested loops, leading to a high number of iterations when processing large-scale input data. Loop unrolling ad- dresses this inefficiency by transforming a single iteration into multiple parallel operations. By increasing the workload of each iteration, it reduces the need for control logic, such as loop counters and branching decisions, thereby improving instruction execution efficiency and maximizing hardware par- allelism. For instance, in convolution operations, a standard loop structure processes input data elements sequentially. With loop unrolling, multiple elements can be processed in a single iteration, or different convolution tasks can be executed simul- taneously, significantly boosting computational throughput. However, the degree of unrolling must be carefully tuned to match the available hardware resources, such as logic units and memory bandwidth. Excessive unrolling can lead to resource congestion, negatively affecting overall performance. There- fore, in CNN hardware acceleration, loop unrolling must be strategically applied to strike a balance between performance gains and efficient hardware utilization. 3) Parallel Computing: Parallel computing is a fundamen- tal technique in CNN hardware acceleration, particularly for computationally intensive tasks like convolution operations. ... ... F Pf ... ... ... ... ... ... Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Pc Pc K Pv Pk Fig. 1.\n\n--- Segment 25 ---\nInput Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Input Feature Kernel Pc Pc K Pv Pk Fig. 1. An illustration depicting the four levels of parallelism in convolution computation across the filter (Pf), channel (Pc), pixel (Pv), and kernel (Pk) dimensions. Convolutions involve numerous multiplications and additions, which can be executed concurrently across multiple processing units, significantly boosting computational speed. In hardware acceleration platforms such as FPGAs and GPUs, convolution operations are naturally suited for parallel execution. By distributing data and tasks across multiple computing units, processing throughput is greatly enhanced. For example, FPGAs can be configured with numerous par- allel computing units to process multiple input data points and convolution kernel elements simultaneously, alleviating the bottleneck of sequential execution. In such architectures, multiplication and summation of input data and convolution kernel elements occur in parallel, fully utilizing the available hardware resources. Additionally, the reconfigurability of FP- GAs allows for the customization of parallel computing archi- tectures tailored to specific CNN models. Techniques such as pipelined data flow processing and hierarchical parallel task scheduling can further enhance CNN inference speed. More- over, loop unrolling, which increases computational workload per iteration while reducing loop control overhead, creates more opportunities for parallel execution, further improving efficiency. Fig. 1 illustrates several parallel computing strategies com- monly used to accelerate convolution operations. Convolution layers consist of multiple nested loops that slide over the kernel and feature maps, offering a vast design space for parallel computing strategies and data caching optimization. These loop transformations have been widely explored in previous research. Expanding different loops directly impacts parallel computing methods, memory access patterns, and convolution engine architectures. To simplify the discussion, we define parallelism in different dimensions as follows: (1) Filter Parallelism (Pf): In CNNs, each input feature map is convolved with multiple filters to generate correspond- ing output feature maps. Traditionally, this process is per- formed sequentially, which underutilizes hardware resources. With filter parallelism, multiple filters operate simultaneously on the same input feature map, generating multiple output 11 channels in parallel. This approach enables efficient reuse of feature data and significantly enhances computational through- put.\n\n--- Segment 26 ---\nWith filter parallelism, multiple filters operate simultaneously on the same input feature map, generating multiple output 11 channels in parallel. This approach enables efficient reuse of feature data and significantly enhances computational through- put. (2) Channel Parallelism (Pc): CNNs typically process multi-channel input feature maps, such as RGB images (three channels) or deeper feature maps generated by previous con- volution layers. Conventionally, each convolution kernel com- putes results for each channel independently before summing them to produce the final output. Channel parallelism accel- erates this process by simultaneously computing convolutions across all input channels, reducing computational dependen- cies and improving efficiency. This method also facilitates effective reuse of weight data. (3) Pixel Parallelism (Pv): In convolution operations, kernels slide across input feature maps to compute convolution results at different spatial positions. Since these computations at different positions are largely independent, they can be pro- cessed in parallel. Pixel parallelism exploits this independence by computing multiple spatial locations simultaneously. While this approach simplifies data storage, it requires multiple levels of buffering for data shifting in on-chip cache design due to the sliding window nature of convolutions. (4) Kernel Parallelism (Pk): Kernel parallelism extends pixel parallelism to the convolution kernel dimension. It pro- cesses multiple weight values for a single feature pixel in parallel. However, this method is often constrained by kernel size and presents challenges in data storage design, making it less flexible for large-scale CNN architectures. B. CNN Hardware Accelerators based on Parallel Computing This section explores and analyzes the integration of data- level parallel computing strategies in the design of CNN hard- ware accelerators, as discussed in existing literature. Various parallelization techniques are employed in different combina- tions to optimize accelerator architectures [6], [7], [14], [28], [71] [84]. This work presents a detailed comparison of various parallelization strategies, evaluating their computational effi- ciency and summarizing key performance metrics. The goal is to provide insights into the latest advancements in CNN hardware acceleration, enabling researchers, developers, and engineers to better understand the trade-offs among different approaches. By examining the characteristics and capabilities of diverse parallel strategies, readers can design optimized ac- celeration solutions tailored to specific tasks. A comprehensive summary of performance comparisons is provided in Table IV. For example, Zhao et al.\n\n--- Segment 27 ---\nA comprehensive summary of performance comparisons is provided in Table IV. For example, Zhao et al. [72] and Liu et al. [71] imple- mented a combination of kernel, pixel, and filter parallelism (Pk, Pv, Pf) to accelerate CNN computations. Liu et al. achieved a processing performance of 107 Gop s on a Zynq ZC7045 FPGA with 16-bit data quantization, reaching an effi- ciency of 0.12 Gop s per DSP. While this approach delivered high computational throughput, it suffered from inefficient resource utilization, ultimately limiting overall accelerator efficiency. Similarly, Rahman et al. [73] and Ma et al. [74] adopted a hybrid strategy combining pixel parallelism (Pv) and filter parallelism (Pf). In their implementation of ResNet-50 on Intel FPGA Stratix V and Arria 10, Ma et al. achieved a throughput of 315.5 Gop s. This method maximized data reuse and improved computational efficiency. However, it introduced challenges in hardware resource utilization, particularly when convolution layer dimensions were not divisible by Pv, leading to underutilized computational units in certain CNN layers. Unlike Pk and Pv, which are constrained by kernel size and feature characteristics, the combined approach of Pc and Pf offers greater flexibility [6], [28], [75] [78]. For example, [28] achieves energy efficiency 3.8 5.6 higher than GPUs and up to 1.4 2.2 better resource efficiency than FPGA-based accelerators for 2D and 3D CNNs, with an overall efficiency of 50.47 . Some studies integrate pixel, channel, and filter parallelism (Pv, Pc, Pf) [79], achieving high throughput. For example, [79] reported 2870 Gop s peak performance for convolution operations. Channel parallelism enables large-scale parallelism more efficiently than pixel parallelism, improving throughput. Other works employ (Pk, Pc, Pf) [14], [80] [82], extending parallelism to additional dimensions for better performance. However, Pc and Pf parallelism face constraints; for example, the first convolution layer typically processes RGB channels, but Pc often scales exponentially by 2, leading to computa- tional imbalances and reduced efficiency.\n\n--- Segment 28 ---\nOther works employ (Pk, Pc, Pf) [14], [80] [82], extending parallelism to additional dimensions for better performance. However, Pc and Pf parallelism face constraints; for example, the first convolution layer typically processes RGB channels, but Pc often scales exponentially by 2, leading to computa- tional imbalances and reduced efficiency. Despite various parallel strategies, architectures exhibit sig- nificant differences in compute efficiency due to fixed paral- lelism in each data dimension, resulting in resource under- utilization and limited adaptability. To address this, some works propose dynamic parallelism [7], [83], [84]. For exam- ple, DCP-CNN [7] dynamically adjusts parallelism based on input size, kernel size, and network configuration, achieving over 800 Gop s throughput and 72 98 compute effi- ciency on Intel Stratix 10 GX650 FPGA, outperforming exist- ing accelerators. Dynamic parallelism optimizes FPGA-based CNN acceleration by tailoring parallel strategies to each layer, preventing load imbalance and enhancing efficiency. However, it introduces design complexity, requiring adaptive data storage and on-chip buffering to handle transitions between parallel strategies. Additionally, varying network parameters pose chal- lenges in identifying optimal configurations. C. Other Optimizations 1) Input Reshaping: Input reshaping is a key optimiza- tion in CNN hardware accelerators that enhances compute efficiency by reorganizing data storage and access patterns to better align with hardware architectures [85] [88]. It ensures efficient data alignment for convolution operations, facilitating optimized memory access and computation. A common approach, Im2Col [89], converts feature maps into matrix form, enabling efficient matrix multiplications instead of direct convolution operations. Additionally, input re- shaping can partition data into cache-friendly blocks, reducing memory accesses and improving processing efficiency. Format conversions, such as between NCHW and NHWC, further optimize performance across different hardware architectures. For sparse inputs, compressed storage formats enhance sparse matrix computation efficiency. While input reshaping improves 12 TABLE IV COMPARISON OF HARDWARE PERFORMANCE OF EXISTING CNN ACCELERATORS USING DIFFERENT PARALLELISM Ref. Employed Parallelism Latency (ms) Throughput (Gop s) Compute EFF. Resource EFF. (Gop s DSP) Clock EFF. Overall EFF.\n\n--- Segment 29 ---\n(Gop s DSP) Clock EFF. Overall EFF. Power (W) Energy EFF. (Gop s W) [71] Pk, Pv, Pf - 107 41.7 0.12 40 16.6 9.6 11.14 [72] 3.7 391.6 - - 40 - 178 2.19 [73] Pv, Pf - 147.82 17.14 0.11 26.7 4.24 - - [74] 71.71 315.48 69.28 0.21 33.3 20.6 - - [28] Pc, Pf 4.62 1667 92.05 1.24 44 36.1 45 37 [6] 5.07 1519 90 1.0 44 54 19.1 79.5 [75] 21.61 61.62 69.1 0.03 20 11.04 18.61 3.31 [76] 47.97 645.25 68.58 0.43 30 20.57 - - [77] - 1218 95 0.76 40 23.18 17.7 68.81 [78] - 1240.8 - 0.32 28.6 - 13.38 92.4 [79] Pv, Pc, Pf - 2870 92.2 0.79 38 24.88 39.5 72.7 [14] Pk, Pc, Pf 364 84.3 80.25 0.17 26.7 18.58 3.5 24 [80] 2.56 565.94 84.6 0.26 31.2 15.84 30.2 18.74 [81] 20.1 136.5 - 0.56 24 - 24.2 5.64 [82] 0.62 2423.63 - 0.89 48.2 - 18.04 143.32 [83] Dynamic - 129.7 41.8 0.13 30 4.63 18 7.2 [84] 178 684 49.47 0.2 40 19.39 26 26.3 [7] 38.3 807 98.5 0.79 40 35.03 6.3 128.1 memory utilization and supports parallelism, it may introduce computational and storage overhead. Effective integration with hardware design is essential to balance reshaping costs with performance gains.\n\n--- Segment 30 ---\n(Gop s W) [71] Pk, Pv, Pf - 107 41.7 0.12 40 16.6 9.6 11.14 [72] 3.7 391.6 - - 40 - 178 2.19 [73] Pv, Pf - 147.82 17.14 0.11 26.7 4.24 - - [74] 71.71 315.48 69.28 0.21 33.3 20.6 - - [28] Pc, Pf 4.62 1667 92.05 1.24 44 36.1 45 37 [6] 5.07 1519 90 1.0 44 54 19.1 79.5 [75] 21.61 61.62 69.1 0.03 20 11.04 18.61 3.31 [76] 47.97 645.25 68.58 0.43 30 20.57 - - [77] - 1218 95 0.76 40 23.18 17.7 68.81 [78] - 1240.8 - 0.32 28.6 - 13.38 92.4 [79] Pv, Pc, Pf - 2870 92.2 0.79 38 24.88 39.5 72.7 [14] Pk, Pc, Pf 364 84.3 80.25 0.17 26.7 18.58 3.5 24 [80] 2.56 565.94 84.6 0.26 31.2 15.84 30.2 18.74 [81] 20.1 136.5 - 0.56 24 - 24.2 5.64 [82] 0.62 2423.63 - 0.89 48.2 - 18.04 143.32 [83] Dynamic - 129.7 41.8 0.13 30 4.63 18 7.2 [84] 178 684 49.47 0.2 40 19.39 26 26.3 [7] 38.3 807 98.5 0.79 40 35.03 6.3 128.1 memory utilization and supports parallelism, it may introduce computational and storage overhead. Effective integration with hardware design is essential to balance reshaping costs with performance gains. 2) DSP Optimization: CNN accelerators benefit from re- duced precision due to their tolerance to small numerical errors, making low-precision computing a key strategy for improving performance.\n\n--- Segment 31 ---\nEffective integration with hardware design is essential to balance reshaping costs with performance gains. 2) DSP Optimization: CNN accelerators benefit from re- duced precision due to their tolerance to small numerical errors, making low-precision computing a key strategy for improving performance. However, CNN operations rely on DSP units, which support limited precision levels, posing challenges for precision reduction. To address this, researchers have explored executing multiple low-precision MAC opera- tions within a single DSP unit to enhance efficiency. Lee et al. [90] introduced a 2-way SIMD MAC design for CNN convolutions, where SIMD multiplications share a common operand, enabling efficient dual MAC operations in a single DSP block. Similarly, Liu et al. [6] proposed an INT8 optimization technique for Intel DSPs, utilizing an 18 19 multiplier to achieve a 1:2 DSP-to-INT8 MAC ratio. By partitioning INT8 computations into smaller multiplications, they optimized DSP usage while leveraging logic resources for additional operations. D. Discussion based on Our Evaluation Framework In this section, we provide a comparison of various works based on different parallel combinations, as shown in Table IV. A key observation from existing research is that many studies evaluate their designs using a limited set of metrics, such as throughput, power consumption, and latency. However, these metrics are not solely influenced by the design itself; they are also highly dependent on the FPGA device used, as per- formance can vary across different CNN models. To address this, we emphasize the importance of compute efficiency and overall efficiency. These metrics, included in Table IV based on reported design parameters, offer a more comprehensive understanding of the evaluated works. Some entries are left blank because the results were not provided in the respective papers. Compute efficiency is closely tied to the number of MAC operations in the design and is relatively unaffected by FPGA device choice. We calculate and summarize compute efficiency from various hardware architectures, such as [73] (17.14 ) and [7] (98.5 ). A key conclusion from our comparison is that parallelization strategy plays a significant role in determining compute efficiency. For example, [73] used a fixed paral- 13 Fig. 2. Energy efficiency comparison across CNN accelerator architectures. The left vertical axis represents energy efficiency, while the right vertical axis shows power consumption.\n\n--- Segment 32 ---\nEnergy efficiency comparison across CNN accelerator architectures. The left vertical axis represents energy efficiency, while the right vertical axis shows power consumption. lelization strategy with constant parallelism degrees (Pv, Pf), which can lead to load imbalances and resource inefficiency. In contrast, [7] dynamically adjusted the parallelization strategy based on the layer being executed, optimizing resource usage and achieving higher compute efficiency. Given these findings, future research should focus on leveraging dynamic parallelism to maximize performance. However, implementing dynamic parallelism introduces de- sign complexities, such as the need for flexible data storage schemes and on-chip buffers to adapt to different parallel strategies. Failure to properly manage these transitions may degrade data transfer efficiency and overall performance. Overall efficiency builds upon compute efficiency but also considers resource utilization and clock efficiency. One key method to improve resource utilization is DSP optimization, which maximizes the use of limited DSP resources. For example, [6] demonstrated high overall efficiency largely due to effective DSP optimization. Improving clock efficiency requires careful attention to pipelining, timing optimization, and resource sharing. We also compare the energy efficiency across designs, shown in Fig. 2. Few designs achieve high energy efficiency with low power consumption; for most, energy efficiency is directly proportional to power consumption, meaning higher power usage leads to higher throughput. Thus, research efforts should focus on balancing low power consumption with high throughput. Additionally, we conduct a comprehensive comparison of latency, throughput, and power consumption for architectures based on different parallelism combinations. As shown in Fig. 3, high throughput and low latency are generally associ- ated with higher power consumption, while low-power designs tend to have lower throughput and higher latency. Future work should focus on optimizing architectures to enhance throughput while maintaining low power consumption. VI. ALGORITHM-HARDWARE CO-DESIGN To meet the growing complexity and accuracy demands of neural architectures, the number of parameters in CNN models has increased significantly. While this boosts performance, it also raises computational and memory requirements, leading to higher time and energy consumption during training and inference. Edge computing platforms, such as mobile and embedded devices, face challenges due to limited processing power, storage, and energy efficiency, making it difficult to deploy these resource-intensive models, especially in real-time applications. Optimizing only from the hardware or software perspective is not enough to address these challenges.\n\n--- Segment 33 ---\nEdge computing platforms, such as mobile and embedded devices, face challenges due to limited processing power, storage, and energy efficiency, making it difficult to deploy these resource-intensive models, especially in real-time applications. Optimizing only from the hardware or software perspective is not enough to address these challenges. Algorithm-hardware co-design integrates both aspects, tailoring algorithms to lever- age hardware features while adapting hardware architectures to meet algorithmic needs. This approach maximizes perfor- mance, resource utilization, and energy efficiency by optimiz- ing the synergy between software and hardware. A. CNN Acceleration Toolflows Toolflows represent a system-level approach to automating the mapping of CNNs onto FPGAs. These toolflows streamline the design process, ensuring rapid deployment and high energy efficiency. As application needs and hardware capabilities evolve, CNN toolflows have adapted to drive continuous innovation in both algorithms and hardware. They encompass a wide range of technologies, from deep learning frame- works to hardware acceleration, automated optimization, and explainability analysis. These tools enable users to generate customized CNN hardware implementations without requiring deep hardware expertise, making FPGA integration more ac- cessible within the deep learning ecosystem. Table V presents various CNN-to-FPGA toolflows that leverage the algorithm- hardware co-design framework to generate optimized FPGA accelerators tailored to specific CNN-FPGA pairs. fpgaConvNet [91] utilized specialized hardware blocks to efficiently map irregular data flows like Inception, Residual, and Dense blocks. It explores the design space based on the Synchronous Data Flow (SDF) model, accounting for platform resource constraints. Several optimizations have been proposed for fpgaConvNet, including a latency-driven design [92] and a 14 Throughput (Gop s) Power (W) Fig. 3. An illustrative comparison of the hardware performance of existing CNN accelerators in terms of power consumption, latency, and throughput. The size of the bubbles corresponds to latency, with larger bubbles indicating higher latency and smaller ones indicating lower latency. The horizontal axis represents power consumption, increasing from left to right, while the vertical axis represents throughput, increasing from bottom to top. heuristic method for pruning the architecture search space. Ve- nieris et al.\n\n--- Segment 34 ---\nheuristic method for pruning the architecture search space. Ve- nieris et al. [93] later extended this framework to handle CNNs with irregular connectivity, treating design space exploration (DSE) as a Multi-Objective Optimization (MOO) problem to address throughput and latency requirements. In contrast, Angel-Eye [14] and Snowflake [94] adopted a flexible computing engine design, allowing the reuse of hard- ware across different CNNs without recompilation. Snowflake further enhanced performance with double buffering to overlap computation and communication, hiding external memory latency. f-CNNx [95] enabled mapping multiple CNNs onto a single FPGA, optimizing resource allocation and memory bandwidth to reduce contention and improve performance. It used a multi-objective cost function for efficient map- ping. FMM-X3D [96] optimized 3D-CNNs for human action recognition (HAR) on FPGAs, while HARFLOW3D [97] introduced runtime parameter reconfiguration to avoid full bitstream reconfiguration, improving latency. fpgaHART [98] extended the SDF model to manage irregular blocks with branching structures in 3D-CNN HAR models. Pflow [23] de- coupled hardware details with an Overlay library and employs adaptive scheduling and operator fusion to maximize hardware utilization, enabling end-to-end acceleration from application to hardware. B. Design Space Exploration Design Space Exploration (DSE) involves systematically optimizing design parameters within set constraints. To maxi- mize FPGA capabilities, factors such as network architecture, hardware resources, memory bandwidth, and latency must be considered. Optimizing these parameters enhances CNN inference while making full use of FPGA hardware features. Table VI compares various DSE methods, including brute- force and heuristic approaches. 1) Brute-Force Method: The Brute-Force method [99] exhaustively searches all possible solutions to find the global optimum. While it guarantees the best result, it is computa- tionally expensive and time-consuming, making it impractical for large-scale optimizations. 2) Simulated Annealing: Simulated Annealing (SA) [93], [99] uses probabilistic jumping to explore the solution space, gradually lowering the search temperature to converge to the global optimum. While it prevents local optima, its conver- gence is slow, making it less efficient for large-scale searches.\n\n--- Segment 35 ---\n2) Simulated Annealing: Simulated Annealing (SA) [93], [99] uses probabilistic jumping to explore the solution space, gradually lowering the search temperature to converge to the global optimum. While it prevents local optima, its conver- gence is slow, making it less efficient for large-scale searches. 3) Rule-Based Method: The Rule-Based method [99] uses a deterministic approach, optimizing variables starting from a minimum resource state. It is faster than SA but may be limited by memory bandwidth constraints, impacting performance. 4) Genetic Algorithm: The Genetic Algorithm (GA) [100], [101] simulates natural selection through selection, crossover, and mutation. It iteratively improves solutions based on fitness functions. GA is highly parallelizable and effective for global search but suffers from slow convergence and the risk of premature local optima. 5) Other Methods: Biggs et al. [102] used probability profiles to optimize network phases based on throughput and area trade-offs. [71] applied Fmincon for optimizing CNN accelerators, although its results can vary with initial condi- tions. [103] introduced a Genetic Simulated Annealing (GSA) method, combining the strengths of GA and SA to explore a broader solution space with reduced computational time and fewer parameter dependencies. C. Performance and Resource Modeling The DSE process typically uses two approximate models: resource and performance models. The resource model esti- mates the hardware requirements for a given architecture on the target FPGA, while the performance model predicts system 15 TABLE V SUMMARY AND COMPARISON OF EXISTING CNN ACCELERATION TOOLFLOWS Ref. Toolflow Name Type Device Throughput (Gop s) Energy EFF. (Gop s W) Resource EFF.\n\n--- Segment 36 ---\nToolflow Name Type Device Throughput (Gop s) Energy EFF. (Gop s W) Resource EFF. (GOp s DSP) [91] fpgaConvNet Streaming Structure Xilinx XC7Z020 12.73 7.27 - [14] Angel-Eye Single Engine Xilinx XC7Z045 XC7Z020 XC7Z045: 137 XC7Z020: 84.3 XC7Z045: 14.2 XC7Z020: 24.1 - [94] Snowflake Single Engine Xilinx XC7Z045 GoogLeNet: 116.5 GoogLeNet: 12.3 - [92] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 134.10 VGG16: 123.12 - AlexNet: 0.18 VGG16: 0.14 [93] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 197.40 VGG16: 155.81 GoogLeNet: 165.30 ResNet-152: 188.18 DenseNet-161: 155.57 AlexNet: 49.35 VGG16: 38.95 GoogLeNet: 41.32 ResNet-152: 47.04 DenseNet-161: 38.9 AlexNet: 0.22 VGG16: 0.17 GoogLeNet: 0.184 ResNet-152: 0.209 DenseNet-161: 0.173 [95] f-CNNx Streaming Structure Xilinx XC7Z045 VGG16: 75.00 VGG16 : 18.75 - [96] FMM-X3D Streaming Structure Xilinx ZCU102 X3D-M: 119.83 - X3D-M: 0.055 [97] HARFLOW3D Streaming Structure Xilinx ZCU102 VC709 ZCU102: C3D: 393.37 Slowonly: 177.05 R(2 1)D-18: 173.91 (R(2 1)D-34): 184.29 X3D-M: 43.78 VC709: C3D: 424.14 Slowonly: 229.01 R(2 1)D-18: 185.13 (R(2 1)D-34: 206.39 X3D-M: 56.14 - ZCU102: C3D: 0.156 Slowonly: 0.07 R(2 1)D-18: 0.069 (R(2 1)D-34: 0.073 X3D-M: 0.017 VC709: C3D: 0.117 Slowonly: 0.063 R(2 1)D-18: 0.051 (R(2 1)D-34: 0.057 X3D-M: 0.015 [98] fpgaHART Streaming Structure Xilinx ZCU102 C3D: 130.84 Slowonly: 144.44 R(2 1)D-18: 39.59 (R(2 1)D-34: 34.26 X3D-M: 85.96 - C3D: 0.052 Slowonly: 0.057 R(2 1)D-18: 0.015 (R(2 1)D-34: 0.013 X3D-M: 0.034 [23] Pflow Streaming Structure Xilinx XCZU3EG XCVU13P XCZU3EG: 272.64 XCVU13P: 3686.4 XCZU3EG: 46.5 XCVU13P: 59.4 XCZU3EG: 0.71 XCVU13P: 0.6 TABLE VI COMPARISON OF EXISTING DSE METHODS DSE Method Local Optimum Large Parameter Space Real-Time Low Complexity Low Parameter Dependency Memory Bandwidth Limitations BruteForce Simulated Annealing Rule-Based Method Genetic Algorithm performance (e.g., latency) based on hardware configuration and algorithm characteristics.\n\n--- Segment 37 ---\n(Gop s W) Resource EFF. (GOp s DSP) [91] fpgaConvNet Streaming Structure Xilinx XC7Z020 12.73 7.27 - [14] Angel-Eye Single Engine Xilinx XC7Z045 XC7Z020 XC7Z045: 137 XC7Z020: 84.3 XC7Z045: 14.2 XC7Z020: 24.1 - [94] Snowflake Single Engine Xilinx XC7Z045 GoogLeNet: 116.5 GoogLeNet: 12.3 - [92] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 134.10 VGG16: 123.12 - AlexNet: 0.18 VGG16: 0.14 [93] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 197.40 VGG16: 155.81 GoogLeNet: 165.30 ResNet-152: 188.18 DenseNet-161: 155.57 AlexNet: 49.35 VGG16: 38.95 GoogLeNet: 41.32 ResNet-152: 47.04 DenseNet-161: 38.9 AlexNet: 0.22 VGG16: 0.17 GoogLeNet: 0.184 ResNet-152: 0.209 DenseNet-161: 0.173 [95] f-CNNx Streaming Structure Xilinx XC7Z045 VGG16: 75.00 VGG16 : 18.75 - [96] FMM-X3D Streaming Structure Xilinx ZCU102 X3D-M: 119.83 - X3D-M: 0.055 [97] HARFLOW3D Streaming Structure Xilinx ZCU102 VC709 ZCU102: C3D: 393.37 Slowonly: 177.05 R(2 1)D-18: 173.91 (R(2 1)D-34): 184.29 X3D-M: 43.78 VC709: C3D: 424.14 Slowonly: 229.01 R(2 1)D-18: 185.13 (R(2 1)D-34: 206.39 X3D-M: 56.14 - ZCU102: C3D: 0.156 Slowonly: 0.07 R(2 1)D-18: 0.069 (R(2 1)D-34: 0.073 X3D-M: 0.017 VC709: C3D: 0.117 Slowonly: 0.063 R(2 1)D-18: 0.051 (R(2 1)D-34: 0.057 X3D-M: 0.015 [98] fpgaHART Streaming Structure Xilinx ZCU102 C3D: 130.84 Slowonly: 144.44 R(2 1)D-18: 39.59 (R(2 1)D-34: 34.26 X3D-M: 85.96 - C3D: 0.052 Slowonly: 0.057 R(2 1)D-18: 0.015 (R(2 1)D-34: 0.013 X3D-M: 0.034 [23] Pflow Streaming Structure Xilinx XCZU3EG XCVU13P XCZU3EG: 272.64 XCVU13P: 3686.4 XCZU3EG: 46.5 XCVU13P: 59.4 XCZU3EG: 0.71 XCVU13P: 0.6 TABLE VI COMPARISON OF EXISTING DSE METHODS DSE Method Local Optimum Large Parameter Space Real-Time Low Complexity Low Parameter Dependency Memory Bandwidth Limitations BruteForce Simulated Annealing Rule-Based Method Genetic Algorithm performance (e.g., latency) based on hardware configuration and algorithm characteristics. These models replace the time- consuming task of running CNNs on actual hardware by providing estimates based on hardware parameters.\n\n--- Segment 38 ---\n(GOp s DSP) [91] fpgaConvNet Streaming Structure Xilinx XC7Z020 12.73 7.27 - [14] Angel-Eye Single Engine Xilinx XC7Z045 XC7Z020 XC7Z045: 137 XC7Z020: 84.3 XC7Z045: 14.2 XC7Z020: 24.1 - [94] Snowflake Single Engine Xilinx XC7Z045 GoogLeNet: 116.5 GoogLeNet: 12.3 - [92] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 134.10 VGG16: 123.12 - AlexNet: 0.18 VGG16: 0.14 [93] fpgaConvNet Streaming Structure Xilinx XC7Z045 AlexNet: 197.40 VGG16: 155.81 GoogLeNet: 165.30 ResNet-152: 188.18 DenseNet-161: 155.57 AlexNet: 49.35 VGG16: 38.95 GoogLeNet: 41.32 ResNet-152: 47.04 DenseNet-161: 38.9 AlexNet: 0.22 VGG16: 0.17 GoogLeNet: 0.184 ResNet-152: 0.209 DenseNet-161: 0.173 [95] f-CNNx Streaming Structure Xilinx XC7Z045 VGG16: 75.00 VGG16 : 18.75 - [96] FMM-X3D Streaming Structure Xilinx ZCU102 X3D-M: 119.83 - X3D-M: 0.055 [97] HARFLOW3D Streaming Structure Xilinx ZCU102 VC709 ZCU102: C3D: 393.37 Slowonly: 177.05 R(2 1)D-18: 173.91 (R(2 1)D-34): 184.29 X3D-M: 43.78 VC709: C3D: 424.14 Slowonly: 229.01 R(2 1)D-18: 185.13 (R(2 1)D-34: 206.39 X3D-M: 56.14 - ZCU102: C3D: 0.156 Slowonly: 0.07 R(2 1)D-18: 0.069 (R(2 1)D-34: 0.073 X3D-M: 0.017 VC709: C3D: 0.117 Slowonly: 0.063 R(2 1)D-18: 0.051 (R(2 1)D-34: 0.057 X3D-M: 0.015 [98] fpgaHART Streaming Structure Xilinx ZCU102 C3D: 130.84 Slowonly: 144.44 R(2 1)D-18: 39.59 (R(2 1)D-34: 34.26 X3D-M: 85.96 - C3D: 0.052 Slowonly: 0.057 R(2 1)D-18: 0.015 (R(2 1)D-34: 0.013 X3D-M: 0.034 [23] Pflow Streaming Structure Xilinx XCZU3EG XCVU13P XCZU3EG: 272.64 XCVU13P: 3686.4 XCZU3EG: 46.5 XCVU13P: 59.4 XCZU3EG: 0.71 XCVU13P: 0.6 TABLE VI COMPARISON OF EXISTING DSE METHODS DSE Method Local Optimum Large Parameter Space Real-Time Low Complexity Low Parameter Dependency Memory Bandwidth Limitations BruteForce Simulated Annealing Rule-Based Method Genetic Algorithm performance (e.g., latency) based on hardware configuration and algorithm characteristics. These models replace the time- consuming task of running CNNs on actual hardware by providing estimates based on hardware parameters. Analytical prediction formulas are often used for simplicity and integra- tion into DSE optimization loops, compared to more time- intensive full simulations like ModelSim [15].\n\n--- Segment 39 ---\nThese models replace the time- consuming task of running CNNs on actual hardware by providing estimates based on hardware parameters. Analytical prediction formulas are often used for simplicity and integra- tion into DSE optimization loops, compared to more time- intensive full simulations like ModelSim [15]. 1) Performance Modeling: Liu et al. [6] combined Gaus- sian Process Regression (GPR) [104] with analytical formu- las [71] to estimate CNN latency on accelerators. Makrani et al. [105] introduced the Pyramid framework, using the Minerva tool and ensemble learning to improve throughput prediction accuracy, achieving over 95 accuracy. The In- tel FPGA Power and Thermal Tool [106] estimated power consumption based on FPGA model, frequency, and resource usage, though discrepancies may occur. PowerGear [107] used a Heterogeneous Edge-Centered Graph Neural Network (HEC- GNN) to predict power consumption efficiently. 2) Resource Modeling: Dai et al. [108] developed a dataset with over 1,300 samples to predict resource usage and timing during HLS with machine learning models. Koeplinger et al. [109] introduced DHDL, a hardware representation method that used parameterized templates to capture locality and parallelism in designs. Lee et al. [110] presented a framework 16 for fast, accurate resource estimation by combining high-level hardware descriptions with low-level performance models, offering cycle-level timing and power estimates with high simulation speed. VII. DISCUSSION AND CONCLUSION A. Discussion Insights and optimization: Based on the analysis presented in previous sections, several key challenges in CNN accel- erator design have been identified, along with corresponding optimization strategies to address these challenges. These insights can guide future research and development efforts in enhancing the performance and efficiency of CNN hardware accelerators. 1) Clock Efficiency Optimization. One of the primary concerns in FPGA-based CNN accelerators is maximizing clock efficiency. FPGAs inherently offer fine-grained control over clock frequencies, which can be leveraged to optimize resource utilization. Dynamic frequency scaling, where high clock frequencies are assigned to computation-heavy tasks (e.g., convolution layers) and lower frequencies to less inten- sive operations, helps minimize energy consumption without sacrificing performance.\n\n--- Segment 40 ---\nFPGAs inherently offer fine-grained control over clock frequencies, which can be leveraged to optimize resource utilization. Dynamic frequency scaling, where high clock frequencies are assigned to computation-heavy tasks (e.g., convolution layers) and lower frequencies to less inten- sive operations, helps minimize energy consumption without sacrificing performance. Additionally, optimizing clock net- work design and clock gating reduces unnecessary switching activity by deactivating idle modules, contributing further to power savings. Pipelining techniques are also employed to reduce critical path delays, ensuring that the FPGA operates efficiently at higher frequencies while maintaining stable syn- chronization. In multi-clock domain systems, asynchronous FIFOs or synchronous bridges are utilized to prevent metasta- bility, enhancing stability and performance across different clock domains. 2) Power Efficiency Optimization. Power consumption is a key challenge, particularly in edge applications where energy efficiency is critical. FPGA-based CNN accelerators employ several techniques to reduce power usage. Weight pruning and feature map sparsification lower the computational load, cutting dynamic power. Low-bit-width quantization reduces both memory usage and computation time, further conserving power. On-chip storage and computation fusion minimize power-hungry off-chip memory accesses. Power management methods like dynamic voltage and frequency scaling (DVFS) adjust power and frequency based on workload, while clock gating shuts down idle modules. Architectural strategies such as deep pipelining, parallelism, and reconfigurable units im- prove both computation and energy efficiency. Choosing low- power FPGA devices and optimizing architecture to remove non-essential resources helps minimize static power, making FPGAs ideal for low-power, real-time applications. Future directions: Several promising directions for future research in CNN hardware acceleration can significantly en- hance the performance, efficiency, and versatility of hardware accelerators, especially in the context of emerging applications in AI, computer vision, and edge computing. The following are key areas for continued exploration: 1) Heterogeneous Computing Architectures. Integrating multiple computing units (CPU, GPU, FPGA, ASIC) on a single chip could significantly enhance resource utilization and energy efficiency for CNN tasks. Key technologies like opti- mized Network-on-Chip (NoC) and intelligent task scheduling will drive this development, though challenges in hardware complexity and software support remain.\n\n--- Segment 41 ---\nIntegrating multiple computing units (CPU, GPU, FPGA, ASIC) on a single chip could significantly enhance resource utilization and energy efficiency for CNN tasks. Key technologies like opti- mized Network-on-Chip (NoC) and intelligent task scheduling will drive this development, though challenges in hardware complexity and software support remain. 2) AI-Driven Design Space Exploration (DSE). As CNN models continue to grow in complexity, the need for efficient design space exploration (DSE) methods becomes increasingly important. Traditional DSE methods often require exhaustive searching of vast design spaces, which is time-consuming and computationally expensive. Future research should focus on integrating machine learning techniques, such as reinforcement learning or neural architecture search, into the DSE process. AI-driven DSE can dynamically explore design spaces based on performance metrics, such as throughput, latency, and power consumption, enabling the rapid discovery of opti- mal hardware configurations. By automating this process, researchers can significantly reduce design time and enhance the ability to tailor hardware accelerators to specific CNN models and tasks. 3) Scalable and Adaptive CNN Hardware Accelerators. As CNN models continue to grow in size and complexity, ac- celerators must be able to scale efficiently. Scalable hardware accelerators capable of dynamically adapting to different CNN architectures and input sizes are essential. Future research could focus on developing adaptive FPGA architectures that allow for dynamic reconfiguration based on the model being run. These accelerators would dynamically adjust resources (e.g., memory, processing units) and parallelism levels to accommodate the requirements of different layers in the CNN. Such adaptive systems could support a broader range of CNN applications, from small, lightweight models to large, deep networks, without requiring a complete redesign of the hardware. B. Conclusion CNNs are integral to AI-driven applications, but the chal- lenge remains to achieve low-cost, low-latency, and high- performance solutions. This paper reviewed various hardware acceleration platforms, comparing their strengths and weak- nesses. We also discussed key evaluation metrics for both software and hardware, such as accuracy, latency, and through- put. By examining popular CNN acceleration techniques, we identified the advantages and limitations of each approach. Our evaluation emphasized the importance of computational efficiency and dynamic parallel computing in optimizing CNN hardware accelerators.\n\n--- Segment 42 ---\nBy examining popular CNN acceleration techniques, we identified the advantages and limitations of each approach. Our evaluation emphasized the importance of computational efficiency and dynamic parallel computing in optimizing CNN hardware accelerators. As the field continues to evolve, future research should focus on dynamic parallel computing and heterogeneous on-chip computing for CNN acceleration. With increasing design complexity, effective design space explo- ration will also become crucial. ACKNOWLEDGEMENT The support of Scientific Research Foundation of Hu- nan Provincial Education Department (Key project 23A0087) and Changsha Municipal Natural Science Foundation (No. kq2502001) is gratefully acknowledged. 17 REFERENCES [1] V. K. Chauhan, J. Zhou, P. Lu, S. Molaei, and D. A. Clifton, A brief review of hypernetworks in deep learning, Artificial Intelligence Review, vol. 57, no. 9, p. 250, 2024. [2] L. Chen, S. Li, Q. Bai, J. Yang, S. Jiang, and Y. Miao, Review of im- age classification algorithms based on convolutional neural networks, Remote Sensing, vol. 13, no. 22, p. 4712, 2021. [3] F. Ashiq et al., Cnn-based object recognition and tracking system to assist visually impaired people, IEEE Access, vol. 10, pp. 14 819 14 834, 2022. [4] S. Gidaris and N. Komodakis, Object detection via a multi-region and semantic segmentation-aware cnn model, in Proc. ICCV, 2015, pp. 1134 1142. [5] A. Wang, H. Chen, Z. Lin, J. Han, and G. Ding, Repvit: Revisiting mobile cnn from vit perspective, in Proc. CVPR, 2024, pp. 15 909 15 920. [6] S. Liu, H. Fan, M. Ferianc, X. Niu, H. Shi, and W. Luk, Toward full- stack acceleration of deep convolutional neural networks on fpgas, IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 8, pp. 3974 3987, 2021.\n\n--- Segment 43 ---\n8, pp. 3974 3987, 2021. [7] K. Dai, Z. Xie, and S. Liu, DCP-CNN: Efficient Acceleration of CNNs With Dynamic Computing Parallelism on FPGA, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [8] K. Hegde, R. Agrawal, Y. Yao, and C. W. Fletcher, Morph: Flexible acceleration for 3d cnn-based video understanding, in Proc. MICRO, 2018, pp. 933 946. [9] D. Moolchandani, A. Kumar, and S. R. Sarangi, Accelerating CNN inference on ASICs: A survey, Journal of Systems Architecture, vol. 113, p. 101887, 2021. [10] D. Gyawali, Comparative analysis of cpu and gpu profiling for deep learning models, arXiv preprint arXiv:2309.02521, 2023. [11] Q. Zhang, M. Zhang, T. Chen, Z. Sun, Y. Ma, and B. Yu, Recent ad- vances in convolutional neural network acceleration, Neurocomputing, vol. 323, pp. 37 51, 2019. [12] S. F. Beldianu and S. G. Ziavras, Asic design of shared vector accelerators for multicore processors, in 2014 IEEE 26th International Symposium on Computer Architecture and High Performance Comput- ing, 2014, pp. 182 189. [13] A. Jose, K. T. Alense, L. Gijo, and J. Jacob, FPGA Implementation of CNN Accelerator with Pruning for ADAS Applications, in 2024 IEEE 9th International Conference for Convergence in Technology (I2CT), 2024, pp. 1 6. [14] K. Guo et al., Angel-eye: A complete design flow for mapping cnn onto embedded fpga, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 37, no. 1, pp. 35 47, 2017. [15] S. Liu and W. Luk, Towards an efficient accelerator for dnn-based remote sensing image segmentation on fpgas, in Proc. FPL, 2019, pp. 187 193.\n\n--- Segment 44 ---\nFPL, 2019, pp. 187 193. [16] J. Wen, Y. Ma, and Z. Wang, An efficient fpga accelerator optimized for high throughput sparse cnn inference, in 2020 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS), 2020, pp. 165 168. [17] E. Nurvitadhi et al., Accelerating Binarized Neural Networks: Com- parison of FPGA, CPU, GPU, and ASIC, in Proc. FPT, 2016, pp. 77 84. [18] M. Buckland and F. Gey, The relationship between recall and preci- sion, Journal of the American society for information science, vol. 45, no. 1, pp. 12 19, 1994. [19] R. Yacouby and D. Axman, Probabilistic extension of precision, recall, and f1 score for more thorough evaluation of classification models, in Proceedings of the first workshop on evaluation and comparison of NLP systems, 2020, pp. 79 91. [20] X. Li, Z. Xie, X. Deng, Y. Wu, and Y. Pi, Traffic sign detection based on improved faster r-cnn for autonomous driving, The Journal of Supercomputing, pp. 1 21, 2022. [21] C. Hema and F. P. G. Marquez, Emotional speech recognition using cnn and deep learning techniques, Applied Acoustics, vol. 211, p. 109492, 2023. [22] C. Jiang, D. Ojika, B. Patel, and H. Lam, Optimized fpga-based deep learning accelerator for sparse cnn using high bandwidth memory, in Proc. FCCM, 2021, pp. 157 164. [23] Y. Wan, X. Xie, L. Yi, B. Jiang, J. Chen, and Y. Jiang, Pflow: An end-to-end heterogeneous acceleration framework for cnn inference on fpgas, Journal of Systems Architecture, vol. 150, p. 103113, 2024. [24] S. M. Nabavinejad et al., An overview of efficient interconnection networks for deep neural network accelerators, IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 10, no. 3, pp. 268 282, 2020.\n\n--- Segment 45 ---\n3, pp. 268 282, 2020. [25] S. Li et al., An efficient cnn accelerator using inter-frame data reuse of videos on fpgas, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 30, no. 11, pp. 1587 1600, 2022. [26] W. Huang et al., Fpga-based high-throughput cnn hardware accelerator with high computing resource utilization ratio, IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 8, pp. 4069 4083, 2022. [27] E. Wu, X. Zhang, D. Berman, I. Cho, and J. Thendean, Compute- efficient neural-network acceleration, in Proc. FPGA, 2019, pp. 191 200. [28] H. Fan, S. Liu, Z. Que, X. Niu, and W. Luk, High-performance acceleration of 2-d and 3-d cnns on fpgas using static block floating point, IEEE Transactions on Neural Networks and Learning Systems, vol. 34, no. 8, pp. 4473 4487, 2021. [29] W. Jiang, H. Yu, F. Chen, and Y. Ha, Aos: An automated overclocking system for high-performance cnn accelerator through timing delay measurement on fpga, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 42, no. 9, pp. 2952 2965, 2023. [30] M. Parker, Understanding peak floating-point performance claims, Technical White Paper WP-012220-1.0, 2014. [31] S. Han, H. Mao, and W. J. Dally, Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, arXiv preprint arXiv:1510.00149, 2015. [32] Z. Liu et al., Learning efficient convolutional networks through network slimming, in Proc. ICCV, 2017, pp. 2736 2744. [33] M. Lin et al., Hrank: Filter pruning using high-rank feature map, in Proc. CVPR, 2020, pp. 1529 1538.\n\n--- Segment 46 ---\nCVPR, 2020, pp. 1529 1538. [34] S.-K. Yeom et al., Pruning by explaining: A novel criterion for deep neural network pruning, Pattern Recognition, vol. 115, p. 107899, 2021. [35] N. Liu et al., Autocompress: An automatic dnn structured pruning framework for ultra-high compression rates, in Proc. AAAI, vol. 34, no. 04, 2020, pp. 4876 4883. [36] H. Tan, S. Wu, F. Du, Y. Chen, Z. Wang, F. Wang, and X. Qi, Data pruning via moving-one-sample-out, Advances in Neural Information Processing Systems, vol. 36, 2024. [37] D. T. Nguyen, T. N. Nguyen, H. Kim, and H.-J. Lee, A high- throughput and power-efficient fpga implementation of yolo cnn for object detection, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 27, no. 8, pp. 1861 1873, 2019. [38] S. Kim and H. Kim, Zero-centered fixed-point quantization with iterative retraining for deep convolutional neural network-based object detectors, IEEE Access, vol. 9, pp. 20 828 20 839, 2021. [39] S. Jain, A. Gural, M. Wu, and C. Dick, Trained quantization thresh- olds for accurate and efficient fixed-point inference of deep neural networks, Proceedings of Machine Learning and Systems, vol. 2, pp. 112 128, 2020. [40] Q. Jin et al., F8net: Fixed-point 8-bit only multiplication for network quantization, arXiv preprint arXiv:2202.05239, 2022. [41] D. Lin, S. Talathi, and S. Annapureddy, Fixed point quantization of deep convolutional networks, in Proc. PMLR, 2016, pp. 2849 2858. [42] R. Li et al., Fully quantized network for object detection, in Proc. CVPR, 2019, pp. 2810 2819.\n\n--- Segment 47 ---\nCVPR, 2019, pp. 2810 2819. [43] S. Cao et al., Seernet: Predicting convolutional neural network feature- map sparsity through low-bit quantization, in Proc. CVPR, 2019, pp. 11 216 11 225. [44] M. Rastegari et al., Xnor-net: Imagenet classification using binary convolutional neural networks, in Proc. ECCV, 2016, pp. 525 542. [45] C. Szegedy et al., Going deeper with convolutions, in Proc. CVPR, 2015, pp. 1 9. [46] F. N. Iandola, SqueezeNet: AlexNet-level accuracy with 50x fewer pa- rameters and 0.5 MB model size, arXiv preprint arXiv:1602.07360, 2016. [47] A. G. Howard, Mobilenets: Efficient convolutional neural networks for mobile vision applications, arXiv preprint arXiv:1704.04861, 2017. [48] M. Sandler et al., Mobilenetv2: Inverted residuals and linear bottle- necks, in Proc. CVPR, 2018, pp. 4510 4520. [49] A. Howard et al., Searching for mobilenetv3, in Proc. ICCV, 2019, pp. 1314 1324. [50] K. Han et al., Ghostnet: More features from cheap operations, in Proc. CVPR, 2020, pp. 1580 1589. [51] J. Chen et al., Run, don t walk: chasing higher flops for faster neural networks, in Proc. CVPR, 2023, pp. 12 021 12 031. [52] G. Hinton, Distilling the knowledge in a neural network, arXiv preprint arXiv:1503.02531, 2015. 18 [53] W. Son, J. Na, J. Choi, and W. Hwang, Densely guided knowledge distillation using multiple teacher assistants, in Proc. ICCV, 2021, pp. 9395 9404. [54] A. Hao, Y. Min, and X. Chen, Self-mutual distillation learning for continuous sign language recognition, in Proc. ICCV, 2021, pp.\n\n--- Segment 48 ---\n[54] A. Hao, Y. Min, and X. Chen, Self-mutual distillation learning for continuous sign language recognition, in Proc. ICCV, 2021, pp. 11 303 11 312. [55] S. Sun et al., Logit standardization in knowledge distillation, in Proc. CVPR, 2024, pp. 15 731 15 740. [56] M. Alwani, H. Chen, M. Ferdman, and P. Milder, Fused-layer cnn accelerators, in Proc. MICRO, 2016, pp. 1 12. [57] T. Abtahi, C. Shea, A. Kulkarni, and T. Mohsenin, Accelerating convolutional neural network with fft on embedded hardware, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 26, no. 9, pp. 1737 1749, 2018. [58] A. Syafeeza et al., Convolutional neural networks with fused layers applied to face recognition, International Journal of Computational Intelligence and Applications, vol. 14, no. 03, p. 1550014, 2015. [59] S. Liu, H. Fan, and W. Luk, Accelerating fully spectral cnns with adaptive activation functions on fpga, in Proc. DATE, 2021, pp. 1530 1535. [60] M. Mathieu, M. Henaff, and Y. LeCun, Fast training of convolutional networks through ffts, arXiv preprint arXiv:1312.5851, 2013. [61] O. Rippel, J. Snoek, and R. P. Adams, Spectral representations for convolutional neural networks, Advances in neural information processing systems, vol. 28, 2015. [62] H. Zeng et al., A framework for generating high throughput cnn implementations on fpgas, in Proc. FPGA, 2018, pp. 117 126. [63] S. Liu, H. Fan, and W. Luk, Design of fully spectral cnns for efficient fpga-based acceleration, IEEE Transactions on Neural Networks and Learning Systems, 2022.\n\n--- Segment 49 ---\n117 126. [63] S. Liu, H. Fan, and W. Luk, Design of fully spectral cnns for efficient fpga-based acceleration, IEEE Transactions on Neural Networks and Learning Systems, 2022. [64] S. O. Ayat, M. Khalil-Hani, A. A.-H. Ab Rahman, and H. Abdellatef, Spectral-based convolutional neural network without multiple spatial- frequency domain switchings, Neurocomputing, vol. 364, pp. 152 167, 2019. [65] T. Watanabe and D. F. Wolf, Image classification in frequency domain with 2srelu: a second harmonics superposition activation function, Applied Soft Computing, vol. 112, p. 107851, 2021. [66] A. Lavin and S. Gray, Fast algorithms for convolutional neural networks, in Proc. CVPR, 2016, pp. 4013 4021. [67] Y. Liang, L. Lu, Q. Xiao, and S. Yan, Evaluating fast algorithms for convolutional neural networks on fpgas, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, no. 4, pp. 857 870, 2019. [68] J. Yepez and S.-B. Ko, Stride 2 1-D, 2-D, and 3-D Winograd for convolutional neural networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 4, pp. 853 863, 2020. [69] M. Yang, S. Cao, W. Zhang, Y. Li, and Z. Jiang, Loop-tiling based compiling optimization for cnn accelerators, in 2023 IEEE 15th International Conference on ASIC (ASICON), 2023, pp. 1 4. [70] H. Huang, X. Hu, X. Li, and X. Xiong, An efficient loop tiling framework for convolutional neural network inference accelerators, IET circuits, devices systems, vol. 16, no. 1, pp. 116 123, 2022. [71] S. Liu et al., Optimizing cnn-based segmentation with deeply cus- tomized convolutional and deconvolutional architectures on fpga, ACM Trans.\n\n--- Segment 50 ---\n116 123, 2022. [71] S. Liu et al., Optimizing cnn-based segmentation with deeply cus- tomized convolutional and deconvolutional architectures on fpga, ACM Trans. Reconfigurable Technol. Syst., vol. 11, no. 22, Dec. 2018. [72] R. Zhao et al., Optimizing cnn-based object detection algorithms on embedded fpga platforms, in Proc. ARC, 2017, pp. 255 267. [73] A. Rahman, J. Lee, and K. Choi, Efficient fpga acceleration of convolutional neural networks using logical-3d compute array, in Proc. DATE, 2016, pp. 1393 1398. [74] Y. Ma et al., End-to-end scalable fpga accelerator for deep residual networks, in Proc. ISCAS, 2017, pp. 1 4. [75] C. Zhang et al., Optimizing fpga-based accelerator design for deep convolutional neural networks, in Proc. FPGA, 2015, pp. 161 170. [76] Y. Ma et al., Optimizing loop operation and dataflow in fpga accel- eration of deep convolutional neural networks, in Proc. FPGA, 2017, pp. 45 54. [77] Y. Yu et al., OPU: An FPGA-based overlay processor for convo- lutional neural networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 1, pp. 35 47, 2019. [78] Z. Zhou, X. Duan, and J. Han, A design framework for generating energy-efficient accelerator on fpga toward low-level vision, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2024. [79] T.-H. Wu, C. Shu, and T.-T. Liu, An efficient fpga-based dilated and transposed convolutional neural network accelerator, IEEE Transac- tions on Circuits and Systems I: Regular Papers, 2024. [80] H. Li et al., A high performance fpga-based accelerator for large-scale convolutional neural networks, in Proc. FPL, 2016, pp. 1 9.\n\n--- Segment 51 ---\nFPL, 2016, pp. 1 9. [81] N. Suda et al., Throughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks, in Proc. FPGA, 2016, pp. 16 25. [82] Z. Li et al., A high-performance pixel-level fully pipelined hardware accelerator for neural networks, IEEE Transactions on Neural Net- works and Learning Systems, 2024. [83] N. Shah, P. Chaudhari, and K. Varghese, Runtime programmable and memory bandwidth optimized fpga-based coprocessor for deep convolutional neural network, IEEE Transactions on Neural Networks and Learning Systems, vol. 29, no. 12, pp. 5922 5934, 2018. [84] F. H. Khan, M. A. Pasha, and S. Masud, Towards designing a hardware accelerator for 3d convolutional neural networks, Computers and Electrical Engineering, vol. 105, p. 108489, 2023. [85] Q. Zhang and X. Zhang, Design of a low-latency general-purpose cnn hardware accelerator based on pulsed arrays on fpgas, in Proc. ICNC-FSKD, 2024, pp. 1 8. [86] J. Li et al., An fpga-based high-throughput keypoint detection acceler- ator using convolutional neural network for mobile robot applications, in 2022 IEEE Asia Pacific Conference on Postgraduate Research in Microelectronics and Electronics (PrimeAsia), 2022, pp. 81 84. [87] H.-Y. Ting et al., System services for reconfigurable hardware accel- eration in mobile devices, in Proc. ReConFig, 2018, pp. 1 6. [88] X. Chen, J. Li, and Y. Zhao, Hardware resource and computational density efficient cnn accelerator design based on fpga, in 2021 IEEE International Conference on Integrated Circuits, Technologies and Applications (ICTA), 2021, pp. 204 205. [89] H. Wang and C. Ma, An optimization of im2col, an important method of cnns, based on continuous address access, in 2021 IEEE Interna- tional Conference on Consumer Electronics and Computer Engineering (ICCECE), 2021, pp. 314 320.\n\n--- Segment 52 ---\n[89] H. Wang and C. Ma, An optimization of im2col, an important method of cnns, based on continuous address access, in 2021 IEEE Interna- tional Conference on Consumer Electronics and Computer Engineering (ICCECE), 2021, pp. 314 320. [90] S. Lee, D. Kim, D. Nguyen, and J. Lee, Double MAC on a DSP: Boosting the performance of convolutional neural networks on FP- GAs, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 38, no. 5, pp. 888 897, 2018. [91] S. I. Venieris and C.-S. Bouganis, fpgaConvNet: A framework for mapping convolutional neural networks on FPGAs, in Proc. FCCM, 2016, pp. 40 47. [92] , Latency-driven design for fpga-based convolutional neural net- works, in Proc. FPL, 2017, pp. 1 8. [93] , fpgaConvNet: Mapping regular and irregular convolutional neural networks on FPGAs, IEEE transactions on neural networks and learning systems, vol. 30, no. 2, pp. 326 342, 2018. [94] V. Gokhale et al., Snowflake: An efficient hardware accelerator for convolutional neural networks, in Proc. ISCAS, 2017, pp. 1 4. [95] S. I. Venieris and C.-S. Bouganis, f-CNNx: A toolflow for mapping multiple convolutional neural networks on FPGAs, in Proc. FPL, 2018, pp. 381 3817. [96] P. Toupas, C.-S. Bouganis, and D. Tzovaras, FMM-X3D: FPGA-based modeling and mapping of X3D for Human Action Recognition, in Proc. ASAP, 2023, pp. 119 126. [97] P. Toupas, A. Montgomerie-Corcoran, C.-S. Bouganis, and D. Tzo- varas, Harflow3d: A latency-oriented 3d-cnn accelerator toolflow for har on fpga devices, in Proc.\n\n--- Segment 53 ---\n119 126. [97] P. Toupas, A. Montgomerie-Corcoran, C.-S. Bouganis, and D. Tzo- varas, Harflow3d: A latency-oriented 3d-cnn accelerator toolflow for har on fpga devices, in Proc. FCCM, 2023, pp. 144 154. [98] P. Toupas, C.-S. Bouganis, and D. Tzovaras, fpgaHART: A toolflow for throughput-oriented acceleration of 3D CNNs for HAR onto FPGAs, in Proc. FPL, 2023, pp. 86 92. [99] A. Montgomerie-Corcoran, Z. Yu, and C.-S. Bouganis, Samo: Opti- mised mapping of convolutional neural networks to streaming archi- tectures, in Proc. FPL, 2022, pp. 418 424. [100] V. A. Shim, K. C. Tan, and H. Tang, Adaptive memetic computing for evolutionary multiobjective optimization, IEEE transactions on cybernetics, vol. 45, no. 4, pp. 610 621, 2014. [101] Y. Yu, Y. Li, S. Che, N. K. Jha, and W. Zhang, Software-defined design space exploration for an efficient dnn accelerator architecture, IEEE Transactions on Computers, vol. 70, no. 1, pp. 45 56, 2020. [102] B. Biggs, C.-S. Bouganis, and G. Constantinides, ATHEENA: A Toolflow for Hardware Early-Exit Network Automation, in Proc. FCCM, 2023, pp. 121 132. [103] Z. Xie et al., Design space exploration of cnn accelerators based on gsa algorithm, in Proc. ICSIP, 2024, pp. 319 323. [104] C. Williams and C. Rasmussen, Gaussian processes for regression, Advances in neural information processing systems, vol. 8, 1995. [105] H. M. Makrani et al., Pyramid: Machine learning framework to estimate the optimal timing and resource usage of a high-level synthesis design, in Proc. FPL, 2019, pp. 397 403.\n\n--- Segment 54 ---\nFPL, 2019, pp. 397 403. 19 [106] Intel , Intel FPGA Power and Thermal Calculator User Guide , 2024. [Online]. Available: docs programmable 683445 22-2 overview-of-the.html [107] Z. Lin et al., Powergear: Early-stage power estimation in fpga hls via heterogeneous edge-centric gnns, in Proc. DATE, 2022, pp. 1341 1346. [108] S. Dai et al., Fast and accurate estimation of quality of results in high-level synthesis with machine learning, in Proc. FCCM, 2018, pp. 129 132. [109] D. Koeplinger et al., Automatic generation of efficient accelerators for reconfigurable hardware, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 115 127, 2016. [110] D. Lee, L. K. John, and A. Gerstlauer, Dynamic power and per- formance back-annotation for fast and accurate functional hardware simulation, in Proc. DATE, 2015, pp. 1126 1131. Junye Jiang received the B.Sc. degree from the School of Electrical and Information Engineering, Wuhan Institute of Technology, Wuhan, China, in 2023. He is currently a M.Sc. student in the Computer Vision and High-Performance Computing group at Hunan Normal University. His research interests include the hardware acceleration of con- volutional neural networks (CNNs) and neural ar- chitecture design optimizations applied to computer vision tasks. Yaan Zhou received the B.Sc. degree in Electronic Information Engineering of Central South Univer- sity of Forestry and Technology, Changsha, China, in 2021. He is currently a M.Sc. student in the Computer Vision and High-Performance Computing group at Hunan Normal University. His current research includes the hardware acceleration of con- volutional neural networks (CNNs). Yuanhao Gong received the B.S. degree in Elec- tronic Information Science and Technology from Hunan Normal University, Changsha, China, in 2023. He is currently pursuing the M.S. degree in Electronic Science and Technology with Hunan Nor- mal University. His research interests mainly focus on particle filter methods and efficient hardware architectures for particle filters. Haoxuan Yuan received the B.S.\n\n--- Segment 55 ---\nHis research interests mainly focus on particle filter methods and efficient hardware architectures for particle filters. Haoxuan Yuan received the B.S. degree in Elec- tronic Information Science and Technology from Hunan Normal University, Changsha, China, in 2024. He is currently a M.Sc. student in the Computer Vision and High-Performance Computing group at Hunan Normal University. His research interests include the reconfigurable acceleration of artificial intelligence algorithms. Shuanglong Liu received the B.Sc. and M.Sc. de- grees from the Department of Electronic Engineer- ing, Tsinghua University, Beijing, China, in 2010 and 2013 respectively, and Ph.D. degree in Electric Engineering from Imperial College London, London, U.K, in 2017. From 2017 to 2020, he was a Research Associate with the Department of Computing, Impe- rial College London. He is currently a Distinguished Professor in the School of Physics and Electronics, Hunan Normal University, Changsha, China. He has published over 40 research papers in peer-referred journals and international conferences. His current research interests include reconfigurable and high performance computing for Convolutional Neural Networks (CNNs) and statistical inference problems.\n\n