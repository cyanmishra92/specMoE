=== ORIGINAL PDF: 2502.08807v2_InTAR_Inter-Task_Auto-Reconfigurable_Accelerator_D.pdf ===\n\nRaw text length: 52953 characters\nCleaned text length: 52352 characters\nNumber of segments: 31\n\n=== CLEANED TEXT ===\n\nInTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs Zifan He , Anderson Truong , Yingqi Cao and Jason Cong University of California, Los Angeles University of California, San Diego Email: Abstract The rise of deep neural networks (DNNs) has driven an increased demand for computing power and memory. Modern DNNs exhibit high data volume variation (HDV) across tasks, which poses challenges for FPGA acceleration: conventional accelerators rely on fixed execution patterns (dataflow or sequen- tial) that can lead to pipeline stalls or necessitate frequent off-chip memory accesses. To address these challenges, we introduce the Inter-Task Auto-Reconfigurable Accelerator (InTAR) 1, a novel accelerator design methodology for HDV applications on FPGAs. InTAR combines the high computational efficiency of sequential execution with the reduced off-chip memory overhead of dataflow execution. It switches execution patterns automatically with a static schedule determined before circuit design based on resource constraints and problem sizes. Unlike previous reconfigurable accelerators, InTAR encodes reconfiguration schedules during circuit design, allowing model-specific optimizations that allocate only the necessary logic and interconnects. Thus, InTAR achieves a high clock frequency with fewer resources and low reconfigura- tion time. Furthermore, InTAR supports high-level tools such as HLS for fast design generation. We implement a set of multi-task HDV DNN kernels using InTAR. Compared with dataflow and sequential accelerators, InTAR exhibits 1.8 and 7.1 speedups correspondingly. Moreover, we extend InTAR to GPT-2 medium as a more complex example, which is 3.65 39.14 faster and a 1.72 10.44 more DSP efficient than SoTA accelerators (Allo and DFX) on FPGAs. Additionally, this design demonstrates 1.66 7.17 better power efficiency than GPUs. I. INTRODUCTION The development of deep neural networks (DNNs) has driven numerous breakthroughs in AI-assisted applications [1], [2] while intensifying demands for compute efficiency due to increasing network complexity and memory requirements. As DNN applications grow in size [3], [4], larger data volumes generated between operations strain hardware resources dur- ing deployment. Consequently, a key challenge in designing DNN accelerators is: Given resource constraints imposed by physical hardware limitations or power considerations, how can execution scheduling be improved to minimize end-to-end latency while accounting for hardware overhead? A common strategy of DNN accelerator designs is grouping computations belonging to the same matrix vector operation into a task and scheduling computations at the task level. For example, self-attention [5] of transformer models involves two tasks: QKT and softmax operation. Previous acceler- ators focus on two types of fixed task execution patterns: dataflow (also called spatial) [6], [7] or sequential (also called 1 temporal) executions [8] [11]. For dataflow executions, each processing element (PE) specializes in a single task. Data is streamed through FIFOs connecting PEs for dependent compu- tations with minimal on-chip buffer for data reuse. Depending on the dataflow graph, it includes both task-pipeline and task-parallel executions. Task-pipeline streams data between dependent tasks, and task-parallel executes independent tasks in parallel. For sequential executions, a versatile PE processes tasks serially and parallelizes the computations inside each task. Resources are reused across operators. Although both execution patterns exhibit strong performance, each of them faces a major challenge for large DNN applications [12], [13]: Dataflow: reduced computation resource efficiency. Due to the coarse and fine-grained pipeline stalls from the data dependencies, some PEs in dataflow execution may remain idle, leading to low computation resource efficiency and high end-to-end latency. Sequential: high off-chip memory access overhead. Out- put data from previous tasks will be stored and reused by the next tasks in sequential execution. Moreover, data may be cached and remain unused until the consumer tasks start executions. Due to the limited on-chip resources, it is often difficult to store all output data in SRAM. If storing data in the off-chip memory, the accelerator will suffer from high latency and energy consumption of off-chip memory access. TABLE I MINIMUM AND MAXIMUM INTERMEDIATE DATA SIZE OF SAMPLE DNNS Model Type Min Data Size Max Data Size GPT 2 [14] Transformer 64L max(4096L, L2) Llama 2 7B [3] Transformer 128L max(22016L, L2) ResNet-50 [15] CNN 151K 802K ResNet-152 CNN 151K 802K ResNext-101 [16] CNN 151K 1.04M To address both challenges simultaneously, we observe another commonly neglected feature of DNNs to exploit: high data-volume variation (HDV). Table I lists a set of DNN applications and the minimum maximum input output data size between operations. L is the sequence length for transformer models. These models offer both a large maximum data size (up to 88 MB with L 2048) and a significant data size variation (5.3 172 from minimum to maximum data size). A potential solution to improve computation resource efficiency while circumventing off-chip memory access for intermediate data is to execute sequentially for tasks that arXiv:2502.08807v2 [cs.AR] 4 Apr 2025 produce small data within on-chip memory capacity, and stream data for tasks that produce large data. Therefore, we propose a novel accelerator design paradigm on FPGAs: inter-task auto-reconfigurable accelerator (INTAR). INTAR can switch execution patterns automatically based on on-chip memory and computation resources. When a task produces large intermediate data, INTAR pipelines multiple tasks to avoid accessing off-chip memory for this data. Otherwise, INTAR will process tasks sequentially to maximize computational efficiency by eliminating pipeline stalls. Compared with other reconfigurable accelerators [17] [21], INTAR allows model-specific circuit optimization that keeps only necessary control logics and interconnects for reconfiguration. Hence, InTAR requires fewer reconfiguration resources, achieves a high clock frequency, and has a low reconfiguration overhead (10 to 20 ns) (Section III). Moreover, since computations are reconfigured at the task level, INTAR is one of the first works regarding FPGA-based reconfigurable accelerators that support high-level hardware generation tools such as High-Level Synthesis (HLS) [22] for fast accelerator design. Specifically, our contributions include: We present INTAR, a novel accelerator design paradigm on FPGAs that balances the memory access and computational efficiency tradeoff for HDV applications (Section III). We illustrate the techniques to design INTAR with HLS and important considerations in placement and routing for hardware implementation of INTAR on FPGAs (Section IV). Evaluations. We implement INTAR for five multi-task ker- nels that broadly exist in DNN applications (Self-Attention, Multi-layer CNN, FFN layer, VAE, and Gating Network) to show its advantages. InTAR exhibits 1.8 and 7.1 speedup compared with the corresponding dataflow and se- quential accelerator. We further present InTAR on the GPT-2 medium model for a complete DNN example, which achieves a speedup of 3.65 39.14 and a 1.72 10.44 improve- ment in DSP efficiency compared to the SoTA accelerators (Allo [6] and DFX [23]). Moreover, INTAR demonstrated 1.66 7.17 better power efficiency compared to GPUs. II. BACKGROUND AND MOTIVATIONS A. Dataflow and Sequential DNN Accelerators Dataflow accelerators allocate resources per DNN task and pipeline computations to conserve memory while preserving performance. For example, FlexCNN [7] composes CNN- based accelerators in HLS, GenGNN [24] accelerates GNNs, and Allo [6], [12] designs FPGA accelerators by parsing Python scripts into HLS code for each operator, automatically combining layer optimizations with customization primitives. On the other hand, sequential accelerators process layers in DNN sequentially and attempt to utilize all available resources. GPUs and TPUs [8] are considered instruction-based sequen- tial accelerators that support various DNNs. ZyncNet [25] is a CNN accelerator deployed on Zync SoC with a specialized topology. FlightLLM [26] is a vector processor implemented on Alveo U280 FPGA for Llama model inference. B. Hybrid Accelerators Some previous works attempt to mitigate the drawbacks of dataflow and sequential accelerators by having a hybrid execution pattern. A naive approach is allocating part of the resource for dataflow execution and the rest for sequential execution [11]. A more recent method is to allocate several PEs and schedule multiple tasks to the same PE. For instance, SSR [13] is a hybrid accelerator implemented on AMD Xilinx Ver- sal ACAP devices for vision transformers. By reserving two groups of AI engines for two types of matrix multiplies, SSR allows sequential executions of matrix multiplies (GEMM) to minimize latency and enables data forwarding between dependent GEMMs to reduce the memory cost. However, PE specializations of hybrid accelerators impede the flexibility of execution pattern switching, which may lead to stalls. Section II-E will further discuss the inefficiency of hybrid accelerators with an example. C. Reconfigurable Accelerators A reconfigurable accelerator can change its microarchitec- ture on the fly. A common implementation of such accelerators is the coarse-grain reconfigurable array (CGRA) [18], [20], [27], which compiles a dataflow graph into a configuration and modifies switches connecting PEs accordingly. However, CGRA has complex reconfiguration logic to ensure general- izability. This introduces substantial area overheads [28] and additional wiring that negatively affects the timing [20]. An alternative method to reconfigure resources is dynamic partial reconfiguration (DPR) [29], which reserves regions on FPGAs to load partial bitstreams for runtime reconfiguration. While having a low resource overhead for the configuration controller, it has a higher reconfiguration overhead (10 to 100 ms) than FPGA-based CGRA (around 10 ns). Other reconfigurable accelerators focus on optimization under only specific computation scenarios [17], [19], [30]. For example, SET [17] performs inter-layer scheduling of convo- lution networks onto the tiled accelerators utilizing a time- space resource-allocation tree. FEATHER [30] mainly resolves reduction efficiency in each CNN layer by reconfiguring the reduction dataflow. Considering these limitations, existing reconfigurable accel- erators can hardly apply to a broad range of modern DNNs, which necessitates the development of a new design paradigm. D. Applications with High Data Volume Variation A multi-task application is defined as high data-volume variation (HDV) if the input output data sizes between tasks are highly varied. Many of the DNNs are HDV for two reasons: Modern DNNs have complex dependencies. Data is pro- duced several layers in advance and can only be discarded after they are reused. Hence, the data production and elimination rates vary widely. The objectives of many DNN workloads involve feature extraction (e.g., classification) that reduces the data size, or data construction (e.g., image text generation), which increases Fig. 1. Example of mapping computations of attention and linear projection of value matrix to the dataflow, sequential, hybrid accelerators, and INTAR. The sequence length is 256, the hidden dimension is 1024, and the weights are 1024 1024 matrices. Depending on the scheduling of the rest of the computations in the entire model, INTAR can choose between task-parallel (left) and sequential modes (right) for better locality. Both will have the same latency. the data size. By the nature of these workloads, the data size will change during execution. E. A Motivating Case Study: Attention Linear Projection We provide a case study to illustrate the advantages of inter- task reconfiguration for HDV DNNs: calculating the attention score and a linear projection of the value matrix. Figure 1 shows the dataflow graph of the attention layer and the timeline for each execution pattern. The direction of the arrows indicates the output production order, and the lengths represent the production rate. Here we focus on calculating Q, K, V , and A in the dashed square. In this example, we assume that: Matrix multiplies are weight-stationary as weights are al- ready stored in on-chip memory. Outputs (A and V ) will be written into off-chip memory. The rest of the on-chip memory size will only store either Q or K, but not both. Input sequence is short, which means A can be stored in on-chip buffer with K or Q. The pattern of data size variation is first increase then decrease. Dataflow execution instantiates PEs for each task. There are three ways to stream and cache data across tasks, illustrated in Figure 1: outer product with aligned and un- aligned production rates of Q and K (Figure 1(a) and (b)), and inner product with unaligned production rates (Figure 1(c)). Inner products with aligned production rates require caching of both Q and K, which is infeasible. Each case incurs compute resource idling due to either varied workload size of A, back- pressure from streaming, or early completion of tasks. Sequential execution instantiates a single PE to execute each task serially, as shown in Figure 1(d). Due to insufficient on- chip memory capacity, it needs to read from and write to the off-chip memory, increasing the overall latency. Some hybrid accelerators [13] can mitigate the issues by executing Q and V sequentially and pipeline K and A. However, since PEs are specialized for either streaming data or cached buffer [13], computation of V cannot be assigned to PE 2 and PE 3, and these two PEs are idle at the beginning, waiting for the arrival of data for Q. For INTAR (Figure 1(f)), PEs can be reconfigured to either sequential or dataflow execution at different times. Therefore, PEs can be efficiently allocated while off-chip memory access for intermediate data is avoided. Section III discusses how INTAR manages this in detail. III. INTER-TASK AUTO-RECONFIGURABLE ACCELERATOR To improve resource efficiency, INTAR minimizes off-chip memory access by employing dataflow execution when nec- essary while keeping the rest of the execution as sequential as possible. The finest granularity of execution pattern switching is the task, which represents a group of computations belong- ing to the same matrix or vector operation. Execution pattern switching is achieved through automatic reconfiguration based on instructions stored in the static configuration buffer (Section III-B), which is hardened into the design. These instructions trigger computation behavior and data movement changes. Unlike other reconfigurable [17], [18], [20], [21], [30] and overlay accelerators [31], [32], which prioritize abstraction and fast compilation, InTAR focuses on reconfiguration for optimization. The reconfiguration schedule in InTAR is deter- mined at circuit design time rather than relying on post-design external commands. For every application, a static schedule is created based on the specific application and resource constraints, and the circuit is then tailored accordingly. This approach reduces reliance on general-purpose control units and cross-bars, retaining only essential reconfiguration logic and interconnects. The result is low resource utilization for reconfiguration controls (49 LUT reduction for a design with 3 throughput compared to [20] for multiple GEMMs), high clock frequency (2.41 that of [20]), and low reconfiguration latency (106 faster than DPR [21], and comparable to [20]) compared with FPGA-based CGRA and DPR approaches. Additionally, InTAR handles reconfiguration at the task level, enabling rapid and convenient development with high-level tools like HLS. This makes InTAR well-suited for FPGAs to adapt quickly to new DNN applications. Lastly, InTAR s reconfiguration scope is not limited to specific computations. Table II compares InTAR with prior DNN accelerator works. Fig. 2. Left: Architecture template of INTAR. Compute cores (CC) compute linear operations (e.g., GEMM, ConvNet), and SFUs compute non-linear operations (e.g., softmax, GeLU). Each CC contains a scratchpad memory, a reconfigurable MAC unit array, a reduction unit, and a data movement control unit. Dashed lines indicate the candidates for connection between CCs and SFUs. Right: example architectures for each execution mode within the template, with n, m 2. For a schedule, we will compose the architectures of the required modes to keep only the necessary interconnects and logic. TABLE II COMPARISON BETWEEN INTAR AND OTHER DNN ACCELERATION WORKS. THE MAJOR REASON FOR HAVING IDLE COMPUTE RESOURCES IS DESCRIBED IN THE PARENTHESES FOR EACH WORK. Prior works Platform Contribution Type Accelerator Inter-task Idle Compute Intermediate When Reconfig Reconfiguration Model-spec. Category Schedule Resource Exist? Data Movement Determined Design Scope Design Opt. Allo [6] FPGA Domain-Spec. Lang. Dataflow Yes (dataflow execution) On-chip only - - FQ-BERT [10], DFX [23] FPGA Accelerator Sequential No On- Off-chip - - SSR [13] FPGA Accelerator Hybrid Yes (PE specialization) On-chip only - - FPCA [18], OverGen [20] FPGA Architecture Reconfigurable Yes (single DNN) On- Off-chip After circuit generation General FEATHER [30] FPGA Accelerator Reconfigurable No On- Off-chip After circuit generation Reduction Network SET [17] ASIC Scheduler Reconfigurable Yes (sub-opt. sched.) On- Off-chip N A Tiled Accelerator INTAR FPGA Design Paradigm Reconfigurable No On-chip only Circuit design time General A. Execution Modes of INTAR Depending on the available resources on FPGAs and the properties of tasks, INTAR configures the resources into sequential, task-pipeline, or task-parallel mode, corresponding to the dataflow and sequential execution patterns mentioned in Section I. Sequential: All resources perform a single task in parallel. Task-pipeline: Same as dataflow execution with pipelined dependent tasks. Computational resources are partitioned pro- portionally to the workloads. Data are streamed from one task to another using FIFOs. Task-parallel: Same as dataflow execution with independent tasks running in parallel. The resource efficiency is the same as the sequential mode, but it may have a better locality for subsequent tasks (e.g., for the example in upper left of Figure 1, if calculating V and O are pipelined, then running Q and V in parallel and assigning V to the PE that will compute V avoids data aggregation overhead from other PEs). Figure 1(f) illustrates the task assignment and timeline of INTAR for the case study in Section II-E. PEs 1 and 2 handle both attention computation and linear projections. Depending on the subsequent operations, INTAR can choose either task- parallel or sequential mode for Q and V . If the scheduler decides to serialize computing V and O, then the sequential mode is more suitable to allow PEs 1 and 2 to execute cooperatively. If the scheduler pipelines computations of V and O, then task-parallel mode is preferred to improve locality. Both choices will not affect the latency but will determine the data movement overhead of the following tasks. After calculating Q and V , INTAR switches to task-pipeline mode to compute K and A, since storing Q consumes all scratchpad memory. B. Architecture Template Figure 2 is the architecture template of INTAR for HDV DNN applications, which consists of a grid of compute cores (CC) computing linear operations and special function units (SFU) inserted in between. Each CC contains: Reconfigurable PE Array (PEA): a multi-dimensional array of PEs that allows changing inputs and precisions. Data communication between PEs is either in a systolic array style or by broadcasting the data to all PEs. Each PE produces the complete or partial result of the linear operation. Scratchpad Memory: store intermediate and weight data. Reduction Unit: read the partial outputs from the PEA if needed and perform accumulations. Data Movement Control Unit: determine the flow of data to the scratchpad memory and other CCs SFUs. When switching execution modes, a global instruction reader will read both the configuration instructions from a static buffer and the input size passed as the kernel argument. It will then modify part of each instruction related to the input size (e.g., the loop bounds) and send it to the CCs. The reconfigurable PEA and data movement control will read the instructions and propagate them among the CCs. The instruc- tion sequence is determined by the static schedule of mode switching at the circuit design time. Each instruction contains a stage index and several loop bounds (e.g., reconfiguring the three loop bounds for GEMMs). A stage refers to the execution of a single or a group of tasks that follows one of the execution modes, seen in Figure 1(f), e.g., computing Q and V in stage 1 and K and A in stage 2. Each CC has a specified behavior for each stage, orchestrated by multiplexers with stage indices as the select signals. Notice that the stage-specific behaviors vary from one application to another. Thus, INTAR only needs to handle the reconfigurations defined in that application-specific schedule. The reconfiguration overhead consists of four parts: reading, modifying, sending, and decoding instructions to generate signals for multiplexers in the CCs. Each of these is done in a single cycle. Thus, reconfigurations have a 4- cycle overhead (10 to 20 ns, depending on frequency), which is negligible compared to modern DNN inference latency. The template has several design parameters for users to derive a specific architecture, including: Number of rows n and columns m of the CCs. Instantiation of the SFUs, which determine the positions and computations of each SFU. FIFO connections between the CCs and SFUs. Scratchpad memory size and number of memory ports. Reconfigurable PE array dimensions. Depending on the schedule, we assign values to these pa- rameters to perform the three execution modes mentioned in Section III-A. The right of Figure 2 depicts the example ar- chitectures of each execution mode derived from the template for n, m 2 for the case study in Section II-E. The colors of connections indicate the purpose of communications (read input and weights, perform reductions, redistribute output, or pipeline intermediate results). For a specific schedule, we com- pose the derived architectures of the execution modes utilized to get the final circuit design. Common interconnections and control logic across execution modes will be merged, and unused interconnects will never be introduced. IV. DESIGNING ACCELERATORS IN INTAR The first step in designing INTAR accelerators is perform- ing design space exploration (DSE). Given a dataflow graph (DFG) and device constraints, one searches through various combinations of architecture parameters and task execution modes (see Section III) to minimize total latency. A. Finding Schedule and Architecture Template Parameters In this work, we first topologically sort the tasks based on dependencies in the DFG and apply a heuristic-based DSE: If the total size of output and previously cached data is larger than the memory constraint, then select the task-pipeline mode. Otherwise, if task-parallel mode can improve locality for the subsequent tasks as illustrated in Section III-A, then pick task-parallel mode. If not, then choose the sequential mode. Non-linear operations are pipelined with previous tasks. Then, we analyze the generated schedule and the hardware configurations and determine architecture template parameters with the following heuristics: The schedule infers the instan- tiation of the SFUs and FIFO connections. The column count of CCs equals the maximum number of tasks of all stages, and the row count is the number of dies on the FPGA necessary to improve floorplanning. We compute the PEA dimensions to maximize utilization with limited cross-die communications and calculate the memory ports needed. Finally, the scratchpad memory is the maximum memory size required over all stages. For applications with variable input size, we schedule based on the maximum possible size and employ batching [33] to prevent inefficiency of the design for small data size. Although the heuristics are not guaranteed optimal, we show that our schedule results improved performance consistently (see Sec- tions V, VI). We leave the development of an optimized DSE engine as future work. B. Designing Reconfigurable Modules: A Case Study in HLS In this work, we require the designer to manually write the reconfiguration designs based on the static schedule. Following the architecture template, there are three types of reconfigurations: data movement, compute, and control. All of them can be implemented in HLS efficiently with conditional dataflow, which utilizes if-else statements to declare the behavior changes and exploit HLS s resource binding pass to merge interconnects and logic as much as possible. Data Movement Reconfiguration: a majority of auto- reconfigurations in INTAR are powered by data movement control. In different stages, each CC may load data from various sources (e.g., other CCs, on-chip buffers, registers, or constants) and send it to multiple destinations. Figure 3 is an example of the data movement control reconfiguration in CC3: data flow from CC1 to buf 1 at stage 0 and flow from buf 2 to CC 2 at stage 1. The corresponding HLS code is a realization using the stage index as the condition to identify which flow the data movement control should adopt. Different stages can share data sources and destinations, which can potentially save interconnects. For instance, if the data flows from CC1 for each stage, the reader module can be reused. Fig. 3. Example of data movement reconfiguration. Left: In different stages, PE 3 either read from PE 1 and write to on-chip buffer buf 1, or read from buf 2 and write to PE 2. Right: the corresponding HLS code. Compute Reconfiguration: Based on the definition of the architecture template, the reconfigurable PEA can change the input operands and the precision. Figure 4 illustrates a PEA switching the data source between stages, where each source is interpreted as a different precision. To change the input operands, we adopt a similar method of data movement reconfiguration to extract inputs from two source buffers conditioned on the stage index. Inputs are packed for data parallelism. Then, when computations start, each PE will select part of the input operand and pad with zeros to make sure the bitwidths are uniform across stages. This guides the HLS to bind compute resources between stages. The two steps cannot be merged, otherwise, the HLS will instantiate two PEAs with different precision specifications. Fig. 4. Example of compute reconfiguration. Left: The reconfigurable PEA consumes different buffers at each stage and supports mixed precision of b-bit and c-bit calculations. Right: the corresponding HLS code. Control Reconfiguration: In INTAR, there are control re- configurations for loop bounds and data-dependent conditional dataflow. For loop-bound control, we can implement variable loop bounds for different stages of computations using the con- figuration instructions (left of Figure 5). For data-dependent conditional dataflow, when the behavior of executions depends on both stage index and other data sources, we can compose the conditions with logical operators (right of Figure 5). Fig. 5. Example of control reconfigurations in HLS. Left: loop-bound control. Right: data-dependent conditional dataflow. All accelerators in the INTAR paradigm are covered by the compositions of the three types of reconfigurations mentioned above, indicating complete support in HLS. C. Important Considerations in Placement and Routing In order to achieve rapid development, we need to make sure that the generated design is both valid and performant. Fol- lowing are several important considerations of implementing a placeable and routable INTAR design with high frequency. Distribute Independent Tasks Across Dies. For multi-die FPGAs [34], [35], cross-die wires are limited and often negatively affect the timing. Thus, when encountering a design with resource utilization higher than the single-die capacity, cross-die wiring latency can be the bottleneck of low clock frequency. A simple heuristic is to place independent tasks (e.g., multi-head attentions) into different dies instead of exe- cuting with all resources to avoid cross-die communications. Since there is no data movement between these tasks, we also do not add FIFOs between these tasks, which further reduces the cross-die wiring. Matching On-chip Memory Parallel Access Dimension. To serve multiple MAC units in each PEA, scratchpad memory is partitioned by access patterns. In INTAR, the PEA in the CCs may require varying patterns between tasks. For example, Figure 6 shows two dependent GEMMs: the first computes 2048 MACs simultaneously (4 8 64), and the second computes 1024 MACs (4 4 64). Since the PEA is reused across tasks, we aim to equalize the throughputs of the two matrix multiplies for efficiency, necessitating further partitioning of the second GEMM. This causes high-fanout nets at the write ports, increases cycle time, and reduces frequency. We address this by tiling operands so that output tiles have equal dimensions (e.g., 16 16), aligning parallel read write dimensions even after matrix transpose. Fig. 6. Example of a design with high-fanout nets on write ports of on-chip memory banks due to unaligned parallel memory access dimensions. To evaluate INTAR, we will show its broadness in DNN accelerations (Section V) and its advantages when deploying more complex DNNs on different FPGAs (Section VI). V. EVALUATION 1: MULTI-TASK KERNELS IN HDV DNNS A. Experiment Setup To demonstrate the broadness of INTAR in HDV DNN accelerations, we construct a testbench with five multi-task kernels that can serve as a standalone DNN application (Multi- layer CNN, Variational Autoencoder) or participate in part of the DNN applications (Self Attention, FFN Layer, Gating Network). Table III illustrates these kernels in the evaluation. Unlike existing benchmarks [41], [42] for FPGA-based ac- celerator evaluation, our testbench has three features crucial for HDV: ①it covers various DNN applications in computer vision, language modeling, graph processing, etc., displayed in the DNN Applications column; ②it adopts hyperparameters used in the real applications (e.g., Gating Network has the identical dimensionalities as Llama 2 [3]); and ③kernels are all HDV, and we impose a memory constraint between the minimum and maximum input output data sizes so that INTAR can exploit its ability to switch execution modes. Using our testbench, we compare INTAR with human- optimized dataflow and sequential accelerators, each designed in four weeks. All designs are evaluated under identical resource constraints, including on-chip memory capacity, the number of DSPs, and off-chip memory bandwidth utilization. All designs are implemented utilizing Xilinx Vitis HLS 2021.2 TABLE III DESCRIPTIONS OF MULTI-TASK KERNELS WITH THE CORRESPONDING PARAMETERS AND DATA SIZE CONSTRAINTS. THE ON-CHIP MEMORY CONSTRAINTS ARE ARTIFICIAL TO MAKE THE KERNEL HDV. Kernel Description DNN Applications Parameters Data Size Constraints Data Variation Pattern Self Attention (Attn) A set of dependent matrix multiplications that extract the contextual relationships between tokens in a sequence. Widely used in sequence modeling (Transformer [5], ViT [36], GAT [37]) 256 token sequences. The hidden dimension is 1024. Min data size: 0.5 MB, Max data size: 1.0 MB, On-chip memory constraint: 0.625 MB Increase (Q, K) De- crease (Q, K A) Increase (V ) Decrease (A, V V ) FFN Layer (FFN) Three layers of linear projections for feature extractions. Input is a single vector. Elementary block in many CV and LLM applications (ResNet [15], YoloV3 [38], Transformer) Input size final output size 256. First layer output size 1024, second layer output size 2048. Min data size: 0.5 KB, Max data size: 2 KB, On-chip mem- ory constraint: 1 KB Increase Increase Decrease Multi-layer CNN (M- CNN) Four layers of convolutions with upsampling and pooling layers Used in many CV applications (ResNet, VGG [39]) Input size 224, kernel size 3, 2 2 upsampling and pooling size. Min data size: 98 KB, Max data size: 1.57 MB, On-chip mem- ory constraint: 125 KB. Increase Increase Decrease Decrease Variational Autoencoder (VAE) Contains an encoder in convolutions for the latent space distribution and a decoder in transpose convolutions. Used for image compression and the generator model in GAN (VAE-GAN [40]) Input size 28, kernel sizes are 4 and 8, 2 channels and 2 filters Min data size: 1.3 KB, Max data size: 3.1 KB, On-chip memory constraint: 1.5 KB Decrease (encoder 1) Decrease (encoder 2) Increase (decoder 1) In- crease (decoder 2) Gating Network (GN) Two parallel linear projections with element-wise product and a sequential linear projection. Llama-family LLMs [3] Sequence length 32, input dimension 4096, hidden dimension 11008 Min data size: 0.25 MB, Max data size: 0.67 MB, On-chip memory constraint: 0.5 MB Increase (up projections) Decrease (element-wise product, down projection) with TAPA [43] and evaluated on Alveo U280 FPGA board, with a clock frequency of 300 MHz. We chose the human- optimized designs as the baseline since: ①There is a lack of existing designs of the kernels in our testbench with the same model and hardware configurations, and ②existing frame- works [6], [7], [44] for DNN application design generation on FPGAs do not handle off-chip memory communications or do not consider the extra resource constraints introduced to evaluate HDV DNNs. Fig. 7. Speedup and DSP efficiency of INTAR over dataflow and sequential execution for the five multi-task kernels in the testbench. Values are normal- ized from sequential accelerators. B. Analysis Figure 7 shows the speedup and DSP efficiency of InTAR over sequential and dataflow accelerators. Overall, InTAR achieves a speedup of 7.1 and 1.8 over sequential and dataflow accelerators, along with 7.5 and 1.9 higher DSP efficiency (GOP s DSP) in geometric mean. The performance boost of InTAR over sequential accelerators mainly comes from the reduction of off-chip memory access, where InTAR has 76 lower off-chip memory access volume than the sequential accelerators in geomean over the five kernels. For dataflow accelerators, InTAR is significantly faster in self- attention and multi-layer CNN than the other three kernels. There are two explanations. First, the two kernels mentioned have a higher data reuse between dependent tasks for matrix multiplications and convolutions, which requires data buffering and preprocessing before continuing the computations (e.g., the pooling layer of the multi-layer CNN will wait until producing two rows of outputs from the previous layer to downsample the data). Since the selected kernels have a long chain of dependent tasks, pipeline stall latency may dominate the end-to-end latency. Second, InTAR can better reuse off- chip memory ports for these two kernels. For example, after loading inputs and finishing the first convolution layer in the multi-layer CNN, InTAR can utilize the memory ports allocated to the inputs to write outputs. Thus, InTAR has a higher effective off-chip read write throughput than dataflow accelerators, which only use the dedicated memory port for input output. For Gating Networks, InTAR cannot further reduce latency compared with dataflow accelerators since only input and output data sizes are within the memory constraint, and our schedule for InTAR is identical to pipelining all tasks. VI. EVALUATION 2: GPT-2 INPUT PREFILLING A. Experiment Setup We choose the GPT-2 medium input prefilling stage as a complex scenario to further evaluate INTAR, which is a frequently used benchmark by prior SoTA FPGA-based LLM accelerators [6], [23]. We selected two platforms (Versal VPK180 [45] and Alveo U280 [34]), which result in different schedules and architecture template parameters based on their resource constraints. For instance, the design for VPK180 has four rows of CCs spreading out to four SLRs of the FPGA, while the design on U280 has 3 rows of CCs to fit its 3-SLR layout. Each SLR includes two CCs and an SFU for both platforms. Thanks to the grid structure of INTAR s architecture template, we can adapt the design conveniently to other FPGA boards with only slight modifications. To generate the circuit design, we employ Xilinx HLS with the TAPA framework [43] and prototype using the Vitis 2021.2 that performs well on older platforms such as U280, and Vitis 2024.1 optimized for VPK180. The floorplanning of the CCs and SFUs is predetermined based on the architectural template, while the remaining modules (e.g., auxiliary buffers) are managed by AutoBridge [46]. The designs are both placed and routed for U280 and VPK180. For comparison, we select Allo [6] and DFX [23] as the SoTA accelerators, both of which utilize U280 as the compute platform; accordingly, we include InTAR implemented on U280. Fig. 8. Left: Latency and DSP efficiency of INTAR (U280, VPK180), Allo, and DFX. Both designs of INTAR are significantly more DSP efficient. Right: Latency and power efficiency of INTAR (U280, VPK180), Allo [6], and GPU solutions for GPT-2 medium model. TABLE IV RESOURCE UTILIZATION AND FREQUENCY OF FPGA-BASED SOLUTIONS (ALLO, DFX, AND INTAR) FOR GPT-2 MEDIUM INPUT PREFILLING TASK. SPEEDUP IS NORMALIZED BASED ON DFX. Allo [6] DFX [23] INTAR INTAR Device U280 U280 U280 VPK180 Frequency 247 MHz 200 MHz 224 MHz 300 MHz BRAM 389 (19 ) 1192 (59 ) 535 (27 ) 700 (14 ) DSP 1780 (20 ) 3533 (39.2 ) 6727 (75 ) 6726 (47 ) LUT 569K (44 ) 520K (40 ) 485K (37 ) 1074K (32 ) FF 653K (25 ) 959K (43 ) 627K (25 ) 1072K (16 ) URAM 111 (12 ) 104 (11 ) 336 (35 ) 412 (16 ) Norm. Speedup 1.83 1.0 14.64 39.14 Table IV lists the resource utilization and frequency. IN- TAR on VPK180 has lower DSP utilization than U280 since VPK180 has fewer cross-SLR wires than U280 and PEA sizes are highly correlated with cross-SLR communications. DFX is executed in FP16, and all other designs employ the W4A8 format. We scale the DSP efficiency for DFX to align the data type. For performance metrics, we use OpenCL profiling functions to get the latency and xbutil to measure the power within 100 runs on U280. Due to a lack of access to a physical device, we employ QEMU and the Xilinx Power Design Manager to calculate the latency and estimate the power consumption of VPK180. DFX is not included in comparing power efficiency due to a lack of data in the original work. We also compare InTAR with various GPUs shown in Table V. PyTorch profiler measures the latency and memory transactions. To measure the power consumption, we employ the NVIDIA management library to probe the power every 10 ms and calculate the average power when it is stable. All GPUs are inference in BFloat16 format, which is a commonly supported and optimized data format for GPUs. TABLE V HARDWARE CONFIGURATIONS OF THE FPGAS AND GPUS U280 VPK180 T4 A100 MI210 Frequency 224 MHz 240 MHz 585 MHz 765 MHz 1.7 GHz Bandwidth 460 GB s 52.1 GB s 320 GB s 1.56 TB s 1.64 TB s Peak Power 75 W 180 W 70 W 250 W 300 W Peak Perf. 8.09 TOP s 20.7 TOP s 65.13 TOP s 311.84 TOP s 181 TOP s Process Node TSMC 16nm TSMC 7nm TSMC 12nm TSMC 7nm TSMC 6nm B. Analysis Comparison with FPGA-based Accelerators. Figure 8 presents the latency, DSP efficiency, and power efficiency comparison between FPGA and GPU accelerators. Compared to Allo and DFX, INTAR on U280 is 7.99 and 14.64 faster, and 2.19 and 3.98 more DSP efficient, respectively. INTAR on VPK180 is 21.39 and 39.14 faster, and 5.66 and 10.44 more DSP efficient than Allo and DFX. INTAR on VPK180 is more DSP efficient than U280 since DSP58 has a higher throughput than DSP48. Additionally, implementing DSPs in dot-product modules on VPK180 trims the logic and further reduces cycles, leading to more than 2 boost in DSP efficiency. Furthermore, similar to Evaluation 1 in Section V, we compared InTAR with our manually implemented dataflow and sequential accelerators, achieving 1.8 and 8.3 speedup and 1.7 and 8.2 DSP efficiency boost. Comparison with GPUs. As shown in Figure 8, INTAR on U280 achieves a 1.07 speedup in geometric mean over T4 and 2.41 speedup on VPK180. Especially for short sequences (L 256), InTAR s performance surpasses both T4 and A100 GPUs. Moreover, with the same process node, INTAR on FPGAs attains higher power efficiency than GPUs. INTAR on U280 is 2.37 more efficient than NVIDIA T4. INTAR on VPK180 is 1.66 7.17 more efficient than T4, A100, MI210. One source of latency and power overhead for GPUs is the off-chip memory access. Since GPUs execute tasks sequentially and GPT-2 medium tends to generate large intermediate data, off-chip memory access is more intense on GPUs than INTAR-optimized FPGA designs. As a result, INTAR has a 20 67 lower off-chip memory access compared to T4, A100, and MI210. VII. CONCLUSION AND FUTURE WORK In this work, we propose InTAR, a novel accelerator design paradigm for DNNs with high data volume variation. InTAR enhances resource efficiency by switching execution patterns and performing model-specific optimizations. It surpasses SoTA FPGA accelerators in speed, DSP, and power efficiency, and, compared with GPUS, it reduces off-chip memory access. Although focused on DNNs, InTAR may also enhance non- DNN HDV applications, which we plan to explore. Future work includes developing an optimized DSE engine to replace heuristic scheduling and automate InTAR design compilation. ACKNOWLEDGEMENT This work was supported in part by PRISM, one of the seven centers in the JUMP 2.0 program sponsored by SRC and DARPA. It is also supported by CDSC industrial partners and the AMD 2 HACC Program. 2J. Cong has a financial interest in AMD REFERENCES [1] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon, Unified language model pre-training for natural language understanding and generation, Advances in neural information processing systems, vol. 32, 2019. [2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [3] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [4] M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib, M. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, A review on large language models: Architectures, applications, taxonomies, open issues and challenges, IEEE Access, 2024. [5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [6] H. Chen, N. Zhang, S. Xiang, Z. Zeng, M. Dai, and Z. Zhang, Allo: A programming model for composable accelerator design, Proceedings of the ACM on Programming Languages, vol. 8, no. PLDI, pp. 593 620, 2024. [7] S. Basalama, A. Sohrabizadeh, J. Wang, L. Guo, and J. Cong, Flexcnn: An end-to-end framework for composing cnn accelerators on fpga, ACM Transactions on Reconfigurable Technology and Systems, vol. 16, no. 2, pp. 1 32, 2023. [8] N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai, N. Patil, S. Subramanian, A. Swing, B. Towles et al., Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware sup- port for embeddings, in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1 14. [9] Y. Bai, H. Zhou, K. Zhao, H. Wang, J. Chen, J. Yu, and K. Wang, Fet-opu: A flexible and efficient fpga-based overlay processor for transformer networks, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 9. [10] Z. Liu, G. Li, and J. Cheng, Hardware acceleration of fully quantized bert for efficient natural language processing, in 2021 Design, Automa- tion Test in Europe Conference Exhibition (DATE). IEEE, 2021, pp. 513 516. [11] X. Zhang, H. Ye, J. Wang, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen, Dnnexplorer: a framework for modeling and exploring a novel paradigm of fpga-based dnn accelerator, in Proceedings of the 39th International Conference on Computer-Aided Design, 2020, pp. 1 9. [12] H. Chen, J. Zhang, Y. Du, S. Xiang, Z. Yue, N. Zhang, Y. Cai, and Z. Zhang, Understanding the potential of fpga-based spatial accel- eration for large language model inference, ACM Transactions on Reconfigurable Technology and Systems, 2024. [13] J. Zhuang, Z. Yang, S. Ji, H. Huang, A. K. Jones, J. Hu, Y. Shi, and P. Zhou, Ssr: Spatial sequential hybrid architecture for latency throughput tradeoff in transformer acceleration, in Proceedings of the 2024 ACM SIGDA International Symposium on Field Programmable Gate Arrays, 2024, pp. 55 66. [14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [15] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770 778. [16] S. Xie, R. Girshick, P. Doll ar, Z. Tu, and K. He, Aggregated residual transformations for deep neural networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1492 1500. [17] J. Cai, Y. Wei, Z. Wu, S. Peng, and K. Ma, Inter-layer scheduling space definition and exploration for tiled accelerators, in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1 17. [18] J. Cong, H. Huang, C. Ma, B. Xiao, and P. Zhou, A fully pipelined and dynamically composable architecture of cgra, in 2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Com- puting Machines. IEEE, 2014, pp. 9 16. [19] E. Baek, D. Kwon, and J. Kim, A multi-neural network acceleration architecture, in 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2020, pp. 940 953. [20] S. Liu, J. Weng, D. Kupsh, A. Sohrabizadeh, Z. Wang, L. Guo, J. Liu, M. Zhulin, R. Mani, L. Zhang et al., Overgen: Improving fpga usability through domain-specific overlay generation, in 2022 55th IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 2022, pp. 35 56. [21] T. Kong, K. Koul, P. Raina, M. Horowitz, and C. Torng, Hardware ab- stractions and hardware mechanisms to support multi-task execution on coarse-grained reconfigurable arrays, arXiv preprint arXiv:2301.00861, 2023. [22] J. Cong, J. Lau, G. Liu, S. Neuendorffer, P. Pan, K. Vissers, and Z. Zhang, Fpga hls today: successes, challenges, and opportunities, ACM Transactions on Reconfigurable Technology and Systems (TRETS), vol. 15, no. 4, pp. 1 42, 2022. [23] S. Hong, S. Moon, J. Kim, S. Lee, M. Kim, D. Lee, and J.-Y. Kim, Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation, in 2022 55th IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 2022, pp. 616 630. [24] S. Abi-Karam, Y. He, R. Sarkar, L. Sathidevi, Z. Qiao, and C. Hao, Gengnn: A generic fpga framework for graph neural network acceler- ation, arXiv preprint arXiv:2201.08475, 2022. [25] D. Gschwend, Zynqnet: An fpga-accelerated embedded convolutional neural network, arXiv preprint arXiv:2005.06892, 2020. [26] S. Zeng, J. Liu, G. Dai, X. Yang, T. Fu, H. Wang, W. Ma, H. Sun, S. Li, Z. Huang et al., Flightllm: Efficient large language model inference with a complete mapping flow on fpgas, in Proceedings of the 2024 ACM SIGDA International Symposium on Field Programmable Gate Arrays, 2024, pp. 223 234. [27] M. Tanomoto, S. Takamaeda-Yamazaki, J. Yao, and Y. Nakashima, A cgra-based approach for accelerating convolutional neural networks, in 2015 IEEE 9th International Symposium on Embedded Multicore Many- core Systems-on-Chip. IEEE, 2015, pp. 73 80. [28] I. Taras and J. H. Anderson, Impact of fpga architecture on area and performance of cgra overlays, in 2019 IEEE 27th Annual Interna- tional Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEE, 2019, pp. 87 95. [29] K. Vipin and S. A. Fahmy, Fpga dynamic and partial reconfiguration: A survey of architectures, methods, and applications, ACM Computing Surveys (CSUR), vol. 51, no. 4, pp. 1 39, 2018. [30] J. Tong, A. Itagi, P. Chatarasi, and T. Krishna, Feather: A reconfigurable accelerator with data reordering support for low-cost on-chip dataflow switching, in Proceedings of the 51th Annual International Symposium on Computer Architecture, ser. ISCA 24. Argentina: Association for Computing Machinery, 2024. [31] Y. Yu, C. Wu, T. Zhao, K. Wang, and L. He, Opu: An fpga-based overlay processor for convolutional neural networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 1, pp. 35 47, 2019. [32] Y. Yu, T. Zhao, K. Wang, and L. He, Light-opu: An fpga-based overlay processor for lightweight convolutional neural networks, in Proceedings of the 2020 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, 2020, pp. 122 132. [33] M. Looks, M. Herreshoff, D. Hutchins, and P. Norvig, Deep learning with dynamic computation graphs, arXiv preprint arXiv:1702.02181, 2017. [34] Alveo u280 data center accelerator card data sheet. [Online]. Available: [35] Amd versal hbm series product selection guide. [Online]. Available: [36] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu et al., A survey on vision transformer, IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 1, pp. 87 110, 2022. [37] P. Veliˇckovi c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Ben- gio, Graph attention networks, arXiv preprint arXiv:1710.10903, 2017. [38] A. Farhadi and J. Redmon, Yolov3: An incremental improvement, in Computer vision and pattern recognition, vol. 1804. Springer Berlin Heidelberg, Germany, 2018, pp. 1 6. [39] A. Vedaldi and A. Zisserman, Vgg convolutional neural networks practical, Department of Engineering Science, University of Oxford, vol. 66, 2016. [40] R. Gao, X. Hou, J. Qin, J. Chen, L. Liu, F. Zhu, Z. Zhang, and L. Shao, Zero-vae-gan: Generating unseen features for generalized and trans- ductive zero-shot learning, IEEE Transactions on Image Processing, vol. 29, pp. 3665 3680, 2020. [41] Y. Zhou, U. Gupta, S. Dai, R. Zhao, N. Srivastava, H. Jin, J. Featherston, Y.-H. Lai, G. Liu, G. A. Velasquez, W. Wang, and Z. Zhang, Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software- Programmable FPGAs, Int l Symp. on Field-Programmable Gate Ar- rays (FPGA), Feb 2018. [42] L.-N. Pouchet et al., Polybench: The polyhedral benchmark suite, URL: cs. ucla. edu pouchet software polybench, vol. 437, pp. 1 1, 2012. [43] L. Guo, Y. Chi, J. Lau, L. Song, X. Tian, M. Khatti, W. Qiao, J. Wang, E. Ustun, Z. Fang et al., Tapa: a scalable task-parallel dataflow programming framework for modern fpgas with co-optimization of hls and physical design, ACM Transactions on Reconfigurable Technology and Systems, vol. 16, no. 4, pp. 1 31, 2023. [44] J. Duarte et al., Fast inference of deep neural networks in FPGAs for particle physics, JINST, vol. 13, no. 07, p. P07027, 2018. [45] Amd versal premium series product selection guide. [Online]. Available: [46] L. Guo, Y. Chi, J. Wang, J. Lau, W. Qiao, E. Ustun, Z. Zhang, and J. Cong, Autobridge: Coupling coarse-grained floorplanning and pipelining for high-frequency hls design on multi-die fpgas, in The 2021 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, 2021, pp. 81 92.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nInTAR: Inter-Task Auto-Reconfigurable Accelerator Design for High Data Volume Variation in DNNs Zifan He , Anderson Truong , Yingqi Cao and Jason Cong University of California, Los Angeles University of California, San Diego Email: Abstract The rise of deep neural networks (DNNs) has driven an increased demand for computing power and memory. Modern DNNs exhibit high data volume variation (HDV) across tasks, which poses challenges for FPGA acceleration: conventional accelerators rely on fixed execution patterns (dataflow or sequen- tial) that can lead to pipeline stalls or necessitate frequent off-chip memory accesses. To address these challenges, we introduce the Inter-Task Auto-Reconfigurable Accelerator (InTAR) 1, a novel accelerator design methodology for HDV applications on FPGAs. InTAR combines the high computational efficiency of sequential execution with the reduced off-chip memory overhead of dataflow execution. It switches execution patterns automatically with a static schedule determined before circuit design based on resource constraints and problem sizes. Unlike previous reconfigurable accelerators, InTAR encodes reconfiguration schedules during circuit design, allowing model-specific optimizations that allocate only the necessary logic and interconnects. Thus, InTAR achieves a high clock frequency with fewer resources and low reconfigura- tion time. Furthermore, InTAR supports high-level tools such as HLS for fast design generation. We implement a set of multi-task HDV DNN kernels using InTAR. Compared with dataflow and sequential accelerators, InTAR exhibits 1.8 and 7.1 speedups correspondingly. Moreover, we extend InTAR to GPT-2 medium as a more complex example, which is 3.65 39.14 faster and a 1.72 10.44 more DSP efficient than SoTA accelerators (Allo and DFX) on FPGAs. Additionally, this design demonstrates 1.66 7.17 better power efficiency than GPUs. I. INTRODUCTION The development of deep neural networks (DNNs) has driven numerous breakthroughs in AI-assisted applications [1], [2] while intensifying demands for compute efficiency due to increasing network complexity and memory requirements. As DNN applications grow in size [3], [4], larger data volumes generated between operations strain hardware resources dur- ing deployment.\n\n--- Segment 2 ---\nINTRODUCTION The development of deep neural networks (DNNs) has driven numerous breakthroughs in AI-assisted applications [1], [2] while intensifying demands for compute efficiency due to increasing network complexity and memory requirements. As DNN applications grow in size [3], [4], larger data volumes generated between operations strain hardware resources dur- ing deployment. Consequently, a key challenge in designing DNN accelerators is: Given resource constraints imposed by physical hardware limitations or power considerations, how can execution scheduling be improved to minimize end-to-end latency while accounting for hardware overhead? A common strategy of DNN accelerator designs is grouping computations belonging to the same matrix vector operation into a task and scheduling computations at the task level. For example, self-attention [5] of transformer models involves two tasks: QKT and softmax operation. Previous acceler- ators focus on two types of fixed task execution patterns: dataflow (also called spatial) [6], [7] or sequential (also called 1 temporal) executions [8] [11]. For dataflow executions, each processing element (PE) specializes in a single task. Data is streamed through FIFOs connecting PEs for dependent compu- tations with minimal on-chip buffer for data reuse. Depending on the dataflow graph, it includes both task-pipeline and task-parallel executions. Task-pipeline streams data between dependent tasks, and task-parallel executes independent tasks in parallel. For sequential executions, a versatile PE processes tasks serially and parallelizes the computations inside each task. Resources are reused across operators. Although both execution patterns exhibit strong performance, each of them faces a major challenge for large DNN applications [12], [13]: Dataflow: reduced computation resource efficiency. Due to the coarse and fine-grained pipeline stalls from the data dependencies, some PEs in dataflow execution may remain idle, leading to low computation resource efficiency and high end-to-end latency. Sequential: high off-chip memory access overhead. Out- put data from previous tasks will be stored and reused by the next tasks in sequential execution. Moreover, data may be cached and remain unused until the consumer tasks start executions. Due to the limited on-chip resources, it is often difficult to store all output data in SRAM.\n\n--- Segment 3 ---\nMoreover, data may be cached and remain unused until the consumer tasks start executions. Due to the limited on-chip resources, it is often difficult to store all output data in SRAM. If storing data in the off-chip memory, the accelerator will suffer from high latency and energy consumption of off-chip memory access. TABLE I MINIMUM AND MAXIMUM INTERMEDIATE DATA SIZE OF SAMPLE DNNS Model Type Min Data Size Max Data Size GPT 2 [14] Transformer 64L max(4096L, L2) Llama 2 7B [3] Transformer 128L max(22016L, L2) ResNet-50 [15] CNN 151K 802K ResNet-152 CNN 151K 802K ResNext-101 [16] CNN 151K 1.04M To address both challenges simultaneously, we observe another commonly neglected feature of DNNs to exploit: high data-volume variation (HDV). Table I lists a set of DNN applications and the minimum maximum input output data size between operations. L is the sequence length for transformer models. These models offer both a large maximum data size (up to 88 MB with L 2048) and a significant data size variation (5.3 172 from minimum to maximum data size). A potential solution to improve computation resource efficiency while circumventing off-chip memory access for intermediate data is to execute sequentially for tasks that arXiv:2502.08807v2 [cs.AR] 4 Apr 2025 produce small data within on-chip memory capacity, and stream data for tasks that produce large data. Therefore, we propose a novel accelerator design paradigm on FPGAs: inter-task auto-reconfigurable accelerator (INTAR). INTAR can switch execution patterns automatically based on on-chip memory and computation resources. When a task produces large intermediate data, INTAR pipelines multiple tasks to avoid accessing off-chip memory for this data. Otherwise, INTAR will process tasks sequentially to maximize computational efficiency by eliminating pipeline stalls. Compared with other reconfigurable accelerators [17] [21], INTAR allows model-specific circuit optimization that keeps only necessary control logics and interconnects for reconfiguration. Hence, InTAR requires fewer reconfiguration resources, achieves a high clock frequency, and has a low reconfiguration overhead (10 to 20 ns) (Section III).\n\n--- Segment 4 ---\nCompared with other reconfigurable accelerators [17] [21], INTAR allows model-specific circuit optimization that keeps only necessary control logics and interconnects for reconfiguration. Hence, InTAR requires fewer reconfiguration resources, achieves a high clock frequency, and has a low reconfiguration overhead (10 to 20 ns) (Section III). Moreover, since computations are reconfigured at the task level, INTAR is one of the first works regarding FPGA-based reconfigurable accelerators that support high-level hardware generation tools such as High-Level Synthesis (HLS) [22] for fast accelerator design. Specifically, our contributions include: We present INTAR, a novel accelerator design paradigm on FPGAs that balances the memory access and computational efficiency tradeoff for HDV applications (Section III). We illustrate the techniques to design INTAR with HLS and important considerations in placement and routing for hardware implementation of INTAR on FPGAs (Section IV). Evaluations. We implement INTAR for five multi-task ker- nels that broadly exist in DNN applications (Self-Attention, Multi-layer CNN, FFN layer, VAE, and Gating Network) to show its advantages. InTAR exhibits 1.8 and 7.1 speedup compared with the corresponding dataflow and se- quential accelerator. We further present InTAR on the GPT-2 medium model for a complete DNN example, which achieves a speedup of 3.65 39.14 and a 1.72 10.44 improve- ment in DSP efficiency compared to the SoTA accelerators (Allo [6] and DFX [23]). Moreover, INTAR demonstrated 1.66 7.17 better power efficiency compared to GPUs. II. BACKGROUND AND MOTIVATIONS A. Dataflow and Sequential DNN Accelerators Dataflow accelerators allocate resources per DNN task and pipeline computations to conserve memory while preserving performance. For example, FlexCNN [7] composes CNN- based accelerators in HLS, GenGNN [24] accelerates GNNs, and Allo [6], [12] designs FPGA accelerators by parsing Python scripts into HLS code for each operator, automatically combining layer optimizations with customization primitives. On the other hand, sequential accelerators process layers in DNN sequentially and attempt to utilize all available resources.\n\n--- Segment 5 ---\nFor example, FlexCNN [7] composes CNN- based accelerators in HLS, GenGNN [24] accelerates GNNs, and Allo [6], [12] designs FPGA accelerators by parsing Python scripts into HLS code for each operator, automatically combining layer optimizations with customization primitives. On the other hand, sequential accelerators process layers in DNN sequentially and attempt to utilize all available resources. GPUs and TPUs [8] are considered instruction-based sequen- tial accelerators that support various DNNs. ZyncNet [25] is a CNN accelerator deployed on Zync SoC with a specialized topology. FlightLLM [26] is a vector processor implemented on Alveo U280 FPGA for Llama model inference. B. Hybrid Accelerators Some previous works attempt to mitigate the drawbacks of dataflow and sequential accelerators by having a hybrid execution pattern. A naive approach is allocating part of the resource for dataflow execution and the rest for sequential execution [11]. A more recent method is to allocate several PEs and schedule multiple tasks to the same PE. For instance, SSR [13] is a hybrid accelerator implemented on AMD Xilinx Ver- sal ACAP devices for vision transformers. By reserving two groups of AI engines for two types of matrix multiplies, SSR allows sequential executions of matrix multiplies (GEMM) to minimize latency and enables data forwarding between dependent GEMMs to reduce the memory cost. However, PE specializations of hybrid accelerators impede the flexibility of execution pattern switching, which may lead to stalls. Section II-E will further discuss the inefficiency of hybrid accelerators with an example. C. Reconfigurable Accelerators A reconfigurable accelerator can change its microarchitec- ture on the fly. A common implementation of such accelerators is the coarse-grain reconfigurable array (CGRA) [18], [20], [27], which compiles a dataflow graph into a configuration and modifies switches connecting PEs accordingly. However, CGRA has complex reconfiguration logic to ensure general- izability. This introduces substantial area overheads [28] and additional wiring that negatively affects the timing [20].\n\n--- Segment 6 ---\nHowever, CGRA has complex reconfiguration logic to ensure general- izability. This introduces substantial area overheads [28] and additional wiring that negatively affects the timing [20]. An alternative method to reconfigure resources is dynamic partial reconfiguration (DPR) [29], which reserves regions on FPGAs to load partial bitstreams for runtime reconfiguration. While having a low resource overhead for the configuration controller, it has a higher reconfiguration overhead (10 to 100 ms) than FPGA-based CGRA (around 10 ns). Other reconfigurable accelerators focus on optimization under only specific computation scenarios [17], [19], [30]. For example, SET [17] performs inter-layer scheduling of convo- lution networks onto the tiled accelerators utilizing a time- space resource-allocation tree. FEATHER [30] mainly resolves reduction efficiency in each CNN layer by reconfiguring the reduction dataflow. Considering these limitations, existing reconfigurable accel- erators can hardly apply to a broad range of modern DNNs, which necessitates the development of a new design paradigm. D. Applications with High Data Volume Variation A multi-task application is defined as high data-volume variation (HDV) if the input output data sizes between tasks are highly varied. Many of the DNNs are HDV for two reasons: Modern DNNs have complex dependencies. Data is pro- duced several layers in advance and can only be discarded after they are reused. Hence, the data production and elimination rates vary widely. The objectives of many DNN workloads involve feature extraction (e.g., classification) that reduces the data size, or data construction (e.g., image text generation), which increases Fig. 1. Example of mapping computations of attention and linear projection of value matrix to the dataflow, sequential, hybrid accelerators, and INTAR. The sequence length is 256, the hidden dimension is 1024, and the weights are 1024 1024 matrices. Depending on the scheduling of the rest of the computations in the entire model, INTAR can choose between task-parallel (left) and sequential modes (right) for better locality. Both will have the same latency. the data size. By the nature of these workloads, the data size will change during execution.\n\n--- Segment 7 ---\nthe data size. By the nature of these workloads, the data size will change during execution. E. A Motivating Case Study: Attention Linear Projection We provide a case study to illustrate the advantages of inter- task reconfiguration for HDV DNNs: calculating the attention score and a linear projection of the value matrix. Figure 1 shows the dataflow graph of the attention layer and the timeline for each execution pattern. The direction of the arrows indicates the output production order, and the lengths represent the production rate. Here we focus on calculating Q, K, V , and A in the dashed square. In this example, we assume that: Matrix multiplies are weight-stationary as weights are al- ready stored in on-chip memory. Outputs (A and V ) will be written into off-chip memory. The rest of the on-chip memory size will only store either Q or K, but not both. Input sequence is short, which means A can be stored in on-chip buffer with K or Q. The pattern of data size variation is first increase then decrease. Dataflow execution instantiates PEs for each task. There are three ways to stream and cache data across tasks, illustrated in Figure 1: outer product with aligned and un- aligned production rates of Q and K (Figure 1(a) and (b)), and inner product with unaligned production rates (Figure 1(c)). Inner products with aligned production rates require caching of both Q and K, which is infeasible. Each case incurs compute resource idling due to either varied workload size of A, back- pressure from streaming, or early completion of tasks. Sequential execution instantiates a single PE to execute each task serially, as shown in Figure 1(d). Due to insufficient on- chip memory capacity, it needs to read from and write to the off-chip memory, increasing the overall latency. Some hybrid accelerators [13] can mitigate the issues by executing Q and V sequentially and pipeline K and A. However, since PEs are specialized for either streaming data or cached buffer [13], computation of V cannot be assigned to PE 2 and PE 3, and these two PEs are idle at the beginning, waiting for the arrival of data for Q. For INTAR (Figure 1(f)), PEs can be reconfigured to either sequential or dataflow execution at different times.\n\n--- Segment 8 ---\nHowever, since PEs are specialized for either streaming data or cached buffer [13], computation of V cannot be assigned to PE 2 and PE 3, and these two PEs are idle at the beginning, waiting for the arrival of data for Q. For INTAR (Figure 1(f)), PEs can be reconfigured to either sequential or dataflow execution at different times. Therefore, PEs can be efficiently allocated while off-chip memory access for intermediate data is avoided. Section III discusses how INTAR manages this in detail. III. INTER-TASK AUTO-RECONFIGURABLE ACCELERATOR To improve resource efficiency, INTAR minimizes off-chip memory access by employing dataflow execution when nec- essary while keeping the rest of the execution as sequential as possible. The finest granularity of execution pattern switching is the task, which represents a group of computations belong- ing to the same matrix or vector operation. Execution pattern switching is achieved through automatic reconfiguration based on instructions stored in the static configuration buffer (Section III-B), which is hardened into the design. These instructions trigger computation behavior and data movement changes. Unlike other reconfigurable [17], [18], [20], [21], [30] and overlay accelerators [31], [32], which prioritize abstraction and fast compilation, InTAR focuses on reconfiguration for optimization. The reconfiguration schedule in InTAR is deter- mined at circuit design time rather than relying on post-design external commands. For every application, a static schedule is created based on the specific application and resource constraints, and the circuit is then tailored accordingly. This approach reduces reliance on general-purpose control units and cross-bars, retaining only essential reconfiguration logic and interconnects. The result is low resource utilization for reconfiguration controls (49 LUT reduction for a design with 3 throughput compared to [20] for multiple GEMMs), high clock frequency (2.41 that of [20]), and low reconfiguration latency (106 faster than DPR [21], and comparable to [20]) compared with FPGA-based CGRA and DPR approaches. Additionally, InTAR handles reconfiguration at the task level, enabling rapid and convenient development with high-level tools like HLS. This makes InTAR well-suited for FPGAs to adapt quickly to new DNN applications.\n\n--- Segment 9 ---\nAdditionally, InTAR handles reconfiguration at the task level, enabling rapid and convenient development with high-level tools like HLS. This makes InTAR well-suited for FPGAs to adapt quickly to new DNN applications. Lastly, InTAR s reconfiguration scope is not limited to specific computations. Table II compares InTAR with prior DNN accelerator works. Fig. 2. Left: Architecture template of INTAR. Compute cores (CC) compute linear operations (e.g., GEMM, ConvNet), and SFUs compute non-linear operations (e.g., softmax, GeLU). Each CC contains a scratchpad memory, a reconfigurable MAC unit array, a reduction unit, and a data movement control unit. Dashed lines indicate the candidates for connection between CCs and SFUs. Right: example architectures for each execution mode within the template, with n, m 2. For a schedule, we will compose the architectures of the required modes to keep only the necessary interconnects and logic. TABLE II COMPARISON BETWEEN INTAR AND OTHER DNN ACCELERATION WORKS. THE MAJOR REASON FOR HAVING IDLE COMPUTE RESOURCES IS DESCRIBED IN THE PARENTHESES FOR EACH WORK. Prior works Platform Contribution Type Accelerator Inter-task Idle Compute Intermediate When Reconfig Reconfiguration Model-spec. Category Schedule Resource Exist? Data Movement Determined Design Scope Design Opt. Allo [6] FPGA Domain-Spec. Lang. Dataflow Yes (dataflow execution) On-chip only - - FQ-BERT [10], DFX [23] FPGA Accelerator Sequential No On- Off-chip - - SSR [13] FPGA Accelerator Hybrid Yes (PE specialization) On-chip only - - FPCA [18], OverGen [20] FPGA Architecture Reconfigurable Yes (single DNN) On- Off-chip After circuit generation General FEATHER [30] FPGA Accelerator Reconfigurable No On- Off-chip After circuit generation Reduction Network SET [17] ASIC Scheduler Reconfigurable Yes (sub-opt. sched.) On- Off-chip N A Tiled Accelerator INTAR FPGA Design Paradigm Reconfigurable No On-chip only Circuit design time General A.\n\n--- Segment 10 ---\nsched.) On- Off-chip N A Tiled Accelerator INTAR FPGA Design Paradigm Reconfigurable No On-chip only Circuit design time General A. Execution Modes of INTAR Depending on the available resources on FPGAs and the properties of tasks, INTAR configures the resources into sequential, task-pipeline, or task-parallel mode, corresponding to the dataflow and sequential execution patterns mentioned in Section I. Sequential: All resources perform a single task in parallel. Task-pipeline: Same as dataflow execution with pipelined dependent tasks. Computational resources are partitioned pro- portionally to the workloads. Data are streamed from one task to another using FIFOs. Task-parallel: Same as dataflow execution with independent tasks running in parallel. The resource efficiency is the same as the sequential mode, but it may have a better locality for subsequent tasks (e.g., for the example in upper left of Figure 1, if calculating V and O are pipelined, then running Q and V in parallel and assigning V to the PE that will compute V avoids data aggregation overhead from other PEs). Figure 1(f) illustrates the task assignment and timeline of INTAR for the case study in Section II-E. PEs 1 and 2 handle both attention computation and linear projections. Depending on the subsequent operations, INTAR can choose either task- parallel or sequential mode for Q and V . If the scheduler decides to serialize computing V and O, then the sequential mode is more suitable to allow PEs 1 and 2 to execute cooperatively. If the scheduler pipelines computations of V and O, then task-parallel mode is preferred to improve locality. Both choices will not affect the latency but will determine the data movement overhead of the following tasks. After calculating Q and V , INTAR switches to task-pipeline mode to compute K and A, since storing Q consumes all scratchpad memory. B. Architecture Template Figure 2 is the architecture template of INTAR for HDV DNN applications, which consists of a grid of compute cores (CC) computing linear operations and special function units (SFU) inserted in between. Each CC contains: Reconfigurable PE Array (PEA): a multi-dimensional array of PEs that allows changing inputs and precisions.\n\n--- Segment 11 ---\nArchitecture Template Figure 2 is the architecture template of INTAR for HDV DNN applications, which consists of a grid of compute cores (CC) computing linear operations and special function units (SFU) inserted in between. Each CC contains: Reconfigurable PE Array (PEA): a multi-dimensional array of PEs that allows changing inputs and precisions. Data communication between PEs is either in a systolic array style or by broadcasting the data to all PEs. Each PE produces the complete or partial result of the linear operation. Scratchpad Memory: store intermediate and weight data. Reduction Unit: read the partial outputs from the PEA if needed and perform accumulations. Data Movement Control Unit: determine the flow of data to the scratchpad memory and other CCs SFUs. When switching execution modes, a global instruction reader will read both the configuration instructions from a static buffer and the input size passed as the kernel argument. It will then modify part of each instruction related to the input size (e.g., the loop bounds) and send it to the CCs. The reconfigurable PEA and data movement control will read the instructions and propagate them among the CCs. The instruc- tion sequence is determined by the static schedule of mode switching at the circuit design time. Each instruction contains a stage index and several loop bounds (e.g., reconfiguring the three loop bounds for GEMMs). A stage refers to the execution of a single or a group of tasks that follows one of the execution modes, seen in Figure 1(f), e.g., computing Q and V in stage 1 and K and A in stage 2. Each CC has a specified behavior for each stage, orchestrated by multiplexers with stage indices as the select signals. Notice that the stage-specific behaviors vary from one application to another. Thus, INTAR only needs to handle the reconfigurations defined in that application-specific schedule. The reconfiguration overhead consists of four parts: reading, modifying, sending, and decoding instructions to generate signals for multiplexers in the CCs. Each of these is done in a single cycle. Thus, reconfigurations have a 4- cycle overhead (10 to 20 ns, depending on frequency), which is negligible compared to modern DNN inference latency. The template has several design parameters for users to derive a specific architecture, including: Number of rows n and columns m of the CCs.\n\n--- Segment 12 ---\nThus, reconfigurations have a 4- cycle overhead (10 to 20 ns, depending on frequency), which is negligible compared to modern DNN inference latency. The template has several design parameters for users to derive a specific architecture, including: Number of rows n and columns m of the CCs. Instantiation of the SFUs, which determine the positions and computations of each SFU. FIFO connections between the CCs and SFUs. Scratchpad memory size and number of memory ports. Reconfigurable PE array dimensions. Depending on the schedule, we assign values to these pa- rameters to perform the three execution modes mentioned in Section III-A. The right of Figure 2 depicts the example ar- chitectures of each execution mode derived from the template for n, m 2 for the case study in Section II-E. The colors of connections indicate the purpose of communications (read input and weights, perform reductions, redistribute output, or pipeline intermediate results). For a specific schedule, we com- pose the derived architectures of the execution modes utilized to get the final circuit design. Common interconnections and control logic across execution modes will be merged, and unused interconnects will never be introduced. IV. DESIGNING ACCELERATORS IN INTAR The first step in designing INTAR accelerators is perform- ing design space exploration (DSE). Given a dataflow graph (DFG) and device constraints, one searches through various combinations of architecture parameters and task execution modes (see Section III) to minimize total latency. A. Finding Schedule and Architecture Template Parameters In this work, we first topologically sort the tasks based on dependencies in the DFG and apply a heuristic-based DSE: If the total size of output and previously cached data is larger than the memory constraint, then select the task-pipeline mode. Otherwise, if task-parallel mode can improve locality for the subsequent tasks as illustrated in Section III-A, then pick task-parallel mode. If not, then choose the sequential mode. Non-linear operations are pipelined with previous tasks. Then, we analyze the generated schedule and the hardware configurations and determine architecture template parameters with the following heuristics: The schedule infers the instan- tiation of the SFUs and FIFO connections.\n\n--- Segment 13 ---\nNon-linear operations are pipelined with previous tasks. Then, we analyze the generated schedule and the hardware configurations and determine architecture template parameters with the following heuristics: The schedule infers the instan- tiation of the SFUs and FIFO connections. The column count of CCs equals the maximum number of tasks of all stages, and the row count is the number of dies on the FPGA necessary to improve floorplanning. We compute the PEA dimensions to maximize utilization with limited cross-die communications and calculate the memory ports needed. Finally, the scratchpad memory is the maximum memory size required over all stages. For applications with variable input size, we schedule based on the maximum possible size and employ batching [33] to prevent inefficiency of the design for small data size. Although the heuristics are not guaranteed optimal, we show that our schedule results improved performance consistently (see Sec- tions V, VI). We leave the development of an optimized DSE engine as future work. B. Designing Reconfigurable Modules: A Case Study in HLS In this work, we require the designer to manually write the reconfiguration designs based on the static schedule. Following the architecture template, there are three types of reconfigurations: data movement, compute, and control. All of them can be implemented in HLS efficiently with conditional dataflow, which utilizes if-else statements to declare the behavior changes and exploit HLS s resource binding pass to merge interconnects and logic as much as possible. Data Movement Reconfiguration: a majority of auto- reconfigurations in INTAR are powered by data movement control. In different stages, each CC may load data from various sources (e.g., other CCs, on-chip buffers, registers, or constants) and send it to multiple destinations. Figure 3 is an example of the data movement control reconfiguration in CC3: data flow from CC1 to buf 1 at stage 0 and flow from buf 2 to CC 2 at stage 1. The corresponding HLS code is a realization using the stage index as the condition to identify which flow the data movement control should adopt. Different stages can share data sources and destinations, which can potentially save interconnects. For instance, if the data flows from CC1 for each stage, the reader module can be reused. Fig. 3. Example of data movement reconfiguration.\n\n--- Segment 14 ---\n3. Example of data movement reconfiguration. Left: In different stages, PE 3 either read from PE 1 and write to on-chip buffer buf 1, or read from buf 2 and write to PE 2. Right: the corresponding HLS code. Compute Reconfiguration: Based on the definition of the architecture template, the reconfigurable PEA can change the input operands and the precision. Figure 4 illustrates a PEA switching the data source between stages, where each source is interpreted as a different precision. To change the input operands, we adopt a similar method of data movement reconfiguration to extract inputs from two source buffers conditioned on the stage index. Inputs are packed for data parallelism. Then, when computations start, each PE will select part of the input operand and pad with zeros to make sure the bitwidths are uniform across stages. This guides the HLS to bind compute resources between stages. The two steps cannot be merged, otherwise, the HLS will instantiate two PEAs with different precision specifications. Fig. 4. Example of compute reconfiguration. Left: The reconfigurable PEA consumes different buffers at each stage and supports mixed precision of b-bit and c-bit calculations. Right: the corresponding HLS code. Control Reconfiguration: In INTAR, there are control re- configurations for loop bounds and data-dependent conditional dataflow. For loop-bound control, we can implement variable loop bounds for different stages of computations using the con- figuration instructions (left of Figure 5). For data-dependent conditional dataflow, when the behavior of executions depends on both stage index and other data sources, we can compose the conditions with logical operators (right of Figure 5). Fig. 5. Example of control reconfigurations in HLS. Left: loop-bound control. Right: data-dependent conditional dataflow. All accelerators in the INTAR paradigm are covered by the compositions of the three types of reconfigurations mentioned above, indicating complete support in HLS. C. Important Considerations in Placement and Routing In order to achieve rapid development, we need to make sure that the generated design is both valid and performant. Fol- lowing are several important considerations of implementing a placeable and routable INTAR design with high frequency. Distribute Independent Tasks Across Dies.\n\n--- Segment 15 ---\nFol- lowing are several important considerations of implementing a placeable and routable INTAR design with high frequency. Distribute Independent Tasks Across Dies. For multi-die FPGAs [34], [35], cross-die wires are limited and often negatively affect the timing. Thus, when encountering a design with resource utilization higher than the single-die capacity, cross-die wiring latency can be the bottleneck of low clock frequency. A simple heuristic is to place independent tasks (e.g., multi-head attentions) into different dies instead of exe- cuting with all resources to avoid cross-die communications. Since there is no data movement between these tasks, we also do not add FIFOs between these tasks, which further reduces the cross-die wiring. Matching On-chip Memory Parallel Access Dimension. To serve multiple MAC units in each PEA, scratchpad memory is partitioned by access patterns. In INTAR, the PEA in the CCs may require varying patterns between tasks. For example, Figure 6 shows two dependent GEMMs: the first computes 2048 MACs simultaneously (4 8 64), and the second computes 1024 MACs (4 4 64). Since the PEA is reused across tasks, we aim to equalize the throughputs of the two matrix multiplies for efficiency, necessitating further partitioning of the second GEMM. This causes high-fanout nets at the write ports, increases cycle time, and reduces frequency. We address this by tiling operands so that output tiles have equal dimensions (e.g., 16 16), aligning parallel read write dimensions even after matrix transpose. Fig. 6. Example of a design with high-fanout nets on write ports of on-chip memory banks due to unaligned parallel memory access dimensions. To evaluate INTAR, we will show its broadness in DNN accelerations (Section V) and its advantages when deploying more complex DNNs on different FPGAs (Section VI). V. EVALUATION 1: MULTI-TASK KERNELS IN HDV DNNS A.\n\n--- Segment 16 ---\nTo evaluate INTAR, we will show its broadness in DNN accelerations (Section V) and its advantages when deploying more complex DNNs on different FPGAs (Section VI). V. EVALUATION 1: MULTI-TASK KERNELS IN HDV DNNS A. Experiment Setup To demonstrate the broadness of INTAR in HDV DNN accelerations, we construct a testbench with five multi-task kernels that can serve as a standalone DNN application (Multi- layer CNN, Variational Autoencoder) or participate in part of the DNN applications (Self Attention, FFN Layer, Gating Network). Table III illustrates these kernels in the evaluation. Unlike existing benchmarks [41], [42] for FPGA-based ac- celerator evaluation, our testbench has three features crucial for HDV: ①it covers various DNN applications in computer vision, language modeling, graph processing, etc., displayed in the DNN Applications column; ②it adopts hyperparameters used in the real applications (e.g., Gating Network has the identical dimensionalities as Llama 2 [3]); and ③kernels are all HDV, and we impose a memory constraint between the minimum and maximum input output data sizes so that INTAR can exploit its ability to switch execution modes. Using our testbench, we compare INTAR with human- optimized dataflow and sequential accelerators, each designed in four weeks. All designs are evaluated under identical resource constraints, including on-chip memory capacity, the number of DSPs, and off-chip memory bandwidth utilization. All designs are implemented utilizing Xilinx Vitis HLS 2021.2 TABLE III DESCRIPTIONS OF MULTI-TASK KERNELS WITH THE CORRESPONDING PARAMETERS AND DATA SIZE CONSTRAINTS. THE ON-CHIP MEMORY CONSTRAINTS ARE ARTIFICIAL TO MAKE THE KERNEL HDV. Kernel Description DNN Applications Parameters Data Size Constraints Data Variation Pattern Self Attention (Attn) A set of dependent matrix multiplications that extract the contextual relationships between tokens in a sequence. Widely used in sequence modeling (Transformer [5], ViT [36], GAT [37]) 256 token sequences. The hidden dimension is 1024.\n\n--- Segment 17 ---\nWidely used in sequence modeling (Transformer [5], ViT [36], GAT [37]) 256 token sequences. The hidden dimension is 1024. Min data size: 0.5 MB, Max data size: 1.0 MB, On-chip memory constraint: 0.625 MB Increase (Q, K) De- crease (Q, K A) Increase (V ) Decrease (A, V V ) FFN Layer (FFN) Three layers of linear projections for feature extractions. Input is a single vector. Elementary block in many CV and LLM applications (ResNet [15], YoloV3 [38], Transformer) Input size final output size 256. First layer output size 1024, second layer output size 2048. Min data size: 0.5 KB, Max data size: 2 KB, On-chip mem- ory constraint: 1 KB Increase Increase Decrease Multi-layer CNN (M- CNN) Four layers of convolutions with upsampling and pooling layers Used in many CV applications (ResNet, VGG [39]) Input size 224, kernel size 3, 2 2 upsampling and pooling size. Min data size: 98 KB, Max data size: 1.57 MB, On-chip mem- ory constraint: 125 KB. Increase Increase Decrease Decrease Variational Autoencoder (VAE) Contains an encoder in convolutions for the latent space distribution and a decoder in transpose convolutions. Used for image compression and the generator model in GAN (VAE-GAN [40]) Input size 28, kernel sizes are 4 and 8, 2 channels and 2 filters Min data size: 1.3 KB, Max data size: 3.1 KB, On-chip memory constraint: 1.5 KB Decrease (encoder 1) Decrease (encoder 2) Increase (decoder 1) In- crease (decoder 2) Gating Network (GN) Two parallel linear projections with element-wise product and a sequential linear projection.\n\n--- Segment 18 ---\nIncrease Increase Decrease Decrease Variational Autoencoder (VAE) Contains an encoder in convolutions for the latent space distribution and a decoder in transpose convolutions. Used for image compression and the generator model in GAN (VAE-GAN [40]) Input size 28, kernel sizes are 4 and 8, 2 channels and 2 filters Min data size: 1.3 KB, Max data size: 3.1 KB, On-chip memory constraint: 1.5 KB Decrease (encoder 1) Decrease (encoder 2) Increase (decoder 1) In- crease (decoder 2) Gating Network (GN) Two parallel linear projections with element-wise product and a sequential linear projection. Llama-family LLMs [3] Sequence length 32, input dimension 4096, hidden dimension 11008 Min data size: 0.25 MB, Max data size: 0.67 MB, On-chip memory constraint: 0.5 MB Increase (up projections) Decrease (element-wise product, down projection) with TAPA [43] and evaluated on Alveo U280 FPGA board, with a clock frequency of 300 MHz. We chose the human- optimized designs as the baseline since: ①There is a lack of existing designs of the kernels in our testbench with the same model and hardware configurations, and ②existing frame- works [6], [7], [44] for DNN application design generation on FPGAs do not handle off-chip memory communications or do not consider the extra resource constraints introduced to evaluate HDV DNNs. Fig. 7. Speedup and DSP efficiency of INTAR over dataflow and sequential execution for the five multi-task kernels in the testbench. Values are normal- ized from sequential accelerators. B. Analysis Figure 7 shows the speedup and DSP efficiency of InTAR over sequential and dataflow accelerators. Overall, InTAR achieves a speedup of 7.1 and 1.8 over sequential and dataflow accelerators, along with 7.5 and 1.9 higher DSP efficiency (GOP s DSP) in geometric mean. The performance boost of InTAR over sequential accelerators mainly comes from the reduction of off-chip memory access, where InTAR has 76 lower off-chip memory access volume than the sequential accelerators in geomean over the five kernels.\n\n--- Segment 19 ---\nOverall, InTAR achieves a speedup of 7.1 and 1.8 over sequential and dataflow accelerators, along with 7.5 and 1.9 higher DSP efficiency (GOP s DSP) in geometric mean. The performance boost of InTAR over sequential accelerators mainly comes from the reduction of off-chip memory access, where InTAR has 76 lower off-chip memory access volume than the sequential accelerators in geomean over the five kernels. For dataflow accelerators, InTAR is significantly faster in self- attention and multi-layer CNN than the other three kernels. There are two explanations. First, the two kernels mentioned have a higher data reuse between dependent tasks for matrix multiplications and convolutions, which requires data buffering and preprocessing before continuing the computations (e.g., the pooling layer of the multi-layer CNN will wait until producing two rows of outputs from the previous layer to downsample the data). Since the selected kernels have a long chain of dependent tasks, pipeline stall latency may dominate the end-to-end latency. Second, InTAR can better reuse off- chip memory ports for these two kernels. For example, after loading inputs and finishing the first convolution layer in the multi-layer CNN, InTAR can utilize the memory ports allocated to the inputs to write outputs. Thus, InTAR has a higher effective off-chip read write throughput than dataflow accelerators, which only use the dedicated memory port for input output. For Gating Networks, InTAR cannot further reduce latency compared with dataflow accelerators since only input and output data sizes are within the memory constraint, and our schedule for InTAR is identical to pipelining all tasks. VI. EVALUATION 2: GPT-2 INPUT PREFILLING A. Experiment Setup We choose the GPT-2 medium input prefilling stage as a complex scenario to further evaluate INTAR, which is a frequently used benchmark by prior SoTA FPGA-based LLM accelerators [6], [23]. We selected two platforms (Versal VPK180 [45] and Alveo U280 [34]), which result in different schedules and architecture template parameters based on their resource constraints.\n\n--- Segment 20 ---\nExperiment Setup We choose the GPT-2 medium input prefilling stage as a complex scenario to further evaluate INTAR, which is a frequently used benchmark by prior SoTA FPGA-based LLM accelerators [6], [23]. We selected two platforms (Versal VPK180 [45] and Alveo U280 [34]), which result in different schedules and architecture template parameters based on their resource constraints. For instance, the design for VPK180 has four rows of CCs spreading out to four SLRs of the FPGA, while the design on U280 has 3 rows of CCs to fit its 3-SLR layout. Each SLR includes two CCs and an SFU for both platforms. Thanks to the grid structure of INTAR s architecture template, we can adapt the design conveniently to other FPGA boards with only slight modifications. To generate the circuit design, we employ Xilinx HLS with the TAPA framework [43] and prototype using the Vitis 2021.2 that performs well on older platforms such as U280, and Vitis 2024.1 optimized for VPK180. The floorplanning of the CCs and SFUs is predetermined based on the architectural template, while the remaining modules (e.g., auxiliary buffers) are managed by AutoBridge [46]. The designs are both placed and routed for U280 and VPK180. For comparison, we select Allo [6] and DFX [23] as the SoTA accelerators, both of which utilize U280 as the compute platform; accordingly, we include InTAR implemented on U280. Fig. 8. Left: Latency and DSP efficiency of INTAR (U280, VPK180), Allo, and DFX. Both designs of INTAR are significantly more DSP efficient. Right: Latency and power efficiency of INTAR (U280, VPK180), Allo [6], and GPU solutions for GPT-2 medium model. TABLE IV RESOURCE UTILIZATION AND FREQUENCY OF FPGA-BASED SOLUTIONS (ALLO, DFX, AND INTAR) FOR GPT-2 MEDIUM INPUT PREFILLING TASK. SPEEDUP IS NORMALIZED BASED ON DFX.\n\n--- Segment 21 ---\nTABLE IV RESOURCE UTILIZATION AND FREQUENCY OF FPGA-BASED SOLUTIONS (ALLO, DFX, AND INTAR) FOR GPT-2 MEDIUM INPUT PREFILLING TASK. SPEEDUP IS NORMALIZED BASED ON DFX. Allo [6] DFX [23] INTAR INTAR Device U280 U280 U280 VPK180 Frequency 247 MHz 200 MHz 224 MHz 300 MHz BRAM 389 (19 ) 1192 (59 ) 535 (27 ) 700 (14 ) DSP 1780 (20 ) 3533 (39.2 ) 6727 (75 ) 6726 (47 ) LUT 569K (44 ) 520K (40 ) 485K (37 ) 1074K (32 ) FF 653K (25 ) 959K (43 ) 627K (25 ) 1072K (16 ) URAM 111 (12 ) 104 (11 ) 336 (35 ) 412 (16 ) Norm. Speedup 1.83 1.0 14.64 39.14 Table IV lists the resource utilization and frequency. IN- TAR on VPK180 has lower DSP utilization than U280 since VPK180 has fewer cross-SLR wires than U280 and PEA sizes are highly correlated with cross-SLR communications. DFX is executed in FP16, and all other designs employ the W4A8 format. We scale the DSP efficiency for DFX to align the data type. For performance metrics, we use OpenCL profiling functions to get the latency and xbutil to measure the power within 100 runs on U280. Due to a lack of access to a physical device, we employ QEMU and the Xilinx Power Design Manager to calculate the latency and estimate the power consumption of VPK180. DFX is not included in comparing power efficiency due to a lack of data in the original work. We also compare InTAR with various GPUs shown in Table V. PyTorch profiler measures the latency and memory transactions. To measure the power consumption, we employ the NVIDIA management library to probe the power every 10 ms and calculate the average power when it is stable. All GPUs are inference in BFloat16 format, which is a commonly supported and optimized data format for GPUs.\n\n--- Segment 22 ---\nTo measure the power consumption, we employ the NVIDIA management library to probe the power every 10 ms and calculate the average power when it is stable. All GPUs are inference in BFloat16 format, which is a commonly supported and optimized data format for GPUs. TABLE V HARDWARE CONFIGURATIONS OF THE FPGAS AND GPUS U280 VPK180 T4 A100 MI210 Frequency 224 MHz 240 MHz 585 MHz 765 MHz 1.7 GHz Bandwidth 460 GB s 52.1 GB s 320 GB s 1.56 TB s 1.64 TB s Peak Power 75 W 180 W 70 W 250 W 300 W Peak Perf. 8.09 TOP s 20.7 TOP s 65.13 TOP s 311.84 TOP s 181 TOP s Process Node TSMC 16nm TSMC 7nm TSMC 12nm TSMC 7nm TSMC 6nm B. Analysis Comparison with FPGA-based Accelerators. Figure 8 presents the latency, DSP efficiency, and power efficiency comparison between FPGA and GPU accelerators. Compared to Allo and DFX, INTAR on U280 is 7.99 and 14.64 faster, and 2.19 and 3.98 more DSP efficient, respectively. INTAR on VPK180 is 21.39 and 39.14 faster, and 5.66 and 10.44 more DSP efficient than Allo and DFX. INTAR on VPK180 is more DSP efficient than U280 since DSP58 has a higher throughput than DSP48. Additionally, implementing DSPs in dot-product modules on VPK180 trims the logic and further reduces cycles, leading to more than 2 boost in DSP efficiency. Furthermore, similar to Evaluation 1 in Section V, we compared InTAR with our manually implemented dataflow and sequential accelerators, achieving 1.8 and 8.3 speedup and 1.7 and 8.2 DSP efficiency boost. Comparison with GPUs. As shown in Figure 8, INTAR on U280 achieves a 1.07 speedup in geometric mean over T4 and 2.41 speedup on VPK180. Especially for short sequences (L 256), InTAR s performance surpasses both T4 and A100 GPUs. Moreover, with the same process node, INTAR on FPGAs attains higher power efficiency than GPUs. INTAR on U280 is 2.37 more efficient than NVIDIA T4.\n\n--- Segment 23 ---\nMoreover, with the same process node, INTAR on FPGAs attains higher power efficiency than GPUs. INTAR on U280 is 2.37 more efficient than NVIDIA T4. INTAR on VPK180 is 1.66 7.17 more efficient than T4, A100, MI210. One source of latency and power overhead for GPUs is the off-chip memory access. Since GPUs execute tasks sequentially and GPT-2 medium tends to generate large intermediate data, off-chip memory access is more intense on GPUs than INTAR-optimized FPGA designs. As a result, INTAR has a 20 67 lower off-chip memory access compared to T4, A100, and MI210. VII. CONCLUSION AND FUTURE WORK In this work, we propose InTAR, a novel accelerator design paradigm for DNNs with high data volume variation. InTAR enhances resource efficiency by switching execution patterns and performing model-specific optimizations. It surpasses SoTA FPGA accelerators in speed, DSP, and power efficiency, and, compared with GPUS, it reduces off-chip memory access. Although focused on DNNs, InTAR may also enhance non- DNN HDV applications, which we plan to explore. Future work includes developing an optimized DSE engine to replace heuristic scheduling and automate InTAR design compilation. ACKNOWLEDGEMENT This work was supported in part by PRISM, one of the seven centers in the JUMP 2.0 program sponsored by SRC and DARPA. It is also supported by CDSC industrial partners and the AMD 2 HACC Program. 2J. Cong has a financial interest in AMD REFERENCES [1] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon, Unified language model pre-training for natural language understanding and generation, Advances in neural information processing systems, vol. 32, 2019.\n\n--- Segment 24 ---\nCong has a financial interest in AMD REFERENCES [1] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H.-W. Hon, Unified language model pre-training for natural language understanding and generation, Advances in neural information processing systems, vol. 32, 2019. [2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [3] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [4] M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib, M. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, A review on large language models: Architectures, applications, taxonomies, open issues and challenges, IEEE Access, 2024. [5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [6] H. Chen, N. Zhang, S. Xiang, Z. Zeng, M. Dai, and Z. Zhang, Allo: A programming model for composable accelerator design, Proceedings of the ACM on Programming Languages, vol. 8, no. PLDI, pp. 593 620, 2024.\n\n--- Segment 25 ---\nPLDI, pp. 593 620, 2024. [7] S. Basalama, A. Sohrabizadeh, J. Wang, L. Guo, and J. Cong, Flexcnn: An end-to-end framework for composing cnn accelerators on fpga, ACM Transactions on Reconfigurable Technology and Systems, vol. 16, no. 2, pp. 1 32, 2023. [8] N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai, N. Patil, S. Subramanian, A. Swing, B. Towles et al., Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware sup- port for embeddings, in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1 14. [9] Y. Bai, H. Zhou, K. Zhao, H. Wang, J. Chen, J. Yu, and K. Wang, Fet-opu: A flexible and efficient fpga-based overlay processor for transformer networks, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 9. [10] Z. Liu, G. Li, and J. Cheng, Hardware acceleration of fully quantized bert for efficient natural language processing, in 2021 Design, Automa- tion Test in Europe Conference Exhibition (DATE). IEEE, 2021, pp. 513 516. [11] X. Zhang, H. Ye, J. Wang, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen, Dnnexplorer: a framework for modeling and exploring a novel paradigm of fpga-based dnn accelerator, in Proceedings of the 39th International Conference on Computer-Aided Design, 2020, pp. 1 9. [12] H. Chen, J. Zhang, Y. Du, S. Xiang, Z. Yue, N. Zhang, Y. Cai, and Z. Zhang, Understanding the potential of fpga-based spatial accel- eration for large language model inference, ACM Transactions on Reconfigurable Technology and Systems, 2024.\n\n--- Segment 26 ---\n[12] H. Chen, J. Zhang, Y. Du, S. Xiang, Z. Yue, N. Zhang, Y. Cai, and Z. Zhang, Understanding the potential of fpga-based spatial accel- eration for large language model inference, ACM Transactions on Reconfigurable Technology and Systems, 2024. [13] J. Zhuang, Z. Yang, S. Ji, H. Huang, A. K. Jones, J. Hu, Y. Shi, and P. Zhou, Ssr: Spatial sequential hybrid architecture for latency throughput tradeoff in transformer acceleration, in Proceedings of the 2024 ACM SIGDA International Symposium on Field Programmable Gate Arrays, 2024, pp. 55 66. [14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [15] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770 778. [16] S. Xie, R. Girshick, P. Doll ar, Z. Tu, and K. He, Aggregated residual transformations for deep neural networks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1492 1500. [17] J. Cai, Y. Wei, Z. Wu, S. Peng, and K. Ma, Inter-layer scheduling space definition and exploration for tiled accelerators, in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 1 17. [18] J. Cong, H. Huang, C. Ma, B. Xiao, and P. Zhou, A fully pipelined and dynamically composable architecture of cgra, in 2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Com- puting Machines. IEEE, 2014, pp. 9 16. [19] E. Baek, D. Kwon, and J. Kim, A multi-neural network acceleration architecture, in 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA).\n\n--- Segment 27 ---\n9 16. [19] E. Baek, D. Kwon, and J. Kim, A multi-neural network acceleration architecture, in 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2020, pp. 940 953. [20] S. Liu, J. Weng, D. Kupsh, A. Sohrabizadeh, Z. Wang, L. Guo, J. Liu, M. Zhulin, R. Mani, L. Zhang et al., Overgen: Improving fpga usability through domain-specific overlay generation, in 2022 55th IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 2022, pp. 35 56. [21] T. Kong, K. Koul, P. Raina, M. Horowitz, and C. Torng, Hardware ab- stractions and hardware mechanisms to support multi-task execution on coarse-grained reconfigurable arrays, arXiv preprint arXiv:2301.00861, 2023. [22] J. Cong, J. Lau, G. Liu, S. Neuendorffer, P. Pan, K. Vissers, and Z. Zhang, Fpga hls today: successes, challenges, and opportunities, ACM Transactions on Reconfigurable Technology and Systems (TRETS), vol. 15, no. 4, pp. 1 42, 2022. [23] S. Hong, S. Moon, J. Kim, S. Lee, M. Kim, D. Lee, and J.-Y. Kim, Dfx: A low-latency multi-fpga appliance for accelerating transformer-based text generation, in 2022 55th IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 2022, pp. 616 630. [24] S. Abi-Karam, Y. He, R. Sarkar, L. Sathidevi, Z. Qiao, and C. Hao, Gengnn: A generic fpga framework for graph neural network acceler- ation, arXiv preprint arXiv:2201.08475, 2022.\n\n--- Segment 28 ---\n[24] S. Abi-Karam, Y. He, R. Sarkar, L. Sathidevi, Z. Qiao, and C. Hao, Gengnn: A generic fpga framework for graph neural network acceler- ation, arXiv preprint arXiv:2201.08475, 2022. [25] D. Gschwend, Zynqnet: An fpga-accelerated embedded convolutional neural network, arXiv preprint arXiv:2005.06892, 2020. [26] S. Zeng, J. Liu, G. Dai, X. Yang, T. Fu, H. Wang, W. Ma, H. Sun, S. Li, Z. Huang et al., Flightllm: Efficient large language model inference with a complete mapping flow on fpgas, in Proceedings of the 2024 ACM SIGDA International Symposium on Field Programmable Gate Arrays, 2024, pp. 223 234. [27] M. Tanomoto, S. Takamaeda-Yamazaki, J. Yao, and Y. Nakashima, A cgra-based approach for accelerating convolutional neural networks, in 2015 IEEE 9th International Symposium on Embedded Multicore Many- core Systems-on-Chip. IEEE, 2015, pp. 73 80. [28] I. Taras and J. H. Anderson, Impact of fpga architecture on area and performance of cgra overlays, in 2019 IEEE 27th Annual Interna- tional Symposium on Field-Programmable Custom Computing Machines (FCCM). IEEE, 2019, pp. 87 95. [29] K. Vipin and S. A. Fahmy, Fpga dynamic and partial reconfiguration: A survey of architectures, methods, and applications, ACM Computing Surveys (CSUR), vol. 51, no. 4, pp. 1 39, 2018. [30] J. Tong, A. Itagi, P. Chatarasi, and T. Krishna, Feather: A reconfigurable accelerator with data reordering support for low-cost on-chip dataflow switching, in Proceedings of the 51th Annual International Symposium on Computer Architecture, ser. ISCA 24. Argentina: Association for Computing Machinery, 2024.\n\n--- Segment 29 ---\nISCA 24. Argentina: Association for Computing Machinery, 2024. [31] Y. Yu, C. Wu, T. Zhao, K. Wang, and L. He, Opu: An fpga-based overlay processor for convolutional neural networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 1, pp. 35 47, 2019. [32] Y. Yu, T. Zhao, K. Wang, and L. He, Light-opu: An fpga-based overlay processor for lightweight convolutional neural networks, in Proceedings of the 2020 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, 2020, pp. 122 132. [33] M. Looks, M. Herreshoff, D. Hutchins, and P. Norvig, Deep learning with dynamic computation graphs, arXiv preprint arXiv:1702.02181, 2017. [34] Alveo u280 data center accelerator card data sheet. [Online]. Available: [35] Amd versal hbm series product selection guide. [Online]. Available: [36] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu et al., A survey on vision transformer, IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 1, pp. 87 110, 2022. [37] P. Veliˇckovi c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Ben- gio, Graph attention networks, arXiv preprint arXiv:1710.10903, 2017. [38] A. Farhadi and J. Redmon, Yolov3: An incremental improvement, in Computer vision and pattern recognition, vol. 1804. Springer Berlin Heidelberg, Germany, 2018, pp. 1 6. [39] A. Vedaldi and A. Zisserman, Vgg convolutional neural networks practical, Department of Engineering Science, University of Oxford, vol. 66, 2016.\n\n--- Segment 30 ---\n[39] A. Vedaldi and A. Zisserman, Vgg convolutional neural networks practical, Department of Engineering Science, University of Oxford, vol. 66, 2016. [40] R. Gao, X. Hou, J. Qin, J. Chen, L. Liu, F. Zhu, Z. Zhang, and L. Shao, Zero-vae-gan: Generating unseen features for generalized and trans- ductive zero-shot learning, IEEE Transactions on Image Processing, vol. 29, pp. 3665 3680, 2020. [41] Y. Zhou, U. Gupta, S. Dai, R. Zhao, N. Srivastava, H. Jin, J. Featherston, Y.-H. Lai, G. Liu, G. A. Velasquez, W. Wang, and Z. Zhang, Rosetta: A Realistic High-Level Synthesis Benchmark Suite for Software- Programmable FPGAs, Int l Symp. on Field-Programmable Gate Ar- rays (FPGA), Feb 2018. [42] L.-N. Pouchet et al., Polybench: The polyhedral benchmark suite, URL: cs. ucla. edu pouchet software polybench, vol. 437, pp. 1 1, 2012. [43] L. Guo, Y. Chi, J. Lau, L. Song, X. Tian, M. Khatti, W. Qiao, J. Wang, E. Ustun, Z. Fang et al., Tapa: a scalable task-parallel dataflow programming framework for modern fpgas with co-optimization of hls and physical design, ACM Transactions on Reconfigurable Technology and Systems, vol. 16, no. 4, pp. 1 31, 2023. [44] J. Duarte et al., Fast inference of deep neural networks in FPGAs for particle physics, JINST, vol. 13, no. 07, p. P07027, 2018. [45] Amd versal premium series product selection guide. [Online].\n\n--- Segment 31 ---\n[45] Amd versal premium series product selection guide. [Online]. Available: [46] L. Guo, Y. Chi, J. Wang, J. Lau, W. Qiao, E. Ustun, Z. Zhang, and J. Cong, Autobridge: Coupling coarse-grained floorplanning and pipelining for high-frequency hls design on multi-die fpgas, in The 2021 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, 2021, pp. 81 92.\n\n