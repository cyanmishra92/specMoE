=== ORIGINAL PDF: 2507.05531v1_Bit-Flip_Fault_Attack_Crushing_Graph_Neural_Networ.pdf ===\n\nRaw text length: 40476 characters\nCleaned text length: 40264 characters\nNumber of segments: 25\n\n=== CLEANED TEXT ===\n\nBit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search Sanaz Kazemi Abharian Department of Electrical and Computer Engineering George Mason University Fairfax, VA, USA Sai Manoj Pudukotai Dinakarrao Department of Electrical and Computer Engineering George Mason University Fairfax, VA, USA Abstract Graph Neural Networks (GNNs) have emerged as a powerful machine learning method for graph-structured data. A plethora of hardware accelerators has been introduced to meet the performance demands of GNNs in real-world applications. However, security challenge of hardware-based attacks have been generally overlooked. In this paper, we investigate the vulnera- bility of GNN models to hardware-based fault attack, wherein an attacker attempts to misclassify output by modifying trained weight parameters through fault injection in a memory device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer- aware bit-flip fault attack, select a vulnerable bit in each selected weights gradually to compromise the GNN s performance by flipping minimal number of bits. To achieve this, GBFA operates in two steps. First, a Markov model is created to predict the execution sequence of layers based on features extracted from memory access patterns, enabling the launch of the attack within a specific layer. Subsequently, GBFA identifies vulnerable bits within the selected weights using gradient ranking through an in- layer search. We evaluate the effectiveness of the proposed GBFA attack on various GNN models for node classification tasks using the Cora and PubMed datasets. Our findings show that GBFA significantly degrades prediction accuracy, and the variation in its impact across different layers highlights the importance of adopting a layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE s prediction accuracy by 17 on the Cora dataset with only a single bit flip in the last layer. Index Terms Graph Neural Networks, Bit flip, Fault, Attack, Hardware Accelerators, Security I. INTRODUCTION Graph Neural Networks (GNNs) have emerged as a practical deep learning approach for graph-structured data, which is use- ful in various real-world applications such as social networks, electronic design automation (EDA) for accelerating hardware verification [26], [7], circuit security tasks [3], [8], [2], and recommendation systems [30]. To address the limitations of software-based GNN implementations, hardware-based GNN accelerators are introduced in the recent years [15]. These GNN hardware accelerators have shown higher energy effi- ciency, lower latency, and improved performance compared to their software counterparts [33]. GNNs involve specialized computations characterized by irregular and sparse graph structures, necessitating efficient hardware accelerators such as GPUs [31], FPGAs [28], and ASICs [11] to handle data movement, parallelize sparse com- putations, and minimize latency. Among these, custom ASIC accelerators offer significant advantages due to their tailored designs, which closely align with the irregular computational patterns of GNN workloads. Specifically, ASICs achieve lower latency, higher throughput, and substantially reduced energy consumption by optimizing data flow and integrating tightly coupled on-chip memory and processing logic. Unlike FPGAs, ASICs do not incur overhead from reconfigurable logic and routing, resulting in better performance density and improved power efficiency. Furthermore, ASIC accelerators often sup- port 32-bit floating-point (FP32) arithmetic to preserve nu- merical precision during both training and inference phases. This design choice is reflected in several recent architectures, including HyGCN [32] and ReGNN [1], which adopt FP32 arithmetic to ensure stable and accurate computations. Despite their superior performance and other benefits, recent works have shown that hardware accelerators are vulnerable to hardware attacks [22], among which bit-flip attacks (BFAs) are considered one of the prominent attacks as they can crush a neural network through maliciously flipping a small number of bits within its weight parameters stored in memory. There are various methods, such as the Laser Beam attack and Row Hammer Attack (RHA), that perform bit-flip operations. RHA, introduced by Kim et al. [13], exploits a vulnerability in dy- namic random access memory (DRAM), allowing an attacker to manipulate data without direct access. This is achieved by repeatedly hammering specific rows of memory cells, which induces bit flips in adjacent rows. While the The impact of BFAs has been widely studied on Deep Neural Network (DNN) accelerators [21], their effect on GNNs remains largely unexplored, and the security risks posed by BFAs in GNN models have been overlooked. Software-based adversarial attacks on GNNs such as Eva- sion attacks and poisoning attacks [12] can be detected and mitigated using the existing techniques such as differential privacy and federated learning [17], [27]. However, the non- volatile nature of BFAs makes them hard to detect and analyze. In this work, we introduce a two-pronged approach for launching our proposed BFA on the GNN hardware accelera- tor. First, we determine the GNN layer that is being executed by the hardware based on recorded memory patterns and use a Markov model to predict the layer sequence. Once the executing layer is identified, the proposed Gradual Bit-Flip arXiv:2507.05531v1 [cs.LG] 7 Jul 2025 Fault Attack (GBFA) on GNN models is introduced, which injects faults into trained weights using a layer-aware method that can cause a GNN to misclassify with a minimal number of bit flips. Our proposed GBFA attack gradually selects weights through an in-layer search based on the Bit Error Rate (BER) to ensure a decrease in prediction accuracy with a minimal number of bit flips. We evaluate the effectiveness of the proposed GBFA Attack across multiple GNN architectures using two widely adopted benchmark datasets: Cora and PubMed [14]. Our experimental results demonstrate that performing a targeted in-layer search is critical for maximizing the impact of GBFA while min- imizing the number of bit flips. Additionally, our analysis reveals that the degree of accuracy degradation induced by GBFA varies across different GNN models, depending on the specific layer targeted. These findings highlight the critical impact of layer-wise vulnerability in enhancing the success of bit-flip fault attacks on GNNs. We observe that the attack consistently leads to a notable degradation in model accuracy across different GNN architectures. These results collectively validate the broad applicability and efficacy of GBFA as a stealthy and impactful fault injection method against GNNs. The novel contributions of this work is summarized as follows: We propose a hardware-based bit-flip fault attack on GNN models to degrade prediction accuracy on node classification task. We demonstrate that in-layer searches are essential for an effective bit-flip attack on GNNs. To identify and target a specific layer, our attack analyzes memory patterns and employs a Markov model, incorporating a Connectionist Temporal Classification (CTC) decoder to predict the run- time layer sequence. To achieve the desired accuracy degradation with a min- imal number of bit flips, our proposed GBFA attack integrates gradient-based ranking and a gradual search within a specific layer to identify a vulnerable bit in each selected weight parameter. The bit-flip operations are then performed along the gradient s ascending direction. Fig. 1. Computational procedure of a single GCNConv layer II. BACKGROUND AND RELATED WORKS A. Graph Neural Networks GNNs are a class of neural networks designed to operate on graph-structured data [24]. Graph Convolutional Networks (GCNs), constructed using GCNConv layer, operate in two fundamental phases feature aggregation and linear feature transformation as shown in Figure 1. The forward pass in the l- th GCNConv layer is functioned into two stages: 1) a feature aggregation step followed by 2) feature transformation step. Each node aggregates feature information from its neighbors through an aggregation function, capturing local structural dependencies. The resulting aggregated feature vectors are then passed through a combination function, typically param- eterized by shared trainable weights to produce updated node representations [4]. Variants of GCNs, such as GraphSAGE, Graph Isomor- phism Network (GIN), Graph Attention Network (GAT), retain the general message-passing framework but differ in their neighborhood aggregation functions, while still conforming to the fundamental propagation scheme of standard GCNs. B. Related Works Recent research demonstrates the vulnerability of Deep neural network (DNN) parameters stored in DRAM to bit-flip fault attacks. Liu et al. [19] proposed a Single Bias Attack (SBA), which targets a specific bias term of a neuron to cause misclassification in a DNN. The results demonstrate that even modifying a single bias can result in misclassification. Hong et al. [9] investigate the impact of bit-flip fault attacks on the weights of various DNN architectures with full precision, leveraging the Row Hammer method. Their study examines how bit position, flip direction, parameter sign, and layer width influence the effects of such attacks. In [25], Rakin et al. introduced the progressive bit search (PBS) method, which leverages the Rowhammer technique to target the quantized weight parameters of DNNs. PBS ranks and progressively searches for the most vulnerable bits, flipping them in a way that maximizes accuracy degradation. The effectiveness of this attack is investigated on ResNet-18 using the ImageNet dataset, causing a significant drop in accuracy. In [16], the injective bit flip attack (IBFA) was introduced on a quantized GNN model. The IBFA targets bit flipping in the injective properties of the neighborhood aggregation function in message passing layers to reduce their ability to distinguish non-isomorphic graphs and to impair the expressivity of the Weisfeiler-Leman test. Limitation of previous works. Prior research on BFAs has primarily focused on DNNs, with limited attention to their im- pact on GNNs. The unique computational structure of GNNs warrants further investigation to assess their vulnerability to BFAs. Additionally, while studies [20] [29] demonstrate that bit-flip faults at different layers affect the output differently, most fault attacks overlook layer-specific effects. To address this, our proposed GBFA attack focuses on in-layer search. Fig. 2. The overview of the Hardware accelerator III. PROPOSED HARDWARE-BASED BIT-FLIP FAULT ATTACK ON ML ACCELERATORS The proposed GBFA is a gray-box fault-injection attack in which the adversary has physical access to the hardware accelerator executing the target GNN model. Furthermore, the adversary has no knowledge of the GNN model, training dataset, or hyperparameters. However, it is assumed that the adversary has access to the gradients and the test dataset. For each trained weight subjected to the fault attack, only a single bit can be affected. Additionally, it is assumed that the training process and its parameters are fault-free prior to the attack. A. Design of GNN Accelerator As aforementioned, a plethora of hardware accelerators for GNNs are proposed in the literature [34]. In this work, we consider Reconfigurable GNN accelerator [1] as our base design and adapt it to the proposed GNN models and datasets. The GNN accelerator shown in Figure 2 is a configurable, pipelined hardware accelerator, designed to efficiently exe- cute GNNs by eliminating computational and communication redundancies. The architecture contains three primary pro- cessing modules: Re-coordinator, Re-aggregator, and Flexible Updaters containing processing elements (PE), which are coordinated by a central controller and interconnected via a dy- namic switch network. The Re-coordinator is responsible for scheduling tasks based on a redundancy-aware graph structure, identifying opportunities to reuse intermediate computations. It consists of a structure prefetcher, vertex prefetcher, and a dispatcher that dynamically allocate EdgeUpdate and Aggre- gation workloads while minimizing redundant data movement. The Re-aggregator performs feature aggregation for both target vertices and redundant neighbor sets using a parallel aggrega- tion engine supported by a neighbor shuffler and task buffer; it caches intermediate aggregation results to reduce compu- tation and communication redundancy. The Flexible Updaters implement matrix-vector multiplications for both EdgeUpdate and VertexUpdate using systolic arrays. In GNNs without EdgeUpdate, these arrays can be reconfigured and combined to maximize utilization. A central controller interprets a three- bit configuration word, indicating the presence of EdgeUpdate, EdgeUpdate Redundancy (ER), and Aggregation Redundancy (AR), to reconfigure micro-configurable switches (SW0-SW3) and dynamically adapt the execution pipeline to various GNN variants. To support high-throughput and energy-efficient execution, ReGNN incorporates a hierarchical memory system compris- ing both on-chip and off-chip components. Off-chip memory is implemented using high-bandwidth memory (HBM) with a bandwidth of 256 GB s to store vertex features, edge informa- tion, and weights. The on-chip memory includes specialized buffers for EdgeUpdate, Aggregation, weights, and structural metadata, as well as a configurable vertex cache. This cache stores original vertex features when redundancy elimination is not applied, and caches intermediate EdgeUpdate and Aggre- gation results when ER and AR are present, respectively. To improve cache efficiency, ReGNN employs a priority- based management strategy, giving preference to high-degree vertices and larger redundant sets. Additionally, double buffer- ing is used across memory modules to hide data transfer latency and overlap communication with computation. This memory architecture ensures minimal DRAM access, max- imized data reuse, and improved pipeline utilization, con- tributing significantly to ReGNN s performance and energy efficiency. B. Deploying GBFA on Hardware Accelerator Figure 3 illustrates the overall workflow of the proposed GBFA attack, which comprises two primary stages: identifying the target layer and injecting bit-flip faults into the selected weight parameters via an in-layer search within the targeted layer. 1) Run-time Layer Sequence Prediction: Input layer, mes- sage passing layer, transformation layer, readout layer, and output layer are some of the common layers in GNN models [18]. Each layer of a GNN model exhibits a distinct memory access pattern and computational cycles per kernel at runtime. These characteristics make the model vulnerable to hardware snooping attacks, enabling an attacker to observe execution patterns. By leveraging this information, the adversary can predict and infer individual layers without access to the neural network s parameters. Record Memory Patterns: An attacker can access GDDR bus to extract memory patterns, either by physically probing the interconnect or utilizing a DMA-capable device to achieve kernel events. The GDDR bus can reveal kernel events and memory copy size (Memcp), which can be further leveraged to infer execution latency (Exelat). Similarly, monitoring the device memory bus allows the extraction of memory request traces, from which key memory-related parameters, such as the number of read and write operations (Rv, Wv) and raw data dependencies dRAW can be inferred. This architectural information leakage is used to form feature inputs for the HMM model. Fig. 3. The overview of the proposed GBFA Attack Architectural features of kernels, such as kernel execution time (Exelat), kernel read volume (Rv), write volume through the memory bus (Wv), Input output data volume ratio of each kernel (Iv Ov), where the output volume (Ov) is measured by the write volume of this kernel, and input volume (Iv) is measured by the write volume of the previously executed ker- nel, kernel dependency distance (kdd) represents the topology influence. kdd is defined as the maximum distance between a given kernel and its preceding dependent kernels during the execution of a kernel sequence. This metric encodes layer structure information within the kernel features which can be computed as kdd max(dRAW). These features are transformed into kernel features by extracting tracking memory patterns. A Layer Sequence Predictor: Once the memory access patterns are recorded, we introduce a layer sequence predictor for predicting the GNN s layer. To build a model to predict layers, two primary information sources are considered: ar- chitectural kernel execution features and the distribution of layer context within the sequence. Thus, the runtime-layer sequence prediction problem can be formalized as a sequence- to-sequence model prediction which is described as follows: The input is kernel execution feature sequence Xt with temporal length of T described as an array: (Exelat, Rv, Wv, Iv Wv, kdd)t obtained from the memory access pattern observation. As the proposed GNN layer pre- diction involves analyzing temporal memory access patterns, we devise a Hidden Markov Model (HMM) incorporating a Connectionist Temporal Classification (CTC) decoder. The Markov Model provides a probabilistic method where each state represents a specific layer, and transitions between states capture the probability of execution order based on the ob- served hardware characteristics i.e., memory access patterns. Thus, we formulate the object function of HMM layer se- quence identifier is to minimize the CTC cost for a given target layer sequence L which is computed as: cost(X) log P(L h(X)) (1) The P(L h(X)) represents the probability of observing result L given the input X. As a case in point, there is a sequence consisting three execution kernels. At each time step (t0, t1, andt2), the HMM produces a probability distribution over the layer operations and CTC decoder employs beam search to determine the sequence with the highest probability. 2) Layer-aware Fault Attack Injection: GBFA detects a vulnerable bit in a weight using a method similar to FGSM [6]. It utilizes the gradient ascent direction w.r.t. the loss function to rank bits. Thus, it calculates the gradient of a weight b, represented in binary format, according to Equation 2: bL L b31 , . . . , L b0 (2) Where L represents the inference loss function of GNN pa- rameterized by b. To flip the bit, the straightforward approach involves directly flipping bits based on the gradients derived in Equation 3, resulting in the perturbed bits: ˆb b sign( bL) (3) However, this may lead incorrect result because of overflow. GBFA decides to flip the bit based on Table I in order to increase the loss function. I is an indicator that specifies whether the bit-flip operation should be performed. Thus, this is formulated in Equation 4. ˆbi sign L bi (bi I) (4) TABLE I GBFA TRUTH TABLE DEPICTING WHETHER THE VALUE OF bi ALTERS OR NOT bi sgn( L bi) ˆbi I 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 Algorithm 1 Layer-aware Fault Attack Injection BitFlip (GNN) 1: Target a Layer 2: Set BER 3: for i in range BER do 4: Select weights randomly in the target layer 5: end for 6: while (num weights! BER) do 7: Compute L w [ L bn ... L b0 ] 8: if (bi 0 sgn( L w ) 0) (bi 1 sgn( L w ) 0) then 9: Flip the bit 10: else 11: Ignore the weight 12: end if 13: end while 14: Conduct a performance evaluation of the GNN model with modified weights. 15: if attack is successful then 16: End procedure 17: else 18: set another BER or target another layer 19: end if After determining the executing layer of the GNN in the previous step, GBFA identifies a vulnerable bit in the selected weight through an in-layer search, as described in Algorithm 1. In each iteration, the gradient of bits in the selected weight is computed and ranked. Then, the bit with the highest gradient and the corresponding I value, according to Table I, is selected for flipping. This targeted bit-flip operation is followed by an inference phase, where GBFA evaluates the loss function and the network s test accuracy after perturbation. The resulting ac- curacy measurements are recorded to generate a test accuracy profile, which reflects the progressive degradation of inference performance. It gradually selects weights based on the BER and evaluates the degradation of inference accuracy to ensure a significant decline while maintaining minimal BER. TABLE II GRAPH DATASET CHARACTERISTICS Dataset Nodes Edges Feature Dimentions Categories Cora 2708 5429 1433 7 PubMed 19717 44338 500 3 TABLE III ACCURACY AND MODEL SIZE OF GNN MODELS Dataset GNN Models Baseline Test Accuracy Total Trained Weights Cora GCN 0.810 93984 GAT 0.780 96661 GraphSAGE 0.798 23040 GIN 0.640 108070 PubMed GCN 0.783 72384 GAT 0.774 194304 GraphSAGE 0.769 32192 GIN 0.613 48360 TABLE IV PREDICTION ERROR RATE ON GNN MODELS Model Error Rate GCN-Cora 0.032 GCN-PubMed 0.050 GAT-Cora 0.020 GAT-PubMed 0.023 GraphSAGE-Cora 0.060 GraphSAGE-PubMed 0.062 GIN-Cora 0.043 GIN-PubMed 0.045 Fig. 4. Weight distribution of GCN trained on the Cora and PubMed datasets IV. RESULTS AND DISCUSSION A. Experimental Setup Datasets. We evaluate the effectiveness of the GBFA attack on node classification tasks using two benchmark datasets, as summarized in Table II. These datasets are chosen for their diversity in size and structural complexity: Cora is a small- scale dataset, whereas PubMed is a large one. Additionally, Cora has high-dimensional sparse features, while PubMed is denser. Moreover, since node classification is a semi- supervised learning task, the label rate is 0.052 for the Cora dataset, and 0.003 for the PubMed dataset, meaning that only about 5.2 of Cora, and 0.3 of PubMed data is labeled for training. GNN Models. We evaluate the effectiveness of the GBFA attack on four widely used GNN models: GCN, GAT, Graph- SAGE, and GIN models. We employed PyTorch [23] and the PyTorch Geometric (PyG) [5] libraries to train GNNs and evalauate GBFA attack. In Table III, we summarize the test accuracy of the fault-free models and provide the number of trained weights in each layer of the GNN models. Moreover, Figure 4 illustrates the weight distribution of the GCN model on the Cora and PubMed datasets, which is representative of the weight range observed in other GNN models. The Cora dataset results in a more constrained weight range compared to PubMed. Evaluation Metrics: We evaluate the performance of the GBFA attack using three metrics: post-attack test accuracy (PAC), the number of flipped bits (nbit), and the Attack Success Rate (ASR). Post-attack test accuracy refers to the percentage of test data correctly classified by the GNN model after the attack. nbit represents the number of bits flipped by TABLE V EVALUATION OF GBFA PERFORMANCE ON GNN MODELS USING CORA AND PUBMED DATASETS AT BER 1E-1 GNN Model Dataset Layer 1 Layer 2 Layer 3 PAC ASR nbit PAC ASR nbit PAC ASR nbit GCN Cora 68 23 917 24 70 204 21 73 22 GCN PubMed 41 63 6400 41 53 819 40 42 19 GAT Cora 37 57 9190 40 58 428 22 76 46 GAT PubMed 48 46 12800 47 48 6 63 27 7 TABLE VI EVALUATION OF GBFA ATTACK ON GRAPHSAGE MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GraphSAGE-Cora 1 76 1e 3 22 35 62 22928 2 66 1e 2 1 13 88 22 GraphSAGE-PubMed 1 75 1e 3 32 42 56 640 2 71 1e 2 1 40 58 38 TABLE VII EVALUATION OF GBFA ATTACK ON GIN MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GIN-Cora 1 58 1e 2 917 12 85 9171 2 55 1e 2 40 13 83 409 3 33 1e 2 40 13 85 409 4 52 1e 2 40 15 83 409 5 60 1e 1 40 60 5 409 GIN-PubMed 1 60 1e 2 320 57 22 3200 2 59 1e 2 40 56 22 409 3 60 1e 2 40 56 19 409 4 60 1e 2 40 58 12 409 5 60 1e 2 40 58 8 409 Fig. 5. Average post-attack test accuracy of the GCN and GAT models on the Cora and PubMed datasets under the GBFA attack at the minimum BER that results in a performance drop. the attacker to reduce the model s accuracy at the defined BER. ASR quantifies the proportion of test samples whose predicted labels are altered as a result of an induced bit-flip fault. B. Layer Sequence Prediction Performance In this section, we assess the accuracy of layer sequence prediction using the layer prediction error rate (LER) defined in Equation 5 [10]. LER is computed as the mean normalized edit distance between the predicted and actual sequence. LER ED(L, L ) L (5) ED(L, L ) represents the edit distance between the pre- dicted layer sequence L and the ground-truth layer sequence L . It is defined as the minimum number of insertions, deletions, and substitutions needed to transform L to L . The term L denotes the length of the ground-truth layer sequence. Table IV represents the LER of GNN models. C. Evaluating the effectiveness of GBFA on various GNN models Figure 5 illustrates the average post-attack test accuracy of GCN and GAT models on Cora and PubMed datasets under the GBFA at minimum BER that result in a performance drop. The results indicate that the BER value varies across models and layers. However, deeper layers in both models are more vulnerable to GBFA attacks. For example, GBFA can achieve 20 accuracy drop by injecting faults in layer three with minimum BER. The summary of evaluation of GBFA at BER of 1e-1 is presented in table V. The results indicate that deeper layers are more susceptible to the attack. Moreover, at a BER of 1e-1, GBFA achieves an average ASR of 75 on GCN and GAT using the Cora dataset by focusing on layer three. Table VI presents the effectiveness of the GBFA attack on the GraphSAGE model using the Cora and PubMed datasets. For both datasets, flipping only one vulnerable bit in layer 2 is sufficient to degrade the accuracy. Moreover, GBFA achieves an ASR of 88 on the Cora dataset and 58 on the PubMed dataset by targeting layer 2 at a BER of 1e-1. Table VII presents a comprehensive evaluation of the GBFA attack on the GIN model across individual layers for the Cora and PubMed datasets, under both the minimum BER and BER of 1e-1. For GIN-Cora, the attack demonstrates varying degrees of performance accuracy collapse (PAC) depending on the targeted layer. Notably, Layer 3 exhibits the most significant drop in performance at Min BER, with PAC falling to 33 , while Layer 5 maintains a relatively high PAC of 60 at a higher BER of 1e-1, accompanied by a notably low attack success rate (ASR) of only 5 . This suggests layer-specific resilience differences. In contrast, for GIN-PubMed, the PAC values remain stable at Min BER across all layers, while under a BER of 1e-1, PAC declines moderately (56 58 ) with ASR values ranging from 22 in the earlier layers to 8 in the final layer. These results indicate that the GBFA s impact is highly dependent on both the targeted layer and dataset, with deeper layers in GIN-Cora being more vulnerable and consistent PAC- ASR trade-offs observed in GIN-PubMed. GBFA Effectiveness Across GNN models: The susceptibility of GNN models to the GBFA attack varies significantly based on their architectural design and depth. We observe that model-specific factors such as layer composition, aggregation mechanisms, and parameter count influence the severity of accuracy degradation under bit-flip faults. Among the evaluated architectures, GIN and GraphSAGE consistently exhibit higher vulnerability. In particular, GIN suffers sharp accuracy drops even at low BER values. This heightened sensitivity stems from GIN s reliance on precise weight updates to maintain injective aggregation, making it particularly fragile to targeted bit-level perturbations. GIN models also exhibited layer-dependent vulnerability. The findings demonstrate that middle layers were especially susceptible. The last layer showed reduced responsiveness to bit flips, suggesting that earlier transformations carry more influence over final predictions in GIN. GCN and GAT models demonstrate more moderate vulner- ability in compare to GIN and GraphSAGE. Moreover, both models are vulnerable on the deeper layer. GAT, despite its attention-based mechanism, does not exhibit stronger resis- tance to GBFA than GCN. GBFA Effectiveness Across Datasets: As Cora and PubMed datasets differ in size, feature sparsity, and graph topology, the results show that GBFA consistently induces more severe degradation on the Cora dataset compared to PubMed, even under the same BER and model configurations. On Cora, a sparse dataset, GBFA is able to significantly reduce model accuracy by flipping a minimal number of bits. Conversely, the PubMed dataset, which features a denser graph and a lower label rate, appears more resilient. Weight distribution analysis further supports this observation. As illustrated in Figure 4, models trained on PubMed exhibit broader weight distributions, suggesting higher tolerance to minor perturbations. In contrast, weights on Cora are more tightly clustered, potentially amplifying the effect of small bit- level changes. TABLE VIII POST-ATTACK TEST ACCURACY OF RANDOM BIT-FLIP FAULT ATTACK AND GBFA ON GNNS USING CORA AND PUBMED DATASETS AT THE MINIMUM BER Model Random GBFA Min Bit-Flip Attack BER GAT-Cora 74 51 1e-4 GCN-Cora 81 80 1e-3 GCN-PubMed 78 77 1e-4 GraphSAGE-PubMed 76 75 1e-3 GIN-Cora 64 63 1e-2 GIN-PubMed 61 60 1e-2 D. Comparison with other methods In this section, we compare our proposed GBFA attack with random bit flip attack and recent existing work of attack. In the random bit-flip attack, the attacker randomly selects weights in the network without considering layer awareness, using the minimum BER that causes a drop in accuracy. Table VIII presents the post-attack test accuracy for both the random bit-flip attack and GBFA at the minimum BER. The results demonstrate that GBFA consistently achieves slightly lower accuracy than random bit-flip attacks, suggesting its effectiveness in targeting model vulnerabilities. Overall, GBFA slightly outperforms random attacks in degrading accuracy, with the largest drop observed for GAT-Cora (51 vs. 74 ). IBFA introduced in [16] is another attack on GIN model. It introduces a theoretically motivated bit-flip attack specifically tailored to degrade the expressivity of GIN by targeting the injectivity of aggregation function, our proposed GBFA attack focuses on flipping trained weight parameters. Unlike IBFA, which assumes full white-box access and leverages properties of the Weisfeiler-Leman (1-WL) test to disrupt graph isomorphism distinctions, GBFA operates under a gray- box assumption, using runtime memory access patterns to infer layer execution and identify vulnerable bits via gradient- guided in-layer search. IBFA is applied only GIN model while our proposed GBFA attack can be applied on different GNN models. On the other hand, as they applied different datasets it is impossible to compare GBFA attack on GIN and IBFA on GIN. Time complexity analysis of the GBFA attack. For each iteration of GBFA to identify single most vulnerable bit in a selected weight in the targeted layer, the time complexity is O(N), where N is the number of bit with highest gradient ranking that will be detected in GBFA method. In general, the time complexity of the proposed GBFA is linear for each iteration. V. CONCLUSION In this paper, we present GBFA, a hardware-based fault injection attack targeting GNNs deployed on the hardware accelerator for node classification tasks. The attack degrades prediction accuracy by selectively flipping bits in the model s stored weights. GBFA leverages memory traces and a Markov model to predict the execution order of layers. In the identified target layer, it flips a vulnerable bit within a selected weight using a gradient descent-based ranking method. Weights are gradually selected based on the BER, ensuring that model accuracy degrades with minimal bit perturbation. We evaluate the effectiveness of GBFA on four GNN architectures: GCN, GAT, GraphSAGE, and GIN, using the Cora and PubMed datasets. Our results demonstrate that GBFA can successfully compromise all the GNN models. Furthermore, comparing the GBFA results with those of a method that injects bit-flip faults without targeting a specific layer highlights the significance of in-layer bit-flip fault injection attacks in compromising model accuracy. Our research highlights the importance of addressing hardware-based attacks and underscores the need for dedicated defense mechanisms to ensure the reliable deployment of GNN models in safety-critical and real-world applications. REFERENCES [1] CHEN, C., LI, K., LI, Y., AND ZOU, X. Regnn: A redundancy- eliminated graph neural networks accelerator. In 2022 IEEE Interna- tional Symposium on High-Performance Computer Architecture (HPCA) (2022), IEEE, pp. 429 443. [2] CHEN, Z., KOLHE, G., RAFATIRAD, S., LU, C.-T., DINAKARRAO, S. M. P., HOMAYOUN, H., AND ZHAO, L. Estimating the circuit de- obfuscation runtime based on graph deep learning. In ACM EDAA IEEE Design Automation and Test in Europe (DATE) (2020). [3] CHEN, Z., ZHANG, L., KOLHE, G., KAMALI, H. M., RAFATIRAD, S., PUDUKOTAI DINAKARRAO, S. M., HOMAYOUN, H., LU, C.-T., AND ZHAO, L. Deep graph learning for circuit deobfuscation. Frontiers in big Data 4 (2021), 608286. [4] ETEMADYRAD, N., GAO, Y., MANOJ PUDUKOTAI DINAKARRAO, S., AND ZHAO, L. Global explanation supervision for graph neural networks. Frontiers in big Data 7 (2024), 1410424. [5] FEY, M., AND LENSSEN, J. E. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428 (2019). [6] GOODFELLOW, I. J., SHLENS, J., AND SZEGEDY, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014). [7] GUBBI, K. I., SABER LATIBARI, B., SRIKANTH, A., SHEAVES, T., BEHESHTI-SHIRAZI, S. A., PD, S. M., RAFATIRAD, S., SASAN, A., HOMAYOUN, H., AND SALEHI, S. Hardware trojan detection using ma- chine learning: A tutorial. ACM Transactions on Embedded Computing Systems 22, 3 (2023), 1 26. [8] HASSAN, R., KOHLE, G., HOMAYOUN, H., AND DINAKARRAO, S. M. P. A neural network-based cognitive obfuscation towards enhanced logic locking. IEEE Transactions on Computer-Aided Design (TCAD) 41, 11 (2022), 4587 4599. [9] HONG, S., FRIGO, P., KAYA, Y., GIUFFRIDA, C., AND DUMITRAS, , T. Terminal brain damage: Exposing the graceless degradation in deep neural networks under hardware fault attacks. In 28th USENIX Security Symposium (USENIX Security 19) (2019), pp. 497 514. [10] HU, X., LIANG, L., LI, S., DENG, L., ZUO, P., JI, Y., XIE, X., DING, Y., LIU, C., SHERWOOD, T., ET AL. Deepsniffer: A dnn model extrac- tion framework based on learning architectural hints. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (2020), pp. 385 399. [11] HU, Y., LIU, Y., AND LIU, Z. A survey on convolutional neural network accelerators: Gpu, fpga and asic. In 2022 14th International Conference on Computer Research and Development (ICCRD) (2022), IEEE, pp. 100 107. [12] JIN, W., LI, Y., XU, H., WANG, Y., JI, S., AGGARWAL, C., AND TANG, J. Adversarial attacks and defenses on graphs. ACM SIGKDD Explorations Newsletter 22, 2 (2021), 19 34. [13] KIM, Y., DALY, R., KIM, J., FALLIN, C., LEE, J. H., LEE, D., WILKERSON, C., LAI, K., AND MUTLU, O. Flipping bits in memory without accessing them: An experimental study of dram disturbance errors. ACM SIGARCH Computer Architecture News 42, 3 (2014), 361 372. [14] KIPF, T. N., AND WELLING, M. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [15] KOSE, H. T., NUNEZ-YANEZ, J., PIECHOCKI, R., AND POPE, J. A survey of computationally efficient graph neural networks for reconfig- urable systems. Information 15, 7 (2024), 377. [16] KUMMER, L., MOUSTAFA, S., SCHRITTWIESER, S., GANSTERER, W., AND KRIEGE, N. Attacking graph neural networks with bit flips: Weisfeiler and leman go indifferent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2024), pp. 1428 1439. [17] LI, G., HARI, S. K. S., SULLIVAN, M., TSAI, T., PATTABIRAMAN, K., EMER, J., AND KECKLER, S. W. Understanding error propagation in deep learning neural network (dnn) accelerators and applications. In Proceedings of the international conference for high performance computing, networking, storage and analysis (2017), pp. 1 12. [18] LIU, M., GAO, H., AND JI, S. Towards deeper graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery data mining (2020), pp. 338 348. [19] LIU, Y., WEI, L., LUO, B., AND XU, Q. Fault injection attack on deep neural network. In 2017 IEEE ACM International Conference on Computer-Aided Design (ICCAD) (2017), IEEE, pp. 131 138. [20] MALEKZADEH, E., ROHBANI, N., LU, Z., AND EBRAHIMI, M. The impact of faults on dnns: A case study. In 2021 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT) (2021), IEEE, pp. 1 6. [21] MITTAL, S. A survey on modeling and improving reliability of dnn algorithms and accelerators. Journal of Systems Architecture 104 (2020), 101689. [22] MITTAL, S., GUPTA, H., AND SRIVASTAVA, S. A survey on hardware security of dnn models and accelerators. Journal of Systems Architecture 117 (2021), 102163. [23] PASZKE, A., GROSS, S., MASSA, F., LERER, A., BRADBURY, J., CHANAN, G., KILLEEN, T., LIN, Z., GIMELSHEIN, N., ANTIGA, L., ET AL. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). [24] RAFATIRAD, S., HOMAYOUN, H., CHEN, Z., AND PUDUKOTAI DI- NAKARRAO, S. M. Graph learning. In Machine Learning for Computer Scientists and Data Analysts: From an Applied Perspective. Springer, 2022, pp. 277 304. [25] RAKIN, A. S., HE, Z., AND FAN, D. Bit-flip attack: Crushing neural network with progressive bit search. In Proceedings of the IEEE CVF International Conference on Computer Vision (2019), pp. 1211 1220. [26] SARAVANAN, R., KASARAPU, S., AND DINAKARRAO, S. M. P. Graph- fuzz: Accelerating hardware testing with graph models. arXiv preprint arXiv:2412.13374 (2024). [27] SHUKLA, S., RAFATIRAD, S., HOMAYOUN, H., AND DINAKARRAO, S. M. P. Federated learning with heterogeneous models for on-device malware detection in iot networks. In Design Automation and Test in Europe (DATE) (2023). [28] TIAN, T., ZHAO, L., WANG, X., WU, Q., YUAN, W., AND JIN, X. Fp-gnn: Adaptive fpga accelerator for graph neural networks. Future Generation Computer Systems 136 (2022), 294 310. [29] WANG, R., LIN, F., MOORE, D., SANKAR, S., AND JIAO, X. Pygfi: Analyzing and enhancing robustness of graph neural networks against hardware errors. arXiv preprint arXiv:2212.03475 (2022). [30] WU, S., SUN, F., ZHANG, W., XIE, X., AND CUI, B. Graph neural networks in recommender systems: a survey. ACM Computing Surveys 55, 5 (2022), 1 37. [31] XIE, X., PENG, H., HASAN, A., HUANG, S., ZHAO, J., FANG, H., ZHANG, W., GENG, T., KHAN, O., AND DING, C. Accel-gcn: High- performance gpu accelerator design for graph convolution networks. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD) (2023), IEEE, pp. 01 09. [32] YAN, M., DENG, L., HU, X., LIANG, L., FENG, Y., YE, X., ZHANG, Z., FAN, D., AND XIE, Y. Hygcn: A gcn accelerator with hybrid archi- tecture. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) (2020), IEEE, pp. 15 29. [33] ZHANG, B., ZENG, H., AND PRASANNA, V. Low-latency mini-batch gnn inference on cpu-fpga heterogeneous platform. In 2022 IEEE 29th International Conference on High Performance Computing, Data, and Analytics (HiPC) (2022), IEEE, pp. 11 21. [34] ZHANG, S., SOHRABIZADEH, A., WAN, C., HUANG, Z., HU, Z., WANG, Y., CONG, J., SUN, Y., ET AL. A survey on graph neural network acceleration: Algorithms, systems, and customized hardware. arXiv preprint arXiv:2306.14052 (2023).\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nBit-Flip Fault Attack: Crushing Graph Neural Networks via Gradual Bit Search Sanaz Kazemi Abharian Department of Electrical and Computer Engineering George Mason University Fairfax, VA, USA Sai Manoj Pudukotai Dinakarrao Department of Electrical and Computer Engineering George Mason University Fairfax, VA, USA Abstract Graph Neural Networks (GNNs) have emerged as a powerful machine learning method for graph-structured data. A plethora of hardware accelerators has been introduced to meet the performance demands of GNNs in real-world applications. However, security challenge of hardware-based attacks have been generally overlooked. In this paper, we investigate the vulnera- bility of GNN models to hardware-based fault attack, wherein an attacker attempts to misclassify output by modifying trained weight parameters through fault injection in a memory device. Thus, we propose Gradual Bit-Flip Fault Attack (GBFA), a layer- aware bit-flip fault attack, select a vulnerable bit in each selected weights gradually to compromise the GNN s performance by flipping minimal number of bits. To achieve this, GBFA operates in two steps. First, a Markov model is created to predict the execution sequence of layers based on features extracted from memory access patterns, enabling the launch of the attack within a specific layer. Subsequently, GBFA identifies vulnerable bits within the selected weights using gradient ranking through an in- layer search. We evaluate the effectiveness of the proposed GBFA attack on various GNN models for node classification tasks using the Cora and PubMed datasets. Our findings show that GBFA significantly degrades prediction accuracy, and the variation in its impact across different layers highlights the importance of adopting a layer-aware attack strategy in GNNs. For example, GBFA degrades GraphSAGE s prediction accuracy by 17 on the Cora dataset with only a single bit flip in the last layer. Index Terms Graph Neural Networks, Bit flip, Fault, Attack, Hardware Accelerators, Security I. INTRODUCTION Graph Neural Networks (GNNs) have emerged as a practical deep learning approach for graph-structured data, which is use- ful in various real-world applications such as social networks, electronic design automation (EDA) for accelerating hardware verification [26], [7], circuit security tasks [3], [8], [2], and recommendation systems [30].\n\n--- Segment 2 ---\nIndex Terms Graph Neural Networks, Bit flip, Fault, Attack, Hardware Accelerators, Security I. INTRODUCTION Graph Neural Networks (GNNs) have emerged as a practical deep learning approach for graph-structured data, which is use- ful in various real-world applications such as social networks, electronic design automation (EDA) for accelerating hardware verification [26], [7], circuit security tasks [3], [8], [2], and recommendation systems [30]. To address the limitations of software-based GNN implementations, hardware-based GNN accelerators are introduced in the recent years [15]. These GNN hardware accelerators have shown higher energy effi- ciency, lower latency, and improved performance compared to their software counterparts [33]. GNNs involve specialized computations characterized by irregular and sparse graph structures, necessitating efficient hardware accelerators such as GPUs [31], FPGAs [28], and ASICs [11] to handle data movement, parallelize sparse com- putations, and minimize latency. Among these, custom ASIC accelerators offer significant advantages due to their tailored designs, which closely align with the irregular computational patterns of GNN workloads. Specifically, ASICs achieve lower latency, higher throughput, and substantially reduced energy consumption by optimizing data flow and integrating tightly coupled on-chip memory and processing logic. Unlike FPGAs, ASICs do not incur overhead from reconfigurable logic and routing, resulting in better performance density and improved power efficiency. Furthermore, ASIC accelerators often sup- port 32-bit floating-point (FP32) arithmetic to preserve nu- merical precision during both training and inference phases. This design choice is reflected in several recent architectures, including HyGCN [32] and ReGNN [1], which adopt FP32 arithmetic to ensure stable and accurate computations. Despite their superior performance and other benefits, recent works have shown that hardware accelerators are vulnerable to hardware attacks [22], among which bit-flip attacks (BFAs) are considered one of the prominent attacks as they can crush a neural network through maliciously flipping a small number of bits within its weight parameters stored in memory. There are various methods, such as the Laser Beam attack and Row Hammer Attack (RHA), that perform bit-flip operations. RHA, introduced by Kim et al.\n\n--- Segment 3 ---\nThere are various methods, such as the Laser Beam attack and Row Hammer Attack (RHA), that perform bit-flip operations. RHA, introduced by Kim et al. [13], exploits a vulnerability in dy- namic random access memory (DRAM), allowing an attacker to manipulate data without direct access. This is achieved by repeatedly hammering specific rows of memory cells, which induces bit flips in adjacent rows. While the The impact of BFAs has been widely studied on Deep Neural Network (DNN) accelerators [21], their effect on GNNs remains largely unexplored, and the security risks posed by BFAs in GNN models have been overlooked. Software-based adversarial attacks on GNNs such as Eva- sion attacks and poisoning attacks [12] can be detected and mitigated using the existing techniques such as differential privacy and federated learning [17], [27]. However, the non- volatile nature of BFAs makes them hard to detect and analyze. In this work, we introduce a two-pronged approach for launching our proposed BFA on the GNN hardware accelera- tor. First, we determine the GNN layer that is being executed by the hardware based on recorded memory patterns and use a Markov model to predict the layer sequence. Once the executing layer is identified, the proposed Gradual Bit-Flip arXiv:2507.05531v1 [cs.LG] 7 Jul 2025 Fault Attack (GBFA) on GNN models is introduced, which injects faults into trained weights using a layer-aware method that can cause a GNN to misclassify with a minimal number of bit flips. Our proposed GBFA attack gradually selects weights through an in-layer search based on the Bit Error Rate (BER) to ensure a decrease in prediction accuracy with a minimal number of bit flips. We evaluate the effectiveness of the proposed GBFA Attack across multiple GNN architectures using two widely adopted benchmark datasets: Cora and PubMed [14]. Our experimental results demonstrate that performing a targeted in-layer search is critical for maximizing the impact of GBFA while min- imizing the number of bit flips. Additionally, our analysis reveals that the degree of accuracy degradation induced by GBFA varies across different GNN models, depending on the specific layer targeted. These findings highlight the critical impact of layer-wise vulnerability in enhancing the success of bit-flip fault attacks on GNNs.\n\n--- Segment 4 ---\nAdditionally, our analysis reveals that the degree of accuracy degradation induced by GBFA varies across different GNN models, depending on the specific layer targeted. These findings highlight the critical impact of layer-wise vulnerability in enhancing the success of bit-flip fault attacks on GNNs. We observe that the attack consistently leads to a notable degradation in model accuracy across different GNN architectures. These results collectively validate the broad applicability and efficacy of GBFA as a stealthy and impactful fault injection method against GNNs. The novel contributions of this work is summarized as follows: We propose a hardware-based bit-flip fault attack on GNN models to degrade prediction accuracy on node classification task. We demonstrate that in-layer searches are essential for an effective bit-flip attack on GNNs. To identify and target a specific layer, our attack analyzes memory patterns and employs a Markov model, incorporating a Connectionist Temporal Classification (CTC) decoder to predict the run- time layer sequence. To achieve the desired accuracy degradation with a min- imal number of bit flips, our proposed GBFA attack integrates gradient-based ranking and a gradual search within a specific layer to identify a vulnerable bit in each selected weight parameter. The bit-flip operations are then performed along the gradient s ascending direction. Fig. 1. Computational procedure of a single GCNConv layer II. BACKGROUND AND RELATED WORKS A. Graph Neural Networks GNNs are a class of neural networks designed to operate on graph-structured data [24]. Graph Convolutional Networks (GCNs), constructed using GCNConv layer, operate in two fundamental phases feature aggregation and linear feature transformation as shown in Figure 1. The forward pass in the l- th GCNConv layer is functioned into two stages: 1) a feature aggregation step followed by 2) feature transformation step. Each node aggregates feature information from its neighbors through an aggregation function, capturing local structural dependencies. The resulting aggregated feature vectors are then passed through a combination function, typically param- eterized by shared trainable weights to produce updated node representations [4]. Variants of GCNs, such as GraphSAGE, Graph Isomor- phism Network (GIN), Graph Attention Network (GAT), retain the general message-passing framework but differ in their neighborhood aggregation functions, while still conforming to the fundamental propagation scheme of standard GCNs. B.\n\n--- Segment 5 ---\nVariants of GCNs, such as GraphSAGE, Graph Isomor- phism Network (GIN), Graph Attention Network (GAT), retain the general message-passing framework but differ in their neighborhood aggregation functions, while still conforming to the fundamental propagation scheme of standard GCNs. B. Related Works Recent research demonstrates the vulnerability of Deep neural network (DNN) parameters stored in DRAM to bit-flip fault attacks. Liu et al. [19] proposed a Single Bias Attack (SBA), which targets a specific bias term of a neuron to cause misclassification in a DNN. The results demonstrate that even modifying a single bias can result in misclassification. Hong et al. [9] investigate the impact of bit-flip fault attacks on the weights of various DNN architectures with full precision, leveraging the Row Hammer method. Their study examines how bit position, flip direction, parameter sign, and layer width influence the effects of such attacks. In [25], Rakin et al. introduced the progressive bit search (PBS) method, which leverages the Rowhammer technique to target the quantized weight parameters of DNNs. PBS ranks and progressively searches for the most vulnerable bits, flipping them in a way that maximizes accuracy degradation. The effectiveness of this attack is investigated on ResNet-18 using the ImageNet dataset, causing a significant drop in accuracy. In [16], the injective bit flip attack (IBFA) was introduced on a quantized GNN model. The IBFA targets bit flipping in the injective properties of the neighborhood aggregation function in message passing layers to reduce their ability to distinguish non-isomorphic graphs and to impair the expressivity of the Weisfeiler-Leman test. Limitation of previous works. Prior research on BFAs has primarily focused on DNNs, with limited attention to their im- pact on GNNs. The unique computational structure of GNNs warrants further investigation to assess their vulnerability to BFAs. Additionally, while studies [20] [29] demonstrate that bit-flip faults at different layers affect the output differently, most fault attacks overlook layer-specific effects. To address this, our proposed GBFA attack focuses on in-layer search. Fig. 2. The overview of the Hardware accelerator III.\n\n--- Segment 6 ---\n2. The overview of the Hardware accelerator III. PROPOSED HARDWARE-BASED BIT-FLIP FAULT ATTACK ON ML ACCELERATORS The proposed GBFA is a gray-box fault-injection attack in which the adversary has physical access to the hardware accelerator executing the target GNN model. Furthermore, the adversary has no knowledge of the GNN model, training dataset, or hyperparameters. However, it is assumed that the adversary has access to the gradients and the test dataset. For each trained weight subjected to the fault attack, only a single bit can be affected. Additionally, it is assumed that the training process and its parameters are fault-free prior to the attack. A. Design of GNN Accelerator As aforementioned, a plethora of hardware accelerators for GNNs are proposed in the literature [34]. In this work, we consider Reconfigurable GNN accelerator [1] as our base design and adapt it to the proposed GNN models and datasets. The GNN accelerator shown in Figure 2 is a configurable, pipelined hardware accelerator, designed to efficiently exe- cute GNNs by eliminating computational and communication redundancies. The architecture contains three primary pro- cessing modules: Re-coordinator, Re-aggregator, and Flexible Updaters containing processing elements (PE), which are coordinated by a central controller and interconnected via a dy- namic switch network. The Re-coordinator is responsible for scheduling tasks based on a redundancy-aware graph structure, identifying opportunities to reuse intermediate computations. It consists of a structure prefetcher, vertex prefetcher, and a dispatcher that dynamically allocate EdgeUpdate and Aggre- gation workloads while minimizing redundant data movement. The Re-aggregator performs feature aggregation for both target vertices and redundant neighbor sets using a parallel aggrega- tion engine supported by a neighbor shuffler and task buffer; it caches intermediate aggregation results to reduce compu- tation and communication redundancy. The Flexible Updaters implement matrix-vector multiplications for both EdgeUpdate and VertexUpdate using systolic arrays. In GNNs without EdgeUpdate, these arrays can be reconfigured and combined to maximize utilization.\n\n--- Segment 7 ---\nThe Flexible Updaters implement matrix-vector multiplications for both EdgeUpdate and VertexUpdate using systolic arrays. In GNNs without EdgeUpdate, these arrays can be reconfigured and combined to maximize utilization. A central controller interprets a three- bit configuration word, indicating the presence of EdgeUpdate, EdgeUpdate Redundancy (ER), and Aggregation Redundancy (AR), to reconfigure micro-configurable switches (SW0-SW3) and dynamically adapt the execution pipeline to various GNN variants. To support high-throughput and energy-efficient execution, ReGNN incorporates a hierarchical memory system compris- ing both on-chip and off-chip components. Off-chip memory is implemented using high-bandwidth memory (HBM) with a bandwidth of 256 GB s to store vertex features, edge informa- tion, and weights. The on-chip memory includes specialized buffers for EdgeUpdate, Aggregation, weights, and structural metadata, as well as a configurable vertex cache. This cache stores original vertex features when redundancy elimination is not applied, and caches intermediate EdgeUpdate and Aggre- gation results when ER and AR are present, respectively. To improve cache efficiency, ReGNN employs a priority- based management strategy, giving preference to high-degree vertices and larger redundant sets. Additionally, double buffer- ing is used across memory modules to hide data transfer latency and overlap communication with computation. This memory architecture ensures minimal DRAM access, max- imized data reuse, and improved pipeline utilization, con- tributing significantly to ReGNN s performance and energy efficiency. B. Deploying GBFA on Hardware Accelerator Figure 3 illustrates the overall workflow of the proposed GBFA attack, which comprises two primary stages: identifying the target layer and injecting bit-flip faults into the selected weight parameters via an in-layer search within the targeted layer. 1) Run-time Layer Sequence Prediction: Input layer, mes- sage passing layer, transformation layer, readout layer, and output layer are some of the common layers in GNN models [18]. Each layer of a GNN model exhibits a distinct memory access pattern and computational cycles per kernel at runtime. These characteristics make the model vulnerable to hardware snooping attacks, enabling an attacker to observe execution patterns. By leveraging this information, the adversary can predict and infer individual layers without access to the neural network s parameters.\n\n--- Segment 8 ---\nThese characteristics make the model vulnerable to hardware snooping attacks, enabling an attacker to observe execution patterns. By leveraging this information, the adversary can predict and infer individual layers without access to the neural network s parameters. Record Memory Patterns: An attacker can access GDDR bus to extract memory patterns, either by physically probing the interconnect or utilizing a DMA-capable device to achieve kernel events. The GDDR bus can reveal kernel events and memory copy size (Memcp), which can be further leveraged to infer execution latency (Exelat). Similarly, monitoring the device memory bus allows the extraction of memory request traces, from which key memory-related parameters, such as the number of read and write operations (Rv, Wv) and raw data dependencies dRAW can be inferred. This architectural information leakage is used to form feature inputs for the HMM model. Fig. 3. The overview of the proposed GBFA Attack Architectural features of kernels, such as kernel execution time (Exelat), kernel read volume (Rv), write volume through the memory bus (Wv), Input output data volume ratio of each kernel (Iv Ov), where the output volume (Ov) is measured by the write volume of this kernel, and input volume (Iv) is measured by the write volume of the previously executed ker- nel, kernel dependency distance (kdd) represents the topology influence. kdd is defined as the maximum distance between a given kernel and its preceding dependent kernels during the execution of a kernel sequence. This metric encodes layer structure information within the kernel features which can be computed as kdd max(dRAW). These features are transformed into kernel features by extracting tracking memory patterns. A Layer Sequence Predictor: Once the memory access patterns are recorded, we introduce a layer sequence predictor for predicting the GNN s layer. To build a model to predict layers, two primary information sources are considered: ar- chitectural kernel execution features and the distribution of layer context within the sequence. Thus, the runtime-layer sequence prediction problem can be formalized as a sequence- to-sequence model prediction which is described as follows: The input is kernel execution feature sequence Xt with temporal length of T described as an array: (Exelat, Rv, Wv, Iv Wv, kdd)t obtained from the memory access pattern observation.\n\n--- Segment 9 ---\nTo build a model to predict layers, two primary information sources are considered: ar- chitectural kernel execution features and the distribution of layer context within the sequence. Thus, the runtime-layer sequence prediction problem can be formalized as a sequence- to-sequence model prediction which is described as follows: The input is kernel execution feature sequence Xt with temporal length of T described as an array: (Exelat, Rv, Wv, Iv Wv, kdd)t obtained from the memory access pattern observation. As the proposed GNN layer pre- diction involves analyzing temporal memory access patterns, we devise a Hidden Markov Model (HMM) incorporating a Connectionist Temporal Classification (CTC) decoder. The Markov Model provides a probabilistic method where each state represents a specific layer, and transitions between states capture the probability of execution order based on the ob- served hardware characteristics i.e., memory access patterns. Thus, we formulate the object function of HMM layer se- quence identifier is to minimize the CTC cost for a given target layer sequence L which is computed as: cost(X) log P(L h(X)) (1) The P(L h(X)) represents the probability of observing result L given the input X. As a case in point, there is a sequence consisting three execution kernels. At each time step (t0, t1, andt2), the HMM produces a probability distribution over the layer operations and CTC decoder employs beam search to determine the sequence with the highest probability. 2) Layer-aware Fault Attack Injection: GBFA detects a vulnerable bit in a weight using a method similar to FGSM [6]. It utilizes the gradient ascent direction w.r.t. the loss function to rank bits. Thus, it calculates the gradient of a weight b, represented in binary format, according to Equation 2: bL L b31 , . . . , L b0 (2) Where L represents the inference loss function of GNN pa- rameterized by b. To flip the bit, the straightforward approach involves directly flipping bits based on the gradients derived in Equation 3, resulting in the perturbed bits: ˆb b sign( bL) (3) However, this may lead incorrect result because of overflow. GBFA decides to flip the bit based on Table I in order to increase the loss function.\n\n--- Segment 10 ---\nTo flip the bit, the straightforward approach involves directly flipping bits based on the gradients derived in Equation 3, resulting in the perturbed bits: ˆb b sign( bL) (3) However, this may lead incorrect result because of overflow. GBFA decides to flip the bit based on Table I in order to increase the loss function. I is an indicator that specifies whether the bit-flip operation should be performed. Thus, this is formulated in Equation 4. ˆbi sign L bi (bi I) (4) TABLE I GBFA TRUTH TABLE DEPICTING WHETHER THE VALUE OF bi ALTERS OR NOT bi sgn( L bi) ˆbi I 0 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 Algorithm 1 Layer-aware Fault Attack Injection BitFlip (GNN) 1: Target a Layer 2: Set BER 3: for i in range BER do 4: Select weights randomly in the target layer 5: end for 6: while (num weights! BER) do 7: Compute L w [ L bn ... L b0 ] 8: if (bi 0 sgn( L w ) 0) (bi 1 sgn( L w ) 0) then 9: Flip the bit 10: else 11: Ignore the weight 12: end if 13: end while 14: Conduct a performance evaluation of the GNN model with modified weights. 15: if attack is successful then 16: End procedure 17: else 18: set another BER or target another layer 19: end if After determining the executing layer of the GNN in the previous step, GBFA identifies a vulnerable bit in the selected weight through an in-layer search, as described in Algorithm 1. In each iteration, the gradient of bits in the selected weight is computed and ranked. Then, the bit with the highest gradient and the corresponding I value, according to Table I, is selected for flipping. This targeted bit-flip operation is followed by an inference phase, where GBFA evaluates the loss function and the network s test accuracy after perturbation. The resulting ac- curacy measurements are recorded to generate a test accuracy profile, which reflects the progressive degradation of inference performance. It gradually selects weights based on the BER and evaluates the degradation of inference accuracy to ensure a significant decline while maintaining minimal BER.\n\n--- Segment 11 ---\nThe resulting ac- curacy measurements are recorded to generate a test accuracy profile, which reflects the progressive degradation of inference performance. It gradually selects weights based on the BER and evaluates the degradation of inference accuracy to ensure a significant decline while maintaining minimal BER. TABLE II GRAPH DATASET CHARACTERISTICS Dataset Nodes Edges Feature Dimentions Categories Cora 2708 5429 1433 7 PubMed 19717 44338 500 3 TABLE III ACCURACY AND MODEL SIZE OF GNN MODELS Dataset GNN Models Baseline Test Accuracy Total Trained Weights Cora GCN 0.810 93984 GAT 0.780 96661 GraphSAGE 0.798 23040 GIN 0.640 108070 PubMed GCN 0.783 72384 GAT 0.774 194304 GraphSAGE 0.769 32192 GIN 0.613 48360 TABLE IV PREDICTION ERROR RATE ON GNN MODELS Model Error Rate GCN-Cora 0.032 GCN-PubMed 0.050 GAT-Cora 0.020 GAT-PubMed 0.023 GraphSAGE-Cora 0.060 GraphSAGE-PubMed 0.062 GIN-Cora 0.043 GIN-PubMed 0.045 Fig. 4. Weight distribution of GCN trained on the Cora and PubMed datasets IV. RESULTS AND DISCUSSION A. Experimental Setup Datasets. We evaluate the effectiveness of the GBFA attack on node classification tasks using two benchmark datasets, as summarized in Table II. These datasets are chosen for their diversity in size and structural complexity: Cora is a small- scale dataset, whereas PubMed is a large one. Additionally, Cora has high-dimensional sparse features, while PubMed is denser. Moreover, since node classification is a semi- supervised learning task, the label rate is 0.052 for the Cora dataset, and 0.003 for the PubMed dataset, meaning that only about 5.2 of Cora, and 0.3 of PubMed data is labeled for training. GNN Models. We evaluate the effectiveness of the GBFA attack on four widely used GNN models: GCN, GAT, Graph- SAGE, and GIN models.\n\n--- Segment 12 ---\nGNN Models. We evaluate the effectiveness of the GBFA attack on four widely used GNN models: GCN, GAT, Graph- SAGE, and GIN models. We employed PyTorch [23] and the PyTorch Geometric (PyG) [5] libraries to train GNNs and evalauate GBFA attack. In Table III, we summarize the test accuracy of the fault-free models and provide the number of trained weights in each layer of the GNN models. Moreover, Figure 4 illustrates the weight distribution of the GCN model on the Cora and PubMed datasets, which is representative of the weight range observed in other GNN models. The Cora dataset results in a more constrained weight range compared to PubMed. Evaluation Metrics: We evaluate the performance of the GBFA attack using three metrics: post-attack test accuracy (PAC), the number of flipped bits (nbit), and the Attack Success Rate (ASR). Post-attack test accuracy refers to the percentage of test data correctly classified by the GNN model after the attack.\n\n--- Segment 13 ---\nEvaluation Metrics: We evaluate the performance of the GBFA attack using three metrics: post-attack test accuracy (PAC), the number of flipped bits (nbit), and the Attack Success Rate (ASR). Post-attack test accuracy refers to the percentage of test data correctly classified by the GNN model after the attack. nbit represents the number of bits flipped by TABLE V EVALUATION OF GBFA PERFORMANCE ON GNN MODELS USING CORA AND PUBMED DATASETS AT BER 1E-1 GNN Model Dataset Layer 1 Layer 2 Layer 3 PAC ASR nbit PAC ASR nbit PAC ASR nbit GCN Cora 68 23 917 24 70 204 21 73 22 GCN PubMed 41 63 6400 41 53 819 40 42 19 GAT Cora 37 57 9190 40 58 428 22 76 46 GAT PubMed 48 46 12800 47 48 6 63 27 7 TABLE VI EVALUATION OF GBFA ATTACK ON GRAPHSAGE MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GraphSAGE-Cora 1 76 1e 3 22 35 62 22928 2 66 1e 2 1 13 88 22 GraphSAGE-PubMed 1 75 1e 3 32 42 56 640 2 71 1e 2 1 40 58 38 TABLE VII EVALUATION OF GBFA ATTACK ON GIN MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GIN-Cora 1 58 1e 2 917 12 85 9171 2 55 1e 2 40 13 83 409 3 33 1e 2 40 13 85 409 4 52 1e 2 40 15 83 409 5 60 1e 1 40 60 5 409 GIN-PubMed 1 60 1e 2 320 57 22 3200 2 59 1e 2 40 56 22 409 3 60 1e 2 40 56 19 409 4 60 1e 2 40 58 12 409 5 60 1e 2 40 58 8 409 Fig.\n\n--- Segment 14 ---\nPost-attack test accuracy refers to the percentage of test data correctly classified by the GNN model after the attack. nbit represents the number of bits flipped by TABLE V EVALUATION OF GBFA PERFORMANCE ON GNN MODELS USING CORA AND PUBMED DATASETS AT BER 1E-1 GNN Model Dataset Layer 1 Layer 2 Layer 3 PAC ASR nbit PAC ASR nbit PAC ASR nbit GCN Cora 68 23 917 24 70 204 21 73 22 GCN PubMed 41 63 6400 41 53 819 40 42 19 GAT Cora 37 57 9190 40 58 428 22 76 46 GAT PubMed 48 46 12800 47 48 6 63 27 7 TABLE VI EVALUATION OF GBFA ATTACK ON GRAPHSAGE MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GraphSAGE-Cora 1 76 1e 3 22 35 62 22928 2 66 1e 2 1 13 88 22 GraphSAGE-PubMed 1 75 1e 3 32 42 56 640 2 71 1e 2 1 40 58 38 TABLE VII EVALUATION OF GBFA ATTACK ON GIN MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GIN-Cora 1 58 1e 2 917 12 85 9171 2 55 1e 2 40 13 83 409 3 33 1e 2 40 13 85 409 4 52 1e 2 40 15 83 409 5 60 1e 1 40 60 5 409 GIN-PubMed 1 60 1e 2 320 57 22 3200 2 59 1e 2 40 56 22 409 3 60 1e 2 40 56 19 409 4 60 1e 2 40 58 12 409 5 60 1e 2 40 58 8 409 Fig. 5.\n\n--- Segment 15 ---\nnbit represents the number of bits flipped by TABLE V EVALUATION OF GBFA PERFORMANCE ON GNN MODELS USING CORA AND PUBMED DATASETS AT BER 1E-1 GNN Model Dataset Layer 1 Layer 2 Layer 3 PAC ASR nbit PAC ASR nbit PAC ASR nbit GCN Cora 68 23 917 24 70 204 21 73 22 GCN PubMed 41 63 6400 41 53 819 40 42 19 GAT Cora 37 57 9190 40 58 428 22 76 46 GAT PubMed 48 46 12800 47 48 6 63 27 7 TABLE VI EVALUATION OF GBFA ATTACK ON GRAPHSAGE MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GraphSAGE-Cora 1 76 1e 3 22 35 62 22928 2 66 1e 2 1 13 88 22 GraphSAGE-PubMed 1 75 1e 3 32 42 56 640 2 71 1e 2 1 40 58 38 TABLE VII EVALUATION OF GBFA ATTACK ON GIN MODEL AT THE LOWEST BER AND AT BER OF 1E-1 GNN Model- Dataset Layer PAC (Min BER) Min BER nbit (Min BER) PAC (1e-1) ASR (1e-1) nbit (1e-1) GIN-Cora 1 58 1e 2 917 12 85 9171 2 55 1e 2 40 13 83 409 3 33 1e 2 40 13 85 409 4 52 1e 2 40 15 83 409 5 60 1e 1 40 60 5 409 GIN-PubMed 1 60 1e 2 320 57 22 3200 2 59 1e 2 40 56 22 409 3 60 1e 2 40 56 19 409 4 60 1e 2 40 58 12 409 5 60 1e 2 40 58 8 409 Fig. 5. Average post-attack test accuracy of the GCN and GAT models on the Cora and PubMed datasets under the GBFA attack at the minimum BER that results in a performance drop. the attacker to reduce the model s accuracy at the defined BER.\n\n--- Segment 16 ---\nAverage post-attack test accuracy of the GCN and GAT models on the Cora and PubMed datasets under the GBFA attack at the minimum BER that results in a performance drop. the attacker to reduce the model s accuracy at the defined BER. ASR quantifies the proportion of test samples whose predicted labels are altered as a result of an induced bit-flip fault. B. Layer Sequence Prediction Performance In this section, we assess the accuracy of layer sequence prediction using the layer prediction error rate (LER) defined in Equation 5 [10]. LER is computed as the mean normalized edit distance between the predicted and actual sequence. LER ED(L, L ) L (5) ED(L, L ) represents the edit distance between the pre- dicted layer sequence L and the ground-truth layer sequence L . It is defined as the minimum number of insertions, deletions, and substitutions needed to transform L to L . The term L denotes the length of the ground-truth layer sequence. Table IV represents the LER of GNN models. C. Evaluating the effectiveness of GBFA on various GNN models Figure 5 illustrates the average post-attack test accuracy of GCN and GAT models on Cora and PubMed datasets under the GBFA at minimum BER that result in a performance drop. The results indicate that the BER value varies across models and layers. However, deeper layers in both models are more vulnerable to GBFA attacks. For example, GBFA can achieve 20 accuracy drop by injecting faults in layer three with minimum BER. The summary of evaluation of GBFA at BER of 1e-1 is presented in table V. The results indicate that deeper layers are more susceptible to the attack. Moreover, at a BER of 1e-1, GBFA achieves an average ASR of 75 on GCN and GAT using the Cora dataset by focusing on layer three. Table VI presents the effectiveness of the GBFA attack on the GraphSAGE model using the Cora and PubMed datasets. For both datasets, flipping only one vulnerable bit in layer 2 is sufficient to degrade the accuracy. Moreover, GBFA achieves an ASR of 88 on the Cora dataset and 58 on the PubMed dataset by targeting layer 2 at a BER of 1e-1.\n\n--- Segment 17 ---\nFor both datasets, flipping only one vulnerable bit in layer 2 is sufficient to degrade the accuracy. Moreover, GBFA achieves an ASR of 88 on the Cora dataset and 58 on the PubMed dataset by targeting layer 2 at a BER of 1e-1. Table VII presents a comprehensive evaluation of the GBFA attack on the GIN model across individual layers for the Cora and PubMed datasets, under both the minimum BER and BER of 1e-1. For GIN-Cora, the attack demonstrates varying degrees of performance accuracy collapse (PAC) depending on the targeted layer. Notably, Layer 3 exhibits the most significant drop in performance at Min BER, with PAC falling to 33 , while Layer 5 maintains a relatively high PAC of 60 at a higher BER of 1e-1, accompanied by a notably low attack success rate (ASR) of only 5 . This suggests layer-specific resilience differences. In contrast, for GIN-PubMed, the PAC values remain stable at Min BER across all layers, while under a BER of 1e-1, PAC declines moderately (56 58 ) with ASR values ranging from 22 in the earlier layers to 8 in the final layer. These results indicate that the GBFA s impact is highly dependent on both the targeted layer and dataset, with deeper layers in GIN-Cora being more vulnerable and consistent PAC- ASR trade-offs observed in GIN-PubMed. GBFA Effectiveness Across GNN models: The susceptibility of GNN models to the GBFA attack varies significantly based on their architectural design and depth. We observe that model-specific factors such as layer composition, aggregation mechanisms, and parameter count influence the severity of accuracy degradation under bit-flip faults. Among the evaluated architectures, GIN and GraphSAGE consistently exhibit higher vulnerability. In particular, GIN suffers sharp accuracy drops even at low BER values. This heightened sensitivity stems from GIN s reliance on precise weight updates to maintain injective aggregation, making it particularly fragile to targeted bit-level perturbations. GIN models also exhibited layer-dependent vulnerability. The findings demonstrate that middle layers were especially susceptible. The last layer showed reduced responsiveness to bit flips, suggesting that earlier transformations carry more influence over final predictions in GIN. GCN and GAT models demonstrate more moderate vulner- ability in compare to GIN and GraphSAGE. Moreover, both models are vulnerable on the deeper layer.\n\n--- Segment 18 ---\nGCN and GAT models demonstrate more moderate vulner- ability in compare to GIN and GraphSAGE. Moreover, both models are vulnerable on the deeper layer. GAT, despite its attention-based mechanism, does not exhibit stronger resis- tance to GBFA than GCN. GBFA Effectiveness Across Datasets: As Cora and PubMed datasets differ in size, feature sparsity, and graph topology, the results show that GBFA consistently induces more severe degradation on the Cora dataset compared to PubMed, even under the same BER and model configurations. On Cora, a sparse dataset, GBFA is able to significantly reduce model accuracy by flipping a minimal number of bits. Conversely, the PubMed dataset, which features a denser graph and a lower label rate, appears more resilient. Weight distribution analysis further supports this observation. As illustrated in Figure 4, models trained on PubMed exhibit broader weight distributions, suggesting higher tolerance to minor perturbations. In contrast, weights on Cora are more tightly clustered, potentially amplifying the effect of small bit- level changes. TABLE VIII POST-ATTACK TEST ACCURACY OF RANDOM BIT-FLIP FAULT ATTACK AND GBFA ON GNNS USING CORA AND PUBMED DATASETS AT THE MINIMUM BER Model Random GBFA Min Bit-Flip Attack BER GAT-Cora 74 51 1e-4 GCN-Cora 81 80 1e-3 GCN-PubMed 78 77 1e-4 GraphSAGE-PubMed 76 75 1e-3 GIN-Cora 64 63 1e-2 GIN-PubMed 61 60 1e-2 D. Comparison with other methods In this section, we compare our proposed GBFA attack with random bit flip attack and recent existing work of attack. In the random bit-flip attack, the attacker randomly selects weights in the network without considering layer awareness, using the minimum BER that causes a drop in accuracy. Table VIII presents the post-attack test accuracy for both the random bit-flip attack and GBFA at the minimum BER. The results demonstrate that GBFA consistently achieves slightly lower accuracy than random bit-flip attacks, suggesting its effectiveness in targeting model vulnerabilities. Overall, GBFA slightly outperforms random attacks in degrading accuracy, with the largest drop observed for GAT-Cora (51 vs. 74 ).\n\n--- Segment 19 ---\nThe results demonstrate that GBFA consistently achieves slightly lower accuracy than random bit-flip attacks, suggesting its effectiveness in targeting model vulnerabilities. Overall, GBFA slightly outperforms random attacks in degrading accuracy, with the largest drop observed for GAT-Cora (51 vs. 74 ). IBFA introduced in [16] is another attack on GIN model. It introduces a theoretically motivated bit-flip attack specifically tailored to degrade the expressivity of GIN by targeting the injectivity of aggregation function, our proposed GBFA attack focuses on flipping trained weight parameters. Unlike IBFA, which assumes full white-box access and leverages properties of the Weisfeiler-Leman (1-WL) test to disrupt graph isomorphism distinctions, GBFA operates under a gray- box assumption, using runtime memory access patterns to infer layer execution and identify vulnerable bits via gradient- guided in-layer search. IBFA is applied only GIN model while our proposed GBFA attack can be applied on different GNN models. On the other hand, as they applied different datasets it is impossible to compare GBFA attack on GIN and IBFA on GIN. Time complexity analysis of the GBFA attack. For each iteration of GBFA to identify single most vulnerable bit in a selected weight in the targeted layer, the time complexity is O(N), where N is the number of bit with highest gradient ranking that will be detected in GBFA method. In general, the time complexity of the proposed GBFA is linear for each iteration. V. CONCLUSION In this paper, we present GBFA, a hardware-based fault injection attack targeting GNNs deployed on the hardware accelerator for node classification tasks. The attack degrades prediction accuracy by selectively flipping bits in the model s stored weights. GBFA leverages memory traces and a Markov model to predict the execution order of layers. In the identified target layer, it flips a vulnerable bit within a selected weight using a gradient descent-based ranking method. Weights are gradually selected based on the BER, ensuring that model accuracy degrades with minimal bit perturbation. We evaluate the effectiveness of GBFA on four GNN architectures: GCN, GAT, GraphSAGE, and GIN, using the Cora and PubMed datasets. Our results demonstrate that GBFA can successfully compromise all the GNN models.\n\n--- Segment 20 ---\nWe evaluate the effectiveness of GBFA on four GNN architectures: GCN, GAT, GraphSAGE, and GIN, using the Cora and PubMed datasets. Our results demonstrate that GBFA can successfully compromise all the GNN models. Furthermore, comparing the GBFA results with those of a method that injects bit-flip faults without targeting a specific layer highlights the significance of in-layer bit-flip fault injection attacks in compromising model accuracy. Our research highlights the importance of addressing hardware-based attacks and underscores the need for dedicated defense mechanisms to ensure the reliable deployment of GNN models in safety-critical and real-world applications. REFERENCES [1] CHEN, C., LI, K., LI, Y., AND ZOU, X. Regnn: A redundancy- eliminated graph neural networks accelerator. In 2022 IEEE Interna- tional Symposium on High-Performance Computer Architecture (HPCA) (2022), IEEE, pp. 429 443. [2] CHEN, Z., KOLHE, G., RAFATIRAD, S., LU, C.-T., DINAKARRAO, S. M. P., HOMAYOUN, H., AND ZHAO, L. Estimating the circuit de- obfuscation runtime based on graph deep learning. In ACM EDAA IEEE Design Automation and Test in Europe (DATE) (2020). [3] CHEN, Z., ZHANG, L., KOLHE, G., KAMALI, H. M., RAFATIRAD, S., PUDUKOTAI DINAKARRAO, S. M., HOMAYOUN, H., LU, C.-T., AND ZHAO, L. Deep graph learning for circuit deobfuscation. Frontiers in big Data 4 (2021), 608286. [4] ETEMADYRAD, N., GAO, Y., MANOJ PUDUKOTAI DINAKARRAO, S., AND ZHAO, L. Global explanation supervision for graph neural networks. Frontiers in big Data 7 (2024), 1410424. [5] FEY, M., AND LENSSEN, J. E. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428 (2019).\n\n--- Segment 21 ---\n[5] FEY, M., AND LENSSEN, J. E. Fast graph representation learning with pytorch geometric. arXiv preprint arXiv:1903.02428 (2019). [6] GOODFELLOW, I. J., SHLENS, J., AND SZEGEDY, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014). [7] GUBBI, K. I., SABER LATIBARI, B., SRIKANTH, A., SHEAVES, T., BEHESHTI-SHIRAZI, S. A., PD, S. M., RAFATIRAD, S., SASAN, A., HOMAYOUN, H., AND SALEHI, S. Hardware trojan detection using ma- chine learning: A tutorial. ACM Transactions on Embedded Computing Systems 22, 3 (2023), 1 26. [8] HASSAN, R., KOHLE, G., HOMAYOUN, H., AND DINAKARRAO, S. M. P. A neural network-based cognitive obfuscation towards enhanced logic locking. IEEE Transactions on Computer-Aided Design (TCAD) 41, 11 (2022), 4587 4599. [9] HONG, S., FRIGO, P., KAYA, Y., GIUFFRIDA, C., AND DUMITRAS, , T. Terminal brain damage: Exposing the graceless degradation in deep neural networks under hardware fault attacks. In 28th USENIX Security Symposium (USENIX Security 19) (2019), pp. 497 514. [10] HU, X., LIANG, L., LI, S., DENG, L., ZUO, P., JI, Y., XIE, X., DING, Y., LIU, C., SHERWOOD, T., ET AL. Deepsniffer: A dnn model extrac- tion framework based on learning architectural hints. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (2020), pp. 385 399. [11] HU, Y., LIU, Y., AND LIU, Z.\n\n--- Segment 22 ---\n385 399. [11] HU, Y., LIU, Y., AND LIU, Z. A survey on convolutional neural network accelerators: Gpu, fpga and asic. In 2022 14th International Conference on Computer Research and Development (ICCRD) (2022), IEEE, pp. 100 107. [12] JIN, W., LI, Y., XU, H., WANG, Y., JI, S., AGGARWAL, C., AND TANG, J. Adversarial attacks and defenses on graphs. ACM SIGKDD Explorations Newsletter 22, 2 (2021), 19 34. [13] KIM, Y., DALY, R., KIM, J., FALLIN, C., LEE, J. H., LEE, D., WILKERSON, C., LAI, K., AND MUTLU, O. Flipping bits in memory without accessing them: An experimental study of dram disturbance errors. ACM SIGARCH Computer Architecture News 42, 3 (2014), 361 372. [14] KIPF, T. N., AND WELLING, M. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [15] KOSE, H. T., NUNEZ-YANEZ, J., PIECHOCKI, R., AND POPE, J. A survey of computationally efficient graph neural networks for reconfig- urable systems. Information 15, 7 (2024), 377. [16] KUMMER, L., MOUSTAFA, S., SCHRITTWIESER, S., GANSTERER, W., AND KRIEGE, N. Attacking graph neural networks with bit flips: Weisfeiler and leman go indifferent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2024), pp. 1428 1439. [17] LI, G., HARI, S. K. S., SULLIVAN, M., TSAI, T., PATTABIRAMAN, K., EMER, J., AND KECKLER, S. W. Understanding error propagation in deep learning neural network (dnn) accelerators and applications.\n\n--- Segment 23 ---\n1428 1439. [17] LI, G., HARI, S. K. S., SULLIVAN, M., TSAI, T., PATTABIRAMAN, K., EMER, J., AND KECKLER, S. W. Understanding error propagation in deep learning neural network (dnn) accelerators and applications. In Proceedings of the international conference for high performance computing, networking, storage and analysis (2017), pp. 1 12. [18] LIU, M., GAO, H., AND JI, S. Towards deeper graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery data mining (2020), pp. 338 348. [19] LIU, Y., WEI, L., LUO, B., AND XU, Q. Fault injection attack on deep neural network. In 2017 IEEE ACM International Conference on Computer-Aided Design (ICCAD) (2017), IEEE, pp. 131 138. [20] MALEKZADEH, E., ROHBANI, N., LU, Z., AND EBRAHIMI, M. The impact of faults on dnns: A case study. In 2021 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT) (2021), IEEE, pp. 1 6. [21] MITTAL, S. A survey on modeling and improving reliability of dnn algorithms and accelerators. Journal of Systems Architecture 104 (2020), 101689. [22] MITTAL, S., GUPTA, H., AND SRIVASTAVA, S. A survey on hardware security of dnn models and accelerators. Journal of Systems Architecture 117 (2021), 102163. [23] PASZKE, A., GROSS, S., MASSA, F., LERER, A., BRADBURY, J., CHANAN, G., KILLEEN, T., LIN, Z., GIMELSHEIN, N., ANTIGA, L., ET AL. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019).\n\n--- Segment 24 ---\nPytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). [24] RAFATIRAD, S., HOMAYOUN, H., CHEN, Z., AND PUDUKOTAI DI- NAKARRAO, S. M. Graph learning. In Machine Learning for Computer Scientists and Data Analysts: From an Applied Perspective. Springer, 2022, pp. 277 304. [25] RAKIN, A. S., HE, Z., AND FAN, D. Bit-flip attack: Crushing neural network with progressive bit search. In Proceedings of the IEEE CVF International Conference on Computer Vision (2019), pp. 1211 1220. [26] SARAVANAN, R., KASARAPU, S., AND DINAKARRAO, S. M. P. Graph- fuzz: Accelerating hardware testing with graph models. arXiv preprint arXiv:2412.13374 (2024). [27] SHUKLA, S., RAFATIRAD, S., HOMAYOUN, H., AND DINAKARRAO, S. M. P. Federated learning with heterogeneous models for on-device malware detection in iot networks. In Design Automation and Test in Europe (DATE) (2023). [28] TIAN, T., ZHAO, L., WANG, X., WU, Q., YUAN, W., AND JIN, X. Fp-gnn: Adaptive fpga accelerator for graph neural networks. Future Generation Computer Systems 136 (2022), 294 310. [29] WANG, R., LIN, F., MOORE, D., SANKAR, S., AND JIAO, X. Pygfi: Analyzing and enhancing robustness of graph neural networks against hardware errors. arXiv preprint arXiv:2212.03475 (2022). [30] WU, S., SUN, F., ZHANG, W., XIE, X., AND CUI, B. Graph neural networks in recommender systems: a survey. ACM Computing Surveys 55, 5 (2022), 1 37.\n\n--- Segment 25 ---\n[30] WU, S., SUN, F., ZHANG, W., XIE, X., AND CUI, B. Graph neural networks in recommender systems: a survey. ACM Computing Surveys 55, 5 (2022), 1 37. [31] XIE, X., PENG, H., HASAN, A., HUANG, S., ZHAO, J., FANG, H., ZHANG, W., GENG, T., KHAN, O., AND DING, C. Accel-gcn: High- performance gpu accelerator design for graph convolution networks. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD) (2023), IEEE, pp. 01 09. [32] YAN, M., DENG, L., HU, X., LIANG, L., FENG, Y., YE, X., ZHANG, Z., FAN, D., AND XIE, Y. Hygcn: A gcn accelerator with hybrid archi- tecture. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) (2020), IEEE, pp. 15 29. [33] ZHANG, B., ZENG, H., AND PRASANNA, V. Low-latency mini-batch gnn inference on cpu-fpga heterogeneous platform. In 2022 IEEE 29th International Conference on High Performance Computing, Data, and Analytics (HiPC) (2022), IEEE, pp. 11 21. [34] ZHANG, S., SOHRABIZADEH, A., WAN, C., HUANG, Z., HU, Z., WANG, Y., CONG, J., SUN, Y., ET AL. A survey on graph neural network acceleration: Algorithms, systems, and customized hardware. arXiv preprint arXiv:2306.14052 (2023).\n\n