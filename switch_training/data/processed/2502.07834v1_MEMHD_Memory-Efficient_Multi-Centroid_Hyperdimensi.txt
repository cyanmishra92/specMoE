=== ORIGINAL PDF: 2502.07834v1_MEMHD_Memory-Efficient_Multi-Centroid_Hyperdimensi.pdf ===\n\nRaw text length: 33491 characters\nCleaned text length: 33204 characters\nNumber of segments: 18\n\n=== CLEANED TEXT ===\n\nMEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for Fully-Utilized In-Memory Computing Architectures Do Yeong Kang1, Yeong Hwan Oh1, Chanwook Hwang1, Jinhee Kim1, Kang Eun Jeon1 and Jong Hwan Ko1, 2 1Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea 2College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea Email: {ksps7106, yh991111, ghkdcks12, a2jinhee, kejeon, Abstract The implementation of Hyperdimensional Com- puting (HDC) on In-Memory Computing (IMC) architectures faces significant challenges due to the mismatch between high- dimensional vectors and IMC array sizes, leading to inefficient memory utilization and increased computation cycles. This pa- per presents MEMHD, a Memory-Efficient Multi-centroid HDC framework designed to address these challenges. MEMHD intro- duces a clustering-based initialization method and quantization- aware iterative learning for multi-centroid associative memory. Through these approaches and its overall architecture, MEMHD achieves a significant reduction in memory requirements while maintaining or improving classification accuracy. Our approach achieves full utilization of IMC arrays and enables one-shot (or few-shot) associative search. Experimental results demonstrate that MEMHD outperforms state-of-the-art binary HDC models, achieving up to 13.69 higher accuracy with the same memory usage, or 13.25x more memory efficiency at the same accuracy level. Moreover, MEMHD reduces computation cycles by up to 80x and array usage by up to 71x compared to baseline IMC mapping methods when mapped to 128x128 IMC arrays, while significantly improving energy and computation cycle efficiency. I. INTRODUCTION HDC is an emerging computational paradigm inspired by the brain s distributed processing and the properties of high- dimensional vector spaces [1]. By representing data as high- dimensional vectors, known as hypervectors, HDC enables efficient and robust computation suitable for various machine learning tasks. The high dimensionality provides inherent ro- bustness to noise and facilitates operations such as binding and superposition, which are analogous to logical operations in traditional computing. Applications of HDC span a wide range, including language processing [2], robotics [3], and biosignal classification [4]. IMC has emerged as a promising approach to overcome the von Neumann bottleneck by performing computations di- rectly within memory arrays [5]. By integrating processing capabilities into memory units, IMC architectures dramatically reduce data movement, leading to enhanced computational speed and energy efficiency for data-intensive applications. Re- cent advancements have demonstrated the potential of IMC in accelerating machine learning algorithms and neural networks, utilizing various memory technologies such as SRAM, DRAM, and non-volatile memories like RRAM and PCM [6], [7]. : Corresponding authors (c) MEMHD (a) Basic HDC IMC array (b) Partitioning High Array Usage Low Utilization Low Array Usage One-shot Computation Inference Fully-Utilization P-cycle Computation Inference Cycle MVM of Classes Basic Dimension (near 10k) Ours Dimension (near 1k) of Partitions of Columns Active columns per cycle Class A Centroids Class B Centroids Class C Centroids Fig. 1: Overview of MEMHD The synergistic integration of HDC and IMC has become a focal point of recent research, aiming to leverage their complementary strengths. Implementing HDC algorithms on IMC architectures accelerates high-dimensional computations while exploiting the parallelism and energy efficiency inherent in IMC. Various IMC-based approaches for HDC have been explored, including analog and digital processing-in-memory structures [8], PCM-based crossbar arrays [9], and scalable subarray architectures using emerging devices such as FeFETs [10]. These diverse implementations demonstrate the potential of IMC to significantly enhance HDC performance and energy efficiency across different hardware platforms. Nevertheless, mapping vectors onto IMC arrays presents significant challenges due to dimensional mismatches, because HDC utilizes high-dimensional vectors to achieve noise re- silience and effective pattern separation. This often results in underutilization of computational resources, as vector dimen- sions frequently exceed row dimensions of individual IMC arrays in Fig. 1-(a). To address this, partitioning has been proposed in [9], dividing hypervectors into smaller segments mapped across previously unused columns in fewer arrays. While this helps fit computations within hardware constraints, it doesn t reduce the total number of computation cycles required for inference, as shown in Fig. 1-(b). The root cause of these issues lies in the fundamental mismatch between HDC s requirements and IMC array structures. On the one hand, HDC relies on high-dimensional vectors that often exceed the row arXiv:2502.07834v1 [cs.AR] 11 Feb 2025 dimensions of IMC arrays. On the other hand, the traditional HDC approach of using a single vector per class results in most columns of the IMC array remaining unused during similarity calculations, causing column underutilization. To address these challenges, we need an approach tailored to the IMC array structure. Such a method would utilize vector dimensions matching the IMC array s row dimension, while employing a number of class vectors corresponding to its column dimension. Building on this insight, we move beyond the HDC s standard associative memory structure where each class is represented by a single class vector. We propose MEMHD, a novel Memory-Efficient Multi-centroid HDC framework that achieves fully-utilized associative memory while significantly reducing the dimensionality of hypervectors and maintaining classification accuracy. To the best of our knowledge, this is the first attempt to develop a multi-centroid model for HDC s associative memory specifically designed to fit IMC array sizes. To effectively train the multi-centroid associative memory, we present two key phases. First, we employ clustering-based initialization methods for initial centroid placement, enabling more stable training that leads to higher accuracy and faster convergence. Second, we utilize iterative learning techniques that account for quantization effects during the training process. MEMHD substantially reduces memory requirements for both encoding and associative search while achieving comparable or superior accuracy to existing binary HDC baselines. This is accomplished by using dimensions compatible with IMC arrays (near 1k), which are significantly smaller than the 10k dimen- sions commonly used in existing HDC approaches. Therefore, our model achieves 13.69 higher accuracy compared to the baselines with the same memory requirements, and 13.25 more efficient memory usage for the same level of accuracy. Compared to the IMC baselines, shown in Fig. 1, MEMHD reduces computation cycles by 80 and array usage by 71 . II. BACKGROUND : HDC CLASSIFICATION A. Overall HDC Structure The HDC framework is structured around two primary mod- ules: the encoding module (EM) and the associative memory (AM). During training, the EM processes the input data to extract relevant features and converts them into hypervectors. The encoded hypervectors are used to create class vectors, each representing a specific class, which are then stored in the AM for future reference. In runtime, new data is encoded into a hypervector using the same method, and an associative search is performed to compare this query hypervector against the class vectors in the AM using similarity measures. B. Hypervector Encoding Random Projection encoding involves Matrix-Vector- Multiplication (MVM) between the f D projection matrix M and the f-dimensional input feature vector F. Each column of M consists of a randomly generated base vector B, which can be represented as either binary [11] or floating-point data type [12]. The encoding is performed as follows: H M F. (1) Another approach is ID-Level encoding, where each input feature is associated with a random binary ID hypervector, and each feature value is represented by a Level hypervector. The input is encoded by element-wise multiplication of ID and Level hypervectors at each position, then summing across all positions: H Pf i 1(IDi Lxi), where IDi is the ID hypervector for the i-th position, and Lxi is the Level hypervector for the value xi. C. Associative Memory Training In HDC, it is possible to learn class vectors in a single pass. This is achieved by constructing the class vector through the summation of all sample hypervectors associated with the same class, as follows: Ck P i Hi k, where Ck is the class vector for class k, and Hi k are the sample hypervectors with label k. Iterative learning updates the AM by focusing exclusively on the hypervectors that were misclassified across the entire training dataset. For each mispredicted hypervector H, the true class vector is adjusted to be more similar to H, while the predicted class vector is adjusted to be less similar. This approach enables the model to improve its predictions over several iterations, and can be summarized as follows: Ctrue Ctrue αH, Cpred Cpred αH (2) where α is the learning rate. For the binary HDC model, the resulting C is binarized to produce a binary class hypervector. D. Associative Search For classification tasks, associative search involves selecting a predicted class based on the similarity between a query vector and class vectors. The class with the highest similarity to the query vector is inferred as the predicted class. A primary similarity metric used in this context is dot similarity: δdot(A, B) A B (3) where A and B are vectors of the same dimension. While various similarity measures such as Hamming distance and cosine similarity exist for associative search, dot similarity is a simple yet effective measure of vector similarity. It aligns well with the multiplication and accumulation operations inherent in IMC arrays. III. MEMHD: MEMORY-EFFICIENT MULTI-CENTROID HD In this section, we introduce the overall method for our multi-centroid AM. To address the time-consuming associative search in existing partitioning methods, we employ an IMC array-optimized multi-centroid AM structure. This approach enables one-shot (or few-shot) associative search and utilizes hypervectors with dimensions tailored to the IMC array size, significantly reducing encoding costs. Fig. 2 illustrates the overall framework of MEMHD. Our multi-centroid AM is constructed using the clustering-based initialization (shown in Fig. 2-(a), referred to in III-A), which enhances accuracy and convergence. For the binary AM, we perform 1-bit quantization of the AM (shown in Fig. 2-(b), referred to in III-B) and sub- sequently refine the model through quantization-aware iterative learning (shown in Fig. 2-(c), referred to in III-C), optimized (a) Multi-centroid Associative Memory Initialization Classwise Clustering Centroids Remaining Columns ! 0 Cluster Allocation Validation Confusion Matrix (b) Associative Memory Quantization (c) Quantization Aware Iterative Learning 1 2 3 2 1 4 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 2 3 - AM Quantization Normalization (d) In-memory Inference Encoding Module 0 0 1 1 1 0 1 0 0 1 1 0 1 0 2 3 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 FP32 AM Binary AM 4 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 Fig. 2: Overall MEMHD framework for efficient IMC mapping. Considering direct mapping to IMC array, we utilize the projection encoding and dot similarity for associative search, which are based on MVM (shown in Fig. 2-(d), referred to in III-D). A. Multi-Centroid AM Initialization In traditional HDC approaches, each class is represented by a single vector. The initialization method for these class vectors, including random sampling, has minimal impact on their ability to effectively represent the overall set of sample hypervectors for that class. Random sampling is commonly used for initialization due to the method s robustness to initial conditions. During the subsequent iterative learning process, all updates corresponding to a single class are applied to one class vector, allowing it to converge to a representative vector for the class with minimal dependence on its initial state. In contrast, multi-centroid models use multiple class vectors per class, each capturing distinct features of that class and learn- ing independently. This makes the initialization method critical for ensuring comprehensive class representation. Randomly- sampled initial class vectors fail to distribute the centroids evenly across point cloud of the class sample hypervectors, leading to poor representation. To address this, we propose the clustering-based initialization method that improves con- vergence and accuracy. 1) Classwise Clustering : First of all, as shown in Fig. 2-(a), MEMHD splits the encoded sample hypervectors by class and applies clustering for each class. We use K-means algorithm for clustering, which allows us to specify the number of clusters directly. The distance metric used in clustering is based on dot similarity, the same metric employed in associative search. This consistency ensures that the clustering process is optimized for subsequent associative search operations. We define a hyperparameter R as the proportion of columns used for initial clustering out of the total memory columns. This approach allows for cluster allocation, which is detailed in (2). Given R, the number of initial clusters n per class is determined by the equation: n max 1, CR k , where C is the total number of columns and k is the number of classes. After clustering, each cluster s centroid becomes an initial class vector. 2) Cluster Allocation : We allocate the remaining columns, C(1-R), based on a validation process using the entire training dataset. This process employs a confusion matrix to assess the distribution of misclassifications among different classes. Classes with higher misclassification rates are assigned addi- tional centroids to enhance their representation in the model. These additional centroids are allocated to classes with higher misprediction rates. This process of validation, cluster assign- ment, and re-clustering is repeated until no remaining columns are left. Once all columns are utilized, resulting in a fully utilized IMC array, the initialization process is complete. B. AM Quantization To optimize memory efficiency for mapping AM into IMC array, we represent the AM in binary format. The initial AM, initialized with floating-point (FP) values after clustering, ex- hibits a distribution resembling a Gaussian curve, as illustrated in Fig. 2-(b). MEMHD performs 1-bit quantization using the mean value as a threshold, binarizing the AM by setting values greater than the mean µ to 1 and the rest to 0. C. Quantization-Aware Iterative Learning The binary AM is trained through quantization-aware itera- tive learning, which optimizes the model while accounting for quantization effects. While [13] introduced quantization-aware learning for basic HDC, our multi-centroid model extends this approach with crucial modifications. Specifically, our method introduces additional considerations for setting learning targets and update strategies, tailored to the unique requirements of the multi-centroid structure. Our learning process is carried out in four steps, as illustrated in Fig. 2-(c): 1) Dot Similarity: For each training data sample hypervector, the dot product similarity with the binary AM is evaluated. Updates are triggered only when a misprediction occurs. 2) Update Target Selection: A misprediction indicates that the predicted class (with the highest similarity score) is in- correct and that its similarity score exceeds similarities with the true class vectors. The mispredicted class vector with the highest similarity score is designated as the target for update: (l , m) arg max i,j δdot(Cbi j , Hb l ) (4) where l denotes the mispredicted class, m represents the sub- label with the highest similarity, j indicates the class index, and i denotes the sub-label index. Ideally, the class vectors within the same class should represent different features, ensuring that each sample vector influences a unique class vector. Therefore, for the true class, the target for update is determined as follows: (l, n) arg max i δdot(Cbi l , Hb l ) (5) where l is the true class and n indicates the sub-label of the class vector with the highest similarity among those within the same class. 3) Iterative Learning: The iterative learning process is carried out on the FP AM based on the selected update targets: Cn l Cn l αHl, Cm l Cm l αHl. (6) The learning rate typically ranges from 0.01 to 0.1, depending on the dataset and hyperparameters D, C. The learning rate is adjusted based on dataset complexity and model size: a lower one is used for more challenging datasets, while a higher one is applied to models with large D or C. 4) Binary AM Update: Following vector operations on the FP AM, we perform a normalization step. This normalization, distinct from standard HDC approaches, ensures an even distri- bution of learning influence across multiple class vectors within the same class, preventing any single vector from dominating. The process concludes with an update to the binary AM, achieved by binarizing the normalized FP AM. D. In-Memory Inference For in-memory inference, both the binary projection matrix as EM and the binary AM are mapped into the IMC array. The inference process utilizes the MVM-based encoding and associative search methods mentioned in Eq. (3): pred arg maxi,j δdot(Cbi j , Hb). IV. EVALUATION A. Experimental Setup Table I shows the characteristics, encoding, and training methods of baseline binary HDC models. SearcHD [14] is the multi-model structure most similar to our approach, utilizing N-vector quantization, where a non-binary class vector is quantized into multiple binary class vectors. In our evaluation, TABLE I: Memory requirements of baseline HDC models Baseline Keywords Encoding Associative Binary HDC Methods Module Memory SearcHD [14] Multi-model ID-Level Single-pass (f L) D k D N QuantHD [13] ID-Level Quantization Aware Iterative-pass (f L) D k D LeHDC [15] ID-Level BNN based Training (f L) D k D BasicHDC Projection Single-pass f D k D MEMHD Multi-centroid Projection Quantization Aware Iterative-pass f D C D Note: f: of features, L: of levels, D: dimensionality, k: of classes, C: of columns and N: vector quantization factor. we fixed N 64. QuantHD [13] was the first to introduce the quantization-aware learning. LeHDC [15], based on BNN, is known as the state-of-the-art binary HDC model. All three methods use ID-level encoding with L 256. In addition to these models, we introduce BasicHDC separately because both its encoding and associative search can be performed using MVM operations, making it directly comparable to IMC array implementations. In contrast, the other baselines use ID-Level encoding, which is not directly compatible with IMC array computations. For IMC baselines, we compare MEMHD with two existing structures in Fig. 1. Our comparison focuses on (1) the required computation cycles for a single IMC array, the number of arrays needed to map the entire structure, and AM utilization, as well as (2) energy consumption and cycles of AM. Those read and write cycles and energy consumption data are derived from SRAM-based IMC arrays simulated using NeuroSim simulator [19], as presented in [20]. Our evaluation employs three datasets: MNIST [16], Fashion-MNIST (FM- NIST) [17], and ISOLET [18]. MEMHD are trained for 100 epochs following initialization. All experiments were conducted with 5 trials, and the average accuracy was reported. B. Accuracy and Memory Requirement Fig. 3 compares the accuracy and memory requirement of our model with the baselines for each dataset. Our model demonstrates accuracy close to or exceeding state-of-the-art performance while using significantly less memory. Notably, for MNIST using the 512x512, our approach achieves an ac- curacy 1.1 higher than the state-of-the-art while being 16.2 more memory-efficient. Furthermore, our mid-range models (256x256 for MNIST, FMNIST and 256x128 for ISOLET) out- perform comparable baselines such as LeHDC and QuantHD. On average, these models achieve 13.69 higher accuracy with the same memory usage, or equivalently, they are 13.25 more memory-efficient at the same accuracy level compared to the baselines. C. Accuracy Heatmap : Dimensions and Memory Columns Fig. 4 presents a heatmap of accuracy when varying the hyperparameters of our multi-centroid AM: dimensions and memory columns from 64 to 1024. This visualization allows us to optimize the AM structure according to the available 60 70 80 90 100 1 10 100 1000 Accuracy ( ) Memory (KB) 50 60 70 80 90 1 10 100 1000 Accuracy ( ) Memory (KB) 50 60 70 80 90 100 1 10 100 1000 Accuracy ( ) Memory (KB) (a) MNIST MEMHD LeHDC [15] SearcHD [14] QuantHD [13] BasicHDC (b) FMNIST (c) ISOLET 22.57 22.44 13.83 14.06 4.67 2.26 Fig. 3: Accuracy and memory requirement (KB). MEMHD used for (a) MNIST (b) FMNIST: 64x64 to 1024x1024 square sizes (DxC), (c) ISOLET: fixed 128 columns, varied dimensions. We set dimensions of baslines from 256D to 10240D. Fig. 4: MEMHD Accuracy Heatmap from 64x64 to 1024x1024. hardware resources by adjusting dimensions and columns. Generally, higher dimensions correlate with improved accuracy, which is likely related to the quality of encoding. For MNIST and FMNIST, we observe that both higher dimensions and more columns (i.e., using more class vectors) correlate with improved accuracy. However, ISOLET dataset exhibits a different pattern, showing peak performance when using 128-256 columns. This divergence in optimal structure reflects the unique characteristics of each dataset. ISOLET, with its small sample size per class (approximately 240 samples per class) and large number of classes, shows peak performance with fewer columns. Using too many columns for ISOLET could lead to overfitting, potentially causing some class vectors to represent outliers rather than general class features. Conversely, MNIST and FMNIST (approximately 6000 samples per class), being larger and more diverse, benefit from additional class vectors without such overfitting concerns. These findings highlight the importance of adapting the AM structure to both the dataset characteristics and available computational resources. D. Clustering-based Initialization Our clustering-based initialization significantly outperforms random sampling in accuracy. Fig. 5 shows that for MNIST (512x512) and ISOLET (1024x256), clustering achieves 8.69 and 19.95 higher initial accuracies respectively, leading to improved final accuracies and faster convergence (within 10- 20 epochs compared to 30-40 epochs for random sampling). 80 85 90 95 100 1 11 21 31 41 51 Accuracy ( ) Epoch 8.69 (a) MNIST 512x512 60 70 80 90 100 1 11 21 31 41 51 Accuracy ( ) Epoch (b) ISOLET 1024x256 19.95 0.8 0.3 Clustering Random Sampling Fig. 5: Accuracy comparison between clustering and random sampling. 70 75 80 85 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy ( ) R (a) FMNIST 84 85 86 87 88 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy ( ) R (b) ISOLET 512x512 512x64 Fig. 6: Accuracy for different initial cluster ratios R (0.1 to 1.0). The initial clusters ratio R influences the distribution of class vectors. As shown in Fig. 6, R has minimal effect at 512x512 but impacts performance at 512x64, with optimal values between 0.8 and 0.9. ISOLET achieves highest accuracy at R 1.0. These findings highlight the importance of proper initialization and R selection in optimizing performance across datasets and C. TABLE II: Computation cycles, arrays and AM utilization for MNIST, FMNIST and ISOLET datasets using 128x128 IMC array. Dataset (a) MNIST, FMNIST (b) ISOLET IMC Mapping Basic Partitioning MEMHD Improv. Basic Partitioning MEMHD Improv. Method P 5 P 10 P 2 P 4 AM Structure 10240x10 2048x50 1024x100 128x128 - 10240x26 5120x52 2560x104 512x128 - of Cycles EM 560 560 560 7 80 400 400 400 20 20 AM 80 80 80 1 80 80 80 80 4 20 Total 640 640 640 8 80 480 480 480 24 20 of Arrays EM 560 560 560 7 80 400 400 400 20 20 AM 80 16 8 1 8 80 40 20 4 5 Total 640 576 568 8 71 480 440 420 24 17.5 Utilization AM 7.81 39.06 78.13 100 21.87 20.31 40.63 81.25 100 18.75 E. Computation Cycle, Array Usage and AM Utilization In Fig. 4, MEMHD demonstrated comparable or higher accuracy compared to BasicHDC with 10240D: our 128x128 achieved higher accuracy for both MNIST and FMNIST, while 512x128 showed comparable results for ISOLET. So, Table II compares our models with IMC baselines with 10240D when using 128x128 IMC array. We evaluate three key metrics: com- putation cycles, array usage, and AM utilization. Computation cycles refer to the number of operations performed when using a single array, while array usage indicates the number of arrays required to map the entire AM structure. AM utilization repre- sents the ratio of actually mapped columns to the total columns in the IMC array. For partitioning, we consider two suitable scenarios for each dataset. Our model leverages lower dimen- sional vectors, which substantially reduces the EM s memory requirements. This reduction in dimensionality leads to fewer computation cycles and requires fewer IMC arrays, enhancing overall efficiency. Compared to the baseline, MEMHD achieves fully-utilization of AM at all times. As shown in Table II-(a), for MNIST and FMNIST datasets, our model demonstrates an 80 improvement in computational efficiency and requires 71 fewer arrays compared to the baseline. For ISOLET dataset, as presented in Table II-(b), our model achieves a 20 increase in computational efficiency and uses 17.5 fewer arrays. F. Energy Consumption and Cycles of AM Fig. 7 presents a comparative analysis of normalized AM energy consumption and cycles across all baseline models. While some baseline binary HDC models use ID-Level en- coding, all models employ MVM-based associative search for inference, enabling a fair comparison of AM performance. The figure showcases models demonstrating equivalent accuracy to MEMHD 128x128 on the FMNIST dataset, as shown in Fig. 3. The total number of arrays required to map the entire AM structure is proportional to the model s dimensionality. When the entire AM is mapped to arrays at once, models without partitioning can perform inference in a single cycle. However, for partitioned models, the number of cycles increases proportionally to the number of partitions. While partitioning strategies effectively reduce the number of required arrays, they proportionally increase the number of cycles, resulting in constant energy consumption across different partitioning schemes. MEMHD distinguishes itself by enabling associative search with just one computation in a single 128x128 array. This unique architecture allows MEMHD to achieve not only single-cycle inference but also significantly reduced energy 0 20 40 60 80 100 0 20 40 60 80 100 of Cycles of Arrays Normalized Energy Consumption (AM) FMNIST BasicHDC 10240x10 BasicHDC 1024x100 (P 10) SearcHD 8000x10 [14] SearcHD 800x100 (P 10) QuantHD 1600x10 [13] QuantHD 160x100 (P 10) LeHDC 400x10 [15] LeHDC 100x40 (P 4) MEMHD 128x128 1 Total Number of Arrays Energy Consumption Computation Cycles Fig. 7: Normalized energy of AM and cycles with array usage. MEMHD (128x128) achieves comparable accuracy to base- lines with higher dimensions: BasicHDC (10240D), SearcHD (8000D), QuantHD (1600D), and LeHDC (400D). consumption. Consequently, MEMHD demonstrates remark- able efficiency gains: it is 80 more efficient than Basic HDC in terms of energy consumption. Moreover, compared to LeHDC, which represents the state-of-the-art in accuracy, MEMHD is 4 more efficient. V. CONCLUSION This paper introduced MEMHD, a Memory-Efficient Multi- centroid HDC framework that optimizes HDC for IMC archi- tectures. MEMHD effectively trains multi-centroid AM through clustering-based initialization and quantization-aware iterative learning. As a result, this approach significantly outperforms binary HDC baselines, achieving up to 13.69 higher accuracy with the same memory usage or 13.25 more memory effi- ciency at the same accuracy level. By enabling full utilization of IMC arrays and one-shot (or few-shot) associative search, MEMHD reduces computation cycles by up to 80 and array usage by up to 71 compared to the partitioning method on 128 128 IMC arrays, substantially improving energy efficiency and cycles. These advancements pave the way for more efficient implementation of HDC in resource-constrained environments. ACKNOWLEDGEMENT This work was partly supported by the National Re- search Foundation of Korea (NRF) grant (No. RS-2024- 00345732); the Institute for Information communications Technology Planning Evaluation (IITP) grants (RS-2020- II201821, IITP-2021-0-02052, RS-2019-II190421, RS-2021- II212068); the Technology Innovation Program (RS-2023- 00235718, 23040-15FC) funded by the Ministry of Trade, Industry Energy (MOTIE, Korea) grant (1415187505); Samsung Electronics Co., Ltd (IO230404-05747-01); and the BK21-FOUR Project. REFERENCES [1] P. Kanerva, Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors, Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009. [2] Rahimi, Abbas, et al. A Robust and Energy-Efficient Classifier Using Brain-Inspired Hyperdimensional Computing. Proceedings of the 2016 International Symposium on Low Power Electronics and Design, 2016. [3] Neubert, Peer, et al. An Introduction to Hyperdimensional Computing for Robotics. KI-K unstliche Intelligenz, vol. 33, no. 4, 2019. [4] Rahimi, Abbas, et al. Efficient Biosignal Processing Using Hyperdi- mensional Computing: Network Templates for Combined Learning and Classification of ExG Signals. Proceedings of the IEEE, vol. 107, no. 1, pp. 123-143, 2018. [5] Sebastian, Abu, et al. Memory Devices and Applications for In-Memory Computing. Nature Nanotechnology, vol. 15, no. 7, pp. 529-544, 2020. [6] Verma, Naveen, et al. In-Memory Computing: Advances and Prospects. IEEE Solid-State Circuits Magazine, vol. 11, no. 3, pp. 43-55, 2019. [7] Shafiee, Ali, et al. ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars. ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 14-26, 2016. [8] Imani, Mohsen, et al. Exploring Hyperdimensional Associative Mem- ory. 2017 IEEE International Symposium on High Performance Com- puter Architecture (HPCA), pp. 445-456, 2017. [9] Karunaratne, Geethan, et al. In-Memory Hyperdimensional Computing. Nature Electronics, vol. 3, no. 6, pp. 327-337, 2020. [10] Kazemi, Arman, et al. Achieving Software-Equivalent Accuracy for Hyperdimensional Computing with Ferroelectric-Based In-Memory Com- puting. Scientific Reports, vol. 12, no. 1, pp. 19201, 2022. [11] Morris, Justin, et al. Locality-Based Encoder and Model Quantization for Efficient Hyper-Dimensional Computing. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 41, no. 4, pp. 897-907, 2021. [12] Thomas, Anthony, et al. A Theoretical Perspective on Hyperdimensional Computing. Journal of Artificial Intelligence Research, vol. 72, pp. 215- 249, 2021. [13] Imani, Mohsen, et al. QuantHD: A Quantization Framework for Hyper- dimensional Computing. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, no. 10, pp. 2268-2278, 2019. [14] Imani, Mohsen, et al. SearcHD: A Memory-Centric Hyperdimensional Computing with Stochastic Training. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, vol. 39, no. 10, pp. 2422-2433, 2019. [15] Duan, S., et al. LEHDC: Learning-based Hyperdimensional Computing Classifier. Proceedings of the 59th ACM IEEE Design Automation Conference, pp. 1111-1116, 2022. [16] LeCun, Y., et al. Gradient-Based Learning Applied to Document Recog- nition. Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998. [17] Xiao, H., et al. Fashion-MNIST: a Novel Image Dataset for Benchmark- ing Machine Learning Algorithms. arXiv preprint arXiv:1708.07747, 2017. [18] Cole, R., et al. The ISOLET Spoken Letter Database. Technical Report CSE 90-004, Dept. of Computer Science and Engineering, Oregon Graduate Institute of Science and Technology, Beaverton, OR, 1990. [19] Peng, Xiaochen, et al. DNN NeuroSim V2. 0: An End-to-End Bench- marking Framework for Compute-in-Memory Accelerators for On-Chip Training. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 40, no. 11, pp. 2306-2319, 2020. [20] Jeon, Kang Eun, et al. Weight-Aware Activation Mapping for Energy- Efficient Convolution on PIM Arrays. 2023 IEEE ACM International Symposium on Low Power Electronics and Design (ISLPED), IEEE, 2023.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nMEMHD: Memory-Efficient Multi-Centroid Hyperdimensional Computing for Fully-Utilized In-Memory Computing Architectures Do Yeong Kang1, Yeong Hwan Oh1, Chanwook Hwang1, Jinhee Kim1, Kang Eun Jeon1 and Jong Hwan Ko1, 2 1Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea 2College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea Email: {ksps7106, yh991111, ghkdcks12, a2jinhee, kejeon, Abstract The implementation of Hyperdimensional Com- puting (HDC) on In-Memory Computing (IMC) architectures faces significant challenges due to the mismatch between high- dimensional vectors and IMC array sizes, leading to inefficient memory utilization and increased computation cycles. This pa- per presents MEMHD, a Memory-Efficient Multi-centroid HDC framework designed to address these challenges. MEMHD intro- duces a clustering-based initialization method and quantization- aware iterative learning for multi-centroid associative memory. Through these approaches and its overall architecture, MEMHD achieves a significant reduction in memory requirements while maintaining or improving classification accuracy. Our approach achieves full utilization of IMC arrays and enables one-shot (or few-shot) associative search. Experimental results demonstrate that MEMHD outperforms state-of-the-art binary HDC models, achieving up to 13.69 higher accuracy with the same memory usage, or 13.25x more memory efficiency at the same accuracy level. Moreover, MEMHD reduces computation cycles by up to 80x and array usage by up to 71x compared to baseline IMC mapping methods when mapped to 128x128 IMC arrays, while significantly improving energy and computation cycle efficiency. I. INTRODUCTION HDC is an emerging computational paradigm inspired by the brain s distributed processing and the properties of high- dimensional vector spaces [1]. By representing data as high- dimensional vectors, known as hypervectors, HDC enables efficient and robust computation suitable for various machine learning tasks. The high dimensionality provides inherent ro- bustness to noise and facilitates operations such as binding and superposition, which are analogous to logical operations in traditional computing.\n\n--- Segment 2 ---\nBy representing data as high- dimensional vectors, known as hypervectors, HDC enables efficient and robust computation suitable for various machine learning tasks. The high dimensionality provides inherent ro- bustness to noise and facilitates operations such as binding and superposition, which are analogous to logical operations in traditional computing. Applications of HDC span a wide range, including language processing [2], robotics [3], and biosignal classification [4]. IMC has emerged as a promising approach to overcome the von Neumann bottleneck by performing computations di- rectly within memory arrays [5]. By integrating processing capabilities into memory units, IMC architectures dramatically reduce data movement, leading to enhanced computational speed and energy efficiency for data-intensive applications. Re- cent advancements have demonstrated the potential of IMC in accelerating machine learning algorithms and neural networks, utilizing various memory technologies such as SRAM, DRAM, and non-volatile memories like RRAM and PCM [6], [7]. : Corresponding authors (c) MEMHD (a) Basic HDC IMC array (b) Partitioning High Array Usage Low Utilization Low Array Usage One-shot Computation Inference Fully-Utilization P-cycle Computation Inference Cycle MVM of Classes Basic Dimension (near 10k) Ours Dimension (near 1k) of Partitions of Columns Active columns per cycle Class A Centroids Class B Centroids Class C Centroids Fig. 1: Overview of MEMHD The synergistic integration of HDC and IMC has become a focal point of recent research, aiming to leverage their complementary strengths. Implementing HDC algorithms on IMC architectures accelerates high-dimensional computations while exploiting the parallelism and energy efficiency inherent in IMC. Various IMC-based approaches for HDC have been explored, including analog and digital processing-in-memory structures [8], PCM-based crossbar arrays [9], and scalable subarray architectures using emerging devices such as FeFETs [10]. These diverse implementations demonstrate the potential of IMC to significantly enhance HDC performance and energy efficiency across different hardware platforms. Nevertheless, mapping vectors onto IMC arrays presents significant challenges due to dimensional mismatches, because HDC utilizes high-dimensional vectors to achieve noise re- silience and effective pattern separation. This often results in underutilization of computational resources, as vector dimen- sions frequently exceed row dimensions of individual IMC arrays in Fig.\n\n--- Segment 3 ---\nNevertheless, mapping vectors onto IMC arrays presents significant challenges due to dimensional mismatches, because HDC utilizes high-dimensional vectors to achieve noise re- silience and effective pattern separation. This often results in underutilization of computational resources, as vector dimen- sions frequently exceed row dimensions of individual IMC arrays in Fig. 1-(a). To address this, partitioning has been proposed in [9], dividing hypervectors into smaller segments mapped across previously unused columns in fewer arrays. While this helps fit computations within hardware constraints, it doesn t reduce the total number of computation cycles required for inference, as shown in Fig. 1-(b). The root cause of these issues lies in the fundamental mismatch between HDC s requirements and IMC array structures. On the one hand, HDC relies on high-dimensional vectors that often exceed the row arXiv:2502.07834v1 [cs.AR] 11 Feb 2025 dimensions of IMC arrays. On the other hand, the traditional HDC approach of using a single vector per class results in most columns of the IMC array remaining unused during similarity calculations, causing column underutilization. To address these challenges, we need an approach tailored to the IMC array structure. Such a method would utilize vector dimensions matching the IMC array s row dimension, while employing a number of class vectors corresponding to its column dimension. Building on this insight, we move beyond the HDC s standard associative memory structure where each class is represented by a single class vector. We propose MEMHD, a novel Memory-Efficient Multi-centroid HDC framework that achieves fully-utilized associative memory while significantly reducing the dimensionality of hypervectors and maintaining classification accuracy. To the best of our knowledge, this is the first attempt to develop a multi-centroid model for HDC s associative memory specifically designed to fit IMC array sizes. To effectively train the multi-centroid associative memory, we present two key phases. First, we employ clustering-based initialization methods for initial centroid placement, enabling more stable training that leads to higher accuracy and faster convergence. Second, we utilize iterative learning techniques that account for quantization effects during the training process. MEMHD substantially reduces memory requirements for both encoding and associative search while achieving comparable or superior accuracy to existing binary HDC baselines.\n\n--- Segment 4 ---\nSecond, we utilize iterative learning techniques that account for quantization effects during the training process. MEMHD substantially reduces memory requirements for both encoding and associative search while achieving comparable or superior accuracy to existing binary HDC baselines. This is accomplished by using dimensions compatible with IMC arrays (near 1k), which are significantly smaller than the 10k dimen- sions commonly used in existing HDC approaches. Therefore, our model achieves 13.69 higher accuracy compared to the baselines with the same memory requirements, and 13.25 more efficient memory usage for the same level of accuracy. Compared to the IMC baselines, shown in Fig. 1, MEMHD reduces computation cycles by 80 and array usage by 71 . II. BACKGROUND : HDC CLASSIFICATION A. Overall HDC Structure The HDC framework is structured around two primary mod- ules: the encoding module (EM) and the associative memory (AM). During training, the EM processes the input data to extract relevant features and converts them into hypervectors. The encoded hypervectors are used to create class vectors, each representing a specific class, which are then stored in the AM for future reference. In runtime, new data is encoded into a hypervector using the same method, and an associative search is performed to compare this query hypervector against the class vectors in the AM using similarity measures. B. Hypervector Encoding Random Projection encoding involves Matrix-Vector- Multiplication (MVM) between the f D projection matrix M and the f-dimensional input feature vector F. Each column of M consists of a randomly generated base vector B, which can be represented as either binary [11] or floating-point data type [12]. The encoding is performed as follows: H M F. (1) Another approach is ID-Level encoding, where each input feature is associated with a random binary ID hypervector, and each feature value is represented by a Level hypervector. The input is encoded by element-wise multiplication of ID and Level hypervectors at each position, then summing across all positions: H Pf i 1(IDi Lxi), where IDi is the ID hypervector for the i-th position, and Lxi is the Level hypervector for the value xi. C. Associative Memory Training In HDC, it is possible to learn class vectors in a single pass.\n\n--- Segment 5 ---\nThe input is encoded by element-wise multiplication of ID and Level hypervectors at each position, then summing across all positions: H Pf i 1(IDi Lxi), where IDi is the ID hypervector for the i-th position, and Lxi is the Level hypervector for the value xi. C. Associative Memory Training In HDC, it is possible to learn class vectors in a single pass. This is achieved by constructing the class vector through the summation of all sample hypervectors associated with the same class, as follows: Ck P i Hi k, where Ck is the class vector for class k, and Hi k are the sample hypervectors with label k. Iterative learning updates the AM by focusing exclusively on the hypervectors that were misclassified across the entire training dataset. For each mispredicted hypervector H, the true class vector is adjusted to be more similar to H, while the predicted class vector is adjusted to be less similar. This approach enables the model to improve its predictions over several iterations, and can be summarized as follows: Ctrue Ctrue αH, Cpred Cpred αH (2) where α is the learning rate. For the binary HDC model, the resulting C is binarized to produce a binary class hypervector. D. Associative Search For classification tasks, associative search involves selecting a predicted class based on the similarity between a query vector and class vectors. The class with the highest similarity to the query vector is inferred as the predicted class. A primary similarity metric used in this context is dot similarity: δdot(A, B) A B (3) where A and B are vectors of the same dimension. While various similarity measures such as Hamming distance and cosine similarity exist for associative search, dot similarity is a simple yet effective measure of vector similarity. It aligns well with the multiplication and accumulation operations inherent in IMC arrays. III. MEMHD: MEMORY-EFFICIENT MULTI-CENTROID HD In this section, we introduce the overall method for our multi-centroid AM. To address the time-consuming associative search in existing partitioning methods, we employ an IMC array-optimized multi-centroid AM structure. This approach enables one-shot (or few-shot) associative search and utilizes hypervectors with dimensions tailored to the IMC array size, significantly reducing encoding costs.\n\n--- Segment 6 ---\nTo address the time-consuming associative search in existing partitioning methods, we employ an IMC array-optimized multi-centroid AM structure. This approach enables one-shot (or few-shot) associative search and utilizes hypervectors with dimensions tailored to the IMC array size, significantly reducing encoding costs. Fig. 2 illustrates the overall framework of MEMHD. Our multi-centroid AM is constructed using the clustering-based initialization (shown in Fig. 2-(a), referred to in III-A), which enhances accuracy and convergence. For the binary AM, we perform 1-bit quantization of the AM (shown in Fig. 2-(b), referred to in III-B) and sub- sequently refine the model through quantization-aware iterative learning (shown in Fig. 2-(c), referred to in III-C), optimized (a) Multi-centroid Associative Memory Initialization Classwise Clustering Centroids Remaining Columns ! 0 Cluster Allocation Validation Confusion Matrix (b) Associative Memory Quantization (c) Quantization Aware Iterative Learning 1 2 3 2 1 4 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 1 2 3 - AM Quantization Normalization (d) In-memory Inference Encoding Module 0 0 1 1 1 0 1 0 0 1 1 0 1 0 2 3 1 1 0 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 0 FP32 AM Binary AM 4 1 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0 0 Fig. 2: Overall MEMHD framework for efficient IMC mapping. Considering direct mapping to IMC array, we utilize the projection encoding and dot similarity for associative search, which are based on MVM (shown in Fig. 2-(d), referred to in III-D). A. Multi-Centroid AM Initialization In traditional HDC approaches, each class is represented by a single vector. The initialization method for these class vectors, including random sampling, has minimal impact on their ability to effectively represent the overall set of sample hypervectors for that class.\n\n--- Segment 7 ---\nA. Multi-Centroid AM Initialization In traditional HDC approaches, each class is represented by a single vector. The initialization method for these class vectors, including random sampling, has minimal impact on their ability to effectively represent the overall set of sample hypervectors for that class. Random sampling is commonly used for initialization due to the method s robustness to initial conditions. During the subsequent iterative learning process, all updates corresponding to a single class are applied to one class vector, allowing it to converge to a representative vector for the class with minimal dependence on its initial state. In contrast, multi-centroid models use multiple class vectors per class, each capturing distinct features of that class and learn- ing independently. This makes the initialization method critical for ensuring comprehensive class representation. Randomly- sampled initial class vectors fail to distribute the centroids evenly across point cloud of the class sample hypervectors, leading to poor representation. To address this, we propose the clustering-based initialization method that improves con- vergence and accuracy. 1) Classwise Clustering : First of all, as shown in Fig. 2-(a), MEMHD splits the encoded sample hypervectors by class and applies clustering for each class. We use K-means algorithm for clustering, which allows us to specify the number of clusters directly. The distance metric used in clustering is based on dot similarity, the same metric employed in associative search. This consistency ensures that the clustering process is optimized for subsequent associative search operations. We define a hyperparameter R as the proportion of columns used for initial clustering out of the total memory columns. This approach allows for cluster allocation, which is detailed in (2). Given R, the number of initial clusters n per class is determined by the equation: n max 1, CR k , where C is the total number of columns and k is the number of classes. After clustering, each cluster s centroid becomes an initial class vector. 2) Cluster Allocation : We allocate the remaining columns, C(1-R), based on a validation process using the entire training dataset. This process employs a confusion matrix to assess the distribution of misclassifications among different classes. Classes with higher misclassification rates are assigned addi- tional centroids to enhance their representation in the model. These additional centroids are allocated to classes with higher misprediction rates.\n\n--- Segment 8 ---\nClasses with higher misclassification rates are assigned addi- tional centroids to enhance their representation in the model. These additional centroids are allocated to classes with higher misprediction rates. This process of validation, cluster assign- ment, and re-clustering is repeated until no remaining columns are left. Once all columns are utilized, resulting in a fully utilized IMC array, the initialization process is complete. B. AM Quantization To optimize memory efficiency for mapping AM into IMC array, we represent the AM in binary format. The initial AM, initialized with floating-point (FP) values after clustering, ex- hibits a distribution resembling a Gaussian curve, as illustrated in Fig. 2-(b). MEMHD performs 1-bit quantization using the mean value as a threshold, binarizing the AM by setting values greater than the mean µ to 1 and the rest to 0. C. Quantization-Aware Iterative Learning The binary AM is trained through quantization-aware itera- tive learning, which optimizes the model while accounting for quantization effects. While [13] introduced quantization-aware learning for basic HDC, our multi-centroid model extends this approach with crucial modifications. Specifically, our method introduces additional considerations for setting learning targets and update strategies, tailored to the unique requirements of the multi-centroid structure. Our learning process is carried out in four steps, as illustrated in Fig. 2-(c): 1) Dot Similarity: For each training data sample hypervector, the dot product similarity with the binary AM is evaluated. Updates are triggered only when a misprediction occurs. 2) Update Target Selection: A misprediction indicates that the predicted class (with the highest similarity score) is in- correct and that its similarity score exceeds similarities with the true class vectors. The mispredicted class vector with the highest similarity score is designated as the target for update: (l , m) arg max i,j δdot(Cbi j , Hb l ) (4) where l denotes the mispredicted class, m represents the sub- label with the highest similarity, j indicates the class index, and i denotes the sub-label index. Ideally, the class vectors within the same class should represent different features, ensuring that each sample vector influences a unique class vector.\n\n--- Segment 9 ---\nThe mispredicted class vector with the highest similarity score is designated as the target for update: (l , m) arg max i,j δdot(Cbi j , Hb l ) (4) where l denotes the mispredicted class, m represents the sub- label with the highest similarity, j indicates the class index, and i denotes the sub-label index. Ideally, the class vectors within the same class should represent different features, ensuring that each sample vector influences a unique class vector. Therefore, for the true class, the target for update is determined as follows: (l, n) arg max i δdot(Cbi l , Hb l ) (5) where l is the true class and n indicates the sub-label of the class vector with the highest similarity among those within the same class. 3) Iterative Learning: The iterative learning process is carried out on the FP AM based on the selected update targets: Cn l Cn l αHl, Cm l Cm l αHl. (6) The learning rate typically ranges from 0.01 to 0.1, depending on the dataset and hyperparameters D, C. The learning rate is adjusted based on dataset complexity and model size: a lower one is used for more challenging datasets, while a higher one is applied to models with large D or C. 4) Binary AM Update: Following vector operations on the FP AM, we perform a normalization step. This normalization, distinct from standard HDC approaches, ensures an even distri- bution of learning influence across multiple class vectors within the same class, preventing any single vector from dominating. The process concludes with an update to the binary AM, achieved by binarizing the normalized FP AM. D. In-Memory Inference For in-memory inference, both the binary projection matrix as EM and the binary AM are mapped into the IMC array. The inference process utilizes the MVM-based encoding and associative search methods mentioned in Eq. (3): pred arg maxi,j δdot(Cbi j , Hb). IV. EVALUATION A. Experimental Setup Table I shows the characteristics, encoding, and training methods of baseline binary HDC models.\n\n--- Segment 10 ---\nEVALUATION A. Experimental Setup Table I shows the characteristics, encoding, and training methods of baseline binary HDC models. SearcHD [14] is the multi-model structure most similar to our approach, utilizing N-vector quantization, where a non-binary class vector is quantized into multiple binary class vectors. In our evaluation, TABLE I: Memory requirements of baseline HDC models Baseline Keywords Encoding Associative Binary HDC Methods Module Memory SearcHD [14] Multi-model ID-Level Single-pass (f L) D k D N QuantHD [13] ID-Level Quantization Aware Iterative-pass (f L) D k D LeHDC [15] ID-Level BNN based Training (f L) D k D BasicHDC Projection Single-pass f D k D MEMHD Multi-centroid Projection Quantization Aware Iterative-pass f D C D Note: f: of features, L: of levels, D: dimensionality, k: of classes, C: of columns and N: vector quantization factor. we fixed N 64. QuantHD [13] was the first to introduce the quantization-aware learning. LeHDC [15], based on BNN, is known as the state-of-the-art binary HDC model. All three methods use ID-level encoding with L 256. In addition to these models, we introduce BasicHDC separately because both its encoding and associative search can be performed using MVM operations, making it directly comparable to IMC array implementations. In contrast, the other baselines use ID-Level encoding, which is not directly compatible with IMC array computations. For IMC baselines, we compare MEMHD with two existing structures in Fig. 1. Our comparison focuses on (1) the required computation cycles for a single IMC array, the number of arrays needed to map the entire structure, and AM utilization, as well as (2) energy consumption and cycles of AM. Those read and write cycles and energy consumption data are derived from SRAM-based IMC arrays simulated using NeuroSim simulator [19], as presented in [20]. Our evaluation employs three datasets: MNIST [16], Fashion-MNIST (FM- NIST) [17], and ISOLET [18]. MEMHD are trained for 100 epochs following initialization.\n\n--- Segment 11 ---\nOur evaluation employs three datasets: MNIST [16], Fashion-MNIST (FM- NIST) [17], and ISOLET [18]. MEMHD are trained for 100 epochs following initialization. All experiments were conducted with 5 trials, and the average accuracy was reported. B. Accuracy and Memory Requirement Fig. 3 compares the accuracy and memory requirement of our model with the baselines for each dataset. Our model demonstrates accuracy close to or exceeding state-of-the-art performance while using significantly less memory. Notably, for MNIST using the 512x512, our approach achieves an ac- curacy 1.1 higher than the state-of-the-art while being 16.2 more memory-efficient. Furthermore, our mid-range models (256x256 for MNIST, FMNIST and 256x128 for ISOLET) out- perform comparable baselines such as LeHDC and QuantHD. On average, these models achieve 13.69 higher accuracy with the same memory usage, or equivalently, they are 13.25 more memory-efficient at the same accuracy level compared to the baselines. C. Accuracy Heatmap : Dimensions and Memory Columns Fig. 4 presents a heatmap of accuracy when varying the hyperparameters of our multi-centroid AM: dimensions and memory columns from 64 to 1024. This visualization allows us to optimize the AM structure according to the available 60 70 80 90 100 1 10 100 1000 Accuracy ( ) Memory (KB) 50 60 70 80 90 1 10 100 1000 Accuracy ( ) Memory (KB) 50 60 70 80 90 100 1 10 100 1000 Accuracy ( ) Memory (KB) (a) MNIST MEMHD LeHDC [15] SearcHD [14] QuantHD [13] BasicHDC (b) FMNIST (c) ISOLET 22.57 22.44 13.83 14.06 4.67 2.26 Fig. 3: Accuracy and memory requirement (KB). MEMHD used for (a) MNIST (b) FMNIST: 64x64 to 1024x1024 square sizes (DxC), (c) ISOLET: fixed 128 columns, varied dimensions. We set dimensions of baslines from 256D to 10240D. Fig. 4: MEMHD Accuracy Heatmap from 64x64 to 1024x1024. hardware resources by adjusting dimensions and columns.\n\n--- Segment 12 ---\n4: MEMHD Accuracy Heatmap from 64x64 to 1024x1024. hardware resources by adjusting dimensions and columns. Generally, higher dimensions correlate with improved accuracy, which is likely related to the quality of encoding. For MNIST and FMNIST, we observe that both higher dimensions and more columns (i.e., using more class vectors) correlate with improved accuracy. However, ISOLET dataset exhibits a different pattern, showing peak performance when using 128-256 columns. This divergence in optimal structure reflects the unique characteristics of each dataset. ISOLET, with its small sample size per class (approximately 240 samples per class) and large number of classes, shows peak performance with fewer columns. Using too many columns for ISOLET could lead to overfitting, potentially causing some class vectors to represent outliers rather than general class features. Conversely, MNIST and FMNIST (approximately 6000 samples per class), being larger and more diverse, benefit from additional class vectors without such overfitting concerns. These findings highlight the importance of adapting the AM structure to both the dataset characteristics and available computational resources. D. Clustering-based Initialization Our clustering-based initialization significantly outperforms random sampling in accuracy. Fig. 5 shows that for MNIST (512x512) and ISOLET (1024x256), clustering achieves 8.69 and 19.95 higher initial accuracies respectively, leading to improved final accuracies and faster convergence (within 10- 20 epochs compared to 30-40 epochs for random sampling). 80 85 90 95 100 1 11 21 31 41 51 Accuracy ( ) Epoch 8.69 (a) MNIST 512x512 60 70 80 90 100 1 11 21 31 41 51 Accuracy ( ) Epoch (b) ISOLET 1024x256 19.95 0.8 0.3 Clustering Random Sampling Fig. 5: Accuracy comparison between clustering and random sampling. 70 75 80 85 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy ( ) R (a) FMNIST 84 85 86 87 88 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy ( ) R (b) ISOLET 512x512 512x64 Fig. 6: Accuracy for different initial cluster ratios R (0.1 to 1.0).\n\n--- Segment 13 ---\n70 75 80 85 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy ( ) R (a) FMNIST 84 85 86 87 88 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Accuracy ( ) R (b) ISOLET 512x512 512x64 Fig. 6: Accuracy for different initial cluster ratios R (0.1 to 1.0). The initial clusters ratio R influences the distribution of class vectors. As shown in Fig. 6, R has minimal effect at 512x512 but impacts performance at 512x64, with optimal values between 0.8 and 0.9. ISOLET achieves highest accuracy at R 1.0. These findings highlight the importance of proper initialization and R selection in optimizing performance across datasets and C. TABLE II: Computation cycles, arrays and AM utilization for MNIST, FMNIST and ISOLET datasets using 128x128 IMC array. Dataset (a) MNIST, FMNIST (b) ISOLET IMC Mapping Basic Partitioning MEMHD Improv. Basic Partitioning MEMHD Improv. Method P 5 P 10 P 2 P 4 AM Structure 10240x10 2048x50 1024x100 128x128 - 10240x26 5120x52 2560x104 512x128 - of Cycles EM 560 560 560 7 80 400 400 400 20 20 AM 80 80 80 1 80 80 80 80 4 20 Total 640 640 640 8 80 480 480 480 24 20 of Arrays EM 560 560 560 7 80 400 400 400 20 20 AM 80 16 8 1 8 80 40 20 4 5 Total 640 576 568 8 71 480 440 420 24 17.5 Utilization AM 7.81 39.06 78.13 100 21.87 20.31 40.63 81.25 100 18.75 E. Computation Cycle, Array Usage and AM Utilization In Fig. 4, MEMHD demonstrated comparable or higher accuracy compared to BasicHDC with 10240D: our 128x128 achieved higher accuracy for both MNIST and FMNIST, while 512x128 showed comparable results for ISOLET. So, Table II compares our models with IMC baselines with 10240D when using 128x128 IMC array.\n\n--- Segment 14 ---\n4, MEMHD demonstrated comparable or higher accuracy compared to BasicHDC with 10240D: our 128x128 achieved higher accuracy for both MNIST and FMNIST, while 512x128 showed comparable results for ISOLET. So, Table II compares our models with IMC baselines with 10240D when using 128x128 IMC array. We evaluate three key metrics: com- putation cycles, array usage, and AM utilization. Computation cycles refer to the number of operations performed when using a single array, while array usage indicates the number of arrays required to map the entire AM structure. AM utilization repre- sents the ratio of actually mapped columns to the total columns in the IMC array. For partitioning, we consider two suitable scenarios for each dataset. Our model leverages lower dimen- sional vectors, which substantially reduces the EM s memory requirements. This reduction in dimensionality leads to fewer computation cycles and requires fewer IMC arrays, enhancing overall efficiency. Compared to the baseline, MEMHD achieves fully-utilization of AM at all times. As shown in Table II-(a), for MNIST and FMNIST datasets, our model demonstrates an 80 improvement in computational efficiency and requires 71 fewer arrays compared to the baseline. For ISOLET dataset, as presented in Table II-(b), our model achieves a 20 increase in computational efficiency and uses 17.5 fewer arrays. F. Energy Consumption and Cycles of AM Fig. 7 presents a comparative analysis of normalized AM energy consumption and cycles across all baseline models. While some baseline binary HDC models use ID-Level en- coding, all models employ MVM-based associative search for inference, enabling a fair comparison of AM performance. The figure showcases models demonstrating equivalent accuracy to MEMHD 128x128 on the FMNIST dataset, as shown in Fig. 3. The total number of arrays required to map the entire AM structure is proportional to the model s dimensionality. When the entire AM is mapped to arrays at once, models without partitioning can perform inference in a single cycle. However, for partitioned models, the number of cycles increases proportionally to the number of partitions. While partitioning strategies effectively reduce the number of required arrays, they proportionally increase the number of cycles, resulting in constant energy consumption across different partitioning schemes. MEMHD distinguishes itself by enabling associative search with just one computation in a single 128x128 array.\n\n--- Segment 15 ---\nWhile partitioning strategies effectively reduce the number of required arrays, they proportionally increase the number of cycles, resulting in constant energy consumption across different partitioning schemes. MEMHD distinguishes itself by enabling associative search with just one computation in a single 128x128 array. This unique architecture allows MEMHD to achieve not only single-cycle inference but also significantly reduced energy 0 20 40 60 80 100 0 20 40 60 80 100 of Cycles of Arrays Normalized Energy Consumption (AM) FMNIST BasicHDC 10240x10 BasicHDC 1024x100 (P 10) SearcHD 8000x10 [14] SearcHD 800x100 (P 10) QuantHD 1600x10 [13] QuantHD 160x100 (P 10) LeHDC 400x10 [15] LeHDC 100x40 (P 4) MEMHD 128x128 1 Total Number of Arrays Energy Consumption Computation Cycles Fig. 7: Normalized energy of AM and cycles with array usage. MEMHD (128x128) achieves comparable accuracy to base- lines with higher dimensions: BasicHDC (10240D), SearcHD (8000D), QuantHD (1600D), and LeHDC (400D). consumption. Consequently, MEMHD demonstrates remark- able efficiency gains: it is 80 more efficient than Basic HDC in terms of energy consumption. Moreover, compared to LeHDC, which represents the state-of-the-art in accuracy, MEMHD is 4 more efficient. V. CONCLUSION This paper introduced MEMHD, a Memory-Efficient Multi- centroid HDC framework that optimizes HDC for IMC archi- tectures. MEMHD effectively trains multi-centroid AM through clustering-based initialization and quantization-aware iterative learning. As a result, this approach significantly outperforms binary HDC baselines, achieving up to 13.69 higher accuracy with the same memory usage or 13.25 more memory effi- ciency at the same accuracy level. By enabling full utilization of IMC arrays and one-shot (or few-shot) associative search, MEMHD reduces computation cycles by up to 80 and array usage by up to 71 compared to the partitioning method on 128 128 IMC arrays, substantially improving energy efficiency and cycles. These advancements pave the way for more efficient implementation of HDC in resource-constrained environments.\n\n--- Segment 16 ---\nBy enabling full utilization of IMC arrays and one-shot (or few-shot) associative search, MEMHD reduces computation cycles by up to 80 and array usage by up to 71 compared to the partitioning method on 128 128 IMC arrays, substantially improving energy efficiency and cycles. These advancements pave the way for more efficient implementation of HDC in resource-constrained environments. ACKNOWLEDGEMENT This work was partly supported by the National Re- search Foundation of Korea (NRF) grant (No. RS-2024- 00345732); the Institute for Information communications Technology Planning Evaluation (IITP) grants (RS-2020- II201821, IITP-2021-0-02052, RS-2019-II190421, RS-2021- II212068); the Technology Innovation Program (RS-2023- 00235718, 23040-15FC) funded by the Ministry of Trade, Industry Energy (MOTIE, Korea) grant (1415187505); Samsung Electronics Co., Ltd (IO230404-05747-01); and the BK21-FOUR Project. REFERENCES [1] P. Kanerva, Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors, Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009. [2] Rahimi, Abbas, et al. A Robust and Energy-Efficient Classifier Using Brain-Inspired Hyperdimensional Computing. Proceedings of the 2016 International Symposium on Low Power Electronics and Design, 2016. [3] Neubert, Peer, et al. An Introduction to Hyperdimensional Computing for Robotics. KI-K unstliche Intelligenz, vol. 33, no. 4, 2019. [4] Rahimi, Abbas, et al. Efficient Biosignal Processing Using Hyperdi- mensional Computing: Network Templates for Combined Learning and Classification of ExG Signals. Proceedings of the IEEE, vol. 107, no. 1, pp. 123-143, 2018. [5] Sebastian, Abu, et al. Memory Devices and Applications for In-Memory Computing. Nature Nanotechnology, vol. 15, no. 7, pp. 529-544, 2020. [6] Verma, Naveen, et al.\n\n--- Segment 17 ---\n529-544, 2020. [6] Verma, Naveen, et al. In-Memory Computing: Advances and Prospects. IEEE Solid-State Circuits Magazine, vol. 11, no. 3, pp. 43-55, 2019. [7] Shafiee, Ali, et al. ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars. ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 14-26, 2016. [8] Imani, Mohsen, et al. Exploring Hyperdimensional Associative Mem- ory. 2017 IEEE International Symposium on High Performance Com- puter Architecture (HPCA), pp. 445-456, 2017. [9] Karunaratne, Geethan, et al. In-Memory Hyperdimensional Computing. Nature Electronics, vol. 3, no. 6, pp. 327-337, 2020. [10] Kazemi, Arman, et al. Achieving Software-Equivalent Accuracy for Hyperdimensional Computing with Ferroelectric-Based In-Memory Com- puting. Scientific Reports, vol. 12, no. 1, pp. 19201, 2022. [11] Morris, Justin, et al. Locality-Based Encoder and Model Quantization for Efficient Hyper-Dimensional Computing. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 41, no. 4, pp. 897-907, 2021. [12] Thomas, Anthony, et al. A Theoretical Perspective on Hyperdimensional Computing. Journal of Artificial Intelligence Research, vol. 72, pp. 215- 249, 2021. [13] Imani, Mohsen, et al. QuantHD: A Quantization Framework for Hyper- dimensional Computing. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 39, no. 10, pp. 2268-2278, 2019. [14] Imani, Mohsen, et al. SearcHD: A Memory-Centric Hyperdimensional Computing with Stochastic Training. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems, vol. 39, no. 10, pp. 2422-2433, 2019. [15] Duan, S., et al.\n\n--- Segment 18 ---\n2422-2433, 2019. [15] Duan, S., et al. LEHDC: Learning-based Hyperdimensional Computing Classifier. Proceedings of the 59th ACM IEEE Design Automation Conference, pp. 1111-1116, 2022. [16] LeCun, Y., et al. Gradient-Based Learning Applied to Document Recog- nition. Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998. [17] Xiao, H., et al. Fashion-MNIST: a Novel Image Dataset for Benchmark- ing Machine Learning Algorithms. arXiv preprint arXiv:1708.07747, 2017. [18] Cole, R., et al. The ISOLET Spoken Letter Database. Technical Report CSE 90-004, Dept. of Computer Science and Engineering, Oregon Graduate Institute of Science and Technology, Beaverton, OR, 1990. [19] Peng, Xiaochen, et al. DNN NeuroSim V2. 0: An End-to-End Bench- marking Framework for Compute-in-Memory Accelerators for On-Chip Training. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 40, no. 11, pp. 2306-2319, 2020. [20] Jeon, Kang Eun, et al. Weight-Aware Activation Mapping for Energy- Efficient Convolution on PIM Arrays. 2023 IEEE ACM International Symposium on Low Power Electronics and Design (ISLPED), IEEE, 2023.\n\n