{
  "source": "cocktail.pdf",
  "raw_length": 98352,
  "cleaned_length": 96890,
  "base_segments": 414,
  "augmented_segments": 828,
  "segments": [
    {
      "text": "Abstract With a growing demand for adopting ML models for a variety of application services, it is vital that the frameworks serving these models are capable of delivering highly accurate predic- tions with minimal latency along with reduced deployment costs in a public cloud environment. Despite high latency, prior works in this domain are crucially limited by the accu- racy offered by individual models. Intuitively, model ensem- bling can address the accuracy gap by intelligently combining different models in parallel.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "Intuitively, model ensem- bling can address the accuracy gap by intelligently combining different models in parallel. However, selecting the appro- priate models dynamically at runtime to meet the desired accuracy with low latency at minimal deployment cost is a nontrivial problem. Towards this, we propose  Cocktail , a cost effective ensembling-based model serving framework.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Towards this, we propose  Cocktail , a cost effective ensembling-based model serving framework. Cock- tail  comprises of two key components: (i) a dynamic model selection framework, which reduces the number of models in the ensemble, while satisfying the accuracy and latency requirements; (ii) an adaptive resource management (RM) framework that employs a distributed proactive autoscaling policy, to efﬁciently allocate resources for the models. The RM framework leverages transient virtual machine (VM) in- stances to reduce the deployment cost in a public cloud.",
      "type": "sliding_window",
      "tokens": 128
    },
    {
      "text": "The RM framework leverages transient virtual machine (VM) in- stances to reduce the deployment cost in a public cloud. A prototype implementation of  Cocktail  on the AWS EC2 plat- form and exhaustive evaluations using a variety of workloads demonstrate that  Cocktail  can reduce deployment cost by 1.45 × , while providing 2 ×  reduction in latency and satisfy- ing the target accuracy for up to 96% of the requests, when compared to state-of-the-art model-serving frameworks. 1 Introduction \nMachine Learning (ML) has revolutionized user experience in various cloud-based application domains such as product recommendations [ 70 ], personalized advertisements [ 44 ], and computer vision [ 13 ,  43 ].",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "1 Introduction \nMachine Learning (ML) has revolutionized user experience in various cloud-based application domains such as product recommendations [ 70 ], personalized advertisements [ 44 ], and computer vision [ 13 ,  43 ]. For instance, Facebook [ 44 ,  82 ] serves trillions of inference requests for user-interactive ap- plications like ranking new-feeds, classifying photos, etc. It is imperative for these applications to deliver accurate predic- tions at sub-millisecond latencies [ 27 , 34 , 35 , 39 , 44 , 83 ] as they critically impact the user experience.",
      "type": "sliding_window",
      "tokens": 140
    },
    {
      "text": "It is imperative for these applications to deliver accurate predic- tions at sub-millisecond latencies [ 27 , 34 , 35 , 39 , 44 , 83 ] as they critically impact the user experience. This trend is expected to perpetuate as a number of applications adopt a variety of ML models to augment their services. These ML models are typically trained and hosted on cloud platforms as service end- points, also known as  model-serving  framework [ 6 ,  28 ,  60 ].",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "These ML models are typically trained and hosted on cloud platforms as service end- points, also known as  model-serving  framework [ 6 ,  28 ,  60 ]. From the myriad of ML ﬂavours, Deep Neural Networks \n(DNNs) [ 54 ] due to their multi-faceted nature, and highly gen- eralized and accurate learning patterns [ 45 , 73 ] are dominating the landscape by making these model-serving frameworks accessible to developers. However, their high variance due to the ﬂuctuations in training data along with compute and mem- ory intensiveness [ 59 , 65 , 84 ] has been a major impediment in designing models with high accuracy and low latency.",
      "type": "sliding_window",
      "tokens": 163
    },
    {
      "text": "However, their high variance due to the ﬂuctuations in training data along with compute and mem- ory intensiveness [ 59 , 65 , 84 ] has been a major impediment in designing models with high accuracy and low latency. Prior model-serving frameworks like InFaas [ 83 ] are conﬁned by the accuracy and latency offered by such individual models. Unlike single-model inferences, more sophisticated tech- niques like  ensemble learning  [ 15 ] have been instrumental in allowing model-serving to further improve accuracy with multiple models.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "Unlike single-model inferences, more sophisticated tech- niques like  ensemble learning  [ 15 ] have been instrumental in allowing model-serving to further improve accuracy with multiple models. For example, by using the ensembling   1 \ntechnique, images can be classiﬁed using multiple models  in parallel  and results can be combined to give a ﬁnal prediction. This signiﬁcantly boosts accuracy compared to single-models, and for this obvious advantage, frameworks like Clipper [ 27 ] leverage ensembling techniques.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "This signiﬁcantly boosts accuracy compared to single-models, and for this obvious advantage, frameworks like Clipper [ 27 ] leverage ensembling techniques. Nevertheless, with ensem- bling, the very high resource footprint due to sheer number of models that need to be run for each request [ 27 , 56 ], ex- acerbates the public cloud deployment costs, as well as leads to high variation in latencies. Since cost plays a crucial role in application-provider consideration, it is quintessential to minimize the deployment costs, while maximizing accuracy with low latency.",
      "type": "sliding_window",
      "tokens": 135
    },
    {
      "text": "Since cost plays a crucial role in application-provider consideration, it is quintessential to minimize the deployment costs, while maximizing accuracy with low latency. Hence, the non-trivial challenge here lies in making the cost of ensembling predictions analogous to single model predictions, while satisfying these requirements. Studying the state-of-the-art ensemble model-serving frameworks, we observe the following critical shortcomings: •  Ensemble model selection policies used in frameworks like Clipper [ 27 ] are static, as they  ensemble all available models  and focus solely on minimizing loss in accuracy.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "Studying the state-of-the-art ensemble model-serving frameworks, we observe the following critical shortcomings: •  Ensemble model selection policies used in frameworks like Clipper [ 27 ] are static, as they  ensemble all available models  and focus solely on minimizing loss in accuracy. This leads to higher latencies and further inﬂates the resource foot- print, thereby accentuating the deployment costs. •  Existing ensemble weight estimation [ 87 ] has  high com- putational complexity  and in practice is limited to a small set of off-the-shelf models.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "•  Existing ensemble weight estimation [ 87 ] has  high com- putational complexity  and in practice is limited to a small set of off-the-shelf models. This leads to signiﬁcant loss in accuracy. Besides, employing linear ensembling techniques such as model averaging are compute intensive [ 80 ] and not scalable for a large number of available models.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "Besides, employing linear ensembling techniques such as model averaging are compute intensive [ 80 ] and not scalable for a large number of available models. •  Ensemble systems [ 27 , 80 ] are  not focused towards model deployment  in a public cloud infrastructure, where resource \n1 We refer to ensemble-learning as ensembling throughout the paper. USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1041 \nselection and procurement play a pivotal role in minimizing the latency and deployment costs.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1041 \nselection and procurement play a pivotal role in minimizing the latency and deployment costs. Further, the resource provi- sioning strategies employed in single model-serving systems are  not directly extendable  to ensemble systems. These shortcomings collectively motivate the central premise of this work:  how to solve the complex optimiza- tion problem of cost, accuracy and latency for an ensem- bling framework?",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "These shortcomings collectively motivate the central premise of this work:  how to solve the complex optimiza- tion problem of cost, accuracy and latency for an ensem- bling framework? In this paper, we present and evaluate Cocktail 2 , which to our knowledge is the ﬁrst work that pro- poses a cost-effective model-serving system by exploiting ensembling techniques for classiﬁcation-based inference, to deliver high accuracy and low latency predictions. Cocktail adopts a three-pronged approach to solve the optimization problem.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "Cocktail adopts a three-pronged approach to solve the optimization problem. First, it uses a dynamic model selection policy to signiﬁcantly reduce the number of models used in an ensem- ble, while meeting the latency and accuracy requirements. Figure 1:  Beneﬁts of  Cocktail .",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Figure 1:  Beneﬁts of  Cocktail . Re- sults are normalized (higher the better). Second, it utilizes dis- tributed autoscaling poli- cies to reduce the la- tency variability and re- source consumption of hosting ensemble mod- els.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Second, it utilizes dis- tributed autoscaling poli- cies to reduce the la- tency variability and re- source consumption of hosting ensemble mod- els. Third, it minimizes the cost of deploying ensembles in a public cloud by taking advan- tage of transient VMs, as they can be 70-90% cheaper [ 3 ] than traditional VMs. Cocktail , by coalescing these beneﬁts, is capable of operating in a region of optimal cost, accuracy and latency (shown in Figure  1 ) that prior works cannot achieve.",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "Cocktail , by coalescing these beneﬁts, is capable of operating in a region of optimal cost, accuracy and latency (shown in Figure  1 ) that prior works cannot achieve. Towards this, the key contributions  of the paper are summarized below: \n1. By characterizing accuracy  vs.  latency of ensemble models, we identify that prudently selecting a subset of available models under a given latency can achieve the target ac- curacy.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "By characterizing accuracy  vs.  latency of ensemble models, we identify that prudently selecting a subset of available models under a given latency can achieve the target ac- curacy. We leverage this in  Cocktail , to design a novel dynamic model selection policy, which ensures accuracy with signiﬁcantly reduced number of models. 2.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "2. Focusing on classiﬁcation-based inferences, it is important to minimize the bias in predictions resulting from multi- ple models. In  Cocktail , we employ a per-class weighted majority voting policy, that makes it scalable and effec- tively breaks ties when compared to traditional weighted averaging, thereby minimizing the accuracy loss.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "In  Cocktail , we employ a per-class weighted majority voting policy, that makes it scalable and effec- tively breaks ties when compared to traditional weighted averaging, thereby minimizing the accuracy loss. 3. We show that uniformly scaling resources for all models in the ensemble leads to over-provisioning of resources and towards minimizing it, we build a distributed weighted auto-scaling policy that utilizes the  importance sampling technique to proactively allocate resources to every model.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "We show that uniformly scaling resources for all models in the ensemble leads to over-provisioning of resources and towards minimizing it, we build a distributed weighted auto-scaling policy that utilizes the  importance sampling technique to proactively allocate resources to every model. Further,  Cocktail  leverages transient VMs as they are cheaper, to drastically minimize the cost for hosting model- serving infrastructure in a public cloud. 2 Cocktail is ascribed to having the perfect blend of models in an ensemble.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "2 Cocktail is ascribed to having the perfect blend of models in an ensemble. Applications \nImage  Recognition NLP Recommender  Systems \nModels \nVM \nVM \nVM VM VM \nVM \nFrameworks \nCloud \nResources \nSLO \nAccuracy \nLatency \nUsers \nBurstables Spot CPU GPU \nCost \nLatency \nFigure 2:  The overall framework for model-serving in public cloud. 4.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "4. We implement a prototype of  Cocktail  using both CPU and GPU instances on AWS EC2 [ 5 ] platform and ex- tensively evaluate it using different request-arrival traces. Our results from exhaustive experimental analysis demon- strate that  Cocktail  can minimize deployment cost by 1.4 × while meeting the accuracy for up-to 96% of the requests and providing 2 ×  reduction in latency, when compared to state-of-the-art model serving systems.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "Our results from exhaustive experimental analysis demon- strate that  Cocktail  can minimize deployment cost by 1.4 × while meeting the accuracy for up-to 96% of the requests and providing 2 ×  reduction in latency, when compared to state-of-the-art model serving systems. 5. We show that ensemble models are inherently fault- tolerant over single models, since in the former, failure of a model would incur some accuracy loss without complete failure of the requests.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "We show that ensemble models are inherently fault- tolerant over single models, since in the former, failure of a model would incur some accuracy loss without complete failure of the requests. It is observed from our failure- resilience results that  Cocktail  can adapt to instance fail- ures by limiting the accuracy loss within 0.6%. 2 Background and Motivation \nWe start by providing a brief overview of model-serving in public cloud and ensembling, followed by a detailed analysis of their performance to motivate the need for  Cocktail .",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "2 Background and Motivation \nWe start by providing a brief overview of model-serving in public cloud and ensembling, followed by a detailed analysis of their performance to motivate the need for  Cocktail . 2.1 Model Serving in Public Cloud \nFigure  2  shows the overall architecture of a model-serving framework. There are diverse applications that are typically developed, trained and hosted as web services.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "There are diverse applications that are typically developed, trained and hosted as web services. These services allow end-users to submit queries via web server interface. Since these inference requests are often user-facing, it is imperative to administer them under a strict service level ob- jective (SLO).",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "Since these inference requests are often user-facing, it is imperative to administer them under a strict service level ob- jective (SLO). We deﬁne SLO as the end-to-end response latency required by an application. Services like Ads and News Feed [ 39 , 44 ] would require SLOs within 100ms, while facial tag recommendation [ 83 ] can tolerate up to 1000ms.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Services like Ads and News Feed [ 39 , 44 ] would require SLOs within 100ms, while facial tag recommendation [ 83 ] can tolerate up to 1000ms. A myriad of model architectures are available to train these applications which by themselves can be deployed on appli- cation frameworks like  TensorFlow  [ 1 ],  PyTorch  [ 62 ] etc. Table  1  shows the different models available for image predic- tion, that are pretrained on Keras using  ImageNet  [ 29 ] dataset.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "Table  1  shows the different models available for image predic- tion, that are pretrained on Keras using  ImageNet  [ 29 ] dataset. Each model has unique accuracy and latencies depending on the model architecture. Typically denser models are designed with more parameters (ex.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Typically denser models are designed with more parameters (ex. NASLarge ) to classify complex \n1042    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nModel (Acronym) Params (10k) \nTop-1 Accuracy(%) \nLatency (ms) P f \nMobileNetV1 (MNet) 4,253 70.40 43.45 10 MobileNetV2 (MNetV2) 4,253 71.30 41.5 10 NASNetMobile (NASMob) 5,326 74.40 78.18 3 DenseNet121 (DNet121) 8,062 75.00 102.35 3 DenseNet201 (DNet201) 20,242 77.30 152.21 2 Xception (Xcep) 22,910 79.00 119.2 4 Inception V3 (Incep) 23,851 77.90 89 5 ResNet50-V2 (RNet50) 25,613 76.00 89.5 6 Resnet50 (RNet50) 25,636 74.90 98.22 5 IncepResnetV2 (IRV2) 55,873 80.30 151.96 1 NasNetLarge (NasLarge) 343,000 82.00 311 1 \nTable 1:  Collection of pretrained models used for image classiﬁcation. classes of images.",
      "type": "sliding_window",
      "tokens": 317
    },
    {
      "text": "classes of images. These 11 models are a representative set to classify all images belonging to 1000 classes in  Imagenet. Depending on the application type, the maximum ensemble size can vary from tens to hundreds of models.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Depending on the application type, the maximum ensemble size can vary from tens to hundreds of models. The entire model framework is typically hosted on re- sources like VMs or containers in public cloud. These re- sources are available in different types including CPU/GPU instances, burstables and transient instances.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "These re- sources are available in different types including CPU/GPU instances, burstables and transient instances. Transient in- stances [ 69 ] are similar to traditional VMs but can be revoked at any time by the cloud provider with an interruption notice. The provisioning latency, instance permanence and packing factor of these resources have a direct impact on the latency and cost of hosting model-serving.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "The provisioning latency, instance permanence and packing factor of these resources have a direct impact on the latency and cost of hosting model-serving. We explain instance “pack- ing factor” and its relationship with latency in Section  2.3.2 . In this paper, we focus on improving the accuracy and latency from the model selection perspective and consider instances types from a cost perspective.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "In this paper, we focus on improving the accuracy and latency from the model selection perspective and consider instances types from a cost perspective. A majority of the model serving systems [ 6 , 83 , 86 ] in public cloud support individual model selection from available models. For instance, InFaas [ 83 ] can choose variants among a same model to maintain accu- racy and latency requirements.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "For instance, InFaas [ 83 ] can choose variants among a same model to maintain accu- racy and latency requirements. However, denser models tend to have up to 6 ×  the size and twice the latency of smaller models to achieve increased accuracy of about 2-3%. Besides using dense models, ensembling [ 15 ] techniques have been used to achieve higher accuracy.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Besides using dense models, ensembling [ 15 ] techniques have been used to achieve higher accuracy. Why Ensembling? An Ensemble is deﬁned as a set of clas- siﬁers whose individual decisions combined in some way to classify new examples.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "An Ensemble is deﬁned as a set of clas- siﬁers whose individual decisions combined in some way to classify new examples. This has proved to be more accurate than traditional single large models because it inherently re- duces incorrect predictions due to variance and bias. The commonly used ensemble method in classiﬁcation problems is bagging [ 33 ] that considers homogeneous weak learners, learns them independently from each other in parallel, and combines them following some kind of deterministic aver- aging process [ 18 ] or majority voting [ 49 ] process.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "The commonly used ensemble method in classiﬁcation problems is bagging [ 33 ] that considers homogeneous weak learners, learns them independently from each other in parallel, and combines them following some kind of deterministic aver- aging process [ 18 ] or majority voting [ 49 ] process. For fur- ther details on ensemble models, we refer the reader to prior works [ 14 , 57 , 58 , 61 , 64 , 77 , 78 , 88 ]. 2.2 Related Work \nEnsembling in practice : Ensembling is supported by com- mercial cloud providers like Azure ML-studio [ 11 ] and AWS Autogluon [ 31 ] to boost the accuracy compared to single models.",
      "type": "sliding_window",
      "tokens": 171
    },
    {
      "text": "2.2 Related Work \nEnsembling in practice : Ensembling is supported by com- mercial cloud providers like Azure ML-studio [ 11 ] and AWS Autogluon [ 31 ] to boost the accuracy compared to single models. Azure initially starts with 5 models and scales up to \nFeatures \nClipper [ 27 ] \nRaﬁki [ 80 ] \nInfaas [ 83 ] \nMArk [ 86 ] \nSagemaker \nSwayam [ 34 ] \nCocktail \nPredictive Scaling \u0017 \u0017 \u0017 \u0013 \u0017 \u0013 \u0013 SLO Guarantees \u0013 \u0017 \u0013 \u0013 \u0017 \u0013 \u0013 Cost Effective \u0017 \u0017 \u0013 \u0013 \u0017 \u0017 \u0013 Ensembling \u0013 \u0013 \u0017 \u0017 \u0013 \u0017 \u0013 Heterogeneous Instances \u0017 \u0013 \u0013 \u0013 \u0013 \u0017 \u0013 Dynamic ensemble selection \u0017 \u0017 \u0017 \u0017 \u0017 \u0017 \u0013 Model abstraction \u0013 \u0013 \u0013 \u0017 \u0017 \u0017 \u0013 \nTable 2:  Comparing  Cocktail  with other related frameworks. 200 using a hill-climb policy [ 17 ] to meet the target accuracy.",
      "type": "sliding_window",
      "tokens": 170
    },
    {
      "text": "200 using a hill-climb policy [ 17 ] to meet the target accuracy. AWS combines about 6-12 models to give the best possible accuracy. Users also have the option to manually mention the ensemble size.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "Users also have the option to manually mention the ensemble size. Unlike them,  Cocktail’s  model selection pol- icy tries to right-size the ensemble for a given latency, while maximizing accuracy. Model-serving in Cloud : The most relevant prior works to Cocktail  are InFaas [ 83 ] and Clipper [ 27 ], which have been extensively discussed and compared to in Section  6 .",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Model-serving in Cloud : The most relevant prior works to Cocktail  are InFaas [ 83 ] and Clipper [ 27 ], which have been extensively discussed and compared to in Section  6 . Recently FrugalML [ 20 ] was proposed to cost-effectively choose from commercial MLaaS APIs. While striking a few similarities with  Cocktail , it is practically limited to image-classiﬁcation applications with very few classes and does not address re- source provisioning challenges.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "While striking a few similarities with  Cocktail , it is practically limited to image-classiﬁcation applications with very few classes and does not address re- source provisioning challenges. Several works [ 37 , 38 ] like MArk [ 86 ] proposed SLO and cost aware resource procure- ment policies for model-serving. Although our heterogeneous instance procurement policy has some similarities with MArk, it is signiﬁcantly different because we consider ensemble models.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Although our heterogeneous instance procurement policy has some similarities with MArk, it is signiﬁcantly different because we consider ensemble models. Raﬁki [ 80 ] considers small model sets and scales up and down the ensemble size by trading off accuracy to match throughput demands. However,  Cocktail’s  resource management is more adaptive to changing request loads and does not drop accuracy.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "However,  Cocktail’s  resource management is more adaptive to changing request loads and does not drop accuracy. Pretzel [ 52 ] and Inferline [ 26 ] are built on top of Clipper to optimize the prediction pipeline and cost due to load variations, respectively. Many prior works [ 2 , 25 , 35 , 63 , 74 , 75 ] have extensively tried to reduce model latency by reducing overheads due to shared resources and hardware interference.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Many prior works [ 2 , 25 , 35 , 63 , 74 , 75 ] have extensively tried to reduce model latency by reducing overheads due to shared resources and hardware interference. We believe that our proposed policies can be complementary and beneﬁcial to these prior works to reduce the cost and resource footprint of ensembling. There are mainstream commercial systems which automate single model-serving like TF-Serving [ 60 ], SageMaker [ 6 ], AzureML [ 10 ], Deep-Studio [ 28 ] etc.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "There are mainstream commercial systems which automate single model-serving like TF-Serving [ 60 ], SageMaker [ 6 ], AzureML [ 10 ], Deep-Studio [ 28 ] etc. Autoscaling in Public Cloud : There are several research works that optimize the resource provisioning cost in pub- lic cloud. These works are broadly categorized into: (i) multiplexing the different instance types (e.g., Spot, On- Demand) [ 12 ,  23 ,  34 ,  41 ,  42 ,  68 ,  79 ], (ii) proactive resource provisioning based on prediction policies [ 34 , 36 , 40 , 41 , 69 , 86 ].",
      "type": "sliding_window",
      "tokens": 166
    },
    {
      "text": "These works are broadly categorized into: (i) multiplexing the different instance types (e.g., Spot, On- Demand) [ 12 ,  23 ,  34 ,  41 ,  42 ,  68 ,  79 ], (ii) proactive resource provisioning based on prediction policies [ 34 , 36 , 40 , 41 , 69 , 86 ]. Cocktail  uses similar load prediction models and auto-scales VMs in a distributed fashion with respect to model ensem- bling. Swayam [ 34 ] is relatively similar to our work as it han- \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1043 \nBaseline(BL) NASLarge IRV2 Xception DNet121 NASMob #Models 10 8 7 5 2 BL_Latency 311(ms) 152(ms) 120(ms) 100(ms) 98(ms) E_Latency 152(ms) 120(ms) 103(ms) 89(ms) 44(ms) \nTable 3:  Comparing latency of Ensembling (E_Latency) with single (baseline) models.",
      "type": "sliding_window",
      "tokens": 286
    },
    {
      "text": "Swayam [ 34 ] is relatively similar to our work as it han- \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1043 \nBaseline(BL) NASLarge IRV2 Xception DNet121 NASMob #Models 10 8 7 5 2 BL_Latency 311(ms) 152(ms) 120(ms) 100(ms) 98(ms) E_Latency 152(ms) 120(ms) 103(ms) 89(ms) 44(ms) \nTable 3:  Comparing latency of Ensembling (E_Latency) with single (baseline) models. dles container provisioning and load-balancing, speciﬁcally catered for single model inferences. Cocktail’s  autoscaling policy strikes parallels with Swayam’s distributed autoscaling; however, we further incorporate novel importance sampling techniques to reduce over-provisioning for under-used models.",
      "type": "sliding_window",
      "tokens": 234
    },
    {
      "text": "Cocktail’s  autoscaling policy strikes parallels with Swayam’s distributed autoscaling; however, we further incorporate novel importance sampling techniques to reduce over-provisioning for under-used models. Table  2  provides a comprehensive comparison of  Cocktail with the most relevant works across key dimensions. 2.3 Pros and Cons of Model Ensembling \nIn this section, we quantitatively evaluate (i) how effective ensembles are in terms of accuracy and latency compared to single models, and (ii) the challenges in deploying en- semble frameworks in a cost-effective fashion on a public cloud.",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "2.3 Pros and Cons of Model Ensembling \nIn this section, we quantitatively evaluate (i) how effective ensembles are in terms of accuracy and latency compared to single models, and (ii) the challenges in deploying en- semble frameworks in a cost-effective fashion on a public cloud. For relevance in comparison to prior work [ 27 ,  83 ] we chose image inference as our ensemble workload. While ensembling is applicable in other classiﬁcation workloads like product recommendations [ 24 , 53 ], text classiﬁcation [ 71 ] etc, the observations drawn are generic and applicable to other applications.",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "While ensembling is applicable in other classiﬁcation workloads like product recommendations [ 24 , 53 ], text classiﬁcation [ 71 ] etc, the observations drawn are generic and applicable to other applications. 2.3.1 Ensembling Compared to Single Models \nTo analyze the accuracy offered by ensemble models, we con- duct an experiment using 10000 images from  ImageNet  [ 29 ] test dataset, on a  C5.xlarge  [ 8 ] instances in AWS EC2 [ 5 ]. For a given baseline model, we combine all models whose latency is lower than that of the baseline, and call it full- ensemble.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "For a given baseline model, we combine all models whose latency is lower than that of the baseline, and call it full- ensemble. We perform ensembling on the predictions using a simple majority voting policy. The latency numbers for the baseline models and the corresponding ensemble models along with the size of the ensemble are shown in Table  3 .",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "The latency numbers for the baseline models and the corresponding ensemble models along with the size of the ensemble are shown in Table  3 . In majority voting, every model votes for a prediction for each input, and the ﬁnal output prediction is the one that receives more than half of the votes. Figure  3a , shows the accuracy comparison of the baseline (single) and static ensemble (ex- plained in Section  3 ) compared to the full-ensemble.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "Figure  3a , shows the accuracy comparison of the baseline (single) and static ensemble (ex- plained in Section  3 ) compared to the full-ensemble. It is evident that full-ensemble can achieve up to 1.65% better accuracy than single models. Besides accuracy again, ensembling can also achieve lower latency.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "Besides accuracy again, ensembling can also achieve lower latency. The latency of the ensemble is calculated as the time between start and end of the longest running model.As shown in Table  3 , in the case of  NASLarge , the ensemble latency is 2 ×  lower (151ms) than the baseline latency (311ms). Even a 10ms reduction in latency is of signiﬁcant importance to the providers [ 35 ].",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Even a 10ms reduction in latency is of signiﬁcant importance to the providers [ 35 ]. We observe a similar trend of higher en- semble accuracy for other four baseline models with a latency reduction of up to 1.3 × . Thus, depending on the model sub- set used in the ensemble, it achieves better accuracy than the baseline at lower latencies.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Thus, depending on the model sub- set used in the ensemble, it achieves better accuracy than the baseline at lower latencies. Note that in our example model-set, the beneﬁts of ensembling will diminish for lower \nNASLarge IRV2 Xception DNet121 NASMob \n0.5 \n1.0 \n1.5 \nAccuracy Loss(%) \nStatic Single \n(a)  Accuracy loss compared to full- ensemble. NASLarge IRV2 XceptionDNet121 NASMob 0 \n2 \n4 \n6 \nCost($) \nSingle-OD Ensemble-OD Ensemble-spot \n(b)  Cost of full-ensembling hosted on OD and Spot instances.",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "NASLarge IRV2 XceptionDNet121 NASMob 0 \n2 \n4 \n6 \nCost($) \nSingle-OD Ensemble-OD Ensemble-spot \n(b)  Cost of full-ensembling hosted on OD and Spot instances. Figure 3:  Cost and accuracy of ensembling vs single models. accuracies (< 75%) because single models can reach those accuracies.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "accuracies (< 75%) because single models can reach those accuracies. Hence, based on the user constraints,  Cocktail chooses between ensemble and single models. 2.3.2 Ensembling Overhead \nWhile ensembling can boost accuracy with low latency, their distinctive resource hungry nature drastically increases the deployment costs when compared to single models.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "2.3.2 Ensembling Overhead \nWhile ensembling can boost accuracy with low latency, their distinctive resource hungry nature drastically increases the deployment costs when compared to single models. This is because more VMs or containers have to be procured to match the resource demands. However, note that the “Packing factor” ( P f  ) for each model also impacts the deployment costs.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "However, note that the “Packing factor” ( P f  ) for each model also impacts the deployment costs. P f  in this context is deﬁned as the number of inferences that can be executed concurrently in a single instance without violating the inference latency (on average). Table  1  provides the  P f  for 11 different models when executed on a  C5.xlarge  instance.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Table  1  provides the  P f  for 11 different models when executed on a  C5.xlarge  instance. There is a linear relationship between  P f  and the instance size. It can be seen that smaller models  (MNet, NASMob ) can be packed 2-5 ×  more when compared to larger models  (IRV2, NASLarge) .",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "It can be seen that smaller models  (MNet, NASMob ) can be packed 2-5 ×  more when compared to larger models  (IRV2, NASLarge) . Thus, the ensembles with models of higher  P f have signiﬁcantly lower cost. The beneﬁts of  P f  is contingent upon the models chosen by the model selection policy.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "The beneﬁts of  P f  is contingent upon the models chosen by the model selection policy. Existing ensemble model se- lection policies used in systems like Clipper use all off-the- shelf models and assign weights to them to calculate accu- racy. However, they do not right-size the model selection to include models which primarily contribute to the major- ity voting.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "However, they do not right-size the model selection to include models which primarily contribute to the major- ity voting. We compare the cost of hosting ensembles using both spot (ensemble-spot) and OD (ensemble-OD) instances with the single models hosted on OD (single-OD) instances. Ensemble-spot is explained further in the next section.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "Ensemble-spot is explained further in the next section. We run the experiment over a period of 1 hour for 10 requests/second. The cost is calculated as the cost per hour of EC2 c5.xlarge instance use, billed by AWS [ 5 ].",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "The cost is calculated as the cost per hour of EC2 c5.xlarge instance use, billed by AWS [ 5 ]. We ensure all instances are fully utilized by packing multiple requests in accordance to the  P f  . As shown in Figure  3b , Ensemble-OD is always ex- pensive than single-OD for the all the models.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "As shown in Figure  3b , Ensemble-OD is always ex- pensive than single-OD for the all the models. Therefore, it is important to ensemble an “optimal” number of less compute intensive models to reduce the cost. 3 Prelude to Cocktail \nTo speciﬁcally address the cost of hosting an ensembling- based model-serving framework in public clouds without sacriﬁcing the accuracy, this section introduces an overview of the two primary design choices employed in  Cocktail .",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "3 Prelude to Cocktail \nTo speciﬁcally address the cost of hosting an ensembling- based model-serving framework in public clouds without sacriﬁcing the accuracy, this section introduces an overview of the two primary design choices employed in  Cocktail . How to reduce resource footprint? The ﬁrst step towards making model ensembling cost effective is to minimize the \n1044    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \number of models by pruning the ensemble, which reduces the overall resource footprint.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "The ﬁrst step towards making model ensembling cost effective is to minimize the \n1044    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \number of models by pruning the ensemble, which reduces the overall resource footprint. In order to estimate the right number of models to participate in a given ensemble, we conduct an experiment where we chose top   N \n2   accurate models (static) from the full-ensemble of size  N . From Figure  3a , it can be seen that the static policy has an accuracy loss of up to 1.45% when compared to full-ensemble, but is still better than single models.",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "From Figure  3a , it can be seen that the static policy has an accuracy loss of up to 1.45% when compared to full-ensemble, but is still better than single models. This implies that the models other than top   N \n2   yields a signiﬁcant 1.45% accuracy improvement in the full-ensemble but they cannot be statically determined. Peacock Panda Quill Slug Cup Class \n0 \n50 \n100 \nAccuracy \nMNetV2 IRV2 NASLarge \nFigure 4:  Class-wise Accuracy.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "Peacock Panda Quill Slug Cup Class \n0 \n50 \n100 \nAccuracy \nMNetV2 IRV2 NASLarge \nFigure 4:  Class-wise Accuracy. Therefore, a full-ensemble model participation is not required for all the inputs because, every model is in- dividually suited to classify certain classes of images when compared to other classes. Figure  4  shows the class-wise accuracy for three models on 5 distinct classes.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Figure  4  shows the class-wise accuracy for three models on 5 distinct classes. It can be seen that for simpler classes like Slug,  MNetV2  can achieve similar accuracy as the bigger models, while for difﬁcult classes, like Cup and Quill, it experiences up to 3% loss in accuracy. Since the model participation for ensembling can vary based on the class of input images being classiﬁed, there is a scope to develop a dy- namic model selection policy that can leverage this class-wise variability to intelligently determine the number of models required for a given input.",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "Since the model participation for ensembling can vary based on the class of input images being classiﬁed, there is a scope to develop a dy- namic model selection policy that can leverage this class-wise variability to intelligently determine the number of models required for a given input. Key Takeaway:  Full ensemble model-selection is an overkill, while static-ensemble leads to accuracy loss. This calls for a dynamic model selection policy which can accurately de- termine the number of models required, contingent upon the accuracy and scalability of the model selection policy.",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "This calls for a dynamic model selection policy which can accurately de- termine the number of models required, contingent upon the accuracy and scalability of the model selection policy. How to save cost? Although dynamic model selection poli- cies can signiﬁcantly reduce the resource footprint as shown in Figure  3b , the cost is still 20-30% higher when compared to a single model inference.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Although dynamic model selection poli- cies can signiﬁcantly reduce the resource footprint as shown in Figure  3b , the cost is still 20-30% higher when compared to a single model inference. Most cloud providers offer tran- sient VMs such as Amazon Spot instances [ 69 ], Google Pre- emptible VMs [ 9 ], and Azure Low-priority VMs [ 7 ], that can reduce cloud computing costs by as much as 10 ×  [ 3 ]. In  Cock- tail , we leverage these transient VMs such as spot instances to drastically reduce the cost of deploying ensembling model framework.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "In  Cock- tail , we leverage these transient VMs such as spot instances to drastically reduce the cost of deploying ensembling model framework. As an example, we host full-ensembling on AWS spot instances. Figure  3b  shows that ensemble-spot can re- duce the cost by up to 3.3 ×  when compared to ensemble-OD.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "Figure  3b  shows that ensemble-spot can re- duce the cost by up to 3.3 ×  when compared to ensemble-OD. For certain baselines like IRV2, ensemble-spot is also 1.5 × cheaper than single-OD. However, the crucial downside of using transient VMs is that they can be unilaterally preempted by the cloud provider at any given point due to reasons like in- crease in bid-price or provider-induced random interruptions.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "However, the crucial downside of using transient VMs is that they can be unilaterally preempted by the cloud provider at any given point due to reasons like in- crease in bid-price or provider-induced random interruptions. As we will discuss further,  Cocktail  is resilient to instance failures owing to the fault-tolerance of ensembling by com- puting multiple inferences for a single request. Key takeaway :  The cost-effectiveness of transient instances, is naturally suitable for hosting ensemble models.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "Key takeaway :  The cost-effectiveness of transient instances, is naturally suitable for hosting ensemble models. Fast  Cache \nMobileNet NasNet \nResNet50 DenseNet121 \nDynamic Model  Selection \n. .",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": ". . Aggregator \nMaster VM \nUser Requests \n… … … … … \nQueries Cost aware Procurement \nImportance Sampling \nModel-1  Model-2  Model-3  Model-4  Model-n  \noutput \nHeterogeneity \nPrediction Policy \nAutoscaler \nResource Controller \nLoad Balancer \n argmax O 1  (latency)  argmin O 2  (accuracy) \nCPU GPU CPU GPU \nObjectives \n1a \n3 \n4b \n1b \n2 4 \n4a \n4b \n1 \n6 \n6b \n6a \nw1 w2 w3 wk w4 \n3 \n5  Bin-Packing \nWeight Matrix \nL \nN \nFigure 5:  High-level overview of  Cocktail  design.",
      "type": "sliding_window",
      "tokens": 153
    },
    {
      "text": "Aggregator \nMaster VM \nUser Requests \n… … … … … \nQueries Cost aware Procurement \nImportance Sampling \nModel-1  Model-2  Model-3  Model-4  Model-n  \noutput \nHeterogeneity \nPrediction Policy \nAutoscaler \nResource Controller \nLoad Balancer \n argmax O 1  (latency)  argmin O 2  (accuracy) \nCPU GPU CPU GPU \nObjectives \n1a \n3 \n4b \n1b \n2 4 \n4a \n4b \n1 \n6 \n6b \n6a \nw1 w2 w3 wk w4 \n3 \n5  Bin-Packing \nWeight Matrix \nL \nN \nFigure 5:  High-level overview of  Cocktail  design. 4 Overall Design of Cocktail \nMotivated by our observations, we design a novel model- serving framework,  Cocktail , that can deliver high-accuracy and low-latency predictions at reduced cost. Figure  5  depicts the high-level design of  Cocktail .",
      "type": "sliding_window",
      "tokens": 204
    },
    {
      "text": "Figure  5  depicts the high-level design of  Cocktail . Users submit requests to a master VM, which runs a model selection algorithm, 1a to decide the models to participate in the ensemble. The participating models are made available in a model cache  1b for faster access and avoid re-computation for requests having similar constraints.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "The participating models are made available in a model cache  1b for faster access and avoid re-computation for requests having similar constraints. Then, individual queries are dispatched to instances pools  2  dedicated for each model. The results from the workers are  ensembled  using an weighted majority voting aggregator  3  to agree upon a correct prediction.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "The results from the workers are  ensembled  using an weighted majority voting aggregator  3  to agree upon a correct prediction. To efﬁciently address the resource management and scalability challenges,  Cocktail  applies multiple strategies. First, it maintains dedicated instance pools to serve indi- vidual models which simpliﬁes the management and load balancing overheads for every model.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "First, it maintains dedicated instance pools to serve indi- vidual models which simpliﬁes the management and load balancing overheads for every model. Next, the resource con- troller  4  handles instance procurement, by exploiting both CPU and GPU instances  4a  in a cost-aware  4b  fashion, while the load balancer  5  ensures all procured instances are bin- packed by assigning queries to appropriate instances. We also design an autoscaler  6  , which utilizes a prediction pol- icy  6a  to forecast the request load and scale instances for every model pool, thereby minimizing over-provisioning of resources.",
      "type": "sliding_window",
      "tokens": 143
    },
    {
      "text": "We also design an autoscaler  6  , which utilizes a prediction pol- icy  6a  to forecast the request load and scale instances for every model pool, thereby minimizing over-provisioning of resources. The autoscaler further employs an importance sam- pling  6b  algorithm to estimate the importance of each model pool by calculating percentage of request served by it in a given time interval. The key components of the design are explained in detail below.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "The key components of the design are explained in detail below. 4.1 Dynamic Model Selection Policy \nWe use a window-based dynamic model selection policy using two objective functions as described below. Objective functions : In order to reduce cost and latency while maximizing the accuracy, we deﬁne a latency-accuracy metric ( µ AL ) and cost metric ( µ c ): \nµ AL  =   Acc target \nLat target µ C  =  k  × N ∑ m = 1 \ninst _ cost \nP f m \nwhere  N  is the number of models used to ensemble and inst _ cost  is the VM cost.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "Objective functions : In order to reduce cost and latency while maximizing the accuracy, we deﬁne a latency-accuracy metric ( µ AL ) and cost metric ( µ c ): \nµ AL  =   Acc target \nLat target µ C  =  k  × N ∑ m = 1 \ninst _ cost \nP f m \nwhere  N  is the number of models used to ensemble and inst _ cost  is the VM cost. Each model  m  has a packing factor \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1045 \nP f m  and k is a constant which depends on the VM size in terms of vCPUs (xlarge, 2xlarge, etc). Our ﬁrst objective function ( O 1 ) is to the maximize  µ AL  such that target accuracy ( Acc target ) is reached within the target latency ( Lat target ).",
      "type": "sliding_window",
      "tokens": 216
    },
    {
      "text": "Our ﬁrst objective function ( O 1 ) is to the maximize  µ AL  such that target accuracy ( Acc target ) is reached within the target latency ( Lat target ). max µ AL  : \u001a Acc target  ≥ Acc target  ± Acc margin Lat target  ≤ Lat target  ± Lat margin \nTo solve  O 1 , we determine an initial model list by choosing the individual models satisfying  Lat target  and then create a probabilistic ensemble that satisﬁes the  Acc target . Cocktail takes the accuracy of each model as a probability of cor- rectness and then iteratively constructs a model list, where the joint probability of them performing the classiﬁcation is within the accuracy target.",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "Cocktail takes the accuracy of each model as a probability of cor- rectness and then iteratively constructs a model list, where the joint probability of them performing the classiﬁcation is within the accuracy target. We tolerate a 0.2% ( Acc margin ) and 5ms ( Lat margin ) variance in  Acc target  and  Lat target , respec- tively. Next, we solve for the second objective function ( O 2 ) by minimizing  µ C , while maintaining the target accuracy.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "Next, we solve for the second objective function ( O 2 ) by minimizing  µ C , while maintaining the target accuracy. min µ C  : \b Acc target  ≥ Acc target  ± Acc margin \n( O 2 )  is solved by resizing the model list of size  N  and fur- ther through intelligence resource procurement (described in section  4.2 ), and thus maximizing  P f  and minimizing  k  simul- taneously. For  N  models, where each model has a minimum accuracy ‘ a ’, we model the ensemble as a coin-toss problem, where  N  biased coins (with probability of head being  a ) are tossed together, and we need to ﬁnd the probability of major- ity of them being heads.",
      "type": "sliding_window",
      "tokens": 173
    },
    {
      "text": "For  N  models, where each model has a minimum accuracy ‘ a ’, we model the ensemble as a coin-toss problem, where  N  biased coins (with probability of head being  a ) are tossed together, and we need to ﬁnd the probability of major- ity of them being heads. For this, we need at least  ⌊ N \n2   ⌋ +  1 models to give the same results. The probability of correct prediction is given by \nN ∑ i = ⌊ N \n2   ⌋ + 1 \n\u0012 N i \n\u0013 a i   ( 1 − a ) ( N − i ) \nModel Selection Algorithm:  To minimize  µ C , we design a policy to downscale the number of models, if more than N/2+1 models vote for the same classiﬁcation result.",
      "type": "sliding_window",
      "tokens": 178
    },
    {
      "text": "The probability of correct prediction is given by \nN ∑ i = ⌊ N \n2   ⌋ + 1 \n\u0012 N i \n\u0013 a i   ( 1 − a ) ( N − i ) \nModel Selection Algorithm:  To minimize  µ C , we design a policy to downscale the number of models, if more than N/2+1 models vote for the same classiﬁcation result. Algo- rithm  1  describes the overall design of the model selection policy  1a  . For every monitoring interval, we keep track of the accuracy obtained from predicting all input images within the interval.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "For every monitoring interval, we keep track of the accuracy obtained from predicting all input images within the interval. If the accuracy of the interval reaches the threshold accuracy (target + error_margin), we scale down the num- ber of available models in the ensemble. For consecutive sampling intervals, we calculate the  Mode  (most frequently occurring) of the majority vote received for every input.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "For consecutive sampling intervals, we calculate the  Mode  (most frequently occurring) of the majority vote received for every input. If the  Mode  is greater than needed votes  ⌊ N / 2 ⌋ +  1  we prune the models to  ⌊ N / 2 ⌋ + 1 . While down-scaling, we drop the models with the least prediction accuracy in that interval.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "While down-scaling, we drop the models with the least prediction accuracy in that interval. If there is a tie, we drop the model with least packing factor ( P f  ). It can so happen that dropping models can lead to drop in accuracy for certain intervals, because the class of images being predicted are different.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "It can so happen that dropping models can lead to drop in accuracy for certain intervals, because the class of images being predicted are different. In such cases, we up-size the models (one at a time) by adding most accurate model from the remaining unused models. Algorithm 1  Model Selection and Weighted Majority Voting \n1:  procedure  F ULL _E NSEMBLE (M ODEL L IST , SLO) 2: for  model  ∈ ModelList  do 3: if  model.latency  ≤ SLO.latency  then 4: Model.add(model) 5: end if 6: end for O 1 7:  end procedure 8:  procedure  D YNAMIC _M ODEL _S CALING ( Models ) 9: if  curr_accuracy  ≥ accuracy_threshold  then \n10: if  max vote  >   N \n2   + 1  then  O 2 \n11: to _ be _ dropped  ← max vote  − N \n2   + 1 12: Models .",
      "type": "sliding_window",
      "tokens": 230
    },
    {
      "text": "Algorithm 1  Model Selection and Weighted Majority Voting \n1:  procedure  F ULL _E NSEMBLE (M ODEL L IST , SLO) 2: for  model  ∈ ModelList  do 3: if  model.latency  ≤ SLO.latency  then 4: Model.add(model) 5: end if 6: end for O 1 7:  end procedure 8:  procedure  D YNAMIC _M ODEL _S CALING ( Models ) 9: if  curr_accuracy  ≥ accuracy_threshold  then \n10: if  max vote  >   N \n2   + 1  then  O 2 \n11: to _ be _ dropped  ← max vote  − N \n2   + 1 12: Models . drop ( to _ be _ dropped ) 13: end if 14: else 15: addModel  ← find _ models ( remaining _ models ) 16: Models . append ( addModel ) 17: end if 18:  end procedure 19:  procedure  W EIGHTED _V OTING ( Models ) 20: for  model in  ∀ Models  do 21: class  ← model .",
      "type": "sliding_window",
      "tokens": 266
    },
    {
      "text": "append ( addModel ) 17: end if 18:  end procedure 19:  procedure  W EIGHTED _V OTING ( Models ) 20: for  model in  ∀ Models  do 21: class  ← model . predicted _ class 22: weighted _ vote [ class ]+ =  weights [ model . class ] 23: end for 24: P class  ← max ( weighted _ vote , key  =  class ) 25: returnP class 26:  end procedure \n4.1.1 Class-based Weighted Majority Voting \nThe model selection policy described above ensures that we only use the necessary models in the majority voting.",
      "type": "sliding_window",
      "tokens": 145
    },
    {
      "text": "class ] 23: end for 24: P class  ← max ( weighted _ vote , key  =  class ) 25: returnP class 26:  end procedure \n4.1.1 Class-based Weighted Majority Voting \nThe model selection policy described above ensures that we only use the necessary models in the majority voting. In or- der to increase the accuracy of majority voting, we design a weighted majority voting policy  3  . The weight matrix is designed by considering the accuracy of each model for each class, giving us a weight matrix of  L × N  dimension, where  L is the number of unique labels and  N  is the number of models used in the ensemble.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "The weight matrix is designed by considering the accuracy of each model for each class, giving us a weight matrix of  L × N  dimension, where  L is the number of unique labels and  N  is the number of models used in the ensemble. The majority vote is calculated as a sum of model-weights for each unique class in the individual prediction of the ensemble. For instance, if there are 3 unique classes predicted by all the ensemble models, we sum the weights for all models of the same class.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "For instance, if there are 3 unique classes predicted by all the ensemble models, we sum the weights for all models of the same class. The class with the maximum weight ( P class ) is the output of the majority vote. Hence, classes that did not get the highest votes can still be the ﬁnal output if the models associated with that class has a higher weight, than the combined weights of highest voted class.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Hence, classes that did not get the highest votes can still be the ﬁnal output if the models associated with that class has a higher weight, than the combined weights of highest voted class. Unlike commonly used voting policies which assign weights based on overall correct predictions, our policy incor- porates class-wise information to the weights, thus making it more adaptable to different images classes. In order to determine the weight of every class, we use a per-class dictionary that keeps track of the correct predic- tions of every model per class.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "In order to determine the weight of every class, we use a per-class dictionary that keeps track of the correct predic- tions of every model per class. We populate the dictionary at runtime to avoid any inherent bias that could result from varying images over time. Similarly, our model selection pol- icy is also changed at runtime based on correct predictions seen during every interval.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Similarly, our model selection pol- icy is also changed at runtime based on correct predictions seen during every interval. An important concern in majority voting is tie-breaking. Ties occur when two sets of equal number of models predict a different result.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Ties occur when two sets of equal number of models predict a different result. The effectiveness \n1046    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nAlgorithm 2  Predictive Weighted Instance Auto Scaling \n1:  procedure  W EIGHTED _A UTOSCALING ( Stages ) 2: Predicted_load  ← DeepARN_Predict (load) 3: for  every Interval  do 4: for  model in  ∀ Models  do 5: model weight  ← get _ popularity ( model ) 6: Weight . append ( model weight ) 7: end for 8: end for 9: if  Predicted_load  ≥ Current_load  then 10: for  model in  ∀ Models  do 11: I_n  ← (Predicted_load - Current_load) × model weight 12: launch_workers ( est_VMs ) 13: model.workers.append ( est_VMs ) 14: end for 15: end if 16:  end procedure \nof weighted voting in breaking ties is discussed in Section  6 .",
      "type": "sliding_window",
      "tokens": 253
    },
    {
      "text": "append ( model weight ) 7: end for 8: end for 9: if  Predicted_load  ≥ Current_load  then 10: for  model in  ∀ Models  do 11: I_n  ← (Predicted_load - Current_load) × model weight 12: launch_workers ( est_VMs ) 13: model.workers.append ( est_VMs ) 14: end for 15: end if 16:  end procedure \nof weighted voting in breaking ties is discussed in Section  6 . 4.2 Resource Management \nBesides model selection, it is crucial to design an optimized resource provisioning and management scheme to host the models cost-effectively. We explain in detail the resource procurement and autoscaling policy employed in  Cocktail .",
      "type": "sliding_window",
      "tokens": 171
    },
    {
      "text": "We explain in detail the resource procurement and autoscaling policy employed in  Cocktail . 4.2.1 Resource Controller \nResource controller determines the cost-effective combina- tion of instances to be procured. We explain the details below.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "We explain the details below. Resource Types : We use both CPU and GPU instances  4a depending on the request arrival load. GPU instances are cost-effective when packed with a large batch of requests for execution.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "GPU instances are cost-effective when packed with a large batch of requests for execution. Hence, inspired from prior work [ 27 , 86 ], we de- sign an adaptive packing policy such that it takes into account the number of requests to schedule at time  T  and  P f  for every instance. The requests are sent to GPU instances only if the load matches the  P f  of the instance.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "The requests are sent to GPU instances only if the load matches the  P f  of the instance. Cost-aware Procurement : The cost of executing in a fully packed instance determines how expensive is each instance. Prior to scaling-up instances, we need to estimate the cost  4b of running them along with existing instances.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "Prior to scaling-up instances, we need to estimate the cost  4b of running them along with existing instances. At any given time  T , based on the predicted load ( L p ) and running instances R N , we use a cost-aware greedy policy to determine the num- ber of additional instances required to serve as  A n  =  L p  − C r , where  C r  =  ∑ N i = 1   P f i , is the request load which can be handled with  R N . To procure  A n  instances, we greedily calculate the least cost instance as  min ∀ i ∈ instances Cost i  × A n / P f i .",
      "type": "sliding_window",
      "tokens": 164
    },
    {
      "text": "To procure  A n  instances, we greedily calculate the least cost instance as  min ∀ i ∈ instances Cost i  × A n / P f i . Depend- ing on the cost-effectiveness ratio of  A n / P f i , GPUs will be preferred over CPU instances. Load Balancer : Apart from procuring instances, it is quintessential to design a load balancing and bin-packing  5 strategy to fully utilize all the provisioned instances.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Load Balancer : Apart from procuring instances, it is quintessential to design a load balancing and bin-packing  5 strategy to fully utilize all the provisioned instances. We maintain a request queue at every model pool. In order to increase the utilization of all instances in a pool at any given time, the load balancer submits every request from the queue to the lease remaining free slots (viz.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "In order to increase the utilization of all instances in a pool at any given time, the load balancer submits every request from the queue to the lease remaining free slots (viz. instance packing factor P f  ). This is similar to an online bin-packing algorithm.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "This is similar to an online bin-packing algorithm. We use an idle-timeout limit for 10 minutes to recycle unused \ninstances from every model pool. Hence, greedily assigning requests enables early scale down of lightly loaded instances.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "Hence, greedily assigning requests enables early scale down of lightly loaded instances. 4.2.2 Autoscaler \nAlong with resource procurement, we need to autoscale instances to satisfy the incoming query load. Though reactive policies (used in Clipper and InFaas) can be employed which take into account metrics like CPU utilization [ 83 ], these policies are slow to react when there is dynamism in request rates.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Though reactive policies (used in Clipper and InFaas) can be employed which take into account metrics like CPU utilization [ 83 ], these policies are slow to react when there is dynamism in request rates. Proactive policies with request prediction are know to have superior performance [ 86 ] and can co-exist with reactive policies. In  Cocktail , we use a load prediction model that can accurately forecast the anticipated load for a given time interval.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "In  Cocktail , we use a load prediction model that can accurately forecast the anticipated load for a given time interval. Using the predicted load  6a  ,  Cocktail  spawns additional instances, if necessary, for every instance pool. In addition, we sample SLO violations for every 10s interval and reactively spawn additional instances to every pool based on aggregate resource utilization of all instances.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "In addition, we sample SLO violations for every 10s interval and reactively spawn additional instances to every pool based on aggregate resource utilization of all instances. This captures SLO violations due to mis-predictions. Prediction Policy : To effectively capture the \nModel RMSE MWA 77.5 EWMA 88.25 Linear R. 87.5 Logsitic R. 78.34 Simple FF.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Prediction Policy : To effectively capture the \nModel RMSE MWA 77.5 EWMA 88.25 Linear R. 87.5 Logsitic R. 78.34 Simple FF. 45.45 LSTM 28.56 DeepArEst 26.67 \nTable 4:  Prediction models. different load arrival patterns, we design a DeepAR- estimator (DeepARest) based prediction model.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "different load arrival patterns, we design a DeepAR- estimator (DeepARest) based prediction model. We zeroed in on the choice of using DeepARest by conducting (Table  4 ) an in-depth com- parison of the accuracy loss when compared with other state-of-the-art traditional and ML-based prediction models used in prior works [ 47 ,  86 ]. As shown in Algorithm  2 , for every model under a periodic scheduling interval of 1 minute ( T s ), we use the  Predicted _load ( L p ) at time  T  +  T p  and compare it with the  current_load  to determine the number of instances ( I n ).",
      "type": "sliding_window",
      "tokens": 163
    },
    {
      "text": "As shown in Algorithm  2 , for every model under a periodic scheduling interval of 1 minute ( T s ), we use the  Predicted _load ( L p ) at time  T  +  T p  and compare it with the  current_load  to determine the number of instances ( I n ). T p  is deﬁned as the average launch time for new instances. ( T s ) is set to 1 minute as it is the typical instance provisioning time for EC2 VMs.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "( T s ) is set to 1 minute as it is the typical instance provisioning time for EC2 VMs. To calculate ( L p ), we sample the arrival rate in adjacent windows of size  W over the past S seconds. Using the global arrival rate from all windows, the model predicts ( L p ) for  T p  time units from  T .",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "Using the global arrival rate from all windows, the model predicts ( L p ) for  T p  time units from  T . T p  is set to 10 minutes because it is sufﬁcient time to capture the variations in long-term future. All these parameters are tunable based on the system needs.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "All these parameters are tunable based on the system needs. Importance Sampling:  An important concern in autoscaling is that the model selection policy dynamically determines the models in the ensemble for a given request constraints. Autoscaling the instances equally for every model based on predicted load, would inherently lead to over-provisioned instances for under-used models.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Autoscaling the instances equally for every model based on predicted load, would inherently lead to over-provisioned instances for under-used models. To address this concern, we design a weighted autoscaling policy which intelligently auto-scales instances for every pool based on the weights. As shown in Algorithm  2 , weights are determined by frequency in which a particular model is chosen for requests ( get_popularity ) with respect to other models in the ensemble.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "As shown in Algorithm  2 , weights are determined by frequency in which a particular model is chosen for requests ( get_popularity ) with respect to other models in the ensemble. USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1047 \nThe weights are multiplied with the predicted load to scale instances  (launch_workers)  for every model pool. We name this as an importance sampling  6b  technique, because the model pools are scaled proportional to their popularity.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "We name this as an importance sampling  6b  technique, because the model pools are scaled proportional to their popularity. 5 Implementation and Evaluation \nWe implemented a prototype of  Cocktail  and deployed it on AWS EC2 [ 5 ] platform The details of the implementation are described below. Cocktail is open-sourced at  https:// github.com/jashwantraj92/cocktail \n5.1 Cocktail Prototype Implementation \nCocktail  is implemented using 10KLOC of  Python .",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Cocktail is open-sourced at  https:// github.com/jashwantraj92/cocktail \n5.1 Cocktail Prototype Implementation \nCocktail  is implemented using 10KLOC of  Python . We de- signed  Cocktail  as a client-server architecture, where one master VM receives all the incoming requests which are sent to individual model worker VMs. Master-Worker Architecture : The master node handles the major tasks such as (i) concord model selection policy, (ii) request dispatch to workers VMs as asynchronous future tasks using  Python asyncio  library, and (iii) ensembling the pre- diction from the worker VMs.",
      "type": "sliding_window",
      "tokens": 156
    },
    {
      "text": "Master-Worker Architecture : The master node handles the major tasks such as (i) concord model selection policy, (ii) request dispatch to workers VMs as asynchronous future tasks using  Python asyncio  library, and (iii) ensembling the pre- diction from the worker VMs. Also all VM speciﬁc metrics such as current_load, CPU utilization, etc. reside in the mas- ter node.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "reside in the mas- ter node. It runs on a  C5.16x  [ 8 ] large instance to handle these large volume of diverse tasks. Each worker VMs runs a client process to serve its corresponding model.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "Each worker VMs runs a client process to serve its corresponding model. The requests are served as independent parallel threads to ensure timely predictions. We use  Python Sanic  web-server for commu- nication with the master and worker VMs.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "We use  Python Sanic  web-server for commu- nication with the master and worker VMs. Each worker VM runs tensorﬂow-serving [ 60 ] to serve the inference requests. Load Balancer : The master VMs runs a separate thread to monitor the importance sampling of all individual model pools.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "Load Balancer : The master VMs runs a separate thread to monitor the importance sampling of all individual model pools. It keeps track of the number of requests served per model in the past 5 minutes. This information is used for cal- culating the weights per model for autoscaling decisions.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "This information is used for cal- culating the weights per model for autoscaling decisions. We integrate a  mongodb  [ 21 ] database in the master node to main- tain all information about procured instances, spot-instance price list, and instance utilization. The load prediction model resides in the master VM which constantly records the arrival rate in adjacent windows.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "The load prediction model resides in the master VM which constantly records the arrival rate in adjacent windows. Recall that the details of the pre- diction were described in Section  4.2.2 . The DeepAREst [ 4 ] model was trained using  Keras  [ 22 ] and  Tensorflow , over 100 epochs with 2 layers, 32 neurons and a batch-size of 1.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "The DeepAREst [ 4 ] model was trained using  Keras  [ 22 ] and  Tensorflow , over 100 epochs with 2 layers, 32 neurons and a batch-size of 1. Model Cache : We keep track of the model selected for en- sembling on a per request constraint basis. The constraints are deﬁned as  <latency,accuracy>  pair.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "The constraints are deﬁned as  <latency,accuracy>  pair. The queries arriving with similar constraints can read the model cache to avoid re-computation for selecting the models. The model cache is implemented as a hash-map using  Redis  [ 16 ] in-memory key-value store for fast access.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "The model cache is implemented as a hash-map using  Redis  [ 16 ] in-memory key-value store for fast access. Constraint speciﬁcation : We expose a simple API to de- velopers, where they can specify the type of inference task (e.g., classiﬁcation) along with the  <latency,accuracy> constraints. Developers also need to indicate the primary ob- jective between these two constraints.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Developers also need to indicate the primary ob- jective between these two constraints. Cocktail  automatically \nDataset Application Classes Train-set Test-set ImageNet [ 29 ] Image 1000 1.2M 50K CIFAR-100 [ 50 ] Image 100 50K 10K SST-2 [ 72 ] Text 2 9.6K 1.8K SemEval [ 66 ] Text 3 50.3K 12.2K \nTable 5:  Benchmark Applications and datasets. chooses a set of single or ensemble models required to meet the developer speciﬁed constraints.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "chooses a set of single or ensemble models required to meet the developer speciﬁed constraints. Discussion:  Our accuracy and latency constraints are limited to the measurements from the available pretrained models. Note that changing the models or/and framework would lead to minor deviations.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Note that changing the models or/and framework would lead to minor deviations. While providing latency and top-1% ac- curacy of the pretrained models is an ofﬂine step in  Cocktail , we can calculate these values through one-time proﬁling and use them in the framework. All decisions related to VM au- toscaling, bin-packing and load-prediction are reliant on the centralized mongodb database, which can become a potential bottleneck in terms of scalability and consistency.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "All decisions related to VM au- toscaling, bin-packing and load-prediction are reliant on the centralized mongodb database, which can become a potential bottleneck in terms of scalability and consistency. This can be mitigated by using fast distributed solutions like Redis [ 16 ] and Zookeeper [ 46 ]. The DeepARest model is pre-trained using 60% of the arrival trace.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "The DeepARest model is pre-trained using 60% of the arrival trace. For varying load patterns, the model parameters can be updated by re-training in the background with new arrival rates. 5.2 Evaluation Methodology \nWe evaluate our prototype implementation on AWS EC2 [ 8 ] platforms.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "5.2 Evaluation Methodology \nWe evaluate our prototype implementation on AWS EC2 [ 8 ] platforms. Speciﬁcally, we use  C5.xlarge, 2xlarge, 4xlarge, 8xlarge  for CPU instances and  p2.xlarge  for GPU instances. Load Generator:  We use different traces which are given as input to the load generator.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Load Generator:  We use different traces which are given as input to the load generator. Firstly, we use real-world re- quest arrival traces from Wikipedia [ 76 ], which exhibit typical characteristics of ML inference workloads as it has recurring diurnal patterns. The second trace is production twitter [ 48 ] trace which is bursty with unexpected load spikes.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "The second trace is production twitter [ 48 ] trace which is bursty with unexpected load spikes. We use the ﬁrst 1 hour sample of both the traces and they are scaled to have an average request rate of 50 req/sec. Workload:  As shown in Table  5  we use image-classiﬁcation and Sentiment Analysis (text) applications with two datasets each for our evaluation.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "Workload:  As shown in Table  5  we use image-classiﬁcation and Sentiment Analysis (text) applications with two datasets each for our evaluation. Sentiment analysis outputs the sen- timent of a given sentence as positive negative and (or) neu- tral. We use 9 different prominently used text-classiﬁcation models from transformers library [ 81 ] (details available in appendix) designed using Google BERT [ 30 ] architecture trained on  SST  [ 72 ] and  SemEval  [ 66 ] dataset.",
      "type": "sliding_window",
      "tokens": 125
    },
    {
      "text": "We use 9 different prominently used text-classiﬁcation models from transformers library [ 81 ] (details available in appendix) designed using Google BERT [ 30 ] architecture trained on  SST  [ 72 ] and  SemEval  [ 66 ] dataset. Each request from the load-generator is modelled after a query with spe- ciﬁc  <latency,accuracy>  constraints. The queries consist of images or sentences, which are randomly picked from the test dataset.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "The queries consist of images or sentences, which are randomly picked from the test dataset. In our experiments, we use ﬁve different types of these constraints. As an example for the  Imagenet  dataset shown in Figure  6 , each constraint is a representative of <latency, accuracy> com- bination offered by single models (shown in Table  1 ).",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "As an example for the  Imagenet  dataset shown in Figure  6 , each constraint is a representative of <latency, accuracy> com- bination offered by single models (shown in Table  1 ). We use one constraint (blue dots) each from ﬁve different regions \n1048    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \n0 \n100 \n200 \n300 \n400 \n70 75 80 85 \nLatency (ms) \nAccuracy (%) \nConst1     Const2       Const3   Const4  Const5 \nFigure 6:  Constraints used in our workloads. (categorized by dotted lines) picked in the increasing order of accuracy.",
      "type": "sliding_window",
      "tokens": 149
    },
    {
      "text": "(categorized by dotted lines) picked in the increasing order of accuracy. Each of these picked constraints (named const1 - const5 in the Figure) represents a single baseline model, whose corresponding ensemble size ranges from small (2) to large (10), as shown in Table  3 . Note that the latency is the raw model execution latency, and does not include the addi- tional network-transfer overheads incurred.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "Note that the latency is the raw model execution latency, and does not include the addi- tional network-transfer overheads incurred. We picked the constraints using a similar procedure by ordering constraints across ﬁve different categories for  CIFAR-100 ,  SST-2  and SemEval  (twitter tweets) datasets. The list of models used for them are given in the Appendix.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "The list of models used for them are given in the Appendix. We model two different workload mixes by using a combination of these ﬁve query constraint types. Based on the decreasing order of accuracy, we categorize them into  Strict  and  Relaxed  workloads.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "Based on the decreasing order of accuracy, we categorize them into  Strict  and  Relaxed  workloads. 5.2.1 Evaluation Metrics \nMost of our evaluations of  Cocktail  for image-classiﬁcation are performed using the  Imagenet  dataset. To further demon- strate the sensitivity of Cocktail to dataset and applicability to other classiﬁcation applications, we also evaluate it us- ing  CIFAR-100  and Sentiment-Analysis application.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "To further demon- strate the sensitivity of Cocktail to dataset and applicability to other classiﬁcation applications, we also evaluate it us- ing  CIFAR-100  and Sentiment-Analysis application. We use three important metrics: response latency, cost and accuracy for evaluating and comparing our design to other state-of- the-art systems. The response latency metric includes model inference latency, communication/network latency and syn- chronization overheads.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "The response latency metric includes model inference latency, communication/network latency and syn- chronization overheads. Queries that do not meet response latency requirements (>700ms) are considered as SLO vio- lations. The cost metric is the billing cost from AWS, and the accuracy metric is measured as the percentage of requests that meet the target accuracy requirements.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "The cost metric is the billing cost from AWS, and the accuracy metric is measured as the percentage of requests that meet the target accuracy requirements. We compare these metrics for  Cocktail  against (i)  In- Faas  [ 83 ], which is our baseline that employs single model selection policy; (ii)  Clipper  [ 27 ], which uses static full model selection policy (analogous to AWS AutoGluon); and (iii) Clipper-X  which is an enhancement to  Clipper  with a simple model selection (drop one model at a time) that does not uti- lize the  mode -based policy enforced in  Cocktail . Both  InFaas and  Clipper  share  Cocktail ’s implementation setup to ensure a fair comparison with respect to our design and execution environment.",
      "type": "sliding_window",
      "tokens": 179
    },
    {
      "text": "Both  InFaas and  Clipper  share  Cocktail ’s implementation setup to ensure a fair comparison with respect to our design and execution environment. For instance, both  Clipper  and  InFaas  employ variants of a reactive autoscaler as described in Section  4.2.2 . However, in our setup, both beneﬁt from the distributed au- toscaling and prediction policies, thus eliminating variability.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "However, in our setup, both beneﬁt from the distributed au- toscaling and prediction policies, thus eliminating variability. Also note that  InFaas  is deployed using OnDemand instances, while both  Clipper  and  Cocktail  use spot instances. 6 Analysis of Results \nThis section discusses the experimental results of  Cocktail using the Wiki and Twitter traces.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "6 Analysis of Results \nThis section discusses the experimental results of  Cocktail using the Wiki and Twitter traces. To summarize the overall results, Cocktail providing 2 ×  reduction in latency, while meeting the accuracy for up-to 96% of the requests under reduced deployment cost by 1.4 × , when compared to  InFaaS and  Clipper . 6.1 Latency, Accuracy and Cost Reduction \nLatency Distribution : Figure  7  shows the distribution of to- tal response latency in a standard box-and-whisker plot.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "6.1 Latency, Accuracy and Cost Reduction \nLatency Distribution : Figure  7  shows the distribution of to- tal response latency in a standard box-and-whisker plot. The boundaries of the box-plots depict the 1st quartile (25th per- centile (PCTL)) and 3rd quartile (75th PCTL), the whiskers plot the minimum and maximum (tail) latency and the middle line inside the box depict the median (50 PCTL). The total response latency includes additional 200-300ms incurred for query serialization and data transfer over network.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "The total response latency includes additional 200-300ms incurred for query serialization and data transfer over network. It can be seen that the maximum latency of  Cocktail  is similar to the 75th PCTL latency of  InFaas . This is because the single model inference have up to 2x higher latency to achieve higher accuracy.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "This is because the single model inference have up to 2x higher latency to achieve higher accuracy. Consequently, this leads to 35% SLO violations for  InFaas  in the case of Strict workload. In contrast, both Cocktail  and  Clipper  can reach the accuracy at lower latency due to ensembling, thus minimizing SLO violations to 1%.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "In contrast, both Cocktail  and  Clipper  can reach the accuracy at lower latency due to ensembling, thus minimizing SLO violations to 1%. Also, the tail latency is higher for Twitter trace (Figure  7c , 7d ) owing to its bursty nature. Note that the tail latency of  Clipper  is still higher than  Cocktail  because  Clipper ensembles more models than  Cocktail , thereby resulting in straggler tasks in the VMs.",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "Note that the tail latency of  Clipper  is still higher than  Cocktail  because  Clipper ensembles more models than  Cocktail , thereby resulting in straggler tasks in the VMs. The difference in latency between Cocktail  and  InFaas  is lower for  Relaxed  workload when compared to  Strict  workload (20% lower in tail). Since the Relaxed  workload has much lower accuracy constraints, smaller models are able to singularly achieve the accuracy requirements at lower latency.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Since the Relaxed  workload has much lower accuracy constraints, smaller models are able to singularly achieve the accuracy requirements at lower latency. Accuracy violations : The accuracy is mea- sured as a moving window average with size 200 for all the requests in the workload. Accuracy Met (%) Scheme Strict Relaxed InFaas 21 71 Clipper 47 89 Cocktail 56 96 \nTable 6:  Requests meeting target accuracy averaged for both Trace.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "Accuracy Met (%) Scheme Strict Relaxed InFaas 21 71 Clipper 47 89 Cocktail 56 96 \nTable 6:  Requests meeting target accuracy averaged for both Trace. Both  Clipper  and  Cock- tail  can meet the ac- curacy for 56% of re- quests, which is 26% and 9% more than  In- Faas  and  Clipper  re- spectively. This is be- cause, intuitively ensembling leads to higher accuracy than single models.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "This is be- cause, intuitively ensembling leads to higher accuracy than single models. However,  Cocktail  is still 9% better than  Clip- per  because the class-based weighted voting, is efﬁcient in breaking ties when compared to weighting averaging used in Clipper . Since majority voting can include ties in votes, we analyzed the number of ties, which were correctly predicted for all the queries.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Since majority voting can include ties in votes, we analyzed the number of ties, which were correctly predicted for all the queries. Cocktail  was able to deliver correct predic- tions for 35% of the tied votes, whereas breaking the ties in Clipper  led only to 20% correct predictions. USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1049 \nInFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1049 \nInFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp. Latency (ms) \n(a)  Wiki-trace:  Strict  workload. InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp. Latency (ms) \n(b)  Wiki-trace:  Relaxed  workload. InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nLatency \n(c)  Twitter-trace:  Strict  workload.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nLatency \n(c)  Twitter-trace:  Strict  workload. InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp. Latency (ms) \n(d)  Twitter-trace:  Relaxed  workload.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "Latency (ms) \n(d)  Twitter-trace:  Relaxed  workload. Figure 7:  Latency Distribution of  InFaas ,  Clipper  and  Cocktail  for two workload mixes using both Wiki and Twitter traces. Strict Relaxed 0 \n20 \n40 \n60 \n80 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(a)  Wiki Trace.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "Strict Relaxed 0 \n20 \n40 \n60 \n80 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(a)  Wiki Trace. Strict Relaxed 0 \n50 \n100 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace. Figure 8:  Cost savings of  Cocktail  compared to three schemes.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Figure 8:  Cost savings of  Cocktail  compared to three schemes. Const1 Const2 Const3 Const4 Query \n0 \n5 \n10 \n#Models \nClipper Clipper-X Cocktail \n(a)  Average number of models used in the ensemble. IRV2 \nDNet201 \nNASMob \nDNet121 \nXcep \nMNet \nIncep \nMNetV2 \nRNet50V2 \nRNet50 \nModel \n0 \n50 \n100 \nImportance(%) \n(b)  Distribution of requests served by each individual model.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "IRV2 \nDNet201 \nNASMob \nDNet121 \nXcep \nMNet \nIncep \nMNetV2 \nRNet50V2 \nRNet50 \nModel \n0 \n50 \n100 \nImportance(%) \n(b)  Distribution of requests served by each individual model. Figure 9:  Beneﬁts of dynamic model selection policy. Note that, changing the target accuracy to tolerate a 0.5% loss, increases the percentage of requests that meet accuracy to 81% for  Cocktail , when compared to 61% for  InFaas .",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "Note that, changing the target accuracy to tolerate a 0.5% loss, increases the percentage of requests that meet accuracy to 81% for  Cocktail , when compared to 61% for  InFaas . The requests meeting accuracy are generally higher for the Relaxed  workload because the target accuracy is much lower. Overall,  Cocktail  was able to deliver an accuracy of 83% and 79.5% on average for the  Strict  and  Relaxed  workloads, respectively.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Overall,  Cocktail  was able to deliver an accuracy of 83% and 79.5% on average for the  Strict  and  Relaxed  workloads, respectively. This translates to 1.5% and 1% better accuracy than  Clipper  and  InFaas . We do not plot the results for Clipper-X , which achieves similar accuracy to  Cocktail , but uses more models as explained in Section  6.2.1 .",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "We do not plot the results for Clipper-X , which achieves similar accuracy to  Cocktail , but uses more models as explained in Section  6.2.1 . Cost Comparison:  Figure  8  plots the cost savings of Cocktail  when compared to  InFaas ,  Clipper  and  Clipper-X policies. It can be seen that,  Cocktail  is up to 1.45 ×  more cost effective than  InFaas  for  Strict  workload.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "It can be seen that,  Cocktail  is up to 1.45 ×  more cost effective than  InFaas  for  Strict  workload. In addition, Cocktail  reduces cost by 1.35 ×  and 1.27 ×  compared to Clipper  and  Clipper-X  policies, owing to its dynamic model selection policy, which minimizes the resource footprint of ensembling. On the other hand,  Clipper  uses all models in ensemble and the  Clipper-X  policy does not right size the models as aggressively as  Clipper , hence they are more expensive.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "On the other hand,  Clipper  uses all models in ensemble and the  Clipper-X  policy does not right size the models as aggressively as  Clipper , hence they are more expensive. Note that, all the schemes incur higher cost for twitter trace (Figure  8b ) compared to wiki trace (Figure  8a ). This is because the twitter workload is bursty, thereby leading to intermittent over-provisioned VMs.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "This is because the twitter workload is bursty, thereby leading to intermittent over-provisioned VMs. 6.2 Key Sources of Improvements \nThe major improvements in terms of cost, latency, and accu- \nracy in  Cocktail  are explained below. For brevity in explana- tion, the results are averaged across Wiki and Twitter traces for strict workload.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "For brevity in explana- tion, the results are averaged across Wiki and Twitter traces for strict workload. 6.2.1 Beneﬁts from dynamic model selection \nFigure  9a  plots the average number of models used for queries falling under the ﬁrst four different constraint (const) types. Here,  Cocktail  reduces the number of models by up to 55% for all four query types.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "Here,  Cocktail  reduces the number of models by up to 55% for all four query types. This is because our dynamic pol- icy ensures that the number of models are well within  N / 2 most of the time, whereas the  Clipper-X  policy does not ag- gressively scale down models. Clipper , on the other hand, is static and always uses all the models.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "Clipper , on the other hand, is static and always uses all the models. The percentage of model-reduction is lower for  Const2 , 3 and 4 because, the total models used in the ensemble is less than  Const1  (8, 7 and 6 models, respectively). Still, the savings in terms of cost will be signiﬁcant because even removing one model from the ensemble amounts to  ∼ 20% cost savings in the long run ( Clipper  vs  Clipper-X  ensemble in Figure  8 ).",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Still, the savings in terms of cost will be signiﬁcant because even removing one model from the ensemble amounts to  ∼ 20% cost savings in the long run ( Clipper  vs  Clipper-X  ensemble in Figure  8 ). Thus, the beneﬁts of  Cocktail  are substantial for large ensembles while reducing the number of models for medium-sized ensembles. Figure  9b  shows the breakdown of the percentage of re- quests ( Const1 ) served by the each model.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "Figure  9b  shows the breakdown of the percentage of re- quests ( Const1 ) served by the each model. As seen, Incep- tionResNetV2, Densenet-201, Densenet121, NasnetMobile and Xception are the top-5 most used models in the ensem- ble. Based on Table  1 , if we had statically taken the top  N / 2 most accurate models, NasNetmobile would not have been included in the ensemble.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "Based on Table  1 , if we had statically taken the top  N / 2 most accurate models, NasNetmobile would not have been included in the ensemble. However, based on the input im- ages sent in each query, our model selection policy has been able to identify NasNetMobile to be a signiﬁcantly contribut- ing model in the ensemble. Further, the other 5 models are used by up to 25% of the images.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Further, the other 5 models are used by up to 25% of the images. Not including them in the ensemble would have led to severe loss in accuracy. But, our dynamic policy with the class-based weighted voting, adapts to input images in a given interval by accurately selecting the best performing model for each class.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "But, our dynamic policy with the class-based weighted voting, adapts to input images in a given interval by accurately selecting the best performing model for each class. To further demonstrate the effectiveness of our dynamic model selection, Figure  10b , 10c  plots the number models in every sampling interval along with cumulative accuracy and window accuracy within each sampling interval for three schemes. We observe that  Cocktail  can effectively scale up and scale down the mod- els while maintaining the cumulative accuracy well within the threshold.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "We observe that  Cocktail  can effectively scale up and scale down the mod- els while maintaining the cumulative accuracy well within the threshold. More than 50% of the time the number of models are maintained between 4 to 5, because the dynamic policy is quick in detecting accuracy failures and recovers immediately \n1050    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \n(a)  Clipper \n(b)  Clipper-X \n(c)  Cocktail \nFigure 10:  Figures (a), (b) and (c) shows the number of models used in ensemble with corresponding cumulative accuracy and window accuracy over a 1 hour period for requests under  Const1 . Figure (d) shows the effects of distributed autoscaling with importance sampling.",
      "type": "sliding_window",
      "tokens": 160
    },
    {
      "text": "Figure (d) shows the effects of distributed autoscaling with importance sampling. Strict Relaxed 0 \n25 \n50 \n75 \n#VMs \nInFaas Clipper Clipper-X Cocktail \n(a)  Wiki Trace. Strict Relaxed 0 \n25 \n50 \n75 \n#VMs \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "Strict Relaxed 0 \n25 \n50 \n75 \n#VMs \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace. Figure 11:  Number of VMs spawned for all four schemes. 0 1000 2000 3000 Time interval (10s) \n0 \n50 \n100 \n#VMs \nBline model1 model2 model3 \n(a)  Cumulative #VMs.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "0 1000 2000 3000 Time interval (10s) \n0 \n50 \n100 \n#VMs \nBline model1 model2 model3 \n(a)  Cumulative #VMs. 0 25 50 75 Time interval (10s) \n79 \n80 \n81 \n82 \nAccuracy \nBL1 BL2 \nBL3 const1 \nconst2 const3 \n(b)  Failure Analysis. Figure 12:  Sensitivity analysis of VMs.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Figure 12:  Sensitivity analysis of VMs. by scaling up models. However,  Clipper-X  does not scale down models as frequently as  Cocktail , while ensuring similar accuracy.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "However,  Clipper-X  does not scale down models as frequently as  Cocktail , while ensuring similar accuracy. Clipper  is less accurate than  Cocktail  and further it uses all 10 models throughout. 6.2.2 Beneﬁts from Autoscaling \nFigure  11  plots the reduction in the number of VMs used by all four schemes.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "6.2.2 Beneﬁts from Autoscaling \nFigure  11  plots the reduction in the number of VMs used by all four schemes. It can be seen that both  Cocktail  and  Clipper-X spawn 49% and 20% fewer VMs than  Clipper  for workload-1 on Twitter trace. Cocktail  spawns 29% lesser VMs on top of Clipper-X , because it is not aggressive enough like  Cocktail to downscale more models at every interval.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "Cocktail  spawns 29% lesser VMs on top of Clipper-X , because it is not aggressive enough like  Cocktail to downscale more models at every interval. It is to be noted that the savings are lower for  Relaxed  workload because, the number of models in the ensemble are inherently low, thus leading to reduced beneﬁts from scaling down the models. Intuitively,  InFaas  has the least number of VMs spawned because it does not ensemble models.",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "Intuitively,  InFaas  has the least number of VMs spawned because it does not ensemble models. Cocktail  spawns upto 50% more VMs than  InFaas , but in turns reduces accuracy loss by up to 96%. To further capture the beneﬁts of the weighted autoscal- ing policy, Figure  12a  plots the number of VMs spawned over time for the top-3 most used models in the ensemble for  Const1 .",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "To further capture the beneﬁts of the weighted autoscal- ing policy, Figure  12a  plots the number of VMs spawned over time for the top-3 most used models in the ensemble for  Const1 . The Bline denotes number of VMs that would be spawned without applying the weights. Not adopting an importance sampling based weighted policy would result in equivalent number of VMs as the Bline for all models.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Not adopting an importance sampling based weighted policy would result in equivalent number of VMs as the Bline for all models. How- ever, since  Cocktail  exploits importance sampling by keeping track of the frequency in which models are selected, the num- \nber of VMs spawned for model1, model2 and model-3 is upto 3 ×  times lesser than uniform scaling. Figure  9b  shows the most used models in decreasing order of importance.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Figure  9b  shows the most used models in decreasing order of importance. The au- toscaling policy effectively utilizes this importance factor in regular intervals of 5 minutes. Despite using multiple models for a single inference, importance sampling combined with aggressive model pruning, greatly reduces the resource foot- print which directly translates to the cost savings in  Cocktail .",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Despite using multiple models for a single inference, importance sampling combined with aggressive model pruning, greatly reduces the resource foot- print which directly translates to the cost savings in  Cocktail . 6.2.3 Beneﬁts of Transient VMs \nThe cost-reductions in  Cocktail  are akin to cost-savings of transient VMs compared to On-Demand (OD) VMs. We pro- ﬁle the spot price of 4 types of  C5  EC2 VMs over a 2-week period in August 2020.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "We pro- ﬁle the spot price of 4 types of  C5  EC2 VMs over a 2-week period in August 2020. It was seen that, the spot instance prices have predictable ﬂuctuations. When compared to the OD price , they were up to 70% cheaper.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "When compared to the OD price , they were up to 70% cheaper. This price gap is cap- italized in  Cocktail  to reduce the cost of instances consumed by ensembling. Note that, we set the bidding price conser- vatively to 40% of OD.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "Note that, we set the bidding price conser- vatively to 40% of OD. Although,  Cocktail  spawns about 50% more VMs than  InFaas , the high  P f  of small models and spot-instance price reductions combined with autoscaling policies lead to the overall 30-40% cost savings. 6.3 Sensitivity Analysis \nIn this section, we analyze the sensitivity of  Cocktail  with respect to various design choices which include (i) sampling interval of the accuracy measurements, (ii) spot-instance fail- ure rate and (iii) type of datasets and applications.",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "6.3 Sensitivity Analysis \nIn this section, we analyze the sensitivity of  Cocktail  with respect to various design choices which include (i) sampling interval of the accuracy measurements, (ii) spot-instance fail- ure rate and (iii) type of datasets and applications. 6.3.1 Sampling Interval \nTo study the sensitivity with respect to the sampling interval for measure accuracy loss/gain, we use four different intervals of 10s, 30s, 60s and 120s. Figure  13  plots the average number of models (bar- left y-axis) and cumulative accuracy (line- right y-axis) for the different sampling intervals for queries with three different constraints.",
      "type": "sliding_window",
      "tokens": 152
    },
    {
      "text": "Figure  13  plots the average number of models (bar- left y-axis) and cumulative accuracy (line- right y-axis) for the different sampling intervals for queries with three different constraints. It can be seen that the 30s interval strikes the right balance with less than 0.2% loss in accuracy and has average number models much lesser than other intervals. This is because, increasing the interval leads to lower number of scale down operations, thus resulting in a bigger ensemble.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "This is because, increasing the interval leads to lower number of scale down operations, thus resulting in a bigger ensemble. As a result, the 120s interval has the highest number of models. USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1051 \n10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n82.25 \n82.50 \n82.75 \nAccuracy \n(a)  Queries under Constraint-1.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1051 \n10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n82.25 \n82.50 \n82.75 \nAccuracy \n(a)  Queries under Constraint-1. 10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n81.0 \n81.2 \n81.4 \nAccuracy \n(b)  Queries under Constraint-2. 10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n79.0 \n79.2 \n79.4 \nAccuracy \n(c)  Queries under Constraint-3.",
      "type": "sliding_window",
      "tokens": 150
    },
    {
      "text": "10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n79.0 \n79.2 \n79.4 \nAccuracy \n(c)  Queries under Constraint-3. Figure 13:  Sensitivity analysis of model selection with respect to sampling interval. The average number of models is in primary axis and cumulative accuracy in secondary axis.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "The average number of models is in primary axis and cumulative accuracy in secondary axis. 6.3.2 Cocktail Failure Resilience \nWe use spot instances to host models in  Cocktail . As previ- ously discussed in Section  3 , spot instances interruptions can lead to intermittent loss in accuracy as certain models will be unavailable in the ensemble.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "As previ- ously discussed in Section  3 , spot instances interruptions can lead to intermittent loss in accuracy as certain models will be unavailable in the ensemble. However for large ensembles (5 models are more), the intermittent accuracy loss is very low. Figure  12b  plots the failure analysis results for top three constraints by comparing the ensemble accuracy to the target accuracy.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "Figure  12b  plots the failure analysis results for top three constraints by comparing the ensemble accuracy to the target accuracy. The desired accuracy for all three constraints are plotted as BL1, BL2 and BL3. We induce failures in the in- stances using  chaosmonkey  [ 19 ] tool with a 20% failure proba- bility.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "We induce failures in the in- stances using  chaosmonkey  [ 19 ] tool with a 20% failure proba- bility. It can be seen that queries in all three constraints suffer an intermittent loss in accuracy of 0.6% between the time period 240 s  and 800 s . Beyond 800 s , they quickly recover back to the required accuracy because additional instances are spawned in place of failed instances.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Beyond 800 s , they quickly recover back to the required accuracy because additional instances are spawned in place of failed instances. However, in the case of InFaas , this would lead to 1% failed requests due to requests being dropped from the failed instances. An alternate solution would be to restart the queries in running instances but that leads to increased latencies for the 1% requests.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "An alternate solution would be to restart the queries in running instances but that leads to increased latencies for the 1% requests. In contrast,  Cocktail  incurs a modest accuracy loss of well within 0.6% and quickly adapts to reach the target accuracy. Thus,  Cocktail  is inherently fault-tolerant owing to the parallel nature in computing multiple inferences for a single request.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "Thus,  Cocktail  is inherently fault-tolerant owing to the parallel nature in computing multiple inferences for a single request. We observe similar accuracy loss or lower for different probability failures of 5%, 10% and 25%, respectively (results/charts omitted in the interest of space). Discussion:  For applications that are latency tolerant, we can potentially redirect requests from failed instances to existing instances, which would lead to increased tail latency.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Discussion:  For applications that are latency tolerant, we can potentially redirect requests from failed instances to existing instances, which would lead to increased tail latency. The results we how are only for latency intolerant applications. Note that, the ensembles used in our experiments are at-least 4 models or more.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "Note that, the ensembles used in our experiments are at-least 4 models or more. For smaller ensembles, instance failures might lead to higher accuracy loss, but in our experiments, single models typically satisfy their constraints. 6.3.3 Sensitivity to Constraints \nFigure  14  plots the sensitivity of model selection policy un- der a wide-range of latency and accuracy constraints.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "6.3.3 Sensitivity to Constraints \nFigure  14  plots the sensitivity of model selection policy un- der a wide-range of latency and accuracy constraints. In Figure  14a , we vary the latency under six different constant accuracy categories. It can be seen that for ﬁxed accuracy of 72%, 78% and 80%, the average number of models increase with increase in latency, but drops to 1 for the highest latency.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "It can be seen that for ﬁxed accuracy of 72%, 78% and 80%, the average number of models increase with increase in latency, but drops to 1 for the highest latency. Intuitively, singe large models with higher latency can satisfy \n0 2 4 6 8 10 \n0 \n100 \n200 \n300 \n400 \n72 78 80 81.5 83.5 85 \nAvegae #Models \nLatency (ms) \nAccuracy (%) \nLatency Average #Models \n(a)  Fixed Accuracy. 0 \n2 \n4 \n6 \n8 \n60 \n70 \n80 \n90 \n60 70 100 120 150 350 \nAverage #Models \nAccuracy (%) \nLatency (ms) \naccuracy Average #Model \n(b)  Fixed Latency.",
      "type": "sliding_window",
      "tokens": 153
    },
    {
      "text": "0 \n2 \n4 \n6 \n8 \n60 \n70 \n80 \n90 \n60 70 100 120 150 350 \nAverage #Models \nAccuracy (%) \nLatency (ms) \naccuracy Average #Model \n(b)  Fixed Latency. Figure 14:  Sensitivity Constraints under ﬁxed latency and accuracy. Bar graphs (latency) plotted using primary y-axis and line graph (#models) plotted using secondary y-axis.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "Bar graphs (latency) plotted using primary y-axis and line graph (#models) plotted using secondary y-axis. Const1 Const2 Const3 Const4 Query \n0 \n5 \n10 \n#Models \nClipper Clipper-X Cocktail \n(a)  Image Classiﬁcation-Cifar-100. Const1 Const2 Const3 Const4 Query \n0.0 \n2.5 \n5.0 \n7.5 \n#Models \nClipper Clipper-X Cocktail \n(b)  Sentiment analysis.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "Const1 Const2 Const3 Const4 Query \n0.0 \n2.5 \n5.0 \n7.5 \n#Models \nClipper Clipper-X Cocktail \n(b)  Sentiment analysis. Figure 15:  Average number of models used in the ensemble. the accuracy, while short latency models need to be ensem- bled to reach the same accuracy.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "the accuracy, while short latency models need to be ensem- bled to reach the same accuracy. For accuracy greater than 80%, the ensemble size drops with higher latencies. This is because the models which offer higher accuracy are typically dense and hence, smaller ensembles are sufﬁcient.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "This is because the models which offer higher accuracy are typically dense and hence, smaller ensembles are sufﬁcient. In Fig- ure  14b , we vary the accuracy under six different constant latency categories. It can be seen that for higher accuracies, Cocktail  tries to ensemble more models to reach the accuracy, while for lower accuracy it resorts to using single models.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "It can be seen that for higher accuracies, Cocktail  tries to ensemble more models to reach the accuracy, while for lower accuracy it resorts to using single models. 6.3.4 Sensitivity to Dataset \nTo demonstrate the applicability of  Cocktail  to multiple datasets, we conducted similar experiments as elucidated in Section  5.2.1  using the  CIFAR-100  dataset [ 50 ]. It comprises of 100 distinct image classes and we trained 11 different models including the nine that are common from Table  1 .",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "It comprises of 100 distinct image classes and we trained 11 different models including the nine that are common from Table  1 . Fig- ure  15a  plots the average number of models used by the three policies for the top four constraints. It can be seen that  Cock- tail  shows similar reduction (as Imagenet) while using only 4.4 models on average.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "It can be seen that  Cock- tail  shows similar reduction (as Imagenet) while using only 4.4 models on average. As expected,  Clipper  and  Clipper-X use more models than  Cocktail  (11 and 5.4, respectively) due to non-aggressive scaling down of the models used. Figure  16a  plots the latency reduction and accuracy boost when compared to  InFaaS  (baseline).",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Figure  16a  plots the latency reduction and accuracy boost when compared to  InFaaS  (baseline). While able to reduce 60% of the models used in the ensemble,  Cocktail  also re- duces latency by up to 50% and boosts accuracy by up to 1.2%. Cocktail  was also able to deliver modest accuracy gain \n1052    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nConst1 Const2 Const3 Const4 \nBaseline \n0 \n20 \n40 \nLatency-reduction \n0.50 \n0.75 \n1.00 \nAccuracy-Gain \n(a)  Image Classiﬁcation:Cifar100.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "Cocktail  was also able to deliver modest accuracy gain \n1052    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nConst1 Const2 Const3 Const4 \nBaseline \n0 \n20 \n40 \nLatency-reduction \n0.50 \n0.75 \n1.00 \nAccuracy-Gain \n(a)  Image Classiﬁcation:Cifar100. Const1 Const2 Const3 Const4 \nBaseline \n0 \n10 \n20 \n30 \nLatency-reduction \n0.6 \n0.8 \n1.0 \n1.2 \nAccuracy-Gain \n(b)  Sentiment Analysis. Figure 16:  Latency reduction (%) plotted as bar graph(primary y- axis) and accuracy gains (%) plotted as line graph (secondary y-axis) over InFaaS.",
      "type": "sliding_window",
      "tokens": 178
    },
    {
      "text": "Figure 16:  Latency reduction (%) plotted as bar graph(primary y- axis) and accuracy gains (%) plotted as line graph (secondary y-axis) over InFaaS. Strict Relaxed 0 \n20 \n40 \n60 \n80 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(a)  Wiki Trace. Strict Relaxed 0 \n25 \n50 \n75 \n100 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Strict Relaxed 0 \n25 \n50 \n75 \n100 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace. Figure 17:  Cost savings of Cocktail for Sentiment Analysis. of 0.5% than  Clipper  (not plotted).",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "of 0.5% than  Clipper  (not plotted). The accuracy gain seen in  CIFAR-100  is lesser than ImageNet dataset because the class-based weighted voting works effectively when handling large number of classes (100 in  CIFAR  vs 1000 in ImageNet). Nevertheless,  Cocktail  is able to deliver the accuracy at 2x lower latency than  InFaaS  and 1.35x lower cost than Clipper.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Nevertheless,  Cocktail  is able to deliver the accuracy at 2x lower latency than  InFaaS  and 1.35x lower cost than Clipper. 6.4 General Applicability of Cocktail \nTo demonstrate the general applicability of  Cocktail  to other classiﬁcation tasks, we evaluated  Cocktail  using a Sentiment Analysis application for two datasets. The results reported are averaged across both the datasets.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "The results reported are averaged across both the datasets. Figure  15b  plots the average number of models used by the three policies for the top four constraints. As shown for  Const1 ,  Cocktail  shows similar reduction (as image-classiﬁcation) with only using 4.8 models on average, which is 40% and 26% lower than Clipper  and  Clipper-X , respectively.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "As shown for  Const1 ,  Cocktail  shows similar reduction (as image-classiﬁcation) with only using 4.8 models on average, which is 40% and 26% lower than Clipper  and  Clipper-X , respectively. Cocktail  is also able to reduce the number of models by 30% and 50% for medium ensembles ( Const2  &  Const3 ) as well. Figure  16b  plots the latency reduction and accuracy gain, compared to  InFaaS  (baseline).",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "Figure  16b  plots the latency reduction and accuracy gain, compared to  InFaaS  (baseline). While being able to reduce 50% of the models used in the ensemble,  Cocktail  also re- duces latency by up to 50% and improves accuracy by up to 1.3%. Both  Cocktail  and  Clipper  deliver the same overall accuracy (96%, 94.5%, 93.5%, and 92%)).",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Both  Cocktail  and  Clipper  deliver the same overall accuracy (96%, 94.5%, 93.5%, and 92%)). Since sentiment analysis only has 2-3 classes, there are no additional accuracy gains by using the class-based weighted voting. However, the model selection policy effectively switches between differ- ent models based on the structure of input text (equivalent to classes in images).",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "However, the model selection policy effectively switches between differ- ent models based on the structure of input text (equivalent to classes in images). For instance, complex sentences are more accurately classiﬁed by denser models compared to smaller. Despite the lower accuracy gains,  Cocktail  is able to reduce the cost (Figure  17 ) of model-serving by 1.45 ×  and 1.37 × for Wiki trace compared to  InFaaS  and  Clipper , respectively.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Despite the lower accuracy gains,  Cocktail  is able to reduce the cost (Figure  17 ) of model-serving by 1.45 ×  and 1.37 × for Wiki trace compared to  InFaaS  and  Clipper , respectively. 7 Concluding Remarks \nThere is an imminent need to develop model serving systems that can deliver highly accurate, low latency predictions at re- duced cost. In this paper, we propose and evaluate  Cocktail , a cost-effective model serving system that exploits ensembling techniques to meet high accuracy under low latency goals.",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "In this paper, we propose and evaluate  Cocktail , a cost-effective model serving system that exploits ensembling techniques to meet high accuracy under low latency goals. In  Cocktail , we adopt a three-fold approach to reduce the resource footprint of model ensembling. More speciﬁcally, we (i) develop a novel dynamic model selection, (ii) design a prudent resource management scheme that utilizes weighted autoscaling for efﬁcient resource allocation, and (iii) lever- age transient VM instances to reduce the deployment costs.",
      "type": "sliding_window",
      "tokens": 125
    },
    {
      "text": "More speciﬁcally, we (i) develop a novel dynamic model selection, (ii) design a prudent resource management scheme that utilizes weighted autoscaling for efﬁcient resource allocation, and (iii) lever- age transient VM instances to reduce the deployment costs. Our results from extensive evaluations using both CPU and GPU instances on AWS EC2 cloud platform demonstrate that Cocktail  can reduce deployment cost by 1.4 × , while reducing latency by 2 ×  and satisfying accuracy for 96% of requests, compared to the state-of-the-art model-serving systems. Acknowledgments \nWe are indebted to our shepherd Manya Ghobadi, the anony- mous reviewers and Anup Sarma for their insightful com- ments to improve the clarity of the presentation.",
      "type": "sliding_window",
      "tokens": 183
    },
    {
      "text": "Acknowledgments \nWe are indebted to our shepherd Manya Ghobadi, the anony- mous reviewers and Anup Sarma for their insightful com- ments to improve the clarity of the presentation. Special mention to Nachiappan Chidambaram N. for his intellec- tual contributions. This research was partially supported by NSF grants #1931531, #1955815, #1763681, #1908793, #1526750, #2116962, #2122155, #2028929 ,and we thank NSF Chameleon Cloud project CH-819640 for their generous compute grant.",
      "type": "sliding_window",
      "tokens": 145
    },
    {
      "text": "This research was partially supported by NSF grants #1931531, #1955815, #1763681, #1908793, #1526750, #2116962, #2122155, #2028929 ,and we thank NSF Chameleon Cloud project CH-819640 for their generous compute grant. All product names used in this publication are for identiﬁcation purposes only and may be trademarks of their respective companies. References \n[1]  Martín Abadi.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "References \n[1]  Martín Abadi. Tensorﬂow: learning functions at scale. In  Acm Sigplan Notices .",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "In  Acm Sigplan Notices . ACM, 2016. [2]  Deepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, and Liang Zhang.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "[2]  Deepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, and Liang Zhang. Laser: A scalable response prediction platform for online advertising. In  Proceedings of the 7th ACM international conference on Web search and data mining , pages 173–182, 2014.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "In  Proceedings of the 7th ACM international conference on Web search and data mining , pages 173–182, 2014. [3]  Ahmed Ali-Eldin, Jonathan Westin, Bin Wang, Prateek Sharma, and Prashant Shenoy. Spotweb: Running latency-sensitive distributed web services on transient cloud servers.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "Spotweb: Running latency-sensitive distributed web services on transient cloud servers. In  Proceedings of the 28th Inter- national Symposium on High-Performance Parallel and Distributed Computing , pages 1–12, 2019. [4]  Amazon.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "[4]  Amazon. Deepar estimator. https://docs.aws.amazon.com/ sagemaker/latest/dg/deepar.html,February2020 .",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "https://docs.aws.amazon.com/ sagemaker/latest/dg/deepar.html,February2020 . [5] Amazon. EC2 pricing.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "EC2 pricing. https://aws.amazon.com/ec2/pricing/. [6]  Amazon.",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "[6]  Amazon. Sagemaker. https://aws.amazon.com/sagemaker/, February 2018.",
      "type": "sliding_window",
      "tokens": 28
    },
    {
      "text": "https://aws.amazon.com/sagemaker/, February 2018. [7]  Amazon. Azure Low priority batch VMs., February 2018. https://docs.microsoft.com/en-us/azure/batch/batch-low-pri-vms .",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Azure Low priority batch VMs., February 2018. https://docs.microsoft.com/en-us/azure/batch/batch-low-pri-vms . [8]  Amazon. EC2 C5 Instances., February 2018. https://aws.amazon.com/ec2/instance-types/c5/ .",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "EC2 C5 Instances., February 2018. https://aws.amazon.com/ec2/instance-types/c5/ . [9]  Amazon. Google Preemptible VMs., February 2018. https://cloud.google.com/preemptible-vms .",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "Google Preemptible VMs., February 2018. https://cloud.google.com/preemptible-vms . [10]  Azure. Machine Learning as a Service., February 2018. https://azure.microsoft.com/en-us/pricing/details/machine-learning- service/ .",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Machine Learning as a Service., February 2018. https://azure.microsoft.com/en-us/pricing/details/machine-learning- service/ . [11]  Azure. Ensembling in Azure ML Studio., February 2020. https://docs.microsoft.com/en-us/azure/machine-learning/studio- module-reference/multiclass-decision-forest .",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Ensembling in Azure ML Studio., February 2020. https://docs.microsoft.com/en-us/azure/machine-learning/studio- module-reference/multiclass-decision-forest . USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1053 \n[12]  Ataollah Fatahi Baarzi, Timothy Zhu, and Bhuvan Urgaonkar. Burscale: Using burstable instances for cost-effective autoscaling in the public cloud.",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "Burscale: Using burstable instances for cost-effective autoscaling in the public cloud. In  Proceedings of the ACM Symposium on Cloud Computing , New York, NY, USA, 2019. Association for Computing Machinery.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Association for Computing Machinery. [13]  Marian Stewart Bartlett, Gwen Littlewort, Mark Frank, Claudia Lain- scsek, Ian Fasel, and Javier Movellan. Recognizing facial expression: machine learning and application to spontaneous behavior.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Recognizing facial expression: machine learning and application to spontaneous behavior. In  2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) , volume 2, pages 568–573. IEEE, 2005.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "IEEE, 2005. [14]  Eric Bauer and Ron Kohavi. An empirical comparison of voting classiﬁcation algorithms: Bagging, boosting, and variants.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "An empirical comparison of voting classiﬁcation algorithms: Bagging, boosting, and variants. Machine learning , 36(1-2):105–139, 1999. [15]  William H Beluch, Tim Genewein, Andreas Nürnberger, and Jan M Köhler.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "[15]  William H Beluch, Tim Genewein, Andreas Nürnberger, and Jan M Köhler. The power of ensembles for active learning in image classiﬁ- cation. In  Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 9368–9377, 2018.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "In  Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 9368–9377, 2018. [16] Josiah L Carlson. Redis in action .",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "Redis in action . Manning Publications Co., 2013. [17]  Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "[17]  Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In  Proceedings of the twenty-ﬁrst international conference on Machine learning , page 18, 2004.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "In  Proceedings of the twenty-ﬁrst international conference on Machine learning , page 18, 2004. [18]  Jesús Cerquides and Ramon López De Mántaras. Robust bayesian linear classiﬁer ensembles.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Robust bayesian linear classiﬁer ensembles. In  European Conference on Machine Learning , pages 72–83. Springer, 2005.",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "Springer, 2005. [19]  Michael Alan Chang, Bredan Tschaen, Theophilus Benson, and Lau- rent Vanbever. Chaos monkey: Increasing sdn reliability through systematic network destruction.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "Chaos monkey: Increasing sdn reliability through systematic network destruction. In  Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication , pages 371–372, 2015. [20] Lingjiao Chen, Matei Zaharia, and James Zou.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "[20] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalml: How to use ml prediction apis more accurately and cheaply. In  Advances in Neural Information Processing Systems (NeurIPS) , 2020.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "In  Advances in Neural Information Processing Systems (NeurIPS) , 2020. [21]  Kristina Chodorow. MongoDB: the deﬁnitive guide: powerful and scalable data storage . \"",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "MongoDB: the deﬁnitive guide: powerful and scalable data storage . \" O’Reilly Media, Inc.\", 2013. [22]  Francois Chollet.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "[22]  Francois Chollet. Deep Learning mit Python und Keras: Das Praxis- Handbuch vom Entwickler der Keras-Bibliothek . MITP-Verlags GmbH & Co. KG, 2018.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "MITP-Verlags GmbH & Co. KG, 2018. [23]  Andrew Chung, Jun Woo Park, and Gregory R. Ganger. Stratus: Cost-aware container scheduling in the public cloud.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "Stratus: Cost-aware container scheduling in the public cloud. In  SoCC , 2018. [24]  Paul Covington, Jay Adams, and Emre Sargin.",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "[24]  Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In  Proceedings of the 10th ACM con- ference on recommender systems , pages 191–198, 2016.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "In  Proceedings of the 10th ACM con- ference on recommender systems , pages 191–198, 2016. [25]  Daniel Crankshaw, Peter Bailis, Joseph E Gonzalez, Haoyuan Li, Zhao Zhang, Michael J Franklin, Ali Ghodsi, and Michael I Jordan. The missing piece in complex analytics: Low latency, scalable model man- agement and serving with velox.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "The missing piece in complex analytics: Low latency, scalable model man- agement and serving with velox. arXiv preprint arXiv:1409.3809 , 2014. [26]  Daniel Crankshaw, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E. Gonzalez, Ion Stoica, and Alexey Tumanov.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "[26]  Daniel Crankshaw, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E. Gonzalez, Ion Stoica, and Alexey Tumanov. Inferline: ML inference pipeline composition framework. CoRR , abs/1812.01776, 2018.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "CoRR , abs/1812.01776, 2018. [27]  Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica. Clipper: A low-latency online pre- diction serving system.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "Clipper: A low-latency online pre- diction serving system. In  14th USENIX Symposium on Networked Sys- tems Design and Implementation (NSDI 17) , pages 613–627, Boston, MA, March 2017. USENIX Association.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "USENIX Association. [28]  Deepstudio. Deep Learning Dtudio, February 2020. https://docs.deepcognition.ai/ .",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "Deep Learning Dtudio, February 2020. https://docs.deepcognition.ai/ . [29]  J. Deng, W. Dong, R. Socher, L. Li, and and. Imagenet: A large-scale hierarchical image database.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "Imagenet: A large-scale hierarchical image database. In  2009 IEEE Conference on Computer Vision and Pattern Recognition , June 2009. [30]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "[30]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language un- derstanding, 2019. [31]  Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "[31]  Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data, 2020. [32]  Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "[32]  Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 , 2020.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "arXiv preprint arXiv:2002.08155 , 2020. [33]  Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera. A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based ap- proaches.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based ap- proaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) , 42(4):463–484, 2012. [34]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "[34]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efﬁciency. In  USENIX Middleware Conference , 2017.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "In  USENIX Middleware Conference , 2017. [35]  Arpan Gujarati, Reza Karimi, Safya Alzayat, Antoine Kaufmann, Ymir Vigfusson, and Jonathan Mace. Serving dnns like clockwork: Perfor- mance predictability from the bottom up.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "Serving dnns like clockwork: Perfor- mance predictability from the bottom up. In  14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20) , Banff, Alberta, November 2020. USENIX Association.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "USENIX Association. [36]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan C.Nachiappan, Mahmut Taylan Kandemir, and Chita R. Das. Fifer: Tackling Resource Underutilization in the Serverless Era.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "Fifer: Tackling Resource Underutilization in the Serverless Era. In  USENIX Middleware Conference , 2020. [37]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Taylan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "[37]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Taylan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das. Spock: Exploiting serverless functions for slo and cost aware resource procure- ment in public cloud. In  IEEE CLOUD , 2019.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "In  IEEE CLOUD , 2019. [38]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kandemir, and Chita R. Das. Towards designing a self-managed machine learning inference serving system inpublic cloud, 2020.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Towards designing a self-managed machine learning inference serving system inpublic cloud, 2020. [39]  U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G. Wei, H. S. Lee, D. Brooks, and C. Wu. Deeprecsys: A system for optimiz- ing end-to-end at-scale neural recommendation inference.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Deeprecsys: A system for optimiz- ing end-to-end at-scale neural recommendation inference. In  2020 ACM/IEEE 47th Annual International Symposium on Computer Archi- tecture (ISCA) , pages 982–995, 2020. [40]  Rui Han, Moustafa M. Ghanem, Li Guo, Yike Guo, and Michelle Osmond.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "[40]  Rui Han, Moustafa M. Ghanem, Li Guo, Yike Guo, and Michelle Osmond. Enabling cost-aware and adaptive elasticity of multi-tier cloud applications. Future Gener.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Syst. , 32(C):82–98, March 2014. [41]  Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "[41]  Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons. Tributary: spot-dancing for elastic services with latency SLOs. In  ATC , 2018.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "In  ATC , 2018. [42]  Aaron Harlap, Alexey Tumanov, Andrew Chung, Gregory R. Ganger, and Phillip B. Gibbons. Proteus: Agile ML Elasticity Through Tiered Reliability in Dynamic Resource Markets.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "Proteus: Agile ML Elasticity Through Tiered Reliability in Dynamic Resource Markets. In  Eurosys , 2017. [43]  Johann Hauswald, Michael A. Laurenzano, Yunqi Zhang, Cheng Li, Austin Rovinski, Arjun Khurana, Ronald G. Dreslinski, Trevor Mudge, Vinicius Petrucci, Lingjia Tang, and Jason Mars.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "[43]  Johann Hauswald, Michael A. Laurenzano, Yunqi Zhang, Cheng Li, Austin Rovinski, Arjun Khurana, Ronald G. Dreslinski, Trevor Mudge, Vinicius Petrucci, Lingjia Tang, and Jason Mars. Sirius: An open end-to-end voice and vision personal assistant and its implications for future warehouse scale computers. In  ASPLOS , 2015.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "In  ASPLOS , 2015. [44]  K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang. Applied machine learning at facebook: A datacenter infrastructure perspective. In  2018 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pages 620–629, Feb 2018.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "In  2018 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pages 620–629, Feb 2018. [45]  Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vi- jay Vasudevan, et al. Searching for mobilenetv3.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "Searching for mobilenetv3. In  Proceedings of the IEEE International Conference on Computer Vision , pages 1314–1324, 2019. [46]  Patrick Hunt, Mahadev Konar, Flavio Paiva Junqueira, and Benjamin Reed.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "[46]  Patrick Hunt, Mahadev Konar, Flavio Paiva Junqueira, and Benjamin Reed. Zookeeper: Wait-free coordination for internet-scale systems. In  USENIX annual technical conference , 2010.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "In  USENIX annual technical conference , 2010. [47]  Minoru Kawashima, Charles E Dorgan, and John W Mitchell. Hourly thermal load prediction for the next 24 hours by arima, ewma, lr and \n1054    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nan artiﬁcial neural network.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Hourly thermal load prediction for the next 24 hours by arima, ewma, lr and \n1054    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nan artiﬁcial neural network. Technical report, American Society of Heating, Refrigerating and Air-Conditioning Engineers ... , 1995. [48]  Abeer Abdel Khaleq and Ilkyeun Ra.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "[48]  Abeer Abdel Khaleq and Ilkyeun Ra. Cloud-based disaster manage- ment as a service: A microservice approach for hurricane twitter data analysis. In  GHTC , 2018.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "In  GHTC , 2018. [49]  J Zico Kolter and Marcus A Maloof. Dynamic weighted majority: An ensemble method for drifting concepts.",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "Dynamic weighted majority: An ensemble method for drifting concepts. Journal of Machine Learning Research , 8(Dec):2755–2790, 2007. [50]  Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "[50]  Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (cana- dian institute for advanced research), 2010.  http://www.cs.toronto. edu/~kriz/cifar.html .",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "edu/~kriz/cifar.html . [51]  Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gim- pel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 , 2019. [52]  Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, and Matteo Interlandi.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "[52]  Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, and Matteo Interlandi. PRETZEL: Opening the black box of machine learning prediction serving systems. In  13th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 18) , pages 611–626, Carlsbad, CA, October 2018.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "In  13th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 18) , pages 611–626, Carlsbad, CA, October 2018. USENIX Association. [53]  Romain Lerallut, Diane Gasselin, and Nicolas Le Roux.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "[53]  Romain Lerallut, Diane Gasselin, and Nicolas Le Roux. Large-scale real-time product recommendation at criteo. In  Proceedings of the 9th ACM Conference on Recommender Systems , pages 232–232, 2015.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "In  Proceedings of the 9th ACM Conference on Recommender Systems , pages 232–232, 2015. [54]  Weibo Liu, Zidong Wang, Xiaohui Liu, Nianyin Zeng, Yurong Liu, and Fuad E Alsaadi. A survey of deep neural network architectures and their applications.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "A survey of deep neural network architectures and their applications. Neurocomputing , 234:11–26, 2017. [55]  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy- anov.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "[55]  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy- anov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 , 2019.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "arXiv preprint arXiv:1907.11692 , 2019. [56]  Zhenyu Lu, Xindong Wu, Xingquan Zhu, and Josh Bongard. Ensemble pruning via individual contribution ordering.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Ensemble pruning via individual contribution ordering. In  Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’10, page 871–880, New York, NY, USA, 2010. Association for Computing Machinery.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Association for Computing Machinery. [57]  Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks. In  2021 Design, Automation Test in Europe Conference Exhibition (DATE) , pages 1414–1419, 2021. [58]  Soo-Jin Moon, Jeffrey Helt, Yifei Yuan, Yves Bieri, Sujata Banerjee, Vyas Sekar, Wenfei Wu, Mihalis Yannakakis, and Ying Zhang.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "[58]  Soo-Jin Moon, Jeffrey Helt, Yifei Yuan, Yves Bieri, Sujata Banerjee, Vyas Sekar, Wenfei Wu, Mihalis Yannakakis, and Ying Zhang. Alem- bic: Automated model inference for stateful network functions. In  16th USENIX Symposium on Networked Systems Design and Implementa- tion (NSDI 19) , pages 699–718, Boston, MA, February 2019.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "In  16th USENIX Symposium on Networked Systems Design and Implementa- tion (NSDI 19) , pages 699–718, Boston, MA, February 2019. USENIX Association. [59]  Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "[59]  Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: generalized pipeline parallelism for dnn training. In  Proceedings of the 27th ACM Symposium on Operating Systems Principles , pages 1–15, 2019.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "In  Proceedings of the 27th ACM Symposium on Operating Systems Principles , pages 1–15, 2019. [60]  Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke. Tensorﬂow-serving: Flexible, high-performance ml serving.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Tensorﬂow-serving: Flexible, high-performance ml serving. arXiv preprint arXiv:1712.06139 , 2017. [61]  Nikunj C Oza.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "[61]  Nikunj C Oza. Online bagging and boosting. In  2005 IEEE interna- tional conference on systems, man and cybernetics , volume 3, pages 2340–2345.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "In  2005 IEEE interna- tional conference on systems, man and cybernetics , volume 3, pages 2340–2345. Ieee, 2005. [62]  Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad- bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "[62]  Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad- bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In  Advances in neural information processing systems , pages 8026–8037, 2019.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "In  Advances in neural information processing systems , pages 8026–8037, 2019. [63]  Heyang Qin, Syed Zawad, Yanqi Zhou, Lei Yang, Dongfang Zhao, and Feng Yan. Swift machine learning model serving scheduling: a region \nbased reinforcement learning approach.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Swift machine learning model serving scheduling: a region \nbased reinforcement learning approach. In  Proceedings of the Inter- national Conference for High Performance Computing, Networking, Storage and Analysis , pages 1–23, 2019. [64]  Xueheng Qiu, Le Zhang, Ye Ren, Ponnuthurai N Suganthan, and Gehan Amaratunga.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "[64]  Xueheng Qiu, Le Zhang, Ye Ren, Ponnuthurai N Suganthan, and Gehan Amaratunga. Ensemble deep learning for regression and time series forecasting. In  2014 IEEE symposium on computational intelligence in ensemble learning (CIEL) , pages 1–6.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "In  2014 IEEE symposium on computational intelligence in ensemble learning (CIEL) , pages 1–6. IEEE, 2014. [65]  Atul Rahman, Jongeun Lee, and Kiyoung Choi.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "[65]  Atul Rahman, Jongeun Lee, and Kiyoung Choi. Efﬁcient fpga acceler- ation of convolutional neural networks using logical-3d compute array. In  2016 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pages 1393–1398.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "In  2016 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pages 1393–1398. IEEE, 2016. [66]  Sara Rosenthal, Noura Farra, and Preslav Nakov.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "[66]  Sara Rosenthal, Noura Farra, and Preslav Nakov. SemEval-2017 task 4: Sentiment analysis in Twitter. In  Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) , pages 502–518, Vancouver, Canada, August 2017.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "In  Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) , pages 502–518, Vancouver, Canada, August 2017. Association for Computational Linguistics. [67]  Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "[67]  Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 , 2019.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "arXiv preprint arXiv:1910.01108 , 2019. [68]  Prateek Sharma, David Irwin, and Prashant Shenoy. Portfolio-driven resource management for transient cloud servers.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "Portfolio-driven resource management for transient cloud servers. Proc. ACM Meas.",
      "type": "sliding_window",
      "tokens": 21
    },
    {
      "text": ", 1(1), June 2017. [69]  Prateek Sharma, Stephen Lee, Tian Guo, David Irwin, and Prashant Shenoy. Spotcheck: Designing a derivative iaas cloud on the spot market.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "Spotcheck: Designing a derivative iaas cloud on the spot market. In  Proceedings of the Tenth European Conference on Computer Systems , pages 1–15, 2015. [70]  Steven A Shaya, Neal Matheson, John Anthony Singarayar, Nikiforos Kollias, and Jeffrey Adam Bloom.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "[70]  Steven A Shaya, Neal Matheson, John Anthony Singarayar, Nikiforos Kollias, and Jeffrey Adam Bloom. Intelligent performance-based prod- uct recommendation system, October 5 2010. US Patent 7,809,601.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "US Patent 7,809,601. [71]  Richard Socher, Yoshua Bengio, and Chris Manning. Deep learning for nlp.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "Deep learning for nlp. Tutorial at Association of Computational Logistics (ACL) , 2012. [72]  Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "[72]  Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631–1642, 2013.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631–1642, 2013. [73]  Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Efﬁcientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946 , 2019. [74]  P. Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir, and C. R. Das.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "[74]  P. Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir, and C. R. Das. Phoenix: A constraint-aware scheduler for heteroge- neous datacenters. In  2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS) , June 2017.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "In  2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS) , June 2017. [75]  P. Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir, and C. R. Das. Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters. In  CLUSTER , 2019. [76]  Guido Urdaneta, Guillaume Pierre, and Maarten Van Steen.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "[76]  Guido Urdaneta, Guillaume Pierre, and Maarten Van Steen. Wikipedia workload analysis for decentralized hosting. Computer Networks , 2009.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "Computer Networks , 2009. [77]  Alexander Vezhnevets and Vladimir Vezhnevets. Modest adaboost- teaching adaboost to generalize better.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "Modest adaboost- teaching adaboost to generalize better. In  Graphicon , pages 987–997, 2005. [78]  Jasper A Vrugt and Bruce A Robinson.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "[78]  Jasper A Vrugt and Bruce A Robinson. Treatment of uncertainty using ensemble methods: Comparison of sequential data assimilation and bayesian model averaging. Water Resources Research , 43(1), 2007.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Water Resources Research , 43(1), 2007. [79]  Cheng Wang, Bhuvan Urgaonkar, Neda Nasiriani, and George Kesidis. Using burstable instances in the public cloud: Why, when and how?",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "Using burstable instances in the public cloud: Why, when and how? SIGMETRICS , June 2017. [80]  Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "[80]  Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. Raﬁki: machine learning as an analytics service system. Proceedings of the VLDB Endowment , 12(2):128–140, 2018.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "Proceedings of the VLDB Endowment , 12(2):128–140, 2018. [81]  Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of- \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1055 \nthe-art natural language processing.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "Huggingface’s transformers: State-of- \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1055 \nthe-art natural language processing. arXiv preprint arXiv:1910.03771 , 2019. [82]  Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choud- hury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, et al.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "[82]  Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choud- hury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, et al. Machine learning at facebook: Understanding inference at the edge. In  2019 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pages 331–344.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "In  2019 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pages 331–344. IEEE, 2019. [83]  Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "[83]  Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. A case for managed and model-less inference serving. In  Proceedings of the Workshop on Hot Topics in Operating Systems , New York, NY, USA, 2019.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "In  Proceedings of the Workshop on Hot Topics in Operating Systems , New York, NY, USA, 2019. Association for Computing Machinery. [84]  Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Vivienne Sze, and Hartwig Adam.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "[84]  Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. CoRR , abs/1804.03230, 2018.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "CoRR , abs/1804.03230, 2018. [85]  Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pre- training for language understanding.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "Xlnet: Generalized autoregressive pre- training for language understanding. arXiv preprint arXiv:1906.08237 , 2019. [86]  Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "[86]  Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. Mark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving. In  ATC , 2019.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "In  ATC , 2019. [87]  Honglei Zhuang, Chi Wang, and Yifan Wang. Identifying outlier arms in multi-armed bandit.",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "Identifying outlier arms in multi-armed bandit. In  Advances in Neural Information Processing Systems , pages 5204–5213, 2017. [88]  Sheikh Ziauddin and Matthew N Dailey.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "[88]  Sheikh Ziauddin and Matthew N Dailey. Iris recognition performance enhancement using weighted majority voting. In  2008 15th IEEE International Conference on Image Processing , pages 277–280.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "In  2008 15th IEEE International Conference on Image Processing , pages 277–280. IEEE, 2008. Appendix \nA Modeling of Ensembling \nWhile performing an ensemble it is important to be sure that we can reach the desired accuracy by combining more models.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Appendix \nA Modeling of Ensembling \nWhile performing an ensemble it is important to be sure that we can reach the desired accuracy by combining more models. In our design, we solve our ﬁrst objective function (described in Section  4.1 ) by combining all available models which meet the latency SLO. To be sure that the combination will give us the desired accuracy of the larger model, we try to theoretically analyse the scenario.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "To be sure that the combination will give us the desired accuracy of the larger model, we try to theoretically analyse the scenario. We formulate the problem conservatively as following. We perform an inference by ensembling ’N’ models, and each of these models have accuracy ’a’.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "We perform an inference by ensembling ’N’ models, and each of these models have accuracy ’a’. Therefore the prob- ability of any model giving a correct classiﬁcation is ’a’. We assume the output to be correct if majority of them, i.e.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "We assume the output to be correct if majority of them, i.e. ⌊ N / 2 ⌋ + 1  of them give the same result. Then, the ﬁnal ac- curacy of this ensemble would be the probability of at least ⌊ N / 2 ⌋ + 1 of them giving a correct result.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "Then, the ﬁnal ac- curacy of this ensemble would be the probability of at least ⌊ N / 2 ⌋ + 1 of them giving a correct result. To we model this problem as a coin-toss problem involving N  biased coins with having probability of occurrence of head to be  a . Relating this to our problem, each coin represents a model, and an occurrence of head represents the model giving the correct classiﬁcation.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Relating this to our problem, each coin represents a model, and an occurrence of head represents the model giving the correct classiﬁcation. Hence, the problem boils down to ﬁnd the probability of at least  ⌊ N / 2 ⌋ + 1  heads when all N coins are tossed together. This is a standard binomial distribution problem and can be solved by using the following formula: \nP head  = N ∑ i = ⌊ N \n2   ⌋ + 1 \n\u0012 N i \n\u0013 a i   ( 1 − a ) ( N − i ) .",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "This is a standard binomial distribution problem and can be solved by using the following formula: \nP head  = N ∑ i = ⌊ N \n2   ⌋ + 1 \n\u0012 N i \n\u0013 a i   ( 1 − a ) ( N − i ) . To further quantify, let us consider the case where we need to determine if we can reach the accuracy of NasNetLarge (82%) by combining rest of the smaller models which have lesser latency than NasNetLarge. We have 10 (therefore N = 10) such models and among them the least accurate model is MobileNetV1 (accuracy 70%, therefore a = 0.70).",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "We have 10 (therefore N = 10) such models and among them the least accurate model is MobileNetV1 (accuracy 70%, therefore a = 0.70). We need to ﬁnd the probability of at least 6 of them being correct. Using the equation above we ﬁnd the probability to be \nP head  = 10 ∑ i = ⌊ 10 \n2   ⌋ + 1 = 6 \n\u0012 10 i \n\u0013 0 .",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Using the equation above we ﬁnd the probability to be \nP head  = 10 ∑ i = ⌊ 10 \n2   ⌋ + 1 = 6 \n\u0012 10 i \n\u0013 0 . 7 i   ( 1 − 0 . 7 ) ( 10 − i )   =  0 .",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "7 ) ( 10 − i )   =  0 . 83 \nThis corresponds to an accuracy of 83%, which is greater than our required accuracy of 82%). Given all the other models have higher accuracy, the least accuracy we can expect with such an ensemble is 83%.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Given all the other models have higher accuracy, the least accuracy we can expect with such an ensemble is 83%. This analysis forms the base of our ensemble technique, and hence proving the combination of multiple available models can be more accurate than the most accurate individual model. B Why DeepARest Model?",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "B Why DeepARest Model? We quantitatively justify the choice of using DeepARest by conducting a brick-by-brick comparison of the accuracy loss \n1056    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nwhen compared with other state-of-the-art prediction models used in prior work. Table  4  shows the root mean squared error (RMSE) in- curred by all the models.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "Table  4  shows the root mean squared error (RMSE) in- curred by all the models. The ML models used in these experiments are pre-trained with 60% of the Twitter arrival trace. It is evident that the LSTM and DeepAREst have lowest RMSE value.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "It is evident that the LSTM and DeepAREst have lowest RMSE value. DeepARest is 10% better than LSTM model. Since the primary contribution in  Cocktail  is to provide high accuracy and low latency predictions at cheaper cost, appli- cation developers can adapt the prediction algorithm to their needs or even plug-in their own prediction models.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Since the primary contribution in  Cocktail  is to provide high accuracy and low latency predictions at cheaper cost, appli- cation developers can adapt the prediction algorithm to their needs or even plug-in their own prediction models. C System Overheads \nWe characterize the system-level overheads incurred due to the design choices in  Cocktail . The  mongodb  database is a centralized server, which resides on the head-node.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "The  mongodb  database is a centralized server, which resides on the head-node. We measure the overall average latency incurred due to all reads/writes in the database, which is well within 1.5ms. The DeepARest prediction model which is not in the critical decision-making path runs as a background process incurring 2.2 ms latency on average.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "The DeepARest prediction model which is not in the critical decision-making path runs as a background process incurring 2.2 ms latency on average. The weighted majority voting takes 0.5ms and the model selection policy takes 0.7ms. The time taken to spawn new VM takes about 60s to 100s de- pending on the size of the VM instance.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "The time taken to spawn new VM takes about 60s to 100s de- pending on the size of the VM instance. The time taken to choose models from the model-cache is less than 1ms. The end-to-end response time to send the image to a worker VM and get the prediction back, was dominated by about 300ms (at maximum) of payload transfer time.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "The end-to-end response time to send the image to a worker VM and get the prediction back, was dominated by about 300ms (at maximum) of payload transfer time. D Instance conﬁguration and Pricing \nInstance vCPUs Memory Price C5a.xlarge 4 8 GiB $0.154 C5a.2xlarge 8 16 GiB $0.308 C5a.4xlarge 16 32 GiB $0.616 C5a.8xlarge 32 64 GiB $1.232 \nTable 7:  Conﬁguration and Pricing for EC2 C5 instances. E CIFAR-100 and BERT Models \nTable  8  shows the different models available for image predic- tion, that are pretrained on Keras using  CIFAR-100  dataset.",
      "type": "sliding_window",
      "tokens": 169
    },
    {
      "text": "E CIFAR-100 and BERT Models \nTable  8  shows the different models available for image predic- tion, that are pretrained on Keras using  CIFAR-100  dataset. Model Params (M) \nTop-1 Accuracy(%) \nLatency (ms) P f \nAlbert-base [ 51 ] 11 91.4 55 7 CodeBert [ 32 ] 125 89 79 6 DistilBert [ 67 ] 66 90.6 92 5 Albert-large 17 92.5 120 4 XLNet [ 85 ] 110 94.6 165 3 Bert [ 30 ] 110 92 185 3 Roberta [ 55 ] 355 94.3 200 2 Albert-xlarge 58 93.8 220 1 Albert-xxlarge 223 95.9 350 1 \nTable 9:  Pretrained models for Sentiment Analysis using BERT. Similarly Table  9  shows the different models trained for BERT-based sentiment analysis on twitter dataset.",
      "type": "sliding_window",
      "tokens": 213
    },
    {
      "text": "Similarly Table  9  shows the different models trained for BERT-based sentiment analysis on twitter dataset. Model Params (M) Top1 Accuracy % Latency (ms) Pf Squeezenet 4,253,864 70.10 43.45 10 MobileNEt V2 4,253,864 68.20 41.5 10 Inception V4 23,851,784 76.74 74 6 Resnet50 95,154,159 79.20 98.22 5 ResNet18 44,964,665 76.26 35 6 DenseNet-201 20,242,984 79.80 152.21 2 DenseNet-121 8,062,504 78.72 102.35 3 Xxception 22,910,480 77.80 119.2 4 NasNet 5,326,716 77.90 120 3 InceptionResnetV2 2,510,000 80.30 251.96 1 \nTable 8:  Pretrained models for CIFAR-100 using Imagenet. F Spot Instance Price Variation \nWe proﬁle the spot price of 4 types of  C5  EC2 VMs over a 2-week period in August 2020.",
      "type": "sliding_window",
      "tokens": 264
    },
    {
      "text": "F Spot Instance Price Variation \nWe proﬁle the spot price of 4 types of  C5  EC2 VMs over a 2-week period in August 2020. The price variation is shown in Fig 18 . 0 100 200 300 Time \n0.1 \n0.2 \n0.3 \nPrice ($) \nxlarge 2xlarge 4xlarge 8xlarge \nFigure 18:  Spot instance price variation (time is in hours).",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "Despite high latency, prior works in this domain are crucially limited by the accu- racy offered by individual models. Abstract With a growing demand for adopting ML models for a variety of application services, it is vital that the frameworks serving these models are capable of delivering highly accurate predic- tions with minimal latency along with reduced deployment costs in a public cloud environment. Intuitively, model ensem- bling can address the accuracy gap by intelligently combining different models in parallel.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "However, selecting the appro- priate models dynamically at runtime to meet the desired accuracy with low latency at minimal deployment cost is a nontrivial problem. Intuitively, model ensem- bling can address the accuracy gap by intelligently combining different models in parallel. Towards this, we propose  Cocktail , a cost effective ensembling-based model serving framework.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "Towards this, we propose  Cocktail , a cost effective ensembling-based model serving framework. Cock- tail  comprises of two key components: (i) a dynamic model selection framework, which reduces the number of models in the ensemble, while satisfying the accuracy and latency requirements; (ii) an adaptive resource management (RM) framework that employs a distributed proactive autoscaling policy, to efﬁciently allocate resources for the models. The RM framework leverages transient virtual machine (VM) in- stances to reduce the deployment cost in a public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 128,
      "augmented": true
    },
    {
      "text": "1 Introduction \nMachine Learning (ML) has revolutionized user experience in various cloud-based application domains such as product recommendations [ 70 ], personalized advertisements [ 44 ], and computer vision [ 13 ,  43 ]. The RM framework leverages transient virtual machine (VM) in- stances to reduce the deployment cost in a public cloud. A prototype implementation of  Cocktail  on the AWS EC2 plat- form and exhaustive evaluations using a variety of workloads demonstrate that  Cocktail  can reduce deployment cost by 1.45 × , while providing 2 ×  reduction in latency and satisfy- ing the target accuracy for up to 96% of the requests, when compared to state-of-the-art model-serving frameworks.",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "1 Introduction \nMachine Learning (ML) has revolutionized user experience in various cloud-based application domains such as product recommendations [ 70 ], personalized advertisements [ 44 ], and computer vision [ 13 ,  43 ]. It is imperative for these applications to deliver accurate predic- tions at sub-millisecond latencies [ 27 , 34 , 35 , 39 , 44 , 83 ] as they critically impact the user experience. For instance, Facebook [ 44 ,  82 ] serves trillions of inference requests for user-interactive ap- plications like ranking new-feeds, classifying photos, etc.",
      "type": "sliding_window_shuffled",
      "tokens": 140,
      "augmented": true
    },
    {
      "text": "It is imperative for these applications to deliver accurate predic- tions at sub-millisecond latencies [ 27 , 34 , 35 , 39 , 44 , 83 ] as they critically impact the user experience. This trend is expected to perpetuate as a number of applications adopt a variety of ML models to augment their services. These ML models are typically trained and hosted on cloud platforms as service end- points, also known as  model-serving  framework [ 6 ,  28 ,  60 ].",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "From the myriad of ML ﬂavours, Deep Neural Networks \n(DNNs) [ 54 ] due to their multi-faceted nature, and highly gen- eralized and accurate learning patterns [ 45 , 73 ] are dominating the landscape by making these model-serving frameworks accessible to developers. These ML models are typically trained and hosted on cloud platforms as service end- points, also known as  model-serving  framework [ 6 ,  28 ,  60 ]. However, their high variance due to the ﬂuctuations in training data along with compute and mem- ory intensiveness [ 59 , 65 , 84 ] has been a major impediment in designing models with high accuracy and low latency.",
      "type": "sliding_window_shuffled",
      "tokens": 163,
      "augmented": true
    },
    {
      "text": "Prior model-serving frameworks like InFaas [ 83 ] are conﬁned by the accuracy and latency offered by such individual models. However, their high variance due to the ﬂuctuations in training data along with compute and mem- ory intensiveness [ 59 , 65 , 84 ] has been a major impediment in designing models with high accuracy and low latency. Unlike single-model inferences, more sophisticated tech- niques like  ensemble learning  [ 15 ] have been instrumental in allowing model-serving to further improve accuracy with multiple models.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "For example, by using the ensembling   1 \ntechnique, images can be classiﬁed using multiple models  in parallel  and results can be combined to give a ﬁnal prediction. This signiﬁcantly boosts accuracy compared to single-models, and for this obvious advantage, frameworks like Clipper [ 27 ] leverage ensembling techniques. Unlike single-model inferences, more sophisticated tech- niques like  ensemble learning  [ 15 ] have been instrumental in allowing model-serving to further improve accuracy with multiple models.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "This signiﬁcantly boosts accuracy compared to single-models, and for this obvious advantage, frameworks like Clipper [ 27 ] leverage ensembling techniques. Since cost plays a crucial role in application-provider consideration, it is quintessential to minimize the deployment costs, while maximizing accuracy with low latency. Nevertheless, with ensem- bling, the very high resource footprint due to sheer number of models that need to be run for each request [ 27 , 56 ], ex- acerbates the public cloud deployment costs, as well as leads to high variation in latencies.",
      "type": "sliding_window_shuffled",
      "tokens": 135,
      "augmented": true
    },
    {
      "text": "Studying the state-of-the-art ensemble model-serving frameworks, we observe the following critical shortcomings: •  Ensemble model selection policies used in frameworks like Clipper [ 27 ] are static, as they  ensemble all available models  and focus solely on minimizing loss in accuracy. Since cost plays a crucial role in application-provider consideration, it is quintessential to minimize the deployment costs, while maximizing accuracy with low latency. Hence, the non-trivial challenge here lies in making the cost of ensembling predictions analogous to single model predictions, while satisfying these requirements.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "This leads to higher latencies and further inﬂates the resource foot- print, thereby accentuating the deployment costs. •  Existing ensemble weight estimation [ 87 ] has  high com- putational complexity  and in practice is limited to a small set of off-the-shelf models. Studying the state-of-the-art ensemble model-serving frameworks, we observe the following critical shortcomings: •  Ensemble model selection policies used in frameworks like Clipper [ 27 ] are static, as they  ensemble all available models  and focus solely on minimizing loss in accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "Besides, employing linear ensembling techniques such as model averaging are compute intensive [ 80 ] and not scalable for a large number of available models. This leads to signiﬁcant loss in accuracy. •  Existing ensemble weight estimation [ 87 ] has  high com- putational complexity  and in practice is limited to a small set of off-the-shelf models.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1041 \nselection and procurement play a pivotal role in minimizing the latency and deployment costs. •  Ensemble systems [ 27 , 80 ] are  not focused towards model deployment  in a public cloud infrastructure, where resource \n1 We refer to ensemble-learning as ensembling throughout the paper. Besides, employing linear ensembling techniques such as model averaging are compute intensive [ 80 ] and not scalable for a large number of available models.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1041 \nselection and procurement play a pivotal role in minimizing the latency and deployment costs. Further, the resource provi- sioning strategies employed in single model-serving systems are  not directly extendable  to ensemble systems. These shortcomings collectively motivate the central premise of this work:  how to solve the complex optimiza- tion problem of cost, accuracy and latency for an ensem- bling framework?",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "These shortcomings collectively motivate the central premise of this work:  how to solve the complex optimiza- tion problem of cost, accuracy and latency for an ensem- bling framework? In this paper, we present and evaluate Cocktail 2 , which to our knowledge is the ﬁrst work that pro- poses a cost-effective model-serving system by exploiting ensembling techniques for classiﬁcation-based inference, to deliver high accuracy and low latency predictions. Cocktail adopts a three-pronged approach to solve the optimization problem.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "First, it uses a dynamic model selection policy to signiﬁcantly reduce the number of models used in an ensem- ble, while meeting the latency and accuracy requirements. Cocktail adopts a three-pronged approach to solve the optimization problem. Figure 1:  Beneﬁts of  Cocktail .",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Re- sults are normalized (higher the better). Second, it utilizes dis- tributed autoscaling poli- cies to reduce the la- tency variability and re- source consumption of hosting ensemble mod- els. Figure 1:  Beneﬁts of  Cocktail .",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "Third, it minimizes the cost of deploying ensembles in a public cloud by taking advan- tage of transient VMs, as they can be 70-90% cheaper [ 3 ] than traditional VMs. Second, it utilizes dis- tributed autoscaling poli- cies to reduce the la- tency variability and re- source consumption of hosting ensemble mod- els. Cocktail , by coalescing these beneﬁts, is capable of operating in a region of optimal cost, accuracy and latency (shown in Figure  1 ) that prior works cannot achieve.",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "By characterizing accuracy  vs.  latency of ensemble models, we identify that prudently selecting a subset of available models under a given latency can achieve the target ac- curacy. Cocktail , by coalescing these beneﬁts, is capable of operating in a region of optimal cost, accuracy and latency (shown in Figure  1 ) that prior works cannot achieve. Towards this, the key contributions  of the paper are summarized below: \n1.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "2. We leverage this in  Cocktail , to design a novel dynamic model selection policy, which ensures accuracy with signiﬁcantly reduced number of models. By characterizing accuracy  vs.  latency of ensemble models, we identify that prudently selecting a subset of available models under a given latency can achieve the target ac- curacy.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "Focusing on classiﬁcation-based inferences, it is important to minimize the bias in predictions resulting from multi- ple models. In  Cocktail , we employ a per-class weighted majority voting policy, that makes it scalable and effec- tively breaks ties when compared to traditional weighted averaging, thereby minimizing the accuracy loss. 2.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "3. We show that uniformly scaling resources for all models in the ensemble leads to over-provisioning of resources and towards minimizing it, we build a distributed weighted auto-scaling policy that utilizes the  importance sampling technique to proactively allocate resources to every model. In  Cocktail , we employ a per-class weighted majority voting policy, that makes it scalable and effec- tively breaks ties when compared to traditional weighted averaging, thereby minimizing the accuracy loss.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "We show that uniformly scaling resources for all models in the ensemble leads to over-provisioning of resources and towards minimizing it, we build a distributed weighted auto-scaling policy that utilizes the  importance sampling technique to proactively allocate resources to every model. 2 Cocktail is ascribed to having the perfect blend of models in an ensemble. Further,  Cocktail  leverages transient VMs as they are cheaper, to drastically minimize the cost for hosting model- serving infrastructure in a public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "4. 2 Cocktail is ascribed to having the perfect blend of models in an ensemble. Applications \nImage  Recognition NLP Recommender  Systems \nModels \nVM \nVM \nVM VM VM \nVM \nFrameworks \nCloud \nResources \nSLO \nAccuracy \nLatency \nUsers \nBurstables Spot CPU GPU \nCost \nLatency \nFigure 2:  The overall framework for model-serving in public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "Our results from exhaustive experimental analysis demon- strate that  Cocktail  can minimize deployment cost by 1.4 × while meeting the accuracy for up-to 96% of the requests and providing 2 ×  reduction in latency, when compared to state-of-the-art model serving systems. 4. We implement a prototype of  Cocktail  using both CPU and GPU instances on AWS EC2 [ 5 ] platform and ex- tensively evaluate it using different request-arrival traces.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "Our results from exhaustive experimental analysis demon- strate that  Cocktail  can minimize deployment cost by 1.4 × while meeting the accuracy for up-to 96% of the requests and providing 2 ×  reduction in latency, when compared to state-of-the-art model serving systems. We show that ensemble models are inherently fault- tolerant over single models, since in the former, failure of a model would incur some accuracy loss without complete failure of the requests. 5.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "It is observed from our failure- resilience results that  Cocktail  can adapt to instance fail- ures by limiting the accuracy loss within 0.6%. 2 Background and Motivation \nWe start by providing a brief overview of model-serving in public cloud and ensembling, followed by a detailed analysis of their performance to motivate the need for  Cocktail . We show that ensemble models are inherently fault- tolerant over single models, since in the former, failure of a model would incur some accuracy loss without complete failure of the requests.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "There are diverse applications that are typically developed, trained and hosted as web services. 2 Background and Motivation \nWe start by providing a brief overview of model-serving in public cloud and ensembling, followed by a detailed analysis of their performance to motivate the need for  Cocktail . 2.1 Model Serving in Public Cloud \nFigure  2  shows the overall architecture of a model-serving framework.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "These services allow end-users to submit queries via web server interface. There are diverse applications that are typically developed, trained and hosted as web services. Since these inference requests are often user-facing, it is imperative to administer them under a strict service level ob- jective (SLO).",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "Services like Ads and News Feed [ 39 , 44 ] would require SLOs within 100ms, while facial tag recommendation [ 83 ] can tolerate up to 1000ms. We deﬁne SLO as the end-to-end response latency required by an application. Since these inference requests are often user-facing, it is imperative to administer them under a strict service level ob- jective (SLO).",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "A myriad of model architectures are available to train these applications which by themselves can be deployed on appli- cation frameworks like  TensorFlow  [ 1 ],  PyTorch  [ 62 ] etc. Table  1  shows the different models available for image predic- tion, that are pretrained on Keras using  ImageNet  [ 29 ] dataset. Services like Ads and News Feed [ 39 , 44 ] would require SLOs within 100ms, while facial tag recommendation [ 83 ] can tolerate up to 1000ms.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "Typically denser models are designed with more parameters (ex. Table  1  shows the different models available for image predic- tion, that are pretrained on Keras using  ImageNet  [ 29 ] dataset. Each model has unique accuracy and latencies depending on the model architecture.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "Typically denser models are designed with more parameters (ex. classes of images. NASLarge ) to classify complex \n1042    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nModel (Acronym) Params (10k) \nTop-1 Accuracy(%) \nLatency (ms) P f \nMobileNetV1 (MNet) 4,253 70.40 43.45 10 MobileNetV2 (MNetV2) 4,253 71.30 41.5 10 NASNetMobile (NASMob) 5,326 74.40 78.18 3 DenseNet121 (DNet121) 8,062 75.00 102.35 3 DenseNet201 (DNet201) 20,242 77.30 152.21 2 Xception (Xcep) 22,910 79.00 119.2 4 Inception V3 (Incep) 23,851 77.90 89 5 ResNet50-V2 (RNet50) 25,613 76.00 89.5 6 Resnet50 (RNet50) 25,636 74.90 98.22 5 IncepResnetV2 (IRV2) 55,873 80.30 151.96 1 NasNetLarge (NasLarge) 343,000 82.00 311 1 \nTable 1:  Collection of pretrained models used for image classiﬁcation.",
      "type": "sliding_window_shuffled",
      "tokens": 317,
      "augmented": true
    },
    {
      "text": "Depending on the application type, the maximum ensemble size can vary from tens to hundreds of models. classes of images. These 11 models are a representative set to classify all images belonging to 1000 classes in  Imagenet.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "Depending on the application type, the maximum ensemble size can vary from tens to hundreds of models. These re- sources are available in different types including CPU/GPU instances, burstables and transient instances. The entire model framework is typically hosted on re- sources like VMs or containers in public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "Transient in- stances [ 69 ] are similar to traditional VMs but can be revoked at any time by the cloud provider with an interruption notice. These re- sources are available in different types including CPU/GPU instances, burstables and transient instances. The provisioning latency, instance permanence and packing factor of these resources have a direct impact on the latency and cost of hosting model-serving.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "The provisioning latency, instance permanence and packing factor of these resources have a direct impact on the latency and cost of hosting model-serving. We explain instance “pack- ing factor” and its relationship with latency in Section  2.3.2 . In this paper, we focus on improving the accuracy and latency from the model selection perspective and consider instances types from a cost perspective.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "For instance, InFaas [ 83 ] can choose variants among a same model to maintain accu- racy and latency requirements. In this paper, we focus on improving the accuracy and latency from the model selection perspective and consider instances types from a cost perspective. A majority of the model serving systems [ 6 , 83 , 86 ] in public cloud support individual model selection from available models.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "Besides using dense models, ensembling [ 15 ] techniques have been used to achieve higher accuracy. However, denser models tend to have up to 6 ×  the size and twice the latency of smaller models to achieve increased accuracy of about 2-3%. For instance, InFaas [ 83 ] can choose variants among a same model to maintain accu- racy and latency requirements.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "Why Ensembling? An Ensemble is deﬁned as a set of clas- siﬁers whose individual decisions combined in some way to classify new examples. Besides using dense models, ensembling [ 15 ] techniques have been used to achieve higher accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "This has proved to be more accurate than traditional single large models because it inherently re- duces incorrect predictions due to variance and bias. An Ensemble is deﬁned as a set of clas- siﬁers whose individual decisions combined in some way to classify new examples. The commonly used ensemble method in classiﬁcation problems is bagging [ 33 ] that considers homogeneous weak learners, learns them independently from each other in parallel, and combines them following some kind of deterministic aver- aging process [ 18 ] or majority voting [ 49 ] process.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "The commonly used ensemble method in classiﬁcation problems is bagging [ 33 ] that considers homogeneous weak learners, learns them independently from each other in parallel, and combines them following some kind of deterministic aver- aging process [ 18 ] or majority voting [ 49 ] process. For fur- ther details on ensemble models, we refer the reader to prior works [ 14 , 57 , 58 , 61 , 64 , 77 , 78 , 88 ]. 2.2 Related Work \nEnsembling in practice : Ensembling is supported by com- mercial cloud providers like Azure ML-studio [ 11 ] and AWS Autogluon [ 31 ] to boost the accuracy compared to single models.",
      "type": "sliding_window_shuffled",
      "tokens": 171,
      "augmented": true
    },
    {
      "text": "2.2 Related Work \nEnsembling in practice : Ensembling is supported by com- mercial cloud providers like Azure ML-studio [ 11 ] and AWS Autogluon [ 31 ] to boost the accuracy compared to single models. 200 using a hill-climb policy [ 17 ] to meet the target accuracy. Azure initially starts with 5 models and scales up to \nFeatures \nClipper [ 27 ] \nRaﬁki [ 80 ] \nInfaas [ 83 ] \nMArk [ 86 ] \nSagemaker \nSwayam [ 34 ] \nCocktail \nPredictive Scaling \u0017 \u0017 \u0017 \u0013 \u0017 \u0013 \u0013 SLO Guarantees \u0013 \u0017 \u0013 \u0013 \u0017 \u0013 \u0013 Cost Effective \u0017 \u0017 \u0013 \u0013 \u0017 \u0017 \u0013 Ensembling \u0013 \u0013 \u0017 \u0017 \u0013 \u0017 \u0013 Heterogeneous Instances \u0017 \u0013 \u0013 \u0013 \u0013 \u0017 \u0013 Dynamic ensemble selection \u0017 \u0017 \u0017 \u0017 \u0017 \u0017 \u0013 Model abstraction \u0013 \u0013 \u0013 \u0017 \u0017 \u0017 \u0013 \nTable 2:  Comparing  Cocktail  with other related frameworks.",
      "type": "sliding_window_shuffled",
      "tokens": 170,
      "augmented": true
    },
    {
      "text": "Users also have the option to manually mention the ensemble size. 200 using a hill-climb policy [ 17 ] to meet the target accuracy. AWS combines about 6-12 models to give the best possible accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "Model-serving in Cloud : The most relevant prior works to Cocktail  are InFaas [ 83 ] and Clipper [ 27 ], which have been extensively discussed and compared to in Section  6 . Unlike them,  Cocktail’s  model selection pol- icy tries to right-size the ensemble for a given latency, while maximizing accuracy. Users also have the option to manually mention the ensemble size.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "While striking a few similarities with  Cocktail , it is practically limited to image-classiﬁcation applications with very few classes and does not address re- source provisioning challenges. Model-serving in Cloud : The most relevant prior works to Cocktail  are InFaas [ 83 ] and Clipper [ 27 ], which have been extensively discussed and compared to in Section  6 . Recently FrugalML [ 20 ] was proposed to cost-effectively choose from commercial MLaaS APIs.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Several works [ 37 , 38 ] like MArk [ 86 ] proposed SLO and cost aware resource procure- ment policies for model-serving. While striking a few similarities with  Cocktail , it is practically limited to image-classiﬁcation applications with very few classes and does not address re- source provisioning challenges. Although our heterogeneous instance procurement policy has some similarities with MArk, it is signiﬁcantly different because we consider ensemble models.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Raﬁki [ 80 ] considers small model sets and scales up and down the ensemble size by trading off accuracy to match throughput demands. Although our heterogeneous instance procurement policy has some similarities with MArk, it is signiﬁcantly different because we consider ensemble models. However,  Cocktail’s  resource management is more adaptive to changing request loads and does not drop accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "However,  Cocktail’s  resource management is more adaptive to changing request loads and does not drop accuracy. Pretzel [ 52 ] and Inferline [ 26 ] are built on top of Clipper to optimize the prediction pipeline and cost due to load variations, respectively. Many prior works [ 2 , 25 , 35 , 63 , 74 , 75 ] have extensively tried to reduce model latency by reducing overheads due to shared resources and hardware interference.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Many prior works [ 2 , 25 , 35 , 63 , 74 , 75 ] have extensively tried to reduce model latency by reducing overheads due to shared resources and hardware interference. There are mainstream commercial systems which automate single model-serving like TF-Serving [ 60 ], SageMaker [ 6 ], AzureML [ 10 ], Deep-Studio [ 28 ] etc. We believe that our proposed policies can be complementary and beneﬁcial to these prior works to reduce the cost and resource footprint of ensembling.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "These works are broadly categorized into: (i) multiplexing the different instance types (e.g., Spot, On- Demand) [ 12 ,  23 ,  34 ,  41 ,  42 ,  68 ,  79 ], (ii) proactive resource provisioning based on prediction policies [ 34 , 36 , 40 , 41 , 69 , 86 ]. There are mainstream commercial systems which automate single model-serving like TF-Serving [ 60 ], SageMaker [ 6 ], AzureML [ 10 ], Deep-Studio [ 28 ] etc. Autoscaling in Public Cloud : There are several research works that optimize the resource provisioning cost in pub- lic cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 166,
      "augmented": true
    },
    {
      "text": "These works are broadly categorized into: (i) multiplexing the different instance types (e.g., Spot, On- Demand) [ 12 ,  23 ,  34 ,  41 ,  42 ,  68 ,  79 ], (ii) proactive resource provisioning based on prediction policies [ 34 , 36 , 40 , 41 , 69 , 86 ]. Cocktail  uses similar load prediction models and auto-scales VMs in a distributed fashion with respect to model ensem- bling. Swayam [ 34 ] is relatively similar to our work as it han- \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1043 \nBaseline(BL) NASLarge IRV2 Xception DNet121 NASMob #Models 10 8 7 5 2 BL_Latency 311(ms) 152(ms) 120(ms) 100(ms) 98(ms) E_Latency 152(ms) 120(ms) 103(ms) 89(ms) 44(ms) \nTable 3:  Comparing latency of Ensembling (E_Latency) with single (baseline) models.",
      "type": "sliding_window_shuffled",
      "tokens": 286,
      "augmented": true
    },
    {
      "text": "Swayam [ 34 ] is relatively similar to our work as it han- \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1043 \nBaseline(BL) NASLarge IRV2 Xception DNet121 NASMob #Models 10 8 7 5 2 BL_Latency 311(ms) 152(ms) 120(ms) 100(ms) 98(ms) E_Latency 152(ms) 120(ms) 103(ms) 89(ms) 44(ms) \nTable 3:  Comparing latency of Ensembling (E_Latency) with single (baseline) models. Cocktail’s  autoscaling policy strikes parallels with Swayam’s distributed autoscaling; however, we further incorporate novel importance sampling techniques to reduce over-provisioning for under-used models. dles container provisioning and load-balancing, speciﬁcally catered for single model inferences.",
      "type": "sliding_window_shuffled",
      "tokens": 234,
      "augmented": true
    },
    {
      "text": "2.3 Pros and Cons of Model Ensembling \nIn this section, we quantitatively evaluate (i) how effective ensembles are in terms of accuracy and latency compared to single models, and (ii) the challenges in deploying en- semble frameworks in a cost-effective fashion on a public cloud. Cocktail’s  autoscaling policy strikes parallels with Swayam’s distributed autoscaling; however, we further incorporate novel importance sampling techniques to reduce over-provisioning for under-used models. Table  2  provides a comprehensive comparison of  Cocktail with the most relevant works across key dimensions.",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "For relevance in comparison to prior work [ 27 ,  83 ] we chose image inference as our ensemble workload. While ensembling is applicable in other classiﬁcation workloads like product recommendations [ 24 , 53 ], text classiﬁcation [ 71 ] etc, the observations drawn are generic and applicable to other applications. 2.3 Pros and Cons of Model Ensembling \nIn this section, we quantitatively evaluate (i) how effective ensembles are in terms of accuracy and latency compared to single models, and (ii) the challenges in deploying en- semble frameworks in a cost-effective fashion on a public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "2.3.1 Ensembling Compared to Single Models \nTo analyze the accuracy offered by ensemble models, we con- duct an experiment using 10000 images from  ImageNet  [ 29 ] test dataset, on a  C5.xlarge  [ 8 ] instances in AWS EC2 [ 5 ]. While ensembling is applicable in other classiﬁcation workloads like product recommendations [ 24 , 53 ], text classiﬁcation [ 71 ] etc, the observations drawn are generic and applicable to other applications. For a given baseline model, we combine all models whose latency is lower than that of the baseline, and call it full- ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "The latency numbers for the baseline models and the corresponding ensemble models along with the size of the ensemble are shown in Table  3 . For a given baseline model, we combine all models whose latency is lower than that of the baseline, and call it full- ensemble. We perform ensembling on the predictions using a simple majority voting policy.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "Figure  3a , shows the accuracy comparison of the baseline (single) and static ensemble (ex- plained in Section  3 ) compared to the full-ensemble. The latency numbers for the baseline models and the corresponding ensemble models along with the size of the ensemble are shown in Table  3 . In majority voting, every model votes for a prediction for each input, and the ﬁnal output prediction is the one that receives more than half of the votes.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "It is evident that full-ensemble can achieve up to 1.65% better accuracy than single models. Besides accuracy again, ensembling can also achieve lower latency. Figure  3a , shows the accuracy comparison of the baseline (single) and static ensemble (ex- plained in Section  3 ) compared to the full-ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "Even a 10ms reduction in latency is of signiﬁcant importance to the providers [ 35 ]. Besides accuracy again, ensembling can also achieve lower latency. The latency of the ensemble is calculated as the time between start and end of the longest running model.As shown in Table  3 , in the case of  NASLarge , the ensemble latency is 2 ×  lower (151ms) than the baseline latency (311ms).",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Thus, depending on the model sub- set used in the ensemble, it achieves better accuracy than the baseline at lower latencies. We observe a similar trend of higher en- semble accuracy for other four baseline models with a latency reduction of up to 1.3 × . Even a 10ms reduction in latency is of signiﬁcant importance to the providers [ 35 ].",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "NASLarge IRV2 XceptionDNet121 NASMob 0 \n2 \n4 \n6 \nCost($) \nSingle-OD Ensemble-OD Ensemble-spot \n(b)  Cost of full-ensembling hosted on OD and Spot instances. Thus, depending on the model sub- set used in the ensemble, it achieves better accuracy than the baseline at lower latencies. Note that in our example model-set, the beneﬁts of ensembling will diminish for lower \nNASLarge IRV2 Xception DNet121 NASMob \n0.5 \n1.0 \n1.5 \nAccuracy Loss(%) \nStatic Single \n(a)  Accuracy loss compared to full- ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "Figure 3:  Cost and accuracy of ensembling vs single models. accuracies (< 75%) because single models can reach those accuracies. NASLarge IRV2 XceptionDNet121 NASMob 0 \n2 \n4 \n6 \nCost($) \nSingle-OD Ensemble-OD Ensemble-spot \n(b)  Cost of full-ensembling hosted on OD and Spot instances.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "accuracies (< 75%) because single models can reach those accuracies. Hence, based on the user constraints,  Cocktail chooses between ensemble and single models. 2.3.2 Ensembling Overhead \nWhile ensembling can boost accuracy with low latency, their distinctive resource hungry nature drastically increases the deployment costs when compared to single models.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "This is because more VMs or containers have to be procured to match the resource demands. 2.3.2 Ensembling Overhead \nWhile ensembling can boost accuracy with low latency, their distinctive resource hungry nature drastically increases the deployment costs when compared to single models. However, note that the “Packing factor” ( P f  ) for each model also impacts the deployment costs.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "P f  in this context is deﬁned as the number of inferences that can be executed concurrently in a single instance without violating the inference latency (on average). However, note that the “Packing factor” ( P f  ) for each model also impacts the deployment costs. Table  1  provides the  P f  for 11 different models when executed on a  C5.xlarge  instance.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "It can be seen that smaller models  (MNet, NASMob ) can be packed 2-5 ×  more when compared to larger models  (IRV2, NASLarge) . There is a linear relationship between  P f  and the instance size. Table  1  provides the  P f  for 11 different models when executed on a  C5.xlarge  instance.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "The beneﬁts of  P f  is contingent upon the models chosen by the model selection policy. It can be seen that smaller models  (MNet, NASMob ) can be packed 2-5 ×  more when compared to larger models  (IRV2, NASLarge) . Thus, the ensembles with models of higher  P f have signiﬁcantly lower cost.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "However, they do not right-size the model selection to include models which primarily contribute to the major- ity voting. The beneﬁts of  P f  is contingent upon the models chosen by the model selection policy. Existing ensemble model se- lection policies used in systems like Clipper use all off-the- shelf models and assign weights to them to calculate accu- racy.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "Ensemble-spot is explained further in the next section. We compare the cost of hosting ensembles using both spot (ensemble-spot) and OD (ensemble-OD) instances with the single models hosted on OD (single-OD) instances. However, they do not right-size the model selection to include models which primarily contribute to the major- ity voting.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "We run the experiment over a period of 1 hour for 10 requests/second. The cost is calculated as the cost per hour of EC2 c5.xlarge instance use, billed by AWS [ 5 ]. Ensemble-spot is explained further in the next section.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "We ensure all instances are fully utilized by packing multiple requests in accordance to the  P f  . As shown in Figure  3b , Ensemble-OD is always ex- pensive than single-OD for the all the models. The cost is calculated as the cost per hour of EC2 c5.xlarge instance use, billed by AWS [ 5 ].",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "3 Prelude to Cocktail \nTo speciﬁcally address the cost of hosting an ensembling- based model-serving framework in public clouds without sacriﬁcing the accuracy, this section introduces an overview of the two primary design choices employed in  Cocktail . Therefore, it is important to ensemble an “optimal” number of less compute intensive models to reduce the cost. As shown in Figure  3b , Ensemble-OD is always ex- pensive than single-OD for the all the models.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "3 Prelude to Cocktail \nTo speciﬁcally address the cost of hosting an ensembling- based model-serving framework in public clouds without sacriﬁcing the accuracy, this section introduces an overview of the two primary design choices employed in  Cocktail . How to reduce resource footprint? The ﬁrst step towards making model ensembling cost effective is to minimize the \n1044    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \number of models by pruning the ensemble, which reduces the overall resource footprint.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "From Figure  3a , it can be seen that the static policy has an accuracy loss of up to 1.45% when compared to full-ensemble, but is still better than single models. The ﬁrst step towards making model ensembling cost effective is to minimize the \n1044    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \number of models by pruning the ensemble, which reduces the overall resource footprint. In order to estimate the right number of models to participate in a given ensemble, we conduct an experiment where we chose top   N \n2   accurate models (static) from the full-ensemble of size  N .",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "Peacock Panda Quill Slug Cup Class \n0 \n50 \n100 \nAccuracy \nMNetV2 IRV2 NASLarge \nFigure 4:  Class-wise Accuracy. This implies that the models other than top   N \n2   yields a signiﬁcant 1.45% accuracy improvement in the full-ensemble but they cannot be statically determined. From Figure  3a , it can be seen that the static policy has an accuracy loss of up to 1.45% when compared to full-ensemble, but is still better than single models.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Peacock Panda Quill Slug Cup Class \n0 \n50 \n100 \nAccuracy \nMNetV2 IRV2 NASLarge \nFigure 4:  Class-wise Accuracy. Figure  4  shows the class-wise accuracy for three models on 5 distinct classes. Therefore, a full-ensemble model participation is not required for all the inputs because, every model is in- dividually suited to classify certain classes of images when compared to other classes.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Figure  4  shows the class-wise accuracy for three models on 5 distinct classes. It can be seen that for simpler classes like Slug,  MNetV2  can achieve similar accuracy as the bigger models, while for difﬁcult classes, like Cup and Quill, it experiences up to 3% loss in accuracy. Since the model participation for ensembling can vary based on the class of input images being classiﬁed, there is a scope to develop a dy- namic model selection policy that can leverage this class-wise variability to intelligently determine the number of models required for a given input.",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "This calls for a dynamic model selection policy which can accurately de- termine the number of models required, contingent upon the accuracy and scalability of the model selection policy. Since the model participation for ensembling can vary based on the class of input images being classiﬁed, there is a scope to develop a dy- namic model selection policy that can leverage this class-wise variability to intelligently determine the number of models required for a given input. Key Takeaway:  Full ensemble model-selection is an overkill, while static-ensemble leads to accuracy loss.",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "Although dynamic model selection poli- cies can signiﬁcantly reduce the resource footprint as shown in Figure  3b , the cost is still 20-30% higher when compared to a single model inference. How to save cost? This calls for a dynamic model selection policy which can accurately de- termine the number of models required, contingent upon the accuracy and scalability of the model selection policy.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "In  Cock- tail , we leverage these transient VMs such as spot instances to drastically reduce the cost of deploying ensembling model framework. Although dynamic model selection poli- cies can signiﬁcantly reduce the resource footprint as shown in Figure  3b , the cost is still 20-30% higher when compared to a single model inference. Most cloud providers offer tran- sient VMs such as Amazon Spot instances [ 69 ], Google Pre- emptible VMs [ 9 ], and Azure Low-priority VMs [ 7 ], that can reduce cloud computing costs by as much as 10 ×  [ 3 ].",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "In  Cock- tail , we leverage these transient VMs such as spot instances to drastically reduce the cost of deploying ensembling model framework. Figure  3b  shows that ensemble-spot can re- duce the cost by up to 3.3 ×  when compared to ensemble-OD. As an example, we host full-ensembling on AWS spot instances.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "For certain baselines like IRV2, ensemble-spot is also 1.5 × cheaper than single-OD. However, the crucial downside of using transient VMs is that they can be unilaterally preempted by the cloud provider at any given point due to reasons like in- crease in bid-price or provider-induced random interruptions. Figure  3b  shows that ensemble-spot can re- duce the cost by up to 3.3 ×  when compared to ensemble-OD.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "However, the crucial downside of using transient VMs is that they can be unilaterally preempted by the cloud provider at any given point due to reasons like in- crease in bid-price or provider-induced random interruptions. As we will discuss further,  Cocktail  is resilient to instance failures owing to the fault-tolerance of ensembling by com- puting multiple inferences for a single request. Key takeaway :  The cost-effectiveness of transient instances, is naturally suitable for hosting ensemble models.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Fast  Cache \nMobileNet NasNet \nResNet50 DenseNet121 \nDynamic Model  Selection \n. . Key takeaway :  The cost-effectiveness of transient instances, is naturally suitable for hosting ensemble models.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": ". . Aggregator \nMaster VM \nUser Requests \n… … … … … \nQueries Cost aware Procurement \nImportance Sampling \nModel-1  Model-2  Model-3  Model-4  Model-n  \noutput \nHeterogeneity \nPrediction Policy \nAutoscaler \nResource Controller \nLoad Balancer \n argmax O 1  (latency)  argmin O 2  (accuracy) \nCPU GPU CPU GPU \nObjectives \n1a \n3 \n4b \n1b \n2 4 \n4a \n4b \n1 \n6 \n6b \n6a \nw1 w2 w3 wk w4 \n3 \n5  Bin-Packing \nWeight Matrix \nL \nN \nFigure 5:  High-level overview of  Cocktail  design.",
      "type": "sliding_window_shuffled",
      "tokens": 153,
      "augmented": true
    },
    {
      "text": "Figure  5  depicts the high-level design of  Cocktail . 4 Overall Design of Cocktail \nMotivated by our observations, we design a novel model- serving framework,  Cocktail , that can deliver high-accuracy and low-latency predictions at reduced cost. Aggregator \nMaster VM \nUser Requests \n… … … … … \nQueries Cost aware Procurement \nImportance Sampling \nModel-1  Model-2  Model-3  Model-4  Model-n  \noutput \nHeterogeneity \nPrediction Policy \nAutoscaler \nResource Controller \nLoad Balancer \n argmax O 1  (latency)  argmin O 2  (accuracy) \nCPU GPU CPU GPU \nObjectives \n1a \n3 \n4b \n1b \n2 4 \n4a \n4b \n1 \n6 \n6b \n6a \nw1 w2 w3 wk w4 \n3 \n5  Bin-Packing \nWeight Matrix \nL \nN \nFigure 5:  High-level overview of  Cocktail  design.",
      "type": "sliding_window_shuffled",
      "tokens": 204,
      "augmented": true
    },
    {
      "text": "Figure  5  depicts the high-level design of  Cocktail . The participating models are made available in a model cache  1b for faster access and avoid re-computation for requests having similar constraints. Users submit requests to a master VM, which runs a model selection algorithm, 1a to decide the models to participate in the ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "The results from the workers are  ensembled  using an weighted majority voting aggregator  3  to agree upon a correct prediction. The participating models are made available in a model cache  1b for faster access and avoid re-computation for requests having similar constraints. Then, individual queries are dispatched to instances pools  2  dedicated for each model.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "First, it maintains dedicated instance pools to serve indi- vidual models which simpliﬁes the management and load balancing overheads for every model. The results from the workers are  ensembled  using an weighted majority voting aggregator  3  to agree upon a correct prediction. To efﬁciently address the resource management and scalability challenges,  Cocktail  applies multiple strategies.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "We also design an autoscaler  6  , which utilizes a prediction pol- icy  6a  to forecast the request load and scale instances for every model pool, thereby minimizing over-provisioning of resources. First, it maintains dedicated instance pools to serve indi- vidual models which simpliﬁes the management and load balancing overheads for every model. Next, the resource con- troller  4  handles instance procurement, by exploiting both CPU and GPU instances  4a  in a cost-aware  4b  fashion, while the load balancer  5  ensures all procured instances are bin- packed by assigning queries to appropriate instances.",
      "type": "sliding_window_shuffled",
      "tokens": 143,
      "augmented": true
    },
    {
      "text": "The autoscaler further employs an importance sam- pling  6b  algorithm to estimate the importance of each model pool by calculating percentage of request served by it in a given time interval. We also design an autoscaler  6  , which utilizes a prediction pol- icy  6a  to forecast the request load and scale instances for every model pool, thereby minimizing over-provisioning of resources. The key components of the design are explained in detail below.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "The key components of the design are explained in detail below. 4.1 Dynamic Model Selection Policy \nWe use a window-based dynamic model selection policy using two objective functions as described below. Objective functions : In order to reduce cost and latency while maximizing the accuracy, we deﬁne a latency-accuracy metric ( µ AL ) and cost metric ( µ c ): \nµ AL  =   Acc target \nLat target µ C  =  k  × N ∑ m = 1 \ninst _ cost \nP f m \nwhere  N  is the number of models used to ensemble and inst _ cost  is the VM cost.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "Objective functions : In order to reduce cost and latency while maximizing the accuracy, we deﬁne a latency-accuracy metric ( µ AL ) and cost metric ( µ c ): \nµ AL  =   Acc target \nLat target µ C  =  k  × N ∑ m = 1 \ninst _ cost \nP f m \nwhere  N  is the number of models used to ensemble and inst _ cost  is the VM cost. Our ﬁrst objective function ( O 1 ) is to the maximize  µ AL  such that target accuracy ( Acc target ) is reached within the target latency ( Lat target ). Each model  m  has a packing factor \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1045 \nP f m  and k is a constant which depends on the VM size in terms of vCPUs (xlarge, 2xlarge, etc).",
      "type": "sliding_window_shuffled",
      "tokens": 216,
      "augmented": true
    },
    {
      "text": "Cocktail takes the accuracy of each model as a probability of cor- rectness and then iteratively constructs a model list, where the joint probability of them performing the classiﬁcation is within the accuracy target. max µ AL  : \u001a Acc target  ≥ Acc target  ± Acc margin Lat target  ≤ Lat target  ± Lat margin \nTo solve  O 1 , we determine an initial model list by choosing the individual models satisfying  Lat target  and then create a probabilistic ensemble that satisﬁes the  Acc target . Our ﬁrst objective function ( O 1 ) is to the maximize  µ AL  such that target accuracy ( Acc target ) is reached within the target latency ( Lat target ).",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "Cocktail takes the accuracy of each model as a probability of cor- rectness and then iteratively constructs a model list, where the joint probability of them performing the classiﬁcation is within the accuracy target. Next, we solve for the second objective function ( O 2 ) by minimizing  µ C , while maintaining the target accuracy. We tolerate a 0.2% ( Acc margin ) and 5ms ( Lat margin ) variance in  Acc target  and  Lat target , respec- tively.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "Next, we solve for the second objective function ( O 2 ) by minimizing  µ C , while maintaining the target accuracy. For  N  models, where each model has a minimum accuracy ‘ a ’, we model the ensemble as a coin-toss problem, where  N  biased coins (with probability of head being  a ) are tossed together, and we need to ﬁnd the probability of major- ity of them being heads. min µ C  : \b Acc target  ≥ Acc target  ± Acc margin \n( O 2 )  is solved by resizing the model list of size  N  and fur- ther through intelligence resource procurement (described in section  4.2 ), and thus maximizing  P f  and minimizing  k  simul- taneously.",
      "type": "sliding_window_shuffled",
      "tokens": 173,
      "augmented": true
    },
    {
      "text": "For  N  models, where each model has a minimum accuracy ‘ a ’, we model the ensemble as a coin-toss problem, where  N  biased coins (with probability of head being  a ) are tossed together, and we need to ﬁnd the probability of major- ity of them being heads. For this, we need at least  ⌊ N \n2   ⌋ +  1 models to give the same results. The probability of correct prediction is given by \nN ∑ i = ⌊ N \n2   ⌋ + 1 \n\u0012 N i \n\u0013 a i   ( 1 − a ) ( N − i ) \nModel Selection Algorithm:  To minimize  µ C , we design a policy to downscale the number of models, if more than N/2+1 models vote for the same classiﬁcation result.",
      "type": "sliding_window_shuffled",
      "tokens": 178,
      "augmented": true
    },
    {
      "text": "For every monitoring interval, we keep track of the accuracy obtained from predicting all input images within the interval. The probability of correct prediction is given by \nN ∑ i = ⌊ N \n2   ⌋ + 1 \n\u0012 N i \n\u0013 a i   ( 1 − a ) ( N − i ) \nModel Selection Algorithm:  To minimize  µ C , we design a policy to downscale the number of models, if more than N/2+1 models vote for the same classiﬁcation result. Algo- rithm  1  describes the overall design of the model selection policy  1a  .",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "If the accuracy of the interval reaches the threshold accuracy (target + error_margin), we scale down the num- ber of available models in the ensemble. For every monitoring interval, we keep track of the accuracy obtained from predicting all input images within the interval. For consecutive sampling intervals, we calculate the  Mode  (most frequently occurring) of the majority vote received for every input.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "While down-scaling, we drop the models with the least prediction accuracy in that interval. If the  Mode  is greater than needed votes  ⌊ N / 2 ⌋ +  1  we prune the models to  ⌊ N / 2 ⌋ + 1 . For consecutive sampling intervals, we calculate the  Mode  (most frequently occurring) of the majority vote received for every input.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "While down-scaling, we drop the models with the least prediction accuracy in that interval. If there is a tie, we drop the model with least packing factor ( P f  ). It can so happen that dropping models can lead to drop in accuracy for certain intervals, because the class of images being predicted are different.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Algorithm 1  Model Selection and Weighted Majority Voting \n1:  procedure  F ULL _E NSEMBLE (M ODEL L IST , SLO) 2: for  model  ∈ ModelList  do 3: if  model.latency  ≤ SLO.latency  then 4: Model.add(model) 5: end if 6: end for O 1 7:  end procedure 8:  procedure  D YNAMIC _M ODEL _S CALING ( Models ) 9: if  curr_accuracy  ≥ accuracy_threshold  then \n10: if  max vote  >   N \n2   + 1  then  O 2 \n11: to _ be _ dropped  ← max vote  − N \n2   + 1 12: Models . It can so happen that dropping models can lead to drop in accuracy for certain intervals, because the class of images being predicted are different. In such cases, we up-size the models (one at a time) by adding most accurate model from the remaining unused models.",
      "type": "sliding_window_shuffled",
      "tokens": 230,
      "augmented": true
    },
    {
      "text": "append ( addModel ) 17: end if 18:  end procedure 19:  procedure  W EIGHTED _V OTING ( Models ) 20: for  model in  ∀ Models  do 21: class  ← model . drop ( to _ be _ dropped ) 13: end if 14: else 15: addModel  ← find _ models ( remaining _ models ) 16: Models . Algorithm 1  Model Selection and Weighted Majority Voting \n1:  procedure  F ULL _E NSEMBLE (M ODEL L IST , SLO) 2: for  model  ∈ ModelList  do 3: if  model.latency  ≤ SLO.latency  then 4: Model.add(model) 5: end if 6: end for O 1 7:  end procedure 8:  procedure  D YNAMIC _M ODEL _S CALING ( Models ) 9: if  curr_accuracy  ≥ accuracy_threshold  then \n10: if  max vote  >   N \n2   + 1  then  O 2 \n11: to _ be _ dropped  ← max vote  − N \n2   + 1 12: Models .",
      "type": "sliding_window_shuffled",
      "tokens": 266,
      "augmented": true
    },
    {
      "text": "class ] 23: end for 24: P class  ← max ( weighted _ vote , key  =  class ) 25: returnP class 26:  end procedure \n4.1.1 Class-based Weighted Majority Voting \nThe model selection policy described above ensures that we only use the necessary models in the majority voting. predicted _ class 22: weighted _ vote [ class ]+ =  weights [ model . append ( addModel ) 17: end if 18:  end procedure 19:  procedure  W EIGHTED _V OTING ( Models ) 20: for  model in  ∀ Models  do 21: class  ← model .",
      "type": "sliding_window_shuffled",
      "tokens": 145,
      "augmented": true
    },
    {
      "text": "class ] 23: end for 24: P class  ← max ( weighted _ vote , key  =  class ) 25: returnP class 26:  end procedure \n4.1.1 Class-based Weighted Majority Voting \nThe model selection policy described above ensures that we only use the necessary models in the majority voting. In or- der to increase the accuracy of majority voting, we design a weighted majority voting policy  3  . The weight matrix is designed by considering the accuracy of each model for each class, giving us a weight matrix of  L × N  dimension, where  L is the number of unique labels and  N  is the number of models used in the ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "The weight matrix is designed by considering the accuracy of each model for each class, giving us a weight matrix of  L × N  dimension, where  L is the number of unique labels and  N  is the number of models used in the ensemble. For instance, if there are 3 unique classes predicted by all the ensemble models, we sum the weights for all models of the same class. The majority vote is calculated as a sum of model-weights for each unique class in the individual prediction of the ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "The class with the maximum weight ( P class ) is the output of the majority vote. Hence, classes that did not get the highest votes can still be the ﬁnal output if the models associated with that class has a higher weight, than the combined weights of highest voted class. For instance, if there are 3 unique classes predicted by all the ensemble models, we sum the weights for all models of the same class.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "In order to determine the weight of every class, we use a per-class dictionary that keeps track of the correct predic- tions of every model per class. Hence, classes that did not get the highest votes can still be the ﬁnal output if the models associated with that class has a higher weight, than the combined weights of highest voted class. Unlike commonly used voting policies which assign weights based on overall correct predictions, our policy incor- porates class-wise information to the weights, thus making it more adaptable to different images classes.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "In order to determine the weight of every class, we use a per-class dictionary that keeps track of the correct predic- tions of every model per class. Similarly, our model selection pol- icy is also changed at runtime based on correct predictions seen during every interval. We populate the dictionary at runtime to avoid any inherent bias that could result from varying images over time.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Similarly, our model selection pol- icy is also changed at runtime based on correct predictions seen during every interval. Ties occur when two sets of equal number of models predict a different result. An important concern in majority voting is tie-breaking.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Ties occur when two sets of equal number of models predict a different result. The effectiveness \n1046    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nAlgorithm 2  Predictive Weighted Instance Auto Scaling \n1:  procedure  W EIGHTED _A UTOSCALING ( Stages ) 2: Predicted_load  ← DeepARN_Predict (load) 3: for  every Interval  do 4: for  model in  ∀ Models  do 5: model weight  ← get _ popularity ( model ) 6: Weight . append ( model weight ) 7: end for 8: end for 9: if  Predicted_load  ≥ Current_load  then 10: for  model in  ∀ Models  do 11: I_n  ← (Predicted_load - Current_load) × model weight 12: launch_workers ( est_VMs ) 13: model.workers.append ( est_VMs ) 14: end for 15: end if 16:  end procedure \nof weighted voting in breaking ties is discussed in Section  6 .",
      "type": "sliding_window_shuffled",
      "tokens": 253,
      "augmented": true
    },
    {
      "text": "4.2 Resource Management \nBesides model selection, it is crucial to design an optimized resource provisioning and management scheme to host the models cost-effectively. append ( model weight ) 7: end for 8: end for 9: if  Predicted_load  ≥ Current_load  then 10: for  model in  ∀ Models  do 11: I_n  ← (Predicted_load - Current_load) × model weight 12: launch_workers ( est_VMs ) 13: model.workers.append ( est_VMs ) 14: end for 15: end if 16:  end procedure \nof weighted voting in breaking ties is discussed in Section  6 . We explain in detail the resource procurement and autoscaling policy employed in  Cocktail .",
      "type": "sliding_window_shuffled",
      "tokens": 171,
      "augmented": true
    },
    {
      "text": "4.2.1 Resource Controller \nResource controller determines the cost-effective combina- tion of instances to be procured. We explain the details below. We explain in detail the resource procurement and autoscaling policy employed in  Cocktail .",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "GPU instances are cost-effective when packed with a large batch of requests for execution. We explain the details below. Resource Types : We use both CPU and GPU instances  4a depending on the request arrival load.",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "Hence, inspired from prior work [ 27 , 86 ], we de- sign an adaptive packing policy such that it takes into account the number of requests to schedule at time  T  and  P f  for every instance. GPU instances are cost-effective when packed with a large batch of requests for execution. The requests are sent to GPU instances only if the load matches the  P f  of the instance.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Prior to scaling-up instances, we need to estimate the cost  4b of running them along with existing instances. Cost-aware Procurement : The cost of executing in a fully packed instance determines how expensive is each instance. The requests are sent to GPU instances only if the load matches the  P f  of the instance.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "Prior to scaling-up instances, we need to estimate the cost  4b of running them along with existing instances. At any given time  T , based on the predicted load ( L p ) and running instances R N , we use a cost-aware greedy policy to determine the num- ber of additional instances required to serve as  A n  =  L p  − C r , where  C r  =  ∑ N i = 1   P f i , is the request load which can be handled with  R N . To procure  A n  instances, we greedily calculate the least cost instance as  min ∀ i ∈ instances Cost i  × A n / P f i .",
      "type": "sliding_window_shuffled",
      "tokens": 164,
      "augmented": true
    },
    {
      "text": "To procure  A n  instances, we greedily calculate the least cost instance as  min ∀ i ∈ instances Cost i  × A n / P f i . Depend- ing on the cost-effectiveness ratio of  A n / P f i , GPUs will be preferred over CPU instances. Load Balancer : Apart from procuring instances, it is quintessential to design a load balancing and bin-packing  5 strategy to fully utilize all the provisioned instances.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "We maintain a request queue at every model pool. Load Balancer : Apart from procuring instances, it is quintessential to design a load balancing and bin-packing  5 strategy to fully utilize all the provisioned instances. In order to increase the utilization of all instances in a pool at any given time, the load balancer submits every request from the queue to the lease remaining free slots (viz.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "instance packing factor P f  ). In order to increase the utilization of all instances in a pool at any given time, the load balancer submits every request from the queue to the lease remaining free slots (viz. This is similar to an online bin-packing algorithm.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "We use an idle-timeout limit for 10 minutes to recycle unused \ninstances from every model pool. Hence, greedily assigning requests enables early scale down of lightly loaded instances. This is similar to an online bin-packing algorithm.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "Hence, greedily assigning requests enables early scale down of lightly loaded instances. Though reactive policies (used in Clipper and InFaas) can be employed which take into account metrics like CPU utilization [ 83 ], these policies are slow to react when there is dynamism in request rates. 4.2.2 Autoscaler \nAlong with resource procurement, we need to autoscale instances to satisfy the incoming query load.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "Though reactive policies (used in Clipper and InFaas) can be employed which take into account metrics like CPU utilization [ 83 ], these policies are slow to react when there is dynamism in request rates. In  Cocktail , we use a load prediction model that can accurately forecast the anticipated load for a given time interval. Proactive policies with request prediction are know to have superior performance [ 86 ] and can co-exist with reactive policies.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Using the predicted load  6a  ,  Cocktail  spawns additional instances, if necessary, for every instance pool. In  Cocktail , we use a load prediction model that can accurately forecast the anticipated load for a given time interval. In addition, we sample SLO violations for every 10s interval and reactively spawn additional instances to every pool based on aggregate resource utilization of all instances.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "Prediction Policy : To effectively capture the \nModel RMSE MWA 77.5 EWMA 88.25 Linear R. 87.5 Logsitic R. 78.34 Simple FF. This captures SLO violations due to mis-predictions. In addition, we sample SLO violations for every 10s interval and reactively spawn additional instances to every pool based on aggregate resource utilization of all instances.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "45.45 LSTM 28.56 DeepArEst 26.67 \nTable 4:  Prediction models. different load arrival patterns, we design a DeepAR- estimator (DeepARest) based prediction model. Prediction Policy : To effectively capture the \nModel RMSE MWA 77.5 EWMA 88.25 Linear R. 87.5 Logsitic R. 78.34 Simple FF.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "We zeroed in on the choice of using DeepARest by conducting (Table  4 ) an in-depth com- parison of the accuracy loss when compared with other state-of-the-art traditional and ML-based prediction models used in prior works [ 47 ,  86 ]. As shown in Algorithm  2 , for every model under a periodic scheduling interval of 1 minute ( T s ), we use the  Predicted _load ( L p ) at time  T  +  T p  and compare it with the  current_load  to determine the number of instances ( I n ). different load arrival patterns, we design a DeepAR- estimator (DeepARest) based prediction model.",
      "type": "sliding_window_shuffled",
      "tokens": 163,
      "augmented": true
    },
    {
      "text": "( T s ) is set to 1 minute as it is the typical instance provisioning time for EC2 VMs. As shown in Algorithm  2 , for every model under a periodic scheduling interval of 1 minute ( T s ), we use the  Predicted _load ( L p ) at time  T  +  T p  and compare it with the  current_load  to determine the number of instances ( I n ). T p  is deﬁned as the average launch time for new instances.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "( T s ) is set to 1 minute as it is the typical instance provisioning time for EC2 VMs. To calculate ( L p ), we sample the arrival rate in adjacent windows of size  W over the past S seconds. Using the global arrival rate from all windows, the model predicts ( L p ) for  T p  time units from  T .",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "T p  is set to 10 minutes because it is sufﬁcient time to capture the variations in long-term future. Using the global arrival rate from all windows, the model predicts ( L p ) for  T p  time units from  T . All these parameters are tunable based on the system needs.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "Autoscaling the instances equally for every model based on predicted load, would inherently lead to over-provisioned instances for under-used models. Importance Sampling:  An important concern in autoscaling is that the model selection policy dynamically determines the models in the ensemble for a given request constraints. All these parameters are tunable based on the system needs.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Autoscaling the instances equally for every model based on predicted load, would inherently lead to over-provisioned instances for under-used models. To address this concern, we design a weighted autoscaling policy which intelligently auto-scales instances for every pool based on the weights. As shown in Algorithm  2 , weights are determined by frequency in which a particular model is chosen for requests ( get_popularity ) with respect to other models in the ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1047 \nThe weights are multiplied with the predicted load to scale instances  (launch_workers)  for every model pool. We name this as an importance sampling  6b  technique, because the model pools are scaled proportional to their popularity. As shown in Algorithm  2 , weights are determined by frequency in which a particular model is chosen for requests ( get_popularity ) with respect to other models in the ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "Cocktail is open-sourced at  https:// github.com/jashwantraj92/cocktail \n5.1 Cocktail Prototype Implementation \nCocktail  is implemented using 10KLOC of  Python . 5 Implementation and Evaluation \nWe implemented a prototype of  Cocktail  and deployed it on AWS EC2 [ 5 ] platform The details of the implementation are described below. We name this as an importance sampling  6b  technique, because the model pools are scaled proportional to their popularity.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Cocktail is open-sourced at  https:// github.com/jashwantraj92/cocktail \n5.1 Cocktail Prototype Implementation \nCocktail  is implemented using 10KLOC of  Python . We de- signed  Cocktail  as a client-server architecture, where one master VM receives all the incoming requests which are sent to individual model worker VMs. Master-Worker Architecture : The master node handles the major tasks such as (i) concord model selection policy, (ii) request dispatch to workers VMs as asynchronous future tasks using  Python asyncio  library, and (iii) ensembling the pre- diction from the worker VMs.",
      "type": "sliding_window_shuffled",
      "tokens": 156,
      "augmented": true
    },
    {
      "text": "reside in the mas- ter node. Also all VM speciﬁc metrics such as current_load, CPU utilization, etc. Master-Worker Architecture : The master node handles the major tasks such as (i) concord model selection policy, (ii) request dispatch to workers VMs as asynchronous future tasks using  Python asyncio  library, and (iii) ensembling the pre- diction from the worker VMs.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "It runs on a  C5.16x  [ 8 ] large instance to handle these large volume of diverse tasks. reside in the mas- ter node. Each worker VMs runs a client process to serve its corresponding model.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "The requests are served as independent parallel threads to ensure timely predictions. We use  Python Sanic  web-server for commu- nication with the master and worker VMs. Each worker VMs runs a client process to serve its corresponding model.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "Each worker VM runs tensorﬂow-serving [ 60 ] to serve the inference requests. We use  Python Sanic  web-server for commu- nication with the master and worker VMs. Load Balancer : The master VMs runs a separate thread to monitor the importance sampling of all individual model pools.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "It keeps track of the number of requests served per model in the past 5 minutes. Load Balancer : The master VMs runs a separate thread to monitor the importance sampling of all individual model pools. This information is used for cal- culating the weights per model for autoscaling decisions.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "We integrate a  mongodb  [ 21 ] database in the master node to main- tain all information about procured instances, spot-instance price list, and instance utilization. This information is used for cal- culating the weights per model for autoscaling decisions. The load prediction model resides in the master VM which constantly records the arrival rate in adjacent windows.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "Recall that the details of the pre- diction were described in Section  4.2.2 . The DeepAREst [ 4 ] model was trained using  Keras  [ 22 ] and  Tensorflow , over 100 epochs with 2 layers, 32 neurons and a batch-size of 1. The load prediction model resides in the master VM which constantly records the arrival rate in adjacent windows.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "The DeepAREst [ 4 ] model was trained using  Keras  [ 22 ] and  Tensorflow , over 100 epochs with 2 layers, 32 neurons and a batch-size of 1. The constraints are deﬁned as  <latency,accuracy>  pair. Model Cache : We keep track of the model selected for en- sembling on a per request constraint basis.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "The queries arriving with similar constraints can read the model cache to avoid re-computation for selecting the models. The model cache is implemented as a hash-map using  Redis  [ 16 ] in-memory key-value store for fast access. The constraints are deﬁned as  <latency,accuracy>  pair.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "The model cache is implemented as a hash-map using  Redis  [ 16 ] in-memory key-value store for fast access. Constraint speciﬁcation : We expose a simple API to de- velopers, where they can specify the type of inference task (e.g., classiﬁcation) along with the  <latency,accuracy> constraints. Developers also need to indicate the primary ob- jective between these two constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Developers also need to indicate the primary ob- jective between these two constraints. chooses a set of single or ensemble models required to meet the developer speciﬁed constraints. Cocktail  automatically \nDataset Application Classes Train-set Test-set ImageNet [ 29 ] Image 1000 1.2M 50K CIFAR-100 [ 50 ] Image 100 50K 10K SST-2 [ 72 ] Text 2 9.6K 1.8K SemEval [ 66 ] Text 3 50.3K 12.2K \nTable 5:  Benchmark Applications and datasets.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Discussion:  Our accuracy and latency constraints are limited to the measurements from the available pretrained models. chooses a set of single or ensemble models required to meet the developer speciﬁed constraints. Note that changing the models or/and framework would lead to minor deviations.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "All decisions related to VM au- toscaling, bin-packing and load-prediction are reliant on the centralized mongodb database, which can become a potential bottleneck in terms of scalability and consistency. Note that changing the models or/and framework would lead to minor deviations. While providing latency and top-1% ac- curacy of the pretrained models is an ofﬂine step in  Cocktail , we can calculate these values through one-time proﬁling and use them in the framework.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "The DeepARest model is pre-trained using 60% of the arrival trace. All decisions related to VM au- toscaling, bin-packing and load-prediction are reliant on the centralized mongodb database, which can become a potential bottleneck in terms of scalability and consistency. This can be mitigated by using fast distributed solutions like Redis [ 16 ] and Zookeeper [ 46 ].",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "For varying load patterns, the model parameters can be updated by re-training in the background with new arrival rates. The DeepARest model is pre-trained using 60% of the arrival trace. 5.2 Evaluation Methodology \nWe evaluate our prototype implementation on AWS EC2 [ 8 ] platforms.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "5.2 Evaluation Methodology \nWe evaluate our prototype implementation on AWS EC2 [ 8 ] platforms. Speciﬁcally, we use  C5.xlarge, 2xlarge, 4xlarge, 8xlarge  for CPU instances and  p2.xlarge  for GPU instances. Load Generator:  We use different traces which are given as input to the load generator.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "The second trace is production twitter [ 48 ] trace which is bursty with unexpected load spikes. Firstly, we use real-world re- quest arrival traces from Wikipedia [ 76 ], which exhibit typical characteristics of ML inference workloads as it has recurring diurnal patterns. Load Generator:  We use different traces which are given as input to the load generator.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "We use the ﬁrst 1 hour sample of both the traces and they are scaled to have an average request rate of 50 req/sec. Workload:  As shown in Table  5  we use image-classiﬁcation and Sentiment Analysis (text) applications with two datasets each for our evaluation. The second trace is production twitter [ 48 ] trace which is bursty with unexpected load spikes.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "Workload:  As shown in Table  5  we use image-classiﬁcation and Sentiment Analysis (text) applications with two datasets each for our evaluation. Sentiment analysis outputs the sen- timent of a given sentence as positive negative and (or) neu- tral. We use 9 different prominently used text-classiﬁcation models from transformers library [ 81 ] (details available in appendix) designed using Google BERT [ 30 ] architecture trained on  SST  [ 72 ] and  SemEval  [ 66 ] dataset.",
      "type": "sliding_window_shuffled",
      "tokens": 125,
      "augmented": true
    },
    {
      "text": "Each request from the load-generator is modelled after a query with spe- ciﬁc  <latency,accuracy>  constraints. We use 9 different prominently used text-classiﬁcation models from transformers library [ 81 ] (details available in appendix) designed using Google BERT [ 30 ] architecture trained on  SST  [ 72 ] and  SemEval  [ 66 ] dataset. The queries consist of images or sentences, which are randomly picked from the test dataset.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "In our experiments, we use ﬁve different types of these constraints. The queries consist of images or sentences, which are randomly picked from the test dataset. As an example for the  Imagenet  dataset shown in Figure  6 , each constraint is a representative of <latency, accuracy> com- bination offered by single models (shown in Table  1 ).",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "(categorized by dotted lines) picked in the increasing order of accuracy. We use one constraint (blue dots) each from ﬁve different regions \n1048    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \n0 \n100 \n200 \n300 \n400 \n70 75 80 85 \nLatency (ms) \nAccuracy (%) \nConst1     Const2       Const3   Const4  Const5 \nFigure 6:  Constraints used in our workloads. As an example for the  Imagenet  dataset shown in Figure  6 , each constraint is a representative of <latency, accuracy> com- bination offered by single models (shown in Table  1 ).",
      "type": "sliding_window_shuffled",
      "tokens": 149,
      "augmented": true
    },
    {
      "text": "Note that the latency is the raw model execution latency, and does not include the addi- tional network-transfer overheads incurred. (categorized by dotted lines) picked in the increasing order of accuracy. Each of these picked constraints (named const1 - const5 in the Figure) represents a single baseline model, whose corresponding ensemble size ranges from small (2) to large (10), as shown in Table  3 .",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "We picked the constraints using a similar procedure by ordering constraints across ﬁve different categories for  CIFAR-100 ,  SST-2  and SemEval  (twitter tweets) datasets. The list of models used for them are given in the Appendix. Note that the latency is the raw model execution latency, and does not include the addi- tional network-transfer overheads incurred.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "We model two different workload mixes by using a combination of these ﬁve query constraint types. Based on the decreasing order of accuracy, we categorize them into  Strict  and  Relaxed  workloads. The list of models used for them are given in the Appendix.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "5.2.1 Evaluation Metrics \nMost of our evaluations of  Cocktail  for image-classiﬁcation are performed using the  Imagenet  dataset. Based on the decreasing order of accuracy, we categorize them into  Strict  and  Relaxed  workloads. To further demon- strate the sensitivity of Cocktail to dataset and applicability to other classiﬁcation applications, we also evaluate it us- ing  CIFAR-100  and Sentiment-Analysis application.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "To further demon- strate the sensitivity of Cocktail to dataset and applicability to other classiﬁcation applications, we also evaluate it us- ing  CIFAR-100  and Sentiment-Analysis application. The response latency metric includes model inference latency, communication/network latency and syn- chronization overheads. We use three important metrics: response latency, cost and accuracy for evaluating and comparing our design to other state-of- the-art systems.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "The cost metric is the billing cost from AWS, and the accuracy metric is measured as the percentage of requests that meet the target accuracy requirements. The response latency metric includes model inference latency, communication/network latency and syn- chronization overheads. Queries that do not meet response latency requirements (>700ms) are considered as SLO vio- lations.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "The cost metric is the billing cost from AWS, and the accuracy metric is measured as the percentage of requests that meet the target accuracy requirements. We compare these metrics for  Cocktail  against (i)  In- Faas  [ 83 ], which is our baseline that employs single model selection policy; (ii)  Clipper  [ 27 ], which uses static full model selection policy (analogous to AWS AutoGluon); and (iii) Clipper-X  which is an enhancement to  Clipper  with a simple model selection (drop one model at a time) that does not uti- lize the  mode -based policy enforced in  Cocktail . Both  InFaas and  Clipper  share  Cocktail ’s implementation setup to ensure a fair comparison with respect to our design and execution environment.",
      "type": "sliding_window_shuffled",
      "tokens": 179,
      "augmented": true
    },
    {
      "text": "For instance, both  Clipper  and  InFaas  employ variants of a reactive autoscaler as described in Section  4.2.2 . Both  InFaas and  Clipper  share  Cocktail ’s implementation setup to ensure a fair comparison with respect to our design and execution environment. However, in our setup, both beneﬁt from the distributed au- toscaling and prediction policies, thus eliminating variability.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "However, in our setup, both beneﬁt from the distributed au- toscaling and prediction policies, thus eliminating variability. Also note that  InFaas  is deployed using OnDemand instances, while both  Clipper  and  Cocktail  use spot instances. 6 Analysis of Results \nThis section discusses the experimental results of  Cocktail using the Wiki and Twitter traces.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "To summarize the overall results, Cocktail providing 2 ×  reduction in latency, while meeting the accuracy for up-to 96% of the requests under reduced deployment cost by 1.4 × , when compared to  InFaaS and  Clipper . 6.1 Latency, Accuracy and Cost Reduction \nLatency Distribution : Figure  7  shows the distribution of to- tal response latency in a standard box-and-whisker plot. 6 Analysis of Results \nThis section discusses the experimental results of  Cocktail using the Wiki and Twitter traces.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "6.1 Latency, Accuracy and Cost Reduction \nLatency Distribution : Figure  7  shows the distribution of to- tal response latency in a standard box-and-whisker plot. The total response latency includes additional 200-300ms incurred for query serialization and data transfer over network. The boundaries of the box-plots depict the 1st quartile (25th per- centile (PCTL)) and 3rd quartile (75th PCTL), the whiskers plot the minimum and maximum (tail) latency and the middle line inside the box depict the median (50 PCTL).",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "It can be seen that the maximum latency of  Cocktail  is similar to the 75th PCTL latency of  InFaas . The total response latency includes additional 200-300ms incurred for query serialization and data transfer over network. This is because the single model inference have up to 2x higher latency to achieve higher accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "This is because the single model inference have up to 2x higher latency to achieve higher accuracy. In contrast, both Cocktail  and  Clipper  can reach the accuracy at lower latency due to ensembling, thus minimizing SLO violations to 1%. Consequently, this leads to 35% SLO violations for  InFaas  in the case of Strict workload.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "Note that the tail latency of  Clipper  is still higher than  Cocktail  because  Clipper ensembles more models than  Cocktail , thereby resulting in straggler tasks in the VMs. In contrast, both Cocktail  and  Clipper  can reach the accuracy at lower latency due to ensembling, thus minimizing SLO violations to 1%. Also, the tail latency is higher for Twitter trace (Figure  7c , 7d ) owing to its bursty nature.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "Note that the tail latency of  Clipper  is still higher than  Cocktail  because  Clipper ensembles more models than  Cocktail , thereby resulting in straggler tasks in the VMs. The difference in latency between Cocktail  and  InFaas  is lower for  Relaxed  workload when compared to  Strict  workload (20% lower in tail). Since the Relaxed  workload has much lower accuracy constraints, smaller models are able to singularly achieve the accuracy requirements at lower latency.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Since the Relaxed  workload has much lower accuracy constraints, smaller models are able to singularly achieve the accuracy requirements at lower latency. Accuracy violations : The accuracy is mea- sured as a moving window average with size 200 for all the requests in the workload. Accuracy Met (%) Scheme Strict Relaxed InFaas 21 71 Clipper 47 89 Cocktail 56 96 \nTable 6:  Requests meeting target accuracy averaged for both Trace.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "Accuracy Met (%) Scheme Strict Relaxed InFaas 21 71 Clipper 47 89 Cocktail 56 96 \nTable 6:  Requests meeting target accuracy averaged for both Trace. Both  Clipper  and  Cock- tail  can meet the ac- curacy for 56% of re- quests, which is 26% and 9% more than  In- Faas  and  Clipper  re- spectively. This is be- cause, intuitively ensembling leads to higher accuracy than single models.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "Since majority voting can include ties in votes, we analyzed the number of ties, which were correctly predicted for all the queries. However,  Cocktail  is still 9% better than  Clip- per  because the class-based weighted voting, is efﬁcient in breaking ties when compared to weighting averaging used in Clipper . This is be- cause, intuitively ensembling leads to higher accuracy than single models.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Cocktail  was able to deliver correct predic- tions for 35% of the tied votes, whereas breaking the ties in Clipper  led only to 20% correct predictions. USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1049 \nInFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp. Since majority voting can include ties in votes, we analyzed the number of ties, which were correctly predicted for all the queries.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1049 \nInFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp. Latency (ms) \n(a)  Wiki-trace:  Strict  workload. InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Latency (ms) \n(b)  Wiki-trace:  Relaxed  workload. InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nLatency \n(c)  Twitter-trace:  Strict  workload. InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "Latency (ms) \n(d)  Twitter-trace:  Relaxed  workload. InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nResp. InFaas Clipper Cocktail Policy \n0 \n500 \n1000 \n1500 \nLatency \n(c)  Twitter-trace:  Strict  workload.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "Figure 7:  Latency Distribution of  InFaas ,  Clipper  and  Cocktail  for two workload mixes using both Wiki and Twitter traces. Latency (ms) \n(d)  Twitter-trace:  Relaxed  workload. Strict Relaxed 0 \n20 \n40 \n60 \n80 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(a)  Wiki Trace.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "Strict Relaxed 0 \n50 \n100 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace. Strict Relaxed 0 \n20 \n40 \n60 \n80 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(a)  Wiki Trace. Figure 8:  Cost savings of  Cocktail  compared to three schemes.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "IRV2 \nDNet201 \nNASMob \nDNet121 \nXcep \nMNet \nIncep \nMNetV2 \nRNet50V2 \nRNet50 \nModel \n0 \n50 \n100 \nImportance(%) \n(b)  Distribution of requests served by each individual model. Const1 Const2 Const3 Const4 Query \n0 \n5 \n10 \n#Models \nClipper Clipper-X Cocktail \n(a)  Average number of models used in the ensemble. Figure 8:  Cost savings of  Cocktail  compared to three schemes.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Figure 9:  Beneﬁts of dynamic model selection policy. Note that, changing the target accuracy to tolerate a 0.5% loss, increases the percentage of requests that meet accuracy to 81% for  Cocktail , when compared to 61% for  InFaas . IRV2 \nDNet201 \nNASMob \nDNet121 \nXcep \nMNet \nIncep \nMNetV2 \nRNet50V2 \nRNet50 \nModel \n0 \n50 \n100 \nImportance(%) \n(b)  Distribution of requests served by each individual model.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "The requests meeting accuracy are generally higher for the Relaxed  workload because the target accuracy is much lower. Overall,  Cocktail  was able to deliver an accuracy of 83% and 79.5% on average for the  Strict  and  Relaxed  workloads, respectively. Note that, changing the target accuracy to tolerate a 0.5% loss, increases the percentage of requests that meet accuracy to 81% for  Cocktail , when compared to 61% for  InFaas .",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Overall,  Cocktail  was able to deliver an accuracy of 83% and 79.5% on average for the  Strict  and  Relaxed  workloads, respectively. We do not plot the results for Clipper-X , which achieves similar accuracy to  Cocktail , but uses more models as explained in Section  6.2.1 . This translates to 1.5% and 1% better accuracy than  Clipper  and  InFaas .",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Cost Comparison:  Figure  8  plots the cost savings of Cocktail  when compared to  InFaas ,  Clipper  and  Clipper-X policies. We do not plot the results for Clipper-X , which achieves similar accuracy to  Cocktail , but uses more models as explained in Section  6.2.1 . It can be seen that,  Cocktail  is up to 1.45 ×  more cost effective than  InFaas  for  Strict  workload.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "In addition, Cocktail  reduces cost by 1.35 ×  and 1.27 ×  compared to Clipper  and  Clipper-X  policies, owing to its dynamic model selection policy, which minimizes the resource footprint of ensembling. It can be seen that,  Cocktail  is up to 1.45 ×  more cost effective than  InFaas  for  Strict  workload. On the other hand,  Clipper  uses all models in ensemble and the  Clipper-X  policy does not right size the models as aggressively as  Clipper , hence they are more expensive.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "This is because the twitter workload is bursty, thereby leading to intermittent over-provisioned VMs. Note that, all the schemes incur higher cost for twitter trace (Figure  8b ) compared to wiki trace (Figure  8a ). On the other hand,  Clipper  uses all models in ensemble and the  Clipper-X  policy does not right size the models as aggressively as  Clipper , hence they are more expensive.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "6.2 Key Sources of Improvements \nThe major improvements in terms of cost, latency, and accu- \nracy in  Cocktail  are explained below. This is because the twitter workload is bursty, thereby leading to intermittent over-provisioned VMs. For brevity in explana- tion, the results are averaged across Wiki and Twitter traces for strict workload.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "Here,  Cocktail  reduces the number of models by up to 55% for all four query types. For brevity in explana- tion, the results are averaged across Wiki and Twitter traces for strict workload. 6.2.1 Beneﬁts from dynamic model selection \nFigure  9a  plots the average number of models used for queries falling under the ﬁrst four different constraint (const) types.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Clipper , on the other hand, is static and always uses all the models. This is because our dynamic pol- icy ensures that the number of models are well within  N / 2 most of the time, whereas the  Clipper-X  policy does not ag- gressively scale down models. Here,  Cocktail  reduces the number of models by up to 55% for all four query types.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "Still, the savings in terms of cost will be signiﬁcant because even removing one model from the ensemble amounts to  ∼ 20% cost savings in the long run ( Clipper  vs  Clipper-X  ensemble in Figure  8 ). The percentage of model-reduction is lower for  Const2 , 3 and 4 because, the total models used in the ensemble is less than  Const1  (8, 7 and 6 models, respectively). Clipper , on the other hand, is static and always uses all the models.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Figure  9b  shows the breakdown of the percentage of re- quests ( Const1 ) served by the each model. Thus, the beneﬁts of  Cocktail  are substantial for large ensembles while reducing the number of models for medium-sized ensembles. Still, the savings in terms of cost will be signiﬁcant because even removing one model from the ensemble amounts to  ∼ 20% cost savings in the long run ( Clipper  vs  Clipper-X  ensemble in Figure  8 ).",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "Figure  9b  shows the breakdown of the percentage of re- quests ( Const1 ) served by the each model. As seen, Incep- tionResNetV2, Densenet-201, Densenet121, NasnetMobile and Xception are the top-5 most used models in the ensem- ble. Based on Table  1 , if we had statically taken the top  N / 2 most accurate models, NasNetmobile would not have been included in the ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "Based on Table  1 , if we had statically taken the top  N / 2 most accurate models, NasNetmobile would not have been included in the ensemble. Further, the other 5 models are used by up to 25% of the images. However, based on the input im- ages sent in each query, our model selection policy has been able to identify NasNetMobile to be a signiﬁcantly contribut- ing model in the ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Further, the other 5 models are used by up to 25% of the images. Not including them in the ensemble would have led to severe loss in accuracy. But, our dynamic policy with the class-based weighted voting, adapts to input images in a given interval by accurately selecting the best performing model for each class.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "But, our dynamic policy with the class-based weighted voting, adapts to input images in a given interval by accurately selecting the best performing model for each class. To further demonstrate the effectiveness of our dynamic model selection, Figure  10b , 10c  plots the number models in every sampling interval along with cumulative accuracy and window accuracy within each sampling interval for three schemes. We observe that  Cocktail  can effectively scale up and scale down the mod- els while maintaining the cumulative accuracy well within the threshold.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Figure (d) shows the effects of distributed autoscaling with importance sampling. We observe that  Cocktail  can effectively scale up and scale down the mod- els while maintaining the cumulative accuracy well within the threshold. More than 50% of the time the number of models are maintained between 4 to 5, because the dynamic policy is quick in detecting accuracy failures and recovers immediately \n1050    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \n(a)  Clipper \n(b)  Clipper-X \n(c)  Cocktail \nFigure 10:  Figures (a), (b) and (c) shows the number of models used in ensemble with corresponding cumulative accuracy and window accuracy over a 1 hour period for requests under  Const1 .",
      "type": "sliding_window_shuffled",
      "tokens": 160,
      "augmented": true
    },
    {
      "text": "Strict Relaxed 0 \n25 \n50 \n75 \n#VMs \nInFaas Clipper Clipper-X Cocktail \n(a)  Wiki Trace. Figure (d) shows the effects of distributed autoscaling with importance sampling. Strict Relaxed 0 \n25 \n50 \n75 \n#VMs \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Figure 11:  Number of VMs spawned for all four schemes. Strict Relaxed 0 \n25 \n50 \n75 \n#VMs \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace. 0 1000 2000 3000 Time interval (10s) \n0 \n50 \n100 \n#VMs \nBline model1 model2 model3 \n(a)  Cumulative #VMs.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "0 25 50 75 Time interval (10s) \n79 \n80 \n81 \n82 \nAccuracy \nBL1 BL2 \nBL3 const1 \nconst2 const3 \n(b)  Failure Analysis. 0 1000 2000 3000 Time interval (10s) \n0 \n50 \n100 \n#VMs \nBline model1 model2 model3 \n(a)  Cumulative #VMs. Figure 12:  Sensitivity analysis of VMs.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "by scaling up models. Figure 12:  Sensitivity analysis of VMs. However,  Clipper-X  does not scale down models as frequently as  Cocktail , while ensuring similar accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "However,  Clipper-X  does not scale down models as frequently as  Cocktail , while ensuring similar accuracy. 6.2.2 Beneﬁts from Autoscaling \nFigure  11  plots the reduction in the number of VMs used by all four schemes. Clipper  is less accurate than  Cocktail  and further it uses all 10 models throughout.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "6.2.2 Beneﬁts from Autoscaling \nFigure  11  plots the reduction in the number of VMs used by all four schemes. Cocktail  spawns 29% lesser VMs on top of Clipper-X , because it is not aggressive enough like  Cocktail to downscale more models at every interval. It can be seen that both  Cocktail  and  Clipper-X spawn 49% and 20% fewer VMs than  Clipper  for workload-1 on Twitter trace.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "It is to be noted that the savings are lower for  Relaxed  workload because, the number of models in the ensemble are inherently low, thus leading to reduced beneﬁts from scaling down the models. Cocktail  spawns 29% lesser VMs on top of Clipper-X , because it is not aggressive enough like  Cocktail to downscale more models at every interval. Intuitively,  InFaas  has the least number of VMs spawned because it does not ensemble models.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "Cocktail  spawns upto 50% more VMs than  InFaas , but in turns reduces accuracy loss by up to 96%. To further capture the beneﬁts of the weighted autoscal- ing policy, Figure  12a  plots the number of VMs spawned over time for the top-3 most used models in the ensemble for  Const1 . Intuitively,  InFaas  has the least number of VMs spawned because it does not ensemble models.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "To further capture the beneﬁts of the weighted autoscal- ing policy, Figure  12a  plots the number of VMs spawned over time for the top-3 most used models in the ensemble for  Const1 . Not adopting an importance sampling based weighted policy would result in equivalent number of VMs as the Bline for all models. The Bline denotes number of VMs that would be spawned without applying the weights.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Figure  9b  shows the most used models in decreasing order of importance. How- ever, since  Cocktail  exploits importance sampling by keeping track of the frequency in which models are selected, the num- \nber of VMs spawned for model1, model2 and model-3 is upto 3 ×  times lesser than uniform scaling. Not adopting an importance sampling based weighted policy would result in equivalent number of VMs as the Bline for all models.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Despite using multiple models for a single inference, importance sampling combined with aggressive model pruning, greatly reduces the resource foot- print which directly translates to the cost savings in  Cocktail . The au- toscaling policy effectively utilizes this importance factor in regular intervals of 5 minutes. Figure  9b  shows the most used models in decreasing order of importance.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "Despite using multiple models for a single inference, importance sampling combined with aggressive model pruning, greatly reduces the resource foot- print which directly translates to the cost savings in  Cocktail . 6.2.3 Beneﬁts of Transient VMs \nThe cost-reductions in  Cocktail  are akin to cost-savings of transient VMs compared to On-Demand (OD) VMs. We pro- ﬁle the spot price of 4 types of  C5  EC2 VMs over a 2-week period in August 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "We pro- ﬁle the spot price of 4 types of  C5  EC2 VMs over a 2-week period in August 2020. When compared to the OD price , they were up to 70% cheaper. It was seen that, the spot instance prices have predictable ﬂuctuations.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "When compared to the OD price , they were up to 70% cheaper. Note that, we set the bidding price conser- vatively to 40% of OD. This price gap is cap- italized in  Cocktail  to reduce the cost of instances consumed by ensembling.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "Note that, we set the bidding price conser- vatively to 40% of OD. Although,  Cocktail  spawns about 50% more VMs than  InFaas , the high  P f  of small models and spot-instance price reductions combined with autoscaling policies lead to the overall 30-40% cost savings. 6.3 Sensitivity Analysis \nIn this section, we analyze the sensitivity of  Cocktail  with respect to various design choices which include (i) sampling interval of the accuracy measurements, (ii) spot-instance fail- ure rate and (iii) type of datasets and applications.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "6.3 Sensitivity Analysis \nIn this section, we analyze the sensitivity of  Cocktail  with respect to various design choices which include (i) sampling interval of the accuracy measurements, (ii) spot-instance fail- ure rate and (iii) type of datasets and applications. 6.3.1 Sampling Interval \nTo study the sensitivity with respect to the sampling interval for measure accuracy loss/gain, we use four different intervals of 10s, 30s, 60s and 120s. Figure  13  plots the average number of models (bar- left y-axis) and cumulative accuracy (line- right y-axis) for the different sampling intervals for queries with three different constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 152,
      "augmented": true
    },
    {
      "text": "Figure  13  plots the average number of models (bar- left y-axis) and cumulative accuracy (line- right y-axis) for the different sampling intervals for queries with three different constraints. It can be seen that the 30s interval strikes the right balance with less than 0.2% loss in accuracy and has average number models much lesser than other intervals. This is because, increasing the interval leads to lower number of scale down operations, thus resulting in a bigger ensemble.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "As a result, the 120s interval has the highest number of models. This is because, increasing the interval leads to lower number of scale down operations, thus resulting in a bigger ensemble. USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1051 \n10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n82.25 \n82.50 \n82.75 \nAccuracy \n(a)  Queries under Constraint-1.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n81.0 \n81.2 \n81.4 \nAccuracy \n(b)  Queries under Constraint-2. 10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n79.0 \n79.2 \n79.4 \nAccuracy \n(c)  Queries under Constraint-3. USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1051 \n10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n82.25 \n82.50 \n82.75 \nAccuracy \n(a)  Queries under Constraint-1.",
      "type": "sliding_window_shuffled",
      "tokens": 150,
      "augmented": true
    },
    {
      "text": "Figure 13:  Sensitivity analysis of model selection with respect to sampling interval. The average number of models is in primary axis and cumulative accuracy in secondary axis. 10 30 60 120 Sampling-Interval \n0 \n2 \n4 \n6 \n#Models \n79.0 \n79.2 \n79.4 \nAccuracy \n(c)  Queries under Constraint-3.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "As previ- ously discussed in Section  3 , spot instances interruptions can lead to intermittent loss in accuracy as certain models will be unavailable in the ensemble. 6.3.2 Cocktail Failure Resilience \nWe use spot instances to host models in  Cocktail . The average number of models is in primary axis and cumulative accuracy in secondary axis.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "As previ- ously discussed in Section  3 , spot instances interruptions can lead to intermittent loss in accuracy as certain models will be unavailable in the ensemble. However for large ensembles (5 models are more), the intermittent accuracy loss is very low. Figure  12b  plots the failure analysis results for top three constraints by comparing the ensemble accuracy to the target accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "We induce failures in the in- stances using  chaosmonkey  [ 19 ] tool with a 20% failure proba- bility. Figure  12b  plots the failure analysis results for top three constraints by comparing the ensemble accuracy to the target accuracy. The desired accuracy for all three constraints are plotted as BL1, BL2 and BL3.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Beyond 800 s , they quickly recover back to the required accuracy because additional instances are spawned in place of failed instances. It can be seen that queries in all three constraints suffer an intermittent loss in accuracy of 0.6% between the time period 240 s  and 800 s . We induce failures in the in- stances using  chaosmonkey  [ 19 ] tool with a 20% failure proba- bility.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "However, in the case of InFaas , this would lead to 1% failed requests due to requests being dropped from the failed instances. An alternate solution would be to restart the queries in running instances but that leads to increased latencies for the 1% requests. Beyond 800 s , they quickly recover back to the required accuracy because additional instances are spawned in place of failed instances.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Thus,  Cocktail  is inherently fault-tolerant owing to the parallel nature in computing multiple inferences for a single request. In contrast,  Cocktail  incurs a modest accuracy loss of well within 0.6% and quickly adapts to reach the target accuracy. An alternate solution would be to restart the queries in running instances but that leads to increased latencies for the 1% requests.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "Discussion:  For applications that are latency tolerant, we can potentially redirect requests from failed instances to existing instances, which would lead to increased tail latency. Thus,  Cocktail  is inherently fault-tolerant owing to the parallel nature in computing multiple inferences for a single request. We observe similar accuracy loss or lower for different probability failures of 5%, 10% and 25%, respectively (results/charts omitted in the interest of space).",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "The results we how are only for latency intolerant applications. Discussion:  For applications that are latency tolerant, we can potentially redirect requests from failed instances to existing instances, which would lead to increased tail latency. Note that, the ensembles used in our experiments are at-least 4 models or more.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "6.3.3 Sensitivity to Constraints \nFigure  14  plots the sensitivity of model selection policy un- der a wide-range of latency and accuracy constraints. Note that, the ensembles used in our experiments are at-least 4 models or more. For smaller ensembles, instance failures might lead to higher accuracy loss, but in our experiments, single models typically satisfy their constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "In Figure  14a , we vary the latency under six different constant accuracy categories. 6.3.3 Sensitivity to Constraints \nFigure  14  plots the sensitivity of model selection policy un- der a wide-range of latency and accuracy constraints. It can be seen that for ﬁxed accuracy of 72%, 78% and 80%, the average number of models increase with increase in latency, but drops to 1 for the highest latency.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "0 \n2 \n4 \n6 \n8 \n60 \n70 \n80 \n90 \n60 70 100 120 150 350 \nAverage #Models \nAccuracy (%) \nLatency (ms) \naccuracy Average #Model \n(b)  Fixed Latency. Intuitively, singe large models with higher latency can satisfy \n0 2 4 6 8 10 \n0 \n100 \n200 \n300 \n400 \n72 78 80 81.5 83.5 85 \nAvegae #Models \nLatency (ms) \nAccuracy (%) \nLatency Average #Models \n(a)  Fixed Accuracy. It can be seen that for ﬁxed accuracy of 72%, 78% and 80%, the average number of models increase with increase in latency, but drops to 1 for the highest latency.",
      "type": "sliding_window_shuffled",
      "tokens": 153,
      "augmented": true
    },
    {
      "text": "0 \n2 \n4 \n6 \n8 \n60 \n70 \n80 \n90 \n60 70 100 120 150 350 \nAverage #Models \nAccuracy (%) \nLatency (ms) \naccuracy Average #Model \n(b)  Fixed Latency. Figure 14:  Sensitivity Constraints under ﬁxed latency and accuracy. Bar graphs (latency) plotted using primary y-axis and line graph (#models) plotted using secondary y-axis.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "Bar graphs (latency) plotted using primary y-axis and line graph (#models) plotted using secondary y-axis. Const1 Const2 Const3 Const4 Query \n0.0 \n2.5 \n5.0 \n7.5 \n#Models \nClipper Clipper-X Cocktail \n(b)  Sentiment analysis. Const1 Const2 Const3 Const4 Query \n0 \n5 \n10 \n#Models \nClipper Clipper-X Cocktail \n(a)  Image Classiﬁcation-Cifar-100.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "Const1 Const2 Const3 Const4 Query \n0.0 \n2.5 \n5.0 \n7.5 \n#Models \nClipper Clipper-X Cocktail \n(b)  Sentiment analysis. Figure 15:  Average number of models used in the ensemble. the accuracy, while short latency models need to be ensem- bled to reach the same accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "For accuracy greater than 80%, the ensemble size drops with higher latencies. the accuracy, while short latency models need to be ensem- bled to reach the same accuracy. This is because the models which offer higher accuracy are typically dense and hence, smaller ensembles are sufﬁcient.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "It can be seen that for higher accuracies, Cocktail  tries to ensemble more models to reach the accuracy, while for lower accuracy it resorts to using single models. This is because the models which offer higher accuracy are typically dense and hence, smaller ensembles are sufﬁcient. In Fig- ure  14b , we vary the accuracy under six different constant latency categories.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "6.3.4 Sensitivity to Dataset \nTo demonstrate the applicability of  Cocktail  to multiple datasets, we conducted similar experiments as elucidated in Section  5.2.1  using the  CIFAR-100  dataset [ 50 ]. It can be seen that for higher accuracies, Cocktail  tries to ensemble more models to reach the accuracy, while for lower accuracy it resorts to using single models. It comprises of 100 distinct image classes and we trained 11 different models including the nine that are common from Table  1 .",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "It can be seen that  Cock- tail  shows similar reduction (as Imagenet) while using only 4.4 models on average. Fig- ure  15a  plots the average number of models used by the three policies for the top four constraints. It comprises of 100 distinct image classes and we trained 11 different models including the nine that are common from Table  1 .",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "It can be seen that  Cock- tail  shows similar reduction (as Imagenet) while using only 4.4 models on average. Figure  16a  plots the latency reduction and accuracy boost when compared to  InFaaS  (baseline). As expected,  Clipper  and  Clipper-X use more models than  Cocktail  (11 and 5.4, respectively) due to non-aggressive scaling down of the models used.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Cocktail  was also able to deliver modest accuracy gain \n1052    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nConst1 Const2 Const3 Const4 \nBaseline \n0 \n20 \n40 \nLatency-reduction \n0.50 \n0.75 \n1.00 \nAccuracy-Gain \n(a)  Image Classiﬁcation:Cifar100. While able to reduce 60% of the models used in the ensemble,  Cocktail  also re- duces latency by up to 50% and boosts accuracy by up to 1.2%. Figure  16a  plots the latency reduction and accuracy boost when compared to  InFaaS  (baseline).",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "Const1 Const2 Const3 Const4 \nBaseline \n0 \n10 \n20 \n30 \nLatency-reduction \n0.6 \n0.8 \n1.0 \n1.2 \nAccuracy-Gain \n(b)  Sentiment Analysis. Cocktail  was also able to deliver modest accuracy gain \n1052    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nConst1 Const2 Const3 Const4 \nBaseline \n0 \n20 \n40 \nLatency-reduction \n0.50 \n0.75 \n1.00 \nAccuracy-Gain \n(a)  Image Classiﬁcation:Cifar100. Figure 16:  Latency reduction (%) plotted as bar graph(primary y- axis) and accuracy gains (%) plotted as line graph (secondary y-axis) over InFaaS.",
      "type": "sliding_window_shuffled",
      "tokens": 178,
      "augmented": true
    },
    {
      "text": "Figure 16:  Latency reduction (%) plotted as bar graph(primary y- axis) and accuracy gains (%) plotted as line graph (secondary y-axis) over InFaaS. Strict Relaxed 0 \n25 \n50 \n75 \n100 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace. Strict Relaxed 0 \n20 \n40 \n60 \n80 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(a)  Wiki Trace.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Figure 17:  Cost savings of Cocktail for Sentiment Analysis. Strict Relaxed 0 \n25 \n50 \n75 \n100 \nCost($) \nInFaas Clipper Clipper-X Cocktail \n(b)  Twitter Trace. of 0.5% than  Clipper  (not plotted).",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "The accuracy gain seen in  CIFAR-100  is lesser than ImageNet dataset because the class-based weighted voting works effectively when handling large number of classes (100 in  CIFAR  vs 1000 in ImageNet). Nevertheless,  Cocktail  is able to deliver the accuracy at 2x lower latency than  InFaaS  and 1.35x lower cost than Clipper. of 0.5% than  Clipper  (not plotted).",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "Nevertheless,  Cocktail  is able to deliver the accuracy at 2x lower latency than  InFaaS  and 1.35x lower cost than Clipper. 6.4 General Applicability of Cocktail \nTo demonstrate the general applicability of  Cocktail  to other classiﬁcation tasks, we evaluated  Cocktail  using a Sentiment Analysis application for two datasets. The results reported are averaged across both the datasets.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "As shown for  Const1 ,  Cocktail  shows similar reduction (as image-classiﬁcation) with only using 4.8 models on average, which is 40% and 26% lower than Clipper  and  Clipper-X , respectively. The results reported are averaged across both the datasets. Figure  15b  plots the average number of models used by the three policies for the top four constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "Figure  16b  plots the latency reduction and accuracy gain, compared to  InFaaS  (baseline). Cocktail  is also able to reduce the number of models by 30% and 50% for medium ensembles ( Const2  &  Const3 ) as well. As shown for  Const1 ,  Cocktail  shows similar reduction (as image-classiﬁcation) with only using 4.8 models on average, which is 40% and 26% lower than Clipper  and  Clipper-X , respectively.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "Both  Cocktail  and  Clipper  deliver the same overall accuracy (96%, 94.5%, 93.5%, and 92%)). Figure  16b  plots the latency reduction and accuracy gain, compared to  InFaaS  (baseline). While being able to reduce 50% of the models used in the ensemble,  Cocktail  also re- duces latency by up to 50% and improves accuracy by up to 1.3%.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "Both  Cocktail  and  Clipper  deliver the same overall accuracy (96%, 94.5%, 93.5%, and 92%)). Since sentiment analysis only has 2-3 classes, there are no additional accuracy gains by using the class-based weighted voting. However, the model selection policy effectively switches between differ- ent models based on the structure of input text (equivalent to classes in images).",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "For instance, complex sentences are more accurately classiﬁed by denser models compared to smaller. However, the model selection policy effectively switches between differ- ent models based on the structure of input text (equivalent to classes in images). Despite the lower accuracy gains,  Cocktail  is able to reduce the cost (Figure  17 ) of model-serving by 1.45 ×  and 1.37 × for Wiki trace compared to  InFaaS  and  Clipper , respectively.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "7 Concluding Remarks \nThere is an imminent need to develop model serving systems that can deliver highly accurate, low latency predictions at re- duced cost. Despite the lower accuracy gains,  Cocktail  is able to reduce the cost (Figure  17 ) of model-serving by 1.45 ×  and 1.37 × for Wiki trace compared to  InFaaS  and  Clipper , respectively. In this paper, we propose and evaluate  Cocktail , a cost-effective model serving system that exploits ensembling techniques to meet high accuracy under low latency goals.",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "More speciﬁcally, we (i) develop a novel dynamic model selection, (ii) design a prudent resource management scheme that utilizes weighted autoscaling for efﬁcient resource allocation, and (iii) lever- age transient VM instances to reduce the deployment costs. In  Cocktail , we adopt a three-fold approach to reduce the resource footprint of model ensembling. In this paper, we propose and evaluate  Cocktail , a cost-effective model serving system that exploits ensembling techniques to meet high accuracy under low latency goals.",
      "type": "sliding_window_shuffled",
      "tokens": 125,
      "augmented": true
    },
    {
      "text": "Acknowledgments \nWe are indebted to our shepherd Manya Ghobadi, the anony- mous reviewers and Anup Sarma for their insightful com- ments to improve the clarity of the presentation. More speciﬁcally, we (i) develop a novel dynamic model selection, (ii) design a prudent resource management scheme that utilizes weighted autoscaling for efﬁcient resource allocation, and (iii) lever- age transient VM instances to reduce the deployment costs. Our results from extensive evaluations using both CPU and GPU instances on AWS EC2 cloud platform demonstrate that Cocktail  can reduce deployment cost by 1.4 × , while reducing latency by 2 ×  and satisfying accuracy for 96% of requests, compared to the state-of-the-art model-serving systems.",
      "type": "sliding_window_shuffled",
      "tokens": 183,
      "augmented": true
    },
    {
      "text": "Acknowledgments \nWe are indebted to our shepherd Manya Ghobadi, the anony- mous reviewers and Anup Sarma for their insightful com- ments to improve the clarity of the presentation. Special mention to Nachiappan Chidambaram N. for his intellec- tual contributions. This research was partially supported by NSF grants #1931531, #1955815, #1763681, #1908793, #1526750, #2116962, #2122155, #2028929 ,and we thank NSF Chameleon Cloud project CH-819640 for their generous compute grant.",
      "type": "sliding_window_shuffled",
      "tokens": 145,
      "augmented": true
    },
    {
      "text": "This research was partially supported by NSF grants #1931531, #1955815, #1763681, #1908793, #1526750, #2116962, #2122155, #2028929 ,and we thank NSF Chameleon Cloud project CH-819640 for their generous compute grant. All product names used in this publication are for identiﬁcation purposes only and may be trademarks of their respective companies. References \n[1]  Martín Abadi.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "In  Acm Sigplan Notices . References \n[1]  Martín Abadi. Tensorﬂow: learning functions at scale.",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "[2]  Deepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, and Liang Zhang. ACM, 2016. In  Acm Sigplan Notices .",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "[2]  Deepak Agarwal, Bo Long, Jonathan Traupman, Doris Xin, and Liang Zhang. In  Proceedings of the 7th ACM international conference on Web search and data mining , pages 173–182, 2014. Laser: A scalable response prediction platform for online advertising.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the 7th ACM international conference on Web search and data mining , pages 173–182, 2014. [3]  Ahmed Ali-Eldin, Jonathan Westin, Bin Wang, Prateek Sharma, and Prashant Shenoy. Spotweb: Running latency-sensitive distributed web services on transient cloud servers.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "[4]  Amazon. In  Proceedings of the 28th Inter- national Symposium on High-Performance Parallel and Distributed Computing , pages 1–12, 2019. Spotweb: Running latency-sensitive distributed web services on transient cloud servers.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "https://docs.aws.amazon.com/ sagemaker/latest/dg/deepar.html,February2020 . Deepar estimator. [4]  Amazon.",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "[5] Amazon. EC2 pricing. https://docs.aws.amazon.com/ sagemaker/latest/dg/deepar.html,February2020 .",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "https://aws.amazon.com/ec2/pricing/. EC2 pricing. [6]  Amazon.",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "Sagemaker. [6]  Amazon. https://aws.amazon.com/sagemaker/, February 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 28,
      "augmented": true
    },
    {
      "text": "[7]  Amazon. https://aws.amazon.com/sagemaker/, February 2018. Azure Low priority batch VMs., February 2018. https://docs.microsoft.com/en-us/azure/batch/batch-low-pri-vms .",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "[8]  Amazon. EC2 C5 Instances., February 2018. https://aws.amazon.com/ec2/instance-types/c5/ . Azure Low priority batch VMs., February 2018. https://docs.microsoft.com/en-us/azure/batch/batch-low-pri-vms .",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "EC2 C5 Instances., February 2018. https://aws.amazon.com/ec2/instance-types/c5/ . [9]  Amazon. Google Preemptible VMs., February 2018. https://cloud.google.com/preemptible-vms .",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "[10]  Azure. Google Preemptible VMs., February 2018. https://cloud.google.com/preemptible-vms . Machine Learning as a Service., February 2018. https://azure.microsoft.com/en-us/pricing/details/machine-learning- service/ .",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "Ensembling in Azure ML Studio., February 2020. https://docs.microsoft.com/en-us/azure/machine-learning/studio- module-reference/multiclass-decision-forest . [11]  Azure. Machine Learning as a Service., February 2018. https://azure.microsoft.com/en-us/pricing/details/machine-learning- service/ .",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "USENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1053 \n[12]  Ataollah Fatahi Baarzi, Timothy Zhu, and Bhuvan Urgaonkar. Ensembling in Azure ML Studio., February 2020. https://docs.microsoft.com/en-us/azure/machine-learning/studio- module-reference/multiclass-decision-forest . Burscale: Using burstable instances for cost-effective autoscaling in the public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the ACM Symposium on Cloud Computing , New York, NY, USA, 2019. Association for Computing Machinery. Burscale: Using burstable instances for cost-effective autoscaling in the public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "Recognizing facial expression: machine learning and application to spontaneous behavior. [13]  Marian Stewart Bartlett, Gwen Littlewort, Mark Frank, Claudia Lain- scsek, Ian Fasel, and Javier Movellan. Association for Computing Machinery.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "IEEE, 2005. In  2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05) , volume 2, pages 568–573. Recognizing facial expression: machine learning and application to spontaneous behavior.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "[14]  Eric Bauer and Ron Kohavi. An empirical comparison of voting classiﬁcation algorithms: Bagging, boosting, and variants. IEEE, 2005.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "[15]  William H Beluch, Tim Genewein, Andreas Nürnberger, and Jan M Köhler. Machine learning , 36(1-2):105–139, 1999. An empirical comparison of voting classiﬁcation algorithms: Bagging, boosting, and variants.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "The power of ensembles for active learning in image classiﬁ- cation. [15]  William H Beluch, Tim Genewein, Andreas Nürnberger, and Jan M Köhler. In  Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 9368–9377, 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "[16] Josiah L Carlson. In  Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 9368–9377, 2018. Redis in action .",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "[17]  Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Manning Publications Co., 2013. Redis in action .",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Ensemble selection from libraries of models. In  Proceedings of the twenty-ﬁrst international conference on Machine learning , page 18, 2004. [17]  Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "Robust bayesian linear classiﬁer ensembles. [18]  Jesús Cerquides and Ramon López De Mántaras. In  Proceedings of the twenty-ﬁrst international conference on Machine learning , page 18, 2004.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "Springer, 2005. Robust bayesian linear classiﬁer ensembles. In  European Conference on Machine Learning , pages 72–83.",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "[19]  Michael Alan Chang, Bredan Tschaen, Theophilus Benson, and Lau- rent Vanbever. Springer, 2005. Chaos monkey: Increasing sdn reliability through systematic network destruction.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "Chaos monkey: Increasing sdn reliability through systematic network destruction. In  Proceedings of the 2015 ACM Conference on Special Interest Group on Data Communication , pages 371–372, 2015. [20] Lingjiao Chen, Matei Zaharia, and James Zou.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "Frugalml: How to use ml prediction apis more accurately and cheaply. [20] Lingjiao Chen, Matei Zaharia, and James Zou. In  Advances in Neural Information Processing Systems (NeurIPS) , 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "MongoDB: the deﬁnitive guide: powerful and scalable data storage . \" [21]  Kristina Chodorow. In  Advances in Neural Information Processing Systems (NeurIPS) , 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "[22]  Francois Chollet. O’Reilly Media, Inc.\", 2013. MongoDB: the deﬁnitive guide: powerful and scalable data storage . \"",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "Deep Learning mit Python und Keras: Das Praxis- Handbuch vom Entwickler der Keras-Bibliothek . MITP-Verlags GmbH & Co. KG, 2018. [22]  Francois Chollet.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "MITP-Verlags GmbH & Co. KG, 2018. Stratus: Cost-aware container scheduling in the public cloud. [23]  Andrew Chung, Jun Woo Park, and Gregory R. Ganger.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "In  SoCC , 2018. [24]  Paul Covington, Jay Adams, and Emre Sargin. Stratus: Cost-aware container scheduling in the public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "Deep neural networks for youtube recommendations. [24]  Paul Covington, Jay Adams, and Emre Sargin. In  Proceedings of the 10th ACM con- ference on recommender systems , pages 191–198, 2016.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "The missing piece in complex analytics: Low latency, scalable model man- agement and serving with velox. [25]  Daniel Crankshaw, Peter Bailis, Joseph E Gonzalez, Haoyuan Li, Zhao Zhang, Michael J Franklin, Ali Ghodsi, and Michael I Jordan. In  Proceedings of the 10th ACM con- ference on recommender systems , pages 191–198, 2016.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "The missing piece in complex analytics: Low latency, scalable model man- agement and serving with velox. [26]  Daniel Crankshaw, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E. Gonzalez, Ion Stoica, and Alexey Tumanov. arXiv preprint arXiv:1409.3809 , 2014.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "CoRR , abs/1812.01776, 2018. [26]  Daniel Crankshaw, Gur-Eyal Sela, Corey Zumar, Xiangxi Mo, Joseph E. Gonzalez, Ion Stoica, and Alexey Tumanov. Inferline: ML inference pipeline composition framework.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "Clipper: A low-latency online pre- diction serving system. CoRR , abs/1812.01776, 2018. [27]  Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "Clipper: A low-latency online pre- diction serving system. In  14th USENIX Symposium on Networked Sys- tems Design and Implementation (NSDI 17) , pages 613–627, Boston, MA, March 2017. USENIX Association.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "USENIX Association. [28]  Deepstudio. Deep Learning Dtudio, February 2020. https://docs.deepcognition.ai/ .",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "[29]  J. Deng, W. Dong, R. Socher, L. Li, and and. Imagenet: A large-scale hierarchical image database. Deep Learning Dtudio, February 2020. https://docs.deepcognition.ai/ .",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "In  2009 IEEE Conference on Computer Vision and Pattern Recognition , June 2009. Imagenet: A large-scale hierarchical image database. [30]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "Bert: Pre-training of deep bidirectional transformers for language un- derstanding, 2019. [30]  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. [31]  Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "[32]  Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. [31]  Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola. Autogluon-tabular: Robust and accurate automl for structured data, 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "[32]  Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. arXiv preprint arXiv:2002.08155 , 2020. Codebert: A pre-trained model for programming and natural languages.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "arXiv preprint arXiv:2002.08155 , 2020. A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based ap- proaches. [33]  Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "[34]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based ap- proaches. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) , 42(4):463–484, 2012.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "In  USENIX Middleware Conference , 2017. [34]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efﬁciency.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "In  USENIX Middleware Conference , 2017. Serving dnns like clockwork: Perfor- mance predictability from the bottom up. [35]  Arpan Gujarati, Reza Karimi, Safya Alzayat, Antoine Kaufmann, Ymir Vigfusson, and Jonathan Mace.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "USENIX Association. Serving dnns like clockwork: Perfor- mance predictability from the bottom up. In  14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20) , Banff, Alberta, November 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "[36]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Nachiappan C.Nachiappan, Mahmut Taylan Kandemir, and Chita R. Das. Fifer: Tackling Resource Underutilization in the Serverless Era. USENIX Association.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "[37]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Taylan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das. In  USENIX Middleware Conference , 2020. Fifer: Tackling Resource Underutilization in the Serverless Era.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Spock: Exploiting serverless functions for slo and cost aware resource procure- ment in public cloud. [37]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Mahmut Taylan Kandemir, Bhuvan Urgaonkar, George Kesidis, and Chita Das. In  IEEE CLOUD , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "[38]  Jashwant Raj Gunasekaran, Prashanth Thinakaran, Cyan Subhra Mishra, Mahmut Taylan Kandemir, and Chita R. Das. Towards designing a self-managed machine learning inference serving system inpublic cloud, 2020. In  IEEE CLOUD , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "[39]  U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G. Wei, H. S. Lee, D. Brooks, and C. Wu. Towards designing a self-managed machine learning inference serving system inpublic cloud, 2020. Deeprecsys: A system for optimiz- ing end-to-end at-scale neural recommendation inference.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "[40]  Rui Han, Moustafa M. Ghanem, Li Guo, Yike Guo, and Michelle Osmond. In  2020 ACM/IEEE 47th Annual International Symposium on Computer Archi- tecture (ISCA) , pages 982–995, 2020. Deeprecsys: A system for optimiz- ing end-to-end at-scale neural recommendation inference.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "[40]  Rui Han, Moustafa M. Ghanem, Li Guo, Yike Guo, and Michelle Osmond. Enabling cost-aware and adaptive elasticity of multi-tier cloud applications. Future Gener.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": ", 32(C):82–98, March 2014. Syst. [41]  Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "Tributary: spot-dancing for elastic services with latency SLOs. In  ATC , 2018. [41]  Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "[42]  Aaron Harlap, Alexey Tumanov, Andrew Chung, Gregory R. Ganger, and Phillip B. Gibbons. In  ATC , 2018. Proteus: Agile ML Elasticity Through Tiered Reliability in Dynamic Resource Markets.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "In  Eurosys , 2017. Proteus: Agile ML Elasticity Through Tiered Reliability in Dynamic Resource Markets. [43]  Johann Hauswald, Michael A. Laurenzano, Yunqi Zhang, Cheng Li, Austin Rovinski, Arjun Khurana, Ronald G. Dreslinski, Trevor Mudge, Vinicius Petrucci, Lingjia Tang, and Jason Mars.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "[43]  Johann Hauswald, Michael A. Laurenzano, Yunqi Zhang, Cheng Li, Austin Rovinski, Arjun Khurana, Ronald G. Dreslinski, Trevor Mudge, Vinicius Petrucci, Lingjia Tang, and Jason Mars. In  ASPLOS , 2015. Sirius: An open end-to-end voice and vision personal assistant and its implications for future warehouse scale computers.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "[44]  K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov, M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. In  ASPLOS , 2015. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "Applied machine learning at facebook: A datacenter infrastructure perspective. In  2018 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pages 620–629, Feb 2018. Law, K. Lee, J. Lu, P. Noordhuis, M. Smelyanskiy, L. Xiong, and X. Wang.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "[45]  Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vi- jay Vasudevan, et al. In  2018 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pages 620–629, Feb 2018. Searching for mobilenetv3.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "Searching for mobilenetv3. [46]  Patrick Hunt, Mahadev Konar, Flavio Paiva Junqueira, and Benjamin Reed. In  Proceedings of the IEEE International Conference on Computer Vision , pages 1314–1324, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "Zookeeper: Wait-free coordination for internet-scale systems. [46]  Patrick Hunt, Mahadev Konar, Flavio Paiva Junqueira, and Benjamin Reed. In  USENIX annual technical conference , 2010.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "In  USENIX annual technical conference , 2010. [47]  Minoru Kawashima, Charles E Dorgan, and John W Mitchell. Hourly thermal load prediction for the next 24 hours by arima, ewma, lr and \n1054    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nan artiﬁcial neural network.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "[48]  Abeer Abdel Khaleq and Ilkyeun Ra. Hourly thermal load prediction for the next 24 hours by arima, ewma, lr and \n1054    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nan artiﬁcial neural network. Technical report, American Society of Heating, Refrigerating and Air-Conditioning Engineers ... , 1995.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "[48]  Abeer Abdel Khaleq and Ilkyeun Ra. Cloud-based disaster manage- ment as a service: A microservice approach for hurricane twitter data analysis. In  GHTC , 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "In  GHTC , 2018. [49]  J Zico Kolter and Marcus A Maloof. Dynamic weighted majority: An ensemble method for drifting concepts.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "[50]  Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Journal of Machine Learning Research , 8(Dec):2755–2790, 2007. Dynamic weighted majority: An ensemble method for drifting concepts.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "Cifar-100 (cana- dian institute for advanced research), 2010.  http://www.cs.toronto. edu/~kriz/cifar.html . [50]  Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "edu/~kriz/cifar.html . Albert: A lite bert for self-supervised learning of language representations. [51]  Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gim- pel, Piyush Sharma, and Radu Soricut.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "arXiv preprint arXiv:1909.11942 , 2019. Albert: A lite bert for self-supervised learning of language representations. [52]  Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, and Matteo Interlandi.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "In  13th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 18) , pages 611–626, Carlsbad, CA, October 2018. [52]  Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico Santambrogio, Markus Weimer, and Matteo Interlandi. PRETZEL: Opening the black box of machine learning prediction serving systems.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "USENIX Association. [53]  Romain Lerallut, Diane Gasselin, and Nicolas Le Roux. In  13th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 18) , pages 611–626, Carlsbad, CA, October 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "[53]  Romain Lerallut, Diane Gasselin, and Nicolas Le Roux. Large-scale real-time product recommendation at criteo. In  Proceedings of the 9th ACM Conference on Recommender Systems , pages 232–232, 2015.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "A survey of deep neural network architectures and their applications. [54]  Weibo Liu, Zidong Wang, Xiaohui Liu, Nianyin Zeng, Yurong Liu, and Fuad E Alsaadi. In  Proceedings of the 9th ACM Conference on Recommender Systems , pages 232–232, 2015.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "Neurocomputing , 234:11–26, 2017. A survey of deep neural network architectures and their applications. [55]  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy- anov.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Roberta: A robustly optimized bert pretraining approach. [55]  Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy- anov. arXiv preprint arXiv:1907.11692 , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "[56]  Zhenyu Lu, Xindong Wu, Xingquan Zhu, and Josh Bongard. arXiv preprint arXiv:1907.11692 , 2019. Ensemble pruning via individual contribution ordering.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Ensemble pruning via individual contribution ordering. In  Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’10, page 871–880, New York, NY, USA, 2010. Association for Computing Machinery.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "[57]  Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Association for Computing Machinery. Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "[58]  Soo-Jin Moon, Jeffrey Helt, Yifei Yuan, Yves Bieri, Sujata Banerjee, Vyas Sekar, Wenfei Wu, Mihalis Yannakakis, and Ying Zhang. Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks. In  2021 Design, Automation Test in Europe Conference Exhibition (DATE) , pages 1414–1419, 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "In  16th USENIX Symposium on Networked Systems Design and Implementa- tion (NSDI 19) , pages 699–718, Boston, MA, February 2019. Alem- bic: Automated model inference for stateful network functions. [58]  Soo-Jin Moon, Jeffrey Helt, Yifei Yuan, Yves Bieri, Sujata Banerjee, Vyas Sekar, Wenfei Wu, Mihalis Yannakakis, and Ying Zhang.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "In  16th USENIX Symposium on Networked Systems Design and Implementa- tion (NSDI 19) , pages 699–718, Boston, MA, February 2019. [59]  Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. USENIX Association.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the 27th ACM Symposium on Operating Systems Principles , pages 1–15, 2019. Pipedream: generalized pipeline parallelism for dnn training. [59]  Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "[60]  Christopher Olston, Noah Fiedel, Kiril Gorovoy, Jeremiah Harmsen, Li Lao, Fangwei Li, Vinu Rajashekhar, Sukriti Ramesh, and Jordan Soyke. Tensorﬂow-serving: Flexible, high-performance ml serving. In  Proceedings of the 27th ACM Symposium on Operating Systems Principles , pages 1–15, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "[61]  Nikunj C Oza. arXiv preprint arXiv:1712.06139 , 2017. Tensorﬂow-serving: Flexible, high-performance ml serving.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "In  2005 IEEE interna- tional conference on systems, man and cybernetics , volume 3, pages 2340–2345. Online bagging and boosting. [61]  Nikunj C Oza.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "[62]  Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad- bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Ieee, 2005. In  2005 IEEE interna- tional conference on systems, man and cybernetics , volume 3, pages 2340–2345.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "In  Advances in neural information processing systems , pages 8026–8037, 2019. [62]  Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad- bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "[63]  Heyang Qin, Syed Zawad, Yanqi Zhou, Lei Yang, Dongfang Zhao, and Feng Yan. Swift machine learning model serving scheduling: a region \nbased reinforcement learning approach. In  Advances in neural information processing systems , pages 8026–8037, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Swift machine learning model serving scheduling: a region \nbased reinforcement learning approach. [64]  Xueheng Qiu, Le Zhang, Ye Ren, Ponnuthurai N Suganthan, and Gehan Amaratunga. In  Proceedings of the Inter- national Conference for High Performance Computing, Networking, Storage and Analysis , pages 1–23, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "In  2014 IEEE symposium on computational intelligence in ensemble learning (CIEL) , pages 1–6. [64]  Xueheng Qiu, Le Zhang, Ye Ren, Ponnuthurai N Suganthan, and Gehan Amaratunga. Ensemble deep learning for regression and time series forecasting.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "In  2014 IEEE symposium on computational intelligence in ensemble learning (CIEL) , pages 1–6. [65]  Atul Rahman, Jongeun Lee, and Kiyoung Choi. IEEE, 2014.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "[65]  Atul Rahman, Jongeun Lee, and Kiyoung Choi. In  2016 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pages 1393–1398. Efﬁcient fpga acceler- ation of convolutional neural networks using logical-3d compute array.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "[66]  Sara Rosenthal, Noura Farra, and Preslav Nakov. In  2016 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pages 1393–1398. IEEE, 2016.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "SemEval-2017 task 4: Sentiment analysis in Twitter. In  Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) , pages 502–518, Vancouver, Canada, August 2017. [66]  Sara Rosenthal, Noura Farra, and Preslav Nakov.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "[67]  Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. In  Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) , pages 502–518, Vancouver, Canada, August 2017. Association for Computational Linguistics.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. [67]  Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. arXiv preprint arXiv:1910.01108 , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Portfolio-driven resource management for transient cloud servers. arXiv preprint arXiv:1910.01108 , 2019. [68]  Prateek Sharma, David Irwin, and Prashant Shenoy.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "Proc. ACM Meas. Portfolio-driven resource management for transient cloud servers.",
      "type": "sliding_window_shuffled",
      "tokens": 21,
      "augmented": true
    },
    {
      "text": "[69]  Prateek Sharma, Stephen Lee, Tian Guo, David Irwin, and Prashant Shenoy. Spotcheck: Designing a derivative iaas cloud on the spot market. , 1(1), June 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "Spotcheck: Designing a derivative iaas cloud on the spot market. In  Proceedings of the Tenth European Conference on Computer Systems , pages 1–15, 2015. [70]  Steven A Shaya, Neal Matheson, John Anthony Singarayar, Nikiforos Kollias, and Jeffrey Adam Bloom.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "US Patent 7,809,601. [70]  Steven A Shaya, Neal Matheson, John Anthony Singarayar, Nikiforos Kollias, and Jeffrey Adam Bloom. Intelligent performance-based prod- uct recommendation system, October 5 2010.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "[71]  Richard Socher, Yoshua Bengio, and Chris Manning. US Patent 7,809,601. Deep learning for nlp.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "[72]  Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Tutorial at Association of Computational Logistics (ACL) , 2012. Deep learning for nlp.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631–1642, 2013. [72]  Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "[73]  Mingxing Tan and Quoc V Le. In Proceedings of the 2013 conference on empirical methods in natural language processing , pages 1631–1642, 2013. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "Efﬁcientnet: Rethinking model scaling for convolutional neural networks. arXiv preprint arXiv:1905.11946 , 2019. [74]  P. Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir, and C. R. Das.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "Phoenix: A constraint-aware scheduler for heteroge- neous datacenters. [74]  P. Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir, and C. R. Das. In  2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS) , June 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "[75]  P. Thinakaran, J. R. Gunasekaran, B. Sharma, M. T. Kandemir, and C. R. Das. Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters. In  2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS) , June 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Kube-Knots: Resource Harvesting through Dynamic Container Orchestration in GPU-based Datacenters. In  CLUSTER , 2019. [76]  Guido Urdaneta, Guillaume Pierre, and Maarten Van Steen.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "Computer Networks , 2009. [76]  Guido Urdaneta, Guillaume Pierre, and Maarten Van Steen. Wikipedia workload analysis for decentralized hosting.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "[77]  Alexander Vezhnevets and Vladimir Vezhnevets. Computer Networks , 2009. Modest adaboost- teaching adaboost to generalize better.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "[78]  Jasper A Vrugt and Bruce A Robinson. In  Graphicon , pages 987–997, 2005. Modest adaboost- teaching adaboost to generalize better.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "Treatment of uncertainty using ensemble methods: Comparison of sequential data assimilation and bayesian model averaging. Water Resources Research , 43(1), 2007. [78]  Jasper A Vrugt and Bruce A Robinson.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "Water Resources Research , 43(1), 2007. [79]  Cheng Wang, Bhuvan Urgaonkar, Neda Nasiriani, and George Kesidis. Using burstable instances in the public cloud: Why, when and how?",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "[80]  Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. SIGMETRICS , June 2017. Using burstable instances in the public cloud: Why, when and how?",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "[80]  Wei Wang, Jinyang Gao, Meihui Zhang, Sheng Wang, Gang Chen, Teck Khim Ng, Beng Chin Ooi, Jie Shao, and Moaz Reyad. Proceedings of the VLDB Endowment , 12(2):128–140, 2018. Raﬁki: machine learning as an analytics service system.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "[81]  Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. Huggingface’s transformers: State-of- \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1055 \nthe-art natural language processing. Proceedings of the VLDB Endowment , 12(2):128–140, 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "[82]  Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choud- hury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, et al. arXiv preprint arXiv:1910.03771 , 2019. Huggingface’s transformers: State-of- \nUSENIX Association 19th USENIX Symposium on Networked Systems Design and Implementation    1055 \nthe-art natural language processing.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Machine learning at facebook: Understanding inference at the edge. In  2019 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pages 331–344. [82]  Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy Choud- hury, Marat Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, et al.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "[83]  Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. In  2019 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pages 331–344. IEEE, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "[83]  Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. A case for managed and model-less inference serving. In  Proceedings of the Workshop on Hot Topics in Operating Systems , New York, NY, USA, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the Workshop on Hot Topics in Operating Systems , New York, NY, USA, 2019. [84]  Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Vivienne Sze, and Hartwig Adam. Association for Computing Machinery.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "CoRR , abs/1804.03230, 2018. [84]  Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "[85]  Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. CoRR , abs/1804.03230, 2018. Xlnet: Generalized autoregressive pre- training for language understanding.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "[86]  Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. Xlnet: Generalized autoregressive pre- training for language understanding. arXiv preprint arXiv:1906.08237 , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "[86]  Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. Mark: Exploiting cloud services for cost-effective, slo-aware machine learning inference serving. In  ATC , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "Identifying outlier arms in multi-armed bandit. [87]  Honglei Zhuang, Chi Wang, and Yifan Wang. In  ATC , 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "In  Advances in Neural Information Processing Systems , pages 5204–5213, 2017. Identifying outlier arms in multi-armed bandit. [88]  Sheikh Ziauddin and Matthew N Dailey.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "Iris recognition performance enhancement using weighted majority voting. In  2008 15th IEEE International Conference on Image Processing , pages 277–280. [88]  Sheikh Ziauddin and Matthew N Dailey.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "In  2008 15th IEEE International Conference on Image Processing , pages 277–280. Appendix \nA Modeling of Ensembling \nWhile performing an ensemble it is important to be sure that we can reach the desired accuracy by combining more models. IEEE, 2008.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "To be sure that the combination will give us the desired accuracy of the larger model, we try to theoretically analyse the scenario. In our design, we solve our ﬁrst objective function (described in Section  4.1 ) by combining all available models which meet the latency SLO. Appendix \nA Modeling of Ensembling \nWhile performing an ensemble it is important to be sure that we can reach the desired accuracy by combining more models.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "To be sure that the combination will give us the desired accuracy of the larger model, we try to theoretically analyse the scenario. We formulate the problem conservatively as following. We perform an inference by ensembling ’N’ models, and each of these models have accuracy ’a’.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "Therefore the prob- ability of any model giving a correct classiﬁcation is ’a’. We assume the output to be correct if majority of them, i.e. We perform an inference by ensembling ’N’ models, and each of these models have accuracy ’a’.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "We assume the output to be correct if majority of them, i.e. ⌊ N / 2 ⌋ + 1  of them give the same result. Then, the ﬁnal ac- curacy of this ensemble would be the probability of at least ⌊ N / 2 ⌋ + 1 of them giving a correct result.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "To we model this problem as a coin-toss problem involving N  biased coins with having probability of occurrence of head to be  a . Relating this to our problem, each coin represents a model, and an occurrence of head represents the model giving the correct classiﬁcation. Then, the ﬁnal ac- curacy of this ensemble would be the probability of at least ⌊ N / 2 ⌋ + 1 of them giving a correct result.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Relating this to our problem, each coin represents a model, and an occurrence of head represents the model giving the correct classiﬁcation. This is a standard binomial distribution problem and can be solved by using the following formula: \nP head  = N ∑ i = ⌊ N \n2   ⌋ + 1 \n\u0012 N i \n\u0013 a i   ( 1 − a ) ( N − i ) . Hence, the problem boils down to ﬁnd the probability of at least  ⌊ N / 2 ⌋ + 1  heads when all N coins are tossed together.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "We have 10 (therefore N = 10) such models and among them the least accurate model is MobileNetV1 (accuracy 70%, therefore a = 0.70). This is a standard binomial distribution problem and can be solved by using the following formula: \nP head  = N ∑ i = ⌊ N \n2   ⌋ + 1 \n\u0012 N i \n\u0013 a i   ( 1 − a ) ( N − i ) . To further quantify, let us consider the case where we need to determine if we can reach the accuracy of NasNetLarge (82%) by combining rest of the smaller models which have lesser latency than NasNetLarge.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "We have 10 (therefore N = 10) such models and among them the least accurate model is MobileNetV1 (accuracy 70%, therefore a = 0.70). We need to ﬁnd the probability of at least 6 of them being correct. Using the equation above we ﬁnd the probability to be \nP head  = 10 ∑ i = ⌊ 10 \n2   ⌋ + 1 = 6 \n\u0012 10 i \n\u0013 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Using the equation above we ﬁnd the probability to be \nP head  = 10 ∑ i = ⌊ 10 \n2   ⌋ + 1 = 6 \n\u0012 10 i \n\u0013 0 . 7 i   ( 1 − 0 . 7 ) ( 10 − i )   =  0 .",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "7 ) ( 10 − i )   =  0 . Given all the other models have higher accuracy, the least accuracy we can expect with such an ensemble is 83%. 83 \nThis corresponds to an accuracy of 83%, which is greater than our required accuracy of 82%).",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Given all the other models have higher accuracy, the least accuracy we can expect with such an ensemble is 83%. This analysis forms the base of our ensemble technique, and hence proving the combination of multiple available models can be more accurate than the most accurate individual model. B Why DeepARest Model?",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "We quantitatively justify the choice of using DeepARest by conducting a brick-by-brick comparison of the accuracy loss \n1056    19th USENIX Symposium on Networked Systems Design and Implementation USENIX Association \nwhen compared with other state-of-the-art prediction models used in prior work. Table  4  shows the root mean squared error (RMSE) in- curred by all the models. B Why DeepARest Model?",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "Table  4  shows the root mean squared error (RMSE) in- curred by all the models. It is evident that the LSTM and DeepAREst have lowest RMSE value. The ML models used in these experiments are pre-trained with 60% of the Twitter arrival trace.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "Since the primary contribution in  Cocktail  is to provide high accuracy and low latency predictions at cheaper cost, appli- cation developers can adapt the prediction algorithm to their needs or even plug-in their own prediction models. DeepARest is 10% better than LSTM model. It is evident that the LSTM and DeepAREst have lowest RMSE value.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "C System Overheads \nWe characterize the system-level overheads incurred due to the design choices in  Cocktail . The  mongodb  database is a centralized server, which resides on the head-node. Since the primary contribution in  Cocktail  is to provide high accuracy and low latency predictions at cheaper cost, appli- cation developers can adapt the prediction algorithm to their needs or even plug-in their own prediction models.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "The DeepARest prediction model which is not in the critical decision-making path runs as a background process incurring 2.2 ms latency on average. The  mongodb  database is a centralized server, which resides on the head-node. We measure the overall average latency incurred due to all reads/writes in the database, which is well within 1.5ms.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "The time taken to spawn new VM takes about 60s to 100s de- pending on the size of the VM instance. The DeepARest prediction model which is not in the critical decision-making path runs as a background process incurring 2.2 ms latency on average. The weighted majority voting takes 0.5ms and the model selection policy takes 0.7ms.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "The time taken to spawn new VM takes about 60s to 100s de- pending on the size of the VM instance. The time taken to choose models from the model-cache is less than 1ms. The end-to-end response time to send the image to a worker VM and get the prediction back, was dominated by about 300ms (at maximum) of payload transfer time.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "D Instance conﬁguration and Pricing \nInstance vCPUs Memory Price C5a.xlarge 4 8 GiB $0.154 C5a.2xlarge 8 16 GiB $0.308 C5a.4xlarge 16 32 GiB $0.616 C5a.8xlarge 32 64 GiB $1.232 \nTable 7:  Conﬁguration and Pricing for EC2 C5 instances. E CIFAR-100 and BERT Models \nTable  8  shows the different models available for image predic- tion, that are pretrained on Keras using  CIFAR-100  dataset. The end-to-end response time to send the image to a worker VM and get the prediction back, was dominated by about 300ms (at maximum) of payload transfer time.",
      "type": "sliding_window_shuffled",
      "tokens": 169,
      "augmented": true
    },
    {
      "text": "E CIFAR-100 and BERT Models \nTable  8  shows the different models available for image predic- tion, that are pretrained on Keras using  CIFAR-100  dataset. Model Params (M) \nTop-1 Accuracy(%) \nLatency (ms) P f \nAlbert-base [ 51 ] 11 91.4 55 7 CodeBert [ 32 ] 125 89 79 6 DistilBert [ 67 ] 66 90.6 92 5 Albert-large 17 92.5 120 4 XLNet [ 85 ] 110 94.6 165 3 Bert [ 30 ] 110 92 185 3 Roberta [ 55 ] 355 94.3 200 2 Albert-xlarge 58 93.8 220 1 Albert-xxlarge 223 95.9 350 1 \nTable 9:  Pretrained models for Sentiment Analysis using BERT. Similarly Table  9  shows the different models trained for BERT-based sentiment analysis on twitter dataset.",
      "type": "sliding_window_shuffled",
      "tokens": 213,
      "augmented": true
    },
    {
      "text": "F Spot Instance Price Variation \nWe proﬁle the spot price of 4 types of  C5  EC2 VMs over a 2-week period in August 2020. Model Params (M) Top1 Accuracy % Latency (ms) Pf Squeezenet 4,253,864 70.10 43.45 10 MobileNEt V2 4,253,864 68.20 41.5 10 Inception V4 23,851,784 76.74 74 6 Resnet50 95,154,159 79.20 98.22 5 ResNet18 44,964,665 76.26 35 6 DenseNet-201 20,242,984 79.80 152.21 2 DenseNet-121 8,062,504 78.72 102.35 3 Xxception 22,910,480 77.80 119.2 4 NasNet 5,326,716 77.90 120 3 InceptionResnetV2 2,510,000 80.30 251.96 1 \nTable 8:  Pretrained models for CIFAR-100 using Imagenet. Similarly Table  9  shows the different models trained for BERT-based sentiment analysis on twitter dataset.",
      "type": "sliding_window_shuffled",
      "tokens": 264,
      "augmented": true
    },
    {
      "text": "0 100 200 300 Time \n0.1 \n0.2 \n0.3 \nPrice ($) \nxlarge 2xlarge 4xlarge 8xlarge \nFigure 18:  Spot instance price variation (time is in hours). F Spot Instance Price Variation \nWe proﬁle the spot price of 4 types of  C5  EC2 VMs over a 2-week period in August 2020. The price variation is shown in Fig 18 .",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    }
  ]
}