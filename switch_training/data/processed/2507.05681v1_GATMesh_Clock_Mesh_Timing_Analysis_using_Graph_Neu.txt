=== ORIGINAL PDF: 2507.05681v1_GATMesh_Clock_Mesh_Timing_Analysis_using_Graph_Neu.pdf ===\n\nRaw text length: 39800 characters\nCleaned text length: 39551 characters\nNumber of segments: 24\n\n=== CLEANED TEXT ===\n\nGATMesh: Clock Mesh Timing Analysis using Graph Neural Networks Muhammad Hadir Khan, Matthew R. Guthaus Computer Science and Engineering University of California Santa Cruz, Santa Cruz, CA 95064 {mkhan33, Abstract Clock meshes are essential in high-performance VLSI systems for minimizing skew and handling PVT variations, but analyzing them is difficult due to reconvergent paths, multi- source driving, and input mesh buffer skew. SPICE simulations are accurate but slow; yet simplified models miss key effects like slew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based framework that models the clock mesh as a graph with augmented structural and physical features. Trained on SPICE data, GATMesh achieves high accuracy with average delay error of 5.27ps on unseen benchmarks, while achieving speed-ups of 47146x over multi-threaded SPICE simulation. I. INTRODUCTION Clock distribution networks (CDNs) are fundamental to high-performance VLSI designs, ensuring precise synchro- nization across millions of sequential elements. Among var- ious architectures, clock meshes stand out for their superior robustness against process, voltage, and temperature (PVT) variations, offering low clock skew and improved tolerance to uncertainties in modern fabrication processes. By leveraging multiple redundant paths as shown in Figure 1, clock meshes effectively distribute the clock signal across the chip. However, these advantages come with significant challenges. One of the primary difficulties in analyzing clock meshes arises from the simultaneous multi-source driving of the net- work. These sources originate from an underlying clock tree, where variations in delay among different branches introduce varying input arrival times to the mesh. Such variations, com- bined with the inherently re-convergent paths within the mesh, make traditional modeling techniques ineffective. SPICE- based simulations, though highly accurate, are computationally prohibitive for large designs, while simplified models, such as first-order delay [1] are inaccurate and fail to capture essential effects like slew propagation, which critically impacts setup and hold constraints. Higher-order model reduction techniques, such as Arnoldi-based methods, are more computationally expensive and often struggle to converge due to the complex interdependencies within the mesh. Furthermore, integrating SPICE-based solutions with static timing analysis (STA) is not practical due to the significant runtime overhead and the difficulty of incorporating SPICE- level accuracy into conventional STA tools. In practice, de- signers of the highest-performance chips often rely on manual simulation [1] [6], clock mesh tuning, and over-design of the mesh to meet timing requirements. This over-design results root clk clk buffer Fig. 1. A clock tree driving the root clock to the mesh buffers (yellow) through clock tree buffers (purple). Mesh buffers then drive the mesh grid with sinks (red squares) to improve skew due to re-converging paths. in increased power consumption and the disconnect between simulation and STA poses a major limitation in timing verifi- cation workflows, restricting efficient design exploration and optimization. To address these challenges, we propose a novel approach using Graph Neural Networks (GNNs) trained on SPICE simulation data to predict clock delays and slew rates ef- ficiently. By treating the clock mesh as a graph, the GNN learns to approximate complex timing relationships, signifi- cantly reducing computational costs while maintaining high accuracy. This data-driven approach can then be applied to unseen designs and enables a rapid, scalable alternative for performance estimation. We demonstrate that our GNN-based framework can pro- vide accurate delay and slew predictions, offering a practical solution for integrating clock mesh analysis into modern VLSI design flows. Our approach lays the groundwork for AI- driven design automation in clock network synthesis, bridging the gap between accuracy and efficiency in timing analysis. Specifically, this paper: Proposes the first GNN methodology for clock-mesh timing analysis. Demonstrates the effectiveness of GNNs in predicting both clock delays and signal slew for better control of arXiv:2507.05681v1 [cs.AR] 8 Jul 2025 setup hold times. Validates the proposed approach against SPICE simula- tions and prior clock-mesh analysis techniques on a range of unseen open-source benchmark designs. II. BACKGROUND A. Clock Meshes Clock meshes have been extensively adopted in high- performance microprocessors by leading semiconductor com- panies [1] [6]. These companies rely on clock meshes to achieve minimal skew, enhance timing reliability, and im- prove tolerance to process, voltage, and temperature (PVT) variations. However, the significant area and power overhead associated with traditional clock meshes have limited their widespread adoption in general ASIC design, necessitating research into more efficient implementations. Several methodologies have been proposed to address the power and area challenges of clock meshes. MeshWorks [7] introduced an automated framework for clock mesh synthesis, enabling efficient planning, optimization, and tradeoff bal- ancing between skew and power consumption. It integrates mesh planning and optimization algorithms, which reduce buffer area, wire length, and power. MeshWorks also intro- duced mesh reduction techniques, which selectively remove redundant mesh segments while maintaining variation toler- ance. This approach significantly reduces power and resource consumption without degrading performance. There have been other works to optimize clock meshes [8] [11], which have further improved upon these results. All of these results, however, have typically relied on restricted models of delay or heuristics instead of delay modeling. In addition, none of the approaches have considered clock signal slew which can have a large impact on timing performance. B. Graph Neural Networks Deep learning over grid structured data like images has shown promising results using Convolutional Neural Networks (CNNs). These same CNNs, however, have been problematic on irregular structured data like graphs because of graph isomorphism (i.e. same graphs with different node order- ings). Graph Neural Networks (GNNs), however, have been a promising technique to solve this by using localized mes- sage passing between neighboring nodes and combining the resulting features into graph, node, or edge predictions. GNNs struggle with deep layers because of the over- smoothing problem [12] where after multiple steps of the ag- gregation operation the node features become indistinguishable from each other. This is particularly problematic on graphs with large spans since communication between distant nodes is limited. Transformers are a recent approach to aid such global communication within a single layer [13], but scale poorly for large graphs. Jumping Knowledge (JK) connections [14] are another approach to dynamically aggregate node representa- tions from both different layers of a GNN to capture both local and global information. While JK connections have somewhat different function than transformers, they scale much better for large graphs. They also enable the model to adaptively choose the most relevant neighborhood range for each node from multiple layers, improving performance on tasks with diverse structural requirements. C. GNNs and Clocks While GNNs have been used for many aspects of EDA, they have had limited use on clock trees and buffer insertion. Buf- Former [15] uses GNNs to predict buffer insertion topologies and locations on non-clock nets. There has also been work on using GNNs to predict the timing of nets, but not necessarily clock nets [16], [17]. More importantly, these prior works did not focus on nets with multiple drivers and reconvergent networks like clock meshes. Single nets and buffered trees are well adapted to traditional STA and higher-moment RC- network analysis methods, but the challenges of clock meshes are not addressed by these methods. III. GATMESH Clock meshes are regular, but they have complexities due to non-uniform drivers, differing input arrival times, irregularity due to reduced meshes and blockages, and uneven distribu- tion of clock sink capacitance. These graph characteristics, however, can be used as input features on GNNs to model clock mesh timing. In particular, we propose to use GNNs inductively to infer results on designs that it has not previously seen. GNNs extend neural networks to graph-structured data, making them ideal for modeling clock meshes, where nodes represent mesh points and edges capture connectivity. Unlike CNNs, which operate on regular grids, GNNs process general graphs, allowing them to learn timing relationships in the meshes more effectively. Additionally, GNNs handle isomor- phism better than CNNs, ensuring consistent predictions across equivalent, but reordered, mesh structures. However, GNNs for clock meshes are still challenged by the over-smoothing problem with deep layers. We model the SPICE netlist of clock meshes as an undi- rected graph as shown in Figure 2. The SPICE netlist clock mesh is a resistor-capacitor (RC) network driven by mesh buffers. We model the interconnect as Wire nodes (W) in the graph. The clock sink pins are modeled as load capacitance to ground which we represent as Sink nodes (S). The mesh buffers are Buffer nodes (B) that connect directly to mesh wires. Each clock sink is connected to a mesh wire through a stub wire represented by an additional W node and a Tap node (T). The exemplary graph representation of a single mesh wire with two sinks and two buffers is shown in Figure 2. One challenge with the above model, and many GNN mod- els in general, is that the number of hops in the graph between buffers and sinks depend on the mesh implementation and the resulting graph topology. If there are many sinks connected to a mesh wire, the mesh wire gets divided into smaller segments with additional T and W nodes. In Figure 2, for example, the buffer B0 drives both the sinks S0 and S1 with S0 being four hops away, whereas S1 is six hops away because it must pass buf1_in B1 T1 S1 T0 buf0_in S0 B0 B0 W1 T0 W2 S0 W3 T1 W4 S1 B1 X0 X1 X2 X3 X4 B0 W1 T0 W2 S0 W3 T1 W4 S1 B1 auxiliary connections W5 W5 Fig. 2. Left: A single mesh wire, its buffers and its graph representation. Right: Graph representation after adding buffer contention (X0) and sink driving (X1 X4) auxiliary connections. through the intermediate T0 and T1 nodes and the divided W1 and W3 node resulting from the split mesh wire. Any GNN model would require an increasingly deeper number of layers to allow message passing through deep graphs, yet so many deep layers would also result in well-known over-smoothing of features in connected regions [12]. While it is possible to reduce the number of nodes by using edge features, most GNN models are primarily node centric. We tried edge feature representations and saw inferior results. We observe, however, that edge feature representations are only a constant factor shallower than a node representation so the problems with deep GNN models still apply in most clock mesh designs. Instead, to enable deep message passing, we rely on two complementary techniques: a static, design inspired augmentation of the graph in this section and a learnable model using JK Connections in Section III-E. The static augmentation of the graph leverages insight from clock mesh designers and previous clock mesh generation papers while the learnable connections leverage machine learning to capture higher-dimension information based on the graph and its corresponding model architecture. A. Sink Driver Auxiliary Connections The key feature of clock meshes is that there are redundant mesh drivers that drive the mesh, and therefore, every sink. While all mesh buffers are driving the same mesh, a clock sink will primarily be driven by nearby clock mesh buffers, but not exclusively. Similar intuition on this locality was used in some prior heuristic algorithms. For example, some clock mesh synthesis algorithms utilize sink coverage metrics to decide where to place buffers [10]. In practice, we have found that considering the two nearest buffers for every sink adds appropriate insight into the neighborhood of buffers driving a sink. This makes sense because each sink is on a mesh wire and buffers are likely coming from two directions along the mesh wire. We include an auxiliary connect between the two nearest buffers in terms of path driving resistance (i.e., the buffer output resistance plus the shortest interconnect path) and corresponding capacitance. For example, in Figure 2, both S0 and S1 are driven by the nearest buffers B0 and B1 so auxiliary connections are added along with nodes X1-X4 to model the features associated with the connections. B. Buffer Contention Auxiliary Connections While the above auxiliary connections model the shared driving of a sink by multiple buffers, buffers can also impede switching if they have drastically different mesh buffer input offset delays, differing output signal slew, and buffer gate delay due to uneven sink density or mismatch between the local capacitance and buffer size. This was first observed in Wilke et al. which analyzed the impact of input tree skew on mesh power [9] but used some novel circuit schemes to reduce the short circuit power. To consider this effect, we add auxiliary connections between mesh buffers so that message passing can learn whether they are switching at the same time or have skew which may affect the nearby sink delays. In practice, we have found that adding the four nearest buffers to every buffer adds appropriate insight into the neighborhood of buffers. While we used two buffers for clock sinks because they are on a mesh wire with two directions, a mesh buffer is usually at an intersection of four mesh wires, so considering four buffers possibly corresponds to all four cardinal directions. Again, like the sink driver connection, we model the minimum path driving resistance and capacitance. For example, in Figure 2, B0 and B1 can have different driving strengths and input delay slew from the root clock tree, so they are connected via an auxiliary connection node X0 to model the features associated with the connection. TABLE I FEATURES AND LABELS (ROWS) ON NODE TYPES (COLUMNS) Buffer Tap Wire Sink Aux Features (B) (T) (W) (S) (X) input delay (ps) input slew (ps) cap (fF) res (ohms) total res (ohms) min res (ohms) region cap (fF) xy loc (µm) Buffer Tap Wire Sink Aux Labels (B) (T) (W) (S) (X) delay out (ps) slew out (ps) C. Input Features The node features used for the GNN model are shown in Table I. The check mark indicates non-zero values for these features whereas the cross mark indicates 0 for these features. The input delay and input slew are the delay and slew respectively of the inputs arriving at the mesh buffers after propagating through a top-level tree (see Figure 1). Since the top-level tree is constructed using a zero-skew Elmore delay model for interconnect and buffers, there is generally some skew between the mesh buffer inputs. In addition, each mesh buffer has differing input slew which can affect the delay of the mesh buffer. The capacitance (cap) and resistance (res) features depend on the origin and type of graph node. For buffers, cap is the buffer output capacitance and res is the buffer output (driving) resistance. For wire segments such as mesh wires or stub wires (i.e., from sink to mesh wire), the res and cap depend on the routing layer parasitics and interconnect length. For sink nodes, the cap feature is the load capacitance of the input pin of the sequential element. The xy loc are the x and y coordinates of the nodes in the layout. Besides the direct features, we also add some pre-computed features to help the model learn about signal propagation in meshes. In particular, we compute the region cap which is the total capacitance within a particular resistance from a node. This effectively models the nearby capacitance to a node and is similar to the downstream capacitance in an Elmore delay model. We chose the resistance limit as the resistance of a single mesh wire so that it will consider all of the nearby sinks. In addition to capacitance, we also model an aggregate driv- ing resistance to a node from the mesh buffers. The total res is the path resistance (i.e. summation of the conductances) of the minimum resistance path from each buffer to the node. The min res is the single lowest resistance from any buffer to the node. The total res provides insight into how many buffers are simultaneously driving a given node while min res provides insight into how well a node is driven by any one buffer. While the augmented features provide insight into the resistance and capacitance behavior of a driven node, they do not help with the topology and message passing. This is done through the auxiliary connections for buffer contention and sink driving which also have res and cap obtained from the minimum resistance paths. D. Output Labels The sink nodes have output labels for the delay and slew computed by SPICE simulation. While delay is important for skew, the slew is equally important as it can affect the setup and hold times of sequential elements. While we could obtain better delay accuracy by not considering slew, slew has a significant impact on the setup and hold times of sequential elements. In Nangate45, for example, a DFF X1 flip-flop has setup and hold constraints as shown in Figure 3, assuming a single fixed data input slew. Most prior clock synthesis results do not consider clock signal slew, but this is an important metric in high-performance clock synthesis and is therefore an important part of our model output. E. Model Architecture Our model architecture, illustrated in Figure 4 with only three layers, uses the Graph Attention Network (GAT) con- volution layers [18] with JK connections [14]. We predict both delay and slew for clock sink nodes so we have a node- level, continuous regression problem. Furthermore, since we do not want to need values for all the nodes, we use a mask to compute the loss only from sink nodes. 0 25 50 75 100 125 150 175 200 Clock Slew (ps) 20 40 60 80 100 120 140 160 Hold Time (ps) Hold Constraints vs. Clock Slew (Fixed data slew 44.93 ps) Hold (data fall) Hold (data rise) 0 25 50 75 100 125 150 175 200 Clock Slew (ps) 20 30 40 50 60 70 80 Setup Time (ps) Setup Constraints vs. Clock Slew (Fixed data slew 44.93 ps) Setup (data fall) Setup (data rise) Fig. 3. Clock signal input slew has a significant impact on setup and hold times and needs to be considered as a primary objective of clock synthesis. The mesh graphs along with node features from Table I are input to GAT Convolutional Layers (GATConv). At each layer k, new node features hk 1 are computed by aggregating the information from the neighbors according to the attention mechanism. Multiple heads are simultaneously computed with learnable attention weights which allows us to give importance to the different neighbors when aggregating their features. For example with four heads, GAT assigns four different weight vectors to the neighbor features when aggregating and concatenates the weighted features together. Intuitively, this may correspond to multiple combinations of driving wires or buffers from different directions on the mesh. To compute the final node features hfinal we utilize the JK connections which are shown in Figure 4 with the red dashed line. Generally, the features from each layer hk, hk 1, ...hl where l are the number of layers are passed to the final layer where they are either concatenated, max-pooled or passed to a Bi-directional LSTM. In our case, we use the max-pooling where it learns to select the most informative layer for each feature coordinate. Concatenation is the most straightforward but it only optimizes the weights to combine the features in a way that works best for the overall dataset which can cause underfitting when the graphs are too complex. On the other end, LSTM is more complex and can cause overfitting. Of these aggregation schemes, JK connections with max pooling shows better performance overall in deep GNNs [14] like ours. The JK connections serve a different purpose compared to the auxiliary connections we added to our graphs. They provide learnable combination of features in each neighbor- hood of the RC mesh whereas the aux connections model two physical aspects of a clock mesh: multiple buffer contention and multiple sink drivers which are statically chosen. IV. METHODOLOGY A. Training Data Generation We analyzed some of the open-source benchmark designs with OpenROAD [19] and implemented them using the default flow in the Nangate45 technology. We used the design statistics of area, number of sinks, and sink density to generate various synthetic designs for clock synthesis as shown in Table II. Clock meshes were generated for these synthetic designs using both uniform buffering [7] and non-uniform buffering [10]. The uniform and non-uniform buffered meshes use the same Mesh Graphs Graph features GATConv GATConv JK Linear Delay Slew -24.5 44.6 12.12 0.15 44.2 44.8 123.45 123.96 124.23 124.19 44.9 45.2 Masking 123.45 44.2 44.8 123.96 124.23 44.9 45.2 124.19 MSE Loss Back propagation Final predictions h1 h2 hfinal Fig. 4. Model architecture overview. As an example three GATConv layers are shown with Jumping Knowledge connections (shown in red). Masking is performed to only calculate the loss for predictions on sink nodes. Meshworks mesh sizing with minimum total wire length while the fixed mesh used a fixed 50µm mesh with non-uniform buffering. We also observed large designs in OpenROAD having clustered sinks with whitespace blockages for macros and memories. To simulate that we also create clusters of sinks for larger synthetic designs to reflect macro blockages. Top-level trees were created to drive the clock buffers using the well known DME [20], [21] combined with balanced buffering [22]. The top-level trees are zero-skew according to the Elmore model but may have some skew due to mismatch between SPICE and Elmore delay models. The arrival input delay and slew of the tree leaves are used as features of the mesh buffers as shown in Table I. TABLE II STATISTICS OF THE TRAINING DATASET Number of Mesh Types Area Density Uniform Non-Uni. Fixed (µm2) (sinks µm2) Small 100 100 100 [500, 1500] 0.033 Medium 100 100 100 [40000, 60000] [0.01, 0.04] Large 100 100 100 [180000, 220000] 0.021 B. Training Procedure Our model makes delay and slew predictions for all the nodes but we compute the loss only for the sink nodes using a mask with a Mean-Squared Error (MSE) loss function together with the target labels which are computed by running SPICE simulation on the clock meshes. The gradients are calculated and back-propagated to the JK layer as well as to the preceding GATConv layers. This allows the JK layer to learn the importance of each feature for different granularity of neighborhood and relate the features to the target delay and slew. Simultaneously, the GATConv layers learn to assign appropriate attention weights to the neighbors at the current level for the delay and slew predictions. We used ADAM [23] as an optimizer with a learning rate of 7.5e 4 and weight decay of 5e 4 for L2-regularization. For non-linear activations we used ELU [24]. We used batch size of 1 and train with a default 1000 epochs with random shuffling of training data each epoch. We also implemented early-stopping in our training loop with a patience of 20. With an ample amount of synthetic training data, L2-regularization, and early-stopping, we did not find the need to add dropout to the layers since the model does not overfit. C. Model Parameters The model hyperparameters we used are shown in Table III. Through empirical evaluations, we found better performance by having a deep model architecture and therefore used eight convolution layers. This is due to the depth of the mesh graph model a sink that is driven by a closest buffer will be at minimum four hops away as shown in Figure 2. However, there can be many other sinks nearby and each of them would create tap point nodes on the mesh wire thereby increasing the depth of the graph. We tried other convolution layer types besides GATConv, but found that learning the weights to combine neighbors is very important, likely because the importance is related to the resistance features of neighbor wires. The auxiliary connections add the insight of nearby driving buffers and improve message passing; however, these only model the minimum resistance paths and not the full mesh, so the local connections through RC mesh nodes also generate information on how the sinks interact within their local interconnect. We observed that four attention heads performed well, likely due to the four orthogonal directions at each mesh point. TABLE III MODEL HYPERPARAMETERS Parameter Value Convolution layer GATConv Number of convolution layers 8 Number of convolution channels 64 Number of attention heads 4 JK Aggregation max D. Experimental Setup We implement GATMesh in Pytorch [25] with the Pytorch Geometric library [26]. The Ngspice and first-order analysis as well as the benchmark generation was run on a dual AMD EPYC 7542 32-Core Processor machine with 512Gb of memory. The training and testing of GATMesh was done on an NVIDIA GeForce RTX 4090 24GiB card. V. RESULTS We compare our GATMesh model with both SPICE which is the golden model and a well-known first-order approx- imation [1] that is used in most of the prior clock mesh synthesis research papers. While RC network analysis may provide more detail than first-order delay estimates, it still falls far short of SPICE-level accuracy because it typically neglects key effects in clock meshes. These methods often assume linear superposition of buffers driving the mesh and ignore the nonlinear behavior of buffers, particularly when multiple drivers with differing input arrival times and output contention are present. These methods also oversimplify the effects of the redundant mesh paths, where signals from the different drivers interact. These interactions can potentially reinforce or cancel each other. As a result, such models can misestimate both delay and slew, making them only marginally more accurate than first-order approximations but still significantly less reliable than full SPICE simulations. We also attempted to use some higher order models, but these always failed to converge. The first-order models, on the other hand, would usually converge. Our work focuses on comparing with SPICE and a first-order model as a representative of the other heuristic models. A. Training and Model Accuracy We train on a dataset of 900 synthetic designs shown in Table II. The training development test set split is 80 10 10, so in total we have 720 samples for training, 90 for validation and 90 for testing. The training takes 200 epochs to converge in under 2 hours. We train and validate using Mean Squared Error (MSE) loss on delay and slew of the sink nodes. The squared term helps the model to penalize the mispredictions especially when the predictions are far from the true labels and is convex. The training converges with an MSE loss of 23.87ps, validation set MSE loss of 31.83ps, and test set MSE loss of 33.02ps. The validation loss tracks the training loss meaning that the model is able to learn well. The testing loss also matches which means we are not over-fitting. The model is trained once on the synthetic data and used for inductive inference on a test set of open-source designs that it has not seen in the next section. B. Accuracy Evaluation In order to evaluate our model s ability to inductively infer results on unseen realistic designs, we evaluate on a subset of open-source designs created with the OpenROAD flow. In contrast to the MSE for training loss, the delay and slew Mean Absolute Error (MAE) from the GATMesh model predictions and first-order model [1] are shown in Table IV. We use SPICE simulation results as the reference to measure the error. Against SPICE, our model performs competitively with the worst delay MAE being only 16.16ps and, on average, only 5.27ps. It also does quite well in predicting slews, with the worst slew MAE being 13.5ps and, on average, 5.98ps. The worst case design for both delay and slew is swerv wrapper which has significantly larger area compared to our training statistics. We could likely improve these results with a more representative training set. On the other hand, our model significantly outperforms the first-order model which has a worst mean delay error of 278.45ps and, on average, 200.19ps. The first-order model has no slew to compare and using an Elmore model approximation such as 2.2 delay would be quite unfair and a poor comparison given that the delays are already quite inaccurate. C. Run-Time Evaluation We also present a runtime analysis with our GATMesh model inference, the first-order model algorithm, and Ngspice simulation [27] in Table V. Ngspice, version 42, is run with the default two threads, but using more threads did not see any significant performance gain on these benchmarks. This is because the model evaluation of the mesh buffers can be easily parallelized, but the solution of the RC mesh equations cannot be parallelized very well at each simulation time step. The first-order model is faster than Ngspice but as the design sizes get bigger the runtime starts to increase even beyond that of our model. The prior work [1] showed more than 24 hour run-times for the first-order algorithm on the largest designs. Overall, our model has a nearly constant runtime even for larger designs due to inference being faster with parallelization on GPUs. It is 1718 faster on average than first-order model and on average 47146 faster than Ngspice. D. Ablation Study We investigated the effectiveness of the different modeling decisions and how they impact the capability of the model to learn mesh timing. Specifically, we examine the static auxiliary connections that improve message passing from drivers to sinks and between buffers. We compare whether this static architecture is preferable to JK connections which learns to group features in different neighborhoods and between different levels of a multi-layer GNN. To do this, we did an ablation study of four different models. The first base case removes both the JK connections and the auxiliary connections in the graph but relies on the RC mesh and associated features. The features include derived ones such as regional capacitance, minimum resistance from nearby buffer, and total resistance from nearby buffers which do give some regional perspective of the RC mesh. These features, however, do not enable improvements in message passing like the auxiliary connections or learning over multiple layers like JK connections. They do, however, utilize multi-head attention for computing features from layer to layer. TABLE IV TIMING ANALYSIS ON OPEN-SOURCE DESIGNS COMPARED TO NGSPICE SIMULATION Open-Source Stats Delay error (ps) Slew error (ps) Designs Area (µm2) Sinks Mesh Size Buffers GAT-Mesh [1] GAT-Mesh [1] gcd 1082.88 35 2x2 4 1.76 79.63 0.31 N A aes 52785.04 562 6x6 31 2.65 157.34 2.69 N A ibex 57585.27 1932 15x15 168 2.91 203.39 7.48 N A dynamic node 54950.01 2257 15x15 180 3.58 199.78 3.18 N A tiny rocket 193770.36 4035 18x18 268 3.67 278.45 7.74 N A jpeg 198612.62 4383 19x19 287 3.46 266.13 5.10 N A swerv 387381.12 11180 34x34 799 7.99 197.13 7.87 N A swerv wrapper 1056663.92 11214 35x35 538 16.16 219.71 13.50 N A Average Error 5.27 200.19 5.98 N A TABLE V RUNTIME ANALYSIS OF GATMESH, FIRST-ORDER MODEL [1], AND NGSPICE Open-Source Runtime (s) Designs GATMesh [1] Ngspice gcd 0.0038 0.0003 0.2094 aes 0.0039 0.0527 2.5333 ibex 0.0041 0.0637 14.4586 dynamic node 0.0041 0.6546 16.8264 tiny rocket 0.0044 2.4565 41.7799 jpeg 0.0046 3.5786 49.8608 swerv 0.0088 29.5014 851.3769 swerv wrapper 0.0087 28.3169 795.6460 Average Runtime 0.0047 8.0780 221.5864 Average Speedup 1 1718 47146 The other models still utilize the same RC mesh and features, except that the second model adds the auxiliary connections but no JK connections, and the third model adds only JK connections but no auxiliary connections. The last is our GATMesh model with both auxiliary connections and JK connections. All the other parameters are the same as described in Table III. We trained and tested each of these models on the identical synthetic dataset for comparison and present the results in Fig- ure 5. For both the delay and slew prediction, our GATMesh model produces substantial improvements over the baseline model without the static aux or learnable JK connections. The auxiliary connections alone produce significant gains on their own with the delay being slightly better than GATMesh but the slew being slightly worse. The JK connections alone achieve only moderate delay improvement, and almost no impact on slew error. VI. CONCLUSION We present GATMesh, a novel framework for accurate and efficient clock mesh timing analysis using Graph Neural Net- works. By modeling the mesh as a graph of resistive-capacitive elements, including some derived features, and incorporating both learned (Jumping Knowledge connections) and domain- driven (auxiliary connections) enhancements, GATMesh effec- tively captures the complex interactions of multiple drivers, reconvergent paths, and spatial design variations. Our method models delay as well as slew of the signal and achieves Baseline Baseline Aux Baseline JK GATMesh 0 1 2 3 4 5 6 7 8 Delay Slew MAE Error (ps) Delay MAE Error (ps) Slew MAE Error (ps) Fig. 5. The addition of the auxiliary connections offers the most benefit for the delay MAE, however a combination of the auxiliary and JK connections proves better for slew. predictions within a few picoseconds of SPICE accuracy while delivering massive speedups. Through extensive experiments and ablation studies, we show that both static and learnable ar- chitectural augmentations help prediction accuracy. GATMesh offers a practical and scalable path toward integrating clock meshes in real-world ASIC design flows by enabling faster and more accurate clock mesh analysis. REFERENCES [1] M. P. Desai, R. Cvijetic, and J. Jensen, Sizing of clock distribution networks for high performance CPU chips, in Design Automation Conference (DAC), pp. 389 394, 1996. [2] P. Restle, C. Carter, J. Eckhardt, B. Krauter, B. McCredie, K. Jenkins, A. Weger, and A. Mule, The clock distribution of the POWER4 mi- croprocessor, in International Solid-State Circuits Conference (ISSCC), pp. 144 145, 2002. [3] M. G.-R. Thomson, P. J. Restle, and N. K. James, A 5GHz duty-cycle correcting clock distribution network for the POWER6 microprocessor, in International Solid-State Circuits Conference (ISSCC), pp. 1522 1529, 2006. [4] V. Sathe, S. Arekapudi, C. Ouyang, M. Papaefthymiou, A. Ishii, and S. Naffziger, Resonant clock design for a power-efficient high-volume x86 64 microprocessor, in 2012 IEEE International Solid-State Circuits Conference, pp. 68 70, 2012. [5] H. McIntyre, S. Arekapudi, E. Busta, T. Fischer, M. Golden, A. Horiuchi, T. Meneghini, S. Naffziger, and J. Vinh, Design of the two-core x86-64 amd bulldozer module in 32 nm soi cmos, IEEE Journal of Solid- State Circuits, vol. 47, no. 1, pp. 164 176, 2012. [6] M. Guthaus, X. Hu, G. Wilke, G. Flache, and R. Reis, High- performance clock mesh optimization, ACM Transactions on Design Automation of Electronic Systems (TODAES), 2012. [7] A. Rajaram and D. Z. Pan, Meshworks: an efficient framework for planning, synthesis and optimization of clock mesh networks, in Asia and South Pacific Design Automation Conference (ASP-DAC), pp. 250 257, 2008. [8] Y. Teng and B. Taskin, Clock mesh synthesis method using the earth mover s distance under transformations, in 2012 IEEE 30th International Conference on Computer Design (ICCD), pp. 121 126, 2012. [9] G. Wilke, R. Fonseca, C. Mezzomo, and R. Reis, A novel scheme to reduce short-circuit power in mesh-based clock architectures, in Symposium on Integrated Circuits and System Design (SBCCI), pp. 117 122, 2008. [10] M. R. Guthaus, G. Wilke, and R. Reis, Non-uniform clock mesh optimization with linear programming buffer insertion, in Design Au- tomation Conference (DAC), 2010. [11] A. Abdelhadi, R. Ginosar, A. Kolodny, and E. G. Friedman, Timing- driven variation-aware nonuniform clock mesh synthesis, in Great lakes Symposium on VLSI (GLSVLSI), pp. 15 20, 2010. [12] T. K. Rusch, M. M. Bronstein, and S. Mishra, A survey on oversmooth- ing in graph neural networks, arXiv:2303.10993, 2023. [13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS 17, (Red Hook, NY, USA), p. 6000 6010, Curran Associates Inc., 2017. [14] K. Xu, C. Li, Y. Tian, T. Sonobe, K. ichi Kawarabayashi, and S. Jegelka, Representation learning on graphs with jumping knowledge networks, arXiv:1806.03536, 2018. [15] R. Liang, S. Nath, A. Rajaram, J. Hu, and H. Ren, Bufformer: A generative ml framework for scalable buffering, in Proceedings of the 28th Asia and South Pacific Design Automation Conference, ASPDAC 23, (New York, NY, USA), p. 264 270, Association for Computing Machinery, 2023. [16] Y. Ye, T. Chen, Y. Gao, H. Yan, B. Yu, and L. Shi, Fast and accurate wire timing estimation based on graph learning, Design, Automation Test in Europe Conference (DATE), 2023. [17] H.-H. Cheng, I. H.-R. Jiang, and O. Ou, Fast and accurate wire timing estimation on tree and non-tree net structures, Design Automation Conference (DAC), 2020. [18] P. Veliˇckovi c, G. Cucurull, A. Casanova, A. Romero, P. Li o, and Y. Bengio, Graph Attention Networks, arXiv:1710.10903, 2018. [19] A. B. Kahng and T. Spyrou, The OpenROAD project: Unleashing hardware innovation, in Proc. GOMAC, 2021. [20] K. Boese and A. Kahng, Zero-skew clock routing trees with minimum wirelength, in ASIC Conf., pp. 1.1.1 1.1.5, 1992. [21] T.-H. Chao, Y.-C. Hsu, and J. Ho, Zero skew clock net routing, in Design Automation Conference (DAC), pp. 518 523, 1992. [22] J.-L. Tsai, T.-H. Chen, and C. C. Chen, Zero skew clock-tree optimiza- tion with buffer insertion sizing and wire sizing, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 23, no. 4, pp. 565 573, 2004. [23] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, 2017. [24] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, Fast and accurate deep network learning by exponential linear units (elus), 2016. [25] J. Ansel et al., PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation, in 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 24), ACM, Apr. 2024. [26] M. Fey and J. E. Lenssen, Fast graph representation learning with PyTorch Geometric, in ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. [27] The NGSpice Project, ngspice open source circuit simulator, version 42. 2024. Accessed: 2025-04-12.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nGATMesh: Clock Mesh Timing Analysis using Graph Neural Networks Muhammad Hadir Khan, Matthew R. Guthaus Computer Science and Engineering University of California Santa Cruz, Santa Cruz, CA 95064 {mkhan33, Abstract Clock meshes are essential in high-performance VLSI systems for minimizing skew and handling PVT variations, but analyzing them is difficult due to reconvergent paths, multi- source driving, and input mesh buffer skew. SPICE simulations are accurate but slow; yet simplified models miss key effects like slew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based framework that models the clock mesh as a graph with augmented structural and physical features. Trained on SPICE data, GATMesh achieves high accuracy with average delay error of 5.27ps on unseen benchmarks, while achieving speed-ups of 47146x over multi-threaded SPICE simulation. I. INTRODUCTION Clock distribution networks (CDNs) are fundamental to high-performance VLSI designs, ensuring precise synchro- nization across millions of sequential elements. Among var- ious architectures, clock meshes stand out for their superior robustness against process, voltage, and temperature (PVT) variations, offering low clock skew and improved tolerance to uncertainties in modern fabrication processes. By leveraging multiple redundant paths as shown in Figure 1, clock meshes effectively distribute the clock signal across the chip. However, these advantages come with significant challenges. One of the primary difficulties in analyzing clock meshes arises from the simultaneous multi-source driving of the net- work. These sources originate from an underlying clock tree, where variations in delay among different branches introduce varying input arrival times to the mesh. Such variations, com- bined with the inherently re-convergent paths within the mesh, make traditional modeling techniques ineffective. SPICE- based simulations, though highly accurate, are computationally prohibitive for large designs, while simplified models, such as first-order delay [1] are inaccurate and fail to capture essential effects like slew propagation, which critically impacts setup and hold constraints. Higher-order model reduction techniques, such as Arnoldi-based methods, are more computationally expensive and often struggle to converge due to the complex interdependencies within the mesh. Furthermore, integrating SPICE-based solutions with static timing analysis (STA) is not practical due to the significant runtime overhead and the difficulty of incorporating SPICE- level accuracy into conventional STA tools.\n\n--- Segment 2 ---\nHigher-order model reduction techniques, such as Arnoldi-based methods, are more computationally expensive and often struggle to converge due to the complex interdependencies within the mesh. Furthermore, integrating SPICE-based solutions with static timing analysis (STA) is not practical due to the significant runtime overhead and the difficulty of incorporating SPICE- level accuracy into conventional STA tools. In practice, de- signers of the highest-performance chips often rely on manual simulation [1] [6], clock mesh tuning, and over-design of the mesh to meet timing requirements. This over-design results root clk clk buffer Fig. 1. A clock tree driving the root clock to the mesh buffers (yellow) through clock tree buffers (purple). Mesh buffers then drive the mesh grid with sinks (red squares) to improve skew due to re-converging paths. in increased power consumption and the disconnect between simulation and STA poses a major limitation in timing verifi- cation workflows, restricting efficient design exploration and optimization. To address these challenges, we propose a novel approach using Graph Neural Networks (GNNs) trained on SPICE simulation data to predict clock delays and slew rates ef- ficiently. By treating the clock mesh as a graph, the GNN learns to approximate complex timing relationships, signifi- cantly reducing computational costs while maintaining high accuracy. This data-driven approach can then be applied to unseen designs and enables a rapid, scalable alternative for performance estimation. We demonstrate that our GNN-based framework can pro- vide accurate delay and slew predictions, offering a practical solution for integrating clock mesh analysis into modern VLSI design flows. Our approach lays the groundwork for AI- driven design automation in clock network synthesis, bridging the gap between accuracy and efficiency in timing analysis. Specifically, this paper: Proposes the first GNN methodology for clock-mesh timing analysis. Demonstrates the effectiveness of GNNs in predicting both clock delays and signal slew for better control of arXiv:2507.05681v1 [cs.AR] 8 Jul 2025 setup hold times. Validates the proposed approach against SPICE simula- tions and prior clock-mesh analysis techniques on a range of unseen open-source benchmark designs. II. BACKGROUND A. Clock Meshes Clock meshes have been extensively adopted in high- performance microprocessors by leading semiconductor com- panies [1] [6].\n\n--- Segment 3 ---\nBACKGROUND A. Clock Meshes Clock meshes have been extensively adopted in high- performance microprocessors by leading semiconductor com- panies [1] [6]. These companies rely on clock meshes to achieve minimal skew, enhance timing reliability, and im- prove tolerance to process, voltage, and temperature (PVT) variations. However, the significant area and power overhead associated with traditional clock meshes have limited their widespread adoption in general ASIC design, necessitating research into more efficient implementations. Several methodologies have been proposed to address the power and area challenges of clock meshes. MeshWorks [7] introduced an automated framework for clock mesh synthesis, enabling efficient planning, optimization, and tradeoff bal- ancing between skew and power consumption. It integrates mesh planning and optimization algorithms, which reduce buffer area, wire length, and power. MeshWorks also intro- duced mesh reduction techniques, which selectively remove redundant mesh segments while maintaining variation toler- ance. This approach significantly reduces power and resource consumption without degrading performance. There have been other works to optimize clock meshes [8] [11], which have further improved upon these results. All of these results, however, have typically relied on restricted models of delay or heuristics instead of delay modeling. In addition, none of the approaches have considered clock signal slew which can have a large impact on timing performance. B. Graph Neural Networks Deep learning over grid structured data like images has shown promising results using Convolutional Neural Networks (CNNs). These same CNNs, however, have been problematic on irregular structured data like graphs because of graph isomorphism (i.e. same graphs with different node order- ings). Graph Neural Networks (GNNs), however, have been a promising technique to solve this by using localized mes- sage passing between neighboring nodes and combining the resulting features into graph, node, or edge predictions. GNNs struggle with deep layers because of the over- smoothing problem [12] where after multiple steps of the ag- gregation operation the node features become indistinguishable from each other. This is particularly problematic on graphs with large spans since communication between distant nodes is limited. Transformers are a recent approach to aid such global communication within a single layer [13], but scale poorly for large graphs. Jumping Knowledge (JK) connections [14] are another approach to dynamically aggregate node representa- tions from both different layers of a GNN to capture both local and global information.\n\n--- Segment 4 ---\nTransformers are a recent approach to aid such global communication within a single layer [13], but scale poorly for large graphs. Jumping Knowledge (JK) connections [14] are another approach to dynamically aggregate node representa- tions from both different layers of a GNN to capture both local and global information. While JK connections have somewhat different function than transformers, they scale much better for large graphs. They also enable the model to adaptively choose the most relevant neighborhood range for each node from multiple layers, improving performance on tasks with diverse structural requirements. C. GNNs and Clocks While GNNs have been used for many aspects of EDA, they have had limited use on clock trees and buffer insertion. Buf- Former [15] uses GNNs to predict buffer insertion topologies and locations on non-clock nets. There has also been work on using GNNs to predict the timing of nets, but not necessarily clock nets [16], [17]. More importantly, these prior works did not focus on nets with multiple drivers and reconvergent networks like clock meshes. Single nets and buffered trees are well adapted to traditional STA and higher-moment RC- network analysis methods, but the challenges of clock meshes are not addressed by these methods. III. GATMESH Clock meshes are regular, but they have complexities due to non-uniform drivers, differing input arrival times, irregularity due to reduced meshes and blockages, and uneven distribu- tion of clock sink capacitance. These graph characteristics, however, can be used as input features on GNNs to model clock mesh timing. In particular, we propose to use GNNs inductively to infer results on designs that it has not previously seen. GNNs extend neural networks to graph-structured data, making them ideal for modeling clock meshes, where nodes represent mesh points and edges capture connectivity. Unlike CNNs, which operate on regular grids, GNNs process general graphs, allowing them to learn timing relationships in the meshes more effectively. Additionally, GNNs handle isomor- phism better than CNNs, ensuring consistent predictions across equivalent, but reordered, mesh structures. However, GNNs for clock meshes are still challenged by the over-smoothing problem with deep layers. We model the SPICE netlist of clock meshes as an undi- rected graph as shown in Figure 2.\n\n--- Segment 5 ---\nHowever, GNNs for clock meshes are still challenged by the over-smoothing problem with deep layers. We model the SPICE netlist of clock meshes as an undi- rected graph as shown in Figure 2. The SPICE netlist clock mesh is a resistor-capacitor (RC) network driven by mesh buffers. We model the interconnect as Wire nodes (W) in the graph. The clock sink pins are modeled as load capacitance to ground which we represent as Sink nodes (S). The mesh buffers are Buffer nodes (B) that connect directly to mesh wires. Each clock sink is connected to a mesh wire through a stub wire represented by an additional W node and a Tap node (T). The exemplary graph representation of a single mesh wire with two sinks and two buffers is shown in Figure 2. One challenge with the above model, and many GNN mod- els in general, is that the number of hops in the graph between buffers and sinks depend on the mesh implementation and the resulting graph topology. If there are many sinks connected to a mesh wire, the mesh wire gets divided into smaller segments with additional T and W nodes. In Figure 2, for example, the buffer B0 drives both the sinks S0 and S1 with S0 being four hops away, whereas S1 is six hops away because it must pass buf1_in B1 T1 S1 T0 buf0_in S0 B0 B0 W1 T0 W2 S0 W3 T1 W4 S1 B1 X0 X1 X2 X3 X4 B0 W1 T0 W2 S0 W3 T1 W4 S1 B1 auxiliary connections W5 W5 Fig. 2. Left: A single mesh wire, its buffers and its graph representation. Right: Graph representation after adding buffer contention (X0) and sink driving (X1 X4) auxiliary connections. through the intermediate T0 and T1 nodes and the divided W1 and W3 node resulting from the split mesh wire. Any GNN model would require an increasingly deeper number of layers to allow message passing through deep graphs, yet so many deep layers would also result in well-known over-smoothing of features in connected regions [12]. While it is possible to reduce the number of nodes by using edge features, most GNN models are primarily node centric. We tried edge feature representations and saw inferior results.\n\n--- Segment 6 ---\nWhile it is possible to reduce the number of nodes by using edge features, most GNN models are primarily node centric. We tried edge feature representations and saw inferior results. We observe, however, that edge feature representations are only a constant factor shallower than a node representation so the problems with deep GNN models still apply in most clock mesh designs. Instead, to enable deep message passing, we rely on two complementary techniques: a static, design inspired augmentation of the graph in this section and a learnable model using JK Connections in Section III-E. The static augmentation of the graph leverages insight from clock mesh designers and previous clock mesh generation papers while the learnable connections leverage machine learning to capture higher-dimension information based on the graph and its corresponding model architecture. A. Sink Driver Auxiliary Connections The key feature of clock meshes is that there are redundant mesh drivers that drive the mesh, and therefore, every sink. While all mesh buffers are driving the same mesh, a clock sink will primarily be driven by nearby clock mesh buffers, but not exclusively. Similar intuition on this locality was used in some prior heuristic algorithms. For example, some clock mesh synthesis algorithms utilize sink coverage metrics to decide where to place buffers [10]. In practice, we have found that considering the two nearest buffers for every sink adds appropriate insight into the neighborhood of buffers driving a sink. This makes sense because each sink is on a mesh wire and buffers are likely coming from two directions along the mesh wire. We include an auxiliary connect between the two nearest buffers in terms of path driving resistance (i.e., the buffer output resistance plus the shortest interconnect path) and corresponding capacitance. For example, in Figure 2, both S0 and S1 are driven by the nearest buffers B0 and B1 so auxiliary connections are added along with nodes X1-X4 to model the features associated with the connections. B. Buffer Contention Auxiliary Connections While the above auxiliary connections model the shared driving of a sink by multiple buffers, buffers can also impede switching if they have drastically different mesh buffer input offset delays, differing output signal slew, and buffer gate delay due to uneven sink density or mismatch between the local capacitance and buffer size. This was first observed in Wilke et al. which analyzed the impact of input tree skew on mesh power [9] but used some novel circuit schemes to reduce the short circuit power.\n\n--- Segment 7 ---\nThis was first observed in Wilke et al. which analyzed the impact of input tree skew on mesh power [9] but used some novel circuit schemes to reduce the short circuit power. To consider this effect, we add auxiliary connections between mesh buffers so that message passing can learn whether they are switching at the same time or have skew which may affect the nearby sink delays. In practice, we have found that adding the four nearest buffers to every buffer adds appropriate insight into the neighborhood of buffers. While we used two buffers for clock sinks because they are on a mesh wire with two directions, a mesh buffer is usually at an intersection of four mesh wires, so considering four buffers possibly corresponds to all four cardinal directions. Again, like the sink driver connection, we model the minimum path driving resistance and capacitance. For example, in Figure 2, B0 and B1 can have different driving strengths and input delay slew from the root clock tree, so they are connected via an auxiliary connection node X0 to model the features associated with the connection. TABLE I FEATURES AND LABELS (ROWS) ON NODE TYPES (COLUMNS) Buffer Tap Wire Sink Aux Features (B) (T) (W) (S) (X) input delay (ps) input slew (ps) cap (fF) res (ohms) total res (ohms) min res (ohms) region cap (fF) xy loc (µm) Buffer Tap Wire Sink Aux Labels (B) (T) (W) (S) (X) delay out (ps) slew out (ps) C. Input Features The node features used for the GNN model are shown in Table I. The check mark indicates non-zero values for these features whereas the cross mark indicates 0 for these features. The input delay and input slew are the delay and slew respectively of the inputs arriving at the mesh buffers after propagating through a top-level tree (see Figure 1). Since the top-level tree is constructed using a zero-skew Elmore delay model for interconnect and buffers, there is generally some skew between the mesh buffer inputs. In addition, each mesh buffer has differing input slew which can affect the delay of the mesh buffer. The capacitance (cap) and resistance (res) features depend on the origin and type of graph node.\n\n--- Segment 8 ---\nIn addition, each mesh buffer has differing input slew which can affect the delay of the mesh buffer. The capacitance (cap) and resistance (res) features depend on the origin and type of graph node. For buffers, cap is the buffer output capacitance and res is the buffer output (driving) resistance. For wire segments such as mesh wires or stub wires (i.e., from sink to mesh wire), the res and cap depend on the routing layer parasitics and interconnect length. For sink nodes, the cap feature is the load capacitance of the input pin of the sequential element. The xy loc are the x and y coordinates of the nodes in the layout. Besides the direct features, we also add some pre-computed features to help the model learn about signal propagation in meshes. In particular, we compute the region cap which is the total capacitance within a particular resistance from a node. This effectively models the nearby capacitance to a node and is similar to the downstream capacitance in an Elmore delay model. We chose the resistance limit as the resistance of a single mesh wire so that it will consider all of the nearby sinks. In addition to capacitance, we also model an aggregate driv- ing resistance to a node from the mesh buffers. The total res is the path resistance (i.e. summation of the conductances) of the minimum resistance path from each buffer to the node. The min res is the single lowest resistance from any buffer to the node. The total res provides insight into how many buffers are simultaneously driving a given node while min res provides insight into how well a node is driven by any one buffer. While the augmented features provide insight into the resistance and capacitance behavior of a driven node, they do not help with the topology and message passing. This is done through the auxiliary connections for buffer contention and sink driving which also have res and cap obtained from the minimum resistance paths. D. Output Labels The sink nodes have output labels for the delay and slew computed by SPICE simulation. While delay is important for skew, the slew is equally important as it can affect the setup and hold times of sequential elements. While we could obtain better delay accuracy by not considering slew, slew has a significant impact on the setup and hold times of sequential elements.\n\n--- Segment 9 ---\nWhile delay is important for skew, the slew is equally important as it can affect the setup and hold times of sequential elements. While we could obtain better delay accuracy by not considering slew, slew has a significant impact on the setup and hold times of sequential elements. In Nangate45, for example, a DFF X1 flip-flop has setup and hold constraints as shown in Figure 3, assuming a single fixed data input slew. Most prior clock synthesis results do not consider clock signal slew, but this is an important metric in high-performance clock synthesis and is therefore an important part of our model output. E. Model Architecture Our model architecture, illustrated in Figure 4 with only three layers, uses the Graph Attention Network (GAT) con- volution layers [18] with JK connections [14]. We predict both delay and slew for clock sink nodes so we have a node- level, continuous regression problem. Furthermore, since we do not want to need values for all the nodes, we use a mask to compute the loss only from sink nodes. 0 25 50 75 100 125 150 175 200 Clock Slew (ps) 20 40 60 80 100 120 140 160 Hold Time (ps) Hold Constraints vs. Clock Slew (Fixed data slew 44.93 ps) Hold (data fall) Hold (data rise) 0 25 50 75 100 125 150 175 200 Clock Slew (ps) 20 30 40 50 60 70 80 Setup Time (ps) Setup Constraints vs. Clock Slew (Fixed data slew 44.93 ps) Setup (data fall) Setup (data rise) Fig. 3. Clock signal input slew has a significant impact on setup and hold times and needs to be considered as a primary objective of clock synthesis. The mesh graphs along with node features from Table I are input to GAT Convolutional Layers (GATConv). At each layer k, new node features hk 1 are computed by aggregating the information from the neighbors according to the attention mechanism. Multiple heads are simultaneously computed with learnable attention weights which allows us to give importance to the different neighbors when aggregating their features. For example with four heads, GAT assigns four different weight vectors to the neighbor features when aggregating and concatenates the weighted features together. Intuitively, this may correspond to multiple combinations of driving wires or buffers from different directions on the mesh.\n\n--- Segment 10 ---\nFor example with four heads, GAT assigns four different weight vectors to the neighbor features when aggregating and concatenates the weighted features together. Intuitively, this may correspond to multiple combinations of driving wires or buffers from different directions on the mesh. To compute the final node features hfinal we utilize the JK connections which are shown in Figure 4 with the red dashed line. Generally, the features from each layer hk, hk 1, ...hl where l are the number of layers are passed to the final layer where they are either concatenated, max-pooled or passed to a Bi-directional LSTM. In our case, we use the max-pooling where it learns to select the most informative layer for each feature coordinate. Concatenation is the most straightforward but it only optimizes the weights to combine the features in a way that works best for the overall dataset which can cause underfitting when the graphs are too complex. On the other end, LSTM is more complex and can cause overfitting. Of these aggregation schemes, JK connections with max pooling shows better performance overall in deep GNNs [14] like ours. The JK connections serve a different purpose compared to the auxiliary connections we added to our graphs. They provide learnable combination of features in each neighbor- hood of the RC mesh whereas the aux connections model two physical aspects of a clock mesh: multiple buffer contention and multiple sink drivers which are statically chosen. IV. METHODOLOGY A. Training Data Generation We analyzed some of the open-source benchmark designs with OpenROAD [19] and implemented them using the default flow in the Nangate45 technology. We used the design statistics of area, number of sinks, and sink density to generate various synthetic designs for clock synthesis as shown in Table II. Clock meshes were generated for these synthetic designs using both uniform buffering [7] and non-uniform buffering [10].\n\n--- Segment 11 ---\nWe used the design statistics of area, number of sinks, and sink density to generate various synthetic designs for clock synthesis as shown in Table II. Clock meshes were generated for these synthetic designs using both uniform buffering [7] and non-uniform buffering [10]. The uniform and non-uniform buffered meshes use the same Mesh Graphs Graph features GATConv GATConv JK Linear Delay Slew -24.5 44.6 12.12 0.15 44.2 44.8 123.45 123.96 124.23 124.19 44.9 45.2 Masking 123.45 44.2 44.8 123.96 124.23 44.9 45.2 124.19 MSE Loss Back propagation Final predictions h1 h2 hfinal Fig. 4. Model architecture overview. As an example three GATConv layers are shown with Jumping Knowledge connections (shown in red). Masking is performed to only calculate the loss for predictions on sink nodes. Meshworks mesh sizing with minimum total wire length while the fixed mesh used a fixed 50µm mesh with non-uniform buffering. We also observed large designs in OpenROAD having clustered sinks with whitespace blockages for macros and memories. To simulate that we also create clusters of sinks for larger synthetic designs to reflect macro blockages. Top-level trees were created to drive the clock buffers using the well known DME [20], [21] combined with balanced buffering [22]. The top-level trees are zero-skew according to the Elmore model but may have some skew due to mismatch between SPICE and Elmore delay models. The arrival input delay and slew of the tree leaves are used as features of the mesh buffers as shown in Table I. TABLE II STATISTICS OF THE TRAINING DATASET Number of Mesh Types Area Density Uniform Non-Uni. Fixed (µm2) (sinks µm2) Small 100 100 100 [500, 1500] 0.033 Medium 100 100 100 [40000, 60000] [0.01, 0.04] Large 100 100 100 [180000, 220000] 0.021 B.\n\n--- Segment 12 ---\nTABLE II STATISTICS OF THE TRAINING DATASET Number of Mesh Types Area Density Uniform Non-Uni. Fixed (µm2) (sinks µm2) Small 100 100 100 [500, 1500] 0.033 Medium 100 100 100 [40000, 60000] [0.01, 0.04] Large 100 100 100 [180000, 220000] 0.021 B. Training Procedure Our model makes delay and slew predictions for all the nodes but we compute the loss only for the sink nodes using a mask with a Mean-Squared Error (MSE) loss function together with the target labels which are computed by running SPICE simulation on the clock meshes. The gradients are calculated and back-propagated to the JK layer as well as to the preceding GATConv layers. This allows the JK layer to learn the importance of each feature for different granularity of neighborhood and relate the features to the target delay and slew. Simultaneously, the GATConv layers learn to assign appropriate attention weights to the neighbors at the current level for the delay and slew predictions. We used ADAM [23] as an optimizer with a learning rate of 7.5e 4 and weight decay of 5e 4 for L2-regularization. For non-linear activations we used ELU [24]. We used batch size of 1 and train with a default 1000 epochs with random shuffling of training data each epoch. We also implemented early-stopping in our training loop with a patience of 20. With an ample amount of synthetic training data, L2-regularization, and early-stopping, we did not find the need to add dropout to the layers since the model does not overfit. C. Model Parameters The model hyperparameters we used are shown in Table III. Through empirical evaluations, we found better performance by having a deep model architecture and therefore used eight convolution layers. This is due to the depth of the mesh graph model a sink that is driven by a closest buffer will be at minimum four hops away as shown in Figure 2. However, there can be many other sinks nearby and each of them would create tap point nodes on the mesh wire thereby increasing the depth of the graph.\n\n--- Segment 13 ---\nThis is due to the depth of the mesh graph model a sink that is driven by a closest buffer will be at minimum four hops away as shown in Figure 2. However, there can be many other sinks nearby and each of them would create tap point nodes on the mesh wire thereby increasing the depth of the graph. We tried other convolution layer types besides GATConv, but found that learning the weights to combine neighbors is very important, likely because the importance is related to the resistance features of neighbor wires. The auxiliary connections add the insight of nearby driving buffers and improve message passing; however, these only model the minimum resistance paths and not the full mesh, so the local connections through RC mesh nodes also generate information on how the sinks interact within their local interconnect. We observed that four attention heads performed well, likely due to the four orthogonal directions at each mesh point. TABLE III MODEL HYPERPARAMETERS Parameter Value Convolution layer GATConv Number of convolution layers 8 Number of convolution channels 64 Number of attention heads 4 JK Aggregation max D. Experimental Setup We implement GATMesh in Pytorch [25] with the Pytorch Geometric library [26]. The Ngspice and first-order analysis as well as the benchmark generation was run on a dual AMD EPYC 7542 32-Core Processor machine with 512Gb of memory. The training and testing of GATMesh was done on an NVIDIA GeForce RTX 4090 24GiB card. V. RESULTS We compare our GATMesh model with both SPICE which is the golden model and a well-known first-order approx- imation [1] that is used in most of the prior clock mesh synthesis research papers. While RC network analysis may provide more detail than first-order delay estimates, it still falls far short of SPICE-level accuracy because it typically neglects key effects in clock meshes. These methods often assume linear superposition of buffers driving the mesh and ignore the nonlinear behavior of buffers, particularly when multiple drivers with differing input arrival times and output contention are present. These methods also oversimplify the effects of the redundant mesh paths, where signals from the different drivers interact. These interactions can potentially reinforce or cancel each other. As a result, such models can misestimate both delay and slew, making them only marginally more accurate than first-order approximations but still significantly less reliable than full SPICE simulations.\n\n--- Segment 14 ---\nThese interactions can potentially reinforce or cancel each other. As a result, such models can misestimate both delay and slew, making them only marginally more accurate than first-order approximations but still significantly less reliable than full SPICE simulations. We also attempted to use some higher order models, but these always failed to converge. The first-order models, on the other hand, would usually converge. Our work focuses on comparing with SPICE and a first-order model as a representative of the other heuristic models. A. Training and Model Accuracy We train on a dataset of 900 synthetic designs shown in Table II. The training development test set split is 80 10 10, so in total we have 720 samples for training, 90 for validation and 90 for testing. The training takes 200 epochs to converge in under 2 hours. We train and validate using Mean Squared Error (MSE) loss on delay and slew of the sink nodes. The squared term helps the model to penalize the mispredictions especially when the predictions are far from the true labels and is convex. The training converges with an MSE loss of 23.87ps, validation set MSE loss of 31.83ps, and test set MSE loss of 33.02ps. The validation loss tracks the training loss meaning that the model is able to learn well. The testing loss also matches which means we are not over-fitting. The model is trained once on the synthetic data and used for inductive inference on a test set of open-source designs that it has not seen in the next section. B. Accuracy Evaluation In order to evaluate our model s ability to inductively infer results on unseen realistic designs, we evaluate on a subset of open-source designs created with the OpenROAD flow. In contrast to the MSE for training loss, the delay and slew Mean Absolute Error (MAE) from the GATMesh model predictions and first-order model [1] are shown in Table IV. We use SPICE simulation results as the reference to measure the error. Against SPICE, our model performs competitively with the worst delay MAE being only 16.16ps and, on average, only 5.27ps. It also does quite well in predicting slews, with the worst slew MAE being 13.5ps and, on average, 5.98ps.\n\n--- Segment 15 ---\nAgainst SPICE, our model performs competitively with the worst delay MAE being only 16.16ps and, on average, only 5.27ps. It also does quite well in predicting slews, with the worst slew MAE being 13.5ps and, on average, 5.98ps. The worst case design for both delay and slew is swerv wrapper which has significantly larger area compared to our training statistics. We could likely improve these results with a more representative training set. On the other hand, our model significantly outperforms the first-order model which has a worst mean delay error of 278.45ps and, on average, 200.19ps. The first-order model has no slew to compare and using an Elmore model approximation such as 2.2 delay would be quite unfair and a poor comparison given that the delays are already quite inaccurate. C. Run-Time Evaluation We also present a runtime analysis with our GATMesh model inference, the first-order model algorithm, and Ngspice simulation [27] in Table V. Ngspice, version 42, is run with the default two threads, but using more threads did not see any significant performance gain on these benchmarks. This is because the model evaluation of the mesh buffers can be easily parallelized, but the solution of the RC mesh equations cannot be parallelized very well at each simulation time step. The first-order model is faster than Ngspice but as the design sizes get bigger the runtime starts to increase even beyond that of our model. The prior work [1] showed more than 24 hour run-times for the first-order algorithm on the largest designs. Overall, our model has a nearly constant runtime even for larger designs due to inference being faster with parallelization on GPUs. It is 1718 faster on average than first-order model and on average 47146 faster than Ngspice. D. Ablation Study We investigated the effectiveness of the different modeling decisions and how they impact the capability of the model to learn mesh timing. Specifically, we examine the static auxiliary connections that improve message passing from drivers to sinks and between buffers. We compare whether this static architecture is preferable to JK connections which learns to group features in different neighborhoods and between different levels of a multi-layer GNN. To do this, we did an ablation study of four different models.\n\n--- Segment 16 ---\nWe compare whether this static architecture is preferable to JK connections which learns to group features in different neighborhoods and between different levels of a multi-layer GNN. To do this, we did an ablation study of four different models. The first base case removes both the JK connections and the auxiliary connections in the graph but relies on the RC mesh and associated features. The features include derived ones such as regional capacitance, minimum resistance from nearby buffer, and total resistance from nearby buffers which do give some regional perspective of the RC mesh. These features, however, do not enable improvements in message passing like the auxiliary connections or learning over multiple layers like JK connections. They do, however, utilize multi-head attention for computing features from layer to layer.\n\n--- Segment 17 ---\nThese features, however, do not enable improvements in message passing like the auxiliary connections or learning over multiple layers like JK connections. They do, however, utilize multi-head attention for computing features from layer to layer. TABLE IV TIMING ANALYSIS ON OPEN-SOURCE DESIGNS COMPARED TO NGSPICE SIMULATION Open-Source Stats Delay error (ps) Slew error (ps) Designs Area (µm2) Sinks Mesh Size Buffers GAT-Mesh [1] GAT-Mesh [1] gcd 1082.88 35 2x2 4 1.76 79.63 0.31 N A aes 52785.04 562 6x6 31 2.65 157.34 2.69 N A ibex 57585.27 1932 15x15 168 2.91 203.39 7.48 N A dynamic node 54950.01 2257 15x15 180 3.58 199.78 3.18 N A tiny rocket 193770.36 4035 18x18 268 3.67 278.45 7.74 N A jpeg 198612.62 4383 19x19 287 3.46 266.13 5.10 N A swerv 387381.12 11180 34x34 799 7.99 197.13 7.87 N A swerv wrapper 1056663.92 11214 35x35 538 16.16 219.71 13.50 N A Average Error 5.27 200.19 5.98 N A TABLE V RUNTIME ANALYSIS OF GATMESH, FIRST-ORDER MODEL [1], AND NGSPICE Open-Source Runtime (s) Designs GATMesh [1] Ngspice gcd 0.0038 0.0003 0.2094 aes 0.0039 0.0527 2.5333 ibex 0.0041 0.0637 14.4586 dynamic node 0.0041 0.6546 16.8264 tiny rocket 0.0044 2.4565 41.7799 jpeg 0.0046 3.5786 49.8608 swerv 0.0088 29.5014 851.3769 swerv wrapper 0.0087 28.3169 795.6460 Average Runtime 0.0047 8.0780 221.5864 Average Speedup 1 1718 47146 The other models still utilize the same RC mesh and features, except that the second model adds the auxiliary connections but no JK connections, and the third model adds only JK connections but no auxiliary connections.\n\n--- Segment 18 ---\nThey do, however, utilize multi-head attention for computing features from layer to layer. TABLE IV TIMING ANALYSIS ON OPEN-SOURCE DESIGNS COMPARED TO NGSPICE SIMULATION Open-Source Stats Delay error (ps) Slew error (ps) Designs Area (µm2) Sinks Mesh Size Buffers GAT-Mesh [1] GAT-Mesh [1] gcd 1082.88 35 2x2 4 1.76 79.63 0.31 N A aes 52785.04 562 6x6 31 2.65 157.34 2.69 N A ibex 57585.27 1932 15x15 168 2.91 203.39 7.48 N A dynamic node 54950.01 2257 15x15 180 3.58 199.78 3.18 N A tiny rocket 193770.36 4035 18x18 268 3.67 278.45 7.74 N A jpeg 198612.62 4383 19x19 287 3.46 266.13 5.10 N A swerv 387381.12 11180 34x34 799 7.99 197.13 7.87 N A swerv wrapper 1056663.92 11214 35x35 538 16.16 219.71 13.50 N A Average Error 5.27 200.19 5.98 N A TABLE V RUNTIME ANALYSIS OF GATMESH, FIRST-ORDER MODEL [1], AND NGSPICE Open-Source Runtime (s) Designs GATMesh [1] Ngspice gcd 0.0038 0.0003 0.2094 aes 0.0039 0.0527 2.5333 ibex 0.0041 0.0637 14.4586 dynamic node 0.0041 0.6546 16.8264 tiny rocket 0.0044 2.4565 41.7799 jpeg 0.0046 3.5786 49.8608 swerv 0.0088 29.5014 851.3769 swerv wrapper 0.0087 28.3169 795.6460 Average Runtime 0.0047 8.0780 221.5864 Average Speedup 1 1718 47146 The other models still utilize the same RC mesh and features, except that the second model adds the auxiliary connections but no JK connections, and the third model adds only JK connections but no auxiliary connections. The last is our GATMesh model with both auxiliary connections and JK connections.\n\n--- Segment 19 ---\nTABLE IV TIMING ANALYSIS ON OPEN-SOURCE DESIGNS COMPARED TO NGSPICE SIMULATION Open-Source Stats Delay error (ps) Slew error (ps) Designs Area (µm2) Sinks Mesh Size Buffers GAT-Mesh [1] GAT-Mesh [1] gcd 1082.88 35 2x2 4 1.76 79.63 0.31 N A aes 52785.04 562 6x6 31 2.65 157.34 2.69 N A ibex 57585.27 1932 15x15 168 2.91 203.39 7.48 N A dynamic node 54950.01 2257 15x15 180 3.58 199.78 3.18 N A tiny rocket 193770.36 4035 18x18 268 3.67 278.45 7.74 N A jpeg 198612.62 4383 19x19 287 3.46 266.13 5.10 N A swerv 387381.12 11180 34x34 799 7.99 197.13 7.87 N A swerv wrapper 1056663.92 11214 35x35 538 16.16 219.71 13.50 N A Average Error 5.27 200.19 5.98 N A TABLE V RUNTIME ANALYSIS OF GATMESH, FIRST-ORDER MODEL [1], AND NGSPICE Open-Source Runtime (s) Designs GATMesh [1] Ngspice gcd 0.0038 0.0003 0.2094 aes 0.0039 0.0527 2.5333 ibex 0.0041 0.0637 14.4586 dynamic node 0.0041 0.6546 16.8264 tiny rocket 0.0044 2.4565 41.7799 jpeg 0.0046 3.5786 49.8608 swerv 0.0088 29.5014 851.3769 swerv wrapper 0.0087 28.3169 795.6460 Average Runtime 0.0047 8.0780 221.5864 Average Speedup 1 1718 47146 The other models still utilize the same RC mesh and features, except that the second model adds the auxiliary connections but no JK connections, and the third model adds only JK connections but no auxiliary connections. The last is our GATMesh model with both auxiliary connections and JK connections. All the other parameters are the same as described in Table III.\n\n--- Segment 20 ---\nThe last is our GATMesh model with both auxiliary connections and JK connections. All the other parameters are the same as described in Table III. We trained and tested each of these models on the identical synthetic dataset for comparison and present the results in Fig- ure 5. For both the delay and slew prediction, our GATMesh model produces substantial improvements over the baseline model without the static aux or learnable JK connections. The auxiliary connections alone produce significant gains on their own with the delay being slightly better than GATMesh but the slew being slightly worse. The JK connections alone achieve only moderate delay improvement, and almost no impact on slew error. VI. CONCLUSION We present GATMesh, a novel framework for accurate and efficient clock mesh timing analysis using Graph Neural Net- works. By modeling the mesh as a graph of resistive-capacitive elements, including some derived features, and incorporating both learned (Jumping Knowledge connections) and domain- driven (auxiliary connections) enhancements, GATMesh effec- tively captures the complex interactions of multiple drivers, reconvergent paths, and spatial design variations. Our method models delay as well as slew of the signal and achieves Baseline Baseline Aux Baseline JK GATMesh 0 1 2 3 4 5 6 7 8 Delay Slew MAE Error (ps) Delay MAE Error (ps) Slew MAE Error (ps) Fig. 5. The addition of the auxiliary connections offers the most benefit for the delay MAE, however a combination of the auxiliary and JK connections proves better for slew. predictions within a few picoseconds of SPICE accuracy while delivering massive speedups. Through extensive experiments and ablation studies, we show that both static and learnable ar- chitectural augmentations help prediction accuracy. GATMesh offers a practical and scalable path toward integrating clock meshes in real-world ASIC design flows by enabling faster and more accurate clock mesh analysis. REFERENCES [1] M. P. Desai, R. Cvijetic, and J. Jensen, Sizing of clock distribution networks for high performance CPU chips, in Design Automation Conference (DAC), pp. 389 394, 1996.\n\n--- Segment 21 ---\nREFERENCES [1] M. P. Desai, R. Cvijetic, and J. Jensen, Sizing of clock distribution networks for high performance CPU chips, in Design Automation Conference (DAC), pp. 389 394, 1996. [2] P. Restle, C. Carter, J. Eckhardt, B. Krauter, B. McCredie, K. Jenkins, A. Weger, and A. Mule, The clock distribution of the POWER4 mi- croprocessor, in International Solid-State Circuits Conference (ISSCC), pp. 144 145, 2002. [3] M. G.-R. Thomson, P. J. Restle, and N. K. James, A 5GHz duty-cycle correcting clock distribution network for the POWER6 microprocessor, in International Solid-State Circuits Conference (ISSCC), pp. 1522 1529, 2006. [4] V. Sathe, S. Arekapudi, C. Ouyang, M. Papaefthymiou, A. Ishii, and S. Naffziger, Resonant clock design for a power-efficient high-volume x86 64 microprocessor, in 2012 IEEE International Solid-State Circuits Conference, pp. 68 70, 2012. [5] H. McIntyre, S. Arekapudi, E. Busta, T. Fischer, M. Golden, A. Horiuchi, T. Meneghini, S. Naffziger, and J. Vinh, Design of the two-core x86-64 amd bulldozer module in 32 nm soi cmos, IEEE Journal of Solid- State Circuits, vol. 47, no. 1, pp. 164 176, 2012. [6] M. Guthaus, X. Hu, G. Wilke, G. Flache, and R. Reis, High- performance clock mesh optimization, ACM Transactions on Design Automation of Electronic Systems (TODAES), 2012. [7] A. Rajaram and D. Z. Pan, Meshworks: an efficient framework for planning, synthesis and optimization of clock mesh networks, in Asia and South Pacific Design Automation Conference (ASP-DAC), pp. 250 257, 2008.\n\n--- Segment 22 ---\n[7] A. Rajaram and D. Z. Pan, Meshworks: an efficient framework for planning, synthesis and optimization of clock mesh networks, in Asia and South Pacific Design Automation Conference (ASP-DAC), pp. 250 257, 2008. [8] Y. Teng and B. Taskin, Clock mesh synthesis method using the earth mover s distance under transformations, in 2012 IEEE 30th International Conference on Computer Design (ICCD), pp. 121 126, 2012. [9] G. Wilke, R. Fonseca, C. Mezzomo, and R. Reis, A novel scheme to reduce short-circuit power in mesh-based clock architectures, in Symposium on Integrated Circuits and System Design (SBCCI), pp. 117 122, 2008. [10] M. R. Guthaus, G. Wilke, and R. Reis, Non-uniform clock mesh optimization with linear programming buffer insertion, in Design Au- tomation Conference (DAC), 2010. [11] A. Abdelhadi, R. Ginosar, A. Kolodny, and E. G. Friedman, Timing- driven variation-aware nonuniform clock mesh synthesis, in Great lakes Symposium on VLSI (GLSVLSI), pp. 15 20, 2010. [12] T. K. Rusch, M. M. Bronstein, and S. Mishra, A survey on oversmooth- ing in graph neural networks, arXiv:2303.10993, 2023. [13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS 17, (Red Hook, NY, USA), p. 6000 6010, Curran Associates Inc., 2017. [14] K. Xu, C. Li, Y. Tian, T. Sonobe, K. ichi Kawarabayashi, and S. Jegelka, Representation learning on graphs with jumping knowledge networks, arXiv:1806.03536, 2018.\n\n--- Segment 23 ---\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS 17, (Red Hook, NY, USA), p. 6000 6010, Curran Associates Inc., 2017. [14] K. Xu, C. Li, Y. Tian, T. Sonobe, K. ichi Kawarabayashi, and S. Jegelka, Representation learning on graphs with jumping knowledge networks, arXiv:1806.03536, 2018. [15] R. Liang, S. Nath, A. Rajaram, J. Hu, and H. Ren, Bufformer: A generative ml framework for scalable buffering, in Proceedings of the 28th Asia and South Pacific Design Automation Conference, ASPDAC 23, (New York, NY, USA), p. 264 270, Association for Computing Machinery, 2023. [16] Y. Ye, T. Chen, Y. Gao, H. Yan, B. Yu, and L. Shi, Fast and accurate wire timing estimation based on graph learning, Design, Automation Test in Europe Conference (DATE), 2023. [17] H.-H. Cheng, I. H.-R. Jiang, and O. Ou, Fast and accurate wire timing estimation on tree and non-tree net structures, Design Automation Conference (DAC), 2020. [18] P. Veliˇckovi c, G. Cucurull, A. Casanova, A. Romero, P. Li o, and Y. Bengio, Graph Attention Networks, arXiv:1710.10903, 2018. [19] A. B. Kahng and T. Spyrou, The OpenROAD project: Unleashing hardware innovation, in Proc. GOMAC, 2021. [20] K. Boese and A. Kahng, Zero-skew clock routing trees with minimum wirelength, in ASIC Conf., pp. 1.1.1 1.1.5, 1992.\n\n--- Segment 24 ---\n[20] K. Boese and A. Kahng, Zero-skew clock routing trees with minimum wirelength, in ASIC Conf., pp. 1.1.1 1.1.5, 1992. [21] T.-H. Chao, Y.-C. Hsu, and J. Ho, Zero skew clock net routing, in Design Automation Conference (DAC), pp. 518 523, 1992. [22] J.-L. Tsai, T.-H. Chen, and C. C. Chen, Zero skew clock-tree optimiza- tion with buffer insertion sizing and wire sizing, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 23, no. 4, pp. 565 573, 2004. [23] D. P. Kingma and J. Ba, Adam: A method for stochastic optimization, 2017. [24] D.-A. Clevert, T. Unterthiner, and S. Hochreiter, Fast and accurate deep network learning by exponential linear units (elus), 2016. [25] J. Ansel et al., PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation, in 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 24), ACM, Apr. 2024. [26] M. Fey and J. E. Lenssen, Fast graph representation learning with PyTorch Geometric, in ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. [27] The NGSpice Project, ngspice open source circuit simulator, version 42. 2024. Accessed: 2025-04-12.\n\n