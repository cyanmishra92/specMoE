=== ORIGINAL PDF: 2507.07683v1_Accelerating_Transposed_Convolutions_on_FPGA-based.pdf ===\n\nRaw text length: 44901 characters\nCleaned text length: 44457 characters\nNumber of segments: 27\n\n=== CLEANED TEXT ===\n\nAccelerating Transposed Convolutions on FPGA-based Edge Devices Jude Haris, Jos e Cano School of Computing Science, University of Glasgow, Scotland, UK Abstract Transposed Convolutions (TCONV) enable the up- scaling mechanism within generative Artificial Intelligence (AI) models. However, the predominant Input-Oriented Mapping (IOM) method for implementing TCONV has complex output mapping, overlapping sums, and ineffectual computations. These inefficiencies further exacerbate the performance bottleneck of TCONV and generative models on resource-constrained edge devices. To address this problem, in this paper we propose MM2IM, a hardware-software co-designed accelerator that com- bines Matrix Multiplication (MatMul) with col2IM to process TCONV layers on resource-constrained edge devices efficiently. Using the SECDA-TFLite design toolkit, we implement MM2IM and evaluate its performance across 261 TCONV problem config- urations, achieving an average speedup of 1.9 against a dual- thread ARM Neon optimized CPU baseline. We then evaluate the performance of MM2IM on a range of TCONV layers from well-known generative models achieving up to 4.2 speedup, and compare it against similar resource-constrained TCONV accelerators, outperforming them by at least 2 GOPs DSP. Finally, we evaluate MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3 speedup and 2.4 energy reduction against the CPU baseline. Index Terms DNN Accelerators, FPGAs, TCONV, Edge AI. I. INTRODUCTION Generative Artificial Intelligence (AI) is used in various applications, including image-super resolution [1], style trans- fer [2] and object detection [3]. Generative AI models such as Generative Adversarial Networks (GANs) and Fully Convolu- tional Neural Networks (FCNs) contain an upscaling mecha- nism to generate new data. For example, generator modules in GANs contain specialized layers to upscale input feature maps, where the Transposed Convolution (TCONV) layer is the core of this upscaling mechanism. As the use of these types of models increases throughout different applications, there is an ever-growing need to make them more efficient on resource- constrained edge devices. However, this requires exploiting across-stack optimizations [4] compared to executing them on devices with powerful CPUs GPUs. When compared to the traditional convolution operation, which has been extensively studied [5], the complex computing properties of the TCONV operation, such as the overlapping sum problem [6], make it challenging to design accelerators that efficiently process TCONV, especially on edge devices with limited computa- tional and memory capabilities. Previous work has focused on improving upon different TCONV implementation methods on specialized accelera- tors. For example, the Zero-Insertion [7] and Transform- ing Deconvolution to Convolution (TDC) [8] methods have Layers Dropped Rate 0.00 0.10 0.20 0.30 DCGAN_1 DCGAN_2 DCGAN_3 FCN Fig. 1: Percentage of cropped outputs for the various TCONV problems benchmarked in our evaluation. computational and transformation overheads, thus researchers have been focusing on the Input-Orientated-Mapping (IOM) method [9] for TCONV. IOM reduces the number of opera- tions required to perform TCONV without requiring additional padding or transformations to inputs or weights. However, efficient execution of IOM on resource- constrained edge devices requires a hardware-software co- designed approach to ensure optimized tiling and offloading of the TCONV operation to the accelerator, while tackling three key interlinked problems effectively: (i) storing in- termediate partial results; (ii) processing overlapping sums; (iii) handling cropped outputs. First, partial results generated during TCONV should remain in on-chip memory to reduce the latency of sending data back to main memory. This means that on a device with limited memory space, storage of results should be optimized so that it takes up minimal space. Second, the overlapping sum problem occurs when multiple spatially separate dot product operations produce partial results of a single output value; these values must be coalesced into a single output value. Since the spatial locality of the partial results corresponding to a single final output varies for each output and also between the TCONV problem dimensions, creating a specialized accelerator to handle the complex mapping of partial values to the output efficiently becomes challenging. Finally, the standard IOM approach us- ing Matrix Multiplication (MatMul) creates additional output data that needs to be cropped from the final results to maintain consistent dimensions across the model execution. Therefore, the IOM approach not only leads to ineffectual computation, as the additional values are dropped later, but also to transfor- mation overheads due to output cropping. Figure 1 highlights the number of outputs dropped during the output cropping process across various TCONV-based GAN layers, which is proportional to the number of wasted operations. To the best of our knowledge, existing accelerator solu- arXiv:2507.07683v1 [cs.AR] 10 Jul 2025 tions [6], [10] [12] for TCONV do not tackle these three prob- lems efficiently on resource-constrained edge devices. They especially neglect the issue of the ineffectual computations due to calculation of outputs that are cropped. To address this problem, in this paper we present MM2IM, a novel hardware-software co-designed accelerator for TCONV that merges MatMul with col2IM [13], a matrix transformation operation that rearranges data columns into blocks. Our approach utilizes algorithmic optimizations along with specialized hardware modules to enable the IOM method on resource-constrained edge devices by efficiently handling the previous three key challenges, i.e., the overlapping sum problem, ineffectual computations due to cropped outputs, and tiling TCONV computations. We develop our design using the SECDA-TFLite [14] toolkit and evaluate its performance on an edge platform that includes an FPGA across various TCONV problems, including end-to-end execution of GAN models. Furthermore, we compare the performance of MM2IM against similar TCONV accelerators for resource-constrained edge FPGAs, demonstrating superior throughput per DSP. The contributions of this paper can be summarized as follows: MM2IM: a new accelerator architecture that solves the three main TCONV problems and efficiently processes TCONV operations on resource-constrained edge devices using our custom IOM-based tiling strategy. Two specialized hardware modules: the TCONV map- ping engine that generates compute and output maps on- the-fly for any TCONV problem configuration, and the processing module that efficiently computes and stores the required outputs while skipping ineffectual computa- tions using the compute and output maps. Integration and evaluation of MM2IM: we integrate MM2IM within TFLite and run a range of experiments that compare MM2IM against our ARM Neon opti- mized CPU baseline and other FPGA-based resource- constrained accelerators for TCONV. We obtain an aver- age speedup of 1.9 across 261 TFLite TCONV problem configurations and, similarly, achieve up to 3 speedup and 2.4 energy reduction across two GAN models while improving GOPs DSP efficiency by at least 2 over other accelerators. II. BACKGROUND A. Transposed Convolution Transposed Convolution (TCONV) is the key operation used within generative AI models to enable upscaling of input data. The TCONV parameters are defined as: out(Oh, Ow, Oc) tconv(Ih, Iw, Ic, Ks, Oc, S) (1) where Ih, Iw, Ic are the input height, width and channels, respectively; with kernel size Ks, output channels Oc and stride S. The output dimensions, height Oh and width Ow, are defined as: Ohw S Ihw. When Ks S, executing the direct TCONV operation requires the coalescing of partial outputs into the same final output due to striding; this coalescing is known as the overlapping sum problem [9]. Since the mapping of partial outputs to final outputs is problem dependent, the complexity of the output mapping increases. Alternatively, there are three optimized methods for imple- menting TCONV: (i) Zero-Insertion; (ii) Transforming Decon- volution to Convolution (TDC); (iii) Input-Oriented Mapping (IOM). Zero-Insertion resolves the overlapping sum prob- lem by padding the input zeros, albeit with added compute, memory, and bandwidth overhead, approximately 75 [11]. The TDC method transforms TCONV operations into Con- volution operations by generating sub-filter kernels to avoid the overlapping problem, but this method requires additional hardware to process the sparse sub-filter efficiently [8]. For the IOM method, introduced within GNA [9], each activation is multiplied by the filters and then the overlapped partial results are summed to produce the final output. The IOM method reduces the number of operations required to perform TCONV, as it does not require additional padding or transformation of inputs or weights. The drawback of the IOM method is that it contains a significant amount (up to 28 for DCGAN [15]) of ineffectual computations due to cropped outputs. Additionally the IOM method also requires an efficient hardware architec- ture to overcome the overlapping sums problem. B. Input-Oriented Mapping using MatMul and col2IM Here we discuss the IOM method in more detail. We express the IOM method in terms of the following operations: out(Oh, Ow, Oc) col2im(mm(I, WT ), Oh, Ow, Oc) (2) where I(Ih, Iw, Ic) is the input data, W(Ks, Ks, Oc, Ic) is the filter data, mm is the matrix multiplication (MatMul) operation, and col2im (column to image) is the operation used to convert the output of the MatMul to the final out- put. Figure 2 highlights the TCONV operation using the IOM method implemented for an example TCONV problem tconv(2, 2, 2, 3, 2, 1). Translating the TCONV dimensions to MatMul dimensions, we define the dimensions as: M Ih Iw, N Ks2 Oc, and the depth dimension K Ic. Hence, the partial output matrix can be represented by dimensions M and N, i.e., the rows and columns of the MatMul operation; and the number of operations required by the IOM method is equivalent to Ih Iw I2 c Ks2 Oc or simply M N K. Once the partial outputs are calculated, the final output is determined through the col2IM operation, which accumulates the partial outputs into the final TCONV outputs. Note that the IOM method produces padded output feature maps, so the perimeter of the output feature maps are cropped, as shown by the gray squares in Figure 2. Each partial output requires a dot product of the input row and filter column. The partial output is then summed to produce the final output; hence, each gray square computed represents K ineffectual computations. These partial outputs must be stored within temporary output buffers until all the related partial outputs are calculated, since the standard IOM method with MM2IM does not consider output mapping during the matrix multiply operation. Fig. 2: Example of TCONV with MatMul col2IM. III. EFFICIENT TRANSPOSED CONVOLUTION A. Optimizing Input-Oriented Mapping To optimize TCONV using the IOM method we first elaborate on its main inefficiencies in detail and then discuss optimizations for better performance on resource-constrained edge devices. 1) IOM Inefficient Computation The baseline IOM method has two inefficiencies: ineffectual computations and the storage of partial outputs. The number of ineffectual computations, i.e., the dropped outputs Do per TCONV problem, can be statically determined using the col2IM algorithm [13]. Overall, for a given TCONV problem, the IOM efficiency can be determined by looking at the drop rate: Dr Do (M N). In the example in Figure 2, where Do 40 and M N 72, Dr 0.55, hence 55 of the computations is unnecessary. 2) IOM Inefficient Storage In terms of storing partial outputs, we can calculate the wasted buffer space Ws as the number of final outputs Fouts minus the number of partial outputs Pouts, assuming that we do not skip ineffectual computations; where Fouts Oc Oh Ow and Pouts M N. In an ideal scenario, we can completely skip storing partial outputs and simply accumulate them to the final output, thus improving buffer space efficiency by Pouts Fouts. In the case of the example in Figure 2, where Pouts 72 and Fouts 32, this would improve space efficiency by 2.25 . If we are also able to skip ineffectual computations, we improve the buffer space efficiency up to 9 for this example. 3) Solving IOM Inefficiencies To solve the inefficiencies of IOM, we first define the output mapping and the compute mapping. In Figure 2, each square in the MatMul Outputs represents a partial TCONV output, and the values inside these squares represent the output index of the final TCONV outputs (shown on the right); this output mapping is a function of S and the input dimensions (Ih, Iw). For example, all the 0 index partial outputs are summed and stored in the 0 index of the final output feature maps. Additionally, calculating the index map of the (light and dark) blue squares in the MatMul Outputs, we derive the compute mapping for the given TCONV problem, that is, the index map of partial outputs that are not dropped out via col2IM. Therefore, our first key insight is that by using the output mapping and the compute mapping, we can solve the IOM inefficiencies and enable an efficient accelerator architecture that can: (i) Skip ineffectual computations of the dropped partial outputs (gray squares); (ii) Remove the need for storing Algorithm 1: Tiled MM2IM Data: Initialize filter step, i end row 1 foreach c 0 to Oc by filter step do 2 SendWeightFilters(c, filter step) 3 starting 0 4 foreach h 0 to Oh do 5 rows to send i end row[h] 1 starting 6 if i end row[h] starting 1 then 7 SendInputRows(starting, rows to send) 8 ComputeOutRow(h, c, filter step) 9 StoreOutRow(h, c, filter step) 10 starting i end row[h] 1 partial sums in temporary memory and to be summed later; (iii) Map the outputs of the MatMul operation directly to the final output values. B. Acceleration Dataflow for Resource-Constrained Devices Data transfer between off-chip and on-chip memory can become a bottleneck, especially on resource-constrained edge devices. Hence, we co-designed Tiled MM2IM, a specialized tiling strategy for MM2IM that enables weight and output stationary dataflow minimizing data transfer redundancy, high- lighted in Algorithm 1. Tiled MM2IM loads filter step filters and produces the corresponding output channels within the outer loop. The filter step is determined by the number of processing modules within our MM2IM architecture (dis- cussed in Section IV). We dynamically load the input rows to calculate one output row per iteration within the inner loop; the i end row array that holds the number of input rows required to compute the current output row is pre-calculated. Therefore, our second key insight is that using the previous dataflow can increase decrease hardware parallelism depend- ing on the resource constraints by adjusting filter step. Additionally, with Tiled MM2IM, we preemptively calculate partial outputs for later output rows depending on the input rows being processed. C. Performance Model We built an analytical model for our MM2IM architecture to estimate performance and guide further design choices. Our performance model accounts for the problem size and the properties of our accelerator design to assess the overall performance. Additionally, we combine accelerator analysis with data movement analysis to estimate the end-to-end perfor- mance for a given TCONV layer. Next we provide an overview of our performance model. First, we calculate problem-specific metrics such as the number of MAC (multiply and accumulate) operations and the number of cropped MatMul outputs for the given TCONV problem. Then, we calculate the accelerator processing time, finding the processing time for each Processing Module (PM) and its components, the Compute Unit (CU) and the Ac- cumulation Unit (AU), which are all discussed in detail in Section IV-D: TP M TCU compute TCU load TCU store TAU (3) Fig. 3: MM2IM Accelerator Architecture. The accelerator is connected to main memory via AXI-Stream buses, which are used to receive instructions and send receive data. Then, we calculate the data transfer time required between main memory and the accelerator: TData (Wsize Isize Osize OMapsize) BW (4) Finally, we calculate the end-to-end latency for the TCONV operation, combining the two factors: Ttotal TP M TData. Therefore, our third key insight, through performance modeling, is that up to 35 of the end-to-end latency (Ttotal) for any given TCONV problem was due to transferring output mapping data between the main memory and the accelerator. Hence, we developed the MM2IM mapper, a hardware module that completely removes the need for output mapping data transfers, discussed in Section IV-E. IV. MM2IM ACCELERATOR ARCHITECTURE Figure 3 overviews MM2IM, our proposed stream-based and scalable accelerator architecture, which utilizes simple instructions to configure, load data and execute TCONV operations. These instructions enable MM2IM to dynamically tile TCONV operations using Algorithm 1. MM2IM exploits two dimensions of parallelism at the Processing Module (PM) level, splitting Oc (output channels) computations across the X number of PMs (used for filter steps in Algorithm 1) and unrolling Ic (input channels) within the compute units with an unrolling factor of UF. Note that for our instantiation, we have set X 8 and UF 16; these parameters could be scaled to meet performance demands and resource constraints. The following sections discuss the key components of MM2IM. A. Instruction Decoder The instruction decoder allows for the reconfiguration of the accelerator to enable the execution of various TCONV layer configurations. It decodes instructions and sends control signals to the Scheduler and the weight data loader. Table I TABLE I: Micro-ISA Opcode Set. Opcode Description 0x01 Configure TCONV (sets configuration registers) 0x02 Loads Bias and Filter (activates Weight Data Loader) 0x04 Load Input (activates Dynamic Input Loader) 0x08 Schedule TCONV (activates Scheduler) 0x10 Store Output (activates Output Crossbar) shows the micro-ISA opcode set for the accelerator and a brief description of each instruction. These instructions are generated and sent to the accelerator by the host-side driver code during the execution of a given TCONV layer. Note that some opcodes, for example 0x01 , are immediately followed by operand data which the accelerator expects once the in- struction is decoded; this enables dynamic reconfiguration of the accelerator or loading new sets of data into the accelerator buffers. B. Scheduler The Scheduler is the main control unit within the acceler- ator. Once activated, it orchestrates the execution of an entire TCONV layer. First it activates the MM2IM Mapper alongside the Dynamic Input Loader, and then it activates the array of Processing Modules (PMs) to execute the operations required by the TCONV layer. Additionally, the Scheduler continuously monitors the Instruction Decoder for new instructions to either load the next row of input data to the Row Buffer or to send back the output data to main memory by activating the Output Crossbar. The Scheduler has fine-grained control over the PMs, allowing them to be turned on or off as needed. C. Data I O There are two data loaders, the Weight Data Loader and the Dynamic Input Loader. The Weight Data Loader loads batches of filter and bias data from main memory (via AXI-Stream) to their respective buffers. Once a batch is loaded into the buffers, the Scheduler allocates the filter and the corresponding bias data across the PMs. The Dynamic Input Loader loads new rows of inputs data dynamically to store within the Row Buffer, which at the request of the Scheduler broadcasts the new row of input data to all the PMs via dedicated FIFOs. The Output Crossbar is an interface module which com- bines the output streams from each of the PMs, and at the request of the Scheduler it sends the output data back to main memory. Note that the Scheduler sends a store request when the Store Output instruction is received by the accelerator. D. Processing Module Array The Processing Module (PM) array is the accelerator s com- putational core. It consists of X PMs that can be individually configured and utilized by the Scheduler to ensure efficient processing of TCONV. Each PM contains an accumulation and a compute unit connected via a FIFO stream. Figure 4 provides a detailed view of the PM architecture with a fine- grained view of the PE array; note that the wide white arrows represent data movement between the rest of the accelerator and the Processing Module. Fig. 4: Processing Module architecture with a detailed view of the PE array. The wide white arrows represent data movement from and to the rest of the accelerator. For each TCONV layer, X filters are partitioned along the PMs. Once all the PMs load their respective filters, rows of input data are streamed to all the PMs. Additionally, the PMs receive the compute map (cmap) and output map (omap) from the MM2IM Mapper (described in Section III-A). Compute Unit: Due to the complex computing nature of TCONV, the Compute Units (CUs) contain additional logic to ensure that ineffectual dot product computations are skipped. This additional logic, the cmap check within the PE Array, takes the cmap, the input row, and the filter data, and computes the dot product of the selected input row and filter column. The partial results are then streamed into the accumulation unit for further processing. Additionally, CUs are scalable and the unrolling factor (UF) defines the number of MACs within the PE array per CU. The UF is used to tile the Ic dimension of the given TCONV layer. Hence, to execute dot-product from Ic, the PE array will take Ic UF number of cycles. Increasing UF will directly increase the number of MACs per cycle per CU while increasing the hardware resources required. Accumulation Unit: The partial sums calculated by the CUs are stored within the output buffers in the correct output indices; the Out Muxer ensures this by using the omap. Subsequent partial sums for the same output accumulate with existing results, avoiding the need for extra buffer space. Once the output is fully calculated for an entire output row, the post-processing unit (PPU) processes the row. The PPU is a specialized processing engine used to perform the post- processing steps required by a given DNN model and then send the output data to the Output Crossbar. E. MM2IM Mapper The MM2IM Mapper is a key component of our accelerator architecture, as shown in Algorithm 2. It generates the cmap and omap corresponding to the row of partial results (i.e., the output row of MatMul) and streams them to the PMs. Note that it takes the current rowid and the number of rows as parameters to ensure that cmap omap are generated only for the required rows. This allows the MM2IM Mapper to support partitioning of the data in a tiled manner, since the rowid can be initialized to the starting row of the output tile instead of the starting row of the output matrix. Overall, the MM2IM Mapper generates the compute and output mappings only once per row, and each map is broadcast to all PMs, thus saving hardware Algorithm 2: MM2IM Mapper 1 rowid load(rowid), rowwidth load(rowwidth) 2 foreach r in MMrows do 3 hpad paddingtop (S (rowid rowwidth)) 4 wpad paddingleft (S (rowid rowwidth)) 5 imdex hpad Ow wpad 6 col 0 7 foreach ih in Ks do 8 foreach iw in Ks do 9 if (ih hpad 0 ih hpad Oh 10 iw wpad 0 iw wpad Ow) then 11 PMs cmap.broadcast write(col) 12 PMs omap.broadcast write(imdex) 13 col , imdex 14 imdex Ow Ks 15 rowid resources and additional computational overhead. Note that the MM2IM Mapper is configured dynamically through the 0x01 opcode, and can support any shape of TCONV layers. F. Computational Flow Finally, we discuss in more detail the flow of computation within each PM and how each PM is able to calculate an output channel across multiple steps. Figure 5 provides an example of an input feature map being processed through a single filter to produce a complete output channel over N steps (N IH); this happens within each PM. The inputs on the left of each step represent the input row (presented as a column in Figure 5), which will be processed by all PMs during that step. Each input row contains Iw Ic data elements; this data is fetched from the Row Buffer after each step, as seen in Figure 3. Before starting the computations, each PM is preloaded with a single filter (shown in the middle); the filter contains Ks Ks Ic data elements, which are stored in the PM s local buffer. Note that the color of filter columns represents the intermediate outputs they produce within that step, and the weight data remains the same as the filter does not change between steps. During each step, the PM array performs a dot product between the input row and all the non-skipped weight columns. Some weight columns are skipped to ensure no ineffectual computation takes place; the indexes for the skipped weight columns are stored with the CMAP . The intermediate outputs of dot product operations for each step are shown in Figure 5. For Step 1 , intermediate outputs A1, A2 and A3 are com- puted. Once computed, the intermediate outputs are sent to the Out Muxer to map and accumulate within the correct output index of the local out buf. We see that, A1 and A3 are mapped to output 1 whereas A2 is mapped to output 2. In Step 2 , B2 is accumulated with the previous results of output 1, B1 is accumulated within output 2, and B3 is stored in output 3. After N steps, the entire output feature map is calculated. One key feature of the MM2IM architecture is that the output data stored in out buf is sent back to main memory as soon as a whole row of outputs is completely accumulated, which enables us to reduce the size of out buf. Fig. 5: Computational flow within each PM in terms of MM2IM computations. V. EVALUATION A. Experimental setup To design, validate and evaluate our MM2IM accelerator, we utilized the SECDA methodology [16] for quick design and integration of its architecture. Since our focus is on edge- based inference, we used the PYNQ-Z1 FPGA board, which includes a dual-core ARM Cortex-A9 CPU and an edge FPGA. Additionally, we used TensorFlow Lite (TFLite) along with the SECDA-TFLite toolkit [14] for the accelerator integration. More specifically, we used SECDA-TFLite to develop a custom MM2IM delegate1 for TFLite; this custom delegate first selects all TCONV layers within the target TFLite model for offloading to our accelerator. During inference, the se- lected layers will be processed by our MM2IM delegate, which offloads all TCONV-related metadata and pointers to the MM2IM driver code. Then, the host driver orchestrates the tiling strategy for accelerating TCONV as described in Algorithm 1, offloading the relevant input and weight data as required by the accelerator to calculate the output feature maps for the layer. As the accelerator finishes calculating each output feature map, the accelerator sends back the output data to be stored in the TFLite allocated tensor in main memory. B. Synthetic benchmarks First, we evaluate the performance of our MM2IM accel- erator across varying sets of TCONV problems. Using the benchmarking suite available within SECDA-TFLite [14], we generate single-layer TCONV models and benchmark the per- formance of MM2IM across 261 TCONV permutations. We permuted the TCONV parameters with the following values that occur commonly in TCONV models: i) Oc [16, 32, 64]; ii) Ks [3, 5, 7]; iii) Ih [7, 9, 11]; iv) Ic [32, 64, 128, 256]; v) S [1, 2]. We discussed these parameters in Section II-A. Figure 6 shows the normalized speedup against dual-thread CPU 8-bit baseline (with NEON-vector instructions enabled) of the same problems on the PYNQ-Z1 board. We group similar problems for ease of visualization of the results. This set of experiments was designed to help us understand our accelerator s performance on a wide variety of TCONV problems, more specifically, to understand the dynamics be- tween TCONV dimensions and accelerator performance. The key takeaways from these experiments are the following: 1A TFLite delegate is a hardware-software backend for DNN operations. Fig. 6: MM2IM speedup normalised to CPU execution time across various TCONV problems. i) On average, MM2IM achieves a 1.9 speedup against the dual-thread CPU; ii) The larger the Ic dimension, the greater the speedup - detailed profiling revealed that since the Ic is not tiled and is processed together without off-chip memory access, the PE array utilization can remain higher; iii) Similarly, higher Ih, Iw, Ks dimensions also achieve greater speedup - this is due to higher data-reuse of the filter (Ks2) data within each PM when the Ih and Iw dimension is larger; iv) As expected, increasing the Oc dimension yields a relatively smaller role in performance uplift; this is because MM2IM is tiled in the Oc dimension, and hence the speedup due to higher Oc is capped by the number of PMs. v) Higher stride values result in lower speedup (on average 54 ), as expected, due to less cropped outputs. To highlight the impact of cropped outputs on the speedup, we generated Figure 7, which highlights the of cropped outputs ( drop rate ) for the various TCONV problems bench- marked within Figure 6. The drop rate is calculated as the ratio of cropped outputs to the total number of outputs. Looking at Figure 7, we can see that increasing Ks results in higher drop rates, while higher Ih and S result in lower drop rates. Comparing the drop rate to the speedup, we can conclude: i) Increased kernel size results in higher drop rates and greater speedup; ii) Increased stride results in lower drop rates and speedup, as expected; iii) Decreased drop rate with increased Ih does not hamper speedup. We theorize that this is due to the increased utilization of the processing modules, as more computation is required for the larger input height and width. TABLE II: Performance Evaluation on Generative Model Layers. Model OC KS IH IW IC OPs Latency (ms) CPU (ms) Speedup (vs CPU) GOPs GOPs W DCGAN 1 512 5 4 1024 420M 46.26 166.56 3.60 9.07 15.64 DCGAN 2 256 5 8 512 420M 33.97 141.05 4.15 12.35 15.03 DCGAN 3 128 5 16 256 420M 35.86 149.70 4.17 11.70 14.92 DCGAN 4 3 5 32 128 20M 4.67 10.71 2.29 4.21 0.87 FCN 21 4 1 21 14K 0.22 0.22 1.00 0.06 0.01 StyleTransfer 1 64 3 64 128 604M 164.62 304.48 1.85 3.67 23.22 StyleTransfer 2 32 3 128 64 604M 282.83 460.23 1.63 2.14 23.65 StyleTransfer 3 3 9 256 32 1020M 264.27 1045.36 3.96 3.86 40.49 FSRCNN 2 9 32 32 11M 5.21 12.47 2.39 2.04 0.51 Fig. 7: Percentage of cropped outputs for the various TCONV problems benchmarked in Figure 6. C. TCONV Model Layer Evaluation We performed an extensive evaluation across specific TCONV layers commonly found in popular generative mod- els [1], [2], [15], [17]. Table II contains the specific layer details, the performance of our accelerator compared to the PYNQ CPU s single-threaded execution, overall throughput, and energy efficiency. On average, we achieve a 2.8 speedup compared to the CPU implementation while achieving an average throughput of 5.5 GOPs, with an average power to performance ratio of 14.9 GOPs W. As realized in the synthetic benchmark, the MM2IM accelerator takes advantage of the larger Ic dimension seen within the DCGAN [15] layers, achieving up to a 4.2 speedup in some layers. D. TCONV Accelerators Comparison We now compare the performance improvement of MM2IM against other TCONV accelerators for resource-constrained edge FPGAs; Table III gives key details about the related works and summaries their performance. Note that we do not compare to ASIC designs, as our work focuses on FPGA-based accelerator architecture for TCONV and direct comparisons to ASIC-based designs would not be fair, as translating area to FPGA resources is non-trivial. In Table III we also present the best reported performance within each work. While MM2IM does not achieve the highest throughput in terms of GOPs, we are able to outperform all the related works in-terms of GOPs DSP performance, achieving a 2 increase compared to the next best work [8], which utilizes the TDC method, while reducing LUT usage by 4 . The GOPs DSP metric is more relevant than GOPs, as it takes into account the scale of the FPGA to ensure a fair comparison between accelerators. The accelerators proposed by Liu et al. [18] and Zhang et al. [6] serve as good points of comparison, as they target similar resource-constrained devices compared to our PYNQ- Z1. Comparing these works, we outperform Zhang et al. by 8.8 in terms of GOPs, while improving in DSP efficiency by 77 compared to Liu et al. [18]. E. End-to-end GAN model evaluation To demonstrate the capability of our accelerator to accel- erate end-to-end DNN execution, we evaluated MM2IM on two popular GAN models, DCGAN [15] and pix2pix [20] with end-to-end TFLite inference. As we use unmodified TFLite models, we omit accuracy as it is unchanged with our accelerator; additionally, to validate correctness, we ensured that the accelerator output matches the CPU baseline output. We accelerate the TCONV layers and the post-layer quan- tization using our MM2IM design. The rest of the layers are executed on the board s CPU. Table IV highlights the performance in terms of latency and power of MM2IM and CPU-only inference across the GAN models. We achieve a latency improvement of 2.4 and an energy reduction of 1.7 . Note that since these GAN models contain different types of layers, the potential end-to-end performance improvement with our accelerator is limited to the TCONV layers within the model. For TCONV-only layers within the models, we achieve an average latency speedup of 2.7 com- pared with the dual-threaded MM2IM execution. F. Performance Model Validation As mentioned in Section III-C, we used our performance model to estimate and guide the design choices of our MM2IM accelerator. To validate our performance model, we compare its expected performance to our accelerator s actual performance. On average, the model estimates the actual performance within 10 of our MM2IM accelerator. Applying the TCONV decoder optimization to our performance model predicts the expected performance improvement within 1 deviation of the actual performance improvement that the optimization provides. This demonstrates the utility of our performance model in guiding design choices through esti- mated performance improvements per proposed optimizations. Using our performance model helped us identify bottlenecks and solve the output mapping problems more efficiently using the MM2IM Mapper module. TABLE III: Comparison with state-of-the-art TCONV accelerators. Source [6] [18] [19] [8] Ours FPGA ZYNQ 7Z020 ZC706 XC7Z045 ZC706 XC7Z045 Kintex-7 XC7K410T PYNQ Z1 Frequency (MHz) 100 200 167 130 200 Precision 12-bit 16-bit 16-bit 13-bit 8-bit DSP Usage 209 640 603 1512 49 (22 ) LUTs Usage 25K 85K 196K 167K 42K (79 ) FFs Usage 30K 110K 158K 49K (46 ) BRAM Usage 48 67 57 24 99 BRAM Space 2.4MB 12.8MB 10.9MB 6.7MB 4.9MB Perf. (GOPS) 2.6 29 236.9 2691 23.0 Perf. (GOPS DSP) 0.01 0.05 0.39 1.78 3.51 TABLE IV: End-to-end model inference performance of MM2IM against CPU-only inference dual-threaded execution. Model Configuration TCONV (ms) Overall (ms) Energy (J pic) DCGAN2 CPU 1T 38 1.0x 49 1.0x 7.9 1.0x ACC CPU 1T 15 2.4x 21 2.3x 4.3 1.8x CPU 2T 24 1.6x 28 1.7x 6.5 1.2x ACC CPU 2T 16 2.4x 20 2.4x 4.3 1.8x pix2pix CPU 1T 2737 1.0x 5238 1.0x 9.8 1.0x ACC CPU 1T 922 3.0x 3360 1.6x 7.9 1.2x CPU 2T 1532 1.8x 2886 1.8x 5.9 1.7x ACC CPU 2T 926 3.0x 2266 2.3x 6.2 1.6x VI. RELATED WORK Various works with different accelerator architectures and optimizations have been proposed to accelerate TCONV op- erations, employing methods such as TDC [8], Winograd- Transformed Transposed Convolution [19], [21], and the Zero- Insert TCONV method [7]. However, these approaches face computational overheads due to algorithmic limitations inher- ent to their respective methods. Some works have proposed implementing the TCONV operation using the IOM method on FPGAs. For example, Ma et al. [10] exploit the intermediate-centric dataflow, a variation on the IOM method, but their accelerator only supports fixed dimensions for given problems. Similarly, the initial work by Sestito et al. [22] exploits High-Level Synthesis (HLS) to create Deconvolution Engines that can solve fixed dimension TCONV problems efficiently. These engines require additional buffers to store the intermediate results until the entire filter is processed, after which the intermediate results are summed together to produce the final output feature map. Additionally, this work does not handle the ineffectual computation preemp- tively, hence performing more MACs than required and also having to post-process the result to achieve the cropped results. The later HLS template-based approach proposed by Sestito et al. [12] suffers from the same constraints, and although they can adjust their accelerator to different problem sizes, this requires re-synthesis and re-mapping of the accelerator. In both works, the computation engine processes a tile of Ic dimension at a time, unlike our design, which processes the entire Ic dimension, keeping the dataflow output stationary, 2We use the Tensorflow-defined version of DCGAN: tensorflow.org tutorials generative dcgan allowing larger TCONV problems to be processed on smaller FPGAs without requiring slow off-chip memory access. Zhang et al. [6] proposed an output-oriented design that solves the overlapping sum problem for edge devices but introduces hardware complexity, degrading the accelerator s performance. Other works such as GNA [9] and FCN-Engine [11] exploit the IOM method with ASIC designs, but similar to all previ- ously mentioned IOM-based approaches they do not consider the cropped outputs. Finally, there are several works [10], [23] which take advantage of large-scale FPGAs to efficiently map TCONV problems fully within the FPGA fabric without requiring slow off-chip memory access. While effective at tackling TCONV on larger FPGAs, they do not consider the limitations of TCONV within resources-constrained FPGAs. VII. CONCLUSION We proposed MM2IM, a novel hardware architecture to effi- ciently accelerate TCONV operations on resource-constrained edge devices with FPGAs. Our efficient hardware-software co- designed solution solves three key challenges: i) the overlap- ping sum mapping problem; ii) ineffectual computations and cropped output mapping; and iii) the need for efficient dataflow strategies for resource-constrained edge devices. We imple- mented our proposed hardware design on an edge FPGA using the SECDA-TFLite toolkit, and evaluated the performance across a large variety of configurations for TCONV problems, achieving an average speedup of 1.9 against a dual-thread ARM CPU. We also compared MM2IM against other TCONV accelerators for similarly resource-constrained edge FPGAs and achieved at least 2 higher GOPs DSP compared to the next best accelerator. Finally, we performed an end-to-end evaluation of the DCGAN and pix2pix models, achieving a 2.4 speedup and 1.7 energy reduction on average compared with the CPU baseline. As future work, we plan to further scale down our accelerator design to enable TCONV in devices with lower resources budgets such as micro-controllers. ACKNOWLEDGMENTS This work was partially supported by the UK Engineering and Physical Sciences Research Council (grant EP R513222 1), the EU Project dAIEDGE (GA Nr 101120726) and the Innovate UK Horizon Europe Guarantee (GA Nr 10090788). REFERENCES [1] C. Dong, C. C. Loy, and X. Tang, Accelerating the Super-Resolution Convolutional Neural Network, in European Conference on Computer Vision (ECCV), 2016, pp. 391 407. [2] J. Johnson, A. Alahi, and L. Fei-Fei, Perceptual Losses for Real- Time Style Transfer and Super-Resolution, in European Conference on Computer Vision (ECCV), 2016, pp. 694 711. [3] L. Liu, M. Muelly, J. Deng, T. Pfister, and L.-J. Li, Generative Modeling for Small-Data Object Detection, in 2019 IEEE CVF International Conference on Computer Vision (ICCV), 2019, pp. 6072 6080. [4] P. Gibson, J. Cano, E. J. Crowley, A. Storkey, and M. O Boyle, DLAS: A Conceptual Model for Across-Stack Deep Learning Acceleration, in ACM Transactions on Architecture and Code Optimization, 2024. [5] S. Mittal, A survey of FPGA-based accelerators for convolutional neural networks, vol. 32, no. 4, pp. 1109 1139. [Online]. Available: [6] X. Zhang, S. Das, O. Neopane, and K. Kreutz-Delgado, A Design Methodology for Efficient Implementation of Deconvolutional Neural Networks on an FPGA, in arXiv:1705.02583, 2017. [7] Y. Yu, T. Zhao, M. Wang, K. Wang, and L. He, Uni-OPU: An FPGA- Based Uniform Accelerator for Convolutional and Transposed Convo- lutional Networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, pp. 1545 1556, 2020. [8] J.-W. Chang, K.-W. Kang, and S.-J. Kang, An Energy-Efficient FPGA- Based Deconvolutional Neural Networks Accelerator for Single Image Super-Resolution, IEEE Transactions on Circuits and Systems for Video Technology, pp. 281 295, 2020. [9] J. Yan, S. Yin, F. Tu, L. Liu, and S. Wei, GNA: Reconfigurable and Efficient Architecture for Generative Network Acceleration, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, pp. 2519 2529, 2018. [10] Z. Ma, T. Dai, X. Wei, and G. Luo, An Intermediate-Centric Dataflow for Transposed Convolution Acceleration on FPGA, ACM Transactions on Embedded Computing Systems, 2022. [11] D. Xu, K. Tu, Y. Wang, C. Liu, B. He, and H. Li, FCN-Engine: Accelerating Deconvolutional Layers in Classic CNN Processors, in 2018 IEEE ACM International Conference on Computer-Aided Design (ICCAD), 2018, pp. 1 6. [12] C. Sestito, S. Perri, and R. Stewart, FPGA Design of Transposed Convolutions for Deep Learning Using High-Level Synthesis, Journal of Signal Processing Systems, 2023. [Online]. Available: [13] M. Devs. Col2im - Rearrange matrix columns into blocks - MATLAB. [Online]. Available: html [14] J. Haris, P. Gibson, J. Cano, N. Bohm Agostini, and D. Kaeli, SECDA- TFLite: A toolkit for efficient development of FPGA-based DNN accelerators for edge inference, Journal of Parallel and Distributed Computing, pp. 140 151, 2023. [15] A. Radford, L. Metz, and S. Chintala, Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, in 4th International Conference on Learning Representations (ICLR), 2016. [16] J. Haris, P. Gibson, J. Cano, N. B. Agostini, and D. Kaeli, SECDA: Efficient Hardware Software Co-Design of FPGA-based DNN Accelera- tors for Edge Inference, in 2021 IEEE 33rd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD), 2021, pp. 33 43. [17] J. Long, E. Shelhamer, and T. Darrell. Fully Convolutional Networks for Semantic Segmentation. [Online]. Available: 1411.4038 [18] S. Liu, H. Fan, X. Niu, H.-c. Ng, Y. Chu, and W. Luk, Optimizing CNN-based Segmentation with Deeply Customized Convolutional and Deconvolutional Architectures on FPGA, ACM Transactions on Recon- figurable Technology and Systems, pp. 1 22, 2018. [19] X. Di, H.-G. Yang, Y. Jia, Z. Huang, and N. Mao, Exploring Effi- cient Acceleration Architecture for Winograd-Transformed Transposed Convolution of GANs on FPGAs, Electronics, p. 286, 2020. [20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, Image-to-Image Trans- lation with Conditional Adversarial Networks, in arXiv:1611.07004. [21] J.-W. Chang, S. Ahn, K.-W. Kang, and S.-J. Kang, Towards Design Methodology of Efficient Fast Algorithms for Accelerating Generative Adversarial Networks on FPGAs, in 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC), 2020, pp. 283 288. [22] C. Sestito, R. Stewart, and S. Perri, High-Level Synthesis of Hardware Accelerators for Deconvolution Engines, pp. 1 4. [23] T.-H. Wu, C. Shu, and T.-T. Liu, An Efficient FPGA-Based Dilated and Transposed Convolutional Neural Network Accelerator, vol. 71, no. 11, pp. 5178 5186.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nAccelerating Transposed Convolutions on FPGA-based Edge Devices Jude Haris, Jos e Cano School of Computing Science, University of Glasgow, Scotland, UK Abstract Transposed Convolutions (TCONV) enable the up- scaling mechanism within generative Artificial Intelligence (AI) models. However, the predominant Input-Oriented Mapping (IOM) method for implementing TCONV has complex output mapping, overlapping sums, and ineffectual computations. These inefficiencies further exacerbate the performance bottleneck of TCONV and generative models on resource-constrained edge devices. To address this problem, in this paper we propose MM2IM, a hardware-software co-designed accelerator that com- bines Matrix Multiplication (MatMul) with col2IM to process TCONV layers on resource-constrained edge devices efficiently. Using the SECDA-TFLite design toolkit, we implement MM2IM and evaluate its performance across 261 TCONV problem config- urations, achieving an average speedup of 1.9 against a dual- thread ARM Neon optimized CPU baseline. We then evaluate the performance of MM2IM on a range of TCONV layers from well-known generative models achieving up to 4.2 speedup, and compare it against similar resource-constrained TCONV accelerators, outperforming them by at least 2 GOPs DSP. Finally, we evaluate MM2IM on the DCGAN and pix2pix GAN models, achieving up to 3 speedup and 2.4 energy reduction against the CPU baseline. Index Terms DNN Accelerators, FPGAs, TCONV, Edge AI. I. INTRODUCTION Generative Artificial Intelligence (AI) is used in various applications, including image-super resolution [1], style trans- fer [2] and object detection [3]. Generative AI models such as Generative Adversarial Networks (GANs) and Fully Convolu- tional Neural Networks (FCNs) contain an upscaling mecha- nism to generate new data. For example, generator modules in GANs contain specialized layers to upscale input feature maps, where the Transposed Convolution (TCONV) layer is the core of this upscaling mechanism.\n\n--- Segment 2 ---\nGenerative AI models such as Generative Adversarial Networks (GANs) and Fully Convolu- tional Neural Networks (FCNs) contain an upscaling mecha- nism to generate new data. For example, generator modules in GANs contain specialized layers to upscale input feature maps, where the Transposed Convolution (TCONV) layer is the core of this upscaling mechanism. As the use of these types of models increases throughout different applications, there is an ever-growing need to make them more efficient on resource- constrained edge devices. However, this requires exploiting across-stack optimizations [4] compared to executing them on devices with powerful CPUs GPUs. When compared to the traditional convolution operation, which has been extensively studied [5], the complex computing properties of the TCONV operation, such as the overlapping sum problem [6], make it challenging to design accelerators that efficiently process TCONV, especially on edge devices with limited computa- tional and memory capabilities. Previous work has focused on improving upon different TCONV implementation methods on specialized accelera- tors. For example, the Zero-Insertion [7] and Transform- ing Deconvolution to Convolution (TDC) [8] methods have Layers Dropped Rate 0.00 0.10 0.20 0.30 DCGAN_1 DCGAN_2 DCGAN_3 FCN Fig. 1: Percentage of cropped outputs for the various TCONV problems benchmarked in our evaluation. computational and transformation overheads, thus researchers have been focusing on the Input-Orientated-Mapping (IOM) method [9] for TCONV. IOM reduces the number of opera- tions required to perform TCONV without requiring additional padding or transformations to inputs or weights. However, efficient execution of IOM on resource- constrained edge devices requires a hardware-software co- designed approach to ensure optimized tiling and offloading of the TCONV operation to the accelerator, while tackling three key interlinked problems effectively: (i) storing in- termediate partial results; (ii) processing overlapping sums; (iii) handling cropped outputs. First, partial results generated during TCONV should remain in on-chip memory to reduce the latency of sending data back to main memory. This means that on a device with limited memory space, storage of results should be optimized so that it takes up minimal space.\n\n--- Segment 3 ---\nFirst, partial results generated during TCONV should remain in on-chip memory to reduce the latency of sending data back to main memory. This means that on a device with limited memory space, storage of results should be optimized so that it takes up minimal space. Second, the overlapping sum problem occurs when multiple spatially separate dot product operations produce partial results of a single output value; these values must be coalesced into a single output value. Since the spatial locality of the partial results corresponding to a single final output varies for each output and also between the TCONV problem dimensions, creating a specialized accelerator to handle the complex mapping of partial values to the output efficiently becomes challenging. Finally, the standard IOM approach us- ing Matrix Multiplication (MatMul) creates additional output data that needs to be cropped from the final results to maintain consistent dimensions across the model execution. Therefore, the IOM approach not only leads to ineffectual computation, as the additional values are dropped later, but also to transfor- mation overheads due to output cropping. Figure 1 highlights the number of outputs dropped during the output cropping process across various TCONV-based GAN layers, which is proportional to the number of wasted operations. To the best of our knowledge, existing accelerator solu- arXiv:2507.07683v1 [cs.AR] 10 Jul 2025 tions [6], [10] [12] for TCONV do not tackle these three prob- lems efficiently on resource-constrained edge devices. They especially neglect the issue of the ineffectual computations due to calculation of outputs that are cropped. To address this problem, in this paper we present MM2IM, a novel hardware-software co-designed accelerator for TCONV that merges MatMul with col2IM [13], a matrix transformation operation that rearranges data columns into blocks. Our approach utilizes algorithmic optimizations along with specialized hardware modules to enable the IOM method on resource-constrained edge devices by efficiently handling the previous three key challenges, i.e., the overlapping sum problem, ineffectual computations due to cropped outputs, and tiling TCONV computations. We develop our design using the SECDA-TFLite [14] toolkit and evaluate its performance on an edge platform that includes an FPGA across various TCONV problems, including end-to-end execution of GAN models.\n\n--- Segment 4 ---\nOur approach utilizes algorithmic optimizations along with specialized hardware modules to enable the IOM method on resource-constrained edge devices by efficiently handling the previous three key challenges, i.e., the overlapping sum problem, ineffectual computations due to cropped outputs, and tiling TCONV computations. We develop our design using the SECDA-TFLite [14] toolkit and evaluate its performance on an edge platform that includes an FPGA across various TCONV problems, including end-to-end execution of GAN models. Furthermore, we compare the performance of MM2IM against similar TCONV accelerators for resource-constrained edge FPGAs, demonstrating superior throughput per DSP. The contributions of this paper can be summarized as follows: MM2IM: a new accelerator architecture that solves the three main TCONV problems and efficiently processes TCONV operations on resource-constrained edge devices using our custom IOM-based tiling strategy. Two specialized hardware modules: the TCONV map- ping engine that generates compute and output maps on- the-fly for any TCONV problem configuration, and the processing module that efficiently computes and stores the required outputs while skipping ineffectual computa- tions using the compute and output maps. Integration and evaluation of MM2IM: we integrate MM2IM within TFLite and run a range of experiments that compare MM2IM against our ARM Neon opti- mized CPU baseline and other FPGA-based resource- constrained accelerators for TCONV. We obtain an aver- age speedup of 1.9 across 261 TFLite TCONV problem configurations and, similarly, achieve up to 3 speedup and 2.4 energy reduction across two GAN models while improving GOPs DSP efficiency by at least 2 over other accelerators. II. BACKGROUND A. Transposed Convolution Transposed Convolution (TCONV) is the key operation used within generative AI models to enable upscaling of input data.\n\n--- Segment 5 ---\nII. BACKGROUND A. Transposed Convolution Transposed Convolution (TCONV) is the key operation used within generative AI models to enable upscaling of input data. The TCONV parameters are defined as: out(Oh, Ow, Oc) tconv(Ih, Iw, Ic, Ks, Oc, S) (1) where Ih, Iw, Ic are the input height, width and channels, respectively; with kernel size Ks, output channels Oc and stride S. The output dimensions, height Oh and width Ow, are defined as: Ohw S Ihw. When Ks S, executing the direct TCONV operation requires the coalescing of partial outputs into the same final output due to striding; this coalescing is known as the overlapping sum problem [9]. Since the mapping of partial outputs to final outputs is problem dependent, the complexity of the output mapping increases. Alternatively, there are three optimized methods for imple- menting TCONV: (i) Zero-Insertion; (ii) Transforming Decon- volution to Convolution (TDC); (iii) Input-Oriented Mapping (IOM). Zero-Insertion resolves the overlapping sum prob- lem by padding the input zeros, albeit with added compute, memory, and bandwidth overhead, approximately 75 [11]. The TDC method transforms TCONV operations into Con- volution operations by generating sub-filter kernels to avoid the overlapping problem, but this method requires additional hardware to process the sparse sub-filter efficiently [8]. For the IOM method, introduced within GNA [9], each activation is multiplied by the filters and then the overlapped partial results are summed to produce the final output. The IOM method reduces the number of operations required to perform TCONV, as it does not require additional padding or transformation of inputs or weights. The drawback of the IOM method is that it contains a significant amount (up to 28 for DCGAN [15]) of ineffectual computations due to cropped outputs. Additionally the IOM method also requires an efficient hardware architec- ture to overcome the overlapping sums problem. B. Input-Oriented Mapping using MatMul and col2IM Here we discuss the IOM method in more detail.\n\n--- Segment 6 ---\nAdditionally the IOM method also requires an efficient hardware architec- ture to overcome the overlapping sums problem. B. Input-Oriented Mapping using MatMul and col2IM Here we discuss the IOM method in more detail. We express the IOM method in terms of the following operations: out(Oh, Ow, Oc) col2im(mm(I, WT ), Oh, Ow, Oc) (2) where I(Ih, Iw, Ic) is the input data, W(Ks, Ks, Oc, Ic) is the filter data, mm is the matrix multiplication (MatMul) operation, and col2im (column to image) is the operation used to convert the output of the MatMul to the final out- put. Figure 2 highlights the TCONV operation using the IOM method implemented for an example TCONV problem tconv(2, 2, 2, 3, 2, 1). Translating the TCONV dimensions to MatMul dimensions, we define the dimensions as: M Ih Iw, N Ks2 Oc, and the depth dimension K Ic. Hence, the partial output matrix can be represented by dimensions M and N, i.e., the rows and columns of the MatMul operation; and the number of operations required by the IOM method is equivalent to Ih Iw I2 c Ks2 Oc or simply M N K. Once the partial outputs are calculated, the final output is determined through the col2IM operation, which accumulates the partial outputs into the final TCONV outputs. Note that the IOM method produces padded output feature maps, so the perimeter of the output feature maps are cropped, as shown by the gray squares in Figure 2. Each partial output requires a dot product of the input row and filter column. The partial output is then summed to produce the final output; hence, each gray square computed represents K ineffectual computations. These partial outputs must be stored within temporary output buffers until all the related partial outputs are calculated, since the standard IOM method with MM2IM does not consider output mapping during the matrix multiply operation. Fig. 2: Example of TCONV with MatMul col2IM. III.\n\n--- Segment 7 ---\n2: Example of TCONV with MatMul col2IM. III. EFFICIENT TRANSPOSED CONVOLUTION A. Optimizing Input-Oriented Mapping To optimize TCONV using the IOM method we first elaborate on its main inefficiencies in detail and then discuss optimizations for better performance on resource-constrained edge devices. 1) IOM Inefficient Computation The baseline IOM method has two inefficiencies: ineffectual computations and the storage of partial outputs. The number of ineffectual computations, i.e., the dropped outputs Do per TCONV problem, can be statically determined using the col2IM algorithm [13]. Overall, for a given TCONV problem, the IOM efficiency can be determined by looking at the drop rate: Dr Do (M N). In the example in Figure 2, where Do 40 and M N 72, Dr 0.55, hence 55 of the computations is unnecessary. 2) IOM Inefficient Storage In terms of storing partial outputs, we can calculate the wasted buffer space Ws as the number of final outputs Fouts minus the number of partial outputs Pouts, assuming that we do not skip ineffectual computations; where Fouts Oc Oh Ow and Pouts M N. In an ideal scenario, we can completely skip storing partial outputs and simply accumulate them to the final output, thus improving buffer space efficiency by Pouts Fouts. In the case of the example in Figure 2, where Pouts 72 and Fouts 32, this would improve space efficiency by 2.25 . If we are also able to skip ineffectual computations, we improve the buffer space efficiency up to 9 for this example. 3) Solving IOM Inefficiencies To solve the inefficiencies of IOM, we first define the output mapping and the compute mapping. In Figure 2, each square in the MatMul Outputs represents a partial TCONV output, and the values inside these squares represent the output index of the final TCONV outputs (shown on the right); this output mapping is a function of S and the input dimensions (Ih, Iw). For example, all the 0 index partial outputs are summed and stored in the 0 index of the final output feature maps.\n\n--- Segment 8 ---\nIn Figure 2, each square in the MatMul Outputs represents a partial TCONV output, and the values inside these squares represent the output index of the final TCONV outputs (shown on the right); this output mapping is a function of S and the input dimensions (Ih, Iw). For example, all the 0 index partial outputs are summed and stored in the 0 index of the final output feature maps. Additionally, calculating the index map of the (light and dark) blue squares in the MatMul Outputs, we derive the compute mapping for the given TCONV problem, that is, the index map of partial outputs that are not dropped out via col2IM. Therefore, our first key insight is that by using the output mapping and the compute mapping, we can solve the IOM inefficiencies and enable an efficient accelerator architecture that can: (i) Skip ineffectual computations of the dropped partial outputs (gray squares); (ii) Remove the need for storing Algorithm 1: Tiled MM2IM Data: Initialize filter step, i end row 1 foreach c 0 to Oc by filter step do 2 SendWeightFilters(c, filter step) 3 starting 0 4 foreach h 0 to Oh do 5 rows to send i end row[h] 1 starting 6 if i end row[h] starting 1 then 7 SendInputRows(starting, rows to send) 8 ComputeOutRow(h, c, filter step) 9 StoreOutRow(h, c, filter step) 10 starting i end row[h] 1 partial sums in temporary memory and to be summed later; (iii) Map the outputs of the MatMul operation directly to the final output values. B. Acceleration Dataflow for Resource-Constrained Devices Data transfer between off-chip and on-chip memory can become a bottleneck, especially on resource-constrained edge devices. Hence, we co-designed Tiled MM2IM, a specialized tiling strategy for MM2IM that enables weight and output stationary dataflow minimizing data transfer redundancy, high- lighted in Algorithm 1. Tiled MM2IM loads filter step filters and produces the corresponding output channels within the outer loop. The filter step is determined by the number of processing modules within our MM2IM architecture (dis- cussed in Section IV).\n\n--- Segment 9 ---\nTiled MM2IM loads filter step filters and produces the corresponding output channels within the outer loop. The filter step is determined by the number of processing modules within our MM2IM architecture (dis- cussed in Section IV). We dynamically load the input rows to calculate one output row per iteration within the inner loop; the i end row array that holds the number of input rows required to compute the current output row is pre-calculated. Therefore, our second key insight is that using the previous dataflow can increase decrease hardware parallelism depend- ing on the resource constraints by adjusting filter step. Additionally, with Tiled MM2IM, we preemptively calculate partial outputs for later output rows depending on the input rows being processed. C. Performance Model We built an analytical model for our MM2IM architecture to estimate performance and guide further design choices. Our performance model accounts for the problem size and the properties of our accelerator design to assess the overall performance. Additionally, we combine accelerator analysis with data movement analysis to estimate the end-to-end perfor- mance for a given TCONV layer. Next we provide an overview of our performance model. First, we calculate problem-specific metrics such as the number of MAC (multiply and accumulate) operations and the number of cropped MatMul outputs for the given TCONV problem. Then, we calculate the accelerator processing time, finding the processing time for each Processing Module (PM) and its components, the Compute Unit (CU) and the Ac- cumulation Unit (AU), which are all discussed in detail in Section IV-D: TP M TCU compute TCU load TCU store TAU (3) Fig. 3: MM2IM Accelerator Architecture. The accelerator is connected to main memory via AXI-Stream buses, which are used to receive instructions and send receive data. Then, we calculate the data transfer time required between main memory and the accelerator: TData (Wsize Isize Osize OMapsize) BW (4) Finally, we calculate the end-to-end latency for the TCONV operation, combining the two factors: Ttotal TP M TData. Therefore, our third key insight, through performance modeling, is that up to 35 of the end-to-end latency (Ttotal) for any given TCONV problem was due to transferring output mapping data between the main memory and the accelerator.\n\n--- Segment 10 ---\nThen, we calculate the data transfer time required between main memory and the accelerator: TData (Wsize Isize Osize OMapsize) BW (4) Finally, we calculate the end-to-end latency for the TCONV operation, combining the two factors: Ttotal TP M TData. Therefore, our third key insight, through performance modeling, is that up to 35 of the end-to-end latency (Ttotal) for any given TCONV problem was due to transferring output mapping data between the main memory and the accelerator. Hence, we developed the MM2IM mapper, a hardware module that completely removes the need for output mapping data transfers, discussed in Section IV-E. IV. MM2IM ACCELERATOR ARCHITECTURE Figure 3 overviews MM2IM, our proposed stream-based and scalable accelerator architecture, which utilizes simple instructions to configure, load data and execute TCONV operations. These instructions enable MM2IM to dynamically tile TCONV operations using Algorithm 1. MM2IM exploits two dimensions of parallelism at the Processing Module (PM) level, splitting Oc (output channels) computations across the X number of PMs (used for filter steps in Algorithm 1) and unrolling Ic (input channels) within the compute units with an unrolling factor of UF. Note that for our instantiation, we have set X 8 and UF 16; these parameters could be scaled to meet performance demands and resource constraints. The following sections discuss the key components of MM2IM. A. Instruction Decoder The instruction decoder allows for the reconfiguration of the accelerator to enable the execution of various TCONV layer configurations. It decodes instructions and sends control signals to the Scheduler and the weight data loader. Table I TABLE I: Micro-ISA Opcode Set. Opcode Description 0x01 Configure TCONV (sets configuration registers) 0x02 Loads Bias and Filter (activates Weight Data Loader) 0x04 Load Input (activates Dynamic Input Loader) 0x08 Schedule TCONV (activates Scheduler) 0x10 Store Output (activates Output Crossbar) shows the micro-ISA opcode set for the accelerator and a brief description of each instruction. These instructions are generated and sent to the accelerator by the host-side driver code during the execution of a given TCONV layer.\n\n--- Segment 11 ---\nOpcode Description 0x01 Configure TCONV (sets configuration registers) 0x02 Loads Bias and Filter (activates Weight Data Loader) 0x04 Load Input (activates Dynamic Input Loader) 0x08 Schedule TCONV (activates Scheduler) 0x10 Store Output (activates Output Crossbar) shows the micro-ISA opcode set for the accelerator and a brief description of each instruction. These instructions are generated and sent to the accelerator by the host-side driver code during the execution of a given TCONV layer. Note that some opcodes, for example 0x01 , are immediately followed by operand data which the accelerator expects once the in- struction is decoded; this enables dynamic reconfiguration of the accelerator or loading new sets of data into the accelerator buffers. B. Scheduler The Scheduler is the main control unit within the acceler- ator. Once activated, it orchestrates the execution of an entire TCONV layer. First it activates the MM2IM Mapper alongside the Dynamic Input Loader, and then it activates the array of Processing Modules (PMs) to execute the operations required by the TCONV layer. Additionally, the Scheduler continuously monitors the Instruction Decoder for new instructions to either load the next row of input data to the Row Buffer or to send back the output data to main memory by activating the Output Crossbar. The Scheduler has fine-grained control over the PMs, allowing them to be turned on or off as needed. C. Data I O There are two data loaders, the Weight Data Loader and the Dynamic Input Loader. The Weight Data Loader loads batches of filter and bias data from main memory (via AXI-Stream) to their respective buffers. Once a batch is loaded into the buffers, the Scheduler allocates the filter and the corresponding bias data across the PMs. The Dynamic Input Loader loads new rows of inputs data dynamically to store within the Row Buffer, which at the request of the Scheduler broadcasts the new row of input data to all the PMs via dedicated FIFOs. The Output Crossbar is an interface module which com- bines the output streams from each of the PMs, and at the request of the Scheduler it sends the output data back to main memory. Note that the Scheduler sends a store request when the Store Output instruction is received by the accelerator.\n\n--- Segment 12 ---\nThe Output Crossbar is an interface module which com- bines the output streams from each of the PMs, and at the request of the Scheduler it sends the output data back to main memory. Note that the Scheduler sends a store request when the Store Output instruction is received by the accelerator. D. Processing Module Array The Processing Module (PM) array is the accelerator s com- putational core. It consists of X PMs that can be individually configured and utilized by the Scheduler to ensure efficient processing of TCONV. Each PM contains an accumulation and a compute unit connected via a FIFO stream. Figure 4 provides a detailed view of the PM architecture with a fine- grained view of the PE array; note that the wide white arrows represent data movement between the rest of the accelerator and the Processing Module. Fig. 4: Processing Module architecture with a detailed view of the PE array. The wide white arrows represent data movement from and to the rest of the accelerator. For each TCONV layer, X filters are partitioned along the PMs. Once all the PMs load their respective filters, rows of input data are streamed to all the PMs. Additionally, the PMs receive the compute map (cmap) and output map (omap) from the MM2IM Mapper (described in Section III-A). Compute Unit: Due to the complex computing nature of TCONV, the Compute Units (CUs) contain additional logic to ensure that ineffectual dot product computations are skipped. This additional logic, the cmap check within the PE Array, takes the cmap, the input row, and the filter data, and computes the dot product of the selected input row and filter column. The partial results are then streamed into the accumulation unit for further processing. Additionally, CUs are scalable and the unrolling factor (UF) defines the number of MACs within the PE array per CU. The UF is used to tile the Ic dimension of the given TCONV layer. Hence, to execute dot-product from Ic, the PE array will take Ic UF number of cycles. Increasing UF will directly increase the number of MACs per cycle per CU while increasing the hardware resources required. Accumulation Unit: The partial sums calculated by the CUs are stored within the output buffers in the correct output indices; the Out Muxer ensures this by using the omap.\n\n--- Segment 13 ---\nIncreasing UF will directly increase the number of MACs per cycle per CU while increasing the hardware resources required. Accumulation Unit: The partial sums calculated by the CUs are stored within the output buffers in the correct output indices; the Out Muxer ensures this by using the omap. Subsequent partial sums for the same output accumulate with existing results, avoiding the need for extra buffer space. Once the output is fully calculated for an entire output row, the post-processing unit (PPU) processes the row. The PPU is a specialized processing engine used to perform the post- processing steps required by a given DNN model and then send the output data to the Output Crossbar. E. MM2IM Mapper The MM2IM Mapper is a key component of our accelerator architecture, as shown in Algorithm 2. It generates the cmap and omap corresponding to the row of partial results (i.e., the output row of MatMul) and streams them to the PMs. Note that it takes the current rowid and the number of rows as parameters to ensure that cmap omap are generated only for the required rows. This allows the MM2IM Mapper to support partitioning of the data in a tiled manner, since the rowid can be initialized to the starting row of the output tile instead of the starting row of the output matrix. Overall, the MM2IM Mapper generates the compute and output mappings only once per row, and each map is broadcast to all PMs, thus saving hardware Algorithm 2: MM2IM Mapper 1 rowid load(rowid), rowwidth load(rowwidth) 2 foreach r in MMrows do 3 hpad paddingtop (S (rowid rowwidth)) 4 wpad paddingleft (S (rowid rowwidth)) 5 imdex hpad Ow wpad 6 col 0 7 foreach ih in Ks do 8 foreach iw in Ks do 9 if (ih hpad 0 ih hpad Oh 10 iw wpad 0 iw wpad Ow) then 11 PMs cmap.broadcast write(col) 12 PMs omap.broadcast write(imdex) 13 col , imdex 14 imdex Ow Ks 15 rowid resources and additional computational overhead.\n\n--- Segment 14 ---\nThis allows the MM2IM Mapper to support partitioning of the data in a tiled manner, since the rowid can be initialized to the starting row of the output tile instead of the starting row of the output matrix. Overall, the MM2IM Mapper generates the compute and output mappings only once per row, and each map is broadcast to all PMs, thus saving hardware Algorithm 2: MM2IM Mapper 1 rowid load(rowid), rowwidth load(rowwidth) 2 foreach r in MMrows do 3 hpad paddingtop (S (rowid rowwidth)) 4 wpad paddingleft (S (rowid rowwidth)) 5 imdex hpad Ow wpad 6 col 0 7 foreach ih in Ks do 8 foreach iw in Ks do 9 if (ih hpad 0 ih hpad Oh 10 iw wpad 0 iw wpad Ow) then 11 PMs cmap.broadcast write(col) 12 PMs omap.broadcast write(imdex) 13 col , imdex 14 imdex Ow Ks 15 rowid resources and additional computational overhead. Note that the MM2IM Mapper is configured dynamically through the 0x01 opcode, and can support any shape of TCONV layers. F. Computational Flow Finally, we discuss in more detail the flow of computation within each PM and how each PM is able to calculate an output channel across multiple steps. Figure 5 provides an example of an input feature map being processed through a single filter to produce a complete output channel over N steps (N IH); this happens within each PM. The inputs on the left of each step represent the input row (presented as a column in Figure 5), which will be processed by all PMs during that step. Each input row contains Iw Ic data elements; this data is fetched from the Row Buffer after each step, as seen in Figure 3. Before starting the computations, each PM is preloaded with a single filter (shown in the middle); the filter contains Ks Ks Ic data elements, which are stored in the PM s local buffer. Note that the color of filter columns represents the intermediate outputs they produce within that step, and the weight data remains the same as the filter does not change between steps.\n\n--- Segment 15 ---\nBefore starting the computations, each PM is preloaded with a single filter (shown in the middle); the filter contains Ks Ks Ic data elements, which are stored in the PM s local buffer. Note that the color of filter columns represents the intermediate outputs they produce within that step, and the weight data remains the same as the filter does not change between steps. During each step, the PM array performs a dot product between the input row and all the non-skipped weight columns. Some weight columns are skipped to ensure no ineffectual computation takes place; the indexes for the skipped weight columns are stored with the CMAP . The intermediate outputs of dot product operations for each step are shown in Figure 5. For Step 1 , intermediate outputs A1, A2 and A3 are com- puted. Once computed, the intermediate outputs are sent to the Out Muxer to map and accumulate within the correct output index of the local out buf. We see that, A1 and A3 are mapped to output 1 whereas A2 is mapped to output 2. In Step 2 , B2 is accumulated with the previous results of output 1, B1 is accumulated within output 2, and B3 is stored in output 3. After N steps, the entire output feature map is calculated. One key feature of the MM2IM architecture is that the output data stored in out buf is sent back to main memory as soon as a whole row of outputs is completely accumulated, which enables us to reduce the size of out buf. Fig. 5: Computational flow within each PM in terms of MM2IM computations. V. EVALUATION A. Experimental setup To design, validate and evaluate our MM2IM accelerator, we utilized the SECDA methodology [16] for quick design and integration of its architecture. Since our focus is on edge- based inference, we used the PYNQ-Z1 FPGA board, which includes a dual-core ARM Cortex-A9 CPU and an edge FPGA. Additionally, we used TensorFlow Lite (TFLite) along with the SECDA-TFLite toolkit [14] for the accelerator integration. More specifically, we used SECDA-TFLite to develop a custom MM2IM delegate1 for TFLite; this custom delegate first selects all TCONV layers within the target TFLite model for offloading to our accelerator.\n\n--- Segment 16 ---\nAdditionally, we used TensorFlow Lite (TFLite) along with the SECDA-TFLite toolkit [14] for the accelerator integration. More specifically, we used SECDA-TFLite to develop a custom MM2IM delegate1 for TFLite; this custom delegate first selects all TCONV layers within the target TFLite model for offloading to our accelerator. During inference, the se- lected layers will be processed by our MM2IM delegate, which offloads all TCONV-related metadata and pointers to the MM2IM driver code. Then, the host driver orchestrates the tiling strategy for accelerating TCONV as described in Algorithm 1, offloading the relevant input and weight data as required by the accelerator to calculate the output feature maps for the layer. As the accelerator finishes calculating each output feature map, the accelerator sends back the output data to be stored in the TFLite allocated tensor in main memory. B. Synthetic benchmarks First, we evaluate the performance of our MM2IM accel- erator across varying sets of TCONV problems. Using the benchmarking suite available within SECDA-TFLite [14], we generate single-layer TCONV models and benchmark the per- formance of MM2IM across 261 TCONV permutations. We permuted the TCONV parameters with the following values that occur commonly in TCONV models: i) Oc [16, 32, 64]; ii) Ks [3, 5, 7]; iii) Ih [7, 9, 11]; iv) Ic [32, 64, 128, 256]; v) S [1, 2]. We discussed these parameters in Section II-A. Figure 6 shows the normalized speedup against dual-thread CPU 8-bit baseline (with NEON-vector instructions enabled) of the same problems on the PYNQ-Z1 board. We group similar problems for ease of visualization of the results. This set of experiments was designed to help us understand our accelerator s performance on a wide variety of TCONV problems, more specifically, to understand the dynamics be- tween TCONV dimensions and accelerator performance. The key takeaways from these experiments are the following: 1A TFLite delegate is a hardware-software backend for DNN operations. Fig. 6: MM2IM speedup normalised to CPU execution time across various TCONV problems.\n\n--- Segment 17 ---\nFig. 6: MM2IM speedup normalised to CPU execution time across various TCONV problems. i) On average, MM2IM achieves a 1.9 speedup against the dual-thread CPU; ii) The larger the Ic dimension, the greater the speedup - detailed profiling revealed that since the Ic is not tiled and is processed together without off-chip memory access, the PE array utilization can remain higher; iii) Similarly, higher Ih, Iw, Ks dimensions also achieve greater speedup - this is due to higher data-reuse of the filter (Ks2) data within each PM when the Ih and Iw dimension is larger; iv) As expected, increasing the Oc dimension yields a relatively smaller role in performance uplift; this is because MM2IM is tiled in the Oc dimension, and hence the speedup due to higher Oc is capped by the number of PMs. v) Higher stride values result in lower speedup (on average 54 ), as expected, due to less cropped outputs. To highlight the impact of cropped outputs on the speedup, we generated Figure 7, which highlights the of cropped outputs ( drop rate ) for the various TCONV problems bench- marked within Figure 6. The drop rate is calculated as the ratio of cropped outputs to the total number of outputs. Looking at Figure 7, we can see that increasing Ks results in higher drop rates, while higher Ih and S result in lower drop rates. Comparing the drop rate to the speedup, we can conclude: i) Increased kernel size results in higher drop rates and greater speedup; ii) Increased stride results in lower drop rates and speedup, as expected; iii) Decreased drop rate with increased Ih does not hamper speedup. We theorize that this is due to the increased utilization of the processing modules, as more computation is required for the larger input height and width. TABLE II: Performance Evaluation on Generative Model Layers.\n\n--- Segment 18 ---\nWe theorize that this is due to the increased utilization of the processing modules, as more computation is required for the larger input height and width. TABLE II: Performance Evaluation on Generative Model Layers. Model OC KS IH IW IC OPs Latency (ms) CPU (ms) Speedup (vs CPU) GOPs GOPs W DCGAN 1 512 5 4 1024 420M 46.26 166.56 3.60 9.07 15.64 DCGAN 2 256 5 8 512 420M 33.97 141.05 4.15 12.35 15.03 DCGAN 3 128 5 16 256 420M 35.86 149.70 4.17 11.70 14.92 DCGAN 4 3 5 32 128 20M 4.67 10.71 2.29 4.21 0.87 FCN 21 4 1 21 14K 0.22 0.22 1.00 0.06 0.01 StyleTransfer 1 64 3 64 128 604M 164.62 304.48 1.85 3.67 23.22 StyleTransfer 2 32 3 128 64 604M 282.83 460.23 1.63 2.14 23.65 StyleTransfer 3 3 9 256 32 1020M 264.27 1045.36 3.96 3.86 40.49 FSRCNN 2 9 32 32 11M 5.21 12.47 2.39 2.04 0.51 Fig. 7: Percentage of cropped outputs for the various TCONV problems benchmarked in Figure 6. C. TCONV Model Layer Evaluation We performed an extensive evaluation across specific TCONV layers commonly found in popular generative mod- els [1], [2], [15], [17]. Table II contains the specific layer details, the performance of our accelerator compared to the PYNQ CPU s single-threaded execution, overall throughput, and energy efficiency. On average, we achieve a 2.8 speedup compared to the CPU implementation while achieving an average throughput of 5.5 GOPs, with an average power to performance ratio of 14.9 GOPs W. As realized in the synthetic benchmark, the MM2IM accelerator takes advantage of the larger Ic dimension seen within the DCGAN [15] layers, achieving up to a 4.2 speedup in some layers.\n\n--- Segment 19 ---\nTable II contains the specific layer details, the performance of our accelerator compared to the PYNQ CPU s single-threaded execution, overall throughput, and energy efficiency. On average, we achieve a 2.8 speedup compared to the CPU implementation while achieving an average throughput of 5.5 GOPs, with an average power to performance ratio of 14.9 GOPs W. As realized in the synthetic benchmark, the MM2IM accelerator takes advantage of the larger Ic dimension seen within the DCGAN [15] layers, achieving up to a 4.2 speedup in some layers. D. TCONV Accelerators Comparison We now compare the performance improvement of MM2IM against other TCONV accelerators for resource-constrained edge FPGAs; Table III gives key details about the related works and summaries their performance. Note that we do not compare to ASIC designs, as our work focuses on FPGA-based accelerator architecture for TCONV and direct comparisons to ASIC-based designs would not be fair, as translating area to FPGA resources is non-trivial. In Table III we also present the best reported performance within each work. While MM2IM does not achieve the highest throughput in terms of GOPs, we are able to outperform all the related works in-terms of GOPs DSP performance, achieving a 2 increase compared to the next best work [8], which utilizes the TDC method, while reducing LUT usage by 4 . The GOPs DSP metric is more relevant than GOPs, as it takes into account the scale of the FPGA to ensure a fair comparison between accelerators. The accelerators proposed by Liu et al. [18] and Zhang et al. [6] serve as good points of comparison, as they target similar resource-constrained devices compared to our PYNQ- Z1. Comparing these works, we outperform Zhang et al. by 8.8 in terms of GOPs, while improving in DSP efficiency by 77 compared to Liu et al. [18]. E. End-to-end GAN model evaluation To demonstrate the capability of our accelerator to accel- erate end-to-end DNN execution, we evaluated MM2IM on two popular GAN models, DCGAN [15] and pix2pix [20] with end-to-end TFLite inference.\n\n--- Segment 20 ---\n[18]. E. End-to-end GAN model evaluation To demonstrate the capability of our accelerator to accel- erate end-to-end DNN execution, we evaluated MM2IM on two popular GAN models, DCGAN [15] and pix2pix [20] with end-to-end TFLite inference. As we use unmodified TFLite models, we omit accuracy as it is unchanged with our accelerator; additionally, to validate correctness, we ensured that the accelerator output matches the CPU baseline output. We accelerate the TCONV layers and the post-layer quan- tization using our MM2IM design. The rest of the layers are executed on the board s CPU. Table IV highlights the performance in terms of latency and power of MM2IM and CPU-only inference across the GAN models. We achieve a latency improvement of 2.4 and an energy reduction of 1.7 . Note that since these GAN models contain different types of layers, the potential end-to-end performance improvement with our accelerator is limited to the TCONV layers within the model. For TCONV-only layers within the models, we achieve an average latency speedup of 2.7 com- pared with the dual-threaded MM2IM execution. F. Performance Model Validation As mentioned in Section III-C, we used our performance model to estimate and guide the design choices of our MM2IM accelerator. To validate our performance model, we compare its expected performance to our accelerator s actual performance. On average, the model estimates the actual performance within 10 of our MM2IM accelerator. Applying the TCONV decoder optimization to our performance model predicts the expected performance improvement within 1 deviation of the actual performance improvement that the optimization provides. This demonstrates the utility of our performance model in guiding design choices through esti- mated performance improvements per proposed optimizations. Using our performance model helped us identify bottlenecks and solve the output mapping problems more efficiently using the MM2IM Mapper module. TABLE III: Comparison with state-of-the-art TCONV accelerators.\n\n--- Segment 21 ---\nUsing our performance model helped us identify bottlenecks and solve the output mapping problems more efficiently using the MM2IM Mapper module. TABLE III: Comparison with state-of-the-art TCONV accelerators. Source [6] [18] [19] [8] Ours FPGA ZYNQ 7Z020 ZC706 XC7Z045 ZC706 XC7Z045 Kintex-7 XC7K410T PYNQ Z1 Frequency (MHz) 100 200 167 130 200 Precision 12-bit 16-bit 16-bit 13-bit 8-bit DSP Usage 209 640 603 1512 49 (22 ) LUTs Usage 25K 85K 196K 167K 42K (79 ) FFs Usage 30K 110K 158K 49K (46 ) BRAM Usage 48 67 57 24 99 BRAM Space 2.4MB 12.8MB 10.9MB 6.7MB 4.9MB Perf. (GOPS) 2.6 29 236.9 2691 23.0 Perf. (GOPS DSP) 0.01 0.05 0.39 1.78 3.51 TABLE IV: End-to-end model inference performance of MM2IM against CPU-only inference dual-threaded execution. Model Configuration TCONV (ms) Overall (ms) Energy (J pic) DCGAN2 CPU 1T 38 1.0x 49 1.0x 7.9 1.0x ACC CPU 1T 15 2.4x 21 2.3x 4.3 1.8x CPU 2T 24 1.6x 28 1.7x 6.5 1.2x ACC CPU 2T 16 2.4x 20 2.4x 4.3 1.8x pix2pix CPU 1T 2737 1.0x 5238 1.0x 9.8 1.0x ACC CPU 1T 922 3.0x 3360 1.6x 7.9 1.2x CPU 2T 1532 1.8x 2886 1.8x 5.9 1.7x ACC CPU 2T 926 3.0x 2266 2.3x 6.2 1.6x VI.\n\n--- Segment 22 ---\n(GOPS DSP) 0.01 0.05 0.39 1.78 3.51 TABLE IV: End-to-end model inference performance of MM2IM against CPU-only inference dual-threaded execution. Model Configuration TCONV (ms) Overall (ms) Energy (J pic) DCGAN2 CPU 1T 38 1.0x 49 1.0x 7.9 1.0x ACC CPU 1T 15 2.4x 21 2.3x 4.3 1.8x CPU 2T 24 1.6x 28 1.7x 6.5 1.2x ACC CPU 2T 16 2.4x 20 2.4x 4.3 1.8x pix2pix CPU 1T 2737 1.0x 5238 1.0x 9.8 1.0x ACC CPU 1T 922 3.0x 3360 1.6x 7.9 1.2x CPU 2T 1532 1.8x 2886 1.8x 5.9 1.7x ACC CPU 2T 926 3.0x 2266 2.3x 6.2 1.6x VI. RELATED WORK Various works with different accelerator architectures and optimizations have been proposed to accelerate TCONV op- erations, employing methods such as TDC [8], Winograd- Transformed Transposed Convolution [19], [21], and the Zero- Insert TCONV method [7]. However, these approaches face computational overheads due to algorithmic limitations inher- ent to their respective methods. Some works have proposed implementing the TCONV operation using the IOM method on FPGAs. For example, Ma et al. [10] exploit the intermediate-centric dataflow, a variation on the IOM method, but their accelerator only supports fixed dimensions for given problems. Similarly, the initial work by Sestito et al. [22] exploits High-Level Synthesis (HLS) to create Deconvolution Engines that can solve fixed dimension TCONV problems efficiently. These engines require additional buffers to store the intermediate results until the entire filter is processed, after which the intermediate results are summed together to produce the final output feature map. Additionally, this work does not handle the ineffectual computation preemp- tively, hence performing more MACs than required and also having to post-process the result to achieve the cropped results.\n\n--- Segment 23 ---\nThese engines require additional buffers to store the intermediate results until the entire filter is processed, after which the intermediate results are summed together to produce the final output feature map. Additionally, this work does not handle the ineffectual computation preemp- tively, hence performing more MACs than required and also having to post-process the result to achieve the cropped results. The later HLS template-based approach proposed by Sestito et al. [12] suffers from the same constraints, and although they can adjust their accelerator to different problem sizes, this requires re-synthesis and re-mapping of the accelerator. In both works, the computation engine processes a tile of Ic dimension at a time, unlike our design, which processes the entire Ic dimension, keeping the dataflow output stationary, 2We use the Tensorflow-defined version of DCGAN: tensorflow.org tutorials generative dcgan allowing larger TCONV problems to be processed on smaller FPGAs without requiring slow off-chip memory access. Zhang et al. [6] proposed an output-oriented design that solves the overlapping sum problem for edge devices but introduces hardware complexity, degrading the accelerator s performance. Other works such as GNA [9] and FCN-Engine [11] exploit the IOM method with ASIC designs, but similar to all previ- ously mentioned IOM-based approaches they do not consider the cropped outputs. Finally, there are several works [10], [23] which take advantage of large-scale FPGAs to efficiently map TCONV problems fully within the FPGA fabric without requiring slow off-chip memory access. While effective at tackling TCONV on larger FPGAs, they do not consider the limitations of TCONV within resources-constrained FPGAs. VII. CONCLUSION We proposed MM2IM, a novel hardware architecture to effi- ciently accelerate TCONV operations on resource-constrained edge devices with FPGAs. Our efficient hardware-software co- designed solution solves three key challenges: i) the overlap- ping sum mapping problem; ii) ineffectual computations and cropped output mapping; and iii) the need for efficient dataflow strategies for resource-constrained edge devices.\n\n--- Segment 24 ---\nCONCLUSION We proposed MM2IM, a novel hardware architecture to effi- ciently accelerate TCONV operations on resource-constrained edge devices with FPGAs. Our efficient hardware-software co- designed solution solves three key challenges: i) the overlap- ping sum mapping problem; ii) ineffectual computations and cropped output mapping; and iii) the need for efficient dataflow strategies for resource-constrained edge devices. We imple- mented our proposed hardware design on an edge FPGA using the SECDA-TFLite toolkit, and evaluated the performance across a large variety of configurations for TCONV problems, achieving an average speedup of 1.9 against a dual-thread ARM CPU. We also compared MM2IM against other TCONV accelerators for similarly resource-constrained edge FPGAs and achieved at least 2 higher GOPs DSP compared to the next best accelerator. Finally, we performed an end-to-end evaluation of the DCGAN and pix2pix models, achieving a 2.4 speedup and 1.7 energy reduction on average compared with the CPU baseline. As future work, we plan to further scale down our accelerator design to enable TCONV in devices with lower resources budgets such as micro-controllers. ACKNOWLEDGMENTS This work was partially supported by the UK Engineering and Physical Sciences Research Council (grant EP R513222 1), the EU Project dAIEDGE (GA Nr 101120726) and the Innovate UK Horizon Europe Guarantee (GA Nr 10090788). REFERENCES [1] C. Dong, C. C. Loy, and X. Tang, Accelerating the Super-Resolution Convolutional Neural Network, in European Conference on Computer Vision (ECCV), 2016, pp. 391 407. [2] J. Johnson, A. Alahi, and L. Fei-Fei, Perceptual Losses for Real- Time Style Transfer and Super-Resolution, in European Conference on Computer Vision (ECCV), 2016, pp. 694 711. [3] L. Liu, M. Muelly, J. Deng, T. Pfister, and L.-J. Li, Generative Modeling for Small-Data Object Detection, in 2019 IEEE CVF International Conference on Computer Vision (ICCV), 2019, pp. 6072 6080.\n\n--- Segment 25 ---\nLi, Generative Modeling for Small-Data Object Detection, in 2019 IEEE CVF International Conference on Computer Vision (ICCV), 2019, pp. 6072 6080. [4] P. Gibson, J. Cano, E. J. Crowley, A. Storkey, and M. O Boyle, DLAS: A Conceptual Model for Across-Stack Deep Learning Acceleration, in ACM Transactions on Architecture and Code Optimization, 2024. [5] S. Mittal, A survey of FPGA-based accelerators for convolutional neural networks, vol. 32, no. 4, pp. 1109 1139. [Online]. Available: [6] X. Zhang, S. Das, O. Neopane, and K. Kreutz-Delgado, A Design Methodology for Efficient Implementation of Deconvolutional Neural Networks on an FPGA, in arXiv:1705.02583, 2017. [7] Y. Yu, T. Zhao, M. Wang, K. Wang, and L. He, Uni-OPU: An FPGA- Based Uniform Accelerator for Convolutional and Transposed Convo- lutional Networks, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, pp. 1545 1556, 2020. [8] J.-W. Chang, K.-W. Kang, and S.-J. Kang, An Energy-Efficient FPGA- Based Deconvolutional Neural Networks Accelerator for Single Image Super-Resolution, IEEE Transactions on Circuits and Systems for Video Technology, pp. 281 295, 2020. [9] J. Yan, S. Yin, F. Tu, L. Liu, and S. Wei, GNA: Reconfigurable and Efficient Architecture for Generative Network Acceleration, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, pp. 2519 2529, 2018. [10] Z. Ma, T. Dai, X. Wei, and G. Luo, An Intermediate-Centric Dataflow for Transposed Convolution Acceleration on FPGA, ACM Transactions on Embedded Computing Systems, 2022. [11] D. Xu, K. Tu, Y. Wang, C. Liu, B.\n\n--- Segment 26 ---\n[10] Z. Ma, T. Dai, X. Wei, and G. Luo, An Intermediate-Centric Dataflow for Transposed Convolution Acceleration on FPGA, ACM Transactions on Embedded Computing Systems, 2022. [11] D. Xu, K. Tu, Y. Wang, C. Liu, B. He, and H. Li, FCN-Engine: Accelerating Deconvolutional Layers in Classic CNN Processors, in 2018 IEEE ACM International Conference on Computer-Aided Design (ICCAD), 2018, pp. 1 6. [12] C. Sestito, S. Perri, and R. Stewart, FPGA Design of Transposed Convolutions for Deep Learning Using High-Level Synthesis, Journal of Signal Processing Systems, 2023. [Online]. Available: [13] M. Devs. Col2im - Rearrange matrix columns into blocks - MATLAB. [Online]. Available: html [14] J. Haris, P. Gibson, J. Cano, N. Bohm Agostini, and D. Kaeli, SECDA- TFLite: A toolkit for efficient development of FPGA-based DNN accelerators for edge inference, Journal of Parallel and Distributed Computing, pp. 140 151, 2023. [15] A. Radford, L. Metz, and S. Chintala, Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, in 4th International Conference on Learning Representations (ICLR), 2016. [16] J. Haris, P. Gibson, J. Cano, N. B. Agostini, and D. Kaeli, SECDA: Efficient Hardware Software Co-Design of FPGA-based DNN Accelera- tors for Edge Inference, in 2021 IEEE 33rd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD), 2021, pp. 33 43. [17] J. Long, E. Shelhamer, and T. Darrell. Fully Convolutional Networks for Semantic Segmentation. [Online].\n\n--- Segment 27 ---\nFully Convolutional Networks for Semantic Segmentation. [Online]. Available: 1411.4038 [18] S. Liu, H. Fan, X. Niu, H.-c. Ng, Y. Chu, and W. Luk, Optimizing CNN-based Segmentation with Deeply Customized Convolutional and Deconvolutional Architectures on FPGA, ACM Transactions on Recon- figurable Technology and Systems, pp. 1 22, 2018. [19] X. Di, H.-G. Yang, Y. Jia, Z. Huang, and N. Mao, Exploring Effi- cient Acceleration Architecture for Winograd-Transformed Transposed Convolution of GANs on FPGAs, Electronics, p. 286, 2020. [20] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, Image-to-Image Trans- lation with Conditional Adversarial Networks, in arXiv:1611.07004. [21] J.-W. Chang, S. Ahn, K.-W. Kang, and S.-J. Kang, Towards Design Methodology of Efficient Fast Algorithms for Accelerating Generative Adversarial Networks on FPGAs, in 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC), 2020, pp. 283 288. [22] C. Sestito, R. Stewart, and S. Perri, High-Level Synthesis of Hardware Accelerators for Deconvolution Engines, pp. 1 4. [23] T.-H. Wu, C. Shu, and T.-T. Liu, An Efficient FPGA-Based Dilated and Transposed Convolutional Neural Network Accelerator, vol. 71, no. 11, pp. 5178 5186.\n\n