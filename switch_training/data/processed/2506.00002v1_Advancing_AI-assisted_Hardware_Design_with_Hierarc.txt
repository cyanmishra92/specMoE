=== ORIGINAL PDF: 2506.00002v1_Advancing_AI-assisted_Hardware_Design_with_Hierarc.pdf ===\n\nRaw text length: 42518 characters\nCleaned text length: 42134 characters\nNumber of segments: 21\n\n=== CLEANED TEXT ===\n\nAdvancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization Hao (Mark) Chen , Zehuan Zhang , Wanru Zhao , Nicholas Lane , Hongxiang Fan Imperial College London, London, UK University of Cambridge, Cambridge, UK {hc1620, zehuan.zhang22, w.luk, {wz341, Abstract Recent years have witnessed a significant increase in the adoption of AI techniques to enhance electronic design automation. In particular, the emergence of Large Language Models (LLMs) has sparked significant interest in LLM-assisted hardware design generation, spanning applications from classi- cal digital circuits to quantum computing. Despite substantial progress in this direction, the quality of LLM-generated hard- ware design still cannot meet the requirements for practical deployment. In this work, we identify three critical challenges hindering the development of LLM-assisted hardware design generation: 1) limited data availability, 2) varied data quality, 3) inadequate inference-time efficiency. To address these fundamen- tal challenges, this paper introduces a two-stage framework for AI-assisted hardware design by exploring decentralized training and personalized inference. In the first stage, we propose to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. To mitigate the impact of low-quality data, we identify optimization opportunities in hardware generation tasks, using user-defined metrics for model aggregation. The second stage focuses on client personalization to enhance both speed and quality. We introduce a new metric, Trueput, to ana- lyze LLM-assisted hardware generation efficiency. To optimize Trueput, we implement personalized inference-time acceleration and customized sampling strategies. Evaluating both classical and quantum benchmarks, our experimental results demonstrate that the proposed two-stage framework can significantly improve the model capability for hardware design generation. As orthogonal enhancements to existing methods, our framework can achieve 33 50 semantic accuracy improvement and 2.3 times speedup, depending on the difficulty of the generation tasks. Both the code and benchmarks will be released publicly to foster further development in this field. I. INTRODUCTION Recent advancements in Large Language Models (LLMs) have demonstrated their great potential in automated software programming [16] and debugging [17]. This impressive capa- bility has sparked significant research and industrial interest in leveraging LLMs to automate hardware design for both classical and quantum domains. In classical hardware design, extensive research has targeted RTL design generation [5], [27], [28], [30], [41], High-Level Synthesis (HLS) coding [25], [44], [45], and hardware debugging [11], [42], [46]. In quan- tum design generation, IBM has pioneered the use of LLMs for quantum programming [9], which has been integrated into their Qiskit Code Assistant tool . Although significant research efforts have been devoted to exploring LLM-assisted design generation for both classical Fig. 1: An overview of our proposed two-stage framework for the future of AI-assisted hardware design. and quantum hardware, there are still three key challenges hindering their practical use and deployment: Challenge-1: Limited availability of hardware design sources for training. Due to the low-resource nature of hardware description languages, the amount of publicly accessible classical and quantum design sources is much lower than that of software programs. For example, the size of the Qiskit dataset [9] is more than 1000 times smaller than that of the Python dataset [29]. Challenge-2: Varied quality of training data. High-quality hardware designs are often proprietary and unavailable to the public. The existing training data sourced from public repositories may lack the embedded knowledge necessary for designing high-quality hardware [28]. Challenge-3: Insufficient generation quality. Existing LLM- assisted methods still exhibit limited accuracy in hardware generation tasks [29]. The lack of personalized and cus- tomized optimizations during deployment time further limits the potential of LLM for hardware design generation. Therefore, current LLM-assisted hardware generation remains in its early stages, with practical deployment limited. To fully unleash the potential of generative AI for the future of AI-assisted hardware design, this work proposes a two-stage framework by leveraging decentralized and person- alized learning. To address Challenge-1 of data availability, we aim to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. This proposed approach includes recent advancements in federated learning [34] and model merging [47], introducing a novel hierarchical model update mechanism to facilitate broad adoption among users and arXiv:2506.00002v1 [cs.AR] 21 Apr 2025 organizations by accommodating varied hardware capabilities, communication infrastructures, and individual preferences. To tackle Challenge-2 of data quality, we propose a metric-based model aggregation and merging strategy. Although data quality control in the decentralized setting is a challenging task [2], [10], [20], we identify the unique optimization opportunities for hardware design code such as correctness and perfor- mance. To overcome Challenge-3 of generation quality, we propose personalized inference-time optimizations to enhance the generation capability. This includes speed optimization using inference-time multi-token acceleration and quality im- provement through customized output token sampling. As shown in Fig. 1, these optimizations follow the decentralized training process, forming a general two-stage framework for the future of AI-assisted hardware design. Overall, our contributions are summarized as follows: A hierarchical decentralized training paradigm with metric- based model aggregation, facilitating a broader and more diverse pool of participants for collaborative training in AI- assisted hardware design (Sec. IV). Personalized inference-time acceleration with customized sampling strategies, improving both efficiency and design quality of LLM for automatic hardware generation (Sec. V). A comprehensive benchmarking and evaluation of the pro- posed two-stage framework in both classical and quantum hardware design, highlighting the effectiveness and versatil- ity of our approach (Sec. III Sec. VI). II. BACKGROUND AND RELATED WORK A. Decentralized Training Decentralized training distributes the model training across multiple nodes and devices, with only the communication of weights or gradients for model updates. The primary benefits of decentralized training are two-fold: 1) Proprietary data preservation: By maintaining data locally on the client side for training, decentralized training circumvents data-sharing constraints. 2) Compute efficiency: The vast computational resources available on billions of client devices can be utilized for training. In this paper, we mainly focus on two main- streaming decentralized training: federated learning and model merging. It is worth noting that our proposed framework is general and can be extended to accommodate any decentral- ized training approach. 1) Federated Learning: As a promising approach to achieve decentralized deep learning [38], federated learning [31] has been extensively studied and optimized over the past decade. The key concept of federated learning is to move model training from a central server to distributed client devices. Depending on the structure of parties (e.g. organizations or individual clients), the scale of participants, and data char- acteristics, federated learning is typically categorized into cross-device and cross-silo methods [18]. By performing the training locally on client devices, federated learning period- ically collects and aggregates model updates from different clients. Following the introduction of the classical FedAvg algorithm [31], recent research in federated learning has pri- marily focused on addressing challenges related to data and system heterogeneity [48] to enable practical deployment. 2) Model Merging: With the recent advancements in lan- guage models and the increasing number of open-sourced pre- trained models [43], significant research efforts have been focused on model merging that integrates the weights of multiple different models to enhance the general capability of the merged model without the need to access the original training data [15]. As the data are not shared during model merging, it provides an efficient and flexible way to learn the different expert knowledge by merging multiple domain- specific models. To preserve the generalizability and capacity of the merged model, various techniques have been introduced such as weighted-based merging [50], subspace-based meth- ods [49], and routing-based approaches [33]. More recently, methods designed for efficient and scalable merging of black- box models have also been introduced [7]. B. Optimization of LLM Inference While various techniques have been introduced to improve the inference efficiency of LLM, this paper mainly focuses on optimization techniques that avoid time-consuming re-training and major modifications to the model architecture. 1) Speculative Parallel Decoding: Due to the sequential nature of autoregressive inference, LLMs suffer from data dependency and memory-bound performance. To address this, speculative decoding [23], [32] and parallel decoding [3], [6] enable multi-token generation via iterative guess-and-verify strategies. Speculative decoding uses a separate model to draft multiple tokens, while parallel decoding leverages lightweight prompt tokens and embeddings. In both methods, the original model verifies the generated tokens. 2) Inference-Time Scaling Strategy: As LLM training im- provements slow [39], recent work has focused on inference- time optimization. One approach is self-refinement, such as recursive introspection [36], where the model iteratively im- proves its answers based on prior outputs. Another is the search-and-verify paradigm [26], where multiple samples are generated and evaluated by a verifier either a reward model or end-to-end evaluator to select the best solution. C. Related Work Hierarchical training has been investigated in previous re- search on federated learning to address network heterogeneity issues [1], [13], [14]. Distinct from prior approaches, this paper considers diverse clients conditions in the context of hardware design generation and explores a hierarchical decentralized training with a novel hybrid use of federated learning and model merging, encouraging a broader and more diverse pool of participants. Applying client personalization after the training of global models has been investigated in the contexts of federated learning [19]. Different basic fine-tuning approaches have been employed for model personalization, such as regularised fine- tuning [40] and selective parameter method [24]. More ad- vanced techniques, such as meta-learning [21], have also been explored for client personalization. Unlike previous methods, our framework integrates inference-time acceleration with a novel budget-aware sampling strategy driven by our novel metric Trueput. III. FRAMEWORK OVERVIEW AND BENCHMARKS A. Framework Overview An overview of our proposed framework is illustrated in Fig. 1. Designed to leverage private-domain data for model training with privacy considerations while maximizing de- ployment efficiency and performance, our framework mainly consists of two stages: decentralized training and personalized inference-time optimization. These stages can be applied iter- atively to collaboratively enhance the model s capabilities for AI-assisted hardware design. The first stage of decentralized training features a hierarchi- cal mechanism (Sec. IV-A) with hybrid federated learning and model merging, which facilitates a broader and more diverse pool of participants by considering varying hardware capa- bilities, connection restrictions, and individual preferences. In the second stage, different inference-time optimizations are personalized (Sec. V) for each client to enhance the inference speed and generation quality, with different hyperparameters optimized and customized to maximize the deployment per- formance and efficiency for diverse use cases. B. Benchmarks To demonstrate the effectiveness of our approaches, we perform evaluation on two different benchmarks: one for clas- sical hardware and the other for quantum hardware. The code files for both scenarios are sourced from publicly available GitHub repositories with compatible licenses, reflecting real- world data heterogeneity. Classical Hardware Benchmark. To validate the appli- cability of our framework in facilitating classical hardware designs, we evaluate it on a C-based High-Level Synthesis (HLS) benchmark [12] comprising 7437 training samples and 1860 test samples. Each sample consists of a high-level design description and a canonical HLS program. The HLS designs include a wide range of domains, such as Matrix and Linear Algebra Operations, Scientific Simulation, etc. Quantum Hardware Benchmark To evaluate the effective- ness of our approach in quantum hardware generation, we use a Qiskit benchmark [4] with 10896 training samples and 50 test samples. Each training sample contains a Qiskit program with human-written comments, while each test sample consists of a functionality description and a canonical Qiskit program. IV. DECENTRALIZED TRAINING A. Hierarchical Decentralized Training Federated learning has demonstrated its potential for decen- tralized training, with both cross-silo and cross-device settings studied for various user scenarios. However, its practicality and effectiveness might decrease when deploying it for clients with poor or unreliable communication. This challenge becomes FL FL Organizations, Institutions, and Clients with Reliable Connections Parties with Communication or Geometric Restrictions FL Phase1: Hybrid Decentralized Training Phase2: Model Merging Fig. 2: The vision and overview of our proposed framework for the future of AI-assisted hardware design. even more pronounced in extra-large-scale collaborative train- ing settings, where geometric and infrastructural restrictions might affect deployment feasibility. Additionally, most feder- ated learning approaches assume a shared network architecture for locally trained models, which becomes impractical in the context of LLM due to the high computational and memory requirements for LLM training on client devices. To promote the broader adoption of decentralized training for foundation models in AI-assisted hardware design, this paper proposes a hierarchical decentralized training scheme. As illustrated in Fig. 2 and Algorithm 1, our hierarchical decentralized training consists of two tiers. The first tier is referred to as hybrid decentralized training. For clients or organizations with reliable communication channels, feder- ated learning is employed for collaborative training. Multiple clients will employ federated learning within each group independently, resulting in several separately trained federated models. Meanwhile, for parties with isolated environments due to geographical or infrastructural limitations, individual local training is performed. In the second tier, different models with diverse domain knowledge, learned via either federated or local training, are combined together using model merging techniques. This hierarchical, two-tier decentralized training framework enables efficient utilization of private domain data regardless of physical or regulatory restrictions. B. Metric-based Aggregation Adaptive methods like client selection and quality-aware aggregation [35] have been explored in federated learning and model merging, primarily for classification and segmentation tasks. Their effectiveness in generative AI remains underex- plored, largely due to challenges in evaluating generated con- tent. Metrics like perplexity depend on reference outputs and often fail to capture functional equivalence e.g., semantically identical programs with different styles may score differently. Neural metrics such as LLM-as-Judge offer alternatives but often lack explainability and analytical rigor. To address these challenges, this paper identifies a key optimization opportunity in hardware design generation tasks. Unlike traditional generative tasks, hardware generation in- herently provides quantitative evaluation metrics including design syntax accuracy, hardware functional correctness, and hardware latency that can serve as robust criteria for model Algorithm 1 Hierarchical Decentralized Training 1: Notation 2: C: Set of all clients 3: CF : Subset of clients with reliable communication, parti- tioned into G groups CF g , where g {1, 2, . . . , G} 4: CL: Subset of clients with no reliable communication 5: FL( ): Federated learning function 6: LT( ): Local training function 7: MM( ): Model merging function 8: Mglobal: Final global model after merging 9: Tier 1: Hybrid Decentralized Training 10: for each group CF g in CF do 11: Train model M F g FL(CF g ) Federated Learning 12: end for 13: for each client CL i in CL do 14: Train model M L i LT(CL i ) Local Training 15: end for 16: Tier 2: Model Merging 17: Gather: M {M F 1 , . . . , M F G } {M L i CL i CL} 18: Merge: Mglobal MM(M) 19: Output: Global model Mglobal aggregation and merging. Leveraging this observation, we propose a flexible aggregation framework that enables users to define custom metrics for weighting model contributions. Formally, given the i-th client model Mi from a set of N client models, the final aggregated model Mf is computed as Mf PN i 1 g(Mi) Mi, where g( ) is a user-defined metric applied to a client model to determine its contribution. For instance, g( ) could be defined using syntax accuracy to filter out model weights from clients trained on syntactically incorrect data, thereby ensuring high-quality training data for the aggregated model. It is worth noting that our framework is not restricted to using hardware-specific metrics such as syntax accuracy and functional correctness. The framework is designed to accommodate a wide variety of model aggregation strategies, improving the versatility and facilitating broader adoption of our approach. For example, if g(.) is parameterized as the ratio of client training samples, the aggregation replicates the FedAvg algorithm. By enabling customization of g(.), our framework caters to diverse requirements across different hardware generation tasks, allowing users to tailor aggregation strategies to their specific needs. V. PERSONALIZED INFERENCE-TIME OPTIMIZATIONS A. Trueput: Efficiency Analysis for Design Generation To analyze the efficiency of LLM-assisted design genera- tion, we propose a new metric, Trueput, which quantifies the number of functionally correct designs generated per unit of time. It is defined as: Trueput Tinf (1) where represents the expected functionality pass rate when k samples are generated, and Tinf denotes the expected inference latency per output design. Next, we analyze Trueput under the constraint of limited computational resources. When batching is employed, the inference latency Tinf is expressed as Tinf(k) since the batch size depends on k. According to the Codex [8], an unbiased estimate of can be written as 1 (1 p)k with functionality pass probability p. Substituting this into the definition of Trueput, we obtain: Trueputbatch 1 (1 p)k Tinf(k) (2) This formulation enables the analysis of efficiency of the inference framework by accounting for both the accuracy of the generated designs and the latency associated with batching during inference. Increasing Trueputbatch requires decreasing the inference time Tinf( ), and improving functionality pass rate p. Given the form in (2), we hypothesize that a global maximum of Trueputbatch exists at some finite value of k, for fixed p and Tinf( ). Therefore, the value of k should be optimized for each client to maximize Trueputbatch. To address these goals, this paper explores personalized test-time optimizations that target both speed enhancement to reduce latency and quality improvement to increase pass rate. B. Inference-Time Speed and Quality Enhancement The scaling law of inference has indicated its potential to improve the performance for most natural language tasks. In this work, we investigate their effectiveness in hardware design generation and propose customization to further enhance their flexibility and efficiency. Customized Quality Improvement. Various test-time op- timizations [39] can enhance output generation quality, with popular methods including Best-of-N sampling and beam search. Since clients have diverse domains, such as classical or quantum, and their focus on designing different hardware architectures, the choice of optimization techniques can vary across different scenarios to maximize the generation quality. Moreover, these techniques introduce multiple hyperparame- ters, presenting a design space for optimization. To leverage this opportunity, our framework enables clients to customize and optimize their sampling strategy and hyperparameters to meet specific requirements, for example, allowing them to balance hardware design quality and generation latency by adjusting the sampling count. Table I presents the test- time optimization strategies supported in our framework with their associated hyperparameters. To tailor the optimization strategy for each client, a grid search can be used to tune the optimization configurations at a fixed compute budget. Personalized Inference-Time Acceleration. Generating an optimized hardware design may require a large number of samples, resulting in high generation latency and energy costs. Since the performance of auto-regressive generation in LLM inference is typically memory-bound, several techniques have been introduced to leverage idle compute resources to accelerate LLM inference. Among these are speculative TABLE I: Test-Time Optimization Strategies Sampling Description Hyperparameters Nucleus Sampling Selects tokens with cumulative probability p p (cumulative probability) Temperature Sampling Scales token probabilities by temperature Temperature, Number of generated candidates Top-k Sampling Chooses from the top k most probable tokens k (number of sequences to consider) Beam Search Expands search using a fixed beam width Beam width w o Finetune Model Merging Hierarchical Merging 0 20 40 60 80 100 accuracy ( ) 52.15 0.38 42.31 40.38 91.18 90.11 Syntax Accuracy Semantic Accuracy (a) HLS MachineEval. w o Finetune Model Merging Hierarchical Merging 0 20 40 60 80 100 accuracy ( ) 12.0 2.0 58.0 50.0 92.0 88.0 Syntax Accuracy Semantic Accuracy (b) HLS HumanEval. w o Finetune Model Merging Hierarchical Merging 0 10 20 30 accuracy ( ) 18.0 20.0 22.0 (c) Qiskit Benchmark. Fig. 3: Effect of hierarchical approach on both classical and quantum hardware benchmarks. decoding [23], [32] and parallel decoding [3], [6], which generate multiple tokens in parallel to improve the processing speed. However, most existing approaches rely on a separate training process to learn the multi-token generation capability. In this work, we propose an inference-time learning ap- proach, where each client locally learns acceleration param- eters during the model s deployment phase while serving real user requests. Specifically, we observe that the learning process of multi-token generation involves tuning the accel- eration parameters to approximate the predictive distribution of the original model. Therefore, rather than depending on a training dataset, our approach utilizes the generation outputs produced during deployment, while serving user requests, for learning multi-token generation. This method offers two key benefits. First, by leveraging user-generated content as labels, the approach can be seen as an unsupervised learning technique, eliminating the need for extra datasets. Second, the learning process is performed during the model deployment time, avoiding a separate training process to learn multi-token generation. In this paper, we consider parallel decoding ap- proaches as they are more training-efficient compared to other speculative decoding methods, making it suitable for online learning. The training objective is formulated as follows: arg min ϕ Ex D KL Pa(yt 1:t k y1:t, x; ϕ), Po(yt 1:t k y1:t, x; θ) where ϕ are the acceleration parameters, and D is the deployment data distribution. The KL-divergence measures the difference between the Pa distribution for acceleration and target Po distributions. yt 1:t k represents the predicted token sequence, and y1:t the previously generated tokens by target model with parameter θ. VI. EXPERIMENTS A. Evaluation Setup Models and Datasets Our proposed framework is applica- ble to a wide range of machine learning methods. However, w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 20 40 60 80 100 accuracy ( ) 52.15 0.38 71.77 65.75 70.22 64.09 Syntax Accuracy Semantic Accuracy (a) HLS MachineEval. w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 20 40 60 80 100 accuracy ( ) 12.0 2.0 68.0 64.0 64.0 60.0 Syntax Accuracy Semantic Accuracy (b) HLS HumanEval. w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 10 20 30 accuracy ( ) 18.0 24.0 26.0 (c) Qiskit Benchmark. Fig. 4: Evaluation of federated learning on both classical and quantum hardware benchmarks. due to their growing popularity and practical relevance, we focus on LLMs in our experiments. For classical hardware experiments, we use CodeLlama-7B [37] as the base model and the HLS benchmark described in Sec. III-B. This bench- mark contains machine-generated instructions (MachineEval) produced by GPT for HLS generation. To evaluate the model s generalizability, we involve human experts to manually refine 50 samples, creating a HumanEval version. For Qiskit quan- tum design generation, we use StarCoder2-3B [29] with the dataset introduced in Sec. III-B. Federated Learning. For both the classical and quantum benchmarks, we simulate real-world data heterogeneity by training 40 clients on datasets partitioned using a Dirichlet distribution [22], based on the repository IDs of the source code. Each round involves training for one epoch with 10 of the clients participating. Two aggregation metrics were tested: the number of data samples (Ratio) and hardware syntax accuracy (Acc). A separate validation dataset was used to calculate syntax accuracy. Model Merging. Hardware syntax accuracy on a valida- tion dataset was used as the weight for model aggregation for both benchmarks. DARE [49] was used for hierarchical model aggregation. In this setting, the datasets are partitioned in a manner similar to federated learning, with each client performing local training on its own subset of data. Personalized Test-Time Optimization. We adopt parallel decoding [6] for inference-time acceleration. A validation dataset with hardware design instructions is used to simulate user requests. Different sampling strategies and the associated hyperparameters are summarized in Table I. B. Effect of Hierarchical Approach To evaluate the effectiveness of our proposed hierarchical approach, we conduct experiments on both classical and quan- tum benchmarks, as shown in Fig. 3. We compare our method against two baselines: the base model without fine-tuning and a model merging without hierarchical aggregation. For classical hardware generated via HLS, we assess both syntax and semantic accuracy with template generation enhancement. For the quantum benchmark, we primarily focus on semantic accuracy evaluation. In both classical and quantum evalua- tions, our approach demonstrated accuracy improvement. As shown in Fig. 3a 3b, the hierarchical approach demonstrates significantly greater improvement for classical hardware gen- eration tasks in both MachineEval and HumanEval, achieving nearly an 80 increase in syntax and semantic accuracy compared to the model without fine-tuning, and approximately 50 over the model obtained through model merging. While improvements on the Qiskit benchmark were less pronounced due to the increased complexity of quantum circuit design, our model still performs comparably to the centrally trained baseline [4]. Its performance can be further enhanced by integrating it into the multi-agent framework proposed in [4], which incorporates Retrieval-Augmented Generation (RAG), Chain-of-Thought (CoT) reasoning, and a semantic analyzer. Training Overhead and Communication Costs: Our hi- erarchical approach significantly cuts communication versus standard FL. Standard FL requires N R central updates (N clients, R rounds). On the other hand, our hierarchical ap- proach confines the frequent communication within G groups (NF L clients total), with only one central merge involving G group models and NL local models (NF L NL N). Thus, central communication drops from O(N R) to O(G NL) transfers, significantly improving scalability and reducing bandwidth needs, as G NL N R typically. This structure efficiently leverages clients idle compute, requiring significantly less compute from central server compared to standard central training. C. Evaluation of Federated Learning Fig. 4 present the evaluation of federated learning on classi- cal and quantum benchmarks using two different aggregation strategies. As shown in Fig. 4c, leveraging hardware syntax ac- curacy during model aggregation achieves the best result in the Qiskit Benchmark. For classical hardware generation as shown in Fig. 4a 4b, Both ratio-based and Acc-based approaches achieve similar results in MachineEval and HumanEval, with up to a 60 increase in semantic accuracy. These findings demonstrate the effectiveness and flexibility of federated learn- ing with metric-based aggregation in training models for both classical and quantum hardware design generation. D. Personalized Inference-Time Optimizations As discussed in Sec. I, the lack of personalized optimiza- tions restricts the potential for maximizing both speed and quality in model inference. Thus, we evaluate the impact of two test-time optimizations: multi-token generation for accel- eration and customized sampling for quality improvement. We show that by tailoring the multi-token generation configuration and sampling strategies to the client s compute budget, the optimization achieves 2.3 speedup ratio and up to 46 syntax accuracy improvement over default greedy decoding. In Fig. 5, we evaluate the speedup ratio with respect to the tree size, a parameter reflecting the token parallelism. While the acceptance ratio increases with larger tree sizes, the speedup ratio peaks at a tree size of 60 due to limited idle compute resources. This aligns with previous research [6], which shows that tree size must be optimized per client to achieve maximum speedup. In Fig. 5, we examine how different sampling strate- gies affect syntax accuracy. Beam search performs best with sample sizes under 3, while combined sampling outperforms others for larger sample sizes. Our findings emphasize the need 60 75 90 105 120 Tree Size 2.5 2.6 2.7 2.8 2.9 Acceptance Ratio Acceptance Ratio 1.9 2.0 2.1 2.2 2.3 2.4 Speedup Ratio Speedup Ratio 1 2 3 4 5 Sample Number 20 30 40 50 60 70 Syntax Accuracy ( ) Temperature Sampling Combined Sampling Top-k Sampling Greedy Sampling Top-p Sampling Beam Search Fig. 5: Personalized Test-Time Optimization. Left: multi- token generation. Right: Customized sampling for HLS mod- els. Combined Sampling uses both top-k and top-p filtering. 1 2 3 4 5 6 7 8 9 10 Sample Number 35 40 45 50 55 60 ( ) Sampling Latency (full GPU Capacity) Sampling Latency (20 GPU Capacity) 13 15 17 19 21 23 25 27 29 31 Latency (s) 1 2 3 4 5 6 7 8 9 10 Sample Number 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Trueput (design s) 4.08 2.91 full GPU capacity 20 GPU capacity Fig. 6: Trueput Evaluation. Left: and latency for different sample sizes. Right: Trueput across GPU capacities. for personalized inference-time optimizations to advance AI- assisted hardware design. The optimal configuration for each client can be determined offline with a one-time computation, avoiding additional system complexity and compute overhead. E. Effect of Sample Number on Trueput Optimization In Sec. V, we introduce Trueput to analyze the efficiency of design generation. To maximize Trueputbatch, in addition to the previously mentioned personalized test-time optimization, it is important to search for the optimal sample number k, as shown in (2). As depicted in Fig. 6, both and sampling latency increase with the sample number k, but with different rates. Fig. 6 shows that a local maximum of Trueputbatch exists, and the optimal value of k varies depending on the GPU capacity. This observation highlights that the sample number should be optimized per client to achieve the maximum Trueput. VII. CONCLUSION Recent advancements in AI have shown great potential in revolutionizing the traditional hardware design process. How- ever, the limited quality and quantity of available data remain critical barriers to the development of AI-assisted hardware design. In this paper, the authors argue that addressing this fundamental challenge requires decentralized and personalized learning approaches. To this end, we present a two-stage framework featuring a novel hierarchical decentralized training paradigm with metric-based model aggregation for model training, along with personalized inference-time optimizations to enhance deployment efficiency. Comprehensive evaluations on both classical and quantum hardware design tasks demon- strate the effectiveness of our approach. We hope that the benchmarking results of this work will encourage broader engagement from both industrial and individual parties in jointly advancing AI-assisted hardware design. REFERENCES [1] Mehdi Salehi Heydar Abad et al. Hierarchical federated learning across heterogeneous cellular networks. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8866 8870. IEEE, 2020. [2] Alon Albalak et al. A survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. [3] Tianle Cai et al. Medusa: Simple LLM Inference Acceleration Frame- work with Multiple Decoding Heads. arXiv preprint arXiv:2401.10774, 2024. [4] Charlie Campbell, Hao Mark Chen, Wayne Luk, and Hongxiang Fan. Enhancing llm-based quantum code generation with multi- agent optimization and quantum error correction. arXiv preprint arXiv:2504.6375344, 2025. [5] Kaiyan Chang et al. Chipgpt: How far are we from natural language hardware design. arXiv preprint arXiv:2305.14019, 2023. [6] Hao Mark Chen et al. Hardware-aware parallel prompt decoding for memory-efficient acceleration of llm inference. arXiv preprint arXiv:2405.18628, 2024. [7] Hao Mark Chen, Shell Xu Hu, Wayne Luk, Timothy Hospedales, and Hongxiang Fan. Fw-merging: Scaling model merging with frank-wolfe optimization. arXiv preprint arXiv:2503.12649, 2025. [8] Mark Chen et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [9] Nicolas Dupuis et al. Qiskit code assistant: Training llms for generating quantum computing code. arXiv preprint arXiv:2405.19495, 2024. [10] Yanai Elazar et al. What s in my big data? In The Twelfth International Conference on Learning Representations, 2024. [11] Weimin Fu et al. Llm4sechw: Leveraging domain-specific large language model for hardware debugging. In 2023 Asian Hardware Oriented Security and Trust Symposium (AsianHOST), pages 1 6. IEEE, 2023. [12] Jiahao Gai, Hao Chen, Zhican Wang, Hongyu Zhou, Wanru Zhao, Nicholas Lane, and Hongxiang Fan. Exploring code language models for automated hls-based hardware generation: Benchmark, infrastructure and analysis. In Proceedings of the 30th Asia and South Pacific Design Automation Conference, pages 988 994, 2025. [13] Wentao Gao et al. Federated learning as a service for hierarchical edge networks with heterogeneous models. arXiv preprint arXiv:2407.20573, 2024. [14] Nathaniel Hudson et al. Flight: A faas-based framework for complex and hierarchical federated learning. arXiv preprint arXiv:2409.16495, 2024. [15] Gabriel Ilharco et al. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. [16] Juyong Jiang et al. A survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. [17] Haolin Jin et al. From llms to llm-based agents for software engi- neering: A survey of current, challenges and future. arXiv preprint arXiv:2408.02479, 2024. [18] Peter Kairouz et al. Advances and open problems in federated learning. Foundations and trends in machine learning, 14(1 2):1 210, 2021. [19] Viraj Kulkarni et al. Survey of personalization techniques for federated learning. In 2020 fourth world conference on smart trends in systems, security and sustainability (WorldS4), pages 794 797. IEEE, 2020. [20] Yongchan Kwon et al. Datainf: Efficiently estimating data influence in loRA-tuned LLMs and diffusion models. In The Twelfth International Conference on Learning Representations, 2024. [21] Royson Lee et al. Fedl2p: Federated learning to personalize. Advances in Neural Information Processing Systems, 36, 2024. [22] Qinbin Li et al. Federated learning on non-iid data silos: An exper- imental study. In 2022 IEEE 38th international conference on data engineering (ICDE), pages 965 978. IEEE, 2022. [23] Yuhui Li et al. EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. arXiv preprint arXiv:2401.15077, 2024. [24] Paul Pu Liang et al. Think locally, act globally: Federated learning with local and global representations. arXiv preprint arXiv:2001.01523, 2020. [25] Yuchao Liao et al. Are llms any good for high-level synthesis? arXiv preprint arXiv:2408.10428, 2024. [26] Hunter Lightman et al. Let s verify step by step. arXiv preprint arXiv:2305.20050, 2023. [27] Mingjie Liu et al. Verilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pages 1 8. IEEE, 2023. [28] Shang Liu et al. Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 1 5. IEEE, 2024. [29] Anton Lozhkov et al. Starcoder 2 and the stack v2: The next generation, 2024. [30] Yao Lu et al. Rtllm: An open-source benchmark for design rtl generation with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 722 727. IEEE, 2024. [31] Brendan McMahan et al. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273 1282. PMLR, 2017. [32] Xupeng Miao et al. SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification. In ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024. [33] Mohammed Muqeeth et al. Soft merging of experts with adaptive routing. arXiv preprint arXiv:2306.03745, 2023. [34] Dinh C Nguyen et al. Federated learning for internet of things: A comprehensive survey. IEEE Communications Surveys Tutorials, 23(3):1622 1658, 2021. [35] Pian Qi et al. Model aggregation techniques in federated learning: A comprehensive survey. Future Generation Computer Systems, 150:272 293, 2024. [36] Yuxiao Qu et al. Recursive introspection: Teaching language model agents how to self-improve. arXiv preprint arXiv:2407.18219, 2024. [37] Baptiste Roziere et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. [38] Reza Shokri et al. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1310 1321, 2015. [39] Charlie Snell et al. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [40] Canh T Dinh et al. Personalized federated learning with moreau en- velopes. Advances in neural information processing systems, 33:21394 21405, 2020. [41] Shailja Thakur et al. Verigen: A large language model for verilog code generation. ACM Transactions on Design Automation of Electronic Systems, 29(3):1 31, 2024. [42] Yun-Da Tsai et al. Rtlfixer: Automatically fixing rtl syntax errors with large language models. arXiv preprint arXiv:2311.16543, 2023. [43] T Wolf. Huggingface s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [44] Chenwei Xiong et al. Hlspilot: Llm-based high-level synthesis. arXiv preprint arXiv:2408.06810, 2024. [45] Haocheng Xu et al. Optimizing high-level synthesis designs with retrieval-augmented large language models. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 1 5. IEEE, 2024. [46] Zhiyuan Yan et al. Assertllm: Generating and evaluating hardware verification assertions from design specifications via multi-llms. arXiv preprint arXiv:2402.00386, 2024. [47] Enneng Yang et al. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666, 2024. [48] Mang Ye et al. Heterogeneous federated learning: State-of-the-art and research challenges. ACM Computing Surveys, 56(3):1 44, 2023. [49] Le Yu et al. Language models are super mario: Absorbing abilities from homologous models as a free lunch. In Forty-first International Conference on Machine Learning, 2024. [50] Frederic Z Zhang et al. Knowledge composition using task vectors with learned anisotropic scaling. arXiv preprint arXiv:2407.02880, 2024.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nAdvancing AI-assisted Hardware Design with Hierarchical Decentralized Training and Personalized Inference-Time Optimization Hao (Mark) Chen , Zehuan Zhang , Wanru Zhao , Nicholas Lane , Hongxiang Fan Imperial College London, London, UK University of Cambridge, Cambridge, UK {hc1620, zehuan.zhang22, w.luk, {wz341, Abstract Recent years have witnessed a significant increase in the adoption of AI techniques to enhance electronic design automation. In particular, the emergence of Large Language Models (LLMs) has sparked significant interest in LLM-assisted hardware design generation, spanning applications from classi- cal digital circuits to quantum computing. Despite substantial progress in this direction, the quality of LLM-generated hard- ware design still cannot meet the requirements for practical deployment. In this work, we identify three critical challenges hindering the development of LLM-assisted hardware design generation: 1) limited data availability, 2) varied data quality, 3) inadequate inference-time efficiency. To address these fundamen- tal challenges, this paper introduces a two-stage framework for AI-assisted hardware design by exploring decentralized training and personalized inference. In the first stage, we propose to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. To mitigate the impact of low-quality data, we identify optimization opportunities in hardware generation tasks, using user-defined metrics for model aggregation. The second stage focuses on client personalization to enhance both speed and quality. We introduce a new metric, Trueput, to ana- lyze LLM-assisted hardware generation efficiency. To optimize Trueput, we implement personalized inference-time acceleration and customized sampling strategies. Evaluating both classical and quantum benchmarks, our experimental results demonstrate that the proposed two-stage framework can significantly improve the model capability for hardware design generation. As orthogonal enhancements to existing methods, our framework can achieve 33 50 semantic accuracy improvement and 2.3 times speedup, depending on the difficulty of the generation tasks. Both the code and benchmarks will be released publicly to foster further development in this field. I. INTRODUCTION Recent advancements in Large Language Models (LLMs) have demonstrated their great potential in automated software programming [16] and debugging [17]. This impressive capa- bility has sparked significant research and industrial interest in leveraging LLMs to automate hardware design for both classical and quantum domains.\n\n--- Segment 2 ---\nINTRODUCTION Recent advancements in Large Language Models (LLMs) have demonstrated their great potential in automated software programming [16] and debugging [17]. This impressive capa- bility has sparked significant research and industrial interest in leveraging LLMs to automate hardware design for both classical and quantum domains. In classical hardware design, extensive research has targeted RTL design generation [5], [27], [28], [30], [41], High-Level Synthesis (HLS) coding [25], [44], [45], and hardware debugging [11], [42], [46]. In quan- tum design generation, IBM has pioneered the use of LLMs for quantum programming [9], which has been integrated into their Qiskit Code Assistant tool . Although significant research efforts have been devoted to exploring LLM-assisted design generation for both classical Fig. 1: An overview of our proposed two-stage framework for the future of AI-assisted hardware design. and quantum hardware, there are still three key challenges hindering their practical use and deployment: Challenge-1: Limited availability of hardware design sources for training. Due to the low-resource nature of hardware description languages, the amount of publicly accessible classical and quantum design sources is much lower than that of software programs. For example, the size of the Qiskit dataset [9] is more than 1000 times smaller than that of the Python dataset [29]. Challenge-2: Varied quality of training data. High-quality hardware designs are often proprietary and unavailable to the public. The existing training data sourced from public repositories may lack the embedded knowledge necessary for designing high-quality hardware [28]. Challenge-3: Insufficient generation quality. Existing LLM- assisted methods still exhibit limited accuracy in hardware generation tasks [29]. The lack of personalized and cus- tomized optimizations during deployment time further limits the potential of LLM for hardware design generation. Therefore, current LLM-assisted hardware generation remains in its early stages, with practical deployment limited. To fully unleash the potential of generative AI for the future of AI-assisted hardware design, this work proposes a two-stage framework by leveraging decentralized and person- alized learning. To address Challenge-1 of data availability, we aim to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints.\n\n--- Segment 3 ---\nTo fully unleash the potential of generative AI for the future of AI-assisted hardware design, this work proposes a two-stage framework by leveraging decentralized and person- alized learning. To address Challenge-1 of data availability, we aim to harness private domain design sources through a hierarchical decentralized training mechanism that addresses data-sharing constraints. This proposed approach includes recent advancements in federated learning [34] and model merging [47], introducing a novel hierarchical model update mechanism to facilitate broad adoption among users and arXiv:2506.00002v1 [cs.AR] 21 Apr 2025 organizations by accommodating varied hardware capabilities, communication infrastructures, and individual preferences. To tackle Challenge-2 of data quality, we propose a metric-based model aggregation and merging strategy. Although data quality control in the decentralized setting is a challenging task [2], [10], [20], we identify the unique optimization opportunities for hardware design code such as correctness and perfor- mance. To overcome Challenge-3 of generation quality, we propose personalized inference-time optimizations to enhance the generation capability. This includes speed optimization using inference-time multi-token acceleration and quality im- provement through customized output token sampling. As shown in Fig. 1, these optimizations follow the decentralized training process, forming a general two-stage framework for the future of AI-assisted hardware design. Overall, our contributions are summarized as follows: A hierarchical decentralized training paradigm with metric- based model aggregation, facilitating a broader and more diverse pool of participants for collaborative training in AI- assisted hardware design (Sec. IV). Personalized inference-time acceleration with customized sampling strategies, improving both efficiency and design quality of LLM for automatic hardware generation (Sec. V). A comprehensive benchmarking and evaluation of the pro- posed two-stage framework in both classical and quantum hardware design, highlighting the effectiveness and versatil- ity of our approach (Sec. III Sec. VI). II. BACKGROUND AND RELATED WORK A. Decentralized Training Decentralized training distributes the model training across multiple nodes and devices, with only the communication of weights or gradients for model updates. The primary benefits of decentralized training are two-fold: 1) Proprietary data preservation: By maintaining data locally on the client side for training, decentralized training circumvents data-sharing constraints. 2) Compute efficiency: The vast computational resources available on billions of client devices can be utilized for training.\n\n--- Segment 4 ---\nThe primary benefits of decentralized training are two-fold: 1) Proprietary data preservation: By maintaining data locally on the client side for training, decentralized training circumvents data-sharing constraints. 2) Compute efficiency: The vast computational resources available on billions of client devices can be utilized for training. In this paper, we mainly focus on two main- streaming decentralized training: federated learning and model merging. It is worth noting that our proposed framework is general and can be extended to accommodate any decentral- ized training approach. 1) Federated Learning: As a promising approach to achieve decentralized deep learning [38], federated learning [31] has been extensively studied and optimized over the past decade. The key concept of federated learning is to move model training from a central server to distributed client devices. Depending on the structure of parties (e.g. organizations or individual clients), the scale of participants, and data char- acteristics, federated learning is typically categorized into cross-device and cross-silo methods [18]. By performing the training locally on client devices, federated learning period- ically collects and aggregates model updates from different clients. Following the introduction of the classical FedAvg algorithm [31], recent research in federated learning has pri- marily focused on addressing challenges related to data and system heterogeneity [48] to enable practical deployment. 2) Model Merging: With the recent advancements in lan- guage models and the increasing number of open-sourced pre- trained models [43], significant research efforts have been focused on model merging that integrates the weights of multiple different models to enhance the general capability of the merged model without the need to access the original training data [15]. As the data are not shared during model merging, it provides an efficient and flexible way to learn the different expert knowledge by merging multiple domain- specific models. To preserve the generalizability and capacity of the merged model, various techniques have been introduced such as weighted-based merging [50], subspace-based meth- ods [49], and routing-based approaches [33]. More recently, methods designed for efficient and scalable merging of black- box models have also been introduced [7]. B. Optimization of LLM Inference While various techniques have been introduced to improve the inference efficiency of LLM, this paper mainly focuses on optimization techniques that avoid time-consuming re-training and major modifications to the model architecture.\n\n--- Segment 5 ---\nMore recently, methods designed for efficient and scalable merging of black- box models have also been introduced [7]. B. Optimization of LLM Inference While various techniques have been introduced to improve the inference efficiency of LLM, this paper mainly focuses on optimization techniques that avoid time-consuming re-training and major modifications to the model architecture. 1) Speculative Parallel Decoding: Due to the sequential nature of autoregressive inference, LLMs suffer from data dependency and memory-bound performance. To address this, speculative decoding [23], [32] and parallel decoding [3], [6] enable multi-token generation via iterative guess-and-verify strategies. Speculative decoding uses a separate model to draft multiple tokens, while parallel decoding leverages lightweight prompt tokens and embeddings. In both methods, the original model verifies the generated tokens. 2) Inference-Time Scaling Strategy: As LLM training im- provements slow [39], recent work has focused on inference- time optimization. One approach is self-refinement, such as recursive introspection [36], where the model iteratively im- proves its answers based on prior outputs. Another is the search-and-verify paradigm [26], where multiple samples are generated and evaluated by a verifier either a reward model or end-to-end evaluator to select the best solution. C. Related Work Hierarchical training has been investigated in previous re- search on federated learning to address network heterogeneity issues [1], [13], [14]. Distinct from prior approaches, this paper considers diverse clients conditions in the context of hardware design generation and explores a hierarchical decentralized training with a novel hybrid use of federated learning and model merging, encouraging a broader and more diverse pool of participants. Applying client personalization after the training of global models has been investigated in the contexts of federated learning [19]. Different basic fine-tuning approaches have been employed for model personalization, such as regularised fine- tuning [40] and selective parameter method [24]. More ad- vanced techniques, such as meta-learning [21], have also been explored for client personalization. Unlike previous methods, our framework integrates inference-time acceleration with a novel budget-aware sampling strategy driven by our novel metric Trueput. III. FRAMEWORK OVERVIEW AND BENCHMARKS A. Framework Overview An overview of our proposed framework is illustrated in Fig.\n\n--- Segment 6 ---\nFRAMEWORK OVERVIEW AND BENCHMARKS A. Framework Overview An overview of our proposed framework is illustrated in Fig. 1. Designed to leverage private-domain data for model training with privacy considerations while maximizing de- ployment efficiency and performance, our framework mainly consists of two stages: decentralized training and personalized inference-time optimization. These stages can be applied iter- atively to collaboratively enhance the model s capabilities for AI-assisted hardware design. The first stage of decentralized training features a hierarchi- cal mechanism (Sec. IV-A) with hybrid federated learning and model merging, which facilitates a broader and more diverse pool of participants by considering varying hardware capa- bilities, connection restrictions, and individual preferences. In the second stage, different inference-time optimizations are personalized (Sec. V) for each client to enhance the inference speed and generation quality, with different hyperparameters optimized and customized to maximize the deployment per- formance and efficiency for diverse use cases. B. Benchmarks To demonstrate the effectiveness of our approaches, we perform evaluation on two different benchmarks: one for clas- sical hardware and the other for quantum hardware. The code files for both scenarios are sourced from publicly available GitHub repositories with compatible licenses, reflecting real- world data heterogeneity. Classical Hardware Benchmark. To validate the appli- cability of our framework in facilitating classical hardware designs, we evaluate it on a C-based High-Level Synthesis (HLS) benchmark [12] comprising 7437 training samples and 1860 test samples. Each sample consists of a high-level design description and a canonical HLS program. The HLS designs include a wide range of domains, such as Matrix and Linear Algebra Operations, Scientific Simulation, etc. Quantum Hardware Benchmark To evaluate the effective- ness of our approach in quantum hardware generation, we use a Qiskit benchmark [4] with 10896 training samples and 50 test samples. Each training sample contains a Qiskit program with human-written comments, while each test sample consists of a functionality description and a canonical Qiskit program. IV. DECENTRALIZED TRAINING A. Hierarchical Decentralized Training Federated learning has demonstrated its potential for decen- tralized training, with both cross-silo and cross-device settings studied for various user scenarios.\n\n--- Segment 7 ---\nIV. DECENTRALIZED TRAINING A. Hierarchical Decentralized Training Federated learning has demonstrated its potential for decen- tralized training, with both cross-silo and cross-device settings studied for various user scenarios. However, its practicality and effectiveness might decrease when deploying it for clients with poor or unreliable communication. This challenge becomes FL FL Organizations, Institutions, and Clients with Reliable Connections Parties with Communication or Geometric Restrictions FL Phase1: Hybrid Decentralized Training Phase2: Model Merging Fig. 2: The vision and overview of our proposed framework for the future of AI-assisted hardware design. even more pronounced in extra-large-scale collaborative train- ing settings, where geometric and infrastructural restrictions might affect deployment feasibility. Additionally, most feder- ated learning approaches assume a shared network architecture for locally trained models, which becomes impractical in the context of LLM due to the high computational and memory requirements for LLM training on client devices. To promote the broader adoption of decentralized training for foundation models in AI-assisted hardware design, this paper proposes a hierarchical decentralized training scheme. As illustrated in Fig. 2 and Algorithm 1, our hierarchical decentralized training consists of two tiers. The first tier is referred to as hybrid decentralized training. For clients or organizations with reliable communication channels, feder- ated learning is employed for collaborative training. Multiple clients will employ federated learning within each group independently, resulting in several separately trained federated models. Meanwhile, for parties with isolated environments due to geographical or infrastructural limitations, individual local training is performed. In the second tier, different models with diverse domain knowledge, learned via either federated or local training, are combined together using model merging techniques. This hierarchical, two-tier decentralized training framework enables efficient utilization of private domain data regardless of physical or regulatory restrictions. B. Metric-based Aggregation Adaptive methods like client selection and quality-aware aggregation [35] have been explored in federated learning and model merging, primarily for classification and segmentation tasks. Their effectiveness in generative AI remains underex- plored, largely due to challenges in evaluating generated con- tent. Metrics like perplexity depend on reference outputs and often fail to capture functional equivalence e.g., semantically identical programs with different styles may score differently.\n\n--- Segment 8 ---\nTheir effectiveness in generative AI remains underex- plored, largely due to challenges in evaluating generated con- tent. Metrics like perplexity depend on reference outputs and often fail to capture functional equivalence e.g., semantically identical programs with different styles may score differently. Neural metrics such as LLM-as-Judge offer alternatives but often lack explainability and analytical rigor. To address these challenges, this paper identifies a key optimization opportunity in hardware design generation tasks. Unlike traditional generative tasks, hardware generation in- herently provides quantitative evaluation metrics including design syntax accuracy, hardware functional correctness, and hardware latency that can serve as robust criteria for model Algorithm 1 Hierarchical Decentralized Training 1: Notation 2: C: Set of all clients 3: CF : Subset of clients with reliable communication, parti- tioned into G groups CF g , where g {1, 2, . . . , G} 4: CL: Subset of clients with no reliable communication 5: FL( ): Federated learning function 6: LT( ): Local training function 7: MM( ): Model merging function 8: Mglobal: Final global model after merging 9: Tier 1: Hybrid Decentralized Training 10: for each group CF g in CF do 11: Train model M F g FL(CF g ) Federated Learning 12: end for 13: for each client CL i in CL do 14: Train model M L i LT(CL i ) Local Training 15: end for 16: Tier 2: Model Merging 17: Gather: M {M F 1 , . . . , M F G } {M L i CL i CL} 18: Merge: Mglobal MM(M) 19: Output: Global model Mglobal aggregation and merging. Leveraging this observation, we propose a flexible aggregation framework that enables users to define custom metrics for weighting model contributions. Formally, given the i-th client model Mi from a set of N client models, the final aggregated model Mf is computed as Mf PN i 1 g(Mi) Mi, where g( ) is a user-defined metric applied to a client model to determine its contribution. For instance, g( ) could be defined using syntax accuracy to filter out model weights from clients trained on syntactically incorrect data, thereby ensuring high-quality training data for the aggregated model.\n\n--- Segment 9 ---\nFormally, given the i-th client model Mi from a set of N client models, the final aggregated model Mf is computed as Mf PN i 1 g(Mi) Mi, where g( ) is a user-defined metric applied to a client model to determine its contribution. For instance, g( ) could be defined using syntax accuracy to filter out model weights from clients trained on syntactically incorrect data, thereby ensuring high-quality training data for the aggregated model. It is worth noting that our framework is not restricted to using hardware-specific metrics such as syntax accuracy and functional correctness. The framework is designed to accommodate a wide variety of model aggregation strategies, improving the versatility and facilitating broader adoption of our approach. For example, if g(.) is parameterized as the ratio of client training samples, the aggregation replicates the FedAvg algorithm. By enabling customization of g(. ), our framework caters to diverse requirements across different hardware generation tasks, allowing users to tailor aggregation strategies to their specific needs. V. PERSONALIZED INFERENCE-TIME OPTIMIZATIONS A. Trueput: Efficiency Analysis for Design Generation To analyze the efficiency of LLM-assisted design genera- tion, we propose a new metric, Trueput, which quantifies the number of functionally correct designs generated per unit of time. It is defined as: Trueput Tinf (1) where represents the expected functionality pass rate when k samples are generated, and Tinf denotes the expected inference latency per output design. Next, we analyze Trueput under the constraint of limited computational resources. When batching is employed, the inference latency Tinf is expressed as Tinf(k) since the batch size depends on k. According to the Codex [8], an unbiased estimate of can be written as 1 (1 p)k with functionality pass probability p. Substituting this into the definition of Trueput, we obtain: Trueputbatch 1 (1 p)k Tinf(k) (2) This formulation enables the analysis of efficiency of the inference framework by accounting for both the accuracy of the generated designs and the latency associated with batching during inference. Increasing Trueputbatch requires decreasing the inference time Tinf( ), and improving functionality pass rate p. Given the form in (2), we hypothesize that a global maximum of Trueputbatch exists at some finite value of k, for fixed p and Tinf( ).\n\n--- Segment 10 ---\nWhen batching is employed, the inference latency Tinf is expressed as Tinf(k) since the batch size depends on k. According to the Codex [8], an unbiased estimate of can be written as 1 (1 p)k with functionality pass probability p. Substituting this into the definition of Trueput, we obtain: Trueputbatch 1 (1 p)k Tinf(k) (2) This formulation enables the analysis of efficiency of the inference framework by accounting for both the accuracy of the generated designs and the latency associated with batching during inference. Increasing Trueputbatch requires decreasing the inference time Tinf( ), and improving functionality pass rate p. Given the form in (2), we hypothesize that a global maximum of Trueputbatch exists at some finite value of k, for fixed p and Tinf( ). Therefore, the value of k should be optimized for each client to maximize Trueputbatch. To address these goals, this paper explores personalized test-time optimizations that target both speed enhancement to reduce latency and quality improvement to increase pass rate. B. Inference-Time Speed and Quality Enhancement The scaling law of inference has indicated its potential to improve the performance for most natural language tasks. In this work, we investigate their effectiveness in hardware design generation and propose customization to further enhance their flexibility and efficiency. Customized Quality Improvement. Various test-time op- timizations [39] can enhance output generation quality, with popular methods including Best-of-N sampling and beam search. Since clients have diverse domains, such as classical or quantum, and their focus on designing different hardware architectures, the choice of optimization techniques can vary across different scenarios to maximize the generation quality. Moreover, these techniques introduce multiple hyperparame- ters, presenting a design space for optimization. To leverage this opportunity, our framework enables clients to customize and optimize their sampling strategy and hyperparameters to meet specific requirements, for example, allowing them to balance hardware design quality and generation latency by adjusting the sampling count. Table I presents the test- time optimization strategies supported in our framework with their associated hyperparameters. To tailor the optimization strategy for each client, a grid search can be used to tune the optimization configurations at a fixed compute budget. Personalized Inference-Time Acceleration. Generating an optimized hardware design may require a large number of samples, resulting in high generation latency and energy costs.\n\n--- Segment 11 ---\nPersonalized Inference-Time Acceleration. Generating an optimized hardware design may require a large number of samples, resulting in high generation latency and energy costs. Since the performance of auto-regressive generation in LLM inference is typically memory-bound, several techniques have been introduced to leverage idle compute resources to accelerate LLM inference. Among these are speculative TABLE I: Test-Time Optimization Strategies Sampling Description Hyperparameters Nucleus Sampling Selects tokens with cumulative probability p p (cumulative probability) Temperature Sampling Scales token probabilities by temperature Temperature, Number of generated candidates Top-k Sampling Chooses from the top k most probable tokens k (number of sequences to consider) Beam Search Expands search using a fixed beam width Beam width w o Finetune Model Merging Hierarchical Merging 0 20 40 60 80 100 accuracy ( ) 52.15 0.38 42.31 40.38 91.18 90.11 Syntax Accuracy Semantic Accuracy (a) HLS MachineEval. w o Finetune Model Merging Hierarchical Merging 0 20 40 60 80 100 accuracy ( ) 12.0 2.0 58.0 50.0 92.0 88.0 Syntax Accuracy Semantic Accuracy (b) HLS HumanEval. w o Finetune Model Merging Hierarchical Merging 0 10 20 30 accuracy ( ) 18.0 20.0 22.0 (c) Qiskit Benchmark. Fig. 3: Effect of hierarchical approach on both classical and quantum hardware benchmarks. decoding [23], [32] and parallel decoding [3], [6], which generate multiple tokens in parallel to improve the processing speed. However, most existing approaches rely on a separate training process to learn the multi-token generation capability. In this work, we propose an inference-time learning ap- proach, where each client locally learns acceleration param- eters during the model s deployment phase while serving real user requests. Specifically, we observe that the learning process of multi-token generation involves tuning the accel- eration parameters to approximate the predictive distribution of the original model. Therefore, rather than depending on a training dataset, our approach utilizes the generation outputs produced during deployment, while serving user requests, for learning multi-token generation. This method offers two key benefits.\n\n--- Segment 12 ---\nTherefore, rather than depending on a training dataset, our approach utilizes the generation outputs produced during deployment, while serving user requests, for learning multi-token generation. This method offers two key benefits. First, by leveraging user-generated content as labels, the approach can be seen as an unsupervised learning technique, eliminating the need for extra datasets. Second, the learning process is performed during the model deployment time, avoiding a separate training process to learn multi-token generation. In this paper, we consider parallel decoding ap- proaches as they are more training-efficient compared to other speculative decoding methods, making it suitable for online learning. The training objective is formulated as follows: arg min ϕ Ex D KL Pa(yt 1:t k y1:t, x; ϕ), Po(yt 1:t k y1:t, x; θ) where ϕ are the acceleration parameters, and D is the deployment data distribution. The KL-divergence measures the difference between the Pa distribution for acceleration and target Po distributions. yt 1:t k represents the predicted token sequence, and y1:t the previously generated tokens by target model with parameter θ. VI. EXPERIMENTS A. Evaluation Setup Models and Datasets Our proposed framework is applica- ble to a wide range of machine learning methods. However, w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 20 40 60 80 100 accuracy ( ) 52.15 0.38 71.77 65.75 70.22 64.09 Syntax Accuracy Semantic Accuracy (a) HLS MachineEval. w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 20 40 60 80 100 accuracy ( ) 12.0 2.0 68.0 64.0 64.0 60.0 Syntax Accuracy Semantic Accuracy (b) HLS HumanEval. w o Finetune FL Finetuned Metric(Ratio) FL Finetuned Metric(Acc) 0 10 20 30 accuracy ( ) 18.0 24.0 26.0 (c) Qiskit Benchmark. Fig. 4: Evaluation of federated learning on both classical and quantum hardware benchmarks. due to their growing popularity and practical relevance, we focus on LLMs in our experiments.\n\n--- Segment 13 ---\n4: Evaluation of federated learning on both classical and quantum hardware benchmarks. due to their growing popularity and practical relevance, we focus on LLMs in our experiments. For classical hardware experiments, we use CodeLlama-7B [37] as the base model and the HLS benchmark described in Sec. III-B. This bench- mark contains machine-generated instructions (MachineEval) produced by GPT for HLS generation. To evaluate the model s generalizability, we involve human experts to manually refine 50 samples, creating a HumanEval version. For Qiskit quan- tum design generation, we use StarCoder2-3B [29] with the dataset introduced in Sec. III-B. Federated Learning. For both the classical and quantum benchmarks, we simulate real-world data heterogeneity by training 40 clients on datasets partitioned using a Dirichlet distribution [22], based on the repository IDs of the source code. Each round involves training for one epoch with 10 of the clients participating. Two aggregation metrics were tested: the number of data samples (Ratio) and hardware syntax accuracy (Acc). A separate validation dataset was used to calculate syntax accuracy. Model Merging. Hardware syntax accuracy on a valida- tion dataset was used as the weight for model aggregation for both benchmarks. DARE [49] was used for hierarchical model aggregation. In this setting, the datasets are partitioned in a manner similar to federated learning, with each client performing local training on its own subset of data. Personalized Test-Time Optimization. We adopt parallel decoding [6] for inference-time acceleration. A validation dataset with hardware design instructions is used to simulate user requests. Different sampling strategies and the associated hyperparameters are summarized in Table I. B. Effect of Hierarchical Approach To evaluate the effectiveness of our proposed hierarchical approach, we conduct experiments on both classical and quan- tum benchmarks, as shown in Fig. 3. We compare our method against two baselines: the base model without fine-tuning and a model merging without hierarchical aggregation. For classical hardware generated via HLS, we assess both syntax and semantic accuracy with template generation enhancement. For the quantum benchmark, we primarily focus on semantic accuracy evaluation. In both classical and quantum evalua- tions, our approach demonstrated accuracy improvement. As shown in Fig.\n\n--- Segment 14 ---\nIn both classical and quantum evalua- tions, our approach demonstrated accuracy improvement. As shown in Fig. 3a 3b, the hierarchical approach demonstrates significantly greater improvement for classical hardware gen- eration tasks in both MachineEval and HumanEval, achieving nearly an 80 increase in syntax and semantic accuracy compared to the model without fine-tuning, and approximately 50 over the model obtained through model merging. While improvements on the Qiskit benchmark were less pronounced due to the increased complexity of quantum circuit design, our model still performs comparably to the centrally trained baseline [4]. Its performance can be further enhanced by integrating it into the multi-agent framework proposed in [4], which incorporates Retrieval-Augmented Generation (RAG), Chain-of-Thought (CoT) reasoning, and a semantic analyzer. Training Overhead and Communication Costs: Our hi- erarchical approach significantly cuts communication versus standard FL. Standard FL requires N R central updates (N clients, R rounds). On the other hand, our hierarchical ap- proach confines the frequent communication within G groups (NF L clients total), with only one central merge involving G group models and NL local models (NF L NL N). Thus, central communication drops from O(N R) to O(G NL) transfers, significantly improving scalability and reducing bandwidth needs, as G NL N R typically. This structure efficiently leverages clients idle compute, requiring significantly less compute from central server compared to standard central training. C. Evaluation of Federated Learning Fig. 4 present the evaluation of federated learning on classi- cal and quantum benchmarks using two different aggregation strategies. As shown in Fig. 4c, leveraging hardware syntax ac- curacy during model aggregation achieves the best result in the Qiskit Benchmark. For classical hardware generation as shown in Fig. 4a 4b, Both ratio-based and Acc-based approaches achieve similar results in MachineEval and HumanEval, with up to a 60 increase in semantic accuracy. These findings demonstrate the effectiveness and flexibility of federated learn- ing with metric-based aggregation in training models for both classical and quantum hardware design generation. D. Personalized Inference-Time Optimizations As discussed in Sec. I, the lack of personalized optimiza- tions restricts the potential for maximizing both speed and quality in model inference.\n\n--- Segment 15 ---\nD. Personalized Inference-Time Optimizations As discussed in Sec. I, the lack of personalized optimiza- tions restricts the potential for maximizing both speed and quality in model inference. Thus, we evaluate the impact of two test-time optimizations: multi-token generation for accel- eration and customized sampling for quality improvement. We show that by tailoring the multi-token generation configuration and sampling strategies to the client s compute budget, the optimization achieves 2.3 speedup ratio and up to 46 syntax accuracy improvement over default greedy decoding. In Fig. 5, we evaluate the speedup ratio with respect to the tree size, a parameter reflecting the token parallelism. While the acceptance ratio increases with larger tree sizes, the speedup ratio peaks at a tree size of 60 due to limited idle compute resources. This aligns with previous research [6], which shows that tree size must be optimized per client to achieve maximum speedup. In Fig. 5, we examine how different sampling strate- gies affect syntax accuracy. Beam search performs best with sample sizes under 3, while combined sampling outperforms others for larger sample sizes. Our findings emphasize the need 60 75 90 105 120 Tree Size 2.5 2.6 2.7 2.8 2.9 Acceptance Ratio Acceptance Ratio 1.9 2.0 2.1 2.2 2.3 2.4 Speedup Ratio Speedup Ratio 1 2 3 4 5 Sample Number 20 30 40 50 60 70 Syntax Accuracy ( ) Temperature Sampling Combined Sampling Top-k Sampling Greedy Sampling Top-p Sampling Beam Search Fig. 5: Personalized Test-Time Optimization. Left: multi- token generation. Right: Customized sampling for HLS mod- els. Combined Sampling uses both top-k and top-p filtering. 1 2 3 4 5 6 7 8 9 10 Sample Number 35 40 45 50 55 60 ( ) Sampling Latency (full GPU Capacity) Sampling Latency (20 GPU Capacity) 13 15 17 19 21 23 25 27 29 31 Latency (s) 1 2 3 4 5 6 7 8 9 10 Sample Number 1.5 2.0 2.5 3.0 3.5 4.0 4.5 Trueput (design s) 4.08 2.91 full GPU capacity 20 GPU capacity Fig. 6: Trueput Evaluation. Left: and latency for different sample sizes.\n\n--- Segment 16 ---\n6: Trueput Evaluation. Left: and latency for different sample sizes. Right: Trueput across GPU capacities. for personalized inference-time optimizations to advance AI- assisted hardware design. The optimal configuration for each client can be determined offline with a one-time computation, avoiding additional system complexity and compute overhead. E. Effect of Sample Number on Trueput Optimization In Sec. V, we introduce Trueput to analyze the efficiency of design generation. To maximize Trueputbatch, in addition to the previously mentioned personalized test-time optimization, it is important to search for the optimal sample number k, as shown in (2). As depicted in Fig. 6, both and sampling latency increase with the sample number k, but with different rates. Fig. 6 shows that a local maximum of Trueputbatch exists, and the optimal value of k varies depending on the GPU capacity. This observation highlights that the sample number should be optimized per client to achieve the maximum Trueput. VII. CONCLUSION Recent advancements in AI have shown great potential in revolutionizing the traditional hardware design process. How- ever, the limited quality and quantity of available data remain critical barriers to the development of AI-assisted hardware design. In this paper, the authors argue that addressing this fundamental challenge requires decentralized and personalized learning approaches. To this end, we present a two-stage framework featuring a novel hierarchical decentralized training paradigm with metric-based model aggregation for model training, along with personalized inference-time optimizations to enhance deployment efficiency. Comprehensive evaluations on both classical and quantum hardware design tasks demon- strate the effectiveness of our approach. We hope that the benchmarking results of this work will encourage broader engagement from both industrial and individual parties in jointly advancing AI-assisted hardware design. REFERENCES [1] Mehdi Salehi Heydar Abad et al. Hierarchical federated learning across heterogeneous cellular networks. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8866 8870. IEEE, 2020. [2] Alon Albalak et al. A survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. [3] Tianle Cai et al. Medusa: Simple LLM Inference Acceleration Frame- work with Multiple Decoding Heads. arXiv preprint arXiv:2401.10774, 2024.\n\n--- Segment 17 ---\nMedusa: Simple LLM Inference Acceleration Frame- work with Multiple Decoding Heads. arXiv preprint arXiv:2401.10774, 2024. [4] Charlie Campbell, Hao Mark Chen, Wayne Luk, and Hongxiang Fan. Enhancing llm-based quantum code generation with multi- agent optimization and quantum error correction. arXiv preprint arXiv:2504.6375344, 2025. [5] Kaiyan Chang et al. Chipgpt: How far are we from natural language hardware design. arXiv preprint arXiv:2305.14019, 2023. [6] Hao Mark Chen et al. Hardware-aware parallel prompt decoding for memory-efficient acceleration of llm inference. arXiv preprint arXiv:2405.18628, 2024. [7] Hao Mark Chen, Shell Xu Hu, Wayne Luk, Timothy Hospedales, and Hongxiang Fan. Fw-merging: Scaling model merging with frank-wolfe optimization. arXiv preprint arXiv:2503.12649, 2025. [8] Mark Chen et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [9] Nicolas Dupuis et al. Qiskit code assistant: Training llms for generating quantum computing code. arXiv preprint arXiv:2405.19495, 2024. [10] Yanai Elazar et al. What s in my big data? In The Twelfth International Conference on Learning Representations, 2024. [11] Weimin Fu et al. Llm4sechw: Leveraging domain-specific large language model for hardware debugging. In 2023 Asian Hardware Oriented Security and Trust Symposium (AsianHOST), pages 1 6. IEEE, 2023. [12] Jiahao Gai, Hao Chen, Zhican Wang, Hongyu Zhou, Wanru Zhao, Nicholas Lane, and Hongxiang Fan. Exploring code language models for automated hls-based hardware generation: Benchmark, infrastructure and analysis. In Proceedings of the 30th Asia and South Pacific Design Automation Conference, pages 988 994, 2025. [13] Wentao Gao et al.\n\n--- Segment 18 ---\nIn Proceedings of the 30th Asia and South Pacific Design Automation Conference, pages 988 994, 2025. [13] Wentao Gao et al. Federated learning as a service for hierarchical edge networks with heterogeneous models. arXiv preprint arXiv:2407.20573, 2024. [14] Nathaniel Hudson et al. Flight: A faas-based framework for complex and hierarchical federated learning. arXiv preprint arXiv:2409.16495, 2024. [15] Gabriel Ilharco et al. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. [16] Juyong Jiang et al. A survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. [17] Haolin Jin et al. From llms to llm-based agents for software engi- neering: A survey of current, challenges and future. arXiv preprint arXiv:2408.02479, 2024. [18] Peter Kairouz et al. Advances and open problems in federated learning. Foundations and trends in machine learning, 14(1 2):1 210, 2021. [19] Viraj Kulkarni et al. Survey of personalization techniques for federated learning. In 2020 fourth world conference on smart trends in systems, security and sustainability (WorldS4), pages 794 797. IEEE, 2020. [20] Yongchan Kwon et al. Datainf: Efficiently estimating data influence in loRA-tuned LLMs and diffusion models. In The Twelfth International Conference on Learning Representations, 2024. [21] Royson Lee et al. Fedl2p: Federated learning to personalize. Advances in Neural Information Processing Systems, 36, 2024. [22] Qinbin Li et al. Federated learning on non-iid data silos: An exper- imental study. In 2022 IEEE 38th international conference on data engineering (ICDE), pages 965 978. IEEE, 2022. [23] Yuhui Li et al. EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. arXiv preprint arXiv:2401.15077, 2024.\n\n--- Segment 19 ---\nEAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty. arXiv preprint arXiv:2401.15077, 2024. [24] Paul Pu Liang et al. Think locally, act globally: Federated learning with local and global representations. arXiv preprint arXiv:2001.01523, 2020. [25] Yuchao Liao et al. Are llms any good for high-level synthesis? arXiv preprint arXiv:2408.10428, 2024. [26] Hunter Lightman et al. Let s verify step by step. arXiv preprint arXiv:2305.20050, 2023. [27] Mingjie Liu et al. Verilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pages 1 8. IEEE, 2023. [28] Shang Liu et al. Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 1 5. IEEE, 2024. [29] Anton Lozhkov et al. Starcoder 2 and the stack v2: The next generation, 2024. [30] Yao Lu et al. Rtllm: An open-source benchmark for design rtl generation with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 722 727. IEEE, 2024. [31] Brendan McMahan et al. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pages 1273 1282. PMLR, 2017. [32] Xupeng Miao et al. SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification. In ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024. [33] Mohammed Muqeeth et al. Soft merging of experts with adaptive routing. arXiv preprint arXiv:2306.03745, 2023. [34] Dinh C Nguyen et al. Federated learning for internet of things: A comprehensive survey.\n\n--- Segment 20 ---\n[34] Dinh C Nguyen et al. Federated learning for internet of things: A comprehensive survey. IEEE Communications Surveys Tutorials, 23(3):1622 1658, 2021. [35] Pian Qi et al. Model aggregation techniques in federated learning: A comprehensive survey. Future Generation Computer Systems, 150:272 293, 2024. [36] Yuxiao Qu et al. Recursive introspection: Teaching language model agents how to self-improve. arXiv preprint arXiv:2407.18219, 2024. [37] Baptiste Roziere et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. [38] Reza Shokri et al. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1310 1321, 2015. [39] Charlie Snell et al. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [40] Canh T Dinh et al. Personalized federated learning with moreau en- velopes. Advances in neural information processing systems, 33:21394 21405, 2020. [41] Shailja Thakur et al. Verigen: A large language model for verilog code generation. ACM Transactions on Design Automation of Electronic Systems, 29(3):1 31, 2024. [42] Yun-Da Tsai et al. Rtlfixer: Automatically fixing rtl syntax errors with large language models. arXiv preprint arXiv:2311.16543, 2023. [43] T Wolf. Huggingface s transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. [44] Chenwei Xiong et al. Hlspilot: Llm-based high-level synthesis. arXiv preprint arXiv:2408.06810, 2024. [45] Haocheng Xu et al. Optimizing high-level synthesis designs with retrieval-augmented large language models.\n\n--- Segment 21 ---\n[45] Haocheng Xu et al. Optimizing high-level synthesis designs with retrieval-augmented large language models. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 1 5. IEEE, 2024. [46] Zhiyuan Yan et al. Assertllm: Generating and evaluating hardware verification assertions from design specifications via multi-llms. arXiv preprint arXiv:2402.00386, 2024. [47] Enneng Yang et al. Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities. arXiv preprint arXiv:2408.07666, 2024. [48] Mang Ye et al. Heterogeneous federated learning: State-of-the-art and research challenges. ACM Computing Surveys, 56(3):1 44, 2023. [49] Le Yu et al. Language models are super mario: Absorbing abilities from homologous models as a free lunch. In Forty-first International Conference on Machine Learning, 2024. [50] Frederic Z Zhang et al. Knowledge composition using task vectors with learned anisotropic scaling. arXiv preprint arXiv:2407.02880, 2024.\n\n