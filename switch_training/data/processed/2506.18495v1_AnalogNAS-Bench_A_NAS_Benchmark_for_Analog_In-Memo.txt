=== ORIGINAL PDF: 2506.18495v1_AnalogNAS-Bench_A_NAS_Benchmark_for_Analog_In-Memo.pdf ===\n\nRaw text length: 51980 characters\nCleaned text length: 51756 characters\nNumber of segments: 36\n\n=== CLEANED TEXT ===\n\narXiv:2506.18495v1 [cs.LG] 23 Jun 2025 AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing Aniss Bessalah1, Hatem Mohamed Abdelmoumen1, Karima Benatchba1, 2 Hadjer Benmeziane3 Equal contribution. 1Ecole Nationale Sup√©rieure d Informatique, 16309 Oued Smar, Algiers, Algeria 2Laboratoire de M√©thodes de Conception des Syst√®mes, 16309 Oued Smar, Algiers, Algeria 3IBM Research Europe, 8803 R√ºschlikon, Switzerland Abstract Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm for accelerating Deep Neural Networks (DNNs), offering significant energy and latency benefits over conventional digital hardware. However, state-of-the-art neural networks are not inherently designed for AIMC, as they fail to account for its unique non-idealities. Neural Architecture Search (NAS) is thus needed to systematically discover neural architectures optimized explicitly for AIMC constraints. However, comparing NAS methodologies and extracting insights about robust architectures for AIMC requires a dedicated NAS benchmark that explicitly accounts for AIMC-specific hardware non-idealities. To address this, we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for AIMC. Our study reveals three key insights: (1) standard quantization techniques fail to capture AIMC- specific noises, (2) robust architectures tend to feature wider and branched blocks, (3) skip connections improve resilience to temporal drift noise. These insights highlight the limitations of current NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the implementations used in this paper can be found at analog-nas tree main analognasbench. 1 Introduction DNNs deployment is increasingly constrained by power consumption and memory bandwidth limitations. Analog In-Memory Computing (AIMC) (Sebastian et al., 2020) has emerged as a promising alternative to traditional digital hardware by performing computations directly within memory arrays, reducing data movement overhead and significantly improving energy efficiency and computational throughput. By leveraging memory devices, AIMC enables matrix-vector multiplications to be executed in a single step (Lammie et al., 2022), offering orders-of-magnitude improvements in energy efficiency compared to conventional Von Neumann platforms. Despite its advantages, AIMC introduces several non-idealities that impact DNN performance (Boybat et al., 2021). Unlike digital accelerators, which rely on precise arithmetic operations, AIMC suffers from device-to-device variations, cycle-to-cycle noise, temporal drift, and limited precision due to analog-digital conversion. These imperfections lead to unpredictable accuracy degradation when deploying conventional DNNs on AIMC platforms. As a result, models that perform well in digital settings often fail to retain their accuracy when executed on AIMC. To address this challenge, researchers have explored two main directions. One approach to mitigate AIMC non-idealities effect is to modify the training process by injecting hardware-specific noise and variations into the learning pipeline. This technique, commonly known as Hardware- Aware Training (HWT) (Rasch et al., 2023), allows the model to adapt to AIMC-induced errors by learning robust representations. An alternative strategy is to design DNNs that are inherently robust to AIMC noises. Neural Architecture Search (NAS) (Elsken et al., 2019) automates the Preprint. Under review. 2025 the authors, released under CC BY 4.0 discovery of efficient network topologies by searching for architectures that maximize accuracy while considering hardware constraints. However, despite the progress in HWT and the numerous analog-aware NAS methodologies, fundamental questions remain: (1) What architectural characteristics enable certain networks to recover their digital accuracy after HWT, while others do not? (2) Can NAS methodologies developed for quantization also be leveraged for AIMC, thereby avoiding the need for costly HWT? (3) How to systematically compare among NAS methodologies for AIMC? Answering these questions requires a systematic evaluation of different architectures under AIMC- specific constraints. Existing NAS benchmarks, such as NAS-Bench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong and Yang, 2020), typically evaluate architectures based on digital accuracy, FLOP count, and latency, but they do not incorporate HWT, which is crucial for assessing model robustness under AIMC conditions. As a result, researchers lack a standardized benchmark to analyze which architectures best withstand AIMC-induced errors and compare AIMC-targeted NAS methodologies. To bridge this gap, we introduce AnalogNAS-Bench, the first NAS benchmark specifically designed for AIMC. Our benchmark enables a structured evaluation of architectures under AIMC constraints, using AIHWKit (Gallo et al., 2023), and provides insights into architectural robustness when combined with HWT. Our key contributions are: 1. We build AnalogNAS-Bench by extending NAS-Bench-201 and incorporating AIMC-specific constraints, allowing for a fair comparison of different architectures under HWT. This provides a standardized framework for evaluating AIMC-aware NAS methodologies. 2. We analyze the limitations of standard Post-Training Quantization (PTQ) (Banner et al., 2019) and Quantization-Aware Training (QAT) (Jacob et al., 2018) in preserving network rankings under AIMC-induced noise, demonstrating that AIMC non-idealities introduce additional challenges that these methods do not address. 3. Through systematic analysis on CIFAR-10 (Krizhevsky and Hinton, 2009), we extract key archi- tectural insights that contribute to AIMC resilience. We study what makes certain architectures more robust to AIMC-induced errors, investigating factors such as network topology, connectiv- ity patterns, and operator choices. Our findings provide guidelines for designing AIMC-optimized architectures and improving NAS methodologies for analog computing. The remainder of the paper is structured as follows: we first describe the AnalogNAS-Bench benchmark in Section 2. We then analyze architectural robustness and extract key insights in Section 3, followed by an initial comparison of NAS methodologies in Section 4. Finally, we discuss limitations and future work in Section 5. 2 AnalogNAS-Bench Description We construct our benchmark based on the search space of NAS-Bench-201 (Dong and Yang, 2020), a widely adopted NAS benchmark. A full description can be found in Section B of the appendix. This search space is particularly well-suited for studying AIMC-based inference due to its small but diverse range of architectures. Despite its architectural diversity, most models achieve high accuracy in standard digital hardware with low variance (median 90.41 , IQR 87.45 91.82 ). This consistency allows for meaningful comparisons when evaluating the impact of analog constraints and supports the choice of analyzing this search space. To systematically assess accuracy degradation and improvements under AIMC settings, we define the following metrics, summarized in Figure 1: Baseline Accuracy represents an architecture s performance in a full-precision digital setting without hardware constraints. This metric serves as an upper bound. 2 Quantization Configuration Full Training Post-Training Quantization Quantization-Aware Training Noisy evaluation Hardware-Aware Training Hardware Configuration Baseline Accuracy PTQ Accuracy QAT Accuracy Noisy Accuracy Noisy Drift Accuracy Analog Drift Accuracy Analog Accuracy Sample architecture NAS-Bench-201 Search Space Direct evaluation on testset AnalogNAS-Bench queryable metrics Temporal Drift Simulation Durations: 60s, 1h, 1d, 30d Average of 25 noisy evaluations to ensure consistency Trained weights Figure 1: Overview of AnalogNAS-Benchmarking process. Noisy Accuracy measures the accuracy degradation that occurs when an architecture is deployed on AIMC hardware without HWT. We compute this by mapping digital-trained weights onto an AIMC crossbar and simulating inference using IBM s AIHWKit (Gallo et al., 2023), which incorporates noise sources from real memristive devices. Analog Accuracy quantifies the performance of an architecture trained with hardware-aware optimization, where AIMC-specific noise and non-idealities are incorporated into the training process. Prior research (Joshi et al., 2019; Rasch et al., 2023; Yang et al., 2022) has shown that integrating AIMC constraints during training enables models to develop compensatory mecha- nisms, enhancing robustness against hardware-induced distortions. The Analog Accuracy metric thus serves as an indicator of the effectiveness of HWT in mitigating AIMC-induced errors and optimizing performance in analog computing environments. In addition to static accuracy metrics, we evaluate model robustness under drift conditions. Noisy Drift represents the accuracy of a model deployed directly onto AIMC hardware and subjected to drift effects over different time intervals. The degradation is measured at 60 seconds, 1 hour, 24 hours, and 30 days. Similarly, Analog Drift quantifies the accuracy degradation for models trained with HWT. Users can easily use our API to query the results of any architecture. 3 Analysis Insights In this section, we analyze the performance of neural network architectures on AIMC, focusing on key hardware-induced constraints: quantization, analog noise, and drift. We examine how these factors affect accuracy, assess the role of HWT in mitigating performance degradation, and identify architectural characteristics that contribute to robustness. We aim to define the structural features that enable architectures to perform reliably under AIMC constraints. We present here the main insights, more analysis can be found in Section F of the appendix. 3.1 Architecture Ranking To evaluate architectural performance across different training and deployment settings, we analyze accuracy rankings and correlations (Figure 2). Noisy performance shows significant variability, 3 Baseline PTQ QAT Noisy Analog Training Setup 0.0 20.0 40.0 60.0 80.0 100.0 Accuracy ( ) (a) Baseline PTQ QAT Noisy Analog Baseline PTQ QAT Noisy Analog 1.00 0.95 0.87 0.33 0.66 0.95 1.00 0.86 0.34 0.65 0.87 0.86 1.00 0.34 0.63 0.33 0.34 0.34 1.00 0.43 0.66 0.65 0.63 0.43 1.00 (b) 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Figure 2: (a) Accuracy distribution and (b) Kendall s tau correlation heatmap across Baseline, PTQ, QAT, Noisy, and Analog training setups on CIFAR-10. with a large accuracy spread (std 25.47 ) and a lower median (60.70 ). The interquartile range (33.99 75.39 ) highlights substantial robustness differences across architectures. Weak correlation between noisy accuracy and baseline accuracies (ùúè 0.33) suggests that performance on standard digital hardware does not reliably predict AIMC behavior. Similarly, quantization robustness does not imply AIMC robustness, as PTQ and QAT also show weak correlation with noisy accuracy (ùúè 0.34). These results indicate that analog noise is the primary factor driving performance degradation. This decline, however, is mitigated as HWT improves accuracy significantly, raising the mean to 81.31 and the median to 85.49 . The stronger correlation between baseline and analog accuracies (ùúè 0.66) confirms that HWT reduces noise-induced losses. However, analog accuracy remains below baseline levels (IQR 81.70 87.55 vs. 87.45 91.82 ), showing that AIMC noise imposes fundamental constraints that HWT cannot fully overcome in certain architectures. 3.2 Noise Robustness Analysis To identify architectures that are naturally robust to analog noise, we analyze its direct impact on accuracy. Specifically, we focus on architectures that achieve high baseline accuracy ( 90 ) and evaluate how their accuracy degrades under AIMC noise. We define noisy drop as the difference between baseline and noisy accuracies. Architectures are then categorized as robust or non-robust based on a threshold set by the lower quartile (12.75 ) of the noisy drop. Operation Distribution. Figure 3 shows the mean percentage of each operation type in robust and non-robust architectures. A clear distinction emerges: robust architectures contain far fewer 1 1 convolutions (9.50 vs. 26.10 in non-robust ones), as these operations utilize less of the crossbar array, making them more susceptible to additional noise. Instead, they favor 3 3 convolutions (32.18 ), which leverage larger crossbar regions. Additionally, higher occurrences of average pooling (19.88 ) and skip connections (20.93 ) suggest that pooling helps reduce noisy features, while skip connections duplicate the information of the actual input and emphasize its importance. Operation Counts. Beyond these general trends, robustness is strongly linked to the specific counts of each operation. Figure 4 presents the proportion of architecture groups for varying occurrences of each operation. When architectures lack 1 1 convolutions, more than half are robust, but robustness quickly declines as their count increases. In contrast, adding multiple 3 3 convolutions consistently improves robustness, though the effect plateaus beyond four. Pooling and skip connections also exhibit an optimal range: architectures with two or three of either operation 4 20.9 17.5 32.2 9.5 19.9 (a) 16.9 16.2 28.1 26.1 12.6 (b) skip_connect zeroize nor_conv_3x3 nor_conv_1x1 avg_pool_3x3 Figure 3: Percentage of each operation type in (a) robust and (b) non-robust architectures. 0 1 2 3 4 0 20 40 60 80 100 Percentage (a) 0 1 2 3 4 5 (b) 0 1 2 3 4 5 6 (c) 0 1 2 3 4 5 (d) 0 1 2 3 4 (e) Operation Count Robust Non-Robust Figure 4: Distribution of operation counts for robust and non-robust architectures. From left to right: (a) skip connection, (b) zeroize, (c) 3 3 convolution, (d) 1 1 convolution, and (e) 3 3 average pooling. tend to be more robust, while having too few or too many reduces performance. Meanwhile, the zeroize operation shows no clear impact, suggesting sparsity alone does not determine robustness. Sequential Patterns. Operation counts alone do not fully explain robustness, as architectures with identical compositions can exhibit vastly different robustness levels. This suggests that the order and positioning of operations, beyond just their frequencies, play a crucial role. An example of this effect is shown in Figure 5. To better understand these sequential patterns, we examine the most frequent one-, two-, and three-operation paths in robust and non-robust architectures. Robust pathways consistently feature 3 3 convolutions combined with skip connections or pooling layers, as seen in sequences like (2,0), (2,4), (2,2,4), and (2,0,2). In contrast, non-robust pathways frequently include 1 1 convolutions, appearing in patterns like (2,3), (3,2,3), and (2,3,3), indicating that their presence propagates instability even when paired with other beneficial operations. 3.3 Hardware-Aware Training Robustness To evaluate the impact of HWT on robustness, we analyzed the performance shift from noisy infer- ence to analog execution. The majority of architectures achieved strong performance post-HWT, with 80.19 achieving an analog accuracy above 80 . Most architectures benefited significantly, with an average improvement of 134.17 , and 73.81 of them showed substantial gains. However, some architectures remained highly susceptible to analog noise despite HWT interventions. HWT vs. Natural Robustness. To assess whether architectures that naturally resist analog noise perform better than those that significantly improve through HWT, we categorized architectures 5 3x3 avg pool 3x3 avg pool skip 1x1 conv 3x3 conv 3x3 conv 3x3 conv 1x1 conv 3x3 avg pool skip 3x3 avg pool 3x3 conv Figure 5: Two architectures with identical operation counts but different robustness. The left architec- ture (2, 3, 0, 2, 4, 4) achieves high noisy accuracy (86.02 ), while the right one (4, 4, 3, 2, 2, 0) performs poorly (only 44.27 ). 0 10 20 30 40 50 (3,) (1,) (0,) (4,) (2,) (a) 0.0 2.5 5.0 7.5 10.0 12.5 (4, 2) (2, 4) (2, 2) (0, 2) (2, 0) (b) 0.0 0.5 1.0 1.5 2.0 2.5 (2, 2, 0) (0, 2, 0) (2, 0, 4) (2, 4, 2) (2, 2, 2) (2, 4, 0) (2, 0, 2) (4, 2, 2) (2, 0, 0) (2, 2, 4) (c) Percentage Paths Figure 6: Most common paths of length (a) one, (b) two and (c) three in robust architectures. Oper- ations are labeled as follows: (0) skip connection, (1) zeroize, (2) 3 3 convolution, (3) 1 1 convolution, and (4) 3 3 average pooling. into three groups based on their noisy accuracy: Naturally robust ( 70 ), Non-Robust ( 20 ), and Moderate (20 70 ). We then evaluated their analog accuracy (post-HWT), setting a threshold of 80 to define high-performing architectures (Figure 7). Naturally robust architectures maintained strong performance, with 82.88 exceeding 85 analog accuracy and requiring minimal improvement ( 10 ) due to their inherent resilience. In contrast, non-robust architectures exhibited the highest variance (mean 65.72 , std 23.53 ), with some failing entirely. However, 34.41 of this group saw dramatic gains (mean improvement 495.99 ), though most remained within 80 85 range, , and only 4.63 surpassed the most frequent analog accuracy observed in the naturally robust group. The moderate group showed mixed results: 86.67 crossed the 80 threshold, while some remained below 70 . Ultimately, naturally robust architectures are the most reliable, as they consistently perform well with minimal intervention, whereas HWT does not always recover all noise-sensitive architectures. HWT-Failing Architectures. We examined the subset of the most underperforming architectures: those that initially performed well on standard hardware (baseline accuracy 90 ) but degraded severely under noise (noisy accuracy 20 ) and failed to recover post-HWT (analog accuracy 40 ). These architectures shared two key traits. First, they exhibited excessive reliance on 1 1 convolutions (31.25 ) while underutilizing 3 3 convolutions (13.33 ), particularly in early-stage connections, resulting in weak noise suppression due to insufficient filtering. Second, they relied heavily on non-learnable operations, which in many cases formed entire paths on their own (33.75 ), particularly in short pathways dominated by skip connections (40 ) and average pooling (37.5 ). Longer sequences also combined these non-learnable operations with 1 1 convolutions (37.75 ), offering little resistance to noise propagation. 6 0 10 20 30 40 50 60 70 80 90 Noisy Accuracy ( ) 0 20 40 60 80 100 Analog Accuracy ( ) Non-Robust Moderate Robust Figure 7: Impact of HWT on accuracy across different architecture categories. Architectures are grouped as Non-Robust, Moderate, or Naturally Robust. The dashed line at 80 marks the high analog accuracy threshold. 3.4 Temporal Drift To assess the impact of temporal drift on neural architectures, we first establish accuracy degradation thresholds for different time durations. Specifically, we analyze performance drops at 60 seconds, 1 hour, 1 day, and 30 days under two distinct conditions: Noisy Drift (i.e., without HWT) and Analog Drift (i.e., with HWT). These thresholds are determined statistically by computing key distribution metrics, including the mean, 25th percentile, and 75th percentile of the accuracy drop of only architectures that perform well in digital (Baseline Accuracy 80) and are noise resilient (Noisy Accuracy 70). Architectures that maintain accuracy within these thresholds are classified as robust, while those exhibiting significant degradation are labeled non-robust. Full statistics and thresholds can be found in Section E of the appendix. Noisy Drift over time. Robust architectures show a consistent dominance of 3x3 convolutions, which become even more prominent as the duration increases. The skip connection operation also remains significant, indicating that robust architectures favor a balance between feature extraction and identity mapping. In contrast, 1x1 convolutions appear minimally in robust archi- tectures, suggesting that fine-grained transformations are less useful for maintaining robustness. However, in non-robust architectures, 1x1 convolutions and zeroize operations tend to be more present. Interestingly, as time progresses, robust architectures seem to increase their reliance on 3x3 convolutions while decreasing their use of 1x1 convolutions. This aligns with the hypothesis that wider receptive fields contribute to robustness, as 3x3 convolutions can better capture spatial correlations, making the model less susceptible to noise. Analog Drift over time. Figure 8 demonstrates that robust architectures show a higher presence of skip connections and 3 3 convolutions, suggesting that these operations contribute to long- term stability. The dominance of 3 3 convolutions indicates that spatial feature extraction plays a crucial role in resilience to analog drift. In contrast, 1 1 convolutions remain minimal. On the other hand, Non-robust architectures exhibit a higher presence of average pooling and zeroize operations both of which lack trainable parameters. This reliance on non-learnable operations likely makes them more vulnerable to analog drift, as HWT offers no direct mechanism to counterbalance the absence of weight adaptability. While both robust and non-robust architectures maintain similar patterns, the gap in operation presence increases, suggesting that robustness characteristics become more pronounced over longer periods. 7 0 1 2 3 4 0 5 10 15 20 25 30 35 Operation Percentage ( ) (a) 0 1 2 3 4 (b) Operations Robust Non-Robust Figure 8: Operation presence over time in (a) Robust and (b) Non-Robust architectures trained with HWT. Operations are: (0) skip connection, (1) zeroize, (2) 3 3 convolution, (3) 1 1 convolu- tion, and (4) 3 3 average pooling. 4 Initial Comparison In this section, we present an initial benchmark evaluating standard NAS methodologies using AnalogNAS-Bench. Table 1 summarizes the performance of the best architectures identified by various search strategies, primarily focusing on maximizing the 1-day accuracy. This comparison establishes a baseline for NAS methods tailored specifically to AIMC-based hardware. Except for BANANAS (White et al., 2021), NAS4RRAM (Yuan et al., 2021), NACIM (Jiang et al., 2021), and Gibbon (Sun et al., 2023) which employ their own specialized surrogate modeling techniques all other methods utilize an XGBoost (Chen and Guestrin, 2016) surrogate model trained on a dataset comprising 900 architectures from the AnalogNAS-Bench search space. The surrogate approach aims to capture realistic search times in practical scenarios. The results in Table 1 demonstrate that among AIMC-specific methods, AnalogNAS (Ben- meziane et al., 2023) and GA for IMC AI hardware (Krestinskaya et al., 2020) achieve the highest 1-day accuracy (90.04 ), nearly matching the performance of the optimal architecture obtained via exhaustive search (90.53 ). General NAS methods, including Random Search and Evolutionary Algorithms, also achieve competitive baseline accuracies; however, they tend to exhibit higher variability in accuracy over one month (AVM). Notably, Bayesian Optimization and BANANAS show lower accuracies and higher AVM, highlighting their limited effectiveness when analog objectives are considered. AIMC-specific approaches consistently demonstrate improved stability and higher robustness against noise, reinforcing the importance of analog-aware search strategies. These findings highlight a promising direction for future research: developing more suitable estimation strategies that simultaneously consider stability (AVM) and robustness (1-day accuracy), thereby potentially enhancing the effectiveness of NAS methods tailored specifically for AIMC hardware. 5 Limitations Future work While the AnalogNAS-Bench provides a comprehensive platform to analyze convolutional neural networks under AIMC constraints, several limitations must be highlighted. First, the current benchmark evaluations are fully completed only on the CIFAR-10 dataset. Although experiments with CIFAR-100 (Krizhevsky and Hinton, 2009) and ImageNet16-120 (Chrabaszcz et al., 2017) are actively underway, the generalization of insights and robustness trends across more complex and diverse datasets remains an essential next step. Moreover, the current benchmark contains a 8 Table 1: Benchmark Comparison of NAS Methodologies on AnalogNAS-Bench Method Baseline Acc. Noisy Acc. 1-day Acc. AVM Parameters (K) Search Time (s) Best Architecture Exhaustive Search 93.46 82.27 90.53 0.28 0.79 0.03 802 - General NAS Methods Random Search 93.34 85.10 89.93 0.25 3.71 0.14 802 201 Evolutionary Algorithm 93.32 82.21 89.34 0.12 0.59 0.11 834 621 Bayesian Optimization 90.21 80.10 82.65 1.54 6.43 1.07 766 764 BANANAS (White et al., 2021) 91.31 80.24 85.97 1.12 5.91 0.87 803 872 AIMC-Specific Methods AnalogNAS (Benmeziane et al., 2023) 93.12 84.37 90.04 0.31 2.13 0.22 567 530 NAS4RRAM (Yuan et al., 2021) 92.45 83.51 89.12 0.42 1.97 0.19 856 612 GA for IMC AI hardware (Krestinskaya et al., 2020) 93.12 84.37 90.04 0.31 2.13 0.22 567 689 NACIM (Jiang et al., 2021) 92.65 82.95 89.30 0.45 1.11 0.20 877 725 Gibbon (Sun et al., 2023) 92.79 83.04 89.45 0.40 1.34 0.18 936 698 AVM: Accuracy Variation over one Month NACIM and Gibbon jointly optimize network and hardware parameters. Hardware optimization is omitted from this comparison. relatively small set of architectures (derived from NAS-Bench-201). While beneficial for systematic analysis, this limited search space also creates an opportunity: the dataset size is suitable for developing zero-cost estimation methods to quickly predict architecture robustness against analog noise, facilitating exploration of substantially larger and more diverse search spaces in the future. Second, our benchmark is limited to convolution-based network architectures. Although these architectures naturally align with analog crossbar implementations, in their im2col (Chellapilla et al., 2006) format, transformer-based models have recently gained significant attention due to their extensive matrix-vector multiplications and large parameter counts, which can benefit substantially from AIMC stationary computations, i.e., avoiding weight loading (Spoon et al., 2021; Sridharan et al., 2023). Thus, incorporating a dedicated transformer-based search space into the benchmark is crucial for extracting insights into architectural robustness specific to transformer models. Lastly, all current evaluations assume a fixed hardware configuration, detailed in the appendix, and are primarily based on Phase-Change Memory (PCM) device characteristics for noise analysis. Future extensions should explore mappings of neural architectures onto diverse analog hardware platforms, including Resistive RAM (RRAM), Ferroelectric RAM (FeRAM), Electrochemical RAM (ECRAM), and other emerging devices (Joshi et al., 2019). Additionally, expanding to heterogeneous systems that combine analog and digital hardware components will provide deeper insights into the architecture-hardware interplay. Exploring these variations will significantly broaden the applicability and robustness insights of AnalogNAS-Bench across multiple analog computing scenarios. 6 Conclusion In this paper, we introduced AnalogNAS-Bench, the first NAS benchmark tailored for AIMC, ex- tending NAS-Bench-201 with AIMC-specific constraints to enable a fair evaluation of architectures under analog noise conditions, bridging a critical gap in existing NAS benchmarks that overlook AIMC non-idealities. Through our analysis, we highlighted the limitations of conventional quan- tization techniques in capturing AIMC-specific noise and identified key architectural traits that enhance resilience: increased skip connections, wider layers, and a preference for larger convolu- tional operations over narrower ones. With this benchmark in place, more NAS algorithms and estimators will be developed by researchers, making it more practical to compare them in AIMC scenarios. 9 Acknowledgements. We thank the computational support from AiMOS, an AI supercomputer made available by the IBM Research AI Hardware Center and Rensselaer Polytechnic Institute s Center for Computational Innovations (CCI). References Banner, R., Nahshan, Y., and Soudry, D. (2019). Post training 4-bit quantization of convolutional networks for rapid-deployment. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7948 7956. Benmeziane, H., Lammie, C., Boybat, I., Rasch, M. J., Gallo, M. L., Tsai, H., Muralidhar, R., Niar, S., Ouarnoughi, H., Narayanan, V., Sebastian, A., and Maghraoui, K. E. (2023). Analognas: A neural network design framework for accurate inference with analog in-memory computing. In IEEE International Conference on Edge Computing and Communications, EDGE 2023, Chicago, IL, USA, July 2-8, 2023, pages 233 244. IEEE. Boybat, I., Kersting, B., Sarwat, S. G., Timoneda, X., Bruce, R. L., BrightSky, M., Le Gallo, M., and Sebastian, A. (2021). Temperature sensitivity of analog in-memory computing using phase-change memory. In 2021 IEEE International Electron Devices Meeting (IEDM), pages 28 3. IEEE. Chellapilla, K., Puri, S., and Simard, P. (2006). High performance convolutional neural networks for document processing. In Tenth international workshop on frontiers in handwriting recognition. Suvisoft. Chen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785 794. Chrabaszcz, P., Loshchilov, I., and Hutter, F. (2017). A downsampled variant of imagenet as an alternative to the CIFAR datasets. CoRR, abs 1707.08819. Dong, X. and Yang, Y. (2020). Nas-bench-201: Extending the scope of reproducible neural architec- ture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Elsken, T., Metzen, J. H., and Hutter, F. (2019). Neural architecture search: A survey. Journal of Machine Learning Research, 20(55):1 21. Gallo, M. L., Lammie, C., B√ºchel, J., Carta, F., Fagbohungbe, O., Mackin, C., Tsai, H., Narayanan, V., Sebastian, A., Maghraoui, K. E., and Rasch, M. J. (2023). Using the IBM analog in-memory hardware acceleration kit for neural network training and inference. CoRR, abs 2307.09357. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A. G., Adam, H., and Kalenichenko, D. (2018). Quantization and training of neural networks for efficient integer-arithmetic-only inference. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 2704 2713. Computer Vision Foundation IEEE Computer Society. Jiang, W., Lou, Q., Yan, Z., Yang, L., Hu, J., Hu, X. S., and Shi, Y. (2021). Device-circuit-architecture co-exploration for computing-in-memory neural accelerators. IEEE Trans. Computers, 70(4):595 605. 10 Joshi, V., Gallo, M. L., Boybat, I., Haefeli, S., Piveteau, C., Dazzi, M., Rajendran, B., Sebastian, A., and Eleftheriou, E. (2019). Accurate deep neural network inference using computational phase-change memory. CoRR, abs 1906.03138. Kadlecov√°, G., Lukasik, J., Pil√°t, M., Vidnerov√°, P., Safari, M., Neruda, R., and Hutter, F. (2024). Sur- prisingly strong performance prediction with neural graph features. In International Conference on Machine Learning, pages 22771 22816. PMLR. Krestinskaya, O., Salama, K., and James, A. P. (2020). Towards hardware optimal neural network selection with multi-objective genetic search. In 2020 IEEE International Symposium on Circuits and Systems (ISCAS), pages 1 5. IEEE. Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario. Lammie, C., Xiang, W., Linares-Barranco, B., and Azghadi, M. R. (2022). Memtorch: An open-source simulation framework for memristive deep learning systems. Neurocomputing, 485:124 133. Loshchilov, I. and Hutter, F. (2017). SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Rasch, M. J., Mackin, C., Gallo, M. L., Chen, A., Fasoli, A., Odermatt, F., Li, N., Nandakumar, S. R., Narayanan, P., Tsai, H., Burr, G. W., Sebastian, A., and Narayanan, V. (2023). Hardware- aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators. CoRR, abs 2302.08469. Ruchte, M., Zela, A., Siems, J., Grabocka, J., and Hutter, F. (2020). Naslib: A modular and flexible neural architecture search library. Sebastian, A., Le Gallo, M., Khaddam-Aljameh, R., and Eleftheriou, E. (2020). Memory devices and applications for in-memory computing. Nature nanotechnology, 15(7):529 544. Spoon, K., Tsai, H., Chen, A., Rasch, M. J., Ambrogio, S., Mackin, C., Fasoli, A., Friz, A. M., Narayanan, P., Stanisavljevic, M., et al. (2021). Toward software-equivalent accuracy on transformer-based deep neural networks with analog memory devices. Frontiers in Computational Neuroscience, 15:675741. Sridharan, S., Stevens, J. R., Roy, K., and Raghunathan, A. (2023). X-former: In-memory acceleration of transformers. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 31(8):1223 1233. Sun, H., Zhu, Z., Wang, C., Ning, X., Dai, G., Yang, H., and Wang, Y. (2023). Gibbon: An efficient co- exploration framework of NN model and processing-in-memory architecture. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(11):4075 4089. White, C., Neiswanger, W., and Savani, Y. (2021). BANANAS: bayesian optimization with neural architectures for neural architecture search. In Thirty-Fifth AAAI Conference on Artificial Intel- ligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 10293 10301. AAAI Press. Yang, X., Wu, C., Li, M., and Chen, Y. (2022). Tolerating noise effects in processing-in-memory systems for neural networks: A hardware-software codesign perspective. Advanced Intelligent Systems, 4(8). 11 Ying, C., Klein, A., Christiansen, E., Real, E., Murphy, K., and Hutter, F. (2019). Nas-bench-101: Towards reproducible neural architecture search. In International conference on machine learning, pages 7105 7114. PMLR. Yuan, Z., Liu, J., Li, X., Yan, L., Chen, H., Wu, B., Yang, Y., and Sun, G. (2021). NAS4RRAM: neural network architecture search for inference on rram-based accelerators. Science China Information Sciences, 64(6). 12 A Related works Hardware-aware Neural Architecture Search (HW-NAS) for AIMC. HW-NAS aims to find the most efficient DNN for a specific dataset and target hardware platform. Many works (Benmeziane et al., 2023; Yuan et al., 2021; Krestinskaya et al., 2020; Jiang et al., 2021; Sun et al., 2023) target AIMC using HW-NAS. AnalogNAS is a NAS framework designed specifically for AIMC. It incorporates hardware constraints, noise modeling and an evolutionary search strategy. NAS4RRAM is a method to find an efficient DNN for a specific RRAM-based accelerator. It uses an evolutionary algorithm, trains each sampled architecture without HWT training, and evaluates each network on a specific hardware instance. NACIM uses co-exploration strategies to find the most efficient architecture and the associated hardware platform. For each sampled architecture, networks are trained considering noise variations. This approach is limited by using a small search space due to the high time complexity of training. Gibbon is a co-exploration framework for neural network models and AIMC architectures, which utilizes an evolutionary search algorithm with adaptive parameter priority to optimize performance and energy efficiency. B NAS-Bench-201 Search Space NAS-Bench-201 defines a compact yet diverse set of architectures, each represented as a Directed Acyclic Graph (DAG). The search space consists of networks with a fixed macro-structure, where the primary design choices lie within a single cell repeated multiple times throughout the architecture. Each cell consists of a 4-node structure, where every edge represents one of five possible operations: (0) Skip connection (or Identity), (1) zeroize, (2) 3 3 convolution, (3) 1 1 convolution, or (4) 3 3 average pooling (Figure 9). An architecture can be represented and encoded using the cell. The search space contains 15,625 architectures, each of which has been trained and evaluated on standard datasets, including CIFAR-10, CIFAR-100, and ImageNet16-120. C Experimental Setup To evaluate neural architectures under AIMC constraints, we conduct large-scale experiments using a high-performance computing cluster equipped with NVIDIA Tesla V100 GPUs. Architectures are sampled from the NAS-Bench-201 search space, and we utilize NASLib (Ruchte et al., 2020) to extract them as PyTorch models. The training pipeline is implemented in PyTorch, with SLURM managing job scheduling across multiple GPUs. Given the large number of architectures 15,625 in total we employ a distributed training strategy where each architecture is mapped to a dedicated GPU in a batched manner. All architectures are trained for 200 epochs using Stochastic Gradient Descent (SGD) with Nesterov momentum, set to 0.9. A cosine annealing learning rate schedule (Loshchilov and Hutter, 2017) is applied, decaying the initial learning rate of 0.1 to 0 over the course of training. To prevent Input Image Convolution Cell x N Residual Block Stride 2 Cell x N Residual Block Stride 2 Cell x N Global Avg. Pool .. Cell 1 Cell N Skip connection Zeroize 3x3 Conv 1x1 Conv Avg. pool Figure 9: NAS-Bench-201 macro-achitecture. 13 Table 2: Hardware Configuration used for experiments in aihwkit Parameter Setting Digital-to-Analog Converter (DAC) 8 bits Analog-to-Digital Converter (ADC) 8 bits Output noise ùúé 0.04 Max. conductance 25 ùúáS Programming noise ùúé 1 Read noise ùúé 1 Global drift compensation Enabled overfitting, weight decay is set to 5 10 4. In addition, standard data augmentation techniques are employed to improve generalization. The training pipeline includes random horizontal flipping with a probability of 0.5, random cropping to 32 32 pixels with 4-pixel padding, and RGB normalization to standardize input features. These preprocessing steps ensure consistency across different architectures and datasets. Model evaluation is performed under multiple conditions to assess both digital and analog performance. Different accuracies (Baseline, PTQ, QAT, Noisy, Analog, Drift) are computed as the mean over 25 independent evaluation runs on the test set. PTQ models are obtained by quantizing weights and activations to int8 without additional fine-tuning, whereas QAT training is conducted using Adam optimizer with a learning rate of 0.001 and a weight decay of 1 10 4, with the ReduceLROnPlateau scheduler applied to dynamically adjust the learning rate based on performance improvements. To simulate analog hardware, we use IBM s AIHWKit framework. Different noise levels are defined through custom hardware configurations, allowing precise control over hardware non- idealities. Table 2 provides a detailed description of the hardware configurations used in our experiments. Experiments are conducted on three benchmark datasets of increasing complexity. Training and evaluation on CIFAR-10 have been completed, while experiments on CIFAR-100 and ImageNet16- 120 are ongoing. These datasets allow us to evaluate architecture generalization across different classification tasks and ensure that our findings extend beyond a single dataset. D Quantization Performance QAT and PTQ exhibit a strong correlation (ùúè 0.86), indicating similar effects on architectures. Their high correlation with baseline accuracy (ùúè 0.95 for QAT, ùúè 0.87 for PTQ) suggests that standard training transfers well to quantized models with minimal ranking disruption. Moreover, the tight distribution overlap further supports that quantization is a manageable constraint for most architectures. Notably, PTQ alone appears sufficient for performance estimation, offering an efficient alternative to QAT. E Temporal Drift Statistics Table 3 summarizes the statistics and thresholds of the different drift metrics. In the case of noisy drift, we aim to identify architectures that remain robust without HWT, which means they are naturally robust. To set a stricter criterion, we define the robustness threshold between the minimum drop and the first quartile, ensuring that only the most resilient architectures are selected. On the other hand, For analog drift, our goal is to identify architectures that remain non-robust even with HWT. Since most architectures recover with adaptation, we set the robustness threshold 14 Table 3: Statistical summary of drift metrics Metric ( ) Mean Std Min 25 50 75 Max Threshold Noisy Drift Drop 60s 14.33 6.72 1.90 9.54 13.05 17.76 50.75 5 Noisy Drift Drop 1h 24.63 7.96 6.67 18.84 23.66 29.59 69.02 10 Noisy Drift Drop 1d 35.76 8.66 12.97 29.63 35.52 41.85 70.20 16 Noisy Drift Drop 30d 47.54 7.87 21.26 42.18 48.26 53.36 70.92 25 Analog Drift Drop 60s 1.59 1.39 -1.34 0.70 1.20 2.07 17.61 2.5 Analog Drift Drop 1h 2.32 2.02 -0.61 1.05 1.74 2.94 32.01 3.5 Analog Drift Drop 1d 3.44 2.96 -0.46 1.59 2.53 4.29 36.95 4.5 Analog Drift Drop 30d 5.27 4.40 -0.13 2.49 3.86 6.49 44.89 7 between the third quartile and the maximum drop, capturing those that still degrade significantly despite adaptation. F Neural Graph Features Analysis To better understand their structure, neural network architectures can be analyzed using graph- based features in terms of node connectivity, path relationships, and the presence of specific operations (Kadlecov√° et al., 2024). These features can be grouped into several categories: operation count (op_count), which measures the frequency of specific operations; minimum path length (min_path_len), which captures the shortest distance between nodes; maximum operations on a path (max_op_on_path), which identifies the most frequent operations along a path; and node degree metrics (node_degree), which include in-degree, out-degree, and average degree measures that describe how connected specific operations are within the architecture. Each of these features is computed for different subsets of operations within the search space. In this section, we analyze how these features correlate with robustness and provide insights into their impact on architectural design. F.1 Noisy Robustness Figure 10 highlights the correlation of graph features with noisy robustness. Higher node degree across diverse operations is associated with increased robustness, while path-based features suggest that well-connected, structured pathways contribute to stability. In terms of operations, architec- tures that heavily rely on 1 1 convolutions tend to be less robust, whereas those incorporating skip connections, 3 3 convolutions, and average pooling in key positions demonstrate greater resilience. F.2 HWT Robustness Figure 11 illustrates the correlation of graph features with HWT robustness impact. Longer paths and higher node degree, when driven by convolutions, suggesting that deep, well-connected architectures adapt better to transformation. However, pathways dominated by non-learnable oper- ations such as pooling and zeroize show reduced improvement, indicating that static components limit adaptability. These results highlight that HWT is most effective in architectures with strong connectivity and learnable operations, while non-learnable elements hinder its impact. 15 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient op_count_(nor_conv_1x1) node_degree_(nor_conv_1x1)_avg_in node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(zeroize,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_out_degree max_op_on_path_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_out_degree node_degree_(zeroize,nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,avg_pool_3x3)_avg_in node_degree_(nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(nor_conv_1x1)_in_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_in_degree max_op_on_path_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3) node_degree_(skip_connect,avg_pool_3x3)_avg_in node_degree_(zeroize,nor_conv_3x3,nor_conv_1x1)_avg_in min_path_len_(nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(zeroize,nor_conv_1x1)_in_degree Figure 10: Top 20 features correlated with noisy robustness. 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,zeroize,avg_pool_3x3)_avg_in node_degree_(skip_connect,zeroize,avg_pool_3x3)_in_degree node_degree_(nor_conv_3x3,nor_conv_1x1)_in_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree node_degree_(zeroize,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(zeroize,avg_pool_3x3)_avg_in node_degree_(nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,avg_pool_3x3)_out_degree max_op_on_path_(nor_conv_3x3,nor_conv_1x1) node_degree_(zeroize,avg_pool_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(avg_pool_3x3)_in_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,nor_conv_1x1)_in_degree min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) node_degree_(skip_connect,zeroize,nor_conv_1x1,avg_pool_3x3)_avg_in Figure 11: Top 20 features correlated with HWT robustness. F.3 Drift Robustness Figure 12 to 15 show the correlation of graph features with drift robustness. In the case of Noisy Drift, the most influential feature in robustness prediction is node degree, which appears most frequently among the top 20 correlated features. Architectures with a high node degree for skip connections and 3 3 convolutions tend to demonstrate higher robustness. In contrast, architectures where the node degree is high for 1 1 convolutions, none operations, and average pooling are more likely to be non-robust. For Analog Drift, the most significant features are related to path properties within the network. Robust architectures exhibit a high maximum path length involving 3 3 convolutions and skip connections, whereas non-robust architectures tend to have a low minimum path length for the same operations. 16 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree max_op_on_path_(skip_connect,nor_conv_3x3) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(nor_conv_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_in_degree node_degree_(nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(skip_connect,none,nor_conv_3x3)_out_degree op_count_(nor_conv_1x1) Figure 12: Top 20 features correlated with robustness over 1 Day for Noisy Drift (without HWT). 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree min_path_len_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) op_count_(skip_connect) node_degree_(skip_connect)_avg_in node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_avg_in max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_out_degree Figure 13: Top 20 features correlated with robustness over 1 Month for Noisy Drift (without HWT). 17 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in node_degree_(none,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree Figure 14: Top 20 features correlated with robustness over 1 Day for Analog Drift (with HWT). 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in min_path_len_(nor_conv_3x3) node_degree_(none,avg_pool_3x3)_in_degree Figure 15: Top 20 features correlated with robustness over 1 Month for Analog Drift (with HWT). 18\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.18495v1 [cs.LG] 23 Jun 2025 AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing Aniss Bessalah1, Hatem Mohamed Abdelmoumen1, Karima Benatchba1, 2 Hadjer Benmeziane3 Equal contribution. 1Ecole Nationale Sup√©rieure d Informatique, 16309 Oued Smar, Algiers, Algeria 2Laboratoire de M√©thodes de Conception des Syst√®mes, 16309 Oued Smar, Algiers, Algeria 3IBM Research Europe, 8803 R√ºschlikon, Switzerland Abstract Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm for accelerating Deep Neural Networks (DNNs), offering significant energy and latency benefits over conventional digital hardware. However, state-of-the-art neural networks are not inherently designed for AIMC, as they fail to account for its unique non-idealities. Neural Architecture Search (NAS) is thus needed to systematically discover neural architectures optimized explicitly for AIMC constraints. However, comparing NAS methodologies and extracting insights about robust architectures for AIMC requires a dedicated NAS benchmark that explicitly accounts for AIMC-specific hardware non-idealities. To address this, we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for AIMC. Our study reveals three key insights: (1) standard quantization techniques fail to capture AIMC- specific noises, (2) robust architectures tend to feature wider and branched blocks, (3) skip connections improve resilience to temporal drift noise. These insights highlight the limitations of current NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the implementations used in this paper can be found at analog-nas tree main analognasbench. 1 Introduction DNNs deployment is increasingly constrained by power consumption and memory bandwidth limitations. Analog In-Memory Computing (AIMC) (Sebastian et al., 2020) has emerged as a promising alternative to traditional digital hardware by performing computations directly within memory arrays, reducing data movement overhead and significantly improving energy efficiency and computational throughput.\n\n--- Segment 2 ---\n1 Introduction DNNs deployment is increasingly constrained by power consumption and memory bandwidth limitations. Analog In-Memory Computing (AIMC) (Sebastian et al., 2020) has emerged as a promising alternative to traditional digital hardware by performing computations directly within memory arrays, reducing data movement overhead and significantly improving energy efficiency and computational throughput. By leveraging memory devices, AIMC enables matrix-vector multiplications to be executed in a single step (Lammie et al., 2022), offering orders-of-magnitude improvements in energy efficiency compared to conventional Von Neumann platforms. Despite its advantages, AIMC introduces several non-idealities that impact DNN performance (Boybat et al., 2021). Unlike digital accelerators, which rely on precise arithmetic operations, AIMC suffers from device-to-device variations, cycle-to-cycle noise, temporal drift, and limited precision due to analog-digital conversion. These imperfections lead to unpredictable accuracy degradation when deploying conventional DNNs on AIMC platforms. As a result, models that perform well in digital settings often fail to retain their accuracy when executed on AIMC. To address this challenge, researchers have explored two main directions. One approach to mitigate AIMC non-idealities effect is to modify the training process by injecting hardware-specific noise and variations into the learning pipeline. This technique, commonly known as Hardware- Aware Training (HWT) (Rasch et al., 2023), allows the model to adapt to AIMC-induced errors by learning robust representations. An alternative strategy is to design DNNs that are inherently robust to AIMC noises. Neural Architecture Search (NAS) (Elsken et al., 2019) automates the Preprint. Under review. 2025 the authors, released under CC BY 4.0 discovery of efficient network topologies by searching for architectures that maximize accuracy while considering hardware constraints. However, despite the progress in HWT and the numerous analog-aware NAS methodologies, fundamental questions remain: (1) What architectural characteristics enable certain networks to recover their digital accuracy after HWT, while others do not? (2) Can NAS methodologies developed for quantization also be leveraged for AIMC, thereby avoiding the need for costly HWT? (3) How to systematically compare among NAS methodologies for AIMC? Answering these questions requires a systematic evaluation of different architectures under AIMC- specific constraints.\n\n--- Segment 3 ---\n(3) How to systematically compare among NAS methodologies for AIMC? Answering these questions requires a systematic evaluation of different architectures under AIMC- specific constraints. Existing NAS benchmarks, such as NAS-Bench-101 (Ying et al., 2019) and NAS-Bench-201 (Dong and Yang, 2020), typically evaluate architectures based on digital accuracy, FLOP count, and latency, but they do not incorporate HWT, which is crucial for assessing model robustness under AIMC conditions. As a result, researchers lack a standardized benchmark to analyze which architectures best withstand AIMC-induced errors and compare AIMC-targeted NAS methodologies. To bridge this gap, we introduce AnalogNAS-Bench, the first NAS benchmark specifically designed for AIMC. Our benchmark enables a structured evaluation of architectures under AIMC constraints, using AIHWKit (Gallo et al., 2023), and provides insights into architectural robustness when combined with HWT. Our key contributions are: 1. We build AnalogNAS-Bench by extending NAS-Bench-201 and incorporating AIMC-specific constraints, allowing for a fair comparison of different architectures under HWT. This provides a standardized framework for evaluating AIMC-aware NAS methodologies. 2. We analyze the limitations of standard Post-Training Quantization (PTQ) (Banner et al., 2019) and Quantization-Aware Training (QAT) (Jacob et al., 2018) in preserving network rankings under AIMC-induced noise, demonstrating that AIMC non-idealities introduce additional challenges that these methods do not address. 3. Through systematic analysis on CIFAR-10 (Krizhevsky and Hinton, 2009), we extract key archi- tectural insights that contribute to AIMC resilience. We study what makes certain architectures more robust to AIMC-induced errors, investigating factors such as network topology, connectiv- ity patterns, and operator choices. Our findings provide guidelines for designing AIMC-optimized architectures and improving NAS methodologies for analog computing. The remainder of the paper is structured as follows: we first describe the AnalogNAS-Bench benchmark in Section 2. We then analyze architectural robustness and extract key insights in Section 3, followed by an initial comparison of NAS methodologies in Section 4. Finally, we discuss limitations and future work in Section 5.\n\n--- Segment 4 ---\nWe then analyze architectural robustness and extract key insights in Section 3, followed by an initial comparison of NAS methodologies in Section 4. Finally, we discuss limitations and future work in Section 5. 2 AnalogNAS-Bench Description We construct our benchmark based on the search space of NAS-Bench-201 (Dong and Yang, 2020), a widely adopted NAS benchmark. A full description can be found in Section B of the appendix. This search space is particularly well-suited for studying AIMC-based inference due to its small but diverse range of architectures. Despite its architectural diversity, most models achieve high accuracy in standard digital hardware with low variance (median 90.41 , IQR 87.45 91.82 ). This consistency allows for meaningful comparisons when evaluating the impact of analog constraints and supports the choice of analyzing this search space. To systematically assess accuracy degradation and improvements under AIMC settings, we define the following metrics, summarized in Figure 1: Baseline Accuracy represents an architecture s performance in a full-precision digital setting without hardware constraints. This metric serves as an upper bound. 2 Quantization Configuration Full Training Post-Training Quantization Quantization-Aware Training Noisy evaluation Hardware-Aware Training Hardware Configuration Baseline Accuracy PTQ Accuracy QAT Accuracy Noisy Accuracy Noisy Drift Accuracy Analog Drift Accuracy Analog Accuracy Sample architecture NAS-Bench-201 Search Space Direct evaluation on testset AnalogNAS-Bench queryable metrics Temporal Drift Simulation Durations: 60s, 1h, 1d, 30d Average of 25 noisy evaluations to ensure consistency Trained weights Figure 1: Overview of AnalogNAS-Benchmarking process. Noisy Accuracy measures the accuracy degradation that occurs when an architecture is deployed on AIMC hardware without HWT. We compute this by mapping digital-trained weights onto an AIMC crossbar and simulating inference using IBM s AIHWKit (Gallo et al., 2023), which incorporates noise sources from real memristive devices. Analog Accuracy quantifies the performance of an architecture trained with hardware-aware optimization, where AIMC-specific noise and non-idealities are incorporated into the training process. Prior research (Joshi et al., 2019; Rasch et al., 2023; Yang et al., 2022) has shown that integrating AIMC constraints during training enables models to develop compensatory mecha- nisms, enhancing robustness against hardware-induced distortions.\n\n--- Segment 5 ---\nAnalog Accuracy quantifies the performance of an architecture trained with hardware-aware optimization, where AIMC-specific noise and non-idealities are incorporated into the training process. Prior research (Joshi et al., 2019; Rasch et al., 2023; Yang et al., 2022) has shown that integrating AIMC constraints during training enables models to develop compensatory mecha- nisms, enhancing robustness against hardware-induced distortions. The Analog Accuracy metric thus serves as an indicator of the effectiveness of HWT in mitigating AIMC-induced errors and optimizing performance in analog computing environments. In addition to static accuracy metrics, we evaluate model robustness under drift conditions. Noisy Drift represents the accuracy of a model deployed directly onto AIMC hardware and subjected to drift effects over different time intervals. The degradation is measured at 60 seconds, 1 hour, 24 hours, and 30 days. Similarly, Analog Drift quantifies the accuracy degradation for models trained with HWT. Users can easily use our API to query the results of any architecture. 3 Analysis Insights In this section, we analyze the performance of neural network architectures on AIMC, focusing on key hardware-induced constraints: quantization, analog noise, and drift. We examine how these factors affect accuracy, assess the role of HWT in mitigating performance degradation, and identify architectural characteristics that contribute to robustness. We aim to define the structural features that enable architectures to perform reliably under AIMC constraints. We present here the main insights, more analysis can be found in Section F of the appendix. 3.1 Architecture Ranking To evaluate architectural performance across different training and deployment settings, we analyze accuracy rankings and correlations (Figure 2).\n\n--- Segment 6 ---\nWe present here the main insights, more analysis can be found in Section F of the appendix. 3.1 Architecture Ranking To evaluate architectural performance across different training and deployment settings, we analyze accuracy rankings and correlations (Figure 2). Noisy performance shows significant variability, 3 Baseline PTQ QAT Noisy Analog Training Setup 0.0 20.0 40.0 60.0 80.0 100.0 Accuracy ( ) (a) Baseline PTQ QAT Noisy Analog Baseline PTQ QAT Noisy Analog 1.00 0.95 0.87 0.33 0.66 0.95 1.00 0.86 0.34 0.65 0.87 0.86 1.00 0.34 0.63 0.33 0.34 0.34 1.00 0.43 0.66 0.65 0.63 0.43 1.00 (b) 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Figure 2: (a) Accuracy distribution and (b) Kendall s tau correlation heatmap across Baseline, PTQ, QAT, Noisy, and Analog training setups on CIFAR-10. with a large accuracy spread (std 25.47 ) and a lower median (60.70 ). The interquartile range (33.99 75.39 ) highlights substantial robustness differences across architectures. Weak correlation between noisy accuracy and baseline accuracies (ùúè 0.33) suggests that performance on standard digital hardware does not reliably predict AIMC behavior. Similarly, quantization robustness does not imply AIMC robustness, as PTQ and QAT also show weak correlation with noisy accuracy (ùúè 0.34). These results indicate that analog noise is the primary factor driving performance degradation. This decline, however, is mitigated as HWT improves accuracy significantly, raising the mean to 81.31 and the median to 85.49 . The stronger correlation between baseline and analog accuracies (ùúè 0.66) confirms that HWT reduces noise-induced losses. However, analog accuracy remains below baseline levels (IQR 81.70 87.55 vs. 87.45 91.82 ), showing that AIMC noise imposes fundamental constraints that HWT cannot fully overcome in certain architectures. 3.2 Noise Robustness Analysis To identify architectures that are naturally robust to analog noise, we analyze its direct impact on accuracy.\n\n--- Segment 7 ---\nHowever, analog accuracy remains below baseline levels (IQR 81.70 87.55 vs. 87.45 91.82 ), showing that AIMC noise imposes fundamental constraints that HWT cannot fully overcome in certain architectures. 3.2 Noise Robustness Analysis To identify architectures that are naturally robust to analog noise, we analyze its direct impact on accuracy. Specifically, we focus on architectures that achieve high baseline accuracy ( 90 ) and evaluate how their accuracy degrades under AIMC noise. We define noisy drop as the difference between baseline and noisy accuracies. Architectures are then categorized as robust or non-robust based on a threshold set by the lower quartile (12.75 ) of the noisy drop. Operation Distribution. Figure 3 shows the mean percentage of each operation type in robust and non-robust architectures. A clear distinction emerges: robust architectures contain far fewer 1 1 convolutions (9.50 vs. 26.10 in non-robust ones), as these operations utilize less of the crossbar array, making them more susceptible to additional noise. Instead, they favor 3 3 convolutions (32.18 ), which leverage larger crossbar regions. Additionally, higher occurrences of average pooling (19.88 ) and skip connections (20.93 ) suggest that pooling helps reduce noisy features, while skip connections duplicate the information of the actual input and emphasize its importance. Operation Counts. Beyond these general trends, robustness is strongly linked to the specific counts of each operation. Figure 4 presents the proportion of architecture groups for varying occurrences of each operation. When architectures lack 1 1 convolutions, more than half are robust, but robustness quickly declines as their count increases. In contrast, adding multiple 3 3 convolutions consistently improves robustness, though the effect plateaus beyond four. Pooling and skip connections also exhibit an optimal range: architectures with two or three of either operation 4 20.9 17.5 32.2 9.5 19.9 (a) 16.9 16.2 28.1 26.1 12.6 (b) skip_connect zeroize nor_conv_3x3 nor_conv_1x1 avg_pool_3x3 Figure 3: Percentage of each operation type in (a) robust and (b) non-robust architectures.\n\n--- Segment 8 ---\nIn contrast, adding multiple 3 3 convolutions consistently improves robustness, though the effect plateaus beyond four. Pooling and skip connections also exhibit an optimal range: architectures with two or three of either operation 4 20.9 17.5 32.2 9.5 19.9 (a) 16.9 16.2 28.1 26.1 12.6 (b) skip_connect zeroize nor_conv_3x3 nor_conv_1x1 avg_pool_3x3 Figure 3: Percentage of each operation type in (a) robust and (b) non-robust architectures. 0 1 2 3 4 0 20 40 60 80 100 Percentage (a) 0 1 2 3 4 5 (b) 0 1 2 3 4 5 6 (c) 0 1 2 3 4 5 (d) 0 1 2 3 4 (e) Operation Count Robust Non-Robust Figure 4: Distribution of operation counts for robust and non-robust architectures. From left to right: (a) skip connection, (b) zeroize, (c) 3 3 convolution, (d) 1 1 convolution, and (e) 3 3 average pooling. tend to be more robust, while having too few or too many reduces performance. Meanwhile, the zeroize operation shows no clear impact, suggesting sparsity alone does not determine robustness. Sequential Patterns. Operation counts alone do not fully explain robustness, as architectures with identical compositions can exhibit vastly different robustness levels. This suggests that the order and positioning of operations, beyond just their frequencies, play a crucial role. An example of this effect is shown in Figure 5. To better understand these sequential patterns, we examine the most frequent one-, two-, and three-operation paths in robust and non-robust architectures. Robust pathways consistently feature 3 3 convolutions combined with skip connections or pooling layers, as seen in sequences like (2,0), (2,4), (2,2,4), and (2,0,2). In contrast, non-robust pathways frequently include 1 1 convolutions, appearing in patterns like (2,3), (3,2,3), and (2,3,3), indicating that their presence propagates instability even when paired with other beneficial operations.\n\n--- Segment 9 ---\nRobust pathways consistently feature 3 3 convolutions combined with skip connections or pooling layers, as seen in sequences like (2,0), (2,4), (2,2,4), and (2,0,2). In contrast, non-robust pathways frequently include 1 1 convolutions, appearing in patterns like (2,3), (3,2,3), and (2,3,3), indicating that their presence propagates instability even when paired with other beneficial operations. 3.3 Hardware-Aware Training Robustness To evaluate the impact of HWT on robustness, we analyzed the performance shift from noisy infer- ence to analog execution. The majority of architectures achieved strong performance post-HWT, with 80.19 achieving an analog accuracy above 80 . Most architectures benefited significantly, with an average improvement of 134.17 , and 73.81 of them showed substantial gains. However, some architectures remained highly susceptible to analog noise despite HWT interventions. HWT vs. Natural Robustness. To assess whether architectures that naturally resist analog noise perform better than those that significantly improve through HWT, we categorized architectures 5 3x3 avg pool 3x3 avg pool skip 1x1 conv 3x3 conv 3x3 conv 3x3 conv 1x1 conv 3x3 avg pool skip 3x3 avg pool 3x3 conv Figure 5: Two architectures with identical operation counts but different robustness. The left architec- ture (2, 3, 0, 2, 4, 4) achieves high noisy accuracy (86.02 ), while the right one (4, 4, 3, 2, 2, 0) performs poorly (only 44.27 ).\n\n--- Segment 10 ---\nTo assess whether architectures that naturally resist analog noise perform better than those that significantly improve through HWT, we categorized architectures 5 3x3 avg pool 3x3 avg pool skip 1x1 conv 3x3 conv 3x3 conv 3x3 conv 1x1 conv 3x3 avg pool skip 3x3 avg pool 3x3 conv Figure 5: Two architectures with identical operation counts but different robustness. The left architec- ture (2, 3, 0, 2, 4, 4) achieves high noisy accuracy (86.02 ), while the right one (4, 4, 3, 2, 2, 0) performs poorly (only 44.27 ). 0 10 20 30 40 50 (3,) (1,) (0,) (4,) (2,) (a) 0.0 2.5 5.0 7.5 10.0 12.5 (4, 2) (2, 4) (2, 2) (0, 2) (2, 0) (b) 0.0 0.5 1.0 1.5 2.0 2.5 (2, 2, 0) (0, 2, 0) (2, 0, 4) (2, 4, 2) (2, 2, 2) (2, 4, 0) (2, 0, 2) (4, 2, 2) (2, 0, 0) (2, 2, 4) (c) Percentage Paths Figure 6: Most common paths of length (a) one, (b) two and (c) three in robust architectures. Oper- ations are labeled as follows: (0) skip connection, (1) zeroize, (2) 3 3 convolution, (3) 1 1 convolution, and (4) 3 3 average pooling. into three groups based on their noisy accuracy: Naturally robust ( 70 ), Non-Robust ( 20 ), and Moderate (20 70 ). We then evaluated their analog accuracy (post-HWT), setting a threshold of 80 to define high-performing architectures (Figure 7). Naturally robust architectures maintained strong performance, with 82.88 exceeding 85 analog accuracy and requiring minimal improvement ( 10 ) due to their inherent resilience. In contrast, non-robust architectures exhibited the highest variance (mean 65.72 , std 23.53 ), with some failing entirely.\n\n--- Segment 11 ---\nNaturally robust architectures maintained strong performance, with 82.88 exceeding 85 analog accuracy and requiring minimal improvement ( 10 ) due to their inherent resilience. In contrast, non-robust architectures exhibited the highest variance (mean 65.72 , std 23.53 ), with some failing entirely. However, 34.41 of this group saw dramatic gains (mean improvement 495.99 ), though most remained within 80 85 range, , and only 4.63 surpassed the most frequent analog accuracy observed in the naturally robust group. The moderate group showed mixed results: 86.67 crossed the 80 threshold, while some remained below 70 . Ultimately, naturally robust architectures are the most reliable, as they consistently perform well with minimal intervention, whereas HWT does not always recover all noise-sensitive architectures. HWT-Failing Architectures. We examined the subset of the most underperforming architectures: those that initially performed well on standard hardware (baseline accuracy 90 ) but degraded severely under noise (noisy accuracy 20 ) and failed to recover post-HWT (analog accuracy 40 ). These architectures shared two key traits. First, they exhibited excessive reliance on 1 1 convolutions (31.25 ) while underutilizing 3 3 convolutions (13.33 ), particularly in early-stage connections, resulting in weak noise suppression due to insufficient filtering. Second, they relied heavily on non-learnable operations, which in many cases formed entire paths on their own (33.75 ), particularly in short pathways dominated by skip connections (40 ) and average pooling (37.5 ). Longer sequences also combined these non-learnable operations with 1 1 convolutions (37.75 ), offering little resistance to noise propagation. 6 0 10 20 30 40 50 60 70 80 90 Noisy Accuracy ( ) 0 20 40 60 80 100 Analog Accuracy ( ) Non-Robust Moderate Robust Figure 7: Impact of HWT on accuracy across different architecture categories. Architectures are grouped as Non-Robust, Moderate, or Naturally Robust. The dashed line at 80 marks the high analog accuracy threshold. 3.4 Temporal Drift To assess the impact of temporal drift on neural architectures, we first establish accuracy degradation thresholds for different time durations. Specifically, we analyze performance drops at 60 seconds, 1 hour, 1 day, and 30 days under two distinct conditions: Noisy Drift (i.e., without HWT) and Analog Drift (i.e., with HWT).\n\n--- Segment 12 ---\n3.4 Temporal Drift To assess the impact of temporal drift on neural architectures, we first establish accuracy degradation thresholds for different time durations. Specifically, we analyze performance drops at 60 seconds, 1 hour, 1 day, and 30 days under two distinct conditions: Noisy Drift (i.e., without HWT) and Analog Drift (i.e., with HWT). These thresholds are determined statistically by computing key distribution metrics, including the mean, 25th percentile, and 75th percentile of the accuracy drop of only architectures that perform well in digital (Baseline Accuracy 80) and are noise resilient (Noisy Accuracy 70). Architectures that maintain accuracy within these thresholds are classified as robust, while those exhibiting significant degradation are labeled non-robust. Full statistics and thresholds can be found in Section E of the appendix. Noisy Drift over time. Robust architectures show a consistent dominance of 3x3 convolutions, which become even more prominent as the duration increases. The skip connection operation also remains significant, indicating that robust architectures favor a balance between feature extraction and identity mapping. In contrast, 1x1 convolutions appear minimally in robust archi- tectures, suggesting that fine-grained transformations are less useful for maintaining robustness. However, in non-robust architectures, 1x1 convolutions and zeroize operations tend to be more present. Interestingly, as time progresses, robust architectures seem to increase their reliance on 3x3 convolutions while decreasing their use of 1x1 convolutions. This aligns with the hypothesis that wider receptive fields contribute to robustness, as 3x3 convolutions can better capture spatial correlations, making the model less susceptible to noise. Analog Drift over time. Figure 8 demonstrates that robust architectures show a higher presence of skip connections and 3 3 convolutions, suggesting that these operations contribute to long- term stability. The dominance of 3 3 convolutions indicates that spatial feature extraction plays a crucial role in resilience to analog drift. In contrast, 1 1 convolutions remain minimal. On the other hand, Non-robust architectures exhibit a higher presence of average pooling and zeroize operations both of which lack trainable parameters. This reliance on non-learnable operations likely makes them more vulnerable to analog drift, as HWT offers no direct mechanism to counterbalance the absence of weight adaptability.\n\n--- Segment 13 ---\nOn the other hand, Non-robust architectures exhibit a higher presence of average pooling and zeroize operations both of which lack trainable parameters. This reliance on non-learnable operations likely makes them more vulnerable to analog drift, as HWT offers no direct mechanism to counterbalance the absence of weight adaptability. While both robust and non-robust architectures maintain similar patterns, the gap in operation presence increases, suggesting that robustness characteristics become more pronounced over longer periods. 7 0 1 2 3 4 0 5 10 15 20 25 30 35 Operation Percentage ( ) (a) 0 1 2 3 4 (b) Operations Robust Non-Robust Figure 8: Operation presence over time in (a) Robust and (b) Non-Robust architectures trained with HWT. Operations are: (0) skip connection, (1) zeroize, (2) 3 3 convolution, (3) 1 1 convolu- tion, and (4) 3 3 average pooling. 4 Initial Comparison In this section, we present an initial benchmark evaluating standard NAS methodologies using AnalogNAS-Bench. Table 1 summarizes the performance of the best architectures identified by various search strategies, primarily focusing on maximizing the 1-day accuracy. This comparison establishes a baseline for NAS methods tailored specifically to AIMC-based hardware. Except for BANANAS (White et al., 2021), NAS4RRAM (Yuan et al., 2021), NACIM (Jiang et al., 2021), and Gibbon (Sun et al., 2023) which employ their own specialized surrogate modeling techniques all other methods utilize an XGBoost (Chen and Guestrin, 2016) surrogate model trained on a dataset comprising 900 architectures from the AnalogNAS-Bench search space. The surrogate approach aims to capture realistic search times in practical scenarios. The results in Table 1 demonstrate that among AIMC-specific methods, AnalogNAS (Ben- meziane et al., 2023) and GA for IMC AI hardware (Krestinskaya et al., 2020) achieve the highest 1-day accuracy (90.04 ), nearly matching the performance of the optimal architecture obtained via exhaustive search (90.53 ). General NAS methods, including Random Search and Evolutionary Algorithms, also achieve competitive baseline accuracies; however, they tend to exhibit higher variability in accuracy over one month (AVM).\n\n--- Segment 14 ---\nThe results in Table 1 demonstrate that among AIMC-specific methods, AnalogNAS (Ben- meziane et al., 2023) and GA for IMC AI hardware (Krestinskaya et al., 2020) achieve the highest 1-day accuracy (90.04 ), nearly matching the performance of the optimal architecture obtained via exhaustive search (90.53 ). General NAS methods, including Random Search and Evolutionary Algorithms, also achieve competitive baseline accuracies; however, they tend to exhibit higher variability in accuracy over one month (AVM). Notably, Bayesian Optimization and BANANAS show lower accuracies and higher AVM, highlighting their limited effectiveness when analog objectives are considered. AIMC-specific approaches consistently demonstrate improved stability and higher robustness against noise, reinforcing the importance of analog-aware search strategies. These findings highlight a promising direction for future research: developing more suitable estimation strategies that simultaneously consider stability (AVM) and robustness (1-day accuracy), thereby potentially enhancing the effectiveness of NAS methods tailored specifically for AIMC hardware. 5 Limitations Future work While the AnalogNAS-Bench provides a comprehensive platform to analyze convolutional neural networks under AIMC constraints, several limitations must be highlighted. First, the current benchmark evaluations are fully completed only on the CIFAR-10 dataset. Although experiments with CIFAR-100 (Krizhevsky and Hinton, 2009) and ImageNet16-120 (Chrabaszcz et al., 2017) are actively underway, the generalization of insights and robustness trends across more complex and diverse datasets remains an essential next step. Moreover, the current benchmark contains a 8 Table 1: Benchmark Comparison of NAS Methodologies on AnalogNAS-Bench Method Baseline Acc. Noisy Acc. 1-day Acc.\n\n--- Segment 15 ---\nNoisy Acc. 1-day Acc. AVM Parameters (K) Search Time (s) Best Architecture Exhaustive Search 93.46 82.27 90.53 0.28 0.79 0.03 802 - General NAS Methods Random Search 93.34 85.10 89.93 0.25 3.71 0.14 802 201 Evolutionary Algorithm 93.32 82.21 89.34 0.12 0.59 0.11 834 621 Bayesian Optimization 90.21 80.10 82.65 1.54 6.43 1.07 766 764 BANANAS (White et al., 2021) 91.31 80.24 85.97 1.12 5.91 0.87 803 872 AIMC-Specific Methods AnalogNAS (Benmeziane et al., 2023) 93.12 84.37 90.04 0.31 2.13 0.22 567 530 NAS4RRAM (Yuan et al., 2021) 92.45 83.51 89.12 0.42 1.97 0.19 856 612 GA for IMC AI hardware (Krestinskaya et al., 2020) 93.12 84.37 90.04 0.31 2.13 0.22 567 689 NACIM (Jiang et al., 2021) 92.65 82.95 89.30 0.45 1.11 0.20 877 725 Gibbon (Sun et al., 2023) 92.79 83.04 89.45 0.40 1.34 0.18 936 698 AVM: Accuracy Variation over one Month NACIM and Gibbon jointly optimize network and hardware parameters. Hardware optimization is omitted from this comparison. relatively small set of architectures (derived from NAS-Bench-201). While beneficial for systematic analysis, this limited search space also creates an opportunity: the dataset size is suitable for developing zero-cost estimation methods to quickly predict architecture robustness against analog noise, facilitating exploration of substantially larger and more diverse search spaces in the future. Second, our benchmark is limited to convolution-based network architectures.\n\n--- Segment 16 ---\nWhile beneficial for systematic analysis, this limited search space also creates an opportunity: the dataset size is suitable for developing zero-cost estimation methods to quickly predict architecture robustness against analog noise, facilitating exploration of substantially larger and more diverse search spaces in the future. Second, our benchmark is limited to convolution-based network architectures. Although these architectures naturally align with analog crossbar implementations, in their im2col (Chellapilla et al., 2006) format, transformer-based models have recently gained significant attention due to their extensive matrix-vector multiplications and large parameter counts, which can benefit substantially from AIMC stationary computations, i.e., avoiding weight loading (Spoon et al., 2021; Sridharan et al., 2023). Thus, incorporating a dedicated transformer-based search space into the benchmark is crucial for extracting insights into architectural robustness specific to transformer models. Lastly, all current evaluations assume a fixed hardware configuration, detailed in the appendix, and are primarily based on Phase-Change Memory (PCM) device characteristics for noise analysis. Future extensions should explore mappings of neural architectures onto diverse analog hardware platforms, including Resistive RAM (RRAM), Ferroelectric RAM (FeRAM), Electrochemical RAM (ECRAM), and other emerging devices (Joshi et al., 2019). Additionally, expanding to heterogeneous systems that combine analog and digital hardware components will provide deeper insights into the architecture-hardware interplay. Exploring these variations will significantly broaden the applicability and robustness insights of AnalogNAS-Bench across multiple analog computing scenarios. 6 Conclusion In this paper, we introduced AnalogNAS-Bench, the first NAS benchmark tailored for AIMC, ex- tending NAS-Bench-201 with AIMC-specific constraints to enable a fair evaluation of architectures under analog noise conditions, bridging a critical gap in existing NAS benchmarks that overlook AIMC non-idealities. Through our analysis, we highlighted the limitations of conventional quan- tization techniques in capturing AIMC-specific noise and identified key architectural traits that enhance resilience: increased skip connections, wider layers, and a preference for larger convolu- tional operations over narrower ones. With this benchmark in place, more NAS algorithms and estimators will be developed by researchers, making it more practical to compare them in AIMC scenarios. 9 Acknowledgements.\n\n--- Segment 17 ---\nWith this benchmark in place, more NAS algorithms and estimators will be developed by researchers, making it more practical to compare them in AIMC scenarios. 9 Acknowledgements. We thank the computational support from AiMOS, an AI supercomputer made available by the IBM Research AI Hardware Center and Rensselaer Polytechnic Institute s Center for Computational Innovations (CCI). References Banner, R., Nahshan, Y., and Soudry, D. (2019). Post training 4-bit quantization of convolutional networks for rapid-deployment. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7948 7956. Benmeziane, H., Lammie, C., Boybat, I., Rasch, M. J., Gallo, M. L., Tsai, H., Muralidhar, R., Niar, S., Ouarnoughi, H., Narayanan, V., Sebastian, A., and Maghraoui, K. E. (2023). Analognas: A neural network design framework for accurate inference with analog in-memory computing. In IEEE International Conference on Edge Computing and Communications, EDGE 2023, Chicago, IL, USA, July 2-8, 2023, pages 233 244. IEEE. Boybat, I., Kersting, B., Sarwat, S. G., Timoneda, X., Bruce, R. L., BrightSky, M., Le Gallo, M., and Sebastian, A. (2021). Temperature sensitivity of analog in-memory computing using phase-change memory. In 2021 IEEE International Electron Devices Meeting (IEDM), pages 28 3. IEEE. Chellapilla, K., Puri, S., and Simard, P. (2006). High performance convolutional neural networks for document processing. In Tenth international workshop on frontiers in handwriting recognition. Suvisoft. Chen, T. and Guestrin, C. (2016). Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785 794.\n\n--- Segment 18 ---\nXgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785 794. Chrabaszcz, P., Loshchilov, I., and Hutter, F. (2017). A downsampled variant of imagenet as an alternative to the CIFAR datasets. CoRR, abs 1707.08819. Dong, X. and Yang, Y. (2020). Nas-bench-201: Extending the scope of reproducible neural architec- ture search. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Elsken, T., Metzen, J. H., and Hutter, F. (2019). Neural architecture search: A survey. Journal of Machine Learning Research, 20(55):1 21. Gallo, M. L., Lammie, C., B√ºchel, J., Carta, F., Fagbohungbe, O., Mackin, C., Tsai, H., Narayanan, V., Sebastian, A., Maghraoui, K. E., and Rasch, M. J. (2023). Using the IBM analog in-memory hardware acceleration kit for neural network training and inference. CoRR, abs 2307.09357. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A. G., Adam, H., and Kalenichenko, D. (2018). Quantization and training of neural networks for efficient integer-arithmetic-only inference. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 2704 2713. Computer Vision Foundation IEEE Computer Society. Jiang, W., Lou, Q., Yan, Z., Yang, L., Hu, J., Hu, X. S., and Shi, Y. (2021). Device-circuit-architecture co-exploration for computing-in-memory neural accelerators. IEEE Trans. Computers, 70(4):595 605.\n\n--- Segment 19 ---\nIEEE Trans. Computers, 70(4):595 605. 10 Joshi, V., Gallo, M. L., Boybat, I., Haefeli, S., Piveteau, C., Dazzi, M., Rajendran, B., Sebastian, A., and Eleftheriou, E. (2019). Accurate deep neural network inference using computational phase-change memory. CoRR, abs 1906.03138. Kadlecov√°, G., Lukasik, J., Pil√°t, M., Vidnerov√°, P., Safari, M., Neruda, R., and Hutter, F. (2024). Sur- prisingly strong performance prediction with neural graph features. In International Conference on Machine Learning, pages 22771 22816. PMLR. Krestinskaya, O., Salama, K., and James, A. P. (2020). Towards hardware optimal neural network selection with multi-objective genetic search. In 2020 IEEE International Symposium on Circuits and Systems (ISCAS), pages 1 5. IEEE. Krizhevsky, A. and Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario. Lammie, C., Xiang, W., Linares-Barranco, B., and Azghadi, M. R. (2022). Memtorch: An open-source simulation framework for memristive deep learning systems. Neurocomputing, 485:124 133. Loshchilov, I. and Hutter, F. (2017). SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Rasch, M. J., Mackin, C., Gallo, M. L., Chen, A., Fasoli, A., Odermatt, F., Li, N., Nandakumar, S. R., Narayanan, P., Tsai, H., Burr, G. W., Sebastian, A., and Narayanan, V. (2023).\n\n--- Segment 20 ---\nOpenReview.net. Rasch, M. J., Mackin, C., Gallo, M. L., Chen, A., Fasoli, A., Odermatt, F., Li, N., Nandakumar, S. R., Narayanan, P., Tsai, H., Burr, G. W., Sebastian, A., and Narayanan, V. (2023). Hardware- aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators. CoRR, abs 2302.08469. Ruchte, M., Zela, A., Siems, J., Grabocka, J., and Hutter, F. (2020). Naslib: A modular and flexible neural architecture search library. Sebastian, A., Le Gallo, M., Khaddam-Aljameh, R., and Eleftheriou, E. (2020). Memory devices and applications for in-memory computing. Nature nanotechnology, 15(7):529 544. Spoon, K., Tsai, H., Chen, A., Rasch, M. J., Ambrogio, S., Mackin, C., Fasoli, A., Friz, A. M., Narayanan, P., Stanisavljevic, M., et al. (2021). Toward software-equivalent accuracy on transformer-based deep neural networks with analog memory devices. Frontiers in Computational Neuroscience, 15:675741. Sridharan, S., Stevens, J. R., Roy, K., and Raghunathan, A. (2023). X-former: In-memory acceleration of transformers. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 31(8):1223 1233. Sun, H., Zhu, Z., Wang, C., Ning, X., Dai, G., Yang, H., and Wang, Y. (2023). Gibbon: An efficient co- exploration framework of NN model and processing-in-memory architecture. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 42(11):4075 4089. White, C., Neiswanger, W., and Savani, Y. (2021).\n\n--- Segment 21 ---\nWhite, C., Neiswanger, W., and Savani, Y. (2021). BANANAS: bayesian optimization with neural architectures for neural architecture search. In Thirty-Fifth AAAI Conference on Artificial Intel- ligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 10293 10301. AAAI Press. Yang, X., Wu, C., Li, M., and Chen, Y. (2022). Tolerating noise effects in processing-in-memory systems for neural networks: A hardware-software codesign perspective. Advanced Intelligent Systems, 4(8). 11 Ying, C., Klein, A., Christiansen, E., Real, E., Murphy, K., and Hutter, F. (2019). Nas-bench-101: Towards reproducible neural architecture search. In International conference on machine learning, pages 7105 7114. PMLR. Yuan, Z., Liu, J., Li, X., Yan, L., Chen, H., Wu, B., Yang, Y., and Sun, G. (2021). NAS4RRAM: neural network architecture search for inference on rram-based accelerators. Science China Information Sciences, 64(6). 12 A Related works Hardware-aware Neural Architecture Search (HW-NAS) for AIMC. HW-NAS aims to find the most efficient DNN for a specific dataset and target hardware platform. Many works (Benmeziane et al., 2023; Yuan et al., 2021; Krestinskaya et al., 2020; Jiang et al., 2021; Sun et al., 2023) target AIMC using HW-NAS. AnalogNAS is a NAS framework designed specifically for AIMC. It incorporates hardware constraints, noise modeling and an evolutionary search strategy. NAS4RRAM is a method to find an efficient DNN for a specific RRAM-based accelerator. It uses an evolutionary algorithm, trains each sampled architecture without HWT training, and evaluates each network on a specific hardware instance. NACIM uses co-exploration strategies to find the most efficient architecture and the associated hardware platform. For each sampled architecture, networks are trained considering noise variations.\n\n--- Segment 22 ---\nNACIM uses co-exploration strategies to find the most efficient architecture and the associated hardware platform. For each sampled architecture, networks are trained considering noise variations. This approach is limited by using a small search space due to the high time complexity of training. Gibbon is a co-exploration framework for neural network models and AIMC architectures, which utilizes an evolutionary search algorithm with adaptive parameter priority to optimize performance and energy efficiency. B NAS-Bench-201 Search Space NAS-Bench-201 defines a compact yet diverse set of architectures, each represented as a Directed Acyclic Graph (DAG). The search space consists of networks with a fixed macro-structure, where the primary design choices lie within a single cell repeated multiple times throughout the architecture. Each cell consists of a 4-node structure, where every edge represents one of five possible operations: (0) Skip connection (or Identity), (1) zeroize, (2) 3 3 convolution, (3) 1 1 convolution, or (4) 3 3 average pooling (Figure 9). An architecture can be represented and encoded using the cell. The search space contains 15,625 architectures, each of which has been trained and evaluated on standard datasets, including CIFAR-10, CIFAR-100, and ImageNet16-120. C Experimental Setup To evaluate neural architectures under AIMC constraints, we conduct large-scale experiments using a high-performance computing cluster equipped with NVIDIA Tesla V100 GPUs. Architectures are sampled from the NAS-Bench-201 search space, and we utilize NASLib (Ruchte et al., 2020) to extract them as PyTorch models. The training pipeline is implemented in PyTorch, with SLURM managing job scheduling across multiple GPUs. Given the large number of architectures 15,625 in total we employ a distributed training strategy where each architecture is mapped to a dedicated GPU in a batched manner. All architectures are trained for 200 epochs using Stochastic Gradient Descent (SGD) with Nesterov momentum, set to 0.9. A cosine annealing learning rate schedule (Loshchilov and Hutter, 2017) is applied, decaying the initial learning rate of 0.1 to 0 over the course of training. To prevent Input Image Convolution Cell x N Residual Block Stride 2 Cell x N Residual Block Stride 2 Cell x N Global Avg.\n\n--- Segment 23 ---\nA cosine annealing learning rate schedule (Loshchilov and Hutter, 2017) is applied, decaying the initial learning rate of 0.1 to 0 over the course of training. To prevent Input Image Convolution Cell x N Residual Block Stride 2 Cell x N Residual Block Stride 2 Cell x N Global Avg. Pool .. Cell 1 Cell N Skip connection Zeroize 3x3 Conv 1x1 Conv Avg. pool Figure 9: NAS-Bench-201 macro-achitecture. 13 Table 2: Hardware Configuration used for experiments in aihwkit Parameter Setting Digital-to-Analog Converter (DAC) 8 bits Analog-to-Digital Converter (ADC) 8 bits Output noise ùúé 0.04 Max. conductance 25 ùúáS Programming noise ùúé 1 Read noise ùúé 1 Global drift compensation Enabled overfitting, weight decay is set to 5 10 4. In addition, standard data augmentation techniques are employed to improve generalization. The training pipeline includes random horizontal flipping with a probability of 0.5, random cropping to 32 32 pixels with 4-pixel padding, and RGB normalization to standardize input features. These preprocessing steps ensure consistency across different architectures and datasets. Model evaluation is performed under multiple conditions to assess both digital and analog performance. Different accuracies (Baseline, PTQ, QAT, Noisy, Analog, Drift) are computed as the mean over 25 independent evaluation runs on the test set. PTQ models are obtained by quantizing weights and activations to int8 without additional fine-tuning, whereas QAT training is conducted using Adam optimizer with a learning rate of 0.001 and a weight decay of 1 10 4, with the ReduceLROnPlateau scheduler applied to dynamically adjust the learning rate based on performance improvements. To simulate analog hardware, we use IBM s AIHWKit framework. Different noise levels are defined through custom hardware configurations, allowing precise control over hardware non- idealities. Table 2 provides a detailed description of the hardware configurations used in our experiments. Experiments are conducted on three benchmark datasets of increasing complexity. Training and evaluation on CIFAR-10 have been completed, while experiments on CIFAR-100 and ImageNet16- 120 are ongoing.\n\n--- Segment 24 ---\nExperiments are conducted on three benchmark datasets of increasing complexity. Training and evaluation on CIFAR-10 have been completed, while experiments on CIFAR-100 and ImageNet16- 120 are ongoing. These datasets allow us to evaluate architecture generalization across different classification tasks and ensure that our findings extend beyond a single dataset. D Quantization Performance QAT and PTQ exhibit a strong correlation (ùúè 0.86), indicating similar effects on architectures. Their high correlation with baseline accuracy (ùúè 0.95 for QAT, ùúè 0.87 for PTQ) suggests that standard training transfers well to quantized models with minimal ranking disruption. Moreover, the tight distribution overlap further supports that quantization is a manageable constraint for most architectures. Notably, PTQ alone appears sufficient for performance estimation, offering an efficient alternative to QAT. E Temporal Drift Statistics Table 3 summarizes the statistics and thresholds of the different drift metrics. In the case of noisy drift, we aim to identify architectures that remain robust without HWT, which means they are naturally robust. To set a stricter criterion, we define the robustness threshold between the minimum drop and the first quartile, ensuring that only the most resilient architectures are selected. On the other hand, For analog drift, our goal is to identify architectures that remain non-robust even with HWT.\n\n--- Segment 25 ---\nTo set a stricter criterion, we define the robustness threshold between the minimum drop and the first quartile, ensuring that only the most resilient architectures are selected. On the other hand, For analog drift, our goal is to identify architectures that remain non-robust even with HWT. Since most architectures recover with adaptation, we set the robustness threshold 14 Table 3: Statistical summary of drift metrics Metric ( ) Mean Std Min 25 50 75 Max Threshold Noisy Drift Drop 60s 14.33 6.72 1.90 9.54 13.05 17.76 50.75 5 Noisy Drift Drop 1h 24.63 7.96 6.67 18.84 23.66 29.59 69.02 10 Noisy Drift Drop 1d 35.76 8.66 12.97 29.63 35.52 41.85 70.20 16 Noisy Drift Drop 30d 47.54 7.87 21.26 42.18 48.26 53.36 70.92 25 Analog Drift Drop 60s 1.59 1.39 -1.34 0.70 1.20 2.07 17.61 2.5 Analog Drift Drop 1h 2.32 2.02 -0.61 1.05 1.74 2.94 32.01 3.5 Analog Drift Drop 1d 3.44 2.96 -0.46 1.59 2.53 4.29 36.95 4.5 Analog Drift Drop 30d 5.27 4.40 -0.13 2.49 3.86 6.49 44.89 7 between the third quartile and the maximum drop, capturing those that still degrade significantly despite adaptation. F Neural Graph Features Analysis To better understand their structure, neural network architectures can be analyzed using graph- based features in terms of node connectivity, path relationships, and the presence of specific operations (Kadlecov√° et al., 2024). These features can be grouped into several categories: operation count (op_count), which measures the frequency of specific operations; minimum path length (min_path_len), which captures the shortest distance between nodes; maximum operations on a path (max_op_on_path), which identifies the most frequent operations along a path; and node degree metrics (node_degree), which include in-degree, out-degree, and average degree measures that describe how connected specific operations are within the architecture.\n\n--- Segment 26 ---\nF Neural Graph Features Analysis To better understand their structure, neural network architectures can be analyzed using graph- based features in terms of node connectivity, path relationships, and the presence of specific operations (Kadlecov√° et al., 2024). These features can be grouped into several categories: operation count (op_count), which measures the frequency of specific operations; minimum path length (min_path_len), which captures the shortest distance between nodes; maximum operations on a path (max_op_on_path), which identifies the most frequent operations along a path; and node degree metrics (node_degree), which include in-degree, out-degree, and average degree measures that describe how connected specific operations are within the architecture. Each of these features is computed for different subsets of operations within the search space. In this section, we analyze how these features correlate with robustness and provide insights into their impact on architectural design. F.1 Noisy Robustness Figure 10 highlights the correlation of graph features with noisy robustness. Higher node degree across diverse operations is associated with increased robustness, while path-based features suggest that well-connected, structured pathways contribute to stability. In terms of operations, architec- tures that heavily rely on 1 1 convolutions tend to be less robust, whereas those incorporating skip connections, 3 3 convolutions, and average pooling in key positions demonstrate greater resilience. F.2 HWT Robustness Figure 11 illustrates the correlation of graph features with HWT robustness impact. Longer paths and higher node degree, when driven by convolutions, suggesting that deep, well-connected architectures adapt better to transformation. However, pathways dominated by non-learnable oper- ations such as pooling and zeroize show reduced improvement, indicating that static components limit adaptability. These results highlight that HWT is most effective in architectures with strong connectivity and learnable operations, while non-learnable elements hinder its impact.\n\n--- Segment 27 ---\nHowever, pathways dominated by non-learnable oper- ations such as pooling and zeroize show reduced improvement, indicating that static components limit adaptability. These results highlight that HWT is most effective in architectures with strong connectivity and learnable operations, while non-learnable elements hinder its impact. 15 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient op_count_(nor_conv_1x1) node_degree_(nor_conv_1x1)_avg_in node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(zeroize,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_out_degree max_op_on_path_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_out_degree node_degree_(zeroize,nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,avg_pool_3x3)_avg_in node_degree_(nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(nor_conv_1x1)_in_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_in_degree max_op_on_path_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3) node_degree_(skip_connect,avg_pool_3x3)_avg_in node_degree_(zeroize,nor_conv_3x3,nor_conv_1x1)_avg_in min_path_len_(nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(zeroize,nor_conv_1x1)_in_degree Figure 10: Top 20 features correlated with noisy robustness.\n\n--- Segment 28 ---\nThese results highlight that HWT is most effective in architectures with strong connectivity and learnable operations, while non-learnable elements hinder its impact. 15 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient op_count_(nor_conv_1x1) node_degree_(nor_conv_1x1)_avg_in node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(zeroize,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_out_degree max_op_on_path_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_out_degree node_degree_(zeroize,nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,avg_pool_3x3)_avg_in node_degree_(nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(nor_conv_1x1)_in_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_in_degree max_op_on_path_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3) node_degree_(skip_connect,avg_pool_3x3)_avg_in node_degree_(zeroize,nor_conv_3x3,nor_conv_1x1)_avg_in min_path_len_(nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(zeroize,nor_conv_1x1)_in_degree Figure 10: Top 20 features correlated with noisy robustness. 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,zeroize,avg_pool_3x3)_avg_in node_degree_(skip_connect,zeroize,avg_pool_3x3)_in_degree node_degree_(nor_conv_3x3,nor_conv_1x1)_in_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree node_degree_(zeroize,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(zeroize,avg_pool_3x3)_avg_in node_degree_(nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,avg_pool_3x3)_out_degree max_op_on_path_(nor_conv_3x3,nor_conv_1x1) node_degree_(zeroize,avg_pool_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(avg_pool_3x3)_in_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,nor_conv_1x1)_in_degree min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) node_degree_(skip_connect,zeroize,nor_conv_1x1,avg_pool_3x3)_avg_in Figure 11: Top 20 features correlated with HWT robustness.\n\n--- Segment 29 ---\n15 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient op_count_(nor_conv_1x1) node_degree_(nor_conv_1x1)_avg_in node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(zeroize,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_out_degree max_op_on_path_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_out_degree node_degree_(zeroize,nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,avg_pool_3x3)_avg_in node_degree_(nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(nor_conv_1x1)_in_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3)_in_degree max_op_on_path_(skip_connect,zeroize,nor_conv_3x3,avg_pool_3x3) node_degree_(skip_connect,avg_pool_3x3)_avg_in node_degree_(zeroize,nor_conv_3x3,nor_conv_1x1)_avg_in min_path_len_(nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(zeroize,nor_conv_1x1)_in_degree Figure 10: Top 20 features correlated with noisy robustness. 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,zeroize,avg_pool_3x3)_avg_in node_degree_(skip_connect,zeroize,avg_pool_3x3)_in_degree node_degree_(nor_conv_3x3,nor_conv_1x1)_in_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree node_degree_(zeroize,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(zeroize,avg_pool_3x3)_avg_in node_degree_(nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,avg_pool_3x3)_out_degree max_op_on_path_(nor_conv_3x3,nor_conv_1x1) node_degree_(zeroize,avg_pool_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(avg_pool_3x3)_in_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,nor_conv_1x1)_in_degree min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) node_degree_(skip_connect,zeroize,nor_conv_1x1,avg_pool_3x3)_avg_in Figure 11: Top 20 features correlated with HWT robustness. F.3 Drift Robustness Figure 12 to 15 show the correlation of graph features with drift robustness.\n\n--- Segment 30 ---\n0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,zeroize,avg_pool_3x3)_avg_in node_degree_(skip_connect,zeroize,avg_pool_3x3)_in_degree node_degree_(nor_conv_3x3,nor_conv_1x1)_in_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree node_degree_(zeroize,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(zeroize,avg_pool_3x3)_avg_in node_degree_(nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(skip_connect,zeroize,avg_pool_3x3)_out_degree max_op_on_path_(nor_conv_3x3,nor_conv_1x1) node_degree_(zeroize,avg_pool_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(avg_pool_3x3)_in_degree node_degree_(skip_connect,zeroize,nor_conv_3x3,nor_conv_1x1)_in_degree min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) node_degree_(skip_connect,zeroize,nor_conv_1x1,avg_pool_3x3)_avg_in Figure 11: Top 20 features correlated with HWT robustness. F.3 Drift Robustness Figure 12 to 15 show the correlation of graph features with drift robustness. In the case of Noisy Drift, the most influential feature in robustness prediction is node degree, which appears most frequently among the top 20 correlated features.\n\n--- Segment 31 ---\nF.3 Drift Robustness Figure 12 to 15 show the correlation of graph features with drift robustness. In the case of Noisy Drift, the most influential feature in robustness prediction is node degree, which appears most frequently among the top 20 correlated features. Architectures with a high node degree for skip connections and 3 3 convolutions tend to demonstrate higher robustness. In contrast, architectures where the node degree is high for 1 1 convolutions, none operations, and average pooling are more likely to be non-robust. For Analog Drift, the most significant features are related to path properties within the network. Robust architectures exhibit a high maximum path length involving 3 3 convolutions and skip connections, whereas non-robust architectures tend to have a low minimum path length for the same operations.\n\n--- Segment 32 ---\nFor Analog Drift, the most significant features are related to path properties within the network. Robust architectures exhibit a high maximum path length involving 3 3 convolutions and skip connections, whereas non-robust architectures tend to have a low minimum path length for the same operations. 16 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree max_op_on_path_(skip_connect,nor_conv_3x3) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(nor_conv_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_in_degree node_degree_(nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(skip_connect,none,nor_conv_3x3)_out_degree op_count_(nor_conv_1x1) Figure 12: Top 20 features correlated with robustness over 1 Day for Noisy Drift (without HWT).\n\n--- Segment 33 ---\nRobust architectures exhibit a high maximum path length involving 3 3 convolutions and skip connections, whereas non-robust architectures tend to have a low minimum path length for the same operations. 16 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree max_op_on_path_(skip_connect,nor_conv_3x3) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(nor_conv_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_in_degree node_degree_(nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(skip_connect,none,nor_conv_3x3)_out_degree op_count_(nor_conv_1x1) Figure 12: Top 20 features correlated with robustness over 1 Day for Noisy Drift (without HWT). 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree min_path_len_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) op_count_(skip_connect) node_degree_(skip_connect)_avg_in node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_avg_in max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_out_degree Figure 13: Top 20 features correlated with robustness over 1 Month for Noisy Drift (without HWT).\n\n--- Segment 34 ---\n16 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree max_op_on_path_(skip_connect,nor_conv_3x3) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(nor_conv_3x3)_out_degree node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_in_degree node_degree_(nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(skip_connect,none,nor_conv_3x3)_out_degree op_count_(nor_conv_1x1) Figure 12: Top 20 features correlated with robustness over 1 Day for Noisy Drift (without HWT). 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree min_path_len_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) op_count_(skip_connect) node_degree_(skip_connect)_avg_in node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_avg_in max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_out_degree Figure 13: Top 20 features correlated with robustness over 1 Month for Noisy Drift (without HWT). 17 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in node_degree_(none,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree Figure 14: Top 20 features correlated with robustness over 1 Day for Analog Drift (with HWT).\n\n--- Segment 35 ---\n0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 Correlation Coefficient node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect)_in_degree node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(none,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree min_path_len_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) op_count_(skip_connect) node_degree_(skip_connect)_avg_in node_degree_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3)_avg_in max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(none,nor_conv_3x3,nor_conv_1x1,avg_pool_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,avg_pool_3x3) node_degree_(nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,none,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1)_out_degree Figure 13: Top 20 features correlated with robustness over 1 Month for Noisy Drift (without HWT). 17 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in node_degree_(none,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree Figure 14: Top 20 features correlated with robustness over 1 Day for Analog Drift (with HWT). 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in min_path_len_(nor_conv_3x3) node_degree_(none,avg_pool_3x3)_in_degree Figure 15: Top 20 features correlated with robustness over 1 Month for Analog Drift (with HWT).\n\n--- Segment 36 ---\n17 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in node_degree_(none,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_in_degree Figure 14: Top 20 features correlated with robustness over 1 Day for Analog Drift (with HWT). 0.6 0.4 0.2 0.0 0.2 0.4 0.6 Correlation Coefficient min_path_len_(skip_connect,nor_conv_3x3) min_path_len_(skip_connect,nor_conv_3x3,nor_conv_1x1) max_op_on_path_(skip_connect,nor_conv_3x3) max_op_on_path_(skip_connect,nor_conv_3x3,nor_conv_1x1) node_degree_(none,nor_conv_1x1,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3)_in_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_in_degree node_degree_(skip_connect,nor_conv_3x3)_out_degree node_degree_(none,nor_conv_1x1,avg_pool_3x3)_out_degree node_degree_(none,avg_pool_3x3)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_avg_in node_degree_(skip_connect,nor_conv_3x3,nor_conv_1x1)_out_degree node_degree_(none,avg_pool_3x3)_out_degree min_path_len_(nor_conv_3x3,nor_conv_1x1) node_degree_(skip_connect,none,nor_conv_1x1,avg_pool_3x3)_avg_in op_count_(nor_conv_3x3) node_degree_(nor_conv_3x3)_avg_in min_path_len_(nor_conv_3x3) node_degree_(none,avg_pool_3x3)_in_degree Figure 15: Top 20 features correlated with robustness over 1 Month for Analog Drift (with HWT). 18\n\n