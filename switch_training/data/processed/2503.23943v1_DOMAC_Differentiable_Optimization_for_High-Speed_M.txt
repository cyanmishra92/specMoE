=== ORIGINAL PDF: 2503.23943v1_DOMAC_Differentiable_Optimization_for_High-Speed_M.pdf ===\n\nRaw text length: 33099 characters\nCleaned text length: 32620 characters\nNumber of segments: 22\n\n=== CLEANED TEXT ===\n\nDOMAC: Differentiable Optimization for High-Speed Multipliers and Multiply-Accumulators Chenhao Xue1, Yi Ren1,2, Jinwei Zhou3, Kezhi Li4, Chen Zhang5, Yibo Lin1,6,7, Lining Zhang8, Qiang Xu4,9, Guangyu Sun1,6,7, 1School of Integrated Circuits, 2School of Software and Microelectronics, Peking University, Beijing, China 3School of Integrated Circuits, Anhui Polytechnic University, Wuhu, China 4Department of Computer Science and Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong S.A.R. 5Shanghai Jiao Tong University, Shanghai, China 6Institute of Electronic Design Automation, Peking University, Wuxi, China 7Beijing Advanced Innovation Center for Integrated Circuits, Beijing, China 8School of Electronic and Computer Engineering, Peking University, Shenzhen, China 9National Center of Technology Innovation for EDA, Nanjing, China {xch927027, yibolin, eelnzhang, {kzli24, Abstract Multipliers and multiply-accumulators (MACs) are funda- mental building blocks for compute-intensive applications such as artifi- cial intelligence. With the diminishing returns of Moore s Law, optimizing multiplier performance now necessitates process-aware architectural innovations rather than relying solely on technology scaling. In this paper, we introduce DOMAC, a novel approach that employs differentiable optimization for designing multipliers and MACs at specific technology nodes. DOMAC establishes an analogy between optimizing multi-staged parallel compressor trees and training deep neural networks. Building on this insight, DOMAC reformulates the discrete optimization challenge into a continuous problem by incorporating differentiable timing and area objectives. This formulation enables us to utilize existing deep learning toolkit for highly efficient implementation of the differentiable solver. Experimental results demonstrate that DOMAC achieves significant enhancements in both performance and area efficiency compared to state- of-the-art baselines and commercial IPs in multiplier and MAC designs. Index Terms Differentiable Optimization, Multiplier, Multiply- Accumulator, Compressor Tree I. INTRODUCTION In digital circuit design, multipliers and multiply-accumulators (MACs) constitute fundamental building blocks, playing a critical role in compute-intensive applications such as artificial intelligence and high-performance computing. As these modules play pivotal roles in determining the performance and efficiency of the entire hardware systems, optimizing for high-speed multipliers and MACs has become increasingly imperative. Modern multipliers and MACs typically consist of three primary components: a partial product generator (PPG), a compression tree (CT), and a final carry-propagate adder (CPA). The CT realizes parallel compression of the partial product array into two rows, which are then processed by the CPA to produce the final result. Previous research has shown that CT accounts for more than 50 of the delay and area in the entire multiplier, underscoring its critical importance in the overall design [1]. The optimization of high-speed CT has been a focal point of research for decades. Early advancements begin with classical struc- tures such as Wallace tree [2] and Dadda tree [3], followed by numerous improvements that introduce compressor variants [4] [6] or customized architectures [7], [8] optimized for specific tech- nology nodes. However, these approaches heavily rely on domain expertise and require laborious redesign efforts when migrating to Corresponding author: Guangyu Sun new technology nodes, which can significantly delay time-to-market. Algorithmic solutions employ heuristic strategies and mathematical programming to automate the exploration of CT architecture variants. GOMIL adopts integer linear programming (ILP) to adjust compres- sor assignment for reduced area [9]. The three-dimensional method proposes a heuristic approach to adjust compressor interconnection for balanced delay paths [10]. UFO-MAC leverages ILP to achieve global optimization of compressor interconnection, enhancing the overall speed [11]. Nevertheless, these methods exhibit a significant limitation in oversimplifying the timing and area characteristics of compressors. Specifically, they assume fixed latency and area for compressors; whereas in practical logic synthesis, each compressor may be mapped to implementations with varying timing and area characteristics. Additionally, factors such as signal transition time and capacitive load can introduce timing variations, which are not fully accounted for. Consequently, the oversimplification may lead to suboptimal solutions for the targeted technology nodes. More recently, reinforcement learning (RL) has been employed to search for physically-optimized multiplier and MAC designs [1], [12], [13]. While delivering promising results, RL-based methods suffer from prohibitive computational complexity, as training RL agents requires extensive trial-and-error iterations and expensive invocations of synthesis tools during exploration. In summary, the process- aware architectural optimization for high-performance multipliers and MACs urgently requires automated solutions that balance optimiza- tion efficiency and effectiveness. To resolve the aforementioned limitations, we introduce an inno- vative solution that leverages differentiable optimization to generate technology-specific high-speed multipliers and MACs, referred to as DOMAC. DOMAC evaluates the performance of CTs with detailed static timing analysis process (STA) [14], and jointly manipulates compressor interconnection and technology mapping to improve post- synthesis timing and area. To efficiently address this large-scale combinatorial optimization problem, we establish an analogy be- tween optimizing multi-staged CT and training deep neural networks (DNNs). Based on the analogy, DOMAC proposes differentiable timing and area objectives with respect to CT interconnection and implementation, which enables highly efficient optimization via gra- dient descent. The key contributions of this work are summarized as follows: arXiv:2503.23943v1 [cs.AR] 31 Mar 2025 Partial Product Generator Carry-Propagate Adder Carry-Propagate Adder Partial Product Generator 1 0 2 3 4 5 7 6 8 9 10 Compressor Tree 1 0 2 3 4 5 7 6 8 9 𝑨𝑨(𝑵𝑵-bit) 𝑩𝑩(𝑵𝑵-bit) 𝑨𝑨(𝑵𝑵-bit) 𝑩𝑩(𝑵𝑵-bit) 𝑪𝑪(𝟐𝟐𝑵𝑵-bit) Stage 1 Stage 2 Stage 3 Stage 1 Stage 2 Stage 3 𝑹𝑹(2𝑵𝑵-bit) 𝑹𝑹(2𝑵𝑵-bit) 2:2 compressor 3:2 compressor a Multiplier Architecture b Fused MAC Architecture Fig. 1. Architecture of (a) multipliers and (b) fused multipliy-accumulators We propose DOMAC, a high-speed multiplier and MAC gener- ation framework that jointly optimizes compressor interconnec- tions and compressor physical implementations. We analogize CT optimization with training DNNs and in- troduce differentiable timing and area objectives for efficient optimization of post-synthesis QoRs. We implement a highly efficient differentiable solver using established deep learning toolkit PyTorch [15]. Experiment results demonstrate that DOMAC optimized mul- tipliers and MACs exceed all baseline designs, with up to 6.5 improvement in delay and 25 reduction in area over commercial IPs. The rest of this paper is organized as follows: Section II provides preliminaries of multiplier and MAC design. Section III details DOMAC framework. Section IV presents the experimental results. Finally, Section V concludes this paper. II. PRELIMINARIES A. Multiplier Architecture As shown in Fig. 1(a), a high-speed multiplier comprises three main components: a partial product generator (PPG), a compressor tree (CT), and a carry-propagate adder (CPA). Partial Product Generator: The PPG generates an array of partial products (PPs), where each PP is shifted into different columns to represent varied power-of-two weights. A commonly employed AND gate-based PPG generates N 2 PPs for an N-bit multiplier. Compressor Tree: The CT compresses PPs for multiple stages until a maximum of two PPs remain in each column. CT typically consists of various types of compressors, with 3:2 and 2:2 compressors being predominantly used. Carry-Propagate Adder: The CPA aggregates the compressed PPs from the CT to produce the final product. High-speed 3:2 Comp. 2:2 Comp. A B CI A B A CO S CO S Z 𝒗𝒗𝟎𝟎 𝒗𝒗𝟏𝟏 𝒗𝒗𝟐𝟐 𝒗𝒗𝟑𝟑 𝒗𝒗𝟒𝟒 𝒗𝒗𝟓𝟓 𝒖𝒖𝟎𝟎 𝒖𝒖𝟏𝟏 𝒖𝒖𝟐𝟐 𝒖𝒖𝟑𝟑 𝒖𝒖𝟒𝟒 𝒖𝒖𝟓𝟓 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 Interconnection Matrix 𝑀𝑀𝑖𝑖,𝑗𝑗 Fig. 2. Example of adjusting compressor tree interconnection, where the partial products and compressor inputs can be assigned interchangeably. A B CI CO S FA_X1 CO S A B CI (a) (b) 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐴𝐴 𝐶𝐶𝐶𝐶 0.0432 𝑛𝑛𝑛𝑛 0.0802 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐵𝐵 𝐶𝐶𝐶𝐶 0.0444 𝑛𝑛𝑛𝑛 0.0830 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐶𝐶𝐶𝐶 𝐶𝐶𝐶𝐶 0.0472 𝑛𝑛𝑛𝑛 0.0743 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐴𝐴 𝑆𝑆 0.0935 𝑛𝑛𝑛𝑛 0.1194 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐵𝐵 𝑆𝑆 0.0946 𝑛𝑛𝑛𝑛 0.1263 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐶𝐶𝐶𝐶 𝑆𝑆 0.0459 𝑛𝑛𝑛𝑛 0.1174 𝑛𝑛𝑛𝑛 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎 6.650 𝜇𝜇𝑚𝑚2 4.256 𝜇𝜇𝑚𝑚2 (a) Composition of Basic Standard Cells (b) Using Specialized Standard Cell (c) Delay Area Comparison Fig. 3. Example of two different implementations for 3:2 compressor using Nangate45 Open Cell Library [17]. We estimate the delay using a nonlinear delay model (NLDM) with islew 0.02ns and oload 3fF for all input- output pairs. multipliers often utilize parallel prefix adders [16] for fast computation. The fused multiply-accumulator (fused MAC) integrates the ac- cumulation operation into the compressor tree to boost computation efficiency. As illustrated in Fig. 1(b), fused MAC exhibits a similar architecture as multiplier. B. Problem Formulation DOMAC focuses on exploring the design space of compressor trees (CTs), one of the most critical modules influencing the performance of multipliers and MACs. This section examines the comprehensive CT design space and formulates the design space exploration problem addressed by DOMAC. Compressor Tree Design Space Factors that affect CT perfor- mance are as follows: Compressor Type: Different types of compressors have similar functions in reducing the number of partial products but differ in terms of the number of inputs or outputs. For a fair comparison with previous methods [2], [3], [9], [13], DOMAC utilizes commonly adopted 3:2 and 2:2 compressors to build CTs. However, DOMAC can be easily extended to support other types of compressors. Compressor Quantity: Each compression stage should comprise a proper amount of compressors. DOMAC utilizes a designated compressor assignment for subsequent optimization. Without loss of generality, DOMAC adopts the quantity derived from classical architectures such as Wallace tree [2] and Dadda tree [3] . Compressor Interconnection: As shown in Fig. 2, the partial products and input ports of compressors located at column i and stage j must be mapped bijectively. Due to the asso- ciative property of addition, this mapping can be arbitrarily permuted without affecting the correctness of final results. We use binary matrix Mi,j to represent the interconnection, where Mi,j[u, v] 1 indicates that the u-th partial product is mapped to the v-th input port. Each row and column in matrix Mi,j sums to 1, making Mi,j a doubly stochastic matrix. Compressor Implementation: As shown in Fig. 3, a compressor may have different implementations with distinct timing and area characteristics. For each compressor c, we use one-hot vector pc to denote its physical implementation, in which pc[k] 1 indicates that c adopts the k-th implementation from the available set Pc. Problem (Compressor Tree Design Space Exploration) Given an initial compressor set C where each compressor c has been assigned to a specific column and stage, the objective is to minimize the absolute values of worst negative slack (WNS), total negative slack (TNS), and total area by simultaneously adjusting compressor interconnection M and compressor physical implementation p. The formal definitions are presented as follows: min M,p t1WNS(M, p) t2TNS(M, p) αArea(p) subject to Mi,j is doubly stochastic, Mi,j M; X k pc[k] 1, c C; (1) The combinatorial optimization problem in Equation (1) presents challenges for conventional discrete solvers. On one hand, the static timing analysis process is highly complex [14], making it difficult to incorporate into standard solvers without substantial simplification. On the other hand, the problem involves numerous variables and intricate constraints, which render discrete optimization algorithms inefficient. III. METHODOLOGY In this section, we propose the differentiable framework for op- timizing technology-specific multipliers and MACs. Section III-A presents an inspiration from deep neural network. Section III-B presents an overview of the framework. Sections III-C, III-D, and III-E give the mathematical details of the area, timing, and constraint objectives. Section III-F details the selection of hyperpa- rameters. A. Inspiration from Deep Neural Networks Recent advancements in the theory and practice of machine learn- ing have brought new inspiration for electronic design automation (EDA). Drawing parallels between EDA challenges and machine learning problems, researchers have developed novel algorithms to address analytical placement [18], [19] and gate sizing [20]. In DOMAC, we expand this approach to the efficient synthesis of technology-specific high-performance multipliers and MAC units. As illustrated in TABLE I, delay propagation in multi-staged com- pressor trees exhibits a striking analogy to the forward propagation process of deep neural networks (DNNs). 1) The net delay propagation from a set of compressor output ports u to the corresponding set of compressor input ports v is determined by interconnection Mi,j. This process can be formulated as AT(v) M T i,j AT(u), which resembles the linear projection WX in DNNs. 2) The cell delay propagation involves adding the internal cell delay to the arrival time at the compressor input ports, which is analogous to adding biases b in DNNs. 3) The timing objective WNS and TNS is computed by measuring the difference between final arrival time and required arrival TABLE I ANALOGY BETWEEN OPTIMIZING COMPRESSOR TREE DESIGNS AND TRAINING DEEP NEURAL NETWORKS. DNN Training CT Optimization Activations X Arrival time AT Weights W Interconnection M Biases b Cell delay d Loss function L Objective WNS, TNS time, which is similar to evaluating the consistency between DNN outputs and data labels through loss function L. The analogy in TABLE I inspires us to optimize CT design using a similar approach to training DNNs. In practice, DNNs are efficiently trained with backpropagation (BP), where weights W and biases b are updated using the gradient of loss function L w.r.t. these parameters. However, BP cannot be directly adopted due to fundamental distinction between the two problems: CT de- sign involves discrete interconnections and physical implementations, whereas DNN parameters are continuous in nature. To address this challenge, we propose to relax the discrete CT optimization problem into a continuous formulation. In a nutshell, we relax the discrete selections of compressor interconnection M and compressor implementation p to continuous variables, making them essentially represent probabilistic distributions. Therefore, we can estimate the expectation of intermediate values in STA, such as signal arrival time and signal transition time (slew), and ultimately derive differentiable timing objectives. We use gradient descent to update M and p in continuous domain and then adopt legalization process to map them back to discrete domain. Consequently, a large portion of the CT optimization problem can be efficiently addressed following the same process as DNN training. Furthermore, we incorporate area objectives to balance the performance and area footprints, as well as constraint objectives to ensure the accuracy of timing and area estimations during continuous optimization. The details will be discussed in the following subsections. B. Framework Overview The DOMAC framework consists of the following steps: 1) Differentiable Optimization: Solve the continuous formulation of the CT optimization problem, which is essentially analogous to training DNNs. Leveraging the established deep learning toolkit PyTorch [15], we efficiently implement our differen- tiable solver. 2) Legalization: Map the continuous solution to a discrete solu- tion. For each compressor interconnection Mi,j M, we em- ploy the Hungarian algorithm to identify the bipartite matching with the maximum probability summation. For each compressor implementation pc p, we apply the argmax operation to select the implementation with the highest probability. 3) Netlist Generation: Generate the synthesized gate-level CT netlists, which can be seamlessly integrated into multiplier and MAC designs. C. Differentiable Area Objective We begin with the total area, which is the simplest objective. For each compressor c, the area of all available implementations is represented by a vector ac. The area expectation can be derived from the probabilistic distribution pc: Areac(pc) X k pc[k] ac[k] (2) The total area of the compressor tree is the sum of the expected area across all compressors: Area(p) X c Areac(pc) (3) D. Differentiable Timing Objective Compared with area objectives, timing objectives are quite com- plex. The compressor implementation p affects the cell delay char- acteristics, whereas the interconnection M affects the delay propa- gation. Moreover, p and M jointly affect the capacitive loads, which further complicates the process of static timing analysis (STA). Next, we detail each stage of the differentiable STA in DOMAC: 1) Pin Load Estimation: Non-linear delay model (NLDM) is extensively adopted to characterize cell timing statistics in STA [14]. In NLDM, cell delay and slew are determined through look-up tables (LUTs). Given input slew and capacitive load of the cell, the delay and output slew are computed using linear interpolation or extrapolation based on a corresponding 2 2 submatrix within the LUT. Therefore, our first step is to derive cell capacitive load before timing propagation. Since both M and p represent probabilistic distributions, the exact values of capacitive load are inaccessible. Our best effort is to compute the expected capacitive load: Cap(v) X k pc[k] Cap(v; Pc[k]) (4a) Load(u) X child v Mi,j[u, v] Cap(v) (4b) where Cap(v) denotes the expected capacitance at the input pin v of the next-level compressor, and Load(u) denotes the expected capacitive load at the output pin u of previous-level compressor. 2) Cell Delay Propagation: Next, we utilize NLDM to compute the delay, slew, and arrival times: Slewu(v) X k pc[k] LUTslew u v(Slew(u), Load(v); Pc[k]) (5a) Delayu(v) X k pc[k] LUTdelay u v(Slew(u), Load(v); Pc[k]) (5b) AT(v) LSEγ{AT(u) Delayu(v) input u} (5c) Slew(v) LSEγ{Slewu(v) input u} (5d) In Equation (5a) and Equation (5b) , u and v are the input pin and output pin of compressor c, respectively. The slew and delay from u to v are estimated using the LUTs extracted from .lib files in the process design kit (PDK). In practice, the slew and delay for each output pin may correspond to multiple LUTs, each associated with a specific input assignment and signal edge (rise or fall). In DOMAC, we extract LUTs corresponding to the worst-case scenarios, where each entry in the LUT is selected from the maximum value across all possible conditions. Similar to Equation (4), we estimate the expectation of delay and slew under probabilistic implementation pc. In Equation (5c) and Equation (5d), we compute the arrival time and slew of pin v by gathering the maximum value from all the related input u. We replace the max operation with a smoothed approxi- mation, the Log-Sum-Exp (LSE), as defined in Equation (6). This approach has been adopted by previous works to alleviate oscillation and instability in the differentiable optimization process [18], [19]. The hyperparameter γ controls the trade-off between the smoothness and approximation accuracy. LSEγ(x1, x2, , xn) γ log( n X i 1 exp xi γ ) (6) 3) Net Delay Propagation: Next, we consider net delay propa- gation. In compressor trees where all nets exhibit limited fanouts and lengths, the slew variation and delay introduced by the nets themselves are negligible. Therefore, we mainly consider the impact of probabilistic interconnection M on net delay propagation. As discussed in Section III-D1, we derive the expected slew and arrival time of next-level compressor input pin v: Slew(v) X parent u Mi,j[u, v] Slew(u) (7a) AT(v) X parent u Mi,j[u, v] AT(u) (7b) where the slew and arrival time of previous-level compressor output pin u have been computed as detailed in Section III-D2. 4) Timing Slack Estimation: Finally, we derive the timing slacks as optimization objectives. For complex combinatorial logic such as compressor tree, our primary focus is to avoid setup timing violations, which occur when the setup slack becomes negative. Specifically, we aim to minimize the absolute value of worst negative slack (WNS) and total negative slack (TNS) defined as follows: Slack(u) RAT(u) AT(u) (8a) WNS(M, p) LSEγ{min(0, Slack(u)) output u} (8b) TNS(M, p) X output u min(0, Slack(u)) (8c) where u is the primary output pin of the compressor tree. RAT(u) is the required arrival time for pin u, assigned by the designer. We replace the max operation in WNS with LSE for better smoothness, following the same discussion in Section III-D2. E. Optimization Constraints Given the relaxed continuous version of the problem defined in Equation (1), we aim to ensure the differentiable optimization yields valid discrete solutions. This is achieved through variable substitution and the introduction of constraint loss functions. 1) Variable Substitution: For a valid probabilistic distribution of compressor implementation pc, the summation of all elements must equal 1. To satisfy this constraint, we introduce auxiliary variables pc R Pc and compute pc using the softmax function: pc softmax( pc) [ e pc[0] P k e pc[k] , , e pc[ P 1] P k e pc[k] ] (9) For the doubly stochastic matrix Mi,j representing compressor in- terconnection, we adopt a similar approach to ensure the sum of each row equals 1. Specifically, we introduce auxiliary variables Mi,j Rl l, where l is the number of partial products at column i and stage j. The matrix Mi,j is computed as: Mi,j[u, ] softmax( Mi,j[u, ]) (10) Variable substitution guarantees that the probabilistic constraints are consistently upheld while permitting the auxiliary variables to be freely adjusted. 2) Bijective Mapping Loss: The doubly stochastic property of Mi,j also demands the sum of each column equals 1, which is fulfilled by introducing the bijective mapping loss in Equation (11). The quadratic form is chosen for better smoothness. LBM(M) X i,j,u ( X v Mi,j[u, v] 1)2 (11) 3) Discretization Loss: To encourage continuous variables to converge to bimodal states (0 or 1), we introduce the discretization loss in Equation (12). This function is minimized at 0 and 1 and exhibits smooth gradients near these minima: LD(x) x2 (1 x)2 (12a) LD(M, p) X i,j,u,v LD(Mi,j[u, v]) X c,k LD(pc[k]) (12b) F. Hyperparameter Selection In the previous sections, we have reformulated the discrete CT optimization problem into a differentiable optimization problem. The complete objective function is summarized as follows: min M, p t1WNS(M, p) t2TNS(M, p) αArea(p) λ1LD(M, p) λ2LBM(M) (13) where t1, t2, α, λ1, λ2 are weights for different objectives. Without loss of generality, we assume the timing slacks are measured in nanoseconds ns and area are measured in square micrometers µm2. The optimization process is conducted over 300 iterations, with incremental adjustments to the hyperparameters starting from the 100th iteration. This approach ensures a balanced trade-off among the objectives at different stages of the optimization. Specifically, α is set between 1 and 5 to trade-off timing and area, and is increased by 0.3 every iteration to counterbalance timing objectives. t1 and t2 are set to 1 and 0.01, respectively, and are increased by 0.5 every iteration to prioritize timing optimization in the later stages. We set required arrival time RAT(u) 0 for all output ports u. λ1 and λ2 are set to 0.1 and 0.5, respectively, and are increased by 1 per iteration to ensure that design constraints are adequately respected without excessively disrupting the optimization process. The smoothing factor γ for LSE operations is set to 0.01, which provides sufficient approximation accuracy and smoothness for differentiable optimization. IV. EXPERIMENT RESULTS A. Experiment Setup All experiments are conducted on a Linux-based platform equipped with 2.8GHz 96-core Intel Xeon Gold 6342 CPUs and 1.5TB memory. Unless specified otherwise, all multipliers and MACs adopt AND-based PPG and default CPA instantiated with s a b style RTL. We compare the multipliers and MACs generated by DO- MAC with several baseline approaches, including: Wallace trees [2], Dadda trees [3], and commercial IPs from Synopsys DesignWare Library [21]. The commercial IPs are instantiated with r a b and r a b c style RTL. Additionally, the multipliers are compared with those produced by GOMIL [9] (ILP-based) and ArithmeticTree [13] (RL-based) leveraging their open-sourced implementation. All de- signs are synthesized by Synopsys Design Compiler (Version R- 2020.09-SP2) [22] using Nangate 45nm Open Cell Library [17] and the compile_ultra command. To illustrate the trade-off between delay and area, we sweep the target delay constraints from 0ns to 2ns. B. Results and Analysis Fig. 4 illustrates the synthesized results of multipliers, displaying the Pareto frontiers with respect to delay and area. In most cases, DOMAC achieves superior performance over all baseline methods. Our framework identifies designs that Pareto-dominate those of the commercial IPs, delivering up to 6.5 reduction in delay and 25 300 400 500 600 Area (um 2) 0.8 1.0 1.2 1.4 Delay (ns) 8b Multiplier 1500 1750 2000 Area (um 2) 1.2 1.4 1.6 Delay (ns) 16b Multiplier 6000 7000 8000 Area (um 2) 1.4 1.6 1.8 2.0 Delay (ns) 32b Multiplier Wallace Dadda GOMIL ArithmeticTree DesignWare DOMAC Fig. 4. Pareto frontiers of the synthesized results on multipliers. 400 500 600 Area (um 2) 0.8 1.0 1.2 1.4 Delay (ns) 8b MAC 1500 1750 2000 2250 Area (um 2) 1.2 1.4 1.6 Delay (ns) 16b MAC 6000 7000 8000 9000 Area (um 2) 1.4 1.6 1.8 2.0 Delay (ns) 32b MAC Wallace Dadda DesignWare DOMAC Fig. 5. Pareto frontiers of the synthesized results on MACs. 8 16 32 Bit Width 0 50 100 150 200 250 300 Time (min) 1.0 4.0 18.0 38.8 80.7 91.8 49.2 135.6 242.4 DOMAC GOMIL ArithmeticTree Fig. 6. Runtime comparison between different methods. reduction in area. GOMIL produces inferior results than DOMAC, as it does not account for physical implementation variations or interconnection orders. ArithmeticTree fails to effectively generate new Pareto-optimal designs, suggesting that the reinforcement learn- ing agents struggle to explore the extensive design space under constrained computational budgets. Fig. 5 presents the synthesis results of multiply-accumulators. Consistent with the trends observed in Fig. 4, DOMAC demonstrates superior area efficiency to commer- cial IPs under relaxed timing constraints and delivers competitive performance when optimized for high-speed operation. We also evaluate the runtime of DOMAC for optimizing multiplier designs, which is shown in Fig. 6. For all bit width configurations, DOMAC requires less than 30 minutes, which is more efficient than the ILP-based approach GOMIL and RL-based method Arithmetic- Tree. The enhanced efficiency of DOMAC can be primarily attributed to two key factors. First, the differentiable formulation of the original CT optimization problem enables more efficient resolution compared to its discrete counterparts. Second, our differentiable solver takes advantage of the well-established automatic differentiation engine from the PyTorch toolkit. V. CONCLUSION This paper introduces DOMAC, a differentiable optimization framework designed to synthesize high-speed multipliers and MACs. The core insight lies in establishing a parallel between compres- sor tree optimization and DNN training, thereby enabling efficient differentiable optimization for compressor trees. In future work, we plan to leverage GPU acceleration to further enhance optimization efficiency and extend the methodology to other critical components of multipliers, such as partial product generators and carry-propagate adders. ACKNOWLEDGEMENT This work is supported in part by Beijing Natural Science Foun- dation (Grant No. L243001), National Natural Science Foundation of China (Grant No. 62032001), and 111 Project (B18001). REFERENCES [1] Y. Feng and C. Wang, Gomarl: Global optimization of multiplier using multi-agent reinforcement learning, in 2024 2nd International Symposium of Electronics Design Automation (ISEDA). IEEE, 2024, pp. 728 733. [2] C. S. Wallace, A suggestion for a fast multiplier, IEEE Transactions on electronic Computers, no. 1, pp. 14 17, 1964. [3] L. Dadda, Some schemes for parallel multipliers. IEEE Computer Society Press, 1990. [4] M. Nagamatsu, S. Tanaka, J. Mori, K. Hirano, T. Noguchi, and K. Hatanaka, A 15-ns 32 32-b cmos multiplier with an improved parallel structure, IEEE Journal of Solid-State Circuits, vol. 25, no. 2, pp. 494 497, 1990. [5] M. Mehta, V. Parmar, and E. Swartzlander, High-speed multiplier de- sign using multi-input counter and compressor circuits, in Proceedings 10th IEEE Symposium on Computer Arithmetic. IEEE Computer Society, 1991, pp. 43 44. [6] P. J. Song and G. De Micheli, Circuit and architecture trade-offs for high-speed multiplication, IEEE Journal of Solid-State Circuits, vol. 26, no. 9, pp. 1184 1198, 1991. [7] K. C. Bickerstaff, M. Schulte, and E. E. Swartzlander, Reduced area multipliers, in Proceedings of International Conference on Application Specific Array Processors (ASAP 93). IEEE, 1993, pp. 478 489. [8] J. Fadavi-Ardekani, M n booth encoded multiplier generator using optimized wallace trees, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 1, no. 2, pp. 120 125, 1993. [9] W. Xiao, W. Qian, and W. Liu, Gomil: Global optimization of multiplier by integer linear programming, in 2021 Design, Automation Test in Europe Conference Exhibition (DATE). IEEE, 2021, pp. 374 379. [10] V. G. Oklobdzija, D. Villeger, and S. S. Liu, A method for speed optimized partial product reduction and generation of fast parallel multi- pliers using an algorithmic approach, IEEE Transactions on computers, vol. 45, no. 3, pp. 294 306, 1996. [11] D. Zuo, J. Zhu, C. Li, and Y. Ma, Ufo-mac: A unified framework for op- timization of high-performance multipliers and multiply-accumulators, arXiv preprint arXiv:2408.06935, 2024. [12] D. Zuo, Y. Ouyang, and Y. Ma, Rl-mul: Multiplier design optimization with deep reinforcement learning, in 2023 60th ACM IEEE Design Automation Conference (DAC). IEEE, 2023, pp. 1 6. [13] Y. Lai, J. Liu, D. Z. Pan, and P. Luo, Scalable and effective arith- metic tree generation for adder and multiplier designs, arXiv preprint arXiv:2405.06758, 2024. [14] J. Bhasker and R. Chadha, Static timing analysis for nanometer designs: A practical approach. Springer Science Business Media, 2009. [15] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, Advances in neural information processing systems, vol. 32, 2019. [16] J. Sklansky, Conditional-sum addition logic, IRE Transactions on Electronic computers, no. 2, pp. 226 231, 1960. [17] NanGate Inc, Nangate freepdk45 open cell library, 2008. [18] Y. Lin, S. Dhar, W. Li, H. Ren, B. Khailany, and D. Z. Pan, Dreamplace: Deep learning toolkit-enabled gpu acceleration for modern vlsi place- ment, in Proceedings of the 56th Annual Design Automation Conference 2019, 2019, pp. 1 6. [19] Z. Guo and Y. Lin, Differentiable-timing-driven global placement, in Proceedings of the 59th ACM IEEE Design Automation Conference, 2022, pp. 1315 1320. [20] Y. Du, Z. Guo, Y. Lin, R. Wang, and R. Huang, Fusion of global placement and gate sizing with differentiable optimization, 2024. [21] I. Synopsys, Designware library - datapath and building block ip, https: www.synopsys.com dw buildingblock.php, 2024. [22] , Dc ultra concurrent timing, area, power and test optimization, signoff datasheets dc-ultra-ds.pdf, 2018.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nDOMAC: Differentiable Optimization for High-Speed Multipliers and Multiply-Accumulators Chenhao Xue1, Yi Ren1,2, Jinwei Zhou3, Kezhi Li4, Chen Zhang5, Yibo Lin1,6,7, Lining Zhang8, Qiang Xu4,9, Guangyu Sun1,6,7, 1School of Integrated Circuits, 2School of Software and Microelectronics, Peking University, Beijing, China 3School of Integrated Circuits, Anhui Polytechnic University, Wuhu, China 4Department of Computer Science and Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong S.A.R. 5Shanghai Jiao Tong University, Shanghai, China 6Institute of Electronic Design Automation, Peking University, Wuxi, China 7Beijing Advanced Innovation Center for Integrated Circuits, Beijing, China 8School of Electronic and Computer Engineering, Peking University, Shenzhen, China 9National Center of Technology Innovation for EDA, Nanjing, China {xch927027, yibolin, eelnzhang, {kzli24, Abstract Multipliers and multiply-accumulators (MACs) are funda- mental building blocks for compute-intensive applications such as artifi- cial intelligence. With the diminishing returns of Moore s Law, optimizing multiplier performance now necessitates process-aware architectural innovations rather than relying solely on technology scaling. In this paper, we introduce DOMAC, a novel approach that employs differentiable optimization for designing multipliers and MACs at specific technology nodes. DOMAC establishes an analogy between optimizing multi-staged parallel compressor trees and training deep neural networks. Building on this insight, DOMAC reformulates the discrete optimization challenge into a continuous problem by incorporating differentiable timing and area objectives. This formulation enables us to utilize existing deep learning toolkit for highly efficient implementation of the differentiable solver. Experimental results demonstrate that DOMAC achieves significant enhancements in both performance and area efficiency compared to state- of-the-art baselines and commercial IPs in multiplier and MAC designs. Index Terms Differentiable Optimization, Multiplier, Multiply- Accumulator, Compressor Tree I.\n\n--- Segment 2 ---\nExperimental results demonstrate that DOMAC achieves significant enhancements in both performance and area efficiency compared to state- of-the-art baselines and commercial IPs in multiplier and MAC designs. Index Terms Differentiable Optimization, Multiplier, Multiply- Accumulator, Compressor Tree I. INTRODUCTION In digital circuit design, multipliers and multiply-accumulators (MACs) constitute fundamental building blocks, playing a critical role in compute-intensive applications such as artificial intelligence and high-performance computing. As these modules play pivotal roles in determining the performance and efficiency of the entire hardware systems, optimizing for high-speed multipliers and MACs has become increasingly imperative. Modern multipliers and MACs typically consist of three primary components: a partial product generator (PPG), a compression tree (CT), and a final carry-propagate adder (CPA). The CT realizes parallel compression of the partial product array into two rows, which are then processed by the CPA to produce the final result. Previous research has shown that CT accounts for more than 50 of the delay and area in the entire multiplier, underscoring its critical importance in the overall design [1]. The optimization of high-speed CT has been a focal point of research for decades. Early advancements begin with classical struc- tures such as Wallace tree [2] and Dadda tree [3], followed by numerous improvements that introduce compressor variants [4] [6] or customized architectures [7], [8] optimized for specific tech- nology nodes. However, these approaches heavily rely on domain expertise and require laborious redesign efforts when migrating to Corresponding author: Guangyu Sun new technology nodes, which can significantly delay time-to-market. Algorithmic solutions employ heuristic strategies and mathematical programming to automate the exploration of CT architecture variants. GOMIL adopts integer linear programming (ILP) to adjust compres- sor assignment for reduced area [9]. The three-dimensional method proposes a heuristic approach to adjust compressor interconnection for balanced delay paths [10]. UFO-MAC leverages ILP to achieve global optimization of compressor interconnection, enhancing the overall speed [11]. Nevertheless, these methods exhibit a significant limitation in oversimplifying the timing and area characteristics of compressors. Specifically, they assume fixed latency and area for compressors; whereas in practical logic synthesis, each compressor may be mapped to implementations with varying timing and area characteristics.\n\n--- Segment 3 ---\nNevertheless, these methods exhibit a significant limitation in oversimplifying the timing and area characteristics of compressors. Specifically, they assume fixed latency and area for compressors; whereas in practical logic synthesis, each compressor may be mapped to implementations with varying timing and area characteristics. Additionally, factors such as signal transition time and capacitive load can introduce timing variations, which are not fully accounted for. Consequently, the oversimplification may lead to suboptimal solutions for the targeted technology nodes. More recently, reinforcement learning (RL) has been employed to search for physically-optimized multiplier and MAC designs [1], [12], [13]. While delivering promising results, RL-based methods suffer from prohibitive computational complexity, as training RL agents requires extensive trial-and-error iterations and expensive invocations of synthesis tools during exploration. In summary, the process- aware architectural optimization for high-performance multipliers and MACs urgently requires automated solutions that balance optimiza- tion efficiency and effectiveness. To resolve the aforementioned limitations, we introduce an inno- vative solution that leverages differentiable optimization to generate technology-specific high-speed multipliers and MACs, referred to as DOMAC. DOMAC evaluates the performance of CTs with detailed static timing analysis process (STA) [14], and jointly manipulates compressor interconnection and technology mapping to improve post- synthesis timing and area. To efficiently address this large-scale combinatorial optimization problem, we establish an analogy be- tween optimizing multi-staged CT and training deep neural networks (DNNs). Based on the analogy, DOMAC proposes differentiable timing and area objectives with respect to CT interconnection and implementation, which enables highly efficient optimization via gra- dient descent.\n\n--- Segment 4 ---\nTo efficiently address this large-scale combinatorial optimization problem, we establish an analogy be- tween optimizing multi-staged CT and training deep neural networks (DNNs). Based on the analogy, DOMAC proposes differentiable timing and area objectives with respect to CT interconnection and implementation, which enables highly efficient optimization via gra- dient descent. The key contributions of this work are summarized as follows: arXiv:2503.23943v1 [cs.AR] 31 Mar 2025 Partial Product Generator Carry-Propagate Adder Carry-Propagate Adder Partial Product Generator 1 0 2 3 4 5 7 6 8 9 10 Compressor Tree 1 0 2 3 4 5 7 6 8 9 𝑨𝑨(𝑵𝑵-bit) 𝑩𝑩(𝑵𝑵-bit) 𝑨𝑨(𝑵𝑵-bit) 𝑩𝑩(𝑵𝑵-bit) 𝑪𝑪(𝟐𝟐𝑵𝑵-bit) Stage 1 Stage 2 Stage 3 Stage 1 Stage 2 Stage 3 𝑹𝑹(2𝑵𝑵-bit) 𝑹𝑹(2𝑵𝑵-bit) 2:2 compressor 3:2 compressor a Multiplier Architecture b Fused MAC Architecture Fig. 1. Architecture of (a) multipliers and (b) fused multipliy-accumulators We propose DOMAC, a high-speed multiplier and MAC gener- ation framework that jointly optimizes compressor interconnec- tions and compressor physical implementations. We analogize CT optimization with training DNNs and in- troduce differentiable timing and area objectives for efficient optimization of post-synthesis QoRs. We implement a highly efficient differentiable solver using established deep learning toolkit PyTorch [15]. Experiment results demonstrate that DOMAC optimized mul- tipliers and MACs exceed all baseline designs, with up to 6.5 improvement in delay and 25 reduction in area over commercial IPs. The rest of this paper is organized as follows: Section II provides preliminaries of multiplier and MAC design. Section III details DOMAC framework. Section IV presents the experimental results. Finally, Section V concludes this paper. II.\n\n--- Segment 5 ---\nFinally, Section V concludes this paper. II. PRELIMINARIES A. Multiplier Architecture As shown in Fig. 1(a), a high-speed multiplier comprises three main components: a partial product generator (PPG), a compressor tree (CT), and a carry-propagate adder (CPA). Partial Product Generator: The PPG generates an array of partial products (PPs), where each PP is shifted into different columns to represent varied power-of-two weights. A commonly employed AND gate-based PPG generates N 2 PPs for an N-bit multiplier. Compressor Tree: The CT compresses PPs for multiple stages until a maximum of two PPs remain in each column. CT typically consists of various types of compressors, with 3:2 and 2:2 compressors being predominantly used. Carry-Propagate Adder: The CPA aggregates the compressed PPs from the CT to produce the final product. High-speed 3:2 Comp. 2:2 Comp. A B CI A B A CO S CO S Z 𝒗𝒗𝟎𝟎 𝒗𝒗𝟏𝟏 𝒗𝒗𝟐𝟐 𝒗𝒗𝟑𝟑 𝒗𝒗𝟒𝟒 𝒗𝒗𝟓𝟓 𝒖𝒖𝟎𝟎 𝒖𝒖𝟏𝟏 𝒖𝒖𝟐𝟐 𝒖𝒖𝟑𝟑 𝒖𝒖𝟒𝟒 𝒖𝒖𝟓𝟓 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 Interconnection Matrix 𝑀𝑀𝑖𝑖,𝑗𝑗 Fig. 2. Example of adjusting compressor tree interconnection, where the partial products and compressor inputs can be assigned interchangeably.\n\n--- Segment 6 ---\n2. Example of adjusting compressor tree interconnection, where the partial products and compressor inputs can be assigned interchangeably. A B CI CO S FA_X1 CO S A B CI (a) (b) 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐴𝐴 𝐶𝐶𝐶𝐶 0.0432 𝑛𝑛𝑛𝑛 0.0802 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐵𝐵 𝐶𝐶𝐶𝐶 0.0444 𝑛𝑛𝑛𝑛 0.0830 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐶𝐶𝐶𝐶 𝐶𝐶𝐶𝐶 0.0472 𝑛𝑛𝑛𝑛 0.0743 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐴𝐴 𝑆𝑆 0.0935 𝑛𝑛𝑛𝑛 0.1194 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐵𝐵 𝑆𝑆 0.0946 𝑛𝑛𝑛𝑛 0.1263 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐶𝐶𝐶𝐶 𝑆𝑆 0.0459 𝑛𝑛𝑛𝑛 0.1174 𝑛𝑛𝑛𝑛 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎 6.650 𝜇𝜇𝑚𝑚2 4.256 𝜇𝜇𝑚𝑚2 (a) Composition of Basic Standard Cells (b) Using Specialized Standard Cell (c) Delay Area Comparison Fig.\n\n--- Segment 7 ---\nExample of adjusting compressor tree interconnection, where the partial products and compressor inputs can be assigned interchangeably. A B CI CO S FA_X1 CO S A B CI (a) (b) 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐴𝐴 𝐶𝐶𝐶𝐶 0.0432 𝑛𝑛𝑛𝑛 0.0802 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐵𝐵 𝐶𝐶𝐶𝐶 0.0444 𝑛𝑛𝑛𝑛 0.0830 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐶𝐶𝐶𝐶 𝐶𝐶𝐶𝐶 0.0472 𝑛𝑛𝑛𝑛 0.0743 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐴𝐴 𝑆𝑆 0.0935 𝑛𝑛𝑛𝑛 0.1194 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐵𝐵 𝑆𝑆 0.0946 𝑛𝑛𝑛𝑛 0.1263 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐶𝐶𝐶𝐶 𝑆𝑆 0.0459 𝑛𝑛𝑛𝑛 0.1174 𝑛𝑛𝑛𝑛 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎 6.650 𝜇𝜇𝑚𝑚2 4.256 𝜇𝜇𝑚𝑚2 (a) Composition of Basic Standard Cells (b) Using Specialized Standard Cell (c) Delay Area Comparison Fig. 3.\n\n--- Segment 8 ---\nA B CI CO S FA_X1 CO S A B CI (a) (b) 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐴𝐴 𝐶𝐶𝐶𝐶 0.0432 𝑛𝑛𝑛𝑛 0.0802 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐵𝐵 𝐶𝐶𝐶𝐶 0.0444 𝑛𝑛𝑛𝑛 0.0830 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐶𝐶𝐶𝐶 𝐶𝐶𝐶𝐶 0.0472 𝑛𝑛𝑛𝑛 0.0743 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐴𝐴 𝑆𝑆 0.0935 𝑛𝑛𝑛𝑛 0.1194 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐵𝐵 𝑆𝑆 0.0946 𝑛𝑛𝑛𝑛 0.1263 𝑛𝑛𝑛𝑛 𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑑𝑦𝑦𝐶𝐶𝐶𝐶 𝑆𝑆 0.0459 𝑛𝑛𝑛𝑛 0.1174 𝑛𝑛𝑛𝑛 𝑎𝑎𝑎𝑎𝑎𝑎𝑎𝑎 6.650 𝜇𝜇𝑚𝑚2 4.256 𝜇𝜇𝑚𝑚2 (a) Composition of Basic Standard Cells (b) Using Specialized Standard Cell (c) Delay Area Comparison Fig. 3. Example of two different implementations for 3:2 compressor using Nangate45 Open Cell Library [17].\n\n--- Segment 9 ---\n3. Example of two different implementations for 3:2 compressor using Nangate45 Open Cell Library [17]. We estimate the delay using a nonlinear delay model (NLDM) with islew 0.02ns and oload 3fF for all input- output pairs. multipliers often utilize parallel prefix adders [16] for fast computation. The fused multiply-accumulator (fused MAC) integrates the ac- cumulation operation into the compressor tree to boost computation efficiency. As illustrated in Fig. 1(b), fused MAC exhibits a similar architecture as multiplier. B. Problem Formulation DOMAC focuses on exploring the design space of compressor trees (CTs), one of the most critical modules influencing the performance of multipliers and MACs. This section examines the comprehensive CT design space and formulates the design space exploration problem addressed by DOMAC. Compressor Tree Design Space Factors that affect CT perfor- mance are as follows: Compressor Type: Different types of compressors have similar functions in reducing the number of partial products but differ in terms of the number of inputs or outputs. For a fair comparison with previous methods [2], [3], [9], [13], DOMAC utilizes commonly adopted 3:2 and 2:2 compressors to build CTs. However, DOMAC can be easily extended to support other types of compressors. Compressor Quantity: Each compression stage should comprise a proper amount of compressors. DOMAC utilizes a designated compressor assignment for subsequent optimization. Without loss of generality, DOMAC adopts the quantity derived from classical architectures such as Wallace tree [2] and Dadda tree [3] . Compressor Interconnection: As shown in Fig. 2, the partial products and input ports of compressors located at column i and stage j must be mapped bijectively. Due to the asso- ciative property of addition, this mapping can be arbitrarily permuted without affecting the correctness of final results. We use binary matrix Mi,j to represent the interconnection, where Mi,j[u, v] 1 indicates that the u-th partial product is mapped to the v-th input port. Each row and column in matrix Mi,j sums to 1, making Mi,j a doubly stochastic matrix. Compressor Implementation: As shown in Fig. 3, a compressor may have different implementations with distinct timing and area characteristics.\n\n--- Segment 10 ---\nCompressor Implementation: As shown in Fig. 3, a compressor may have different implementations with distinct timing and area characteristics. For each compressor c, we use one-hot vector pc to denote its physical implementation, in which pc[k] 1 indicates that c adopts the k-th implementation from the available set Pc. Problem (Compressor Tree Design Space Exploration) Given an initial compressor set C where each compressor c has been assigned to a specific column and stage, the objective is to minimize the absolute values of worst negative slack (WNS), total negative slack (TNS), and total area by simultaneously adjusting compressor interconnection M and compressor physical implementation p. The formal definitions are presented as follows: min M,p t1WNS(M, p) t2TNS(M, p) αArea(p) subject to Mi,j is doubly stochastic, Mi,j M; X k pc[k] 1, c C; (1) The combinatorial optimization problem in Equation (1) presents challenges for conventional discrete solvers. On one hand, the static timing analysis process is highly complex [14], making it difficult to incorporate into standard solvers without substantial simplification. On the other hand, the problem involves numerous variables and intricate constraints, which render discrete optimization algorithms inefficient. III. METHODOLOGY In this section, we propose the differentiable framework for op- timizing technology-specific multipliers and MACs. Section III-A presents an inspiration from deep neural network. Section III-B presents an overview of the framework. Sections III-C, III-D, and III-E give the mathematical details of the area, timing, and constraint objectives. Section III-F details the selection of hyperpa- rameters. A. Inspiration from Deep Neural Networks Recent advancements in the theory and practice of machine learn- ing have brought new inspiration for electronic design automation (EDA). Drawing parallels between EDA challenges and machine learning problems, researchers have developed novel algorithms to address analytical placement [18], [19] and gate sizing [20]. In DOMAC, we expand this approach to the efficient synthesis of technology-specific high-performance multipliers and MAC units. As illustrated in TABLE I, delay propagation in multi-staged com- pressor trees exhibits a striking analogy to the forward propagation process of deep neural networks (DNNs).\n\n--- Segment 11 ---\nIn DOMAC, we expand this approach to the efficient synthesis of technology-specific high-performance multipliers and MAC units. As illustrated in TABLE I, delay propagation in multi-staged com- pressor trees exhibits a striking analogy to the forward propagation process of deep neural networks (DNNs). 1) The net delay propagation from a set of compressor output ports u to the corresponding set of compressor input ports v is determined by interconnection Mi,j. This process can be formulated as AT(v) M T i,j AT(u), which resembles the linear projection WX in DNNs. 2) The cell delay propagation involves adding the internal cell delay to the arrival time at the compressor input ports, which is analogous to adding biases b in DNNs. 3) The timing objective WNS and TNS is computed by measuring the difference between final arrival time and required arrival TABLE I ANALOGY BETWEEN OPTIMIZING COMPRESSOR TREE DESIGNS AND TRAINING DEEP NEURAL NETWORKS. DNN Training CT Optimization Activations X Arrival time AT Weights W Interconnection M Biases b Cell delay d Loss function L Objective WNS, TNS time, which is similar to evaluating the consistency between DNN outputs and data labels through loss function L. The analogy in TABLE I inspires us to optimize CT design using a similar approach to training DNNs. In practice, DNNs are efficiently trained with backpropagation (BP), where weights W and biases b are updated using the gradient of loss function L w.r.t. these parameters. However, BP cannot be directly adopted due to fundamental distinction between the two problems: CT de- sign involves discrete interconnections and physical implementations, whereas DNN parameters are continuous in nature. To address this challenge, we propose to relax the discrete CT optimization problem into a continuous formulation. In a nutshell, we relax the discrete selections of compressor interconnection M and compressor implementation p to continuous variables, making them essentially represent probabilistic distributions. Therefore, we can estimate the expectation of intermediate values in STA, such as signal arrival time and signal transition time (slew), and ultimately derive differentiable timing objectives. We use gradient descent to update M and p in continuous domain and then adopt legalization process to map them back to discrete domain.\n\n--- Segment 12 ---\nTherefore, we can estimate the expectation of intermediate values in STA, such as signal arrival time and signal transition time (slew), and ultimately derive differentiable timing objectives. We use gradient descent to update M and p in continuous domain and then adopt legalization process to map them back to discrete domain. Consequently, a large portion of the CT optimization problem can be efficiently addressed following the same process as DNN training. Furthermore, we incorporate area objectives to balance the performance and area footprints, as well as constraint objectives to ensure the accuracy of timing and area estimations during continuous optimization. The details will be discussed in the following subsections. B. Framework Overview The DOMAC framework consists of the following steps: 1) Differentiable Optimization: Solve the continuous formulation of the CT optimization problem, which is essentially analogous to training DNNs. Leveraging the established deep learning toolkit PyTorch [15], we efficiently implement our differen- tiable solver. 2) Legalization: Map the continuous solution to a discrete solu- tion. For each compressor interconnection Mi,j M, we em- ploy the Hungarian algorithm to identify the bipartite matching with the maximum probability summation. For each compressor implementation pc p, we apply the argmax operation to select the implementation with the highest probability. 3) Netlist Generation: Generate the synthesized gate-level CT netlists, which can be seamlessly integrated into multiplier and MAC designs. C. Differentiable Area Objective We begin with the total area, which is the simplest objective. For each compressor c, the area of all available implementations is represented by a vector ac. The area expectation can be derived from the probabilistic distribution pc: Areac(pc) X k pc[k] ac[k] (2) The total area of the compressor tree is the sum of the expected area across all compressors: Area(p) X c Areac(pc) (3) D. Differentiable Timing Objective Compared with area objectives, timing objectives are quite com- plex. The compressor implementation p affects the cell delay char- acteristics, whereas the interconnection M affects the delay propa- gation. Moreover, p and M jointly affect the capacitive loads, which further complicates the process of static timing analysis (STA).\n\n--- Segment 13 ---\nThe compressor implementation p affects the cell delay char- acteristics, whereas the interconnection M affects the delay propa- gation. Moreover, p and M jointly affect the capacitive loads, which further complicates the process of static timing analysis (STA). Next, we detail each stage of the differentiable STA in DOMAC: 1) Pin Load Estimation: Non-linear delay model (NLDM) is extensively adopted to characterize cell timing statistics in STA [14]. In NLDM, cell delay and slew are determined through look-up tables (LUTs). Given input slew and capacitive load of the cell, the delay and output slew are computed using linear interpolation or extrapolation based on a corresponding 2 2 submatrix within the LUT. Therefore, our first step is to derive cell capacitive load before timing propagation. Since both M and p represent probabilistic distributions, the exact values of capacitive load are inaccessible. Our best effort is to compute the expected capacitive load: Cap(v) X k pc[k] Cap(v; Pc[k]) (4a) Load(u) X child v Mi,j[u, v] Cap(v) (4b) where Cap(v) denotes the expected capacitance at the input pin v of the next-level compressor, and Load(u) denotes the expected capacitive load at the output pin u of previous-level compressor. 2) Cell Delay Propagation: Next, we utilize NLDM to compute the delay, slew, and arrival times: Slewu(v) X k pc[k] LUTslew u v(Slew(u), Load(v); Pc[k]) (5a) Delayu(v) X k pc[k] LUTdelay u v(Slew(u), Load(v); Pc[k]) (5b) AT(v) LSEγ{AT(u) Delayu(v) input u} (5c) Slew(v) LSEγ{Slewu(v) input u} (5d) In Equation (5a) and Equation (5b) , u and v are the input pin and output pin of compressor c, respectively.\n\n--- Segment 14 ---\nOur best effort is to compute the expected capacitive load: Cap(v) X k pc[k] Cap(v; Pc[k]) (4a) Load(u) X child v Mi,j[u, v] Cap(v) (4b) where Cap(v) denotes the expected capacitance at the input pin v of the next-level compressor, and Load(u) denotes the expected capacitive load at the output pin u of previous-level compressor. 2) Cell Delay Propagation: Next, we utilize NLDM to compute the delay, slew, and arrival times: Slewu(v) X k pc[k] LUTslew u v(Slew(u), Load(v); Pc[k]) (5a) Delayu(v) X k pc[k] LUTdelay u v(Slew(u), Load(v); Pc[k]) (5b) AT(v) LSEγ{AT(u) Delayu(v) input u} (5c) Slew(v) LSEγ{Slewu(v) input u} (5d) In Equation (5a) and Equation (5b) , u and v are the input pin and output pin of compressor c, respectively. The slew and delay from u to v are estimated using the LUTs extracted from .lib files in the process design kit (PDK). In practice, the slew and delay for each output pin may correspond to multiple LUTs, each associated with a specific input assignment and signal edge (rise or fall). In DOMAC, we extract LUTs corresponding to the worst-case scenarios, where each entry in the LUT is selected from the maximum value across all possible conditions. Similar to Equation (4), we estimate the expectation of delay and slew under probabilistic implementation pc. In Equation (5c) and Equation (5d), we compute the arrival time and slew of pin v by gathering the maximum value from all the related input u. We replace the max operation with a smoothed approxi- mation, the Log-Sum-Exp (LSE), as defined in Equation (6). This approach has been adopted by previous works to alleviate oscillation and instability in the differentiable optimization process [18], [19].\n\n--- Segment 15 ---\nWe replace the max operation with a smoothed approxi- mation, the Log-Sum-Exp (LSE), as defined in Equation (6). This approach has been adopted by previous works to alleviate oscillation and instability in the differentiable optimization process [18], [19]. The hyperparameter γ controls the trade-off between the smoothness and approximation accuracy. LSEγ(x1, x2, , xn) γ log( n X i 1 exp xi γ ) (6) 3) Net Delay Propagation: Next, we consider net delay propa- gation. In compressor trees where all nets exhibit limited fanouts and lengths, the slew variation and delay introduced by the nets themselves are negligible. Therefore, we mainly consider the impact of probabilistic interconnection M on net delay propagation. As discussed in Section III-D1, we derive the expected slew and arrival time of next-level compressor input pin v: Slew(v) X parent u Mi,j[u, v] Slew(u) (7a) AT(v) X parent u Mi,j[u, v] AT(u) (7b) where the slew and arrival time of previous-level compressor output pin u have been computed as detailed in Section III-D2. 4) Timing Slack Estimation: Finally, we derive the timing slacks as optimization objectives. For complex combinatorial logic such as compressor tree, our primary focus is to avoid setup timing violations, which occur when the setup slack becomes negative. Specifically, we aim to minimize the absolute value of worst negative slack (WNS) and total negative slack (TNS) defined as follows: Slack(u) RAT(u) AT(u) (8a) WNS(M, p) LSEγ{min(0, Slack(u)) output u} (8b) TNS(M, p) X output u min(0, Slack(u)) (8c) where u is the primary output pin of the compressor tree. RAT(u) is the required arrival time for pin u, assigned by the designer. We replace the max operation in WNS with LSE for better smoothness, following the same discussion in Section III-D2.\n\n--- Segment 16 ---\nRAT(u) is the required arrival time for pin u, assigned by the designer. We replace the max operation in WNS with LSE for better smoothness, following the same discussion in Section III-D2. E. Optimization Constraints Given the relaxed continuous version of the problem defined in Equation (1), we aim to ensure the differentiable optimization yields valid discrete solutions. This is achieved through variable substitution and the introduction of constraint loss functions. 1) Variable Substitution: For a valid probabilistic distribution of compressor implementation pc, the summation of all elements must equal 1. To satisfy this constraint, we introduce auxiliary variables pc R Pc and compute pc using the softmax function: pc softmax( pc) [ e pc[0] P k e pc[k] , , e pc[ P 1] P k e pc[k] ] (9) For the doubly stochastic matrix Mi,j representing compressor in- terconnection, we adopt a similar approach to ensure the sum of each row equals 1. Specifically, we introduce auxiliary variables Mi,j Rl l, where l is the number of partial products at column i and stage j. The matrix Mi,j is computed as: Mi,j[u, ] softmax( Mi,j[u, ]) (10) Variable substitution guarantees that the probabilistic constraints are consistently upheld while permitting the auxiliary variables to be freely adjusted. 2) Bijective Mapping Loss: The doubly stochastic property of Mi,j also demands the sum of each column equals 1, which is fulfilled by introducing the bijective mapping loss in Equation (11). The quadratic form is chosen for better smoothness. LBM(M) X i,j,u ( X v Mi,j[u, v] 1)2 (11) 3) Discretization Loss: To encourage continuous variables to converge to bimodal states (0 or 1), we introduce the discretization loss in Equation (12).\n\n--- Segment 17 ---\nThe quadratic form is chosen for better smoothness. LBM(M) X i,j,u ( X v Mi,j[u, v] 1)2 (11) 3) Discretization Loss: To encourage continuous variables to converge to bimodal states (0 or 1), we introduce the discretization loss in Equation (12). This function is minimized at 0 and 1 and exhibits smooth gradients near these minima: LD(x) x2 (1 x)2 (12a) LD(M, p) X i,j,u,v LD(Mi,j[u, v]) X c,k LD(pc[k]) (12b) F. Hyperparameter Selection In the previous sections, we have reformulated the discrete CT optimization problem into a differentiable optimization problem. The complete objective function is summarized as follows: min M, p t1WNS(M, p) t2TNS(M, p) αArea(p) λ1LD(M, p) λ2LBM(M) (13) where t1, t2, α, λ1, λ2 are weights for different objectives. Without loss of generality, we assume the timing slacks are measured in nanoseconds ns and area are measured in square micrometers µm2. The optimization process is conducted over 300 iterations, with incremental adjustments to the hyperparameters starting from the 100th iteration. This approach ensures a balanced trade-off among the objectives at different stages of the optimization. Specifically, α is set between 1 and 5 to trade-off timing and area, and is increased by 0.3 every iteration to counterbalance timing objectives. t1 and t2 are set to 1 and 0.01, respectively, and are increased by 0.5 every iteration to prioritize timing optimization in the later stages. We set required arrival time RAT(u) 0 for all output ports u. λ1 and λ2 are set to 0.1 and 0.5, respectively, and are increased by 1 per iteration to ensure that design constraints are adequately respected without excessively disrupting the optimization process. The smoothing factor γ for LSE operations is set to 0.01, which provides sufficient approximation accuracy and smoothness for differentiable optimization. IV. EXPERIMENT RESULTS A.\n\n--- Segment 18 ---\nIV. EXPERIMENT RESULTS A. Experiment Setup All experiments are conducted on a Linux-based platform equipped with 2.8GHz 96-core Intel Xeon Gold 6342 CPUs and 1.5TB memory. Unless specified otherwise, all multipliers and MACs adopt AND-based PPG and default CPA instantiated with s a b style RTL. We compare the multipliers and MACs generated by DO- MAC with several baseline approaches, including: Wallace trees [2], Dadda trees [3], and commercial IPs from Synopsys DesignWare Library [21]. The commercial IPs are instantiated with r a b and r a b c style RTL. Additionally, the multipliers are compared with those produced by GOMIL [9] (ILP-based) and ArithmeticTree [13] (RL-based) leveraging their open-sourced implementation. All de- signs are synthesized by Synopsys Design Compiler (Version R- 2020.09-SP2) [22] using Nangate 45nm Open Cell Library [17] and the compile_ultra command. To illustrate the trade-off between delay and area, we sweep the target delay constraints from 0ns to 2ns. B. Results and Analysis Fig. 4 illustrates the synthesized results of multipliers, displaying the Pareto frontiers with respect to delay and area. In most cases, DOMAC achieves superior performance over all baseline methods. Our framework identifies designs that Pareto-dominate those of the commercial IPs, delivering up to 6.5 reduction in delay and 25 300 400 500 600 Area (um 2) 0.8 1.0 1.2 1.4 Delay (ns) 8b Multiplier 1500 1750 2000 Area (um 2) 1.2 1.4 1.6 Delay (ns) 16b Multiplier 6000 7000 8000 Area (um 2) 1.4 1.6 1.8 2.0 Delay (ns) 32b Multiplier Wallace Dadda GOMIL ArithmeticTree DesignWare DOMAC Fig. 4. Pareto frontiers of the synthesized results on multipliers.\n\n--- Segment 19 ---\n4. Pareto frontiers of the synthesized results on multipliers. 400 500 600 Area (um 2) 0.8 1.0 1.2 1.4 Delay (ns) 8b MAC 1500 1750 2000 2250 Area (um 2) 1.2 1.4 1.6 Delay (ns) 16b MAC 6000 7000 8000 9000 Area (um 2) 1.4 1.6 1.8 2.0 Delay (ns) 32b MAC Wallace Dadda DesignWare DOMAC Fig. 5. Pareto frontiers of the synthesized results on MACs. 8 16 32 Bit Width 0 50 100 150 200 250 300 Time (min) 1.0 4.0 18.0 38.8 80.7 91.8 49.2 135.6 242.4 DOMAC GOMIL ArithmeticTree Fig. 6. Runtime comparison between different methods. reduction in area. GOMIL produces inferior results than DOMAC, as it does not account for physical implementation variations or interconnection orders. ArithmeticTree fails to effectively generate new Pareto-optimal designs, suggesting that the reinforcement learn- ing agents struggle to explore the extensive design space under constrained computational budgets. Fig. 5 presents the synthesis results of multiply-accumulators. Consistent with the trends observed in Fig. 4, DOMAC demonstrates superior area efficiency to commer- cial IPs under relaxed timing constraints and delivers competitive performance when optimized for high-speed operation. We also evaluate the runtime of DOMAC for optimizing multiplier designs, which is shown in Fig. 6. For all bit width configurations, DOMAC requires less than 30 minutes, which is more efficient than the ILP-based approach GOMIL and RL-based method Arithmetic- Tree. The enhanced efficiency of DOMAC can be primarily attributed to two key factors. First, the differentiable formulation of the original CT optimization problem enables more efficient resolution compared to its discrete counterparts. Second, our differentiable solver takes advantage of the well-established automatic differentiation engine from the PyTorch toolkit. V. CONCLUSION This paper introduces DOMAC, a differentiable optimization framework designed to synthesize high-speed multipliers and MACs. The core insight lies in establishing a parallel between compres- sor tree optimization and DNN training, thereby enabling efficient differentiable optimization for compressor trees.\n\n--- Segment 20 ---\nV. CONCLUSION This paper introduces DOMAC, a differentiable optimization framework designed to synthesize high-speed multipliers and MACs. The core insight lies in establishing a parallel between compres- sor tree optimization and DNN training, thereby enabling efficient differentiable optimization for compressor trees. In future work, we plan to leverage GPU acceleration to further enhance optimization efficiency and extend the methodology to other critical components of multipliers, such as partial product generators and carry-propagate adders. ACKNOWLEDGEMENT This work is supported in part by Beijing Natural Science Foun- dation (Grant No. L243001), National Natural Science Foundation of China (Grant No. 62032001), and 111 Project (B18001). REFERENCES [1] Y. Feng and C. Wang, Gomarl: Global optimization of multiplier using multi-agent reinforcement learning, in 2024 2nd International Symposium of Electronics Design Automation (ISEDA). IEEE, 2024, pp. 728 733. [2] C. S. Wallace, A suggestion for a fast multiplier, IEEE Transactions on electronic Computers, no. 1, pp. 14 17, 1964. [3] L. Dadda, Some schemes for parallel multipliers. IEEE Computer Society Press, 1990. [4] M. Nagamatsu, S. Tanaka, J. Mori, K. Hirano, T. Noguchi, and K. Hatanaka, A 15-ns 32 32-b cmos multiplier with an improved parallel structure, IEEE Journal of Solid-State Circuits, vol. 25, no. 2, pp. 494 497, 1990. [5] M. Mehta, V. Parmar, and E. Swartzlander, High-speed multiplier de- sign using multi-input counter and compressor circuits, in Proceedings 10th IEEE Symposium on Computer Arithmetic. IEEE Computer Society, 1991, pp. 43 44. [6] P. J. Song and G. De Micheli, Circuit and architecture trade-offs for high-speed multiplication, IEEE Journal of Solid-State Circuits, vol. 26, no. 9, pp. 1184 1198, 1991.\n\n--- Segment 21 ---\n9, pp. 1184 1198, 1991. [7] K. C. Bickerstaff, M. Schulte, and E. E. Swartzlander, Reduced area multipliers, in Proceedings of International Conference on Application Specific Array Processors (ASAP 93). IEEE, 1993, pp. 478 489. [8] J. Fadavi-Ardekani, M n booth encoded multiplier generator using optimized wallace trees, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 1, no. 2, pp. 120 125, 1993. [9] W. Xiao, W. Qian, and W. Liu, Gomil: Global optimization of multiplier by integer linear programming, in 2021 Design, Automation Test in Europe Conference Exhibition (DATE). IEEE, 2021, pp. 374 379. [10] V. G. Oklobdzija, D. Villeger, and S. S. Liu, A method for speed optimized partial product reduction and generation of fast parallel multi- pliers using an algorithmic approach, IEEE Transactions on computers, vol. 45, no. 3, pp. 294 306, 1996. [11] D. Zuo, J. Zhu, C. Li, and Y. Ma, Ufo-mac: A unified framework for op- timization of high-performance multipliers and multiply-accumulators, arXiv preprint arXiv:2408.06935, 2024. [12] D. Zuo, Y. Ouyang, and Y. Ma, Rl-mul: Multiplier design optimization with deep reinforcement learning, in 2023 60th ACM IEEE Design Automation Conference (DAC). IEEE, 2023, pp. 1 6. [13] Y. Lai, J. Liu, D. Z. Pan, and P. Luo, Scalable and effective arith- metic tree generation for adder and multiplier designs, arXiv preprint arXiv:2405.06758, 2024. [14] J. Bhasker and R. Chadha, Static timing analysis for nanometer designs: A practical approach. Springer Science Business Media, 2009.\n\n--- Segment 22 ---\n[14] J. Bhasker and R. Chadha, Static timing analysis for nanometer designs: A practical approach. Springer Science Business Media, 2009. [15] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, Advances in neural information processing systems, vol. 32, 2019. [16] J. Sklansky, Conditional-sum addition logic, IRE Transactions on Electronic computers, no. 2, pp. 226 231, 1960. [17] NanGate Inc, Nangate freepdk45 open cell library, 2008. [18] Y. Lin, S. Dhar, W. Li, H. Ren, B. Khailany, and D. Z. Pan, Dreamplace: Deep learning toolkit-enabled gpu acceleration for modern vlsi place- ment, in Proceedings of the 56th Annual Design Automation Conference 2019, 2019, pp. 1 6. [19] Z. Guo and Y. Lin, Differentiable-timing-driven global placement, in Proceedings of the 59th ACM IEEE Design Automation Conference, 2022, pp. 1315 1320. [20] Y. Du, Z. Guo, Y. Lin, R. Wang, and R. Huang, Fusion of global placement and gate sizing with differentiable optimization, 2024. [21] I. Synopsys, Designware library - datapath and building block ip, https: www.synopsys.com dw buildingblock.php, 2024. [22] , Dc ultra concurrent timing, area, power and test optimization, signoff datasheets dc-ultra-ds.pdf, 2018.\n\n