=== ORIGINAL PDF: 2506.06773v1_Taming_Wild_Branches_Overcoming_Hard-to-Predict_Br.pdf ===\n\nRaw text length: 24790 characters\nCleaned text length: 24320 characters\nNumber of segments: 14\n\n=== CLEANED TEXT ===\n\narXiv:2506.06773v1 [cs.AR] 7 Jun 2025 Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor Emet Behrendt The University of British Columbia Canada Shing Wai Pun The University of British Columbia Canada Prashant J. Nair The University of British Columbia Canada Abstract Branch prediction is key to the performance of out-of-order proces- sors. While the CBP-2016 winner TAGE-SC-L combines geometric- history tables, a statistical corrector, and a loop predictor, over half of its remaining mispredictions stem from a small set of hard-to- predict (H2P) branches. These branches occur under diverse global histories, causing repeated thrashing in TAGE and eviction before usefulness counters can mature. Prior work shows that simply enlarging the tables offers only marginal improvement. We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P- targeted subsystem called the Bullseye predictor. It identifies prob- lematic PCs using a set-associative H2P Identification Table (HIT) and steers them to one of two branch-specific perceptrons, one indexed by hashed local history and the other by folded global history. A short trial phase tracks head-to-head accuracy in an H2P cache. A branch becomes perceptron-resident only if the percep- tron s sustained accuracy and output magnitude exceed dynamic thresholds, after which TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache, and perceptron operate fully in parallel with TAGE-SC-L, providing higher fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI of 145.09. CCS Concepts Computer systems organization Multicore architectures. Keywords Branch Prediction, Hard-to-Predict branches, TAGE, Perceptron ACM Reference Format: Emet Behrendt, Shing Wai Pun, and Prashant J. Nair. . Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor. In Pro- ceedings of 6th Championship Branch Prediction ( 6th Championship Branch Prediction (CBP) ). ACM, New York, NY, USA, 5 pages. 1 Introduction Modern out-of-order CPUs depend heavily on aggressive specula- tive execution, where each fetched instruction block is predicted to either fall through or take a branch long before the branch con- dition is resolved. A single misprediction triggers a pipeline flush, drains the front-end, and requires re-fetching along the correct path, typically incurring a penalty of 15 30 cycles. As a result, the design and accuracy of the branch predictor have direct and significant performance implications, affecting instruction throughput, energy efficiency, and speculation-dependent microarchitectural features. 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan . Branch prediction accuracy remains a first-order design con- straint, shaping front-end bandwidth, retirement rates, and secu- rity mitigation strategies. Over the past three decades, predictors have evolved from simple bimodal schemes to sophisticated hybrid, perceptron-based, and multi-component designs. The TAGE-SC- L predictor, the CBP-2016 competition winner, combines tagged geometric history tables (TAGE) with a statistical corrector and a loop predictor to cover a broad spectrum of control-flow behaviors. However, despite this architectural complexity, TAGE-SC-L still leaves substantial accuracy on the table, especially for a small but critical subset of dynamic branches. Despite TAGE-SC-L s advanced use of long-range geometric histories, statistical correction, and loop-specific components, its residual mispredictions remain highly skewed. Lin et al. [2] showed that in a 30-million-instruction SPEC-INT 2017 trace, over 50 of mispredictions stem from fewer than ten static hard-to-predict (H2P) branches. These branches exhibit volatile control-flow be- havior, with dynamic instances appearing under widely varying global histories. As a result, TAGE s tag-matching struggles to re- trieve consistent predictor state, leading to frequent allocations with low confidence. Since usefulness counters only increment on correct predictions, these entries are quickly evicted, creating a self-reinforcing cycle of misprediction, reallocation, and eviction that prevents the predictor from learning stable correlations. An H2P branch may see hundreds of distinct TAGE entries within a single millisecond of execution, none surviving long enough to gather confidence. Intuitively, one might expect that adding more storage, such as enlarging each component table or appending extra history lengths, would amortize the thrashing. Yet idealized studies by Seznec [5] and the independent branch-runahead framework of Pruett et al. [3], demonstrate that even a hypothetical TAGE-SC-L with unbounded capacity converges only marginally beyond the accuracy of the published 64 kB design. Capacity alone cannot compensate for the entropy inherent in the branch s context. Thus, the predictor tends to lack a representation capable of generalizing across diverging histories that precede each H2P instance. We introduce Bullseye, a compact H2P subsystem that augments a 159.3 kB TAGE-SC-L to address these challenges. Bullseye first pinpoints the few branches that dominate the residual error and then hands them off to branch-specific perceptrons. Detection is handled by the H2P Identification Table (HIT), a set-associative array that records each static branch s execution and misprediction counts. A branch becomes H2P-active only after it surpasses adap- tive thresholds on executions, mispredictions, and accuracy limits that tighten as the number of active H2P branches grows. Real work- loads seldom activate more than eight to ten PCs simultaneously, so the HIT remains small and latency-neutral. 1 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan Emet Behrendt, Shing Wai Pun, and Prashant J. Nair Once Bullseye flags a program counter, the branch enters a brief trial phase: TAGE-SC-L and two perceptron engines predict in paral- lel while an H2P cache tracks their head-to-head accuracy. Suppose either perceptron maintains higher accuracy than TAGE-SC-L and produces outputs above a dynamic magnitude threshold. In that case, Bullseye promotes the branch to perceptron-resident status and suppresses further TAGE updates for that PC, eliminating ta- ble pollution. Branches that fail the trial fall back to the HIT with negligible overhead. Bullseye s prediction engines comprise two lightweight per- ceptrons that capture patterns that TAGE s geometric indexing misses. One leverages hashed-window local history for fine-grained, per-branch behavior; the other uses a folded global history vector to learn long-range correlations. During each fetch, the HIT probe, both perceptron evaluations, and the baseline TAGE-SC-L lookup proceed in parallel. A single-cycle arbiter selects a perceptron s output when its confidence dominates; otherwise, TAGE-SC-L sup- plies the prediction. By confining extra complexity to the stubborn H2P tail, Bullseye sharpens overall accuracy while preserving the critical-path speed of the underlying predictor. 2 Background and Related Work Modern branch predictors can be broadly classified into TAGE- based and perceptron-based categories. 2.1 The TAGE-based Predictor The TAGE predictor architecture combines a tagless bimodal table of PC-indexed 2-bit counters with multiple tagged components in- dexed using geometrically increasing history lengths. Each tagged entry includes a prediction counter, a partial tag, and a usefulness counter. The final prediction is selected from the tagged compo- nent with the longest matching history. TAGE-SC-L enhances this core design with two additional components: (i) a statistical correc- tor (SC) that aggregates predictions from biased, global, and local history tables to override low-confidence TAGE outcomes, and (ii) a loop predictor (L) optimized for detecting constant-iteration loops [6]. TAGE-SC-L was the winning submission in CBP-2016 and serves as the baseline for the predictor proposed in this work. 2.2 The Perceptron-Based Predictor Perceptron-based predictors take a fundamentally different ap- proach by framing branch prediction as a form of single-layer neu- ral inference. Each prediction is computed as a weighted sum of recent branch outcomes combined with a bias term; the sign of the resulting sum determines whether the branch is predicted as taken or not taken [1]. Weights are updated at retirement based on the correctness of the prediction, allowing the perceptron to learn long-term correlations that traditional counter-based predic- tors typically miss. While perceptrons are well-suited for linearly separable patterns, they introduce higher latency and storage costs, especially as history lengths grow and weight vectors expand. 2.3 Why use H2P-Tailored Predictors? While continued process scaling has enabled significantly larger branch predictor budgets, idealized studies reveal diminishing re- turns in accuracy per kilobyte as predictor size increases [5]. Even with sophisticated designs like TAGE-SC-L, detailed execution traces show that a small number of hard-to-predict (H2P) branches account for a disproportionate share of mispredictions, limiting overall IPC gains [2]. These insights motivate hybrid predictor architectures, such as the one proposed in this work, that retain TAGE-SC-L s low-latency, high-throughput backbone while inte- grating lightweight perceptron models explicitly targeted at these high-impact, difficult-to-model branches. 3 Bullseye: High-Level Design Overview Figure 1: A high-level overview of the architecture of the Bullseye prediction system. Figure 1 shows the control flow through Bullseye s H2P pipeline. Execution begins on the baseline TAGE-SC-L, which supplies both predictions and two running statistics to the H2P Identification Table (HIT). Namely, the per-PC execution count and mispredic- tion count. When a branch s counters exceed Bullseye s adaptive thresholds, the HIT flags it H2P-active and enqueues the PC, in FIFO order, into two small tag-RAMs: the local-history H2P cache and the global-history H2P cache. These caches hold only a hand- ful of H2P PCs observed in practice ( 10), ensuring constant-time look-ups without inflating front-end latency. 2 Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan On every fetch, both caches probe in parallel. A hit launches the corresponding local-history or global-history perceptron pre- dictor, which reads its weights, produces an output, and forwards the result to a confidence arbiter. If either perceptron s magnitude and running win rate exceed Bullseye s dynamic threshold, the arbiter overrides TAGE-SC-L with the perceptron prediction; oth- erwise, the system returns to the baseline forecast. This selective substitution confines the extra latency of neural evaluation to the rare H2P branches while preserving the single-cycle critical path for the typical case, thereby sharpening overall accuracy without compromising pipeline depth. 4 Bullseye Predictor Operation Our branch predictor uses a naively scaled-up 159.3 kB version of the provided TAGE-SC-L with additional logic to identify and predict H2P branches. This section details the functionality of each key component in the branch predictor. 4.1 Hard-to-Predict Identification Table (HIT) Bullseye locates key H2P branches with a small, set-associative Hard-to-Predict Identification Table (HIT). Each HIT entry maintains three running statistics for a static branch ğ‘: Exec(ğ‘): cumulative dynamic executions, Mispred(ğ‘): cumulative TAGE-SC-L mispredictions, Acc(ğ‘) 1 Mispred(ğ‘) Exec(ğ‘): running accuracy. 4.1.1 Running Statistics. Let ğ‘H2P denote the current number of branches already classified as hard-to-predict (H2P-active) and therefore resident in the perceptron layer. A new branch becomes H2P-active, and is queued into both the local- and global-history H2P caches, when it first satisfies the adaptive rule set in Equa- tions (1) (a d): Exec(ğ‘) 2048 16 ğ‘H2P, (1a) Mispred(ğ‘) 256, (1b) Acc(ğ‘) ğ‘“ ğ‘H2P . (1c) ğ‘“(ğ‘) 1 0.01 ğ‘ 32, ğ‘ 32, 0.95 0.01(ğ‘ 32), 32 ğ‘ 71, 0.60, ğ‘ 71. (1d) 4.1.2 Leveraging the Statistics. Equation (1a) raises the execution threshold by 16 for every additional H2P-active branch, ensuring that transient bursts cannot flood the perceptron layer. Equation (1b) enforces a fixed lower bound of 256 mispredictions, filtering out seldom-executed yet highly biased branches. The piece-wise func- tion ğ‘“(ğ‘) in Equation (1d) tightens the accuracy ceiling as ğ‘H2P grows, dropping from 95 when the perceptron layer is empty to 60 at saturation (ğ‘H2P 71). Because empirical workloads rarely exceed ğ‘H2P 10, the HIT remains compact. 4.2 H2P Cache: Trial, Admission, and Eviction After the HIT flags a branch as H2P-active, its PC is inserted into both the local- and global-history perceptron engines and into a small, fully- associative H2P cache. The cache serves two purposes: (i) it records which PCs are currently predicted by Bullseye s per- ceptrons and (ii) it mediates a head-to-head trial between each perceptron and the baseline TAGE-SC-L. 4.2.1 Trial phase. A newly admitted entry is granted a warm-up window of 512 dynamic occurrences to train the perceptron weights. During this window, the branch is never considered for eviction. During and after the trial phase, two counters are used to determine H2P prediction confidence. Every prediction updates a saturating relative performance counter based on the winning predictor. As the performance metric saturates, the stability of the relative per- formance counter is then measured with a confidence counter with linear growth and exponential decay. Confidence is incremented by ğ¶ min(ğ¶ 1, 255) if the current relative performance trend contin- ues and ğ¶ ğ¶ 2 if the update goes against the running trend. This creates a policy that slowly rewards sustained predictor superiority yet quickly penalizes failing branches. 4.2.2 Eviction policy. A branch is evicted when either (i) the con- fidence counter has saturated in favor of TAGE-SC-L or (ii) the entry is not referenced for 216 dynamic branches ( stale timeout). Eviction occurs only when at least one HIT-qualifying branch is waiting to enter, thus guaranteeing high utility for every occupied slot. Branches evicted from the cache revert to standard TAGE-SC-L prediction but may re-enter if they later satisfy the H2P criteria in Equations (1) (a d). This gating mechanism ensures that Bullseye deploys its per- ceptron resources only where they continue to beat the geometric core while bounding both storage and latency. Empirically, no more than 10 PCs reside in the cache simultaneously, keeping look-ups single-cycle and energy-efficient. 4.3 Hashed-Window Local-History Perceptron Bullseye s first neural component targets correlations in branch s local history. For each H2P-active PC, the predictor constructs a feature vector whose ğ‘–th element is the parity of a fixed-width win- dow ğ‘Šğ‘–of the branch s outcome history, where window sizes grow with age (e.g.,ğ‘Š0 4,ğ‘Š1 8,ğ‘Š2 16, ...). Successive windows start at offsets separated by a constant stride ğ‘†, ensuring non-overlapping coverage of up to a few hundred past outcomes. Each feature is mapped to two independent weight words by the hash â„ğ‘– PC,ğ‘Šğ‘– , implemented as a 32-bit XOR-shift scrambler. Dual hashing mitigates aliasing: a collision in one weight location is usually resolved by the second. The perceptron output is the integer sum of the selected weights plus a bias term. Finally, the sign of the output determines the prediction. Weights are updated using the dynamic-threshold rule of Seznec and Vintan s O-GEHL predictor [4]. Specifically, a global threshold ğœƒtracks the absolute output magnitude at misprediction time and adapts toward the smallest value that keeps training activity near 50 . A weight ğ‘¤is incremented (decremented) when the actual outcome is taken (not-taken) and output ğœƒ, enabling fast con- vergence without saturation. This hashed-window design yields high resolution on recent local patterns while maintaining low storage per perceptron. 3 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan Emet Behrendt, Shing Wai Pun, and Prashant J. Nair 4.4 Folded Global-History Perceptron The global-history perceptron captures long-range, cross-branch correlations. The feature vector consists of the most recent ğ»ğ‘”out- comes from the global branch history register, folded by XOR into a fixed ğ‘Šğ‘”-bit index; each bit selects a single signed weight word. With a minimal number of weights, the global history perceptron size is an order-of-magnitude smaller than the local model. Prediction and learning follow the same dynamic-threshold rule outlined in Section 4.3. Specifically, the perceptron sums its ğ‘Šğ‘” signed weights and compares the magnitude to a shared, runtime-tuned threshold ğœƒ: outğ‘” ğ‘Šğ‘” 1 ğ‘– 0 ğ‘¤ğ‘– ğ‘¥ğ‘–, prediction sign(outğ‘”). Weights are updated only when outğ‘” ğœƒor the prediction is incorrect, ensuring rapid adaptation and bounded training costs. Although its standalone accuracy gain is modest ( 1 BPC), the global perceptron provides a backup view that often corrects rare, history-spanning patterns missed by both the local model and TAGE-SC-L, while adding negligible footprint. 4.5 Prediction Arbitration Whenever an H2P-resident branch is fetched, all engines fire con- currently: the baseline TAGE tables, the statistical corrector (SC), and Bullseye s two perceptrons. The final outcome is chosen by a lightweight, confidence-based arbiter: i Perceptron gate: Each perceptron supplies a two-bit conf field that encodes (a) its running win-rate over TAGE-SC-L (high if win-rate 55 ), and (b) whether output ğœƒfor the current instance. A perceptron asserts strong confidence only when both tests pass. ii TAGE gate: TAGE-SC-L asserts strong confidence when (a) the selected TAGE component s usefulness 3 or (b) the SC overrides with a magnitude 0. iii Decision rule: If at least one perceptron has strong confi- dence and TAGE-SC-L does not, Bullseye chooses the percep- tron outcome. Otherwise, the arbiter defaults to TAGE-SC-L. This rule preserves accuracy on easy branches while allow- ing neural takeover only during sustained benefits. iv Decision rule: If at least one perceptron has strong confi- dence, Bullseye chooses the perceptron outcome. Otherwise, the arbiter defaults to TAGE-SC-L. This rule preserves accu- racy on easy branches while allowing neural takeover only during sustained benefits. 4.6 Selective TAGE Filtering Once a branch has delivered 128 consecutive correct perceptron pre- dictions without a single TAGE-SC-L win, its updates are filtered i.e., subsequent outcomes bypass all TAGE and SC tables. Filtering prevents low-utility data from evicting well-trained entries and cuts energy by avoiding unneeded SRAM writes. If the perceptron later falters (confidence drops below the strong threshold), filtering is automatically revoked, ensuring that valuable geometric history is never lost permanently. Empirically, this mechanism yields a modest accuracy gain ( 0.3 BPC) for negligible extra storage. 5 Experimental Results and Analysis We present results for CycWpPKI and BrMisPKI metrics. Overall, Bullseye achieves an average CycWpPKI of 145.09 and an average BrMisPKI of 3.405. Table 1 compares the average BrMisPKI of TAGE- SC-L with and without the Bullseye Predictor. Figure 2: The BrMisPKI across all workloads for Bullseye. Branch Predictor BrMisPKI 159 kB TAGE-SC-L Bullseye (Total 187 kB) 3.4045 192 kB TAGE-SC-L 3.4277 159 kB TAGE-SC-L 3.4513 Table 1: Comparison of BrMisPKI Across Predictors 6 Discussion Bullseye identifies pathological control flow by dynamically adjust- ing HIT thresholds based on the current H2P population rather than absolute miss rates, leading to significant misprediction reductions. Its workload-agnostic design allows the underlying TAGE-SC-L pre- dictor to handle typical branches, while the perceptron tier activates only for statistically qualifying branches, facilitating seamless de- ployment across diverse workloads without retuning. Additionally, Bullseye s lightweight H2P identification mechanism can augment other predictors. However, it does not capture data-dependent cor- relations, a limitation we explored but found offered only marginal accuracy gains within our area constraints, highlighting an oppor- tunity for future H2P engines that integrate branch history with lightweight data value predictors. 7 Conclusions Bullseye shows that selectively augmenting a compact 159 KB TAGE-SC-L with a lightweight, H2P-aware neural tier yields out- sized accuracy gains. The H2P Identification Table (HIT) isolates the small set of static branches that dominate the misprediction tail and admits them to local- and global-history perceptrons for a gated trial. It then applies adaptive thresholds to prevent resource thrashing. A confidence-based arbiter allows neural predictions to override TAGE-SC-L only when they provide consistent benefit, while update filtering protects TAGE-SC-L from low-value training noise. Across workloads, Bullseye achieves an MPKI of 3.405. 4 Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan References [1] D.A. Jimenez and C. Lin. 2001. Dynamic branch prediction with perceptrons. In Proceedings HPCA Seventh International Symposium on High-Performance Computer Architecture. IEEE, Nuevo LeÃ³n, Mexico, 197 206. 2001.903263 [2] Chit-Kwan Lin and Stephen J. Tarsa. 2019. Branch Prediction Is Not A Solved Problem: Measurements, Opportunities, and Future Directions. In 2019 IEEE In- ternational Symposium on Workload Characterization (IISWC). IEEE, Orlando, FL, USA, 228 238. [3] Stephen Pruett and Yale Patt. 2021. Branch Runahead: An Alternative to Branch Prediction for Impossible to Predict Branches. In Proceedings of the 54th Annual IEEE ACM International Symposium on Microarchitecture (MICRO-54). ACM, Virtual Event, 804 815. [4] A. Seznec. 2005. Analysis of the O-GEometric history length branch predictor. In 32nd International Symposium on Computer Architecture (ISCA 05). IEEE, Madison, WI, USA, 394 405. [5] AndrÃ© Seznec. 2007. The Idealistic GTL Predictor. J. Instr. Level Parallelism 9 (2007), 1 12. [6] AndrÃ© Seznec and Pierre Michaud. 2006. A case for (partially) tagged geometric history length branch prediction. A Cost Analysis Table 2 provides a breakdown of the memory usage by each com- ponent. Between prediction time and update time the following is stored: for the local history perceptron, the local history; for the global history perceptron, the global history; and for TAGE-SC-L the requirements are unchanged from the base version with the same local and global history required to update. Component Details of each field of each entry and memory breakdown Cost TAGE-SC-L Calculated using the built in TAGE-SC-L memory usage calculator 159.34 kB H2P Identifica- tion Table (HIT Cache) PC Tag: 10 bits (16 bit tag, but 10 bits stored with set-association) 26 sets 8 ways 5120 bits Correct Prediction Counters: 16 bits 26 sets 8 ways 8192 bits Incorrect Prediction Counters: 12 bits 26 sets 8 ways 6144 bits 2.375 kB Global History Perceptron and FIFO H2P PC Tag: 62 bit PC 16 H2P entries 992 bits PC Queue: 62 PC bits 64 queue entries 3968 bits Global History: 128 bits Weights: 12 bit precision 128 tables 16 H2P entries 24576 bits Perceptron Bias: 10 bit precision 24 table entries 16 H2P entries 2560 Update Thresh. Counters: (14 7) 16 H2P entries 336 bits Branch Management Counters: (6 8 9 16) 16 H2P entries 624 bits 4.05 kB Local History Perceptron and FIFO H2P PC Tag: 62 bit PC 32 H2P entries 1984 bits PC Queue: 62 PC bits 64 queue entries 3968 bits Weights: 10 bit precision 64 tables table size 28 163840 bits Local History: 124 bit history 32 H2P entries 3968 bits Perceptron Bias: 12 bit precision 21 table entries 32 H2P entries 768 bits Update Thresh. Counters: (10 7) 32 H2P entries 544 bits Branch Management Counters: (6 8 9 16) 32 H2P entries 1248 bits 21.52 kB TOTAL 187.28 kB Table 2: Memory Usage Breakdown 5\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.06773v1 [cs.AR] 7 Jun 2025 Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor Emet Behrendt The University of British Columbia Canada Shing Wai Pun The University of British Columbia Canada Prashant J. Nair The University of British Columbia Canada Abstract Branch prediction is key to the performance of out-of-order proces- sors. While the CBP-2016 winner TAGE-SC-L combines geometric- history tables, a statistical corrector, and a loop predictor, over half of its remaining mispredictions stem from a small set of hard-to- predict (H2P) branches. These branches occur under diverse global histories, causing repeated thrashing in TAGE and eviction before usefulness counters can mature. Prior work shows that simply enlarging the tables offers only marginal improvement. We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P- targeted subsystem called the Bullseye predictor. It identifies prob- lematic PCs using a set-associative H2P Identification Table (HIT) and steers them to one of two branch-specific perceptrons, one indexed by hashed local history and the other by folded global history. A short trial phase tracks head-to-head accuracy in an H2P cache. A branch becomes perceptron-resident only if the percep- tron s sustained accuracy and output magnitude exceed dynamic thresholds, after which TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache, and perceptron operate fully in parallel with TAGE-SC-L, providing higher fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI of 145.09. CCS Concepts Computer systems organization Multicore architectures. Keywords Branch Prediction, Hard-to-Predict branches, TAGE, Perceptron ACM Reference Format: Emet Behrendt, Shing Wai Pun, and Prashant J. Nair. . Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor. In Pro- ceedings of 6th Championship Branch Prediction ( 6th Championship Branch Prediction (CBP) ). ACM, New York, NY, USA, 5 pages.\n\n--- Segment 2 ---\nIn Pro- ceedings of 6th Championship Branch Prediction ( 6th Championship Branch Prediction (CBP) ). ACM, New York, NY, USA, 5 pages. 1 Introduction Modern out-of-order CPUs depend heavily on aggressive specula- tive execution, where each fetched instruction block is predicted to either fall through or take a branch long before the branch con- dition is resolved. A single misprediction triggers a pipeline flush, drains the front-end, and requires re-fetching along the correct path, typically incurring a penalty of 15 30 cycles. As a result, the design and accuracy of the branch predictor have direct and significant performance implications, affecting instruction throughput, energy efficiency, and speculation-dependent microarchitectural features. 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan . Branch prediction accuracy remains a first-order design con- straint, shaping front-end bandwidth, retirement rates, and secu- rity mitigation strategies. Over the past three decades, predictors have evolved from simple bimodal schemes to sophisticated hybrid, perceptron-based, and multi-component designs. The TAGE-SC- L predictor, the CBP-2016 competition winner, combines tagged geometric history tables (TAGE) with a statistical corrector and a loop predictor to cover a broad spectrum of control-flow behaviors. However, despite this architectural complexity, TAGE-SC-L still leaves substantial accuracy on the table, especially for a small but critical subset of dynamic branches. Despite TAGE-SC-L s advanced use of long-range geometric histories, statistical correction, and loop-specific components, its residual mispredictions remain highly skewed. Lin et al. [2] showed that in a 30-million-instruction SPEC-INT 2017 trace, over 50 of mispredictions stem from fewer than ten static hard-to-predict (H2P) branches. These branches exhibit volatile control-flow be- havior, with dynamic instances appearing under widely varying global histories. As a result, TAGE s tag-matching struggles to re- trieve consistent predictor state, leading to frequent allocations with low confidence. Since usefulness counters only increment on correct predictions, these entries are quickly evicted, creating a self-reinforcing cycle of misprediction, reallocation, and eviction that prevents the predictor from learning stable correlations.\n\n--- Segment 3 ---\nAs a result, TAGE s tag-matching struggles to re- trieve consistent predictor state, leading to frequent allocations with low confidence. Since usefulness counters only increment on correct predictions, these entries are quickly evicted, creating a self-reinforcing cycle of misprediction, reallocation, and eviction that prevents the predictor from learning stable correlations. An H2P branch may see hundreds of distinct TAGE entries within a single millisecond of execution, none surviving long enough to gather confidence. Intuitively, one might expect that adding more storage, such as enlarging each component table or appending extra history lengths, would amortize the thrashing. Yet idealized studies by Seznec [5] and the independent branch-runahead framework of Pruett et al. [3], demonstrate that even a hypothetical TAGE-SC-L with unbounded capacity converges only marginally beyond the accuracy of the published 64 kB design. Capacity alone cannot compensate for the entropy inherent in the branch s context. Thus, the predictor tends to lack a representation capable of generalizing across diverging histories that precede each H2P instance. We introduce Bullseye, a compact H2P subsystem that augments a 159.3 kB TAGE-SC-L to address these challenges. Bullseye first pinpoints the few branches that dominate the residual error and then hands them off to branch-specific perceptrons. Detection is handled by the H2P Identification Table (HIT), a set-associative array that records each static branch s execution and misprediction counts. A branch becomes H2P-active only after it surpasses adap- tive thresholds on executions, mispredictions, and accuracy limits that tighten as the number of active H2P branches grows. Real work- loads seldom activate more than eight to ten PCs simultaneously, so the HIT remains small and latency-neutral. 1 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan Emet Behrendt, Shing Wai Pun, and Prashant J. Nair Once Bullseye flags a program counter, the branch enters a brief trial phase: TAGE-SC-L and two perceptron engines predict in paral- lel while an H2P cache tracks their head-to-head accuracy.\n\n--- Segment 4 ---\nReal work- loads seldom activate more than eight to ten PCs simultaneously, so the HIT remains small and latency-neutral. 1 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan Emet Behrendt, Shing Wai Pun, and Prashant J. Nair Once Bullseye flags a program counter, the branch enters a brief trial phase: TAGE-SC-L and two perceptron engines predict in paral- lel while an H2P cache tracks their head-to-head accuracy. Suppose either perceptron maintains higher accuracy than TAGE-SC-L and produces outputs above a dynamic magnitude threshold. In that case, Bullseye promotes the branch to perceptron-resident status and suppresses further TAGE updates for that PC, eliminating ta- ble pollution. Branches that fail the trial fall back to the HIT with negligible overhead. Bullseye s prediction engines comprise two lightweight per- ceptrons that capture patterns that TAGE s geometric indexing misses. One leverages hashed-window local history for fine-grained, per-branch behavior; the other uses a folded global history vector to learn long-range correlations. During each fetch, the HIT probe, both perceptron evaluations, and the baseline TAGE-SC-L lookup proceed in parallel. A single-cycle arbiter selects a perceptron s output when its confidence dominates; otherwise, TAGE-SC-L sup- plies the prediction. By confining extra complexity to the stubborn H2P tail, Bullseye sharpens overall accuracy while preserving the critical-path speed of the underlying predictor. 2 Background and Related Work Modern branch predictors can be broadly classified into TAGE- based and perceptron-based categories. 2.1 The TAGE-based Predictor The TAGE predictor architecture combines a tagless bimodal table of PC-indexed 2-bit counters with multiple tagged components in- dexed using geometrically increasing history lengths. Each tagged entry includes a prediction counter, a partial tag, and a usefulness counter. The final prediction is selected from the tagged compo- nent with the longest matching history.\n\n--- Segment 5 ---\nEach tagged entry includes a prediction counter, a partial tag, and a usefulness counter. The final prediction is selected from the tagged compo- nent with the longest matching history. TAGE-SC-L enhances this core design with two additional components: (i) a statistical correc- tor (SC) that aggregates predictions from biased, global, and local history tables to override low-confidence TAGE outcomes, and (ii) a loop predictor (L) optimized for detecting constant-iteration loops [6]. TAGE-SC-L was the winning submission in CBP-2016 and serves as the baseline for the predictor proposed in this work. 2.2 The Perceptron-Based Predictor Perceptron-based predictors take a fundamentally different ap- proach by framing branch prediction as a form of single-layer neu- ral inference. Each prediction is computed as a weighted sum of recent branch outcomes combined with a bias term; the sign of the resulting sum determines whether the branch is predicted as taken or not taken [1]. Weights are updated at retirement based on the correctness of the prediction, allowing the perceptron to learn long-term correlations that traditional counter-based predic- tors typically miss. While perceptrons are well-suited for linearly separable patterns, they introduce higher latency and storage costs, especially as history lengths grow and weight vectors expand. 2.3 Why use H2P-Tailored Predictors? While continued process scaling has enabled significantly larger branch predictor budgets, idealized studies reveal diminishing re- turns in accuracy per kilobyte as predictor size increases [5]. Even with sophisticated designs like TAGE-SC-L, detailed execution traces show that a small number of hard-to-predict (H2P) branches account for a disproportionate share of mispredictions, limiting overall IPC gains [2]. These insights motivate hybrid predictor architectures, such as the one proposed in this work, that retain TAGE-SC-L s low-latency, high-throughput backbone while inte- grating lightweight perceptron models explicitly targeted at these high-impact, difficult-to-model branches. 3 Bullseye: High-Level Design Overview Figure 1: A high-level overview of the architecture of the Bullseye prediction system. Figure 1 shows the control flow through Bullseye s H2P pipeline.\n\n--- Segment 6 ---\n3 Bullseye: High-Level Design Overview Figure 1: A high-level overview of the architecture of the Bullseye prediction system. Figure 1 shows the control flow through Bullseye s H2P pipeline. Execution begins on the baseline TAGE-SC-L, which supplies both predictions and two running statistics to the H2P Identification Table (HIT). Namely, the per-PC execution count and mispredic- tion count. When a branch s counters exceed Bullseye s adaptive thresholds, the HIT flags it H2P-active and enqueues the PC, in FIFO order, into two small tag-RAMs: the local-history H2P cache and the global-history H2P cache. These caches hold only a hand- ful of H2P PCs observed in practice ( 10), ensuring constant-time look-ups without inflating front-end latency. 2 Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan On every fetch, both caches probe in parallel. A hit launches the corresponding local-history or global-history perceptron pre- dictor, which reads its weights, produces an output, and forwards the result to a confidence arbiter. If either perceptron s magnitude and running win rate exceed Bullseye s dynamic threshold, the arbiter overrides TAGE-SC-L with the perceptron prediction; oth- erwise, the system returns to the baseline forecast. This selective substitution confines the extra latency of neural evaluation to the rare H2P branches while preserving the single-cycle critical path for the typical case, thereby sharpening overall accuracy without compromising pipeline depth. 4 Bullseye Predictor Operation Our branch predictor uses a naively scaled-up 159.3 kB version of the provided TAGE-SC-L with additional logic to identify and predict H2P branches. This section details the functionality of each key component in the branch predictor. 4.1 Hard-to-Predict Identification Table (HIT) Bullseye locates key H2P branches with a small, set-associative Hard-to-Predict Identification Table (HIT).\n\n--- Segment 7 ---\nThis section details the functionality of each key component in the branch predictor. 4.1 Hard-to-Predict Identification Table (HIT) Bullseye locates key H2P branches with a small, set-associative Hard-to-Predict Identification Table (HIT). Each HIT entry maintains three running statistics for a static branch ğ‘: Exec(ğ‘): cumulative dynamic executions, Mispred(ğ‘): cumulative TAGE-SC-L mispredictions, Acc(ğ‘) 1 Mispred(ğ‘) Exec(ğ‘): running accuracy. 4.1.1 Running Statistics. Let ğ‘H2P denote the current number of branches already classified as hard-to-predict (H2P-active) and therefore resident in the perceptron layer. A new branch becomes H2P-active, and is queued into both the local- and global-history H2P caches, when it first satisfies the adaptive rule set in Equa- tions (1) (a d): Exec(ğ‘) 2048 16 ğ‘H2P, (1a) Mispred(ğ‘) 256, (1b) Acc(ğ‘) ğ‘“ ğ‘H2P . (1c) ğ‘“(ğ‘) 1 0.01 ğ‘ 32, ğ‘ 32, 0.95 0.01(ğ‘ 32), 32 ğ‘ 71, 0.60, ğ‘ 71. (1d) 4.1.2 Leveraging the Statistics. Equation (1a) raises the execution threshold by 16 for every additional H2P-active branch, ensuring that transient bursts cannot flood the perceptron layer. Equation (1b) enforces a fixed lower bound of 256 mispredictions, filtering out seldom-executed yet highly biased branches. The piece-wise func- tion ğ‘“(ğ‘) in Equation (1d) tightens the accuracy ceiling as ğ‘H2P grows, dropping from 95 when the perceptron layer is empty to 60 at saturation (ğ‘H2P 71). Because empirical workloads rarely exceed ğ‘H2P 10, the HIT remains compact.\n\n--- Segment 8 ---\nThe piece-wise func- tion ğ‘“(ğ‘) in Equation (1d) tightens the accuracy ceiling as ğ‘H2P grows, dropping from 95 when the perceptron layer is empty to 60 at saturation (ğ‘H2P 71). Because empirical workloads rarely exceed ğ‘H2P 10, the HIT remains compact. 4.2 H2P Cache: Trial, Admission, and Eviction After the HIT flags a branch as H2P-active, its PC is inserted into both the local- and global-history perceptron engines and into a small, fully- associative H2P cache. The cache serves two purposes: (i) it records which PCs are currently predicted by Bullseye s per- ceptrons and (ii) it mediates a head-to-head trial between each perceptron and the baseline TAGE-SC-L. 4.2.1 Trial phase. A newly admitted entry is granted a warm-up window of 512 dynamic occurrences to train the perceptron weights. During this window, the branch is never considered for eviction. During and after the trial phase, two counters are used to determine H2P prediction confidence. Every prediction updates a saturating relative performance counter based on the winning predictor. As the performance metric saturates, the stability of the relative per- formance counter is then measured with a confidence counter with linear growth and exponential decay. Confidence is incremented by ğ¶ min(ğ¶ 1, 255) if the current relative performance trend contin- ues and ğ¶ ğ¶ 2 if the update goes against the running trend. This creates a policy that slowly rewards sustained predictor superiority yet quickly penalizes failing branches. 4.2.2 Eviction policy. A branch is evicted when either (i) the con- fidence counter has saturated in favor of TAGE-SC-L or (ii) the entry is not referenced for 216 dynamic branches ( stale timeout). Eviction occurs only when at least one HIT-qualifying branch is waiting to enter, thus guaranteeing high utility for every occupied slot. Branches evicted from the cache revert to standard TAGE-SC-L prediction but may re-enter if they later satisfy the H2P criteria in Equations (1) (a d).\n\n--- Segment 9 ---\nEviction occurs only when at least one HIT-qualifying branch is waiting to enter, thus guaranteeing high utility for every occupied slot. Branches evicted from the cache revert to standard TAGE-SC-L prediction but may re-enter if they later satisfy the H2P criteria in Equations (1) (a d). This gating mechanism ensures that Bullseye deploys its per- ceptron resources only where they continue to beat the geometric core while bounding both storage and latency. Empirically, no more than 10 PCs reside in the cache simultaneously, keeping look-ups single-cycle and energy-efficient. 4.3 Hashed-Window Local-History Perceptron Bullseye s first neural component targets correlations in branch s local history. For each H2P-active PC, the predictor constructs a feature vector whose ğ‘–th element is the parity of a fixed-width win- dow ğ‘Šğ‘–of the branch s outcome history, where window sizes grow with age (e.g.,ğ‘Š0 4,ğ‘Š1 8,ğ‘Š2 16, ...). Successive windows start at offsets separated by a constant stride ğ‘†, ensuring non-overlapping coverage of up to a few hundred past outcomes. Each feature is mapped to two independent weight words by the hash â„ğ‘– PC,ğ‘Šğ‘– , implemented as a 32-bit XOR-shift scrambler. Dual hashing mitigates aliasing: a collision in one weight location is usually resolved by the second. The perceptron output is the integer sum of the selected weights plus a bias term. Finally, the sign of the output determines the prediction. Weights are updated using the dynamic-threshold rule of Seznec and Vintan s O-GEHL predictor [4]. Specifically, a global threshold ğœƒtracks the absolute output magnitude at misprediction time and adapts toward the smallest value that keeps training activity near 50 . A weight ğ‘¤is incremented (decremented) when the actual outcome is taken (not-taken) and output ğœƒ, enabling fast con- vergence without saturation. This hashed-window design yields high resolution on recent local patterns while maintaining low storage per perceptron.\n\n--- Segment 10 ---\nA weight ğ‘¤is incremented (decremented) when the actual outcome is taken (not-taken) and output ğœƒ, enabling fast con- vergence without saturation. This hashed-window design yields high resolution on recent local patterns while maintaining low storage per perceptron. 3 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan Emet Behrendt, Shing Wai Pun, and Prashant J. Nair 4.4 Folded Global-History Perceptron The global-history perceptron captures long-range, cross-branch correlations. The feature vector consists of the most recent ğ»ğ‘”out- comes from the global branch history register, folded by XOR into a fixed ğ‘Šğ‘”-bit index; each bit selects a single signed weight word. With a minimal number of weights, the global history perceptron size is an order-of-magnitude smaller than the local model. Prediction and learning follow the same dynamic-threshold rule outlined in Section 4.3. Specifically, the perceptron sums its ğ‘Šğ‘” signed weights and compares the magnitude to a shared, runtime-tuned threshold ğœƒ: outğ‘” ğ‘Šğ‘” 1 ğ‘– 0 ğ‘¤ğ‘– ğ‘¥ğ‘–, prediction sign(outğ‘”). Weights are updated only when outğ‘” ğœƒor the prediction is incorrect, ensuring rapid adaptation and bounded training costs. Although its standalone accuracy gain is modest ( 1 BPC), the global perceptron provides a backup view that often corrects rare, history-spanning patterns missed by both the local model and TAGE-SC-L, while adding negligible footprint. 4.5 Prediction Arbitration Whenever an H2P-resident branch is fetched, all engines fire con- currently: the baseline TAGE tables, the statistical corrector (SC), and Bullseye s two perceptrons. The final outcome is chosen by a lightweight, confidence-based arbiter: i Perceptron gate: Each perceptron supplies a two-bit conf field that encodes (a) its running win-rate over TAGE-SC-L (high if win-rate 55 ), and (b) whether output ğœƒfor the current instance.\n\n--- Segment 11 ---\n4.5 Prediction Arbitration Whenever an H2P-resident branch is fetched, all engines fire con- currently: the baseline TAGE tables, the statistical corrector (SC), and Bullseye s two perceptrons. The final outcome is chosen by a lightweight, confidence-based arbiter: i Perceptron gate: Each perceptron supplies a two-bit conf field that encodes (a) its running win-rate over TAGE-SC-L (high if win-rate 55 ), and (b) whether output ğœƒfor the current instance. A perceptron asserts strong confidence only when both tests pass. ii TAGE gate: TAGE-SC-L asserts strong confidence when (a) the selected TAGE component s usefulness 3 or (b) the SC overrides with a magnitude 0. iii Decision rule: If at least one perceptron has strong confi- dence and TAGE-SC-L does not, Bullseye chooses the percep- tron outcome. Otherwise, the arbiter defaults to TAGE-SC-L. This rule preserves accuracy on easy branches while allow- ing neural takeover only during sustained benefits. iv Decision rule: If at least one perceptron has strong confi- dence, Bullseye chooses the perceptron outcome. Otherwise, the arbiter defaults to TAGE-SC-L. This rule preserves accu- racy on easy branches while allowing neural takeover only during sustained benefits. 4.6 Selective TAGE Filtering Once a branch has delivered 128 consecutive correct perceptron pre- dictions without a single TAGE-SC-L win, its updates are filtered i.e., subsequent outcomes bypass all TAGE and SC tables. Filtering prevents low-utility data from evicting well-trained entries and cuts energy by avoiding unneeded SRAM writes. If the perceptron later falters (confidence drops below the strong threshold), filtering is automatically revoked, ensuring that valuable geometric history is never lost permanently. Empirically, this mechanism yields a modest accuracy gain ( 0.3 BPC) for negligible extra storage. 5 Experimental Results and Analysis We present results for CycWpPKI and BrMisPKI metrics. Overall, Bullseye achieves an average CycWpPKI of 145.09 and an average BrMisPKI of 3.405.\n\n--- Segment 12 ---\n5 Experimental Results and Analysis We present results for CycWpPKI and BrMisPKI metrics. Overall, Bullseye achieves an average CycWpPKI of 145.09 and an average BrMisPKI of 3.405. Table 1 compares the average BrMisPKI of TAGE- SC-L with and without the Bullseye Predictor. Figure 2: The BrMisPKI across all workloads for Bullseye. Branch Predictor BrMisPKI 159 kB TAGE-SC-L Bullseye (Total 187 kB) 3.4045 192 kB TAGE-SC-L 3.4277 159 kB TAGE-SC-L 3.4513 Table 1: Comparison of BrMisPKI Across Predictors 6 Discussion Bullseye identifies pathological control flow by dynamically adjust- ing HIT thresholds based on the current H2P population rather than absolute miss rates, leading to significant misprediction reductions. Its workload-agnostic design allows the underlying TAGE-SC-L pre- dictor to handle typical branches, while the perceptron tier activates only for statistically qualifying branches, facilitating seamless de- ployment across diverse workloads without retuning. Additionally, Bullseye s lightweight H2P identification mechanism can augment other predictors. However, it does not capture data-dependent cor- relations, a limitation we explored but found offered only marginal accuracy gains within our area constraints, highlighting an oppor- tunity for future H2P engines that integrate branch history with lightweight data value predictors. 7 Conclusions Bullseye shows that selectively augmenting a compact 159 KB TAGE-SC-L with a lightweight, H2P-aware neural tier yields out- sized accuracy gains. The H2P Identification Table (HIT) isolates the small set of static branches that dominate the misprediction tail and admits them to local- and global-history perceptrons for a gated trial. It then applies adaptive thresholds to prevent resource thrashing. A confidence-based arbiter allows neural predictions to override TAGE-SC-L only when they provide consistent benefit, while update filtering protects TAGE-SC-L from low-value training noise. Across workloads, Bullseye achieves an MPKI of 3.405.\n\n--- Segment 13 ---\nA confidence-based arbiter allows neural predictions to override TAGE-SC-L only when they provide consistent benefit, while update filtering protects TAGE-SC-L from low-value training noise. Across workloads, Bullseye achieves an MPKI of 3.405. 4 Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor 6th Championship Branch Prediction (CBP) , June 21, 2025, Tokyo, Japan References [1] D.A. Jimenez and C. Lin. 2001. Dynamic branch prediction with perceptrons. In Proceedings HPCA Seventh International Symposium on High-Performance Computer Architecture. IEEE, Nuevo LeÃ³n, Mexico, 197 206. 2001.903263 [2] Chit-Kwan Lin and Stephen J. Tarsa. 2019. Branch Prediction Is Not A Solved Problem: Measurements, Opportunities, and Future Directions. In 2019 IEEE In- ternational Symposium on Workload Characterization (IISWC). IEEE, Orlando, FL, USA, 228 238. [3] Stephen Pruett and Yale Patt. 2021. Branch Runahead: An Alternative to Branch Prediction for Impossible to Predict Branches. In Proceedings of the 54th Annual IEEE ACM International Symposium on Microarchitecture (MICRO-54). ACM, Virtual Event, 804 815. [4] A. Seznec. 2005. Analysis of the O-GEometric history length branch predictor. In 32nd International Symposium on Computer Architecture (ISCA 05). IEEE, Madison, WI, USA, 394 405. [5] AndrÃ© Seznec. 2007. The Idealistic GTL Predictor. J. Instr. Level Parallelism 9 (2007), 1 12. [6] AndrÃ© Seznec and Pierre Michaud. 2006. A case for (partially) tagged geometric history length branch prediction. A Cost Analysis Table 2 provides a breakdown of the memory usage by each com- ponent. Between prediction time and update time the following is stored: for the local history perceptron, the local history; for the global history perceptron, the global history; and for TAGE-SC-L the requirements are unchanged from the base version with the same local and global history required to update.\n\n--- Segment 14 ---\nA Cost Analysis Table 2 provides a breakdown of the memory usage by each com- ponent. Between prediction time and update time the following is stored: for the local history perceptron, the local history; for the global history perceptron, the global history; and for TAGE-SC-L the requirements are unchanged from the base version with the same local and global history required to update. Component Details of each field of each entry and memory breakdown Cost TAGE-SC-L Calculated using the built in TAGE-SC-L memory usage calculator 159.34 kB H2P Identifica- tion Table (HIT Cache) PC Tag: 10 bits (16 bit tag, but 10 bits stored with set-association) 26 sets 8 ways 5120 bits Correct Prediction Counters: 16 bits 26 sets 8 ways 8192 bits Incorrect Prediction Counters: 12 bits 26 sets 8 ways 6144 bits 2.375 kB Global History Perceptron and FIFO H2P PC Tag: 62 bit PC 16 H2P entries 992 bits PC Queue: 62 PC bits 64 queue entries 3968 bits Global History: 128 bits Weights: 12 bit precision 128 tables 16 H2P entries 24576 bits Perceptron Bias: 10 bit precision 24 table entries 16 H2P entries 2560 Update Thresh. Counters: (14 7) 16 H2P entries 336 bits Branch Management Counters: (6 8 9 16) 16 H2P entries 624 bits 4.05 kB Local History Perceptron and FIFO H2P PC Tag: 62 bit PC 32 H2P entries 1984 bits PC Queue: 62 PC bits 64 queue entries 3968 bits Weights: 10 bit precision 64 tables table size 28 163840 bits Local History: 124 bit history 32 H2P entries 3968 bits Perceptron Bias: 12 bit precision 21 table entries 32 H2P entries 768 bits Update Thresh. Counters: (10 7) 32 H2P entries 544 bits Branch Management Counters: (6 8 9 16) 32 H2P entries 1248 bits 21.52 kB TOTAL 187.28 kB Table 2: Memory Usage Breakdown 5\n\n