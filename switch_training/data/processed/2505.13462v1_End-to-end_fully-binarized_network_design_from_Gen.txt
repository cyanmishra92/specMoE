=== ORIGINAL PDF: 2505.13462v1_End-to-end_fully-binarized_network_design_from_Gen.pdf ===\n\nRaw text length: 27161 characters\nCleaned text length: 26764 characters\nNumber of segments: 18\n\n=== CLEANED TEXT ===\n\nEnd-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning Thien Nguyen and William Guicquero Smart Integrated Circuits for Imaging Laboratory, CEA-LETI F-38000, Grenoble, France. Email: [vanthien.nguyen, Abstract Existing works on Binary Neural Network (BNN) mainly focus on model s weights and activations while discarding considerations on the input raw data. This article introduces Generic Learned Thermometer (GLT), an encoding technique to improve input data representation for BNN, relying on learning non linear quantization thresholds. This technique consists in multiple data binarizations which can advantageously replace a conventional Analog to Digital Conversion (ADC) that uses natural binary coding. Additionally, we jointly propose a compact topology with light-weight grouped convolutions being trained thanks to block pruning and Knowledge Distillation (KD), aiming at reducing furthermore the model size so as its computational complexity. We show that GLT brings versatility to the BNN by intrinsically performing global tone mapping, enabling significant accuracy gains in practice (demonstrated by simulations on the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed block-pruning technique, we successfully achieve lightweight (under 1Mb), fully-binarized models with limited accuracy degradation while being suitable for in-sensor always-on inference use cases. Index Terms Binarized Neural Networks, thermometric en- coding, structured pruning, nonlinear ADC I. INTRODUCTION Designing light-weight, fully-binarized models [1], [2] is a promising avenue to boost the efficiency of deep neu- ral network (DNN) execution towards always-on resourced- constrained inference [3] [7]. For instance, the multiply- accumulate (MAC) operation between 1-bit signed weights and activations can be implemented using XNORs and pop- counts only, thus enabling advantageous hardware simplifi- cation compared to its full-precision counterpart. Besides, it goes without saying that model binarization also offers a 32 reduction of memory needs for storing model s weights and local activations. However, despite remarkable progress in BNNs, most of the works only focus on the weights and activations while keeping regular input data to cap accu- racy loss. Unfortunately, it remains an issue for a concrete hardware implementation on devices that only support 1-bit computations, without the capability of dealing with integer inputs. To design small, fully-binarized networks with limited performance loss, we propose the following two contributions: a Generic Learned Thermometer (GLT) which is an input data binarization being jointly trained with a BNN, enabling DNNs execution with 1-bit computations only; This work is part of the IPCEI Microelectronics and Connectivity and was supported by the French Public Authorities within the frame of France 2030. a method to gradually replace complex processing blocks in BNNs by lighter grouped convolutions while limiting accuracy drop thanks to a distributional loss. II. RELATED WORKS Input data binarization: The de-facto choice to repre- sents input data with a normalized [0, 1] dynamic range is to consider M-bit fixed-point coding (e.g., M 8). In terms of hardware, it thus requires large bit-width multipliers to perform the scalar products, at least at the first layer of the model. To address this issue, [8] and [9] propose to directly use the base-2 fixed-point representation of each input value (Fig. 1 (a)), with additional depth-wise and point-wise convolutions to combine the resulting bit planes. However, in addition to not being robust to spurious inputs and bit-flips, it is clear that a base-2 fixed-point encoding can only be used in the digital domain. Another class of input data binarization is thermometer encoding [10], [11]. For instance, [11] intro- duces a fixed thermometer (FT) encoding with quantization thresholds following a linear ramp (Fig. 1 (b)). It is noteworthy mentioning that the thresholds of these thermometric encoding are manually designed and therefore may not be optimal for a wide variety of targeted tasks. On the contrary, GLT allows to automatically learn the thresholds as model s parameters that can be taken into account at the ADC level (Fig. 2) to involve a nonlinear input response, as a global tone mapping. 1 0 1 1 1 1 0 1 27 26 25 24 23 22 21 20 0 0 1 1 1 1 1 1 (b) Linear thermometer encoding [hand-crafted example] 112 80 48 16 144 176 208 240 189 0 1 1 1 1 1 1 1 (c) Generic Learned Thermometer (GLT) [typical example] 4 3 2 1 5 6 7 8 Pixel 44 19 11 5 87 139 188 223 (a) Base-2 fixed-point encoding [natural binary] Fig. 1: Three examples of encoding techniques for input image data binary representation. Here, for the sake of simplicity, input pixel dynamic range is considered between 0 and 255. BNNs pruning: Network pruning [12] [15] aims at remov- ing unnecessary operations, operands and weights to reduce model size and complexity. While most existing techniques are mainly designed for full-precision models, only some works address the pruning problem in the context of BNNs. For example, [14] employs weight flipping frequency while Copyright 2025 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending an email to arXiv:2505.13462v1 [cs.LG] 5 May 2025 [16] leverages the magnitude of latent weights as the pruning criteria. However, these methods result in irregular structures that require additional hardware features to handle sparse computations. In contrast, our block-based pruning scheme replaces complex blocks by lightweight grouped convolution layers, hence can be directly mapped to canonical hardware platforms. Compared to the existing depth pruning method [15], our method is specifically tailored for BNNs, by gradu- ally pruning complex blocks combining with a distributional loss [17] to avoid severe accuracy degradation. III. GENERIC LEARNED THERMOMETER Our encoding method (GLT) is illustrated in Fig. 1 (c), which reports a typical example of thermometric thresholds provided as model s parameters, being optimized for a specific inference task via model training. In practice, it intrinsically performs a global tone mapping operation that can be imple- mented directly during the analog-to-digital conversion stage, if relying on a programmable slope ADC as schematically depicted in Fig. 2. This top-level schematic relies on a 1bit comparator that sequentially outputs comparisons between Vpix (the pixel voltage to convert) and Vti. For i [[1, M]], Vti successively takes the M different threshold values ti encoded on Nb bits through the use of a DAC composed of a multiplexer and a set of voltage references ranging from Vmin to Vmax. Note that for this example Nb 8 and M 8. Vpix t8, t7, t6, t5, t4, t3, t2, t1 V Nb 8 Nb linear DAC 0, 1, 1, 1, 1, 1, 1, 1 1b comparator Vmin 00000000 Vmin Vstep 00000001 Vmin 2Vstep 00000010 Vmax-Vstep 11111111 Vmax-2Vstep 11111110 Vstep (Vmax Vmin) Nb 2 M 8 ti M 8 Fig. 2: Nonlinear ramp ADC top-level view schematic. A. Detailed mathematical formulation Let us consider an M-bit thermometer encoding with M thresholds 0 t1 t2 ... tM 1 tM 1. The thermo- vector TV RM corresponding to a value x is defined as: TVi Heaviside(x ti) ( 1 if x ti, 0 otherwise. (1) where x is normalized to the interval [0, 1] for ease of later optimization. The choice of the threshold values {ti}i [[1,M]] deeply impacts the quantity of relevant information preserved and thus model s performance, therefore our goal is to find optimal thresholds by minimizing any task loss. Let us denote t RM 1 as the learnable latent threshold parameters that are kept always positive during the optimization process. We first compute the normalized vector t as follows: t t PM 1 i 1 ti (2) With 0 ti 1 and PM 1 i 1 ti 1, the thermometer thresholds {ti}i [[1,M]] are then computed as a cumulative sum: ti i X j 1 tj (3) It can be easily verified that 0 ti ti 1 1 for i [[1, M]], and the optimal threshold values are indirectly learned through the optimization of t. Let us denote L as the task loss; I RH W as the input image data of resolution H W that is normalized within [0, 1]; and Ib i {0, 1}H W as the encoded bit plane at the bit position i [[1, M]]. Indeed, the gradient with respect to tj can be written by the chain rule as: L tj X x,y,i L Ib x,y,i Ib x,y,i ti ! ti tj X x,y,i L Ib x,y,i Heaviside(Ix,y ti) ti ! ti tj (4) where the term ti tj is calculated from Eqs. 2 and 3. The gradient vanishing problem caused by Heaviside function could be overcame using approximation techniques [18] [20]. Intuitively, near-threshold values should have higher influence (i.e., larger gradient magnitude) than far-threshold counterparts. Therefore, we employ the ReSTE approximation proposed in [20] and propose a modified version as follows: Heaviside(u) u 1 mmin 1 p u 1 p p , m (5) where p 1 controls the narrowness of the bell-shaped gradient curve and m is the clipping threshold. Concretely, p 2 and m 5 for all of our experiments. Since each ti is shared for all pixels within the same bit plane, a small change of the latent threshold parameters may notably tamper the model s behavior. Therefore, to stabilize the model convergence during training, the update magnitude of the latent parameters tj is scaled by a factor 尾 2 HWM. B. Latent parameter s initialization and constraint An important question is how to properly initialize the latent threshold parameters t. For the sake of simplicity, the straightforward option is to choose the initial values such that the resulting thermometer thresholds follow a linear ramp like proposed in [11]. In details, the threshold of the i-th bit-plane is defined as ti s(i 0.5) (2Nb 1) where s 2Nb M is the uniform step size and 1 (2Nb 1) is the normalization factor (e.g., 1 255 with 8b image data). Besides, the initialization of the non-normalized latent parameters t is scaled as: ti 0.5sk if i 1, sk if 1 i M, (0.5s 1)k if i M 1. (6) Here the scale factor k aims to balance the stabilization with the updatability of the GLT. For optimization purposes and with respect to training stability issues, the latent parameters are forced to be positive and large enough, i.e., ti 0.05. Coherently, the scaling value k is set such that the initial latent parameters remain higher than the aforementioned clipping threshold, e.g., by setting k M 1280. IV. BLOCK PRUNING WITH DISTRIBUTIONAL LOSS A. Proposed method BNNs usually need to be compressed further to match strict hardware specifications of edge devices, e.g., typically under 1Mb SRAM. In this section, we propose a method to gradually prune complex processing blocks and replace it with a lightweight convolution (LWC) block involving a g- group 3 3 convolution (GConv) of strides 2 followed by a channel shuffle (if g 1). Figure 3 illustrates how our idea is applied to MUXORNet-11 [21]. The number of channels is doubled after the GConv, thus allowing the pruned model to have the same data dimension as the baseline model. The LWC shows a far better hardware mapping than its corresponding block in the baseline model. We leverage the Kullback-Leibler divergence loss forcing the pruned model to learn similar class distributions as the baseline model: Ldistr T 2 N N X i 1 C X j 1 ybj(Xi; T)log ybj(Xi; T) ypj(Xi; T) (7) where ybj(Xi; T) and ypj(Xi; T) are the softened probability corresponding to class j of the baseline and the pruned model, given the input image Xi, i.e., the pre-softmax divided by a given temperature T. The total loss function is as follows: L (1 位)Lce 位Ldistr (8) where 位 is the hyperparameter controlling the balance between the KD loss Ldistr and the cross-entropy loss Lce. Concretely, we set T 8 and 位 0.5 for later experiments. Assume that the model contains Nb blocks, we gradually prune them in the direction from block Nb to block 1. For each stage, we replace each block by a LWC block such that the output dimension is kept unchanged. Without loss of generality, we also notice that here a block may contains a single or several layers. The pruned model is initialized with weights of the previous stage and then retrained with the aforementioned loss function. The complete pruning procedure is presented in Algorithm 1. Algorithm 1 Block pruning algorithm for BNNs. Input: (1) pre-trained baselines with first layers and classifier models (FL and Cls); (2) X the training set; (3) list of Nb blocks to be pruned W with Pb [[1, Nb]]; 1: Wp W 2: for b Nb to Pb do 3: Wp[b] Initialize LWCb() 4: PrunedNet Append(FL, Wp, Cls) 5: PrunedNet TrainToConvergence(PrunedNet; X) 6: end for Output: Optimized pruned model with FL, Wp and Cls Input image Block 1 Block 2 Block 3  LWC3  GAP FC GAP FC GLT 1 0 CBH, F CBH, F T-GAP CBMH, 2F GConv, s 2,  BN Heaviside CBH, F CBMH, F Conv, F BN Heaviside Conv, F BN MaxPool Heaviside Output classifier First layers not pruned Last layers prunable part Pruned model Baseline model (T)-GAP: (Thresholded) Global Average Pooling Fig. 3: Block pruning with an auxiliary lightweight grouped convolution (LWC) module. Note that our pruning method can be applied to an arbitrary network. V. EXPERIMENTS A. Validation of the GLT Implementation details: Each channel of the RGB input image is encoded separately into M binary planes (i.e., M {8, 16, 32}), then the resulting binary planes are concatenated together to feed the model. Note that here we only evaluate the effectiveness of the encoding method, therefore we do not consider extra architectures to combine binary planes like proposed in [8], [9]. For Visual Wake Words (VWW) dataset [22] we resize images to 132 176 for training and 120 160 for testing. We then perform data augmentation using random cropping to 120 160 and random horizontal flipping. For STL-10 dataset [23] with 96 96 RGB images, we adopt random crop from all-sided 12-pixel zero-padded images combined with random horizontal flip and random cutout [24] of 24 24 patches. We first pre-train the real- valued model with binarized input and ReLU activations, then use it to initialize the weights of the fully-binarized model with 1-b Signed weights and Heaviside activations with STE [18] gradient. Each model is trained during 100 epochs for VWW and 250 epochs for STL-10 with Radam optimizer [25]. The learning rate is initialized at 10 3 and reduced to 10 8 using cosine decay scheduler. VWW: We first conduct a benchmark on the gamma- inversed images (纬 2.2) of VWW with MUXORNet-11 model. Table I shows that models with GLT achieve higher accuracy on both train and test sets compared to FT and even the baseline gamma-inversed images in the case of binarized models. Specifically, at M 16, we obtain a gap of nearly 0.9 on training and 1.5 on testing. These results demonstrate that GLT allows model to learn more effectively during training and hence generalize better at inference time. STL-10: Table II and III report the accuracy of models after each stage of training on both the original and the gamma- inversed STL-10 datasets. It is shown that in most cases, model with GLT-encoded input achieves highest accuracy compared to model whose input is encoded by other methods. For the original dataset, at M 8, GLT has a slight gain of 1.07 TABLE I: MUXNORNet-11 on gamma-inversed VWW. Input encoding method planes (M) FP Bin. Train Test Train Test Baseline gamma-inversed 32-b 96.95 91.57 89.70 87.67 Fixed Linear Thermometer [11] 8 96.10 89.43 90.34 86.91 16 96.37 90.51 91.25 87.39 32 96.92 90.93 92.04 88.72 Ours GLT 8 96.24 90.83 90.94 88.66 16 96.83 91.09 92.13 88.87 32 97.17 91.23 92.39 89.43 0 50 100 150 200 250 x 1 2 3 4 5 6 7 8 M X i 1 Heaviside(x ti) (a) M 8 0 50 100 150 200 250 x 2 4 6 8 10 12 14 16 M X i 1 Heaviside(x ti) (b) M 16 0 50 100 150 200 250 x 5 10 15 20 25 30 M X i 1 Heaviside(x ti) (c) M 32 Fig. 4: Encoding curves of MUXORNet-11 GLT layers trained on gamma-inversed STL-10. Black: fixed linear curve [11], Red Green Blue: learned curves of R G B channels. for VGG-Small and 0.85 for MUXORNet-11 compared to FT [11]. This gain slightly decreases for larger M, as more bit planes will increase inter-channel redundancy. On the other hand, for the gamma-inversed dataset, the gain of GLT over FT is strongly boosted up to 2.5 for M 8 and it can even retain the accuracy level of FT on the original post-gamma correction dataset. In particular, for M 32, MUXORNet-11 with GLT even achieves a slightly higher accuracy than the baseline model with gamma-inversed input. Figure 4 shows the curves of the encoding bit-count level (from 1 to M) as a function of the thresholds learned on gamma-inversed dataset. It is shown that although being linearly initialized, our GLT can successfully learn proper nonlinear curves (i.e., global tone mapping), providing a higher accuracy and being well- suited to a real-world deployment scenario. It thus suggests the possibility of using GLT to perform inference directly from analog data, bypassing all the image rendering stages. TABLE II: Accuracy ( ) on original STL-10 dataset. Input encoding method planes (M) VGG-Small MUXORNet-11 FP Bin. FP Bin. Baseline integer (8-bit) 32-b 83.02 79.95 84.24 79.74 Base-2 fixed-point [8] 8 78.32 73.15 80.06 75.83 Fixed Linear Thermometer [11] 8 79.35 76.40 81.39 77.43 16 80.63 77.49 82.17 78.47 32 81.50 77.81 82.51 78.90 Ours GLT 8 79.93 77.47 81.62 78.28 16 80.87 78.02 82.73 78.60 32 81.40 78.24 82.79 79.06 TABLE III: Accuracy ( ) on gamma-inversed STL-10 dataset. Input encoding method planes (M) VGG-Small MUXORNet-11 FP Bin. FP Bin. Baseline gamma-inversed 32-b 82.29 79.44 83.03 78.44 Fixed Linear Thermometer [11] 8 76.51 73.40 78.39 74.99 16 77.99 75.09 80.81 76.98 32 78.86 75.86 80.85 77.30 Ours GLT 8 78.24 75.69 80.90 77.45 16 79.54 76.46 81.78 78.05 32 80.35 77.88 82.06 78.51 B. Validation of the block pruning on fully-binarized model In this part, we consider the MUXORNet-11 model with GLT and M 32, trained on gamma-inversed dataset (78.51 acc.), to conduct experiment. The pre-trained model after the first stage in V-A is used as teacher in our KD scheme. We set the groups g as 1, 2, 8 for three blocks, respectively, since the last layers are less sensitive to model s performance. We compare our gradual block pruning method with three competitors, involving 1) baseline: the pruned model but trained from scratch; 2) depth: depth pruning [15] using our LWCs as auxiliary model but trained in one shot without the KD loss Ldistr; 3) magnitude: channel-pruning based on the magnitude of the latent weights (inspired by [16]) in which the pruning ratio is computed to have the same model size as the block-pruned model. The pruned models are trained during 300 epochs with learning rate initialized at 10 3 and reduced to 10 10 using cosine decay scheduler. Figure 5 shows the trade-off of model size BOPs-accuracy loss of the pruned models. At each pruning point, our method always offers more than 3.6 higher accuracy compared to other methods. In particular, when pruning the third block, we successfully reduce the model size by 70 and the number of BOPs [26] by 16 , this nearly without accuracy drop. Even if we seek for a model of under 0.5Mb and 1GBOPs, our method still reaches 73 accuracy while other methods cannot exceed 68 . These results demonstrate the effectiveness of our method on designing extremely tiny, fully-binarized models. 15 30 100 Preserved ratio ( ) 0.5 1.0 3.0 Model size (Mb) 12 10 8 6 4 2 0 Accuracy loss ( ) Magnitude Depth Baseline Ours 60 80 100 Preserved ratio ( ) 1.0 1.2 1.4 1.6 1.8 BOPs (109) 12 10 8 6 4 2 0 Accuracy loss ( ) 3.6 Magnitude Depth Baseline Ours Fig. 5: Trade-off curve for model size BOPs reduction and accuracy loss of pruned MUXORNet-11 (original acc: 78.5 ). VI. CONCLUSION This work addresses the end-to-end design of small-sized, fully-binarized networks for always-on inference use cases. To do this, we first introduce the GLT encoding scheme to prop- erly manage input images binarization, jointly with performing a trained global tone mapping. Then, we propose a gradual block-based pruning strategy combined with a distributional loss to limit the overall accuracy drop. Experimental results on VWW and STL-10 demonstrate that GLT enables an efficient training stage, with a better generalization on unseen dataset. Our methods enable highly accurate models ( 78.5 accuracy on STL-10) that require 1-bit computations only with a model size under 1Mb, while relying on inputs of 32 bit-planes. Future works may investigate the performance on practical in-sensor data (e.g., on mosaiced frames impaired by fixed pattern noise and dead pixels), enabling ISP-free but still highly accurate always-on inference modules. REFERENCES [1] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, Binarized Neural Networks, in Advances in Neural Information Pro- cessing Systems (NeurIPS), 2016, pp. 4107 4115. [2] J. Bethge, C. Bartz, H. Yang, Y. Chen, and C. Meinel, MeliusNet: An Improved Network Architecture for Binary Neural Networks, in Proceedings of the IEEE CVF Winter Conference on Applications of Computer Vision (WACV), January 2021, pp. 1439 1448. [3] J. Choi, Review of low power image sensors for always-on imaging, in 2016 International SoC Design Conference (ISOCC), 2016, pp. 11 12. [4] M. Gazivoda and V. Bilas, Always-On Sparse Event Wake-Up Detec- tors: A Review, IEEE Sensors Journal, vol. 22, no. 9, pp. 8313 8326, 2022. [5] D. Garrett, Y. S. Park, S. Kim, J. Sharma, W. Huang, M. Shaghaghi, V. Parthasarathy, S. Gibellini, S. Bailey, M. Moturi, P. Vorenkamp, K. Busch, J. Holleman, B. Javid, A. Yousefi, M. Judy, and A. Gupta, A 1mW Always-on Computer Vision Deep Learning Neural Decision Processor, in 2023 IEEE International Solid-State Circuits Conference (ISSCC), 2023, pp. 8 10. [6] A. Verdant, W. Guicquero, D. Coriat, G. Moritz, N. Royer, S. Thuries, A. Mollard, V. Teil, Y. Desprez, G. Monnot, P. Malinge, B. Paille, G. Caubit, A. Bourge, L. Tardif, S. Bigault, and J. Chossat, A wake-up module featuring auto-bracketed 3-scale log- corrected pattern recognition and motion detection in a 1.5mpix 8t global shutter imager, in 2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits), 2024, pp. 1 2. [7] J. Vohra, A. Gupta, and M. Alioto, Imager with In-Sensor Event Detec- tion and Morphological Transformations with 2.9pJ pixel frame Object Segmentation FOM for Always-On Surveillance in 40nm, in 2024 IEEE International Solid-State Circuits Conference (ISSCC), vol. 67, 2024, pp. 104 106. [8] R. D urichen, T. Rocznik, O. Renz, and C. Peters, Binary Input Layer: Training of CNN models with binary input data, CoRR, vol. abs 1812.03410, 2018. [Online]. Available: 03410 [9] L. Vorabbi, D. Maltoni, and S. Santi, Input Layer Binarization with Bit-Plane Encoding, in International Conference on Artificial Neural Networks, 2023. [10] D. Bankman, L. Yang, B. Moons, M. Verhelst, and B. Murmann, An Always-On 3.8 碌 J 86 CIFAR-10 Mixed-Signal Binary CNN Processor With All Memory on Chip in 28-nm CMOS, IEEE Journal of Solid-State Circuits, vol. 54, no. 1, pp. 158 172, 2019. [11] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations, in The 2021 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA 21. New York, NY, USA: Association for Computing Machinery, 2021, p. 171 182. [12] S. Han, J. Pool, J. Tran, and W. Dally, Learning both Weights and Connections for Efficient Neural Network, in Advances in Neural Information Processing Systems, C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, Eds., vol. 28. Curran Associates, Inc., 2015. [13] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, What is the State of Neural Network Pruning? in Proceedings of Machine Learning and Systems, I. Dhillon, D. Papailiopoulos, and V. Sze, Eds., vol. 2, 2020, pp. 129 146. [14] Y. Li and F. Ren, BNN Pruning: Pruning Binary Neural Network Guided by Weight Flipping Frequency, in 2020 21st International Symposium on Quality Electronic Design (ISQED), 2020, pp. 306 311. [15] J. D. De Leon and R. Atienza, Depth Pruning with Auxiliary Networks for Tinyml, in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 3963 3967. [16] T. Chen, N. Anderson, and Y. Kim, Latent Weight-based Pruning for Small Binary Neural Networks, in 2023 28th Asia and South Pacific Design Automation Conference (ASP-DAC), 2023, pp. 751 756. [17] Z. Liu, Z. Shen, M. Savvides, and K.-T. Cheng, Reactnet: Towards precise binary neural network with generalized activation functions, in European Conference on Computer Vision (ECCV), 2020. [18] Y. Bengio, N. L eonard, and A. Courville, Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation, arXiv:1308.3432 [cs], Aug. 2013. [19] H. Le, R. K. H酶ier, C.-T. Lin, and C. Zach, AdaSTE: An Adaptive Straight-Through Estimator To Train Binary Neural Networks, in Pro- ceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022, pp. 460 469. [20] X.-M. Wu, D. Zheng, Z. Liu, and W.-S. Zheng, Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training, in Proceedings of the IEEE CVF International Conference on Computer Vision (ICCV), October 2023, pp. 17 055 17 064. [21] V. T. Nguyen, W. Guicquero, and G. Sicard, Histogram-Equalized Quantization for logic-gated Residual Neural Networks, in 2022 IEEE International Symposium on Circuits and Systems (ISCAS), 2022, pp. 1289 1293. [22] A. Chowdhery, P. Warden, J. Shlens, A. G. Howard, and R. Rhodes, Visual wake words dataset, ArXiv, vol. abs 1906.05721, 2019. [23] A. Coates, A. Ng, and H. Lee, An Analysis of Single-Layer Networks in Unsupervised Feature Learning, in AISTATS, 2011. [24] T. Devries and G. W. Taylor, Improved regularization of convolutional neural networks with cutout, ArXiv, vol. abs 1708.04552, 2017. [25] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, On the Variance of the Adaptive Learning Rate and Beyond, in International Conference on Learning Representations, 2020. [26] Y. Wang, Y. Lu, and T. Blankevoort, Differentiable joint pruning and quantization for hardware efficiency, in Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceed- ings, Part XXIX, ser. Lecture Notes in Computer Science, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, Eds., vol. 12374. Springer, pp. 259 277.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nEnd-to-end fully-binarized network design: from Generic Learned Thermometer to Block Pruning Thien Nguyen and William Guicquero Smart Integrated Circuits for Imaging Laboratory, CEA-LETI F-38000, Grenoble, France. Email: [vanthien.nguyen, Abstract Existing works on Binary Neural Network (BNN) mainly focus on model s weights and activations while discarding considerations on the input raw data. This article introduces Generic Learned Thermometer (GLT), an encoding technique to improve input data representation for BNN, relying on learning non linear quantization thresholds. This technique consists in multiple data binarizations which can advantageously replace a conventional Analog to Digital Conversion (ADC) that uses natural binary coding. Additionally, we jointly propose a compact topology with light-weight grouped convolutions being trained thanks to block pruning and Knowledge Distillation (KD), aiming at reducing furthermore the model size so as its computational complexity. We show that GLT brings versatility to the BNN by intrinsically performing global tone mapping, enabling significant accuracy gains in practice (demonstrated by simulations on the STL-10 and VWW datasets). Moreover, when combining GLT with our proposed block-pruning technique, we successfully achieve lightweight (under 1Mb), fully-binarized models with limited accuracy degradation while being suitable for in-sensor always-on inference use cases. Index Terms Binarized Neural Networks, thermometric en- coding, structured pruning, nonlinear ADC I. INTRODUCTION Designing light-weight, fully-binarized models [1], [2] is a promising avenue to boost the efficiency of deep neu- ral network (DNN) execution towards always-on resourced- constrained inference [3] [7]. For instance, the multiply- accumulate (MAC) operation between 1-bit signed weights and activations can be implemented using XNORs and pop- counts only, thus enabling advantageous hardware simplifi- cation compared to its full-precision counterpart. Besides, it goes without saying that model binarization also offers a 32 reduction of memory needs for storing model s weights and local activations. However, despite remarkable progress in BNNs, most of the works only focus on the weights and activations while keeping regular input data to cap accu- racy loss.\n\n--- Segment 2 ---\nBesides, it goes without saying that model binarization also offers a 32 reduction of memory needs for storing model s weights and local activations. However, despite remarkable progress in BNNs, most of the works only focus on the weights and activations while keeping regular input data to cap accu- racy loss. Unfortunately, it remains an issue for a concrete hardware implementation on devices that only support 1-bit computations, without the capability of dealing with integer inputs. To design small, fully-binarized networks with limited performance loss, we propose the following two contributions: a Generic Learned Thermometer (GLT) which is an input data binarization being jointly trained with a BNN, enabling DNNs execution with 1-bit computations only; This work is part of the IPCEI Microelectronics and Connectivity and was supported by the French Public Authorities within the frame of France 2030. a method to gradually replace complex processing blocks in BNNs by lighter grouped convolutions while limiting accuracy drop thanks to a distributional loss. II. RELATED WORKS Input data binarization: The de-facto choice to repre- sents input data with a normalized [0, 1] dynamic range is to consider M-bit fixed-point coding (e.g., M 8). In terms of hardware, it thus requires large bit-width multipliers to perform the scalar products, at least at the first layer of the model. To address this issue, [8] and [9] propose to directly use the base-2 fixed-point representation of each input value (Fig. 1 (a)), with additional depth-wise and point-wise convolutions to combine the resulting bit planes. However, in addition to not being robust to spurious inputs and bit-flips, it is clear that a base-2 fixed-point encoding can only be used in the digital domain. Another class of input data binarization is thermometer encoding [10], [11]. For instance, [11] intro- duces a fixed thermometer (FT) encoding with quantization thresholds following a linear ramp (Fig. 1 (b)). It is noteworthy mentioning that the thresholds of these thermometric encoding are manually designed and therefore may not be optimal for a wide variety of targeted tasks. On the contrary, GLT allows to automatically learn the thresholds as model s parameters that can be taken into account at the ADC level (Fig.\n\n--- Segment 3 ---\nIt is noteworthy mentioning that the thresholds of these thermometric encoding are manually designed and therefore may not be optimal for a wide variety of targeted tasks. On the contrary, GLT allows to automatically learn the thresholds as model s parameters that can be taken into account at the ADC level (Fig. 2) to involve a nonlinear input response, as a global tone mapping. 1 0 1 1 1 1 0 1 27 26 25 24 23 22 21 20 0 0 1 1 1 1 1 1 (b) Linear thermometer encoding [hand-crafted example] 112 80 48 16 144 176 208 240 189 0 1 1 1 1 1 1 1 (c) Generic Learned Thermometer (GLT) [typical example] 4 3 2 1 5 6 7 8 Pixel 44 19 11 5 87 139 188 223 (a) Base-2 fixed-point encoding [natural binary] Fig. 1: Three examples of encoding techniques for input image data binary representation. Here, for the sake of simplicity, input pixel dynamic range is considered between 0 and 255. BNNs pruning: Network pruning [12] [15] aims at remov- ing unnecessary operations, operands and weights to reduce model size and complexity. While most existing techniques are mainly designed for full-precision models, only some works address the pruning problem in the context of BNNs. For example, [14] employs weight flipping frequency while Copyright 2025 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending an email to arXiv:2505.13462v1 [cs.LG] 5 May 2025 [16] leverages the magnitude of latent weights as the pruning criteria. However, these methods result in irregular structures that require additional hardware features to handle sparse computations. In contrast, our block-based pruning scheme replaces complex blocks by lightweight grouped convolution layers, hence can be directly mapped to canonical hardware platforms. Compared to the existing depth pruning method [15], our method is specifically tailored for BNNs, by gradu- ally pruning complex blocks combining with a distributional loss [17] to avoid severe accuracy degradation. III.\n\n--- Segment 4 ---\nCompared to the existing depth pruning method [15], our method is specifically tailored for BNNs, by gradu- ally pruning complex blocks combining with a distributional loss [17] to avoid severe accuracy degradation. III. GENERIC LEARNED THERMOMETER Our encoding method (GLT) is illustrated in Fig. 1 (c), which reports a typical example of thermometric thresholds provided as model s parameters, being optimized for a specific inference task via model training. In practice, it intrinsically performs a global tone mapping operation that can be imple- mented directly during the analog-to-digital conversion stage, if relying on a programmable slope ADC as schematically depicted in Fig. 2. This top-level schematic relies on a 1bit comparator that sequentially outputs comparisons between Vpix (the pixel voltage to convert) and Vti. For i [[1, M]], Vti successively takes the M different threshold values ti encoded on Nb bits through the use of a DAC composed of a multiplexer and a set of voltage references ranging from Vmin to Vmax. Note that for this example Nb 8 and M 8. Vpix t8, t7, t6, t5, t4, t3, t2, t1 V Nb 8 Nb linear DAC 0, 1, 1, 1, 1, 1, 1, 1 1b comparator Vmin 00000000 Vmin Vstep 00000001 Vmin 2Vstep 00000010 Vmax-Vstep 11111111 Vmax-2Vstep 11111110 Vstep (Vmax Vmin) Nb 2 M 8 ti M 8 Fig. 2: Nonlinear ramp ADC top-level view schematic. A. Detailed mathematical formulation Let us consider an M-bit thermometer encoding with M thresholds 0 t1 t2 ... tM 1 tM 1. The thermo- vector TV RM corresponding to a value x is defined as: TVi Heaviside(x ti) ( 1 if x ti, 0 otherwise. (1) where x is normalized to the interval [0, 1] for ease of later optimization. The choice of the threshold values {ti}i [[1,M]] deeply impacts the quantity of relevant information preserved and thus model s performance, therefore our goal is to find optimal thresholds by minimizing any task loss.\n\n--- Segment 5 ---\n(1) where x is normalized to the interval [0, 1] for ease of later optimization. The choice of the threshold values {ti}i [[1,M]] deeply impacts the quantity of relevant information preserved and thus model s performance, therefore our goal is to find optimal thresholds by minimizing any task loss. Let us denote t RM 1 as the learnable latent threshold parameters that are kept always positive during the optimization process. We first compute the normalized vector t as follows: t t PM 1 i 1 ti (2) With 0 ti 1 and PM 1 i 1 ti 1, the thermometer thresholds {ti}i [[1,M]] are then computed as a cumulative sum: ti i X j 1 tj (3) It can be easily verified that 0 ti ti 1 1 for i [[1, M]], and the optimal threshold values are indirectly learned through the optimization of t. Let us denote L as the task loss; I RH W as the input image data of resolution H W that is normalized within [0, 1]; and Ib i {0, 1}H W as the encoded bit plane at the bit position i [[1, M]]. Indeed, the gradient with respect to tj can be written by the chain rule as: L tj X x,y,i L Ib x,y,i Ib x,y,i ti ! ti tj X x,y,i L Ib x,y,i Heaviside(Ix,y ti) ti ! ti tj (4) where the term ti tj is calculated from Eqs. 2 and 3. The gradient vanishing problem caused by Heaviside function could be overcame using approximation techniques [18] [20]. Intuitively, near-threshold values should have higher influence (i.e., larger gradient magnitude) than far-threshold counterparts. Therefore, we employ the ReSTE approximation proposed in [20] and propose a modified version as follows: Heaviside(u) u 1 mmin 1 p u 1 p p , m (5) where p 1 controls the narrowness of the bell-shaped gradient curve and m is the clipping threshold. Concretely, p 2 and m 5 for all of our experiments. Since each ti is shared for all pixels within the same bit plane, a small change of the latent threshold parameters may notably tamper the model s behavior.\n\n--- Segment 6 ---\nConcretely, p 2 and m 5 for all of our experiments. Since each ti is shared for all pixels within the same bit plane, a small change of the latent threshold parameters may notably tamper the model s behavior. Therefore, to stabilize the model convergence during training, the update magnitude of the latent parameters tj is scaled by a factor 尾 2 HWM. B. Latent parameter s initialization and constraint An important question is how to properly initialize the latent threshold parameters t. For the sake of simplicity, the straightforward option is to choose the initial values such that the resulting thermometer thresholds follow a linear ramp like proposed in [11]. In details, the threshold of the i-th bit-plane is defined as ti s(i 0.5) (2Nb 1) where s 2Nb M is the uniform step size and 1 (2Nb 1) is the normalization factor (e.g., 1 255 with 8b image data). Besides, the initialization of the non-normalized latent parameters t is scaled as: ti 0.5sk if i 1, sk if 1 i M, (0.5s 1)k if i M 1. (6) Here the scale factor k aims to balance the stabilization with the updatability of the GLT. For optimization purposes and with respect to training stability issues, the latent parameters are forced to be positive and large enough, i.e., ti 0.05. Coherently, the scaling value k is set such that the initial latent parameters remain higher than the aforementioned clipping threshold, e.g., by setting k M 1280. IV. BLOCK PRUNING WITH DISTRIBUTIONAL LOSS A. Proposed method BNNs usually need to be compressed further to match strict hardware specifications of edge devices, e.g., typically under 1Mb SRAM. In this section, we propose a method to gradually prune complex processing blocks and replace it with a lightweight convolution (LWC) block involving a g- group 3 3 convolution (GConv) of strides 2 followed by a channel shuffle (if g 1). Figure 3 illustrates how our idea is applied to MUXORNet-11 [21]. The number of channels is doubled after the GConv, thus allowing the pruned model to have the same data dimension as the baseline model. The LWC shows a far better hardware mapping than its corresponding block in the baseline model.\n\n--- Segment 7 ---\nThe number of channels is doubled after the GConv, thus allowing the pruned model to have the same data dimension as the baseline model. The LWC shows a far better hardware mapping than its corresponding block in the baseline model. We leverage the Kullback-Leibler divergence loss forcing the pruned model to learn similar class distributions as the baseline model: Ldistr T 2 N N X i 1 C X j 1 ybj(Xi; T)log ybj(Xi; T) ypj(Xi; T) (7) where ybj(Xi; T) and ypj(Xi; T) are the softened probability corresponding to class j of the baseline and the pruned model, given the input image Xi, i.e., the pre-softmax divided by a given temperature T. The total loss function is as follows: L (1 位)Lce 位Ldistr (8) where 位 is the hyperparameter controlling the balance between the KD loss Ldistr and the cross-entropy loss Lce. Concretely, we set T 8 and 位 0.5 for later experiments. Assume that the model contains Nb blocks, we gradually prune them in the direction from block Nb to block 1. For each stage, we replace each block by a LWC block such that the output dimension is kept unchanged. Without loss of generality, we also notice that here a block may contains a single or several layers. The pruned model is initialized with weights of the previous stage and then retrained with the aforementioned loss function. The complete pruning procedure is presented in Algorithm 1. Algorithm 1 Block pruning algorithm for BNNs.\n\n--- Segment 8 ---\nThe complete pruning procedure is presented in Algorithm 1. Algorithm 1 Block pruning algorithm for BNNs. Input: (1) pre-trained baselines with first layers and classifier models (FL and Cls); (2) X the training set; (3) list of Nb blocks to be pruned W with Pb [[1, Nb]]; 1: Wp W 2: for b Nb to Pb do 3: Wp[b] Initialize LWCb() 4: PrunedNet Append(FL, Wp, Cls) 5: PrunedNet TrainToConvergence(PrunedNet; X) 6: end for Output: Optimized pruned model with FL, Wp and Cls Input image Block 1 Block 2 Block 3  LWC3  GAP FC GAP FC GLT 1 0 CBH, F CBH, F T-GAP CBMH, 2F GConv, s 2,  BN Heaviside CBH, F CBMH, F Conv, F BN Heaviside Conv, F BN MaxPool Heaviside Output classifier First layers not pruned Last layers prunable part Pruned model Baseline model (T)-GAP: (Thresholded) Global Average Pooling Fig. 3: Block pruning with an auxiliary lightweight grouped convolution (LWC) module. Note that our pruning method can be applied to an arbitrary network. V. EXPERIMENTS A. Validation of the GLT Implementation details: Each channel of the RGB input image is encoded separately into M binary planes (i.e., M {8, 16, 32}), then the resulting binary planes are concatenated together to feed the model. Note that here we only evaluate the effectiveness of the encoding method, therefore we do not consider extra architectures to combine binary planes like proposed in [8], [9]. For Visual Wake Words (VWW) dataset [22] we resize images to 132 176 for training and 120 160 for testing. We then perform data augmentation using random cropping to 120 160 and random horizontal flipping.\n\n--- Segment 9 ---\nFor Visual Wake Words (VWW) dataset [22] we resize images to 132 176 for training and 120 160 for testing. We then perform data augmentation using random cropping to 120 160 and random horizontal flipping. For STL-10 dataset [23] with 96 96 RGB images, we adopt random crop from all-sided 12-pixel zero-padded images combined with random horizontal flip and random cutout [24] of 24 24 patches. We first pre-train the real- valued model with binarized input and ReLU activations, then use it to initialize the weights of the fully-binarized model with 1-b Signed weights and Heaviside activations with STE [18] gradient. Each model is trained during 100 epochs for VWW and 250 epochs for STL-10 with Radam optimizer [25]. The learning rate is initialized at 10 3 and reduced to 10 8 using cosine decay scheduler. VWW: We first conduct a benchmark on the gamma- inversed images (纬 2.2) of VWW with MUXORNet-11 model. Table I shows that models with GLT achieve higher accuracy on both train and test sets compared to FT and even the baseline gamma-inversed images in the case of binarized models. Specifically, at M 16, we obtain a gap of nearly 0.9 on training and 1.5 on testing. These results demonstrate that GLT allows model to learn more effectively during training and hence generalize better at inference time. STL-10: Table II and III report the accuracy of models after each stage of training on both the original and the gamma- inversed STL-10 datasets. It is shown that in most cases, model with GLT-encoded input achieves highest accuracy compared to model whose input is encoded by other methods. For the original dataset, at M 8, GLT has a slight gain of 1.07 TABLE I: MUXNORNet-11 on gamma-inversed VWW. Input encoding method planes (M) FP Bin.\n\n--- Segment 10 ---\nFor the original dataset, at M 8, GLT has a slight gain of 1.07 TABLE I: MUXNORNet-11 on gamma-inversed VWW. Input encoding method planes (M) FP Bin. Train Test Train Test Baseline gamma-inversed 32-b 96.95 91.57 89.70 87.67 Fixed Linear Thermometer [11] 8 96.10 89.43 90.34 86.91 16 96.37 90.51 91.25 87.39 32 96.92 90.93 92.04 88.72 Ours GLT 8 96.24 90.83 90.94 88.66 16 96.83 91.09 92.13 88.87 32 97.17 91.23 92.39 89.43 0 50 100 150 200 250 x 1 2 3 4 5 6 7 8 M X i 1 Heaviside(x ti) (a) M 8 0 50 100 150 200 250 x 2 4 6 8 10 12 14 16 M X i 1 Heaviside(x ti) (b) M 16 0 50 100 150 200 250 x 5 10 15 20 25 30 M X i 1 Heaviside(x ti) (c) M 32 Fig. 4: Encoding curves of MUXORNet-11 GLT layers trained on gamma-inversed STL-10. Black: fixed linear curve [11], Red Green Blue: learned curves of R G B channels. for VGG-Small and 0.85 for MUXORNet-11 compared to FT [11]. This gain slightly decreases for larger M, as more bit planes will increase inter-channel redundancy. On the other hand, for the gamma-inversed dataset, the gain of GLT over FT is strongly boosted up to 2.5 for M 8 and it can even retain the accuracy level of FT on the original post-gamma correction dataset. In particular, for M 32, MUXORNet-11 with GLT even achieves a slightly higher accuracy than the baseline model with gamma-inversed input. Figure 4 shows the curves of the encoding bit-count level (from 1 to M) as a function of the thresholds learned on gamma-inversed dataset.\n\n--- Segment 11 ---\nIn particular, for M 32, MUXORNet-11 with GLT even achieves a slightly higher accuracy than the baseline model with gamma-inversed input. Figure 4 shows the curves of the encoding bit-count level (from 1 to M) as a function of the thresholds learned on gamma-inversed dataset. It is shown that although being linearly initialized, our GLT can successfully learn proper nonlinear curves (i.e., global tone mapping), providing a higher accuracy and being well- suited to a real-world deployment scenario. It thus suggests the possibility of using GLT to perform inference directly from analog data, bypassing all the image rendering stages. TABLE II: Accuracy ( ) on original STL-10 dataset. Input encoding method planes (M) VGG-Small MUXORNet-11 FP Bin. FP Bin. Baseline integer (8-bit) 32-b 83.02 79.95 84.24 79.74 Base-2 fixed-point [8] 8 78.32 73.15 80.06 75.83 Fixed Linear Thermometer [11] 8 79.35 76.40 81.39 77.43 16 80.63 77.49 82.17 78.47 32 81.50 77.81 82.51 78.90 Ours GLT 8 79.93 77.47 81.62 78.28 16 80.87 78.02 82.73 78.60 32 81.40 78.24 82.79 79.06 TABLE III: Accuracy ( ) on gamma-inversed STL-10 dataset. Input encoding method planes (M) VGG-Small MUXORNet-11 FP Bin. FP Bin.\n\n--- Segment 12 ---\nInput encoding method planes (M) VGG-Small MUXORNet-11 FP Bin. FP Bin. Baseline gamma-inversed 32-b 82.29 79.44 83.03 78.44 Fixed Linear Thermometer [11] 8 76.51 73.40 78.39 74.99 16 77.99 75.09 80.81 76.98 32 78.86 75.86 80.85 77.30 Ours GLT 8 78.24 75.69 80.90 77.45 16 79.54 76.46 81.78 78.05 32 80.35 77.88 82.06 78.51 B. Validation of the block pruning on fully-binarized model In this part, we consider the MUXORNet-11 model with GLT and M 32, trained on gamma-inversed dataset (78.51 acc. ), to conduct experiment. The pre-trained model after the first stage in V-A is used as teacher in our KD scheme. We set the groups g as 1, 2, 8 for three blocks, respectively, since the last layers are less sensitive to model s performance. We compare our gradual block pruning method with three competitors, involving 1) baseline: the pruned model but trained from scratch; 2) depth: depth pruning [15] using our LWCs as auxiliary model but trained in one shot without the KD loss Ldistr; 3) magnitude: channel-pruning based on the magnitude of the latent weights (inspired by [16]) in which the pruning ratio is computed to have the same model size as the block-pruned model. The pruned models are trained during 300 epochs with learning rate initialized at 10 3 and reduced to 10 10 using cosine decay scheduler. Figure 5 shows the trade-off of model size BOPs-accuracy loss of the pruned models. At each pruning point, our method always offers more than 3.6 higher accuracy compared to other methods. In particular, when pruning the third block, we successfully reduce the model size by 70 and the number of BOPs [26] by 16 , this nearly without accuracy drop. Even if we seek for a model of under 0.5Mb and 1GBOPs, our method still reaches 73 accuracy while other methods cannot exceed 68 .\n\n--- Segment 13 ---\nIn particular, when pruning the third block, we successfully reduce the model size by 70 and the number of BOPs [26] by 16 , this nearly without accuracy drop. Even if we seek for a model of under 0.5Mb and 1GBOPs, our method still reaches 73 accuracy while other methods cannot exceed 68 . These results demonstrate the effectiveness of our method on designing extremely tiny, fully-binarized models. 15 30 100 Preserved ratio ( ) 0.5 1.0 3.0 Model size (Mb) 12 10 8 6 4 2 0 Accuracy loss ( ) Magnitude Depth Baseline Ours 60 80 100 Preserved ratio ( ) 1.0 1.2 1.4 1.6 1.8 BOPs (109) 12 10 8 6 4 2 0 Accuracy loss ( ) 3.6 Magnitude Depth Baseline Ours Fig. 5: Trade-off curve for model size BOPs reduction and accuracy loss of pruned MUXORNet-11 (original acc: 78.5 ). VI. CONCLUSION This work addresses the end-to-end design of small-sized, fully-binarized networks for always-on inference use cases. To do this, we first introduce the GLT encoding scheme to prop- erly manage input images binarization, jointly with performing a trained global tone mapping. Then, we propose a gradual block-based pruning strategy combined with a distributional loss to limit the overall accuracy drop. Experimental results on VWW and STL-10 demonstrate that GLT enables an efficient training stage, with a better generalization on unseen dataset. Our methods enable highly accurate models ( 78.5 accuracy on STL-10) that require 1-bit computations only with a model size under 1Mb, while relying on inputs of 32 bit-planes. Future works may investigate the performance on practical in-sensor data (e.g., on mosaiced frames impaired by fixed pattern noise and dead pixels), enabling ISP-free but still highly accurate always-on inference modules. REFERENCES [1] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, Binarized Neural Networks, in Advances in Neural Information Pro- cessing Systems (NeurIPS), 2016, pp. 4107 4115.\n\n--- Segment 14 ---\nREFERENCES [1] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, Binarized Neural Networks, in Advances in Neural Information Pro- cessing Systems (NeurIPS), 2016, pp. 4107 4115. [2] J. Bethge, C. Bartz, H. Yang, Y. Chen, and C. Meinel, MeliusNet: An Improved Network Architecture for Binary Neural Networks, in Proceedings of the IEEE CVF Winter Conference on Applications of Computer Vision (WACV), January 2021, pp. 1439 1448. [3] J. Choi, Review of low power image sensors for always-on imaging, in 2016 International SoC Design Conference (ISOCC), 2016, pp. 11 12. [4] M. Gazivoda and V. Bilas, Always-On Sparse Event Wake-Up Detec- tors: A Review, IEEE Sensors Journal, vol. 22, no. 9, pp. 8313 8326, 2022. [5] D. Garrett, Y. S. Park, S. Kim, J. Sharma, W. Huang, M. Shaghaghi, V. Parthasarathy, S. Gibellini, S. Bailey, M. Moturi, P. Vorenkamp, K. Busch, J. Holleman, B. Javid, A. Yousefi, M. Judy, and A. Gupta, A 1mW Always-on Computer Vision Deep Learning Neural Decision Processor, in 2023 IEEE International Solid-State Circuits Conference (ISSCC), 2023, pp. 8 10.\n\n--- Segment 15 ---\n[5] D. Garrett, Y. S. Park, S. Kim, J. Sharma, W. Huang, M. Shaghaghi, V. Parthasarathy, S. Gibellini, S. Bailey, M. Moturi, P. Vorenkamp, K. Busch, J. Holleman, B. Javid, A. Yousefi, M. Judy, and A. Gupta, A 1mW Always-on Computer Vision Deep Learning Neural Decision Processor, in 2023 IEEE International Solid-State Circuits Conference (ISSCC), 2023, pp. 8 10. [6] A. Verdant, W. Guicquero, D. Coriat, G. Moritz, N. Royer, S. Thuries, A. Mollard, V. Teil, Y. Desprez, G. Monnot, P. Malinge, B. Paille, G. Caubit, A. Bourge, L. Tardif, S. Bigault, and J. Chossat, A wake-up module featuring auto-bracketed 3-scale log- corrected pattern recognition and motion detection in a 1.5mpix 8t global shutter imager, in 2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits), 2024, pp. 1 2. [7] J. Vohra, A. Gupta, and M. Alioto, Imager with In-Sensor Event Detec- tion and Morphological Transformations with 2.9pJ pixel frame Object Segmentation FOM for Always-On Surveillance in 40nm, in 2024 IEEE International Solid-State Circuits Conference (ISSCC), vol. 67, 2024, pp. 104 106. [8] R. D urichen, T. Rocznik, O. Renz, and C. Peters, Binary Input Layer: Training of CNN models with binary input data, CoRR, vol. abs 1812.03410, 2018. [Online]. Available: 03410 [9] L. Vorabbi, D. Maltoni, and S. Santi, Input Layer Binarization with Bit-Plane Encoding, in International Conference on Artificial Neural Networks, 2023.\n\n--- Segment 16 ---\n[Online]. Available: 03410 [9] L. Vorabbi, D. Maltoni, and S. Santi, Input Layer Binarization with Bit-Plane Encoding, in International Conference on Artificial Neural Networks, 2023. [10] D. Bankman, L. Yang, B. Moons, M. Verhelst, and B. Murmann, An Always-On 3.8 碌 J 86 CIFAR-10 Mixed-Signal Binary CNN Processor With All Memory on Chip in 28-nm CMOS, IEEE Journal of Solid-State Circuits, vol. 54, no. 1, pp. 158 172, 2019. [11] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations, in The 2021 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA 21. New York, NY, USA: Association for Computing Machinery, 2021, p. 171 182. [12] S. Han, J. Pool, J. Tran, and W. Dally, Learning both Weights and Connections for Efficient Neural Network, in Advances in Neural Information Processing Systems, C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, Eds., vol. 28. Curran Associates, Inc., 2015. [13] D. Blalock, J. J. Gonzalez Ortiz, J. Frankle, and J. Guttag, What is the State of Neural Network Pruning? in Proceedings of Machine Learning and Systems, I. Dhillon, D. Papailiopoulos, and V. Sze, Eds., vol. 2, 2020, pp. 129 146. [14] Y. Li and F. Ren, BNN Pruning: Pruning Binary Neural Network Guided by Weight Flipping Frequency, in 2020 21st International Symposium on Quality Electronic Design (ISQED), 2020, pp. 306 311.\n\n--- Segment 17 ---\n[14] Y. Li and F. Ren, BNN Pruning: Pruning Binary Neural Network Guided by Weight Flipping Frequency, in 2020 21st International Symposium on Quality Electronic Design (ISQED), 2020, pp. 306 311. [15] J. D. De Leon and R. Atienza, Depth Pruning with Auxiliary Networks for Tinyml, in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 3963 3967. [16] T. Chen, N. Anderson, and Y. Kim, Latent Weight-based Pruning for Small Binary Neural Networks, in 2023 28th Asia and South Pacific Design Automation Conference (ASP-DAC), 2023, pp. 751 756. [17] Z. Liu, Z. Shen, M. Savvides, and K.-T. Cheng, Reactnet: Towards precise binary neural network with generalized activation functions, in European Conference on Computer Vision (ECCV), 2020. [18] Y. Bengio, N. L eonard, and A. Courville, Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation, arXiv:1308.3432 [cs], Aug. 2013. [19] H. Le, R. K. H酶ier, C.-T. Lin, and C. Zach, AdaSTE: An Adaptive Straight-Through Estimator To Train Binary Neural Networks, in Pro- ceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2022, pp. 460 469. [20] X.-M. Wu, D. Zheng, Z. Liu, and W.-S. Zheng, Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training, in Proceedings of the IEEE CVF International Conference on Computer Vision (ICCV), October 2023, pp. 17 055 17 064. [21] V. T. Nguyen, W. Guicquero, and G. Sicard, Histogram-Equalized Quantization for logic-gated Residual Neural Networks, in 2022 IEEE International Symposium on Circuits and Systems (ISCAS), 2022, pp. 1289 1293.\n\n--- Segment 18 ---\n[21] V. T. Nguyen, W. Guicquero, and G. Sicard, Histogram-Equalized Quantization for logic-gated Residual Neural Networks, in 2022 IEEE International Symposium on Circuits and Systems (ISCAS), 2022, pp. 1289 1293. [22] A. Chowdhery, P. Warden, J. Shlens, A. G. Howard, and R. Rhodes, Visual wake words dataset, ArXiv, vol. abs 1906.05721, 2019. [23] A. Coates, A. Ng, and H. Lee, An Analysis of Single-Layer Networks in Unsupervised Feature Learning, in AISTATS, 2011. [24] T. Devries and G. W. Taylor, Improved regularization of convolutional neural networks with cutout, ArXiv, vol. abs 1708.04552, 2017. [25] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, On the Variance of the Adaptive Learning Rate and Beyond, in International Conference on Learning Representations, 2020. [26] Y. Wang, Y. Lu, and T. Blankevoort, Differentiable joint pruning and quantization for hardware efficiency, in Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceed- ings, Part XXIX, ser. Lecture Notes in Computer Science, A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, Eds., vol. 12374. Springer, pp. 259 277.\n\n