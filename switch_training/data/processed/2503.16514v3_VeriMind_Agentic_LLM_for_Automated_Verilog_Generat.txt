=== ORIGINAL PDF: 2503.16514v3_VeriMind_Agentic_LLM_for_Automated_Verilog_Generat.pdf ===\n\nRaw text length: 43394 characters\nCleaned text length: 42298 characters\nNumber of segments: 31\n\n=== CLEANED TEXT ===\n\nVeriMind: Agentic LLM for Automated Verilog Generation with a Novel Evaluation Metric 1st Bardia Nadimi Dept. of Computer Science Eng. University of South Florida Tampa, Florida, United States 2nd Ghali Omar Boutaib Dept. of Computer Science Eng. University of South Florida Tampa, Florida, United States 3rd Hao Zheng Dept. of Computer Science Eng. University of South Florida Tampa, Florida, United States Abstract Designing Verilog modules requires meticulous at- tention to correctness, efficiency, and adherence to design spec- ifications. However, manually writing Verilog code remains a complex and time-consuming task that demands both expert knowledge and iterative refinement. Leveraging recent advance- ments in large language models (LLMs) and their structured text generation capabilities, we propose VeriMind, an agentic LLM framework for Verilog code generation that significantly automates and optimizes the synthesis process. Unlike traditional LLM-based code generators, VeriMind employs a structured reasoning approach: given a user-provided prompt describing design requirements, the system first formulates a detailed train of thought before the final Verilog code is generated. This multi-step methodology enhances interpretability, accuracy, and adaptability in hardware design. In addition, we introduce a novel evaluation combines the conventional measure with Average Refinement Cycles (ARC) to capture both success rate and the efficiency of iterative refinement. Experimental results on diverse hardware design tasks demonstrated that our approach achieved up to 8.3 improvement on metric and 8.1 on metric. These findings underscore the transformative potential of agentic LLMs in automated hardware design, RTL development, and digital system synthesis. Index Terms large language models, agentic AI, code gener- ation, transformers. I. INTRODUCTION With the introduction of attention-based models and trans- formers [1], the field of natural language processing (NLP) underwent a significant transformation. In recent years, trans- formers have become a fundamental component in the archi- tecture of most AI models. Various models, such as Generative Pre-trained Transformers (GPT) [2], Bidirectional Encoder Representations from Transformers (BERT) [3], Language Model for Dialogue Applications (LaMDA) [4], and Vision Transformers (ViT) [5], have utilized transformer architectures to push the boundaries of performance across natural language processing and computer vision tasks. The success of transformers stems from their ability to effectively model long-range dependencies, enabling them to capture contextual relationships in textual, visual, and multimodal data with unprecedented accuracy. This has led to significant advancements in diverse applications, includ- ing text generation, machine translation, code synthesis, and image understanding. Despite these achievements, applying transformers to specialized domains such as hardware design and Verilog code generation remains relatively unexplored. The complexity of hardware description languages (HDLs), the need for logical correctness, and the structured nature of Verilog pose unique challenges that traditional AI-driven code generation techniques struggle to address. The utilization of large language models (LLMs) for hardware code generation is driven by the goal of developing an advanced tool that simplifies the hardware modeling process for designers [6] [9]. Furthermore, automating hardware code generation using LLMs plays a crucial role in minimizing human error. Due to the inherently complex and highly technical nature of hardware design, manual coding is susceptible to inaccuracies and inconsistencies [10]. Integrating these models into hardware development workflows reduces the inherent complexities of traditional design methods. Leveraging LLM capabilities enables a more intuitive and efficient approach to hardware modeling, thereby enhancing automation and design optimiza- tion. In addition, ensuring the security of LLM-generated code is paramount [11], [12]; rigorous code validation and adversarial testing must be implemented to safeguard against vulnerabilities and unauthorized modifications. As AI-driven automation continues to expand, leveraging agentic AI for Verilog code generation presents a promising opportunity to enhance productivity, reduce human effort, and improve design efficiency in hardware development. In this work, we introduce VeriMind, an agentic AI approach for Verilog code generation that leverages the reasoning and iterative refinement capabilities of LLMs. Unlike conventional AI-driven code generation, which passively produces code based on a single prompt, our method employs a structured multi-step process to enhance the accuracy, interpretability, and adaptability of the generated hardware descriptions. By integrating cognitive reasoning and planning, our framework systematically decomposes the Verilog generation task, en- abling the AI to outline a logical design process before synthesizing the final code. Furthermore, to facilitate a more comprehensive evaluation of approaches that employ refine- ment loops, we introduce a novel metric termed (Average Refinement Cycles). This metric integrates the tra- ditional pass rate with the number of refinement iterations required to generate a correct code. By penalizing models that arXiv:2503.16514v3 [cs.AR] 16 Apr 2025 Supervisor Agent LLM Checker Agent Prompt Eng. Agent Verilog Agent Testbench Agent Input Prompt Final Output Generate a Verilog testbench code considering the following: 1... 2... Refine the input prompt and make sure it has the following: 1... 2... Generate a Verilog code considering the following: 1... 2... Fig. 1. Proposed Overall Architecture. require more cycles, provides a more nuanced assessment of both accuracy and efficiency, making it an invaluable tool for comparing and optimizing iterative code generation architectures. Our method adopts an interactive, self-improving workflow. Upon receiving a design specification from the user, the proposed architecture iteratively refines the generated Ver- ilog code ensuring both syntactic and functional correct- ness with dedicated agents managing specific subtasks. A detailed description of the architecture and its individual agents is provided in Section III. To evaluate the effectiveness of our method, we apply it to a diverse set of Verilog design tasks, ranging from simple combinational circuits to complex sequential logic. Our results demonstrate that agentic AI significantly improves the quality, correctness, and adaptability of generated Verilog code com- pared to traditional LLM-based code generation. This work highlights the potential of integrating structured reasoning and interactive validation into AI-driven hardware development, paving the way for more autonomous and reliable HDL design automation. The overall architecture of the proposed approach is illustrated in Fig.1. The key contributions of this paper are summarized as follows: Agentic AI Framework for Verilog Code Generation: Introduces a structured, reasoning-driven approach that enhances interpretability, correctness, and adaptability in hardware description language (HDL) generation. Interactive and Iterative Refinement Process: Incorporates intermediate validation steps where the AI outlines its reasoning before generating Verilog code, allowing user feedback to guide improvements. Enhanced Code Quality and Design Consistency: Evalu- ates the proposed approach across diverse Verilog design tasks, demonstrating improved syntactic correctness, log- ical coherence, and alignment with design specifications compared to conventional LLM-based generation. We introduce a novel evaluation metric, specifically designed to provide a fair and comprehensive assessment of architectures that employ iterative refine- ment loops. The remainder of this paper is structured as follows: Section II presents an overview of related work and background information. Section III details the proposed architecture. Sec- tion IV discusses the evaluation results and analysis. Finally, Section V outlines the conclusions and potential directions for future research. II. RELATED WORKS AND BACKGROUNDS High-quality Verilog datasets are crucial yet challenging for effective hardware code generation with LLMs. Nadimi et al. [13] addressed this by developing PyraNet, an open-source, hierarchical dataset that enhances the syntactic and functional accuracy of generated Verilog code. By using quality-based tiers and curriculum learning, models fine-tuned with PyraNet improved performance by up to 32.6 over CodeLlama-7B and 16.7 over state-of-the-art models on VerilogEval. Advancements in Verilog dataset augmentation and LLM fine-tuning include approaches such as CodeV [14] and MG- Verilog [15]. CodeV employs multi-level summarization on a dataset of 165,000 Verilog modules enriched with GPT-3.5 annotations, achieving state-of-the-art results on VerilogEval and RTLLM surpassing GPT-4 by 22.1 on VerilogEval. Similarly, MG-Verilog provides a multi-granular dataset of over 11,000 samples, enhancing LLM generalization and im- proving code synthesis accuracy. Beyond dataset-driven improvements, several studies have optimized Verilog generation through fine-tuning, retrieval mechanisms, and structured prompting. BetterV [16] uses instruct-tuning with generative discriminators and Verilog-C program alignment to enhance key EDA processes. Mean- while, AutoVCoder [17] leverages retrieval-augmented gen- eration with a two-phase fine-tuning strategy to better align general-purpose LLMs with RTL constraints, achieving a 3.4 improvement in functional correctness on the RTLLM bench- mark. This demonstrates the significant impact of retrieval- enhanced architectures on RTL automation. Another important line of research focuses on self-reflective learning and compiler-based error correction. OriGen [18] introduces a self-correcting framework that combines code- to-code augmentation with compiler feedback loops for error detection and refinement. OriGen trains two specialized Low- Rank Adaptation (LoRA) models one for initial Verilog generation and another for syntax correction resulting in a 12.8 performance boost over existing open-source models and a 19.9 improvement in syntactic accuracy compared to GPT-4 Turbo. Incorporating iterative self-reflection, OriGen greatly improves the reliability of LLM-generated Verilog. To tackle challenges related to hierarchical Verilog gen- eration, Tang et al. [19] proposed HiVeGen, a structured framework that decomposes HDL synthesis into hierarchical submodules. Traditional LLMs often generate monolithic, hallucination-prone HDL blocks, particularly for complex designs like domain-specific accelerators. HiVeGen mitigates this issue by introducing automated design space exploration, structured code reuse, and human-computer interaction mech- anisms to reduce debugging overhead. Empirical evaluations show that HiVeGen enhances the quality and scalability of AI- generated HDL, making it a promising approach for modular chip design automation. Structured prompting techniques have been explored to enhance Verilog synthesis. ROME [20] employs hierarchical prompting to decompose large hardware modules into orga- nized submodules and integrates an eight-stage automation pipeline that combines human-guided and autonomous refine- ment. Evaluations show that fine-tuned LLMs using ROME outperform traditional flat prompting in generating complex HDL, reducing both development time and computational overhead. Notably, ROME produced the first LLM-designed processor without human intervention, underscoring its revo- lutionary potential in AI-assisted chip design. Multi-agent frameworks have proven beneficial for RTL design by enhancing efficiency, interpretability, and perfor- mance. RTLSquad [21] introduces a collaborative system that divides the RTL workflow into three phases exploration, implementation, and verification each managed by special- ized agent squads to ensure seamless communication and transparent decision-making. Experimental results demonstrate that RTLSquad not only produces functionally correct RTL code but also improves power, performance, and area metrics, underscoring its value in scalable AI-assisted hardware devel- opment. III. METHODOLOGY This section begins with an overview of the complete agentic architecture, providing a high-level understanding of its structure and functionality. Subsequently, each agent is examined in detail, outlining its specific role and operational mechanisms within the framework. A. Agentic Framework: Structure and Workflow Our proposed architecture as shown in Fig. 1 comprises five interconnected agents Supervisor, Prompt Engineer, Verilog Code Generator, Verilog Testbench Generator, and Checker, that collaborate to generate verified Verilog designs. The Supervisor Agent orchestrates the workflow by initially send- ing the user s prompt to the Prompt Engineer Agent for refinement. The refined prompt is then dispatched to both the Verilog Code Generator and Testbench Generator Agents to produce the corresponding outputs, which are subsequently validated by the Checker Agent through compilation and simulation. Should errors arise, the Checker Agent triggers a re-evaluation cycle via the Supervisor Agent until the final, error-free Verilog code is produced. This iterative approach ensures high accuracy, functionality, and reliability in the generated designs, with further details on each agent provided in the following sections. B. Supervisor Agent The Supervisor Agent is the central coordinator in our multi- agent architecture for Verilog code generation, orchestrating the entire process from prompt refinement to final verification. This agent employs a lightweight LLM solely to manage inter-agent communications, including selecting appropriate agents at various stages and modifying prompts as necessary. In our experiments, we employed the gpt-4o-mini model for this agent. The process begins by receiving the initial design specification from the user and immediately dispatches this prompt to the Prompt Engineer Agent. This agent refines and optimizes the prompt, ensuring that it is well-structured for subsequent processing. Once the refined prompt is obtained, the Supervisor Agent simultaneously sends it to both the Verilog Code Generator Agent and the Verilog Testbench Generator Agent, enabling parallel generation of the Verilog module and its corresponding testbench. After these agents complete their tasks, the generated out- puts are forwarded to the Checker Agent, which validates the code by performing compilation and simulation tests. If the Checker Agent detects any errors such as compilation failures, simulation mismatches, or logical inconsistencies it promptly notifies the Supervisor Agent. The Supervisor then evaluates the reported issues and decides whether to re-send the prompt for further refinement, request corrections from the Verilog Code Generator, or ask the Testbench Generator to adjust its output. This iterative loop continues until the Checker Agent confirms that the generated Verilog design is both syntactically correct and functionally robust. Moreover, the Supervisor Agent employs a structured mes- sage passing mechanism that ensures effective inter-agent communication and maintains a detailed state tracking system. This system monitors each agent s progress and allows the Supervisor to dynamically adjust execution priorities in the event of delays or unexpected outcomes, thereby optimizing overall workflow efficiency. Overall, the Supervisor Agent plays a critical role by seam- lessly integrating specialized tasks refinement, code gener- ation, testbench generation, and validation into a cohesive and adaptive process. This structured and iterative approach not only enhances the quality and accuracy of the generated Verilog modules but also significantly reduces manual inter- vention and error rates in hardware design. C. Prompt Engineer Agent The Prompt Engineer Agent refines the input prompt to ensure that the generated Verilog code is structurally sound, functionally relevant, and aligned with the user s intent. By modifying and reformatting the input, it increases the likelihood of producing high-quality code that meets design specifications. Like the Supervisor Agent, this agent uses a compact LLM in our evaluation, we employed the gpt-4o- mini model for this role. Upon receiving a raw prompt from the Supervisor Agent, the Prompt Engineer Agent enhances clarity, structure, and specificity. This process involves rewriting ambiguous specifi- cations, expanding under-specified prompts with necessary de- sign constraints (e.g., clocking details, functional properties), reformatting inputs into structured templates, and removing redundant information. For example, a vague request such as Generate a Verilog module for a 4-bit counter may be transformed into Generate a Verilog module for a 4-bit syn- chronous up-counter with a clock input (clk), an asynchronous reset (rst), and an enable signal (en), where the output (count) is a 4-bit register. Additionally, the agent tailors its refinements to the task at hand whether it involves combinational logic, sequential circuits, or finite state machines by ensuring that the prompt includes all necessary details (e.g., Boolean expressions for combinational circuits or explicit state transitions for FSMs). If validation errors occur later, the Checker Agent informs the Supervisor Agent, prompting further modifications until an effective prompt is achieved. D. Verilog Code Generator Agent The Verilog Code Generator Agent uses a refined prompt from the Supervisor Agent and a specialized LLM for HDL generation to produce Verilog modules that are both struc- turally sound and functionally accurate. It is essential to the multi-agent framework, ensuring adherence to digital design best practices while maintaining efficiency. This agent employs a fine-tuned LLM to manage the sophisticated task of Verilog code generation. For comparative analysis, we evaluated our architecture using a range of LLMs in this role, including both fine-tuned models and commercial alternatives. Once the Prompt Engineer Agent refines the input request, the Supervisor Agent dispatches the structured prompt to the Verilog Code Generator Agent, initiating the code synthesis process. The Verilog Code Generator Agent is designed to handle a diverse range of digital designs, ensuring adaptability across different hardware modules: Combinational Circuits: Generates Verilog for logic gates, multiplexers, arithmetic operations, and other state- less logic. Sequential Circuits: Implements flip-flops, registers, counters, and clocked state transitions. Finite State Machines (FSMs): Constructs Mealy and Moore state machines, including state encoding and tran- sition logic. Parameterized Modules: Creates reusable Verilog com- ponents with adjustable parameters (generate statements, parameter values). For example, when generating an FSM for a traffic light controller, the agent ensures that state transitions are clearly defined and that the design follows a systematic approach; a representative sample is provided in Appendix A. E. Testbench Generator Agent The Verilog Testbench Generator Agent is responsible for creating a corresponding testbench to verify the functional- ity of the generated Verilog module. By simulating various input conditions and monitoring the expected outputs, this agent ensures that the generated hardware design behaves as intended. Mirroring the approach used by the Verilog Code Generator Agent, this agent utilizes a fine-tuned model for testbench generation. For comparative analysis, we evaluated our architecture with a variety of LLMs in this role, including both fine-tuned models and commercial alternatives. Upon receiving the refined prompt, the Supervisor Agent instructs the Verilog Testbench Generator Agent to create a structured testbench. This process involves: 1) defining stimuli and input signals to drive the module under test (MUT), 2) in- stantiating the Verilog module within the testbench framework, 3) creating a simulation sequence, including clock generation, reset initialization, and test scenarios, and 4) using assertions or monitor statements to validate expected outputs. For instance, the testbench generated for the traffic light controller described in Section III-D is presented in Appendix B. If the Checker Agent detects issues during compilation or simulation, the Supervisor Agent instructs the Verilog Test- bench Generator Agent to modify the testbench accordingly. This iterative process ensures that: 1) the testbench correctly stimulates edge cases and corner conditions, 2) the verification covers all functional aspects of the Verilog module, and 3) the generated testbench is efficient, concise, and reusable for validation purposes. F. Checker Agent The Checker Agent ensures the accuracy of the generated Verilog module and its corresponding testbench by conducting compilation and simulation tests. Serving as a verification layer, it guarantees that the output complies with Verilog syntax rules and meets functional expectations. This agent leverages compilation and simulation tools, including Icarus Verilog [22] and Verilator [23], to systematically validate the correctness of the generated HDL code. Upon receiving the Verilog module and testbench, the Checker Agent conducts a structured validation process to ensure the correct functionality of the design. It first compiles the Verilog module and testbench, checking for syntax er- rors, missing dependencies, or structural inconsistencies. Once compilation is successful, the agent performs a simulation to verify functional correctness, ensuring that the module operates as expected across different test scenarios. During the simulation, it monitors signal behavior and assertions using display, or assertion-based verification methods to track discrepancies in output. If the Checker Agent encounters issues such as compi- lation errors, logical inconsistencies, or incorrect simulation results it notifies the Supervisor Agent with such informa- tion, prompting a refinement loop where the Verilog Code Generator Agent or Verilog Testbench Generator Agent mod- ifies the output to resolve the detected errors. This iterative TABLE I EXPERIMENTAL RESULTS USING VERILOGEVAL [24] Model VerilogEval-Machine VerilogEval-Human GPT-4o-mini 60.1 62.2 65 44.2 49.4 56.4 Claude3.5-Sonnet [25] 74.8 76.9 79.7 56.4 60.9 69.9 CodeLlama-70B-Instruct [26] 54.5 58 63.6 40.4 43.6 48.7 DeepSeek-Coder-V2-Instruct [27] 63.6 73.4 78.3 48.1 52.6 57.1 PyraNet-DeepSeek [13] 77.6 84.6 89.5 58.3 62.8 67.9 RTLCoder-DeepSeek [28] 61.2 76.5 81.8 41.6 50.1 53.4 Origen-DeepSeek [18] 74.1 82.5 85.3 54.5 60.3 64.1 AutoVCoder-CodeQwen [17] 68.5 79.7 79.7 48.7 55.8 55.8 CodeV-DeepSeek [14] 77.9 88.6 90.7 52.7 62.5 67.3 VeriMind-GPT-4o-mini 60.1 68.5 72.6 44.2 54.5 62.8 VeriMind-Claude3.5-Sonnet 74.8 83.2 85.3 56.4 63.5 71.8 VeriMind-CodeLlama-70b-Instruct 54.5 65 70.5 40.4 48.7 53.8 VeriMind-Deep-Seek-Coder-V2-Instruct 63.6 76.2 81.8 48.1 58.3 65.4 VeriMind-PyraNet-DeepSeek 77.6 90.9 96.5 58.3 67.9 74.4 validation process ensures that the final Verilog module is both syntactically correct and functionally reliable before being delivered to the user. IV. EVALUATION AND DISCUSSION To evaluate our agentic AI framework for Verilog code generation, we used the VerilogEval benchmark suite [24]. Recognizing that refinement loops require a nuanced evalu- ation, we introduced the metric which combines with Average Refinement Cycles (ARC) (details in subsequent subsections). These benchmarks standardize the assessment of syntactic correctness, functional accuracy, and iterative efficiency. A. Experimental Setup Benchmarks: VerilogEval benchmark consists of two parts: 1) VerilogEval Machine and 2) VerilogEval Human with 143 and 156 problems respectively. They all sourced from the HDLbits platform1, covering a wide range of Verilog codes from combinational circuits to complex sequential circuits. In addition, for the metric, we extend the VerilogEval platform by incorporating the average number of refinement cycles into the evaluation framework. Experimental setup: For the comparison purposes, we im- plemented the proposed design using multiple baseline LLMs including GPT-4o-mini, Claude3.5-Sonnet, CodeLlama-70b- Instruct, DeepSeek-Coder-V2-Instruct and PyraNet. We com- pared the results with other SOTA works including PyraNet [13], Origen [18], AutoVCoder [17], CodeV [14], and also with the commercial LLMs without any agents. All the exper- imental results can be found in Tables I and II. It is important to note that in all implemented designs, the Supervisor and Prompt Engineer agents consistently employ the gpt-4o-mini model, while the Verilog and Testbench Generator agents utilize different LLMs. B. Shortcoming and Solution One major challenge was selecting an appropriate evaluation metric. Traditional approaches typically rely on the metric. However, applying to architectures that em- ploy an iterative refinement loop can be misleading, since a single task may require multiple attempts before reaching a correct output. More specifically, reporting for a system that iteratively corrects errors would be inaccurate 1 sets TABLE II EXPERIMENTAL RESULTS USING METRIC Model Verilog-Machine Verilog-Human pass rate ARC pass rate ARC GPT-4o-mini 65 4.48 57.6 56.4 5.76 45 Claude3.5-Sonnet [25] 79.7 3.16 76.1 69.9 4.7 60.9 CodeLlama-70B-Instruct [26] 63.6 4.91 54.6 48.7 6.21 37.2 DeepSeek-Coder-V2-Instruct [27] 78.3 3.78 72.5 57.1 5.45 46.8 PyraNet-DeepSeek [13] 89.5 2.66 87.1 67.9 4.53 60 RTLCoder-DeepSeek [28] 81.8 3.73 75.9 53.4 5.83 42.1 Origen-DeepSeek [18] 85.3 2.91 82.3 64.1 4.81 55.5 AutoVCoder-CodeQwen [17] 79.7 3.27 75.7 55.8 5.26 46.5 CodeV-DeepSeek [14] 90.7 2.45 89 67.3 4.76 58.4 VeriMind-GPT-4o-mini 72.6 4.17 65.8 62.8 5.51 51.3 VeriMind-Claude3.5-Sonnet 85.3 2.85 82.5 71.8 4.57 63.2 VeriMind-CodeLlama-70b-Instruct 70.5 4.57 62.2 53.8 5.95 42.1 VeriMind-Deep-Seek-Coder-V2-Instruct 81.8 3.64 76.3 65.4 5.16 55 VeriMind-PyraNet-DeepSeek 96.5 2.35 94.8 74.4 4.27 66.8 if some tasks take more than one try. In our framework, which inherently incorporates a feedback loop to verify and fix generated code, using alone would be deceptive it could potentially report a 100 rate if the loop functions perfectly. For more accurate evaluation and comparison, we adopted a dual approach. First, for a fair comparison, we excluded samples that required more than k refinement iterations when computing for k 1, 5, 10. This ensures that the metric is not artificially inflated by the iterative process. Sec- ond, we introduced a new metric, Average Refinement Cycles (ARC), which measures the average number of iterations the system takes to produce correct Verilog code; lower ARC values indicate a more efficient process. This combination provides a more accurate and comprehensive evaluation of the performance of our framework. Equation 1 defines our newly proposed metric, which integrates the traditional measure with the ARC to capture both the success rate and the iterative refinement efficiency. PassRate e 0.01 (ARC 1)2 (1) In this formula, PassRate represents the model s success rate and ARC quantifies the average number of iterations required to produce correct output. The exponential term, e 0.01 (ARC 1)2 serves as a penalty factor that equals 1 when ARC is 1 indicating a perfect one-shot performance and decays as ARC increases. This decay penalizes models that require additional refinement iterations, with the factor 0.01 controlling the severity of the penalty. By combining pass rate with ARC, the metric effectively balances accuracy with refinement efficiency, providing a comprehensive assessment of the model s performance. This combined metric widens the evaluative scope, allowing for a more nuanced comparison across different architectures. It is important to note that the constant in the equation was chosen so that when ARC exceeds 10, the resulting metric value becomes negligibly small effectively treating samples requiring more than 10 iterations as incomplete. If evaluation is needed for models that take more than 10 iterations to complete a task, this limitation can be overcome by adjusting the constant in Equation 1. Fig. 2 presents a 3D surface plot of the proposed metric across its full range. The figure clearly demonstrates that when ARC equals 1, the metric is equivalent to while higher ARC values lead to a gradual decrease in (as shown on the z-axis). 0 20 40 60 80 100 5 10 0 50 100 Pass Rate ( ) ARC ( ) Fig. 2. Surface plot of the metric: z x e 0.01 (y 1)2, where x is the Pass Rate and y is ARC. C. Results Explained Our experiments on the VerilogEval-Machine and VerilogEval-Human benchmarks reveal that baseline models (e.g., GPT-4o-mini, Claude3.5-Sonnet, CodeLlama-70B- Instruct, DeepSeek-Coder-V2-Instruct) achieve moderate performance, with improvements at higher levels. In contrast, our VeriMind models significantly boost and metrics. For example, while GPT-4o- mini attains a 60.1 on the Machine benchmark, VeriMind-GPT-4o-mini increases and to 68.5 and 72.6 , respectively. Similarly, VeriMind-PyraNet- DeepSeek records of 77.6 , of 90.9 , and of 96.5 . These results demonstrate that our feedback loop and iterative refinement process substantially enhance code generation quality without compromising initial performance. Overall, improvements in higher-order metrics indicate that our approach increases the likelihood of generating correct Verilog code with fewer refinement cycles, as further confirmed by our metric. Following the presentation of individual and scores, we further evaluate our framework us- ing the metric, which combines the initial pass rate with Average Refinement Cycles through an exponential penalty that increases with additional iterations. This design ensures that a model with high and minimal refine- ment (ARC 1) incurs little penalty, whereas additional cycles lower the score. Our experiments reveal that, while baseline models show moderate perfor- mance, the VeriMind variants consistently achieve higher values; for instance, VeriMind-PyraNet-DeepSeek records a of 96.5 and, with an ARC of 2.35, a of 94.8 , significantly outperforming the baseline PyraNet-DeepSeek (87.1 ). Similar trends are observed with the VeriMind versions of GPT-4o-mini, Claude3.5-Sonnet, CodeLlama-70B-Instruct, and DeepSeek-Coder-V2-Instruct, highlighting that our feedback loop reduces the necessary refinement iterations and provides a more comprehensive per- formance metric. Fig. 3 illustrates a unified 3D visualization of the pass rate, ARC, and metrics. 60 80 100 2 2.5 3 3.5 4 4.5 5 0 50 100 Pass Rate ( ) ARC ( ) VerilogEval-Machine VeriMind-PyraNet-DeepSeek PyraNet-DeepSeek VeriMind-DeepSeek-Coder-V2-Instruct DeepSeek-Coder-V2-Instruct VeriMind-CodeLlama-70b-Instruct CodeLlama-70B-Instruct VeriMind-Claude3.5-Sonnet Claude3.5-Sonnet VeriMind-GPT-4o-mini GPT-4o-mini RTLCoder-DeepSeek Origen-DeepSeek AutoVCoder-CodeQwen CodeV-DeepSeek 50 60 70 4.5 5 5.5 6 0 50 Pass Rate ( ) ARC ( ) VerilogEval-Human Fig. 3. Bar chart showing Pass Rate (x-axis), ARC (y-axis), and (z-axis) comparing the results of VeriMind and other SOTA methods. V. CONCLUSION AND FUTURE WORKS In this paper, we introduce VeriMind, an agentic AI frame- work for Verilog code generation that employs specialized agents Supervisor, Prompt Engineer, Verilog Code Gener- ator, Testbench Generator, and Checker in an iterative re- finement process. By decomposing the task into manageable subtasks and leveraging a feedback loop, VeriMind enhances both the syntactic and functional correctness of generated Ver- ilog code while reducing refinement cycles. We also propose the metric, which combines with Average Refinement Cycles to offer a more comprehensive evalua- tion of initial accuracy and iterative efficiency. Evaluations on VerilogEval-Machine and VerilogEval-Human benchmarks show improvements of up to 8.3 on and 8.1 on underscoring the effectiveness of our approach in advancing hardware design automation. Future work will focus on refining the iterative process, improve security, and extend the framework to support additional hardware description languages and complex designs. REFERENCES [1] Vaswani et al. Attention is all you need. 2017. [2] Radford et al. Improving language understanding by generative pre- training. 2018. [3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language un- derstanding. arXiv preprint arXiv:1810.04805, 2018. [4] Cohen et al. Lamda: Language models for dialog applications. In arXiv. 2022. [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis- senborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. [6] Bardia Nadimi and Hao Zheng. A multi-expert large language model architecture for verilog code generation. In IEEE LLM Aided Design Workshop (LAD), pages 1 5, June 2024. [7] Md Rubel Ahmed, Bardia Nadimi, and Hao Zheng. System-on-chip message flow mining with masked-language models. In 2023 IEEE 66th International Midwest Symposium on Circuits and Systems (MWSCAS), pages 496 500, 2023. [8] Deepak Vungarala, Sakila Alam, Arnob Ghosh, and Shaahin Angizi. Spicepilot: Navigating spice code generation and simulation with ai guidance, 2024. [9] Deepak Vungarala, Mahmoud Nazzal, Mehrdad Morsali, Chao Zhang, Arnob Ghosh, Abdallah Khreishah, and Shaahin Angizi. Sa-ds: A dataset for large language model-driven ai accelerator design generation, 2024. [10] Dessouky et al. HardFails: Insights into Software-Exploitable hardware bugs. In 28th USENIX Security Symposium (USENIX Security 19), pages 213 230, Santa Clara, CA, August 2019. USENIX Association. [11] Seyedsina Nabavirazavi, Samira Zad, and Sundararaja Sitharama Iyen- gar. Evaluating the universality of do anything now jailbreak prompts on large language models: Content warning: This paper contains un- filtered and harmful examples. In 2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC), pages 00691 00696, 2025. [12] Vishal Rathod, Seyedsina Nabavirazavi, Samira Zad, and Sun- dararaja Sitharama Iyengar. Privacy and security challenges in large language models. In 2025 IEEE 15th Annual Computing and Commu- nication Workshop and Conference (CCWC), pages 00746 00752, 2025. [13] Bardia Nadimi, Ghali Omar Boutaib, and Hao Zheng. Pyranet: A multi- layered hierarchical dataset for verilog, 2025. [14] Yang Zhao et al. Codev: Empowering llms for verilog generation through multi-level summarization. ArXiv, abs 2407.10424, 2024. [15] Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, and Yingyan Ce- line Lin. Mg-verilog: Multi-grained dataset towards enhanced llm- assisted verilog generation, 2024. [16] Zehua Pei, Huiling Zhen, Mingxuan Yuan, Yu Huang, and Bei Yu. BetterV: Controlled verilog generation with discriminative guidance. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 40145 40153. PMLR, 21 27 Jul 2024. [17] Mingzhe Gao et al. Autovcoder: A systematic framework for automated verilog code generation using llms, 2024. [18] Fan Cui et al. Origen:enhancing rtl code generation with code-to-code augmentation and self-reflection, 2024. [19] Jinwei Tang, Jiayin Qin, Kiran Thorat, Chen Zhu-Tian, Yu Cao, Yang, Zhao, and Caiwen Ding. Hivegen hierarchical llm-based verilog generation for scalable chip design, 2024. [20] Andre Nakkab, Sai Qian Zhang, Ramesh Karri, and Siddharth Garg. Rome was not built in a single step: Hierarchical prompting for llm- based chip design, 2024. [21] Bowei Wang, Qi Xiong, Zeqing Xiang, Lei Wang, and Renzhi Chen. Rtlsquad: Multi-agent based interpretable rtl design, 2025. [22] Stephen Williams. Icarus verilog. [23] Wilson Snyder Wilson and contributors. Verilator: Fast and free verilog hdl simulator, 2024. Accessed: February 25, 2025. [24] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. Verilogeval: Evaluating large language models for verilog code genera- tion, 2023. [25] Anthropic. Claude 3.5 sonnet model card addendum, 2024. Accessed: 03-2025. [26] Baptiste Rozi ere et al. Code llama: Open foundation models for code, 2024. [27] Qihao et al. Zhu. Deepseek-coder-v2: Breaking the barrier of closed- source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. [28] Liu et al. Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, 2024. Appendix A: module traffic_light_controller( input wire clk, Clock input input wire reset, Active high reset output reg [2:0] north_south_lights, For N-S dir output reg [2:0] east_west_lights For E-W dir ); Light encoding: parameter RED 3'b100; parameter YELLOW 3'b010; parameter GREEN 3'b001; State encoding parameter S0 2'b00; NS: Green, EW: Red parameter S1 2'b01; NS: Yellow, EW: Red parameter S2 2'b10; NS: Red, EW: Green parameter S3 2'b11; NS: Red, EW: Yellow Timing parameters (adjust according to your clock frequency) 50M cycles 1 second 50MHz parameter GREEN_TIME 32'd50000000; 15M cycles 0.3 seconds 50MHz parameter YELLOW_TIME 32'd15000000; reg [1:0] state, next_state; reg [31:0] timer; State register and timer always (posedge clk or posedge reset) begin if (reset) begin state S0; timer 32'd0; end else begin if (timer 32'd0) begin state next_state; Load appropriate timer value based on next state case (next_state) S0, S2: timer GREEN_TIME; S1, S3: timer YELLOW_TIME; default: timer GREEN_TIME; endcase end else begin timer timer - 32'd1; end end end Next state logic always ( ) begin case (state) S0: next_state (timer 32'd0) ? S1 : S0; S1: next_state (timer 32'd0) ? S2 : S1; S2: next_state (timer 32'd0) ? S3 : S2; S3: next_state (timer 32'd0) ? S0 : S3; default: next_state S0; endcase end Output logic always ( ) begin case (state) S0: begin north_south_lights GREEN; east_west_lights RED; end S1: begin north_south_lights YELLOW; east_west_lights RED; end S2: begin north_south_lights RED; east_west_lights GREEN; end S3: begin north_south_lights RED; east_west_lights YELLOW; end default: begin north_south_lights RED; east_west_lights RED; end endcase end endmodule Appendix B: timescale 1ns 1ps module traffic_light_controller_tb; Parameters for simulation parameter CLK_PERIOD 20; 50MHz clock (20ns period) parameter SIM_TIME 5000000; Simulation time in ns Signal declarations reg clk; reg reset; wire [2:0] north_south_lights; wire [2:0] east_west_lights; Instantiate the module under test (MUT) traffic_light_controller UUT ( .clk(clk), .reset(reset), .north_south_lights(north_south_lights), .east_west_lights(east_west_lights) ); Clock generation initial begin clk 0; forever (CLK_PERIOD 2) clk clk; end Light state definitions for easier monitoring wire ns_red, ns_yellow, ns_green, ew_red, ew_yellow, ew_green; assign ns_red (north_south_lights 3'b100); assign ns_yellow (north_south_lights 3'b010); assign ns_green (north_south_lights 3'b001); assign ew_red (east_west_lights 3'b100); assign ew_yellow (east_west_lights 3'b010); assign ew_green (east_west_lights 3'b001); Monitor changes in light states initial begin monitor("Time: 0t, NS: s, EW: s", time, ns_red ? "RED " : ns_yellow ? "YELLOW" : "GREEN ", ew_red ? "RED " : ew_yellow ? "YELLOW" : "GREEN "); end State tracking variables reg [31:0] state_start_time; reg [31:0] state_duration; reg [2:0] prev_ns_lights; reg [2:0] prev_ew_lights; Stimulus and test sequence initial begin Initialize signals reset 1; state_start_time 0; prev_ns_lights 3'b000; prev_ew_lights 3'b000; Apply reset 100 reset 0; Run simulation for specified time SIM_TIME; Report test results display("Test completed at time 0t", time); finish; end State transition checks always (posedge clk) begin Detect state changes if (north_south_lights ! prev_ns_lights east_west_lights ! prev_ew_lights) begin state_duration time - state_start_time; Report state duration if (state_start_time 0) begin display("State duration: 0t ns", state_duration); Check timing (adjusted for clock cycles) if (prev_ns_lights 3'b001 prev_ew_lights 3'b001) begin Green state should last approximately 1 second (50M cycles 20ns 1000000ns) if (state_duration 990000 state_duration 1010000) begin display("WARNING:"); display("Green state duration ( 0t ns) outside expected range", state_duration); end end if (prev_ns_lights 3'b010 prev_ew_lights 3'b010) begin Yellow state should last approximately 0.3 seconds (15M cycles 20ns 300000ns) if (state_duration 290000 state_duration 310000) begin display("WARNING:"); display("Yellow state duration ( 0t ns) outside expected range", state_duration); end end end Update state tracking state_start_time time; prev_ns_lights north_south_lights; prev_ew_lights east_west_lights; end end Check for invalid states always (posedge clk) begin Check for invalid light combinations if (north_south_lights 3'b000 east_west_lights 3'b000) begin display("ERROR: Invalid light state detected at time 0t", time); display("NS lights: b, EW lights: b", north_south_lights, east_west_lights); end Check for conflicting green signals if (ns_green ew_green) begin display("ERROR: Both directions have green light at time 0t!", time); end Check for conflicting yellow signals if (ns_yellow ew_yellow) begin display("ERROR: Both directions have yellow light at time 0t!", time); end Check that when one direction is green or yellow, the other is red if ((ns_green ns_yellow) !ew_red) begin display("ERROR: East-West should be RED when North-South is GREEN or YELLOW at time 0t", time); end if ((ew_green ew_yellow) !ns_red) begin display("ERROR: North-South should be RED when East-West is GREEN or YELLOW at time 0t", time); end end endmodule\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nVeriMind: Agentic LLM for Automated Verilog Generation with a Novel Evaluation Metric 1st Bardia Nadimi Dept. of Computer Science Eng. University of South Florida Tampa, Florida, United States 2nd Ghali Omar Boutaib Dept. of Computer Science Eng. University of South Florida Tampa, Florida, United States 3rd Hao Zheng Dept. of Computer Science Eng. University of South Florida Tampa, Florida, United States Abstract Designing Verilog modules requires meticulous at- tention to correctness, efficiency, and adherence to design spec- ifications. However, manually writing Verilog code remains a complex and time-consuming task that demands both expert knowledge and iterative refinement. Leveraging recent advance- ments in large language models (LLMs) and their structured text generation capabilities, we propose VeriMind, an agentic LLM framework for Verilog code generation that significantly automates and optimizes the synthesis process. Unlike traditional LLM-based code generators, VeriMind employs a structured reasoning approach: given a user-provided prompt describing design requirements, the system first formulates a detailed train of thought before the final Verilog code is generated. This multi-step methodology enhances interpretability, accuracy, and adaptability in hardware design. In addition, we introduce a novel evaluation combines the conventional measure with Average Refinement Cycles (ARC) to capture both success rate and the efficiency of iterative refinement. Experimental results on diverse hardware design tasks demonstrated that our approach achieved up to 8.3 improvement on metric and 8.1 on metric. These findings underscore the transformative potential of agentic LLMs in automated hardware design, RTL development, and digital system synthesis. Index Terms large language models, agentic AI, code gener- ation, transformers. I. INTRODUCTION With the introduction of attention-based models and trans- formers [1], the field of natural language processing (NLP) underwent a significant transformation. In recent years, trans- formers have become a fundamental component in the archi- tecture of most AI models.\n\n--- Segment 2 ---\nINTRODUCTION With the introduction of attention-based models and trans- formers [1], the field of natural language processing (NLP) underwent a significant transformation. In recent years, trans- formers have become a fundamental component in the archi- tecture of most AI models. Various models, such as Generative Pre-trained Transformers (GPT) [2], Bidirectional Encoder Representations from Transformers (BERT) [3], Language Model for Dialogue Applications (LaMDA) [4], and Vision Transformers (ViT) [5], have utilized transformer architectures to push the boundaries of performance across natural language processing and computer vision tasks. The success of transformers stems from their ability to effectively model long-range dependencies, enabling them to capture contextual relationships in textual, visual, and multimodal data with unprecedented accuracy. This has led to significant advancements in diverse applications, includ- ing text generation, machine translation, code synthesis, and image understanding. Despite these achievements, applying transformers to specialized domains such as hardware design and Verilog code generation remains relatively unexplored. The complexity of hardware description languages (HDLs), the need for logical correctness, and the structured nature of Verilog pose unique challenges that traditional AI-driven code generation techniques struggle to address. The utilization of large language models (LLMs) for hardware code generation is driven by the goal of developing an advanced tool that simplifies the hardware modeling process for designers [6] [9]. Furthermore, automating hardware code generation using LLMs plays a crucial role in minimizing human error. Due to the inherently complex and highly technical nature of hardware design, manual coding is susceptible to inaccuracies and inconsistencies [10]. Integrating these models into hardware development workflows reduces the inherent complexities of traditional design methods. Leveraging LLM capabilities enables a more intuitive and efficient approach to hardware modeling, thereby enhancing automation and design optimiza- tion. In addition, ensuring the security of LLM-generated code is paramount [11], [12]; rigorous code validation and adversarial testing must be implemented to safeguard against vulnerabilities and unauthorized modifications. As AI-driven automation continues to expand, leveraging agentic AI for Verilog code generation presents a promising opportunity to enhance productivity, reduce human effort, and improve design efficiency in hardware development.\n\n--- Segment 3 ---\nIn addition, ensuring the security of LLM-generated code is paramount [11], [12]; rigorous code validation and adversarial testing must be implemented to safeguard against vulnerabilities and unauthorized modifications. As AI-driven automation continues to expand, leveraging agentic AI for Verilog code generation presents a promising opportunity to enhance productivity, reduce human effort, and improve design efficiency in hardware development. In this work, we introduce VeriMind, an agentic AI approach for Verilog code generation that leverages the reasoning and iterative refinement capabilities of LLMs. Unlike conventional AI-driven code generation, which passively produces code based on a single prompt, our method employs a structured multi-step process to enhance the accuracy, interpretability, and adaptability of the generated hardware descriptions. By integrating cognitive reasoning and planning, our framework systematically decomposes the Verilog generation task, en- abling the AI to outline a logical design process before synthesizing the final code. Furthermore, to facilitate a more comprehensive evaluation of approaches that employ refine- ment loops, we introduce a novel metric termed (Average Refinement Cycles). This metric integrates the tra- ditional pass rate with the number of refinement iterations required to generate a correct code. By penalizing models that arXiv:2503.16514v3 [cs.AR] 16 Apr 2025 Supervisor Agent LLM Checker Agent Prompt Eng. Agent Verilog Agent Testbench Agent Input Prompt Final Output Generate a Verilog testbench code considering the following: 1... 2... Refine the input prompt and make sure it has the following: 1... 2... Generate a Verilog code considering the following: 1... 2... Fig. 1. Proposed Overall Architecture. require more cycles, provides a more nuanced assessment of both accuracy and efficiency, making it an invaluable tool for comparing and optimizing iterative code generation architectures. Our method adopts an interactive, self-improving workflow. Upon receiving a design specification from the user, the proposed architecture iteratively refines the generated Ver- ilog code ensuring both syntactic and functional correct- ness with dedicated agents managing specific subtasks. A detailed description of the architecture and its individual agents is provided in Section III. To evaluate the effectiveness of our method, we apply it to a diverse set of Verilog design tasks, ranging from simple combinational circuits to complex sequential logic.\n\n--- Segment 4 ---\nA detailed description of the architecture and its individual agents is provided in Section III. To evaluate the effectiveness of our method, we apply it to a diverse set of Verilog design tasks, ranging from simple combinational circuits to complex sequential logic. Our results demonstrate that agentic AI significantly improves the quality, correctness, and adaptability of generated Verilog code com- pared to traditional LLM-based code generation. This work highlights the potential of integrating structured reasoning and interactive validation into AI-driven hardware development, paving the way for more autonomous and reliable HDL design automation. The overall architecture of the proposed approach is illustrated in Fig.1. The key contributions of this paper are summarized as follows: Agentic AI Framework for Verilog Code Generation: Introduces a structured, reasoning-driven approach that enhances interpretability, correctness, and adaptability in hardware description language (HDL) generation. Interactive and Iterative Refinement Process: Incorporates intermediate validation steps where the AI outlines its reasoning before generating Verilog code, allowing user feedback to guide improvements. Enhanced Code Quality and Design Consistency: Evalu- ates the proposed approach across diverse Verilog design tasks, demonstrating improved syntactic correctness, log- ical coherence, and alignment with design specifications compared to conventional LLM-based generation. We introduce a novel evaluation metric, specifically designed to provide a fair and comprehensive assessment of architectures that employ iterative refine- ment loops. The remainder of this paper is structured as follows: Section II presents an overview of related work and background information. Section III details the proposed architecture. Sec- tion IV discusses the evaluation results and analysis. Finally, Section V outlines the conclusions and potential directions for future research. II. RELATED WORKS AND BACKGROUNDS High-quality Verilog datasets are crucial yet challenging for effective hardware code generation with LLMs. Nadimi et al. [13] addressed this by developing PyraNet, an open-source, hierarchical dataset that enhances the syntactic and functional accuracy of generated Verilog code. By using quality-based tiers and curriculum learning, models fine-tuned with PyraNet improved performance by up to 32.6 over CodeLlama-7B and 16.7 over state-of-the-art models on VerilogEval.\n\n--- Segment 5 ---\n[13] addressed this by developing PyraNet, an open-source, hierarchical dataset that enhances the syntactic and functional accuracy of generated Verilog code. By using quality-based tiers and curriculum learning, models fine-tuned with PyraNet improved performance by up to 32.6 over CodeLlama-7B and 16.7 over state-of-the-art models on VerilogEval. Advancements in Verilog dataset augmentation and LLM fine-tuning include approaches such as CodeV [14] and MG- Verilog [15]. CodeV employs multi-level summarization on a dataset of 165,000 Verilog modules enriched with GPT-3.5 annotations, achieving state-of-the-art results on VerilogEval and RTLLM surpassing GPT-4 by 22.1 on VerilogEval. Similarly, MG-Verilog provides a multi-granular dataset of over 11,000 samples, enhancing LLM generalization and im- proving code synthesis accuracy. Beyond dataset-driven improvements, several studies have optimized Verilog generation through fine-tuning, retrieval mechanisms, and structured prompting. BetterV [16] uses instruct-tuning with generative discriminators and Verilog-C program alignment to enhance key EDA processes. Mean- while, AutoVCoder [17] leverages retrieval-augmented gen- eration with a two-phase fine-tuning strategy to better align general-purpose LLMs with RTL constraints, achieving a 3.4 improvement in functional correctness on the RTLLM bench- mark. This demonstrates the significant impact of retrieval- enhanced architectures on RTL automation. Another important line of research focuses on self-reflective learning and compiler-based error correction. OriGen [18] introduces a self-correcting framework that combines code- to-code augmentation with compiler feedback loops for error detection and refinement. OriGen trains two specialized Low- Rank Adaptation (LoRA) models one for initial Verilog generation and another for syntax correction resulting in a 12.8 performance boost over existing open-source models and a 19.9 improvement in syntactic accuracy compared to GPT-4 Turbo. Incorporating iterative self-reflection, OriGen greatly improves the reliability of LLM-generated Verilog.\n\n--- Segment 6 ---\nOriGen trains two specialized Low- Rank Adaptation (LoRA) models one for initial Verilog generation and another for syntax correction resulting in a 12.8 performance boost over existing open-source models and a 19.9 improvement in syntactic accuracy compared to GPT-4 Turbo. Incorporating iterative self-reflection, OriGen greatly improves the reliability of LLM-generated Verilog. To tackle challenges related to hierarchical Verilog gen- eration, Tang et al. [19] proposed HiVeGen, a structured framework that decomposes HDL synthesis into hierarchical submodules. Traditional LLMs often generate monolithic, hallucination-prone HDL blocks, particularly for complex designs like domain-specific accelerators. HiVeGen mitigates this issue by introducing automated design space exploration, structured code reuse, and human-computer interaction mech- anisms to reduce debugging overhead. Empirical evaluations show that HiVeGen enhances the quality and scalability of AI- generated HDL, making it a promising approach for modular chip design automation. Structured prompting techniques have been explored to enhance Verilog synthesis. ROME [20] employs hierarchical prompting to decompose large hardware modules into orga- nized submodules and integrates an eight-stage automation pipeline that combines human-guided and autonomous refine- ment. Evaluations show that fine-tuned LLMs using ROME outperform traditional flat prompting in generating complex HDL, reducing both development time and computational overhead. Notably, ROME produced the first LLM-designed processor without human intervention, underscoring its revo- lutionary potential in AI-assisted chip design. Multi-agent frameworks have proven beneficial for RTL design by enhancing efficiency, interpretability, and perfor- mance. RTLSquad [21] introduces a collaborative system that divides the RTL workflow into three phases exploration, implementation, and verification each managed by special- ized agent squads to ensure seamless communication and transparent decision-making. Experimental results demonstrate that RTLSquad not only produces functionally correct RTL code but also improves power, performance, and area metrics, underscoring its value in scalable AI-assisted hardware devel- opment. III. METHODOLOGY This section begins with an overview of the complete agentic architecture, providing a high-level understanding of its structure and functionality. Subsequently, each agent is examined in detail, outlining its specific role and operational mechanisms within the framework.\n\n--- Segment 7 ---\nMETHODOLOGY This section begins with an overview of the complete agentic architecture, providing a high-level understanding of its structure and functionality. Subsequently, each agent is examined in detail, outlining its specific role and operational mechanisms within the framework. A. Agentic Framework: Structure and Workflow Our proposed architecture as shown in Fig. 1 comprises five interconnected agents Supervisor, Prompt Engineer, Verilog Code Generator, Verilog Testbench Generator, and Checker, that collaborate to generate verified Verilog designs. The Supervisor Agent orchestrates the workflow by initially send- ing the user s prompt to the Prompt Engineer Agent for refinement. The refined prompt is then dispatched to both the Verilog Code Generator and Testbench Generator Agents to produce the corresponding outputs, which are subsequently validated by the Checker Agent through compilation and simulation. Should errors arise, the Checker Agent triggers a re-evaluation cycle via the Supervisor Agent until the final, error-free Verilog code is produced. This iterative approach ensures high accuracy, functionality, and reliability in the generated designs, with further details on each agent provided in the following sections. B. Supervisor Agent The Supervisor Agent is the central coordinator in our multi- agent architecture for Verilog code generation, orchestrating the entire process from prompt refinement to final verification. This agent employs a lightweight LLM solely to manage inter-agent communications, including selecting appropriate agents at various stages and modifying prompts as necessary. In our experiments, we employed the gpt-4o-mini model for this agent. The process begins by receiving the initial design specification from the user and immediately dispatches this prompt to the Prompt Engineer Agent. This agent refines and optimizes the prompt, ensuring that it is well-structured for subsequent processing. Once the refined prompt is obtained, the Supervisor Agent simultaneously sends it to both the Verilog Code Generator Agent and the Verilog Testbench Generator Agent, enabling parallel generation of the Verilog module and its corresponding testbench. After these agents complete their tasks, the generated out- puts are forwarded to the Checker Agent, which validates the code by performing compilation and simulation tests. If the Checker Agent detects any errors such as compilation failures, simulation mismatches, or logical inconsistencies it promptly notifies the Supervisor Agent.\n\n--- Segment 8 ---\nAfter these agents complete their tasks, the generated out- puts are forwarded to the Checker Agent, which validates the code by performing compilation and simulation tests. If the Checker Agent detects any errors such as compilation failures, simulation mismatches, or logical inconsistencies it promptly notifies the Supervisor Agent. The Supervisor then evaluates the reported issues and decides whether to re-send the prompt for further refinement, request corrections from the Verilog Code Generator, or ask the Testbench Generator to adjust its output. This iterative loop continues until the Checker Agent confirms that the generated Verilog design is both syntactically correct and functionally robust. Moreover, the Supervisor Agent employs a structured mes- sage passing mechanism that ensures effective inter-agent communication and maintains a detailed state tracking system. This system monitors each agent s progress and allows the Supervisor to dynamically adjust execution priorities in the event of delays or unexpected outcomes, thereby optimizing overall workflow efficiency. Overall, the Supervisor Agent plays a critical role by seam- lessly integrating specialized tasks refinement, code gener- ation, testbench generation, and validation into a cohesive and adaptive process. This structured and iterative approach not only enhances the quality and accuracy of the generated Verilog modules but also significantly reduces manual inter- vention and error rates in hardware design. C. Prompt Engineer Agent The Prompt Engineer Agent refines the input prompt to ensure that the generated Verilog code is structurally sound, functionally relevant, and aligned with the user s intent. By modifying and reformatting the input, it increases the likelihood of producing high-quality code that meets design specifications. Like the Supervisor Agent, this agent uses a compact LLM in our evaluation, we employed the gpt-4o- mini model for this role. Upon receiving a raw prompt from the Supervisor Agent, the Prompt Engineer Agent enhances clarity, structure, and specificity. This process involves rewriting ambiguous specifi- cations, expanding under-specified prompts with necessary de- sign constraints (e.g., clocking details, functional properties), reformatting inputs into structured templates, and removing redundant information.\n\n--- Segment 9 ---\nUpon receiving a raw prompt from the Supervisor Agent, the Prompt Engineer Agent enhances clarity, structure, and specificity. This process involves rewriting ambiguous specifi- cations, expanding under-specified prompts with necessary de- sign constraints (e.g., clocking details, functional properties), reformatting inputs into structured templates, and removing redundant information. For example, a vague request such as Generate a Verilog module for a 4-bit counter may be transformed into Generate a Verilog module for a 4-bit syn- chronous up-counter with a clock input (clk), an asynchronous reset (rst), and an enable signal (en), where the output (count) is a 4-bit register. Additionally, the agent tailors its refinements to the task at hand whether it involves combinational logic, sequential circuits, or finite state machines by ensuring that the prompt includes all necessary details (e.g., Boolean expressions for combinational circuits or explicit state transitions for FSMs). If validation errors occur later, the Checker Agent informs the Supervisor Agent, prompting further modifications until an effective prompt is achieved. D. Verilog Code Generator Agent The Verilog Code Generator Agent uses a refined prompt from the Supervisor Agent and a specialized LLM for HDL generation to produce Verilog modules that are both struc- turally sound and functionally accurate. It is essential to the multi-agent framework, ensuring adherence to digital design best practices while maintaining efficiency. This agent employs a fine-tuned LLM to manage the sophisticated task of Verilog code generation. For comparative analysis, we evaluated our architecture using a range of LLMs in this role, including both fine-tuned models and commercial alternatives. Once the Prompt Engineer Agent refines the input request, the Supervisor Agent dispatches the structured prompt to the Verilog Code Generator Agent, initiating the code synthesis process. The Verilog Code Generator Agent is designed to handle a diverse range of digital designs, ensuring adaptability across different hardware modules: Combinational Circuits: Generates Verilog for logic gates, multiplexers, arithmetic operations, and other state- less logic. Sequential Circuits: Implements flip-flops, registers, counters, and clocked state transitions. Finite State Machines (FSMs): Constructs Mealy and Moore state machines, including state encoding and tran- sition logic.\n\n--- Segment 10 ---\nSequential Circuits: Implements flip-flops, registers, counters, and clocked state transitions. Finite State Machines (FSMs): Constructs Mealy and Moore state machines, including state encoding and tran- sition logic. Parameterized Modules: Creates reusable Verilog com- ponents with adjustable parameters (generate statements, parameter values). For example, when generating an FSM for a traffic light controller, the agent ensures that state transitions are clearly defined and that the design follows a systematic approach; a representative sample is provided in Appendix A. E. Testbench Generator Agent The Verilog Testbench Generator Agent is responsible for creating a corresponding testbench to verify the functional- ity of the generated Verilog module. By simulating various input conditions and monitoring the expected outputs, this agent ensures that the generated hardware design behaves as intended. Mirroring the approach used by the Verilog Code Generator Agent, this agent utilizes a fine-tuned model for testbench generation. For comparative analysis, we evaluated our architecture with a variety of LLMs in this role, including both fine-tuned models and commercial alternatives. Upon receiving the refined prompt, the Supervisor Agent instructs the Verilog Testbench Generator Agent to create a structured testbench. This process involves: 1) defining stimuli and input signals to drive the module under test (MUT), 2) in- stantiating the Verilog module within the testbench framework, 3) creating a simulation sequence, including clock generation, reset initialization, and test scenarios, and 4) using assertions or monitor statements to validate expected outputs. For instance, the testbench generated for the traffic light controller described in Section III-D is presented in Appendix B. If the Checker Agent detects issues during compilation or simulation, the Supervisor Agent instructs the Verilog Test- bench Generator Agent to modify the testbench accordingly. This iterative process ensures that: 1) the testbench correctly stimulates edge cases and corner conditions, 2) the verification covers all functional aspects of the Verilog module, and 3) the generated testbench is efficient, concise, and reusable for validation purposes. F. Checker Agent The Checker Agent ensures the accuracy of the generated Verilog module and its corresponding testbench by conducting compilation and simulation tests. Serving as a verification layer, it guarantees that the output complies with Verilog syntax rules and meets functional expectations.\n\n--- Segment 11 ---\nF. Checker Agent The Checker Agent ensures the accuracy of the generated Verilog module and its corresponding testbench by conducting compilation and simulation tests. Serving as a verification layer, it guarantees that the output complies with Verilog syntax rules and meets functional expectations. This agent leverages compilation and simulation tools, including Icarus Verilog [22] and Verilator [23], to systematically validate the correctness of the generated HDL code. Upon receiving the Verilog module and testbench, the Checker Agent conducts a structured validation process to ensure the correct functionality of the design. It first compiles the Verilog module and testbench, checking for syntax er- rors, missing dependencies, or structural inconsistencies. Once compilation is successful, the agent performs a simulation to verify functional correctness, ensuring that the module operates as expected across different test scenarios. During the simulation, it monitors signal behavior and assertions using display, or assertion-based verification methods to track discrepancies in output. If the Checker Agent encounters issues such as compi- lation errors, logical inconsistencies, or incorrect simulation results it notifies the Supervisor Agent with such informa- tion, prompting a refinement loop where the Verilog Code Generator Agent or Verilog Testbench Generator Agent mod- ifies the output to resolve the detected errors.\n\n--- Segment 12 ---\nDuring the simulation, it monitors signal behavior and assertions using display, or assertion-based verification methods to track discrepancies in output. If the Checker Agent encounters issues such as compi- lation errors, logical inconsistencies, or incorrect simulation results it notifies the Supervisor Agent with such informa- tion, prompting a refinement loop where the Verilog Code Generator Agent or Verilog Testbench Generator Agent mod- ifies the output to resolve the detected errors. This iterative TABLE I EXPERIMENTAL RESULTS USING VERILOGEVAL [24] Model VerilogEval-Machine VerilogEval-Human GPT-4o-mini 60.1 62.2 65 44.2 49.4 56.4 Claude3.5-Sonnet [25] 74.8 76.9 79.7 56.4 60.9 69.9 CodeLlama-70B-Instruct [26] 54.5 58 63.6 40.4 43.6 48.7 DeepSeek-Coder-V2-Instruct [27] 63.6 73.4 78.3 48.1 52.6 57.1 PyraNet-DeepSeek [13] 77.6 84.6 89.5 58.3 62.8 67.9 RTLCoder-DeepSeek [28] 61.2 76.5 81.8 41.6 50.1 53.4 Origen-DeepSeek [18] 74.1 82.5 85.3 54.5 60.3 64.1 AutoVCoder-CodeQwen [17] 68.5 79.7 79.7 48.7 55.8 55.8 CodeV-DeepSeek [14] 77.9 88.6 90.7 52.7 62.5 67.3 VeriMind-GPT-4o-mini 60.1 68.5 72.6 44.2 54.5 62.8 VeriMind-Claude3.5-Sonnet 74.8 83.2 85.3 56.4 63.5 71.8 VeriMind-CodeLlama-70b-Instruct 54.5 65 70.5 40.4 48.7 53.8 VeriMind-Deep-Seek-Coder-V2-Instruct 63.6 76.2 81.8 48.1 58.3 65.4 VeriMind-PyraNet-DeepSeek 77.6 90.9 96.5 58.3 67.9 74.4 validation process ensures that the final Verilog module is both syntactically correct and functionally reliable before being delivered to the user.\n\n--- Segment 13 ---\nIf the Checker Agent encounters issues such as compi- lation errors, logical inconsistencies, or incorrect simulation results it notifies the Supervisor Agent with such informa- tion, prompting a refinement loop where the Verilog Code Generator Agent or Verilog Testbench Generator Agent mod- ifies the output to resolve the detected errors. This iterative TABLE I EXPERIMENTAL RESULTS USING VERILOGEVAL [24] Model VerilogEval-Machine VerilogEval-Human GPT-4o-mini 60.1 62.2 65 44.2 49.4 56.4 Claude3.5-Sonnet [25] 74.8 76.9 79.7 56.4 60.9 69.9 CodeLlama-70B-Instruct [26] 54.5 58 63.6 40.4 43.6 48.7 DeepSeek-Coder-V2-Instruct [27] 63.6 73.4 78.3 48.1 52.6 57.1 PyraNet-DeepSeek [13] 77.6 84.6 89.5 58.3 62.8 67.9 RTLCoder-DeepSeek [28] 61.2 76.5 81.8 41.6 50.1 53.4 Origen-DeepSeek [18] 74.1 82.5 85.3 54.5 60.3 64.1 AutoVCoder-CodeQwen [17] 68.5 79.7 79.7 48.7 55.8 55.8 CodeV-DeepSeek [14] 77.9 88.6 90.7 52.7 62.5 67.3 VeriMind-GPT-4o-mini 60.1 68.5 72.6 44.2 54.5 62.8 VeriMind-Claude3.5-Sonnet 74.8 83.2 85.3 56.4 63.5 71.8 VeriMind-CodeLlama-70b-Instruct 54.5 65 70.5 40.4 48.7 53.8 VeriMind-Deep-Seek-Coder-V2-Instruct 63.6 76.2 81.8 48.1 58.3 65.4 VeriMind-PyraNet-DeepSeek 77.6 90.9 96.5 58.3 67.9 74.4 validation process ensures that the final Verilog module is both syntactically correct and functionally reliable before being delivered to the user. IV.\n\n--- Segment 14 ---\nThis iterative TABLE I EXPERIMENTAL RESULTS USING VERILOGEVAL [24] Model VerilogEval-Machine VerilogEval-Human GPT-4o-mini 60.1 62.2 65 44.2 49.4 56.4 Claude3.5-Sonnet [25] 74.8 76.9 79.7 56.4 60.9 69.9 CodeLlama-70B-Instruct [26] 54.5 58 63.6 40.4 43.6 48.7 DeepSeek-Coder-V2-Instruct [27] 63.6 73.4 78.3 48.1 52.6 57.1 PyraNet-DeepSeek [13] 77.6 84.6 89.5 58.3 62.8 67.9 RTLCoder-DeepSeek [28] 61.2 76.5 81.8 41.6 50.1 53.4 Origen-DeepSeek [18] 74.1 82.5 85.3 54.5 60.3 64.1 AutoVCoder-CodeQwen [17] 68.5 79.7 79.7 48.7 55.8 55.8 CodeV-DeepSeek [14] 77.9 88.6 90.7 52.7 62.5 67.3 VeriMind-GPT-4o-mini 60.1 68.5 72.6 44.2 54.5 62.8 VeriMind-Claude3.5-Sonnet 74.8 83.2 85.3 56.4 63.5 71.8 VeriMind-CodeLlama-70b-Instruct 54.5 65 70.5 40.4 48.7 53.8 VeriMind-Deep-Seek-Coder-V2-Instruct 63.6 76.2 81.8 48.1 58.3 65.4 VeriMind-PyraNet-DeepSeek 77.6 90.9 96.5 58.3 67.9 74.4 validation process ensures that the final Verilog module is both syntactically correct and functionally reliable before being delivered to the user. IV. EVALUATION AND DISCUSSION To evaluate our agentic AI framework for Verilog code generation, we used the VerilogEval benchmark suite [24].\n\n--- Segment 15 ---\nIV. EVALUATION AND DISCUSSION To evaluate our agentic AI framework for Verilog code generation, we used the VerilogEval benchmark suite [24]. Recognizing that refinement loops require a nuanced evalu- ation, we introduced the metric which combines with Average Refinement Cycles (ARC) (details in subsequent subsections). These benchmarks standardize the assessment of syntactic correctness, functional accuracy, and iterative efficiency. A. Experimental Setup Benchmarks: VerilogEval benchmark consists of two parts: 1) VerilogEval Machine and 2) VerilogEval Human with 143 and 156 problems respectively. They all sourced from the HDLbits platform1, covering a wide range of Verilog codes from combinational circuits to complex sequential circuits. In addition, for the metric, we extend the VerilogEval platform by incorporating the average number of refinement cycles into the evaluation framework. Experimental setup: For the comparison purposes, we im- plemented the proposed design using multiple baseline LLMs including GPT-4o-mini, Claude3.5-Sonnet, CodeLlama-70b- Instruct, DeepSeek-Coder-V2-Instruct and PyraNet. We com- pared the results with other SOTA works including PyraNet [13], Origen [18], AutoVCoder [17], CodeV [14], and also with the commercial LLMs without any agents. All the exper- imental results can be found in Tables I and II. It is important to note that in all implemented designs, the Supervisor and Prompt Engineer agents consistently employ the gpt-4o-mini model, while the Verilog and Testbench Generator agents utilize different LLMs. B. Shortcoming and Solution One major challenge was selecting an appropriate evaluation metric. Traditional approaches typically rely on the metric. However, applying to architectures that em- ploy an iterative refinement loop can be misleading, since a single task may require multiple attempts before reaching a correct output.\n\n--- Segment 16 ---\nTraditional approaches typically rely on the metric. However, applying to architectures that em- ploy an iterative refinement loop can be misleading, since a single task may require multiple attempts before reaching a correct output. More specifically, reporting for a system that iteratively corrects errors would be inaccurate 1 sets TABLE II EXPERIMENTAL RESULTS USING METRIC Model Verilog-Machine Verilog-Human pass rate ARC pass rate ARC GPT-4o-mini 65 4.48 57.6 56.4 5.76 45 Claude3.5-Sonnet [25] 79.7 3.16 76.1 69.9 4.7 60.9 CodeLlama-70B-Instruct [26] 63.6 4.91 54.6 48.7 6.21 37.2 DeepSeek-Coder-V2-Instruct [27] 78.3 3.78 72.5 57.1 5.45 46.8 PyraNet-DeepSeek [13] 89.5 2.66 87.1 67.9 4.53 60 RTLCoder-DeepSeek [28] 81.8 3.73 75.9 53.4 5.83 42.1 Origen-DeepSeek [18] 85.3 2.91 82.3 64.1 4.81 55.5 AutoVCoder-CodeQwen [17] 79.7 3.27 75.7 55.8 5.26 46.5 CodeV-DeepSeek [14] 90.7 2.45 89 67.3 4.76 58.4 VeriMind-GPT-4o-mini 72.6 4.17 65.8 62.8 5.51 51.3 VeriMind-Claude3.5-Sonnet 85.3 2.85 82.5 71.8 4.57 63.2 VeriMind-CodeLlama-70b-Instruct 70.5 4.57 62.2 53.8 5.95 42.1 VeriMind-Deep-Seek-Coder-V2-Instruct 81.8 3.64 76.3 65.4 5.16 55 VeriMind-PyraNet-DeepSeek 96.5 2.35 94.8 74.4 4.27 66.8 if some tasks take more than one try.\n\n--- Segment 17 ---\nHowever, applying to architectures that em- ploy an iterative refinement loop can be misleading, since a single task may require multiple attempts before reaching a correct output. More specifically, reporting for a system that iteratively corrects errors would be inaccurate 1 sets TABLE II EXPERIMENTAL RESULTS USING METRIC Model Verilog-Machine Verilog-Human pass rate ARC pass rate ARC GPT-4o-mini 65 4.48 57.6 56.4 5.76 45 Claude3.5-Sonnet [25] 79.7 3.16 76.1 69.9 4.7 60.9 CodeLlama-70B-Instruct [26] 63.6 4.91 54.6 48.7 6.21 37.2 DeepSeek-Coder-V2-Instruct [27] 78.3 3.78 72.5 57.1 5.45 46.8 PyraNet-DeepSeek [13] 89.5 2.66 87.1 67.9 4.53 60 RTLCoder-DeepSeek [28] 81.8 3.73 75.9 53.4 5.83 42.1 Origen-DeepSeek [18] 85.3 2.91 82.3 64.1 4.81 55.5 AutoVCoder-CodeQwen [17] 79.7 3.27 75.7 55.8 5.26 46.5 CodeV-DeepSeek [14] 90.7 2.45 89 67.3 4.76 58.4 VeriMind-GPT-4o-mini 72.6 4.17 65.8 62.8 5.51 51.3 VeriMind-Claude3.5-Sonnet 85.3 2.85 82.5 71.8 4.57 63.2 VeriMind-CodeLlama-70b-Instruct 70.5 4.57 62.2 53.8 5.95 42.1 VeriMind-Deep-Seek-Coder-V2-Instruct 81.8 3.64 76.3 65.4 5.16 55 VeriMind-PyraNet-DeepSeek 96.5 2.35 94.8 74.4 4.27 66.8 if some tasks take more than one try. In our framework, which inherently incorporates a feedback loop to verify and fix generated code, using alone would be deceptive it could potentially report a 100 rate if the loop functions perfectly.\n\n--- Segment 18 ---\nMore specifically, reporting for a system that iteratively corrects errors would be inaccurate 1 sets TABLE II EXPERIMENTAL RESULTS USING METRIC Model Verilog-Machine Verilog-Human pass rate ARC pass rate ARC GPT-4o-mini 65 4.48 57.6 56.4 5.76 45 Claude3.5-Sonnet [25] 79.7 3.16 76.1 69.9 4.7 60.9 CodeLlama-70B-Instruct [26] 63.6 4.91 54.6 48.7 6.21 37.2 DeepSeek-Coder-V2-Instruct [27] 78.3 3.78 72.5 57.1 5.45 46.8 PyraNet-DeepSeek [13] 89.5 2.66 87.1 67.9 4.53 60 RTLCoder-DeepSeek [28] 81.8 3.73 75.9 53.4 5.83 42.1 Origen-DeepSeek [18] 85.3 2.91 82.3 64.1 4.81 55.5 AutoVCoder-CodeQwen [17] 79.7 3.27 75.7 55.8 5.26 46.5 CodeV-DeepSeek [14] 90.7 2.45 89 67.3 4.76 58.4 VeriMind-GPT-4o-mini 72.6 4.17 65.8 62.8 5.51 51.3 VeriMind-Claude3.5-Sonnet 85.3 2.85 82.5 71.8 4.57 63.2 VeriMind-CodeLlama-70b-Instruct 70.5 4.57 62.2 53.8 5.95 42.1 VeriMind-Deep-Seek-Coder-V2-Instruct 81.8 3.64 76.3 65.4 5.16 55 VeriMind-PyraNet-DeepSeek 96.5 2.35 94.8 74.4 4.27 66.8 if some tasks take more than one try. In our framework, which inherently incorporates a feedback loop to verify and fix generated code, using alone would be deceptive it could potentially report a 100 rate if the loop functions perfectly. For more accurate evaluation and comparison, we adopted a dual approach.\n\n--- Segment 19 ---\nIn our framework, which inherently incorporates a feedback loop to verify and fix generated code, using alone would be deceptive it could potentially report a 100 rate if the loop functions perfectly. For more accurate evaluation and comparison, we adopted a dual approach. First, for a fair comparison, we excluded samples that required more than k refinement iterations when computing for k 1, 5, 10. This ensures that the metric is not artificially inflated by the iterative process. Sec- ond, we introduced a new metric, Average Refinement Cycles (ARC), which measures the average number of iterations the system takes to produce correct Verilog code; lower ARC values indicate a more efficient process. This combination provides a more accurate and comprehensive evaluation of the performance of our framework. Equation 1 defines our newly proposed metric, which integrates the traditional measure with the ARC to capture both the success rate and the iterative refinement efficiency. PassRate e 0.01 (ARC 1)2 (1) In this formula, PassRate represents the model s success rate and ARC quantifies the average number of iterations required to produce correct output. The exponential term, e 0.01 (ARC 1)2 serves as a penalty factor that equals 1 when ARC is 1 indicating a perfect one-shot performance and decays as ARC increases. This decay penalizes models that require additional refinement iterations, with the factor 0.01 controlling the severity of the penalty. By combining pass rate with ARC, the metric effectively balances accuracy with refinement efficiency, providing a comprehensive assessment of the model s performance. This combined metric widens the evaluative scope, allowing for a more nuanced comparison across different architectures. It is important to note that the constant in the equation was chosen so that when ARC exceeds 10, the resulting metric value becomes negligibly small effectively treating samples requiring more than 10 iterations as incomplete. If evaluation is needed for models that take more than 10 iterations to complete a task, this limitation can be overcome by adjusting the constant in Equation 1. Fig. 2 presents a 3D surface plot of the proposed metric across its full range. The figure clearly demonstrates that when ARC equals 1, the metric is equivalent to while higher ARC values lead to a gradual decrease in (as shown on the z-axis). 0 20 40 60 80 100 5 10 0 50 100 Pass Rate ( ) ARC ( ) Fig. 2.\n\n--- Segment 20 ---\n0 20 40 60 80 100 5 10 0 50 100 Pass Rate ( ) ARC ( ) Fig. 2. Surface plot of the metric: z x e 0.01 (y 1)2, where x is the Pass Rate and y is ARC. C. Results Explained Our experiments on the VerilogEval-Machine and VerilogEval-Human benchmarks reveal that baseline models (e.g., GPT-4o-mini, Claude3.5-Sonnet, CodeLlama-70B- Instruct, DeepSeek-Coder-V2-Instruct) achieve moderate performance, with improvements at higher levels. In contrast, our VeriMind models significantly boost and metrics. For example, while GPT-4o- mini attains a 60.1 on the Machine benchmark, VeriMind-GPT-4o-mini increases and to 68.5 and 72.6 , respectively. Similarly, VeriMind-PyraNet- DeepSeek records of 77.6 , of 90.9 , and of 96.5 . These results demonstrate that our feedback loop and iterative refinement process substantially enhance code generation quality without compromising initial performance. Overall, improvements in higher-order metrics indicate that our approach increases the likelihood of generating correct Verilog code with fewer refinement cycles, as further confirmed by our metric. Following the presentation of individual and scores, we further evaluate our framework us- ing the metric, which combines the initial pass rate with Average Refinement Cycles through an exponential penalty that increases with additional iterations. This design ensures that a model with high and minimal refine- ment (ARC 1) incurs little penalty, whereas additional cycles lower the score. Our experiments reveal that, while baseline models show moderate perfor- mance, the VeriMind variants consistently achieve higher values; for instance, VeriMind-PyraNet-DeepSeek records a of 96.5 and, with an ARC of 2.35, a of 94.8 , significantly outperforming the baseline PyraNet-DeepSeek (87.1 ). Similar trends are observed with the VeriMind versions of GPT-4o-mini, Claude3.5-Sonnet, CodeLlama-70B-Instruct, and DeepSeek-Coder-V2-Instruct, highlighting that our feedback loop reduces the necessary refinement iterations and provides a more comprehensive per- formance metric. Fig.\n\n--- Segment 21 ---\nSimilar trends are observed with the VeriMind versions of GPT-4o-mini, Claude3.5-Sonnet, CodeLlama-70B-Instruct, and DeepSeek-Coder-V2-Instruct, highlighting that our feedback loop reduces the necessary refinement iterations and provides a more comprehensive per- formance metric. Fig. 3 illustrates a unified 3D visualization of the pass rate, ARC, and metrics. 60 80 100 2 2.5 3 3.5 4 4.5 5 0 50 100 Pass Rate ( ) ARC ( ) VerilogEval-Machine VeriMind-PyraNet-DeepSeek PyraNet-DeepSeek VeriMind-DeepSeek-Coder-V2-Instruct DeepSeek-Coder-V2-Instruct VeriMind-CodeLlama-70b-Instruct CodeLlama-70B-Instruct VeriMind-Claude3.5-Sonnet Claude3.5-Sonnet VeriMind-GPT-4o-mini GPT-4o-mini RTLCoder-DeepSeek Origen-DeepSeek AutoVCoder-CodeQwen CodeV-DeepSeek 50 60 70 4.5 5 5.5 6 0 50 Pass Rate ( ) ARC ( ) VerilogEval-Human Fig. 3. Bar chart showing Pass Rate (x-axis), ARC (y-axis), and (z-axis) comparing the results of VeriMind and other SOTA methods. V. CONCLUSION AND FUTURE WORKS In this paper, we introduce VeriMind, an agentic AI frame- work for Verilog code generation that employs specialized agents Supervisor, Prompt Engineer, Verilog Code Gener- ator, Testbench Generator, and Checker in an iterative re- finement process. By decomposing the task into manageable subtasks and leveraging a feedback loop, VeriMind enhances both the syntactic and functional correctness of generated Ver- ilog code while reducing refinement cycles. We also propose the metric, which combines with Average Refinement Cycles to offer a more comprehensive evalua- tion of initial accuracy and iterative efficiency.\n\n--- Segment 22 ---\nBy decomposing the task into manageable subtasks and leveraging a feedback loop, VeriMind enhances both the syntactic and functional correctness of generated Ver- ilog code while reducing refinement cycles. We also propose the metric, which combines with Average Refinement Cycles to offer a more comprehensive evalua- tion of initial accuracy and iterative efficiency. Evaluations on VerilogEval-Machine and VerilogEval-Human benchmarks show improvements of up to 8.3 on and 8.1 on underscoring the effectiveness of our approach in advancing hardware design automation. Future work will focus on refining the iterative process, improve security, and extend the framework to support additional hardware description languages and complex designs. REFERENCES [1] Vaswani et al. Attention is all you need. 2017. [2] Radford et al. Improving language understanding by generative pre- training. 2018. [3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language un- derstanding. arXiv preprint arXiv:1810.04805, 2018. [4] Cohen et al. Lamda: Language models for dialog applications. In arXiv. 2022. [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis- senborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. [6] Bardia Nadimi and Hao Zheng. A multi-expert large language model architecture for verilog code generation. In IEEE LLM Aided Design Workshop (LAD), pages 1 5, June 2024. [7] Md Rubel Ahmed, Bardia Nadimi, and Hao Zheng. System-on-chip message flow mining with masked-language models. In 2023 IEEE 66th International Midwest Symposium on Circuits and Systems (MWSCAS), pages 496 500, 2023. [8] Deepak Vungarala, Sakila Alam, Arnob Ghosh, and Shaahin Angizi.\n\n--- Segment 23 ---\nIn 2023 IEEE 66th International Midwest Symposium on Circuits and Systems (MWSCAS), pages 496 500, 2023. [8] Deepak Vungarala, Sakila Alam, Arnob Ghosh, and Shaahin Angizi. Spicepilot: Navigating spice code generation and simulation with ai guidance, 2024. [9] Deepak Vungarala, Mahmoud Nazzal, Mehrdad Morsali, Chao Zhang, Arnob Ghosh, Abdallah Khreishah, and Shaahin Angizi. Sa-ds: A dataset for large language model-driven ai accelerator design generation, 2024. [10] Dessouky et al. HardFails: Insights into Software-Exploitable hardware bugs. In 28th USENIX Security Symposium (USENIX Security 19), pages 213 230, Santa Clara, CA, August 2019. USENIX Association. [11] Seyedsina Nabavirazavi, Samira Zad, and Sundararaja Sitharama Iyen- gar. Evaluating the universality of do anything now jailbreak prompts on large language models: Content warning: This paper contains un- filtered and harmful examples. In 2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC), pages 00691 00696, 2025. [12] Vishal Rathod, Seyedsina Nabavirazavi, Samira Zad, and Sun- dararaja Sitharama Iyengar. Privacy and security challenges in large language models. In 2025 IEEE 15th Annual Computing and Commu- nication Workshop and Conference (CCWC), pages 00746 00752, 2025. [13] Bardia Nadimi, Ghali Omar Boutaib, and Hao Zheng. Pyranet: A multi- layered hierarchical dataset for verilog, 2025. [14] Yang Zhao et al. Codev: Empowering llms for verilog generation through multi-level summarization. ArXiv, abs 2407.10424, 2024. [15] Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, and Yingyan Ce- line Lin. Mg-verilog: Multi-grained dataset towards enhanced llm- assisted verilog generation, 2024.\n\n--- Segment 24 ---\n[15] Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, and Yingyan Ce- line Lin. Mg-verilog: Multi-grained dataset towards enhanced llm- assisted verilog generation, 2024. [16] Zehua Pei, Huiling Zhen, Mingxuan Yuan, Yu Huang, and Bei Yu. BetterV: Controlled verilog generation with discriminative guidance. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 40145 40153. PMLR, 21 27 Jul 2024. [17] Mingzhe Gao et al. Autovcoder: A systematic framework for automated verilog code generation using llms, 2024. [18] Fan Cui et al. Origen:enhancing rtl code generation with code-to-code augmentation and self-reflection, 2024. [19] Jinwei Tang, Jiayin Qin, Kiran Thorat, Chen Zhu-Tian, Yu Cao, Yang, Zhao, and Caiwen Ding. Hivegen hierarchical llm-based verilog generation for scalable chip design, 2024. [20] Andre Nakkab, Sai Qian Zhang, Ramesh Karri, and Siddharth Garg. Rome was not built in a single step: Hierarchical prompting for llm- based chip design, 2024. [21] Bowei Wang, Qi Xiong, Zeqing Xiang, Lei Wang, and Renzhi Chen. Rtlsquad: Multi-agent based interpretable rtl design, 2025. [22] Stephen Williams. Icarus verilog. [23] Wilson Snyder Wilson and contributors. Verilator: Fast and free verilog hdl simulator, 2024. Accessed: February 25, 2025. [24] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. Verilogeval: Evaluating large language models for verilog code genera- tion, 2023. [25] Anthropic. Claude 3.5 sonnet model card addendum, 2024. Accessed: 03-2025. [26] Baptiste Rozi ere et al. Code llama: Open foundation models for code, 2024. [27] Qihao et al. Zhu.\n\n--- Segment 25 ---\n[27] Qihao et al. Zhu. Deepseek-coder-v2: Breaking the barrier of closed- source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. [28] Liu et al. Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, 2024. Appendix A: module traffic_light_controller( input wire clk, Clock input input wire reset, Active high reset output reg [2:0] north_south_lights, For N-S dir output reg [2:0] east_west_lights For E-W dir ); Light encoding: parameter RED 3'b100; parameter YELLOW 3'b010; parameter GREEN 3'b001; State encoding parameter S0 2'b00; NS: Green, EW: Red parameter S1 2'b01; NS: Yellow, EW: Red parameter S2 2'b10; NS: Red, EW: Green parameter S3 2'b11; NS: Red, EW: Yellow Timing parameters (adjust according to your clock frequency) 50M cycles 1 second 50MHz parameter GREEN_TIME 32'd50000000; 15M cycles 0.3 seconds 50MHz parameter YELLOW_TIME 32'd15000000; reg [1:0] state, next_state; reg [31:0] timer; State register and timer always (posedge clk or posedge reset) begin if (reset) begin state S0; timer 32'd0; end else begin if (timer 32'd0) begin state next_state; Load appropriate timer value based on next state case (next_state) S0, S2: timer GREEN_TIME; S1, S3: timer YELLOW_TIME; default: timer GREEN_TIME; endcase end else begin timer timer - 32'd1; end end end Next state logic always ( ) begin case (state) S0: next_state (timer 32'd0) ? S1 : S0; S1: next_state (timer 32'd0) ? S2 : S1; S2: next_state (timer 32'd0) ? S3 : S2; S3: next_state (timer 32'd0) ?\n\n--- Segment 26 ---\nS2 : S1; S2: next_state (timer 32'd0) ? S3 : S2; S3: next_state (timer 32'd0) ? S0 : S3; default: next_state S0; endcase end Output logic always ( ) begin case (state) S0: begin north_south_lights GREEN; east_west_lights RED; end S1: begin north_south_lights YELLOW; east_west_lights RED; end S2: begin north_south_lights RED; east_west_lights GREEN; end S3: begin north_south_lights RED; east_west_lights YELLOW; end default: begin north_south_lights RED; east_west_lights RED; end endcase end endmodule Appendix B: timescale 1ns 1ps module traffic_light_controller_tb; Parameters for simulation parameter CLK_PERIOD 20; 50MHz clock (20ns period) parameter SIM_TIME 5000000; Simulation time in ns Signal declarations reg clk; reg reset; wire [2:0] north_south_lights; wire [2:0] east_west_lights; Instantiate the module under test (MUT) traffic_light_controller UUT ( .clk(clk), .reset(reset), .north_south_lights(north_south_lights), .east_west_lights(east_west_lights) ); Clock generation initial begin clk 0; forever (CLK_PERIOD 2) clk clk; end Light state definitions for easier monitoring wire ns_red, ns_yellow, ns_green, ew_red, ew_yellow, ew_green; assign ns_red (north_south_lights 3'b100); assign ns_yellow (north_south_lights 3'b010); assign ns_green (north_south_lights 3'b001); assign ew_red (east_west_lights 3'b100); assign ew_yellow (east_west_lights 3'b010); assign ew_green (east_west_lights 3'b001); Monitor changes in light states initial begin monitor("Time: 0t, NS: s, EW: s", time, ns_red ? "RED " : ns_yellow ?\n\n--- Segment 27 ---\nS0 : S3; default: next_state S0; endcase end Output logic always ( ) begin case (state) S0: begin north_south_lights GREEN; east_west_lights RED; end S1: begin north_south_lights YELLOW; east_west_lights RED; end S2: begin north_south_lights RED; east_west_lights GREEN; end S3: begin north_south_lights RED; east_west_lights YELLOW; end default: begin north_south_lights RED; east_west_lights RED; end endcase end endmodule Appendix B: timescale 1ns 1ps module traffic_light_controller_tb; Parameters for simulation parameter CLK_PERIOD 20; 50MHz clock (20ns period) parameter SIM_TIME 5000000; Simulation time in ns Signal declarations reg clk; reg reset; wire [2:0] north_south_lights; wire [2:0] east_west_lights; Instantiate the module under test (MUT) traffic_light_controller UUT ( .clk(clk), .reset(reset), .north_south_lights(north_south_lights), .east_west_lights(east_west_lights) ); Clock generation initial begin clk 0; forever (CLK_PERIOD 2) clk clk; end Light state definitions for easier monitoring wire ns_red, ns_yellow, ns_green, ew_red, ew_yellow, ew_green; assign ns_red (north_south_lights 3'b100); assign ns_yellow (north_south_lights 3'b010); assign ns_green (north_south_lights 3'b001); assign ew_red (east_west_lights 3'b100); assign ew_yellow (east_west_lights 3'b010); assign ew_green (east_west_lights 3'b001); Monitor changes in light states initial begin monitor("Time: 0t, NS: s, EW: s", time, ns_red ? "RED " : ns_yellow ? "YELLOW" : "GREEN ", ew_red ? "RED " : ew_yellow ?\n\n--- Segment 28 ---\n"YELLOW" : "GREEN ", ew_red ? "RED " : ew_yellow ? "YELLOW" : "GREEN "); end State tracking variables reg [31:0] state_start_time; reg [31:0] state_duration; reg [2:0] prev_ns_lights; reg [2:0] prev_ew_lights; Stimulus and test sequence initial begin Initialize signals reset 1; state_start_time 0; prev_ns_lights 3'b000; prev_ew_lights 3'b000; Apply reset 100 reset 0; Run simulation for specified time SIM_TIME; Report test results display("Test completed at time 0t", time); finish; end State transition checks always (posedge clk) begin Detect state changes if (north_south_lights ! prev_ns_lights east_west_lights !\n\n--- Segment 29 ---\n"YELLOW" : "GREEN "); end State tracking variables reg [31:0] state_start_time; reg [31:0] state_duration; reg [2:0] prev_ns_lights; reg [2:0] prev_ew_lights; Stimulus and test sequence initial begin Initialize signals reset 1; state_start_time 0; prev_ns_lights 3'b000; prev_ew_lights 3'b000; Apply reset 100 reset 0; Run simulation for specified time SIM_TIME; Report test results display("Test completed at time 0t", time); finish; end State transition checks always (posedge clk) begin Detect state changes if (north_south_lights ! prev_ns_lights east_west_lights ! prev_ew_lights) begin state_duration time - state_start_time; Report state duration if (state_start_time 0) begin display("State duration: 0t ns", state_duration); Check timing (adjusted for clock cycles) if (prev_ns_lights 3'b001 prev_ew_lights 3'b001) begin Green state should last approximately 1 second (50M cycles 20ns 1000000ns) if (state_duration 990000 state_duration 1010000) begin display("WARNING:"); display("Green state duration ( 0t ns) outside expected range", state_duration); end end if (prev_ns_lights 3'b010 prev_ew_lights 3'b010) begin Yellow state should last approximately 0.3 seconds (15M cycles 20ns 300000ns) if (state_duration 290000 state_duration 310000) begin display("WARNING:"); display("Yellow state duration ( 0t ns) outside expected range", state_duration); end end end Update state tracking state_start_time time; prev_ns_lights north_south_lights; prev_ew_lights east_west_lights; end end Check for invalid states always (posedge clk) begin Check for invalid light combinations if (north_south_lights 3'b000 east_west_lights 3'b000) begin display("ERROR: Invalid light state detected at time 0t", time); display("NS lights: b, EW lights: b", north_south_lights, east_west_lights); end Check for conflicting green signals if (ns_green ew_green) begin display("ERROR: Both directions have green light at time 0t!\n\n--- Segment 30 ---\nprev_ns_lights east_west_lights ! prev_ew_lights) begin state_duration time - state_start_time; Report state duration if (state_start_time 0) begin display("State duration: 0t ns", state_duration); Check timing (adjusted for clock cycles) if (prev_ns_lights 3'b001 prev_ew_lights 3'b001) begin Green state should last approximately 1 second (50M cycles 20ns 1000000ns) if (state_duration 990000 state_duration 1010000) begin display("WARNING:"); display("Green state duration ( 0t ns) outside expected range", state_duration); end end if (prev_ns_lights 3'b010 prev_ew_lights 3'b010) begin Yellow state should last approximately 0.3 seconds (15M cycles 20ns 300000ns) if (state_duration 290000 state_duration 310000) begin display("WARNING:"); display("Yellow state duration ( 0t ns) outside expected range", state_duration); end end end Update state tracking state_start_time time; prev_ns_lights north_south_lights; prev_ew_lights east_west_lights; end end Check for invalid states always (posedge clk) begin Check for invalid light combinations if (north_south_lights 3'b000 east_west_lights 3'b000) begin display("ERROR: Invalid light state detected at time 0t", time); display("NS lights: b, EW lights: b", north_south_lights, east_west_lights); end Check for conflicting green signals if (ns_green ew_green) begin display("ERROR: Both directions have green light at time 0t! ", time); end Check for conflicting yellow signals if (ns_yellow ew_yellow) begin display("ERROR: Both directions have yellow light at time 0t!\n\n--- Segment 31 ---\nprev_ew_lights) begin state_duration time - state_start_time; Report state duration if (state_start_time 0) begin display("State duration: 0t ns", state_duration); Check timing (adjusted for clock cycles) if (prev_ns_lights 3'b001 prev_ew_lights 3'b001) begin Green state should last approximately 1 second (50M cycles 20ns 1000000ns) if (state_duration 990000 state_duration 1010000) begin display("WARNING:"); display("Green state duration ( 0t ns) outside expected range", state_duration); end end if (prev_ns_lights 3'b010 prev_ew_lights 3'b010) begin Yellow state should last approximately 0.3 seconds (15M cycles 20ns 300000ns) if (state_duration 290000 state_duration 310000) begin display("WARNING:"); display("Yellow state duration ( 0t ns) outside expected range", state_duration); end end end Update state tracking state_start_time time; prev_ns_lights north_south_lights; prev_ew_lights east_west_lights; end end Check for invalid states always (posedge clk) begin Check for invalid light combinations if (north_south_lights 3'b000 east_west_lights 3'b000) begin display("ERROR: Invalid light state detected at time 0t", time); display("NS lights: b, EW lights: b", north_south_lights, east_west_lights); end Check for conflicting green signals if (ns_green ew_green) begin display("ERROR: Both directions have green light at time 0t! ", time); end Check for conflicting yellow signals if (ns_yellow ew_yellow) begin display("ERROR: Both directions have yellow light at time 0t! ", time); end Check that when one direction is green or yellow, the other is red if ((ns_green ns_yellow) !ew_red) begin display("ERROR: East-West should be RED when North-South is GREEN or YELLOW at time 0t", time); end if ((ew_green ew_yellow) !ns_red) begin display("ERROR: North-South should be RED when East-West is GREEN or YELLOW at time 0t", time); end end endmodule\n\n