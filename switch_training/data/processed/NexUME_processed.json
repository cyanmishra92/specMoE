{
  "source": "NexUME.pdf",
  "raw_length": 95162,
  "cleaned_length": 94385,
  "base_segments": 299,
  "augmented_segments": 598,
  "segments": [
    {
      "text": "Abstract \nThe deployment of Deep Neural Networks (DNNs) in energy-constrained envi- ronments, such as Energy Harvesting Wireless Sensor Networks (EH-WSNs), introduces significant challenges due to the intermittent nature of power availability. This study introduces  NExUME , a novel training methodology designed specifically for DNNs operating under such constraints. We propose a dynamic adjustment of training parameters—dropout rates and quantization levels—that adapt in real-time to the available energy, which varies in energy harvesting scenarios.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "We propose a dynamic adjustment of training parameters—dropout rates and quantization levels—that adapt in real-time to the available energy, which varies in energy harvesting scenarios. This approach utilizes a model that integrates the characteristics of the network architecture and the specific energy harvesting profile. It dynamically adjusts training strategies, such as the intensity and timing of dropout and quantization, based on predictions of energy availability.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "It dynamically adjusts training strategies, such as the intensity and timing of dropout and quantization, based on predictions of energy availability. This method not only conserves energy but also enhances the network’s adaptability, ensuring robust learning and inference capabilities even under stringent power constraints. Our results show a 6% to 22% improvement in accuracy over current methods, with an increase of less than 5% in computational overhead.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Our results show a 6% to 22% improvement in accuracy over current methods, with an increase of less than 5% in computational overhead. This paper details the development of the adaptive training framework, describes the integration of energy profiles with dropout and quantization adjustments, and presents a comprehensive evaluation using real- world data. Additionally, we introduce a novel dataset aimed at furthering the application of energy harvesting in computational settings.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Additionally, we introduce a novel dataset aimed at furthering the application of energy harvesting in computational settings. 1 Introduction \nThe increasing demand for ubiquitous, sustainable, and energy-efficient computing, combined with advancements in energy harvesting systems, has spurred significant research into battery-less devices (Gobieski et al., 2019; Resch et al., 2020; Mishra et al., 2021; Saffari et al., 2021; Afzal et al., 2022). Such platforms represent the future of the Internet of Things (IoT) and energy harvesting wireless sensor networks (EH-WSNs).",
      "type": "sliding_window",
      "tokens": 150
    },
    {
      "text": "Such platforms represent the future of the Internet of Things (IoT) and energy harvesting wireless sensor networks (EH-WSNs). Equipped with modern machine learning (ML) techniques, these devices can revolutionize computing, monitoring, and analytics in remote, risky, and critical environments such as oil wells, mines, deep forests, oceans, remote industries, and smart cities. However, the intermittent and limited energy income of these deployments demands optimizations for ML applications at the algorithm (Yang et al., 2017; Shen et al., 2022; Mendis et al., 2021), orchestration (Maeng & Lucia, 2018; Mishra et al., 2021), compilation (Gobieski et al., 2018), and hardware development (Qiu et al., 2020; Islam et al., 2022; Mishra et al., 2024) layers.",
      "type": "sliding_window",
      "tokens": 219
    },
    {
      "text": "However, the intermittent and limited energy income of these deployments demands optimizations for ML applications at the algorithm (Yang et al., 2017; Shen et al., 2022; Mendis et al., 2021), orchestration (Maeng & Lucia, 2018; Mishra et al., 2021), compilation (Gobieski et al., 2018), and hardware development (Qiu et al., 2020; Islam et al., 2022; Mishra et al., 2024) layers. Despite these advancements, achieving consistent and accurate inference—thereby meeting service level objectives (SLOs)—in such intermittent environments remains a significant challenge, exacerbated by unpredictable resources, form-factor limitations, and variable computational availability, particularly when employing task-optimized deep neural networks (DNNs). There are two major problems with performing DNN inference under intermittent power.",
      "type": "sliding_window",
      "tokens": 220
    },
    {
      "text": "There are two major problems with performing DNN inference under intermittent power. (I) Energy Variability : Even though DNNs can be tailored to match the average energy income of the energy harvesting (EH) source through pruning, quantization, distillation, or network architecture search \nPreprint. Under review.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "Under review. arXiv:2408.13696v2  [cs.LG]  26 Jan 2025 \n(NAS) (Yang et al., 2018, 2017; Mendis et al., 2021), there is no guarantee that the energy income con- sistently meets or exceeds this average. When the income falls below the threshold, the system halts the inference and checkpoints the intermediate states (via software or persistent hardware) (Maeng & Lucia, 2018; Qiu et al., 2020), resuming upon energy recovery.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "When the income falls below the threshold, the system halts the inference and checkpoints the intermediate states (via software or persistent hardware) (Maeng & Lucia, 2018; Qiu et al., 2020), resuming upon energy recovery. Depending on the EH profile, this might lead to significant delays and SLO violations. (II) Computational Approximation : To address (I) and maintain continuous operation, EH-WSNs may skip some compute during energy shortfalls by dropping neurons (zero padding) or by approximating computations (quantization).",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "(II) Computational Approximation : To address (I) and maintain continuous operation, EH-WSNs may skip some compute during energy shortfalls by dropping neurons (zero padding) or by approximating computations (quantization). Adding further approximation to save energy atop an already heavily reduced network can propagate errors through the layers, leading to significant accuracy drops (Islam & Nirjon, 2019; Kang et al., 2022; Lv & Xu, 2022; Kang et al., 2020), further violating SLOs. In certain energy-critical scenarios, even EH-WSNs applying state-of-the-art techniques fail to consistently meet SLOs, sometimes skipping entire inferences to deliver results on time.",
      "type": "sliding_window",
      "tokens": 183
    },
    {
      "text": "In certain energy-critical scenarios, even EH-WSNs applying state-of-the-art techniques fail to consistently meet SLOs, sometimes skipping entire inferences to deliver results on time. Funda- mentally, while current DNNs can be trained or fine-tuned to fit within a given resource budget—be it compute, memory, or energy—they are  not  trained to expect a variable or intermittent resource income. Although intermittency-aware NAS (Mendis et al., 2021), could alleviate certain problems, they often assume fixed resource constraints and do not account for real-time energy fluctuations.",
      "type": "sliding_window",
      "tokens": 142
    },
    {
      "text": "Although intermittency-aware NAS (Mendis et al., 2021), could alleviate certain problems, they often assume fixed resource constraints and do not account for real-time energy fluctuations. Moreover, existing works like Keep in Balance (Yen et al., 2023), Stateful Neural Networks (Yen et al., 2022), ePerceptive (Montanari et al., 2020), and Zygarde (Islam & Nirjon, 2019) address aspects of intermittent computing but do not integrate energy variability awareness directly into the training and inference processes to enable dynamic adaptation. This calls for revisiting the entire training process; we need to train the DNN in such a way that it is aware of the intermittency and adapts  to it.",
      "type": "sliding_window",
      "tokens": 182
    },
    {
      "text": "This calls for revisiting the entire training process; we need to train the DNN in such a way that it is aware of the intermittency and adapts  to it. Motivated by these challenges, we propose  NExUME  ( N eural  Ex ecution  U nder Inter M ittent E nvironments), a novel framework designed specifically for environments with intermittent power and EH-WSNs, with potential applications in any ultra-low-power inference system. NExUME uniquely integrates energy variability awareness directly into both the training ( DynFit ) and inference ( DynInfer ) processes, enabling DNNs to dynamically adapt computations based on real-time energy availability.",
      "type": "sliding_window",
      "tokens": 163
    },
    {
      "text": "NExUME uniquely integrates energy variability awareness directly into both the training ( DynFit ) and inference ( DynInfer ) processes, enabling DNNs to dynamically adapt computations based on real-time energy availability. This involves an innovative strategy of learning instantaneous energy-aware dynamic dropout and quantization selection during training, and an intermittency-aware task scheduler during inference. The method includes targeted fine-tuning that not only regularizes the model but also pre- vents overfitting, enhancing robustness to fluctuations in resource availability.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "The method includes targeted fine-tuning that not only regularizes the model but also pre- vents overfitting, enhancing robustness to fluctuations in resource availability. Our key contributions can be summarized as follows: \n•  DynFit : A novel training optimizer that embeds energy variability awareness directly into the DNN training process. This optimizer allows for dynamic adjustments of dropout rates and quantization levels based on real-time energy availability, thus maintaining learning stability and improving model accuracy under power constraints.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "This optimizer allows for dynamic adjustments of dropout rates and quantization levels based on real-time energy availability, thus maintaining learning stability and improving model accuracy under power constraints. •  DynInfer : An intermittency- and platform-aware task scheduler that optimizes computational tasks for intermittent power supply, ensuring consistent and reliable DNN operation. DynInfer leverages software-compiler-hardware co-design to manage and deploy tasks.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "DynInfer leverages software-compiler-hardware co-design to manage and deploy tasks. With the help of DynFit, DynInfer provides  6% – 22%  accuracy improvements with  ≤ 5%  additional compute over existing methods. •  Dataset : A first-of-its-kind machine status monitoring dataset, involving multiple types of EH sensors mounted at various locations on a Bridgeport machine to monitor its activity status, facilitating research in predictive maintenance and Industry 4.0 applications.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "•  Dataset : A first-of-its-kind machine status monitoring dataset, involving multiple types of EH sensors mounted at various locations on a Bridgeport machine to monitor its activity status, facilitating research in predictive maintenance and Industry 4.0 applications. 2 Background and Related Work \nEnergy Harvesting and Intermittent Computing:  The exploding usage of IoTs, connected devices, and wearable electronics project the number of battery operated devices to be 24.1 Billion by 2030 (Insights, 2023). This has a significant economic (users, products and data generating dollar value) as well as environmental (battery and e-waste) impact (Mishra et al., 2024).",
      "type": "sliding_window",
      "tokens": 160
    },
    {
      "text": "This has a significant economic (users, products and data generating dollar value) as well as environmental (battery and e-waste) impact (Mishra et al., 2024). In fact, advances in EH has lead to a staggering development in intermittently powered battery-free devices (Maeng & Lucia, 2018; Gobieski et al., 2019; Qiu et al., 2020; Saffari et al., 2021; Afzal et al., 2022). A typical EH setup consists of 5 components, namely, energy capture (solar panel, thermocouple, etc), power conditioning, voltage regulation (buck or boost converter), energy storage (super capacitor) and compute unit (refer §Appendix B for details about each of them).",
      "type": "sliding_window",
      "tokens": 192
    },
    {
      "text": "A typical EH setup consists of 5 components, namely, energy capture (solar panel, thermocouple, etc), power conditioning, voltage regulation (buck or boost converter), energy storage (super capacitor) and compute unit (refer §Appendix B for details about each of them). To cater towards the sporadic \n2 \npower income and failures, an existing body of works explores algorithms, orchestration, compiler support, and hardware development (Yang et al., 2017, 2018; Mendis et al., 2021; Maeng & Lucia, 2018; Gobieski et al., 2018; Qiu et al., 2020; Islam et al., 2022; Mishra et al., 2024, 2021; Ma et al., 2016, 2017; Liu et al., 2015). Most of these works rely on software checkpointing (static and dynamic (Maeng & Lucia, 2018), refer §Appendix C) to save and restore, while some of the prior works developed nonvolatile hardware (Ma et al., 2016, 2017) which inherently takes care of the checkpointing.",
      "type": "sliding_window",
      "tokens": 270
    },
    {
      "text": "Most of these works rely on software checkpointing (static and dynamic (Maeng & Lucia, 2018), refer §Appendix C) to save and restore, while some of the prior works developed nonvolatile hardware (Ma et al., 2016, 2017) which inherently takes care of the checkpointing. Considering the scope of these initiatives, it is crucial to acknowledge that, despite the substantial support for energy harvesting and intermittency management, developing intermittency-aware applications and hardware necessitates multi-dimensional efforts that span from theoretical foundations to circuit design. Intermittent DNN Execution/Training:  As the applications deployed on such EH devices demand analytics, executing DNNs on EH devices and EH-WSNs have become prominent (Lv & Xu, 2022; Gobieski et al., 2019; Qiu et al., 2020; Mishra et al., 2021).",
      "type": "sliding_window",
      "tokens": 226
    },
    {
      "text": "Intermittent DNN Execution/Training:  As the applications deployed on such EH devices demand analytics, executing DNNs on EH devices and EH-WSNs have become prominent (Lv & Xu, 2022; Gobieski et al., 2019; Qiu et al., 2020; Mishra et al., 2021). However, due to computational constraints, limited memory capacity and restricted operating frequencies, many of these applications fail to complete inference execution with satisfactory SLOs, despite comprehensive software and hardware support (Mishra et al., 2021). While the works relying on loop-decomposition or task partition (e.g., see (Qiu et al., 2020; Gobieski et al., 2019) and the references therein) ensure “forward progress”, they do not guarantee an inference completion while meeting SLOs.",
      "type": "sliding_window",
      "tokens": 214
    },
    {
      "text": "While the works relying on loop-decomposition or task partition (e.g., see (Qiu et al., 2020; Gobieski et al., 2019) and the references therein) ensure “forward progress”, they do not guarantee an inference completion while meeting SLOs. Optimizing DNNs for the energy constraints (Yang et al., 2018, 2017), or performing early exit and depth-first slicing (Lv & Xu, 2022; Islam & Nirjon, 2019) does ensure more forward progress, but such approaches compromise accuracy while often imposing scheduling overheads and higher memory footprint. One major issue is, most of the works leverage “pre-existing” DNNs, which are typically designed for running on a stable resource environment, while being deployed on an intermittent environment with pseudo notion of stability via check-pointing, and therefore, one direction of works (Mendis et al., 2021) looks for performing network architecture search for intermittent devices.",
      "type": "sliding_window",
      "tokens": 228
    },
    {
      "text": "One major issue is, most of the works leverage “pre-existing” DNNs, which are typically designed for running on a stable resource environment, while being deployed on an intermittent environment with pseudo notion of stability via check-pointing, and therefore, one direction of works (Mendis et al., 2021) looks for performing network architecture search for intermittent devices. However, this research direction only accounts for fixed lower and upper bounds of energy and compute capacities, overlooking the “sporadic” nature of energy availability and the elasticity of the compute hardware (i.e., the ability to dynamically scale frequency, compute, and memory). Moreover, while the DNN is designed to operate within a specific power window, it is  not  trained to adapt to these fluctuations.",
      "type": "sliding_window",
      "tokens": 168
    },
    {
      "text": "Moreover, while the DNN is designed to operate within a specific power window, it is  not  trained to adapt to these fluctuations. Consequently, during extended periods of energy scarcity, the system lacks mechanisms for computational approximation, such as dynamic dropouts (neuron skipping) and dynamic quantization. Essentially, the DNN is trained to manage within a static resource budget, ignoring the “dynamism” of the resources .",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Essentially, the DNN is trained to manage within a static resource budget, ignoring the “dynamism” of the resources . In contrast, our work prioritizes the integration of this dynamism in both the network architecture search (NAS) and the training phases, adapting more effectively to fluctuating energy and compute conditions. 3 NExUME Framework \nTo address the issues with  intermittency-aware  DNN training and inference, we propose NExUME: ( N eural  Ex ecution  U nder Inter M ittent  E nvironments).",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "3 NExUME Framework \nTo address the issues with  intermittency-aware  DNN training and inference, we propose NExUME: ( N eural  Ex ecution  U nder Inter M ittent  E nvironments). NExUME has three interrelated compo- nents: (1)  DynNAS : Intermittency- and platform-aware neural architecture search; (2)  DynFit : Intermittency- and platform-aware DNN training with dynamic dropouts and quantization; and (3) DynInfer : Intermittency- and platform-aware task scheduling for inference. While each component can individually optimize DNNs for intermittent environments, their combination yields the best results.",
      "type": "sliding_window",
      "tokens": 168
    },
    {
      "text": "While each component can individually optimize DNNs for intermittent environments, their combination yields the best results. Our innovation lies in the integration of energy variability awareness directly into both the training and inference processes, enabling dynamic adaptation to real-time energy conditions, which is not addressed by existing methods (Mendis et al., 2021; Yen et al., 2023, 2022; Montanari et al., 2020; Islam & Nirjon, 2019). To search for the best architecture for the given intermittent environ- ment, DynNAS utilizes the approach proposed by iNAS (Mendis et al., 2021).",
      "type": "sliding_window",
      "tokens": 148
    },
    {
      "text": "To search for the best architecture for the given intermittent environ- ment, DynNAS utilizes the approach proposed by iNAS (Mendis et al., 2021). After the network architecture is determined, DynFit is used to train the network considering energy intermittency, and DynInfer is employed to perform inference under intermittent power conditions. In this section, we elaborate on the key components, focusing on DynFit and DynInfer, and explain how they uniquely adapt DNN training and inference to intermittent power conditions.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "In this section, we elaborate on the key components, focusing on DynFit and DynInfer, and explain how they uniquely adapt DNN training and inference to intermittent power conditions. 3.1 DynFit: Intermittency-Aware Learning \nDynFit  is designed to optimize deep neural networks (DNNs) for execution in environments char- acterized by intermittent power supply due to energy harvesting. The primary goal of DynFit is to \n3 \nadapt the DNN’s training process to operate efficiently under unpredictable energy budgets while maintaining acceptable accuracy and adhering to predefined service level objectives (SLOs).",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "The primary goal of DynFit is to \n3 \nadapt the DNN’s training process to operate efficiently under unpredictable energy budgets while maintaining acceptable accuracy and adhering to predefined service level objectives (SLOs). DynFit introduces key mechanisms to dynamically adjust computational complexity based on energy availability, thereby enabling energy-efficient execution of DNN models in constrained environments. These mechanisms include: (i)  Dynamic Dropout , which adjusts the dropout rates based on available energy to reduce computational load; (ii)  Dynamic Quantization , which modifies quantization levels in response to energy constraints to save energy; and (iii)  QuantaTask  design, which defines atomic computational units that can be executed without interruption given the energy budget.",
      "type": "sliding_window",
      "tokens": 170
    },
    {
      "text": "These mechanisms include: (i)  Dynamic Dropout , which adjusts the dropout rates based on available energy to reduce computational load; (ii)  Dynamic Quantization , which modifies quantization levels in response to energy constraints to save energy; and (iii)  QuantaTask  design, which defines atomic computational units that can be executed without interruption given the energy budget. Unlike standard implementations where dropout rates and quantization levels are fixed or adjusted solely based on training dynamics, DynFit adjusts these parameters in real-time based on the energy profile of the device. Specifically, during training, we simulate energy variability by incorporating energy traces into the training loop.",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "Specifically, during training, we simulate energy variability by incorporating energy traces into the training loop. At each training iteration, the available energy  E b  is sampled from these traces. Based on  E b , we adjust the dropout rate  d i  for each layer  i  according to: \nd i  =  d max \n\u0012 1  − E b \nE max \n\u0013 , (1) \nwhere  d max  is the maximum allowable dropout rate, and  E max  is the maximum energy observed in the traces.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "Based on  E b , we adjust the dropout rate  d i  for each layer  i  according to: \nd i  =  d max \n\u0012 1  − E b \nE max \n\u0013 , (1) \nwhere  d max  is the maximum allowable dropout rate, and  E max  is the maximum energy observed in the traces. Similarly, the quantization levels  q j  are adjusted: \nq j  =  q min  + ( q max  − q min )   E b \nE max . (2) \nThis ensures that when energy is low, higher dropout rates and lower quantization bit-widths are used to reduce computational load, and vice versa.",
      "type": "sliding_window",
      "tokens": 147
    },
    {
      "text": "(2) \nThis ensures that when energy is low, higher dropout rates and lower quantization bit-widths are used to reduce computational load, and vice versa. Modeling Energy Consumption:  The energy consumption of DNN operations is modeled based on empirical profiling data from the hardware platform. Let  e op  denote the energy consumed per computational operation, which varies with operation type and data precision.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Let  e op  denote the energy consumed per computational operation, which varies with operation type and data precision. The total energy consumption of a QuantaTask  q  is modeled as  E q  =  e op  ×  ℓ q , where  ℓ q  is the number of operations in the task. By integrating the energy model into the training process, DynFit ensures the adjustments to dropout and quantization directly correspond to actual energy savings on the target hardware.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "By integrating the energy model into the training process, DynFit ensures the adjustments to dropout and quantization directly correspond to actual energy savings on the target hardware. A  QuantaTask  is defined as the smallest atomic unit of computation that can be executed entirely without interruption under the current energy and hardware constraints. Each QuantaTask ensures that execution proceeds without partial computation, which would otherwise lead to overhead from checkpointing and potential data corruption.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Each QuantaTask ensures that execution proceeds without partial computation, which would otherwise lead to overhead from checkpointing and potential data corruption. The main properties of QuantaTasks are atomicity and respect for energy constraints. Figure 1 illustrates QuantaTask execution with a simple example.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "Figure 1 illustrates QuantaTask execution with a simple example. A3 B3 \nB2 \nB1 \nX X X \nA2 \nA1 \nX \nFigure 1: An example of variable QuantaTask in a matrix multiplication scenario. Depending on the available energy, the task (vector inner product) can be divided into multiple iterations such that each QuantaTask is guaranteed to finish given the energy availability.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Depending on the available energy, the task (vector inner product) can be divided into multiple iterations such that each QuantaTask is guaranteed to finish given the energy availability. E  is available energy, and  E b  is the energy required to finish one inner product. Optimization Variables, Constraints, and Objective Function:  The optimization problem is formulated with variables: the weights  W , dropout rates  d , quantization levels  q , and QuantaTask sizes  ℓ .",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "Optimization Variables, Constraints, and Objective Function:  The optimization problem is formulated with variables: the weights  W , dropout rates  d , quantization levels  q , and QuantaTask sizes  ℓ . The objective is to minimize the total loss, including prediction loss and regularization terms penalizing energy consumption (subject to energy constraints): \nmin W , d , q , ℓ L (   ˆ Y ,  Y ) +  λ 1 \nM X \nj =1 c q ( q j ) +  λ 2 \nN X \ni =1 c d ( d i ) . (3) \nFormulation of the Composite Optimization Problem:  The problem is non-convex due to the discrete nature of quantization levels and dropout rates.",
      "type": "sliding_window",
      "tokens": 188
    },
    {
      "text": "(3) \nFormulation of the Composite Optimization Problem:  The problem is non-convex due to the discrete nature of quantization levels and dropout rates. We employ an alternating optimization strategy, iteratively optimizing subsets of variables while keeping others fixed. Our method differs from standard approaches by integrating energy constraints directly into the optimization, ensuring that the network learns to adapt its parameters based on energy availability.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Our method differs from standard approaches by integrating energy constraints directly into the optimization, ensuring that the network learns to adapt its parameters based on energy availability. 4 \n3.1.1 Adaptive Regularization Strategy \nDynFit introduces an adaptive regularization strategy to address potential overfitting and under- training due to uneven weight updates caused by dynamic dropout and quantization. We monitor the update frequency  F p  of each weight  w p  over a window of  T  iterations: \nF p  =  1 \nT \nT X \nt =1 U p ( t ) , U p ( t ) = \u001a 1 , if  w p  is updated at iteration  t 0 , otherwise (4) \nWeights with  F p  < θ low  are considered under-trained, and those with  F p  > θ high  are considered overfitting.",
      "type": "sliding_window",
      "tokens": 194
    },
    {
      "text": "We monitor the update frequency  F p  of each weight  w p  over a window of  T  iterations: \nF p  =  1 \nT \nT X \nt =1 U p ( t ) , U p ( t ) = \u001a 1 , if  w p  is updated at iteration  t 0 , otherwise (4) \nWeights with  F p  < θ low  are considered under-trained, and those with  F p  > θ high  are considered overfitting. We adjust dropout rates and apply L2 regularization accordingly to balance the training process. This adaptive strategy ensures that all weights are adequately trained despite the dynamic adjustments.",
      "type": "sliding_window",
      "tokens": 152
    },
    {
      "text": "This adaptive strategy ensures that all weights are adequately trained despite the dynamic adjustments. Dropout scheduling techniques are incorporated, where dropout rates are increased or decreased over time based on the training progress and energy availability, mitigating potential overfitting introduced by static dropout variations. Complexity Analysis of DynFit:  The time complexity of DynFit during training is  O ( N  ·  T ) , where N  is the number of weights and  T  is the number of training iterations.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "Complexity Analysis of DynFit:  The time complexity of DynFit during training is  O ( N  ·  T ) , where N  is the number of weights and  T  is the number of training iterations. The overhead introduced by monitoring update frequencies and adjusting dropout rates is negligible compared to the overall training time, as these operations are simple arithmetic computations per iteration. The space complexity is  O ( N )  for storing the update frequencies and additional parameters.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "The space complexity is  O ( N )  for storing the update frequencies and additional parameters. Compared to classical training, DynFit adds minimal overhead, with a tradeoff of  ≤ 5%  additional compute for significant gains in accuracy under intermittent power conditions. 3.2 DynInfer: Intermittency-Aware Inference Scheduling \nDynInfer  optimizes the inference phase of DNNs operating under intermittent power conditions.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "3.2 DynInfer: Intermittency-Aware Inference Scheduling \nDynInfer  optimizes the inference phase of DNNs operating under intermittent power conditions. Unlike traditional systems with stable power, intermittent environments pose unique challenges for executing inference tasks efficiently and reliably. The inference process is represented as a set of tasks  T  =  { T 1 , T 2 , .",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "The inference process is represented as a set of tasks  T  =  { T 1 , T 2 , . . .",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": ". , T N } , where each task  T i  is characterized by its energy requirement  E i , execution time  τ i , priority  p i , deadline  D i , and criticality level  c i . At any given time  t , the available energy is denoted as  E b ( t ) .",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "At any given time  t , the available energy is denoted as  E b ( t ) . Task Fusion and Scheduling:  DynInfer introduces a novel task scheduling algorithm that dynam- ically adjusts to real-time energy availability. When the energy required for executing multiple QuantaTasks exceeds the available energy budget, DynInfer employs  task fusion  to combine smaller tasks into larger atomic units that can be executed within the energy constraints.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "When the energy required for executing multiple QuantaTasks exceeds the available energy budget, DynInfer employs  task fusion  to combine smaller tasks into larger atomic units that can be executed within the energy constraints. Formal Definition of Task Fusion:  Let  Q  =  { q 1 , q 2 , . .",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": ". . , q k }  be a set of QuantaTasks with in- dividual energy requirements  E q i .",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": ", q k }  be a set of QuantaTasks with in- dividual energy requirements  E q i . If   P \ni   E q i   ≤ E b , the available energy budget, then tasks can be executed sequentially without interruption. However, if   P \ni   E q i   > E b , we aim to fuse tasks to minimize checkpointing overhead.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "However, if   P \ni   E q i   > E b , we aim to fuse tasks to minimize checkpointing overhead. Task fusion is formalized as finding a partition of  Q  into subsets Q 1 ,  Q 2 , . .",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": ". . ,  Q m  such that, for each subset  Q j ,   P q i ∈Q j   E q i   ≤ E b , and  m  is minimized.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": ",  Q m  such that, for each subset  Q j ,   P q i ∈Q j   E q i   ≤ E b , and  m  is minimized. This reduces the number of checkpoints and the overhead associated with task switching. For example, Consider two convolution operations  C 1  and  C 2  with energy requirements  E C 1  and  E C 2 , respectively.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "For example, Consider two convolution operations  C 1  and  C 2  with energy requirements  E C 1  and  E C 2 , respectively. If individually  E C 1 , E C 2  > E b  but  E C 1  +  E C 2  ≤ E b , we fuse  C 1  and  C 2  into a single task. The fused task executes both convolutions atomically within the energy budget, avoiding the overhead of checkpointing between them.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "The fused task executes both convolutions atomically within the energy budget, avoiding the overhead of checkpointing between them. Scheduling Problem Formulation:  The scheduling problem is formulated with decision variables s i  (task start times) and binary variables  x i  ∈{ 0 ,  1 }  (indicating whether a task is scheduled). The energy availability constraint over time is expressed as (subject to energy and task constraints): P \ni : s i ≤ t<f i   E i  ≤ E b ( t )  The objective is to maximize the total weighted priority of scheduled tasks: \nmax { x i ,s i } \nN X \ni =1 \n\u0000 p i  − αE i  − β ( f i  − D i ) + \u0001 x i .",
      "type": "sliding_window",
      "tokens": 201
    },
    {
      "text": "The energy availability constraint over time is expressed as (subject to energy and task constraints): P \ni : s i ≤ t<f i   E i  ≤ E b ( t )  The objective is to maximize the total weighted priority of scheduled tasks: \nmax { x i ,s i } \nN X \ni =1 \n\u0000 p i  − αE i  − β ( f i  − D i ) + \u0001 x i . (5) \nScheduling Performance Assurance:  Our scheduling heuristic,  Energy-Aware Priority Scheduling , while sub-optimal in the theoretical sense, is designed to perform near-optimally in practice for real- time systems. We ensure its performance by: 1.",
      "type": "sliding_window",
      "tokens": 184
    },
    {
      "text": "We ensure its performance by: 1. Empirical Validation : We compare the heuristic’s \n5 \nperformance with the optimal solution on smaller problem instances using exhaustive search and find that the heuristic achieves within 95% of the optimal task completion rate. 2.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "2. Theoretical Analysis : The heuristic prioritizes tasks based on effective priority  P   eff i = p i E i   ×  ϕ i , where  ϕ i  accounts for deadline urgency. This balances task importance against energy consumption, leading to efficient utilization of available energy.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "This balances task importance against energy consumption, leading to efficient utilization of available energy. 3. Complexity Analysis : The heuristic has a time complexity of O ( N  log  N )  due to sorting tasks based on  P   eff i   , which is acceptable for real-time applications.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "Complexity Analysis : The heuristic has a time complexity of O ( N  log  N )  due to sorting tasks based on  P   eff i   , which is acceptable for real-time applications. Complexity Analysis of DynInfer:  The time complexity of the scheduling algorithm is  O ( N  log  N ) due to sorting tasks, and the space complexity is  O ( N )  for storing task parameters. Compared to classical inference, DynInfer introduces additional overhead for scheduling and task fusion, but this is offset by the gains in reliability and efficiency under intermittent power.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "Compared to classical inference, DynInfer introduces additional overhead for scheduling and task fusion, but this is offset by the gains in reliability and efficiency under intermittent power. Handling Extremely Low or Sporadic Energy Levels:  In environments with extremely low or sporadic energy levels where consistent dropout and quantization adjustments may not be feasible, NExUME handles this by: 1. Implementing a minimum viable model configuration that operates at the lowest acceptable energy consumption, achieved by maximizing dropout rates and using the lowest quantization bit-widths.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "Implementing a minimum viable model configuration that operates at the lowest acceptable energy consumption, achieved by maximizing dropout rates and using the lowest quantization bit-widths. 2. Prioritizing essential tasks and deferring non-critical computations.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "Prioritizing essential tasks and deferring non-critical computations. 3. Employing predictive energy harvesting models to anticipate energy availability and adjust computations proactively.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "Employing predictive energy harvesting models to anticipate energy availability and adjust computations proactively. In extreme cases, the system can enter into a low-power standby mode and resume operation when sufficient energy is available. These strategies ensure that the system remains operational and provides degraded but acceptable performance under severe energy constraints.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "These strategies ensure that the system remains operational and provides degraded but acceptable performance under severe energy constraints. Novelty in Energy-Aware Scheduling:  While energy-aware scheduling is not novel in itself, our contribution lies in adapting scheduling algorithms specifically for intermittent power environments. Existing scheduling algorithms typically assume stable energy availability and do not account for the atomicity constraints imposed by intermittent power supply.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Existing scheduling algorithms typically assume stable energy availability and do not account for the atomicity constraints imposed by intermittent power supply. Our scheduling approach uniquely integrates: 1. Real-time energy availability into scheduling decisions.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "Real-time energy availability into scheduling decisions. 2. Task fusion to minimize checkpointing overhead, which is critical in intermittent environments.",
      "type": "sliding_window",
      "tokens": 27
    },
    {
      "text": "Task fusion to minimize checkpointing overhead, which is critical in intermittent environments. 3. Dynamic adjustment of computational tasks based on both energy and task criticality.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "Dynamic adjustment of computational tasks based on both energy and task criticality. These innovations enable efficient and reliable DNN inference under intermittent power conditions, differentiating our work from existing energy-aware schedulers. Rationale Behind Method Design:  The overall method design of NExUME is motivated by the need to enable DNNs to function reliably in environments with intermittent and unpredictable energy supply.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Rationale Behind Method Design:  The overall method design of NExUME is motivated by the need to enable DNNs to function reliably in environments with intermittent and unpredictable energy supply. By integrating energy variability into both training and inference, we allow the DNN to adapt its computational load dynamically, ensuring that critical tasks are completed within energy constraints. This holistic approach addresses the limitations of existing methods that treat training and inference separately or do not account for real-time energy fluctuations.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "This holistic approach addresses the limitations of existing methods that treat training and inference separately or do not account for real-time energy fluctuations. Implementation Details: We design a full software-compiler-hardware co-designed execution framework for commercial devices with non-volatility support (like MSP- EXP430FR5994 with FeRAM). Figure 2 shows a detailed overview of our execution design.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "Recover(IS#)   Infer(RData) \nRdf4mFRAM(#3) \nret 2 //T=task_next ret -1 //pwr emgncy \nret 1 //Coreset ret 2 //T=task_next \nInt Pred_Pwr(task_next)    ...    ret 2 //T=task_next    ret 1 //save to coreset    ret 0 //backup    ret -1 //pwr emgncy \nPower  Emergency \nHardware Supported Monitoring, Backup and Restore \nP1 P2 L1 \nC1 C2 \nCn \nT1 T2 T3 \nLb \nLr \nL3 \nT4 \nT5 \nL3 \nFigure 2: Software-Compiler-Hardware Driven DynInfer Flow. To support user programs (  P1  ), we implement a moving window-based power predictor (  P2  ) which takes its input from the on-board EH capacitor. Considering the energy available, the predictor makes an informed decision on how to proceed.",
      "type": "sliding_window",
      "tokens": 247
    },
    {
      "text": "Considering the energy available, the predictor makes an informed decision on how to proceed. The compiler deconstructs the program into jobs to perform seamless program execution. These jobs form the functional program execution DAG.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "These jobs form the functional program execution DAG. For ex- ample, for a DNN execution, the jobs could be CONV2D (  C1  ), batch normalization (  C2  ), etc. However, certain jobs could be too big to execute atomically on harvested en- ergy.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "However, certain jobs could be too big to execute atomically on harvested en- ergy. Therefore, we profile the tasks using the compute platform (in this case using the MSP-EXP430FR5994 and the LEA in it) to further divide the jobs into Power Atomic Tasks (QuantaTasks). These QuantaTasks are carefully coded with optimized assembly language to maximize their efficiency.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "These QuantaTasks are carefully coded with optimized assembly language to maximize their efficiency. We take advantage of the on-board NV FeRAM to perform backup and restore in case of power emergencies. In case of a power emergency, the task is abandoned and a hardware-assisted backup and restore is performed.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "In case of a power emergency, the task is abandoned and a hardware-assisted backup and restore is performed. 6 \n4 Experimental Results \nNExUME can be seamlessly integrated as a “plug-in” for  both  training and inference frameworks in deep neural network (DNN) applications, specifically designed for intermittent and (ultra) low-power deployments. In this section, we discuss the effectiveness of NExUME across two distinct types of environments, highlighting its versatility and broad applicability.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "In this section, we discuss the effectiveness of NExUME across two distinct types of environments, highlighting its versatility and broad applicability. Firstly, we evaluate NExUME using publicly available datasets (§4.2) commonly utilized in embedded applications across multiple modalities—including image, time series sensor, and audio data. These datasets represent typical use cases in embedded systems where energy efficiency and minimal computational overhead are crucial.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "These datasets represent typical use cases in embedded systems where energy efficiency and minimal computational overhead are crucial. We use both commercial-off-the-shelf (COTS) hardware and state-of-the-art ReRAM Xbar- based hardware for this evaluation. Secondly, we introduce a novel dataset aimed at advancing research in predictive maintenance and Industry 4.0 (Lasi et al., 2014), and test NExUME on a real manufacturing testbed (§4.3) with COTS hardware.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "Secondly, we introduce a novel dataset aimed at advancing research in predictive maintenance and Industry 4.0 (Lasi et al., 2014), and test NExUME on a real manufacturing testbed (§4.3) with COTS hardware. We have developed a first-of-its-kind machine status monitoring dataset, available at  https://hackmd.io/@Galben/rk7YN6jmR , which involves mounting multiple types of sensors at various locations on a Bridgeport machine to monitor its activity status. 4.1 Development and Profiling of NExUME \nNExUME uses a combination of programming languages and technologies to optimize its functional- ity in intermittent and low-power computing environments.",
      "type": "sliding_window",
      "tokens": 164
    },
    {
      "text": "4.1 Development and Profiling of NExUME \nNExUME uses a combination of programming languages and technologies to optimize its functional- ity in intermittent and low-power computing environments. The software stack comprises Python3 (2.7k lines of code), CUDA (1.1k lines of code), and Embedded C (2.1k lines of code, not including DSP libraries). Our training infrastructure utilizes NVIDIA A6000 GPUs with 48 GiB of memory, supported by a 24-core Intel Xeon Gold 6336Y CPU.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "Our training infrastructure utilizes NVIDIA A6000 GPUs with 48 GiB of memory, supported by a 24-core Intel Xeon Gold 6336Y CPU. We employ PyTorch v2.3.0 coupled with CUDA version 11.8 as our primary training framework. To assess the computational overhead introduced by DynFit, a component of NExUME, we use NVIDIA Nsight Compute.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "To assess the computational overhead introduced by DynFit, a component of NExUME, we use NVIDIA Nsight Compute. During the training sessions enhanced by DynFit, we observed an increase in the number of instructions ranging from a minimum of 11.4% to a maximum of 34.2%. While the overhead in streaming multi-processor (SM) utilization was marginal (within 5%), there was a noticeable increase in memory bandwidth usage, ranging from 6% to 17%.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "While the overhead in streaming multi-processor (SM) utilization was marginal (within 5%), there was a noticeable increase in memory bandwidth usage, ranging from 6% to 17%. Moreover, we have implemented a modified version of the matrix multiplication operation that strategically skips the loading of rows and/or columns from the input matrices into the GPU’s shared memory and register files. This adaptation is guided by the dropout mask vector and the specific type of sparse matrix operation being performed.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "This adaptation is guided by the dropout mask vector and the specific type of sparse matrix operation being performed. This technique effectively reduces the number of load operations by an average of 12%, thereby enhancing the efficiency of computations under energy constraints and contributing to the overall performance improvements in NExUME. 4.2 NExUME on Publicly Available Datasets \nDatasets:  For image data, we consider the Fashion-MNIST (Xiao et al., 2017) and CIFAR10 (Alex, 2009) datasets; for time series sensor data, we focus on popular human activity recognition (HAR) datasets, MHEALTH (Banos et al., 2014) and PAMAP2 (Reiss & Stricker, 2012); and for audio, we use the AudioMNIST (Becker et al., 2023) dataset.",
      "type": "sliding_window",
      "tokens": 197
    },
    {
      "text": "4.2 NExUME on Publicly Available Datasets \nDatasets:  For image data, we consider the Fashion-MNIST (Xiao et al., 2017) and CIFAR10 (Alex, 2009) datasets; for time series sensor data, we focus on popular human activity recognition (HAR) datasets, MHEALTH (Banos et al., 2014) and PAMAP2 (Reiss & Stricker, 2012); and for audio, we use the AudioMNIST (Becker et al., 2023) dataset. Inference Deployment Embedded Platforms:  For commercially off-the-shelf micro-controllers, we choose Texas Instruments MSP430FR5994 (Instruments, 2024a), and Arduino Nano 33 BLE Sense (Arduino, 2024) as our deployment platforms with a Pixel-5 phone as the host device. The host device is used for data logging—collecting SLOs, violations, power failures, etc., along with running the “baseline” inferences without intermittency.",
      "type": "sliding_window",
      "tokens": 256
    },
    {
      "text": "The host device is used for data logging—collecting SLOs, violations, power failures, etc., along with running the “baseline” inferences without intermittency. Baselines:  We take the combination of best available approaches for DNN inference on intermittent environment as baselines. All these DNNs are executed with the state-of-the-art checkpointing and scheduling approach (Maeng & Lucia, 2018).",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "All these DNNs are executed with the state-of-the-art checkpointing and scheduling approach (Maeng & Lucia, 2018). Baseline  Full Power  is a DNN designed by iNAS (Mendis et al., 2021) for running while the system is battery-powered and has to hit a target SLO (latency < 500ms). Baseline  AP  is a DNN compressed to fit the average power of the energy harvesting (EH) environment using iNAS (Mendis et al., 2021) and energy-aware pruning (EAP) (Yang et al., 2017, 2018).",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "Baseline  AP  is a DNN compressed to fit the average power of the energy harvesting (EH) environment using iNAS (Mendis et al., 2021) and energy-aware pruning (EAP) (Yang et al., 2017, 2018). Baseline  PT  takes the  Full Power  DNN and uses techniques proposed by (Yang et al., 2018) and (Yang et al., 2017) to prune, quantize, and compress the model. Baseline  iNAS+PT designs the network from the ground up while combining the work of iNAS (Mendis et al., 2021) and EAP (Yang et al., 2018, 2017).",
      "type": "sliding_window",
      "tokens": 165
    },
    {
      "text": "Baseline  iNAS+PT designs the network from the ground up while combining the work of iNAS (Mendis et al., 2021) and EAP (Yang et al., 2018, 2017). 7 \nWe also compare our approach with recent state-of-the-art methods specifically designed for in- termittent systems, namely  Stateful  (Yen et al., 2022),  ePerceptive  (Montanari et al., 2020), and DynBal  (Yen et al., 2023). These methods introduce various techniques such as embedding state information into the DNN, multi-resolution inference, multi-exit architectures, and runtime reconfig- urability to handle intermittency in energy-harvesting devices.",
      "type": "sliding_window",
      "tokens": 185
    },
    {
      "text": "These methods introduce various techniques such as embedding state information into the DNN, multi-resolution inference, multi-exit architectures, and runtime reconfig- urability to handle intermittency in energy-harvesting devices. We have faithfully re-implemented these methods as per the descriptions and adjusted them for a fair comparison under our setup. Results:  Table 1 shows the accuracy of our approach against the baselines and the recent state-of-the- art methods using the TI MSP board powered by piezoelectric energy harvesting.",
      "type": "sliding_window",
      "tokens": 126
    },
    {
      "text": "Results:  Table 1 shows the accuracy of our approach against the baselines and the recent state-of-the- art methods using the TI MSP board powered by piezoelectric energy harvesting. The inferences meeting the SLO requirements are the only ones considered for accuracy; i.e., a correct classification violating the latency SLO is considered as “incorrect”. Datasets Full Power AP PT iNAS+PT Stateful ePerceptive DynBal NExUME \nFMNIST 98.70 71.90 79.72 83.68 85.40 86.25 87.50 88.90 CIFAR10 89.81 55.05 62.00 66.98 68.50 70.20 71.75 76.29 MHEALTH 89.62 59.76 65.40 71.56 73.80 74.95 76.10 80.75 PAMAP 87.30 57.38 65.77 70.33 72.20 73.35 74.50 75.16 AudioMNIST 88.20 67.29 73.16 75.41 76.80 77.95 78.60 80.01 Table 1: Accuracy comparison on TI MSP board using piezoelectric energy harvesting.",
      "type": "sliding_window",
      "tokens": 300
    },
    {
      "text": "Datasets Full Power AP PT iNAS+PT Stateful ePerceptive DynBal NExUME \nFMNIST 98.70 71.90 79.72 83.68 85.40 86.25 87.50 88.90 CIFAR10 89.81 55.05 62.00 66.98 68.50 70.20 71.75 76.29 MHEALTH 89.62 59.76 65.40 71.56 73.80 74.95 76.10 80.75 PAMAP 87.30 57.38 65.77 70.33 72.20 73.35 74.50 75.16 AudioMNIST 88.20 67.29 73.16 75.41 76.80 77.95 78.60 80.01 Table 1: Accuracy comparison on TI MSP board using piezoelectric energy harvesting. As observed in Table 1, NExUME consistently outperforms the state-of-the-art methods across all datasets. For instance, on CIFAR10, NExUME achieves an accuracy of 76.29%, which is approximately 4.54% higher than DynBal, the next best method.",
      "type": "sliding_window",
      "tokens": 285
    },
    {
      "text": "For instance, on CIFAR10, NExUME achieves an accuracy of 76.29%, which is approximately 4.54% higher than DynBal, the next best method. This improvement is significant in the context of energy-harvesting intermittent systems, where achieving high accuracy under strict energy constraints is challenging. The superior performance of NExUME can be attributed to its unique integration of energy variability awareness directly into both the training (DynFit) and inference (DynInfer) processes.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "The superior performance of NExUME can be attributed to its unique integration of energy variability awareness directly into both the training (DynFit) and inference (DynInfer) processes. Unlike other methods that either focus on modifying the DNN architecture or optimizing inference configurations, NExUME adapts the DNN’s computational complexity in real-time based on instantaneous energy availability, leading to more efficient use of scarce energy resources and improved accuracy. Dataset Platform Energy Source Stateful ePerceptive DynBal NExUME \nFMNIST MSP430FR5994 Piezoelectric 20.1 20.8 21.5 23.4 CIFAR10 Arduino Nano Thermal 16.0 16.5 17.0 18.5 MHEALTH ESP32 S3 Eye Piezoelectric 18.5 19.0 19.6 21.0 PAMAP STM32H7 Thermal 16.5 17.0 17.5 19.0 AudioMNIST Raspberry Pi Pico Piezoelectric 20.5 21.0 21.7 23.2 Table 2: Energy efficiency comparison on different hardware platforms.",
      "type": "sliding_window",
      "tokens": 232
    },
    {
      "text": "Dataset Platform Energy Source Stateful ePerceptive DynBal NExUME \nFMNIST MSP430FR5994 Piezoelectric 20.1 20.8 21.5 23.4 CIFAR10 Arduino Nano Thermal 16.0 16.5 17.0 18.5 MHEALTH ESP32 S3 Eye Piezoelectric 18.5 19.0 19.6 21.0 PAMAP STM32H7 Thermal 16.5 17.0 17.5 19.0 AudioMNIST Raspberry Pi Pico Piezoelectric 20.5 21.0 21.7 23.2 Table 2: Energy efficiency comparison on different hardware platforms. Table 2 presents the energy efficiency in MOps/Joule for each dataset on different hardware platforms using piezoelectric and thermal energy harvesting. NExUME achieves the highest energy efficiency across all platforms and datasets.",
      "type": "sliding_window",
      "tokens": 175
    },
    {
      "text": "NExUME achieves the highest energy efficiency across all platforms and datasets. This demonstrates that NExUME not only improves accuracy but also enhances energy utilization, making it highly suitable for deployment in energy-constrained intermittent environments. The improvements in energy efficiency are due to NExUME’s ability to adjust computational workload dynamically, minimizing energy wastage and ensuring that computa- tions are matched to the available energy budget.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "The improvements in energy efficiency are due to NExUME’s ability to adjust computational workload dynamically, minimizing energy wastage and ensuring that computa- tions are matched to the available energy budget. NExUME, thanks to its inherent learnt adaptability, significantly reduces saves, restores, reconfigurations and READ/WRITE from/to nonvolatile memory or to the flash memory in the cases and devices where NVMs are not present which gives it edge over the baselines across multiple devices. Discussion of Results:  1.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "Discussion of Results:  1. Dynamic Adaptation:  NExUME’s DynFit and DynInfer components enable real-time adjustments of dropout rates and quantization levels during training and inference based on instantaneous energy availability. This allows the DNN to maintain high accuracy even under severe energy constraints.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "This allows the DNN to maintain high accuracy even under severe energy constraints. 2. Energy Variability Awareness:  By integrating energy profiles directly into the training process, NExUME ensures that the model learns to handle fluctuations in energy supply, leading to more robust performance compared to methods that do not consider energy variability during training.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "Energy Variability Awareness:  By integrating energy profiles directly into the training process, NExUME ensures that the model learns to handle fluctuations in energy supply, leading to more robust performance compared to methods that do not consider energy variability during training. 3. Efficient Scheduling:  DynInfer’s energy-aware task scheduling and task fusion mechanisms reduce overhead from checkpointing and optimize the execution of tasks within \n8 \nthe available energy budget.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Efficient Scheduling:  DynInfer’s energy-aware task scheduling and task fusion mechanisms reduce overhead from checkpointing and optimize the execution of tasks within \n8 \nthe available energy budget. 4. Holistic Approach:  Unlike other methods that focus on either training or inference optimizations, NExUME provides a comprehensive solution that addresses both phases, leading to superior overall performance.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Holistic Approach:  Unlike other methods that focus on either training or inference optimizations, NExUME provides a comprehensive solution that addresses both phases, leading to superior overall performance. 4.3 NExUME on Machine Status Monitoring  [Our New Dataset] \nAutomation and monitoring and analytics are the key ingredients in the upcoming Industry 4.0. To enable sustainable machine status monitoring with energy harvesting (from machine vibrations or Wifi signals) we evaluate our setup using Bridgeport machines for monitoring their status.",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "To enable sustainable machine status monitoring with energy harvesting (from machine vibrations or Wifi signals) we evaluate our setup using Bridgeport machines for monitoring their status. Prior works (Center, 2018) majorly focused on fault analysis but there are little to no datasets on predictive maintenance. Setup and Sensor Arrangement:  Two different types of 3-axis accelerometers (with 100Hz and 200Hz sampling rate) were placed in three different locations of a Bridgeport machine to collect and analyze data under different operating status.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Setup and Sensor Arrangement:  Two different types of 3-axis accelerometers (with 100Hz and 200Hz sampling rate) were placed in three different locations of a Bridgeport machine to collect and analyze data under different operating status. There were 5 operating statuses: three different speeds of rotation of the spindle ( R1: 100RPM ,  R2: 200RPM ,  R3: 300RMP  with no job; RPM – rotations per minute), spindle under job ( SJ ), and spindle idle ( SI ). We collected over 700,000 samples over a period of 2 hours for each of the sensors.",
      "type": "sliding_window",
      "tokens": 143
    },
    {
      "text": "We collected over 700,000 samples over a period of 2 hours for each of the sensors. The sensor data were cleaned, normalized, and converted to the power spectrum density for further analysis. We use iNAS (Mendis et al., 2021) to find the DNNs meeting the energy income and train them using our proposed DynFit.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "We use iNAS (Mendis et al., 2021) to find the DNNs meeting the energy income and train them using our proposed DynFit. Table 3 shows the accuracy of classification tasks against the different baselines and state-of-the-art methods. Class Full Power AP PT iNAS+PT Stateful ePerceptive DynBal NExUME \nR1 84.93 74.46 77.02 79.62 80.85 81.50 82.15 83.60 R2 85.85 76.21 79.18 80.36 81.95 82.60 83.25 84.50 R3 81.09 72.43 75.38 78.18 79.05 79.70 80.35 80.85 SJ 90.95 82.33 85.00 87.58 88.60 89.15 89.80 90.50 SI 94.76 85.31 88.05 89.90 91.00 91.65 92.30 93.00 Table 3: Accuracy of NExUME and other methods for industry status monitoring dataset using TI MSP board and piezoelectric energy source.",
      "type": "sliding_window",
      "tokens": 278
    },
    {
      "text": "Class Full Power AP PT iNAS+PT Stateful ePerceptive DynBal NExUME \nR1 84.93 74.46 77.02 79.62 80.85 81.50 82.15 83.60 R2 85.85 76.21 79.18 80.36 81.95 82.60 83.25 84.50 R3 81.09 72.43 75.38 78.18 79.05 79.70 80.35 80.85 SJ 90.95 82.33 85.00 87.58 88.60 89.15 89.80 90.50 SI 94.76 85.31 88.05 89.90 91.00 91.65 92.30 93.00 Table 3: Accuracy of NExUME and other methods for industry status monitoring dataset using TI MSP board and piezoelectric energy source. Results collected over 200 experiment cycles. NExUME demonstrates superior performance across all operating classes, achieving the highest accuracy in each case.",
      "type": "sliding_window",
      "tokens": 245
    },
    {
      "text": "NExUME demonstrates superior performance across all operating classes, achieving the highest accuracy in each case. For example, for the spindle idle (SI) class, NExUME attains an accuracy of 93.00%, outperforming DynBal by 0.70%. While the margins may appear small, in industrial settings, even minor improvements in classification accuracy can have significant implications for predictive maintenance and operational efficiency.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "While the margins may appear small, in industrial settings, even minor improvements in classification accuracy can have significant implications for predictive maintenance and operational efficiency. The improved performance of NExUME in this real-world application further validates its effectiveness and practical utility. By effectively managing energy constraints and adapting to intermittent power conditions, NExUME enables more reliable and accurate monitoring in industrial environments where energy harvesting is a viable power solution.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "By effectively managing energy constraints and adapting to intermittent power conditions, NExUME enables more reliable and accurate monitoring in industrial environments where energy harvesting is a viable power solution. 4.4 Sensitivity and Ablation Studies of NExUME \nTo elucidate the influence of variable SLOs and hardware-specific settings on system performance, we conducted a comprehensive sensitivity study. This study involved adjusting the acceptable latency and the capacitance of the energy harvesting setup to assess their impacts on accuracy.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "This study involved adjusting the acceptable latency and the capacitance of the energy harvesting setup to assess their impacts on accuracy. As shown in Figure 3a, the accuracy improves with increased latency, but with diminishing returns. Similarly, Figure 3b demonstrates that, while increasing capacitance should theoretically stabilize the system, its charging characteristics can lead to extended charging times, thus exceeding the latency SLO.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Similarly, Figure 3b demonstrates that, while increasing capacitance should theoretically stabilize the system, its charging characteristics can lead to extended charging times, thus exceeding the latency SLO. Notably, some anomalies in the data were attributed to abrupt power failures, a common challenge in intermittent energy harvesting systems. An ablation study evaluates the contributions of individual components within NExUME.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "An ablation study evaluates the contributions of individual components within NExUME. The results, plotted in Figure 3c, indicate that the greatest improvements are derived from the “synergistic operation” of all components, particularly DynFit and DynInfer. Although iNAS enhances network selection, its lack of intermittency awareness significantly impacts accuracy.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "Although iNAS enhances network selection, its lack of intermittency awareness significantly impacts accuracy. 4.5 Limitations and Discussion \nWe recognize that modern architectures like Transformers have become prevalent in the ML commu- nity due to their superior performance on large-scale datasets. However, deploying such architectures on ultra-low-power, energy-harvesting devices presents significant challenges due to their substantial \n9 \n200 250 300 350 400 450 500 550 600 \nLatency (ms) \n78 80 82 84 86 88 90 92 94 \nAccuracy (%) \nAccuracy vs. Latency for Different Classes \nR1 R2 R3 SJ SI \n(a) Accuracy vs Latency \n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Capacitance (F) \n78 \n80 \n82 \n84 \n86 \n88 \n90 \n92 \nAccuracy (%) \nAccuracy vs. Capacitance for Different Classes \nR1 R2 R3 SJ SI \n(b) Accuracy vs Capacitance \nFMNIST CIFAR10 MHEALTH PAMAP AudioMNISTMachine Dataset \n0 \n20 \n40 \n60 \n80 \n100 \nAccuracy (%) \nAbalation Study \nDN DN+DF DN+DF+DI \n(c) Ablation Study Figure 3: Sensitivity and ablation study.",
      "type": "sliding_window",
      "tokens": 308
    },
    {
      "text": "However, deploying such architectures on ultra-low-power, energy-harvesting devices presents significant challenges due to their substantial \n9 \n200 250 300 350 400 450 500 550 600 \nLatency (ms) \n78 80 82 84 86 88 90 92 94 \nAccuracy (%) \nAccuracy vs. Latency for Different Classes \nR1 R2 R3 SJ SI \n(a) Accuracy vs Latency \n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Capacitance (F) \n78 \n80 \n82 \n84 \n86 \n88 \n90 \n92 \nAccuracy (%) \nAccuracy vs. Capacitance for Different Classes \nR1 R2 R3 SJ SI \n(b) Accuracy vs Capacitance \nFMNIST CIFAR10 MHEALTH PAMAP AudioMNISTMachine Dataset \n0 \n20 \n40 \n60 \n80 \n100 \nAccuracy (%) \nAbalation Study \nDN DN+DF DN+DF+DI \n(c) Ablation Study Figure 3: Sensitivity and ablation study. DN is DynNAS, DF is DynFit, and DI is DynInfer. computational and memory requirements.",
      "type": "sliding_window",
      "tokens": 273
    },
    {
      "text": "computational and memory requirements. NExUME focuses on enabling efficient and reliable deployment of DNNs in intermittent environments, which are often constrained in terms of com- putational resources and energy availability. In many real-world applications, especially in IoT and edge computing, there is a critical need for smaller, energy-efficient models that can operate autonomously without reliance on batteries.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "In many real-world applications, especially in IoT and edge computing, there is a critical need for smaller, energy-efficient models that can operate autonomously without reliance on batteries. These tiny, reusable devices contribute to reducing embodied carbon and represent a significant step toward sustainability. Moreover, we believe that advancing the capabilities of smaller models in intermittent environments is crucial for widespread adoption of sustainable, battery-free devices in various domains, including environmental monitoring, industrial IoT, and healthcare.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Moreover, we believe that advancing the capabilities of smaller models in intermittent environments is crucial for widespread adoption of sustainable, battery-free devices in various domains, including environmental monitoring, industrial IoT, and healthcare. By addressing the challenges of intermittent computing, our work contributes to the broader goal of enabling pervasive, sustainable intelligence at the edge. NExUME is especially advantageous in intermittent environments, and its utility extends to ultra- low-power or energy scavenging systems.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "NExUME is especially advantageous in intermittent environments, and its utility extends to ultra- low-power or energy scavenging systems. However, the efficacy of DynFit and iNAS is contingent upon the breadth and depth of the available dataset. Additionally, profiling devices to ascertain their energy consumption, computational capabilities, and memory footprint necessitates detailed micro- profiling using embedded programming.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Additionally, profiling devices to ascertain their energy consumption, computational capabilities, and memory footprint necessitates detailed micro- profiling using embedded programming. This process, while informative, yields only approximate models that are inherently prone to errors. DynFit, with its stochastic dropout features, occasionally leads to overfitting, necessitating meticulous fine-tuning.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "DynFit, with its stochastic dropout features, occasionally leads to overfitting, necessitating meticulous fine-tuning. While effective in smaller networks, our studies involving larger datasets (such as ImageNet) and more complex network architectures (like MobileNetV2 and ResNet) reveal challenges in achieving convergence without precise fine-tuning. DynFit tends to introduce multiple intermediate states during the training process, resulting in approximately 14% additional wall-time on average.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "DynFit tends to introduce multiple intermediate states during the training process, resulting in approximately 14% additional wall-time on average. The development of DynInfer requires an in-depth understanding of microcontroller programming and compiler directives. The absence of comprehensive library functions along with the need for computational efficiency frequently necessitates the development of in-line assembly code for certain computational kernels.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "The absence of comprehensive library functions along with the need for computational efficiency frequently necessitates the development of in-line assembly code for certain computational kernels. 5 Conclusions \nThis study presents NExUME, an advanced framework designed to optimize the training and inference phases of deep neural networks within the constraints of intermittently powered, energy-harvesting devices. By integrating adaptive neural architecture and energy-aware training techniques, NExUME significantly enhances the viability of deploying machine learning models in environments with limited and unreliable energy sources.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "By integrating adaptive neural architecture and energy-aware training techniques, NExUME significantly enhances the viability of deploying machine learning models in environments with limited and unreliable energy sources. The results from our extensive evaluations demonstrate that NExUME can substantially outperform traditional methods in energy-constrained settings, with improvements in accuracy and efficiency that facilitate real-world applications in remote and wearable technology. Specifically, improvements ranging from 6.10% to 17.13% over existing methods highlight NExUME’s capability to adapt dynamically to fluctuating energy conditions, ensuring both operational longevity and computational integrity.",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "Specifically, improvements ranging from 6.10% to 17.13% over existing methods highlight NExUME’s capability to adapt dynamically to fluctuating energy conditions, ensuring both operational longevity and computational integrity. The broader implication of this work extends beyond technological advancements, suggesting a paradigm shift in how the machine learning community approaches the design and deployment of systems in energy-limited environments. By prioritizing energy efficiency and system adaptability, NExUME contributes to the sustainability and accessibility of machine learning solutions, enabling their deployment in regions where power infrastructure is absent or unreliable.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "By prioritizing energy efficiency and system adaptability, NExUME contributes to the sustainability and accessibility of machine learning solutions, enabling their deployment in regions where power infrastructure is absent or unreliable. This is particularly crucial in developing regions where such technology can drive innovation in healthcare, agriculture, and education. Furthermore, the development of energy-efficient, adaptive systems like NExUME is aligned with the growing need for sustainable computing practices across all disciplines of technology.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Furthermore, the development of energy-efficient, adaptive systems like NExUME is aligned with the growing need for sustainable computing practices across all disciplines of technology. It challenges the machine learning community to consider not only the accuracy and efficiency of algorithms but also their environmental impact and accessibility, ensuring a broader positive social impact. 10 \nReferences \nSayed Saad Afzal, Waleed Akbar, Osvy Rodriguez, Mario Doumet, Unsoo Ha, Reza Ghaffarivar- davagh, and Fadel Adib.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "10 \nReferences \nSayed Saad Afzal, Waleed Akbar, Osvy Rodriguez, Mario Doumet, Unsoo Ha, Reza Ghaffarivar- davagh, and Fadel Adib. Battery-free wireless imaging of underwater environments. Nature communications , 13(1):5546, 2022.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "Nature communications , 13(1):5546, 2022. Krizhevsky Alex. Learning multiple layers of features from tiny images.",
      "type": "sliding_window",
      "tokens": 30
    },
    {
      "text": "Learning multiple layers of features from tiny images. https://www. cs.",
      "type": "sliding_window",
      "tokens": 18
    },
    {
      "text": "edu/kriz/learning-features-2009-TR. pdf , 2009. Arduino.",
      "type": "sliding_window",
      "tokens": 23
    },
    {
      "text": "Arduino. Arduino nano 33 ble sense with headers. https://store-usa.arduino.cc/products/ arduino-nano-33-ble-sense-with-headers , 2024.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "https://store-usa.arduino.cc/products/ arduino-nano-33-ble-sense-with-headers , 2024. Accessed on 05/19/2024. Oresti Banos, Rafael Garcia, Juan A Holgado-Terriza, Miguel Damas, Hector Pomares, Ignacio Rojas, Alejandro Saez, and Claudia Villalonga.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Oresti Banos, Rafael Garcia, Juan A Holgado-Terriza, Miguel Damas, Hector Pomares, Ignacio Rojas, Alejandro Saez, and Claudia Villalonga. mhealthdroid: a novel framework for agile development of mobile health applications. In  Ambient Assisted Living and Daily Activities: 6th International Work-Conference, IWAAL 2014, Belfast, UK, December 2-5, 2014.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "In  Ambient Assisted Living and Daily Activities: 6th International Work-Conference, IWAAL 2014, Belfast, UK, December 2-5, 2014. Proceedings 6 , pp. 91–98.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "91–98. Springer, 2014. Sören Becker, Johanna Vielhaben, Marcel Ackermann, Klaus-Robert Müller, Sebastian Lapuschkin, and Wojciech Samek.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "Sören Becker, Johanna Vielhaben, Marcel Ackermann, Klaus-Robert Müller, Sebastian Lapuschkin, and Wojciech Samek. Audiomnist: Exploring explainable artificial intelligence for audio analysis on a simple benchmark. Journal of the Franklin Institute , 2023.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Journal of the Franklin Institute , 2023. ISSN 0016-0032. doi: https: //doi.org/10.1016/j.jfranklin.2023.11.038. URL  https://www.sciencedirect.com/science/ article/pii/S0016003223007536 .",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "URL  https://www.sciencedirect.com/science/ article/pii/S0016003223007536 . Case Western Reserve University Bearing Data Center. Bearing fault data.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "Bearing fault data. https://engineering. case.edu/bearingdatacenter/download-data-file , 2018.",
      "type": "sliding_window",
      "tokens": 30
    },
    {
      "text": "case.edu/bearingdatacenter/download-data-file , 2018. Accessed: 2024-11-27. Graham Gobieski, Nathan Beckmann, and Brandon Lucia.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "Graham Gobieski, Nathan Beckmann, and Brandon Lucia. Intermittent deep neural network inference. In  SysML Conference , pp.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "In  SysML Conference , pp. 1–3, 2018. Graham Gobieski, Brandon Lucia, and Nathan Beckmann.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "Graham Gobieski, Brandon Lucia, and Nathan Beckmann. Intelligence beyond the edge: Inference on intermittent embedded systems. In  Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems , pp.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "In  Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems , pp. 199–213, 2019. Transforma Insights.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "Transforma Insights. Iot & ai market forecasts. https://transformainsights.com/research/ tam/market , 2023.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "https://transformainsights.com/research/ tam/market , 2023. Accessed: 05/19/2021. Texas Instruments.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "Texas Instruments. Msp430fr5994 mixed-signal microcontrollers. https://www.ti.com/ product/MSP430FR5994 , 2024a.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "https://www.ti.com/ product/MSP430FR5994 , 2024a. Accessed: 05/19/2024. Texas Instruments.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "Texas Instruments. Msp dsp library: Low energy accelerator (lea) user’s guide. https://software-dl.ti.com/msp430/msp430_public_sw/mcu/msp430/DSPLib/ 1_30_00_02/exports/html/usersguide_lea.html , 2024b.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "https://software-dl.ti.com/msp430/msp430_public_sw/mcu/msp430/DSPLib/ 1_30_00_02/exports/html/usersguide_lea.html , 2024b. Accessed: 05/19/2024. Bashima Islam and Shahriar Nirjon.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Bashima Islam and Shahriar Nirjon. Zygarde: Time-sensitive on-device deep inference and adaptation on intermittently-powered systems. arXiv preprint arXiv:1905.03854 , 2019.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "arXiv preprint arXiv:1905.03854 , 2019. Sahidul Islam, Jieren Deng, Shanglin Zhou, Chen Pan, Caiwen Ding, and Mimi Xie. Enabling fast deep learning on tiny energy-harvesting iot devices.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "Enabling fast deep learning on tiny energy-harvesting iot devices. In  2022 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pp. 921–926.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "921–926. IEEE, 2022. Chih-Kai Kang, Hashan Roshantha Mendis, Chun-Han Lin, Ming-Syan Chen, and Pi-Cheng Hsiu.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Chih-Kai Kang, Hashan Roshantha Mendis, Chun-Han Lin, Ming-Syan Chen, and Pi-Cheng Hsiu. Everything leaves footprints: Hardware accelerated intermittent deep inference. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 39(11):3479–3491, 2020.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 39(11):3479–3491, 2020. Chih-Kai Kang, Hashan Roshantha Mendis, Chun-Han Lin, Ming-Syan Chen, and Pi-Cheng Hsiu. More is less: Model augmentation for intermittent deep inference.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "More is less: Model augmentation for intermittent deep inference. ACM Transactions on Embedded Computing Systems (TECS) , 21(5):1–26, 2022. Heiner Lasi, Peter Fettke, Hans-Georg Kemper, Thomas Feld, and Michael Hoffmann.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "Heiner Lasi, Peter Fettke, Hans-Georg Kemper, Thomas Feld, and Michael Hoffmann. Industry 4.0. Business & information systems engineering , 6(4):239–242, 2014. 11 \nYongpan Liu, Zewei Li, Hehe Li, Yiqun Wang, Xueqing Li, Kaisheng Ma, Shuangchen Li, Meng-Fan Chang, Sampson John, Yuan Xie, et al.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "11 \nYongpan Liu, Zewei Li, Hehe Li, Yiqun Wang, Xueqing Li, Kaisheng Ma, Shuangchen Li, Meng-Fan Chang, Sampson John, Yuan Xie, et al. Ambient energy harvesting nonvolatile processors: From circuit to system. In  Proceedings of the 52nd Annual Design Automation Conference , pp.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "In  Proceedings of the 52nd Annual Design Automation Conference , pp. 1–6, 2015. Mingsong Lv and Enyu Xu.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "Mingsong Lv and Enyu Xu. Efficient dnn execution on intermittently-powered iot devices with depth-first inference. IEEE Access , 10:101999–102008, 2022.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "IEEE Access , 10:101999–102008, 2022. Kaisheng Ma, Xueqing Li, Karthik Swaminathan, Yang Zheng, Shuangchen Li, Yongpan Liu, Yuan Xie, John Jack Sampson, and Vijaykrishnan Narayanan. Nonvolatile processor architectures: Efficient, reliable progress with unstable power.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Nonvolatile processor architectures: Efficient, reliable progress with unstable power. IEEE Micro , 36(3):72–83, 2016. Kaisheng Ma, Xueqing Li, Jinyang Li, Yongpan Liu, Yuan Xie, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Kaisheng Ma, Xueqing Li, Jinyang Li, Yongpan Liu, Yuan Xie, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Incidental computing on iot nonvolatile processors. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture , pp.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture , pp. 204–218, 2017. Kiwan Maeng and Brandon Lucia.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "Kiwan Maeng and Brandon Lucia. Adaptive dynamic checkpointing for safe efficient intermittent computing. In  13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18) , pp.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "In  13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18) , pp. 129–144, 2018. Hashan Roshantha Mendis, Chih-Kai Kang, and Pi-cheng Hsiu.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Hashan Roshantha Mendis, Chih-Kai Kang, and Pi-cheng Hsiu. Intermittent-aware neural architecture search. ACM Transactions on Embedded Computing Systems (TECS) , 20(5s):1–27, 2021.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "ACM Transactions on Embedded Computing Systems (TECS) , 20(5s):1–27, 2021. Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks. In  2021 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pp. 1414–1419.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "1414–1419. IEEE, 2021. Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan, and Chita R Das.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan, and Chita R Das. Usas: A sustainable continuous-learning´ framework for edge servers. In  2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA) , pp.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "In  2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA) , pp. 891–907. IEEE, 2024.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "IEEE, 2024. Alessandro Montanari, Manuja Sharma, Dainius Jenkus, Mohammed Alloulah, Lorena Qendro, and Fahim Kawsar. eperceptive: energy reactive embedded intelligence for batteryless sensors.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "eperceptive: energy reactive embedded intelligence for batteryless sensors. In Proceedings of the 18th Conference on Embedded Networked Sensor Systems , pp. 382–394, 2020.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "382–394, 2020. Keni Qiu, Nicholas Jao, Mengying Zhao, Cyan Subhra Mishra, Gulsum Gudukbay, Sethu Jose, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent embedded processors.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent embedded processors. In  2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pp. 315–327.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "315–327. IEEE, 2020. Attila Reiss and Didier Stricker.",
      "type": "sliding_window",
      "tokens": 25
    },
    {
      "text": "Attila Reiss and Didier Stricker. Introducing a new benchmarked dataset for activity monitoring. In 2012 16th international symposium on wearable computers , pp.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "In 2012 16th international symposium on wearable computers , pp. 108–109. IEEE, 2012.",
      "type": "sliding_window",
      "tokens": 24
    },
    {
      "text": "IEEE, 2012. Salonik Resch, S Karen Khatamifard, Zamshed I Chowdhury, Masoud Zabihi, Zhengyang Zhao, Husrev Cilasun, Jian-Ping Wang, Sachin S Sapatnekar, and Ulya R Karpuzcu. Mouse: Inference in non-volatile memory for energy harvesting applications.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Mouse: Inference in non-volatile memory for energy harvesting applications. In  2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO) , pp. 400–414.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "400–414. IEEE, 2020. Ali Saffari, Sin Yong Tan, Mohamad Katanbaf, Homagni Saha, Joshua R Smith, and Soumik Sarkar.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Ali Saffari, Sin Yong Tan, Mohamad Katanbaf, Homagni Saha, Joshua R Smith, and Soumik Sarkar. Battery-free camera occupancy detection system. In  Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning , pp.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "In  Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning , pp. 13–18, 2021. Tianyi Shen, Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "Tianyi Shen, Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. An efficient edge-cloud partitioning of random forests for distributed sensor networks. IEEE Embedded Systems Letters , 2022.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "IEEE Embedded Systems Letters , 2022. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017. 12 \nTien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "12 \nTien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pp.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "In  Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 5687–5695, 2017. Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of the European conference on computer vision (ECCV) , pp.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "In Proceedings of the European conference on computer vision (ECCV) , pp. 285–300, 2018. Chih-Hsuan Yen, Hashan Roshantha Mendis, Tei-Wei Kuo, and Pi-Cheng Hsiu.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "Chih-Hsuan Yen, Hashan Roshantha Mendis, Tei-Wei Kuo, and Pi-Cheng Hsiu. Stateful neural networks for intermittent systems. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 41(11):4229–4240, 2022.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 41(11):4229–4240, 2022. Chih-Hsuan Yen, Hashan Roshantha Mendis, Tei-Wei Kuo, and Pi-Cheng Hsiu. Keep in balance: Runtime-reconfigurable intermittent deep inference.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "Keep in balance: Runtime-reconfigurable intermittent deep inference. ACM Transactions on Embedded Computing Systems , 22(5s):1–25, 2023. 13 \nA More Results on Other Platforms and EH Sources \nFigure 4: Hardware setup of NExUME using MSP-EXP430FR5994 as the edge compute, Adafruit ItsyBitsy nRF52840 Express for communicating, Energy Harvester Breakout - LTC3588 with super- capacitors as energy rectification and storage and a Pixel-5 phone as the host.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "13 \nA More Results on Other Platforms and EH Sources \nFigure 4: Hardware setup of NExUME using MSP-EXP430FR5994 as the edge compute, Adafruit ItsyBitsy nRF52840 Express for communicating, Energy Harvester Breakout - LTC3588 with super- capacitors as energy rectification and storage and a Pixel-5 phone as the host. Datasets Full Power MSP on Piezo AP PT iNAS+PT NExUME Better FMNIST 98.70 71.90 79.72 83.68 88.90 6.24% CIFAR10 89.81 55.05 62.00 66.98 76.29 13.90% MHEALTH 89.62 59.76 65.40 71.56 80.75 12.84% PAMAP 87.30 57.38 65.77 65.38 75.16 14.97% AudioMNIST 88.20 67.29 73.16 75.41 80.01 6.10% Table 4: Accuracy of NExUME on MSP board using vibration from a Piezoelectric harvestor. Better refers to the improvement over iNAS+PT baseline.",
      "type": "sliding_window",
      "tokens": 280
    },
    {
      "text": "Better refers to the improvement over iNAS+PT baseline. Datasets Full Power MSP on Thermal AP PT iNAS+PT NExUME Better FMNIST 98.70 80.92 86.32 88.93 95.62 7.53% CIFAR10 89.81 64.78 69.29 71.53 83.78 17.13% MHEALTH 89.62 69.77 73.99 77.70 89.62 15.34% PAMAP 87.30 66.33 71.84 74.47 85.24 14.46% AudioMNIST 88.20 73.84 78.03 81.60 87.64 7.40% Table 5: Accuracy of NExUME on MSP board using thermocouple based thermal harvester. Better refers to the improvement over iNAS+PT baseline.",
      "type": "sliding_window",
      "tokens": 204
    },
    {
      "text": "Better refers to the improvement over iNAS+PT baseline. Datasets Full Power Arduino on RF AP PT iNAS+PT NExUME Better FMNIST 98.70 74.44 79.63 83.61 90.44 8.17% CIFAR10 89.81 58.11 63.91 65.01 79.60 22.44% MHEALTH 89.62 63.52 67.40 74.30 83.86 12.87% PAMAP 87.30 61.39 67.24 69.45 77.00 10.87% AudioMNIST 88.20 66.11 74.28 76.60 78.87 2.97% Table 6: Accuracy of NExUME on Arduino nano board using WiFi based RF harvester. Better refers to the improvement over iNAS+PT baseline.",
      "type": "sliding_window",
      "tokens": 205
    },
    {
      "text": "Better refers to the improvement over iNAS+PT baseline. 14 \nDatasets Full Power Arduino on Thermal AP PT iNAS+PT NExUME Better FMNIST 98.70 77.04 80.44 83.08 89.90 8.20% CIFAR10 89.81 60.38 65.90 66.98 80.70 20.48% MHEALTH 89.62 65.74 69.88 72.41 85.75 18.42% PAMAP 87.30 62.76 65.93 71.46 81.27 13.73% AudioMNIST 88.20 69.12 73.86 77.79 83.54 7.39% Table 7: Accuracy of NExUME on Arduino nano board using thermocouple based thermal harvester. Better refers to the improvement over iNAS+PT baseline.",
      "type": "sliding_window",
      "tokens": 201
    },
    {
      "text": "Better refers to the improvement over iNAS+PT baseline. B Details on Energy Harvesting \nA typical energy harvesting (EH) setup captures and converts environmental energy into usable electrical power, which can then support various electronic devices. Here’s a simplified breakdown of the process: \n1.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "Here’s a simplified breakdown of the process: \n1. Energy Capture : The setup begins with a harvester, such as a solar panel, piezoelectric sensor, or thermocouple. These devices are designed to collect energy from their surround- ings—light, mechanical vibrations, or heat, respectively.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "These devices are designed to collect energy from their surround- ings—light, mechanical vibrations, or heat, respectively. 2. Power Conditioning : Once energy is harvested, it often needs to be converted and stabilized for use.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "Power Conditioning : Once energy is harvested, it often needs to be converted and stabilized for use. This is done using a rectifier, which transforms alternating current (AC) into a more usable direct current (DC). 3.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "3. Voltage Regulation : After rectification, the power might not be at the right voltage for the device it needs to support. A matching circuit, including components like buck or boost converters, adjusts the voltage to the appropriate level, ensuring the device receives the correct current and voltage.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "A matching circuit, including components like buck or boost converters, adjusts the voltage to the appropriate level, ensuring the device receives the correct current and voltage. 4. Energy Storage : Finally, to ensure a continuous power supply even when the immediate energy source is inconsistent (like when a cloud passes over a solar panel), the system includes a temporary storage unit, such as a super-capacitor.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Energy Storage : Finally, to ensure a continuous power supply even when the immediate energy source is inconsistent (like when a cloud passes over a solar panel), the system includes a temporary storage unit, such as a super-capacitor. This component helps smooth out the supply, providing steady power to the compute circuit. By integrating these components, an EH system can sustainably power devices without relying on traditional power grids, making it ideal for remote or mobile applications.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "By integrating these components, an EH system can sustainably power devices without relying on traditional power grids, making it ideal for remote or mobile applications. C Intermittent Computing and Check-pointing \nC.1 Intermittency-Aware General Matrix Multiplication (GeMM) \nHere we explain the operation of an energy-aware algorithm for performing General Matrix Multipli- cation (GeMM). The algorithm is designed to operate in environments where energy availability is intermittent, such as in devices powered by energy harvesting.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "The algorithm is designed to operate in environments where energy availability is intermittent, such as in devices powered by energy harvesting. It includes mechanisms for loop tiling, checkpointing, and resumption to manage computation across power interruptions effectively. C.1.1 Algorithm Overview \nThe GeMM operation, typically expressed as  C  =  A  ×  B , where  A ,  B , and  C  are matrices, is imple- mented with considerations for energy limitations.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "C.1.1 Algorithm Overview \nThe GeMM operation, typically expressed as  C  =  A  ×  B , where  A ,  B , and  C  are matrices, is imple- mented with considerations for energy limitations. The algorithm breaks the matrix multiplication into smaller chunks (tiles), periodically saves the state before potential power losses, and resumes computation from the last saved state upon power restoration. C.1.2 Function Definitions \n•  SAVE_STATE : Saves the current indices and the partial result of the output matrix  C  to non-volatile memory to allow recovery after a power interruption.",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "C.1.2 Function Definitions \n•  SAVE_STATE : Saves the current indices and the partial result of the output matrix  C  to non-volatile memory to allow recovery after a power interruption. •  LOAD_STATE : Retrieves the last saved indices and partial result from non-volatile memory to resume computation. 15 \nC.1.3 Loop Tiling \nThe algorithm uses loop tiling to divide the computation into smaller blocks that can be managed between power interruptions.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "15 \nC.1.3 Loop Tiling \nThe algorithm uses loop tiling to divide the computation into smaller blocks that can be managed between power interruptions. This tiling not only makes the computation manageable but also optimizes memory usage and cache performance, which is critical in constrained environments. C.1.4 Check-pointing Mechanism \nBefore each power interruption, detected through an energy monitoring system, the algorithm saves the current state using the  SAVE_STATE  function.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "C.1.4 Check-pointing Mechanism \nBefore each power interruption, detected through an energy monitoring system, the algorithm saves the current state using the  SAVE_STATE  function. This state includes the loop indices and the current value of the element being processed in  C . This ensures that no computation is lost when the power goes out.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "This ensures that no computation is lost when the power goes out. C.1.5 Resumption Mechanism \nUpon resuming, the algorithm loads the saved state using the  LOAD_STATE  function. This state is used to continue the computation exactly where it left off, minimizing redundant operations and ensuring efficiency.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "This state is used to continue the computation exactly where it left off, minimizing redundant operations and ensuring efficiency. D Formulation of Dynamic Dropouts: \nD.1 L2 Dynamic Dropout with QuantaTask Optimization \nL2 Dynamic Dropout leverages the L2 norm of the weights to influence dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation:  Let  W  be the weight matrix of a layer.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Mathematical Formulation:  Let  W  be the weight matrix of a layer. The L2 norm of the weights is calculated as: \n∥ W ∥ 2  = sX \ni,j W   2 ij \nDefine the dropout probability  p i  for neuron  i  based on the L2 norm of its corresponding weights. The idea is to use the inverse of the L2 norm to determine the probability: \np i  = α ∥ W i ∥ 2  +  ϵ \nwhere  α  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero.",
      "type": "sliding_window",
      "tokens": 145
    },
    {
      "text": "The idea is to use the inverse of the L2 norm to determine the probability: \np i  = α ∥ W i ∥ 2  +  ϵ \nwhere  α  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. Define a binary dropout mask  m  = [ m 1 , m 2 , . .",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with L2 Dynamic Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  α .",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with L2 Dynamic Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  α . Define the energy budget  E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l .",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "Initialize the loop iteration parameters  l . Compute the activations a  and apply the dropout mask: a dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. Calculate the gradients of the loss with respect to the weights: \n∂ L ∂W ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \n16 \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the L2 norm of the weights: \np i  = α ∥ W i ∥ 2  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise Perform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication.",
      "type": "sliding_window",
      "tokens": 360
    },
    {
      "text": "Calculate the gradients of the loss with respect to the weights: \n∂ L ∂W ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \n16 \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the L2 norm of the weights: \np i  = α ∥ W i ∥ 2  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise Perform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with L2 Dynamic Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window",
      "tokens": 336
    },
    {
      "text": "If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output  Y .",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "Perform the forward pass with the updated dropout mask to obtain the output  Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the L2 norm of the weights, along with the QuantaTask optimization to handle energy constraints. D.2 Optimal Brain Damage Dropout with QuantaTask Optimization \nOptimal Brain Damage Dropout leverages a simplified version of the Optimal Brain Damage pruning method to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "D.2 Optimal Brain Damage Dropout with QuantaTask Optimization \nOptimal Brain Damage Dropout leverages a simplified version of the Optimal Brain Damage pruning method to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation:  Let  W  be the weight matrix of a layer. The sensitivity of each weight W ij  is calculated using the second-order Taylor expansion of the loss function  L : \n∆ L ≈ 1 \n2 \nX \ni,j \n∂ 2 L ∂W   2 ij ( W ij ) 2 \nwhere ∂ 2 L ∂W   2 ij   is the second-order derivative (Hessian) of the loss with respect to the weights.",
      "type": "sliding_window",
      "tokens": 174
    },
    {
      "text": "The sensitivity of each weight W ij  is calculated using the second-order Taylor expansion of the loss function  L : \n∆ L ≈ 1 \n2 \nX \ni,j \n∂ 2 L ∂W   2 ij ( W ij ) 2 \nwhere ∂ 2 L ∂W   2 ij   is the second-order derivative (Hessian) of the loss with respect to the weights. Define the dropout probability  p i  for neuron  i  based on the sensitivity of its corresponding weights. The idea is to use the sensitivity to determine the probability: \np i  = β   P \nj ∂ 2 L ∂W   2 ij   ( W ij ) 2 \nmax \u0010P \nj ∂ 2 L ∂W   2 ij   ( W ij ) 2 \u0011 +  ϵ \nwhere  β  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero.",
      "type": "sliding_window",
      "tokens": 218
    },
    {
      "text": "The idea is to use the sensitivity to determine the probability: \np i  = β   P \nj ∂ 2 L ∂W   2 ij   ( W ij ) 2 \nmax \u0010P \nj ∂ 2 L ∂W   2 ij   ( W ij ) 2 \u0011 +  ϵ \nwhere  β  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. Define a binary dropout mask  m  = [ m 1 , m 2 , . .",
      "type": "sliding_window",
      "tokens": 125
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Optimal Brain Damage Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  β .",
      "type": "sliding_window",
      "tokens": 156
    },
    {
      "text": "Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Optimal Brain Damage Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  β . Define the energy budget  E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l .",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Initialize the loop iteration parameters  l . 17 \nCompute the activations  a  and apply the dropout mask: \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. Calculate the gradients and Hessians of the loss with respect to the weights: \n∂ L ∂W ij , ∂ 2 L ∂W   2 ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) If  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the sensitivities: \np i  = β   P j ∂ 2 L ∂W   2 ij   ( W ij ) 2 \nmax \u0010P j ∂ 2 L ∂W   2 ij   ( W ij ) 2 \u0011 +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication.",
      "type": "sliding_window",
      "tokens": 412
    },
    {
      "text": "Calculate the gradients and Hessians of the loss with respect to the weights: \n∂ L ∂W ij , ∂ 2 L ∂W   2 ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) If  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the sensitivities: \np i  = β   P j ∂ 2 L ∂W   2 ij   ( W ij ) 2 \nmax \u0010P j ∂ 2 L ∂W   2 ij   ( W ij ) 2 \u0011 +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with Optimal Brain Damage Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window",
      "tokens": 388
    },
    {
      "text": "If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output Y .",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "Perform the forward pass with the updated dropout mask to obtain the output Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the sensitivity of the weights, along with the QuantaTask optimization to handle energy constraints. D.3 Feature Map Reconstruction Error Dropout with QuantaTask Optimization \nFeature Map Reconstruction Error Dropout leverages the reconstruction error of feature maps to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems.",
      "type": "sliding_window",
      "tokens": 128
    },
    {
      "text": "D.3 Feature Map Reconstruction Error Dropout with QuantaTask Optimization \nFeature Map Reconstruction Error Dropout leverages the reconstruction error of feature maps to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation:  Let  W  be the weight matrix of a layer and  F  be the feature maps produced by the layer. The reconstruction error of a feature map  F i  is calculated as: \nRE i  =  ∥ F i  − ˆ F i ∥ 2 \nwhere   ˆ F i  is the reconstructed feature map, and  ∥· ∥ 2  denotes the L2 norm.",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "The reconstruction error of a feature map  F i  is calculated as: \nRE i  =  ∥ F i  − ˆ F i ∥ 2 \nwhere   ˆ F i  is the reconstructed feature map, and  ∥· ∥ 2  denotes the L2 norm. Define the dropout probability  p i  for neuron  i  based on the reconstruction error of its corresponding feature map. The idea is to use the reconstruction error to determine the probability: \np i  = γ  RE i max( RE ) +  ϵ \n18 \nwhere  γ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero.",
      "type": "sliding_window",
      "tokens": 153
    },
    {
      "text": "The idea is to use the reconstruction error to determine the probability: \np i  = γ  RE i max( RE ) +  ϵ \n18 \nwhere  γ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. Define a binary dropout mask  m  = [ m 1 , m 2 , . .",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Feature Map Reconstruction Error Dropout and QuantaTask Optimization: Initialize the network parameters  W , dropout mask  m , and scaling factor  γ .",
      "type": "sliding_window",
      "tokens": 159
    },
    {
      "text": "Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Feature Map Reconstruction Error Dropout and QuantaTask Optimization: Initialize the network parameters  W , dropout mask  m , and scaling factor  γ . Define the energy budget E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l .",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "Initialize the loop iteration parameters  l . Compute the activations  a  and apply the dropout mask: \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. Calculate the gradients of the loss with respect to the weights: \n∂ L ∂W ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the reconstruction error of the feature maps: \np i  = γ  RE i max( RE ) +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication.",
      "type": "sliding_window",
      "tokens": 358
    },
    {
      "text": "Calculate the gradients of the loss with respect to the weights: \n∂ L ∂W ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the reconstruction error of the feature maps: \np i  = γ  RE i max( RE ) +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with Feature Map Reconstruction Error Dropout and QuantaTask Optimization: Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window",
      "tokens": 338
    },
    {
      "text": "If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output  Y .",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "Perform the forward pass with the updated dropout mask to obtain the output  Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the reconstruction error of the feature maps, along with the QuantaTask optimization to handle energy constraints. 19 \nD.4 Learning Sparse Masks Dropout with QuantaTask Optimization \nLearning Sparse Masks Dropout adapts dropout masks as learnable parameters within the network, inspired by Wen et al.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "19 \nD.4 Learning Sparse Masks Dropout with QuantaTask Optimization \nLearning Sparse Masks Dropout adapts dropout masks as learnable parameters within the network, inspired by Wen et al. (2016), combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation:  Let  W  be the weight matrix of a layer.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Mathematical Formulation:  Let  W  be the weight matrix of a layer. Define a binary dropout mask m  = [ m 1 , m 2 , . .",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . In Learning Sparse Masks Dropout, the dropout masks are treated as learnable parameters. The mask values are determined using a sigmoid function to ensure they lie between 0 and 1: m i  =  σ ( z i ) where  z i  are learnable parameters and  σ ( · )  is the sigmoid function.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "The mask values are determined using a sigmoid function to ensure they lie between 0 and 1: m i  =  σ ( z i ) where  z i  are learnable parameters and  σ ( · )  is the sigmoid function. Apply the dropout mask during the forward pass. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output.",
      "type": "sliding_window",
      "tokens": 142
    },
    {
      "text": "Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. DynFit integrates closely with DynAgent, which serves as a repository of EH profiles and hardware characteristics. Let  Q  represent the set of execution quanta, where each quanta  q  ∈Q  is defined by a tuple  ( l, e ) : q  = ( l, e ) Here,  l  is the number of loop iterations and  e  is the estimated energy required for these iterations.",
      "type": "sliding_window",
      "tokens": 172
    },
    {
      "text": "Let  Q  represent the set of execution quanta, where each quanta  q  ∈Q  is defined by a tuple  ( l, e ) : q  = ( l, e ) Here,  l  is the number of loop iterations and  e  is the estimated energy required for these iterations. The goal is to optimize the loop iteration parameter  l  such that the energy consumption  E q  for each quanta  q  is within the energy budget  E b : \nminimize X \nq ∈Q E q subject to E q  ≤ E b \nTraining with Learning Sparse Masks Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask parameters  z , and scaling factor  α . Define the energy budget E b  for a single quanta and for the entire inference.",
      "type": "sliding_window",
      "tokens": 197
    },
    {
      "text": "Define the energy budget E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l . Compute the activations  a  and apply the dropout mask: \nm i  =  σ ( z i ) \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y ) .",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "Compute the activations  a  and apply the dropout mask: \nm i  =  σ ( z i ) \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y ) . Calculate the gradients of the loss with respect to the weights and dropout mask parameters: ∂ L ∂W ij , ∂ L ∂z i For each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) If  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask parameters  z  based on the gradients: \nz i  ← z i  − η  ∂ L \n∂z i \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \n20 \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with Learning Sparse Masks Dropout and QuantaTask Optimization:  Check the available energy using DynAgent.",
      "type": "sliding_window",
      "tokens": 363
    },
    {
      "text": "Inference with Learning Sparse Masks Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout with learnable mask parameters, along with the QuantaTask optimization to handle energy constraints.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout with learnable mask parameters, along with the QuantaTask optimization to handle energy constraints. D.5 Neuron Shapley Value Dropout with QuantaTask Optimization \nNeuron Shapley Value Dropout applies the concept of Shapley values from game theory (Aas et al., 2021) to assess neuron importance for dropout, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation:  The Shapley value  ϕ i  of neuron  i  is a measure of its contribution to the overall network performance.",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "Mathematical Formulation:  The Shapley value  ϕ i  of neuron  i  is a measure of its contribution to the overall network performance. It is calculated by considering all possible subsets of neurons and computing the marginal contribution of neuron  i  to the network’s output: \nϕ i  = 1 |N| ! X \nS ⊆N\\{ i } \n| S | !",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "X \nS ⊆N\\{ i } \n| S | ! ( |N| −| S | − 1)! |N| [ L ( S  ∪{ i } )  −L ( S )] \nwhere  N  is the set of all neurons,  S  is a subset of neurons not containing  i , and  L ( · )  denotes the loss function.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "|N| [ L ( S  ∪{ i } )  −L ( S )] \nwhere  N  is the set of all neurons,  S  is a subset of neurons not containing  i , and  L ( · )  denotes the loss function. Define the dropout probability  p i  for neuron  i  based on its Shapley value. Neurons with lower Shapley values are more likely to be dropped: \np i  = δ ϕ i  +  ϵ where  δ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "Neurons with lower Shapley values are more likely to be dropped: \np i  = δ ϕ i  +  ϵ where  δ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. Define a binary dropout mask  m  = [ m 1 , m 2 , . .",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Neuron Shapley Value Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  δ .",
      "type": "sliding_window",
      "tokens": 157
    },
    {
      "text": "Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Neuron Shapley Value Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  δ . Define the energy budget  E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l .",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Initialize the loop iteration parameters  l . Compute the activations  a  and apply the dropout mask: \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. Calculate the Shapley values  ϕ i  for each neuron based on their contribution to the network’s perfor- mance.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "Calculate the Shapley values  ϕ i  for each neuron based on their contribution to the network’s perfor- mance. For each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) If  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \n21 \nUpdate the dropout mask  m  based on the Shapley values: \np i  = δ ϕ i  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with Neuron Shapley Value Dropout and QuantaTask Optimization:  Check the available energy using DynAgent.",
      "type": "sliding_window",
      "tokens": 313
    },
    {
      "text": "Inference with Neuron Shapley Value Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output  Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the Shapley values of the neurons, along with the QuantaTask optimization to handle energy constraints.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the Shapley values of the neurons, along with the QuantaTask optimization to handle energy constraints. D.6 Taylor Expansion Dropout with QuantaTask Optimization \nTaylor Expansion Dropout uses Taylor expansion (Li et al., 2016) to evaluate the impact of neurons on loss for dropout adjustments, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation:  Let  W  be the weight matrix of a layer.",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "Mathematical Formulation:  Let  W  be the weight matrix of a layer. The impact of neuron  i  on the loss function  L  can be approximated using the first-order Taylor expansion: \n∆ L i  ≈ \f\f\f\f ∂ L ∂ a i a i \n\f\f\f\f \nwhere  a i  is the activation of neuron  i , and   ∂ L \n∂ a i   is the gradient of the loss with respect to the activation. Define the dropout probability  p i  for neuron  i  based on the Taylor expansion approximation of its impact on the loss: \np i  = λ \f\f\f  ∂ L \n∂ a i   a i \f\f\f  +  ϵ \nwhere  λ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero.",
      "type": "sliding_window",
      "tokens": 187
    },
    {
      "text": "Define the dropout probability  p i  for neuron  i  based on the Taylor expansion approximation of its impact on the loss: \np i  = λ \f\f\f  ∂ L \n∂ a i   a i \f\f\f  +  ϵ \nwhere  λ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. Define a binary dropout mask  m  = [ m 1 , m 2 , . .",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Taylor Expansion Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  λ .",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Taylor Expansion Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  λ . Define the energy budget  E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l .",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "Initialize the loop iteration parameters  l . Compute the activations  a  and apply the dropout mask: \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. 22 \nCalculate the gradients of the loss with respect to the activations: \n∂ L ∂ a i \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the Taylor expansion approximation: \np i  = λ \f\f\f  ∂ L \n∂ a i   a i \f\f\f  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication.",
      "type": "sliding_window",
      "tokens": 364
    },
    {
      "text": "22 \nCalculate the gradients of the loss with respect to the activations: \n∂ L ∂ a i \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the Taylor expansion approximation: \np i  = λ \f\f\f  ∂ L \n∂ a i   a i \f\f\f  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with Taylor Expansion Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window",
      "tokens": 340
    },
    {
      "text": "If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output Y .",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "Perform the forward pass with the updated dropout mask to obtain the output Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the Taylor expansion approximation of the neurons’ impact on the loss, along with the QuantaTask optimization to handle energy constraints. E Workings of Re-RAM Crossbar \nE.1 Re-RAM cross-bar for DNN inference: \nReRAM x-bars are an emerging class of computing devices that leverage resistive random-access memory (ReRAM) technology for efficient and low-power computing.",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "E Workings of Re-RAM Crossbar \nE.1 Re-RAM cross-bar for DNN inference: \nReRAM x-bars are an emerging class of computing devices that leverage resistive random-access memory (ReRAM) technology for efficient and low-power computing. These devices can perform multiplication and addition operations in a single operation, making them ideal for many signal pro- cessing and machine learning applications. Moreover, these devices can also be used for performing convolution operations, which are widely used in image and signal processing applications.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "Moreover, these devices can also be used for performing convolution operations, which are widely used in image and signal processing applications. E.1.1 Simple Single Cell Example: \nconsider a simple example of a ReRAM crossbar array with two cells, where V1 and V2 are the input voltages, G1 and G2 are the conductance values of the ReRAM devices, and I1 and I2 are the resulting output currents. To perform multiplication-addition, we first apply the input voltages V1 and V2 to the rows of the crossbar array.",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "To perform multiplication-addition, we first apply the input voltages V1 and V2 to the rows of the crossbar array. The conductance values G1 and G2 of the ReRAM devices are set to the corresponding weight values for the multiplication operation. The output currents I1 and I2 are then computed as follows: \nI  =  I 1 +  I 2 =  G 1  ×  V  1 +  G 2  ×  V  2 \n23 \n(a) Re-RAM Cell \n(b) A Full Re-RAM tile \nFigure 5: DNN computation using ReRAM xBAR.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "The output currents I1 and I2 are then computed as follows: \nI  =  I 1 +  I 2 =  G 1  ×  V  1 +  G 2  ×  V  2 \n23 \n(a) Re-RAM Cell \n(b) A Full Re-RAM tile \nFigure 5: DNN computation using ReRAM xBAR. Here, the output currents I1 and I2 are the result of the multiplication of the input voltages V1 and V2 by their respective weight values, which are summed together using the crossbar wires. Please refer to Figure 5a for more details.",
      "type": "sliding_window",
      "tokens": 126
    },
    {
      "text": "Please refer to Figure 5a for more details. As we can see, the input voltages V1 and V2 are applied to the rows of the crossbar array, while the conductance values G1 and G2 are applied to the columns. The output currents I1 and I2 are the result of the multiplication-addition operation, and are obtained by summing the currents flowing through the ReRAM devices.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "The output currents I1 and I2 are the result of the multiplication-addition operation, and are obtained by summing the currents flowing through the ReRAM devices. In practice, ReRAM crossbar arrays can have many more cells, and can be used to perform more complex multiplication-addition and convolution operations. However, the basic principle remains the same, where the input signals are applied to the rows, the weights are applied to the columns, and the output signals are obtained by summing the currents flowing through the ReRAM devices.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "However, the basic principle remains the same, where the input signals are applied to the rows, the weights are applied to the columns, and the output signals are obtained by summing the currents flowing through the ReRAM devices. E.1.2 Extending to Complex Compute: \nIn order to perform multiplication-addition in ReRAM x-bars, two arrays of weights and inputs are used. The inputs are fed to the x-bar, which is a two-dimensional array of ReRAM crossbar arrays.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "The inputs are fed to the x-bar, which is a two-dimensional array of ReRAM crossbar arrays. The crossbar arrays are composed of a set of row and column wires that intersect at a set of ReRAM devices (refer Figure 5b). The ReRAM devices are programmed to have different resistance values, which are used to store the weights.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "The ReRAM devices are programmed to have different resistance values, which are used to store the weights. During the multiplication-addition operation, the input signals are applied to the rows of the x-bar, and the weights are applied to the columns. The output of each ReRAM device is the product of the input and weight signals, which are added together using the crossbar wires.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "The output of each ReRAM device is the product of the input and weight signals, which are added together using the crossbar wires. This results in a single output signal that represents the sum of the weighted inputs. To perform convolution, ReRAM x-bars use a similar approach, but with a more complex circuit.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "To perform convolution, ReRAM x-bars use a similar approach, but with a more complex circuit. The input signal is applied to the x-bar in the same way, but the weights are now applied in a more structured way. Specifically, the weights are arranged in a way that mimics the convolution operation, such that each weight corresponds to a specific location in the input signal.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Specifically, the weights are arranged in a way that mimics the convolution operation, such that each weight corresponds to a specific location in the input signal. To perform the convolution operation, the input signal is applied to the rows of the x-bar, and the weights are applied to the columns in a structured way. The output signal is obtained by summing the weighted input signals over a sliding window, which moves across the input signal to compute the convolution.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "The output signal is obtained by summing the weighted input signals over a sliding window, which moves across the input signal to compute the convolution. At the circuit level, the ReRAM x-bar for multiplication-addition typically includes several com- ponents, such as digital-to-analog converters (DACs), analog-to-digital converters (ADCs), shift registers, and hold capacitors. The DACs and ADCs are used to convert the digital input and weight signals into analog signals that can be applied to the rows and columns of the x-bar.",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "The DACs and ADCs are used to convert the digital input and weight signals into analog signals that can be applied to the rows and columns of the x-bar. The shift registers are used to apply the weight signals in a structured way, and the hold capacitors are used to store the analog signals during the multiplication-addition operation. Similarly, for performing convolution, the ReRAM x-bar typically includes additional components, such as delay lines and adders.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Similarly, for performing convolution, the ReRAM x-bar typically includes additional components, such as delay lines and adders. The delay lines are used to implement the sliding window for the convolution operation, while the adders are used to sum the weighted input signals over the sliding window. 24 \nF Pseudo Codes \nF.1 Depth-wise Separable Convolution 2D Using TI LEA \nDepth-wise separable convolution is an efficient form of convolution that reduces the computational cost compared to standard convolution.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "24 \nF Pseudo Codes \nF.1 Depth-wise Separable Convolution 2D Using TI LEA \nDepth-wise separable convolution is an efficient form of convolution that reduces the computational cost compared to standard convolution. Here we describe the implementation of depth-wise sep- arable convolution 2D using the Low Energy Accelerator (LEA) in Texas Instruments’ MSP430 microcontrollers. F.1.1 depth-wise Separable Convolution 2D Using Conv1D \nThe pseudo code described in Algorithm 1 implements a depth-wise separable convolution 2D (DWSConv2D) using a 1D convolution primitive function (conv1D).",
      "type": "sliding_window",
      "tokens": 168
    },
    {
      "text": "F.1.1 depth-wise Separable Convolution 2D Using Conv1D \nThe pseudo code described in Algorithm 1 implements a depth-wise separable convolution 2D (DWSConv2D) using a 1D convolution primitive function (conv1D). The DWSConv2D function takes four inputs: an input matrix, depth-wise kernels (DWsKernels), point-wise kernels (PtWsKernel), and an output matrix. The depth-wise separable convolution is performed in two main steps: depth-wise convolution and point-wise convolution.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "The function is designed to handle energy constraints by decomposing the convolution loops into smaller quanta tasks. Foloowing are the outline of the requirements: \n1. Define ‘QuantaTask‘ as the minimum iterations that can run.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Define ‘QuantaTask‘ as the minimum iterations that can run. 2. Decomposable loops: Each ‘QuantaTask‘ runs a certain part of the loop.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "Decomposable loops: Each ‘QuantaTask‘ runs a certain part of the loop. 3. Check for sufficient energy before launching a ‘QuantaTask‘.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "Check for sufficient energy before launching a ‘QuantaTask‘. 4. Fuse multiple ‘QuantaTask‘s to minimize load/store operations.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "Fuse multiple ‘QuantaTask‘s to minimize load/store operations. 5. Check for power loss after each ‘QuantaTask‘ or fused ‘QuantaTask‘ and checkpoint if necessary.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "We propose a dynamic adjustment of training parameters—dropout rates and quantization levels—that adapt in real-time to the available energy, which varies in energy harvesting scenarios. This study introduces  NExUME , a novel training methodology designed specifically for DNNs operating under such constraints. Abstract \nThe deployment of Deep Neural Networks (DNNs) in energy-constrained envi- ronments, such as Energy Harvesting Wireless Sensor Networks (EH-WSNs), introduces significant challenges due to the intermittent nature of power availability.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "It dynamically adjusts training strategies, such as the intensity and timing of dropout and quantization, based on predictions of energy availability. We propose a dynamic adjustment of training parameters—dropout rates and quantization levels—that adapt in real-time to the available energy, which varies in energy harvesting scenarios. This approach utilizes a model that integrates the characteristics of the network architecture and the specific energy harvesting profile.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "It dynamically adjusts training strategies, such as the intensity and timing of dropout and quantization, based on predictions of energy availability. This method not only conserves energy but also enhances the network’s adaptability, ensuring robust learning and inference capabilities even under stringent power constraints. Our results show a 6% to 22% improvement in accuracy over current methods, with an increase of less than 5% in computational overhead.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "Our results show a 6% to 22% improvement in accuracy over current methods, with an increase of less than 5% in computational overhead. This paper details the development of the adaptive training framework, describes the integration of energy profiles with dropout and quantization adjustments, and presents a comprehensive evaluation using real- world data. Additionally, we introduce a novel dataset aimed at furthering the application of energy harvesting in computational settings.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "Additionally, we introduce a novel dataset aimed at furthering the application of energy harvesting in computational settings. Such platforms represent the future of the Internet of Things (IoT) and energy harvesting wireless sensor networks (EH-WSNs). 1 Introduction \nThe increasing demand for ubiquitous, sustainable, and energy-efficient computing, combined with advancements in energy harvesting systems, has spurred significant research into battery-less devices (Gobieski et al., 2019; Resch et al., 2020; Mishra et al., 2021; Saffari et al., 2021; Afzal et al., 2022).",
      "type": "sliding_window_shuffled",
      "tokens": 150,
      "augmented": true
    },
    {
      "text": "However, the intermittent and limited energy income of these deployments demands optimizations for ML applications at the algorithm (Yang et al., 2017; Shen et al., 2022; Mendis et al., 2021), orchestration (Maeng & Lucia, 2018; Mishra et al., 2021), compilation (Gobieski et al., 2018), and hardware development (Qiu et al., 2020; Islam et al., 2022; Mishra et al., 2024) layers. Such platforms represent the future of the Internet of Things (IoT) and energy harvesting wireless sensor networks (EH-WSNs). Equipped with modern machine learning (ML) techniques, these devices can revolutionize computing, monitoring, and analytics in remote, risky, and critical environments such as oil wells, mines, deep forests, oceans, remote industries, and smart cities.",
      "type": "sliding_window_shuffled",
      "tokens": 219,
      "augmented": true
    },
    {
      "text": "There are two major problems with performing DNN inference under intermittent power. Despite these advancements, achieving consistent and accurate inference—thereby meeting service level objectives (SLOs)—in such intermittent environments remains a significant challenge, exacerbated by unpredictable resources, form-factor limitations, and variable computational availability, particularly when employing task-optimized deep neural networks (DNNs). However, the intermittent and limited energy income of these deployments demands optimizations for ML applications at the algorithm (Yang et al., 2017; Shen et al., 2022; Mendis et al., 2021), orchestration (Maeng & Lucia, 2018; Mishra et al., 2021), compilation (Gobieski et al., 2018), and hardware development (Qiu et al., 2020; Islam et al., 2022; Mishra et al., 2024) layers.",
      "type": "sliding_window_shuffled",
      "tokens": 220,
      "augmented": true
    },
    {
      "text": "Under review. (I) Energy Variability : Even though DNNs can be tailored to match the average energy income of the energy harvesting (EH) source through pruning, quantization, distillation, or network architecture search \nPreprint. There are two major problems with performing DNN inference under intermittent power.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "Under review. arXiv:2408.13696v2  [cs.LG]  26 Jan 2025 \n(NAS) (Yang et al., 2018, 2017; Mendis et al., 2021), there is no guarantee that the energy income con- sistently meets or exceeds this average. When the income falls below the threshold, the system halts the inference and checkpoints the intermediate states (via software or persistent hardware) (Maeng & Lucia, 2018; Qiu et al., 2020), resuming upon energy recovery.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "(II) Computational Approximation : To address (I) and maintain continuous operation, EH-WSNs may skip some compute during energy shortfalls by dropping neurons (zero padding) or by approximating computations (quantization). Depending on the EH profile, this might lead to significant delays and SLO violations. When the income falls below the threshold, the system halts the inference and checkpoints the intermediate states (via software or persistent hardware) (Maeng & Lucia, 2018; Qiu et al., 2020), resuming upon energy recovery.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "In certain energy-critical scenarios, even EH-WSNs applying state-of-the-art techniques fail to consistently meet SLOs, sometimes skipping entire inferences to deliver results on time. Adding further approximation to save energy atop an already heavily reduced network can propagate errors through the layers, leading to significant accuracy drops (Islam & Nirjon, 2019; Kang et al., 2022; Lv & Xu, 2022; Kang et al., 2020), further violating SLOs. (II) Computational Approximation : To address (I) and maintain continuous operation, EH-WSNs may skip some compute during energy shortfalls by dropping neurons (zero padding) or by approximating computations (quantization).",
      "type": "sliding_window_shuffled",
      "tokens": 183,
      "augmented": true
    },
    {
      "text": "Funda- mentally, while current DNNs can be trained or fine-tuned to fit within a given resource budget—be it compute, memory, or energy—they are  not  trained to expect a variable or intermittent resource income. Although intermittency-aware NAS (Mendis et al., 2021), could alleviate certain problems, they often assume fixed resource constraints and do not account for real-time energy fluctuations. In certain energy-critical scenarios, even EH-WSNs applying state-of-the-art techniques fail to consistently meet SLOs, sometimes skipping entire inferences to deliver results on time.",
      "type": "sliding_window_shuffled",
      "tokens": 142,
      "augmented": true
    },
    {
      "text": "This calls for revisiting the entire training process; we need to train the DNN in such a way that it is aware of the intermittency and adapts  to it. Although intermittency-aware NAS (Mendis et al., 2021), could alleviate certain problems, they often assume fixed resource constraints and do not account for real-time energy fluctuations. Moreover, existing works like Keep in Balance (Yen et al., 2023), Stateful Neural Networks (Yen et al., 2022), ePerceptive (Montanari et al., 2020), and Zygarde (Islam & Nirjon, 2019) address aspects of intermittent computing but do not integrate energy variability awareness directly into the training and inference processes to enable dynamic adaptation.",
      "type": "sliding_window_shuffled",
      "tokens": 182,
      "augmented": true
    },
    {
      "text": "Motivated by these challenges, we propose  NExUME  ( N eural  Ex ecution  U nder Inter M ittent E nvironments), a novel framework designed specifically for environments with intermittent power and EH-WSNs, with potential applications in any ultra-low-power inference system. NExUME uniquely integrates energy variability awareness directly into both the training ( DynFit ) and inference ( DynInfer ) processes, enabling DNNs to dynamically adapt computations based on real-time energy availability. This calls for revisiting the entire training process; we need to train the DNN in such a way that it is aware of the intermittency and adapts  to it.",
      "type": "sliding_window_shuffled",
      "tokens": 163,
      "augmented": true
    },
    {
      "text": "The method includes targeted fine-tuning that not only regularizes the model but also pre- vents overfitting, enhancing robustness to fluctuations in resource availability. This involves an innovative strategy of learning instantaneous energy-aware dynamic dropout and quantization selection during training, and an intermittency-aware task scheduler during inference. NExUME uniquely integrates energy variability awareness directly into both the training ( DynFit ) and inference ( DynInfer ) processes, enabling DNNs to dynamically adapt computations based on real-time energy availability.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "Our key contributions can be summarized as follows: \n•  DynFit : A novel training optimizer that embeds energy variability awareness directly into the DNN training process. The method includes targeted fine-tuning that not only regularizes the model but also pre- vents overfitting, enhancing robustness to fluctuations in resource availability. This optimizer allows for dynamic adjustments of dropout rates and quantization levels based on real-time energy availability, thus maintaining learning stability and improving model accuracy under power constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "•  DynInfer : An intermittency- and platform-aware task scheduler that optimizes computational tasks for intermittent power supply, ensuring consistent and reliable DNN operation. DynInfer leverages software-compiler-hardware co-design to manage and deploy tasks. This optimizer allows for dynamic adjustments of dropout rates and quantization levels based on real-time energy availability, thus maintaining learning stability and improving model accuracy under power constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "With the help of DynFit, DynInfer provides  6% – 22%  accuracy improvements with  ≤ 5%  additional compute over existing methods. DynInfer leverages software-compiler-hardware co-design to manage and deploy tasks. •  Dataset : A first-of-its-kind machine status monitoring dataset, involving multiple types of EH sensors mounted at various locations on a Bridgeport machine to monitor its activity status, facilitating research in predictive maintenance and Industry 4.0 applications.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "2 Background and Related Work \nEnergy Harvesting and Intermittent Computing:  The exploding usage of IoTs, connected devices, and wearable electronics project the number of battery operated devices to be 24.1 Billion by 2030 (Insights, 2023). •  Dataset : A first-of-its-kind machine status monitoring dataset, involving multiple types of EH sensors mounted at various locations on a Bridgeport machine to monitor its activity status, facilitating research in predictive maintenance and Industry 4.0 applications. This has a significant economic (users, products and data generating dollar value) as well as environmental (battery and e-waste) impact (Mishra et al., 2024).",
      "type": "sliding_window_shuffled",
      "tokens": 160,
      "augmented": true
    },
    {
      "text": "A typical EH setup consists of 5 components, namely, energy capture (solar panel, thermocouple, etc), power conditioning, voltage regulation (buck or boost converter), energy storage (super capacitor) and compute unit (refer §Appendix B for details about each of them). In fact, advances in EH has lead to a staggering development in intermittently powered battery-free devices (Maeng & Lucia, 2018; Gobieski et al., 2019; Qiu et al., 2020; Saffari et al., 2021; Afzal et al., 2022). This has a significant economic (users, products and data generating dollar value) as well as environmental (battery and e-waste) impact (Mishra et al., 2024).",
      "type": "sliding_window_shuffled",
      "tokens": 192,
      "augmented": true
    },
    {
      "text": "To cater towards the sporadic \n2 \npower income and failures, an existing body of works explores algorithms, orchestration, compiler support, and hardware development (Yang et al., 2017, 2018; Mendis et al., 2021; Maeng & Lucia, 2018; Gobieski et al., 2018; Qiu et al., 2020; Islam et al., 2022; Mishra et al., 2024, 2021; Ma et al., 2016, 2017; Liu et al., 2015). Most of these works rely on software checkpointing (static and dynamic (Maeng & Lucia, 2018), refer §Appendix C) to save and restore, while some of the prior works developed nonvolatile hardware (Ma et al., 2016, 2017) which inherently takes care of the checkpointing. A typical EH setup consists of 5 components, namely, energy capture (solar panel, thermocouple, etc), power conditioning, voltage regulation (buck or boost converter), energy storage (super capacitor) and compute unit (refer §Appendix B for details about each of them).",
      "type": "sliding_window_shuffled",
      "tokens": 270,
      "augmented": true
    },
    {
      "text": "Most of these works rely on software checkpointing (static and dynamic (Maeng & Lucia, 2018), refer §Appendix C) to save and restore, while some of the prior works developed nonvolatile hardware (Ma et al., 2016, 2017) which inherently takes care of the checkpointing. Considering the scope of these initiatives, it is crucial to acknowledge that, despite the substantial support for energy harvesting and intermittency management, developing intermittency-aware applications and hardware necessitates multi-dimensional efforts that span from theoretical foundations to circuit design. Intermittent DNN Execution/Training:  As the applications deployed on such EH devices demand analytics, executing DNNs on EH devices and EH-WSNs have become prominent (Lv & Xu, 2022; Gobieski et al., 2019; Qiu et al., 2020; Mishra et al., 2021).",
      "type": "sliding_window_shuffled",
      "tokens": 226,
      "augmented": true
    },
    {
      "text": "While the works relying on loop-decomposition or task partition (e.g., see (Qiu et al., 2020; Gobieski et al., 2019) and the references therein) ensure “forward progress”, they do not guarantee an inference completion while meeting SLOs. Intermittent DNN Execution/Training:  As the applications deployed on such EH devices demand analytics, executing DNNs on EH devices and EH-WSNs have become prominent (Lv & Xu, 2022; Gobieski et al., 2019; Qiu et al., 2020; Mishra et al., 2021). However, due to computational constraints, limited memory capacity and restricted operating frequencies, many of these applications fail to complete inference execution with satisfactory SLOs, despite comprehensive software and hardware support (Mishra et al., 2021).",
      "type": "sliding_window_shuffled",
      "tokens": 214,
      "augmented": true
    },
    {
      "text": "While the works relying on loop-decomposition or task partition (e.g., see (Qiu et al., 2020; Gobieski et al., 2019) and the references therein) ensure “forward progress”, they do not guarantee an inference completion while meeting SLOs. One major issue is, most of the works leverage “pre-existing” DNNs, which are typically designed for running on a stable resource environment, while being deployed on an intermittent environment with pseudo notion of stability via check-pointing, and therefore, one direction of works (Mendis et al., 2021) looks for performing network architecture search for intermittent devices. Optimizing DNNs for the energy constraints (Yang et al., 2018, 2017), or performing early exit and depth-first slicing (Lv & Xu, 2022; Islam & Nirjon, 2019) does ensure more forward progress, but such approaches compromise accuracy while often imposing scheduling overheads and higher memory footprint.",
      "type": "sliding_window_shuffled",
      "tokens": 228,
      "augmented": true
    },
    {
      "text": "However, this research direction only accounts for fixed lower and upper bounds of energy and compute capacities, overlooking the “sporadic” nature of energy availability and the elasticity of the compute hardware (i.e., the ability to dynamically scale frequency, compute, and memory). Moreover, while the DNN is designed to operate within a specific power window, it is  not  trained to adapt to these fluctuations. One major issue is, most of the works leverage “pre-existing” DNNs, which are typically designed for running on a stable resource environment, while being deployed on an intermittent environment with pseudo notion of stability via check-pointing, and therefore, one direction of works (Mendis et al., 2021) looks for performing network architecture search for intermittent devices.",
      "type": "sliding_window_shuffled",
      "tokens": 168,
      "augmented": true
    },
    {
      "text": "Consequently, during extended periods of energy scarcity, the system lacks mechanisms for computational approximation, such as dynamic dropouts (neuron skipping) and dynamic quantization. Moreover, while the DNN is designed to operate within a specific power window, it is  not  trained to adapt to these fluctuations. Essentially, the DNN is trained to manage within a static resource budget, ignoring the “dynamism” of the resources .",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "In contrast, our work prioritizes the integration of this dynamism in both the network architecture search (NAS) and the training phases, adapting more effectively to fluctuating energy and compute conditions. Essentially, the DNN is trained to manage within a static resource budget, ignoring the “dynamism” of the resources . 3 NExUME Framework \nTo address the issues with  intermittency-aware  DNN training and inference, we propose NExUME: ( N eural  Ex ecution  U nder Inter M ittent  E nvironments).",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "NExUME has three interrelated compo- nents: (1)  DynNAS : Intermittency- and platform-aware neural architecture search; (2)  DynFit : Intermittency- and platform-aware DNN training with dynamic dropouts and quantization; and (3) DynInfer : Intermittency- and platform-aware task scheduling for inference. While each component can individually optimize DNNs for intermittent environments, their combination yields the best results. 3 NExUME Framework \nTo address the issues with  intermittency-aware  DNN training and inference, we propose NExUME: ( N eural  Ex ecution  U nder Inter M ittent  E nvironments).",
      "type": "sliding_window_shuffled",
      "tokens": 168,
      "augmented": true
    },
    {
      "text": "To search for the best architecture for the given intermittent environ- ment, DynNAS utilizes the approach proposed by iNAS (Mendis et al., 2021). Our innovation lies in the integration of energy variability awareness directly into both the training and inference processes, enabling dynamic adaptation to real-time energy conditions, which is not addressed by existing methods (Mendis et al., 2021; Yen et al., 2023, 2022; Montanari et al., 2020; Islam & Nirjon, 2019). While each component can individually optimize DNNs for intermittent environments, their combination yields the best results.",
      "type": "sliding_window_shuffled",
      "tokens": 148,
      "augmented": true
    },
    {
      "text": "To search for the best architecture for the given intermittent environ- ment, DynNAS utilizes the approach proposed by iNAS (Mendis et al., 2021). After the network architecture is determined, DynFit is used to train the network considering energy intermittency, and DynInfer is employed to perform inference under intermittent power conditions. In this section, we elaborate on the key components, focusing on DynFit and DynInfer, and explain how they uniquely adapt DNN training and inference to intermittent power conditions.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "In this section, we elaborate on the key components, focusing on DynFit and DynInfer, and explain how they uniquely adapt DNN training and inference to intermittent power conditions. 3.1 DynFit: Intermittency-Aware Learning \nDynFit  is designed to optimize deep neural networks (DNNs) for execution in environments char- acterized by intermittent power supply due to energy harvesting. The primary goal of DynFit is to \n3 \nadapt the DNN’s training process to operate efficiently under unpredictable energy budgets while maintaining acceptable accuracy and adhering to predefined service level objectives (SLOs).",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "DynFit introduces key mechanisms to dynamically adjust computational complexity based on energy availability, thereby enabling energy-efficient execution of DNN models in constrained environments. The primary goal of DynFit is to \n3 \nadapt the DNN’s training process to operate efficiently under unpredictable energy budgets while maintaining acceptable accuracy and adhering to predefined service level objectives (SLOs). These mechanisms include: (i)  Dynamic Dropout , which adjusts the dropout rates based on available energy to reduce computational load; (ii)  Dynamic Quantization , which modifies quantization levels in response to energy constraints to save energy; and (iii)  QuantaTask  design, which defines atomic computational units that can be executed without interruption given the energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 170,
      "augmented": true
    },
    {
      "text": "Unlike standard implementations where dropout rates and quantization levels are fixed or adjusted solely based on training dynamics, DynFit adjusts these parameters in real-time based on the energy profile of the device. These mechanisms include: (i)  Dynamic Dropout , which adjusts the dropout rates based on available energy to reduce computational load; (ii)  Dynamic Quantization , which modifies quantization levels in response to energy constraints to save energy; and (iii)  QuantaTask  design, which defines atomic computational units that can be executed without interruption given the energy budget. Specifically, during training, we simulate energy variability by incorporating energy traces into the training loop.",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "Based on  E b , we adjust the dropout rate  d i  for each layer  i  according to: \nd i  =  d max \n\u0012 1  − E b \nE max \n\u0013 , (1) \nwhere  d max  is the maximum allowable dropout rate, and  E max  is the maximum energy observed in the traces. At each training iteration, the available energy  E b  is sampled from these traces. Specifically, during training, we simulate energy variability by incorporating energy traces into the training loop.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "(2) \nThis ensures that when energy is low, higher dropout rates and lower quantization bit-widths are used to reduce computational load, and vice versa. Similarly, the quantization levels  q j  are adjusted: \nq j  =  q min  + ( q max  − q min )   E b \nE max . Based on  E b , we adjust the dropout rate  d i  for each layer  i  according to: \nd i  =  d max \n\u0012 1  − E b \nE max \n\u0013 , (1) \nwhere  d max  is the maximum allowable dropout rate, and  E max  is the maximum energy observed in the traces.",
      "type": "sliding_window_shuffled",
      "tokens": 147,
      "augmented": true
    },
    {
      "text": "Let  e op  denote the energy consumed per computational operation, which varies with operation type and data precision. Modeling Energy Consumption:  The energy consumption of DNN operations is modeled based on empirical profiling data from the hardware platform. (2) \nThis ensures that when energy is low, higher dropout rates and lower quantization bit-widths are used to reduce computational load, and vice versa.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "By integrating the energy model into the training process, DynFit ensures the adjustments to dropout and quantization directly correspond to actual energy savings on the target hardware. The total energy consumption of a QuantaTask  q  is modeled as  E q  =  e op  ×  ℓ q , where  ℓ q  is the number of operations in the task. Let  e op  denote the energy consumed per computational operation, which varies with operation type and data precision.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "By integrating the energy model into the training process, DynFit ensures the adjustments to dropout and quantization directly correspond to actual energy savings on the target hardware. Each QuantaTask ensures that execution proceeds without partial computation, which would otherwise lead to overhead from checkpointing and potential data corruption. A  QuantaTask  is defined as the smallest atomic unit of computation that can be executed entirely without interruption under the current energy and hardware constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Figure 1 illustrates QuantaTask execution with a simple example. Each QuantaTask ensures that execution proceeds without partial computation, which would otherwise lead to overhead from checkpointing and potential data corruption. The main properties of QuantaTasks are atomicity and respect for energy constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "Depending on the available energy, the task (vector inner product) can be divided into multiple iterations such that each QuantaTask is guaranteed to finish given the energy availability. Figure 1 illustrates QuantaTask execution with a simple example. A3 B3 \nB2 \nB1 \nX X X \nA2 \nA1 \nX \nFigure 1: An example of variable QuantaTask in a matrix multiplication scenario.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Depending on the available energy, the task (vector inner product) can be divided into multiple iterations such that each QuantaTask is guaranteed to finish given the energy availability. Optimization Variables, Constraints, and Objective Function:  The optimization problem is formulated with variables: the weights  W , dropout rates  d , quantization levels  q , and QuantaTask sizes  ℓ . E  is available energy, and  E b  is the energy required to finish one inner product.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "Optimization Variables, Constraints, and Objective Function:  The optimization problem is formulated with variables: the weights  W , dropout rates  d , quantization levels  q , and QuantaTask sizes  ℓ . The objective is to minimize the total loss, including prediction loss and regularization terms penalizing energy consumption (subject to energy constraints): \nmin W , d , q , ℓ L (   ˆ Y ,  Y ) +  λ 1 \nM X \nj =1 c q ( q j ) +  λ 2 \nN X \ni =1 c d ( d i ) . (3) \nFormulation of the Composite Optimization Problem:  The problem is non-convex due to the discrete nature of quantization levels and dropout rates.",
      "type": "sliding_window_shuffled",
      "tokens": 188,
      "augmented": true
    },
    {
      "text": "We employ an alternating optimization strategy, iteratively optimizing subsets of variables while keeping others fixed. (3) \nFormulation of the Composite Optimization Problem:  The problem is non-convex due to the discrete nature of quantization levels and dropout rates. Our method differs from standard approaches by integrating energy constraints directly into the optimization, ensuring that the network learns to adapt its parameters based on energy availability.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "4 \n3.1.1 Adaptive Regularization Strategy \nDynFit introduces an adaptive regularization strategy to address potential overfitting and under- training due to uneven weight updates caused by dynamic dropout and quantization. We monitor the update frequency  F p  of each weight  w p  over a window of  T  iterations: \nF p  =  1 \nT \nT X \nt =1 U p ( t ) , U p ( t ) = \u001a 1 , if  w p  is updated at iteration  t 0 , otherwise (4) \nWeights with  F p  < θ low  are considered under-trained, and those with  F p  > θ high  are considered overfitting. Our method differs from standard approaches by integrating energy constraints directly into the optimization, ensuring that the network learns to adapt its parameters based on energy availability.",
      "type": "sliding_window_shuffled",
      "tokens": 194,
      "augmented": true
    },
    {
      "text": "We adjust dropout rates and apply L2 regularization accordingly to balance the training process. We monitor the update frequency  F p  of each weight  w p  over a window of  T  iterations: \nF p  =  1 \nT \nT X \nt =1 U p ( t ) , U p ( t ) = \u001a 1 , if  w p  is updated at iteration  t 0 , otherwise (4) \nWeights with  F p  < θ low  are considered under-trained, and those with  F p  > θ high  are considered overfitting. This adaptive strategy ensures that all weights are adequately trained despite the dynamic adjustments.",
      "type": "sliding_window_shuffled",
      "tokens": 152,
      "augmented": true
    },
    {
      "text": "This adaptive strategy ensures that all weights are adequately trained despite the dynamic adjustments. Complexity Analysis of DynFit:  The time complexity of DynFit during training is  O ( N  ·  T ) , where N  is the number of weights and  T  is the number of training iterations. Dropout scheduling techniques are incorporated, where dropout rates are increased or decreased over time based on the training progress and energy availability, mitigating potential overfitting introduced by static dropout variations.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "The space complexity is  O ( N )  for storing the update frequencies and additional parameters. Complexity Analysis of DynFit:  The time complexity of DynFit during training is  O ( N  ·  T ) , where N  is the number of weights and  T  is the number of training iterations. The overhead introduced by monitoring update frequencies and adjusting dropout rates is negligible compared to the overall training time, as these operations are simple arithmetic computations per iteration.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Compared to classical training, DynFit adds minimal overhead, with a tradeoff of  ≤ 5%  additional compute for significant gains in accuracy under intermittent power conditions. The space complexity is  O ( N )  for storing the update frequencies and additional parameters. 3.2 DynInfer: Intermittency-Aware Inference Scheduling \nDynInfer  optimizes the inference phase of DNNs operating under intermittent power conditions.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "The inference process is represented as a set of tasks  T  =  { T 1 , T 2 , . 3.2 DynInfer: Intermittency-Aware Inference Scheduling \nDynInfer  optimizes the inference phase of DNNs operating under intermittent power conditions. Unlike traditional systems with stable power, intermittent environments pose unique challenges for executing inference tasks efficiently and reliably.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": ". . The inference process is represented as a set of tasks  T  =  { T 1 , T 2 , .",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": ". At any given time  t , the available energy is denoted as  E b ( t ) . , T N } , where each task  T i  is characterized by its energy requirement  E i , execution time  τ i , priority  p i , deadline  D i , and criticality level  c i .",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Task Fusion and Scheduling:  DynInfer introduces a novel task scheduling algorithm that dynam- ically adjusts to real-time energy availability. At any given time  t , the available energy is denoted as  E b ( t ) . When the energy required for executing multiple QuantaTasks exceeds the available energy budget, DynInfer employs  task fusion  to combine smaller tasks into larger atomic units that can be executed within the energy constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Formal Definition of Task Fusion:  Let  Q  =  { q 1 , q 2 , . . When the energy required for executing multiple QuantaTasks exceeds the available energy budget, DynInfer employs  task fusion  to combine smaller tasks into larger atomic units that can be executed within the energy constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": ", q k }  be a set of QuantaTasks with in- dividual energy requirements  E q i . . .",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "However, if   P \ni   E q i   > E b , we aim to fuse tasks to minimize checkpointing overhead. , q k }  be a set of QuantaTasks with in- dividual energy requirements  E q i . If   P \ni   E q i   ≤ E b , the available energy budget, then tasks can be executed sequentially without interruption.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": ". Task fusion is formalized as finding a partition of  Q  into subsets Q 1 ,  Q 2 , . However, if   P \ni   E q i   > E b , we aim to fuse tasks to minimize checkpointing overhead.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": ",  Q m  such that, for each subset  Q j ,   P q i ∈Q j   E q i   ≤ E b , and  m  is minimized. . .",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": ",  Q m  such that, for each subset  Q j ,   P q i ∈Q j   E q i   ≤ E b , and  m  is minimized. For example, Consider two convolution operations  C 1  and  C 2  with energy requirements  E C 1  and  E C 2 , respectively. This reduces the number of checkpoints and the overhead associated with task switching.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "If individually  E C 1 , E C 2  > E b  but  E C 1  +  E C 2  ≤ E b , we fuse  C 1  and  C 2  into a single task. For example, Consider two convolution operations  C 1  and  C 2  with energy requirements  E C 1  and  E C 2 , respectively. The fused task executes both convolutions atomically within the energy budget, avoiding the overhead of checkpointing between them.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "The fused task executes both convolutions atomically within the energy budget, avoiding the overhead of checkpointing between them. The energy availability constraint over time is expressed as (subject to energy and task constraints): P \ni : s i ≤ t<f i   E i  ≤ E b ( t )  The objective is to maximize the total weighted priority of scheduled tasks: \nmax { x i ,s i } \nN X \ni =1 \n\u0000 p i  − αE i  − β ( f i  − D i ) + \u0001 x i . Scheduling Problem Formulation:  The scheduling problem is formulated with decision variables s i  (task start times) and binary variables  x i  ∈{ 0 ,  1 }  (indicating whether a task is scheduled).",
      "type": "sliding_window_shuffled",
      "tokens": 201,
      "augmented": true
    },
    {
      "text": "We ensure its performance by: 1. (5) \nScheduling Performance Assurance:  Our scheduling heuristic,  Energy-Aware Priority Scheduling , while sub-optimal in the theoretical sense, is designed to perform near-optimally in practice for real- time systems. The energy availability constraint over time is expressed as (subject to energy and task constraints): P \ni : s i ≤ t<f i   E i  ≤ E b ( t )  The objective is to maximize the total weighted priority of scheduled tasks: \nmax { x i ,s i } \nN X \ni =1 \n\u0000 p i  − αE i  − β ( f i  − D i ) + \u0001 x i .",
      "type": "sliding_window_shuffled",
      "tokens": 184,
      "augmented": true
    },
    {
      "text": "We ensure its performance by: 1. 2. Empirical Validation : We compare the heuristic’s \n5 \nperformance with the optimal solution on smaller problem instances using exhaustive search and find that the heuristic achieves within 95% of the optimal task completion rate.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "Theoretical Analysis : The heuristic prioritizes tasks based on effective priority  P   eff i = p i E i   ×  ϕ i , where  ϕ i  accounts for deadline urgency. 2. This balances task importance against energy consumption, leading to efficient utilization of available energy.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "3. This balances task importance against energy consumption, leading to efficient utilization of available energy. Complexity Analysis : The heuristic has a time complexity of O ( N  log  N )  due to sorting tasks based on  P   eff i   , which is acceptable for real-time applications.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "Complexity Analysis of DynInfer:  The time complexity of the scheduling algorithm is  O ( N  log  N ) due to sorting tasks, and the space complexity is  O ( N )  for storing task parameters. Complexity Analysis : The heuristic has a time complexity of O ( N  log  N )  due to sorting tasks based on  P   eff i   , which is acceptable for real-time applications. Compared to classical inference, DynInfer introduces additional overhead for scheduling and task fusion, but this is offset by the gains in reliability and efficiency under intermittent power.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "Compared to classical inference, DynInfer introduces additional overhead for scheduling and task fusion, but this is offset by the gains in reliability and efficiency under intermittent power. Handling Extremely Low or Sporadic Energy Levels:  In environments with extremely low or sporadic energy levels where consistent dropout and quantization adjustments may not be feasible, NExUME handles this by: 1. Implementing a minimum viable model configuration that operates at the lowest acceptable energy consumption, achieved by maximizing dropout rates and using the lowest quantization bit-widths.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "Prioritizing essential tasks and deferring non-critical computations. Implementing a minimum viable model configuration that operates at the lowest acceptable energy consumption, achieved by maximizing dropout rates and using the lowest quantization bit-widths. 2.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "Employing predictive energy harvesting models to anticipate energy availability and adjust computations proactively. Prioritizing essential tasks and deferring non-critical computations. 3.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "Employing predictive energy harvesting models to anticipate energy availability and adjust computations proactively. These strategies ensure that the system remains operational and provides degraded but acceptable performance under severe energy constraints. In extreme cases, the system can enter into a low-power standby mode and resume operation when sufficient energy is available.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "These strategies ensure that the system remains operational and provides degraded but acceptable performance under severe energy constraints. Novelty in Energy-Aware Scheduling:  While energy-aware scheduling is not novel in itself, our contribution lies in adapting scheduling algorithms specifically for intermittent power environments. Existing scheduling algorithms typically assume stable energy availability and do not account for the atomicity constraints imposed by intermittent power supply.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Existing scheduling algorithms typically assume stable energy availability and do not account for the atomicity constraints imposed by intermittent power supply. Our scheduling approach uniquely integrates: 1. Real-time energy availability into scheduling decisions.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Task fusion to minimize checkpointing overhead, which is critical in intermittent environments. Real-time energy availability into scheduling decisions. 2.",
      "type": "sliding_window_shuffled",
      "tokens": 27,
      "augmented": true
    },
    {
      "text": "Task fusion to minimize checkpointing overhead, which is critical in intermittent environments. 3. Dynamic adjustment of computational tasks based on both energy and task criticality.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "Dynamic adjustment of computational tasks based on both energy and task criticality. These innovations enable efficient and reliable DNN inference under intermittent power conditions, differentiating our work from existing energy-aware schedulers. Rationale Behind Method Design:  The overall method design of NExUME is motivated by the need to enable DNNs to function reliably in environments with intermittent and unpredictable energy supply.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "By integrating energy variability into both training and inference, we allow the DNN to adapt its computational load dynamically, ensuring that critical tasks are completed within energy constraints. This holistic approach addresses the limitations of existing methods that treat training and inference separately or do not account for real-time energy fluctuations. Rationale Behind Method Design:  The overall method design of NExUME is motivated by the need to enable DNNs to function reliably in environments with intermittent and unpredictable energy supply.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "This holistic approach addresses the limitations of existing methods that treat training and inference separately or do not account for real-time energy fluctuations. Implementation Details: We design a full software-compiler-hardware co-designed execution framework for commercial devices with non-volatility support (like MSP- EXP430FR5994 with FeRAM). Figure 2 shows a detailed overview of our execution design.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "To support user programs (  P1  ), we implement a moving window-based power predictor (  P2  ) which takes its input from the on-board EH capacitor. Considering the energy available, the predictor makes an informed decision on how to proceed. Recover(IS#)   Infer(RData) \nRdf4mFRAM(#3) \nret 2 //T=task_next ret -1 //pwr emgncy \nret 1 //Coreset ret 2 //T=task_next \nInt Pred_Pwr(task_next)    ...    ret 2 //T=task_next    ret 1 //save to coreset    ret 0 //backup    ret -1 //pwr emgncy \nPower  Emergency \nHardware Supported Monitoring, Backup and Restore \nP1 P2 L1 \nC1 C2 \nCn \nT1 T2 T3 \nLb \nLr \nL3 \nT4 \nT5 \nL3 \nFigure 2: Software-Compiler-Hardware Driven DynInfer Flow.",
      "type": "sliding_window_shuffled",
      "tokens": 247,
      "augmented": true
    },
    {
      "text": "These jobs form the functional program execution DAG. Considering the energy available, the predictor makes an informed decision on how to proceed. The compiler deconstructs the program into jobs to perform seamless program execution.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "However, certain jobs could be too big to execute atomically on harvested en- ergy. For ex- ample, for a DNN execution, the jobs could be CONV2D (  C1  ), batch normalization (  C2  ), etc. These jobs form the functional program execution DAG.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "However, certain jobs could be too big to execute atomically on harvested en- ergy. Therefore, we profile the tasks using the compute platform (in this case using the MSP-EXP430FR5994 and the LEA in it) to further divide the jobs into Power Atomic Tasks (QuantaTasks). These QuantaTasks are carefully coded with optimized assembly language to maximize their efficiency.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "In case of a power emergency, the task is abandoned and a hardware-assisted backup and restore is performed. These QuantaTasks are carefully coded with optimized assembly language to maximize their efficiency. We take advantage of the on-board NV FeRAM to perform backup and restore in case of power emergencies.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "6 \n4 Experimental Results \nNExUME can be seamlessly integrated as a “plug-in” for  both  training and inference frameworks in deep neural network (DNN) applications, specifically designed for intermittent and (ultra) low-power deployments. In this section, we discuss the effectiveness of NExUME across two distinct types of environments, highlighting its versatility and broad applicability. In case of a power emergency, the task is abandoned and a hardware-assisted backup and restore is performed.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "These datasets represent typical use cases in embedded systems where energy efficiency and minimal computational overhead are crucial. In this section, we discuss the effectiveness of NExUME across two distinct types of environments, highlighting its versatility and broad applicability. Firstly, we evaluate NExUME using publicly available datasets (§4.2) commonly utilized in embedded applications across multiple modalities—including image, time series sensor, and audio data.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Secondly, we introduce a novel dataset aimed at advancing research in predictive maintenance and Industry 4.0 (Lasi et al., 2014), and test NExUME on a real manufacturing testbed (§4.3) with COTS hardware. These datasets represent typical use cases in embedded systems where energy efficiency and minimal computational overhead are crucial. We use both commercial-off-the-shelf (COTS) hardware and state-of-the-art ReRAM Xbar- based hardware for this evaluation.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "4.1 Development and Profiling of NExUME \nNExUME uses a combination of programming languages and technologies to optimize its functional- ity in intermittent and low-power computing environments. We have developed a first-of-its-kind machine status monitoring dataset, available at  https://hackmd.io/@Galben/rk7YN6jmR , which involves mounting multiple types of sensors at various locations on a Bridgeport machine to monitor its activity status. Secondly, we introduce a novel dataset aimed at advancing research in predictive maintenance and Industry 4.0 (Lasi et al., 2014), and test NExUME on a real manufacturing testbed (§4.3) with COTS hardware.",
      "type": "sliding_window_shuffled",
      "tokens": 164,
      "augmented": true
    },
    {
      "text": "Our training infrastructure utilizes NVIDIA A6000 GPUs with 48 GiB of memory, supported by a 24-core Intel Xeon Gold 6336Y CPU. 4.1 Development and Profiling of NExUME \nNExUME uses a combination of programming languages and technologies to optimize its functional- ity in intermittent and low-power computing environments. The software stack comprises Python3 (2.7k lines of code), CUDA (1.1k lines of code), and Embedded C (2.1k lines of code, not including DSP libraries).",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Our training infrastructure utilizes NVIDIA A6000 GPUs with 48 GiB of memory, supported by a 24-core Intel Xeon Gold 6336Y CPU. To assess the computational overhead introduced by DynFit, a component of NExUME, we use NVIDIA Nsight Compute. We employ PyTorch v2.3.0 coupled with CUDA version 11.8 as our primary training framework.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "While the overhead in streaming multi-processor (SM) utilization was marginal (within 5%), there was a noticeable increase in memory bandwidth usage, ranging from 6% to 17%. To assess the computational overhead introduced by DynFit, a component of NExUME, we use NVIDIA Nsight Compute. During the training sessions enhanced by DynFit, we observed an increase in the number of instructions ranging from a minimum of 11.4% to a maximum of 34.2%.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "This adaptation is guided by the dropout mask vector and the specific type of sparse matrix operation being performed. Moreover, we have implemented a modified version of the matrix multiplication operation that strategically skips the loading of rows and/or columns from the input matrices into the GPU’s shared memory and register files. While the overhead in streaming multi-processor (SM) utilization was marginal (within 5%), there was a noticeable increase in memory bandwidth usage, ranging from 6% to 17%.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "This adaptation is guided by the dropout mask vector and the specific type of sparse matrix operation being performed. This technique effectively reduces the number of load operations by an average of 12%, thereby enhancing the efficiency of computations under energy constraints and contributing to the overall performance improvements in NExUME. 4.2 NExUME on Publicly Available Datasets \nDatasets:  For image data, we consider the Fashion-MNIST (Xiao et al., 2017) and CIFAR10 (Alex, 2009) datasets; for time series sensor data, we focus on popular human activity recognition (HAR) datasets, MHEALTH (Banos et al., 2014) and PAMAP2 (Reiss & Stricker, 2012); and for audio, we use the AudioMNIST (Becker et al., 2023) dataset.",
      "type": "sliding_window_shuffled",
      "tokens": 197,
      "augmented": true
    },
    {
      "text": "Inference Deployment Embedded Platforms:  For commercially off-the-shelf micro-controllers, we choose Texas Instruments MSP430FR5994 (Instruments, 2024a), and Arduino Nano 33 BLE Sense (Arduino, 2024) as our deployment platforms with a Pixel-5 phone as the host device. The host device is used for data logging—collecting SLOs, violations, power failures, etc., along with running the “baseline” inferences without intermittency. 4.2 NExUME on Publicly Available Datasets \nDatasets:  For image data, we consider the Fashion-MNIST (Xiao et al., 2017) and CIFAR10 (Alex, 2009) datasets; for time series sensor data, we focus on popular human activity recognition (HAR) datasets, MHEALTH (Banos et al., 2014) and PAMAP2 (Reiss & Stricker, 2012); and for audio, we use the AudioMNIST (Becker et al., 2023) dataset.",
      "type": "sliding_window_shuffled",
      "tokens": 256,
      "augmented": true
    },
    {
      "text": "Baselines:  We take the combination of best available approaches for DNN inference on intermittent environment as baselines. All these DNNs are executed with the state-of-the-art checkpointing and scheduling approach (Maeng & Lucia, 2018). The host device is used for data logging—collecting SLOs, violations, power failures, etc., along with running the “baseline” inferences without intermittency.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "All these DNNs are executed with the state-of-the-art checkpointing and scheduling approach (Maeng & Lucia, 2018). Baseline  Full Power  is a DNN designed by iNAS (Mendis et al., 2021) for running while the system is battery-powered and has to hit a target SLO (latency < 500ms). Baseline  AP  is a DNN compressed to fit the average power of the energy harvesting (EH) environment using iNAS (Mendis et al., 2021) and energy-aware pruning (EAP) (Yang et al., 2017, 2018).",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "Baseline  iNAS+PT designs the network from the ground up while combining the work of iNAS (Mendis et al., 2021) and EAP (Yang et al., 2018, 2017). Baseline  AP  is a DNN compressed to fit the average power of the energy harvesting (EH) environment using iNAS (Mendis et al., 2021) and energy-aware pruning (EAP) (Yang et al., 2017, 2018). Baseline  PT  takes the  Full Power  DNN and uses techniques proposed by (Yang et al., 2018) and (Yang et al., 2017) to prune, quantize, and compress the model.",
      "type": "sliding_window_shuffled",
      "tokens": 165,
      "augmented": true
    },
    {
      "text": "These methods introduce various techniques such as embedding state information into the DNN, multi-resolution inference, multi-exit architectures, and runtime reconfig- urability to handle intermittency in energy-harvesting devices. 7 \nWe also compare our approach with recent state-of-the-art methods specifically designed for in- termittent systems, namely  Stateful  (Yen et al., 2022),  ePerceptive  (Montanari et al., 2020), and DynBal  (Yen et al., 2023). Baseline  iNAS+PT designs the network from the ground up while combining the work of iNAS (Mendis et al., 2021) and EAP (Yang et al., 2018, 2017).",
      "type": "sliding_window_shuffled",
      "tokens": 185,
      "augmented": true
    },
    {
      "text": "These methods introduce various techniques such as embedding state information into the DNN, multi-resolution inference, multi-exit architectures, and runtime reconfig- urability to handle intermittency in energy-harvesting devices. We have faithfully re-implemented these methods as per the descriptions and adjusted them for a fair comparison under our setup. Results:  Table 1 shows the accuracy of our approach against the baselines and the recent state-of-the- art methods using the TI MSP board powered by piezoelectric energy harvesting.",
      "type": "sliding_window_shuffled",
      "tokens": 126,
      "augmented": true
    },
    {
      "text": "Results:  Table 1 shows the accuracy of our approach against the baselines and the recent state-of-the- art methods using the TI MSP board powered by piezoelectric energy harvesting. The inferences meeting the SLO requirements are the only ones considered for accuracy; i.e., a correct classification violating the latency SLO is considered as “incorrect”. Datasets Full Power AP PT iNAS+PT Stateful ePerceptive DynBal NExUME \nFMNIST 98.70 71.90 79.72 83.68 85.40 86.25 87.50 88.90 CIFAR10 89.81 55.05 62.00 66.98 68.50 70.20 71.75 76.29 MHEALTH 89.62 59.76 65.40 71.56 73.80 74.95 76.10 80.75 PAMAP 87.30 57.38 65.77 70.33 72.20 73.35 74.50 75.16 AudioMNIST 88.20 67.29 73.16 75.41 76.80 77.95 78.60 80.01 Table 1: Accuracy comparison on TI MSP board using piezoelectric energy harvesting.",
      "type": "sliding_window_shuffled",
      "tokens": 300,
      "augmented": true
    },
    {
      "text": "Datasets Full Power AP PT iNAS+PT Stateful ePerceptive DynBal NExUME \nFMNIST 98.70 71.90 79.72 83.68 85.40 86.25 87.50 88.90 CIFAR10 89.81 55.05 62.00 66.98 68.50 70.20 71.75 76.29 MHEALTH 89.62 59.76 65.40 71.56 73.80 74.95 76.10 80.75 PAMAP 87.30 57.38 65.77 70.33 72.20 73.35 74.50 75.16 AudioMNIST 88.20 67.29 73.16 75.41 76.80 77.95 78.60 80.01 Table 1: Accuracy comparison on TI MSP board using piezoelectric energy harvesting. As observed in Table 1, NExUME consistently outperforms the state-of-the-art methods across all datasets. For instance, on CIFAR10, NExUME achieves an accuracy of 76.29%, which is approximately 4.54% higher than DynBal, the next best method.",
      "type": "sliding_window_shuffled",
      "tokens": 285,
      "augmented": true
    },
    {
      "text": "This improvement is significant in the context of energy-harvesting intermittent systems, where achieving high accuracy under strict energy constraints is challenging. For instance, on CIFAR10, NExUME achieves an accuracy of 76.29%, which is approximately 4.54% higher than DynBal, the next best method. The superior performance of NExUME can be attributed to its unique integration of energy variability awareness directly into both the training (DynFit) and inference (DynInfer) processes.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "Unlike other methods that either focus on modifying the DNN architecture or optimizing inference configurations, NExUME adapts the DNN’s computational complexity in real-time based on instantaneous energy availability, leading to more efficient use of scarce energy resources and improved accuracy. The superior performance of NExUME can be attributed to its unique integration of energy variability awareness directly into both the training (DynFit) and inference (DynInfer) processes. Dataset Platform Energy Source Stateful ePerceptive DynBal NExUME \nFMNIST MSP430FR5994 Piezoelectric 20.1 20.8 21.5 23.4 CIFAR10 Arduino Nano Thermal 16.0 16.5 17.0 18.5 MHEALTH ESP32 S3 Eye Piezoelectric 18.5 19.0 19.6 21.0 PAMAP STM32H7 Thermal 16.5 17.0 17.5 19.0 AudioMNIST Raspberry Pi Pico Piezoelectric 20.5 21.0 21.7 23.2 Table 2: Energy efficiency comparison on different hardware platforms.",
      "type": "sliding_window_shuffled",
      "tokens": 232,
      "augmented": true
    },
    {
      "text": "Dataset Platform Energy Source Stateful ePerceptive DynBal NExUME \nFMNIST MSP430FR5994 Piezoelectric 20.1 20.8 21.5 23.4 CIFAR10 Arduino Nano Thermal 16.0 16.5 17.0 18.5 MHEALTH ESP32 S3 Eye Piezoelectric 18.5 19.0 19.6 21.0 PAMAP STM32H7 Thermal 16.5 17.0 17.5 19.0 AudioMNIST Raspberry Pi Pico Piezoelectric 20.5 21.0 21.7 23.2 Table 2: Energy efficiency comparison on different hardware platforms. Table 2 presents the energy efficiency in MOps/Joule for each dataset on different hardware platforms using piezoelectric and thermal energy harvesting. NExUME achieves the highest energy efficiency across all platforms and datasets.",
      "type": "sliding_window_shuffled",
      "tokens": 175,
      "augmented": true
    },
    {
      "text": "This demonstrates that NExUME not only improves accuracy but also enhances energy utilization, making it highly suitable for deployment in energy-constrained intermittent environments. NExUME achieves the highest energy efficiency across all platforms and datasets. The improvements in energy efficiency are due to NExUME’s ability to adjust computational workload dynamically, minimizing energy wastage and ensuring that computa- tions are matched to the available energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "The improvements in energy efficiency are due to NExUME’s ability to adjust computational workload dynamically, minimizing energy wastage and ensuring that computa- tions are matched to the available energy budget. NExUME, thanks to its inherent learnt adaptability, significantly reduces saves, restores, reconfigurations and READ/WRITE from/to nonvolatile memory or to the flash memory in the cases and devices where NVMs are not present which gives it edge over the baselines across multiple devices. Discussion of Results:  1.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "Discussion of Results:  1. Dynamic Adaptation:  NExUME’s DynFit and DynInfer components enable real-time adjustments of dropout rates and quantization levels during training and inference based on instantaneous energy availability. This allows the DNN to maintain high accuracy even under severe energy constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "Energy Variability Awareness:  By integrating energy profiles directly into the training process, NExUME ensures that the model learns to handle fluctuations in energy supply, leading to more robust performance compared to methods that do not consider energy variability during training. This allows the DNN to maintain high accuracy even under severe energy constraints. 2.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "3. Energy Variability Awareness:  By integrating energy profiles directly into the training process, NExUME ensures that the model learns to handle fluctuations in energy supply, leading to more robust performance compared to methods that do not consider energy variability during training. Efficient Scheduling:  DynInfer’s energy-aware task scheduling and task fusion mechanisms reduce overhead from checkpointing and optimize the execution of tasks within \n8 \nthe available energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Holistic Approach:  Unlike other methods that focus on either training or inference optimizations, NExUME provides a comprehensive solution that addresses both phases, leading to superior overall performance. Efficient Scheduling:  DynInfer’s energy-aware task scheduling and task fusion mechanisms reduce overhead from checkpointing and optimize the execution of tasks within \n8 \nthe available energy budget. 4.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "Holistic Approach:  Unlike other methods that focus on either training or inference optimizations, NExUME provides a comprehensive solution that addresses both phases, leading to superior overall performance. 4.3 NExUME on Machine Status Monitoring  [Our New Dataset] \nAutomation and monitoring and analytics are the key ingredients in the upcoming Industry 4.0. To enable sustainable machine status monitoring with energy harvesting (from machine vibrations or Wifi signals) we evaluate our setup using Bridgeport machines for monitoring their status.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "To enable sustainable machine status monitoring with energy harvesting (from machine vibrations or Wifi signals) we evaluate our setup using Bridgeport machines for monitoring their status. Prior works (Center, 2018) majorly focused on fault analysis but there are little to no datasets on predictive maintenance. Setup and Sensor Arrangement:  Two different types of 3-axis accelerometers (with 100Hz and 200Hz sampling rate) were placed in three different locations of a Bridgeport machine to collect and analyze data under different operating status.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "We collected over 700,000 samples over a period of 2 hours for each of the sensors. There were 5 operating statuses: three different speeds of rotation of the spindle ( R1: 100RPM ,  R2: 200RPM ,  R3: 300RMP  with no job; RPM – rotations per minute), spindle under job ( SJ ), and spindle idle ( SI ). Setup and Sensor Arrangement:  Two different types of 3-axis accelerometers (with 100Hz and 200Hz sampling rate) were placed in three different locations of a Bridgeport machine to collect and analyze data under different operating status.",
      "type": "sliding_window_shuffled",
      "tokens": 143,
      "augmented": true
    },
    {
      "text": "We collected over 700,000 samples over a period of 2 hours for each of the sensors. We use iNAS (Mendis et al., 2021) to find the DNNs meeting the energy income and train them using our proposed DynFit. The sensor data were cleaned, normalized, and converted to the power spectrum density for further analysis.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "Table 3 shows the accuracy of classification tasks against the different baselines and state-of-the-art methods. Class Full Power AP PT iNAS+PT Stateful ePerceptive DynBal NExUME \nR1 84.93 74.46 77.02 79.62 80.85 81.50 82.15 83.60 R2 85.85 76.21 79.18 80.36 81.95 82.60 83.25 84.50 R3 81.09 72.43 75.38 78.18 79.05 79.70 80.35 80.85 SJ 90.95 82.33 85.00 87.58 88.60 89.15 89.80 90.50 SI 94.76 85.31 88.05 89.90 91.00 91.65 92.30 93.00 Table 3: Accuracy of NExUME and other methods for industry status monitoring dataset using TI MSP board and piezoelectric energy source. We use iNAS (Mendis et al., 2021) to find the DNNs meeting the energy income and train them using our proposed DynFit.",
      "type": "sliding_window_shuffled",
      "tokens": 278,
      "augmented": true
    },
    {
      "text": "Results collected over 200 experiment cycles. Class Full Power AP PT iNAS+PT Stateful ePerceptive DynBal NExUME \nR1 84.93 74.46 77.02 79.62 80.85 81.50 82.15 83.60 R2 85.85 76.21 79.18 80.36 81.95 82.60 83.25 84.50 R3 81.09 72.43 75.38 78.18 79.05 79.70 80.35 80.85 SJ 90.95 82.33 85.00 87.58 88.60 89.15 89.80 90.50 SI 94.76 85.31 88.05 89.90 91.00 91.65 92.30 93.00 Table 3: Accuracy of NExUME and other methods for industry status monitoring dataset using TI MSP board and piezoelectric energy source. NExUME demonstrates superior performance across all operating classes, achieving the highest accuracy in each case.",
      "type": "sliding_window_shuffled",
      "tokens": 245,
      "augmented": true
    },
    {
      "text": "While the margins may appear small, in industrial settings, even minor improvements in classification accuracy can have significant implications for predictive maintenance and operational efficiency. For example, for the spindle idle (SI) class, NExUME attains an accuracy of 93.00%, outperforming DynBal by 0.70%. NExUME demonstrates superior performance across all operating classes, achieving the highest accuracy in each case.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "By effectively managing energy constraints and adapting to intermittent power conditions, NExUME enables more reliable and accurate monitoring in industrial environments where energy harvesting is a viable power solution. The improved performance of NExUME in this real-world application further validates its effectiveness and practical utility. While the margins may appear small, in industrial settings, even minor improvements in classification accuracy can have significant implications for predictive maintenance and operational efficiency.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "This study involved adjusting the acceptable latency and the capacitance of the energy harvesting setup to assess their impacts on accuracy. By effectively managing energy constraints and adapting to intermittent power conditions, NExUME enables more reliable and accurate monitoring in industrial environments where energy harvesting is a viable power solution. 4.4 Sensitivity and Ablation Studies of NExUME \nTo elucidate the influence of variable SLOs and hardware-specific settings on system performance, we conducted a comprehensive sensitivity study.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "As shown in Figure 3a, the accuracy improves with increased latency, but with diminishing returns. Similarly, Figure 3b demonstrates that, while increasing capacitance should theoretically stabilize the system, its charging characteristics can lead to extended charging times, thus exceeding the latency SLO. This study involved adjusting the acceptable latency and the capacitance of the energy harvesting setup to assess their impacts on accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "Similarly, Figure 3b demonstrates that, while increasing capacitance should theoretically stabilize the system, its charging characteristics can lead to extended charging times, thus exceeding the latency SLO. An ablation study evaluates the contributions of individual components within NExUME. Notably, some anomalies in the data were attributed to abrupt power failures, a common challenge in intermittent energy harvesting systems.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "The results, plotted in Figure 3c, indicate that the greatest improvements are derived from the “synergistic operation” of all components, particularly DynFit and DynInfer. An ablation study evaluates the contributions of individual components within NExUME. Although iNAS enhances network selection, its lack of intermittency awareness significantly impacts accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "However, deploying such architectures on ultra-low-power, energy-harvesting devices presents significant challenges due to their substantial \n9 \n200 250 300 350 400 450 500 550 600 \nLatency (ms) \n78 80 82 84 86 88 90 92 94 \nAccuracy (%) \nAccuracy vs. Latency for Different Classes \nR1 R2 R3 SJ SI \n(a) Accuracy vs Latency \n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Capacitance (F) \n78 \n80 \n82 \n84 \n86 \n88 \n90 \n92 \nAccuracy (%) \nAccuracy vs. Capacitance for Different Classes \nR1 R2 R3 SJ SI \n(b) Accuracy vs Capacitance \nFMNIST CIFAR10 MHEALTH PAMAP AudioMNISTMachine Dataset \n0 \n20 \n40 \n60 \n80 \n100 \nAccuracy (%) \nAbalation Study \nDN DN+DF DN+DF+DI \n(c) Ablation Study Figure 3: Sensitivity and ablation study. Although iNAS enhances network selection, its lack of intermittency awareness significantly impacts accuracy. 4.5 Limitations and Discussion \nWe recognize that modern architectures like Transformers have become prevalent in the ML commu- nity due to their superior performance on large-scale datasets.",
      "type": "sliding_window_shuffled",
      "tokens": 308,
      "augmented": true
    },
    {
      "text": "However, deploying such architectures on ultra-low-power, energy-harvesting devices presents significant challenges due to their substantial \n9 \n200 250 300 350 400 450 500 550 600 \nLatency (ms) \n78 80 82 84 86 88 90 92 94 \nAccuracy (%) \nAccuracy vs. Latency for Different Classes \nR1 R2 R3 SJ SI \n(a) Accuracy vs Latency \n1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Capacitance (F) \n78 \n80 \n82 \n84 \n86 \n88 \n90 \n92 \nAccuracy (%) \nAccuracy vs. Capacitance for Different Classes \nR1 R2 R3 SJ SI \n(b) Accuracy vs Capacitance \nFMNIST CIFAR10 MHEALTH PAMAP AudioMNISTMachine Dataset \n0 \n20 \n40 \n60 \n80 \n100 \nAccuracy (%) \nAbalation Study \nDN DN+DF DN+DF+DI \n(c) Ablation Study Figure 3: Sensitivity and ablation study. computational and memory requirements. DN is DynNAS, DF is DynFit, and DI is DynInfer.",
      "type": "sliding_window_shuffled",
      "tokens": 273,
      "augmented": true
    },
    {
      "text": "NExUME focuses on enabling efficient and reliable deployment of DNNs in intermittent environments, which are often constrained in terms of com- putational resources and energy availability. In many real-world applications, especially in IoT and edge computing, there is a critical need for smaller, energy-efficient models that can operate autonomously without reliance on batteries. computational and memory requirements.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Moreover, we believe that advancing the capabilities of smaller models in intermittent environments is crucial for widespread adoption of sustainable, battery-free devices in various domains, including environmental monitoring, industrial IoT, and healthcare. These tiny, reusable devices contribute to reducing embodied carbon and represent a significant step toward sustainability. In many real-world applications, especially in IoT and edge computing, there is a critical need for smaller, energy-efficient models that can operate autonomously without reliance on batteries.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "By addressing the challenges of intermittent computing, our work contributes to the broader goal of enabling pervasive, sustainable intelligence at the edge. NExUME is especially advantageous in intermittent environments, and its utility extends to ultra- low-power or energy scavenging systems. Moreover, we believe that advancing the capabilities of smaller models in intermittent environments is crucial for widespread adoption of sustainable, battery-free devices in various domains, including environmental monitoring, industrial IoT, and healthcare.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Additionally, profiling devices to ascertain their energy consumption, computational capabilities, and memory footprint necessitates detailed micro- profiling using embedded programming. However, the efficacy of DynFit and iNAS is contingent upon the breadth and depth of the available dataset. NExUME is especially advantageous in intermittent environments, and its utility extends to ultra- low-power or energy scavenging systems.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "This process, while informative, yields only approximate models that are inherently prone to errors. Additionally, profiling devices to ascertain their energy consumption, computational capabilities, and memory footprint necessitates detailed micro- profiling using embedded programming. DynFit, with its stochastic dropout features, occasionally leads to overfitting, necessitating meticulous fine-tuning.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "DynFit, with its stochastic dropout features, occasionally leads to overfitting, necessitating meticulous fine-tuning. While effective in smaller networks, our studies involving larger datasets (such as ImageNet) and more complex network architectures (like MobileNetV2 and ResNet) reveal challenges in achieving convergence without precise fine-tuning. DynFit tends to introduce multiple intermediate states during the training process, resulting in approximately 14% additional wall-time on average.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "The absence of comprehensive library functions along with the need for computational efficiency frequently necessitates the development of in-line assembly code for certain computational kernels. DynFit tends to introduce multiple intermediate states during the training process, resulting in approximately 14% additional wall-time on average. The development of DynInfer requires an in-depth understanding of microcontroller programming and compiler directives.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "By integrating adaptive neural architecture and energy-aware training techniques, NExUME significantly enhances the viability of deploying machine learning models in environments with limited and unreliable energy sources. The absence of comprehensive library functions along with the need for computational efficiency frequently necessitates the development of in-line assembly code for certain computational kernels. 5 Conclusions \nThis study presents NExUME, an advanced framework designed to optimize the training and inference phases of deep neural networks within the constraints of intermittently powered, energy-harvesting devices.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "By integrating adaptive neural architecture and energy-aware training techniques, NExUME significantly enhances the viability of deploying machine learning models in environments with limited and unreliable energy sources. Specifically, improvements ranging from 6.10% to 17.13% over existing methods highlight NExUME’s capability to adapt dynamically to fluctuating energy conditions, ensuring both operational longevity and computational integrity. The results from our extensive evaluations demonstrate that NExUME can substantially outperform traditional methods in energy-constrained settings, with improvements in accuracy and efficiency that facilitate real-world applications in remote and wearable technology.",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "The broader implication of this work extends beyond technological advancements, suggesting a paradigm shift in how the machine learning community approaches the design and deployment of systems in energy-limited environments. By prioritizing energy efficiency and system adaptability, NExUME contributes to the sustainability and accessibility of machine learning solutions, enabling their deployment in regions where power infrastructure is absent or unreliable. Specifically, improvements ranging from 6.10% to 17.13% over existing methods highlight NExUME’s capability to adapt dynamically to fluctuating energy conditions, ensuring both operational longevity and computational integrity.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "This is particularly crucial in developing regions where such technology can drive innovation in healthcare, agriculture, and education. Furthermore, the development of energy-efficient, adaptive systems like NExUME is aligned with the growing need for sustainable computing practices across all disciplines of technology. By prioritizing energy efficiency and system adaptability, NExUME contributes to the sustainability and accessibility of machine learning solutions, enabling their deployment in regions where power infrastructure is absent or unreliable.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "It challenges the machine learning community to consider not only the accuracy and efficiency of algorithms but also their environmental impact and accessibility, ensuring a broader positive social impact. 10 \nReferences \nSayed Saad Afzal, Waleed Akbar, Osvy Rodriguez, Mario Doumet, Unsoo Ha, Reza Ghaffarivar- davagh, and Fadel Adib. Furthermore, the development of energy-efficient, adaptive systems like NExUME is aligned with the growing need for sustainable computing practices across all disciplines of technology.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "Battery-free wireless imaging of underwater environments. Nature communications , 13(1):5546, 2022. 10 \nReferences \nSayed Saad Afzal, Waleed Akbar, Osvy Rodriguez, Mario Doumet, Unsoo Ha, Reza Ghaffarivar- davagh, and Fadel Adib.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "Nature communications , 13(1):5546, 2022. Learning multiple layers of features from tiny images. Krizhevsky Alex.",
      "type": "sliding_window_shuffled",
      "tokens": 30,
      "augmented": true
    },
    {
      "text": "cs. Learning multiple layers of features from tiny images. https://www.",
      "type": "sliding_window_shuffled",
      "tokens": 18,
      "augmented": true
    },
    {
      "text": "edu/kriz/learning-features-2009-TR. pdf , 2009. Arduino.",
      "type": "sliding_window_shuffled",
      "tokens": 23,
      "augmented": true
    },
    {
      "text": "Arduino nano 33 ble sense with headers. Arduino. https://store-usa.arduino.cc/products/ arduino-nano-33-ble-sense-with-headers , 2024.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "Oresti Banos, Rafael Garcia, Juan A Holgado-Terriza, Miguel Damas, Hector Pomares, Ignacio Rojas, Alejandro Saez, and Claudia Villalonga. Accessed on 05/19/2024. https://store-usa.arduino.cc/products/ arduino-nano-33-ble-sense-with-headers , 2024.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "In  Ambient Assisted Living and Daily Activities: 6th International Work-Conference, IWAAL 2014, Belfast, UK, December 2-5, 2014. mhealthdroid: a novel framework for agile development of mobile health applications. Oresti Banos, Rafael Garcia, Juan A Holgado-Terriza, Miguel Damas, Hector Pomares, Ignacio Rojas, Alejandro Saez, and Claudia Villalonga.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "In  Ambient Assisted Living and Daily Activities: 6th International Work-Conference, IWAAL 2014, Belfast, UK, December 2-5, 2014. 91–98. Proceedings 6 , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "Sören Becker, Johanna Vielhaben, Marcel Ackermann, Klaus-Robert Müller, Sebastian Lapuschkin, and Wojciech Samek. Springer, 2014. 91–98.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "Journal of the Franklin Institute , 2023. Sören Becker, Johanna Vielhaben, Marcel Ackermann, Klaus-Robert Müller, Sebastian Lapuschkin, and Wojciech Samek. Audiomnist: Exploring explainable artificial intelligence for audio analysis on a simple benchmark.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "ISSN 0016-0032. doi: https: //doi.org/10.1016/j.jfranklin.2023.11.038. Journal of the Franklin Institute , 2023. URL  https://www.sciencedirect.com/science/ article/pii/S0016003223007536 .",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "Bearing fault data. URL  https://www.sciencedirect.com/science/ article/pii/S0016003223007536 . Case Western Reserve University Bearing Data Center.",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "case.edu/bearingdatacenter/download-data-file , 2018. Bearing fault data. https://engineering.",
      "type": "sliding_window_shuffled",
      "tokens": 30,
      "augmented": true
    },
    {
      "text": "Graham Gobieski, Nathan Beckmann, and Brandon Lucia. Accessed: 2024-11-27. case.edu/bearingdatacenter/download-data-file , 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "In  SysML Conference , pp. Graham Gobieski, Nathan Beckmann, and Brandon Lucia. Intermittent deep neural network inference.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "1–3, 2018. In  SysML Conference , pp. Graham Gobieski, Brandon Lucia, and Nathan Beckmann.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "Graham Gobieski, Brandon Lucia, and Nathan Beckmann. Intelligence beyond the edge: Inference on intermittent embedded systems. In  Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems , pp. 199–213, 2019. Transforma Insights.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "Iot & ai market forecasts. Transforma Insights. https://transformainsights.com/research/ tam/market , 2023.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Accessed: 05/19/2021. Texas Instruments. https://transformainsights.com/research/ tam/market , 2023.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "Texas Instruments. Msp430fr5994 mixed-signal microcontrollers. https://www.ti.com/ product/MSP430FR5994 , 2024a.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "Accessed: 05/19/2024. Texas Instruments. https://www.ti.com/ product/MSP430FR5994 , 2024a.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "Msp dsp library: Low energy accelerator (lea) user’s guide. Texas Instruments. https://software-dl.ti.com/msp430/msp430_public_sw/mcu/msp430/DSPLib/ 1_30_00_02/exports/html/usersguide_lea.html , 2024b.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Accessed: 05/19/2024. https://software-dl.ti.com/msp430/msp430_public_sw/mcu/msp430/DSPLib/ 1_30_00_02/exports/html/usersguide_lea.html , 2024b. Bashima Islam and Shahriar Nirjon.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "arXiv preprint arXiv:1905.03854 , 2019. Zygarde: Time-sensitive on-device deep inference and adaptation on intermittently-powered systems. Bashima Islam and Shahriar Nirjon.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "arXiv preprint arXiv:1905.03854 , 2019. Enabling fast deep learning on tiny energy-harvesting iot devices. Sahidul Islam, Jieren Deng, Shanglin Zhou, Chen Pan, Caiwen Ding, and Mimi Xie.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "921–926. Enabling fast deep learning on tiny energy-harvesting iot devices. In  2022 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "921–926. IEEE, 2022. Chih-Kai Kang, Hashan Roshantha Mendis, Chun-Han Lin, Ming-Syan Chen, and Pi-Cheng Hsiu.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Chih-Kai Kang, Hashan Roshantha Mendis, Chun-Han Lin, Ming-Syan Chen, and Pi-Cheng Hsiu. Everything leaves footprints: Hardware accelerated intermittent deep inference. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 39(11):3479–3491, 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "Chih-Kai Kang, Hashan Roshantha Mendis, Chun-Han Lin, Ming-Syan Chen, and Pi-Cheng Hsiu. More is less: Model augmentation for intermittent deep inference. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 39(11):3479–3491, 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "Heiner Lasi, Peter Fettke, Hans-Georg Kemper, Thomas Feld, and Michael Hoffmann. ACM Transactions on Embedded Computing Systems (TECS) , 21(5):1–26, 2022. More is less: Model augmentation for intermittent deep inference.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "11 \nYongpan Liu, Zewei Li, Hehe Li, Yiqun Wang, Xueqing Li, Kaisheng Ma, Shuangchen Li, Meng-Fan Chang, Sampson John, Yuan Xie, et al. Heiner Lasi, Peter Fettke, Hans-Georg Kemper, Thomas Feld, and Michael Hoffmann. Industry 4.0. Business & information systems engineering , 6(4):239–242, 2014.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the 52nd Annual Design Automation Conference , pp. Ambient energy harvesting nonvolatile processors: From circuit to system. 11 \nYongpan Liu, Zewei Li, Hehe Li, Yiqun Wang, Xueqing Li, Kaisheng Ma, Shuangchen Li, Meng-Fan Chang, Sampson John, Yuan Xie, et al.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "1–6, 2015. Mingsong Lv and Enyu Xu. In  Proceedings of the 52nd Annual Design Automation Conference , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "Efficient dnn execution on intermittently-powered iot devices with depth-first inference. Mingsong Lv and Enyu Xu. IEEE Access , 10:101999–102008, 2022.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "IEEE Access , 10:101999–102008, 2022. Kaisheng Ma, Xueqing Li, Karthik Swaminathan, Yang Zheng, Shuangchen Li, Yongpan Liu, Yuan Xie, John Jack Sampson, and Vijaykrishnan Narayanan. Nonvolatile processor architectures: Efficient, reliable progress with unstable power.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "IEEE Micro , 36(3):72–83, 2016. Kaisheng Ma, Xueqing Li, Jinyang Li, Yongpan Liu, Yuan Xie, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Nonvolatile processor architectures: Efficient, reliable progress with unstable power.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Incidental computing on iot nonvolatile processors. Kaisheng Ma, Xueqing Li, Jinyang Li, Yongpan Liu, Yuan Xie, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "In Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture , pp. 204–218, 2017. Kiwan Maeng and Brandon Lucia.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "Kiwan Maeng and Brandon Lucia. In  13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18) , pp. Adaptive dynamic checkpointing for safe efficient intermittent computing.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "Hashan Roshantha Mendis, Chih-Kai Kang, and Pi-cheng Hsiu. In  13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18) , pp. 129–144, 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "Hashan Roshantha Mendis, Chih-Kai Kang, and Pi-cheng Hsiu. Intermittent-aware neural architecture search. ACM Transactions on Embedded Computing Systems (TECS) , 20(5s):1–27, 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "ACM Transactions on Embedded Computing Systems (TECS) , 20(5s):1–27, 2021. Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks. Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "Origin: Enabling on-device intelligence for human activity recognition using energy harvesting wireless sensor networks. 1414–1419. In  2021 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan, and Chita R Das. 1414–1419. IEEE, 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "In  2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA) , pp. Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, Vijaykrishnan Narayanan, and Chita R Das. Usas: A sustainable continuous-learning´ framework for edge servers.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "IEEE, 2024. In  2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA) , pp. 891–907.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "Alessandro Montanari, Manuja Sharma, Dainius Jenkus, Mohammed Alloulah, Lorena Qendro, and Fahim Kawsar. eperceptive: energy reactive embedded intelligence for batteryless sensors. IEEE, 2024.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "In Proceedings of the 18th Conference on Embedded Networked Sensor Systems , pp. 382–394, 2020. eperceptive: energy reactive embedded intelligence for batteryless sensors.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "Keni Qiu, Nicholas Jao, Mengying Zhao, Cyan Subhra Mishra, Gulsum Gudukbay, Sethu Jose, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent embedded processors. 382–394, 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "In  2020 IEEE International Symposium on High Performance Computer Architecture (HPCA) , pp. 315–327. Resirca: A resilient energy harvesting reram crossbar-based accelerator for intelligent embedded processors.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "IEEE, 2020. 315–327. Attila Reiss and Didier Stricker.",
      "type": "sliding_window_shuffled",
      "tokens": 25,
      "augmented": true
    },
    {
      "text": "In 2012 16th international symposium on wearable computers , pp. Introducing a new benchmarked dataset for activity monitoring. Attila Reiss and Didier Stricker.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "108–109. In 2012 16th international symposium on wearable computers , pp. IEEE, 2012.",
      "type": "sliding_window_shuffled",
      "tokens": 24,
      "augmented": true
    },
    {
      "text": "Mouse: Inference in non-volatile memory for energy harvesting applications. Salonik Resch, S Karen Khatamifard, Zamshed I Chowdhury, Masoud Zabihi, Zhengyang Zhao, Husrev Cilasun, Jian-Ping Wang, Sachin S Sapatnekar, and Ulya R Karpuzcu. IEEE, 2012.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "Mouse: Inference in non-volatile memory for energy harvesting applications. 400–414. In  2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO) , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "IEEE, 2020. 400–414. Ali Saffari, Sin Yong Tan, Mohamad Katanbaf, Homagni Saha, Joshua R Smith, and Soumik Sarkar.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning , pp. Battery-free camera occupancy detection system. Ali Saffari, Sin Yong Tan, Mohamad Katanbaf, Homagni Saha, Joshua R Smith, and Soumik Sarkar.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "13–18, 2021. Tianyi Shen, Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. In  Proceedings of the 5th International Workshop on Embedded and Mobile Deep Learning , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "Tianyi Shen, Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. IEEE Embedded Systems Letters , 2022. An efficient edge-cloud partitioning of random forests for distributed sensor networks.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "IEEE Embedded Systems Letters , 2022. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. 12 \nTien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. arXiv preprint arXiv:1708.07747 , 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 12 \nTien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using energy-aware pruning.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "5687–5695, 2017. In  Proceedings of the IEEE conference on computer vision and pattern recognition , pp. Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of the European conference on computer vision (ECCV) , pp. Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "285–300, 2018. Chih-Hsuan Yen, Hashan Roshantha Mendis, Tei-Wei Kuo, and Pi-Cheng Hsiu. In Proceedings of the European conference on computer vision (ECCV) , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "Stateful neural networks for intermittent systems. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 41(11):4229–4240, 2022. Chih-Hsuan Yen, Hashan Roshantha Mendis, Tei-Wei Kuo, and Pi-Cheng Hsiu.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 41(11):4229–4240, 2022. Keep in balance: Runtime-reconfigurable intermittent deep inference. Chih-Hsuan Yen, Hashan Roshantha Mendis, Tei-Wei Kuo, and Pi-Cheng Hsiu.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "13 \nA More Results on Other Platforms and EH Sources \nFigure 4: Hardware setup of NExUME using MSP-EXP430FR5994 as the edge compute, Adafruit ItsyBitsy nRF52840 Express for communicating, Energy Harvester Breakout - LTC3588 with super- capacitors as energy rectification and storage and a Pixel-5 phone as the host. ACM Transactions on Embedded Computing Systems , 22(5s):1–25, 2023. Keep in balance: Runtime-reconfigurable intermittent deep inference.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "Better refers to the improvement over iNAS+PT baseline. 13 \nA More Results on Other Platforms and EH Sources \nFigure 4: Hardware setup of NExUME using MSP-EXP430FR5994 as the edge compute, Adafruit ItsyBitsy nRF52840 Express for communicating, Energy Harvester Breakout - LTC3588 with super- capacitors as energy rectification and storage and a Pixel-5 phone as the host. Datasets Full Power MSP on Piezo AP PT iNAS+PT NExUME Better FMNIST 98.70 71.90 79.72 83.68 88.90 6.24% CIFAR10 89.81 55.05 62.00 66.98 76.29 13.90% MHEALTH 89.62 59.76 65.40 71.56 80.75 12.84% PAMAP 87.30 57.38 65.77 65.38 75.16 14.97% AudioMNIST 88.20 67.29 73.16 75.41 80.01 6.10% Table 4: Accuracy of NExUME on MSP board using vibration from a Piezoelectric harvestor.",
      "type": "sliding_window_shuffled",
      "tokens": 280,
      "augmented": true
    },
    {
      "text": "Better refers to the improvement over iNAS+PT baseline. Datasets Full Power MSP on Thermal AP PT iNAS+PT NExUME Better FMNIST 98.70 80.92 86.32 88.93 95.62 7.53% CIFAR10 89.81 64.78 69.29 71.53 83.78 17.13% MHEALTH 89.62 69.77 73.99 77.70 89.62 15.34% PAMAP 87.30 66.33 71.84 74.47 85.24 14.46% AudioMNIST 88.20 73.84 78.03 81.60 87.64 7.40% Table 5: Accuracy of NExUME on MSP board using thermocouple based thermal harvester. Better refers to the improvement over iNAS+PT baseline.",
      "type": "sliding_window_shuffled",
      "tokens": 204,
      "augmented": true
    },
    {
      "text": "Better refers to the improvement over iNAS+PT baseline. Datasets Full Power Arduino on RF AP PT iNAS+PT NExUME Better FMNIST 98.70 74.44 79.63 83.61 90.44 8.17% CIFAR10 89.81 58.11 63.91 65.01 79.60 22.44% MHEALTH 89.62 63.52 67.40 74.30 83.86 12.87% PAMAP 87.30 61.39 67.24 69.45 77.00 10.87% AudioMNIST 88.20 66.11 74.28 76.60 78.87 2.97% Table 6: Accuracy of NExUME on Arduino nano board using WiFi based RF harvester. Better refers to the improvement over iNAS+PT baseline.",
      "type": "sliding_window_shuffled",
      "tokens": 205,
      "augmented": true
    },
    {
      "text": "Better refers to the improvement over iNAS+PT baseline. 14 \nDatasets Full Power Arduino on Thermal AP PT iNAS+PT NExUME Better FMNIST 98.70 77.04 80.44 83.08 89.90 8.20% CIFAR10 89.81 60.38 65.90 66.98 80.70 20.48% MHEALTH 89.62 65.74 69.88 72.41 85.75 18.42% PAMAP 87.30 62.76 65.93 71.46 81.27 13.73% AudioMNIST 88.20 69.12 73.86 77.79 83.54 7.39% Table 7: Accuracy of NExUME on Arduino nano board using thermocouple based thermal harvester. Better refers to the improvement over iNAS+PT baseline.",
      "type": "sliding_window_shuffled",
      "tokens": 201,
      "augmented": true
    },
    {
      "text": "Here’s a simplified breakdown of the process: \n1. Better refers to the improvement over iNAS+PT baseline. B Details on Energy Harvesting \nA typical energy harvesting (EH) setup captures and converts environmental energy into usable electrical power, which can then support various electronic devices.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "Here’s a simplified breakdown of the process: \n1. Energy Capture : The setup begins with a harvester, such as a solar panel, piezoelectric sensor, or thermocouple. These devices are designed to collect energy from their surround- ings—light, mechanical vibrations, or heat, respectively.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "2. These devices are designed to collect energy from their surround- ings—light, mechanical vibrations, or heat, respectively. Power Conditioning : Once energy is harvested, it often needs to be converted and stabilized for use.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "This is done using a rectifier, which transforms alternating current (AC) into a more usable direct current (DC). 3. Power Conditioning : Once energy is harvested, it often needs to be converted and stabilized for use.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "A matching circuit, including components like buck or boost converters, adjusts the voltage to the appropriate level, ensuring the device receives the correct current and voltage. Voltage Regulation : After rectification, the power might not be at the right voltage for the device it needs to support. 3.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "A matching circuit, including components like buck or boost converters, adjusts the voltage to the appropriate level, ensuring the device receives the correct current and voltage. Energy Storage : Finally, to ensure a continuous power supply even when the immediate energy source is inconsistent (like when a cloud passes over a solar panel), the system includes a temporary storage unit, such as a super-capacitor. 4.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "By integrating these components, an EH system can sustainably power devices without relying on traditional power grids, making it ideal for remote or mobile applications. This component helps smooth out the supply, providing steady power to the compute circuit. Energy Storage : Finally, to ensure a continuous power supply even when the immediate energy source is inconsistent (like when a cloud passes over a solar panel), the system includes a temporary storage unit, such as a super-capacitor.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "By integrating these components, an EH system can sustainably power devices without relying on traditional power grids, making it ideal for remote or mobile applications. C Intermittent Computing and Check-pointing \nC.1 Intermittency-Aware General Matrix Multiplication (GeMM) \nHere we explain the operation of an energy-aware algorithm for performing General Matrix Multipli- cation (GeMM). The algorithm is designed to operate in environments where energy availability is intermittent, such as in devices powered by energy harvesting.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "It includes mechanisms for loop tiling, checkpointing, and resumption to manage computation across power interruptions effectively. The algorithm is designed to operate in environments where energy availability is intermittent, such as in devices powered by energy harvesting. C.1.1 Algorithm Overview \nThe GeMM operation, typically expressed as  C  =  A  ×  B , where  A ,  B , and  C  are matrices, is imple- mented with considerations for energy limitations.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "C.1.2 Function Definitions \n•  SAVE_STATE : Saves the current indices and the partial result of the output matrix  C  to non-volatile memory to allow recovery after a power interruption. C.1.1 Algorithm Overview \nThe GeMM operation, typically expressed as  C  =  A  ×  B , where  A ,  B , and  C  are matrices, is imple- mented with considerations for energy limitations. The algorithm breaks the matrix multiplication into smaller chunks (tiles), periodically saves the state before potential power losses, and resumes computation from the last saved state upon power restoration.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "15 \nC.1.3 Loop Tiling \nThe algorithm uses loop tiling to divide the computation into smaller blocks that can be managed between power interruptions. C.1.2 Function Definitions \n•  SAVE_STATE : Saves the current indices and the partial result of the output matrix  C  to non-volatile memory to allow recovery after a power interruption. •  LOAD_STATE : Retrieves the last saved indices and partial result from non-volatile memory to resume computation.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "15 \nC.1.3 Loop Tiling \nThe algorithm uses loop tiling to divide the computation into smaller blocks that can be managed between power interruptions. This tiling not only makes the computation manageable but also optimizes memory usage and cache performance, which is critical in constrained environments. C.1.4 Check-pointing Mechanism \nBefore each power interruption, detected through an energy monitoring system, the algorithm saves the current state using the  SAVE_STATE  function.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "This state includes the loop indices and the current value of the element being processed in  C . C.1.4 Check-pointing Mechanism \nBefore each power interruption, detected through an energy monitoring system, the algorithm saves the current state using the  SAVE_STATE  function. This ensures that no computation is lost when the power goes out.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "This ensures that no computation is lost when the power goes out. C.1.5 Resumption Mechanism \nUpon resuming, the algorithm loads the saved state using the  LOAD_STATE  function. This state is used to continue the computation exactly where it left off, minimizing redundant operations and ensuring efficiency.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "D Formulation of Dynamic Dropouts: \nD.1 L2 Dynamic Dropout with QuantaTask Optimization \nL2 Dynamic Dropout leverages the L2 norm of the weights to influence dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. This state is used to continue the computation exactly where it left off, minimizing redundant operations and ensuring efficiency. Mathematical Formulation:  Let  W  be the weight matrix of a layer.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "Mathematical Formulation:  Let  W  be the weight matrix of a layer. The idea is to use the inverse of the L2 norm to determine the probability: \np i  = α ∥ W i ∥ 2  +  ϵ \nwhere  α  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. The L2 norm of the weights is calculated as: \n∥ W ∥ 2  = sX \ni,j W   2 ij \nDefine the dropout probability  p i  for neuron  i  based on the L2 norm of its corresponding weights.",
      "type": "sliding_window_shuffled",
      "tokens": 145,
      "augmented": true
    },
    {
      "text": "The idea is to use the inverse of the L2 norm to determine the probability: \np i  = α ∥ W i ∥ 2  +  ϵ \nwhere  α  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. Define a binary dropout mask  m  = [ m 1 , m 2 , . .",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with L2 Dynamic Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  α . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass.",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "Define the energy budget  E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l . Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with L2 Dynamic Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  α .",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "Compute the activations a  and apply the dropout mask: a dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. Calculate the gradients of the loss with respect to the weights: \n∂ L ∂W ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \n16 \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the L2 norm of the weights: \np i  = α ∥ W i ∥ 2  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise Perform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Initialize the loop iteration parameters  l .",
      "type": "sliding_window_shuffled",
      "tokens": 360,
      "augmented": true
    },
    {
      "text": "Inference with L2 Dynamic Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. Calculate the gradients of the loss with respect to the weights: \n∂ L ∂W ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \n16 \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the L2 norm of the weights: \np i  = α ∥ W i ∥ 2  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise Perform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 336,
      "augmented": true
    },
    {
      "text": "Perform the forward pass with the updated dropout mask to obtain the output  Y . If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "D.2 Optimal Brain Damage Dropout with QuantaTask Optimization \nOptimal Brain Damage Dropout leverages a simplified version of the Optimal Brain Damage pruning method to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Perform the forward pass with the updated dropout mask to obtain the output  Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the L2 norm of the weights, along with the QuantaTask optimization to handle energy constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "Mathematical Formulation:  Let  W  be the weight matrix of a layer. D.2 Optimal Brain Damage Dropout with QuantaTask Optimization \nOptimal Brain Damage Dropout leverages a simplified version of the Optimal Brain Damage pruning method to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. The sensitivity of each weight W ij  is calculated using the second-order Taylor expansion of the loss function  L : \n∆ L ≈ 1 \n2 \nX \ni,j \n∂ 2 L ∂W   2 ij ( W ij ) 2 \nwhere ∂ 2 L ∂W   2 ij   is the second-order derivative (Hessian) of the loss with respect to the weights.",
      "type": "sliding_window_shuffled",
      "tokens": 174,
      "augmented": true
    },
    {
      "text": "The idea is to use the sensitivity to determine the probability: \np i  = β   P \nj ∂ 2 L ∂W   2 ij   ( W ij ) 2 \nmax \u0010P \nj ∂ 2 L ∂W   2 ij   ( W ij ) 2 \u0011 +  ϵ \nwhere  β  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. The sensitivity of each weight W ij  is calculated using the second-order Taylor expansion of the loss function  L : \n∆ L ≈ 1 \n2 \nX \ni,j \n∂ 2 L ∂W   2 ij ( W ij ) 2 \nwhere ∂ 2 L ∂W   2 ij   is the second-order derivative (Hessian) of the loss with respect to the weights. Define the dropout probability  p i  for neuron  i  based on the sensitivity of its corresponding weights.",
      "type": "sliding_window_shuffled",
      "tokens": 218,
      "augmented": true
    },
    {
      "text": ". Define a binary dropout mask  m  = [ m 1 , m 2 , . The idea is to use the sensitivity to determine the probability: \np i  = β   P \nj ∂ 2 L ∂W   2 ij   ( W ij ) 2 \nmax \u0010P \nj ∂ 2 L ∂W   2 ij   ( W ij ) 2 \u0011 +  ϵ \nwhere  β  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero.",
      "type": "sliding_window_shuffled",
      "tokens": 125,
      "augmented": true
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Optimal Brain Damage Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  β .",
      "type": "sliding_window_shuffled",
      "tokens": 156,
      "augmented": true
    },
    {
      "text": "Define the energy budget  E b  for a single quanta and for the entire inference. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Optimal Brain Damage Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  β . Initialize the loop iteration parameters  l .",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Calculate the gradients and Hessians of the loss with respect to the weights: \n∂ L ∂W ij , ∂ 2 L ∂W   2 ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) If  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the sensitivities: \np i  = β   P j ∂ 2 L ∂W   2 ij   ( W ij ) 2 \nmax \u0010P j ∂ 2 L ∂W   2 ij   ( W ij ) 2 \u0011 +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Initialize the loop iteration parameters  l . 17 \nCompute the activations  a  and apply the dropout mask: \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output.",
      "type": "sliding_window_shuffled",
      "tokens": 412,
      "augmented": true
    },
    {
      "text": "Calculate the gradients and Hessians of the loss with respect to the weights: \n∂ L ∂W ij , ∂ 2 L ∂W   2 ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) If  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the sensitivities: \np i  = β   P j ∂ 2 L ∂W   2 ij   ( W ij ) 2 \nmax \u0010P j ∂ 2 L ∂W   2 ij   ( W ij ) 2 \u0011 +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with Optimal Brain Damage Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 388,
      "augmented": true
    },
    {
      "text": "Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output Y . If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the sensitivity of the weights, along with the QuantaTask optimization to handle energy constraints. D.3 Feature Map Reconstruction Error Dropout with QuantaTask Optimization \nFeature Map Reconstruction Error Dropout leverages the reconstruction error of feature maps to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Perform the forward pass with the updated dropout mask to obtain the output Y .",
      "type": "sliding_window_shuffled",
      "tokens": 128,
      "augmented": true
    },
    {
      "text": "The reconstruction error of a feature map  F i  is calculated as: \nRE i  =  ∥ F i  − ˆ F i ∥ 2 \nwhere   ˆ F i  is the reconstructed feature map, and  ∥· ∥ 2  denotes the L2 norm. Mathematical Formulation:  Let  W  be the weight matrix of a layer and  F  be the feature maps produced by the layer. D.3 Feature Map Reconstruction Error Dropout with QuantaTask Optimization \nFeature Map Reconstruction Error Dropout leverages the reconstruction error of feature maps to adjust dropout rates, combined with the QuantaTask optimization to handle energy constraints in intermittent systems.",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "Define the dropout probability  p i  for neuron  i  based on the reconstruction error of its corresponding feature map. The reconstruction error of a feature map  F i  is calculated as: \nRE i  =  ∥ F i  − ˆ F i ∥ 2 \nwhere   ˆ F i  is the reconstructed feature map, and  ∥· ∥ 2  denotes the L2 norm. The idea is to use the reconstruction error to determine the probability: \np i  = γ  RE i max( RE ) +  ϵ \n18 \nwhere  γ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero.",
      "type": "sliding_window_shuffled",
      "tokens": 153,
      "augmented": true
    },
    {
      "text": "Define a binary dropout mask  m  = [ m 1 , m 2 , . . The idea is to use the reconstruction error to determine the probability: \np i  = γ  RE i max( RE ) +  ϵ \n18 \nwhere  γ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": ", m n ]  where  m i  ∈{ 0 ,  1 } . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Feature Map Reconstruction Error Dropout and QuantaTask Optimization: Initialize the network parameters  W , dropout mask  m , and scaling factor  γ .",
      "type": "sliding_window_shuffled",
      "tokens": 159,
      "augmented": true
    },
    {
      "text": "Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Feature Map Reconstruction Error Dropout and QuantaTask Optimization: Initialize the network parameters  W , dropout mask  m , and scaling factor  γ . Define the energy budget E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l .",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Initialize the loop iteration parameters  l . Compute the activations  a  and apply the dropout mask: \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. Calculate the gradients of the loss with respect to the weights: \n∂ L ∂W ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the reconstruction error of the feature maps: \np i  = γ  RE i max( RE ) +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication.",
      "type": "sliding_window_shuffled",
      "tokens": 358,
      "augmented": true
    },
    {
      "text": "Calculate the gradients of the loss with respect to the weights: \n∂ L ∂W ij \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the reconstruction error of the feature maps: \np i  = γ  RE i max( RE ) +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with Feature Map Reconstruction Error Dropout and QuantaTask Optimization: Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 338,
      "augmented": true
    },
    {
      "text": "Perform the forward pass with the updated dropout mask to obtain the output  Y . Otherwise, maintain or reduce the dropout rate to improve accuracy. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "19 \nD.4 Learning Sparse Masks Dropout with QuantaTask Optimization \nLearning Sparse Masks Dropout adapts dropout masks as learnable parameters within the network, inspired by Wen et al. This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the reconstruction error of the feature maps, along with the QuantaTask optimization to handle energy constraints. Perform the forward pass with the updated dropout mask to obtain the output  Y .",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "19 \nD.4 Learning Sparse Masks Dropout with QuantaTask Optimization \nLearning Sparse Masks Dropout adapts dropout masks as learnable parameters within the network, inspired by Wen et al. (2016), combined with the QuantaTask optimization to handle energy constraints in intermittent systems. Mathematical Formulation:  Let  W  be the weight matrix of a layer.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": ". Mathematical Formulation:  Let  W  be the weight matrix of a layer. Define a binary dropout mask m  = [ m 1 , m 2 , .",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "The mask values are determined using a sigmoid function to ensure they lie between 0 and 1: m i  =  σ ( z i ) where  z i  are learnable parameters and  σ ( · )  is the sigmoid function. , m n ]  where  m i  ∈{ 0 ,  1 } . In Learning Sparse Masks Dropout, the dropout masks are treated as learnable parameters.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "The mask values are determined using a sigmoid function to ensure they lie between 0 and 1: m i  =  σ ( z i ) where  z i  are learnable parameters and  σ ( · )  is the sigmoid function. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. Apply the dropout mask during the forward pass.",
      "type": "sliding_window_shuffled",
      "tokens": 142,
      "augmented": true
    },
    {
      "text": "Let  Q  represent the set of execution quanta, where each quanta  q  ∈Q  is defined by a tuple  ( l, e ) : q  = ( l, e ) Here,  l  is the number of loop iterations and  e  is the estimated energy required for these iterations. DynFit integrates closely with DynAgent, which serves as a repository of EH profiles and hardware characteristics. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output.",
      "type": "sliding_window_shuffled",
      "tokens": 172,
      "augmented": true
    },
    {
      "text": "Define the energy budget E b  for a single quanta and for the entire inference. Let  Q  represent the set of execution quanta, where each quanta  q  ∈Q  is defined by a tuple  ( l, e ) : q  = ( l, e ) Here,  l  is the number of loop iterations and  e  is the estimated energy required for these iterations. The goal is to optimize the loop iteration parameter  l  such that the energy consumption  E q  for each quanta  q  is within the energy budget  E b : \nminimize X \nq ∈Q E q subject to E q  ≤ E b \nTraining with Learning Sparse Masks Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask parameters  z , and scaling factor  α .",
      "type": "sliding_window_shuffled",
      "tokens": 197,
      "augmented": true
    },
    {
      "text": "Compute the activations  a  and apply the dropout mask: \nm i  =  σ ( z i ) \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y ) . Define the energy budget E b  for a single quanta and for the entire inference. Initialize the loop iteration parameters  l .",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "Compute the activations  a  and apply the dropout mask: \nm i  =  σ ( z i ) \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y ) . Inference with Learning Sparse Masks Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. Calculate the gradients of the loss with respect to the weights and dropout mask parameters: ∂ L ∂W ij , ∂ L ∂z i For each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) If  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask parameters  z  based on the gradients: \nz i  ← z i  − η  ∂ L \n∂z i \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \n20 \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication.",
      "type": "sliding_window_shuffled",
      "tokens": 363,
      "augmented": true
    },
    {
      "text": "If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Inference with Learning Sparse Masks Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. Otherwise, maintain or reduce the dropout rate to improve accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout with learnable mask parameters, along with the QuantaTask optimization to handle energy constraints. Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output Y .",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "Mathematical Formulation:  The Shapley value  ϕ i  of neuron  i  is a measure of its contribution to the overall network performance. D.5 Neuron Shapley Value Dropout with QuantaTask Optimization \nNeuron Shapley Value Dropout applies the concept of Shapley values from game theory (Aas et al., 2021) to assess neuron importance for dropout, combined with the QuantaTask optimization to handle energy constraints in intermittent systems. This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout with learnable mask parameters, along with the QuantaTask optimization to handle energy constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "Mathematical Formulation:  The Shapley value  ϕ i  of neuron  i  is a measure of its contribution to the overall network performance. X \nS ⊆N\\{ i } \n| S | ! It is calculated by considering all possible subsets of neurons and computing the marginal contribution of neuron  i  to the network’s output: \nϕ i  = 1 |N| !",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "|N| [ L ( S  ∪{ i } )  −L ( S )] \nwhere  N  is the set of all neurons,  S  is a subset of neurons not containing  i , and  L ( · )  denotes the loss function. X \nS ⊆N\\{ i } \n| S | ! ( |N| −| S | − 1)!",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Neurons with lower Shapley values are more likely to be dropped: \np i  = δ ϕ i  +  ϵ where  δ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. |N| [ L ( S  ∪{ i } )  −L ( S )] \nwhere  N  is the set of all neurons,  S  is a subset of neurons not containing  i , and  L ( · )  denotes the loss function. Define the dropout probability  p i  for neuron  i  based on its Shapley value.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "Neurons with lower Shapley values are more likely to be dropped: \np i  = δ ϕ i  +  ϵ where  δ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. . Define a binary dropout mask  m  = [ m 1 , m 2 , .",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Neuron Shapley Value Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  δ . Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. , m n ]  where  m i  ∈{ 0 ,  1 } .",
      "type": "sliding_window_shuffled",
      "tokens": 157,
      "augmented": true
    },
    {
      "text": "Define the energy budget  E b  for a single quanta and for the entire inference. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Neuron Shapley Value Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  δ . Initialize the loop iteration parameters  l .",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "Initialize the loop iteration parameters  l . Calculate the Shapley values  ϕ i  for each neuron based on their contribution to the network’s perfor- mance. Compute the activations  a  and apply the dropout mask: \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Calculate the Shapley values  ϕ i  for each neuron based on their contribution to the network’s perfor- mance. Inference with Neuron Shapley Value Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. For each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) If  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \n21 \nUpdate the dropout mask  m  based on the Shapley values: \np i  = δ ϕ i  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication.",
      "type": "sliding_window_shuffled",
      "tokens": 313,
      "augmented": true
    },
    {
      "text": "Inference with Neuron Shapley Value Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget. Otherwise, maintain or reduce the dropout rate to improve accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Perform the forward pass with the updated dropout mask to obtain the output  Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the Shapley values of the neurons, along with the QuantaTask optimization to handle energy constraints. Otherwise, maintain or reduce the dropout rate to improve accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "Mathematical Formulation:  Let  W  be the weight matrix of a layer. This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the Shapley values of the neurons, along with the QuantaTask optimization to handle energy constraints. D.6 Taylor Expansion Dropout with QuantaTask Optimization \nTaylor Expansion Dropout uses Taylor expansion (Li et al., 2016) to evaluate the impact of neurons on loss for dropout adjustments, combined with the QuantaTask optimization to handle energy constraints in intermittent systems.",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "Define the dropout probability  p i  for neuron  i  based on the Taylor expansion approximation of its impact on the loss: \np i  = λ \f\f\f  ∂ L \n∂ a i   a i \f\f\f  +  ϵ \nwhere  λ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. The impact of neuron  i  on the loss function  L  can be approximated using the first-order Taylor expansion: \n∆ L i  ≈ \f\f\f\f ∂ L ∂ a i a i \n\f\f\f\f \nwhere  a i  is the activation of neuron  i , and   ∂ L \n∂ a i   is the gradient of the loss with respect to the activation. Mathematical Formulation:  Let  W  be the weight matrix of a layer.",
      "type": "sliding_window_shuffled",
      "tokens": 187,
      "augmented": true
    },
    {
      "text": "Define the dropout probability  p i  for neuron  i  based on the Taylor expansion approximation of its impact on the loss: \np i  = λ \f\f\f  ∂ L \n∂ a i   a i \f\f\f  +  ϵ \nwhere  λ  is a scaling factor to adjust the overall dropout rate, and  ϵ  is a small constant to avoid division by zero. Define a binary dropout mask  m  = [ m 1 , m 2 , . .",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "Each element of the mask is determined by sampling from a Bernoulli distribution with probability  1  − p i : \nm i  ∼ Bernoulli (1  − p i ) \nApply the dropout mask during the forward pass. , m n ]  where  m i  ∈{ 0 ,  1 } . Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Taylor Expansion Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  λ .",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "Initialize the loop iteration parameters  l . Define the energy budget  E b  for a single quanta and for the entire inference. Let  a i  denote the activation of neuron  i : \na dropout i =  a i  ·  m i \nTraining with Taylor Expansion Dropout and QuantaTask Optimization:  Initialize the network parameters  W , dropout mask  m , and scaling factor  λ .",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "22 \nCalculate the gradients of the loss with respect to the activations: \n∂ L ∂ a i \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the Taylor expansion approximation: \np i  = λ \f\f\f  ∂ L \n∂ a i   a i \f\f\f  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Compute the activations  a  and apply the dropout mask: \na dropout i =  a i  ·  m i \nCompute the loss  L ( Y ,   ˆ Y )  where  Y  is the output of the network and   ˆ Y  is the target output. Initialize the loop iteration parameters  l .",
      "type": "sliding_window_shuffled",
      "tokens": 364,
      "augmented": true
    },
    {
      "text": "22 \nCalculate the gradients of the loss with respect to the activations: \n∂ L ∂ a i \nFor each layer  L  and loop  i  within the layer, estimate the energy  E i  required for the current quanta size  l i : E i  ← DynAgent.estimateEnergy ( L, i, l i ) \nIf  E i  > E b , fuse tasks to reduce the overhead: \nFuseTasks ( L, i, l i , E b ) \nUpdate  E i  after task fusion: \nE i  ← DynAgent.estimateEnergy ( L, i, l i ) \nUpdate the dropout mask  m  based on the Taylor expansion approximation: \np i  = λ \f\f\f  ∂ L \n∂ a i   a i \f\f\f  +  ϵ \nm i  = \u001a 0 if Bernoulli (1  − p i ) = 0 1 otherwise \nPerform the backward pass to update the network weights, considering the dropout mask: \nW  ← W  − η  ∂ L \n∂ W   ⊙ m \nwhere  η  is the learning rate and  ⊙ denotes element-wise multiplication. Inference with Taylor Expansion Dropout and QuantaTask Optimization:  Check the available energy using DynAgent. If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 340,
      "augmented": true
    },
    {
      "text": "Otherwise, maintain or reduce the dropout rate to improve accuracy. Perform the forward pass with the updated dropout mask to obtain the output Y . If energy is below a threshold, increase the dropout rate to ensure the inference can be completed within the energy budget.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "E Workings of Re-RAM Crossbar \nE.1 Re-RAM cross-bar for DNN inference: \nReRAM x-bars are an emerging class of computing devices that leverage resistive random-access memory (ReRAM) technology for efficient and low-power computing. Perform the forward pass with the updated dropout mask to obtain the output Y . This approach ensures that the network is robust to varying energy conditions by incorporating dynamic dropout influenced by the Taylor expansion approximation of the neurons’ impact on the loss, along with the QuantaTask optimization to handle energy constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "E Workings of Re-RAM Crossbar \nE.1 Re-RAM cross-bar for DNN inference: \nReRAM x-bars are an emerging class of computing devices that leverage resistive random-access memory (ReRAM) technology for efficient and low-power computing. These devices can perform multiplication and addition operations in a single operation, making them ideal for many signal pro- cessing and machine learning applications. Moreover, these devices can also be used for performing convolution operations, which are widely used in image and signal processing applications.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "E.1.1 Simple Single Cell Example: \nconsider a simple example of a ReRAM crossbar array with two cells, where V1 and V2 are the input voltages, G1 and G2 are the conductance values of the ReRAM devices, and I1 and I2 are the resulting output currents. To perform multiplication-addition, we first apply the input voltages V1 and V2 to the rows of the crossbar array. Moreover, these devices can also be used for performing convolution operations, which are widely used in image and signal processing applications.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "The conductance values G1 and G2 of the ReRAM devices are set to the corresponding weight values for the multiplication operation. To perform multiplication-addition, we first apply the input voltages V1 and V2 to the rows of the crossbar array. The output currents I1 and I2 are then computed as follows: \nI  =  I 1 +  I 2 =  G 1  ×  V  1 +  G 2  ×  V  2 \n23 \n(a) Re-RAM Cell \n(b) A Full Re-RAM tile \nFigure 5: DNN computation using ReRAM xBAR.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "Please refer to Figure 5a for more details. The output currents I1 and I2 are then computed as follows: \nI  =  I 1 +  I 2 =  G 1  ×  V  1 +  G 2  ×  V  2 \n23 \n(a) Re-RAM Cell \n(b) A Full Re-RAM tile \nFigure 5: DNN computation using ReRAM xBAR. Here, the output currents I1 and I2 are the result of the multiplication of the input voltages V1 and V2 by their respective weight values, which are summed together using the crossbar wires.",
      "type": "sliding_window_shuffled",
      "tokens": 126,
      "augmented": true
    },
    {
      "text": "As we can see, the input voltages V1 and V2 are applied to the rows of the crossbar array, while the conductance values G1 and G2 are applied to the columns. The output currents I1 and I2 are the result of the multiplication-addition operation, and are obtained by summing the currents flowing through the ReRAM devices. Please refer to Figure 5a for more details.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "In practice, ReRAM crossbar arrays can have many more cells, and can be used to perform more complex multiplication-addition and convolution operations. The output currents I1 and I2 are the result of the multiplication-addition operation, and are obtained by summing the currents flowing through the ReRAM devices. However, the basic principle remains the same, where the input signals are applied to the rows, the weights are applied to the columns, and the output signals are obtained by summing the currents flowing through the ReRAM devices.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "E.1.2 Extending to Complex Compute: \nIn order to perform multiplication-addition in ReRAM x-bars, two arrays of weights and inputs are used. However, the basic principle remains the same, where the input signals are applied to the rows, the weights are applied to the columns, and the output signals are obtained by summing the currents flowing through the ReRAM devices. The inputs are fed to the x-bar, which is a two-dimensional array of ReRAM crossbar arrays.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "The ReRAM devices are programmed to have different resistance values, which are used to store the weights. The inputs are fed to the x-bar, which is a two-dimensional array of ReRAM crossbar arrays. The crossbar arrays are composed of a set of row and column wires that intersect at a set of ReRAM devices (refer Figure 5b).",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "During the multiplication-addition operation, the input signals are applied to the rows of the x-bar, and the weights are applied to the columns. The output of each ReRAM device is the product of the input and weight signals, which are added together using the crossbar wires. The ReRAM devices are programmed to have different resistance values, which are used to store the weights.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "To perform convolution, ReRAM x-bars use a similar approach, but with a more complex circuit. The output of each ReRAM device is the product of the input and weight signals, which are added together using the crossbar wires. This results in a single output signal that represents the sum of the weighted inputs.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "Specifically, the weights are arranged in a way that mimics the convolution operation, such that each weight corresponds to a specific location in the input signal. The input signal is applied to the x-bar in the same way, but the weights are now applied in a more structured way. To perform convolution, ReRAM x-bars use a similar approach, but with a more complex circuit.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "To perform the convolution operation, the input signal is applied to the rows of the x-bar, and the weights are applied to the columns in a structured way. The output signal is obtained by summing the weighted input signals over a sliding window, which moves across the input signal to compute the convolution. Specifically, the weights are arranged in a way that mimics the convolution operation, such that each weight corresponds to a specific location in the input signal.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "The output signal is obtained by summing the weighted input signals over a sliding window, which moves across the input signal to compute the convolution. The DACs and ADCs are used to convert the digital input and weight signals into analog signals that can be applied to the rows and columns of the x-bar. At the circuit level, the ReRAM x-bar for multiplication-addition typically includes several com- ponents, such as digital-to-analog converters (DACs), analog-to-digital converters (ADCs), shift registers, and hold capacitors.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "The DACs and ADCs are used to convert the digital input and weight signals into analog signals that can be applied to the rows and columns of the x-bar. The shift registers are used to apply the weight signals in a structured way, and the hold capacitors are used to store the analog signals during the multiplication-addition operation. Similarly, for performing convolution, the ReRAM x-bar typically includes additional components, such as delay lines and adders.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Similarly, for performing convolution, the ReRAM x-bar typically includes additional components, such as delay lines and adders. 24 \nF Pseudo Codes \nF.1 Depth-wise Separable Convolution 2D Using TI LEA \nDepth-wise separable convolution is an efficient form of convolution that reduces the computational cost compared to standard convolution. The delay lines are used to implement the sliding window for the convolution operation, while the adders are used to sum the weighted input signals over the sliding window.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "F.1.1 depth-wise Separable Convolution 2D Using Conv1D \nThe pseudo code described in Algorithm 1 implements a depth-wise separable convolution 2D (DWSConv2D) using a 1D convolution primitive function (conv1D). Here we describe the implementation of depth-wise sep- arable convolution 2D using the Low Energy Accelerator (LEA) in Texas Instruments’ MSP430 microcontrollers. 24 \nF Pseudo Codes \nF.1 Depth-wise Separable Convolution 2D Using TI LEA \nDepth-wise separable convolution is an efficient form of convolution that reduces the computational cost compared to standard convolution.",
      "type": "sliding_window_shuffled",
      "tokens": 168,
      "augmented": true
    },
    {
      "text": "The depth-wise separable convolution is performed in two main steps: depth-wise convolution and point-wise convolution. F.1.1 depth-wise Separable Convolution 2D Using Conv1D \nThe pseudo code described in Algorithm 1 implements a depth-wise separable convolution 2D (DWSConv2D) using a 1D convolution primitive function (conv1D). The DWSConv2D function takes four inputs: an input matrix, depth-wise kernels (DWsKernels), point-wise kernels (PtWsKernel), and an output matrix.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "Foloowing are the outline of the requirements: \n1. The function is designed to handle energy constraints by decomposing the convolution loops into smaller quanta tasks. Define ‘QuantaTask‘ as the minimum iterations that can run.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "Decomposable loops: Each ‘QuantaTask‘ runs a certain part of the loop. Define ‘QuantaTask‘ as the minimum iterations that can run. 2.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "3. Decomposable loops: Each ‘QuantaTask‘ runs a certain part of the loop. Check for sufficient energy before launching a ‘QuantaTask‘.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "Fuse multiple ‘QuantaTask‘s to minimize load/store operations. 4. Check for sufficient energy before launching a ‘QuantaTask‘.",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "5. Check for power loss after each ‘QuantaTask‘ or fused ‘QuantaTask‘ and checkpoint if necessary. Fuse multiple ‘QuantaTask‘s to minimize load/store operations.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    }
  ]
}