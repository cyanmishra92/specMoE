=== ORIGINAL PDF: 2504.19797v1_Dynamic_Tsetlin_Machine_Accelerators_for_On-Chip_T.pdf ===\n\nRaw text length: 70800 characters\nCleaned text length: 69913 characters\nNumber of segments: 46\n\n=== CLEANED TEXT ===\n\nPREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 1 Dynamic Tsetlin Machine Accelerators for On-Chip Training using FPGAs Gang Mao , Tousif Rahman , Sidharth Maheshwari , Bob Pattison , Zhuang Shao , Rishad Shafik , Senior Member, Alex Yakovlev , Fellow Abstract The increased demand for data privacy and security in machine learning (ML) applications has put impetus on effective edge training on Internet-of-Things (IoT) nodes. Edge training aims to leverage speed, energy efficiency and adaptability within the resource constraints of the nodes. Deploying and training Deep Neural Networks (DNNs)-based models at the edge, although accurate, posit significant challenges from the back-propagation algorithm s complexity, bit precision trade- offs, and heterogeneity of DNN layers. This paper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an al- ternative to DNN implementations. DTM utilizes logic-based on- chip inference with finite-state automata-driven learning within the same Field Programmable Gate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin Machine algorithms, the dynamic aspect of the accelerator design allows for a run-time reconfiguration targeting different datasets, model architectures, and model sizes without resynthesis. This makes the DTM suitable for targeting multivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer multiply- accumulates, devoid of derivative computation. It is a data-centric ML algorithm that learns by aligning Tsetlin automata with input data to form logical propositions enabling efficient Look- up-Table (LUT) mapping and frugal Block RAM usage in FPGA training implementations. The proposed accelerator offers 2.54x more Giga operations per second per Watt (GOP s per W) and uses 6x less power than the next-best comparable design. Index Terms Edge Training, Coalesced Tsetlin Machines, Dynamic Tsetlin Machines, Embedded FPGA, Machine Learning Accelerator, On-Chip Learning, Logic-based-learning. I. INTRODUCTION M ACHINE Learning (ML) offers a generalized approach to developing autonomous applications from Internet- of-Things (IoT) sensor data. Having ML execution units in close proximity to the sensor, at the so-called edge, enables faster task execution with high data security and privacy. However, sensor degradation and environmental factors may require recalibration [1] or user-personalized on-field train- ing [2] to ensure continued functionality. Implementing so- lutions to these challenges is nontrivial. It requires finding the right balance between achieving the appropriate learning efficacy for the ML problem and the restrictive compute memory resources available on the platforms [3]. This work was supported by EPSRC EP X036006 1 Scalability Oriented Novel Network of Event Triggered Systems (SONNETS) project and by EPSRC EP X039943 1 UKRI-RCN: Exploiting the dynamics of self-timed machine learning hardware (ESTEEM) project. Indicates equal contribution. Microsystems Research Group, School of Engineering, Newcastle Univer- sity, Newcastle upon Tyne, NE1 7RU, United Kingdom (UK). IIT Jammu, NH-44 , PO Nagrota, Jagti, Jammu Kashmir 181221, India. For ML inference tasks on edge nodes, these challenges have been widely explored, e.g., quantization [4], sparsity- based compression, and pruning for the most commonly used Deep Neural Network (DNN) models [3], [5], [6]. So far, very few works have dealt with DNN on-field training due to two important design challenges: firstly, the main compute for DNN on-field training typically requires retaining full precision floating point general matrix multiplications and secondly the complexity of the backpropagation algorithm through the heterogeneous DNN layers [7]. These challenges materialize from algorithmically intrinsic attributes of DNNs. State-of-the-art approaches to alleviating these challenges are explored in Section III. This work addresses the aforemen- tioned challenges in on-field training through two fronts: firstly, on the algorithm front, whether the same (or better) edge training performance can be harnessed from simpler and more hardware-oriented ML algorithms; and secondly, on the hardware front, how such algorithms can be efficiently mapped to edge training platforms. These fronts are addressed in turn: On the algorithm front, this work explores ML algorithms with lower training complexity compared to DNNs called Tsetlin Machines (TMs) [8]. The TM and its variants are logic- based learning algorithms. They learn logical propositions called clauses to identify classes. The learning process for creating these logic propositions involves state transitions of finite-state automata. The TM and its family of algorithms ([9] [11]) alleviate the above two core challenges faced by DNNs: TMs utilize logic-based computation and simpler nested decision logic to adjust the learning elements during training instead of floating-point weights and backpropagation. On the hardware front, this work argues the case for very resource-constrained, low-cost, low-power, System on Chips (SoCs) operating as on-site single edge nodes. The recent adoption of such SoCs as embedded FPGAs (eFPGAs) offers the ability to integrate custom hardware accelerators in conjunction with CPU processing to leverage faster inference latency within edge node power budgets [6], [7]. The FPGA component, in particular, offers advantages in customization, cost, ease of prototyping, and lower implementation risk compared to ASIC solutions with better power efficiency than CPU and GPU training implementations [12]. Additionally, FPGA accelerator designs can be easily scaled to smaller or larger platforms, depending on the application needs. The cul- mination of the two fronts above forms the main contributions towards the development of Dynamic Tsetlin Machine (DTM) FPGA hardware implementations: arXiv:2504.19797v1 [cs.AR] 28 Apr 2025 PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 2 Fig. 1: Block diagram of the fundamental components of the Tsetlin Machine algorithms for inference - they build the Vanilla TM (pink) and the Coalesced TM (CoTM) (yellow). Component breakdown: a) shows the pre-processing or Booleanization process (adapted from [13]) to generate the inputs to TMs using an MNIST datapoint example; b) shows the learning element - the Tsetlin Automata (TA); c) shows how each TA relates to its respective Boolean literal to create the Clause Output. The pink and yellow blocks for the Vanilla and CoTM show Class sum computation for each class (d and e). DTM envisages generalized FPGA hardware for training and inference for all variants of TM algorithm. This paper currently generalizes two variants of the TM algorithm viz. Vanilla and Coalesced TM (CoTM) 1. Pseudo Random Number (PRN) Bandwidth and qual- ity: Efficient TM training requires a large amount of high- quality PRNs with high bandwidth. Considering resource constraints at the edge, an XOR-shift-based re-seeding mechanism is developed providing fresh seeds after 2L cycles for every L-bit Linear Feedback Shift Registers (LFSRs) enhancing training accuracy for small L. Scalability: The DTM architecture can be scaled accord- ing to the available hardware resource budget but retains the capability to execute a range of model sizes. Flexibility: For an implemented hardware architecture, DTM enables data-centric inter-model switching through precomputed masks and matrix iteration cycles at run- time without needing resynthesis (Section IV-A). Reducing computation complexity: DTM training im- proves design metrics by replacing floating-point arith- metic with integer-arithmetic retaining desired accuracy (Section IV-B). Optimized Clause-Level feedback: Feedback to clauses reduces as the model converges. DTM optimally skips groups of clauses that do not receive any feedback in the TA update stage, therefore, reducing the total number of 1The standard TM algorithm as described in [8] will be referred to as Vanilla but still abbreviated as TM. memory accesses and operations (Section IV-B). The remaining sections explore these contributions through examining TM algorithms and related designs in Sections II and III, the proposed architecture in Section IV and evaluation in Section V. II. BACKGROUND: TSETLIN MACHINE ALGORITHMS This paper introduces the first CoTM hardware and Dy- namic TM architecture. This section presents the fundamental components of the algorithms in the DTM architecture. A. Inference: Figure 1 shows inference components in four parts: a) shows input data preprocessing called Booleanization, b) shows Tsetlin Automata (TA) - the learning element in TM algorithms, c) shows how processed input data interact with their respective TAs to create a Clause Output, and the two Vanilla and CoTM blocks show how class sums are computed. a) Booleanization: Booleanization is the process of con- verting raw input data into a binary encoding and extending this to each binary feature and complement (x, x) - these are referred to as Boolean features (seen in grey) and Boolean Literals (l) (grey and green), respectively, as seen in Fig. 1a with an MNIST [14] 0 digit example. In this example, only one threshold is used to convert the raw feature to the Boolean Feature (f). However, in practice, many thresholds can be applied to each raw feature such that they are binned into discrete binary encodings. Details on Booleanization and dif- ferent Booleanization strategies are explored in detail in [15]. Higher complexity datasets such as CIFAR-10 and CIFAR- 100 are targeted using Composite TM architectures [16], [17]. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 3 Fig. 2: Block diagram of learning process used in the Vanilla TM and CoTM referred to as feedback. Feedback is used to transition the learning elements of the model, i.e., the TAs and weights (weights for CoTM). These processes are presented visually here, but will be explored as algorithm blocks in the subsequent section. Composite TMs involve a group composed predominantly of Convolution TMs [10] that each work on their own respective Booleanized input space derived via adaptive thresholding (adaptive Gaussian and Otsu with kernels up to 10 10), Canny edge detection, Color thermometers (kernels of 3 3 to 5 5) and Histogram of Gradients. Vanilla and Coalesced TMs feature less prevalently in such Composite TM architectures being suitable for smaller multivariate, image and audio prob- lems (explored later in Section V). Future work will add the Convolutional TM modules and pre-processing frameworks as modules for DTM enabling Composite TM training support. b) Tsetlin Automata (TA): The fundamental component underpinning all TM algorithms is Tsetlin Automata (TA). Each TA behaves like a finite state machine with two possible actions with respect to its corresponding Boolean literal. As seen in Fig. 1b, TA has 2J states; if the automaton s state value is J, then the automaton s action is to Exclude the corresponding literal, while if the state value J, then the automaton s action is to Include the corresponding literal (Fig. 1b). These Include Exclude decisions allow the TM to build logic expressions seen in the next image section (Fig. 1c). The convergence of TMs relies on the self-organizing behavior of TAs as they learn optimal logic expressions to classify data. Each TA operates independently, receiving Reward (brown arrow) and Penalty (blue arrow) signals to transition their state as they compete to form the best decision boundaries, their interactions can be modeled as a game where each automaton tries to maximize classification accuracy. A Nash Equilibrium is reached when no TA can improve its decision without detriment in the performance of another [18]. c) Clause Output: Fig. 1c shows how each Boolean literal in a MNIST datapoint is interfaced with its own TA to create a clause. The clause computation is composed of AND, OR and NOT operations and generates a single bit Clause Output (cl marked with brown). The training process determines which Boolean literals to include and exclude using their respective TAs to build logic expressions for each clause. Clauses are the fundamental modules of TMs; however, they are used differently in each TM algorithm. The pink and yellow boxes illustrate clause usage for class sums in Vanilla and CoTMs. d) Class sum compute for Vanilla TM: Focusing on Vanilla TM first, Fig. 1d shows how each class is composed of a set number of clauses. The clauses are divided into two teams (Positive Team and Negative Team). The one-bit outputs of each clause are summed and multiplied by either 1 or 1 polarity depending on the clause team. The summation of both teams polarity-adjusted sums produces the class sum. The argmax of each of these class sums across all classes generates the final inference classification. e) Class sum compute for CoTM: CoTM uses clause PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 4 Fig. 3: The log scale comparison of logic-based (clause compute) vs. integer-based (class sum compute) arithmetic operations in CoTM inference. The number of operations increase with clauses. outputs differently. Instead of instancing clauses for every class, it creates a shared pool of clauses (SC), shown in Fig. 1e. These clause outputs are shared across all classes. To generate a class sum, each class has its own set of learnable signed integer weights (w) which act as multiplicands with their respective clause in the SC pool. The clauses in the SC pool no longer have polarity like the Vanilla TM; instead, their polarity is assigned when multiplied with their respective weight in each class. Despite the additional weight multiplications, as seen in Fig. 3, the inference operations are still dominated by the logical operator computation. B. Learning (Feedback): a) TM learning algorithms (feedback): During training, the learning process occurs for each datapoint after the inference is completed, i.e. the clause outputs and class sums have been generated. TM learning algorithms find the optimum states for every TA in the system and the values for the weights for each clause for every class of the CoTM. To transition a TA, that is, increment or decrement its state, feedback must be issued down the hierarchy of the TM. Fig. 2 shows how this first starts with Class-level feedback, to Clause-level feedback, and finally how this leads to TA increment and decrements. b) Class level feedback: Feedback decisions for both TM algorithms begin at the class level, and both Vanilla and CoTM follow the same class-level feedback procedure. The learning in pairwise, for each datapoint, two rounds of feedback will happen (these are referred to as class updates), each round updates one particular class. Firstly, the target class Class y (that is, the class to which the data point corresponds) followed by a randomly chosen class Class y. If the target class is selected, then yc 1 otherwise yc 0. This forms the inputs to the decision process used for clause-level feedback. c) Clause level feedback: Once the class yc has been selected for feedback, the class sum of this chosen class will be used to evaluate the clause update probability C1 or C2 (the class sum would already have been computed during inference). The clause update probability is a comparison between a random number and an expression that is formed by the clipped class sum (csum clip) and the Threshold hyperparameter T. If the clause update probability is 1 then this clause has been selected for feedback. For Vanilla TM, the selected clause s polarity is used to determine whether it should receive Type I or Type II feedback. These are used to provide feedback at the TA level. d) Clause level feedback (Weight Feedback): As seen in the inference figure, CoTM has a weight assigned to each clause in each class. The weight feedback is dependent on the generated clause output for this particular clause and the class- level feedback yc. The weight update is controlled through the clause update probabilities C1 or C2. e) TA level feedback: Type I and Type II feedbacks both iterate through every literal in the selected clause to determine whether to transition it s associated TA. For Type I feedback, there are TA update probabilities 1 s and s 1 s for both TA state value increment and decrement, respectively. These update probability expressions introduce the Sensitivity hyperparameter s. TM algorithms also have a mode that is called boost true positive . When using this mode, the increase in Type I feedback will always occur regardless of the condition S2. Type II feedback is deterministic. The feedback process involves two hyperparameters s and T. The effect of these parameters on TA updates and clause updates, respectively, is explored in [19] for the Vanilla TM. The following section will illustrate how these probabilities are translated to FPGA. III. RELATED ACCELERATOR DESIGNS This section explores training accelerator designs targeting FPGAs for DNNs to address how intrinsic attributes of the TM algorithm can offer reduced complexity2. a) Quantization in Training: One of the algorithmic drawbacks of all DNN-based algorithms is the use of 32-bit floating point weights and computation. Most works address this through quantization of the these floats [4], [20] [26]. Extreme quantization to binary weights and data, referred to as Binary Neural Networks (BNNs), leads to a reduction in the computation of floating point multiplication and addition to XNOR and popcount. This is incredibly effective for high-throughput inference accelerators [4], [20], [21]. Trans- lating the training of these quantized DNNs to accelerators is challenging because the precision of the weights cannot be reduced as significantly. For example, for training, the authors in [22] retain 20-bit weights for their accelerator. This results in substantial Block RAM (BRAM) usage for weight storage and many DSP blocks for their computation. Batched training to improve throughput is seen in [27] (Low Batch CNN). b) Design Approaches for Backpropagation: DNN train- ing requires backpropagation for weight updates during train- ing. This involves computationally expensive partial derivative calculations. The backpropagation computation in the training accelerators in [28], is kept as floating point, but happens on the ARM processor of an SoC-FPGA. The proposed accelera- tor is also developed for an SoC-FPGA but the ARM processor is used for configuring the run-time architecture search. The work in [27] (Reconfig DNN) also provides reconfigurability to adopt different applications. c) Addressing Layer Heterogeneity: Stochastic gradient descent allows training in minibatches. This approach for backpropagation is used by [29] [32], but this time developed on the FPGA fabric itself; this approach retains 32-bit floats throughput. This work also addresses another design challenge with DNNs: Layer heterogeneity. They partition the training process according to the different layers and use a stream- based reconfiguration to configure basic modules for each 2The related works are qualitatively compared here to understand the design choices. In the Accelerator Evaluation section, these implementations are considered more quantitatively against the proposed accelerator. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 5 layer s workload. The main overhead is memory transfer for each layer from off-chip memory. d) Spiking Neural Network Approaches: Two digital Spiking Neural Network (SNN) inference FPGA implemen- tations are reported in [33], [34]. For the three-layer SNN network [33] mapped to Xilinx Kintex-7 reported in this paper, it can infer 233 images per second for MNIST and FMNIST, while achieving 97.81 and 83.16 accuracy. In [35], a 65nm SNN training AISC simulation is reported. The simulated training throughput for MNIST is around 5000 images s with 99.00 test accuracy. e) Advantages of TMs and related works: The TM algorithms intrinsically use bitwise operations for clause com- putation and comparisons using random numbers in the feed- back process. The entire dataflow can, naturally, operate in the integer-arithmetic domain on hardware mitigating many design challenges encountered in quantization and backpropagation. The first preliminary TM training and inference implemen- tation [36] uses an ultra-low power ASIC solution based on 65nm technology customized for the 3-class Binary Iris data set [37]. REDRESS [38] presents a microcontroller-based custom high-throughput inference implementation employing bit-parallel optimization. A custom TM accelerator framework (MATADOR) [39] focuses on automated deployment for max- imum throughput, once again for a specific task, with a custom hard-wired Vanilla TM model. In MATADOR, the TA actions are hardcoded into LUTs and cannot be calibrated without resynthesis, which means MATADOR cannot be extended to a training accelerator. A recent FPGA-based customized Convolutional Tsetlin Machine (Conv TM) training acceler- ator [40] reports solving customized 28 28 booleanized images, with a 10 10 convolution window. Conv TM is designed for fast throughput at the expense of resources. Section V presents further comparison between DTM and Conv TM. In contrast to related works, the DTM design showcases real-time recalibration and flexibility at the expense of throughput. These qualities are more valuable in on-chip training implementations that require re-calibration. IV. PROPOSED DYNAMIC TSETLIN MACHINE (DTM) ACCELERATOR The following two subsections explore how the TM and CoTM concepts are translated to the DTM architecture im- plementation with SoC integration. Fig. 1 shows that the TM algorithms are intrinsically modular with the Vanilla and CoTM architectures composed of clause computation blocks. Unlike layered DNN architectures, the model architectures of both Vanilla and CoTMs are controlled by the number of clauses (alleviating layer heterogeneity design challenges). This shared basis across the two TM algorithms allows for greater reuse. The Dynamic in the DTM architecture represents flexibility and reconfigurability at runtime. The compute and memory components of the DTM architecture are presented in Fig. 4. The architecture is divided into five compute modules: Clause Matrix, Weight Matrix, Weight Update Matrix, TA Update Matrix and Argmax and their associated memory modules. The Clause Matrix, Weight Matrix and Argmax are used for inference, while the remaining update blocks are used for training. The next two subsections examine these blocks and their data buffers in inference and training. A. DTM Inference Modules: Inference involves computing of clause outputs (for every class for Vanilla TM), applying the weights (for the CoTM) and then summing (class sums) and performing argmax. This subsection walks through this process from how features enter the accelerator to how the classification is generated. a) Feeding in the Features: The accelerator is interfaced with the SoC processor via an AXI4-Stream channel, Boolean input features are received and placed in the Feature Buffer (see Fig. 4). Users does not need to set this buffer to the problem s feature size explicitly, the feature buffer size only determines a maximum feature number. If the DTM accelerator is to be deployed on a resource-limited FPGA chip (e.g., Zynq7020) the Feature Buffer will be registers, but when scaling to larger chips (e.g., Ultrascale ZU-7EV), the Feature Buffer will be instanced as BRAM. This allows DTM to handle larger datasets in a more resource-efficient way. Suppose that the width of the AXI bus is W bits; the DTM will receive W Boolean features in every clock cycle. In the clause computation phase, it selects x 2 features and generates their respective x literals (these features and their complements) and sends them to the Clause Matrix. b) Clause Computation: Clause computation occurs in the Clause Matrix (Block 1 in Fig. 4), the detailed mapping of the clause computation to the LUTs is shown in Fig. 4-6. Loading all literals (lit) and TA states (ta) for every TM clause at once demands high bandwidth and logic resources. Complete clause computations create long com- binational chains, reducing operational frequency; to reduce resource pressure and latency, TA actions won t all load into the Clause Matrix at once. p cl1 p cl2 ... p cly x i 1(Li TAi 1) x i 1(Li TAi 2) ... x i 1(Li TAi y) p cl 1 p cl 2 ... p cl y (1) Algorithm 1: CLAUSE Function Input: L (Literal), l mask (literal mask), Action (TA action, 1 means Inc, 0 means Exc), cl buffer mask (the clause buffer mask), x (literal width of clause computation matrix), y (clause width of clause computation matrix) 1 CLAUSE(TM type, l mask, cl buffer mask, h, x, y, L, Action) foreach i y do 2 foreach j x do 3 p cl[i] p cl[i] cl buffer mask[i] ((L[j] l mask[j]) Action[i][j]); 4 return p cl; c) Introducing Partial Clauses: Instead, drawing from previous works [39], [41], the full clause matrix computa- PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 6 Fig. 4: The architecture of the proposed Dynamic Tsetlin Machine (DTM) training accelerator. The DTM accommodates both the Vanilla and CoTM algorithms. It consists of 5 core modules: 1-Clause Matrix, 2-Weight Matrix, 3-Weight Update Matrix, 4-TA State Update Matrix, 5-Argmax Output. Blocks 1, 2 and 5 are for Vanilla and CoTM inference while TM training needs both blocks 3 and 4. All buffers and Pseudo Random Number Generator (PRNG) blocks are used for both algorithms. 6 also shows how the clause computation is mapped into LUT6 FPGA elements. tion is now decomposed into smaller partial clause matrix computations. For a given datapoint, DTM uses computation slices of a group of clauses that utilize only a subset of all TA actions and their corresponding literals. These are partial clauses. Suppose that a TM model contains f features (so f 2 literals), c clauses per class for Vanilla TM and c clauses altogether for CoTM, and h classes3: The Clause Matrix will process x of these literals to compute y partial clauses, using x y corresponding TA actions x y for each computation. The next few paragraphs discuss the iterations of this divided computation and how redundancies are handled when the matrices cannot be divided without remainders. d) Partial Clause Matrix Computation: Equation 1 shows how partial clauses are computed in the Clause Matrix. 3These variables will now be used throughout the remainder of the paper and their relationships will be further elaborated in the DTM algorithm blocks. The reader is encouraged to refer back to Fig. 1 and Fig. 5 to better understand the design choices made with the hardware implementation. Algorithm 2: CSUM Function Input: cl (loaded clause), cl mask (clause mask in weight computation matrix), W (loaded weights), m (clause width of weight computation matrix), n (class width of weight computation matrix), Lcsum (Maximum length of class sum) 1 CSUM(m, n, cl mask, cl, W, Lcsum) foreach i n do 2 foreach j m do 3 p cs[i] p cs[i] (cl[j] cl mask[j]) W[i][j]; 4 return p cs; The vector p cl represents the result of the previous partial clauses while p cl represents the current partial clauses result. p cl will be set to a 1 vector at the beginning of a group of clause computation. The computation remains the same PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 7 (a) Partial clause computation slices and iteration sequence (ST A represents one round of compute for the Clause Matrix). (b) (Class sum computation slice in memory (Sw represents the Weight Matrix computation slice). Fig. 5: Visualizing the iterations required to compute full clause and weight matrices. These figures show the rounds of compute required for the Clause Matrix and Weight Matrix. The colors of (a) and (b) correspond to the colors of the matrix modules in Fig. 4. between the literal L and the TA actions subset TAi j for the ith literal in the jth clause in this matrix. e) Partial Clause Iteration Sequence: Fig. 5a shows the iterations for the Clause Matrix block. The matrix needs a 2 f x clock cycles to compute a group of y clauses and b c y groups to compute one class of clauses. For CoTM, this is the full iteration as the CoTM will use the shared clause pool (see Fig. 1), but when computing clauses for Vanilla TM, b h groups of clauses will be computed. f) Masking Remainder Compute: There may be cases of remainder literals in the last slice fed into the Clause Matrix. In these cases, the remainder literals (X) will be masked. This is shown in Fig. 6a. An inverted version of this mask is also used for TA Update Matrix. Similarly to the remainder literal mask when there are remainder clauses, a clause mask is used to prevent writing these remainder clauses into the clause buffer (see Fig. 6b). The mask uses logic 0 for these remaining elements so that when logic AND d, they will is always 0. All these ideas are culminated in (Algorithm 1). g) Feeding TA actions for inference: Considering the model size and resource limitations of the target FPGA plat- form, it is more efficient to store the TA states in BRAM instead of registers. For a clause computation with x literals and y clauses, assuming the TA state length is LT A bits and the BRAM used to store TA states is configured as WT A RAM bits, at least x y LT A WT A RAM BRAMs will be configured as TA RAMs. The clause computation matrix will read the actions from one row of TA states data in each clock cycle (see Fig. 4-1). Each clause computation slice in Fig. 5a takes one row of on-chip RAM (e.g. BRAM and URAM). p cs1 p cs2 ... p csn w1 1 w2 1 . . . wm 1 w1 2 w2 2 . . . wm 2 ... ... ... ... w1 n w2 n . . . wm n cl1 cl2 ... clm p cs 1 p cs 2 ... p cs n (2) After a group of clauses is computed in the Clause Matrix, the clause outputs will be stored in either registers or (a) Literal mask (Clause Ma- trix) (b) Clause buffer mask (c) Clause mask (Weight Ma- trix) (d) Class mask Fig. 6: Masks for remainder compute elements (marked in red). BRAM depending on the FPGA resources - this is the clause buffer. The clause buffer stores only 1 class of clause outputs. h) Class Sum Computation: A similar slicing strategy is also applied in the Weight Matrix for class sum computa- tion (see Fig. 4). Loading all weights and clauses to compute one class sum is resource-intensive. i) Partial Class Sums: Splitting class sums into multi- ple partial matrix multiplications provides better performance (running at higher frequencies) and lower resource costs. Thus, full class sum computation is split into multiple rounds of partial weight multiplication and vector addition. The weight matrix computes n partial class sums using the m clauses in each iteration (see Equation 2). In Equation 2, p cs represents the previous partial class sum result, p cs represents the current partial class sum result.p cs will be set to 0 when start a group of class sum computation, clj represents the pth clause in this class, and wj i represents the weight of the jth clause of the ith class in this matrix. For the CoTM, the accelerator takes p c m clock cycles to compute a group of n class sums, and q h n groups of partial class sums to get all class sums for h classes. For Vanilla TM class sum computation, the weights be- PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 8 come 1 for positive clauses and -1 for negative clauses. Positive clause are even indexed while negative clauses odd indexed. Therefore, a class sum for class i can be obtained by Equation 3 using the same dimension vectors and matrix as in Equation 2, where p csi and p cs i represent the current and the next partial class sum of the ith class, and cli j represent the jth clause in the ith class. In this case, the weights loaded into Fig. 4- 2 become 1 and 1 for even clauses and odd clauses respectively. The accelerator takes p c m clock cycles to compute one full class sum for the Vanilla TM. p csi p csi ... p csi 1 1 . . . 1 1 1 . . . 1 ... ... ... ... 1 1 . . . 1 cli 1 cli 2 ... cli m p cs i p cs i ... p cs i (3) j) Feeding the Weights: Considering the size of all weights, it is more feasible to store them in BRAM on FPGA. For an accelerator with a weight matrix sized as m n, assume that each weight has w bits, and that the BRAM has used to store the weights are configured as u bits. Then m n w u BRAMs are required for weight storage. When computing a group of class sums, only one row of data will be read as the weights for the computation in each clock cycle (see Fig. 4-2). Vanilla TM inference only requires constant weights, so there is no reading from BRAM. k) Masking Remainder Compute: When there is a remain- der clause in Weight Matrix, a clause mask is applied with logic 0 for the remainder bits and logic AND d with the remainder clause bits so they will not contribute to the class sum (see Fig. 6c). When there are remainder class sums computed, a class sum mask is applied as post-processing which assigns 2Lcsum 1 in binary (Lcsum is the maximum length of the class sum in binary, and 2Lcsum 1 is the minimum possible class sum in binary) to the remainder class sums while the others remain, so that the remainder class sum will never be greater than used classes (see Fig. 6d). l) Class Sum and Argmax: When computing the class sums for CoTM, the binary multiplication is optimized to a logic AND operation between the clause and the corresponding weight bit by bit (see Algorithm 2). The Argmax is a comparison tree with n class sums as input. The maximum class sum and its class index will be cached and then used for comparison with the maximum class sum in the next group of class sums until all class sums are compared. B. DTM Training Modules: This subsection presents the implementation details of the training modules for the DTM. It is recommended to read this section and its algorithms with reference to Fig. 24. Much like in the background section, this subsection presents how the feedback to TAs is passed from class level to clause level to TA level. a) Class-Level Feedback: Training requires both the target class and a randomly chosen class (referred to as the negated class) for each training datapoint. The target class is padded into the input data stream sent to the accelerator (presented 4For example if a paragraph has a bold label Class-level feedback - view this part of Fig 2 against the same algorithm name in this section. This should make it easier to follow the nested condition parts in the algorithms Algorithm 3: Class-level Feedback Generation Input: T (hyper parameter), yc (input class, 1 means target class, 0 means negated class), c rand (random number for negated class choice), Lw rand (random number length for clause-level feedback compute), Target Class (target class index), Update csum (Class sum for updated class), Class num (Class number) 1 NC_Gen(c rand, Target Class, Class num) RNC c rand (Class num 2); 2 if RNC Target Class then 3 Negated Class RNC; 4 else 5 Negated Class RNC 1; 6 return Negated Class; 7 Update_Probability_Compute(yc, c rand, Lw rand, Target Class, Class num, Update csum) 8 if yc 1 then 9 Update csum cum[Target Class]; 10 else 11 Negated index NC_Gen(c rand, Target Class, Class num); 12 Update csum cum[Negated index]; 13 CSum clip(Update csum, [ T, T]); 14 if yc 1 then 15 PCl Update (T CSum) 2LT 1; 16 else 17 PCl Update (T CSum) 2LT 1; 18 return PCl Update; later), while the randomly chosen negated class is determined within the hardware accelerator (see Function NC Gen in Algorithm 3 (NC_Gen)). The clause-level update probability is computed in Function UpdateProbabilityCompute with some changes to be more easily translated to hardware. b) Clause-Level Feedback: After selecting a class to be updated (yc), the clause-level feedback should be determined based on the comparison result between the product of a Lw rand-bit random number w rand with the hyperparameter T and the clause update probability (see Algorithm 3). The clause-level feedback will be stored in Clause Feedback Buffer as either registers or BRAM. In this architecture, for better simplicity of design, the size of the Weight Update Matrix (see Fig. 4-3) matches the Weight Matrix. Thus, in each clock cycle, clause-level feedbacks for m clauses will be computed, and m weights for the corresponding clauses will be updated. The weights update only in CoTM mode. Algorithm 4 shows the clause- level feedback and weight updates within Weight Update Matrix in 4- 3. The clause mask from Section IV is reused to prevent remainder clauses from updating. c) TA-Level Feedback: Similarly, to match the incom- ing data rate when computing the clauses, the TA Update Matrix is designed to be the same size as the Clause Matrix. Hence, in each clock cycle, y clause-level feedback PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 9 Algorithm 4: Clause Level Feedback Generation Input: TM type (TM type, 2 b01 represents Vanilla TM and 2 b10 represents CoTM), index (index of updated class), PCl Update (Update probability), T (hyper parameter), cl (Clause), cl mask (clause mask for weight computation matrix), m (clause number in weight computation matrix), W (Signed weight of this clause), w rand (random number for clause-level feedback compute), yc (the updated class, 1 represents target class, 0 represents negated class) 1 Clause_Update(cl, W, cl mask, w rand, yc) foreach i m do 2 if cl mask[i] 1 then 3 if PCl Update T w rand then 4 if yc 1 then 5 if W[index][i] 0 then 6 Feedback[i] 2 b01; 7 else if W[index][i] 0 then 8 Feedback[i] 2 b10; 9 if cl[i] 1 then 10 if TM type 2 b10 then 11 W[index][i] W[index][i] 1; 12 if yc 0 then 13 if W[index][i] 0 then 14 Feedback 2 b01; 15 else if W[index][i] 0 then 16 Feedback 2 b10; 17 if cl[i] 1 then 18 if TM type 2 b10 then 19 W[index][i] W[index][i] 1; 20 return Feedback, W; will be read from the buffer registers. Then y corresponding clauses, x features, and x y LT A rand-bit random numbers will be fed into the TA Update Matrix where TA rand is the random number for the TA-level feedback computation, and LT A rand is the length of these random numbers. In the accelerator, s is defined as Ls bits. To reduce resource utiliza- tion, the TA update probability 1 s is precomputed, because the random numbers generated in the hardware are LT A rand-bit binary numbers so that the TA update probability PT A Update becomes 2LT A rand s (see Algorithm 5). Fig. 4-4 shows the architecture of the TA Update Matrix, it contains x y TA update blocks, and for the TM update, the TA update block takes a clock cycles to update the TAs in a group of x clauses, and TAs in one class require a b clock cycles. The inverted literal mask (Section IV) and clause buffer mask (Section IV) become control signals preventing updates of remainder TAs in a clause computation slice. Function TAFD in Algorithm 5 is used to update the TAs in one TA slice. d) Clause-Level Feedback Optimization: In the proposed Algorithm 5: Group Clause TA Update Input: PT A Update (TA update probability), Feedback (loaded Clause level feedback), L (loaded literal), l mask (literal mask), TA (loaded unsigned TA state), action (TA action of the loaded TAs), LT A rand (random number length), TA rand (random number), cl (loaded clause), cl buffer mask (the clause buffer mask), x (literal width of clause computation matrix), y (clause width of clause computation matrix) 1 TAFD(x, y, L, TA, cl, TA rand, PT A Update, Feedback, l mask, cl buffer mask, action) 2 foreach i y do 3 foreach j x do 4 if cl buffer mask[i] l mask[j] then 5 if Feedback[i] 2 b01 then 6 if cl[i] (cl[i] L[j]) then 7 if PT A Update TA rand then 8 if TA[i][j] 0 then 9 TA[i][j] TA[i][j] 1; 10 else if cl[i] L[j] then 11 if PT A Update TA rand then 12 if TA[i][j] 2LT A 1 then 13 TA[i][j] TA[i][j] 1; 14 else if Feedback[i] 2 b10 then 15 if cl[i] L[j] action[i][j] 1 then 16 if TA[i][j] 2LT A 1 then 17 TA[i][j] TA[i][j] 1; 18 return TA; Algorithm 6: Optimized TA Update Input: Feedback (loaded feedback for the current group of clauses) 1 TAFD_OPT(x, y, L, TA, action, cl, TA rand, Feedback, PT A Update, l mask, cl buffer mask) 2 while TA update not finished do 3 if i y such that Feedback[i] 2 b00 then 4 TAFD(x, y, TA, action, TA rand, PT A Update, L, Feedback, cl, l mask, cl buffer mask); 5 else 6 Allocate TA RAM address to the start of the next group of clauses; 7 return TA; architecture, the TAs in a group of clauses are updated concur- rently. As the model converges while training, the frequency of feedback to clauses reduces [38]. DTM skips loading of all TA slices from BRAM and updation when there is no clause-level feedback to a particular group of clauses, saving PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 10 Fig. 7: Training time per epoch. Fig. 8: The block diagram of the PRNG cluster. precious cycles. Explanation: when the clause matrix slides through the slices (see Fig. 5a), if no feedback is present for the corresponding y clauses, the computation slice moves directly to the next row without traversing the current row s bottom column. This is implemented by adjusting the read write addresses of the TA RAM, as shown in Algorithm 6. With this optimization, the CoTM gets 40 improvement in training time as shown in Fig. 7, while achieving similar test accuracy. The reduction in training time indicates fewer clause updates each epoch. The training time eventually saturates to a constant value, which approximately equals the inference time plus the weight update time for both target and negated class updating. C. Pseudo Random Number Generator (PRNG): To gener- ate a large number of random numbers for parameter updating in real time, the accelerator is integrated with a master-slave architecture PRNG cluster developed from [42]. In the PRNG cluster, the master PRNG is used to generate the seeds for the slave PRNGs. For each slave PRNG, if the random number generation finishes one PRNG cycle period, it will request a new seed from the master PRNG; this is called seed refresh. Fig. 8 shows the architecture of the PRNG cluster. The authors in [42] map the PRNG cluster to DSP blocks; however, the slave PRNG used in this design is LFSR based due to DSP block limitations in the chosen FPGAs. In the LFSR version of the master-slave PRNG cluster, the master PRNG will generate and set the seeds for the slave PRNG after 2LLF SR clock cycles, where LLF SR is the length of LFSR. One DSP slice can only afford 16-bits Mid-square based PRNG, hence in this accelerator, LFSR is used as the slave PRNG in the PRNG cluster. The processor supplies and programs the master PRNG seed in real time. D. DTM Control and Data Flows: a) Programming: Before inference or training, the configuration data including the TM type, feature number, clause number and class number are sent to the accelerator, allowing it to calculate clause and weight compute rounds. Upon receiving these data, the necessary clauses and weight masks are derived, as outlined Fig. 9: Accelerator Timing: a) timing diagram for CoTM inference; b) timing diagram for Vanilla TM inference; c) timing diagram for DTM training (same procedure for both Vanilla and CoTM). in IV.For training, hyperparameters like the TA state update probability 1 s and threshold t are provided. The accelerator calculates 1 s, stores t, and initializes the TA states and weights in RAM using PRNGs once the model size is configured. The programming data (model size, PRNG seeds, and hyperpa- rameters) are sent via an AXI-stream channel. The execution mode (inference or training) and the target class index are embedded within the feature data stream. Fig. 10a shows the AXIS integration and how the instruction and feature data flow through the system for the inference and training processes. b) Class Sum Computation: During training, only the target class sum is computed; in inference, class sums for all classes are calculated. The feature data are stored when received; the system uses this to identify the mode (train- ing inference), and loads the target class if in training mode. The TA RAM controller allocates the start clause slice, and the Clause Matrix loads TA actions and literals to compute clauses in block 1. Inference is pipelined: When one class of clause computation completes and the weight computation matrix is idle, the weight RAM controller allocates the weight slice, loading weights, and clauses to compute class sum in block 2. In inference-only mode, once classes are processed, a new data point is handled. The timing diagrams for the Vanilla TM and CoTM inference are shown in Figs. 9-a and 9-b. c) Parameter Update: In training, updates start with class- level feedback from the Clause Update Control (block 5 in Fig. 4), which generates clause-level update probabilities. The Training Accelerator updates weights and generates clause feedback (block 3). For CoTM, the weight RAM controller loads the class slice, updates weights, and writes them back to RAM. TA states update in block 4, where the TA RAM controller allocates the clause slice, feeding TAs and literals to the TA update matrix. Updated TA states are written to RAM. TM updates proceed in two rounds: first, the target class, then a random class. Fig. 9-c shows this update flow. After updating, inference resumes by updating class sums, weights, and TA states, while the controller, stream channel, and buffer reset for the next feature stream. The state graph of the system during the training and inference process is shown in Fig. 10b. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 11 (a) The data flow in IP core. (b) The process flow of the proposed architecture. Fig. 10: DTM accelerator data flows. V. EVALUATION The DTM accelerator is designed to train on edge applica- tions, particularly cases where recalibration is required. This is typical with IoT sensor data inputs for edge tasks. Three Evaluation Points: This section presents three different evalu- ation points: firstly, comparing the accelerator against the state of the art in terms of the ability to accelerate the operations per second within a given power budget and associated energy and resource overheads; secondly, the scalability of the design to both larger and smaller FPGA platforms when targeting the kind of IoT edge datasets this design should handle; and finally, discussing the design trade-offs when adjusting weight precision and the LFSR length in the PRNG design. The design mapped on PYNQ-Z1 with Xilinx XC7Z020 SoC is referred to as DTM-S while the design mapped on ZCU- 104 with Xilinx ZU-7EV SoC is referred to as DTM-L. Two designs are implemented through AMD s (Xilinx) Vivado Design Suite. This section reports accuracy from 250 training epochs, latency from average training time per data point, and the power from Vivado s implementation reports. The resource utilization of each block and latency in each stage for the DTM-L configuration is presented in Fig. 12 and Fig. 13. Dataset Rationale: MNIST [14], FMNIST [43], KM- NIST [44], Google Speech Commands Keyword Spotting with 6 Keywords (yes, no, up, down, left, right - referred to as KWS-6) [45] (Booleanized as per [46]) are used as evaluation datasets. Often, deployed models need re-calibration to data affected by sensor aging, temperature, humidity, and other environmental changes [47]. Therefore, the ability to re-train and adapt, in situ, is more energy-efficient than continuous offsite cloud transfers. Additionally, personalized edge training keeps user data private and secure. The MNIST datasets offer benchmarking insights against other designs (Table. I, including both system and IP power of DTM), while the KWS6 dataset represents an edge IoT sensor task requiring in-field recalibration and personalized training - this is the type of application this accelerator is designed for (Table. II). Point 1: Comparison with State-of-the-Art: The DTM designs with different matrix sizes and platforms are compared in terms of Giga-Operations per second per Watt (GOP s per W) against the related works described in Section III through Fig. 11: Conv-TM accelerator [40], FP-BNN [20], Reconfig DNN [27], Low-batch CNN [29], F-CNN [30], YOLO v3 [23], Fig. 11: Closest comparable State of The Art designs that implement ML training on FPGAs and report GOP s. DTMs shown in red. BFP-NN [31], FireFly [34], A2NN [24], WSQ-AdderNet [25], and Flex Block [32]. For DTM and Conv TM design, system- level power including ARM core is reported, named as DTM- SYS and Conv TM-SYS respectively while IP-only power is shown as DTM-IP and Conv TM-IP. The parameterized matrix sizes in the DTM design allow for scalability as seen through DTM-S and DTM-L (Table I). The DTM design philosophy is to utilize larger matrix sizes on larger FPGAs. Therefore, the clause compute matrix size and weight compute matrix size for these two designs are: DTM S - 32 literals 16 clauses, 2 clauses 4 classes and DTM L - 32 literals 27 clauses, 8 clauses 4 classes. The LFSR length for these two designs is 12-bits and 24- bits, respectively. The training speed, however, depends on TM model sizes and frequency. The accelerators run at 50 MHz and 100 MHz, respectively, due to the FPGAs being based on different technology nodes. Using GOP s in Fig. 11 allows for gauging the memory and compute bounds of different implementations. DTM designs are second only to FP-BNN (inference only) and Flex Block in GOP s. YOLO v3 [23], and BFP-NN [31] have fewer GOP s compared to the DTMs but also target more complex datasets. The flexibility of DTMs contrasts sharply with the fixed, customized designs seen in Conv-TM accelerator [40], SATA [35], FireFly [34], CNN [22], SNN [33] implementa- tions shown in Table I. Although optimized for specific tasks, they lack the adaptability required for diverse applications or PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 12 Fig. 12: Resource utilization of each module in DTM-L. Fig. 13: Latency during training and inference for DTM-L. on-field model recalibration. The simplicity of the TM training process makes DTM less reliant on DSPs than the DNN accelerators. The DTM algorithms are mapped into LUTs and BRAMs except for the clause update probability comparison in clause feedback. This results in the trade-off between larger resource usage but lower power. In terms of training latency, DTM implementation demon- strates significantly lower latency compared to non-TM solu- tions SATA [35] and CNN [22]. The design in Conv TM [40] offers better training latency, but only for 128 clauses. This architecture requires all clauses in each patch to be computed in one clock cycle. Scaling up further would have a significant impact on resources because it stores all TA actions in the register buffer, limiting its scalability to larger models with more features or clauses required. This paper also provides preliminary inference insights for CIFAR-10 using Composite TM [16]. While they show the feasibility of TMs to solve more complex problems, with the addition of Convolution TM modules into the DTM framework, a DTM-based architecture would be able to offer better resource usage and flexibility. Integration of these Conv TM modules is left to further work. Point 2: Flexibility and Target Usage: Table II highlights the flexibility of the DTM training architecture using the KWS6 dataset. The same DTM design can switch between the CoTM and the Vanilla TM with different clauses. The accuracy is comparable to Xilinx s FINN approach [5] and MATADOR [39] but DTM sacrifices throughput in favor of flexibility to change TM algorithm type, number of clauses and train online. As shown in the table, higher accuracies are possi- ble through higher clauses but at the cost of lower throughput. For KWS6, needing faster throughput means choosing CoTM while better performance means switching to Vanilla TM.5 Point 3: Design Trade-offs: The training process in the TM is affected by the quality of random numbers [42] and the precision of the CoTM weights. Figs. 14 and 15 demonstrate the impact of learning efficacy (i.e, test accuracy) and the 5This analysis of throughput implications vs number of clauses is continued in Supplementary Material for more datasets. These datasets are also reflective of the type of target usage for this DTM implementation. Fig. 14: Resource utilization and power for different weight length. Fig. 15: Resource utilization and power for different LFSR length. overhead in power and resources when varying the weight bit precision and LFSR length in the PRNG design. The results indicate that at least 12-bit weight precision is required to achieve good learning efficacy on the MNIST dataset (Fig. 14a); however, the test accuracy, utilization, and power consumption saturate as weight precision increases, suggesting 12-bits are sufficient. Fig. 15a and Fig. 15b focus on learning efficacy, power, and resources when varying the LFSR length in the PRNG block. Higher precision and better quality random numbers yield better test accuracy, but the improvement becomes less substantial. Notice that with seed refreshing in the slave PRNG blocks, the accuracy at 8-bits is much higher than without (Fig. 15a), seed refreshing leads to better accuracies across all LFSR precisions. The LFSR precision plays a larger role in the overall resource and power than the weight precision. VI. CONCLUSIONS AND FUTURE WORK This paper presented an FPGA-based Dynamic TM (DTM) accelerator architecture that supports both inference and train- ing acceleration for Vanilla TM and CoTMs. The architecture offers a competitive GOP s W ratio against comparable works but can also be reconfigured at run-time to adapt to different models and edge sensor tasks. This flexibility makes it suitable for on-field recalibration and personalized training. The TM clause computation maps very efficiently to FPGAs and the DTM design is parameterized to target both smaller low- power eFPGAs and larger FPGA fabrics. DTM is envisioned as a modular framework that can create a reconfigurable accelerator out of all TM algorithms. This paper explored the Vanilla and Coalesced variants. However, future work will extend to templates for Convolution and Regression along with Composite TM [17] support for larger datasets (CIFAR-10 and beyond). Users can use the FPGA to create required subsets from the global TM library for their application. REFERENCES [1] Y. Tian, M. A. Chao, C. Kulkarni, K. Goebel, and O. Fink, Real- time model calibration with deep reinforcement learning, Mechanical Systems and Signal Processing, 2022. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 13 TABLE I: Comparison with other FPGA accelerators suitable for similar edge sensor-based applications. (The - indicates this result is not reported. The power in the brackets is the IP-only power for Conv TM and DTM designs. The training latency of DTM is calculated using the average training latency over 250 epochs. The training and inference latency rows show the values for MNIST, FMNIST and KMNIST respectively for the Conv-TM and DTM designs.) Design SATA [35] FireFly [34] CNN [22] SNN [33] Conv TM [40] DTM-L (Large) DTM-S (Small) Platform 65nm ASIC XCZU-3EG XCZU-9EG Kintex-7 XCZU-7EV XCZU-7EV XC7Z20 Algorithm ANN,SNN SCNN CNN SNN Conv TM Coalesced TM, Vanilla TM Coalesced TM, Vanilla TM Accelerator Type Training Inference Training Inference Training Training Training Flexibility Reconfigurable Reconfigurable Customized Customized Customized Reconfigurable Reconfigurable LUT ASIC Simulation 15000 32589 46371 196252 104222 43497 FF - 33585 30417 73303 59610 33256 DSP 288 143 65 129 25 6 BRAM 162 95 150 0 37 138 URAM 0 0 0 42.5 96 0 Freq(MHz) 400 300 100 200 50 100 50 Test Accuracy 99.00 (MNIST) 98.12 (MNIST) 98.64 (MNIST) 97.81 (MNIST), 83.16 (FMNIST) 97.60 (MNIST), 84.10 (FMNIST), 82.80 (KMNIST) 97.74 (MNIST), 86.38 (FMNIST), 83.11 (KMNIST) 95.11 (MNIST), 80.62 (FMNIST), 72.52 (KMNIST) Training Latency(us) 204.67 Inference only 3581.81 Inference only 27, 27, 27 88.48, 97.69, 98.92 50.08, 58.67, 60.57 Inference Latency(us) - 491.16 - 4290, 4290 9.4, 9.4, 9.4 44.72, 44.72, 44.72 22.32, 22.32, 22.32 Power(W) - 2.550 - 0.535 4.543 (2.000) 4.359 (1.646) 1.687 (0.424) TABLE II: Comparison against MATADOR [39], FINN [4] and DNNBuilder [26] using KWS-6 dataset. Design Model LUT FF DSP BRAM URAM Test Acc Training Datapoint s Inference Datapoint s DTM-CoTM 2000 Clauses per class 104222 59610 25 37 96 86.07 18281 42878 1000 Clauses per class 83.76 34,128 83,084 500 Clauses per class 81.15 63,696 163,241 DTM-Vanilla TM 700 Clauses per class 87.12 45591 25873 500 Clauses per class 85.87 58,603 35,326 300 Clauses per class 83.17 86,663 55,670 MATADOR 700 Clauses per class 6063 10658 0 3 0 87.10 Inference only 8333333 FINN BNN (377-512-256-6) 42757 45473 0 26.5 0 84.60 Inference only 750188 DNN Builder DNN (273-64-128-64-6) 3035 3790 4 36 0 81.00 Inference only 5703 [2] A. Tashakori, W. Zhang, Z. Jane Wang, and P. Servati, SemiPFL: Personalized Semi-Supervised Federated Learning Framework for Edge Intelligence, IEEE Internet of Things Journal, 2023. [3] X. Wang, Y. Han, V. C. M. Leung, D. Niyato, X. Yan, and X. Chen, Convergence of edge computing and deep learning: A comprehensive survey, IEEE Communications Surveys Tutorials, 2020. [4] M. Blott, T. B. Preußer, N. J. Fraser, G. Gambardella, K. O brien, Y. Umuroglu, M. Leeser, and K. Vissers, FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks, ACM Trans. Reconfigurable Technol. Syst., 2018. [5] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong, M. Jahre, and K. Vissers, FINN: A Framework for Fast, Scalable Binarized Neural Network Inference, in ACM SIGDA, 2017. [6] X. Zhang, A. Ramachandran, C. Zhuge, D. He, W. Zuo, Z. Cheng, K. Rupnow, and D. Chen, Machine learning on FPGAs to face the IoT revolution, in IEEE ACM ICCAD, 2017. [7] R. Chen, H. Zhang, Y. Li, R. Zhang, G. Li, J. Yu, and K. Wang, Edge FPGA-based Onsite Neural Network Training, in IEEE ISCAS, 2023. [8] O.-C. Granmo, The Tsetlin Machine A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic, 2021. [Online]. Available: [9] S. Glimsdal and O.-C. Granmo, Coalesced Multi-Output Tsetlin Machines with Clause Sharing, 2021. [Online]. Available: https: arxiv.org abs 2108.07594 [10] O.-C. Granmo, S. Glimsdal, L. Jiao, M. Goodwin, C. W. Omlin, and G. T. Berge, The Convolutional Tsetlin Machine, 2019. [Online]. Available: [11] K. D. Abeyrathna, O.-C. Granmo, L. Jiao, and M. Goodwin, The Regression Tsetlin Machine: A Tsetlin Machine for Continuous Output Problems, 2019. [Online]. Available: [12] M. M. H. Shuvo, S. K. Islam, J. Cheng, and B. I. Morshed, Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review, Proceedings of the IEEE, 2023. [13] O. Tarasyuk, A. Gorbenko, T. Rahman, R. Shafik, and A. Yakovlev, Logic-Based Machine Learning with Reproducible Decision Model Using the Tsetlin Machine, in IEEE IDAACS, 2023. [14] L. Deng, The mnist database of handwritten digit images for machine learning research, IEEE Signal Processing Magazine, 2012. [15] T. Rahman, A. Wheeldon, R. Shafik, A. Yakovlev, J. Lei, O.-C. Granmo, and S. Das, Data Booleanization for Energy Efficient On-Chip Learning using Logic Driven AI, in IEEE ISTM, 2022. [16] Y. Grønningsæter, H. S. Smørvik, and O.-C. Granmo, An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine Composites, 2024. [Online]. Available: [17] O.-C. Granmo, TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines, 2023. [Online]. Available: https: arxiv.org abs 2309.04801 [18] X. Zhang, L. Jiao, O.-C. Granmo, and M. Goodwin, On the conver- gence of tsetlin machines for the identity- and not operators, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [19] O. Tarasyuk, T. Rahman, R. Shafik, A. Yakovlev, A. Gorbenko, O.-C. Granmo, and L. Jiao, Systematic Search for Optimal Hyper-parameters of the Tsetlin Machine on MNIST Dataset, in 2023 IEEE ISTM, 2023. [20] S. Liang, S. Yin, L. Liu, W. Luk, and S. Wei, FP-BNN, Neurocomput, 2018. [Online]. Available: [21] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations, 2020. [Online]. Available: [22] M. Cho and Y. Kim, Implementation of Data-optimized FPGA-based Accelerator for Convolutional Neural Network, in IEEE ICEIC, 2020. [23] M. Kim, K. Oh, Y. Cho, H. Seo, X. T. Nguyen, and H.-J. Lee, A Low- Latency FPGA Accelerator for YOLOv3-Tiny With Flexible Layerwise Mapping and Dataflow, IEEE Transactions on Circuits and Systems I: Regular Papers, 2024. [24] N. Zhang, S. Ni, L. Chen, T. Wang, and H. Chen, High-Throughput and Energy-Efficient FPGA-Based Accelerator for All Adder Neural Networks, IEEE Internet of Things Journal, 2025. [25] Y. Zhang, B. Sun, W. Jiang, Y. Ha, M. Hu, and W. Zhao, WSQ- AdderNet: Efficient Weight Standardization based Quantized AdderNet FPGA Accelerator Design with High-Density INT8 DSP-LUT Co- Packing Optimization, in 2022 IEEE ACM ICCAD, 2022. [26] X. Zhang, J. Wang, C. Zhu, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen, DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs, in IEEE ACM ICCAD, 2018. [27] J. Lu, J. Lin, and Z. Wang, A Reconfigurable DNN Training Accelerator on FPGA, in IEEE SiPS, 2020. [28] S. Fox, J. Faraone, D. Boland, K. Vissers, and P. H. Leong, Training Deep Neural Networks in Low-Precision with High Accuracy Using FPGAs, in IEEE ICFPT, 2019. [29] S. K. Venkataramanaiah, H.-S. Suh, S. Yin, E. Nurvitadhi, A. Dasu, Y. Cao, and J.-S. Seo, FPGA-based Low-Batch Training Accelerator PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 14 for Modern CNNs Featuring High Bandwidth Memory, in IEEE ACM ICCAD, 2020. [30] W. Zhao, H. Fu, W. Luk, T. Yu, S. Wang, B. Feng, Y. Ma, and G. Yang, F-CNN: An FPGA-based framework for training Convolutional Neural Networks, in IEEE ASAP, 2016. [31] T.-H. Tsai and D.-B. Lin, An On-Chip Fully Connected Neural Network Training Hardware Accelerator Based on Brain Float Point and Sparsity Awareness, IEEE Open Journal of Circuits and Systems, 2023. [32] S.-H. Noh, J. Koo, S. Lee, J. Park, and J. Kung, FlexBlock: A Flexible DNN Training Accelerator With Multi-Mode Block Floating Point Support, IEEE Transactions on Computers, 2023. [33] Y. Liu, Y. Chen, W. Ye, and Y. Gui, FPGA-NHAP: A General FPGA- Based Neuromorphic Hardware Acceleration Platform With High Speed and Low Power, IEEE Transactions on Circuits and Systems I: Regular Papers, 2022. [34] J. Li, G. Shen, D. Zhao, Q. Zhang, and Y. Zeng, FireFly: A High- Throughput Hardware Accelerator for Spiking Neural Networks With Efficient DSP and Memory Optimization, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2023. [35] R. Yin, A. Moitra, A. Bhattacharjee, Y. Kim, and P. Panda, SATA: Sparsity-Aware Training Accelerator for Spiking Neural Networks, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2023. [36] A. Wheeldon, R. Shafik, T. Rahman, J. Lei, A. Yakovlev, and O.-C. Granmo, Learning automata based energy-efficient AI hardware design for IoT applications, Philos. Trans. R. Soc. A Math. Phys. Eng. Sci., 2020. [37] R. A. Fisher, Iris, UCI Machine Learning Repository, 1988, DOI: [38] S. Maheshwari, T. Rahman, R. Shafik, A. Yakovlev, A. Rafiev, L. Jiao, and O.-C. Granmo, REDRESS: Generating Compressed Models for Edge Inference Using Tsetlin Machines, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [39] T. Rahman, G. Mao, S. Maheshwari, R. Shafik, and A. Yakovlev, MATADOR: Automated System-on-Chip Tsetlin Machine Design Gen- eration for Edge Applications, in IEEE DATE, 2024. [40] S. A. Tunheim, L. Jiao, R. Shafik, A. Yakovlev, and O.-C. Granmo, Tsetlin Machine-Based Image Classification FPGA Accelerator With On-Device Training, IEEE Transactions on Circuits and Systems I: Regular Papers, 2025. [41] O. Ghazal, S. Singh, T. Rahman, S. Yu, Y. Zheng, D. Balsamo, S. Patkar, F. Merchant, F. Xia, A. Yakovlev, and R. Shafik, IMBUE: In-Memory Boolean-to-CUrrent Inference ArchitecturE for Tsetlin Machines, in IEEE ACM ISLPED, 2023. [42] T. Rahman, G. Mao, S. Maheshwari, K. Krishnamurthy, R. Shafik, and A. Yakovlev, Parallel Symbiotic Random Number Generator for Training Tsetlin Machines on FPGA, in IEEE ISTM, 2023. [43] H. Xiao, K. Rasul, and R. Vollgraf, Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms, 2017. [Online]. Available: [44] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha, Deep learning for classical japanese literature, arXiv preprint arXiv:1812.01718, 2018. [45] P. Warden, Speech commands: A dataset for limited-vocabulary speech recognition, arXiv preprint arXiv:1804.03209, 2018. [46] J. Lei, T. Rahman, R. Shafik, A. Wheeldon, A. Yakovlev, O.-C. Granmo, F. Kawsar, and A. Mathur, Low-Power Audio Keyword Spotting Using Tsetlin Machines, Journal of Low Power Electronics and Applications, 2021. [Online]. Available: [47] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, and G. Zhang, Learning under Concept Drift: A Review, IEEE Transactions on Knowledge and Data Engineering, 2019. Gang Mao received B.E degree from Northeastern University, China in 2016, and M.E degree from Newcastle University, Newcastle upon Tyne, UK. He is a PhD student at Newcastle University, Newcastle upon Tyne, UK. His research interests are focused on asynchronous circuit design and developing Machine Learning accelerators. Tousif Rahman is a PhD student at Newcastle University, Newcastle upon Tyne, UK. He holds an MEng degree from the same institution. His research interests are focused on designing novel algorithms, architectures, and automation tools for translating Machine Learning applications to low- power, energy-efficient edge devices. Sidharth Maheshwari is an Assistant Professor in the department of Computer Science and En- gineering at Indian Institute of Technology (IIT) Jammu. He completed his B.Tech in Electronics and Electrical Engineering at IIT Guwahati in 2013. He worked at Newcastle University for his PhD and Postdoc between 2016-2022. His interests include using hardware software co-design to mitigate com- putational and energy bottlenecks of Big Data ap- plications. He is looking towards solving real-world challenges such as improving water quality through molecular diagnostics that includes genome analysis and antimicrobial resis- tance surveillance. He is also working in the domain of battery-management systems and novel battery-pack design for Indian tropical climatic conditions in order to address challenges in the e-mobility sector. Another vertical in his research interest lies in exploring novel machine learning algorithms. Bob Pattison received a BEng (1st class Hons) degree from Newcastle University in 2024. Currently a 1st year PhD student in the Microsystems group at Newcastle University. His research focus is on explainable and energy-efficient machine learning algorithms for low-power devices. Zhuang Shao is a Lecturer in Data Engineering and AI with the School of Engineering, Newcastle University, Newcastle upon Tyne, UK. He holds a PhD from the University of Warwick. His research interests include hardware-software co-design, low energy consumption machine learning, and vision- language learning. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 15 Rishad Shafik is a Professor of Microelectronic Systems within the School of Engineering, Newcas- tle University, UK. Dr Shafik received his PhD, and MSc (with distinction) degrees from Southampton in 2010, and 2005; and BSc (with distinction) from the IUT, Bangladesh in 2001. He is one of the editors of the Springer USA book Energy-efficient Fault-tolerant Systems . He is also author co-author of 200 IEEE ACM peer-reviewed articles, with 4 best paper nominations and 4 best paper poster awards. He recently chaired multiple international conferences symposiums, UKCAS2020, ISTM2022; guest edited two special theme issues in Royal Society Philosophical Transactions A; he recently co-chaired 2nd International Symposium on the TM (ISTM), 2023. His research interests include hardware software co-design for energy-efficiency and autonomy. Alex Yakovlev received the Ph.D. degree from the St. Petersburg Electrical Engineering Institute, St. Petersburg, USSR, in 1982, and D.Sc. from Newcastle University, UK, in 2006. He is cur- rently a Professor of Computer Systems Design, who founded and leads the Microsystems Research Group, and co-founded the Asynchronous Systems Laboratory, Newcastle University. He was awarded an EPSRC Dream Fellowship from 2011 to 2013. He has published more than 500 articles in various journals and conferences, in the area of concurrent and asynchronous systems, with several best paper awards and nominations. He has chaired organizational committees of major international conferences. He has been principal investigator on more than 30 research grants and supervised over 70 Ph.D. students. He is a fellow of the Royal Academy of Engineering, UK.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nPREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 1 Dynamic Tsetlin Machine Accelerators for On-Chip Training using FPGAs Gang Mao , Tousif Rahman , Sidharth Maheshwari , Bob Pattison , Zhuang Shao , Rishad Shafik , Senior Member, Alex Yakovlev , Fellow Abstract The increased demand for data privacy and security in machine learning (ML) applications has put impetus on effective edge training on Internet-of-Things (IoT) nodes. Edge training aims to leverage speed, energy efficiency and adaptability within the resource constraints of the nodes. Deploying and training Deep Neural Networks (DNNs)-based models at the edge, although accurate, posit significant challenges from the back-propagation algorithm s complexity, bit precision trade- offs, and heterogeneity of DNN layers. This paper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an al- ternative to DNN implementations. DTM utilizes logic-based on- chip inference with finite-state automata-driven learning within the same Field Programmable Gate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin Machine algorithms, the dynamic aspect of the accelerator design allows for a run-time reconfiguration targeting different datasets, model architectures, and model sizes without resynthesis. This makes the DTM suitable for targeting multivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer multiply- accumulates, devoid of derivative computation. It is a data-centric ML algorithm that learns by aligning Tsetlin automata with input data to form logical propositions enabling efficient Look- up-Table (LUT) mapping and frugal Block RAM usage in FPGA training implementations. The proposed accelerator offers 2.54x more Giga operations per second per Watt (GOP s per W) and uses 6x less power than the next-best comparable design. Index Terms Edge Training, Coalesced Tsetlin Machines, Dynamic Tsetlin Machines, Embedded FPGA, Machine Learning Accelerator, On-Chip Learning, Logic-based-learning. I. INTRODUCTION M ACHINE Learning (ML) offers a generalized approach to developing autonomous applications from Internet- of-Things (IoT) sensor data.\n\n--- Segment 2 ---\nI. INTRODUCTION M ACHINE Learning (ML) offers a generalized approach to developing autonomous applications from Internet- of-Things (IoT) sensor data. Having ML execution units in close proximity to the sensor, at the so-called edge, enables faster task execution with high data security and privacy. However, sensor degradation and environmental factors may require recalibration [1] or user-personalized on-field train- ing [2] to ensure continued functionality. Implementing so- lutions to these challenges is nontrivial. It requires finding the right balance between achieving the appropriate learning efficacy for the ML problem and the restrictive compute memory resources available on the platforms [3]. This work was supported by EPSRC EP X036006 1 Scalability Oriented Novel Network of Event Triggered Systems (SONNETS) project and by EPSRC EP X039943 1 UKRI-RCN: Exploiting the dynamics of self-timed machine learning hardware (ESTEEM) project. Indicates equal contribution. Microsystems Research Group, School of Engineering, Newcastle Univer- sity, Newcastle upon Tyne, NE1 7RU, United Kingdom (UK). IIT Jammu, NH-44 , PO Nagrota, Jagti, Jammu Kashmir 181221, India. For ML inference tasks on edge nodes, these challenges have been widely explored, e.g., quantization [4], sparsity- based compression, and pruning for the most commonly used Deep Neural Network (DNN) models [3], [5], [6]. So far, very few works have dealt with DNN on-field training due to two important design challenges: firstly, the main compute for DNN on-field training typically requires retaining full precision floating point general matrix multiplications and secondly the complexity of the backpropagation algorithm through the heterogeneous DNN layers [7]. These challenges materialize from algorithmically intrinsic attributes of DNNs. State-of-the-art approaches to alleviating these challenges are explored in Section III. This work addresses the aforemen- tioned challenges in on-field training through two fronts: firstly, on the algorithm front, whether the same (or better) edge training performance can be harnessed from simpler and more hardware-oriented ML algorithms; and secondly, on the hardware front, how such algorithms can be efficiently mapped to edge training platforms.\n\n--- Segment 3 ---\nState-of-the-art approaches to alleviating these challenges are explored in Section III. This work addresses the aforemen- tioned challenges in on-field training through two fronts: firstly, on the algorithm front, whether the same (or better) edge training performance can be harnessed from simpler and more hardware-oriented ML algorithms; and secondly, on the hardware front, how such algorithms can be efficiently mapped to edge training platforms. These fronts are addressed in turn: On the algorithm front, this work explores ML algorithms with lower training complexity compared to DNNs called Tsetlin Machines (TMs) [8]. The TM and its variants are logic- based learning algorithms. They learn logical propositions called clauses to identify classes. The learning process for creating these logic propositions involves state transitions of finite-state automata. The TM and its family of algorithms ([9] [11]) alleviate the above two core challenges faced by DNNs: TMs utilize logic-based computation and simpler nested decision logic to adjust the learning elements during training instead of floating-point weights and backpropagation. On the hardware front, this work argues the case for very resource-constrained, low-cost, low-power, System on Chips (SoCs) operating as on-site single edge nodes. The recent adoption of such SoCs as embedded FPGAs (eFPGAs) offers the ability to integrate custom hardware accelerators in conjunction with CPU processing to leverage faster inference latency within edge node power budgets [6], [7]. The FPGA component, in particular, offers advantages in customization, cost, ease of prototyping, and lower implementation risk compared to ASIC solutions with better power efficiency than CPU and GPU training implementations [12]. Additionally, FPGA accelerator designs can be easily scaled to smaller or larger platforms, depending on the application needs. The cul- mination of the two fronts above forms the main contributions towards the development of Dynamic Tsetlin Machine (DTM) FPGA hardware implementations: arXiv:2504.19797v1 [cs.AR] 28 Apr 2025 PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 2 Fig. 1: Block diagram of the fundamental components of the Tsetlin Machine algorithms for inference - they build the Vanilla TM (pink) and the Coalesced TM (CoTM) (yellow).\n\n--- Segment 4 ---\nThe cul- mination of the two fronts above forms the main contributions towards the development of Dynamic Tsetlin Machine (DTM) FPGA hardware implementations: arXiv:2504.19797v1 [cs.AR] 28 Apr 2025 PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 2 Fig. 1: Block diagram of the fundamental components of the Tsetlin Machine algorithms for inference - they build the Vanilla TM (pink) and the Coalesced TM (CoTM) (yellow). Component breakdown: a) shows the pre-processing or Booleanization process (adapted from [13]) to generate the inputs to TMs using an MNIST datapoint example; b) shows the learning element - the Tsetlin Automata (TA); c) shows how each TA relates to its respective Boolean literal to create the Clause Output. The pink and yellow blocks for the Vanilla and CoTM show Class sum computation for each class (d and e). DTM envisages generalized FPGA hardware for training and inference for all variants of TM algorithm. This paper currently generalizes two variants of the TM algorithm viz. Vanilla and Coalesced TM (CoTM) 1. Pseudo Random Number (PRN) Bandwidth and qual- ity: Efficient TM training requires a large amount of high- quality PRNs with high bandwidth. Considering resource constraints at the edge, an XOR-shift-based re-seeding mechanism is developed providing fresh seeds after 2L cycles for every L-bit Linear Feedback Shift Registers (LFSRs) enhancing training accuracy for small L. Scalability: The DTM architecture can be scaled accord- ing to the available hardware resource budget but retains the capability to execute a range of model sizes. Flexibility: For an implemented hardware architecture, DTM enables data-centric inter-model switching through precomputed masks and matrix iteration cycles at run- time without needing resynthesis (Section IV-A). Reducing computation complexity: DTM training im- proves design metrics by replacing floating-point arith- metic with integer-arithmetic retaining desired accuracy (Section IV-B). Optimized Clause-Level feedback: Feedback to clauses reduces as the model converges.\n\n--- Segment 5 ---\nReducing computation complexity: DTM training im- proves design metrics by replacing floating-point arith- metic with integer-arithmetic retaining desired accuracy (Section IV-B). Optimized Clause-Level feedback: Feedback to clauses reduces as the model converges. DTM optimally skips groups of clauses that do not receive any feedback in the TA update stage, therefore, reducing the total number of 1The standard TM algorithm as described in [8] will be referred to as Vanilla but still abbreviated as TM. memory accesses and operations (Section IV-B). The remaining sections explore these contributions through examining TM algorithms and related designs in Sections II and III, the proposed architecture in Section IV and evaluation in Section V. II. BACKGROUND: TSETLIN MACHINE ALGORITHMS This paper introduces the first CoTM hardware and Dy- namic TM architecture. This section presents the fundamental components of the algorithms in the DTM architecture. A. Inference: Figure 1 shows inference components in four parts: a) shows input data preprocessing called Booleanization, b) shows Tsetlin Automata (TA) - the learning element in TM algorithms, c) shows how processed input data interact with their respective TAs to create a Clause Output, and the two Vanilla and CoTM blocks show how class sums are computed. a) Booleanization: Booleanization is the process of con- verting raw input data into a binary encoding and extending this to each binary feature and complement (x, x) - these are referred to as Boolean features (seen in grey) and Boolean Literals (l) (grey and green), respectively, as seen in Fig. 1a with an MNIST [14] 0 digit example. In this example, only one threshold is used to convert the raw feature to the Boolean Feature (f). However, in practice, many thresholds can be applied to each raw feature such that they are binned into discrete binary encodings. Details on Booleanization and dif- ferent Booleanization strategies are explored in detail in [15]. Higher complexity datasets such as CIFAR-10 and CIFAR- 100 are targeted using Composite TM architectures [16], [17]. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 3 Fig. 2: Block diagram of learning process used in the Vanilla TM and CoTM referred to as feedback.\n\n--- Segment 6 ---\nPREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 3 Fig. 2: Block diagram of learning process used in the Vanilla TM and CoTM referred to as feedback. Feedback is used to transition the learning elements of the model, i.e., the TAs and weights (weights for CoTM). These processes are presented visually here, but will be explored as algorithm blocks in the subsequent section. Composite TMs involve a group composed predominantly of Convolution TMs [10] that each work on their own respective Booleanized input space derived via adaptive thresholding (adaptive Gaussian and Otsu with kernels up to 10 10), Canny edge detection, Color thermometers (kernels of 3 3 to 5 5) and Histogram of Gradients. Vanilla and Coalesced TMs feature less prevalently in such Composite TM architectures being suitable for smaller multivariate, image and audio prob- lems (explored later in Section V). Future work will add the Convolutional TM modules and pre-processing frameworks as modules for DTM enabling Composite TM training support. b) Tsetlin Automata (TA): The fundamental component underpinning all TM algorithms is Tsetlin Automata (TA). Each TA behaves like a finite state machine with two possible actions with respect to its corresponding Boolean literal. As seen in Fig. 1b, TA has 2J states; if the automaton s state value is J, then the automaton s action is to Exclude the corresponding literal, while if the state value J, then the automaton s action is to Include the corresponding literal (Fig. 1b). These Include Exclude decisions allow the TM to build logic expressions seen in the next image section (Fig. 1c). The convergence of TMs relies on the self-organizing behavior of TAs as they learn optimal logic expressions to classify data. Each TA operates independently, receiving Reward (brown arrow) and Penalty (blue arrow) signals to transition their state as they compete to form the best decision boundaries, their interactions can be modeled as a game where each automaton tries to maximize classification accuracy. A Nash Equilibrium is reached when no TA can improve its decision without detriment in the performance of another [18]. c) Clause Output: Fig.\n\n--- Segment 7 ---\nA Nash Equilibrium is reached when no TA can improve its decision without detriment in the performance of another [18]. c) Clause Output: Fig. 1c shows how each Boolean literal in a MNIST datapoint is interfaced with its own TA to create a clause. The clause computation is composed of AND, OR and NOT operations and generates a single bit Clause Output (cl marked with brown). The training process determines which Boolean literals to include and exclude using their respective TAs to build logic expressions for each clause. Clauses are the fundamental modules of TMs; however, they are used differently in each TM algorithm. The pink and yellow boxes illustrate clause usage for class sums in Vanilla and CoTMs. d) Class sum compute for Vanilla TM: Focusing on Vanilla TM first, Fig. 1d shows how each class is composed of a set number of clauses. The clauses are divided into two teams (Positive Team and Negative Team). The one-bit outputs of each clause are summed and multiplied by either 1 or 1 polarity depending on the clause team. The summation of both teams polarity-adjusted sums produces the class sum. The argmax of each of these class sums across all classes generates the final inference classification. e) Class sum compute for CoTM: CoTM uses clause PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 4 Fig. 3: The log scale comparison of logic-based (clause compute) vs. integer-based (class sum compute) arithmetic operations in CoTM inference. The number of operations increase with clauses. outputs differently. Instead of instancing clauses for every class, it creates a shared pool of clauses (SC), shown in Fig. 1e. These clause outputs are shared across all classes. To generate a class sum, each class has its own set of learnable signed integer weights (w) which act as multiplicands with their respective clause in the SC pool. The clauses in the SC pool no longer have polarity like the Vanilla TM; instead, their polarity is assigned when multiplied with their respective weight in each class. Despite the additional weight multiplications, as seen in Fig. 3, the inference operations are still dominated by the logical operator computation. B.\n\n--- Segment 8 ---\n3, the inference operations are still dominated by the logical operator computation. B. Learning (Feedback): a) TM learning algorithms (feedback): During training, the learning process occurs for each datapoint after the inference is completed, i.e. the clause outputs and class sums have been generated. TM learning algorithms find the optimum states for every TA in the system and the values for the weights for each clause for every class of the CoTM. To transition a TA, that is, increment or decrement its state, feedback must be issued down the hierarchy of the TM. Fig. 2 shows how this first starts with Class-level feedback, to Clause-level feedback, and finally how this leads to TA increment and decrements. b) Class level feedback: Feedback decisions for both TM algorithms begin at the class level, and both Vanilla and CoTM follow the same class-level feedback procedure. The learning in pairwise, for each datapoint, two rounds of feedback will happen (these are referred to as class updates), each round updates one particular class. Firstly, the target class Class y (that is, the class to which the data point corresponds) followed by a randomly chosen class Class y. If the target class is selected, then yc 1 otherwise yc 0. This forms the inputs to the decision process used for clause-level feedback. c) Clause level feedback: Once the class yc has been selected for feedback, the class sum of this chosen class will be used to evaluate the clause update probability C1 or C2 (the class sum would already have been computed during inference). The clause update probability is a comparison between a random number and an expression that is formed by the clipped class sum (csum clip) and the Threshold hyperparameter T. If the clause update probability is 1 then this clause has been selected for feedback. For Vanilla TM, the selected clause s polarity is used to determine whether it should receive Type I or Type II feedback. These are used to provide feedback at the TA level. d) Clause level feedback (Weight Feedback): As seen in the inference figure, CoTM has a weight assigned to each clause in each class. The weight feedback is dependent on the generated clause output for this particular clause and the class- level feedback yc. The weight update is controlled through the clause update probabilities C1 or C2.\n\n--- Segment 9 ---\nThe weight feedback is dependent on the generated clause output for this particular clause and the class- level feedback yc. The weight update is controlled through the clause update probabilities C1 or C2. e) TA level feedback: Type I and Type II feedbacks both iterate through every literal in the selected clause to determine whether to transition it s associated TA. For Type I feedback, there are TA update probabilities 1 s and s 1 s for both TA state value increment and decrement, respectively. These update probability expressions introduce the Sensitivity hyperparameter s. TM algorithms also have a mode that is called boost true positive . When using this mode, the increase in Type I feedback will always occur regardless of the condition S2. Type II feedback is deterministic. The feedback process involves two hyperparameters s and T. The effect of these parameters on TA updates and clause updates, respectively, is explored in [19] for the Vanilla TM. The following section will illustrate how these probabilities are translated to FPGA. III. RELATED ACCELERATOR DESIGNS This section explores training accelerator designs targeting FPGAs for DNNs to address how intrinsic attributes of the TM algorithm can offer reduced complexity2. a) Quantization in Training: One of the algorithmic drawbacks of all DNN-based algorithms is the use of 32-bit floating point weights and computation. Most works address this through quantization of the these floats [4], [20] [26]. Extreme quantization to binary weights and data, referred to as Binary Neural Networks (BNNs), leads to a reduction in the computation of floating point multiplication and addition to XNOR and popcount. This is incredibly effective for high-throughput inference accelerators [4], [20], [21]. Trans- lating the training of these quantized DNNs to accelerators is challenging because the precision of the weights cannot be reduced as significantly. For example, for training, the authors in [22] retain 20-bit weights for their accelerator. This results in substantial Block RAM (BRAM) usage for weight storage and many DSP blocks for their computation. Batched training to improve throughput is seen in [27] (Low Batch CNN). b) Design Approaches for Backpropagation: DNN train- ing requires backpropagation for weight updates during train- ing. This involves computationally expensive partial derivative calculations.\n\n--- Segment 10 ---\nb) Design Approaches for Backpropagation: DNN train- ing requires backpropagation for weight updates during train- ing. This involves computationally expensive partial derivative calculations. The backpropagation computation in the training accelerators in [28], is kept as floating point, but happens on the ARM processor of an SoC-FPGA. The proposed accelera- tor is also developed for an SoC-FPGA but the ARM processor is used for configuring the run-time architecture search. The work in [27] (Reconfig DNN) also provides reconfigurability to adopt different applications. c) Addressing Layer Heterogeneity: Stochastic gradient descent allows training in minibatches. This approach for backpropagation is used by [29] [32], but this time developed on the FPGA fabric itself; this approach retains 32-bit floats throughput. This work also addresses another design challenge with DNNs: Layer heterogeneity. They partition the training process according to the different layers and use a stream- based reconfiguration to configure basic modules for each 2The related works are qualitatively compared here to understand the design choices. In the Accelerator Evaluation section, these implementations are considered more quantitatively against the proposed accelerator. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 5 layer s workload. The main overhead is memory transfer for each layer from off-chip memory. d) Spiking Neural Network Approaches: Two digital Spiking Neural Network (SNN) inference FPGA implemen- tations are reported in [33], [34]. For the three-layer SNN network [33] mapped to Xilinx Kintex-7 reported in this paper, it can infer 233 images per second for MNIST and FMNIST, while achieving 97.81 and 83.16 accuracy. In [35], a 65nm SNN training AISC simulation is reported. The simulated training throughput for MNIST is around 5000 images s with 99.00 test accuracy. e) Advantages of TMs and related works: The TM algorithms intrinsically use bitwise operations for clause com- putation and comparisons using random numbers in the feed- back process.\n\n--- Segment 11 ---\nThe simulated training throughput for MNIST is around 5000 images s with 99.00 test accuracy. e) Advantages of TMs and related works: The TM algorithms intrinsically use bitwise operations for clause com- putation and comparisons using random numbers in the feed- back process. The entire dataflow can, naturally, operate in the integer-arithmetic domain on hardware mitigating many design challenges encountered in quantization and backpropagation. The first preliminary TM training and inference implemen- tation [36] uses an ultra-low power ASIC solution based on 65nm technology customized for the 3-class Binary Iris data set [37]. REDRESS [38] presents a microcontroller-based custom high-throughput inference implementation employing bit-parallel optimization. A custom TM accelerator framework (MATADOR) [39] focuses on automated deployment for max- imum throughput, once again for a specific task, with a custom hard-wired Vanilla TM model. In MATADOR, the TA actions are hardcoded into LUTs and cannot be calibrated without resynthesis, which means MATADOR cannot be extended to a training accelerator. A recent FPGA-based customized Convolutional Tsetlin Machine (Conv TM) training acceler- ator [40] reports solving customized 28 28 booleanized images, with a 10 10 convolution window. Conv TM is designed for fast throughput at the expense of resources. Section V presents further comparison between DTM and Conv TM. In contrast to related works, the DTM design showcases real-time recalibration and flexibility at the expense of throughput. These qualities are more valuable in on-chip training implementations that require re-calibration. IV. PROPOSED DYNAMIC TSETLIN MACHINE (DTM) ACCELERATOR The following two subsections explore how the TM and CoTM concepts are translated to the DTM architecture im- plementation with SoC integration. Fig. 1 shows that the TM algorithms are intrinsically modular with the Vanilla and CoTM architectures composed of clause computation blocks. Unlike layered DNN architectures, the model architectures of both Vanilla and CoTMs are controlled by the number of clauses (alleviating layer heterogeneity design challenges). This shared basis across the two TM algorithms allows for greater reuse. The Dynamic in the DTM architecture represents flexibility and reconfigurability at runtime. The compute and memory components of the DTM architecture are presented in Fig.\n\n--- Segment 12 ---\nThe Dynamic in the DTM architecture represents flexibility and reconfigurability at runtime. The compute and memory components of the DTM architecture are presented in Fig. 4. The architecture is divided into five compute modules: Clause Matrix, Weight Matrix, Weight Update Matrix, TA Update Matrix and Argmax and their associated memory modules. The Clause Matrix, Weight Matrix and Argmax are used for inference, while the remaining update blocks are used for training. The next two subsections examine these blocks and their data buffers in inference and training. A. DTM Inference Modules: Inference involves computing of clause outputs (for every class for Vanilla TM), applying the weights (for the CoTM) and then summing (class sums) and performing argmax. This subsection walks through this process from how features enter the accelerator to how the classification is generated. a) Feeding in the Features: The accelerator is interfaced with the SoC processor via an AXI4-Stream channel, Boolean input features are received and placed in the Feature Buffer (see Fig. 4). Users does not need to set this buffer to the problem s feature size explicitly, the feature buffer size only determines a maximum feature number. If the DTM accelerator is to be deployed on a resource-limited FPGA chip (e.g., Zynq7020) the Feature Buffer will be registers, but when scaling to larger chips (e.g., Ultrascale ZU-7EV), the Feature Buffer will be instanced as BRAM. This allows DTM to handle larger datasets in a more resource-efficient way. Suppose that the width of the AXI bus is W bits; the DTM will receive W Boolean features in every clock cycle. In the clause computation phase, it selects x 2 features and generates their respective x literals (these features and their complements) and sends them to the Clause Matrix. b) Clause Computation: Clause computation occurs in the Clause Matrix (Block 1 in Fig. 4), the detailed mapping of the clause computation to the LUTs is shown in Fig. 4-6. Loading all literals (lit) and TA states (ta) for every TM clause at once demands high bandwidth and logic resources. Complete clause computations create long com- binational chains, reducing operational frequency; to reduce resource pressure and latency, TA actions won t all load into the Clause Matrix at once.\n\n--- Segment 13 ---\nLoading all literals (lit) and TA states (ta) for every TM clause at once demands high bandwidth and logic resources. Complete clause computations create long com- binational chains, reducing operational frequency; to reduce resource pressure and latency, TA actions won t all load into the Clause Matrix at once. p cl1 p cl2 ... p cly x i 1(Li TAi 1) x i 1(Li TAi 2) ... x i 1(Li TAi y) p cl 1 p cl 2 ... p cl y (1) Algorithm 1: CLAUSE Function Input: L (Literal), l mask (literal mask), Action (TA action, 1 means Inc, 0 means Exc), cl buffer mask (the clause buffer mask), x (literal width of clause computation matrix), y (clause width of clause computation matrix) 1 CLAUSE(TM type, l mask, cl buffer mask, h, x, y, L, Action) foreach i y do 2 foreach j x do 3 p cl[i] p cl[i] cl buffer mask[i] ((L[j] l mask[j]) Action[i][j]); 4 return p cl; c) Introducing Partial Clauses: Instead, drawing from previous works [39], [41], the full clause matrix computa- PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 6 Fig. 4: The architecture of the proposed Dynamic Tsetlin Machine (DTM) training accelerator. The DTM accommodates both the Vanilla and CoTM algorithms. It consists of 5 core modules: 1-Clause Matrix, 2-Weight Matrix, 3-Weight Update Matrix, 4-TA State Update Matrix, 5-Argmax Output. Blocks 1, 2 and 5 are for Vanilla and CoTM inference while TM training needs both blocks 3 and 4. All buffers and Pseudo Random Number Generator (PRNG) blocks are used for both algorithms. 6 also shows how the clause computation is mapped into LUT6 FPGA elements. tion is now decomposed into smaller partial clause matrix computations. For a given datapoint, DTM uses computation slices of a group of clauses that utilize only a subset of all TA actions and their corresponding literals. These are partial clauses.\n\n--- Segment 14 ---\nFor a given datapoint, DTM uses computation slices of a group of clauses that utilize only a subset of all TA actions and their corresponding literals. These are partial clauses. Suppose that a TM model contains f features (so f 2 literals), c clauses per class for Vanilla TM and c clauses altogether for CoTM, and h classes3: The Clause Matrix will process x of these literals to compute y partial clauses, using x y corresponding TA actions x y for each computation. The next few paragraphs discuss the iterations of this divided computation and how redundancies are handled when the matrices cannot be divided without remainders. d) Partial Clause Matrix Computation: Equation 1 shows how partial clauses are computed in the Clause Matrix. 3These variables will now be used throughout the remainder of the paper and their relationships will be further elaborated in the DTM algorithm blocks. The reader is encouraged to refer back to Fig. 1 and Fig. 5 to better understand the design choices made with the hardware implementation. Algorithm 2: CSUM Function Input: cl (loaded clause), cl mask (clause mask in weight computation matrix), W (loaded weights), m (clause width of weight computation matrix), n (class width of weight computation matrix), Lcsum (Maximum length of class sum) 1 CSUM(m, n, cl mask, cl, W, Lcsum) foreach i n do 2 foreach j m do 3 p cs[i] p cs[i] (cl[j] cl mask[j]) W[i][j]; 4 return p cs; The vector p cl represents the result of the previous partial clauses while p cl represents the current partial clauses result. p cl will be set to a 1 vector at the beginning of a group of clause computation. The computation remains the same PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 7 (a) Partial clause computation slices and iteration sequence (ST A represents one round of compute for the Clause Matrix). (b) (Class sum computation slice in memory (Sw represents the Weight Matrix computation slice). Fig. 5: Visualizing the iterations required to compute full clause and weight matrices. These figures show the rounds of compute required for the Clause Matrix and Weight Matrix. The colors of (a) and (b) correspond to the colors of the matrix modules in Fig.\n\n--- Segment 15 ---\nThese figures show the rounds of compute required for the Clause Matrix and Weight Matrix. The colors of (a) and (b) correspond to the colors of the matrix modules in Fig. 4. between the literal L and the TA actions subset TAi j for the ith literal in the jth clause in this matrix. e) Partial Clause Iteration Sequence: Fig. 5a shows the iterations for the Clause Matrix block. The matrix needs a 2 f x clock cycles to compute a group of y clauses and b c y groups to compute one class of clauses. For CoTM, this is the full iteration as the CoTM will use the shared clause pool (see Fig. 1), but when computing clauses for Vanilla TM, b h groups of clauses will be computed. f) Masking Remainder Compute: There may be cases of remainder literals in the last slice fed into the Clause Matrix. In these cases, the remainder literals (X) will be masked. This is shown in Fig. 6a. An inverted version of this mask is also used for TA Update Matrix. Similarly to the remainder literal mask when there are remainder clauses, a clause mask is used to prevent writing these remainder clauses into the clause buffer (see Fig. 6b). The mask uses logic 0 for these remaining elements so that when logic AND d, they will is always 0. All these ideas are culminated in (Algorithm 1). g) Feeding TA actions for inference: Considering the model size and resource limitations of the target FPGA plat- form, it is more efficient to store the TA states in BRAM instead of registers. For a clause computation with x literals and y clauses, assuming the TA state length is LT A bits and the BRAM used to store TA states is configured as WT A RAM bits, at least x y LT A WT A RAM BRAMs will be configured as TA RAMs. The clause computation matrix will read the actions from one row of TA states data in each clock cycle (see Fig. 4-1). Each clause computation slice in Fig. 5a takes one row of on-chip RAM (e.g. BRAM and URAM). p cs1 p cs2 ... p csn w1 1 w2 1 . . . wm 1 w1 2 w2 2 . . . wm 2 ... ... ... ... w1 n w2 n . . .\n\n--- Segment 16 ---\n. . wm n cl1 cl2 ... clm p cs 1 p cs 2 ... p cs n (2) After a group of clauses is computed in the Clause Matrix, the clause outputs will be stored in either registers or (a) Literal mask (Clause Ma- trix) (b) Clause buffer mask (c) Clause mask (Weight Ma- trix) (d) Class mask Fig. 6: Masks for remainder compute elements (marked in red). BRAM depending on the FPGA resources - this is the clause buffer. The clause buffer stores only 1 class of clause outputs. h) Class Sum Computation: A similar slicing strategy is also applied in the Weight Matrix for class sum computa- tion (see Fig. 4). Loading all weights and clauses to compute one class sum is resource-intensive. i) Partial Class Sums: Splitting class sums into multi- ple partial matrix multiplications provides better performance (running at higher frequencies) and lower resource costs. Thus, full class sum computation is split into multiple rounds of partial weight multiplication and vector addition. The weight matrix computes n partial class sums using the m clauses in each iteration (see Equation 2). In Equation 2, p cs represents the previous partial class sum result, p cs represents the current partial class sum result.p cs will be set to 0 when start a group of class sum computation, clj represents the pth clause in this class, and wj i represents the weight of the jth clause of the ith class in this matrix. For the CoTM, the accelerator takes p c m clock cycles to compute a group of n class sums, and q h n groups of partial class sums to get all class sums for h classes. For Vanilla TM class sum computation, the weights be- PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 8 come 1 for positive clauses and -1 for negative clauses. Positive clause are even indexed while negative clauses odd indexed. Therefore, a class sum for class i can be obtained by Equation 3 using the same dimension vectors and matrix as in Equation 2, where p csi and p cs i represent the current and the next partial class sum of the ith class, and cli j represent the jth clause in the ith class. In this case, the weights loaded into Fig.\n\n--- Segment 17 ---\nTherefore, a class sum for class i can be obtained by Equation 3 using the same dimension vectors and matrix as in Equation 2, where p csi and p cs i represent the current and the next partial class sum of the ith class, and cli j represent the jth clause in the ith class. In this case, the weights loaded into Fig. 4- 2 become 1 and 1 for even clauses and odd clauses respectively. The accelerator takes p c m clock cycles to compute one full class sum for the Vanilla TM. p csi p csi ... p csi 1 1 . . . 1 1 1 . . . 1 ... ... ... ... 1 1 . . . 1 cli 1 cli 2 ... cli m p cs i p cs i ... p cs i (3) j) Feeding the Weights: Considering the size of all weights, it is more feasible to store them in BRAM on FPGA. For an accelerator with a weight matrix sized as m n, assume that each weight has w bits, and that the BRAM has used to store the weights are configured as u bits. Then m n w u BRAMs are required for weight storage. When computing a group of class sums, only one row of data will be read as the weights for the computation in each clock cycle (see Fig. 4-2). Vanilla TM inference only requires constant weights, so there is no reading from BRAM. k) Masking Remainder Compute: When there is a remain- der clause in Weight Matrix, a clause mask is applied with logic 0 for the remainder bits and logic AND d with the remainder clause bits so they will not contribute to the class sum (see Fig. 6c). When there are remainder class sums computed, a class sum mask is applied as post-processing which assigns 2Lcsum 1 in binary (Lcsum is the maximum length of the class sum in binary, and 2Lcsum 1 is the minimum possible class sum in binary) to the remainder class sums while the others remain, so that the remainder class sum will never be greater than used classes (see Fig. 6d). l) Class Sum and Argmax: When computing the class sums for CoTM, the binary multiplication is optimized to a logic AND operation between the clause and the corresponding weight bit by bit (see Algorithm 2). The Argmax is a comparison tree with n class sums as input.\n\n--- Segment 18 ---\nl) Class Sum and Argmax: When computing the class sums for CoTM, the binary multiplication is optimized to a logic AND operation between the clause and the corresponding weight bit by bit (see Algorithm 2). The Argmax is a comparison tree with n class sums as input. The maximum class sum and its class index will be cached and then used for comparison with the maximum class sum in the next group of class sums until all class sums are compared. B. DTM Training Modules: This subsection presents the implementation details of the training modules for the DTM. It is recommended to read this section and its algorithms with reference to Fig. 24. Much like in the background section, this subsection presents how the feedback to TAs is passed from class level to clause level to TA level. a) Class-Level Feedback: Training requires both the target class and a randomly chosen class (referred to as the negated class) for each training datapoint. The target class is padded into the input data stream sent to the accelerator (presented 4For example if a paragraph has a bold label Class-level feedback - view this part of Fig 2 against the same algorithm name in this section.\n\n--- Segment 19 ---\na) Class-Level Feedback: Training requires both the target class and a randomly chosen class (referred to as the negated class) for each training datapoint. The target class is padded into the input data stream sent to the accelerator (presented 4For example if a paragraph has a bold label Class-level feedback - view this part of Fig 2 against the same algorithm name in this section. This should make it easier to follow the nested condition parts in the algorithms Algorithm 3: Class-level Feedback Generation Input: T (hyper parameter), yc (input class, 1 means target class, 0 means negated class), c rand (random number for negated class choice), Lw rand (random number length for clause-level feedback compute), Target Class (target class index), Update csum (Class sum for updated class), Class num (Class number) 1 NC_Gen(c rand, Target Class, Class num) RNC c rand (Class num 2); 2 if RNC Target Class then 3 Negated Class RNC; 4 else 5 Negated Class RNC 1; 6 return Negated Class; 7 Update_Probability_Compute(yc, c rand, Lw rand, Target Class, Class num, Update csum) 8 if yc 1 then 9 Update csum cum[Target Class]; 10 else 11 Negated index NC_Gen(c rand, Target Class, Class num); 12 Update csum cum[Negated index]; 13 CSum clip(Update csum, [ T, T]); 14 if yc 1 then 15 PCl Update (T CSum) 2LT 1; 16 else 17 PCl Update (T CSum) 2LT 1; 18 return PCl Update; later), while the randomly chosen negated class is determined within the hardware accelerator (see Function NC Gen in Algorithm 3 (NC_Gen)). The clause-level update probability is computed in Function UpdateProbabilityCompute with some changes to be more easily translated to hardware. b) Clause-Level Feedback: After selecting a class to be updated (yc), the clause-level feedback should be determined based on the comparison result between the product of a Lw rand-bit random number w rand with the hyperparameter T and the clause update probability (see Algorithm 3). The clause-level feedback will be stored in Clause Feedback Buffer as either registers or BRAM.\n\n--- Segment 20 ---\nb) Clause-Level Feedback: After selecting a class to be updated (yc), the clause-level feedback should be determined based on the comparison result between the product of a Lw rand-bit random number w rand with the hyperparameter T and the clause update probability (see Algorithm 3). The clause-level feedback will be stored in Clause Feedback Buffer as either registers or BRAM. In this architecture, for better simplicity of design, the size of the Weight Update Matrix (see Fig. 4-3) matches the Weight Matrix. Thus, in each clock cycle, clause-level feedbacks for m clauses will be computed, and m weights for the corresponding clauses will be updated. The weights update only in CoTM mode. Algorithm 4 shows the clause- level feedback and weight updates within Weight Update Matrix in 4- 3. The clause mask from Section IV is reused to prevent remainder clauses from updating. c) TA-Level Feedback: Similarly, to match the incom- ing data rate when computing the clauses, the TA Update Matrix is designed to be the same size as the Clause Matrix.\n\n--- Segment 21 ---\nThe clause mask from Section IV is reused to prevent remainder clauses from updating. c) TA-Level Feedback: Similarly, to match the incom- ing data rate when computing the clauses, the TA Update Matrix is designed to be the same size as the Clause Matrix. Hence, in each clock cycle, y clause-level feedback PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 9 Algorithm 4: Clause Level Feedback Generation Input: TM type (TM type, 2 b01 represents Vanilla TM and 2 b10 represents CoTM), index (index of updated class), PCl Update (Update probability), T (hyper parameter), cl (Clause), cl mask (clause mask for weight computation matrix), m (clause number in weight computation matrix), W (Signed weight of this clause), w rand (random number for clause-level feedback compute), yc (the updated class, 1 represents target class, 0 represents negated class) 1 Clause_Update(cl, W, cl mask, w rand, yc) foreach i m do 2 if cl mask[i] 1 then 3 if PCl Update T w rand then 4 if yc 1 then 5 if W[index][i] 0 then 6 Feedback[i] 2 b01; 7 else if W[index][i] 0 then 8 Feedback[i] 2 b10; 9 if cl[i] 1 then 10 if TM type 2 b10 then 11 W[index][i] W[index][i] 1; 12 if yc 0 then 13 if W[index][i] 0 then 14 Feedback 2 b01; 15 else if W[index][i] 0 then 16 Feedback 2 b10; 17 if cl[i] 1 then 18 if TM type 2 b10 then 19 W[index][i] W[index][i] 1; 20 return Feedback, W; will be read from the buffer registers. Then y corresponding clauses, x features, and x y LT A rand-bit random numbers will be fed into the TA Update Matrix where TA rand is the random number for the TA-level feedback computation, and LT A rand is the length of these random numbers. In the accelerator, s is defined as Ls bits.\n\n--- Segment 22 ---\nThen y corresponding clauses, x features, and x y LT A rand-bit random numbers will be fed into the TA Update Matrix where TA rand is the random number for the TA-level feedback computation, and LT A rand is the length of these random numbers. In the accelerator, s is defined as Ls bits. To reduce resource utiliza- tion, the TA update probability 1 s is precomputed, because the random numbers generated in the hardware are LT A rand-bit binary numbers so that the TA update probability PT A Update becomes 2LT A rand s (see Algorithm 5). Fig. 4-4 shows the architecture of the TA Update Matrix, it contains x y TA update blocks, and for the TM update, the TA update block takes a clock cycles to update the TAs in a group of x clauses, and TAs in one class require a b clock cycles. The inverted literal mask (Section IV) and clause buffer mask (Section IV) become control signals preventing updates of remainder TAs in a clause computation slice. Function TAFD in Algorithm 5 is used to update the TAs in one TA slice.\n\n--- Segment 23 ---\nThe inverted literal mask (Section IV) and clause buffer mask (Section IV) become control signals preventing updates of remainder TAs in a clause computation slice. Function TAFD in Algorithm 5 is used to update the TAs in one TA slice. d) Clause-Level Feedback Optimization: In the proposed Algorithm 5: Group Clause TA Update Input: PT A Update (TA update probability), Feedback (loaded Clause level feedback), L (loaded literal), l mask (literal mask), TA (loaded unsigned TA state), action (TA action of the loaded TAs), LT A rand (random number length), TA rand (random number), cl (loaded clause), cl buffer mask (the clause buffer mask), x (literal width of clause computation matrix), y (clause width of clause computation matrix) 1 TAFD(x, y, L, TA, cl, TA rand, PT A Update, Feedback, l mask, cl buffer mask, action) 2 foreach i y do 3 foreach j x do 4 if cl buffer mask[i] l mask[j] then 5 if Feedback[i] 2 b01 then 6 if cl[i] (cl[i] L[j]) then 7 if PT A Update TA rand then 8 if TA[i][j] 0 then 9 TA[i][j] TA[i][j] 1; 10 else if cl[i] L[j] then 11 if PT A Update TA rand then 12 if TA[i][j] 2LT A 1 then 13 TA[i][j] TA[i][j] 1; 14 else if Feedback[i] 2 b10 then 15 if cl[i] L[j] action[i][j] 1 then 16 if TA[i][j] 2LT A 1 then 17 TA[i][j] TA[i][j] 1; 18 return TA; Algorithm 6: Optimized TA Update Input: Feedback (loaded feedback for the current group of clauses) 1 TAFD_OPT(x, y, L, TA, action, cl, TA rand, Feedback, PT A Update, l mask, cl buffer mask) 2 while TA update not finished do 3 if i y such that Feedback[i] 2 b00 then 4 TAFD(x, y, TA, action, TA rand, PT A Update, L, Feedback, cl, l mask, cl buffer mask); 5 else 6 Allocate TA RAM address to the start of the next group of clauses; 7 return TA; architecture, the TAs in a group of clauses are updated concur- rently.\n\n--- Segment 24 ---\nFunction TAFD in Algorithm 5 is used to update the TAs in one TA slice. d) Clause-Level Feedback Optimization: In the proposed Algorithm 5: Group Clause TA Update Input: PT A Update (TA update probability), Feedback (loaded Clause level feedback), L (loaded literal), l mask (literal mask), TA (loaded unsigned TA state), action (TA action of the loaded TAs), LT A rand (random number length), TA rand (random number), cl (loaded clause), cl buffer mask (the clause buffer mask), x (literal width of clause computation matrix), y (clause width of clause computation matrix) 1 TAFD(x, y, L, TA, cl, TA rand, PT A Update, Feedback, l mask, cl buffer mask, action) 2 foreach i y do 3 foreach j x do 4 if cl buffer mask[i] l mask[j] then 5 if Feedback[i] 2 b01 then 6 if cl[i] (cl[i] L[j]) then 7 if PT A Update TA rand then 8 if TA[i][j] 0 then 9 TA[i][j] TA[i][j] 1; 10 else if cl[i] L[j] then 11 if PT A Update TA rand then 12 if TA[i][j] 2LT A 1 then 13 TA[i][j] TA[i][j] 1; 14 else if Feedback[i] 2 b10 then 15 if cl[i] L[j] action[i][j] 1 then 16 if TA[i][j] 2LT A 1 then 17 TA[i][j] TA[i][j] 1; 18 return TA; Algorithm 6: Optimized TA Update Input: Feedback (loaded feedback for the current group of clauses) 1 TAFD_OPT(x, y, L, TA, action, cl, TA rand, Feedback, PT A Update, l mask, cl buffer mask) 2 while TA update not finished do 3 if i y such that Feedback[i] 2 b00 then 4 TAFD(x, y, TA, action, TA rand, PT A Update, L, Feedback, cl, l mask, cl buffer mask); 5 else 6 Allocate TA RAM address to the start of the next group of clauses; 7 return TA; architecture, the TAs in a group of clauses are updated concur- rently. As the model converges while training, the frequency of feedback to clauses reduces [38].\n\n--- Segment 25 ---\nd) Clause-Level Feedback Optimization: In the proposed Algorithm 5: Group Clause TA Update Input: PT A Update (TA update probability), Feedback (loaded Clause level feedback), L (loaded literal), l mask (literal mask), TA (loaded unsigned TA state), action (TA action of the loaded TAs), LT A rand (random number length), TA rand (random number), cl (loaded clause), cl buffer mask (the clause buffer mask), x (literal width of clause computation matrix), y (clause width of clause computation matrix) 1 TAFD(x, y, L, TA, cl, TA rand, PT A Update, Feedback, l mask, cl buffer mask, action) 2 foreach i y do 3 foreach j x do 4 if cl buffer mask[i] l mask[j] then 5 if Feedback[i] 2 b01 then 6 if cl[i] (cl[i] L[j]) then 7 if PT A Update TA rand then 8 if TA[i][j] 0 then 9 TA[i][j] TA[i][j] 1; 10 else if cl[i] L[j] then 11 if PT A Update TA rand then 12 if TA[i][j] 2LT A 1 then 13 TA[i][j] TA[i][j] 1; 14 else if Feedback[i] 2 b10 then 15 if cl[i] L[j] action[i][j] 1 then 16 if TA[i][j] 2LT A 1 then 17 TA[i][j] TA[i][j] 1; 18 return TA; Algorithm 6: Optimized TA Update Input: Feedback (loaded feedback for the current group of clauses) 1 TAFD_OPT(x, y, L, TA, action, cl, TA rand, Feedback, PT A Update, l mask, cl buffer mask) 2 while TA update not finished do 3 if i y such that Feedback[i] 2 b00 then 4 TAFD(x, y, TA, action, TA rand, PT A Update, L, Feedback, cl, l mask, cl buffer mask); 5 else 6 Allocate TA RAM address to the start of the next group of clauses; 7 return TA; architecture, the TAs in a group of clauses are updated concur- rently. As the model converges while training, the frequency of feedback to clauses reduces [38]. DTM skips loading of all TA slices from BRAM and updation when there is no clause-level feedback to a particular group of clauses, saving PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 10 Fig.\n\n--- Segment 26 ---\nAs the model converges while training, the frequency of feedback to clauses reduces [38]. DTM skips loading of all TA slices from BRAM and updation when there is no clause-level feedback to a particular group of clauses, saving PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 10 Fig. 7: Training time per epoch. Fig. 8: The block diagram of the PRNG cluster. precious cycles. Explanation: when the clause matrix slides through the slices (see Fig. 5a), if no feedback is present for the corresponding y clauses, the computation slice moves directly to the next row without traversing the current row s bottom column. This is implemented by adjusting the read write addresses of the TA RAM, as shown in Algorithm 6. With this optimization, the CoTM gets 40 improvement in training time as shown in Fig. 7, while achieving similar test accuracy. The reduction in training time indicates fewer clause updates each epoch. The training time eventually saturates to a constant value, which approximately equals the inference time plus the weight update time for both target and negated class updating. C. Pseudo Random Number Generator (PRNG): To gener- ate a large number of random numbers for parameter updating in real time, the accelerator is integrated with a master-slave architecture PRNG cluster developed from [42]. In the PRNG cluster, the master PRNG is used to generate the seeds for the slave PRNGs. For each slave PRNG, if the random number generation finishes one PRNG cycle period, it will request a new seed from the master PRNG; this is called seed refresh. Fig. 8 shows the architecture of the PRNG cluster. The authors in [42] map the PRNG cluster to DSP blocks; however, the slave PRNG used in this design is LFSR based due to DSP block limitations in the chosen FPGAs. In the LFSR version of the master-slave PRNG cluster, the master PRNG will generate and set the seeds for the slave PRNG after 2LLF SR clock cycles, where LLF SR is the length of LFSR. One DSP slice can only afford 16-bits Mid-square based PRNG, hence in this accelerator, LFSR is used as the slave PRNG in the PRNG cluster.\n\n--- Segment 27 ---\nIn the LFSR version of the master-slave PRNG cluster, the master PRNG will generate and set the seeds for the slave PRNG after 2LLF SR clock cycles, where LLF SR is the length of LFSR. One DSP slice can only afford 16-bits Mid-square based PRNG, hence in this accelerator, LFSR is used as the slave PRNG in the PRNG cluster. The processor supplies and programs the master PRNG seed in real time. D. DTM Control and Data Flows: a) Programming: Before inference or training, the configuration data including the TM type, feature number, clause number and class number are sent to the accelerator, allowing it to calculate clause and weight compute rounds. Upon receiving these data, the necessary clauses and weight masks are derived, as outlined Fig. 9: Accelerator Timing: a) timing diagram for CoTM inference; b) timing diagram for Vanilla TM inference; c) timing diagram for DTM training (same procedure for both Vanilla and CoTM). in IV.For training, hyperparameters like the TA state update probability 1 s and threshold t are provided. The accelerator calculates 1 s, stores t, and initializes the TA states and weights in RAM using PRNGs once the model size is configured. The programming data (model size, PRNG seeds, and hyperpa- rameters) are sent via an AXI-stream channel. The execution mode (inference or training) and the target class index are embedded within the feature data stream. Fig. 10a shows the AXIS integration and how the instruction and feature data flow through the system for the inference and training processes. b) Class Sum Computation: During training, only the target class sum is computed; in inference, class sums for all classes are calculated. The feature data are stored when received; the system uses this to identify the mode (train- ing inference), and loads the target class if in training mode. The TA RAM controller allocates the start clause slice, and the Clause Matrix loads TA actions and literals to compute clauses in block 1. Inference is pipelined: When one class of clause computation completes and the weight computation matrix is idle, the weight RAM controller allocates the weight slice, loading weights, and clauses to compute class sum in block 2. In inference-only mode, once classes are processed, a new data point is handled.\n\n--- Segment 28 ---\nInference is pipelined: When one class of clause computation completes and the weight computation matrix is idle, the weight RAM controller allocates the weight slice, loading weights, and clauses to compute class sum in block 2. In inference-only mode, once classes are processed, a new data point is handled. The timing diagrams for the Vanilla TM and CoTM inference are shown in Figs. 9-a and 9-b. c) Parameter Update: In training, updates start with class- level feedback from the Clause Update Control (block 5 in Fig. 4), which generates clause-level update probabilities. The Training Accelerator updates weights and generates clause feedback (block 3). For CoTM, the weight RAM controller loads the class slice, updates weights, and writes them back to RAM. TA states update in block 4, where the TA RAM controller allocates the clause slice, feeding TAs and literals to the TA update matrix. Updated TA states are written to RAM. TM updates proceed in two rounds: first, the target class, then a random class. Fig. 9-c shows this update flow. After updating, inference resumes by updating class sums, weights, and TA states, while the controller, stream channel, and buffer reset for the next feature stream. The state graph of the system during the training and inference process is shown in Fig. 10b. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 11 (a) The data flow in IP core. (b) The process flow of the proposed architecture. Fig. 10: DTM accelerator data flows. V. EVALUATION The DTM accelerator is designed to train on edge applica- tions, particularly cases where recalibration is required. This is typical with IoT sensor data inputs for edge tasks. Three Evaluation Points: This section presents three different evalu- ation points: firstly, comparing the accelerator against the state of the art in terms of the ability to accelerate the operations per second within a given power budget and associated energy and resource overheads; secondly, the scalability of the design to both larger and smaller FPGA platforms when targeting the kind of IoT edge datasets this design should handle; and finally, discussing the design trade-offs when adjusting weight precision and the LFSR length in the PRNG design.\n\n--- Segment 29 ---\nThis is typical with IoT sensor data inputs for edge tasks. Three Evaluation Points: This section presents three different evalu- ation points: firstly, comparing the accelerator against the state of the art in terms of the ability to accelerate the operations per second within a given power budget and associated energy and resource overheads; secondly, the scalability of the design to both larger and smaller FPGA platforms when targeting the kind of IoT edge datasets this design should handle; and finally, discussing the design trade-offs when adjusting weight precision and the LFSR length in the PRNG design. The design mapped on PYNQ-Z1 with Xilinx XC7Z020 SoC is referred to as DTM-S while the design mapped on ZCU- 104 with Xilinx ZU-7EV SoC is referred to as DTM-L. Two designs are implemented through AMD s (Xilinx) Vivado Design Suite. This section reports accuracy from 250 training epochs, latency from average training time per data point, and the power from Vivado s implementation reports. The resource utilization of each block and latency in each stage for the DTM-L configuration is presented in Fig. 12 and Fig. 13. Dataset Rationale: MNIST [14], FMNIST [43], KM- NIST [44], Google Speech Commands Keyword Spotting with 6 Keywords (yes, no, up, down, left, right - referred to as KWS-6) [45] (Booleanized as per [46]) are used as evaluation datasets. Often, deployed models need re-calibration to data affected by sensor aging, temperature, humidity, and other environmental changes [47]. Therefore, the ability to re-train and adapt, in situ, is more energy-efficient than continuous offsite cloud transfers. Additionally, personalized edge training keeps user data private and secure. The MNIST datasets offer benchmarking insights against other designs (Table. I, including both system and IP power of DTM), while the KWS6 dataset represents an edge IoT sensor task requiring in-field recalibration and personalized training - this is the type of application this accelerator is designed for (Table. II).\n\n--- Segment 30 ---\nI, including both system and IP power of DTM), while the KWS6 dataset represents an edge IoT sensor task requiring in-field recalibration and personalized training - this is the type of application this accelerator is designed for (Table. II). Point 1: Comparison with State-of-the-Art: The DTM designs with different matrix sizes and platforms are compared in terms of Giga-Operations per second per Watt (GOP s per W) against the related works described in Section III through Fig. 11: Conv-TM accelerator [40], FP-BNN [20], Reconfig DNN [27], Low-batch CNN [29], F-CNN [30], YOLO v3 [23], Fig. 11: Closest comparable State of The Art designs that implement ML training on FPGAs and report GOP s. DTMs shown in red. BFP-NN [31], FireFly [34], A2NN [24], WSQ-AdderNet [25], and Flex Block [32]. For DTM and Conv TM design, system- level power including ARM core is reported, named as DTM- SYS and Conv TM-SYS respectively while IP-only power is shown as DTM-IP and Conv TM-IP. The parameterized matrix sizes in the DTM design allow for scalability as seen through DTM-S and DTM-L (Table I). The DTM design philosophy is to utilize larger matrix sizes on larger FPGAs. Therefore, the clause compute matrix size and weight compute matrix size for these two designs are: DTM S - 32 literals 16 clauses, 2 clauses 4 classes and DTM L - 32 literals 27 clauses, 8 clauses 4 classes. The LFSR length for these two designs is 12-bits and 24- bits, respectively. The training speed, however, depends on TM model sizes and frequency. The accelerators run at 50 MHz and 100 MHz, respectively, due to the FPGAs being based on different technology nodes. Using GOP s in Fig. 11 allows for gauging the memory and compute bounds of different implementations. DTM designs are second only to FP-BNN (inference only) and Flex Block in GOP s. YOLO v3 [23], and BFP-NN [31] have fewer GOP s compared to the DTMs but also target more complex datasets.\n\n--- Segment 31 ---\n11 allows for gauging the memory and compute bounds of different implementations. DTM designs are second only to FP-BNN (inference only) and Flex Block in GOP s. YOLO v3 [23], and BFP-NN [31] have fewer GOP s compared to the DTMs but also target more complex datasets. The flexibility of DTMs contrasts sharply with the fixed, customized designs seen in Conv-TM accelerator [40], SATA [35], FireFly [34], CNN [22], SNN [33] implementa- tions shown in Table I. Although optimized for specific tasks, they lack the adaptability required for diverse applications or PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 12 Fig. 12: Resource utilization of each module in DTM-L. Fig. 13: Latency during training and inference for DTM-L. on-field model recalibration. The simplicity of the TM training process makes DTM less reliant on DSPs than the DNN accelerators. The DTM algorithms are mapped into LUTs and BRAMs except for the clause update probability comparison in clause feedback. This results in the trade-off between larger resource usage but lower power. In terms of training latency, DTM implementation demon- strates significantly lower latency compared to non-TM solu- tions SATA [35] and CNN [22]. The design in Conv TM [40] offers better training latency, but only for 128 clauses. This architecture requires all clauses in each patch to be computed in one clock cycle. Scaling up further would have a significant impact on resources because it stores all TA actions in the register buffer, limiting its scalability to larger models with more features or clauses required. This paper also provides preliminary inference insights for CIFAR-10 using Composite TM [16]. While they show the feasibility of TMs to solve more complex problems, with the addition of Convolution TM modules into the DTM framework, a DTM-based architecture would be able to offer better resource usage and flexibility. Integration of these Conv TM modules is left to further work. Point 2: Flexibility and Target Usage: Table II highlights the flexibility of the DTM training architecture using the KWS6 dataset. The same DTM design can switch between the CoTM and the Vanilla TM with different clauses.\n\n--- Segment 32 ---\nPoint 2: Flexibility and Target Usage: Table II highlights the flexibility of the DTM training architecture using the KWS6 dataset. The same DTM design can switch between the CoTM and the Vanilla TM with different clauses. The accuracy is comparable to Xilinx s FINN approach [5] and MATADOR [39] but DTM sacrifices throughput in favor of flexibility to change TM algorithm type, number of clauses and train online. As shown in the table, higher accuracies are possi- ble through higher clauses but at the cost of lower throughput. For KWS6, needing faster throughput means choosing CoTM while better performance means switching to Vanilla TM.5 Point 3: Design Trade-offs: The training process in the TM is affected by the quality of random numbers [42] and the precision of the CoTM weights. Figs. 14 and 15 demonstrate the impact of learning efficacy (i.e, test accuracy) and the 5This analysis of throughput implications vs number of clauses is continued in Supplementary Material for more datasets. These datasets are also reflective of the type of target usage for this DTM implementation. Fig. 14: Resource utilization and power for different weight length. Fig. 15: Resource utilization and power for different LFSR length. overhead in power and resources when varying the weight bit precision and LFSR length in the PRNG design. The results indicate that at least 12-bit weight precision is required to achieve good learning efficacy on the MNIST dataset (Fig. 14a); however, the test accuracy, utilization, and power consumption saturate as weight precision increases, suggesting 12-bits are sufficient. Fig. 15a and Fig. 15b focus on learning efficacy, power, and resources when varying the LFSR length in the PRNG block. Higher precision and better quality random numbers yield better test accuracy, but the improvement becomes less substantial. Notice that with seed refreshing in the slave PRNG blocks, the accuracy at 8-bits is much higher than without (Fig. 15a), seed refreshing leads to better accuracies across all LFSR precisions. The LFSR precision plays a larger role in the overall resource and power than the weight precision. VI. CONCLUSIONS AND FUTURE WORK This paper presented an FPGA-based Dynamic TM (DTM) accelerator architecture that supports both inference and train- ing acceleration for Vanilla TM and CoTMs.\n\n--- Segment 33 ---\nVI. CONCLUSIONS AND FUTURE WORK This paper presented an FPGA-based Dynamic TM (DTM) accelerator architecture that supports both inference and train- ing acceleration for Vanilla TM and CoTMs. The architecture offers a competitive GOP s W ratio against comparable works but can also be reconfigured at run-time to adapt to different models and edge sensor tasks. This flexibility makes it suitable for on-field recalibration and personalized training. The TM clause computation maps very efficiently to FPGAs and the DTM design is parameterized to target both smaller low- power eFPGAs and larger FPGA fabrics. DTM is envisioned as a modular framework that can create a reconfigurable accelerator out of all TM algorithms. This paper explored the Vanilla and Coalesced variants. However, future work will extend to templates for Convolution and Regression along with Composite TM [17] support for larger datasets (CIFAR-10 and beyond). Users can use the FPGA to create required subsets from the global TM library for their application. REFERENCES [1] Y. Tian, M. A. Chao, C. Kulkarni, K. Goebel, and O. Fink, Real- time model calibration with deep reinforcement learning, Mechanical Systems and Signal Processing, 2022. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 13 TABLE I: Comparison with other FPGA accelerators suitable for similar edge sensor-based applications. (The - indicates this result is not reported. The power in the brackets is the IP-only power for Conv TM and DTM designs. The training latency of DTM is calculated using the average training latency over 250 epochs. The training and inference latency rows show the values for MNIST, FMNIST and KMNIST respectively for the Conv-TM and DTM designs.)\n\n--- Segment 34 ---\nThe training latency of DTM is calculated using the average training latency over 250 epochs. The training and inference latency rows show the values for MNIST, FMNIST and KMNIST respectively for the Conv-TM and DTM designs.) Design SATA [35] FireFly [34] CNN [22] SNN [33] Conv TM [40] DTM-L (Large) DTM-S (Small) Platform 65nm ASIC XCZU-3EG XCZU-9EG Kintex-7 XCZU-7EV XCZU-7EV XC7Z20 Algorithm ANN,SNN SCNN CNN SNN Conv TM Coalesced TM, Vanilla TM Coalesced TM, Vanilla TM Accelerator Type Training Inference Training Inference Training Training Training Flexibility Reconfigurable Reconfigurable Customized Customized Customized Reconfigurable Reconfigurable LUT ASIC Simulation 15000 32589 46371 196252 104222 43497 FF - 33585 30417 73303 59610 33256 DSP 288 143 65 129 25 6 BRAM 162 95 150 0 37 138 URAM 0 0 0 42.5 96 0 Freq(MHz) 400 300 100 200 50 100 50 Test Accuracy 99.00 (MNIST) 98.12 (MNIST) 98.64 (MNIST) 97.81 (MNIST), 83.16 (FMNIST) 97.60 (MNIST), 84.10 (FMNIST), 82.80 (KMNIST) 97.74 (MNIST), 86.38 (FMNIST), 83.11 (KMNIST) 95.11 (MNIST), 80.62 (FMNIST), 72.52 (KMNIST) Training Latency(us) 204.67 Inference only 3581.81 Inference only 27, 27, 27 88.48, 97.69, 98.92 50.08, 58.67, 60.57 Inference Latency(us) - 491.16 - 4290, 4290 9.4, 9.4, 9.4 44.72, 44.72, 44.72 22.32, 22.32, 22.32 Power(W) - 2.550 - 0.535 4.543 (2.000) 4.359 (1.646) 1.687 (0.424) TABLE II: Comparison against MATADOR [39], FINN [4] and DNNBuilder [26] using KWS-6 dataset.\n\n--- Segment 35 ---\nThe training and inference latency rows show the values for MNIST, FMNIST and KMNIST respectively for the Conv-TM and DTM designs.) Design SATA [35] FireFly [34] CNN [22] SNN [33] Conv TM [40] DTM-L (Large) DTM-S (Small) Platform 65nm ASIC XCZU-3EG XCZU-9EG Kintex-7 XCZU-7EV XCZU-7EV XC7Z20 Algorithm ANN,SNN SCNN CNN SNN Conv TM Coalesced TM, Vanilla TM Coalesced TM, Vanilla TM Accelerator Type Training Inference Training Inference Training Training Training Flexibility Reconfigurable Reconfigurable Customized Customized Customized Reconfigurable Reconfigurable LUT ASIC Simulation 15000 32589 46371 196252 104222 43497 FF - 33585 30417 73303 59610 33256 DSP 288 143 65 129 25 6 BRAM 162 95 150 0 37 138 URAM 0 0 0 42.5 96 0 Freq(MHz) 400 300 100 200 50 100 50 Test Accuracy 99.00 (MNIST) 98.12 (MNIST) 98.64 (MNIST) 97.81 (MNIST), 83.16 (FMNIST) 97.60 (MNIST), 84.10 (FMNIST), 82.80 (KMNIST) 97.74 (MNIST), 86.38 (FMNIST), 83.11 (KMNIST) 95.11 (MNIST), 80.62 (FMNIST), 72.52 (KMNIST) Training Latency(us) 204.67 Inference only 3581.81 Inference only 27, 27, 27 88.48, 97.69, 98.92 50.08, 58.67, 60.57 Inference Latency(us) - 491.16 - 4290, 4290 9.4, 9.4, 9.4 44.72, 44.72, 44.72 22.32, 22.32, 22.32 Power(W) - 2.550 - 0.535 4.543 (2.000) 4.359 (1.646) 1.687 (0.424) TABLE II: Comparison against MATADOR [39], FINN [4] and DNNBuilder [26] using KWS-6 dataset. Design Model LUT FF DSP BRAM URAM Test Acc Training Datapoint s Inference Datapoint s DTM-CoTM 2000 Clauses per class 104222 59610 25 37 96 86.07 18281 42878 1000 Clauses per class 83.76 34,128 83,084 500 Clauses per class 81.15 63,696 163,241 DTM-Vanilla TM 700 Clauses per class 87.12 45591 25873 500 Clauses per class 85.87 58,603 35,326 300 Clauses per class 83.17 86,663 55,670 MATADOR 700 Clauses per class 6063 10658 0 3 0 87.10 Inference only 8333333 FINN BNN (377-512-256-6) 42757 45473 0 26.5 0 84.60 Inference only 750188 DNN Builder DNN (273-64-128-64-6) 3035 3790 4 36 0 81.00 Inference only 5703 [2] A. Tashakori, W. Zhang, Z. Jane Wang, and P. Servati, SemiPFL: Personalized Semi-Supervised Federated Learning Framework for Edge Intelligence, IEEE Internet of Things Journal, 2023.\n\n--- Segment 36 ---\nDesign SATA [35] FireFly [34] CNN [22] SNN [33] Conv TM [40] DTM-L (Large) DTM-S (Small) Platform 65nm ASIC XCZU-3EG XCZU-9EG Kintex-7 XCZU-7EV XCZU-7EV XC7Z20 Algorithm ANN,SNN SCNN CNN SNN Conv TM Coalesced TM, Vanilla TM Coalesced TM, Vanilla TM Accelerator Type Training Inference Training Inference Training Training Training Flexibility Reconfigurable Reconfigurable Customized Customized Customized Reconfigurable Reconfigurable LUT ASIC Simulation 15000 32589 46371 196252 104222 43497 FF - 33585 30417 73303 59610 33256 DSP 288 143 65 129 25 6 BRAM 162 95 150 0 37 138 URAM 0 0 0 42.5 96 0 Freq(MHz) 400 300 100 200 50 100 50 Test Accuracy 99.00 (MNIST) 98.12 (MNIST) 98.64 (MNIST) 97.81 (MNIST), 83.16 (FMNIST) 97.60 (MNIST), 84.10 (FMNIST), 82.80 (KMNIST) 97.74 (MNIST), 86.38 (FMNIST), 83.11 (KMNIST) 95.11 (MNIST), 80.62 (FMNIST), 72.52 (KMNIST) Training Latency(us) 204.67 Inference only 3581.81 Inference only 27, 27, 27 88.48, 97.69, 98.92 50.08, 58.67, 60.57 Inference Latency(us) - 491.16 - 4290, 4290 9.4, 9.4, 9.4 44.72, 44.72, 44.72 22.32, 22.32, 22.32 Power(W) - 2.550 - 0.535 4.543 (2.000) 4.359 (1.646) 1.687 (0.424) TABLE II: Comparison against MATADOR [39], FINN [4] and DNNBuilder [26] using KWS-6 dataset. Design Model LUT FF DSP BRAM URAM Test Acc Training Datapoint s Inference Datapoint s DTM-CoTM 2000 Clauses per class 104222 59610 25 37 96 86.07 18281 42878 1000 Clauses per class 83.76 34,128 83,084 500 Clauses per class 81.15 63,696 163,241 DTM-Vanilla TM 700 Clauses per class 87.12 45591 25873 500 Clauses per class 85.87 58,603 35,326 300 Clauses per class 83.17 86,663 55,670 MATADOR 700 Clauses per class 6063 10658 0 3 0 87.10 Inference only 8333333 FINN BNN (377-512-256-6) 42757 45473 0 26.5 0 84.60 Inference only 750188 DNN Builder DNN (273-64-128-64-6) 3035 3790 4 36 0 81.00 Inference only 5703 [2] A. Tashakori, W. Zhang, Z. Jane Wang, and P. Servati, SemiPFL: Personalized Semi-Supervised Federated Learning Framework for Edge Intelligence, IEEE Internet of Things Journal, 2023. [3] X. Wang, Y. Han, V. C. M. Leung, D. Niyato, X. Yan, and X. Chen, Convergence of edge computing and deep learning: A comprehensive survey, IEEE Communications Surveys Tutorials, 2020.\n\n--- Segment 37 ---\nDesign Model LUT FF DSP BRAM URAM Test Acc Training Datapoint s Inference Datapoint s DTM-CoTM 2000 Clauses per class 104222 59610 25 37 96 86.07 18281 42878 1000 Clauses per class 83.76 34,128 83,084 500 Clauses per class 81.15 63,696 163,241 DTM-Vanilla TM 700 Clauses per class 87.12 45591 25873 500 Clauses per class 85.87 58,603 35,326 300 Clauses per class 83.17 86,663 55,670 MATADOR 700 Clauses per class 6063 10658 0 3 0 87.10 Inference only 8333333 FINN BNN (377-512-256-6) 42757 45473 0 26.5 0 84.60 Inference only 750188 DNN Builder DNN (273-64-128-64-6) 3035 3790 4 36 0 81.00 Inference only 5703 [2] A. Tashakori, W. Zhang, Z. Jane Wang, and P. Servati, SemiPFL: Personalized Semi-Supervised Federated Learning Framework for Edge Intelligence, IEEE Internet of Things Journal, 2023. [3] X. Wang, Y. Han, V. C. M. Leung, D. Niyato, X. Yan, and X. Chen, Convergence of edge computing and deep learning: A comprehensive survey, IEEE Communications Surveys Tutorials, 2020. [4] M. Blott, T. B. Preußer, N. J. Fraser, G. Gambardella, K. O brien, Y. Umuroglu, M. Leeser, and K. Vissers, FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks, ACM Trans. Reconfigurable Technol. Syst., 2018. [5] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong, M. Jahre, and K. Vissers, FINN: A Framework for Fast, Scalable Binarized Neural Network Inference, in ACM SIGDA, 2017.\n\n--- Segment 38 ---\nSyst., 2018. [5] Y. Umuroglu, N. J. Fraser, G. Gambardella, M. Blott, P. Leong, M. Jahre, and K. Vissers, FINN: A Framework for Fast, Scalable Binarized Neural Network Inference, in ACM SIGDA, 2017. [6] X. Zhang, A. Ramachandran, C. Zhuge, D. He, W. Zuo, Z. Cheng, K. Rupnow, and D. Chen, Machine learning on FPGAs to face the IoT revolution, in IEEE ACM ICCAD, 2017. [7] R. Chen, H. Zhang, Y. Li, R. Zhang, G. Li, J. Yu, and K. Wang, Edge FPGA-based Onsite Neural Network Training, in IEEE ISCAS, 2023. [8] O.-C. Granmo, The Tsetlin Machine A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic, 2021. [Online]. Available: [9] S. Glimsdal and O.-C. Granmo, Coalesced Multi-Output Tsetlin Machines with Clause Sharing, 2021. [Online]. Available: https: arxiv.org abs 2108.07594 [10] O.-C. Granmo, S. Glimsdal, L. Jiao, M. Goodwin, C. W. Omlin, and G. T. Berge, The Convolutional Tsetlin Machine, 2019. [Online]. Available: [11] K. D. Abeyrathna, O.-C. Granmo, L. Jiao, and M. Goodwin, The Regression Tsetlin Machine: A Tsetlin Machine for Continuous Output Problems, 2019. [Online]. Available: [12] M. M. H. Shuvo, S. K. Islam, J. Cheng, and B. I. Morshed, Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review, Proceedings of the IEEE, 2023.\n\n--- Segment 39 ---\n[Online]. Available: [12] M. M. H. Shuvo, S. K. Islam, J. Cheng, and B. I. Morshed, Efficient Acceleration of Deep Learning Inference on Resource-Constrained Edge Devices: A Review, Proceedings of the IEEE, 2023. [13] O. Tarasyuk, A. Gorbenko, T. Rahman, R. Shafik, and A. Yakovlev, Logic-Based Machine Learning with Reproducible Decision Model Using the Tsetlin Machine, in IEEE IDAACS, 2023. [14] L. Deng, The mnist database of handwritten digit images for machine learning research, IEEE Signal Processing Magazine, 2012. [15] T. Rahman, A. Wheeldon, R. Shafik, A. Yakovlev, J. Lei, O.-C. Granmo, and S. Das, Data Booleanization for Energy Efficient On-Chip Learning using Logic Driven AI, in IEEE ISTM, 2022. [16] Y. Grønningsæter, H. S. Smørvik, and O.-C. Granmo, An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine Composites, 2024. [Online]. Available: [17] O.-C. Granmo, TMComposites: Plug-and-Play Collaboration Between Specialized Tsetlin Machines, 2023. [Online]. Available: https: arxiv.org abs 2309.04801 [18] X. Zhang, L. Jiao, O.-C. Granmo, and M. Goodwin, On the conver- gence of tsetlin machines for the identity- and not operators, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [19] O. Tarasyuk, T. Rahman, R. Shafik, A. Yakovlev, A. Gorbenko, O.-C. Granmo, and L. Jiao, Systematic Search for Optimal Hyper-parameters of the Tsetlin Machine on MNIST Dataset, in 2023 IEEE ISTM, 2023. [20] S. Liang, S. Yin, L. Liu, W. Luk, and S. Wei, FP-BNN, Neurocomput, 2018. [Online].\n\n--- Segment 40 ---\n[20] S. Liang, S. Yin, L. Liu, W. Luk, and S. Wei, FP-BNN, Neurocomput, 2018. [Online]. Available: [21] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations, 2020. [Online]. Available: [22] M. Cho and Y. Kim, Implementation of Data-optimized FPGA-based Accelerator for Convolutional Neural Network, in IEEE ICEIC, 2020. [23] M. Kim, K. Oh, Y. Cho, H. Seo, X. T. Nguyen, and H.-J. Lee, A Low- Latency FPGA Accelerator for YOLOv3-Tiny With Flexible Layerwise Mapping and Dataflow, IEEE Transactions on Circuits and Systems I: Regular Papers, 2024. [24] N. Zhang, S. Ni, L. Chen, T. Wang, and H. Chen, High-Throughput and Energy-Efficient FPGA-Based Accelerator for All Adder Neural Networks, IEEE Internet of Things Journal, 2025. [25] Y. Zhang, B. Sun, W. Jiang, Y. Ha, M. Hu, and W. Zhao, WSQ- AdderNet: Efficient Weight Standardization based Quantized AdderNet FPGA Accelerator Design with High-Density INT8 DSP-LUT Co- Packing Optimization, in 2022 IEEE ACM ICCAD, 2022. [26] X. Zhang, J. Wang, C. Zhu, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen, DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs, in IEEE ACM ICCAD, 2018. [27] J. Lu, J. Lin, and Z. Wang, A Reconfigurable DNN Training Accelerator on FPGA, in IEEE SiPS, 2020.\n\n--- Segment 41 ---\n[26] X. Zhang, J. Wang, C. Zhu, Y. Lin, J. Xiong, W.-m. Hwu, and D. Chen, DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs, in IEEE ACM ICCAD, 2018. [27] J. Lu, J. Lin, and Z. Wang, A Reconfigurable DNN Training Accelerator on FPGA, in IEEE SiPS, 2020. [28] S. Fox, J. Faraone, D. Boland, K. Vissers, and P. H. Leong, Training Deep Neural Networks in Low-Precision with High Accuracy Using FPGAs, in IEEE ICFPT, 2019. [29] S. K. Venkataramanaiah, H.-S. Suh, S. Yin, E. Nurvitadhi, A. Dasu, Y. Cao, and J.-S. Seo, FPGA-based Low-Batch Training Accelerator PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 14 for Modern CNNs Featuring High Bandwidth Memory, in IEEE ACM ICCAD, 2020. [30] W. Zhao, H. Fu, W. Luk, T. Yu, S. Wang, B. Feng, Y. Ma, and G. Yang, F-CNN: An FPGA-based framework for training Convolutional Neural Networks, in IEEE ASAP, 2016. [31] T.-H. Tsai and D.-B. Lin, An On-Chip Fully Connected Neural Network Training Hardware Accelerator Based on Brain Float Point and Sparsity Awareness, IEEE Open Journal of Circuits and Systems, 2023. [32] S.-H. Noh, J. Koo, S. Lee, J. Park, and J. Kung, FlexBlock: A Flexible DNN Training Accelerator With Multi-Mode Block Floating Point Support, IEEE Transactions on Computers, 2023.\n\n--- Segment 42 ---\n[32] S.-H. Noh, J. Koo, S. Lee, J. Park, and J. Kung, FlexBlock: A Flexible DNN Training Accelerator With Multi-Mode Block Floating Point Support, IEEE Transactions on Computers, 2023. [33] Y. Liu, Y. Chen, W. Ye, and Y. Gui, FPGA-NHAP: A General FPGA- Based Neuromorphic Hardware Acceleration Platform With High Speed and Low Power, IEEE Transactions on Circuits and Systems I: Regular Papers, 2022. [34] J. Li, G. Shen, D. Zhao, Q. Zhang, and Y. Zeng, FireFly: A High- Throughput Hardware Accelerator for Spiking Neural Networks With Efficient DSP and Memory Optimization, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2023. [35] R. Yin, A. Moitra, A. Bhattacharjee, Y. Kim, and P. Panda, SATA: Sparsity-Aware Training Accelerator for Spiking Neural Networks, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2023. [36] A. Wheeldon, R. Shafik, T. Rahman, J. Lei, A. Yakovlev, and O.-C. Granmo, Learning automata based energy-efficient AI hardware design for IoT applications, Philos. Trans. R. Soc. A Math. Phys. Eng. Sci., 2020. [37] R. A. Fisher, Iris, UCI Machine Learning Repository, 1988, DOI: [38] S. Maheshwari, T. Rahman, R. Shafik, A. Yakovlev, A. Rafiev, L. Jiao, and O.-C. Granmo, REDRESS: Generating Compressed Models for Edge Inference Using Tsetlin Machines, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [39] T. Rahman, G. Mao, S. Maheshwari, R. Shafik, and A. Yakovlev, MATADOR: Automated System-on-Chip Tsetlin Machine Design Gen- eration for Edge Applications, in IEEE DATE, 2024.\n\n--- Segment 43 ---\n[37] R. A. Fisher, Iris, UCI Machine Learning Repository, 1988, DOI: [38] S. Maheshwari, T. Rahman, R. Shafik, A. Yakovlev, A. Rafiev, L. Jiao, and O.-C. Granmo, REDRESS: Generating Compressed Models for Edge Inference Using Tsetlin Machines, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [39] T. Rahman, G. Mao, S. Maheshwari, R. Shafik, and A. Yakovlev, MATADOR: Automated System-on-Chip Tsetlin Machine Design Gen- eration for Edge Applications, in IEEE DATE, 2024. [40] S. A. Tunheim, L. Jiao, R. Shafik, A. Yakovlev, and O.-C. Granmo, Tsetlin Machine-Based Image Classification FPGA Accelerator With On-Device Training, IEEE Transactions on Circuits and Systems I: Regular Papers, 2025. [41] O. Ghazal, S. Singh, T. Rahman, S. Yu, Y. Zheng, D. Balsamo, S. Patkar, F. Merchant, F. Xia, A. Yakovlev, and R. Shafik, IMBUE: In-Memory Boolean-to-CUrrent Inference ArchitecturE for Tsetlin Machines, in IEEE ACM ISLPED, 2023. [42] T. Rahman, G. Mao, S. Maheshwari, K. Krishnamurthy, R. Shafik, and A. Yakovlev, Parallel Symbiotic Random Number Generator for Training Tsetlin Machines on FPGA, in IEEE ISTM, 2023. [43] H. Xiao, K. Rasul, and R. Vollgraf, Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms, 2017. [Online]. Available: [44] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha, Deep learning for classical japanese literature, arXiv preprint arXiv:1812.01718, 2018.\n\n--- Segment 44 ---\n[Online]. Available: [44] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha, Deep learning for classical japanese literature, arXiv preprint arXiv:1812.01718, 2018. [45] P. Warden, Speech commands: A dataset for limited-vocabulary speech recognition, arXiv preprint arXiv:1804.03209, 2018. [46] J. Lei, T. Rahman, R. Shafik, A. Wheeldon, A. Yakovlev, O.-C. Granmo, F. Kawsar, and A. Mathur, Low-Power Audio Keyword Spotting Using Tsetlin Machines, Journal of Low Power Electronics and Applications, 2021. [Online]. Available: [47] J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, and G. Zhang, Learning under Concept Drift: A Review, IEEE Transactions on Knowledge and Data Engineering, 2019. Gang Mao received B.E degree from Northeastern University, China in 2016, and M.E degree from Newcastle University, Newcastle upon Tyne, UK. He is a PhD student at Newcastle University, Newcastle upon Tyne, UK. His research interests are focused on asynchronous circuit design and developing Machine Learning accelerators. Tousif Rahman is a PhD student at Newcastle University, Newcastle upon Tyne, UK. He holds an MEng degree from the same institution. His research interests are focused on designing novel algorithms, architectures, and automation tools for translating Machine Learning applications to low- power, energy-efficient edge devices. Sidharth Maheshwari is an Assistant Professor in the department of Computer Science and En- gineering at Indian Institute of Technology (IIT) Jammu. He completed his B.Tech in Electronics and Electrical Engineering at IIT Guwahati in 2013. He worked at Newcastle University for his PhD and Postdoc between 2016-2022. His interests include using hardware software co-design to mitigate com- putational and energy bottlenecks of Big Data ap- plications. He is looking towards solving real-world challenges such as improving water quality through molecular diagnostics that includes genome analysis and antimicrobial resis- tance surveillance.\n\n--- Segment 45 ---\nHis interests include using hardware software co-design to mitigate com- putational and energy bottlenecks of Big Data ap- plications. He is looking towards solving real-world challenges such as improving water quality through molecular diagnostics that includes genome analysis and antimicrobial resis- tance surveillance. He is also working in the domain of battery-management systems and novel battery-pack design for Indian tropical climatic conditions in order to address challenges in the e-mobility sector. Another vertical in his research interest lies in exploring novel machine learning algorithms. Bob Pattison received a BEng (1st class Hons) degree from Newcastle University in 2024. Currently a 1st year PhD student in the Microsystems group at Newcastle University. His research focus is on explainable and energy-efficient machine learning algorithms for low-power devices. Zhuang Shao is a Lecturer in Data Engineering and AI with the School of Engineering, Newcastle University, Newcastle upon Tyne, UK. He holds a PhD from the University of Warwick. His research interests include hardware-software co-design, low energy consumption machine learning, and vision- language learning. PREPRINT - ACCEPTED IN IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I: REGULAR PAPERS 15 Rishad Shafik is a Professor of Microelectronic Systems within the School of Engineering, Newcas- tle University, UK. Dr Shafik received his PhD, and MSc (with distinction) degrees from Southampton in 2010, and 2005; and BSc (with distinction) from the IUT, Bangladesh in 2001. He is one of the editors of the Springer USA book Energy-efficient Fault-tolerant Systems . He is also author co-author of 200 IEEE ACM peer-reviewed articles, with 4 best paper nominations and 4 best paper poster awards. He recently chaired multiple international conferences symposiums, UKCAS2020, ISTM2022; guest edited two special theme issues in Royal Society Philosophical Transactions A; he recently co-chaired 2nd International Symposium on the TM (ISTM), 2023. His research interests include hardware software co-design for energy-efficiency and autonomy. Alex Yakovlev received the Ph.D. degree from the St. Petersburg Electrical Engineering Institute, St. Petersburg, USSR, in 1982, and D.Sc. from Newcastle University, UK, in 2006.\n\n--- Segment 46 ---\nAlex Yakovlev received the Ph.D. degree from the St. Petersburg Electrical Engineering Institute, St. Petersburg, USSR, in 1982, and D.Sc. from Newcastle University, UK, in 2006. He is cur- rently a Professor of Computer Systems Design, who founded and leads the Microsystems Research Group, and co-founded the Asynchronous Systems Laboratory, Newcastle University. He was awarded an EPSRC Dream Fellowship from 2011 to 2013. He has published more than 500 articles in various journals and conferences, in the area of concurrent and asynchronous systems, with several best paper awards and nominations. He has chaired organizational committees of major international conferences. He has been principal investigator on more than 30 research grants and supervised over 70 Ph.D. students. He is a fellow of the Royal Academy of Engineering, UK.\n\n