=== ORIGINAL PDF: 2502.10001v1_EmbBERT-Q_Breaking_Memory_Barriers_in_Embedded_NLP.pdf ===\n\nRaw text length: 82517 characters\nCleaned text length: 80316 characters\nNumber of segments: 57\n\n=== CLEANED TEXT ===\n\nEmbBERT-Q: Breaking Memory Barriers in Embedded NLP Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino , and Manuel Roveri Department of Electronics, Information and Bioengineering, Politecnico di Milano, Via Ponzio 34 5, Milano, 20133, Italy Abstract Large Language Models (LLMs) have revolutionized natural language processing, setting new standards across a wide range of applications. However, their relevant memory and computa- tional demands make them impractical for deployment on technologically-constrained tiny devices such as wearable devices and Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a novel tiny language model specifically designed for tiny devices with stringent memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in Natural Language Processing tasks in this scenario, with a total memory footprint (weights and activations) of just 781 kB, representing a 25 reduction in size with respect to SotA models. By combining ar- chitectural innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently outperforms several baseline models scaled down to a 2 MB memory budget (i.e., the maximum memory typically available in tiny devices), including heavily compressed versions of BERT and MAMBA. Extensive experimental evaluations on both a selected benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with re- spect to existing approaches, achieving an unmatched balance between memory and performance. To ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at 1 Introduction The proliferation of Internet-of-Things (IoT) systems and the availability of off-the-shelf, energy- efficient pervasive technologies have sparked a growing demand for intelligent on-device computa- tion [1]. Tiny devices such as microcontrollers and wearables are now key stringent technological solutions in relevant application scenarios ranging from smart homes to industrial automation [2]. De- spite significant advances in their memory and processing capabilities, these tiny devices still operate under stringent constraints, making very challening the design and development of machine (ML) and deep learning (DL) models meant to be executed on these devices [3, 4]. This need pushed the re- search field of Tiny Machine Learning (TinyML), which enables efficient ML DL inference on devices characterized by limited memory (e.g., less than 2 MB memory), low processing power (e.g., less than 100 MHz CPUs), and strict energy budgets [5], hence allowing tiny devices to locally process data, reducing latency, improving the real-time responsiveness of smart applications, and enhancing privacy by keeping sensitive data on-device. While TinyML has made strides in areas like keyword spotting [6, 7], image classification [8, 9], and object detection [10], deploying natural language processing (NLP) models on tiny devices remains a significant challenge. Modern decoder-based Large Language Models (LLMs) relying on the attention mechanism, such as BERT [11], XLNet [12], DistilBERT [13], SpanBERT [14], ALBERT [15], RoBERTa [16] and State-Space-Models (SSMs) such as MAMBA [17], rely on millions or even billions of parameters and extensive memory resources to achieve SotA accuracy across a wide range of NLP 1 arXiv:2502.10001v1 [cs.CL] 14 Feb 2025 tasks. Even scaled-down variants like MobileBERT (25.3M parameters) [18] are orders of magnitude too large for deployment on microcontrollers with less than 2 MB memory budgets. To fill this gap, we introduce EmbBERT-Q, a Tiny Language Model (TLM), specifically designed to operate on tiny devices, and under the stringent 2 MB memory budget. In particular EmbBERT-Q, comprises a novel TLM architecture optimized for microcontroller units and other resource-constrained devices. Using techniques such as hardware-compatible 8-bit quantization [19], EmbBERT-Q achieves SotA performance with only 781 kB of memory, a 25 reduction in memory with respect to the SotA models characterized by the smallest memory demands, such as BERT-Tiny [20]. Even when compared with other models specifically adapted to work within the 2 MB constraints, EmbBERT-Q resulted to be both the most effective and the most efficient model. Our contributions include: 1. EmbBERT-Q: We propose a new TLM model specifically designed for tiny devices, combining efficiency and effectiveness. 2. Memory and Computational Analysis: We analytically evaluate the memory usage and compu- tational complexity of EmbBERT-Q and its components, providing a useful tool to evaluate the weights and activations memory trade-offs required to operate within tiny device constraints. 3. Custom Benchmark: We design a specialized benchmark tailored to assess the NLP capabilities of TLMs, enabling consistent and fair evaluations in resource-constrained environments. The remainder of this paper is organized as follows. Section 2 reviews recent work on model compression and training techniques tailored for resource-constrained platforms, setting the stage for our contributions. Section 3 provides a detailed description of EmbBERT-Q, our proposed model for NLP in tiny devices, and includes precise calculations of memory requirements for its layers. In Section 4, we outline the experimental setup, including the training procedures and datasets used to validate our approach. Section 5 presents a comprehensive evaluation of our model on the TinyNLP benchmark suite and GLUE datasets, comparing downscaled versions of BERT and MAMBA, and highlighting the significant performance improvements achieved by EmbBERT-Q. Section 6 delves into an ablation study to assess the individual contributions of the architectural modules within EmbBERT- Q to its overall performance. Finally, Section 7 concludes the paper by discussing the experimental findings, exploring future research directions, and considering the broader implications of deploying TLMs in tiny devices. 2 Related Work This section introduces the SotA Small Language Models and the main techniques to reduce the memory and computational demand of LLMs. Small Language Models Several models, including BERT-Tiny [20], NanoBERT [21], Mobile- BERT [18], ConvBERT [22], and I-BERT [23] are considered at the SotA for Small Language Models each leveraging different techniques to reach their desired level of compression. BERT-Tiny [20] employs a shallow architecture with fewer layers and reduced hidden dimensions with respect to conventional BERT architectures, trading accuracy for a smaller size and lower compu- tational costs (4.4M parameters corresponding to a 20 MB memory footprint). It employs a knowledge distillation approach from a larger teacher model to a compact student model, achieving competitive accuracy through task-specific and data-augmented distillation. NanoBERT [21] represents, in current literature, the smallest model built for tiny devices. It manages to achieve it s astounding results by introducing a Nano Embedder, a clever mechanism that aims to reduce the memory footprint of the standard embedder. Still, with weights in the range 700-800k, experimental results confined to a few datasets and missing code and checkpoints it falls behind other more prominent publications. MobileBERT [18], on the other hand, employs bottleneck structures and a teacher-student training to compress BERT, retaining high accuracy at the expense of a higher number of weights (25.3M), making it better suited for edge devices than actual tiny devices. ConvBERT [22], with it s small size of 14M parameters, introduces convolutional operations in self- attention to reduce computational complexity and efficently capture local dependencies. I-BERT [23] 2 employs integer-only quantization, guaranteeing that all computations, including softmax and matrix multiplications, rely on integer arithmetic. This allows significantly improve memory and energy efficiency without compromising accuracy, despite its larger size of 355M parameters. Advanced Training and Pre-Training Techniques Training schemes beyond traditional masked language modeling (MLM) [11] play a crucial role in enhancing the efficiency and performance of smaller models. Techniques like ELECTRA generative-discriminative training [24] and LoRA low-rank updates [25] enable fine-tuning with reduced trainable parameters, optimizing computational demands while retaining accuracy. Additionally, data augmentation and synthetic text generation [26, 27] expand training datasets, enhancing knowledge transfer and improving small-model generalization. These methods are particularly relevant for small language models, as they provide mechanisms to further reduce resource requirements while maintaining competitive performance. For instance, generative-discriminative training, such as in ELECTRA, efficiently trains models to distinguish be- tween real and synthetically generated tokens, leading to faster convergence. Similarly, LoRA low-rank parameter updates focus computational resources on task-specific fine-tuning rather than full model retraining, an advantage for tiny devices. Quantization and Knowledge Distillation Quantization, a cornerstone of model compression, converts weights and activations to lower-precision formats, such as 8-bit integers, drastically reducing memory usage and improving inference efficiency [28]. Integer arithmetic, as demonstrated by I- BERT [23], aligns well with embedded hardware capabilities. Knowledge distillation [29], employed in models like Bert-Tiny and MobileBERT, further reduces model size by training smaller networks to replicate larger teacher models behavior. Exploring Alternative Architectures Beyond transformers, alternative architectures like recur- rent neural networks (RNNs) with state-space models (SSMs) represent a viable option for tiny devices. For example, MAMBA [17], with 140M parameters, leverages RNNs to mitigate the attention quadratic complexity, offering efficient text generation solutions that could be even tailored for environments with limited parallelism capabilities due to it s inherent recursive structure. Summing up, despite significant advancements across all these dimensions - innovative small-model designs, advanced training strategies, effective quantization techniques, and explorations of alternative architectures - most existing models fall short of meeting the stringent memory and computational constraints imposed by tiny devices. While NanoBERT achieves a parameter size of approximately 2 MB (excluding activation memory), its experimental limitations and performance deficits highlight the challenges of extreme compression. Other models like ConvBERT (14M parameters) and I-BERT (355M parameters) demonstrate substantial improvements in their respective niches but remain un- suitable for the smallest hardware due to their larger memory footprints. Crucially, these works underscore the importance of a multifaceted approach to compression. Small model designs benefit from integrating knowledge distillation and quantization for immediate memory reduction, while advanced training techniques like ELECTRA or LoRA further refine performance by leveraging enhanced pre-training and fine-tuning efficiency. Simultaneously, alternative architectures like MAMBA suggest promising directions for bypassing inherent transformer limitations, particularly for devices with limited computational parallelism. Our work builds on these foundational insights to propose tailored architectural innovations and memory-optimized techniques, advancing the SotA for embedded NLP applications. By synthesizing these diverse strategies, we aim to deliver models that not only push the limits of transformer down- scaling but also satisfy the most stringent resource requirements, bridging the gap between theoretical advancements and real-world applicability. 3 EmbBERT-Q: A Novel Language Model for Tiny Devices EmbBERT-Q is a compact and efficient language model designed specifically for deployment in memory- constrained environments. Unlike conventional approaches that compromise either performance or 3 N softmax SiLU V K Q - EmbBERT-Q Normalization Conv Skip Eff Attention Tokens Embedded tokens Output Positional embedder fc fc W₁ W₂ W₃ Conv-1D Token embedder Ef icient Encoder Embedder Block ' λCS λEA Figure 1: Overview of the EmbBERT-Q architecture, a specialized Language Model designed for tiny devices and extreme resource constraints. The architecture features an Embedder block and multiple Efficient Encoder with an Efficient Attention block and a Convolutional Skip Connection block. Instead of a final sum to aggregate the two Efficient Encoder paths, EmbBERT-Q uses a weighted difference with learnable weights for enhanced performance. memory footprint, EmbBERT-Q strikes a fine balance by leveraging tailored architectural optimiza- tions and an 8-bit quantization scheme. These innovations make it particularly suited for real-world natural language processing tasks in TinyML scenarios, where resource efficiency is paramount. 3.1 The EmbBERT-Q Architecture EmbBERT-Q achieves its efficiency through a redesign of traditional encoder-based BERT models, prioritizing lightweight attention mechanisms and memory-aware component selection. This architec- ture focuses on minimizing memory usage without sacrificing accuracy, surpassing other lightweight models within similar constraints. The adoption of 8-bit weight quantization, along with half-precision activation quantization and parameter-efficient fine-tuning techniques [30], ensures robustness to quan- tization and maintains state-of-the-art performance despite the limited hardware capabilities. As illustrated in Fig. 1, EmbBERT-Q comprises two main modules (delineated by dashed gray lines): a Nano Embedder Block, responsible for generating compact yet expressive token representations called δs; and a sequence of N Efficient Encoder blocks integrating efficient attention, convolutional layers, and weighted difference aggregation to process embeddings with minimal memory overhead. Additionally, an optional Output Block can be appended to adapt the architecture for specific down- stream regression, classification or generative tasks. In the following sections, we explore the functionality and design of these core modules, highlighting the architectural innovations that make EmbBERT-Q a SotA solution for tiny devices. 3.1.1 The Embedder block The Embedder block in EmbBERT-Q is adapted from the Nano Embedder introduced in [21] (whose memory requirements are analyzed in B). The Embedder block generates a d-dimensional representation δ for each token in a vocabulary of size v using its token embedder. These embeddings are positioned in a d-dimensional space, reflecting semantic similarity among tokens. To capture word order, the model includes a positional encoding component that maps each token position (up to a maximum window size ℓ, usually referred to as sentence length) into a d-dimensional vector, ensuring that token order is understood alongside their meaning. Unlike the original Nano Embedder [21], our Embedder block employs a learned positional embedding, built on the same principles as the token embedder. This approach not only enhances representation quality but also reduces memory requirements compared to using a fixed 4 positional embedder. Additionally, segment embeddings, similar to those used in BERT, assign distinct d-dimensional vectors to differentiate between input segments (e.g., sentences), enabling the model to interpret relationships across boundaries. A key part of the Embedder block memory efficiency is in its use of dense layers after the embed- dings. Specifically, the Embedder maps input tokens into a reduced space of dimension rd, and the fully connected layer projects them into the desired d-dimensional space. This results in a d ℓmatrix of embedded tokens δs for a sentence length ℓ. This approach significantly reduces the Embedder size while maintaining sufficient representational power for classification tasks. By decreasing mem- ory requirements for the embedding layer, the Embedder Block frees up space for other parts of the network, making it a crucial enabler for running language models on devices with limited memory. 3.1.2 The Efficient Encoder The core of EmbBERT-Q is its sequence of N Efficient Encoders shown in the right gray dashed block of Fig. 1, designed to balance expressive power with minimal resource requirements. The Efficient Encoder takes as input the embedded token matrix which gets first processed with a normalization layer, and successively by two blocks, operating in parallel: an Efficient Attention block and a Convolutional Skip Connection block. Let Norm( ) be the output of the Normalization layer of the Efficient Encoder, that becomes the input of both the Efficient Attention and the Convolutional Skip Connection blocks. The Efficient Attention block operates through a sequence of matrix multiplications involving the Query (Q), Key (K), and Value (V ) matrices. The Q matrix is defined as the output of a fully connected layer W1( ) processing , i.e., Q W1( ), while both K and V are identical to . Differently from other architectures using the attention mechanism, EmbBERT-Q uses always a number of attention heads h 1 for reduced activations and because higher head counts has been found by [31] to be less effective at this size. The Q matrix is multiplied by the K matrix, the resulting product undergoes a SoftMax activation, and it is multiplied by V . Finally, the obtained matrix is processed with a fully connected layer W2( ) obtaining the output EA of the Efficient Attention block: EA W2 Softmax Q K d V . The parallel Convolutional Skip Connection block takes as input the same matrix provided to the Efficient Attention block, processing it through a 1D convolutional (Conv1D) layer, with convolutional kernel size k, that expands the dimension of from d to d α, where α is a scaling factor. The result of this operation is followed by a SiLU activation function and a fully connected layer, that restores the original dimension d, obtaining the final output CS of the block: CS W3(SiLU(Conv1D( ))). The outputs EA and CS get combined through a weighted difference with learned weights λEA and λCS, to obtain the output of the Efficient Encoder: EA λEA CS λCS, that then serves as input to the next block, i.e. another Efficient Encoder or the final output block. By integrating the Convolutional Skip Connection, the Efficient Attention, and the weighted differ- ence, our Efficient Encoder block simultaneously optimizes architectural depth and memory efficiency. This is accomplished by eliminating separate normalization and feed-forward layers, while leveraging lightweight operations such as 1D convolutions and single-head attention. These novel design leads to a substantial reduction in both the weights and activation footprints of each components, enabling the exploration of diverse architectural configurations. This flexibility facilitates exploring trade-offs among embedding size, attention capacity, and memory usage, effectively balancing accuracy with deployability for EmbBERT-Q on devices constrained by memory limits of 1-2 MB. 3.1.3 Quantizing the EmbBERT-Q model Finally, we combine architectural efficiency with an hardware-compatible quantization scheme. To this aim we employ an 8-bit block-wise quantization, showing that our EmbBERT-Q model has minimal or 5 Table 1: Formulas for calculating the weights and activation sizes of the blocks and components of EmbBERT-Q. Layers Weights Activations Embedder block rd (v ℓ 2d) 2d rd ℓ 2d ℓ Norm layer 2d 2d ℓ Eff. Attention block 2d2 2d ℓ ℓ2 Conv Skip block d2 α k d α d ℓ(2 α) Efficient Encoder 2d 2d2 d2 α k dα max(2d ℓ ℓ2; d ℓ(2 α)) no performance degradation with respect to the 32-bit model, at a small fraction of the memory cost. This approach applies 8-bit floating-point representation to weights within a range of 6 [32]. Weights outside this range are stored in 16-bit floating-point (FP16) format to ensure numerical stability for extreme values. Additionally, all activations, initially in 32-bit floating-point (FP32), are converted to FP16. Further parameter-efficient fine-tuning (PEFT) is performed for additional two epochs with the 8-bit AdamW optimizer and a fixed learning rate of 1 10 4. We modify only a small subset of pa- rameters (approximately 8 of the total weights), focusing primarily on task-specific layers to improve performance while retaining the benefits of reduced precision. The quantization and fine-tuning process ensure that EmbBERT-Q is both computationally efficient and capable of delivering high performance on the target tasks. 3.2 Computing memory requirements of EmbBERT-Q An accurate computation of the total memory requirements for running EmbBERT-Q models is re- quired in severely resource-constrained settings. The total memory, Mtot, is the sum of the memory needed for its weights and activations, and can be expressed as: Mtot (Wemb N Wenc) Pw max(Aemb, Aenc) Pa, where Wemb and Wenc are the weights of the embedder and encoder, respectively, and Aemb and Aenc are their respective activations. Pw and Pa denote the precision (in bits) used to store weights and activations. The formulas used to compute the memory required for the weights and activations of the com- ponents of EmbBERT-Q are reported in Table 1. The next subsections are dedicated to an in-depth analysis of their memory and computational requirements. 3.2.1 The Embedder block The Embedder block of EmbBERT-Q is composed of 5 smaller components: token embedder of size v rd, positional embedder of size ℓ rd, segment embedder of size 2d and two fully connected of size rd d (where v is the vocabulary size; ℓthe embedding size, and rd the reduced embedding size). Together, they result in a total parameter count: Wemb rd (v ℓ 2d) 2d, achieving a reduction in parameter size of almost a d rd factor with respect to a standard Embedder. However, the total activation size Aemb required by the Embedder block slightly increases due to the added projection step, resulting in: Aemb rd ℓ 2d ℓ. During inference, the Embedder block performs as a first step ℓ 2ℓ rd memory accesses for token and position encoding, followed by 4ℓ rd d memory accesses, 2ℓ rd d ℓ d summations, and 2ℓ rd d multiplications for the linear layers, and in case segment embeddings are needed another ℓ 2d ℓ d accesses and ℓ d summations. 6 3.2.2 The Efficient Encoder We now proceed to analyze the memory usage and computational complexity of the three primary blocks that define the Efficient Encoder of EmbBERT-Q. The normalization layer s weights consist just of two vectors of size d representing the averages and standard deviations required for normalization. This block also has a minimal activation size, requiring only 2d ℓvalues. The operations within this layer necessitate 2d (ℓ 1) memory accesses, along with d ℓsummations and multiplications. From a memory perspective, the Efficient Attention block requires 2d2 weights and has an activation size of 2ℓ d ℓ2. While its memory demands are relatively small, this efficiency comes at the cost of a high computational complexity. Due to the various matrix multiplications and the softmax operation, the block requires: 4ℓ d2 4ℓ2 d 2ℓ2 memory accesses, 2ℓ d2 2ℓ2 d ℓ2 ℓ d summations and 2ℓ d2 2ℓ2 d 2ℓ2 multiplications. The Convolutional Skip Connection block, compared to the feed-forward block that typically fol- lows the standard attention layer and that is not present in EmbBERT-Q, features reduced memory requirements for the weights. This reduction arises from replacing the fully connected expansion layer with a Conv-1D expansion layer. Consequently, this block requires: d2 α k d α weights and d ℓ(2 α) values for activations. Its computational requirements are d ℓ(k 4 2d α) memory accesses, d ℓ(k 2 d α) sums and d ℓ(k 7 d α) multiplications. For the aggregation step, the block requires only 2d ℓmultiplications and memory accesses, with half as many summations. This step does not introduce any additional weights or activations. Summing up, the Efficient Encoder of EmbBERT-Q shown in Fig. 1 requires: Wenc 2d2 d2 α k d α 2d weights and has a total activation size of: Aenc max(2d ℓ ℓ2; d ℓ(2 α)). Overall, the self-attention mechanism s structured weighting of sequence elements enables robust con- textual representations, ensuring that tokens are influenced by all relevant inputs regardless of distance. Table 1 summarizes the memory requirements of the components in our proposed EmbBERT-Q archi- tecture. To further enhance the efficiency of this architecture, the implemented quantization strategy reduces the size of Pw from 4 Bytes (FP32) to approximately 1 Byte1. Similarly, the size of Pa is reduced from 4 Bytes (FP32) to exactly 2 Bytes (FP16). These optimizations resulted in a 2.5 reduction in memory consumption, significantly improving overall resource allocation. 3.3 Selecting the Architectural Parameters for EmbBERT-Q The selection of optimal architectural parameters for the EmbBERT-Q model is a critical step in ensuring both performance and memory efficiency. Key parameters such as vocabulary size v, sentence length ℓ, and embedding dimension d play a significant role in determining the model s memory usage and overall effectiveness. These parameters heavily influence the embedder memory occupation and cannot be significantly reduced without adversely affecting model accuracy. To adapt the model to the 2MB memory constraint, the first step involves identifying the smallest feasible values for v and ℓthat maintain acceptable performance. This study evaluated v within the range of 2000 to 10000 to maintain the model expressiveness, while ℓwas chosen between 256 and 512. The choice of ℓwas informed by the average sentence lengths in the selected datasets and their interplay with vocabulary size. The embedding dimension d and the reduced embedding dimension rd were then tuned. While these dimensions can be scaled down more aggressively, reducing d below 64 results in significant performance degradation, as shown in [31]. For rd, values between 16 and 32 were found to yield optimal results, aligning with the findings of [21]. Finally, structural parameters such as the scaling factor α and the number of layers N were fine- tuned to balance memory constraints with performance objectives, ensuring the model operated effec- tively within the given limitations. 1For the majority (92 ) of the weights, Pw is reduced to 1 Byte, while weights selected through the fallback procedure (8 ) are stored with a precision of 2 Bytes (FP16). 7 4 Experimental setup Table 2: Architectural parameters used for training model architectures. Columns represent: vocabulary size v, sentence length ℓ, embedding dimension d, reduced embedding dimension rd, forward expansion α, SSM internal memory size ds, convolutional kernel size k, number of heads h and number of layers. Hyperparameter v ℓ d rd α ds k h layers Weights Activations Total size BERT(2MB) [11] 2048 256 80 2 2 2 289 K 213 K 2,008 MB MAMBA(2MB) [17] 2048 256 64 1 6 4 5 220 K 265 K 1,941 MB Embedder 8192 256 320 32 1 293 K 164 K 1,826 MB Embedder conv 8192 256 320 32 16 1 298 K 164 K 1,848 MB EmbBERT-Q (ours) 8192 256 128 16 1 32 1 4 357 K 131 K 781 KB BERT-Tiny (20MB) [20] 32768 512 128 2 2 2 4.4 M 786 K 20,746 MB This section outlines our experimental protocol, focused on training EmbBERT-Q and several base- line models for comparison under a strict 2 MB memory budget. Our comprehensive and reproducible experimental campaign spans 6 different models trained across 17 datasets, providing insights into architecture design strategies that balance performance and memory efficiency. We detail the compar- ison models, training protocols, datasets, and evaluation metrics, setting the stage for the performance analysis in Section 5. 4.1 Baseline Models and Comparisons As a comparison to the proposed EmbBERT-Q model, we evaluated a diverse set of architectures, each constrained to a maximum memory footprint of 2 MB (including parameters and activations). Below, we summarize the key characteristics of these baseline models: 1. BERT(2MB): A scaled-down variant of the original BERT architecture [11], preserving the stan- dard embedding layers and encoder blocks. 2. MAMBA(2MB): A scaled-down adaptation of the MAMBA model [17], incorporating its native embedding mechanism and SSM-based recurrent blocks. 3. Embedder Only: This baseline leverages the Nano Embedder [21] without any mechanism for token interaction. While not a fully functional language model, it highlights the parameter budget allocated to the embedding layer and evaluates the embedder standalone capability. 4. Embedder Conv: Extends the Embedder Only configuration by adding a lightweight 1D con- volutional layer. This enables local token interactions within a fixed-size context window, intro- ducing minimal parameter overhead. We further include BERT-Tiny [20], a minimal variant of BERT which is approximately 10 larger than the 2 MB models evaluated in this study, as a SotA reference point. Despite its significantly larger size, it serves as a useful benchmark for performance comparison. Each model incorporates a task-specific output layer, adapted to the dataset and classification task. Given its small size and high customizability, this layer memory contribution has been excluded from the calculations of effective memory usage. Table 2 presents a detailed overview of the architectural parameters, weight counts, and activation sizes for each model, including EmbBERT-Q, and illustrates their total memory footprint. Further details on ther architecture and on their memory and computational requirements can be found in B. 4.2 Training and pretraining This section outlines the procedures used for both the pretraining and finetuning of EmbBERT-Q and the baseline models discussed in this work. 8 Pretraining Protocol For models supporting BERT-style pretraining [11], we use the publicly available BookCorpus dataset [33]. After training a Byte Pair Encoding (BPE) tokenizer tailored to the required dictionary size of each model, we construct sentence pairs for Masked Language Modeling (MLM) and Next-Sentence Prediction (NSP). Sentence pairs are generated by pairing half of the tokenized sentences contiguously and the other half randomly. Within each pair, tokens are masked with a 1 6 chance, and masking is applied with the following probabilities: 70 of masked tokens are replaced with the MASK token; 15 are substituted with a random token; 15 are left unchanged. This masking strategy promotes contextual reasoning over random guessing. Pretraining is performed for one epoch with a batch size of 32 and a learning rate of 5 10 4 using the standard AdamW optimizer2. Finetuning protocol Following pretraining, models are finetuned on target datasets for 10 epochs using a fixed3 learning rate of 3 10 4. Validation is conducted at the end of each epoch using the Matthews Correlation Coefficient (MCC) for classification tasks or the Spearman Correlation Coefficient (SCC) for regression tasks. The best-performing checkpoint, determined by the highest validation metric, is saved and subsequently used for final testing. Note that two models, namely Embedder and Embedder Conv, do not undergo pretraining. Due to their extremely simple architectural structure, these models cannot effectively absorb BERT-style pretraining. Instead, they are trained directly on the target datasets for 20 epochs, doubling the standard finetuning protocol to ensure they consume the same amount of computation as the other models. 4.3 Datasets In order to meaningfully compare the performance of EmbBERT-Q and the baseline models, we use two benchmark datasets: the TinyNLP benchmark (introduced in this paper) and GLUE [34]. In the following subsections we proceed to detail the tasks contained in both these benchmarks datasets, and the procedure for the train-validation-test splitting. 4.3.1 The TinyNLP benchmark To better evaluate TLMs in the real-world scenarios and resource-constrained environments they are ex- pected to operate, we introduce the TinyNLP benchmark. This curated collection of existing datasets is specifically tailored to the constrained yet practical applications of language models on embedded devices. Details of the dataset selection in the TinyNLP benchmark are presented in Table 3. The selection of these datasets represents application scenarios suited to models with restricted memory footprints, and is guided by the practical aim of assessing TLM deployment on embedded devices. Building on the discussion in Section 1, the TinyNLP benchmark focuses on tasks that are narrower in scope and less computationally demanding compared to standard (LLM) benchmarks. These tasks are grouped into three broad categories: i) Request Classification: Relevant to virtual assistants in TinyML contexts, these datasets involve discerning the type of user request (e.g., requests for information, action, or assistance). As datasets focused on this kind of task, we have included nlu [35] and Snips in the TinyNLP benchmark. ii) Sentiment Analysis: Focuses on determining the emotional tone or opinion expressed in text. This commonly involves classifying content as positive, negative, or neutral, and sees wide usage in analyzing customer reviews or social media feedback. As datasets focused on this kind of task, we have included IMDb [36] and Emotion [37]. iii) Context Understanding: Involves identifying the broader context in which text is generated. For example, distinguishing whether the text describes a particular situation or environment. 2Due to the stringent memory constraints considered in this work, more complex pre-training strategies such as ELECTRA-style training [24] had to be excluded. ELECTRA requires both a generator and a discriminator, with the generator typically being about half the size of the discriminator. Under the strict 2 MB memory constraint, it is infeasible to construct a generator of sufficient size while maintaining a capable discriminator. 3To maintain consistency we omit complex learning rate schedulers, as different models may exhibit varying responses to specific schedules. Future work could systematically explore scheduling strategies for these models. 9 Table 3: Datasets selected as benchmarks for TinyML NLP, covering various tasks (TinyNLP). Name Size Classes Kind IMDb 50k 2 Sentiment analysis ag news 127.6k 4 Document classification cyberbull 46k 6 Racism classification LiMiT 24.6k 2 Movement presence Emotion 20k 6 Sentiment analysis nlu 25.7k 18 Request classification Snips 14.5k 7 Request classification As datasets focused on this kind of task, we have included ag news [38], cyberbull [39] and LiMiT [40]. 4.3.2 The GLUE benchmark The General Language Understanding Evaluation (GLUE) benchmark [34] is a widely adopted NLP benchmark comprising multiple, diverse and complex datasets, designed to test generalization and performance across diverse NLP tasks (see Table 4). It encompasses multiple subtasks, including sentiment classification and regression on sentence pairs. Because the official GLUE labels are only publicly released for the training and validation splits - and in line with prior approaches (e.g., [31]) - we treat the validation set as our test split throughout this study. 4.3.3 Train-Validation-Test Dataset Splittings For both the TinyNLP and GLUE benchmarks, each dataset is divided into training, validation, and test sets according to one of the following protocols, listed in order of priority: i) Provided splits: When the dataset creators supply official train, validation, and test splits, we use these directly to ensure consistency with prior work. ii) For datasets with only a single official split (e.g., train-test only), we designate the larger portion as the training set and the smaller portion as the test set. From the training set, we withhold 10 of the samples to create a validation set. iii) No provided splits: For datasets lacking any predefined splits, we partition the data into a 90-10 ratio for training and testing. Subsequently, 10 of the training set is withheld to create a validation set. 4.4 Evaluation The pretraining of all models on the BookCorpus dataset is conducted once, while the fine-tuning phase on each target dataset is repeated five times with different random seeds corresponding to different AdamW mini-batch shuffling, to ensure robustness of the results. Evaluation metrics are computed as the average of these five runs. For the sake of simplicity, in the experimental results reported in Sec. 5, as evaluation metrics we focus on Accuracy for the TinyNLP benchmark, and on the metric used for computing the average Score in each dataset in the GLUE benchmark: SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Complete results across all evaluation metrics, 10 Table 4: Datasets from the GLUE benchmark [34], used for comparison with larger SotA models. Name Size Classes Kind cola 10.7k 2 grammatical semantical correctness mnli-m 403k 3 correct summarization mnli-mm 403k 3 correct summarization mrpc 5.8k 2 semantical equality qnli 116k 2 question answer entailment qqp 795k 2 concept repetition rte 5.77k 2 correct summarization sst2 70k 2 sentiment classification stsb 8.63k phrase similarity regression wnli 852 2 phrases entailment including Loss, Accuracy, F1 Score, Precision, Recall, and the Matthews Correlation Coefficient (MCC) for classification tasks, are reported in Appendix A. Average results for the two benchmark datasets were also calculated and are reported in the last column of Tables 5 and 6. Average accuracy was used as the average metric for the TinyNLP bench- mark; while the average score for the GLUE benchmark was computed following the standard GLUE protocol. 5 Experimental Results Table 5: Model performance on the TinyNLP benchmark, reporting accuracy for each individual dataset and overall averages. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average Acc. Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT(2MB) [11] 79,38 89,00 83,90 74,72 77,34 86,14 97,00 83,93 MAMBA(2MB) [17] 81,86 89,40 81,38 74,72 45,72 70,10 96,40 77,08 EmbBERT-Q (ours) 84,01 90,63 86,60 74,10 89,90 94,05 97,93 88,17 BERT-Tiny(20MB) [20] 85,69 91,93 83,38 72,40 88,86 88,53 98,16 86,99 We evaluate EmbBERT-Q and the baseline models on the TinyNLP and GLUE benchmark datasets, comparing their performance respectively in Tables 5 and 6, showing the superiority of EmbBERT-Q on both datasets and over all baselines, for what concerns both performance and memory requirements (shown in Table 2). We compare the performances of all models on both TinyNLP and GLUE under pre-trained and non-pre-trained conditions, as reported in the Tables of A. This comparison allows to make a significant observation: architectures equipped with attention mechanisms or SSMs can benefit more substantially from large, diverse pretraining corpora compared to simpler embedding-based approaches, but with the limitations given by the strict constraints of our 2 MB memory budget they still struggle even to reach comparable results to very simple models if not purposefully adjusted for the task. To account for the models different capabilities in leveraging pretraining, the results reported in Tables 5 and 6 refer to two different training scenarios: i) a non-pretrained protocol, for the Embedder 11 Table 6: Model performance on the GLUE benchmark. Metrics are MCC for CoLA, F1 score for MRPC and QQP, Spearman s Correlation Coefficient (SCC) for STSB, and accuracy for the remaining tasks, as required for the official calculation of the overall GLUE score. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT(2MB) [11] -0,86 71,28 64,66 73,04 60,56 61,58 60,82 48,24 66,20 15,48 52,10 MAMBA(2MB) [17] 2,56 81,16 64,62 79,18 61,22 61,40 63,20 50,20 23,38 10,16 49,71 EmbBERT-Q (ours) 9,56 80,96 67,99 82,45 67,10 68,05 68,06 47,29 87,32 49,28 62,81 BERT-Tiny(20MB) [20] 0,00 83,20 71,10 62,20 70,20 70,30 81,50 57,2 62,30 73,60 63,16 and Embedder Convolution models, which lack architectural mechanisms to fully exploit pretraining; and ii) a pretrained and fine-tuned protocol, for all other models. The next sections provide a detailed analysis of results obtained on the TinyNLP and GLUE benchmarks. 5.1 TinyNLP Benchmark Results On the TinyNLP benchmark, where models are evaluated based on Accuracy, EmbBERT-Q demon- strates the best performance compared to the other models, achieving an Average Accuracy of 88.17 , as shown in Table 5. Notably, EmbBERT-Q outperforms BERT-Tiny, which requires around 25 more memory but only achieves the second-highest average Accuracy of 86.99 . Interestingly, the only models in the 2 MB range that offer comparable results were the Embedder and Embedder Conv configurations. Despite their seemingly simplistic design, these models perform well on the TinyNLP tasks, which generally feature shorter and less complex phrases with respect to the GLUE benchmark. For these tasks, the pretraining applied to the other models had a relatively limited impact, as shown in A. These results highlight the Embedder models ability to handle lightweight tasks effectively. The 2 MB down-scaled versions of the BERT and MAMBA models, on the other hand, score lower on Average Accuracy with respect to Embedder models, indicating that these models may be less suitable for environments with stringent memory budgets. This suggests that the overall architecture of EmbBERT-Q, with highly optimized embedding and attention structures, is particularly well-suited for the TinyNLP classification tasks in memory-constrained scenarios, with respect to down-scaled versions of standard models. 5.2 GLUE Benchmark Results On the GLUE benchmark, models were evaluated using various metrics, including Matthews Correla- tion Coefficient (MCC), F1 score, Spearman s Correlation Coefficient (SCC), and Accuracy, depending on the nature of each task. EmbBERT-Q once again emerges as the top-performing model within the 2 MB memory range, achieving an average score of 62.81. It demonstrates competitive performance across multiple tasks, excelling particularly in datasets such as WNLI (87.32 F1 score) and STSB (49.25 F1 score). Detailed results for the score metrics are presented in Table 6, along with the average computed across all datasets (in the last column), while complete results can be found in A. Remarkably, EmbBERT-Q (781 kB) achieves a performance on GLUE comparable to the 25 larger BERT-Tiny model (20 MB) despite the substantial difference in memory requirements, even outperforming BERT-Tiny in 4 out of the 10 datasets included in the benchmark. The Embedder and Embedder Conv models again outperformed BERT and MAMBA down-scaled within the 2 MB range, but obtaining a significant 7-point difference in score compared to EmbBERT-Q, highlighting its superior performance with respect to the baselines. 5.3 Discussion of the Results The results from both the TinyNLP and GLUE benchmarks establish the proposed EmbBERT-Q as the current SotA Language Model for TinyML hardwares and NLP applications. Among the available 12 BERT (2MB) BERT NE BERT EA BERT NE EA EmbBERT EmbBERT-Q Models 0 250 500 750 1000 1250 1500 1750 2000 Memory Requirements (KB) 1156K 852K 1080K 892K 1292K 696K 1464K 524K 1428K 524K 518K 262K TinyNLP Memory occupations Weights Activations (K) 84 85 86 87 88 Accuracy ( ) 83.93 86.21 85.64 87.04 87.19 88.17 BERT (2MB) BERT NE BERT EA BERT NE EA EmbBERT EmbBERT-Q Models 0 250 500 750 1000 1250 1500 1750 2000 Memory Requirements (KB) 1156K 852K 1080K 892K 1292K 696K 1464K 524K 1428K 524K 518K 262K GLUE Memory Occupations Weights Activations (K) 52 54 56 58 60 62 64 Score ( ) 52.10 57.50 51.09 61.20 63.50 62.81 Figure 2: Number of weights (blue), number of activations (grey), and Accuracy Score results obtained by each model analyzed in Sec. 6 on the TinyNLP benchmark (left panel) and on the GLUE benchmark (right panel). LMs, it obtains the best experimentally observed balance between memory requirements and task performance. These results become even more significant when the memory requirements of competitor models are taken into account. EmbBERT-Q requires just 781 kB of total memory for both weights and activations, representing a reduction of approximately 2.4 compared to the other models we tested in the 2 MB range. The contrast is even more pronounced when compared to BERT-Tiny, the smallest SotA LM available in the literature, which demands nearly 25 more memory than EmbBERT-Q. 6 Evaluating the Impact of The Architectural Components of EmbBERT-Q In this section, we analyze the contributions of EmbBERT-Q architectural components to its overall performance on both the TinyNLP and GLUE benchmarks. Taking BERT (2 MB) as a baseline, we systematically introduced improved and optimized key components, defining in this way EmbBERT-Q. Through this process, we evaluate the individual and collective impact of these changes, always adhering with the strict memory constraints of 2 MB. Figure 2 shows the memory requirements of various model configurations alongside their corre- sponding accuracy on TinyNLP and score on GLUE. Base model - BERT(2MB) BERT(2MB) is a compressed variant of BERT that serves as our vanilla baseline model. Compared to the larger BERT-Tiny model [20], which has a 10 memory footprint, BERT(2MB) shows a notable performance degradation that could be attributed to its re- duced parameter count. Despite these limitations, BERT(2MB) marks a first step toward adapting transformer architectures for ultra-low-memory environments, demonstrating the feasibility of scaling down LM models while maintaining some level of task performance. BERT Nano Embedder (BERT NE) To address the limitations of BERT(2MB), we replace its Embedder with the Nano Embedder [21], which is designed to optimize embedding representations without increasing the overall parameter count. This substitution expands the effective vocabulary space within the same memory budget, resulting in notable performance improvements on both the TinyNLP and GLUE benchmarks, as can be seen in Fig. 2. BERT Efficient Attention (BERT EA) To reduce activation overhead, we proceed to replace the default multi-head attention module with Efficient Attention [41], aiming to lower weight and activation memory costs. This reduction allows for increased embedding dimensionality and or additional layers. This modification significantly improves performance on the TinyNLP benchmark but, when not paired with other architectural modules, results in a slight decrease in performance with respect to the base BERT(2MB) model on the GLUE benchmark, as can be appreciated in Fig. 2. 13 BERT NE EA We combine the Nano Embedder with Efficient Attention to create the BERT NE EA model, leveraging a broader vocabulary together with reduced weights and acti- vations overhead. This combination leads to a performance gain on both TinyNLP and GLUE tasks, where BERT NE EA achieves respectively an accuracy of 87.51 and a score of 61.20, i.e. re- spectively over 3 and 9 points with respect to the original BERT(2MB). These results highlight the advantage of combining embedding efficiency with an optimized attention mechanism in ultra-compact models. EmbBERT By finally integrating a parallel path with a Conv-1D followed by an Fully Connected layer we obtain the EmbBERT architecture. In this way, we add with a minimal memory overhead a modified feed forward block to the main attention path of BERT NE EA, i.e. a further simple token-mixing convolutional and fully connected layer. This addition provides a marginal improvement on the TinyNLP benchmark, but achieves significant success on the more complex tasks contained in the GLUE benchmark. Specifically, it improves performance by over 2 points in the GLUE score compared to the BERT NE EA model. EmbBERT-Q Finally, the EmbBERT model is quantized using the 8-bit post-training quantization procedure outlined in Section 3.1.3, resulting in the proposed EmbBERT-Q model. In the TinyNLP benchmark, EmbBERT-Q achieves an average accuracy of 88.17 , marking an improvement of nearly 1 percentage point over its unquantized counterpart. This increase can be attributed to the regularization effect that quantization may induce under certain conditions, combined with the relative simplicity of the tasks in this benchmark. On the GLUE benchmark, which evaluates broader natural language understanding tasks, EmbBERT-Q demonstrates exceptional robustness to quantization, achieving an overall GLUE score of 62.81. This represents a minimal performance drop of 0.7 percentage points compared to the unquantized EmbBERT version. In Appendix A we show that the BERT NE EA model, instead, suffers significant performance drops of up to 15 percentage points on GLUE due to post-training 8-bit quantization. The quantization process gives substantial memory savings, reducing the total memory required to store and execute EmbBERT-Q from the around 2 MB of the unquantized EmbBERT to just 781 kB, considering both weights and activations (a 2.4 reduction in memory demand). While quantization often introduces trade-offs in accuracy, the robustness of the EmbBERT architecture highlights its suitability for deployment in constrained environments where such memory optimization techniques are critical. Through the comprehensive ablation study performed in this section, we have examined the contri- butions of key architectural components, including the Nano Embedder and Efficient Encoder. Main- taining the total memory usage below the 2 MB budget throughout the study, we have demonstrated that the inclusion of these architectural components and of 8-bit quantization in EmbBERT-Q leads to an Average Accuracy improvement of 4.24 percentage points on the TinyNLP benchmark and a 10.71 point increase on the GLUE benchmark score, with respect to the original BERT(2MB) model. With its carefully designed architecture and 8-bit quantization, EmbBERT-Q pushes the frontier of ultra-compact language models, delivering state-of-the-art performance in environments with stringent memory and computational constraints. 7 Conclusions In this work, we presented EmbBERT-Q, a novel language model specifically designed for tiny devices and extreme resource constraints. EmbBERT-Q achieves Accuracies and Scores comparable to the ones of models with up to 25 its memory footprint, and stands out as the highest-performing solution within the 2 MB memory budget explored in this paper. By leveraging an innovative architectural design specifically tailored for extremely memory- and computationally-constrained environments, EmbBERT-Q effectively balances parameter efficiency and competitive accuracy. Its Embedder Block, Efficient Encoder, and the application of post-training 8-bit quantization significantly reduce the model s memory footprint while maintaining high performance both on the newly proposed TinyNLP benchmark and on the GLUE benchmark, crucially demonstrat- ing the effectiveness and the efficiency of EmbBERT-Q in resource-constrained environments. 14 Future research will explore further compression techniques, novel architectural designs, targeted knowledge distillation, and even more extreme quantizations tailored to emerging hardware accelerators (e.g., 1-bit quantization). Combining these advancements with next-generation hardware has the potential to further optimize model memory and computation footprints while preserving, or even enhancing, performance. Our work establishes a systematic foundation for designing efficient language models capable of operating effectively within the most stringent memory constraints. Acknowledgment This paper is supported by PNRR-PE-AI FAIR project funded by the NextGeneration EU program. A Complete results In this section, we provide the complete results of our experiments, spanning all datasets and models in both pretrained and non-pretrained contexts. The detailed results, concerning model quantization as well, are presented in Tables 7, 8, 9, 10, 11, and 12. A.1 Evaluation on the TinyNLP Benchmark For what concerns non-pretrained models, as shown in Table 7 BERT(2MB) and MAMBA(2MB) demonstrate compatible results with respect to other models, with average accuracies of 83.74 and 83.86 , respectively. Notably, Embedder Conv and Embedder outperform others models, achieving 86.50 and 86.41 average accuracy, respectively. The Embedder Conv model particularly excells on datasets like AG News, scoring 91.50 , the highest across the board for this task. When non-pretrained and evaluated with the MCC score, as shown in Table 8 the Embedder-based models again deliver competitive performances, with Embedder Conv achieving an average MCC of 78.49 . In contrast, BERT-based models generally lagged behind. EmbBERT and EmbBERT-Q achieved an MCC score of 80.59 , showing limited performance in the non-pretrained context. For what concerns instead pretrained and finetuned models on TinyNLP, as shown in Table 9, EmbBERT-Q consistently delivered the top Accuracy performance, achieving an average accuracy of 88.17 . BERT NE EA followed closely with a score of 87.04 . The MCC results shown in Table 10 reflect the Accuracy trends, with EmbBERT-Q leading, showing its robustness to 8-bit quantization across diverse datasets. A.2 Evaluation on the GLUE Benchmark The GLUE benchmark evaluates models on NLP tasks such as sentiment analysis (SST-2), natural language inference (MNLI), and semantic similarity (STSB). For non-pretrained models, as shown in Table 11 Embedder achieves a GLUE score of 56.92, followed closely by Embedder Conv at 56.41. EmbBERT surpasses both, showing (even if by a small margin in the non-pretrained context) its architectural superiority on difficult NLP tasks. Among pretrained models, as shown in Table 12, EmbBERT outperforms all variants by a significant margin, showing its superior ability to absorb pretraining with respect to all other models. EmbBERT-Q shows minimal accuracy degradation and robustness to 8-bit quantization. B Exact Computation of Memory and Computational Cost of LLM Layers The architecture of Large Language Models (LLMs) is primarily based on the Transformer model introduced by [42]. This architecture has revolutionized natural language processing by enabling models to effectively handle long-range dependencies in text. Encoder-based text classification models typically consist of two main components: an embedder and an encoder. The encoder, in turn, is primarily composed of an Attention Mechanism and a Feed-Forward Neural Network. In this section, we provide a comprehensive analytical, layer-by-layer evaluation of the memory and computational requirements of common components used in Language Models, as well as an 15 Table 7: Accuracy of non-pretrained models on the TinyNLP benchmark. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 78,98 88,93 82,63 70,10 83,35 85,63 96,58 83,74 MAMBA(2MB) 78,18 91,08 83,74 71,66 77,40 87,60 97,34 83,86 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 82,52 90,86 82,96 69,82 78,34 84,06 96,74 83,61 BERT NE EA 81,60 90,53 82,30 55,57 83,70 84,90 96,50 82,16 EmbBERT 83,10 90,82 82,50 68,90 68,48 76,78 95,18 80,82 Table 8: Evaluation of MCC score of non-pretrained models on the TinyNLP benchmark. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,10 85,25 79,45 40,13 78,35 84,05 96,00 74,48 MAMBA(2MB) 56,62 88,14 80,82 42,62 70,84 86,22 96,86 74,59 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 65,06 87,82 79,86 38,30 72,34 82,42 96,20 74,57 BERT NE EA 63,32 87,35 79,13 20,67 78,85 83,30 95,93 72,65 EmbBERT 66,32 87,78 79,26 38,42 60,78 74,66 94,40 71,66 Table 9: Accuracy of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets). Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 79,38 89,00 83,90 74,72 77,34 86,14 97,00 83,93 MAMBA(2MB) 84,76 90,68 81,20 73,98 74,58 73,24 93,42 81,69 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 81,86 89,40 81,38 74,72 45,72 70,10 96,40 77,08 BERT EA 80,46 89,46 84,58 74,12 85,78 87,44 97,62 85,64 BERT NE EA 83,19 90,80 84,13 75,80 88,70 88,88 97,79 87,04 BERT NE EA (8bit) 82,35 89,51 85,58 76,40 80,85 89,46 97,29 85,92 EmbBERT 84,10 90,46 83,97 76,36 89,58 88,16 97,67 87,19 EmbBERT-Q 84,01 90,63 86,60 74,10 89,90 94,05 97,93 88,17 Table 10: Evaluation of MCC score of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets). Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,88 85,38 80,86 47,14 71,26 84,74 96,50 74,97 MAMBA(2MB) 63,78 85,90 77,92 46,56 30,40 66,92 95,76 66,75 Embedder 65,18 88,13 80,10 41,10 86,23 88,30 97,58 78,09 Embedder Conv 68,15 88,68 80,45 40,45 86,18 88,13 97,38 78,49 BERT NE 66,64 87,54 80,94 45,98 83,62 85,12 97,54 78,20 BERT EA 61,08 85,94 81,66 47,86 81,64 86,10 97,26 77,36 BERT NE EA 68,04 88,08 82,60 51,24 85,62 86,46 97,06 79,87 BERT NE EA (8bit) 64,70 86,02 82,77 47,89 74,55 88,20 96,84 77,28 EmbBERT 68,25 87,31 80,87 48,10 86,26 86,76 97,29 79,26 EmbBERT-Q 68,04 87,51 84,10 46,84 86,71 93,36 97,59 80,59 16 Table 11: Evauation of non-pretrained models on the GLUE benchmark. We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) 3,93 75,20 63,88 81,03 64,95 63,65 62,75 49,38 70,68 6,74 54,22 MAMBA(2MB) 7,22 80,60 60,72 80,66 64,27 64,44 59,78 51,76 80,00 4,58 55,40 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 5,22 77,34 63,18 81,12 64,14 64,53 66,76 51,14 85,62 4,72 56,38 BERT NE EA 2,66 78,68 61,90 83,16 62,65 61,63 62,36 48,38 85,62 8,94 55,60 EmbBERT 5,32 78,50 62,54 82,58 63,82 65,78 63,68 51,26 87,30 9,76 57,05 Table 12: Evaluation of pretrained and finetuned models on the GLUE benchmark (Embedder and Embedder Conv are directly trained on the donwstream datasets). We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) -0,86 71,28 64,66 73,04 60,56 61,58 60,82 48,24 66,20 15,48 52,10 MAMBA(2MB) 2,56 81,16 64,62 79,18 61,22 61,40 63,20 50,20 76,62 10,16 55,03 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 9,04 78,82 65,04 79,96 63,08 63,30 63,20 51,76 87,30 13,46 57,50 BERT EA 10,06 79,44 66,48 78,88 60,82 60,82 63,50 50,76 22,24 17,92 51,09 BERT NE EA 18,70 79,60 65,36 82,66 67,06 67,44 67,44 53,66 86,74 23,34 61,20 BERT NE EA (8bit) 9,57 77,87 63,40 48,93 34,19 33,59 59,58 53,79 59,15 19,58 45,97 EmbBERT 11,01 79,33 69,19 83,25 67,83 68,63 68,92 49,96 87,61 49,25 63,50 EmbBERT-Q 9,56 80,96 67,99 82,45 67,10 68,05 68,06 47,29 87,32 49,28 62,81 overall view of their functionality. We emphasize that our computational evaluations account for the complexity and memory access profiling associated with all needed model layers. Particular attention is given to CPU-based operations, including summation, multiplication, and memory retrievals. For memory evaluation, we assume a non-parallel program that retains only the minimum required data in memory to execute effectively. This approach reflects realistic constraints in resource-constrained environments, such as those encountered in TinyML applications. The following assumptions are made during our detailed computation of weight and activation matrices memory requirements for hardware deployment: Operations such as sums, element-wise multiplication, and activation functions are performed in-place, occupying only the memory of the input matrices, as intermediate results are discarded. Matrix multiplications require memory for both input matrices as well as the output matrix. Fully connected layers are treated as matrix multiplications where only one input and the out- put matrix contribute to activation memory, since weights do not increase activation memory requirements. The maximum memory consumption per layer is recorded at peak usage during processing. All calculations are based on inference-only processing, without accounting for training-related overheads. The final memory occupations, memory accesses, and formulas for sums and multiplications for each block are provided in Tables ?? and 14. B.1 BERT BERT (Bidirectional Encoder Representations from Transformers) [11] is a Transformer Encoder- based foundational NLP model widely used for tasks such as text classification, question answering, 17 and text generation. It leverages Transformer layers to generate contextualized representations of input text, capturing both left-to-right and right-to-left dependencies. The architecture of BERT typically consists of an Embedder, followed by a series of Attention and Feed-Forward layers, interleaved with normalization layers. These components are repeated N times. Embedder The standard Embedder, illustrated in Fig. 3, is responsible for generating token embed- dings, positional encodings, and segment embeddings using learned dictionaries. These embeddings are then summed to produce the final input encoding fed into the model. The total parameter count for the embedder, Wemb, is calculated as the sum of the sizes of the token embedding matrix (v d), the positional embedding matrix (ℓ d), and segment embedding matrix (2d): Wemb d (v ℓ 2), (1) where v is the vocabulary size, ℓis the sequence length, and d is the embedding dimension. The maximum activation size, Aemb, results from storing token, positional, and segment embeddings as matrices of size ℓ d during inference. These embeddings are summed in pairs, leading to: Aemb 2d ℓ. (2) The embedding operations required to compute the output of this layer involve ℓ (4d 2) 2d memory accesses and ℓ 2d summations. Attention The standard Attention Mechanism [42] allows models to selectively focus on the most relevant parts of an input sequence.Initially designed for machine translation, attention assigns vary- ing weights to tokens based on their relevance to the task or context, enabling models to capture dependencies between distant words. The self-attention variant computes relationships within a sequence by enabling each token to attend to all others, creating contextualized representations that encode both local and global depen- dencies. The input is processed through three fully connected layers to produce the Query, Key, and Value matrices, each with size d2. This step generates an activation size of 4 d ℓand requires: i) 6ℓ d2 memory accesses, ii) 3ℓ d2 summations, and iii) 3ℓ d2 multiplications. The Query and Key matrices are then multiplied to compute a Weight matrix of size ℓ ℓfor each of the h attention heads. Due to the quadratic growth in the Weight matrix, the context length has a significant impact on activation size, which increases to 3d ℓ ℓ2 h. This step also adds: i) 2ℓ2 d h memory accesses, ii) ℓ2 d h summations, and iii) ℓ2 d h multiplications. The Weight matrix undergoes a softmax operation, contributing: i) 2ℓ2 h memory accesses, ii) ℓ2 h summations, and iii) 3ℓ2 h multiplications, without increasing activation size. Next, the Weighted matrix is multiplied by the Value matrix, producing an output activation size of 2d ℓ ℓ2 h. This step introduces i) 2ℓ2 d h memory accesses, ii) ℓ2 d h summations, and iii) ℓ2 d h multiplications. Finally, the output passes through a fully connected layer of size d2 d with a skip connection. Although the activation size remains unchanged, this stage involves: i) ℓ d2 2 memory accesses, ii) ℓ d2 ℓ d summations, and iii) ℓ d2 multiplications. B.2 NanoBERT The NanoBERT model, introduced in [21], is an ultra-compact version of BERT, designed for efficient operation on resource-constrained devices. It achieves this by employing techniques such as word embedding factorization and Low-Rank Adaptation (LoRA). A key component of its efficiency is the Nano Embedder (shown in Fig. 3), which serves the same purpose as the standard Embedder but introduces a critical optimization: instead of embedding tokens and positions directly into vectors of size d, it maps these inputs to a reduced embedding dimension rd using a fully connected layer. This reduced embedding is then projected back to the original dimension d through a fully connected layer. Segment embeddings are excluded from this process. This approach modifies the parameter count to: Wnemb rd (v ℓ 2d) 2d, (3) 18 EMBEDDER positions tokens l embedding l d l d l d NANO EMBEDDER fc positions tokens l rd l rd l d l d l d l embedding fc Figure 3: Memory layout of the Standard Embedder (left) and Nano Embedder (right) layers. ℓis the sentence length, d is the embedding dimension, and the trapezoid labeled fc denotes fully connected layers. The dashed gray box highlights the operations requiring the maximum activation size. The Nano Embedder reduces the number of weights while maintaining a similar activation size. which can be lower than that of the Standard Embedder if rd is sufficiently small relative to d. However, the total activation size, Anemb, increases slightly due to the projection step, resulting in: Anemb rd ℓ 2d ℓ. (4) During inference, the Nano Embedder performs the following operations: i) ℓ 2ℓ rd memory accesses for token and position encoding, followed by ii) 4ℓ rd d memory accesses, iii) 2ℓ rd d ℓ d summations, and iv) ℓ rd d 2 multiplications for the linear layers; and if segment embeddings are needed another ℓ d ℓ d memory accesses and ℓ d summations. This balance of parameter efficiency with a slight increase in activation memory illustrates the Nano Embedder ability to reduce the overall model size while maintaining embedding functionality. It is particularly advantageous in resource-constrained scenarios or when prioritizing higher dictionary sizes for improved performance. B.3 BERT Efficient In [41], the authors introduce Efficient Attention, an approach to effectively halve the parameter count of the Standard Attention mechanism. Instead of using three fully connected layers at the beginning of the layer and one at the output, Efficient Attention employs only one fully connected layer to generate the Query matrix. The Key and Value matrices are directly taken as the input. A single fully connected layer processes the attention output - both fully connected layers have dimensions d2. The total number of parameters, Weatt, for the Efficient Attention layers is calculated as: Weatt 2d2 (5) Like the Standard Attention layer, the Efficient Attention layer s highest activation size occurs during the matrix multiplication step. This step produces an activation size of: Aeatt 2ℓ d ℓ2 (6) To calculate the operations and memory accesses required, the expressions from the Standard Attention layer can be simplified by omitting terms associated with the two additional fully connected layers, as well as the h (attention heads) term from all equations (see Table 14). As shown in [41], these modifications are such that not hinder the effective modeling capabilities of the attention layer. It retains similar contextual performance while significantly reducing computational cost, making it highly advantageous for resource-constrained scenarios. 19 Input ATTENTION Q O fc fc fc Input softmax EFFICIENT ATTENTION K V Q l d W l l l d l d l d l d K l d V l d l d l d O fc l d softmax W l l h Figure 4: Memory layout of the Attention (left) and Efficient Attention (right) layers. ℓis the sentence length, d is the embedding dimension, and the trapezoid labeled fc denotes fully connected layers. The dashed gray box highlights the operations requiring the maximum activation size. Efficient Attention significantly reduces activation size. B.4 EmbBERT-Q This section focuses on analyzing the memory and computational costs of the Efficient Encoder block of EmbBERT-Q. Its first path consists in Efficient Attention, so we consider its weight count and activation size, and introduce the modifications due to the Convolutional Skip Connection and the weighted sum mechanisms. For the weights, we include the contributions from the Convolutional and Fully Connected layers. However, the four vectors used for the weighted sum during training do not contribute to the final memory footprint, as they can be discarded and replaced by two weights computed at runtime. This results in: WEffEnc 2d2 d2 α k d α (7) For the activations, we only need to consider the maximum between those resulting from the Efficient Attention and those from the Convolutional Skip component. The new component requires an activation size of at most d ℓ(2 α), which arises from the processing of the fully connected layer and the attention result that must be retained in memory. This results in a total activation size of: AEffEnc max(2d ℓ ℓ2; d ℓ(2 α)) (8) For the computational complexity, we start with the operations required by the Efficient Attention and add those introduced by the Convolutional Skip Connection. The convolution step requires d l (k 1) memory accesses, along with d l k sums and multiplications. Next, the SiLU activation requires d l memory accesses, approximately d l summations, and 4d l multiplications. 20 Table 13: Formulas for calculating weights and activation sizes per layer, based on their architectural parameters. Layers Weights Activations Embedder (v ℓ 2) d 2d ℓ NanoEmbedder rd (v ℓ 2d) 2d rd ℓ 2d ℓ Normalization 2d 2d ℓ Feed Forward 2d2 α d ℓ (2α) Attention 4d2 4d ℓ ℓ2 h Efficient Attention 2d2 2d ℓ ℓ2 Eff Attention Conv Skip 2d2 d2 α c dα max(2d ℓ ℓ2; d ℓ(2 α)) Table 14: Formulas for calculating memory accesses, summations and multiplication performed by each layer, based on their architectural parameters. Layers Memory accesses Summations Multiplications Embedder ℓ (4d 2) 2d ℓ 2d 0 NanoEmbedder ℓ (rd 4d d 2rd 2) 2d 2ℓ d (rd 1) ℓ rd d 2 Normalization (ℓ 1) d 2 ℓ d ℓ d Feed Forward 4ℓ d (d α 1) 2ℓ d (d α 1) 2ℓ d (d α 1) Attention 8ℓ d2 2ℓ2 h (2d 1) ℓ d (4d 1) ℓ2 h (2d 1) 4ℓ d2 ℓ2 h(2d 3) Efficient Attention 4ℓ d2 4ℓ2 d 2ℓ2 2ℓ d2 2ℓ2 d ℓ2 ℓ d 2ℓ d2 2ℓ2 d 2ℓ2 Eff Diff Skip Attention ℓ d(4d 4ℓ 2d α k 4) 2ℓ2 ℓ d(2d 2ℓ d α k 3) ℓ2 ℓ d(2d 2ℓ d α k 7) ℓ2 MAMBA main ℓ i (d 6 9) ℓ d c SSM ℓ i (d 3 2 c) ℓ d SSM ℓ i (d 3 5 c) SSM SSM ℓ i (ds 18 ρ 4 8) i ℓ i (ds 4 ρ 2 2) ℓ i (ds 7 ρ 2 2) The fully connected layer introduces 2d2 α ℓmemory accesses, along with half as many summations and multiplications (d2 α ℓ). Finally, the aggregation step requires 2d ℓmultiplications and memory accesses, with only half as many summations. B.5 Other blocks In this section, we synthetically review the two main other blocks required for a complete analysis of the Transformer BERT architecture. Feed Forward block This block, typically appended to the Attention or Efficient Attention layers after a normalization step, consists of two fully connected layers with size d2 α. These layers sequen- tially increase decrease the embedding dimension by a factor ofα with an activation function applied in between, followed by a skip connection. The total parameter count for this block is: Wff 2d2 α (9) and activation size of: Aff 2ℓ d ℓ d α. (10) Overall, the Feed Forward block requires 4ℓ d (d α 1) memory accesses and half as many summations and multiplications, with the majority of these operations required by the matrix multiplications in the fully connected layers. Normalization Layer The normalization layer performs simple layer-wise normalization, requir- ing 2 d parameters to store the mean and variance values. During inference, it performs (ℓ 1) d 2 memory accesses and ℓ d summations and multiplications. The required activation size is ℓ d 2, which is negligible compared to the activation sizes of the more computationally intensive layers. 21 References [1] H. Kopetz, W. Steiner, Internet of things (2022). doi:10.1007 978-3-031-11992-7_13. URL [2] A. Ometov, V. Shubina, L. Klus, J. Skibi nska, S. Saafi, P. Pascacio, L. Flueratoru, D. Q. Gaibor, N. Chukhno, O. Chukhno, et al., A survey on wearable technology: History, state-of-the-art and current challenges, Computer Networks 193 (2021) 108074. [3] J. Lin, W.-M. Chen, Y. Lin, J. Cohn, C. Gan, S. Han, Mcunet: Tiny deep learning on iot devices, arXiv:2007.10319 [cs] (11 2020). URL [4] Resource-efficient neural networks for embedded systems (2024). URL [5] J. Lin, L. Zhu, W.-M. Chen, W.-C. Wang, S. Han, Tiny machine learning: progress and futures [feature], IEEE Circuits and Systems Magazine 23 (3) (2023) 8 34. [6] C. Cioflan, L. Cavigelli, M. Rusci, d. Prado, L. Benini, On-device domain learning for keyword spotting on low-power extreme edge embedded systems (2024). URL [7] M. Rusci, T. Tuytelaars, On-device customization of tiny deep learning models for keyword spot- ting with few examples, IEEE Micro 43 (2023) 50 57. doi:10.1109 mm.2023.3311826. URL [8] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen, Mobilenetv2: Inverted residuals and linear bottlenecks (2018). URL [9] Y. Li, Y. Chen, X. Dai, D. Chen, M. Liu, L. Yuan, Z. Liu, L. Zhang, N. Vasconcelos, Micronet: Towards image recognition with extremely low flops (2020). URL [10] M. Tan, R. Pang, Q. V. Le, Efficientdet: Scalable and efficient object detection, arXiv:1911.09070 [cs, eess] (07 2020). URL [11] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional trans- formers for language understanding (10 2018). URL [12] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, Q. V. Le, Xlnet: Generalized autore- gressive pretraining for language understanding (2019). URL [13] V. Sanh, L. Debut, J. Chaumond, T. Wolf, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter (2019). URL [14] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, O. Levy, Spanbert: Improving pre-training by representing and predicting spans, arXiv:1907.10529 [cs] (01 2020). URL [15] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, Albert: A lite bert for self- supervised learning of language representations, arXiv:1909.11942 [cs] (02 2020). URL [16] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoy- anov, Roberta: A robustly optimized bert pretraining approach (07 2019). URL 22 [17] A. Gu, T. Dao, Mamba: Linear-time sequence modeling with selective state spaces (12 2023). doi:10.48550 arXiv.2312.00752. URL [18] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, D. Zhou, Mobilebert: a compact task-agnostic bert for resource-limited devices (2020). URL [19] Y. Tang, Y. Wang, J. Guo, Z. Tu, K. Han, H. Hu, D. Tao, A survey on transformer compression (04 2024). doi:10.48550 arXiv.2402.05964. URL [20] I. Turc, M.-W. Chang, K. Lee, K. Toutanova, Well-read students learn better: On the importance of pre-training compact models, arXiv preprint arXiv:1908.08962 (2019). [21] K. Maity, A. T. Chaulwar, V. Vala, R. S. Guntur, Nanobert: An extremely compact language model, in: Proceedings of the 7th Joint International Conference on Data Science Management of Data (11th ACM IKDD CODS and 29th COMAD), 2024, pp. 342 349. [22] Z. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, S. Yan, Convbert: Improving bert with span-based dynamic convolution, arXiv (Cornell University) (08 2020). doi:10.48550 arxiv.2008.02496. [23] S. Kim, A. Gholaminejad, Z. Yao, M. Mahoney, E. K. Keutzer, I-bert: Integer-only bert quanti- zation, arXiv (Cornell University) (01 2021). doi:10.48550 arxiv.2101.01321. [24] K. Clark, M.-T. Luong, Q. V. Le, C. D. Manning, Electra: Pre-training text encoders as discrim- inators rather than generators (2020). URL [25] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: Low-rank adaptation of large language models, arXiv:2106.09685 [cs] (10 2021). URL [26] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt understands, too (2021). URL [27] Y. Lu, M. Shen, H. Wang, X. Wang, C. van Rechem, W. Wei, Machine learning for synthetic data generation: a review, arXiv preprint arXiv:2302.04062 (2023). [28] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, K. Keutzer, A survey of quantization methods for efficient neural network inference, in: Low-Power Computer Vision, Chapman and Hall CRC, 2022, pp. 291 326. [29] G. Hinton, Distilling the knowledge in a neural network, arXiv preprint arXiv:1503.02531 (2015). [30] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, Advances in Neural Information Processing Systems 36 (2024). [31] C. Fields, C. Kennington, Exploring transformers as compact, data-efficient language models (12 2023). doi:10.18653 v1 2023.conll-1.35. URL [32] HuggingFace, Bitsandbytes (2024). URL [33] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, S. Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, in: The IEEE International Conference on Computer Vision (ICCV), 2015, pp. . [34] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue: A multi-task benchmark and analysis platform for natural language understanding, arXiv:1804.07461 [cs] (02 2019). URL 23 [35] P. S. Xingkun Liu, Arash Eshghi, V. Rieser, Benchmarking natural language understanding ser- vices for building conversational agents, in: Proceedings of the Tenth International Workshop on Spoken Dialogue Systems Technology (IWSDS), Springer, Ortigia, Siracusa (SR), Italy, 2019, pp. xxx xxx. URL [36] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, C. Potts, Learning word vectors for sentiment analysis, in: Proceedings of the 49th Annual Meeting of the Association for Compu- tational Linguistics: Human Language Technologies, Association for Computational Linguistics, Portland, Oregon, USA, 2011, pp. 142 150. URL [37] E. Saravia, H.-C. T. Liu, Y.-H. Huang, J. Wu, Y.-S. Chen, CARER: Contextualized affect repre- sentations for emotion recognition, in: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Brussels, Belgium, 2018, pp. 3687 3697. doi:10.18653 v1 D18-1404. URL [38] X. Zhang, J. J. Zhao, Y. LeCun, Character-level convolutional networks for text classification, in: NIPS, 2015, pp. 0 1. [39] J. Wang, K. Fu, C.-T. Lu, Sosnet: A graph convolutional network approach to fine-grained cyberbullying detection (2020). URL [40] I. Manotas, N. P. A. Vo, V. Sheinin, LiMiT: The literal motion in text dataset, in: Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020, pp. 991 1000. doi:10.18653 v1 2020.findings-emnlp.88. URL [41] M. Hosseini, P. Hosseini, You need to pay better attention: Rethinking the mathematics of attention mechanism (2024). URL [42] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems (2017). 24\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nEmbBERT-Q: Breaking Memory Barriers in Embedded NLP Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino , and Manuel Roveri Department of Electronics, Information and Bioengineering, Politecnico di Milano, Via Ponzio 34 5, Milano, 20133, Italy Abstract Large Language Models (LLMs) have revolutionized natural language processing, setting new standards across a wide range of applications. However, their relevant memory and computa- tional demands make them impractical for deployment on technologically-constrained tiny devices such as wearable devices and Internet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a novel tiny language model specifically designed for tiny devices with stringent memory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in Natural Language Processing tasks in this scenario, with a total memory footprint (weights and activations) of just 781 kB, representing a 25 reduction in size with respect to SotA models. By combining ar- chitectural innovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently outperforms several baseline models scaled down to a 2 MB memory budget (i.e., the maximum memory typically available in tiny devices), including heavily compressed versions of BERT and MAMBA. Extensive experimental evaluations on both a selected benchmark dataset, TinyNLP, specifically curated to evaluate Tiny Language Models in NLP tasks and real-world scenarios, and the GLUE benchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with re- spect to existing approaches, achieving an unmatched balance between memory and performance. To ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at 1 Introduction The proliferation of Internet-of-Things (IoT) systems and the availability of off-the-shelf, energy- efficient pervasive technologies have sparked a growing demand for intelligent on-device computa- tion [1]. Tiny devices such as microcontrollers and wearables are now key stringent technological solutions in relevant application scenarios ranging from smart homes to industrial automation [2].\n\n--- Segment 2 ---\nTo ensure the complete and immediate reproducibility of all our results, we release all code, scripts, and model checkpoints at 1 Introduction The proliferation of Internet-of-Things (IoT) systems and the availability of off-the-shelf, energy- efficient pervasive technologies have sparked a growing demand for intelligent on-device computa- tion [1]. Tiny devices such as microcontrollers and wearables are now key stringent technological solutions in relevant application scenarios ranging from smart homes to industrial automation [2]. De- spite significant advances in their memory and processing capabilities, these tiny devices still operate under stringent constraints, making very challening the design and development of machine (ML) and deep learning (DL) models meant to be executed on these devices [3, 4]. This need pushed the re- search field of Tiny Machine Learning (TinyML), which enables efficient ML DL inference on devices characterized by limited memory (e.g., less than 2 MB memory), low processing power (e.g., less than 100 MHz CPUs), and strict energy budgets [5], hence allowing tiny devices to locally process data, reducing latency, improving the real-time responsiveness of smart applications, and enhancing privacy by keeping sensitive data on-device. While TinyML has made strides in areas like keyword spotting [6, 7], image classification [8, 9], and object detection [10], deploying natural language processing (NLP) models on tiny devices remains a significant challenge. Modern decoder-based Large Language Models (LLMs) relying on the attention mechanism, such as BERT [11], XLNet [12], DistilBERT [13], SpanBERT [14], ALBERT [15], RoBERTa [16] and State-Space-Models (SSMs) such as MAMBA [17], rely on millions or even billions of parameters and extensive memory resources to achieve SotA accuracy across a wide range of NLP 1 arXiv:2502.10001v1 [cs.CL] 14 Feb 2025 tasks. Even scaled-down variants like MobileBERT (25.3M parameters) [18] are orders of magnitude too large for deployment on microcontrollers with less than 2 MB memory budgets. To fill this gap, we introduce EmbBERT-Q, a Tiny Language Model (TLM), specifically designed to operate on tiny devices, and under the stringent 2 MB memory budget.\n\n--- Segment 3 ---\nEven scaled-down variants like MobileBERT (25.3M parameters) [18] are orders of magnitude too large for deployment on microcontrollers with less than 2 MB memory budgets. To fill this gap, we introduce EmbBERT-Q, a Tiny Language Model (TLM), specifically designed to operate on tiny devices, and under the stringent 2 MB memory budget. In particular EmbBERT-Q, comprises a novel TLM architecture optimized for microcontroller units and other resource-constrained devices. Using techniques such as hardware-compatible 8-bit quantization [19], EmbBERT-Q achieves SotA performance with only 781 kB of memory, a 25 reduction in memory with respect to the SotA models characterized by the smallest memory demands, such as BERT-Tiny [20]. Even when compared with other models specifically adapted to work within the 2 MB constraints, EmbBERT-Q resulted to be both the most effective and the most efficient model. Our contributions include: 1. EmbBERT-Q: We propose a new TLM model specifically designed for tiny devices, combining efficiency and effectiveness. 2. Memory and Computational Analysis: We analytically evaluate the memory usage and compu- tational complexity of EmbBERT-Q and its components, providing a useful tool to evaluate the weights and activations memory trade-offs required to operate within tiny device constraints. 3. Custom Benchmark: We design a specialized benchmark tailored to assess the NLP capabilities of TLMs, enabling consistent and fair evaluations in resource-constrained environments. The remainder of this paper is organized as follows. Section 2 reviews recent work on model compression and training techniques tailored for resource-constrained platforms, setting the stage for our contributions. Section 3 provides a detailed description of EmbBERT-Q, our proposed model for NLP in tiny devices, and includes precise calculations of memory requirements for its layers. In Section 4, we outline the experimental setup, including the training procedures and datasets used to validate our approach. Section 5 presents a comprehensive evaluation of our model on the TinyNLP benchmark suite and GLUE datasets, comparing downscaled versions of BERT and MAMBA, and highlighting the significant performance improvements achieved by EmbBERT-Q. Section 6 delves into an ablation study to assess the individual contributions of the architectural modules within EmbBERT- Q to its overall performance.\n\n--- Segment 4 ---\nSection 5 presents a comprehensive evaluation of our model on the TinyNLP benchmark suite and GLUE datasets, comparing downscaled versions of BERT and MAMBA, and highlighting the significant performance improvements achieved by EmbBERT-Q. Section 6 delves into an ablation study to assess the individual contributions of the architectural modules within EmbBERT- Q to its overall performance. Finally, Section 7 concludes the paper by discussing the experimental findings, exploring future research directions, and considering the broader implications of deploying TLMs in tiny devices. 2 Related Work This section introduces the SotA Small Language Models and the main techniques to reduce the memory and computational demand of LLMs. Small Language Models Several models, including BERT-Tiny [20], NanoBERT [21], Mobile- BERT [18], ConvBERT [22], and I-BERT [23] are considered at the SotA for Small Language Models each leveraging different techniques to reach their desired level of compression. BERT-Tiny [20] employs a shallow architecture with fewer layers and reduced hidden dimensions with respect to conventional BERT architectures, trading accuracy for a smaller size and lower compu- tational costs (4.4M parameters corresponding to a 20 MB memory footprint). It employs a knowledge distillation approach from a larger teacher model to a compact student model, achieving competitive accuracy through task-specific and data-augmented distillation. NanoBERT [21] represents, in current literature, the smallest model built for tiny devices. It manages to achieve it s astounding results by introducing a Nano Embedder, a clever mechanism that aims to reduce the memory footprint of the standard embedder. Still, with weights in the range 700-800k, experimental results confined to a few datasets and missing code and checkpoints it falls behind other more prominent publications. MobileBERT [18], on the other hand, employs bottleneck structures and a teacher-student training to compress BERT, retaining high accuracy at the expense of a higher number of weights (25.3M), making it better suited for edge devices than actual tiny devices. ConvBERT [22], with it s small size of 14M parameters, introduces convolutional operations in self- attention to reduce computational complexity and efficently capture local dependencies. I-BERT [23] 2 employs integer-only quantization, guaranteeing that all computations, including softmax and matrix multiplications, rely on integer arithmetic.\n\n--- Segment 5 ---\nConvBERT [22], with it s small size of 14M parameters, introduces convolutional operations in self- attention to reduce computational complexity and efficently capture local dependencies. I-BERT [23] 2 employs integer-only quantization, guaranteeing that all computations, including softmax and matrix multiplications, rely on integer arithmetic. This allows significantly improve memory and energy efficiency without compromising accuracy, despite its larger size of 355M parameters. Advanced Training and Pre-Training Techniques Training schemes beyond traditional masked language modeling (MLM) [11] play a crucial role in enhancing the efficiency and performance of smaller models. Techniques like ELECTRA generative-discriminative training [24] and LoRA low-rank updates [25] enable fine-tuning with reduced trainable parameters, optimizing computational demands while retaining accuracy. Additionally, data augmentation and synthetic text generation [26, 27] expand training datasets, enhancing knowledge transfer and improving small-model generalization. These methods are particularly relevant for small language models, as they provide mechanisms to further reduce resource requirements while maintaining competitive performance. For instance, generative-discriminative training, such as in ELECTRA, efficiently trains models to distinguish be- tween real and synthetically generated tokens, leading to faster convergence. Similarly, LoRA low-rank parameter updates focus computational resources on task-specific fine-tuning rather than full model retraining, an advantage for tiny devices. Quantization and Knowledge Distillation Quantization, a cornerstone of model compression, converts weights and activations to lower-precision formats, such as 8-bit integers, drastically reducing memory usage and improving inference efficiency [28]. Integer arithmetic, as demonstrated by I- BERT [23], aligns well with embedded hardware capabilities. Knowledge distillation [29], employed in models like Bert-Tiny and MobileBERT, further reduces model size by training smaller networks to replicate larger teacher models behavior. Exploring Alternative Architectures Beyond transformers, alternative architectures like recur- rent neural networks (RNNs) with state-space models (SSMs) represent a viable option for tiny devices. For example, MAMBA [17], with 140M parameters, leverages RNNs to mitigate the attention quadratic complexity, offering efficient text generation solutions that could be even tailored for environments with limited parallelism capabilities due to it s inherent recursive structure.\n\n--- Segment 6 ---\nExploring Alternative Architectures Beyond transformers, alternative architectures like recur- rent neural networks (RNNs) with state-space models (SSMs) represent a viable option for tiny devices. For example, MAMBA [17], with 140M parameters, leverages RNNs to mitigate the attention quadratic complexity, offering efficient text generation solutions that could be even tailored for environments with limited parallelism capabilities due to it s inherent recursive structure. Summing up, despite significant advancements across all these dimensions - innovative small-model designs, advanced training strategies, effective quantization techniques, and explorations of alternative architectures - most existing models fall short of meeting the stringent memory and computational constraints imposed by tiny devices. While NanoBERT achieves a parameter size of approximately 2 MB (excluding activation memory), its experimental limitations and performance deficits highlight the challenges of extreme compression. Other models like ConvBERT (14M parameters) and I-BERT (355M parameters) demonstrate substantial improvements in their respective niches but remain un- suitable for the smallest hardware due to their larger memory footprints. Crucially, these works underscore the importance of a multifaceted approach to compression. Small model designs benefit from integrating knowledge distillation and quantization for immediate memory reduction, while advanced training techniques like ELECTRA or LoRA further refine performance by leveraging enhanced pre-training and fine-tuning efficiency. Simultaneously, alternative architectures like MAMBA suggest promising directions for bypassing inherent transformer limitations, particularly for devices with limited computational parallelism. Our work builds on these foundational insights to propose tailored architectural innovations and memory-optimized techniques, advancing the SotA for embedded NLP applications. By synthesizing these diverse strategies, we aim to deliver models that not only push the limits of transformer down- scaling but also satisfy the most stringent resource requirements, bridging the gap between theoretical advancements and real-world applicability. 3 EmbBERT-Q: A Novel Language Model for Tiny Devices EmbBERT-Q is a compact and efficient language model designed specifically for deployment in memory- constrained environments.\n\n--- Segment 7 ---\nBy synthesizing these diverse strategies, we aim to deliver models that not only push the limits of transformer down- scaling but also satisfy the most stringent resource requirements, bridging the gap between theoretical advancements and real-world applicability. 3 EmbBERT-Q: A Novel Language Model for Tiny Devices EmbBERT-Q is a compact and efficient language model designed specifically for deployment in memory- constrained environments. Unlike conventional approaches that compromise either performance or 3 N softmax SiLU V K Q - EmbBERT-Q Normalization Conv Skip Eff Attention Tokens Embedded tokens Output Positional embedder fc fc W₁ W₂ W₃ Conv-1D Token embedder Ef icient Encoder Embedder Block ' λCS λEA Figure 1: Overview of the EmbBERT-Q architecture, a specialized Language Model designed for tiny devices and extreme resource constraints. The architecture features an Embedder block and multiple Efficient Encoder with an Efficient Attention block and a Convolutional Skip Connection block. Instead of a final sum to aggregate the two Efficient Encoder paths, EmbBERT-Q uses a weighted difference with learnable weights for enhanced performance. memory footprint, EmbBERT-Q strikes a fine balance by leveraging tailored architectural optimiza- tions and an 8-bit quantization scheme. These innovations make it particularly suited for real-world natural language processing tasks in TinyML scenarios, where resource efficiency is paramount. 3.1 The EmbBERT-Q Architecture EmbBERT-Q achieves its efficiency through a redesign of traditional encoder-based BERT models, prioritizing lightweight attention mechanisms and memory-aware component selection. This architec- ture focuses on minimizing memory usage without sacrificing accuracy, surpassing other lightweight models within similar constraints. The adoption of 8-bit weight quantization, along with half-precision activation quantization and parameter-efficient fine-tuning techniques [30], ensures robustness to quan- tization and maintains state-of-the-art performance despite the limited hardware capabilities. As illustrated in Fig.\n\n--- Segment 8 ---\nThe adoption of 8-bit weight quantization, along with half-precision activation quantization and parameter-efficient fine-tuning techniques [30], ensures robustness to quan- tization and maintains state-of-the-art performance despite the limited hardware capabilities. As illustrated in Fig. 1, EmbBERT-Q comprises two main modules (delineated by dashed gray lines): a Nano Embedder Block, responsible for generating compact yet expressive token representations called δs; and a sequence of N Efficient Encoder blocks integrating efficient attention, convolutional layers, and weighted difference aggregation to process embeddings with minimal memory overhead. Additionally, an optional Output Block can be appended to adapt the architecture for specific down- stream regression, classification or generative tasks. In the following sections, we explore the functionality and design of these core modules, highlighting the architectural innovations that make EmbBERT-Q a SotA solution for tiny devices. 3.1.1 The Embedder block The Embedder block in EmbBERT-Q is adapted from the Nano Embedder introduced in [21] (whose memory requirements are analyzed in B). The Embedder block generates a d-dimensional representation δ for each token in a vocabulary of size v using its token embedder. These embeddings are positioned in a d-dimensional space, reflecting semantic similarity among tokens. To capture word order, the model includes a positional encoding component that maps each token position (up to a maximum window size ℓ, usually referred to as sentence length) into a d-dimensional vector, ensuring that token order is understood alongside their meaning. Unlike the original Nano Embedder [21], our Embedder block employs a learned positional embedding, built on the same principles as the token embedder. This approach not only enhances representation quality but also reduces memory requirements compared to using a fixed 4 positional embedder. Additionally, segment embeddings, similar to those used in BERT, assign distinct d-dimensional vectors to differentiate between input segments (e.g., sentences), enabling the model to interpret relationships across boundaries. A key part of the Embedder block memory efficiency is in its use of dense layers after the embed- dings. Specifically, the Embedder maps input tokens into a reduced space of dimension rd, and the fully connected layer projects them into the desired d-dimensional space.\n\n--- Segment 9 ---\nA key part of the Embedder block memory efficiency is in its use of dense layers after the embed- dings. Specifically, the Embedder maps input tokens into a reduced space of dimension rd, and the fully connected layer projects them into the desired d-dimensional space. This results in a d ℓmatrix of embedded tokens δs for a sentence length ℓ. This approach significantly reduces the Embedder size while maintaining sufficient representational power for classification tasks. By decreasing mem- ory requirements for the embedding layer, the Embedder Block frees up space for other parts of the network, making it a crucial enabler for running language models on devices with limited memory. 3.1.2 The Efficient Encoder The core of EmbBERT-Q is its sequence of N Efficient Encoders shown in the right gray dashed block of Fig. 1, designed to balance expressive power with minimal resource requirements. The Efficient Encoder takes as input the embedded token matrix which gets first processed with a normalization layer, and successively by two blocks, operating in parallel: an Efficient Attention block and a Convolutional Skip Connection block. Let Norm( ) be the output of the Normalization layer of the Efficient Encoder, that becomes the input of both the Efficient Attention and the Convolutional Skip Connection blocks. The Efficient Attention block operates through a sequence of matrix multiplications involving the Query (Q), Key (K), and Value (V ) matrices. The Q matrix is defined as the output of a fully connected layer W1( ) processing , i.e., Q W1( ), while both K and V are identical to . Differently from other architectures using the attention mechanism, EmbBERT-Q uses always a number of attention heads h 1 for reduced activations and because higher head counts has been found by [31] to be less effective at this size. The Q matrix is multiplied by the K matrix, the resulting product undergoes a SoftMax activation, and it is multiplied by V . Finally, the obtained matrix is processed with a fully connected layer W2( ) obtaining the output EA of the Efficient Attention block: EA W2 Softmax Q K d V .\n\n--- Segment 10 ---\nThe Q matrix is multiplied by the K matrix, the resulting product undergoes a SoftMax activation, and it is multiplied by V . Finally, the obtained matrix is processed with a fully connected layer W2( ) obtaining the output EA of the Efficient Attention block: EA W2 Softmax Q K d V . The parallel Convolutional Skip Connection block takes as input the same matrix provided to the Efficient Attention block, processing it through a 1D convolutional (Conv1D) layer, with convolutional kernel size k, that expands the dimension of from d to d α, where α is a scaling factor. The result of this operation is followed by a SiLU activation function and a fully connected layer, that restores the original dimension d, obtaining the final output CS of the block: CS W3(SiLU(Conv1D( ))). The outputs EA and CS get combined through a weighted difference with learned weights λEA and λCS, to obtain the output of the Efficient Encoder: EA λEA CS λCS, that then serves as input to the next block, i.e. another Efficient Encoder or the final output block. By integrating the Convolutional Skip Connection, the Efficient Attention, and the weighted differ- ence, our Efficient Encoder block simultaneously optimizes architectural depth and memory efficiency. This is accomplished by eliminating separate normalization and feed-forward layers, while leveraging lightweight operations such as 1D convolutions and single-head attention. These novel design leads to a substantial reduction in both the weights and activation footprints of each components, enabling the exploration of diverse architectural configurations. This flexibility facilitates exploring trade-offs among embedding size, attention capacity, and memory usage, effectively balancing accuracy with deployability for EmbBERT-Q on devices constrained by memory limits of 1-2 MB. 3.1.3 Quantizing the EmbBERT-Q model Finally, we combine architectural efficiency with an hardware-compatible quantization scheme. To this aim we employ an 8-bit block-wise quantization, showing that our EmbBERT-Q model has minimal or 5 Table 1: Formulas for calculating the weights and activation sizes of the blocks and components of EmbBERT-Q.\n\n--- Segment 11 ---\n3.1.3 Quantizing the EmbBERT-Q model Finally, we combine architectural efficiency with an hardware-compatible quantization scheme. To this aim we employ an 8-bit block-wise quantization, showing that our EmbBERT-Q model has minimal or 5 Table 1: Formulas for calculating the weights and activation sizes of the blocks and components of EmbBERT-Q. Layers Weights Activations Embedder block rd (v ℓ 2d) 2d rd ℓ 2d ℓ Norm layer 2d 2d ℓ Eff. Attention block 2d2 2d ℓ ℓ2 Conv Skip block d2 α k d α d ℓ(2 α) Efficient Encoder 2d 2d2 d2 α k dα max(2d ℓ ℓ2; d ℓ(2 α)) no performance degradation with respect to the 32-bit model, at a small fraction of the memory cost. This approach applies 8-bit floating-point representation to weights within a range of 6 [32]. Weights outside this range are stored in 16-bit floating-point (FP16) format to ensure numerical stability for extreme values. Additionally, all activations, initially in 32-bit floating-point (FP32), are converted to FP16. Further parameter-efficient fine-tuning (PEFT) is performed for additional two epochs with the 8-bit AdamW optimizer and a fixed learning rate of 1 10 4. We modify only a small subset of pa- rameters (approximately 8 of the total weights), focusing primarily on task-specific layers to improve performance while retaining the benefits of reduced precision. The quantization and fine-tuning process ensure that EmbBERT-Q is both computationally efficient and capable of delivering high performance on the target tasks. 3.2 Computing memory requirements of EmbBERT-Q An accurate computation of the total memory requirements for running EmbBERT-Q models is re- quired in severely resource-constrained settings.\n\n--- Segment 12 ---\nThe quantization and fine-tuning process ensure that EmbBERT-Q is both computationally efficient and capable of delivering high performance on the target tasks. 3.2 Computing memory requirements of EmbBERT-Q An accurate computation of the total memory requirements for running EmbBERT-Q models is re- quired in severely resource-constrained settings. The total memory, Mtot, is the sum of the memory needed for its weights and activations, and can be expressed as: Mtot (Wemb N Wenc) Pw max(Aemb, Aenc) Pa, where Wemb and Wenc are the weights of the embedder and encoder, respectively, and Aemb and Aenc are their respective activations. Pw and Pa denote the precision (in bits) used to store weights and activations. The formulas used to compute the memory required for the weights and activations of the com- ponents of EmbBERT-Q are reported in Table 1. The next subsections are dedicated to an in-depth analysis of their memory and computational requirements. 3.2.1 The Embedder block The Embedder block of EmbBERT-Q is composed of 5 smaller components: token embedder of size v rd, positional embedder of size ℓ rd, segment embedder of size 2d and two fully connected of size rd d (where v is the vocabulary size; ℓthe embedding size, and rd the reduced embedding size). Together, they result in a total parameter count: Wemb rd (v ℓ 2d) 2d, achieving a reduction in parameter size of almost a d rd factor with respect to a standard Embedder. However, the total activation size Aemb required by the Embedder block slightly increases due to the added projection step, resulting in: Aemb rd ℓ 2d ℓ. During inference, the Embedder block performs as a first step ℓ 2ℓ rd memory accesses for token and position encoding, followed by 4ℓ rd d memory accesses, 2ℓ rd d ℓ d summations, and 2ℓ rd d multiplications for the linear layers, and in case segment embeddings are needed another ℓ 2d ℓ d accesses and ℓ d summations.\n\n--- Segment 13 ---\nHowever, the total activation size Aemb required by the Embedder block slightly increases due to the added projection step, resulting in: Aemb rd ℓ 2d ℓ. During inference, the Embedder block performs as a first step ℓ 2ℓ rd memory accesses for token and position encoding, followed by 4ℓ rd d memory accesses, 2ℓ rd d ℓ d summations, and 2ℓ rd d multiplications for the linear layers, and in case segment embeddings are needed another ℓ 2d ℓ d accesses and ℓ d summations. 6 3.2.2 The Efficient Encoder We now proceed to analyze the memory usage and computational complexity of the three primary blocks that define the Efficient Encoder of EmbBERT-Q. The normalization layer s weights consist just of two vectors of size d representing the averages and standard deviations required for normalization. This block also has a minimal activation size, requiring only 2d ℓvalues. The operations within this layer necessitate 2d (ℓ 1) memory accesses, along with d ℓsummations and multiplications. From a memory perspective, the Efficient Attention block requires 2d2 weights and has an activation size of 2ℓ d ℓ2. While its memory demands are relatively small, this efficiency comes at the cost of a high computational complexity. Due to the various matrix multiplications and the softmax operation, the block requires: 4ℓ d2 4ℓ2 d 2ℓ2 memory accesses, 2ℓ d2 2ℓ2 d ℓ2 ℓ d summations and 2ℓ d2 2ℓ2 d 2ℓ2 multiplications. The Convolutional Skip Connection block, compared to the feed-forward block that typically fol- lows the standard attention layer and that is not present in EmbBERT-Q, features reduced memory requirements for the weights. This reduction arises from replacing the fully connected expansion layer with a Conv-1D expansion layer. Consequently, this block requires: d2 α k d α weights and d ℓ(2 α) values for activations.\n\n--- Segment 14 ---\nThis reduction arises from replacing the fully connected expansion layer with a Conv-1D expansion layer. Consequently, this block requires: d2 α k d α weights and d ℓ(2 α) values for activations. Its computational requirements are d ℓ(k 4 2d α) memory accesses, d ℓ(k 2 d α) sums and d ℓ(k 7 d α) multiplications. For the aggregation step, the block requires only 2d ℓmultiplications and memory accesses, with half as many summations. This step does not introduce any additional weights or activations. Summing up, the Efficient Encoder of EmbBERT-Q shown in Fig. 1 requires: Wenc 2d2 d2 α k d α 2d weights and has a total activation size of: Aenc max(2d ℓ ℓ2; d ℓ(2 α)). Overall, the self-attention mechanism s structured weighting of sequence elements enables robust con- textual representations, ensuring that tokens are influenced by all relevant inputs regardless of distance. Table 1 summarizes the memory requirements of the components in our proposed EmbBERT-Q archi- tecture. To further enhance the efficiency of this architecture, the implemented quantization strategy reduces the size of Pw from 4 Bytes (FP32) to approximately 1 Byte1. Similarly, the size of Pa is reduced from 4 Bytes (FP32) to exactly 2 Bytes (FP16). These optimizations resulted in a 2.5 reduction in memory consumption, significantly improving overall resource allocation. 3.3 Selecting the Architectural Parameters for EmbBERT-Q The selection of optimal architectural parameters for the EmbBERT-Q model is a critical step in ensuring both performance and memory efficiency. Key parameters such as vocabulary size v, sentence length ℓ, and embedding dimension d play a significant role in determining the model s memory usage and overall effectiveness. These parameters heavily influence the embedder memory occupation and cannot be significantly reduced without adversely affecting model accuracy. To adapt the model to the 2MB memory constraint, the first step involves identifying the smallest feasible values for v and ℓthat maintain acceptable performance. This study evaluated v within the range of 2000 to 10000 to maintain the model expressiveness, while ℓwas chosen between 256 and 512.\n\n--- Segment 15 ---\nTo adapt the model to the 2MB memory constraint, the first step involves identifying the smallest feasible values for v and ℓthat maintain acceptable performance. This study evaluated v within the range of 2000 to 10000 to maintain the model expressiveness, while ℓwas chosen between 256 and 512. The choice of ℓwas informed by the average sentence lengths in the selected datasets and their interplay with vocabulary size. The embedding dimension d and the reduced embedding dimension rd were then tuned. While these dimensions can be scaled down more aggressively, reducing d below 64 results in significant performance degradation, as shown in [31]. For rd, values between 16 and 32 were found to yield optimal results, aligning with the findings of [21]. Finally, structural parameters such as the scaling factor α and the number of layers N were fine- tuned to balance memory constraints with performance objectives, ensuring the model operated effec- tively within the given limitations. 1For the majority (92 ) of the weights, Pw is reduced to 1 Byte, while weights selected through the fallback procedure (8 ) are stored with a precision of 2 Bytes (FP16). 7 4 Experimental setup Table 2: Architectural parameters used for training model architectures. Columns represent: vocabulary size v, sentence length ℓ, embedding dimension d, reduced embedding dimension rd, forward expansion α, SSM internal memory size ds, convolutional kernel size k, number of heads h and number of layers.\n\n--- Segment 16 ---\n7 4 Experimental setup Table 2: Architectural parameters used for training model architectures. Columns represent: vocabulary size v, sentence length ℓ, embedding dimension d, reduced embedding dimension rd, forward expansion α, SSM internal memory size ds, convolutional kernel size k, number of heads h and number of layers. Hyperparameter v ℓ d rd α ds k h layers Weights Activations Total size BERT(2MB) [11] 2048 256 80 2 2 2 289 K 213 K 2,008 MB MAMBA(2MB) [17] 2048 256 64 1 6 4 5 220 K 265 K 1,941 MB Embedder 8192 256 320 32 1 293 K 164 K 1,826 MB Embedder conv 8192 256 320 32 16 1 298 K 164 K 1,848 MB EmbBERT-Q (ours) 8192 256 128 16 1 32 1 4 357 K 131 K 781 KB BERT-Tiny (20MB) [20] 32768 512 128 2 2 2 4.4 M 786 K 20,746 MB This section outlines our experimental protocol, focused on training EmbBERT-Q and several base- line models for comparison under a strict 2 MB memory budget. Our comprehensive and reproducible experimental campaign spans 6 different models trained across 17 datasets, providing insights into architecture design strategies that balance performance and memory efficiency. We detail the compar- ison models, training protocols, datasets, and evaluation metrics, setting the stage for the performance analysis in Section 5. 4.1 Baseline Models and Comparisons As a comparison to the proposed EmbBERT-Q model, we evaluated a diverse set of architectures, each constrained to a maximum memory footprint of 2 MB (including parameters and activations). Below, we summarize the key characteristics of these baseline models: 1. BERT(2MB): A scaled-down variant of the original BERT architecture [11], preserving the stan- dard embedding layers and encoder blocks. 2. MAMBA(2MB): A scaled-down adaptation of the MAMBA model [17], incorporating its native embedding mechanism and SSM-based recurrent blocks. 3. Embedder Only: This baseline leverages the Nano Embedder [21] without any mechanism for token interaction.\n\n--- Segment 17 ---\n3. Embedder Only: This baseline leverages the Nano Embedder [21] without any mechanism for token interaction. While not a fully functional language model, it highlights the parameter budget allocated to the embedding layer and evaluates the embedder standalone capability. 4. Embedder Conv: Extends the Embedder Only configuration by adding a lightweight 1D con- volutional layer. This enables local token interactions within a fixed-size context window, intro- ducing minimal parameter overhead. We further include BERT-Tiny [20], a minimal variant of BERT which is approximately 10 larger than the 2 MB models evaluated in this study, as a SotA reference point. Despite its significantly larger size, it serves as a useful benchmark for performance comparison. Each model incorporates a task-specific output layer, adapted to the dataset and classification task. Given its small size and high customizability, this layer memory contribution has been excluded from the calculations of effective memory usage. Table 2 presents a detailed overview of the architectural parameters, weight counts, and activation sizes for each model, including EmbBERT-Q, and illustrates their total memory footprint. Further details on ther architecture and on their memory and computational requirements can be found in B. 4.2 Training and pretraining This section outlines the procedures used for both the pretraining and finetuning of EmbBERT-Q and the baseline models discussed in this work. 8 Pretraining Protocol For models supporting BERT-style pretraining [11], we use the publicly available BookCorpus dataset [33]. After training a Byte Pair Encoding (BPE) tokenizer tailored to the required dictionary size of each model, we construct sentence pairs for Masked Language Modeling (MLM) and Next-Sentence Prediction (NSP). Sentence pairs are generated by pairing half of the tokenized sentences contiguously and the other half randomly. Within each pair, tokens are masked with a 1 6 chance, and masking is applied with the following probabilities: 70 of masked tokens are replaced with the MASK token; 15 are substituted with a random token; 15 are left unchanged. This masking strategy promotes contextual reasoning over random guessing. Pretraining is performed for one epoch with a batch size of 32 and a learning rate of 5 10 4 using the standard AdamW optimizer2.\n\n--- Segment 18 ---\nThis masking strategy promotes contextual reasoning over random guessing. Pretraining is performed for one epoch with a batch size of 32 and a learning rate of 5 10 4 using the standard AdamW optimizer2. Finetuning protocol Following pretraining, models are finetuned on target datasets for 10 epochs using a fixed3 learning rate of 3 10 4. Validation is conducted at the end of each epoch using the Matthews Correlation Coefficient (MCC) for classification tasks or the Spearman Correlation Coefficient (SCC) for regression tasks. The best-performing checkpoint, determined by the highest validation metric, is saved and subsequently used for final testing. Note that two models, namely Embedder and Embedder Conv, do not undergo pretraining. Due to their extremely simple architectural structure, these models cannot effectively absorb BERT-style pretraining. Instead, they are trained directly on the target datasets for 20 epochs, doubling the standard finetuning protocol to ensure they consume the same amount of computation as the other models. 4.3 Datasets In order to meaningfully compare the performance of EmbBERT-Q and the baseline models, we use two benchmark datasets: the TinyNLP benchmark (introduced in this paper) and GLUE [34]. In the following subsections we proceed to detail the tasks contained in both these benchmarks datasets, and the procedure for the train-validation-test splitting. 4.3.1 The TinyNLP benchmark To better evaluate TLMs in the real-world scenarios and resource-constrained environments they are ex- pected to operate, we introduce the TinyNLP benchmark. This curated collection of existing datasets is specifically tailored to the constrained yet practical applications of language models on embedded devices. Details of the dataset selection in the TinyNLP benchmark are presented in Table 3. The selection of these datasets represents application scenarios suited to models with restricted memory footprints, and is guided by the practical aim of assessing TLM deployment on embedded devices. Building on the discussion in Section 1, the TinyNLP benchmark focuses on tasks that are narrower in scope and less computationally demanding compared to standard (LLM) benchmarks. These tasks are grouped into three broad categories: i) Request Classification: Relevant to virtual assistants in TinyML contexts, these datasets involve discerning the type of user request (e.g., requests for information, action, or assistance).\n\n--- Segment 19 ---\nBuilding on the discussion in Section 1, the TinyNLP benchmark focuses on tasks that are narrower in scope and less computationally demanding compared to standard (LLM) benchmarks. These tasks are grouped into three broad categories: i) Request Classification: Relevant to virtual assistants in TinyML contexts, these datasets involve discerning the type of user request (e.g., requests for information, action, or assistance). As datasets focused on this kind of task, we have included nlu [35] and Snips in the TinyNLP benchmark. ii) Sentiment Analysis: Focuses on determining the emotional tone or opinion expressed in text. This commonly involves classifying content as positive, negative, or neutral, and sees wide usage in analyzing customer reviews or social media feedback. As datasets focused on this kind of task, we have included IMDb [36] and Emotion [37]. iii) Context Understanding: Involves identifying the broader context in which text is generated. For example, distinguishing whether the text describes a particular situation or environment. 2Due to the stringent memory constraints considered in this work, more complex pre-training strategies such as ELECTRA-style training [24] had to be excluded. ELECTRA requires both a generator and a discriminator, with the generator typically being about half the size of the discriminator. Under the strict 2 MB memory constraint, it is infeasible to construct a generator of sufficient size while maintaining a capable discriminator. 3To maintain consistency we omit complex learning rate schedulers, as different models may exhibit varying responses to specific schedules. Future work could systematically explore scheduling strategies for these models. 9 Table 3: Datasets selected as benchmarks for TinyML NLP, covering various tasks (TinyNLP). Name Size Classes Kind IMDb 50k 2 Sentiment analysis ag news 127.6k 4 Document classification cyberbull 46k 6 Racism classification LiMiT 24.6k 2 Movement presence Emotion 20k 6 Sentiment analysis nlu 25.7k 18 Request classification Snips 14.5k 7 Request classification As datasets focused on this kind of task, we have included ag news [38], cyberbull [39] and LiMiT [40].\n\n--- Segment 20 ---\n9 Table 3: Datasets selected as benchmarks for TinyML NLP, covering various tasks (TinyNLP). Name Size Classes Kind IMDb 50k 2 Sentiment analysis ag news 127.6k 4 Document classification cyberbull 46k 6 Racism classification LiMiT 24.6k 2 Movement presence Emotion 20k 6 Sentiment analysis nlu 25.7k 18 Request classification Snips 14.5k 7 Request classification As datasets focused on this kind of task, we have included ag news [38], cyberbull [39] and LiMiT [40]. 4.3.2 The GLUE benchmark The General Language Understanding Evaluation (GLUE) benchmark [34] is a widely adopted NLP benchmark comprising multiple, diverse and complex datasets, designed to test generalization and performance across diverse NLP tasks (see Table 4). It encompasses multiple subtasks, including sentiment classification and regression on sentence pairs. Because the official GLUE labels are only publicly released for the training and validation splits - and in line with prior approaches (e.g., [31]) - we treat the validation set as our test split throughout this study. 4.3.3 Train-Validation-Test Dataset Splittings For both the TinyNLP and GLUE benchmarks, each dataset is divided into training, validation, and test sets according to one of the following protocols, listed in order of priority: i) Provided splits: When the dataset creators supply official train, validation, and test splits, we use these directly to ensure consistency with prior work. ii) For datasets with only a single official split (e.g., train-test only), we designate the larger portion as the training set and the smaller portion as the test set. From the training set, we withhold 10 of the samples to create a validation set. iii) No provided splits: For datasets lacking any predefined splits, we partition the data into a 90-10 ratio for training and testing. Subsequently, 10 of the training set is withheld to create a validation set. 4.4 Evaluation The pretraining of all models on the BookCorpus dataset is conducted once, while the fine-tuning phase on each target dataset is repeated five times with different random seeds corresponding to different AdamW mini-batch shuffling, to ensure robustness of the results. Evaluation metrics are computed as the average of these five runs.\n\n--- Segment 21 ---\n4.4 Evaluation The pretraining of all models on the BookCorpus dataset is conducted once, while the fine-tuning phase on each target dataset is repeated five times with different random seeds corresponding to different AdamW mini-batch shuffling, to ensure robustness of the results. Evaluation metrics are computed as the average of these five runs. For the sake of simplicity, in the experimental results reported in Sec. 5, as evaluation metrics we focus on Accuracy for the TinyNLP benchmark, and on the metric used for computing the average Score in each dataset in the GLUE benchmark: SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Complete results across all evaluation metrics, 10 Table 4: Datasets from the GLUE benchmark [34], used for comparison with larger SotA models. Name Size Classes Kind cola 10.7k 2 grammatical semantical correctness mnli-m 403k 3 correct summarization mnli-mm 403k 3 correct summarization mrpc 5.8k 2 semantical equality qnli 116k 2 question answer entailment qqp 795k 2 concept repetition rte 5.77k 2 correct summarization sst2 70k 2 sentiment classification stsb 8.63k phrase similarity regression wnli 852 2 phrases entailment including Loss, Accuracy, F1 Score, Precision, Recall, and the Matthews Correlation Coefficient (MCC) for classification tasks, are reported in Appendix A. Average results for the two benchmark datasets were also calculated and are reported in the last column of Tables 5 and 6. Average accuracy was used as the average metric for the TinyNLP bench- mark; while the average score for the GLUE benchmark was computed following the standard GLUE protocol. 5 Experimental Results Table 5: Model performance on the TinyNLP benchmark, reporting accuracy for each individual dataset and overall averages. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average Acc.\n\n--- Segment 22 ---\n5 Experimental Results Table 5: Model performance on the TinyNLP benchmark, reporting accuracy for each individual dataset and overall averages. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average Acc. Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT(2MB) [11] 79,38 89,00 83,90 74,72 77,34 86,14 97,00 83,93 MAMBA(2MB) [17] 81,86 89,40 81,38 74,72 45,72 70,10 96,40 77,08 EmbBERT-Q (ours) 84,01 90,63 86,60 74,10 89,90 94,05 97,93 88,17 BERT-Tiny(20MB) [20] 85,69 91,93 83,38 72,40 88,86 88,53 98,16 86,99 We evaluate EmbBERT-Q and the baseline models on the TinyNLP and GLUE benchmark datasets, comparing their performance respectively in Tables 5 and 6, showing the superiority of EmbBERT-Q on both datasets and over all baselines, for what concerns both performance and memory requirements (shown in Table 2). We compare the performances of all models on both TinyNLP and GLUE under pre-trained and non-pre-trained conditions, as reported in the Tables of A. This comparison allows to make a significant observation: architectures equipped with attention mechanisms or SSMs can benefit more substantially from large, diverse pretraining corpora compared to simpler embedding-based approaches, but with the limitations given by the strict constraints of our 2 MB memory budget they still struggle even to reach comparable results to very simple models if not purposefully adjusted for the task. To account for the models different capabilities in leveraging pretraining, the results reported in Tables 5 and 6 refer to two different training scenarios: i) a non-pretrained protocol, for the Embedder 11 Table 6: Model performance on the GLUE benchmark.\n\n--- Segment 23 ---\nThis comparison allows to make a significant observation: architectures equipped with attention mechanisms or SSMs can benefit more substantially from large, diverse pretraining corpora compared to simpler embedding-based approaches, but with the limitations given by the strict constraints of our 2 MB memory budget they still struggle even to reach comparable results to very simple models if not purposefully adjusted for the task. To account for the models different capabilities in leveraging pretraining, the results reported in Tables 5 and 6 refer to two different training scenarios: i) a non-pretrained protocol, for the Embedder 11 Table 6: Model performance on the GLUE benchmark. Metrics are MCC for CoLA, F1 score for MRPC and QQP, Spearman s Correlation Coefficient (SCC) for STSB, and accuracy for the remaining tasks, as required for the official calculation of the overall GLUE score. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT(2MB) [11] -0,86 71,28 64,66 73,04 60,56 61,58 60,82 48,24 66,20 15,48 52,10 MAMBA(2MB) [17] 2,56 81,16 64,62 79,18 61,22 61,40 63,20 50,20 23,38 10,16 49,71 EmbBERT-Q (ours) 9,56 80,96 67,99 82,45 67,10 68,05 68,06 47,29 87,32 49,28 62,81 BERT-Tiny(20MB) [20] 0,00 83,20 71,10 62,20 70,20 70,30 81,50 57,2 62,30 73,60 63,16 and Embedder Convolution models, which lack architectural mechanisms to fully exploit pretraining; and ii) a pretrained and fine-tuned protocol, for all other models.\n\n--- Segment 24 ---\nMetrics are MCC for CoLA, F1 score for MRPC and QQP, Spearman s Correlation Coefficient (SCC) for STSB, and accuracy for the remaining tasks, as required for the official calculation of the overall GLUE score. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT(2MB) [11] -0,86 71,28 64,66 73,04 60,56 61,58 60,82 48,24 66,20 15,48 52,10 MAMBA(2MB) [17] 2,56 81,16 64,62 79,18 61,22 61,40 63,20 50,20 23,38 10,16 49,71 EmbBERT-Q (ours) 9,56 80,96 67,99 82,45 67,10 68,05 68,06 47,29 87,32 49,28 62,81 BERT-Tiny(20MB) [20] 0,00 83,20 71,10 62,20 70,20 70,30 81,50 57,2 62,30 73,60 63,16 and Embedder Convolution models, which lack architectural mechanisms to fully exploit pretraining; and ii) a pretrained and fine-tuned protocol, for all other models. The next sections provide a detailed analysis of results obtained on the TinyNLP and GLUE benchmarks. 5.1 TinyNLP Benchmark Results On the TinyNLP benchmark, where models are evaluated based on Accuracy, EmbBERT-Q demon- strates the best performance compared to the other models, achieving an Average Accuracy of 88.17 , as shown in Table 5. Notably, EmbBERT-Q outperforms BERT-Tiny, which requires around 25 more memory but only achieves the second-highest average Accuracy of 86.99 .\n\n--- Segment 25 ---\n5.1 TinyNLP Benchmark Results On the TinyNLP benchmark, where models are evaluated based on Accuracy, EmbBERT-Q demon- strates the best performance compared to the other models, achieving an Average Accuracy of 88.17 , as shown in Table 5. Notably, EmbBERT-Q outperforms BERT-Tiny, which requires around 25 more memory but only achieves the second-highest average Accuracy of 86.99 . Interestingly, the only models in the 2 MB range that offer comparable results were the Embedder and Embedder Conv configurations. Despite their seemingly simplistic design, these models perform well on the TinyNLP tasks, which generally feature shorter and less complex phrases with respect to the GLUE benchmark. For these tasks, the pretraining applied to the other models had a relatively limited impact, as shown in A. These results highlight the Embedder models ability to handle lightweight tasks effectively. The 2 MB down-scaled versions of the BERT and MAMBA models, on the other hand, score lower on Average Accuracy with respect to Embedder models, indicating that these models may be less suitable for environments with stringent memory budgets. This suggests that the overall architecture of EmbBERT-Q, with highly optimized embedding and attention structures, is particularly well-suited for the TinyNLP classification tasks in memory-constrained scenarios, with respect to down-scaled versions of standard models. 5.2 GLUE Benchmark Results On the GLUE benchmark, models were evaluated using various metrics, including Matthews Correla- tion Coefficient (MCC), F1 score, Spearman s Correlation Coefficient (SCC), and Accuracy, depending on the nature of each task. EmbBERT-Q once again emerges as the top-performing model within the 2 MB memory range, achieving an average score of 62.81. It demonstrates competitive performance across multiple tasks, excelling particularly in datasets such as WNLI (87.32 F1 score) and STSB (49.25 F1 score). Detailed results for the score metrics are presented in Table 6, along with the average computed across all datasets (in the last column), while complete results can be found in A.\n\n--- Segment 26 ---\nIt demonstrates competitive performance across multiple tasks, excelling particularly in datasets such as WNLI (87.32 F1 score) and STSB (49.25 F1 score). Detailed results for the score metrics are presented in Table 6, along with the average computed across all datasets (in the last column), while complete results can be found in A. Remarkably, EmbBERT-Q (781 kB) achieves a performance on GLUE comparable to the 25 larger BERT-Tiny model (20 MB) despite the substantial difference in memory requirements, even outperforming BERT-Tiny in 4 out of the 10 datasets included in the benchmark. The Embedder and Embedder Conv models again outperformed BERT and MAMBA down-scaled within the 2 MB range, but obtaining a significant 7-point difference in score compared to EmbBERT-Q, highlighting its superior performance with respect to the baselines. 5.3 Discussion of the Results The results from both the TinyNLP and GLUE benchmarks establish the proposed EmbBERT-Q as the current SotA Language Model for TinyML hardwares and NLP applications. Among the available 12 BERT (2MB) BERT NE BERT EA BERT NE EA EmbBERT EmbBERT-Q Models 0 250 500 750 1000 1250 1500 1750 2000 Memory Requirements (KB) 1156K 852K 1080K 892K 1292K 696K 1464K 524K 1428K 524K 518K 262K TinyNLP Memory occupations Weights Activations (K) 84 85 86 87 88 Accuracy ( ) 83.93 86.21 85.64 87.04 87.19 88.17 BERT (2MB) BERT NE BERT EA BERT NE EA EmbBERT EmbBERT-Q Models 0 250 500 750 1000 1250 1500 1750 2000 Memory Requirements (KB) 1156K 852K 1080K 892K 1292K 696K 1464K 524K 1428K 524K 518K 262K GLUE Memory Occupations Weights Activations (K) 52 54 56 58 60 62 64 Score ( ) 52.10 57.50 51.09 61.20 63.50 62.81 Figure 2: Number of weights (blue), number of activations (grey), and Accuracy Score results obtained by each model analyzed in Sec.\n\n--- Segment 27 ---\n5.3 Discussion of the Results The results from both the TinyNLP and GLUE benchmarks establish the proposed EmbBERT-Q as the current SotA Language Model for TinyML hardwares and NLP applications. Among the available 12 BERT (2MB) BERT NE BERT EA BERT NE EA EmbBERT EmbBERT-Q Models 0 250 500 750 1000 1250 1500 1750 2000 Memory Requirements (KB) 1156K 852K 1080K 892K 1292K 696K 1464K 524K 1428K 524K 518K 262K TinyNLP Memory occupations Weights Activations (K) 84 85 86 87 88 Accuracy ( ) 83.93 86.21 85.64 87.04 87.19 88.17 BERT (2MB) BERT NE BERT EA BERT NE EA EmbBERT EmbBERT-Q Models 0 250 500 750 1000 1250 1500 1750 2000 Memory Requirements (KB) 1156K 852K 1080K 892K 1292K 696K 1464K 524K 1428K 524K 518K 262K GLUE Memory Occupations Weights Activations (K) 52 54 56 58 60 62 64 Score ( ) 52.10 57.50 51.09 61.20 63.50 62.81 Figure 2: Number of weights (blue), number of activations (grey), and Accuracy Score results obtained by each model analyzed in Sec. 6 on the TinyNLP benchmark (left panel) and on the GLUE benchmark (right panel). LMs, it obtains the best experimentally observed balance between memory requirements and task performance. These results become even more significant when the memory requirements of competitor models are taken into account. EmbBERT-Q requires just 781 kB of total memory for both weights and activations, representing a reduction of approximately 2.4 compared to the other models we tested in the 2 MB range. The contrast is even more pronounced when compared to BERT-Tiny, the smallest SotA LM available in the literature, which demands nearly 25 more memory than EmbBERT-Q. 6 Evaluating the Impact of The Architectural Components of EmbBERT-Q In this section, we analyze the contributions of EmbBERT-Q architectural components to its overall performance on both the TinyNLP and GLUE benchmarks.\n\n--- Segment 28 ---\nThe contrast is even more pronounced when compared to BERT-Tiny, the smallest SotA LM available in the literature, which demands nearly 25 more memory than EmbBERT-Q. 6 Evaluating the Impact of The Architectural Components of EmbBERT-Q In this section, we analyze the contributions of EmbBERT-Q architectural components to its overall performance on both the TinyNLP and GLUE benchmarks. Taking BERT (2 MB) as a baseline, we systematically introduced improved and optimized key components, defining in this way EmbBERT-Q. Through this process, we evaluate the individual and collective impact of these changes, always adhering with the strict memory constraints of 2 MB. Figure 2 shows the memory requirements of various model configurations alongside their corre- sponding accuracy on TinyNLP and score on GLUE. Base model - BERT(2MB) BERT(2MB) is a compressed variant of BERT that serves as our vanilla baseline model. Compared to the larger BERT-Tiny model [20], which has a 10 memory footprint, BERT(2MB) shows a notable performance degradation that could be attributed to its re- duced parameter count. Despite these limitations, BERT(2MB) marks a first step toward adapting transformer architectures for ultra-low-memory environments, demonstrating the feasibility of scaling down LM models while maintaining some level of task performance. BERT Nano Embedder (BERT NE) To address the limitations of BERT(2MB), we replace its Embedder with the Nano Embedder [21], which is designed to optimize embedding representations without increasing the overall parameter count. This substitution expands the effective vocabulary space within the same memory budget, resulting in notable performance improvements on both the TinyNLP and GLUE benchmarks, as can be seen in Fig. 2. BERT Efficient Attention (BERT EA) To reduce activation overhead, we proceed to replace the default multi-head attention module with Efficient Attention [41], aiming to lower weight and activation memory costs. This reduction allows for increased embedding dimensionality and or additional layers. This modification significantly improves performance on the TinyNLP benchmark but, when not paired with other architectural modules, results in a slight decrease in performance with respect to the base BERT(2MB) model on the GLUE benchmark, as can be appreciated in Fig. 2.\n\n--- Segment 29 ---\nThis modification significantly improves performance on the TinyNLP benchmark but, when not paired with other architectural modules, results in a slight decrease in performance with respect to the base BERT(2MB) model on the GLUE benchmark, as can be appreciated in Fig. 2. 13 BERT NE EA We combine the Nano Embedder with Efficient Attention to create the BERT NE EA model, leveraging a broader vocabulary together with reduced weights and acti- vations overhead. This combination leads to a performance gain on both TinyNLP and GLUE tasks, where BERT NE EA achieves respectively an accuracy of 87.51 and a score of 61.20, i.e. re- spectively over 3 and 9 points with respect to the original BERT(2MB). These results highlight the advantage of combining embedding efficiency with an optimized attention mechanism in ultra-compact models. EmbBERT By finally integrating a parallel path with a Conv-1D followed by an Fully Connected layer we obtain the EmbBERT architecture. In this way, we add with a minimal memory overhead a modified feed forward block to the main attention path of BERT NE EA, i.e. a further simple token-mixing convolutional and fully connected layer. This addition provides a marginal improvement on the TinyNLP benchmark, but achieves significant success on the more complex tasks contained in the GLUE benchmark. Specifically, it improves performance by over 2 points in the GLUE score compared to the BERT NE EA model. EmbBERT-Q Finally, the EmbBERT model is quantized using the 8-bit post-training quantization procedure outlined in Section 3.1.3, resulting in the proposed EmbBERT-Q model. In the TinyNLP benchmark, EmbBERT-Q achieves an average accuracy of 88.17 , marking an improvement of nearly 1 percentage point over its unquantized counterpart. This increase can be attributed to the regularization effect that quantization may induce under certain conditions, combined with the relative simplicity of the tasks in this benchmark. On the GLUE benchmark, which evaluates broader natural language understanding tasks, EmbBERT-Q demonstrates exceptional robustness to quantization, achieving an overall GLUE score of 62.81. This represents a minimal performance drop of 0.7 percentage points compared to the unquantized EmbBERT version.\n\n--- Segment 30 ---\nOn the GLUE benchmark, which evaluates broader natural language understanding tasks, EmbBERT-Q demonstrates exceptional robustness to quantization, achieving an overall GLUE score of 62.81. This represents a minimal performance drop of 0.7 percentage points compared to the unquantized EmbBERT version. In Appendix A we show that the BERT NE EA model, instead, suffers significant performance drops of up to 15 percentage points on GLUE due to post-training 8-bit quantization. The quantization process gives substantial memory savings, reducing the total memory required to store and execute EmbBERT-Q from the around 2 MB of the unquantized EmbBERT to just 781 kB, considering both weights and activations (a 2.4 reduction in memory demand). While quantization often introduces trade-offs in accuracy, the robustness of the EmbBERT architecture highlights its suitability for deployment in constrained environments where such memory optimization techniques are critical. Through the comprehensive ablation study performed in this section, we have examined the contri- butions of key architectural components, including the Nano Embedder and Efficient Encoder. Main- taining the total memory usage below the 2 MB budget throughout the study, we have demonstrated that the inclusion of these architectural components and of 8-bit quantization in EmbBERT-Q leads to an Average Accuracy improvement of 4.24 percentage points on the TinyNLP benchmark and a 10.71 point increase on the GLUE benchmark score, with respect to the original BERT(2MB) model. With its carefully designed architecture and 8-bit quantization, EmbBERT-Q pushes the frontier of ultra-compact language models, delivering state-of-the-art performance in environments with stringent memory and computational constraints. 7 Conclusions In this work, we presented EmbBERT-Q, a novel language model specifically designed for tiny devices and extreme resource constraints. EmbBERT-Q achieves Accuracies and Scores comparable to the ones of models with up to 25 its memory footprint, and stands out as the highest-performing solution within the 2 MB memory budget explored in this paper. By leveraging an innovative architectural design specifically tailored for extremely memory- and computationally-constrained environments, EmbBERT-Q effectively balances parameter efficiency and competitive accuracy.\n\n--- Segment 31 ---\nEmbBERT-Q achieves Accuracies and Scores comparable to the ones of models with up to 25 its memory footprint, and stands out as the highest-performing solution within the 2 MB memory budget explored in this paper. By leveraging an innovative architectural design specifically tailored for extremely memory- and computationally-constrained environments, EmbBERT-Q effectively balances parameter efficiency and competitive accuracy. Its Embedder Block, Efficient Encoder, and the application of post-training 8-bit quantization significantly reduce the model s memory footprint while maintaining high performance both on the newly proposed TinyNLP benchmark and on the GLUE benchmark, crucially demonstrat- ing the effectiveness and the efficiency of EmbBERT-Q in resource-constrained environments. 14 Future research will explore further compression techniques, novel architectural designs, targeted knowledge distillation, and even more extreme quantizations tailored to emerging hardware accelerators (e.g., 1-bit quantization). Combining these advancements with next-generation hardware has the potential to further optimize model memory and computation footprints while preserving, or even enhancing, performance. Our work establishes a systematic foundation for designing efficient language models capable of operating effectively within the most stringent memory constraints. Acknowledgment This paper is supported by PNRR-PE-AI FAIR project funded by the NextGeneration EU program. A Complete results In this section, we provide the complete results of our experiments, spanning all datasets and models in both pretrained and non-pretrained contexts. The detailed results, concerning model quantization as well, are presented in Tables 7, 8, 9, 10, 11, and 12. A.1 Evaluation on the TinyNLP Benchmark For what concerns non-pretrained models, as shown in Table 7 BERT(2MB) and MAMBA(2MB) demonstrate compatible results with respect to other models, with average accuracies of 83.74 and 83.86 , respectively. Notably, Embedder Conv and Embedder outperform others models, achieving 86.50 and 86.41 average accuracy, respectively. The Embedder Conv model particularly excells on datasets like AG News, scoring 91.50 , the highest across the board for this task. When non-pretrained and evaluated with the MCC score, as shown in Table 8 the Embedder-based models again deliver competitive performances, with Embedder Conv achieving an average MCC of 78.49 .\n\n--- Segment 32 ---\nThe Embedder Conv model particularly excells on datasets like AG News, scoring 91.50 , the highest across the board for this task. When non-pretrained and evaluated with the MCC score, as shown in Table 8 the Embedder-based models again deliver competitive performances, with Embedder Conv achieving an average MCC of 78.49 . In contrast, BERT-based models generally lagged behind. EmbBERT and EmbBERT-Q achieved an MCC score of 80.59 , showing limited performance in the non-pretrained context. For what concerns instead pretrained and finetuned models on TinyNLP, as shown in Table 9, EmbBERT-Q consistently delivered the top Accuracy performance, achieving an average accuracy of 88.17 . BERT NE EA followed closely with a score of 87.04 . The MCC results shown in Table 10 reflect the Accuracy trends, with EmbBERT-Q leading, showing its robustness to 8-bit quantization across diverse datasets. A.2 Evaluation on the GLUE Benchmark The GLUE benchmark evaluates models on NLP tasks such as sentiment analysis (SST-2), natural language inference (MNLI), and semantic similarity (STSB). For non-pretrained models, as shown in Table 11 Embedder achieves a GLUE score of 56.92, followed closely by Embedder Conv at 56.41. EmbBERT surpasses both, showing (even if by a small margin in the non-pretrained context) its architectural superiority on difficult NLP tasks. Among pretrained models, as shown in Table 12, EmbBERT outperforms all variants by a significant margin, showing its superior ability to absorb pretraining with respect to all other models. EmbBERT-Q shows minimal accuracy degradation and robustness to 8-bit quantization. B Exact Computation of Memory and Computational Cost of LLM Layers The architecture of Large Language Models (LLMs) is primarily based on the Transformer model introduced by [42]. This architecture has revolutionized natural language processing by enabling models to effectively handle long-range dependencies in text. Encoder-based text classification models typically consist of two main components: an embedder and an encoder. The encoder, in turn, is primarily composed of an Attention Mechanism and a Feed-Forward Neural Network.\n\n--- Segment 33 ---\nEncoder-based text classification models typically consist of two main components: an embedder and an encoder. The encoder, in turn, is primarily composed of an Attention Mechanism and a Feed-Forward Neural Network. In this section, we provide a comprehensive analytical, layer-by-layer evaluation of the memory and computational requirements of common components used in Language Models, as well as an 15 Table 7: Accuracy of non-pretrained models on the TinyNLP benchmark. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 78,98 88,93 82,63 70,10 83,35 85,63 96,58 83,74 MAMBA(2MB) 78,18 91,08 83,74 71,66 77,40 87,60 97,34 83,86 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 82,52 90,86 82,96 69,82 78,34 84,06 96,74 83,61 BERT NE EA 81,60 90,53 82,30 55,57 83,70 84,90 96,50 82,16 EmbBERT 83,10 90,82 82,50 68,90 68,48 76,78 95,18 80,82 Table 8: Evaluation of MCC score of non-pretrained models on the TinyNLP benchmark.\n\n--- Segment 34 ---\nIn this section, we provide a comprehensive analytical, layer-by-layer evaluation of the memory and computational requirements of common components used in Language Models, as well as an 15 Table 7: Accuracy of non-pretrained models on the TinyNLP benchmark. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 78,98 88,93 82,63 70,10 83,35 85,63 96,58 83,74 MAMBA(2MB) 78,18 91,08 83,74 71,66 77,40 87,60 97,34 83,86 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 82,52 90,86 82,96 69,82 78,34 84,06 96,74 83,61 BERT NE EA 81,60 90,53 82,30 55,57 83,70 84,90 96,50 82,16 EmbBERT 83,10 90,82 82,50 68,90 68,48 76,78 95,18 80,82 Table 8: Evaluation of MCC score of non-pretrained models on the TinyNLP benchmark. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,10 85,25 79,45 40,13 78,35 84,05 96,00 74,48 MAMBA(2MB) 56,62 88,14 80,82 42,62 70,84 86,22 96,86 74,59 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 65,06 87,82 79,86 38,30 72,34 82,42 96,20 74,57 BERT NE EA 63,32 87,35 79,13 20,67 78,85 83,30 95,93 72,65 EmbBERT 66,32 87,78 79,26 38,42 60,78 74,66 94,40 71,66 Table 9: Accuracy of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets).\n\n--- Segment 35 ---\nModels IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 78,98 88,93 82,63 70,10 83,35 85,63 96,58 83,74 MAMBA(2MB) 78,18 91,08 83,74 71,66 77,40 87,60 97,34 83,86 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 82,52 90,86 82,96 69,82 78,34 84,06 96,74 83,61 BERT NE EA 81,60 90,53 82,30 55,57 83,70 84,90 96,50 82,16 EmbBERT 83,10 90,82 82,50 68,90 68,48 76,78 95,18 80,82 Table 8: Evaluation of MCC score of non-pretrained models on the TinyNLP benchmark. Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,10 85,25 79,45 40,13 78,35 84,05 96,00 74,48 MAMBA(2MB) 56,62 88,14 80,82 42,62 70,84 86,22 96,86 74,59 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 65,06 87,82 79,86 38,30 72,34 82,42 96,20 74,57 BERT NE EA 63,32 87,35 79,13 20,67 78,85 83,30 95,93 72,65 EmbBERT 66,32 87,78 79,26 38,42 60,78 74,66 94,40 71,66 Table 9: Accuracy of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets). Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 79,38 89,00 83,90 74,72 77,34 86,14 97,00 83,93 MAMBA(2MB) 84,76 90,68 81,20 73,98 74,58 73,24 93,42 81,69 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 81,86 89,40 81,38 74,72 45,72 70,10 96,40 77,08 BERT EA 80,46 89,46 84,58 74,12 85,78 87,44 97,62 85,64 BERT NE EA 83,19 90,80 84,13 75,80 88,70 88,88 97,79 87,04 BERT NE EA (8bit) 82,35 89,51 85,58 76,40 80,85 89,46 97,29 85,92 EmbBERT 84,10 90,46 83,97 76,36 89,58 88,16 97,67 87,19 EmbBERT-Q 84,01 90,63 86,60 74,10 89,90 94,05 97,93 88,17 Table 10: Evaluation of MCC score of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets).\n\n--- Segment 36 ---\nModels IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,10 85,25 79,45 40,13 78,35 84,05 96,00 74,48 MAMBA(2MB) 56,62 88,14 80,82 42,62 70,84 86,22 96,86 74,59 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 65,06 87,82 79,86 38,30 72,34 82,42 96,20 74,57 BERT NE EA 63,32 87,35 79,13 20,67 78,85 83,30 95,93 72,65 EmbBERT 66,32 87,78 79,26 38,42 60,78 74,66 94,40 71,66 Table 9: Accuracy of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets). Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 79,38 89,00 83,90 74,72 77,34 86,14 97,00 83,93 MAMBA(2MB) 84,76 90,68 81,20 73,98 74,58 73,24 93,42 81,69 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 81,86 89,40 81,38 74,72 45,72 70,10 96,40 77,08 BERT EA 80,46 89,46 84,58 74,12 85,78 87,44 97,62 85,64 BERT NE EA 83,19 90,80 84,13 75,80 88,70 88,88 97,79 87,04 BERT NE EA (8bit) 82,35 89,51 85,58 76,40 80,85 89,46 97,29 85,92 EmbBERT 84,10 90,46 83,97 76,36 89,58 88,16 97,67 87,19 EmbBERT-Q 84,01 90,63 86,60 74,10 89,90 94,05 97,93 88,17 Table 10: Evaluation of MCC score of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets). Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,88 85,38 80,86 47,14 71,26 84,74 96,50 74,97 MAMBA(2MB) 63,78 85,90 77,92 46,56 30,40 66,92 95,76 66,75 Embedder 65,18 88,13 80,10 41,10 86,23 88,30 97,58 78,09 Embedder Conv 68,15 88,68 80,45 40,45 86,18 88,13 97,38 78,49 BERT NE 66,64 87,54 80,94 45,98 83,62 85,12 97,54 78,20 BERT EA 61,08 85,94 81,66 47,86 81,64 86,10 97,26 77,36 BERT NE EA 68,04 88,08 82,60 51,24 85,62 86,46 97,06 79,87 BERT NE EA (8bit) 64,70 86,02 82,77 47,89 74,55 88,20 96,84 77,28 EmbBERT 68,25 87,31 80,87 48,10 86,26 86,76 97,29 79,26 EmbBERT-Q 68,04 87,51 84,10 46,84 86,71 93,36 97,59 80,59 16 Table 11: Evauation of non-pretrained models on the GLUE benchmark.\n\n--- Segment 37 ---\nModels IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 79,38 89,00 83,90 74,72 77,34 86,14 97,00 83,93 MAMBA(2MB) 84,76 90,68 81,20 73,98 74,58 73,24 93,42 81,69 Embedder 82,60 91,10 82,78 71,60 89,40 89,50 97,93 86,41 Embedder Conv 84,08 91,50 83,10 70,32 89,45 89,33 97,75 86,50 BERT NE 81,86 89,40 81,38 74,72 45,72 70,10 96,40 77,08 BERT EA 80,46 89,46 84,58 74,12 85,78 87,44 97,62 85,64 BERT NE EA 83,19 90,80 84,13 75,80 88,70 88,88 97,79 87,04 BERT NE EA (8bit) 82,35 89,51 85,58 76,40 80,85 89,46 97,29 85,92 EmbBERT 84,10 90,46 83,97 76,36 89,58 88,16 97,67 87,19 EmbBERT-Q 84,01 90,63 86,60 74,10 89,90 94,05 97,93 88,17 Table 10: Evaluation of MCC score of pretrained and finetuned models on the TinyNLP benchmark (Embedder and Embedder Conv are directly trained on the downstream datasets). Models IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,88 85,38 80,86 47,14 71,26 84,74 96,50 74,97 MAMBA(2MB) 63,78 85,90 77,92 46,56 30,40 66,92 95,76 66,75 Embedder 65,18 88,13 80,10 41,10 86,23 88,30 97,58 78,09 Embedder Conv 68,15 88,68 80,45 40,45 86,18 88,13 97,38 78,49 BERT NE 66,64 87,54 80,94 45,98 83,62 85,12 97,54 78,20 BERT EA 61,08 85,94 81,66 47,86 81,64 86,10 97,26 77,36 BERT NE EA 68,04 88,08 82,60 51,24 85,62 86,46 97,06 79,87 BERT NE EA (8bit) 64,70 86,02 82,77 47,89 74,55 88,20 96,84 77,28 EmbBERT 68,25 87,31 80,87 48,10 86,26 86,76 97,29 79,26 EmbBERT-Q 68,04 87,51 84,10 46,84 86,71 93,36 97,59 80,59 16 Table 11: Evauation of non-pretrained models on the GLUE benchmark. We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks.\n\n--- Segment 38 ---\nModels IMDb ag news cyberbull LiMiT Emotion nlu Snips Average BERT(2MB) 58,88 85,38 80,86 47,14 71,26 84,74 96,50 74,97 MAMBA(2MB) 63,78 85,90 77,92 46,56 30,40 66,92 95,76 66,75 Embedder 65,18 88,13 80,10 41,10 86,23 88,30 97,58 78,09 Embedder Conv 68,15 88,68 80,45 40,45 86,18 88,13 97,38 78,49 BERT NE 66,64 87,54 80,94 45,98 83,62 85,12 97,54 78,20 BERT EA 61,08 85,94 81,66 47,86 81,64 86,10 97,26 77,36 BERT NE EA 68,04 88,08 82,60 51,24 85,62 86,46 97,06 79,87 BERT NE EA (8bit) 64,70 86,02 82,77 47,89 74,55 88,20 96,84 77,28 EmbBERT 68,25 87,31 80,87 48,10 86,26 86,76 97,29 79,26 EmbBERT-Q 68,04 87,51 84,10 46,84 86,71 93,36 97,59 80,59 16 Table 11: Evauation of non-pretrained models on the GLUE benchmark. We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) 3,93 75,20 63,88 81,03 64,95 63,65 62,75 49,38 70,68 6,74 54,22 MAMBA(2MB) 7,22 80,60 60,72 80,66 64,27 64,44 59,78 51,76 80,00 4,58 55,40 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 5,22 77,34 63,18 81,12 64,14 64,53 66,76 51,14 85,62 4,72 56,38 BERT NE EA 2,66 78,68 61,90 83,16 62,65 61,63 62,36 48,38 85,62 8,94 55,60 EmbBERT 5,32 78,50 62,54 82,58 63,82 65,78 63,68 51,26 87,30 9,76 57,05 Table 12: Evaluation of pretrained and finetuned models on the GLUE benchmark (Embedder and Embedder Conv are directly trained on the donwstream datasets).\n\n--- Segment 39 ---\nWe report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) 3,93 75,20 63,88 81,03 64,95 63,65 62,75 49,38 70,68 6,74 54,22 MAMBA(2MB) 7,22 80,60 60,72 80,66 64,27 64,44 59,78 51,76 80,00 4,58 55,40 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 5,22 77,34 63,18 81,12 64,14 64,53 66,76 51,14 85,62 4,72 56,38 BERT NE EA 2,66 78,68 61,90 83,16 62,65 61,63 62,36 48,38 85,62 8,94 55,60 EmbBERT 5,32 78,50 62,54 82,58 63,82 65,78 63,68 51,26 87,30 9,76 57,05 Table 12: Evaluation of pretrained and finetuned models on the GLUE benchmark (Embedder and Embedder Conv are directly trained on the donwstream datasets). We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks.\n\n--- Segment 40 ---\nModels COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) 3,93 75,20 63,88 81,03 64,95 63,65 62,75 49,38 70,68 6,74 54,22 MAMBA(2MB) 7,22 80,60 60,72 80,66 64,27 64,44 59,78 51,76 80,00 4,58 55,40 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 5,22 77,34 63,18 81,12 64,14 64,53 66,76 51,14 85,62 4,72 56,38 BERT NE EA 2,66 78,68 61,90 83,16 62,65 61,63 62,36 48,38 85,62 8,94 55,60 EmbBERT 5,32 78,50 62,54 82,58 63,82 65,78 63,68 51,26 87,30 9,76 57,05 Table 12: Evaluation of pretrained and finetuned models on the GLUE benchmark (Embedder and Embedder Conv are directly trained on the donwstream datasets). We report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) -0,86 71,28 64,66 73,04 60,56 61,58 60,82 48,24 66,20 15,48 52,10 MAMBA(2MB) 2,56 81,16 64,62 79,18 61,22 61,40 63,20 50,20 76,62 10,16 55,03 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 9,04 78,82 65,04 79,96 63,08 63,30 63,20 51,76 87,30 13,46 57,50 BERT EA 10,06 79,44 66,48 78,88 60,82 60,82 63,50 50,76 22,24 17,92 51,09 BERT NE EA 18,70 79,60 65,36 82,66 67,06 67,44 67,44 53,66 86,74 23,34 61,20 BERT NE EA (8bit) 9,57 77,87 63,40 48,93 34,19 33,59 59,58 53,79 59,15 19,58 45,97 EmbBERT 11,01 79,33 69,19 83,25 67,83 68,63 68,92 49,96 87,61 49,25 63,50 EmbBERT-Q 9,56 80,96 67,99 82,45 67,10 68,05 68,06 47,29 87,32 49,28 62,81 overall view of their functionality.\n\n--- Segment 41 ---\nWe report SCC for STSB, MCC for CoLA, F1 score for QQP and MRPC, Accuracy for the remaining GLUE tasks. Models COLA SST-2 MRPC QQP MNLI-m MNLI-mm QNLI RTE WNLI STSB Score BERT(2MB) -0,86 71,28 64,66 73,04 60,56 61,58 60,82 48,24 66,20 15,48 52,10 MAMBA(2MB) 2,56 81,16 64,62 79,18 61,22 61,40 63,20 50,20 76,62 10,16 55,03 Embedder 9,65 78,90 62,25 83,28 62,06 62,17 65,40 52,73 77,20 15,58 56,92 Embedder Conv 9,25 79,10 60,50 82,98 61,98 60,93 62,08 52,00 79,16 16,10 56,41 BERT NE 9,04 78,82 65,04 79,96 63,08 63,30 63,20 51,76 87,30 13,46 57,50 BERT EA 10,06 79,44 66,48 78,88 60,82 60,82 63,50 50,76 22,24 17,92 51,09 BERT NE EA 18,70 79,60 65,36 82,66 67,06 67,44 67,44 53,66 86,74 23,34 61,20 BERT NE EA (8bit) 9,57 77,87 63,40 48,93 34,19 33,59 59,58 53,79 59,15 19,58 45,97 EmbBERT 11,01 79,33 69,19 83,25 67,83 68,63 68,92 49,96 87,61 49,25 63,50 EmbBERT-Q 9,56 80,96 67,99 82,45 67,10 68,05 68,06 47,29 87,32 49,28 62,81 overall view of their functionality. We emphasize that our computational evaluations account for the complexity and memory access profiling associated with all needed model layers. Particular attention is given to CPU-based operations, including summation, multiplication, and memory retrievals.\n\n--- Segment 42 ---\nWe emphasize that our computational evaluations account for the complexity and memory access profiling associated with all needed model layers. Particular attention is given to CPU-based operations, including summation, multiplication, and memory retrievals. For memory evaluation, we assume a non-parallel program that retains only the minimum required data in memory to execute effectively. This approach reflects realistic constraints in resource-constrained environments, such as those encountered in TinyML applications. The following assumptions are made during our detailed computation of weight and activation matrices memory requirements for hardware deployment: Operations such as sums, element-wise multiplication, and activation functions are performed in-place, occupying only the memory of the input matrices, as intermediate results are discarded. Matrix multiplications require memory for both input matrices as well as the output matrix. Fully connected layers are treated as matrix multiplications where only one input and the out- put matrix contribute to activation memory, since weights do not increase activation memory requirements. The maximum memory consumption per layer is recorded at peak usage during processing. All calculations are based on inference-only processing, without accounting for training-related overheads. The final memory occupations, memory accesses, and formulas for sums and multiplications for each block are provided in Tables ? ? and 14. B.1 BERT BERT (Bidirectional Encoder Representations from Transformers) [11] is a Transformer Encoder- based foundational NLP model widely used for tasks such as text classification, question answering, 17 and text generation. It leverages Transformer layers to generate contextualized representations of input text, capturing both left-to-right and right-to-left dependencies. The architecture of BERT typically consists of an Embedder, followed by a series of Attention and Feed-Forward layers, interleaved with normalization layers. These components are repeated N times. Embedder The standard Embedder, illustrated in Fig. 3, is responsible for generating token embed- dings, positional encodings, and segment embeddings using learned dictionaries. These embeddings are then summed to produce the final input encoding fed into the model.\n\n--- Segment 43 ---\n3, is responsible for generating token embed- dings, positional encodings, and segment embeddings using learned dictionaries. These embeddings are then summed to produce the final input encoding fed into the model. The total parameter count for the embedder, Wemb, is calculated as the sum of the sizes of the token embedding matrix (v d), the positional embedding matrix (ℓ d), and segment embedding matrix (2d): Wemb d (v ℓ 2), (1) where v is the vocabulary size, ℓis the sequence length, and d is the embedding dimension. The maximum activation size, Aemb, results from storing token, positional, and segment embeddings as matrices of size ℓ d during inference. These embeddings are summed in pairs, leading to: Aemb 2d ℓ. (2) The embedding operations required to compute the output of this layer involve ℓ (4d 2) 2d memory accesses and ℓ 2d summations. Attention The standard Attention Mechanism [42] allows models to selectively focus on the most relevant parts of an input sequence.Initially designed for machine translation, attention assigns vary- ing weights to tokens based on their relevance to the task or context, enabling models to capture dependencies between distant words. The self-attention variant computes relationships within a sequence by enabling each token to attend to all others, creating contextualized representations that encode both local and global depen- dencies. The input is processed through three fully connected layers to produce the Query, Key, and Value matrices, each with size d2. This step generates an activation size of 4 d ℓand requires: i) 6ℓ d2 memory accesses, ii) 3ℓ d2 summations, and iii) 3ℓ d2 multiplications. The Query and Key matrices are then multiplied to compute a Weight matrix of size ℓ ℓfor each of the h attention heads. Due to the quadratic growth in the Weight matrix, the context length has a significant impact on activation size, which increases to 3d ℓ ℓ2 h. This step also adds: i) 2ℓ2 d h memory accesses, ii) ℓ2 d h summations, and iii) ℓ2 d h multiplications.\n\n--- Segment 44 ---\nThe Query and Key matrices are then multiplied to compute a Weight matrix of size ℓ ℓfor each of the h attention heads. Due to the quadratic growth in the Weight matrix, the context length has a significant impact on activation size, which increases to 3d ℓ ℓ2 h. This step also adds: i) 2ℓ2 d h memory accesses, ii) ℓ2 d h summations, and iii) ℓ2 d h multiplications. The Weight matrix undergoes a softmax operation, contributing: i) 2ℓ2 h memory accesses, ii) ℓ2 h summations, and iii) 3ℓ2 h multiplications, without increasing activation size. Next, the Weighted matrix is multiplied by the Value matrix, producing an output activation size of 2d ℓ ℓ2 h. This step introduces i) 2ℓ2 d h memory accesses, ii) ℓ2 d h summations, and iii) ℓ2 d h multiplications. Finally, the output passes through a fully connected layer of size d2 d with a skip connection. Although the activation size remains unchanged, this stage involves: i) ℓ d2 2 memory accesses, ii) ℓ d2 ℓ d summations, and iii) ℓ d2 multiplications. B.2 NanoBERT The NanoBERT model, introduced in [21], is an ultra-compact version of BERT, designed for efficient operation on resource-constrained devices. It achieves this by employing techniques such as word embedding factorization and Low-Rank Adaptation (LoRA). A key component of its efficiency is the Nano Embedder (shown in Fig. 3), which serves the same purpose as the standard Embedder but introduces a critical optimization: instead of embedding tokens and positions directly into vectors of size d, it maps these inputs to a reduced embedding dimension rd using a fully connected layer. This reduced embedding is then projected back to the original dimension d through a fully connected layer. Segment embeddings are excluded from this process.\n\n--- Segment 45 ---\nThis reduced embedding is then projected back to the original dimension d through a fully connected layer. Segment embeddings are excluded from this process. This approach modifies the parameter count to: Wnemb rd (v ℓ 2d) 2d, (3) 18 EMBEDDER positions tokens l embedding l d l d l d NANO EMBEDDER fc positions tokens l rd l rd l d l d l d l embedding fc Figure 3: Memory layout of the Standard Embedder (left) and Nano Embedder (right) layers. ℓis the sentence length, d is the embedding dimension, and the trapezoid labeled fc denotes fully connected layers. The dashed gray box highlights the operations requiring the maximum activation size. The Nano Embedder reduces the number of weights while maintaining a similar activation size. which can be lower than that of the Standard Embedder if rd is sufficiently small relative to d. However, the total activation size, Anemb, increases slightly due to the projection step, resulting in: Anemb rd ℓ 2d ℓ. (4) During inference, the Nano Embedder performs the following operations: i) ℓ 2ℓ rd memory accesses for token and position encoding, followed by ii) 4ℓ rd d memory accesses, iii) 2ℓ rd d ℓ d summations, and iv) ℓ rd d 2 multiplications for the linear layers; and if segment embeddings are needed another ℓ d ℓ d memory accesses and ℓ d summations. This balance of parameter efficiency with a slight increase in activation memory illustrates the Nano Embedder ability to reduce the overall model size while maintaining embedding functionality. It is particularly advantageous in resource-constrained scenarios or when prioritizing higher dictionary sizes for improved performance. B.3 BERT Efficient In [41], the authors introduce Efficient Attention, an approach to effectively halve the parameter count of the Standard Attention mechanism. Instead of using three fully connected layers at the beginning of the layer and one at the output, Efficient Attention employs only one fully connected layer to generate the Query matrix. The Key and Value matrices are directly taken as the input.\n\n--- Segment 46 ---\nInstead of using three fully connected layers at the beginning of the layer and one at the output, Efficient Attention employs only one fully connected layer to generate the Query matrix. The Key and Value matrices are directly taken as the input. A single fully connected layer processes the attention output - both fully connected layers have dimensions d2. The total number of parameters, Weatt, for the Efficient Attention layers is calculated as: Weatt 2d2 (5) Like the Standard Attention layer, the Efficient Attention layer s highest activation size occurs during the matrix multiplication step. This step produces an activation size of: Aeatt 2ℓ d ℓ2 (6) To calculate the operations and memory accesses required, the expressions from the Standard Attention layer can be simplified by omitting terms associated with the two additional fully connected layers, as well as the h (attention heads) term from all equations (see Table 14). As shown in [41], these modifications are such that not hinder the effective modeling capabilities of the attention layer. It retains similar contextual performance while significantly reducing computational cost, making it highly advantageous for resource-constrained scenarios. 19 Input ATTENTION Q O fc fc fc Input softmax EFFICIENT ATTENTION K V Q l d W l l l d l d l d l d K l d V l d l d l d O fc l d softmax W l l h Figure 4: Memory layout of the Attention (left) and Efficient Attention (right) layers. ℓis the sentence length, d is the embedding dimension, and the trapezoid labeled fc denotes fully connected layers. The dashed gray box highlights the operations requiring the maximum activation size. Efficient Attention significantly reduces activation size. B.4 EmbBERT-Q This section focuses on analyzing the memory and computational costs of the Efficient Encoder block of EmbBERT-Q. Its first path consists in Efficient Attention, so we consider its weight count and activation size, and introduce the modifications due to the Convolutional Skip Connection and the weighted sum mechanisms. For the weights, we include the contributions from the Convolutional and Fully Connected layers. However, the four vectors used for the weighted sum during training do not contribute to the final memory footprint, as they can be discarded and replaced by two weights computed at runtime.\n\n--- Segment 47 ---\nFor the weights, we include the contributions from the Convolutional and Fully Connected layers. However, the four vectors used for the weighted sum during training do not contribute to the final memory footprint, as they can be discarded and replaced by two weights computed at runtime. This results in: WEffEnc 2d2 d2 α k d α (7) For the activations, we only need to consider the maximum between those resulting from the Efficient Attention and those from the Convolutional Skip component. The new component requires an activation size of at most d ℓ(2 α), which arises from the processing of the fully connected layer and the attention result that must be retained in memory. This results in a total activation size of: AEffEnc max(2d ℓ ℓ2; d ℓ(2 α)) (8) For the computational complexity, we start with the operations required by the Efficient Attention and add those introduced by the Convolutional Skip Connection. The convolution step requires d l (k 1) memory accesses, along with d l k sums and multiplications. Next, the SiLU activation requires d l memory accesses, approximately d l summations, and 4d l multiplications. 20 Table 13: Formulas for calculating weights and activation sizes per layer, based on their architectural parameters. Layers Weights Activations Embedder (v ℓ 2) d 2d ℓ NanoEmbedder rd (v ℓ 2d) 2d rd ℓ 2d ℓ Normalization 2d 2d ℓ Feed Forward 2d2 α d ℓ (2α) Attention 4d2 4d ℓ ℓ2 h Efficient Attention 2d2 2d ℓ ℓ2 Eff Attention Conv Skip 2d2 d2 α c dα max(2d ℓ ℓ2; d ℓ(2 α)) Table 14: Formulas for calculating memory accesses, summations and multiplication performed by each layer, based on their architectural parameters.\n\n--- Segment 48 ---\n20 Table 13: Formulas for calculating weights and activation sizes per layer, based on their architectural parameters. Layers Weights Activations Embedder (v ℓ 2) d 2d ℓ NanoEmbedder rd (v ℓ 2d) 2d rd ℓ 2d ℓ Normalization 2d 2d ℓ Feed Forward 2d2 α d ℓ (2α) Attention 4d2 4d ℓ ℓ2 h Efficient Attention 2d2 2d ℓ ℓ2 Eff Attention Conv Skip 2d2 d2 α c dα max(2d ℓ ℓ2; d ℓ(2 α)) Table 14: Formulas for calculating memory accesses, summations and multiplication performed by each layer, based on their architectural parameters. Layers Memory accesses Summations Multiplications Embedder ℓ (4d 2) 2d ℓ 2d 0 NanoEmbedder ℓ (rd 4d d 2rd 2) 2d 2ℓ d (rd 1) ℓ rd d 2 Normalization (ℓ 1) d 2 ℓ d ℓ d Feed Forward 4ℓ d (d α 1) 2ℓ d (d α 1) 2ℓ d (d α 1) Attention 8ℓ d2 2ℓ2 h (2d 1) ℓ d (4d 1) ℓ2 h (2d 1) 4ℓ d2 ℓ2 h(2d 3) Efficient Attention 4ℓ d2 4ℓ2 d 2ℓ2 2ℓ d2 2ℓ2 d ℓ2 ℓ d 2ℓ d2 2ℓ2 d 2ℓ2 Eff Diff Skip Attention ℓ d(4d 4ℓ 2d α k 4) 2ℓ2 ℓ d(2d 2ℓ d α k 3) ℓ2 ℓ d(2d 2ℓ d α k 7) ℓ2 MAMBA main ℓ i (d 6 9) ℓ d c SSM ℓ i (d 3 2 c) ℓ d SSM ℓ i (d 3 5 c) SSM SSM ℓ i (ds 18 ρ 4 8) i ℓ i (ds 4 ρ 2 2) ℓ i (ds 7 ρ 2 2) The fully connected layer introduces 2d2 α ℓmemory accesses, along with half as many summations and multiplications (d2 α ℓ).\n\n--- Segment 49 ---\nLayers Weights Activations Embedder (v ℓ 2) d 2d ℓ NanoEmbedder rd (v ℓ 2d) 2d rd ℓ 2d ℓ Normalization 2d 2d ℓ Feed Forward 2d2 α d ℓ (2α) Attention 4d2 4d ℓ ℓ2 h Efficient Attention 2d2 2d ℓ ℓ2 Eff Attention Conv Skip 2d2 d2 α c dα max(2d ℓ ℓ2; d ℓ(2 α)) Table 14: Formulas for calculating memory accesses, summations and multiplication performed by each layer, based on their architectural parameters. Layers Memory accesses Summations Multiplications Embedder ℓ (4d 2) 2d ℓ 2d 0 NanoEmbedder ℓ (rd 4d d 2rd 2) 2d 2ℓ d (rd 1) ℓ rd d 2 Normalization (ℓ 1) d 2 ℓ d ℓ d Feed Forward 4ℓ d (d α 1) 2ℓ d (d α 1) 2ℓ d (d α 1) Attention 8ℓ d2 2ℓ2 h (2d 1) ℓ d (4d 1) ℓ2 h (2d 1) 4ℓ d2 ℓ2 h(2d 3) Efficient Attention 4ℓ d2 4ℓ2 d 2ℓ2 2ℓ d2 2ℓ2 d ℓ2 ℓ d 2ℓ d2 2ℓ2 d 2ℓ2 Eff Diff Skip Attention ℓ d(4d 4ℓ 2d α k 4) 2ℓ2 ℓ d(2d 2ℓ d α k 3) ℓ2 ℓ d(2d 2ℓ d α k 7) ℓ2 MAMBA main ℓ i (d 6 9) ℓ d c SSM ℓ i (d 3 2 c) ℓ d SSM ℓ i (d 3 5 c) SSM SSM ℓ i (ds 18 ρ 4 8) i ℓ i (ds 4 ρ 2 2) ℓ i (ds 7 ρ 2 2) The fully connected layer introduces 2d2 α ℓmemory accesses, along with half as many summations and multiplications (d2 α ℓ). Finally, the aggregation step requires 2d ℓmultiplications and memory accesses, with only half as many summations.\n\n--- Segment 50 ---\nLayers Memory accesses Summations Multiplications Embedder ℓ (4d 2) 2d ℓ 2d 0 NanoEmbedder ℓ (rd 4d d 2rd 2) 2d 2ℓ d (rd 1) ℓ rd d 2 Normalization (ℓ 1) d 2 ℓ d ℓ d Feed Forward 4ℓ d (d α 1) 2ℓ d (d α 1) 2ℓ d (d α 1) Attention 8ℓ d2 2ℓ2 h (2d 1) ℓ d (4d 1) ℓ2 h (2d 1) 4ℓ d2 ℓ2 h(2d 3) Efficient Attention 4ℓ d2 4ℓ2 d 2ℓ2 2ℓ d2 2ℓ2 d ℓ2 ℓ d 2ℓ d2 2ℓ2 d 2ℓ2 Eff Diff Skip Attention ℓ d(4d 4ℓ 2d α k 4) 2ℓ2 ℓ d(2d 2ℓ d α k 3) ℓ2 ℓ d(2d 2ℓ d α k 7) ℓ2 MAMBA main ℓ i (d 6 9) ℓ d c SSM ℓ i (d 3 2 c) ℓ d SSM ℓ i (d 3 5 c) SSM SSM ℓ i (ds 18 ρ 4 8) i ℓ i (ds 4 ρ 2 2) ℓ i (ds 7 ρ 2 2) The fully connected layer introduces 2d2 α ℓmemory accesses, along with half as many summations and multiplications (d2 α ℓ). Finally, the aggregation step requires 2d ℓmultiplications and memory accesses, with only half as many summations. B.5 Other blocks In this section, we synthetically review the two main other blocks required for a complete analysis of the Transformer BERT architecture.\n\n--- Segment 51 ---\nFinally, the aggregation step requires 2d ℓmultiplications and memory accesses, with only half as many summations. B.5 Other blocks In this section, we synthetically review the two main other blocks required for a complete analysis of the Transformer BERT architecture. Feed Forward block This block, typically appended to the Attention or Efficient Attention layers after a normalization step, consists of two fully connected layers with size d2 α. These layers sequen- tially increase decrease the embedding dimension by a factor ofα with an activation function applied in between, followed by a skip connection. The total parameter count for this block is: Wff 2d2 α (9) and activation size of: Aff 2ℓ d ℓ d α. (10) Overall, the Feed Forward block requires 4ℓ d (d α 1) memory accesses and half as many summations and multiplications, with the majority of these operations required by the matrix multiplications in the fully connected layers. Normalization Layer The normalization layer performs simple layer-wise normalization, requir- ing 2 d parameters to store the mean and variance values. During inference, it performs (ℓ 1) d 2 memory accesses and ℓ d summations and multiplications. The required activation size is ℓ d 2, which is negligible compared to the activation sizes of the more computationally intensive layers. 21 References [1] H. Kopetz, W. Steiner, Internet of things (2022). doi:10.1007 978-3-031-11992-7_13. URL [2] A. Ometov, V. Shubina, L. Klus, J. Skibi nska, S. Saafi, P. Pascacio, L. Flueratoru, D. Q. Gaibor, N. Chukhno, O. Chukhno, et al., A survey on wearable technology: History, state-of-the-art and current challenges, Computer Networks 193 (2021) 108074. [3] J. Lin, W.-M. Chen, Y. Lin, J. Cohn, C. Gan, S. Han, Mcunet: Tiny deep learning on iot devices, arXiv:2007.10319 [cs] (11 2020).\n\n--- Segment 52 ---\nURL [2] A. Ometov, V. Shubina, L. Klus, J. Skibi nska, S. Saafi, P. Pascacio, L. Flueratoru, D. Q. Gaibor, N. Chukhno, O. Chukhno, et al., A survey on wearable technology: History, state-of-the-art and current challenges, Computer Networks 193 (2021) 108074. [3] J. Lin, W.-M. Chen, Y. Lin, J. Cohn, C. Gan, S. Han, Mcunet: Tiny deep learning on iot devices, arXiv:2007.10319 [cs] (11 2020). URL [4] Resource-efficient neural networks for embedded systems (2024). URL [5] J. Lin, L. Zhu, W.-M. Chen, W.-C. Wang, S. Han, Tiny machine learning: progress and futures [feature], IEEE Circuits and Systems Magazine 23 (3) (2023) 8 34. [6] C. Cioflan, L. Cavigelli, M. Rusci, d. Prado, L. Benini, On-device domain learning for keyword spotting on low-power extreme edge embedded systems (2024). URL [7] M. Rusci, T. Tuytelaars, On-device customization of tiny deep learning models for keyword spot- ting with few examples, IEEE Micro 43 (2023) 50 57. doi:10.1109 mm.2023.3311826. URL [8] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen, Mobilenetv2: Inverted residuals and linear bottlenecks (2018). URL [9] Y. Li, Y. Chen, X. Dai, D. Chen, M. Liu, L. Yuan, Z. Liu, L. Zhang, N. Vasconcelos, Micronet: Towards image recognition with extremely low flops (2020). URL [10] M. Tan, R. Pang, Q. V. Le, Efficientdet: Scalable and efficient object detection, arXiv:1911.09070 [cs, eess] (07 2020).\n\n--- Segment 53 ---\nURL [9] Y. Li, Y. Chen, X. Dai, D. Chen, M. Liu, L. Yuan, Z. Liu, L. Zhang, N. Vasconcelos, Micronet: Towards image recognition with extremely low flops (2020). URL [10] M. Tan, R. Pang, Q. V. Le, Efficientdet: Scalable and efficient object detection, arXiv:1911.09070 [cs, eess] (07 2020). URL [11] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional trans- formers for language understanding (10 2018). URL [12] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, Q. V. Le, Xlnet: Generalized autore- gressive pretraining for language understanding (2019). URL [13] V. Sanh, L. Debut, J. Chaumond, T. Wolf, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter (2019). URL [14] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, O. Levy, Spanbert: Improving pre-training by representing and predicting spans, arXiv:1907.10529 [cs] (01 2020). URL [15] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, R. Soricut, Albert: A lite bert for self- supervised learning of language representations, arXiv:1909.11942 [cs] (02 2020). URL [16] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoy- anov, Roberta: A robustly optimized bert pretraining approach (07 2019). URL 22 [17] A. Gu, T. Dao, Mamba: Linear-time sequence modeling with selective state spaces (12 2023). doi:10.48550 arXiv.2312.00752.\n\n--- Segment 54 ---\nURL 22 [17] A. Gu, T. Dao, Mamba: Linear-time sequence modeling with selective state spaces (12 2023). doi:10.48550 arXiv.2312.00752. URL [18] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, D. Zhou, Mobilebert: a compact task-agnostic bert for resource-limited devices (2020). URL [19] Y. Tang, Y. Wang, J. Guo, Z. Tu, K. Han, H. Hu, D. Tao, A survey on transformer compression (04 2024). doi:10.48550 arXiv.2402.05964. URL [20] I. Turc, M.-W. Chang, K. Lee, K. Toutanova, Well-read students learn better: On the importance of pre-training compact models, arXiv preprint arXiv:1908.08962 (2019). [21] K. Maity, A. T. Chaulwar, V. Vala, R. S. Guntur, Nanobert: An extremely compact language model, in: Proceedings of the 7th Joint International Conference on Data Science Management of Data (11th ACM IKDD CODS and 29th COMAD), 2024, pp. 342 349. [22] Z. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, S. Yan, Convbert: Improving bert with span-based dynamic convolution, arXiv (Cornell University) (08 2020). doi:10.48550 arxiv.2008.02496. [23] S. Kim, A. Gholaminejad, Z. Yao, M. Mahoney, E. K. Keutzer, I-bert: Integer-only bert quanti- zation, arXiv (Cornell University) (01 2021). doi:10.48550 arxiv.2101.01321. [24] K. Clark, M.-T. Luong, Q. V. Le, C. D. Manning, Electra: Pre-training text encoders as discrim- inators rather than generators (2020).\n\n--- Segment 55 ---\ndoi:10.48550 arxiv.2101.01321. [24] K. Clark, M.-T. Luong, Q. V. Le, C. D. Manning, Electra: Pre-training text encoders as discrim- inators rather than generators (2020). URL [25] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, Lora: Low-rank adaptation of large language models, arXiv:2106.09685 [cs] (10 2021). URL [26] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, J. Tang, Gpt understands, too (2021). URL [27] Y. Lu, M. Shen, H. Wang, X. Wang, C. van Rechem, W. Wei, Machine learning for synthetic data generation: a review, arXiv preprint arXiv:2302.04062 (2023). [28] A. Gholami, S. Kim, Z. Dong, Z. Yao, M. W. Mahoney, K. Keutzer, A survey of quantization methods for efficient neural network inference, in: Low-Power Computer Vision, Chapman and Hall CRC, 2022, pp. 291 326. [29] G. Hinton, Distilling the knowledge in a neural network, arXiv preprint arXiv:1503.02531 (2015). [30] T. Dettmers, A. Pagnoni, A. Holtzman, L. Zettlemoyer, Qlora: Efficient finetuning of quantized llms, Advances in Neural Information Processing Systems 36 (2024). [31] C. Fields, C. Kennington, Exploring transformers as compact, data-efficient language models (12 2023). doi:10.18653 v1 2023.conll-1.35. URL [32] HuggingFace, Bitsandbytes (2024).\n\n--- Segment 56 ---\ndoi:10.18653 v1 2023.conll-1.35. URL [32] HuggingFace, Bitsandbytes (2024). URL [33] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, S. Fidler, Aligning books and movies: Towards story-like visual explanations by watching movies and reading books, in: The IEEE International Conference on Computer Vision (ICCV), 2015, pp. . [34] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, S. R. Bowman, Glue: A multi-task benchmark and analysis platform for natural language understanding, arXiv:1804.07461 [cs] (02 2019). URL 23 [35] P. S. Xingkun Liu, Arash Eshghi, V. Rieser, Benchmarking natural language understanding ser- vices for building conversational agents, in: Proceedings of the Tenth International Workshop on Spoken Dialogue Systems Technology (IWSDS), Springer, Ortigia, Siracusa (SR), Italy, 2019, pp. xxx xxx. URL [36] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, C. Potts, Learning word vectors for sentiment analysis, in: Proceedings of the 49th Annual Meeting of the Association for Compu- tational Linguistics: Human Language Technologies, Association for Computational Linguistics, Portland, Oregon, USA, 2011, pp. 142 150. URL [37] E. Saravia, H.-C. T. Liu, Y.-H. Huang, J. Wu, Y.-S. Chen, CARER: Contextualized affect repre- sentations for emotion recognition, in: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, Brussels, Belgium, 2018, pp. 3687 3697. doi:10.18653 v1 D18-1404. URL [38] X. Zhang, J. J. Zhao, Y. LeCun, Character-level convolutional networks for text classification, in: NIPS, 2015, pp. 0 1.\n\n--- Segment 57 ---\nURL [38] X. Zhang, J. J. Zhao, Y. LeCun, Character-level convolutional networks for text classification, in: NIPS, 2015, pp. 0 1. [39] J. Wang, K. Fu, C.-T. Lu, Sosnet: A graph convolutional network approach to fine-grained cyberbullying detection (2020). URL [40] I. Manotas, N. P. A. Vo, V. Sheinin, LiMiT: The literal motion in text dataset, in: Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020, pp. 991 1000. doi:10.18653 v1 2020.findings-emnlp.88. URL [41] M. Hosseini, P. Hosseini, You need to pay better attention: Rethinking the mathematics of attention mechanism (2024). URL [42] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems (2017). 24\n\n