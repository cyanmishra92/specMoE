=== ORIGINAL PDF: 2506.15697v1_DeepRTL2_A_Versatile_Model_for_RTL-Related_Tasks.pdf ===\n\nRaw text length: 69808 characters\nCleaned text length: 69235 characters\nNumber of segments: 46\n\n=== CLEANED TEXT ===\n\narXiv:2506.15697v1 [cs.LG] 28 May 2025 DeepRTL2: A Versatile Model for RTL-Related Tasks Yi Liu1,2 , Hongji Zhang1,2 , Yunhao Zhou1,2, Zhengyuan Shi1,2, Changran Xu1,2, Qiang Xu1,2 1The Chinese University of Hong Kong 2National Technology Innovation Center for EDA Abstract The integration of large language models (LLMs) into electronic design automation (EDA) has significantly advanced the field, offering transformative benefits, particularly in register transfer level (RTL) code genera- tion and understanding. While previous stud- ies have demonstrated the efficacy of fine- tuning LLMs for these generation-based tasks, embedding-based tasks, which are equally crit- ical to EDA workflows, have been largely over- looked. These tasks, including natural language code search, RTL code functionality equiva- lence checking, and performance prediction, are essential for accelerating and optimizing the hardware design process. To address this gap, we present DeepRTL2, a family of ver- satile LLMs that unifies both generation- and embedding-based tasks related to RTL. By si- multaneously tackling a broad range of tasks, DeepRTL2 represents the first model to provide a comprehensive solution to the diverse chal- lenges in EDA. Through extensive experiments, we show that DeepRTL2 achieves state-of-the- art performance across all evaluated tasks. 1 Introduction The rapid advancement of large language mod- els (LLMs) has had a profound impact on var- ious domains (Singhal et al., 2023; Bran and Schwaller, 2024), including electronic design au- tomation (EDA). Recently, LLMs have shown re- markable potential in automating and enhancing tasks related to the generation and understanding of register transfer level (RTL) code (Liu et al., 2024; Zehua et al., 2024; Zhao et al., 2024; Liu et al., 2025). These models are capable of gener- ating RTL code from high-level natural language instructions or summarizing the functionality of existing RTL code, thereby substantially improv- ing the efficiency of hardware design workflows. These authors contributed equally. While the application of LLMs to these generation- based tasks has yielded impressive results, their full potential at the RTL stage remains underex- plored, particularly in embedding-based tasks that are equally crucial to the design process. Embedding-based tasks like natural language code search, RTL code functionality equivalence checking, and performance prediction are vital for accelerating and optimizing the hardware design process. Natural language code search allows de- signers to quickly query large RTL codebases with simple natural language descriptions, enabling effi- cient identification and reuse of relevant modules, thus reducing search time. Moreover, verification and optimization are two key time-consuming bot- tlenecks in hardware design. RTL code functional- ity equivalence checking can significantly reduce the time spent on verification by quickly assessing whether two designs are functionally equivalent. Performance prediction tasks, such as power, per- formance, and area (PPA) estimation, enable early evaluation of RTL design efficiency. Accurate per- formance predictions can guide RTL code opti- mization, minimizing the need for time-intensive trial-and-error. Together, these tasks enhance code reuse, verify functionality, and provide early per- formance feedback, resulting in a more streamlined and efficient design workflow. Previous methods have attempted to apply machine learning solu- tions for hardware design verification (Vasudevan et al., 2021) and performance prediction (Fang et al., 2023), but they are typically design-specific, lack generalizable representations for RTL designs, or do not operate directly at the RTL stage. In this paper, we introduce DeepRTL2, a fam- ily of versatile LLMs designed to address both generation- and embedding-based tasks related to RTL. By unifying these tasks in a single model, DeepRTL2 offers a comprehensive solution to the multifaceted challenges inherent in EDA. Unlike previous work, which has primarily focused on RTL Code Rewritten Code Functional Description Code Generation Code Understanding RTL Code Embedding Rewritten Code Embedding Functional Description Embedding Area Delay XGBoost Generation-Based Tasks Embedding-Based Tasks Equal Unequal Match Mismatch DeepRTL2 Annotate Rewrite Figure 1: The overview of DeepRTL2. It can handle both generation- and embedding-based tasks at the RTL stage. For generation-based tasks, it performs RTL code generation and understanding. For embedding-based tasks, it uses cosine similarity scores between the embeddings of RTL code and functional descriptions to assess their match, enabling natural language code search. Additionally, cosine similarity between RTL code embeddings and rewritten code embeddings is used for functionality equivalence checking. Furthermore, a prediction model, such as XGBoost, can be applied to predict area and delay metrics based on code embeddings. generation, DeepRTL2 is the first model to provide a unified framework for handling a broad range of critical EDA tasks, including code generation, understanding, natural language code search, func- tionality equivalence checking, and performance prediction. Figure 1 provides an overview of our model. To achieve this, we have carefully curated a comprehensive dataset and developed new bench- marks for each task, with a particular focus on the embedding-based tasks, for which no existing datasets or benchmarks are available. We have adopted state-of-the-art decoder-only models, such as Llama-3.1 (Dubey et al., 2024) and DeepSeek- Coder (Guo et al., 2024), as our base models for fine-tuning, given their superior performance over other architectures in the open-source LLM space. To enable these models to handle both generation- and embedding-based tasks, we adapt the genera- tive representational instruction tuning (GRIT) ap- proach (Muennighoff et al., 2025) for fine-tuning, ensuring that DeepRTL2 can effectively manage the diverse tasks at the RTL stage. Through ex- tensive experimentation, we demonstrate that the DeepRTL2 series achieves state-of-the-art perfor- mance across all evaluated tasks. 2 Related Works 2.1 Register Transfer Level in EDA Register transfer level (RTL) is a key abstraction in EDA that describes the flow of data between registers and the operations performed on this data. It is typically expressed using hardware description languages (HDLs), with Verilog being the most widely used HDL in the industry. Thus, throughout this paper, we use the terms RTL code and Ver- ilog code interchangeably. In modern hardware design, engineers usually begin with specifications in natural language, which are then manually trans- lated into HDLs before synthesizing the circuit elements (Blocklove et al., 2023). RTL serves as an intermediary between high-level design specifi- cations and low-level implementation details, en- abling designers to describe intricate digital sys- tems while retaining flexibility for synthesis into gate-level representations. Within EDA workflows, RTL plays a crucial role in various phases, includ- ing functional verification, performance estimation, synthesis, and optimization. Efficient handling of RTL code is essential for minimizing design time, improving performance, and ensuring correctness. 2.2 LLMs for RTL With the rapid development of artificial intelligence (AI), there has been increasing interest in leverag- ing these technologies to automate and enhance RTL-based design workflows (Chen et al., 2024). A key area of focus has been the use of LLMs for RTL code generation and understanding, which has shown great promise in improving hardware design efficiency (Thakur et al., 2023; Liu et al., 2023; Lu et al., 2024). Recent works have fine-tuned open-source LLMs to generate high-quality RTL code from natural language descriptions (Chang et al., 2024; Liu et al., 2024; Thakur et al., 2024; Zehua et al., 2024; Zhang et al., 2024; Cui et al., 2024; Zhao et al., 2024), achieving significant im- provements in the automation of hardware design process. Additionally, models like DeepRTL (Liu et al., 2025) have extended these capabilities by introducing RTL code understanding tasks, i.e., summarizing the functionality of existing code, which facilitates collaboration and comprehension among hardware designers. Despite the great suc- cess achieved in these generation-based tasks, prior research has largely overlooked embedding-based tasks, which are equally critical for addressing chal- lenges in EDA. Embedding-based tasks, such as natural language code search, RTL code function- ality equivalence checking, and performance pre- diction, are essential for improving the efficiency of code reuse, verification, and optimization within hardware design workflows. Unlike generation- based tasks, which focus on producing new RTL code, embedding-based tasks involve understand- ing and analyzing existing designs, providing valu- able insights into design reusability, correctness, and performance. Meanwhile, even if some studies have applied machine learning techniques for hard- ware design verification (Vasudevan et al., 2021) and performance prediction (Fang et al., 2023), these efforts are either design-specific, lack gener- alizable representations for RTL designs, or do not operate directly at the RTL stage. In contrast, this work introduces DeepRTL2, a versatile model ca- pable of handling both generation- and embedding- based tasks, achieving superior performance across all evaluated tasks despite its versatility. 2.3 Embedding Capabilities of Decoder-Only LLMs Compared to bidirectional encoders like BERT (De- vlin, 2018) and encoder-decoder architectures like T5 (Raffel et al., 2020), decoder-only LLMs have demonstrated superior performance across a range of language tasks (Brown et al., 2020). However, their potential for text embedding tasks was largely overlooked until recently. In recent years, sev- eral studies have focused on adapting decoder-only LLMs for language embedding tasks (Jiang et al., 2023; Wang et al., 2023; BehnamGhader et al., 2024; Springer et al., 2024; Lei et al., 2024; Lee et al., 2024). Notably, Muennighoff et al. (2025) introduce the GRIT training strategy, which em- ploys a multi-task training objective function to enable a single decoder-only LLM to both generate content and encode text into fixed-length vectors. Despite their success on various language embed- ding benchmarks, these models primarily focus on general embedding tasks, which limits their effec- tiveness on specialized tasks like RTL embedding- based tasks. To the best of our knowledge, there is no model that has been specifically trained for RTL embedding, despite its critical role in optimiz- ing hardware design workflows. DeepRTL2 is the first model explicitly designed for RTL embedding- based tasks, outperforming general-purpose text embedding models on our benchmarks. 3 Dataset Previous research has predominantly focused on generation-based tasks, resulting in a notable gap in available datasets for the embedding-based tasks considered in this paper. Moreover, the availability of RTL code is limited even for generation-based tasks, due to the proprietary nature of hardware designs. To fill this gap, we have curated a compre- hensive dataset tailored to support both generation- and embedding-based tasks at the RTL stage. Fur- thermore, we have established new benchmarks specifically for the embedding-based tasks, which have been largely neglected in previous work. 3.1 Generation-Based Tasks 3.1.1 RTL Code Generation RTL code generation involves automatically syn- thesizing RTL code from user-defined natural lan- guage descriptions, streamlining hardware design and enabling a more accessible development pro- cess. To construct a high-quality dataset for this task, we follow the data construction pipeline proposed in DeepRTL (Liu et al., 2025), given its demonstrated effectiveness in generation-based tasks. The process begins by collecting .v files from GitHub1 using the keyword Verilog. Each file is then segmented into individual Verilog mod- ules, with each module representing a distinct func- tional unit. To ensure dataset quality and reduce redundancy, we remove modules that are predomi- nantly composed of comments or lack structurally complete module and endmodule declarations. Ad- ditionally, we apply MinHash and Jaccard similar- ity metrics (Yan et al., 2017) to eliminate duplicates. To further refine the dataset, we employ the Stagira Verilog parser (Chen et al., 2023) to filter out mod- 1 Original Code Refined Code Specification Line Comment Commented Code Functional Description Clean Add Comment Extract Summarize Distill User Query Rephrase Figure 2: The annotation process for the RTL code generation understanding dataset. After obtaining the high-level functional description, we prompt GPT-4o to rephrase it into a user query format, which is then used to construct the natural language code search dataset. ules containing syntax errors, ensuring that only syntactically valid Verilog code is retained. For annotation, we adopt the chain-of-thought (CoT) prompting strategy used in DeepRTL, lever- aging GPT-4o (Hurst et al., 2024), a state-of-the-art LLM, to generate structured and informative an- notations. Specifically, we first query GPT-4o to insert line-level comments into the Verilog mod- ules, then extract line-level descriptions, pairing individual lines of RTL code with corresponding natural language explanations. Next, we prompt GPT-4o to generate a detailed specification for each module, comprising a summary of the module s functionality and a comprehensive explanation of its implementation process. By integrating these specifications with the module code, we construct high-level functional descriptions succinct one- sentence summaries that capture the core func- tionality of each Verilog module. The resulting dataset consists of Verilog modules enriched with line-level comments, detailed specifications, and succinct high-level functional descriptions, facili- tating both generation and understanding tasks in RTL design. Figure 2 provides an overview of the annotation process. For specifics on the prompts used, please refer to DeepRTL. To ensure the qual- ity of the generated annotations, we have conducted human evaluations, as detailed in Appendix A. To further expand the training dataset and im- prove model performance, we augment our dataset with open-source Verilog datasets from RTL- Coder (Liu et al., 2024), MG-Verilog (Zhang et al., 2024), and DeepCircuitX (Li et al., 2025). These datasets provide additional RTL designs with di- verse structures and functionalities, while also in- corporating different annotation strategies. The di- versity in annotations improves the model s adapt- ability to varying description styles, enhancing its robustness across various RTL-related tasks. 3.1.2 RTL Code Understanding RTL code understanding focuses on summarizing the functionality of existing Verilog code, enhanc- ing collaboration and comprehension among hard- ware designers. The dataset for this task is derived from the RTL code generation dataset, with Verilog code as input and corresponding natural language description as output. In the absence of a standard- ized benchmark for this task, we build upon the benchmark introduced in DeepRTL, which origi- nally comprises 100 Verilog designs. To improve evaluation reliability and ensure broader coverage, we extend this benchmark to include 500 high- quality Verilog modules with diverse functional- ities. Each module is annotated by professional hardware designers with a concise summary of its functionality along with a detailed description of the specific operations involved in its execution. This extended benchmark establishes a more ro- bust and comprehensive foundation for evaluating RTL code understanding capabilities. 3.2 Embedding-Based Tasks 3.2.1 Natural Language Code Search Natural language code search refers to the process of querying a large codebase using natural language to find relevant code snippets. It involves embed- ding both the user query and each code snippet into vectors, then calculating their similarity. The snip- pet with the highest similarity score is considered the best match for the user s requirements. This task is particularly crucial for hardware design, as it enables code reuse, improves efficiency, and ac- celerates the transition from user specifications to RTL code. For this task, we reuse the dataset and benchmark from the RTL code understanding task. However, since the functional descriptions in the understanding dataset often contain specific iden- tifiers, introducing the risk of data leakage, and are too complex for direct use in practical code search, we employ GPT-4o to rephrase the descrip- tions into a user query format, as shown in Figure 2. The rephrasing ensures that the new descriptions meet the following conditions: (1) no references to specific identifiers, (2) retention of the core func- tionality and high-level logic, and (3) clarity and simplicity, resembling how a user would query for relevant code based on its functionality. After this rephrasing process, we obtain the natural language Code Internal Logic Extract Rewritten Code LEC Feedback LLM Generate Figure 3: The feedback-driven code rewrite process. code search dataset and benchmark in the format {(user_queryi, RTL_codei)}n i 1. For details on the prompt used to rephrase the functional descriptions, please refer to the Appendix B. 3.2.2 Functionality Equivalence Checking Functionality equivalence checking is a critical ver- ification step in hardware design, ensuring that dif- ferent RTL implementations exhibit identical be- havior despite structural differences. To construct a dataset for this task, we develop a feedback-driven CoT prompting strategy using GPT-4o, as shown in Figure 3. Given a Verilog module, we first prompt GPT-4o to introduce significant modifications to its internal logic while preserving its intended func- tionality. We then use Yosys (Wolf et al., 2013) to perform logic equivalence checking (LEC), which verifies whether the original and modified designs are functionally equivalent. Based on Yosys feed- back classified as equivalent, inequivalent, or syn- tax error we iteratively refine the modifications. Specifically, we incorporate the original design, rewritten design, and verification results into the prompt to guide GPT-4o in generating alternative implementations. This process is repeated for two to three rounds per design, ensuring a diverse set of functionally equivalent and inequivalent pairs. The resulting dataset consists of paired RTL de- signs, where some maintain functional equivalence while others introduce subtle variations. Since only implementation details differ, distinguishing equiv- alent from inequivalent designs presents a signifi- cant challenge for models. Additionally, we adapt RTLLM v2.0 (Lu et al., 2024), a Verilog generation benchmark, to construct a new benchmark for func- tionality equivalence checking. Applying the same feedback-driven CoT strategy to its 50 verified Ver- ilog designs, we generate multiple alternative im- plementations, expanding our benchmark to 400 code pairs. This benchmark provides a diverse and well-validated resource for evaluating functionality equivalence checking. For further details on this process, please refer to the Appendix C. 3.2.3 Performance Prediction Performance prediction plays a crucial role in the early stages of hardware design, enabling designers to estimate key circuit characteristics before physi- cal implementation. Accurate predictions allow for informed architectural decisions, reducing design iterations and improving overall efficiency. Among the commonly used PPA metrics, delay and area are the primary focus in early-stage evaluations, as ac- curate power estimation requires detailed workload to specify the circuit s dynamic behavior, which is unavailable at the RTL stage. In this work, we con- struct a performance prediction dataset by synthe- sizing and mapping RTL designs into netlists using Yosys (Wolf et al., 2013) with the SkyWater 130nm technology library (Google, 2021). We then uti- lize open-source ABC (Brayton and Mishchenko, 2010) tool to extract delay and area metrics, where delay metric is reported by the static timing analy- sis, and area metric reflects the total logic footprint, which directly impacts manufacturing cost. This process provides a dataset that captures essential performance characteristics of RTL designs, facili- tating learning-based performance estimation. For a comprehensive summary of all dataset statistics, please refer to the Appendix D. 4 Methodology 4.1 Model Training We choose Llama-3.1 (Dubey et al., 2024) and DeepSeek-Coder (Guo et al., 2024) as the base models for training. Specifically, we fine-tune meta-llama Llama-3.1-8B-Instruct2 and deepseek- ai deepseek-coder-6.7b-instruct3. Our training con- sists of two stages. In the first stage, we follow the curriculum learning strategy adopted by Deep- RTL (Liu et al., 2025) and train the base model solely on RTL code generation and understanding data. In the second stage, we incorporate embed- ding data into the training set and train the model on both RTL code generation understanding and embedding tasks, utilizing the training framework of GRIT (Muennighoff et al., 2025). 4.1.1 First-Stage Training Following DeepRTL, we apply a curriculum learn- ing strategy in the first stage of our training pipeline, which can be further divided into four sub-stages: 2 1-8B-Instruct 3 deepseek-coder-6.7b-instruct Model syntax function GPT-3.5 56.50 69.72 71.75 30.10 39.59 41.40 GPT-4o 72.00 77.31 78.53 49.70 56.80 58.84 o1-preview 76.20 83.71 84.00 50.00 60.86 62.52 CodeV-CodeLlama 47.70 74.96 82.20 22.00 39.49 45.74 CodeV-CodeQwen 51.50 77.71 82.17 23.10 44.54 52.22 CodeV-DeepSeek 57.60 80.23 83.25 30.00 49.63 54.74 DeepRTL-220m 60.69 78.81 80.88 28.79 45.86 49.66 DeepRTL-16b 63.79 74.82 80.05 38.91 47.24 51.72 Llama-3.1 32.40 57.01 62.76 14.60 26.04 30.16 DeepSeek-Coder 59.30 72.38 74.67 31.40 39.59 42.57 DeepRTL21st-Direct (Llama) 54.48 63.52 67.99 16.28 28.78 32.76 DeepRTL21st-Direct (DeepSeek) 60.60 73.12 75.70 32.50 44.42 47.96 DeepRTL21st (Llama) 67.90 77.53 79.52 43.70 49.98 50.00 DeepRTL21st (DeepSeek) 63.50 76.74 80.10 39.70 51.96 54.70 DeepRTL2 (Llama) 68.30 81.31 83.36 33.70 49.57 52.90 DeepRTL2 (DeepSeek) 71.60 80.58 81.75 38.50 52.62 55.99 Table 1: The performance evaluation for RTL code generation using the metric, with k set to 1, 5, and 10. The best results among all models are bolded, and the best results among open-source models are underlined. Model F1 text-embedding-3-small 0.189 text-embedding-3-large 0.290 GritLM-7B 0.269 DeepRTL2no-hard (Llama) 0.476 DeepRTL2no-hard (DeepSeek) 0.464 DeepRTL2 (Llama) 0.463 DeepRTL2 (DeepSeek) 0.453 Table 2: The performance evaluation for natural lan- guage code search using the F1 metric. The best result is bolded, and the second-best result is underscored. training with line-level data, module-level data with specifications, module-level data with high-level descriptions, data with varying prompts. For details on these sub-stages, please refer to Appendix E. 4.1.2 Second-Stage Training Following GRIT, in the second stage of training, we combine the generation understanding and em- bedding tasks. For the generation understanding training, we reuse the high-quality data from the fourth sub-stage of the first-stage training. For the embedding task, we employ contrastive learning to learn contextualized representations that preserve the semantic information of the original text and code. Details for constructing the contrastive learn- ing training set can be found in Appendix F. In the embedding part of the second-stage training, we first use data that does not contain hard negatives and then incorporate data with hard negative sam- ples. For more details on the loss functions at dif- ferent sub-stages, please refer to Appendix G. For additional details on the hyperparameters and hard- ware resources used, please refer to Appendix H. 4.2 Model Evaluation For RTL code generation, we utilize the latest ver- sion of the widely adopted RTLLM v2.0 bench- mark (Lu et al., 2024), which contains 50 designs paired with corresponding natural language descrip- tions and testbenches. To measure Verilog gener- ation accuracy, we use the metric, which estimates the proportion of problems that can be solved at least once within k attempts: : E problems " 1 n c k n k (1) where n k represents the total number of trials for each problem, and c denotes the number of trails that pass the functional check. In our experiments, we set n 20 to mitigate randomness in results. The metric is reported for both syntactical and functional accuracy. Following RTLCoder (Liu et al., 2024), we evaluate performance across multi- ple generation temperatures (0.2, 0.5, and 0.8) and report the best performance across these settings. For RTL code understanding, we use the bench- mark constructed in Section 3.1.2. To evaluate the model s performance, we apply both traditional ma- chine translation metrics BLEU (Papineni et al., Model BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR Emb. Sim. GPT Score GPT-3.5 3.34 28.20 10.46 25.11 20.36 0.740 0.510 GPT-4o 4.59 29.26 11.48 25.74 22.78 0.761 0.549 o1-preview 3.73 28.00 10.39 24.98 20.48 0.748 0.535 CodeV-DeepSeek 3.05 25.14 9.78 23.25 20.23 0.705 0.495 CodeV-CodeQwen 2.80 24.91 8.27 22.75 21.07 0.747 0.499 DeepRTL-220m 13.06 37.56 19.85 34.72 34.37 0.806 0.600 DeepRTL-16b 12.85 37.43 19.34 34.63 33.09 0.802 0.597 Llama-3.1 2.68 25.37 10.39 23.75 17.16 0.730 0.430 DeepSeek-Coder 2.56 24.52 7.72 22.45 22.83 0.756 0.571 DeepRTL21st-Direct (Llama) 11.28 34.29 16.35 33.63 27.73 0.754 0.580 DeepRTL21st-Direct (DeepSeek) 12.07 36.37 17.78 33.78 28.56 0.767 0.602 DeepRTL21st (Llama) 13.34 37.74 19.54 34.76 33.46 0.798 0.594 DeepRTL21st (DeepSeek) 13.53 37.52 19.68 34.68 33.28 0.814 0.612 DeepRTL2 (Llama) 13.84 37.97 20.69 34.42 34.75 0.813 0.603 DeepRTL2 (DeepSeek) 13.96 37.93 20.73 34.34 34.74 0.820 0.616 Table 3: The performance evaluation for RTL code understanding. BLEU-4 refers to the smoothed BLEU-4 score, while Emb. Sim. represents the embedding similarity metric. The best results are highlighted in bold, and the second-best results are underscored. 2002), ROUGE (Lin, 2004), and METEOR (Baner- jee and Lavie, 2005) which primarily assess lexi- cal similarity, as well as the embedding similar- ity and GPT score metrics introduced in Deep- RTL (Liu et al., 2025), which focus on semantic similarity. This combination of evaluation met- rics provides a comprehensive assessment of the model s ability to understand RTL code, capturing both surface-level and deeper, semantic-level un- derstanding. For further details on how to compute these metrics, please refer to the Appendix I. For natural language code search, we utilize the benchmark introduced in Section 3.2.1. To assess the model s ability to retrieve relevant code from a large codebase based on a user s query, we fol- low the bitext mining setting from MTEB (Muen- nighoff et al., 2022). In our evaluation process, the inputs consist of two sets: the first set contains functional descriptions, while the second set con- sists of Verilog code snippets. For each description in the first set, the best matching code snippet in the second set is identified using cosine similarity. We report F1 score, precision, and recall for each model, with F1 serving as the primary evaluation metric for natural language code search. For functionality equivalence checking, we uti- lize the benchmark introduced in Section 3.2.2. To evaluate the models ability to check functional equivalence, we follow the pair classification set- ting from MTEB (Muennighoff et al., 2022). In this evaluation, the inputs consist of several pairs of RTL codes. For each pair, the model assigns a binary label: 1 for "functionally equivalent" and 0 for "functionally inequivalent". The binary label is determined by calculating the cosine similarity of their embeddings and comparing the similarity score to a predefined threshold. For each model, we first identify the optimal accuracy threshold and compute the accuracy score. We then determine the best F1 threshold and report the F1, precision, and recall scores. Finally, we calculate the average pre- cision score based on the similarity scores of the code pairs and their corresponding ground-truth labels. Average precision is the primary evalua- tion metric for RTL code functionality equivalence checking, with other metrics also reported. For performance prediction, we use the dataset introduced in Section 3.2.3. This task aims to test the expressive power of code embeddings for pre- dicting performance metrics, such as area and de- lay, at the early stage of RTL design. To achieve this, we first encode each code snippet into a fixed- length vector and create a new dataset in the for- mat {(code_embedding Rp, area R, delay R)i}n i 1, where p is the embedding dimension and n is the dataset size. The dataset is then split into training and test sets at an 80:20 ratio. In this paper, we use XGBoost (Chen and Guestrin, 2016) as the regression model, training separate models for area and delay prediction. The trained models are eval- uated on the test set using r2_score, mean absolute percentage error (MAPE) and root relative squared error (RRSE), with their formulas provided below: r2_score(y, ˆy) 1 Pn i 1(yi ˆyi)2 Pn i 1(yi y)2 (2) MAPE(y, ˆy) 1 n n X i 1 yi ˆyi yi 100 (3) RRSE(y, ˆy) sPn i 1(yi ˆyi)2 Pn i 1(yi yi)2 (4) 5 Experimental Results 5.1 Generation-Based Tasks For comparison, we select several baseline models: the state-of-the-art commercial models, OpenAI s GPT-3.5, GPT-4o, and o1-preview, which represent the most advanced general-purpose LLMs currently available. We also include the CodeV series (Zhao et al., 2024), a collection of leading open-source models specifically designed for RTL code genera- tion, as well as the original DeepRTL models (Liu et al., 2025), which have shown strong performance in both RTL code generation and understanding. All these models have demonstrated excellent capa- bilities in Verilog generation-based tasks (Liu et al., 2025), making them strong baselines for evaluating the performance of DeepRTL2. Additionally, we report the performance of base models, Llama-3.1 and DeepSeek-Coder, to show the effectiveness of our dataset construction and training strategy. Table 1 reports the results for RTL code generation across different models, with k set to 1, 5, and 10. The results show that o1-preview out- performs all other models, likely due to its design for addressing complex tasks, including program- ming. The DeepRTL2 models, however, achieve the best performance among all open-source mod- els, with results comparable to GPT-4o. The per- formance improvement from base models to Deep- RTL2 highlights the effectiveness of our dataset construction process and training strategy. Further- more, DeepRTL2 outperforms the original Deep- RTL models, likely due to the incorporation of additional open-source datasets, aside from data sourced from GitHub, and the inclusion of more diverse problem formulations that enhance Deep- RTL2 s generalization ability. Given that Deep- RTL2 is a multi-task model and the generation benchmark may overlap with the training data used by OpenAI s models, these results highlight Deep- RTL2 s impressive performance for this task. Table 3 presents the results for RTL code un- derstanding. Since the CodeV-CodeLlama model outputs random messages for this task, we ex- clude it from the comparison. The results show that DeepRTL2 models significantly outperform all other models, including the previous state-of- the-art DeepRTL models, underscoring its strong capabilities in RTL code understanding. Notably, DeepRTL2 surpasses GPT-4o by a substantial mar- gin, despite the fact that its training data is anno- tated using GPT-4o. The main reason is that during benchmark testing, all models, including GPT-4o, are required to generate high-level functional de- scriptions directly from RTL code. As shown in Appendix A, CoT-based annotations are more ac- curate than direct annotations. This enhanced anno- tation quality contributes to DeepRTL2 s superior performance in RTL code understanding. 5.2 Embedding-Based Tasks Since none of the existing models are specifi- cally designed for RTL embedding-based tasks, the baselines used for the generation-based tasks, e.g., CodeV series and DeepRTL models, perform poorly in this setting. These models show near- zero performance, with an F1 score close to 0 on the natural language code search task and an aver- age precision of approximately 0.5 on the func- tionality equivalence checking task. Therefore, we select state-of-the-art general-purpose embed- ding models as baselines for comparison. These include OpenAI s text embedding models (text- embedding-3-small, text-embedding-3-large) (Nee- lakantan et al., 2022) and open-source models like GritLM-7B (Muennighoff et al., 2025). Table 2 presents the F1 scores for the natural lan- guage code search task. The results show that our DeepRTL2 models outperform all baseline models by a significant margin, demonstrating the effec- tiveness of our dataset and training strategy for this task. For the full evaluation results on natural language code search, please refer to Appendix J. Table 5 presents the average precision scores for the functionality equivalence checking task. The results show that DeepRTL2 models outperform all other baselines, demonstrating their effective- ness in capturing functional relationships between RTL modules. The full evaluation results are in Appendix J. It is important to emphasize that our embedding-based verification is not intended to re- place the traditional verification process, but rather to serve as an efficient preliminary step that can significantly streamline the verification flow. Table 4 presents the results for performance pre- diction on area and delay. Our DeepRTL2 series models outperform the baseline models across all metrics. These results highlight that the code em- beddings generated by the DeepRTL2 models are more expressive for predicting performance-related metrics such as area and delay. Model Area Delay r2_score MAPE RRSE r2_score MAPE RRSE text-embedding-3-small 0.603 5.568 0.630 0.608 0.883 0.626 text-embedding-3-large 0.699 4.446 0.548 0.699 0.705 0.548 GritLM-7B 0.651 3.878 0.591 0.651 0.726 0.591 DeepRTL2no-hard (Llama) 0.510 2.828 0.700 0.735 0.471 0.515 DeepRTL2no-hard (DeepSeek) 0.805 2.947 0.445 0.743 0.449 0.507 DeepRTL2 (Llama) 0.759 1.966 0.490 0.773 0.469 0.476 DeepRTL2 (DeepSeek) 0.773 1.598 0.476 0.772 0.448 0.478 Table 4: The performance evaluation for performance prediction on area and delay using r2_score, MAPE and RRSE metrics. The best results among all models are bolded, and the second-best results are underscored. Model Average Precision text-embedding-3-small 0.565 text-embedding-3-large 0.498 GritLM-7B 0.541 DeepRTL2no-hard (Llama) 0.518 DeepRTL2no-hard (DeepSeek) 0.481 DeepRTL2 (Llama) 0.667 DeepRTL2 (DeepSeek) 0.591 Table 5: The performance evaluation for RTL code functionality equivalence checking using the average precision metric. The best result among all models is bolded, and the second-best result is underscored. 5.3 Ablation Studies In this section, we conduct ablation studies to demonstrate the effectiveness of different dataset components and training strategies. In the first training stage, we adopt a curriculum learning strat- egy, where the model is progressively trained on line-level data, module-level data with specifica- tions, module-level data with high-level descrip- tions, and data with varying prompts. While the benefits of curriculum learning have been shown in DeepRTL (Liu et al., 2025), we extend this analysis with additional comparisons. Specifically, we compare our first-stage model (DeepRTL21st) with a variant trained without curriculum learning (DeepRTL21st-Direct), both focused on generation- based tasks. As shown in Table 1 and Table 3, the incorporation of curriculum learning significantly improves performance for both code generation and understanding tasks. When we further intro- duce the second-stage training, i.e., GRIT-based fine-tuning, the performance improves even more, demonstrating the effectiveness of both curriculum learning and GRIT-based fine-tuning strategies. In the second training stage, we combine con- trastive learning and curriculum learning to ensure that our model performs effectively on embedding- based tasks. Specifically, we start with data that excludes hard negatives and gradually introduce hard negative samples, which improves overall performance. To evaluate this strategy, we com- pare DeepRTL2 with and without hard negatives (DeepRTL2no-hard) in Tables 2, 4, and 5. Since hard negatives primarily influence contrastive learn- ing, these comparisons focus on embedding-based tasks, with negligible impact on generation-based performance. The results show a minor drop in natural language code search accuracy but substan- tial gains in functionality equivalence checking and performance prediction. Despite the small accuracy decrease in the natural language code search task, DeepRTL2 still outperforms powerful baseline em- bedding models. This improvement in functionality equivalence checking and performance prediction justifies our decision to integrate hard negatives into the training process. 6 Conclusion In this work, we present DeepRTL2, a novel fam- ily of LLMs that unifies both generation- and embedding-based tasks at the RTL stage, offer- ing a comprehensive solution to the diverse chal- lenges in EDA. By addressing critical tasks in- cluding RTL code generation, understanding, nat- ural language code search, functionality equiva- lence checking, and performance prediction, Deep- RTL2 significantly improves the efficiency of hard- ware design workflows. To develop DeepRTL2, we have curated a comprehensive dataset and es- tablished new benchmarks specifically designed for these tasks, particularly the embedding-based ones, for which no suitable resources previously existed. Furthermore, we adapt the GRIT approach to fine-tune the model, enabling it to manage both generation- and embedding-based tasks effectively. Extensive experimentation demonstrates that Deep- RTL2 achieves state-of-the-art performance across all evaluated tasks, advancing the application of LLMs in hardware design. 7 Limitations There are two main limitations in our work. First, due to the multi-task nature of our model and con- straints in time and computing resources, we may not have employed the most optimal training strat- egy and hyperparameter settings to maximize per- formance across all tasks. Second, performance prediction directly at the RTL stage is challenging, as RTL designs typically lack detailed information about delay and area metrics. Although our model outperforms others in the evaluation, a significant gap remains in achieving accurate predictions. We hypothesize that incorporating the control data flow graph (CDFG) of RTL designs, which offers a more structured representation of the design s behavior, may facilitate better learning of performance char- acteristics. In future work, we plan to explore how incorporating CDFG into the DeepRTL2 model se- ries could improve the model s ability to predict performance metrics more accurately. References Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and or summariza- tion, pages 65 72. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961. Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. 2023. Chip-chat: Challenges and opportunities in conversational hardware design. In 2023 ACM IEEE 5th Workshop on Machine Learning for CAD (MLCAD), pages 1 6. IEEE. Andres M Bran and Philippe Schwaller. 2024. Trans- formers and large language models for chemistry and drug discovery. In Drug Development Supported by Informatics, pages 143 163. Springer. Robert Brayton and Alan Mishchenko. 2010. Abc: An academic industrial-strength verification tool. In Computer Aided Verification: 22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings 22, pages 24 40. Springer. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877 1901. Kaiyan Chang, Kun Wang, Nan Yang, Ying Wang, Dan- tong Jin, Wenlong Zhu, Zhirong Chen, Cangyuan Li, Hao Yan, Yunhao Zhou, et al. 2024. Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework. In Proceedings of the 61st ACM IEEE Design Automa- tion Conference, pages 1 6. Lei Chen, Yiqi Chen, Zhufei Chu, Wenji Fang, Tsung- Yi Ho, Ru Huang, Yu Huang, Sadaf Khan, Min Li, Xingquan Li, et al. 2024. Large circuit models: op- portunities and challenges. Science China Informa- tion Sciences, 67(10):1 42. Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785 794. Xiangli Chen, Yuehua Meng, and Gang Chen. 2023. In- cremental verilog parser. In 2023 International Sym- posium of Electronics Design Automation (ISEDA), pages 236 240. IEEE. Fan Cui, Chenyang Yin, Kexing Zhou, Youwei Xiao, Guangyu Sun, Qiang Xu, Qipeng Guo, Demin Song, Dahua Lin, Xingcheng Zhang, et al. 2024. Ori- gen: Enhancing rtl code generation with code-to- code augmentation and self-reflection. arXiv preprint arXiv:2407.16237. Jacob Devlin. 2018. Bert: Pre-training of deep bidi- rectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Wenji Fang, Yao Lu, Shang Liu, Qijun Zhang, Ceyu Xu, Lisa Wu Wills, Hongce Zhang, and Zhiyao Xie. 2023. Masterrtl: A pre-synthesis ppa estimation framework for any rtl design. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pages 1 9. IEEE. Google. 2021. Skywater pdk. Accessed: 2025-02-11. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196. Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. 2023. Scaling sentence embeddings with large language models. arXiv preprint arXiv:2307.16645. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428. Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, and Andrew Yates. 2024. Meta-task prompting elicits embedding from large language models. arXiv preprint arXiv:2402.18458. Zeju Li, Changran Xu, Zhengyuan Shi, Zedong Peng, Yi Liu, Yunhao Zhou, Lingfeng Zhou, Chengyu Ma, Jianyuan Zhong, Xi Wang, et al. 2025. Deepcircuitx: A comprehensive repository-level dataset for rtl code understanding, generation, and ppa analysis. arXiv preprint arXiv:2502.18297. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74 81. Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. 2023. Verilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pages 1 8. IEEE. Shang Liu, Wenji Fang, Yao Lu, Jing Wang, Qijun Zhang, Hongce Zhang, and Zhiyao Xie. 2024. Rtl- coder: Fully open-source and efficient llm-assisted rtl code generation technique. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Yi Liu, Changran XU, Yunhao Zhou, Zeju Li, and Qiang Xu. 2025. DeepRTL: Bridging verilog understanding and generation with a unified representation model. In The Thirteenth International Conference on Learn- ing Representations. Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. 2024. Rtllm: An open-source benchmark for design rtl gen- eration with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 722 727. IEEE. Niklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904. Niklas Muennighoff, Hongjin SU, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2025. Generative representational in- struction tuning. In The Thirteenth International Conference on Learning Representations. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad- ford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pre- training. arXiv preprint arXiv:2201.10005. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computa- tional Linguistics, pages 311 318. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the lim- its of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1 67. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah- davi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172 180. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. 2024. Rep- etition improves language model embeddings. arXiv preprint arXiv:2402.15449. Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Ham- mond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2023. Bench- marking large language models for automated ver- ilog rtl code generation. In 2023 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1 6. IEEE. Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, and Siddharth Garg. 2024. Verigen: A large language model for verilog code generation. ACM Transac- tions on Design Automation of Electronic Systems, 29(3):1 31. Shobha Vasudevan, Wenjie Joe Jiang, David Bieber, Rishabh Singh, C Richard Ho, Charles Sutton, et al. 2021. Learning semantic representations to verify hardware designs. Advances in Neural Information Processing Systems, 34:23491 23504. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368. Clifford Wolf, Johann Glaser, and Johannes Kepler. 2013. Yosys - a free Verilog synthesis suite. In Proceedings of the 21st Austrian Workshop on Micro- electronics (Austrochip), volume 97. Ziqi Yan, Jiqiang Liu, Gang Li, Zhen Han, and Shuo Qiu. 2017. Privmin: Differentially private minhash for jaccard similarity computation. arXiv preprint arXiv:1705.07258. PEI Zehua, Huiling Zhen, Mingxuan Yuan, Yu Huang, and Bei Yu. 2024. Betterv: Controlled verilog gen- eration with discriminative guidance. In Forty-first International Conference on Machine Learning. Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, and Yingyan Celine Lin. 2024. Mg-verilog: Multi- grained dataset towards enhanced llm-assisted ver- ilog generation. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 1 5. IEEE. Yang Zhao, Di Huang, Chongxiao Li, Pengwei Jin, Ziyuan Nan, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, et al. 2024. Codev: Empowering llms for verilog generation through multi-level summarization. arXiv preprint arXiv:2407.10424. A Human Evaluation of Generated Annotations To evaluate the reliability and accuracy of GPT-4o- generated annotations, we conduct a human eval- uation focusing primarily on the accuracy of high- level functional descriptions, as this is the most challenging and critical aspect of the generation- based tasks. We randomly sample 200 annotated RTL modules and ask professional hardware de- signers to verify the correctness of the generated descriptions. The human evaluation results show that approximately 90 of these annotations are accurate. In comparison, when we test direct an- notations, i.e., generating high-level functional de- scriptions directly from the original code, the ac- curacy drops significantly to 70 . This significant difference further demonstrates the effectiveness of the CoT-based annotation strategy. Additionally, GPT-4o is employed for rewriting RTL code in the functionality equivalence check- ing task. For this task, we address concerns about accuracy by using EDA tools to verify the function- ality equivalence of the rewritten code against the original code. Therefore, all the data collected for this task is validated as ground truth, ensuring the quality and correctness of the rewritten RTL code. B Prompt For Rephrasing Descriptions Figure 4 shows the instruction given to GPT-4o to rephrase the code descriptions into their corre- sponding user query formats. C Code Rewrite Instructions Figure 5 illustrates the code rewrite instructions provided to GPT-4o for constructing the function- ality equivalence checking dataset. The leftmost I will provide a Verilog code snippet that defines a module, along with its corresponding natural language description. The description may reference specific identifiers in the code, such as variable names and module names. Your task is to rephrase the description by following these steps: 1. Remove any references to specific identifiers (e.g., module names, variable names, and specific signals). 2. Retain the core functional meaning and high-level logic of the description. 3. Ensure the rephrased description is concise and suitable for a query, resembling how a user would search for the relevant code based on its functionality. 4. Avoid unnecessary technical details and long-winded explanations. Keep it simple and to the point. Please return only the rephrased description. code: {code} text: {text} Figure 4: The instruction for rephrasing the code de- scription into the user query format. column presents the instruction used during the ini- tial rewrite process, where only the original RTL code is available. The subsequent three columns represent instructions based on previously rewrit- ten code, corresponding to the following cases: (1) equivalent rewritten code, (2) inequivalent rewrit- ten code, and (3) rewritten code with syntax errors. Notably, in addition to the code itself, we also in- clude the functional description and specification from Section 3.1.1. This additional context helps the model better understand the intended function- ality, leading to improved accuracy in rewriting the code while preserving its functionality. D Dataset Statistics Table 6 presents the overall statistics for all datasets used across the evaluated tasks. Except the perfor- mance prediction datasets, all datasets listed in this table are utilized for model training. For the per- formance prediction datasets, we split them in an 80:20 ratio, creating a training set with 15,000 sam- ples and a test set with 3,766 samples. For perfor- mance prediction, we regress area and delay based on code embeddings, without tuning the model. E Details of First-Stage Training In Section 3.1.1, we construct a dataset consisting of Verilog modules enriched with line-level com- ments, detailed specifications, and succinct high- level functional descriptions. These three levels of annotations correspond to the first three sub- stages of our first-stage training pipeline. In the first sub-stage, we train the model using line-level data, where each line of Verilog code is paired with a corresponding natural language comment. The You are an expert in Verilog RTL code design, with extensive experience in optimizing code for performance, power, and area (PPA). Given the following Verilog code, please rewrite it to achieve the same functionality using a different implementation approach, while considering potential improvements to PPA metrics. The provided code is: "{code}" Functional Description (a high-level description of the functionality of the Verilog code): "{functional_description}" Specification (detailed implementation requirements of the Verilog code): "{specification}" Please rewrite the code to retain the same functionality but with a different implementation style. You can use the functional description and specification as a reference, but note that these are not fully accurate. Therefore, treat the provided code as the "golden reference" for the intended functionality. I encourage you to propose significant changes that may lead to improvements in PPA after synthesis. However, it is crucial that the rewritten code performs exactly the same as the original. Please provide the rewritten code in the following format: verilog [rewritten code] Note: Output only the rewritten code and do not include any additional explanations or comments. Additionally, ensure that only the implementation of the internal logic of the code is modified; you are forbidden to change the module head declaration. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Significantly change the implementation style. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Ensure that the new implementation differs from the provided rewritten code, using a different implementation style. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Take into account the discrepancies between the original code and the rewritten code. Use the rewritten code as a reference and make adjustments to ensure the final version retains the intended functionality of the original code. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Take into account the syntax errors present in the rewritten code. Ensure that the new code is syntactically correct and functions as intended, while preserving the original functionality. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. Rewritten Code (the existing rewritten code that is functionally equivalent to the original code): "{rewritten_code}" Rewritten Code (the existing rewritten code that is functionally inequivalent to the original code): "{rewritten_code}" Rewritten Code (the existing rewritten code that contains syntax errors): "{rewritten_code}" None Initial Equal Unequal Syntax Error Figure 5: The code rewrite instructions used to construct the functionality equivalence checking dataset. second sub-stage utilizes module-level data with specifications, providing more detailed descriptions of the Verilog modules. The third sub-stage focuses on module-level data with high-level functional de- scriptions, offering a broader functional overview of the code. To further refine the dataset and adapt it to a wider range of scenarios, we introduce a fourth sub-stage, where GPT-4o generates varying prompts based on the high-quality data from the third sub-stage. These varying prompts represent different problem descriptions used to generate Ver- ilog code. We find that incorporating this sub-stage improves the model s performance and robustness, as it allows the model to better generalize across a wide range of code generation tasks. F Contrastive Learning Training Set Construction In the second-stage training, we apply contrastive learning to enable the model to (1) determine whether a Verilog module matches a given func- tional description; and (2) assess whether two Ver- ilog code snippets are functionally equivalent. To construct a dataset for contrastive learning, we first prompt GPT-4o to rewrite Verilog code snippets from the natural language code search training set. The rewrite process is illustrated in Figure 3. After several iterations, we combine the original natural language code search training set with their rewritten code snippets, resulting in four types of new data samples: type a: {original_text, original_code} type b: {original_text, original_code, equiva- lent_code} type c: {original_text, original_code, inequiv- alent_code} type d: {original_text, original_code, equiva- lent_code, inequivalent_code} Since the format of an original data sample in the natural language code search training set is {orig- inal_text, original_code}, the four types of data samples correspond to the following scenarios: type a corresponds to the case where all rewrit- ten code snippets contain syntax errors. type b corresponds to the case where all rewrit- ten code snippets, free of syntax errors, are functionally equivalent to the original code. type c corresponds to the case where all rewrit- ten code snippets, free of syntax errors, are not functionally equivalent to the original code. Task Description Source Count RTL Code Generation Understanding Line Level DeepRTL2 341310 Module Level (Detailed Specification) DeepRTL2 45519 MG-Verilog 10035 DeepCircuitX 32809 Module Level (High-Level Description) DeepRTL2 46876 RTLCoder 25001 MG-Verilog 10037 DeepCircuitX 38179 Natural Language Code Search N A DeepRTL2 59700 Functionality Equivalence Checking Equal Pairs DeepRTL2 9532 Unequal Pairs DeepRTL2 23330 Performance Prediction Area DeepRTL2 18766 Delay DeepRTL2 18766 Table 6: The overall dataset statistics for all evaluated tasks. type d corresponds to the case where some rewritten code snippets, free of syntax er- rors, are functionally equivalent to the original code, while others are not. For all four types of data samples, we convert them into contrastive learning samples as follows: type a: {"query": original_code, "pos": origi- nal_text, "neg": None} {"query": original_text, "pos": origi- nal_code, "neg": None} type b: {"query": original_code, "pos": origi- nal_text, "neg": None} {"query": original_text, "pos": origi- nal_code, "neg": None} {"query": original_code, "pos": equiva- lent_code, "neg": None} {"query": equivalent_code, "pos": origi- nal_code, "neg": None} type c: {"query": original_code, "pos": origi- nal_text, "neg": inequivalent_code} {"query": original_text, "pos": origi- nal_code, "neg": None} type d: {"query": original_code, "pos": origi- nal_text, "neg": inequivalent_code} {"query": original_code, "pos": equiva- lent_code, "neg": inequivalent_code} {"query": original_text, "pos": origi- nal_code, "neg": None} {"query": equivalent_code, "pos": origi- nal_code, "neg": inequivalent_code} In each of the contrastive learning samples above, the key pos refers to the positive instance of the query code text, while the key neg refers to the hard negative instance. In the embedding part of the second-stage training, we first use samples col- ored blue that do not contain hard negatives and then incorporate samples colored purple with hard negative instances. G Training Loss Function In the second stage of training, we combine gen- eration understanding and embedding tasks. For generation understanding, we reuse high-quality data from the fourth sub-stage of the first training stage. For the embedding tasks, we apply con- trastive learning to learn contextualized represen- tations that preserve the semantic information of text and code. In the embedding part of the second- stage training, we first use data without hard nega- tives and later incorporate data with hard negatives. The embedding loss function is defined as follows: E i exp σ(fθ(xi), fθ(x i )) τ (5) S i M X j 1 exp σ(fθ(xi), fθ(x j )) τ ! (6) S i M X j 1 exp σ(fθ(xi), fθ(x j )) τ ! (7) Model Precision Recall F1 (Main Metric) text-embedding-3-small 0.173 0.241 0.189 text-embedding-3-large 0.273 0.340 0.290 GritLM-7B 0.255 0.320 0.269 DeepRTL2no-hard (Llama) 0.469 0.497 0.476 DeepRTL2no-hard (DeepSeek) 0.456 0.489 0.464 DeepRTL2 (Llama) 0.450 0.493 0.463 DeepRTL2 (DeepSeek) 0.443 0.481 0.453 Table 7: The full performance evaluation results for natural language code search. The best results among all models are bolded, and the second-best results are underscored. Lemb1 1 M M X i 1 log E i S i (8) Lemb2 1 M M X i 1 log E i S i S i (9) where M is the batch size, xi is the i-th training sample, fθ is the embedding function (in this paper, we use position-weighted mean pooling method introduced in SGPT (Muennighoff, 2022) to ob- tain sentence embeddings), τ is the temperature hyperparameter, and σ is the similarity function (typically cosine similarity). x i is the positive in- stance of the i-th training sample, while x i is the hard negative of the i-th training sample. Lemb1 represents the embedding loss when no hard neg- ative is available for each training sample, while Lemb2 corresponds to the embedding loss when a hard negative instance is present for each sample. For generation understanding, we adopt the tra- ditional next-token cross-entropy loss: Lgen 1 N N X i 1 log P(fθ,η(x(i)) fθ,η(x( i))) (10) where η is the language modeling head used for generation-based tasks. In the second-stage train- ing, we first use L1 Lemb1 Lgen as the loss function, then switch to L2 Lemb2 Lgen. H Hyperparameters All experiments are conducted on a cluster equipped with eight NVIDIA A800 GPUs, each with 80GB of memory. Tables 9 and 10 present the hyperparameter settings used in the first-stage and second-stage training, respectively. I Understanding Evaluation Metrics To evaluate the models understanding capabilities of RTL code, we apply both traditional machine translation metrics BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) which primarily assess lexical simi- larity, as well as the embedding similarity and GPT score introduced in DeepRTL (Liu et al., 2025), which focus on semantic similarity. These met- rics measure the similarity between the generated descriptions and the ground truth summaries. Specifically, BLEU measures the proportion of n-grams (sequences of n words) in the generated text that also appear in the reference text. It cal- culates the overlap of n-grams (typically up to a length of 4), with higher scores assigned to more matches. BLEU is precision-focused and rewards the accurate use of words or phrases in the gener- ated descriptions. In our evaluation, we report the smoothed BLEU-4 score to address zero counts in higher-order n-grams, which helps to avoid penal- izing models for small discrepancies. ROUGE is a recall-based metric that evaluates the proportion of n-grams in the reference summary that are present in the generated summary. For our evaluation, we report ROUGE-1 (unigram over- lap), ROUGE-2 (bigram overlap), and ROUGE-L (longest common subsequence). METEOR combines both precision and recall while also accounting for synonymy, stemming, and word order. It computes unigram precision and recall and applies a penalty for word order mis- matches. For calculating these traditional machine translation metrics, we directly use the correspond- ing functions from Python libraries nltk (for BLEU and METEOR) and rouge (for ROUGE). In contrast to the lexical metrics, embedding similarity and GPT score evaluate semantic simi- larity by assessing how well the generated descrip- Model Average Precision (Main Metric) Accuracy F1 Precision Recall text-embedding-3-small 0.565 0.581 0.646 0.525 0.840 text-embedding-3-large 0.498 0.544 0.647 0.478 1.000 GritLM-7B 0.541 0.613 0.661 0.503 0.960 DeepRTL2no-hard (Llama) 0.518 0.594 0.661 0.497 0.987 DeepRTL2no-hard (DeepSeek) 0.481 0.581 0.658 0.497 0.973 DeepRTL2 (Llama) 0.667 0.681 0.723 0.575 0.973 DeepRTL2 (DeepSeek) 0.591 0.619 0.708 0.552 0.987 Table 8: The full performance evaluation results for RTL code functionality equivalence checking. The best results among all models are bolded, and the second-best results are underscored. Hyperparameter Name Value finetuning_type lora per_device_train_batch_size 4 gradient_accumulation_steps 4 lr_scheduler_type cosine warm_up_ratio 0.1 learning_rate 5e-5 epochs 3 Table 9: Hyperparameters selected for the first training stage of DeepRTL2. Hyperparameter Name Value finetuning_type full per_device_embedding_batch_size 4 per_device_generative_batch_size 4 gradient_accumulation_steps 8 lr_scheduler_type linear warmup_ratio 0.03 learning_rate 2e-5 epochs 1 temperature (τ) 0.02 Table 10: Hyperparameters selected for the second train- ing stage of DeepRTL2. tion captures the underlying meaning of the RTL code, rather than focusing solely on surface-level word matches. Embedding similarity computes the cosine similarity between the embeddings of the generated description and the ground truth sum- mary, derived from OpenAI s text-embedding-3- large model. This metric rewards models for pro- ducing descriptions that are semantically closer to the reference, even if the wording differs. The GPT score, based on GPT-4o, quantifies the semantic coherence between descriptions by assigning a sim- ilarity score between 0 and 1, where 1 indicates perfect alignment. Unlike lexical metrics, the GPT score focuses on semantic accuracy rather than ex- act word matching. For the prompt used in calcu- lating the GPT score, please refer to DeepRTL. Together, these metrics offer a comprehensive evaluation of both lexical precision and semantic accuracy, providing a holistic view of the model s understanding of RTL code. J Full Evaluation Results J.1 Natural Language Code Search The full evaluation results for natural language code search are presented in Table 7. Results show that the DeepRTL2 models significantly out- perform all baseline models across all metrics. Specifically, DeepRTL2 (Llama) and DeepRTL2 (DeepSeek) achieve F1 scores of 0.463 and 0.453, respectively, surpassing the best baseline model, GritLM-7B, which scores 0.269. The higher pre- cision and recall scores for the DeepRTL2 models indicate that they are more effective at retrieving relevant code snippets based on user queries, high- lighting the strength of our dataset and training framework. These results confirm that DeepRTL2 excels in natural language code search, demonstrat- ing its superior ability to handle hardware-specific queries compared to the baseline models. J.2 Functionality Equivalence Checking The full evaluation results for RTL code function- ality equivalence checking are presented in Table 8. Results show that the DeepRTL2 models outper- form all baseline models across all metrics. Specif- ically, DeepRTL2 (Llama) achieves the highest performance with an average precision score of 0.667, F1 score of 0.723, and accuracy of 0.681. In comparison, the best-performing baseline model, GritLM-7B, achieves an average precision of 0.541, an F1 score of 0.661, and accuracy of 0.613. More- over, DeepRTL2 (DeepSeek) also shows strong performance, with an average precision of 0.591 and an F1 score of 0.708. The significantly higher precision and recall scores for DeepRTL2 mod- els indicate their superior capability in identifying functionally equivalent RTL code compared to the baseline models. These results confirm that Deep- RTL2 excels in functionality equivalence check- ing, demonstrating its effectiveness in hardware- specific tasks over general-purpose models.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.15697v1 [cs.LG] 28 May 2025 DeepRTL2: A Versatile Model for RTL-Related Tasks Yi Liu1,2 , Hongji Zhang1,2 , Yunhao Zhou1,2, Zhengyuan Shi1,2, Changran Xu1,2, Qiang Xu1,2 1The Chinese University of Hong Kong 2National Technology Innovation Center for EDA Abstract The integration of large language models (LLMs) into electronic design automation (EDA) has significantly advanced the field, offering transformative benefits, particularly in register transfer level (RTL) code genera- tion and understanding. While previous stud- ies have demonstrated the efficacy of fine- tuning LLMs for these generation-based tasks, embedding-based tasks, which are equally crit- ical to EDA workflows, have been largely over- looked. These tasks, including natural language code search, RTL code functionality equiva- lence checking, and performance prediction, are essential for accelerating and optimizing the hardware design process. To address this gap, we present DeepRTL2, a family of ver- satile LLMs that unifies both generation- and embedding-based tasks related to RTL. By si- multaneously tackling a broad range of tasks, DeepRTL2 represents the first model to provide a comprehensive solution to the diverse chal- lenges in EDA. Through extensive experiments, we show that DeepRTL2 achieves state-of-the- art performance across all evaluated tasks. 1 Introduction The rapid advancement of large language mod- els (LLMs) has had a profound impact on var- ious domains (Singhal et al., 2023; Bran and Schwaller, 2024), including electronic design au- tomation (EDA). Recently, LLMs have shown re- markable potential in automating and enhancing tasks related to the generation and understanding of register transfer level (RTL) code (Liu et al., 2024; Zehua et al., 2024; Zhao et al., 2024; Liu et al., 2025). These models are capable of gener- ating RTL code from high-level natural language instructions or summarizing the functionality of existing RTL code, thereby substantially improv- ing the efficiency of hardware design workflows. These authors contributed equally.\n\n--- Segment 2 ---\nThese models are capable of gener- ating RTL code from high-level natural language instructions or summarizing the functionality of existing RTL code, thereby substantially improv- ing the efficiency of hardware design workflows. These authors contributed equally. While the application of LLMs to these generation- based tasks has yielded impressive results, their full potential at the RTL stage remains underex- plored, particularly in embedding-based tasks that are equally crucial to the design process. Embedding-based tasks like natural language code search, RTL code functionality equivalence checking, and performance prediction are vital for accelerating and optimizing the hardware design process. Natural language code search allows de- signers to quickly query large RTL codebases with simple natural language descriptions, enabling effi- cient identification and reuse of relevant modules, thus reducing search time. Moreover, verification and optimization are two key time-consuming bot- tlenecks in hardware design. RTL code functional- ity equivalence checking can significantly reduce the time spent on verification by quickly assessing whether two designs are functionally equivalent. Performance prediction tasks, such as power, per- formance, and area (PPA) estimation, enable early evaluation of RTL design efficiency. Accurate per- formance predictions can guide RTL code opti- mization, minimizing the need for time-intensive trial-and-error. Together, these tasks enhance code reuse, verify functionality, and provide early per- formance feedback, resulting in a more streamlined and efficient design workflow. Previous methods have attempted to apply machine learning solu- tions for hardware design verification (Vasudevan et al., 2021) and performance prediction (Fang et al., 2023), but they are typically design-specific, lack generalizable representations for RTL designs, or do not operate directly at the RTL stage. In this paper, we introduce DeepRTL2, a fam- ily of versatile LLMs designed to address both generation- and embedding-based tasks related to RTL. By unifying these tasks in a single model, DeepRTL2 offers a comprehensive solution to the multifaceted challenges inherent in EDA.\n\n--- Segment 3 ---\nIn this paper, we introduce DeepRTL2, a fam- ily of versatile LLMs designed to address both generation- and embedding-based tasks related to RTL. By unifying these tasks in a single model, DeepRTL2 offers a comprehensive solution to the multifaceted challenges inherent in EDA. Unlike previous work, which has primarily focused on RTL Code Rewritten Code Functional Description Code Generation Code Understanding RTL Code Embedding Rewritten Code Embedding Functional Description Embedding Area Delay XGBoost Generation-Based Tasks Embedding-Based Tasks Equal Unequal Match Mismatch DeepRTL2 Annotate Rewrite Figure 1: The overview of DeepRTL2. It can handle both generation- and embedding-based tasks at the RTL stage. For generation-based tasks, it performs RTL code generation and understanding. For embedding-based tasks, it uses cosine similarity scores between the embeddings of RTL code and functional descriptions to assess their match, enabling natural language code search. Additionally, cosine similarity between RTL code embeddings and rewritten code embeddings is used for functionality equivalence checking. Furthermore, a prediction model, such as XGBoost, can be applied to predict area and delay metrics based on code embeddings. generation, DeepRTL2 is the first model to provide a unified framework for handling a broad range of critical EDA tasks, including code generation, understanding, natural language code search, func- tionality equivalence checking, and performance prediction. Figure 1 provides an overview of our model. To achieve this, we have carefully curated a comprehensive dataset and developed new bench- marks for each task, with a particular focus on the embedding-based tasks, for which no existing datasets or benchmarks are available. We have adopted state-of-the-art decoder-only models, such as Llama-3.1 (Dubey et al., 2024) and DeepSeek- Coder (Guo et al., 2024), as our base models for fine-tuning, given their superior performance over other architectures in the open-source LLM space.\n\n--- Segment 4 ---\nTo achieve this, we have carefully curated a comprehensive dataset and developed new bench- marks for each task, with a particular focus on the embedding-based tasks, for which no existing datasets or benchmarks are available. We have adopted state-of-the-art decoder-only models, such as Llama-3.1 (Dubey et al., 2024) and DeepSeek- Coder (Guo et al., 2024), as our base models for fine-tuning, given their superior performance over other architectures in the open-source LLM space. To enable these models to handle both generation- and embedding-based tasks, we adapt the genera- tive representational instruction tuning (GRIT) ap- proach (Muennighoff et al., 2025) for fine-tuning, ensuring that DeepRTL2 can effectively manage the diverse tasks at the RTL stage. Through ex- tensive experimentation, we demonstrate that the DeepRTL2 series achieves state-of-the-art perfor- mance across all evaluated tasks. 2 Related Works 2.1 Register Transfer Level in EDA Register transfer level (RTL) is a key abstraction in EDA that describes the flow of data between registers and the operations performed on this data. It is typically expressed using hardware description languages (HDLs), with Verilog being the most widely used HDL in the industry. Thus, throughout this paper, we use the terms RTL code and Ver- ilog code interchangeably. In modern hardware design, engineers usually begin with specifications in natural language, which are then manually trans- lated into HDLs before synthesizing the circuit elements (Blocklove et al., 2023). RTL serves as an intermediary between high-level design specifi- cations and low-level implementation details, en- abling designers to describe intricate digital sys- tems while retaining flexibility for synthesis into gate-level representations. Within EDA workflows, RTL plays a crucial role in various phases, includ- ing functional verification, performance estimation, synthesis, and optimization. Efficient handling of RTL code is essential for minimizing design time, improving performance, and ensuring correctness. 2.2 LLMs for RTL With the rapid development of artificial intelligence (AI), there has been increasing interest in leverag- ing these technologies to automate and enhance RTL-based design workflows (Chen et al., 2024).\n\n--- Segment 5 ---\nEfficient handling of RTL code is essential for minimizing design time, improving performance, and ensuring correctness. 2.2 LLMs for RTL With the rapid development of artificial intelligence (AI), there has been increasing interest in leverag- ing these technologies to automate and enhance RTL-based design workflows (Chen et al., 2024). A key area of focus has been the use of LLMs for RTL code generation and understanding, which has shown great promise in improving hardware design efficiency (Thakur et al., 2023; Liu et al., 2023; Lu et al., 2024). Recent works have fine-tuned open-source LLMs to generate high-quality RTL code from natural language descriptions (Chang et al., 2024; Liu et al., 2024; Thakur et al., 2024; Zehua et al., 2024; Zhang et al., 2024; Cui et al., 2024; Zhao et al., 2024), achieving significant im- provements in the automation of hardware design process. Additionally, models like DeepRTL (Liu et al., 2025) have extended these capabilities by introducing RTL code understanding tasks, i.e., summarizing the functionality of existing code, which facilitates collaboration and comprehension among hardware designers. Despite the great suc- cess achieved in these generation-based tasks, prior research has largely overlooked embedding-based tasks, which are equally critical for addressing chal- lenges in EDA. Embedding-based tasks, such as natural language code search, RTL code function- ality equivalence checking, and performance pre- diction, are essential for improving the efficiency of code reuse, verification, and optimization within hardware design workflows. Unlike generation- based tasks, which focus on producing new RTL code, embedding-based tasks involve understand- ing and analyzing existing designs, providing valu- able insights into design reusability, correctness, and performance. Meanwhile, even if some studies have applied machine learning techniques for hard- ware design verification (Vasudevan et al., 2021) and performance prediction (Fang et al., 2023), these efforts are either design-specific, lack gener- alizable representations for RTL designs, or do not operate directly at the RTL stage.\n\n--- Segment 6 ---\nUnlike generation- based tasks, which focus on producing new RTL code, embedding-based tasks involve understand- ing and analyzing existing designs, providing valu- able insights into design reusability, correctness, and performance. Meanwhile, even if some studies have applied machine learning techniques for hard- ware design verification (Vasudevan et al., 2021) and performance prediction (Fang et al., 2023), these efforts are either design-specific, lack gener- alizable representations for RTL designs, or do not operate directly at the RTL stage. In contrast, this work introduces DeepRTL2, a versatile model ca- pable of handling both generation- and embedding- based tasks, achieving superior performance across all evaluated tasks despite its versatility. 2.3 Embedding Capabilities of Decoder-Only LLMs Compared to bidirectional encoders like BERT (De- vlin, 2018) and encoder-decoder architectures like T5 (Raffel et al., 2020), decoder-only LLMs have demonstrated superior performance across a range of language tasks (Brown et al., 2020). However, their potential for text embedding tasks was largely overlooked until recently. In recent years, sev- eral studies have focused on adapting decoder-only LLMs for language embedding tasks (Jiang et al., 2023; Wang et al., 2023; BehnamGhader et al., 2024; Springer et al., 2024; Lei et al., 2024; Lee et al., 2024). Notably, Muennighoff et al. (2025) introduce the GRIT training strategy, which em- ploys a multi-task training objective function to enable a single decoder-only LLM to both generate content and encode text into fixed-length vectors. Despite their success on various language embed- ding benchmarks, these models primarily focus on general embedding tasks, which limits their effec- tiveness on specialized tasks like RTL embedding- based tasks. To the best of our knowledge, there is no model that has been specifically trained for RTL embedding, despite its critical role in optimiz- ing hardware design workflows. DeepRTL2 is the first model explicitly designed for RTL embedding- based tasks, outperforming general-purpose text embedding models on our benchmarks.\n\n--- Segment 7 ---\nTo the best of our knowledge, there is no model that has been specifically trained for RTL embedding, despite its critical role in optimiz- ing hardware design workflows. DeepRTL2 is the first model explicitly designed for RTL embedding- based tasks, outperforming general-purpose text embedding models on our benchmarks. 3 Dataset Previous research has predominantly focused on generation-based tasks, resulting in a notable gap in available datasets for the embedding-based tasks considered in this paper. Moreover, the availability of RTL code is limited even for generation-based tasks, due to the proprietary nature of hardware designs. To fill this gap, we have curated a compre- hensive dataset tailored to support both generation- and embedding-based tasks at the RTL stage. Fur- thermore, we have established new benchmarks specifically for the embedding-based tasks, which have been largely neglected in previous work. 3.1 Generation-Based Tasks 3.1.1 RTL Code Generation RTL code generation involves automatically syn- thesizing RTL code from user-defined natural lan- guage descriptions, streamlining hardware design and enabling a more accessible development pro- cess. To construct a high-quality dataset for this task, we follow the data construction pipeline proposed in DeepRTL (Liu et al., 2025), given its demonstrated effectiveness in generation-based tasks. The process begins by collecting .v files from GitHub1 using the keyword Verilog. Each file is then segmented into individual Verilog mod- ules, with each module representing a distinct func- tional unit. To ensure dataset quality and reduce redundancy, we remove modules that are predomi- nantly composed of comments or lack structurally complete module and endmodule declarations. Ad- ditionally, we apply MinHash and Jaccard similar- ity metrics (Yan et al., 2017) to eliminate duplicates. To further refine the dataset, we employ the Stagira Verilog parser (Chen et al., 2023) to filter out mod- 1 Original Code Refined Code Specification Line Comment Commented Code Functional Description Clean Add Comment Extract Summarize Distill User Query Rephrase Figure 2: The annotation process for the RTL code generation understanding dataset.\n\n--- Segment 8 ---\nAd- ditionally, we apply MinHash and Jaccard similar- ity metrics (Yan et al., 2017) to eliminate duplicates. To further refine the dataset, we employ the Stagira Verilog parser (Chen et al., 2023) to filter out mod- 1 Original Code Refined Code Specification Line Comment Commented Code Functional Description Clean Add Comment Extract Summarize Distill User Query Rephrase Figure 2: The annotation process for the RTL code generation understanding dataset. After obtaining the high-level functional description, we prompt GPT-4o to rephrase it into a user query format, which is then used to construct the natural language code search dataset. ules containing syntax errors, ensuring that only syntactically valid Verilog code is retained. For annotation, we adopt the chain-of-thought (CoT) prompting strategy used in DeepRTL, lever- aging GPT-4o (Hurst et al., 2024), a state-of-the-art LLM, to generate structured and informative an- notations. Specifically, we first query GPT-4o to insert line-level comments into the Verilog mod- ules, then extract line-level descriptions, pairing individual lines of RTL code with corresponding natural language explanations. Next, we prompt GPT-4o to generate a detailed specification for each module, comprising a summary of the module s functionality and a comprehensive explanation of its implementation process. By integrating these specifications with the module code, we construct high-level functional descriptions succinct one- sentence summaries that capture the core func- tionality of each Verilog module. The resulting dataset consists of Verilog modules enriched with line-level comments, detailed specifications, and succinct high-level functional descriptions, facili- tating both generation and understanding tasks in RTL design. Figure 2 provides an overview of the annotation process. For specifics on the prompts used, please refer to DeepRTL. To ensure the qual- ity of the generated annotations, we have conducted human evaluations, as detailed in Appendix A. To further expand the training dataset and im- prove model performance, we augment our dataset with open-source Verilog datasets from RTL- Coder (Liu et al., 2024), MG-Verilog (Zhang et al., 2024), and DeepCircuitX (Li et al., 2025).\n\n--- Segment 9 ---\nTo ensure the qual- ity of the generated annotations, we have conducted human evaluations, as detailed in Appendix A. To further expand the training dataset and im- prove model performance, we augment our dataset with open-source Verilog datasets from RTL- Coder (Liu et al., 2024), MG-Verilog (Zhang et al., 2024), and DeepCircuitX (Li et al., 2025). These datasets provide additional RTL designs with di- verse structures and functionalities, while also in- corporating different annotation strategies. The di- versity in annotations improves the model s adapt- ability to varying description styles, enhancing its robustness across various RTL-related tasks. 3.1.2 RTL Code Understanding RTL code understanding focuses on summarizing the functionality of existing Verilog code, enhanc- ing collaboration and comprehension among hard- ware designers. The dataset for this task is derived from the RTL code generation dataset, with Verilog code as input and corresponding natural language description as output. In the absence of a standard- ized benchmark for this task, we build upon the benchmark introduced in DeepRTL, which origi- nally comprises 100 Verilog designs. To improve evaluation reliability and ensure broader coverage, we extend this benchmark to include 500 high- quality Verilog modules with diverse functional- ities. Each module is annotated by professional hardware designers with a concise summary of its functionality along with a detailed description of the specific operations involved in its execution. This extended benchmark establishes a more ro- bust and comprehensive foundation for evaluating RTL code understanding capabilities. 3.2 Embedding-Based Tasks 3.2.1 Natural Language Code Search Natural language code search refers to the process of querying a large codebase using natural language to find relevant code snippets. It involves embed- ding both the user query and each code snippet into vectors, then calculating their similarity. The snip- pet with the highest similarity score is considered the best match for the user s requirements. This task is particularly crucial for hardware design, as it enables code reuse, improves efficiency, and ac- celerates the transition from user specifications to RTL code. For this task, we reuse the dataset and benchmark from the RTL code understanding task.\n\n--- Segment 10 ---\nThis task is particularly crucial for hardware design, as it enables code reuse, improves efficiency, and ac- celerates the transition from user specifications to RTL code. For this task, we reuse the dataset and benchmark from the RTL code understanding task. However, since the functional descriptions in the understanding dataset often contain specific iden- tifiers, introducing the risk of data leakage, and are too complex for direct use in practical code search, we employ GPT-4o to rephrase the descrip- tions into a user query format, as shown in Figure 2. The rephrasing ensures that the new descriptions meet the following conditions: (1) no references to specific identifiers, (2) retention of the core func- tionality and high-level logic, and (3) clarity and simplicity, resembling how a user would query for relevant code based on its functionality. After this rephrasing process, we obtain the natural language Code Internal Logic Extract Rewritten Code LEC Feedback LLM Generate Figure 3: The feedback-driven code rewrite process. code search dataset and benchmark in the format {(user_queryi, RTL_codei)}n i 1. For details on the prompt used to rephrase the functional descriptions, please refer to the Appendix B. 3.2.2 Functionality Equivalence Checking Functionality equivalence checking is a critical ver- ification step in hardware design, ensuring that dif- ferent RTL implementations exhibit identical be- havior despite structural differences. To construct a dataset for this task, we develop a feedback-driven CoT prompting strategy using GPT-4o, as shown in Figure 3. Given a Verilog module, we first prompt GPT-4o to introduce significant modifications to its internal logic while preserving its intended func- tionality. We then use Yosys (Wolf et al., 2013) to perform logic equivalence checking (LEC), which verifies whether the original and modified designs are functionally equivalent. Based on Yosys feed- back classified as equivalent, inequivalent, or syn- tax error we iteratively refine the modifications. Specifically, we incorporate the original design, rewritten design, and verification results into the prompt to guide GPT-4o in generating alternative implementations. This process is repeated for two to three rounds per design, ensuring a diverse set of functionally equivalent and inequivalent pairs.\n\n--- Segment 11 ---\nSpecifically, we incorporate the original design, rewritten design, and verification results into the prompt to guide GPT-4o in generating alternative implementations. This process is repeated for two to three rounds per design, ensuring a diverse set of functionally equivalent and inequivalent pairs. The resulting dataset consists of paired RTL de- signs, where some maintain functional equivalence while others introduce subtle variations. Since only implementation details differ, distinguishing equiv- alent from inequivalent designs presents a signifi- cant challenge for models. Additionally, we adapt RTLLM v2.0 (Lu et al., 2024), a Verilog generation benchmark, to construct a new benchmark for func- tionality equivalence checking. Applying the same feedback-driven CoT strategy to its 50 verified Ver- ilog designs, we generate multiple alternative im- plementations, expanding our benchmark to 400 code pairs. This benchmark provides a diverse and well-validated resource for evaluating functionality equivalence checking. For further details on this process, please refer to the Appendix C. 3.2.3 Performance Prediction Performance prediction plays a crucial role in the early stages of hardware design, enabling designers to estimate key circuit characteristics before physi- cal implementation. Accurate predictions allow for informed architectural decisions, reducing design iterations and improving overall efficiency. Among the commonly used PPA metrics, delay and area are the primary focus in early-stage evaluations, as ac- curate power estimation requires detailed workload to specify the circuit s dynamic behavior, which is unavailable at the RTL stage. In this work, we con- struct a performance prediction dataset by synthe- sizing and mapping RTL designs into netlists using Yosys (Wolf et al., 2013) with the SkyWater 130nm technology library (Google, 2021). We then uti- lize open-source ABC (Brayton and Mishchenko, 2010) tool to extract delay and area metrics, where delay metric is reported by the static timing analy- sis, and area metric reflects the total logic footprint, which directly impacts manufacturing cost. This process provides a dataset that captures essential performance characteristics of RTL designs, facili- tating learning-based performance estimation.\n\n--- Segment 12 ---\nWe then uti- lize open-source ABC (Brayton and Mishchenko, 2010) tool to extract delay and area metrics, where delay metric is reported by the static timing analy- sis, and area metric reflects the total logic footprint, which directly impacts manufacturing cost. This process provides a dataset that captures essential performance characteristics of RTL designs, facili- tating learning-based performance estimation. For a comprehensive summary of all dataset statistics, please refer to the Appendix D. 4 Methodology 4.1 Model Training We choose Llama-3.1 (Dubey et al., 2024) and DeepSeek-Coder (Guo et al., 2024) as the base models for training. Specifically, we fine-tune meta-llama Llama-3.1-8B-Instruct2 and deepseek- ai deepseek-coder-6.7b-instruct3. Our training con- sists of two stages. In the first stage, we follow the curriculum learning strategy adopted by Deep- RTL (Liu et al., 2025) and train the base model solely on RTL code generation and understanding data. In the second stage, we incorporate embed- ding data into the training set and train the model on both RTL code generation understanding and embedding tasks, utilizing the training framework of GRIT (Muennighoff et al., 2025).\n\n--- Segment 13 ---\nIn the first stage, we follow the curriculum learning strategy adopted by Deep- RTL (Liu et al., 2025) and train the base model solely on RTL code generation and understanding data. In the second stage, we incorporate embed- ding data into the training set and train the model on both RTL code generation understanding and embedding tasks, utilizing the training framework of GRIT (Muennighoff et al., 2025). 4.1.1 First-Stage Training Following DeepRTL, we apply a curriculum learn- ing strategy in the first stage of our training pipeline, which can be further divided into four sub-stages: 2 1-8B-Instruct 3 deepseek-coder-6.7b-instruct Model syntax function GPT-3.5 56.50 69.72 71.75 30.10 39.59 41.40 GPT-4o 72.00 77.31 78.53 49.70 56.80 58.84 o1-preview 76.20 83.71 84.00 50.00 60.86 62.52 CodeV-CodeLlama 47.70 74.96 82.20 22.00 39.49 45.74 CodeV-CodeQwen 51.50 77.71 82.17 23.10 44.54 52.22 CodeV-DeepSeek 57.60 80.23 83.25 30.00 49.63 54.74 DeepRTL-220m 60.69 78.81 80.88 28.79 45.86 49.66 DeepRTL-16b 63.79 74.82 80.05 38.91 47.24 51.72 Llama-3.1 32.40 57.01 62.76 14.60 26.04 30.16 DeepSeek-Coder 59.30 72.38 74.67 31.40 39.59 42.57 DeepRTL21st-Direct (Llama) 54.48 63.52 67.99 16.28 28.78 32.76 DeepRTL21st-Direct (DeepSeek) 60.60 73.12 75.70 32.50 44.42 47.96 DeepRTL21st (Llama) 67.90 77.53 79.52 43.70 49.98 50.00 DeepRTL21st (DeepSeek) 63.50 76.74 80.10 39.70 51.96 54.70 DeepRTL2 (Llama) 68.30 81.31 83.36 33.70 49.57 52.90 DeepRTL2 (DeepSeek) 71.60 80.58 81.75 38.50 52.62 55.99 Table 1: The performance evaluation for RTL code generation using the metric, with k set to 1, 5, and 10.\n\n--- Segment 14 ---\nIn the second stage, we incorporate embed- ding data into the training set and train the model on both RTL code generation understanding and embedding tasks, utilizing the training framework of GRIT (Muennighoff et al., 2025). 4.1.1 First-Stage Training Following DeepRTL, we apply a curriculum learn- ing strategy in the first stage of our training pipeline, which can be further divided into four sub-stages: 2 1-8B-Instruct 3 deepseek-coder-6.7b-instruct Model syntax function GPT-3.5 56.50 69.72 71.75 30.10 39.59 41.40 GPT-4o 72.00 77.31 78.53 49.70 56.80 58.84 o1-preview 76.20 83.71 84.00 50.00 60.86 62.52 CodeV-CodeLlama 47.70 74.96 82.20 22.00 39.49 45.74 CodeV-CodeQwen 51.50 77.71 82.17 23.10 44.54 52.22 CodeV-DeepSeek 57.60 80.23 83.25 30.00 49.63 54.74 DeepRTL-220m 60.69 78.81 80.88 28.79 45.86 49.66 DeepRTL-16b 63.79 74.82 80.05 38.91 47.24 51.72 Llama-3.1 32.40 57.01 62.76 14.60 26.04 30.16 DeepSeek-Coder 59.30 72.38 74.67 31.40 39.59 42.57 DeepRTL21st-Direct (Llama) 54.48 63.52 67.99 16.28 28.78 32.76 DeepRTL21st-Direct (DeepSeek) 60.60 73.12 75.70 32.50 44.42 47.96 DeepRTL21st (Llama) 67.90 77.53 79.52 43.70 49.98 50.00 DeepRTL21st (DeepSeek) 63.50 76.74 80.10 39.70 51.96 54.70 DeepRTL2 (Llama) 68.30 81.31 83.36 33.70 49.57 52.90 DeepRTL2 (DeepSeek) 71.60 80.58 81.75 38.50 52.62 55.99 Table 1: The performance evaluation for RTL code generation using the metric, with k set to 1, 5, and 10. The best results among all models are bolded, and the best results among open-source models are underlined.\n\n--- Segment 15 ---\n4.1.1 First-Stage Training Following DeepRTL, we apply a curriculum learn- ing strategy in the first stage of our training pipeline, which can be further divided into four sub-stages: 2 1-8B-Instruct 3 deepseek-coder-6.7b-instruct Model syntax function GPT-3.5 56.50 69.72 71.75 30.10 39.59 41.40 GPT-4o 72.00 77.31 78.53 49.70 56.80 58.84 o1-preview 76.20 83.71 84.00 50.00 60.86 62.52 CodeV-CodeLlama 47.70 74.96 82.20 22.00 39.49 45.74 CodeV-CodeQwen 51.50 77.71 82.17 23.10 44.54 52.22 CodeV-DeepSeek 57.60 80.23 83.25 30.00 49.63 54.74 DeepRTL-220m 60.69 78.81 80.88 28.79 45.86 49.66 DeepRTL-16b 63.79 74.82 80.05 38.91 47.24 51.72 Llama-3.1 32.40 57.01 62.76 14.60 26.04 30.16 DeepSeek-Coder 59.30 72.38 74.67 31.40 39.59 42.57 DeepRTL21st-Direct (Llama) 54.48 63.52 67.99 16.28 28.78 32.76 DeepRTL21st-Direct (DeepSeek) 60.60 73.12 75.70 32.50 44.42 47.96 DeepRTL21st (Llama) 67.90 77.53 79.52 43.70 49.98 50.00 DeepRTL21st (DeepSeek) 63.50 76.74 80.10 39.70 51.96 54.70 DeepRTL2 (Llama) 68.30 81.31 83.36 33.70 49.57 52.90 DeepRTL2 (DeepSeek) 71.60 80.58 81.75 38.50 52.62 55.99 Table 1: The performance evaluation for RTL code generation using the metric, with k set to 1, 5, and 10. The best results among all models are bolded, and the best results among open-source models are underlined. Model F1 text-embedding-3-small 0.189 text-embedding-3-large 0.290 GritLM-7B 0.269 DeepRTL2no-hard (Llama) 0.476 DeepRTL2no-hard (DeepSeek) 0.464 DeepRTL2 (Llama) 0.463 DeepRTL2 (DeepSeek) 0.453 Table 2: The performance evaluation for natural lan- guage code search using the F1 metric.\n\n--- Segment 16 ---\nThe best results among all models are bolded, and the best results among open-source models are underlined. Model F1 text-embedding-3-small 0.189 text-embedding-3-large 0.290 GritLM-7B 0.269 DeepRTL2no-hard (Llama) 0.476 DeepRTL2no-hard (DeepSeek) 0.464 DeepRTL2 (Llama) 0.463 DeepRTL2 (DeepSeek) 0.453 Table 2: The performance evaluation for natural lan- guage code search using the F1 metric. The best result is bolded, and the second-best result is underscored. training with line-level data, module-level data with specifications, module-level data with high-level descriptions, data with varying prompts. For details on these sub-stages, please refer to Appendix E. 4.1.2 Second-Stage Training Following GRIT, in the second stage of training, we combine the generation understanding and em- bedding tasks. For the generation understanding training, we reuse the high-quality data from the fourth sub-stage of the first-stage training. For the embedding task, we employ contrastive learning to learn contextualized representations that preserve the semantic information of the original text and code. Details for constructing the contrastive learn- ing training set can be found in Appendix F. In the embedding part of the second-stage training, we first use data that does not contain hard negatives and then incorporate data with hard negative sam- ples. For more details on the loss functions at dif- ferent sub-stages, please refer to Appendix G. For additional details on the hyperparameters and hard- ware resources used, please refer to Appendix H. 4.2 Model Evaluation For RTL code generation, we utilize the latest ver- sion of the widely adopted RTLLM v2.0 bench- mark (Lu et al., 2024), which contains 50 designs paired with corresponding natural language descrip- tions and testbenches.\n\n--- Segment 17 ---\nDetails for constructing the contrastive learn- ing training set can be found in Appendix F. In the embedding part of the second-stage training, we first use data that does not contain hard negatives and then incorporate data with hard negative sam- ples. For more details on the loss functions at dif- ferent sub-stages, please refer to Appendix G. For additional details on the hyperparameters and hard- ware resources used, please refer to Appendix H. 4.2 Model Evaluation For RTL code generation, we utilize the latest ver- sion of the widely adopted RTLLM v2.0 bench- mark (Lu et al., 2024), which contains 50 designs paired with corresponding natural language descrip- tions and testbenches. To measure Verilog gener- ation accuracy, we use the metric, which estimates the proportion of problems that can be solved at least once within k attempts: : E problems " 1 n c k n k (1) where n k represents the total number of trials for each problem, and c denotes the number of trails that pass the functional check. In our experiments, we set n 20 to mitigate randomness in results. The metric is reported for both syntactical and functional accuracy. Following RTLCoder (Liu et al., 2024), we evaluate performance across multi- ple generation temperatures (0.2, 0.5, and 0.8) and report the best performance across these settings. For RTL code understanding, we use the bench- mark constructed in Section 3.1.2. To evaluate the model s performance, we apply both traditional ma- chine translation metrics BLEU (Papineni et al., Model BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR Emb. Sim.\n\n--- Segment 18 ---\nTo evaluate the model s performance, we apply both traditional ma- chine translation metrics BLEU (Papineni et al., Model BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L METEOR Emb. Sim. GPT Score GPT-3.5 3.34 28.20 10.46 25.11 20.36 0.740 0.510 GPT-4o 4.59 29.26 11.48 25.74 22.78 0.761 0.549 o1-preview 3.73 28.00 10.39 24.98 20.48 0.748 0.535 CodeV-DeepSeek 3.05 25.14 9.78 23.25 20.23 0.705 0.495 CodeV-CodeQwen 2.80 24.91 8.27 22.75 21.07 0.747 0.499 DeepRTL-220m 13.06 37.56 19.85 34.72 34.37 0.806 0.600 DeepRTL-16b 12.85 37.43 19.34 34.63 33.09 0.802 0.597 Llama-3.1 2.68 25.37 10.39 23.75 17.16 0.730 0.430 DeepSeek-Coder 2.56 24.52 7.72 22.45 22.83 0.756 0.571 DeepRTL21st-Direct (Llama) 11.28 34.29 16.35 33.63 27.73 0.754 0.580 DeepRTL21st-Direct (DeepSeek) 12.07 36.37 17.78 33.78 28.56 0.767 0.602 DeepRTL21st (Llama) 13.34 37.74 19.54 34.76 33.46 0.798 0.594 DeepRTL21st (DeepSeek) 13.53 37.52 19.68 34.68 33.28 0.814 0.612 DeepRTL2 (Llama) 13.84 37.97 20.69 34.42 34.75 0.813 0.603 DeepRTL2 (DeepSeek) 13.96 37.93 20.73 34.34 34.74 0.820 0.616 Table 3: The performance evaluation for RTL code understanding.\n\n--- Segment 19 ---\nSim. GPT Score GPT-3.5 3.34 28.20 10.46 25.11 20.36 0.740 0.510 GPT-4o 4.59 29.26 11.48 25.74 22.78 0.761 0.549 o1-preview 3.73 28.00 10.39 24.98 20.48 0.748 0.535 CodeV-DeepSeek 3.05 25.14 9.78 23.25 20.23 0.705 0.495 CodeV-CodeQwen 2.80 24.91 8.27 22.75 21.07 0.747 0.499 DeepRTL-220m 13.06 37.56 19.85 34.72 34.37 0.806 0.600 DeepRTL-16b 12.85 37.43 19.34 34.63 33.09 0.802 0.597 Llama-3.1 2.68 25.37 10.39 23.75 17.16 0.730 0.430 DeepSeek-Coder 2.56 24.52 7.72 22.45 22.83 0.756 0.571 DeepRTL21st-Direct (Llama) 11.28 34.29 16.35 33.63 27.73 0.754 0.580 DeepRTL21st-Direct (DeepSeek) 12.07 36.37 17.78 33.78 28.56 0.767 0.602 DeepRTL21st (Llama) 13.34 37.74 19.54 34.76 33.46 0.798 0.594 DeepRTL21st (DeepSeek) 13.53 37.52 19.68 34.68 33.28 0.814 0.612 DeepRTL2 (Llama) 13.84 37.97 20.69 34.42 34.75 0.813 0.603 DeepRTL2 (DeepSeek) 13.96 37.93 20.73 34.34 34.74 0.820 0.616 Table 3: The performance evaluation for RTL code understanding. BLEU-4 refers to the smoothed BLEU-4 score, while Emb. Sim. represents the embedding similarity metric. The best results are highlighted in bold, and the second-best results are underscored.\n\n--- Segment 20 ---\nrepresents the embedding similarity metric. The best results are highlighted in bold, and the second-best results are underscored. 2002), ROUGE (Lin, 2004), and METEOR (Baner- jee and Lavie, 2005) which primarily assess lexi- cal similarity, as well as the embedding similar- ity and GPT score metrics introduced in Deep- RTL (Liu et al., 2025), which focus on semantic similarity. This combination of evaluation met- rics provides a comprehensive assessment of the model s ability to understand RTL code, capturing both surface-level and deeper, semantic-level un- derstanding. For further details on how to compute these metrics, please refer to the Appendix I. For natural language code search, we utilize the benchmark introduced in Section 3.2.1. To assess the model s ability to retrieve relevant code from a large codebase based on a user s query, we fol- low the bitext mining setting from MTEB (Muen- nighoff et al., 2022). In our evaluation process, the inputs consist of two sets: the first set contains functional descriptions, while the second set con- sists of Verilog code snippets. For each description in the first set, the best matching code snippet in the second set is identified using cosine similarity. We report F1 score, precision, and recall for each model, with F1 serving as the primary evaluation metric for natural language code search. For functionality equivalence checking, we uti- lize the benchmark introduced in Section 3.2.2. To evaluate the models ability to check functional equivalence, we follow the pair classification set- ting from MTEB (Muennighoff et al., 2022). In this evaluation, the inputs consist of several pairs of RTL codes. For each pair, the model assigns a binary label: 1 for "functionally equivalent" and 0 for "functionally inequivalent". The binary label is determined by calculating the cosine similarity of their embeddings and comparing the similarity score to a predefined threshold. For each model, we first identify the optimal accuracy threshold and compute the accuracy score. We then determine the best F1 threshold and report the F1, precision, and recall scores. Finally, we calculate the average pre- cision score based on the similarity scores of the code pairs and their corresponding ground-truth labels.\n\n--- Segment 21 ---\nWe then determine the best F1 threshold and report the F1, precision, and recall scores. Finally, we calculate the average pre- cision score based on the similarity scores of the code pairs and their corresponding ground-truth labels. Average precision is the primary evalua- tion metric for RTL code functionality equivalence checking, with other metrics also reported. For performance prediction, we use the dataset introduced in Section 3.2.3. This task aims to test the expressive power of code embeddings for pre- dicting performance metrics, such as area and de- lay, at the early stage of RTL design. To achieve this, we first encode each code snippet into a fixed- length vector and create a new dataset in the for- mat {(code_embedding Rp, area R, delay R)i}n i 1, where p is the embedding dimension and n is the dataset size. The dataset is then split into training and test sets at an 80:20 ratio. In this paper, we use XGBoost (Chen and Guestrin, 2016) as the regression model, training separate models for area and delay prediction. The trained models are eval- uated on the test set using r2_score, mean absolute percentage error (MAPE) and root relative squared error (RRSE), with their formulas provided below: r2_score(y, ˆy) 1 Pn i 1(yi ˆyi)2 Pn i 1(yi y)2 (2) MAPE(y, ˆy) 1 n n X i 1 yi ˆyi yi 100 (3) RRSE(y, ˆy) sPn i 1(yi ˆyi)2 Pn i 1(yi yi)2 (4) 5 Experimental Results 5.1 Generation-Based Tasks For comparison, we select several baseline models: the state-of-the-art commercial models, OpenAI s GPT-3.5, GPT-4o, and o1-preview, which represent the most advanced general-purpose LLMs currently available.\n\n--- Segment 22 ---\nIn this paper, we use XGBoost (Chen and Guestrin, 2016) as the regression model, training separate models for area and delay prediction. The trained models are eval- uated on the test set using r2_score, mean absolute percentage error (MAPE) and root relative squared error (RRSE), with their formulas provided below: r2_score(y, ˆy) 1 Pn i 1(yi ˆyi)2 Pn i 1(yi y)2 (2) MAPE(y, ˆy) 1 n n X i 1 yi ˆyi yi 100 (3) RRSE(y, ˆy) sPn i 1(yi ˆyi)2 Pn i 1(yi yi)2 (4) 5 Experimental Results 5.1 Generation-Based Tasks For comparison, we select several baseline models: the state-of-the-art commercial models, OpenAI s GPT-3.5, GPT-4o, and o1-preview, which represent the most advanced general-purpose LLMs currently available. We also include the CodeV series (Zhao et al., 2024), a collection of leading open-source models specifically designed for RTL code genera- tion, as well as the original DeepRTL models (Liu et al., 2025), which have shown strong performance in both RTL code generation and understanding. All these models have demonstrated excellent capa- bilities in Verilog generation-based tasks (Liu et al., 2025), making them strong baselines for evaluating the performance of DeepRTL2. Additionally, we report the performance of base models, Llama-3.1 and DeepSeek-Coder, to show the effectiveness of our dataset construction and training strategy. Table 1 reports the results for RTL code generation across different models, with k set to 1, 5, and 10. The results show that o1-preview out- performs all other models, likely due to its design for addressing complex tasks, including program- ming. The DeepRTL2 models, however, achieve the best performance among all open-source mod- els, with results comparable to GPT-4o. The per- formance improvement from base models to Deep- RTL2 highlights the effectiveness of our dataset construction process and training strategy.\n\n--- Segment 23 ---\nThe DeepRTL2 models, however, achieve the best performance among all open-source mod- els, with results comparable to GPT-4o. The per- formance improvement from base models to Deep- RTL2 highlights the effectiveness of our dataset construction process and training strategy. Further- more, DeepRTL2 outperforms the original Deep- RTL models, likely due to the incorporation of additional open-source datasets, aside from data sourced from GitHub, and the inclusion of more diverse problem formulations that enhance Deep- RTL2 s generalization ability. Given that Deep- RTL2 is a multi-task model and the generation benchmark may overlap with the training data used by OpenAI s models, these results highlight Deep- RTL2 s impressive performance for this task. Table 3 presents the results for RTL code un- derstanding. Since the CodeV-CodeLlama model outputs random messages for this task, we ex- clude it from the comparison. The results show that DeepRTL2 models significantly outperform all other models, including the previous state-of- the-art DeepRTL models, underscoring its strong capabilities in RTL code understanding. Notably, DeepRTL2 surpasses GPT-4o by a substantial mar- gin, despite the fact that its training data is anno- tated using GPT-4o. The main reason is that during benchmark testing, all models, including GPT-4o, are required to generate high-level functional de- scriptions directly from RTL code. As shown in Appendix A, CoT-based annotations are more ac- curate than direct annotations. This enhanced anno- tation quality contributes to DeepRTL2 s superior performance in RTL code understanding. 5.2 Embedding-Based Tasks Since none of the existing models are specifi- cally designed for RTL embedding-based tasks, the baselines used for the generation-based tasks, e.g., CodeV series and DeepRTL models, perform poorly in this setting. These models show near- zero performance, with an F1 score close to 0 on the natural language code search task and an aver- age precision of approximately 0.5 on the func- tionality equivalence checking task. Therefore, we select state-of-the-art general-purpose embed- ding models as baselines for comparison.\n\n--- Segment 24 ---\nThese models show near- zero performance, with an F1 score close to 0 on the natural language code search task and an aver- age precision of approximately 0.5 on the func- tionality equivalence checking task. Therefore, we select state-of-the-art general-purpose embed- ding models as baselines for comparison. These include OpenAI s text embedding models (text- embedding-3-small, text-embedding-3-large) (Nee- lakantan et al., 2022) and open-source models like GritLM-7B (Muennighoff et al., 2025). Table 2 presents the F1 scores for the natural lan- guage code search task. The results show that our DeepRTL2 models outperform all baseline models by a significant margin, demonstrating the effec- tiveness of our dataset and training strategy for this task. For the full evaluation results on natural language code search, please refer to Appendix J. Table 5 presents the average precision scores for the functionality equivalence checking task. The results show that DeepRTL2 models outperform all other baselines, demonstrating their effective- ness in capturing functional relationships between RTL modules. The full evaluation results are in Appendix J. It is important to emphasize that our embedding-based verification is not intended to re- place the traditional verification process, but rather to serve as an efficient preliminary step that can significantly streamline the verification flow. Table 4 presents the results for performance pre- diction on area and delay. Our DeepRTL2 series models outperform the baseline models across all metrics. These results highlight that the code em- beddings generated by the DeepRTL2 models are more expressive for predicting performance-related metrics such as area and delay.\n\n--- Segment 25 ---\nOur DeepRTL2 series models outperform the baseline models across all metrics. These results highlight that the code em- beddings generated by the DeepRTL2 models are more expressive for predicting performance-related metrics such as area and delay. Model Area Delay r2_score MAPE RRSE r2_score MAPE RRSE text-embedding-3-small 0.603 5.568 0.630 0.608 0.883 0.626 text-embedding-3-large 0.699 4.446 0.548 0.699 0.705 0.548 GritLM-7B 0.651 3.878 0.591 0.651 0.726 0.591 DeepRTL2no-hard (Llama) 0.510 2.828 0.700 0.735 0.471 0.515 DeepRTL2no-hard (DeepSeek) 0.805 2.947 0.445 0.743 0.449 0.507 DeepRTL2 (Llama) 0.759 1.966 0.490 0.773 0.469 0.476 DeepRTL2 (DeepSeek) 0.773 1.598 0.476 0.772 0.448 0.478 Table 4: The performance evaluation for performance prediction on area and delay using r2_score, MAPE and RRSE metrics. The best results among all models are bolded, and the second-best results are underscored. Model Average Precision text-embedding-3-small 0.565 text-embedding-3-large 0.498 GritLM-7B 0.541 DeepRTL2no-hard (Llama) 0.518 DeepRTL2no-hard (DeepSeek) 0.481 DeepRTL2 (Llama) 0.667 DeepRTL2 (DeepSeek) 0.591 Table 5: The performance evaluation for RTL code functionality equivalence checking using the average precision metric. The best result among all models is bolded, and the second-best result is underscored. 5.3 Ablation Studies In this section, we conduct ablation studies to demonstrate the effectiveness of different dataset components and training strategies.\n\n--- Segment 26 ---\nThe best result among all models is bolded, and the second-best result is underscored. 5.3 Ablation Studies In this section, we conduct ablation studies to demonstrate the effectiveness of different dataset components and training strategies. In the first training stage, we adopt a curriculum learning strat- egy, where the model is progressively trained on line-level data, module-level data with specifica- tions, module-level data with high-level descrip- tions, and data with varying prompts. While the benefits of curriculum learning have been shown in DeepRTL (Liu et al., 2025), we extend this analysis with additional comparisons. Specifically, we compare our first-stage model (DeepRTL21st) with a variant trained without curriculum learning (DeepRTL21st-Direct), both focused on generation- based tasks. As shown in Table 1 and Table 3, the incorporation of curriculum learning significantly improves performance for both code generation and understanding tasks. When we further intro- duce the second-stage training, i.e., GRIT-based fine-tuning, the performance improves even more, demonstrating the effectiveness of both curriculum learning and GRIT-based fine-tuning strategies. In the second training stage, we combine con- trastive learning and curriculum learning to ensure that our model performs effectively on embedding- based tasks. Specifically, we start with data that excludes hard negatives and gradually introduce hard negative samples, which improves overall performance. To evaluate this strategy, we com- pare DeepRTL2 with and without hard negatives (DeepRTL2no-hard) in Tables 2, 4, and 5. Since hard negatives primarily influence contrastive learn- ing, these comparisons focus on embedding-based tasks, with negligible impact on generation-based performance. The results show a minor drop in natural language code search accuracy but substan- tial gains in functionality equivalence checking and performance prediction. Despite the small accuracy decrease in the natural language code search task, DeepRTL2 still outperforms powerful baseline em- bedding models. This improvement in functionality equivalence checking and performance prediction justifies our decision to integrate hard negatives into the training process.\n\n--- Segment 27 ---\nDespite the small accuracy decrease in the natural language code search task, DeepRTL2 still outperforms powerful baseline em- bedding models. This improvement in functionality equivalence checking and performance prediction justifies our decision to integrate hard negatives into the training process. 6 Conclusion In this work, we present DeepRTL2, a novel fam- ily of LLMs that unifies both generation- and embedding-based tasks at the RTL stage, offer- ing a comprehensive solution to the diverse chal- lenges in EDA. By addressing critical tasks in- cluding RTL code generation, understanding, nat- ural language code search, functionality equiva- lence checking, and performance prediction, Deep- RTL2 significantly improves the efficiency of hard- ware design workflows. To develop DeepRTL2, we have curated a comprehensive dataset and es- tablished new benchmarks specifically designed for these tasks, particularly the embedding-based ones, for which no suitable resources previously existed. Furthermore, we adapt the GRIT approach to fine-tune the model, enabling it to manage both generation- and embedding-based tasks effectively. Extensive experimentation demonstrates that Deep- RTL2 achieves state-of-the-art performance across all evaluated tasks, advancing the application of LLMs in hardware design. 7 Limitations There are two main limitations in our work. First, due to the multi-task nature of our model and con- straints in time and computing resources, we may not have employed the most optimal training strat- egy and hyperparameter settings to maximize per- formance across all tasks. Second, performance prediction directly at the RTL stage is challenging, as RTL designs typically lack detailed information about delay and area metrics. Although our model outperforms others in the evaluation, a significant gap remains in achieving accurate predictions. We hypothesize that incorporating the control data flow graph (CDFG) of RTL designs, which offers a more structured representation of the design s behavior, may facilitate better learning of performance char- acteristics. In future work, we plan to explore how incorporating CDFG into the DeepRTL2 model se- ries could improve the model s ability to predict performance metrics more accurately. References Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments.\n\n--- Segment 28 ---\n2005. Meteor: An automatic metric for mt evaluation with improved cor- relation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and or summariza- tion, pages 65 72. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961. Jason Blocklove, Siddharth Garg, Ramesh Karri, and Hammond Pearce. 2023. Chip-chat: Challenges and opportunities in conversational hardware design. In 2023 ACM IEEE 5th Workshop on Machine Learning for CAD (MLCAD), pages 1 6. IEEE. Andres M Bran and Philippe Schwaller. 2024. Trans- formers and large language models for chemistry and drug discovery. In Drug Development Supported by Informatics, pages 143 163. Springer. Robert Brayton and Alan Mishchenko. 2010. Abc: An academic industrial-strength verification tool. In Computer Aided Verification: 22nd International Conference, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings 22, pages 24 40. Springer. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877 1901. Kaiyan Chang, Kun Wang, Nan Yang, Ying Wang, Dan- tong Jin, Wenlong Zhu, Zhirong Chen, Cangyuan Li, Hao Yan, Yunhao Zhou, et al. 2024. Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework. In Proceedings of the 61st ACM IEEE Design Automa- tion Conference, pages 1 6.\n\n--- Segment 29 ---\nData is all you need: Finetuning llms for chip design via an automated design-data augmentation framework. In Proceedings of the 61st ACM IEEE Design Automa- tion Conference, pages 1 6. Lei Chen, Yiqi Chen, Zhufei Chu, Wenji Fang, Tsung- Yi Ho, Ru Huang, Yu Huang, Sadaf Khan, Min Li, Xingquan Li, et al. 2024. Large circuit models: op- portunities and challenges. Science China Informa- tion Sciences, 67(10):1 42. Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining, pages 785 794. Xiangli Chen, Yuehua Meng, and Gang Chen. 2023. In- cremental verilog parser. In 2023 International Sym- posium of Electronics Design Automation (ISEDA), pages 236 240. IEEE. Fan Cui, Chenyang Yin, Kexing Zhou, Youwei Xiao, Guangyu Sun, Qiang Xu, Qipeng Guo, Demin Song, Dahua Lin, Xingcheng Zhang, et al. 2024. Ori- gen: Enhancing rtl code generation with code-to- code augmentation and self-reflection. arXiv preprint arXiv:2407.16237. Jacob Devlin. 2018. Bert: Pre-training of deep bidi- rectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Wenji Fang, Yao Lu, Shang Liu, Qijun Zhang, Ceyu Xu, Lisa Wu Wills, Hongce Zhang, and Zhiyao Xie. 2023. Masterrtl: A pre-synthesis ppa estimation framework for any rtl design.\n\n--- Segment 30 ---\n2023. Masterrtl: A pre-synthesis ppa estimation framework for any rtl design. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pages 1 9. IEEE. Google. 2021. Skywater pdk. Accessed: 2025-02-11. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196. Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os- trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. 2023. Scaling sentence embeddings with large language models. arXiv preprint arXiv:2307.16645. Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nv-embed: Improved techniques for training llms as generalist embedding models. arXiv preprint arXiv:2405.17428. Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, and Andrew Yates. 2024. Meta-task prompting elicits embedding from large language models. arXiv preprint arXiv:2402.18458. Zeju Li, Changran Xu, Zhengyuan Shi, Zedong Peng, Yi Liu, Yunhao Zhou, Lingfeng Zhou, Chengyu Ma, Jianyuan Zhong, Xi Wang, et al. 2025. Deepcircuitx: A comprehensive repository-level dataset for rtl code understanding, generation, and ppa analysis. arXiv preprint arXiv:2502.18297. Chin-Yew Lin.\n\n--- Segment 31 ---\narXiv preprint arXiv:2502.18297. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74 81. Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. 2023. Verilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), pages 1 8. IEEE. Shang Liu, Wenji Fang, Yao Lu, Jing Wang, Qijun Zhang, Hongce Zhang, and Zhiyao Xie. 2024. Rtl- coder: Fully open-source and efficient llm-assisted rtl code generation technique. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Yi Liu, Changran XU, Yunhao Zhou, Zeju Li, and Qiang Xu. 2025. DeepRTL: Bridging verilog understanding and generation with a unified representation model. In The Thirteenth International Conference on Learn- ing Representations. Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. 2024. Rtllm: An open-source benchmark for design rtl gen- eration with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 722 727. IEEE. Niklas Muennighoff. 2022. Sgpt: Gpt sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904. Niklas Muennighoff, Hongjin SU, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2025. Generative representational in- struction tuning. In The Thirteenth International Conference on Learning Representations. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316.\n\n--- Segment 32 ---\nMteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad- ford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pre- training. arXiv preprint arXiv:2201.10005. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computa- tional Linguistics, pages 311 318. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the lim- its of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1 67. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah- davi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172 180. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. 2024. Rep- etition improves language model embeddings. arXiv preprint arXiv:2402.15449. Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Ham- mond Pearce, Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2023. Bench- marking large language models for automated ver- ilog rtl code generation. In 2023 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1 6. IEEE.\n\n--- Segment 33 ---\nIn 2023 Design, Automation Test in Europe Conference Exhibition (DATE), pages 1 6. IEEE. Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri, and Siddharth Garg. 2024. Verigen: A large language model for verilog code generation. ACM Transac- tions on Design Automation of Electronic Systems, 29(3):1 31. Shobha Vasudevan, Wenjie Joe Jiang, David Bieber, Rishabh Singh, C Richard Ho, Charles Sutton, et al. 2021. Learning semantic representations to verify hardware designs. Advances in Neural Information Processing Systems, 34:23491 23504. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368. Clifford Wolf, Johann Glaser, and Johannes Kepler. 2013. Yosys - a free Verilog synthesis suite. In Proceedings of the 21st Austrian Workshop on Micro- electronics (Austrochip), volume 97. Ziqi Yan, Jiqiang Liu, Gang Li, Zhen Han, and Shuo Qiu. 2017. Privmin: Differentially private minhash for jaccard similarity computation. arXiv preprint arXiv:1705.07258. PEI Zehua, Huiling Zhen, Mingxuan Yuan, Yu Huang, and Bei Yu. 2024. Betterv: Controlled verilog gen- eration with discriminative guidance. In Forty-first International Conference on Machine Learning. Yongan Zhang, Zhongzhi Yu, Yonggan Fu, Cheng Wan, and Yingyan Celine Lin. 2024. Mg-verilog: Multi- grained dataset towards enhanced llm-assisted ver- ilog generation. In 2024 IEEE LLM Aided Design Workshop (LAD), pages 1 5. IEEE. Yang Zhao, Di Huang, Chongxiao Li, Pengwei Jin, Ziyuan Nan, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, et al. 2024.\n\n--- Segment 34 ---\nYang Zhao, Di Huang, Chongxiao Li, Pengwei Jin, Ziyuan Nan, Tianyun Ma, Lei Qi, Yansong Pan, Zhenxing Zhang, Rui Zhang, et al. 2024. Codev: Empowering llms for verilog generation through multi-level summarization. arXiv preprint arXiv:2407.10424. A Human Evaluation of Generated Annotations To evaluate the reliability and accuracy of GPT-4o- generated annotations, we conduct a human eval- uation focusing primarily on the accuracy of high- level functional descriptions, as this is the most challenging and critical aspect of the generation- based tasks. We randomly sample 200 annotated RTL modules and ask professional hardware de- signers to verify the correctness of the generated descriptions. The human evaluation results show that approximately 90 of these annotations are accurate. In comparison, when we test direct an- notations, i.e., generating high-level functional de- scriptions directly from the original code, the ac- curacy drops significantly to 70 . This significant difference further demonstrates the effectiveness of the CoT-based annotation strategy. Additionally, GPT-4o is employed for rewriting RTL code in the functionality equivalence check- ing task. For this task, we address concerns about accuracy by using EDA tools to verify the function- ality equivalence of the rewritten code against the original code. Therefore, all the data collected for this task is validated as ground truth, ensuring the quality and correctness of the rewritten RTL code. B Prompt For Rephrasing Descriptions Figure 4 shows the instruction given to GPT-4o to rephrase the code descriptions into their corre- sponding user query formats. C Code Rewrite Instructions Figure 5 illustrates the code rewrite instructions provided to GPT-4o for constructing the function- ality equivalence checking dataset. The leftmost I will provide a Verilog code snippet that defines a module, along with its corresponding natural language description. The description may reference specific identifiers in the code, such as variable names and module names. Your task is to rephrase the description by following these steps: 1. Remove any references to specific identifiers (e.g., module names, variable names, and specific signals). 2. Retain the core functional meaning and high-level logic of the description. 3.\n\n--- Segment 35 ---\nRetain the core functional meaning and high-level logic of the description. 3. Ensure the rephrased description is concise and suitable for a query, resembling how a user would search for the relevant code based on its functionality. 4. Avoid unnecessary technical details and long-winded explanations. Keep it simple and to the point. Please return only the rephrased description. code: {code} text: {text} Figure 4: The instruction for rephrasing the code de- scription into the user query format. column presents the instruction used during the ini- tial rewrite process, where only the original RTL code is available. The subsequent three columns represent instructions based on previously rewrit- ten code, corresponding to the following cases: (1) equivalent rewritten code, (2) inequivalent rewrit- ten code, and (3) rewritten code with syntax errors. Notably, in addition to the code itself, we also in- clude the functional description and specification from Section 3.1.1. This additional context helps the model better understand the intended function- ality, leading to improved accuracy in rewriting the code while preserving its functionality. D Dataset Statistics Table 6 presents the overall statistics for all datasets used across the evaluated tasks. Except the perfor- mance prediction datasets, all datasets listed in this table are utilized for model training. For the per- formance prediction datasets, we split them in an 80:20 ratio, creating a training set with 15,000 sam- ples and a test set with 3,766 samples. For perfor- mance prediction, we regress area and delay based on code embeddings, without tuning the model. E Details of First-Stage Training In Section 3.1.1, we construct a dataset consisting of Verilog modules enriched with line-level com- ments, detailed specifications, and succinct high- level functional descriptions. These three levels of annotations correspond to the first three sub- stages of our first-stage training pipeline. In the first sub-stage, we train the model using line-level data, where each line of Verilog code is paired with a corresponding natural language comment. The You are an expert in Verilog RTL code design, with extensive experience in optimizing code for performance, power, and area (PPA).\n\n--- Segment 36 ---\nIn the first sub-stage, we train the model using line-level data, where each line of Verilog code is paired with a corresponding natural language comment. The You are an expert in Verilog RTL code design, with extensive experience in optimizing code for performance, power, and area (PPA). Given the following Verilog code, please rewrite it to achieve the same functionality using a different implementation approach, while considering potential improvements to PPA metrics. The provided code is: "{code}" Functional Description (a high-level description of the functionality of the Verilog code): "{functional_description}" Specification (detailed implementation requirements of the Verilog code): "{specification}" Please rewrite the code to retain the same functionality but with a different implementation style. You can use the functional description and specification as a reference, but note that these are not fully accurate. Therefore, treat the provided code as the "golden reference" for the intended functionality. I encourage you to propose significant changes that may lead to improvements in PPA after synthesis. However, it is crucial that the rewritten code performs exactly the same as the original. Please provide the rewritten code in the following format: verilog [rewritten code] Note: Output only the rewritten code and do not include any additional explanations or comments. Additionally, ensure that only the implementation of the internal logic of the code is modified; you are forbidden to change the module head declaration. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Significantly change the implementation style. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Ensure that the new implementation differs from the provided rewritten code, using a different implementation style. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Take into account the discrepancies between the original code and the rewritten code. Use the rewritten code as a reference and make adjustments to ensure the final version retains the intended functionality of the original code. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption.\n\n--- Segment 37 ---\n3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. In summary, your task is to: 1. Keep the same functionality as the original code. 2. Take into account the syntax errors present in the rewritten code. Ensure that the new code is syntactically correct and functions as intended, while preserving the original functionality. 3. Consider potential improvements to PPA metrics after synthesis, such as optimization for area, timing, or power consumption. Rewritten Code (the existing rewritten code that is functionally equivalent to the original code): "{rewritten_code}" Rewritten Code (the existing rewritten code that is functionally inequivalent to the original code): "{rewritten_code}" Rewritten Code (the existing rewritten code that contains syntax errors): "{rewritten_code}" None Initial Equal Unequal Syntax Error Figure 5: The code rewrite instructions used to construct the functionality equivalence checking dataset. second sub-stage utilizes module-level data with specifications, providing more detailed descriptions of the Verilog modules. The third sub-stage focuses on module-level data with high-level functional de- scriptions, offering a broader functional overview of the code. To further refine the dataset and adapt it to a wider range of scenarios, we introduce a fourth sub-stage, where GPT-4o generates varying prompts based on the high-quality data from the third sub-stage. These varying prompts represent different problem descriptions used to generate Ver- ilog code. We find that incorporating this sub-stage improves the model s performance and robustness, as it allows the model to better generalize across a wide range of code generation tasks. F Contrastive Learning Training Set Construction In the second-stage training, we apply contrastive learning to enable the model to (1) determine whether a Verilog module matches a given func- tional description; and (2) assess whether two Ver- ilog code snippets are functionally equivalent. To construct a dataset for contrastive learning, we first prompt GPT-4o to rewrite Verilog code snippets from the natural language code search training set. The rewrite process is illustrated in Figure 3.\n\n--- Segment 38 ---\nTo construct a dataset for contrastive learning, we first prompt GPT-4o to rewrite Verilog code snippets from the natural language code search training set. The rewrite process is illustrated in Figure 3. After several iterations, we combine the original natural language code search training set with their rewritten code snippets, resulting in four types of new data samples: type a: {original_text, original_code} type b: {original_text, original_code, equiva- lent_code} type c: {original_text, original_code, inequiv- alent_code} type d: {original_text, original_code, equiva- lent_code, inequivalent_code} Since the format of an original data sample in the natural language code search training set is {orig- inal_text, original_code}, the four types of data samples correspond to the following scenarios: type a corresponds to the case where all rewrit- ten code snippets contain syntax errors. type b corresponds to the case where all rewrit- ten code snippets, free of syntax errors, are functionally equivalent to the original code. type c corresponds to the case where all rewrit- ten code snippets, free of syntax errors, are not functionally equivalent to the original code. Task Description Source Count RTL Code Generation Understanding Line Level DeepRTL2 341310 Module Level (Detailed Specification) DeepRTL2 45519 MG-Verilog 10035 DeepCircuitX 32809 Module Level (High-Level Description) DeepRTL2 46876 RTLCoder 25001 MG-Verilog 10037 DeepCircuitX 38179 Natural Language Code Search N A DeepRTL2 59700 Functionality Equivalence Checking Equal Pairs DeepRTL2 9532 Unequal Pairs DeepRTL2 23330 Performance Prediction Area DeepRTL2 18766 Delay DeepRTL2 18766 Table 6: The overall dataset statistics for all evaluated tasks. type d corresponds to the case where some rewritten code snippets, free of syntax er- rors, are functionally equivalent to the original code, while others are not.\n\n--- Segment 39 ---\nTask Description Source Count RTL Code Generation Understanding Line Level DeepRTL2 341310 Module Level (Detailed Specification) DeepRTL2 45519 MG-Verilog 10035 DeepCircuitX 32809 Module Level (High-Level Description) DeepRTL2 46876 RTLCoder 25001 MG-Verilog 10037 DeepCircuitX 38179 Natural Language Code Search N A DeepRTL2 59700 Functionality Equivalence Checking Equal Pairs DeepRTL2 9532 Unequal Pairs DeepRTL2 23330 Performance Prediction Area DeepRTL2 18766 Delay DeepRTL2 18766 Table 6: The overall dataset statistics for all evaluated tasks. type d corresponds to the case where some rewritten code snippets, free of syntax er- rors, are functionally equivalent to the original code, while others are not. For all four types of data samples, we convert them into contrastive learning samples as follows: type a: {"query": original_code, "pos": origi- nal_text, "neg": None} {"query": original_text, "pos": origi- nal_code, "neg": None} type b: {"query": original_code, "pos": origi- nal_text, "neg": None} {"query": original_text, "pos": origi- nal_code, "neg": None} {"query": original_code, "pos": equiva- lent_code, "neg": None} {"query": equivalent_code, "pos": origi- nal_code, "neg": None} type c: {"query": original_code, "pos": origi- nal_text, "neg": inequivalent_code} {"query": original_text, "pos": origi- nal_code, "neg": None} type d: {"query": original_code, "pos": origi- nal_text, "neg": inequivalent_code} {"query": original_code, "pos": equiva- lent_code, "neg": inequivalent_code} {"query": original_text, "pos": origi- nal_code, "neg": None} {"query": equivalent_code, "pos": origi- nal_code, "neg": inequivalent_code} In each of the contrastive learning samples above, the key pos refers to the positive instance of the query code text, while the key neg refers to the hard negative instance.\n\n--- Segment 40 ---\ntype d corresponds to the case where some rewritten code snippets, free of syntax er- rors, are functionally equivalent to the original code, while others are not. For all four types of data samples, we convert them into contrastive learning samples as follows: type a: {"query": original_code, "pos": origi- nal_text, "neg": None} {"query": original_text, "pos": origi- nal_code, "neg": None} type b: {"query": original_code, "pos": origi- nal_text, "neg": None} {"query": original_text, "pos": origi- nal_code, "neg": None} {"query": original_code, "pos": equiva- lent_code, "neg": None} {"query": equivalent_code, "pos": origi- nal_code, "neg": None} type c: {"query": original_code, "pos": origi- nal_text, "neg": inequivalent_code} {"query": original_text, "pos": origi- nal_code, "neg": None} type d: {"query": original_code, "pos": origi- nal_text, "neg": inequivalent_code} {"query": original_code, "pos": equiva- lent_code, "neg": inequivalent_code} {"query": original_text, "pos": origi- nal_code, "neg": None} {"query": equivalent_code, "pos": origi- nal_code, "neg": inequivalent_code} In each of the contrastive learning samples above, the key pos refers to the positive instance of the query code text, while the key neg refers to the hard negative instance. In the embedding part of the second-stage training, we first use samples col- ored blue that do not contain hard negatives and then incorporate samples colored purple with hard negative instances. G Training Loss Function In the second stage of training, we combine gen- eration understanding and embedding tasks. For generation understanding, we reuse high-quality data from the fourth sub-stage of the first training stage. For the embedding tasks, we apply con- trastive learning to learn contextualized represen- tations that preserve the semantic information of text and code.\n\n--- Segment 41 ---\nFor generation understanding, we reuse high-quality data from the fourth sub-stage of the first training stage. For the embedding tasks, we apply con- trastive learning to learn contextualized represen- tations that preserve the semantic information of text and code. In the embedding part of the second- stage training, we first use data without hard nega- tives and later incorporate data with hard negatives. The embedding loss function is defined as follows: E i exp σ(fθ(xi), fθ(x i )) τ (5) S i M X j 1 exp σ(fθ(xi), fθ(x j )) τ ! (6) S i M X j 1 exp σ(fθ(xi), fθ(x j )) τ ! (7) Model Precision Recall F1 (Main Metric) text-embedding-3-small 0.173 0.241 0.189 text-embedding-3-large 0.273 0.340 0.290 GritLM-7B 0.255 0.320 0.269 DeepRTL2no-hard (Llama) 0.469 0.497 0.476 DeepRTL2no-hard (DeepSeek) 0.456 0.489 0.464 DeepRTL2 (Llama) 0.450 0.493 0.463 DeepRTL2 (DeepSeek) 0.443 0.481 0.453 Table 7: The full performance evaluation results for natural language code search. The best results among all models are bolded, and the second-best results are underscored. Lemb1 1 M M X i 1 log E i S i (8) Lemb2 1 M M X i 1 log E i S i S i (9) where M is the batch size, xi is the i-th training sample, fθ is the embedding function (in this paper, we use position-weighted mean pooling method introduced in SGPT (Muennighoff, 2022) to ob- tain sentence embeddings), τ is the temperature hyperparameter, and σ is the similarity function (typically cosine similarity). x i is the positive in- stance of the i-th training sample, while x i is the hard negative of the i-th training sample.\n\n--- Segment 42 ---\nLemb1 1 M M X i 1 log E i S i (8) Lemb2 1 M M X i 1 log E i S i S i (9) where M is the batch size, xi is the i-th training sample, fθ is the embedding function (in this paper, we use position-weighted mean pooling method introduced in SGPT (Muennighoff, 2022) to ob- tain sentence embeddings), τ is the temperature hyperparameter, and σ is the similarity function (typically cosine similarity). x i is the positive in- stance of the i-th training sample, while x i is the hard negative of the i-th training sample. Lemb1 represents the embedding loss when no hard neg- ative is available for each training sample, while Lemb2 corresponds to the embedding loss when a hard negative instance is present for each sample. For generation understanding, we adopt the tra- ditional next-token cross-entropy loss: Lgen 1 N N X i 1 log P(fθ,η(x(i)) fθ,η(x( i))) (10) where η is the language modeling head used for generation-based tasks. In the second-stage train- ing, we first use L1 Lemb1 Lgen as the loss function, then switch to L2 Lemb2 Lgen. H Hyperparameters All experiments are conducted on a cluster equipped with eight NVIDIA A800 GPUs, each with 80GB of memory. Tables 9 and 10 present the hyperparameter settings used in the first-stage and second-stage training, respectively. I Understanding Evaluation Metrics To evaluate the models understanding capabilities of RTL code, we apply both traditional machine translation metrics BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) which primarily assess lexical simi- larity, as well as the embedding similarity and GPT score introduced in DeepRTL (Liu et al., 2025), which focus on semantic similarity. These met- rics measure the similarity between the generated descriptions and the ground truth summaries. Specifically, BLEU measures the proportion of n-grams (sequences of n words) in the generated text that also appear in the reference text.\n\n--- Segment 43 ---\nThese met- rics measure the similarity between the generated descriptions and the ground truth summaries. Specifically, BLEU measures the proportion of n-grams (sequences of n words) in the generated text that also appear in the reference text. It cal- culates the overlap of n-grams (typically up to a length of 4), with higher scores assigned to more matches. BLEU is precision-focused and rewards the accurate use of words or phrases in the gener- ated descriptions. In our evaluation, we report the smoothed BLEU-4 score to address zero counts in higher-order n-grams, which helps to avoid penal- izing models for small discrepancies. ROUGE is a recall-based metric that evaluates the proportion of n-grams in the reference summary that are present in the generated summary. For our evaluation, we report ROUGE-1 (unigram over- lap), ROUGE-2 (bigram overlap), and ROUGE-L (longest common subsequence). METEOR combines both precision and recall while also accounting for synonymy, stemming, and word order. It computes unigram precision and recall and applies a penalty for word order mis- matches. For calculating these traditional machine translation metrics, we directly use the correspond- ing functions from Python libraries nltk (for BLEU and METEOR) and rouge (for ROUGE).\n\n--- Segment 44 ---\nIt computes unigram precision and recall and applies a penalty for word order mis- matches. For calculating these traditional machine translation metrics, we directly use the correspond- ing functions from Python libraries nltk (for BLEU and METEOR) and rouge (for ROUGE). In contrast to the lexical metrics, embedding similarity and GPT score evaluate semantic simi- larity by assessing how well the generated descrip- Model Average Precision (Main Metric) Accuracy F1 Precision Recall text-embedding-3-small 0.565 0.581 0.646 0.525 0.840 text-embedding-3-large 0.498 0.544 0.647 0.478 1.000 GritLM-7B 0.541 0.613 0.661 0.503 0.960 DeepRTL2no-hard (Llama) 0.518 0.594 0.661 0.497 0.987 DeepRTL2no-hard (DeepSeek) 0.481 0.581 0.658 0.497 0.973 DeepRTL2 (Llama) 0.667 0.681 0.723 0.575 0.973 DeepRTL2 (DeepSeek) 0.591 0.619 0.708 0.552 0.987 Table 8: The full performance evaluation results for RTL code functionality equivalence checking. The best results among all models are bolded, and the second-best results are underscored. Hyperparameter Name Value finetuning_type lora per_device_train_batch_size 4 gradient_accumulation_steps 4 lr_scheduler_type cosine warm_up_ratio 0.1 learning_rate 5e-5 epochs 3 Table 9: Hyperparameters selected for the first training stage of DeepRTL2. Hyperparameter Name Value finetuning_type full per_device_embedding_batch_size 4 per_device_generative_batch_size 4 gradient_accumulation_steps 8 lr_scheduler_type linear warmup_ratio 0.03 learning_rate 2e-5 epochs 1 temperature (τ) 0.02 Table 10: Hyperparameters selected for the second train- ing stage of DeepRTL2.\n\n--- Segment 45 ---\nHyperparameter Name Value finetuning_type lora per_device_train_batch_size 4 gradient_accumulation_steps 4 lr_scheduler_type cosine warm_up_ratio 0.1 learning_rate 5e-5 epochs 3 Table 9: Hyperparameters selected for the first training stage of DeepRTL2. Hyperparameter Name Value finetuning_type full per_device_embedding_batch_size 4 per_device_generative_batch_size 4 gradient_accumulation_steps 8 lr_scheduler_type linear warmup_ratio 0.03 learning_rate 2e-5 epochs 1 temperature (τ) 0.02 Table 10: Hyperparameters selected for the second train- ing stage of DeepRTL2. tion captures the underlying meaning of the RTL code, rather than focusing solely on surface-level word matches. Embedding similarity computes the cosine similarity between the embeddings of the generated description and the ground truth sum- mary, derived from OpenAI s text-embedding-3- large model. This metric rewards models for pro- ducing descriptions that are semantically closer to the reference, even if the wording differs. The GPT score, based on GPT-4o, quantifies the semantic coherence between descriptions by assigning a sim- ilarity score between 0 and 1, where 1 indicates perfect alignment. Unlike lexical metrics, the GPT score focuses on semantic accuracy rather than ex- act word matching. For the prompt used in calcu- lating the GPT score, please refer to DeepRTL. Together, these metrics offer a comprehensive evaluation of both lexical precision and semantic accuracy, providing a holistic view of the model s understanding of RTL code. J Full Evaluation Results J.1 Natural Language Code Search The full evaluation results for natural language code search are presented in Table 7. Results show that the DeepRTL2 models significantly out- perform all baseline models across all metrics. Specifically, DeepRTL2 (Llama) and DeepRTL2 (DeepSeek) achieve F1 scores of 0.463 and 0.453, respectively, surpassing the best baseline model, GritLM-7B, which scores 0.269.\n\n--- Segment 46 ---\nResults show that the DeepRTL2 models significantly out- perform all baseline models across all metrics. Specifically, DeepRTL2 (Llama) and DeepRTL2 (DeepSeek) achieve F1 scores of 0.463 and 0.453, respectively, surpassing the best baseline model, GritLM-7B, which scores 0.269. The higher pre- cision and recall scores for the DeepRTL2 models indicate that they are more effective at retrieving relevant code snippets based on user queries, high- lighting the strength of our dataset and training framework. These results confirm that DeepRTL2 excels in natural language code search, demonstrat- ing its superior ability to handle hardware-specific queries compared to the baseline models. J.2 Functionality Equivalence Checking The full evaluation results for RTL code function- ality equivalence checking are presented in Table 8. Results show that the DeepRTL2 models outper- form all baseline models across all metrics. Specif- ically, DeepRTL2 (Llama) achieves the highest performance with an average precision score of 0.667, F1 score of 0.723, and accuracy of 0.681. In comparison, the best-performing baseline model, GritLM-7B, achieves an average precision of 0.541, an F1 score of 0.661, and accuracy of 0.613. More- over, DeepRTL2 (DeepSeek) also shows strong performance, with an average precision of 0.591 and an F1 score of 0.708. The significantly higher precision and recall scores for DeepRTL2 mod- els indicate their superior capability in identifying functionally equivalent RTL code compared to the baseline models. These results confirm that Deep- RTL2 excels in functionality equivalence check- ing, demonstrating its effectiveness in hardware- specific tasks over general-purpose models.\n\n