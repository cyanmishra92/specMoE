=== ORIGINAL PDF: 2503.11246v1_Cost-effective_Deep_Learning_Infrastructure_with_N.pdf ===\n\nRaw text length: 37027 characters\nCleaned text length: 36308 characters\nNumber of segments: 18\n\n=== CLEANED TEXT ===\n\nCOST-EFFECTIVE DEEP LEARNING INFRASTRUCTURE WITH NVIDIA GPU Aatiz Ghimire Central Department of Physics Tribhuvan University Kirtipur, Kathmandu 44613, Nepal Shahnawaz Alam Herald College Kathmandu University of Wolverhampton Naxal, Kathmandu 44600, Nepal Siman Giri Herald College Kathmandu University of Wolverhampton Naxal, Kathmandu 44600, Nepal Madhav Prasad Ghimire Central Department of Physics Tribhuvan University Kirtipur, Kathmandu 44613, Nepal March 17, 2025 ABSTRACT The growing demand for computational power is driven by advancements in deep learning, the increasing need for big data processing, and the requirements of scientific simulations for academic and research purposes. Developing countries like Nepal often struggle with the resources needed to invest in new and better hardware for these purposes. However, optimizing and building on existing technology can still meet these computing demands effectively. To address these needs, we built a cluster using four NVIDIA GeForce GTX 1650 GPUs. The cluster consists of four nodes: one master node that controls and manages the entire cluster, and three compute nodes dedicated to processing tasks. The master node is equipped with all necessary software for package management, resource scheduling, and deployment, such as Anaconda and Slurm. In addition, a Network File Storage (NFS) system was integrated to provide the additional storage required by the cluster. Given that the cluster is accessible via ssh by a public domain address, which poses significant cybersecurity risks, we implemented fail2ban to mitigate brute force attacks and enhance security. Despite the continuous challenges encountered during the design and implementation process, this project demonstrates how powerful computational clusters can be built to handle resource-intensive tasks in various demanding fields. Keywords Deep Learning Infrastructure Beowulf Cluster High-Performance Computing (HPC) GPU Cluster Architecture 1 Introduction Deep learning has become a cornerstone of modern artificial intelligence, driving advances in various fields such as natural language processing, computer vision, and autonomous systems. However, training deep learning models often requires substantial computational resources, which can be prohibitively expensive for many institutions and researchers. Cloud-based GPU solutions, while offering scalability, come with recurring costs and potential data privacy concerns, making them less feasible for long-term or resource-intensive projects. [1] [2] [3] High-performance computing (HPC) or Clusters is increasingly shifting towards heterogeneous GPU-based systems, driven by the growing demand for massively parallel computing in deep learning and scientific applications[4]. With the exponential growth of AI-driven research and simulations, traditional CPU-based architectures often struggle to arXiv:2503.11246v1 [cs.DC] 14 Mar 2025 A PREPRINT - MARCH 17, 2025 keep up with computational demands. GPUs, with their high throughput and parallel processing capabilities, have emerged as a crucial component in accelerating tasks such as model training, data analysis, and large-scale simulations. Building an in-house, cost-effective deep learning infrastructure offers a practical alternative to address these challenges. Local clusters equipped with GPUs not only reduce operational costs but also provide greater control over data and computational resources. However, achieving a balance between performance, scalability, and affordability remains a significant challenge, especially for setups that use consumer-grade GPUs and standard hardware components. [5] This study focuses on designing and implementing a cost-effective deep learning cluster using readily available hardware and open source software. The objectives are to evaluate the feasibility of such a system, address its limitations, and highlight its potential for supporting computational research in resource-constrained environments. Using locally available resources and optimizing configurations, this study aims to contribute to the growing demand for accessible and efficient AI infrastructure. The rest of this paper is structured as follows: Section 2 provides an overview of related studies. Section 3 walks through the step-by-step process of building the GPU cluster, including the essential software and its role at each stage. In Section 4, we discuss the results, while Section 5 presents a summarized configuration and software stack. Finally, Section 6 concludes the paper with key takeaways. 2 Related Works The Beowulf Raspberry Pi cluster has gained attention as an affordable and scalable alternative for parallel computing applications. Various studies have explored its implementation, highlighting its potential for educational and experi- mental purposes [6, 7, 8, 9, 10, 11]. While Raspberry Pi clusters provide a cost-effective way to introduce students and researchers to distributed computing concepts, their performance is limited due to the low computational power of individual nodes. Furthermore, Raspberry Pi lacks dedicated GPU acceleration, restricting its usability for deep learning and large-scale parallel computing tasks. This limitation makes it unsuitable for workloads requiring significant matrix operations, such as AI model training. Another notable low-cost alternative is the Odroid-XU4 board, which offers more processing capability than Raspberry Pi. Studies have demonstrated its viability in HPC education and small-scale computing clusters [12]. Odroid-XU4 features an ARM-based architecture with multi-core processing, making it more efficient for parallel computations than Raspberry Pi. However, similar to Raspberry Pi, it lacks dedicated GPU support, making it impractical for deep learning tasks that heavily rely on GPU acceleration. While software optimization can help mitigate some performance constraints, these devices are not suitable for handling large-scale AI and scientific computing workloads that demand high floating-point performance. Recycling old computers by integrating them into a Rocks cluster is another cost-efficient strategy for setting up HPC environments [13]. Many academic institutions have repurposed outdated lab machines to build functional compute clusters, extending their usability for scientific computing and parallel processing workloads [14]. However, these older systems often rely solely on CPUs and lack modern GPUs, which are essential for accelerating deep learning computations. This absence of GPU support significantly limits their effectiveness in AI research and computational tasks that require tensor operations. While CPU-based clustering can still be useful for general-purpose parallel processing, it falls short in performance compared to modern GPU-accelerated systems. Despite the availability of low-cost solutions, pre-compiled DGX OS, designed specifically for deep learning systems, does not support commodity hardware, restricting its deployment to specialized NVIDIA DGX systems [15]. The NVIDIA DGX systems, optimized for deep learning workloads, has been widely adopted in AI research due to its integrated software stack and high-performance GPUs [16, 17, 18, 19]. Unlike the previously mentioned hardware solutions, NVIDIA DGX systems is built with dedicated NVIDIA GPUs, enabling high-speed parallel computations essential for deep learning. However, its high cost makes it inaccessible for many institutions and researchers, pushing them to explore alternative, budget-friendly solutions that attempt to bridge the gap between affordability and computational efficiency. 3 MATERIALS AND METHODS On the hardware side, our aim was to maintain a minimal configuration to make use of commonly available computers in standard computing labs. Our setup consisted of four computers equipped with GPUs, connected via a switch using CAT6 Ethernet cables for network communication. We assigned the four computers as Master, C1, C2, and C3 nodes. 2 A PREPRINT - MARCH 17, 2025 Figure 1: Cluster Network Configuration The basic configuration of our cluster is shown in the figure above. The computers are connected to a switch using Ethernet cables. Alternatively, a router can be used instead of a switch. One port of the switch is connected to the internet, enabling remote access to the cluster. We configured Network Address Translation (NAT) to convert the cluster s local private IP addresses (Master node IP) into a global public IP address for internet accessibility. Initially, we attempted to set up the cluster using Ubuntu Server 22.04.1. However, it required extensive package installations, had higher configuration complexity, and lacked enterprise-level features such as robust security, package stability, and infrequent updates. To address these limitations, we switched to Rocky Linux 9.4, which provides RHEL features. We performed a clean installation of Rocky Linux on each cluster node and updated them to the latest version. For simplicity in installation and cluster configuration, we only set up the root user with same password and did not create additional user accounts at this stage. [20] The next step was to configure static IP addresses, which are essential for inter-server communication needed for tasks such as SLURM, MPI, and data transfer. To achieve this, we used "nmtui", a Network Manager tool, to assign static IP addresses to the nodes, as illustrated in the figure above. To streamline communication within the cluster, we configured the etc hostname file on all nodes. The host file maps IP addresses to hostnames, allowing us to use simple names instead of IP addresses for communication. We assigned 10.80.0.100 to the master node and sequential IP addresses (e.g., 10.80.0.101 , 10.80.0.102 , 10.80.0.103 ) to the compute nodes c1 , c2 , and c3 . Additionally, we updated the etc hostname file on each node to define its hostname, ensuring consistent identification across the cluster. This configuration simplifies node communication and is essential for seamless cluster operations, including MPI, SLURM, and other distributed tasks. We installed the SSH package on all nodes to enable secure communication within the cluster. To streamline operations, we configured passwordless SSH, allowing quick and seamless login between nodes without requiring repeated password entry. This setup simplifies administrative tasks and facilitates efficient execution of distributed processes across the cluster. [21] All logins to the cluster are routed through the master node, which serves as the central point for user access and management. The master node is equipped with all necessary software for package management, resource scheduling, and deployment, ensuring efficient coordination and operation of the cluster. This centralized setup simplifies user management and resource allocation across the cluster. To streamline software deployment across multiple nodes in the cluster, we utilize pdsh, a parallel shell tool that enables simultaneous execution of commands. First, pdsh is installed on the head node using dnf install -y pdsh, ensuring SSH access is configured for passwordless authentication across all compute nodes. With pdsh, software packages can be installed in parallel, significantly reducing deployment time. For instance, executing pdsh -w node[1-4] sudo dnf install -y package-name installs the required software on all designated nodes simultaneously. This method enhances administrative efficiency, ensuring consistency across the cluster without manual intervention, making it well-suited for high-performance computing environments running Rocky Linux.[22] Additionally, maintaining time synchronization across the cluster is crucial for job scheduling, logging accuracy, and distributed computing. We achieve this by installing and configuring Network Time Protocol (NTP) on all nodes. Using pdsh, NTP can be installed in parallel with pdsh -w node[1-4] sudo dnf install -y chrony . The NTP service is then enabled and started with pdsh -w node[1-4] sudo systemctl enable now chronyd . The head node is configured as the 3 A PREPRINT - MARCH 17, 2025 Figure 2: Cluster Login Screen via SSH primary NTP server, while compute nodes sync their time with it. This setup ensures clock consistency, preventing synchronization issues that could disrupt HPC workloads and SLURM job execution. Similary, we installed MUNGE, which is a lightweight authentication service essential for secure, token-based communication between nodes in clusters. It simplifies user authentication, integrates seamlessly with job schedulers like SLURM, and provides scalability and ease of maintenance across clusters, making it ideal for efficient cluster management. [23] We added an additional 1TB of storage to the system to accommodate large-scale dataset downloads and enable long-term storage of results. The storage was mounted at mnt storage0 and bind-mounted to each user s directory as home username storage0 . This setup allows users to access the additional storage directly from their home directory, and we recommend utilizing this location for data management. [24] We set up a Network File System (NFS) on mnt storage0 to enable efficient file sharing between the server and client nodes in our cluster. The configuration involved installing the nfs-utils package, creating shared directories with appropriate permissions, and defining NFS exports in the etc exports file. We configured the firewall to allow NFS services and created mount points on the client nodes. The NFS shares were mounted manually for testing and configured in etc fstab for persistence after reboots. This setup ensures secure and reliable file sharing across the cluster. [25] We utilized FreeIPA for centralized user management and synchronization across all cluster nodes. FreeIPA provided seamless integration of user authentication and access control, ensuring consistent user accounts and credentials throughout the system. This streamlined administrative tasks, enhanced security, and simplified user management in the cluster environment. Figure 3: Slurm Command sinfo for listing cluster User management in HPC clusters requires a balance between security and ease of administration. While NIS is widely used in many HPC environments due to its simplicity, it is considered less secure than modern alternatives [26, 27]. FreeIPA, which integrates LDAP and Kerberos, offers enhanced security and centralized authentication but is more complex to install and configure [28]. Our cluster utilizes FreeIPA with LDAP to provide a robust identity management 4 A PREPRINT - MARCH 17, 2025 system, ensuring secure user authentication and streamlined access control. However, for clusters prioritizing ease of use over security, NIS remains a simpler alternative despite its vulnerabilities. Slurm is an open-source workload manager designed for efficient resource allocation, job scheduling, and queue management in high-performance computing (HPC) clusters. To set up Slurm in our cluster, we installed the necessary packages on all nodes and configured the slurm.conf file to match our cluster s specifications. The configuration file was distributed to all nodes, and appropriate permissions were set for Slurm directories. On the controller node, the Slurm controller daemon ( slurmctld ) was enabled and started, while the Slurm node daemon ( slurmd ) was activated on all worker nodes. The installation was verified using commands like sinfo for cluster information and srun for test jobs. This setup ensures efficient job scheduling and resource management, optimizing the performance of the cluster. [23]. We configured the master node to support computations but chose not to use it for this purpose. Utilizing the master node for processing tasks would add significant load to it, as it is already responsible for managing multiple tasks, including job scheduling, resource allocation, and acting as the login node for multiple users. Prioritizing these critical functions ensures the stability and efficiency of the cluster. At the time of setup, NVIDIA driver installation was not supported directly through the Rocky Linux package manager (dnf). Therefore, we manually installed the driver using the .run file obtained from the NVIDIA website. The GPU driver was installed first, followed by the installation of CUDA to enable GPU-accelerated computations in the cluster. To enable GPU integration in SLURM, we configured the slurm.conf file to recognize GPU resources by adding the Gres (Generic Resources) parameter. Each node was configured with its specific GPU details, including type and count. SLURM s gres.conf file was updated accordingly, ensuring seamless allocation of GPU resources to jobs. This setup allowed efficient scheduling and utilization of GPUs across the cluster. [29] We installed Lmod, a Lua-based environment modules system, to efficiently manage software environments in the cluster. Lmod enables users to dynamically load, unload, and switch between different versions of software and their dependencies. Using Lmod, we created module files for various versions of Python, PyTorch, etc., allowing users to easily switch between them based on their project requirements. After installation, we configured the module files directory and integrated Lmod with the shell environment by updating the system s profile files. This setup, powered by Lua scripting for custom module configurations, ensures flexibility and simplicity in managing diverse software stacks, optimizing the cluster for various computational workflows. [30] We configured Lmod and Lua on all cluster nodes to ensure consistent environment management and seamless job execution across the cluster. Lmod and Lua were installed on each node, and module files were synchronized using a shared directory accessible to all nodes. The shell environment on each node was updated to load Lmod and set the appropriate MODULEPATH . This setup ensures that all nodes can access the same software environments, allowing SLURM to execute jobs reliably and efficiently with the required configurations. To further streamline software management, we integrated EasyBuild, an automated framework for building and managing software installations in HPC environments. EasyBuild simplifies the process of compiling and installing complex scientific software by handling dependencies and environment configurations automatically. By using EasyBuild alongside Lmod, we generated module files dynamically, ensuring that all installed software could be easily managed within the cluster s module system. This approach reduces manual configuration efforts while maintaining reproducibility and consistency across all nodes. Additionally, EasyBuild allows us to install optimized versions of libraries and applications, improving performance for computational workloads. [31] While our cluster primarily utilizes EasyBuild, Spack [32] is also a powerful and flexible alternative for software deployment. Spack enables users to install multiple software versions and manage dependencies efficiently, offering a more modular approach than traditional package managers. Its extensive package repository and customizable builds make it a preferred choice for many HPC environments. Although we opted for EasyBuild due to its structured installation and seamless module integration, Spack remains an excellent choice for clusters requiring greater flexibility in software management. We installed Anaconda in a shared directory accessible across the cluster and integrated it with Lmod for dynamic environment management. A custom Lmod module file was created to configure Anaconda, setting environment variables like PATH for seamless activation. Using the shared directory, all cluster nodes can access the Anaconda installation, enabling users to load and unload Anaconda dynamically through Lmod. This centralized setup allows users to create isolated Conda environments, which can be loaded as part of SLURM job scripts, ensuring consistent software environments for diverse computational tasks across the cluster. We installed multiple versions of Python alongside Anaconda in a shared directory to provide flexibility for various computational tasks. Each Python version was configured with its own Lmod module file, allowing users to dynamically load the desired version as needed. These Python modules, like Anaconda, are accessible across all cluster nodes via 5 A PREPRINT - MARCH 17, 2025 Figure 4: Deep Learning Cluster Installation Flowchart the shared directory. Users can load specific Python versions using Lmod commands and utilize them in SLURM job scripts, ensuring compatibility with diverse project requirements while maintaining a consistent and centralized environment management system. We set up the Message Passing Interface (MPI) using MPICH on the cluster to enable efficient parallel computing. Open MPI was installed on all nodes, ensuring the availability of runtime libraries, compiler wrappers, and development tools. Environment variables such as PATH and LD_LIBRARY_PATH were configured globally or managed dynamically using Lmod. We verified the installation by compiling and running test MPI programs using mpicc and mpirun . Static IPs and passwordless SSH were configured already before to facilitate seamless communication between nodes. Additionally, MPI was integrated with SLURM, utilizing srun for job execution to leverage SLURM s resource management capabilities. This setup provides a robust framework for scalable parallel computing in the cluster. The NVIDIA GTX 1650 GPUs in our cluster support CUDA for GPU-accelerated parallel computations but lack the features required for clustering GPUs across nodes, such as GPUDirect RDMA. This limitation prevents the GPUs from being used together in a unified multi-GPU setup. [33] Instead, each GPU operates independently, and tasks must be run on individual GPUs within their respective nodes. While this restricts the scalability of GPU workloads, it remains suitable for single-node GPU computations and smaller parallel tasks. Figure 5: Failed Login attempts in cluster 6 A PREPRINT - MARCH 17, 2025 Monitoring is a critical aspect of HPC cluster management, ensuring system health, performance optimization, and fault detection. We installed Ganglia due to its ease of setup and widespread use in HPC environments, allowing efficient tracking of system metrics across all compute nodes [13, 10]. Ganglia provides real-time resource utilization insights, helping to maintain workload balance and optimize job scheduling. Additionally, we tested Prometheus (on and Grafana (on as alternative monitoring solutions. Prometheus, with its time-series monitoring and alerting capabilities, allows detailed system metric collection via exporters [17]. Grafana, when integrated with Prometheus, provides an intuitive visualization interface for tracking CPU, memory, and GPU usage in real time. While both Prometheus and Grafana are viable options under the given IP configurations, we preferred Ganglia for its simpler deployment and efficient monitoring of our SLURM-managed cluster. Our cluster faced a significant cybersecurity threat daily due to internet connectivity with nearly 2000-6000 failed login attempts to the root account within 24 hours, which we suspect were primarily bot-driven attacks. To mitigate this risk, we disable login with root account in SSH and we installed the Fail2Ban package to enhance SSH security. Fail2Ban monitors login attempts and, after six consecutive failed password entries, automatically blacklists the offending IP address for an hour, blocking further SSH communication. This proactive approach effectively reduced unauthorized access attempts and strengthened the cluster s overall security. 4 RESULTS AND DISCUSSION The implementation of our cluster setup demonstrates a cost-effective and practical solution for high-performance computing tasks. The master node efficiently managed resource allocation and user authentication, ensuring seamless operation across the cluster. Security was significantly enhanced with the integration of Fail2Ban, which successfully reduced unauthorized login attempts by blacklisting suspicious IP addresses. The individual GPU performance was validated for CUDA computations, proving effective for single-node tasks despite the hardware limitation of not being able to cluster GPUs. MPI communication over Ethernet was reliable, providing acceptable latency and bandwidth for medium-scale workloads. Additionally, the integration of MPI with SLURM ensured efficient resource scheduling for distributed computations. The adoption of Lmod and Lua simplified software environment management, enabling users to switch dynamically between various versions of Python, and other required software. The setup of Anaconda further streamlined Python- based workflows, offering flexibility and efficiency for diverse computational tasks. These features significantly improved the user experience in managing software environments within the cluster. Figure 6: Cluster Setup The addition of 1TB of shared storage, bind-mounted to user directories, facilitated the handling of large-scale datasets and ensured easy access to results. This enhanced the system s ability to support data-intensive applications and long-term storage needs. Containerization plays a crucial role in deep learning workflows by providing isolated and reproducible environments. In our cluster, we evaluated both Singularity and Docker as potential solutions for containerized deep learning environments. While Docker is widely used for its flexibility and ease of deployment, it requires root privileges, making it less suitable for shared HPC environments. Conversely, Singularity enables users to run containers securely without requiring elevated permissions, making it a more HPC-friendly alternative. Both containerization tools facilitate seamless execution of deep learning frameworks like TensorFlow and PyTorch, ensuring dependency consistency across different nodes. Based on our evaluation, Singularity is preferred for our cluster due to its security model and compatibility with 7 A PREPRINT - MARCH 17, 2025 HPC resource managers like SLURM, but Docker remains a viable option for standalone deep learning projects and development environments. [16] [19] [34] In summary, the results demonstrate the feasibility of building a robust HPC cluster using commonly available hardware and open-source software. While the system effectively balances cost, scalability, and performance, limitations such as the inability to cluster GPUs and reliance on Ethernet for MPI communication introduce constraints for high-demand, large-scale applications. Future improvements could focus on integrating high-speed interconnects and upgrading to HPC-oriented GPUs to enhance the cluster s performance and scalability further. 5 TABLES AND COMPARISON The following contains the summary of components and software used: Component Specification Master Node 10.80.0.100, Intel i5 CPU, 16 GB RAM Compute Nodes c1, c2, c3 (10.80.0.101-103), Intel i5 CPU, 16 GB RAM GPUs NVIDIA GTX 1650 (on all nodes) Storage 1TB (shared across all nodes) Network Ethernet (1 Gbps) Table 1: Cluster Configuration Summary Software Version Purpose Rocky Linux 9.4 Operating System SLURM 22.05 Job Scheduling Lmod Installed Environment Management Anaconda 2023.09 Python-based Workflows Table 2: Software Stack 6 CONCLUSION Setting up a local GPU cluster within a small department or university presents a highly cost-effective alternative to renting cloud GPUs. Our cost analysis demonstrates that operating a locally installed cluster with four NVIDIA GTX 1650 GPUs (4 x 4GB 16GB total) incurs a monthly electricity cost of only NPR 5,760 (based on an 800W power consumption per node) in Nepal. In contrast, renting an NVIDIA T4 GPU (16GB) in the cloud costs approximately NPR 123.50 per hour, amounting to NPR 29,640 per month for similar usage [35, 36]. This stark difference underscores the long-term financial benefits of local GPU clusters, particularly in regions like Nepal, where electricity costs are relatively low. Beyond cost savings, a locally managed cluster offers greater control over data privacy, independence from internet connectivity, and uninterrupted access to computational resources, making it an ideal solution for research and education. While the initial hardware investment may seem substantial, many institutions can leverage existing computing labs to set up these clusters efficiently. Ultimately, the autonomy, sustainability, and reduced recurring costs of an in-house GPU cluster make it a far more viable and strategic investment for academic and research institutions compared to reliance on cloud-based alternatives. 7 ACKNOWLEDGEMENTS We express our sincere gratitude to Herald College Kathmandu for providing the research space, hardware, and infrastructure essential for this project. For detailed instructions on installing, and configuring, we have made our setup process and related scripts available at For running this cluster, We have also made slurm with gpu sbatch file and instruction at 8 A PREPRINT - MARCH 17, 2025 References [1] Dheeraj Chahal, Mayank Mishra, Surya Palepu, and Rekha Singhal. Performance and cost comparison of cloud services for deep learning workload. In Companion of the ACM SPEC International Conference on Performance Engineering, ICPE 21, page 49 55, New York, NY, USA, 2021. Association for Computing Machinery. [2] John Lawrence, Jonas Malmsten, Andrey Rybka, Daniel A Sabol, and Ken Triplin. Comparing tensorflow deep learning performance using cpus, gpus, local pcs and cloud. 2017. [3] Vanderlei Munhoz, Antoine Bonfils, Márcio Castro, and Odorico Mendizabal. A performance comparison of HPC workloads on traditional and cloud-based HPC clusters. In 2023 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW), pages 108 114. IEEE. [4] Jingoo Han, Luna Xu, M. Mustafa Rafique, Ali R. Butt, and Seung-Hwan Lim. A quantitative study of deep learning training on heterogeneous supercomputers. In 2019 IEEE International Conference on Cluster Computing (CLUSTER), pages 1 12. IEEE. [5] Emil Vaihela. The performance and optimization of hpc clusters. 2022. [6] Heramb Penyala, Soad Ibrahim, and Ayman El Mesalami. The raspberry pi education mine: For teaching engineering and computer science students concepts like, computer clusters, parallel computing, and distributed computing. In 2020 IEEE International Conference on Electro Information Technology (EIT), pages 624 628. IEEE. [7] Vincent A. Cicirello. Design, configuration, implementation, and performance of a simple 32 core raspberry pi cluster. [8] Sandino Vargas-Pérez. Designing an independent study to create HPC learning experiences for undergraduates. In 2022 IEEE 29th International Conference on High Performance Computing, Data and Analytics Workshop (HiPCW), pages 6 11. IEEE. [9] Nkundwe Moses Mwasaga and Mike Joy. Implementing micro high performance computing (µHPC) artifact: Affordable HPC facilities for academia. In 2020 IEEE Frontiers in Education Conference (FIE), pages 1 9. IEEE. [10] S. Mollova, M. Zhekov, A. Kostadinov, and P. Georgieva. Laboratory model for research on computer cluster systems. In 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pages 1388 1393. IEEE. [11] Heramb Penyala, Soad Ibrahim, and Ayman El Mesalami. The raspberry pi education mine: For teaching engineering and computer science students concepts like, computer clusters, parallel computing, and distributed computing. In 2020 IEEE International Conference on Electro Information Technology (EIT), pages 624 628. IEEE. [12] Lluc Alvarez, Eduard Ayguade, and Filippo Mantovani. Teaching HPC systems and parallel programming with small-scale clusters. In 2018 IEEE ACM Workshop on Education for High-Performance Computing (EduHPC), pages 1 10. IEEE. [13] Chavala Mutyala Rao and K. Shyamala. Analysis and implementation of a parallel computing cluster for solving computational problems in data analytics. In 2020 5th International Conference on Computing, Communication and Security (ICCCS), pages 1 5. IEEE. [14] Dileep Kumar, Sheeraz Memon, and Liaquat Ali Thebo. Design, implementation performance analysis of low cost high performance computing (HPC) clusters. In 2018 12th International Conference on Signal Processing and Communication Systems (ICSPCS), pages 1 6. IEEE. [15] Release notes NVIDIA DGX OS 6 user guide. [16] Vaihela Emil. Creation and optimization of HPC clusters. [17] Arindam Majee. DeepOps SLURM: Your GPU cluster guide. [18] Deploying a scalable GPU-as-a-service platform and building a deep learning project in under 80 minutes GTC digital march 2020 NVIDIA on-demand. [19] Deploying rich cluster API on DGX for multi-user sharing. [20] Wale Soyinka. Installing Rocky Linux 9 - Documentation docs.rockylinux.org. org guides installation . [Accessed 29-11-2024]. [21] Ravi Saive. How to Setup SSH Passwordless Login in Linux tecmint.com. ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps . [Accessed 29-11-2024]. 9 A PREPRINT - MARCH 17, 2025 [22] Linux cluster sysadmin Parallel command execution with PDSH rittmanmead.com. linux-cluster-sysadmin-parallel-command-execution-with-pdsh . [Accessed 30-01-2025]. [23] GitHub - SergioMEV slurm-for-dummies: A dummy s guide to setting up (and using) HPC clusters on Ubuntu 22.04LTS using Slurm and Munge. Created by the Quant Club UIowa. github.com. com SergioMEV slurm-for-dummies. [Accessed 29-11-2024]. [24] Bind Mounts in Linux docs.rackspace.com. bind-mounts-in-linux. [Accessed 29-11-2024]. [25] How to set up an NFS Mount on Rocky Linux 8 howtoforge.com. how-to-set-up-an-nfs-mount-on-rocky-linux-8 . [Accessed 29-11-2024]. [26] Giovanni Battista Barone, Davide Bottalico, Luisa Carracciuolo, Alessandra Doria, Davide Michelino, Silvio Pardi, Guido Russo, Gianluca Sabella, and Bernardino Spisso. Designing and implementing a high-performance computing heterogeneous cluster. In 2022 International Conference on Electrical, Computer and Energy Technologies (ICECET), pages 1 6. IEEE. [27] Mandeep Kumar. Distributed execution of dask on HPC: A case study. In 2023 World Conference on Communica- tion Computing (WCONF), pages 1 4. IEEE. [28] Andria Arisal, Syam Budi Iryanto, and Zaenal Akbar. Managing multi-services for multi-users in heteroge- neous cluster computing system. In 2019 International Conference on Computer, Control, Informatics and its Applications (IC3INA), pages 103 107. IEEE. [29] Slurm Workload Manager - Generic Resource (GRES) Scheduling slurm.schedmd.com. schedmd.com gres.html. [Accessed 29-11-2024]. [30] TACC. How to Transition to Lmod (or how to test Lmod without installing it for all); Lmod 8.7.53 documentation lmod.readthedocs.io. [Accessed 29-11-2024]. [31] Modules, LMod and Easybuild - Big Data Analytics nesusws-tutorials-bd-dl.readthedocs.io. https: nesusws-tutorials-bd-dl.readthedocs.io en latest hands-on easybuild . [Accessed 30-01- 2025]. [32] Todd Gamblin, Matthew LeGendre, Michael R Collette, Gregory L Lee, Adam Moody, Bronis R De Supinski, and Scott Futral. The spack package manager: bringing order to hpc software chaos. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 12, 2015. [33] J.J. Vegas Olmos, Liran Liss, Tzahi Oved, Zachi Binshtock, and Dror Goldenberg. Advanced software architectures and technologies in high performance computing and data centers. In Optical Fiber Communication Conference (OFC) 2020, page T3K.3. Optica Publishing Group. [34] How to run NGC deep learning containers with singularity. [35] Nepal electricity prices, March 2024 GlobalPetrolPrices.com globalpetrolprices.com. globalpetrolprices.com Nepal electricity_prices . [Accessed 29-11-2024]. [36] NVIDIA Tesla T4 GPUs now available in beta Google Cloud Blog cloud.google.com. nvidia-tesla-t4-gpus-now-available-in-beta. [Accessed 29-11-2024]. 10\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nCOST-EFFECTIVE DEEP LEARNING INFRASTRUCTURE WITH NVIDIA GPU Aatiz Ghimire Central Department of Physics Tribhuvan University Kirtipur, Kathmandu 44613, Nepal Shahnawaz Alam Herald College Kathmandu University of Wolverhampton Naxal, Kathmandu 44600, Nepal Siman Giri Herald College Kathmandu University of Wolverhampton Naxal, Kathmandu 44600, Nepal Madhav Prasad Ghimire Central Department of Physics Tribhuvan University Kirtipur, Kathmandu 44613, Nepal March 17, 2025 ABSTRACT The growing demand for computational power is driven by advancements in deep learning, the increasing need for big data processing, and the requirements of scientific simulations for academic and research purposes. Developing countries like Nepal often struggle with the resources needed to invest in new and better hardware for these purposes. However, optimizing and building on existing technology can still meet these computing demands effectively. To address these needs, we built a cluster using four NVIDIA GeForce GTX 1650 GPUs. The cluster consists of four nodes: one master node that controls and manages the entire cluster, and three compute nodes dedicated to processing tasks. The master node is equipped with all necessary software for package management, resource scheduling, and deployment, such as Anaconda and Slurm. In addition, a Network File Storage (NFS) system was integrated to provide the additional storage required by the cluster. Given that the cluster is accessible via ssh by a public domain address, which poses significant cybersecurity risks, we implemented fail2ban to mitigate brute force attacks and enhance security. Despite the continuous challenges encountered during the design and implementation process, this project demonstrates how powerful computational clusters can be built to handle resource-intensive tasks in various demanding fields. Keywords Deep Learning Infrastructure Beowulf Cluster High-Performance Computing (HPC) GPU Cluster Architecture 1 Introduction Deep learning has become a cornerstone of modern artificial intelligence, driving advances in various fields such as natural language processing, computer vision, and autonomous systems. However, training deep learning models often requires substantial computational resources, which can be prohibitively expensive for many institutions and researchers. Cloud-based GPU solutions, while offering scalability, come with recurring costs and potential data privacy concerns, making them less feasible for long-term or resource-intensive projects.\n\n--- Segment 2 ---\nHowever, training deep learning models often requires substantial computational resources, which can be prohibitively expensive for many institutions and researchers. Cloud-based GPU solutions, while offering scalability, come with recurring costs and potential data privacy concerns, making them less feasible for long-term or resource-intensive projects. [1] [2] [3] High-performance computing (HPC) or Clusters is increasingly shifting towards heterogeneous GPU-based systems, driven by the growing demand for massively parallel computing in deep learning and scientific applications[4]. With the exponential growth of AI-driven research and simulations, traditional CPU-based architectures often struggle to arXiv:2503.11246v1 [cs.DC] 14 Mar 2025 A PREPRINT - MARCH 17, 2025 keep up with computational demands. GPUs, with their high throughput and parallel processing capabilities, have emerged as a crucial component in accelerating tasks such as model training, data analysis, and large-scale simulations. Building an in-house, cost-effective deep learning infrastructure offers a practical alternative to address these challenges. Local clusters equipped with GPUs not only reduce operational costs but also provide greater control over data and computational resources. However, achieving a balance between performance, scalability, and affordability remains a significant challenge, especially for setups that use consumer-grade GPUs and standard hardware components. [5] This study focuses on designing and implementing a cost-effective deep learning cluster using readily available hardware and open source software. The objectives are to evaluate the feasibility of such a system, address its limitations, and highlight its potential for supporting computational research in resource-constrained environments. Using locally available resources and optimizing configurations, this study aims to contribute to the growing demand for accessible and efficient AI infrastructure. The rest of this paper is structured as follows: Section 2 provides an overview of related studies. Section 3 walks through the step-by-step process of building the GPU cluster, including the essential software and its role at each stage. In Section 4, we discuss the results, while Section 5 presents a summarized configuration and software stack. Finally, Section 6 concludes the paper with key takeaways. 2 Related Works The Beowulf Raspberry Pi cluster has gained attention as an affordable and scalable alternative for parallel computing applications. Various studies have explored its implementation, highlighting its potential for educational and experi- mental purposes [6, 7, 8, 9, 10, 11].\n\n--- Segment 3 ---\n2 Related Works The Beowulf Raspberry Pi cluster has gained attention as an affordable and scalable alternative for parallel computing applications. Various studies have explored its implementation, highlighting its potential for educational and experi- mental purposes [6, 7, 8, 9, 10, 11]. While Raspberry Pi clusters provide a cost-effective way to introduce students and researchers to distributed computing concepts, their performance is limited due to the low computational power of individual nodes. Furthermore, Raspberry Pi lacks dedicated GPU acceleration, restricting its usability for deep learning and large-scale parallel computing tasks. This limitation makes it unsuitable for workloads requiring significant matrix operations, such as AI model training. Another notable low-cost alternative is the Odroid-XU4 board, which offers more processing capability than Raspberry Pi. Studies have demonstrated its viability in HPC education and small-scale computing clusters [12]. Odroid-XU4 features an ARM-based architecture with multi-core processing, making it more efficient for parallel computations than Raspberry Pi. However, similar to Raspberry Pi, it lacks dedicated GPU support, making it impractical for deep learning tasks that heavily rely on GPU acceleration. While software optimization can help mitigate some performance constraints, these devices are not suitable for handling large-scale AI and scientific computing workloads that demand high floating-point performance. Recycling old computers by integrating them into a Rocks cluster is another cost-efficient strategy for setting up HPC environments [13]. Many academic institutions have repurposed outdated lab machines to build functional compute clusters, extending their usability for scientific computing and parallel processing workloads [14]. However, these older systems often rely solely on CPUs and lack modern GPUs, which are essential for accelerating deep learning computations. This absence of GPU support significantly limits their effectiveness in AI research and computational tasks that require tensor operations. While CPU-based clustering can still be useful for general-purpose parallel processing, it falls short in performance compared to modern GPU-accelerated systems. Despite the availability of low-cost solutions, pre-compiled DGX OS, designed specifically for deep learning systems, does not support commodity hardware, restricting its deployment to specialized NVIDIA DGX systems [15]. The NVIDIA DGX systems, optimized for deep learning workloads, has been widely adopted in AI research due to its integrated software stack and high-performance GPUs [16, 17, 18, 19].\n\n--- Segment 4 ---\nDespite the availability of low-cost solutions, pre-compiled DGX OS, designed specifically for deep learning systems, does not support commodity hardware, restricting its deployment to specialized NVIDIA DGX systems [15]. The NVIDIA DGX systems, optimized for deep learning workloads, has been widely adopted in AI research due to its integrated software stack and high-performance GPUs [16, 17, 18, 19]. Unlike the previously mentioned hardware solutions, NVIDIA DGX systems is built with dedicated NVIDIA GPUs, enabling high-speed parallel computations essential for deep learning. However, its high cost makes it inaccessible for many institutions and researchers, pushing them to explore alternative, budget-friendly solutions that attempt to bridge the gap between affordability and computational efficiency. 3 MATERIALS AND METHODS On the hardware side, our aim was to maintain a minimal configuration to make use of commonly available computers in standard computing labs. Our setup consisted of four computers equipped with GPUs, connected via a switch using CAT6 Ethernet cables for network communication. We assigned the four computers as Master, C1, C2, and C3 nodes. 2 A PREPRINT - MARCH 17, 2025 Figure 1: Cluster Network Configuration The basic configuration of our cluster is shown in the figure above. The computers are connected to a switch using Ethernet cables. Alternatively, a router can be used instead of a switch. One port of the switch is connected to the internet, enabling remote access to the cluster. We configured Network Address Translation (NAT) to convert the cluster s local private IP addresses (Master node IP) into a global public IP address for internet accessibility. Initially, we attempted to set up the cluster using Ubuntu Server 22.04.1. However, it required extensive package installations, had higher configuration complexity, and lacked enterprise-level features such as robust security, package stability, and infrequent updates. To address these limitations, we switched to Rocky Linux 9.4, which provides RHEL features. We performed a clean installation of Rocky Linux on each cluster node and updated them to the latest version. For simplicity in installation and cluster configuration, we only set up the root user with same password and did not create additional user accounts at this stage. [20] The next step was to configure static IP addresses, which are essential for inter-server communication needed for tasks such as SLURM, MPI, and data transfer.\n\n--- Segment 5 ---\nFor simplicity in installation and cluster configuration, we only set up the root user with same password and did not create additional user accounts at this stage. [20] The next step was to configure static IP addresses, which are essential for inter-server communication needed for tasks such as SLURM, MPI, and data transfer. To achieve this, we used "nmtui", a Network Manager tool, to assign static IP addresses to the nodes, as illustrated in the figure above. To streamline communication within the cluster, we configured the etc hostname file on all nodes. The host file maps IP addresses to hostnames, allowing us to use simple names instead of IP addresses for communication. We assigned 10.80.0.100 to the master node and sequential IP addresses (e.g., 10.80.0.101 , 10.80.0.102 , 10.80.0.103 ) to the compute nodes c1 , c2 , and c3 . Additionally, we updated the etc hostname file on each node to define its hostname, ensuring consistent identification across the cluster. This configuration simplifies node communication and is essential for seamless cluster operations, including MPI, SLURM, and other distributed tasks. We installed the SSH package on all nodes to enable secure communication within the cluster. To streamline operations, we configured passwordless SSH, allowing quick and seamless login between nodes without requiring repeated password entry. This setup simplifies administrative tasks and facilitates efficient execution of distributed processes across the cluster. [21] All logins to the cluster are routed through the master node, which serves as the central point for user access and management. The master node is equipped with all necessary software for package management, resource scheduling, and deployment, ensuring efficient coordination and operation of the cluster. This centralized setup simplifies user management and resource allocation across the cluster. To streamline software deployment across multiple nodes in the cluster, we utilize pdsh, a parallel shell tool that enables simultaneous execution of commands. First, pdsh is installed on the head node using dnf install -y pdsh, ensuring SSH access is configured for passwordless authentication across all compute nodes. With pdsh, software packages can be installed in parallel, significantly reducing deployment time. For instance, executing pdsh -w node[1-4] sudo dnf install -y package-name installs the required software on all designated nodes simultaneously.\n\n--- Segment 6 ---\nWith pdsh, software packages can be installed in parallel, significantly reducing deployment time. For instance, executing pdsh -w node[1-4] sudo dnf install -y package-name installs the required software on all designated nodes simultaneously. This method enhances administrative efficiency, ensuring consistency across the cluster without manual intervention, making it well-suited for high-performance computing environments running Rocky Linux. [22] Additionally, maintaining time synchronization across the cluster is crucial for job scheduling, logging accuracy, and distributed computing. We achieve this by installing and configuring Network Time Protocol (NTP) on all nodes. Using pdsh, NTP can be installed in parallel with pdsh -w node[1-4] sudo dnf install -y chrony . The NTP service is then enabled and started with pdsh -w node[1-4] sudo systemctl enable now chronyd . The head node is configured as the 3 A PREPRINT - MARCH 17, 2025 Figure 2: Cluster Login Screen via SSH primary NTP server, while compute nodes sync their time with it. This setup ensures clock consistency, preventing synchronization issues that could disrupt HPC workloads and SLURM job execution. Similary, we installed MUNGE, which is a lightweight authentication service essential for secure, token-based communication between nodes in clusters. It simplifies user authentication, integrates seamlessly with job schedulers like SLURM, and provides scalability and ease of maintenance across clusters, making it ideal for efficient cluster management. [23] We added an additional 1TB of storage to the system to accommodate large-scale dataset downloads and enable long-term storage of results. The storage was mounted at mnt storage0 and bind-mounted to each user s directory as home username storage0 . This setup allows users to access the additional storage directly from their home directory, and we recommend utilizing this location for data management. [24] We set up a Network File System (NFS) on mnt storage0 to enable efficient file sharing between the server and client nodes in our cluster. The configuration involved installing the nfs-utils package, creating shared directories with appropriate permissions, and defining NFS exports in the etc exports file. We configured the firewall to allow NFS services and created mount points on the client nodes. The NFS shares were mounted manually for testing and configured in etc fstab for persistence after reboots.\n\n--- Segment 7 ---\nWe configured the firewall to allow NFS services and created mount points on the client nodes. The NFS shares were mounted manually for testing and configured in etc fstab for persistence after reboots. This setup ensures secure and reliable file sharing across the cluster. [25] We utilized FreeIPA for centralized user management and synchronization across all cluster nodes. FreeIPA provided seamless integration of user authentication and access control, ensuring consistent user accounts and credentials throughout the system. This streamlined administrative tasks, enhanced security, and simplified user management in the cluster environment. Figure 3: Slurm Command sinfo for listing cluster User management in HPC clusters requires a balance between security and ease of administration. While NIS is widely used in many HPC environments due to its simplicity, it is considered less secure than modern alternatives [26, 27]. FreeIPA, which integrates LDAP and Kerberos, offers enhanced security and centralized authentication but is more complex to install and configure [28]. Our cluster utilizes FreeIPA with LDAP to provide a robust identity management 4 A PREPRINT - MARCH 17, 2025 system, ensuring secure user authentication and streamlined access control. However, for clusters prioritizing ease of use over security, NIS remains a simpler alternative despite its vulnerabilities. Slurm is an open-source workload manager designed for efficient resource allocation, job scheduling, and queue management in high-performance computing (HPC) clusters. To set up Slurm in our cluster, we installed the necessary packages on all nodes and configured the slurm.conf file to match our cluster s specifications. The configuration file was distributed to all nodes, and appropriate permissions were set for Slurm directories. On the controller node, the Slurm controller daemon ( slurmctld ) was enabled and started, while the Slurm node daemon ( slurmd ) was activated on all worker nodes. The installation was verified using commands like sinfo for cluster information and srun for test jobs. This setup ensures efficient job scheduling and resource management, optimizing the performance of the cluster. [23]. We configured the master node to support computations but chose not to use it for this purpose. Utilizing the master node for processing tasks would add significant load to it, as it is already responsible for managing multiple tasks, including job scheduling, resource allocation, and acting as the login node for multiple users. Prioritizing these critical functions ensures the stability and efficiency of the cluster.\n\n--- Segment 8 ---\nUtilizing the master node for processing tasks would add significant load to it, as it is already responsible for managing multiple tasks, including job scheduling, resource allocation, and acting as the login node for multiple users. Prioritizing these critical functions ensures the stability and efficiency of the cluster. At the time of setup, NVIDIA driver installation was not supported directly through the Rocky Linux package manager (dnf). Therefore, we manually installed the driver using the .run file obtained from the NVIDIA website. The GPU driver was installed first, followed by the installation of CUDA to enable GPU-accelerated computations in the cluster. To enable GPU integration in SLURM, we configured the slurm.conf file to recognize GPU resources by adding the Gres (Generic Resources) parameter. Each node was configured with its specific GPU details, including type and count. SLURM s gres.conf file was updated accordingly, ensuring seamless allocation of GPU resources to jobs. This setup allowed efficient scheduling and utilization of GPUs across the cluster. [29] We installed Lmod, a Lua-based environment modules system, to efficiently manage software environments in the cluster. Lmod enables users to dynamically load, unload, and switch between different versions of software and their dependencies. Using Lmod, we created module files for various versions of Python, PyTorch, etc., allowing users to easily switch between them based on their project requirements. After installation, we configured the module files directory and integrated Lmod with the shell environment by updating the system s profile files. This setup, powered by Lua scripting for custom module configurations, ensures flexibility and simplicity in managing diverse software stacks, optimizing the cluster for various computational workflows. [30] We configured Lmod and Lua on all cluster nodes to ensure consistent environment management and seamless job execution across the cluster. Lmod and Lua were installed on each node, and module files were synchronized using a shared directory accessible to all nodes. The shell environment on each node was updated to load Lmod and set the appropriate MODULEPATH . This setup ensures that all nodes can access the same software environments, allowing SLURM to execute jobs reliably and efficiently with the required configurations. To further streamline software management, we integrated EasyBuild, an automated framework for building and managing software installations in HPC environments. EasyBuild simplifies the process of compiling and installing complex scientific software by handling dependencies and environment configurations automatically.\n\n--- Segment 9 ---\nTo further streamline software management, we integrated EasyBuild, an automated framework for building and managing software installations in HPC environments. EasyBuild simplifies the process of compiling and installing complex scientific software by handling dependencies and environment configurations automatically. By using EasyBuild alongside Lmod, we generated module files dynamically, ensuring that all installed software could be easily managed within the cluster s module system. This approach reduces manual configuration efforts while maintaining reproducibility and consistency across all nodes. Additionally, EasyBuild allows us to install optimized versions of libraries and applications, improving performance for computational workloads. [31] While our cluster primarily utilizes EasyBuild, Spack [32] is also a powerful and flexible alternative for software deployment. Spack enables users to install multiple software versions and manage dependencies efficiently, offering a more modular approach than traditional package managers. Its extensive package repository and customizable builds make it a preferred choice for many HPC environments. Although we opted for EasyBuild due to its structured installation and seamless module integration, Spack remains an excellent choice for clusters requiring greater flexibility in software management. We installed Anaconda in a shared directory accessible across the cluster and integrated it with Lmod for dynamic environment management. A custom Lmod module file was created to configure Anaconda, setting environment variables like PATH for seamless activation. Using the shared directory, all cluster nodes can access the Anaconda installation, enabling users to load and unload Anaconda dynamically through Lmod. This centralized setup allows users to create isolated Conda environments, which can be loaded as part of SLURM job scripts, ensuring consistent software environments for diverse computational tasks across the cluster. We installed multiple versions of Python alongside Anaconda in a shared directory to provide flexibility for various computational tasks. Each Python version was configured with its own Lmod module file, allowing users to dynamically load the desired version as needed. These Python modules, like Anaconda, are accessible across all cluster nodes via 5 A PREPRINT - MARCH 17, 2025 Figure 4: Deep Learning Cluster Installation Flowchart the shared directory. Users can load specific Python versions using Lmod commands and utilize them in SLURM job scripts, ensuring compatibility with diverse project requirements while maintaining a consistent and centralized environment management system. We set up the Message Passing Interface (MPI) using MPICH on the cluster to enable efficient parallel computing. Open MPI was installed on all nodes, ensuring the availability of runtime libraries, compiler wrappers, and development tools.\n\n--- Segment 10 ---\nWe set up the Message Passing Interface (MPI) using MPICH on the cluster to enable efficient parallel computing. Open MPI was installed on all nodes, ensuring the availability of runtime libraries, compiler wrappers, and development tools. Environment variables such as PATH and LD_LIBRARY_PATH were configured globally or managed dynamically using Lmod. We verified the installation by compiling and running test MPI programs using mpicc and mpirun . Static IPs and passwordless SSH were configured already before to facilitate seamless communication between nodes. Additionally, MPI was integrated with SLURM, utilizing srun for job execution to leverage SLURM s resource management capabilities. This setup provides a robust framework for scalable parallel computing in the cluster. The NVIDIA GTX 1650 GPUs in our cluster support CUDA for GPU-accelerated parallel computations but lack the features required for clustering GPUs across nodes, such as GPUDirect RDMA. This limitation prevents the GPUs from being used together in a unified multi-GPU setup. [33] Instead, each GPU operates independently, and tasks must be run on individual GPUs within their respective nodes. While this restricts the scalability of GPU workloads, it remains suitable for single-node GPU computations and smaller parallel tasks. Figure 5: Failed Login attempts in cluster 6 A PREPRINT - MARCH 17, 2025 Monitoring is a critical aspect of HPC cluster management, ensuring system health, performance optimization, and fault detection. We installed Ganglia due to its ease of setup and widespread use in HPC environments, allowing efficient tracking of system metrics across all compute nodes [13, 10]. Ganglia provides real-time resource utilization insights, helping to maintain workload balance and optimize job scheduling. Additionally, we tested Prometheus (on and Grafana (on as alternative monitoring solutions. Prometheus, with its time-series monitoring and alerting capabilities, allows detailed system metric collection via exporters [17]. Grafana, when integrated with Prometheus, provides an intuitive visualization interface for tracking CPU, memory, and GPU usage in real time. While both Prometheus and Grafana are viable options under the given IP configurations, we preferred Ganglia for its simpler deployment and efficient monitoring of our SLURM-managed cluster. Our cluster faced a significant cybersecurity threat daily due to internet connectivity with nearly 2000-6000 failed login attempts to the root account within 24 hours, which we suspect were primarily bot-driven attacks.\n\n--- Segment 11 ---\nWhile both Prometheus and Grafana are viable options under the given IP configurations, we preferred Ganglia for its simpler deployment and efficient monitoring of our SLURM-managed cluster. Our cluster faced a significant cybersecurity threat daily due to internet connectivity with nearly 2000-6000 failed login attempts to the root account within 24 hours, which we suspect were primarily bot-driven attacks. To mitigate this risk, we disable login with root account in SSH and we installed the Fail2Ban package to enhance SSH security. Fail2Ban monitors login attempts and, after six consecutive failed password entries, automatically blacklists the offending IP address for an hour, blocking further SSH communication. This proactive approach effectively reduced unauthorized access attempts and strengthened the cluster s overall security. 4 RESULTS AND DISCUSSION The implementation of our cluster setup demonstrates a cost-effective and practical solution for high-performance computing tasks. The master node efficiently managed resource allocation and user authentication, ensuring seamless operation across the cluster. Security was significantly enhanced with the integration of Fail2Ban, which successfully reduced unauthorized login attempts by blacklisting suspicious IP addresses. The individual GPU performance was validated for CUDA computations, proving effective for single-node tasks despite the hardware limitation of not being able to cluster GPUs. MPI communication over Ethernet was reliable, providing acceptable latency and bandwidth for medium-scale workloads. Additionally, the integration of MPI with SLURM ensured efficient resource scheduling for distributed computations. The adoption of Lmod and Lua simplified software environment management, enabling users to switch dynamically between various versions of Python, and other required software. The setup of Anaconda further streamlined Python- based workflows, offering flexibility and efficiency for diverse computational tasks. These features significantly improved the user experience in managing software environments within the cluster. Figure 6: Cluster Setup The addition of 1TB of shared storage, bind-mounted to user directories, facilitated the handling of large-scale datasets and ensured easy access to results. This enhanced the system s ability to support data-intensive applications and long-term storage needs. Containerization plays a crucial role in deep learning workflows by providing isolated and reproducible environments. In our cluster, we evaluated both Singularity and Docker as potential solutions for containerized deep learning environments. While Docker is widely used for its flexibility and ease of deployment, it requires root privileges, making it less suitable for shared HPC environments.\n\n--- Segment 12 ---\nIn our cluster, we evaluated both Singularity and Docker as potential solutions for containerized deep learning environments. While Docker is widely used for its flexibility and ease of deployment, it requires root privileges, making it less suitable for shared HPC environments. Conversely, Singularity enables users to run containers securely without requiring elevated permissions, making it a more HPC-friendly alternative. Both containerization tools facilitate seamless execution of deep learning frameworks like TensorFlow and PyTorch, ensuring dependency consistency across different nodes. Based on our evaluation, Singularity is preferred for our cluster due to its security model and compatibility with 7 A PREPRINT - MARCH 17, 2025 HPC resource managers like SLURM, but Docker remains a viable option for standalone deep learning projects and development environments. [16] [19] [34] In summary, the results demonstrate the feasibility of building a robust HPC cluster using commonly available hardware and open-source software. While the system effectively balances cost, scalability, and performance, limitations such as the inability to cluster GPUs and reliance on Ethernet for MPI communication introduce constraints for high-demand, large-scale applications. Future improvements could focus on integrating high-speed interconnects and upgrading to HPC-oriented GPUs to enhance the cluster s performance and scalability further. 5 TABLES AND COMPARISON The following contains the summary of components and software used: Component Specification Master Node 10.80.0.100, Intel i5 CPU, 16 GB RAM Compute Nodes c1, c2, c3 (10.80.0.101-103), Intel i5 CPU, 16 GB RAM GPUs NVIDIA GTX 1650 (on all nodes) Storage 1TB (shared across all nodes) Network Ethernet (1 Gbps) Table 1: Cluster Configuration Summary Software Version Purpose Rocky Linux 9.4 Operating System SLURM 22.05 Job Scheduling Lmod Installed Environment Management Anaconda 2023.09 Python-based Workflows Table 2: Software Stack 6 CONCLUSION Setting up a local GPU cluster within a small department or university presents a highly cost-effective alternative to renting cloud GPUs. Our cost analysis demonstrates that operating a locally installed cluster with four NVIDIA GTX 1650 GPUs (4 x 4GB 16GB total) incurs a monthly electricity cost of only NPR 5,760 (based on an 800W power consumption per node) in Nepal.\n\n--- Segment 13 ---\n5 TABLES AND COMPARISON The following contains the summary of components and software used: Component Specification Master Node 10.80.0.100, Intel i5 CPU, 16 GB RAM Compute Nodes c1, c2, c3 (10.80.0.101-103), Intel i5 CPU, 16 GB RAM GPUs NVIDIA GTX 1650 (on all nodes) Storage 1TB (shared across all nodes) Network Ethernet (1 Gbps) Table 1: Cluster Configuration Summary Software Version Purpose Rocky Linux 9.4 Operating System SLURM 22.05 Job Scheduling Lmod Installed Environment Management Anaconda 2023.09 Python-based Workflows Table 2: Software Stack 6 CONCLUSION Setting up a local GPU cluster within a small department or university presents a highly cost-effective alternative to renting cloud GPUs. Our cost analysis demonstrates that operating a locally installed cluster with four NVIDIA GTX 1650 GPUs (4 x 4GB 16GB total) incurs a monthly electricity cost of only NPR 5,760 (based on an 800W power consumption per node) in Nepal. In contrast, renting an NVIDIA T4 GPU (16GB) in the cloud costs approximately NPR 123.50 per hour, amounting to NPR 29,640 per month for similar usage [35, 36]. This stark difference underscores the long-term financial benefits of local GPU clusters, particularly in regions like Nepal, where electricity costs are relatively low. Beyond cost savings, a locally managed cluster offers greater control over data privacy, independence from internet connectivity, and uninterrupted access to computational resources, making it an ideal solution for research and education. While the initial hardware investment may seem substantial, many institutions can leverage existing computing labs to set up these clusters efficiently. Ultimately, the autonomy, sustainability, and reduced recurring costs of an in-house GPU cluster make it a far more viable and strategic investment for academic and research institutions compared to reliance on cloud-based alternatives. 7 ACKNOWLEDGEMENTS We express our sincere gratitude to Herald College Kathmandu for providing the research space, hardware, and infrastructure essential for this project.\n\n--- Segment 14 ---\nUltimately, the autonomy, sustainability, and reduced recurring costs of an in-house GPU cluster make it a far more viable and strategic investment for academic and research institutions compared to reliance on cloud-based alternatives. 7 ACKNOWLEDGEMENTS We express our sincere gratitude to Herald College Kathmandu for providing the research space, hardware, and infrastructure essential for this project. For detailed instructions on installing, and configuring, we have made our setup process and related scripts available at For running this cluster, We have also made slurm with gpu sbatch file and instruction at 8 A PREPRINT - MARCH 17, 2025 References [1] Dheeraj Chahal, Mayank Mishra, Surya Palepu, and Rekha Singhal. Performance and cost comparison of cloud services for deep learning workload. In Companion of the ACM SPEC International Conference on Performance Engineering, ICPE 21, page 49 55, New York, NY, USA, 2021. Association for Computing Machinery. [2] John Lawrence, Jonas Malmsten, Andrey Rybka, Daniel A Sabol, and Ken Triplin. Comparing tensorflow deep learning performance using cpus, gpus, local pcs and cloud. 2017. [3] Vanderlei Munhoz, Antoine Bonfils, Márcio Castro, and Odorico Mendizabal. A performance comparison of HPC workloads on traditional and cloud-based HPC clusters. In 2023 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW), pages 108 114. IEEE. [4] Jingoo Han, Luna Xu, M. Mustafa Rafique, Ali R. Butt, and Seung-Hwan Lim. A quantitative study of deep learning training on heterogeneous supercomputers. In 2019 IEEE International Conference on Cluster Computing (CLUSTER), pages 1 12. IEEE. [5] Emil Vaihela. The performance and optimization of hpc clusters. 2022. [6] Heramb Penyala, Soad Ibrahim, and Ayman El Mesalami. The raspberry pi education mine: For teaching engineering and computer science students concepts like, computer clusters, parallel computing, and distributed computing. In 2020 IEEE International Conference on Electro Information Technology (EIT), pages 624 628. IEEE. [7] Vincent A. Cicirello.\n\n--- Segment 15 ---\nIEEE. [7] Vincent A. Cicirello. Design, configuration, implementation, and performance of a simple 32 core raspberry pi cluster. [8] Sandino Vargas-Pérez. Designing an independent study to create HPC learning experiences for undergraduates. In 2022 IEEE 29th International Conference on High Performance Computing, Data and Analytics Workshop (HiPCW), pages 6 11. IEEE. [9] Nkundwe Moses Mwasaga and Mike Joy. Implementing micro high performance computing (µHPC) artifact: Affordable HPC facilities for academia. In 2020 IEEE Frontiers in Education Conference (FIE), pages 1 9. IEEE. [10] S. Mollova, M. Zhekov, A. Kostadinov, and P. Georgieva. Laboratory model for research on computer cluster systems. In 2018 41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pages 1388 1393. IEEE. [11] Heramb Penyala, Soad Ibrahim, and Ayman El Mesalami. The raspberry pi education mine: For teaching engineering and computer science students concepts like, computer clusters, parallel computing, and distributed computing. In 2020 IEEE International Conference on Electro Information Technology (EIT), pages 624 628. IEEE. [12] Lluc Alvarez, Eduard Ayguade, and Filippo Mantovani. Teaching HPC systems and parallel programming with small-scale clusters. In 2018 IEEE ACM Workshop on Education for High-Performance Computing (EduHPC), pages 1 10. IEEE. [13] Chavala Mutyala Rao and K. Shyamala. Analysis and implementation of a parallel computing cluster for solving computational problems in data analytics. In 2020 5th International Conference on Computing, Communication and Security (ICCCS), pages 1 5. IEEE. [14] Dileep Kumar, Sheeraz Memon, and Liaquat Ali Thebo. Design, implementation performance analysis of low cost high performance computing (HPC) clusters. In 2018 12th International Conference on Signal Processing and Communication Systems (ICSPCS), pages 1 6. IEEE. [15] Release notes NVIDIA DGX OS 6 user guide. [16] Vaihela Emil. Creation and optimization of HPC clusters. [17] Arindam Majee.\n\n--- Segment 16 ---\nCreation and optimization of HPC clusters. [17] Arindam Majee. DeepOps SLURM: Your GPU cluster guide. [18] Deploying a scalable GPU-as-a-service platform and building a deep learning project in under 80 minutes GTC digital march 2020 NVIDIA on-demand. [19] Deploying rich cluster API on DGX for multi-user sharing. [20] Wale Soyinka. Installing Rocky Linux 9 - Documentation docs.rockylinux.org. org guides installation . [Accessed 29-11-2024]. [21] Ravi Saive. How to Setup SSH Passwordless Login in Linux tecmint.com. ssh-passwordless-login-using-ssh-keygen-in-5-easy-steps . [Accessed 29-11-2024]. 9 A PREPRINT - MARCH 17, 2025 [22] Linux cluster sysadmin Parallel command execution with PDSH rittmanmead.com. linux-cluster-sysadmin-parallel-command-execution-with-pdsh . [Accessed 30-01-2025]. [23] GitHub - SergioMEV slurm-for-dummies: A dummy s guide to setting up (and using) HPC clusters on Ubuntu 22.04LTS using Slurm and Munge. Created by the Quant Club UIowa. github.com. com SergioMEV slurm-for-dummies. [Accessed 29-11-2024]. [24] Bind Mounts in Linux docs.rackspace.com. bind-mounts-in-linux. [Accessed 29-11-2024]. [25] How to set up an NFS Mount on Rocky Linux 8 howtoforge.com. how-to-set-up-an-nfs-mount-on-rocky-linux-8 . [Accessed 29-11-2024]. [26] Giovanni Battista Barone, Davide Bottalico, Luisa Carracciuolo, Alessandra Doria, Davide Michelino, Silvio Pardi, Guido Russo, Gianluca Sabella, and Bernardino Spisso. Designing and implementing a high-performance computing heterogeneous cluster. In 2022 International Conference on Electrical, Computer and Energy Technologies (ICECET), pages 1 6. IEEE.\n\n--- Segment 17 ---\nIn 2022 International Conference on Electrical, Computer and Energy Technologies (ICECET), pages 1 6. IEEE. [27] Mandeep Kumar. Distributed execution of dask on HPC: A case study. In 2023 World Conference on Communica- tion Computing (WCONF), pages 1 4. IEEE. [28] Andria Arisal, Syam Budi Iryanto, and Zaenal Akbar. Managing multi-services for multi-users in heteroge- neous cluster computing system. In 2019 International Conference on Computer, Control, Informatics and its Applications (IC3INA), pages 103 107. IEEE. [29] Slurm Workload Manager - Generic Resource (GRES) Scheduling slurm.schedmd.com. schedmd.com gres.html. [Accessed 29-11-2024]. [30] TACC. How to Transition to Lmod (or how to test Lmod without installing it for all); Lmod 8.7.53 documentation lmod.readthedocs.io. [Accessed 29-11-2024]. [31] Modules, LMod and Easybuild - Big Data Analytics nesusws-tutorials-bd-dl.readthedocs.io. https: nesusws-tutorials-bd-dl.readthedocs.io en latest hands-on easybuild . [Accessed 30-01- 2025]. [32] Todd Gamblin, Matthew LeGendre, Michael R Collette, Gregory L Lee, Adam Moody, Bronis R De Supinski, and Scott Futral. The spack package manager: bringing order to hpc software chaos. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 12, 2015. [33] J.J. Vegas Olmos, Liran Liss, Tzahi Oved, Zachi Binshtock, and Dror Goldenberg. Advanced software architectures and technologies in high performance computing and data centers. In Optical Fiber Communication Conference (OFC) 2020, page T3K.3. Optica Publishing Group. [34] How to run NGC deep learning containers with singularity. [35] Nepal electricity prices, March 2024 GlobalPetrolPrices.com globalpetrolprices.com.\n\n--- Segment 18 ---\n[34] How to run NGC deep learning containers with singularity. [35] Nepal electricity prices, March 2024 GlobalPetrolPrices.com globalpetrolprices.com. globalpetrolprices.com Nepal electricity_prices . [Accessed 29-11-2024]. [36] NVIDIA Tesla T4 GPUs now available in beta Google Cloud Blog cloud.google.com. nvidia-tesla-t4-gpus-now-available-in-beta. [Accessed 29-11-2024]. 10\n\n