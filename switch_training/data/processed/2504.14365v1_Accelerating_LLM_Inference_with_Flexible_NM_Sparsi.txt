=== ORIGINAL PDF: 2504.14365v1_Accelerating_LLM_Inference_with_Flexible_NM_Sparsi.pdf ===\n\nRaw text length: 52976 characters\nCleaned text length: 52654 characters\nNumber of segments: 38\n\n=== CLEANED TEXT ===\n\nAccelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator Akshat Ramachandran 1, Souvik Kundu2, Arnab Raha3, Shamik Kundu3, Deepak K. Mathaikutty3, Tushar Krishna1 1Georgia Institute of Technology, USA, 2Intel Labs, USA, 3Intel Corporation, USA Corresponding email: Abstract Large language model (LLM) pruning with fixed N:M structured sparsity significantly limits the expressivity of the sparse model, yielding sub-optimal performance. On the contrary, support for more than one N:M pattern to provide sparse representational freedom yields a costly overhead in the hardware. To mitigate these challenges for LLMs, we first present a flexible layer-wise outlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the identification of optimal layer-wise N and M values (from a given range) by simultaneously accounting for the presence and distribution of outliers, allowing a higher degree of representational freedom. To deploy the sparse models with such N:M flexibility, we then present a flexible low overhead, digital compute- in-memory architecture (FlexCiM). FlexCiM enables support for diverse sparsity patterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros which are adaptively aggregated and disaggregated through distribution and merging mechanisms for different values of N and M. Extensive experiments on both transformer-based and recurrence-based state space foundation models (SSMs) demonstrate FLOW to outperform existing alternatives with an accuracy improvement of up to 36 , while FlexCiM delivers up to 1.75 lower inference latency and 1.5 lower energy consumption compared to existing sparse accelerators. Code is available at: I. INTRODUCTION To reduce the colossal size of large language models (LLMs) and enable their efficient deployment on resource-constrained devices, post-training pruning has emerged as an effective model compression method [9], [33], [37]. It reduces the memory footprint of the pre- trained LLMs by removing ineffectual model parameters, at the granularity of individual weights (unstructured) or blocks of weights (structured), and storing sparse tensors in a compressed format (CSR CSC) [14]. Notably, model pruning may yield compute acceler- ation via skipping ineffectual computations associated with the zero- valued weight activation. However, traditional weight pruning often requires fine-tuning, which becomes exceedingly compute-heavy for LLMs. Furthermore, this often requires the model to yield structured pruned weights, which can cause a high accuracy drop compared to the models pruned via an unstructured approach. To mitigate the dilemma of accuracy vs. compute efficiency, structured N:M sparsity [13], [38] has emerged as a popular solution that can yield models with compute acceleration benefits while maintaining good accuracy. In specific, many of the existing commodity accelerators have enabled sparse N:M acceleration for fixed N:M values [23]. Therefore, this work focuses on N:M structured sparsity for efficient LLM inference. To enable post-training pruning and mitigate fine-tuning cost, SparseGPT [9] proposed to employ row-Hessian information to rank the importance of pre-trained weights and prune the least important ones. SparseGPT still required high compute owing to its iterative computation of second-order information (Hessian) for each layer. Wanda [33] avoided the computationally expensive second-order details by introducing a pruning metric that considers both the weight magnitudes and their corresponding input activations. Considering the performance benefits of structured N:M sparsity, both these works [9], [33] extended their approach to support a fixed and pre-determined Work done during an internship at Intel. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Dense SA DCiM VEGETA SDP FlexCiM Dense SA DCiM VEGETA SDP FlexCiM Dense SA DCiM VEGETA SDP FlexCiM LLaMA2-7B LLaMA3-70B LLaVa2-7B Normalized Energy Compute Local Memory Access Global Memory Access 0.79 0.73 0.63 0.61 0.81 0.72 0.78 0.69 0.52 Fig. 1: Normalized energy comparison between several digital and DCiM accelerators and the proposed FlexCiM across different models for flexible N:M inference acceleration. N:M pattern for all the layers. However, only recently [38] researchers have determined that due to the presence of a large number of outliers1 in LLMs, different layers are inherently heterogeneous and exhibit different tolerances to pruning. Based on this, outlier weighed layer-wise sparsity (OWL) [38] was proposed. OWL differs from prior techniques in its assignment of different sparsity ratios for different layers through allocations of different N values in the N:M pattern with fixed M. However, this is still sub-optimal and has limited representational freedom due to their static selection of M value across all the layers. Moreover, existing hardware accelerators cost significant overhead to employ different values of N and or M patterns in a single architecture, which was beyond the scope of [38] to address. It is well established that the decode stage of LLM inference is memory bound [1], [26], with weights loading constituting a sig- nificant bottleneck. Traditional Von-Neumann accelerators consume significant energy and time when moving weights from on-chip buffers to processing elements (PEs). This makes existing traditional accelerators with flexible structured sparsity support [14], [36] poor candidates to accelerate N:M sparse LLMs for inference. An al- ternative computing paradigm, namely, digital compute-in-memory (DCiM) offers an efficient solution to the above challenge by moving the compute within the memory arrays. This mitigates the memory access bottleneck. However, despite the advantages of DCiM over traditional digital accelerators, there has been limited exploration of flexible structured sparsity support for DCiM-based architectures. This can be associated with the rigid crossbar structure of memory arrays and the infeasibility of transferring the flexible structured sparsity techniques employed for digital accelerators to DCiM ( III). Our contributions. We present two significant contributions toward addressing the above-mentioned algorithmic and hardware limita- tions. Specifically, we present a pruning algorithm that can assign different N:M sparsity with a higher degree of freedom and then present the first DCiM accelerator to support such patterns with minimal overhead, as summarized below. 1Scalars with larger magnitude compared to the rest of the tensor. arXiv:2504.14365v1 [cs.LG] 19 Apr 2025 We first explore the efficient and optimal assignment of the N:M sparsity pattern to each layer of an LLM. Specifically, we identify that compared to static N or M, having the luxury to choose both N and M allows us to yield better sparse representational freedom necessary to maintain improved model accuracy. To this end, we propose FLOW flexible layer-wise outlier-density-aware N:M spar- sity selection method. FLOW is based on a key insight that both the presence of outliers and their distribution in different layers con- tributes to the layer heterogeneity, impacting their tolerance towards model pruning. We leverage these two characteristics presence and distribution of outliers to assign diverse N:M values with the ability to determine the optimal N and M for a layer simultaneously. Towards deploying the flexible N:M sparse LLMs in DCiM, we first analyze its challenges in directly incorporating techniques employed by digital accelerators such as [14], [36], particularly the infeasibility of incorporating large multiplexers within each memory cell of DCiM due to the rigid crossbar structure of the memory array. We then introduce a novel DCiM-based accelerator with flex- ible structured sparsity support FlexCiM. It decomposes an existing DCiM macro into multiple partitions called sub-macros (partitioned along the row dimension) and introduces two new units distribution and merging units. These units coordinate the mapping of weights to each sub-macro and broadcasting of input activations (iActs) to support diverse N:M sparsity with minimal overheads. We conduct extensive experiments with foundation models based on both transformer and state space model (SSM) primitives. Specifically, experiments across various LLMs and VLMs demon- strate FLOW outperforming SoTA pruning techniques by achieving up to 36 better accuracy at high sparsity. Our accelerator, FlexCiM, on the other hand, can yield up to 1.75 lower inference latency with 1.5 lower energy (see Figure 1) compared to the SoTA sparse accelerators with a minimal area overhead of 6 . II. BACKGROUND AND RELATED WORKS LLM pruning with N:M Sparsity. N:M sparsity retains N non- zero elements within a fine-grained block of consecutive M elements. Existing LLM pruning works, including SparseGPT [9] and Wanda [33], when extended to structured N:M sparsity, were limited in their exploration to a fixed N:M pattern for all layers. More recently, [38] highlighted the importance of a heterogeneous sparsity budget for different LLM layers. Specifically, it proposed a non-uniform layer- wise N:M sparsity scheme with different N values for different layers determined based on the number of outliers in each layer. However, unlike existing methods [9], [33], [37], [38], [43], in this work, to yield more optimally pruned models, we present a higher degree of sparse representational freedom by identifying both the optimal N and M values of an N:M pattern. DCiM accelerators. Conventional digital accelerators [24], [26], [27] employ compute engines separate from memory (buffers) to handle LLM inference, often leading to memory access bottlenecks, increasing the memory access cost [32]. Furthermore, the decode stage of LLM inference is primarily memory-bounded, making weight activation loading a significant bottleneck [18]. In light of these, CiM architectures [10], [32], [39] have been propelled into the mainstream. In CiM, the processing elements and weight local memory (SRAM, ReRAM, DRAM, etc.) are merged into a single macro [16], [35], effectively reducing on-chip memory access, yield- ing accelerated computation. Early work on CiM focused on analog CiM architectures [5], [19], embedding analog multiply-accumulate (MAC) units within memory arrays. While analog CiM architectures demonstrate high energy efficiency, they have various limitations, including vulnerability to noise process variations, limited bit pre- cision, and significant ADC DAC overheads, collectively restricting 0.61 0.65 0.52 0.63 0.41 0.42 0.57 0.48 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 1 3 4 7 15 18 22 26 27 Normalized D Layer Index 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 BERT-large OPT-6.7B LLaMA2-13B LLaMA3-8B OF-9B VILA-7B Average Normalized D Model (a) (b) Fig. 2: (a) Outlier distribution measured by pairwise L1 distance between outliers (normalized to BERT-large) across different models. (b) Intra-model variations in outlier distribution across different layers of a LLaMA3-8B model. their scalable adoption. Recently, DCiM architectures have gained considerable traction, with various industry accelerators [10], [32], [35] adopting these macros to embed digital MAC within memory directly. This eliminates the need for costly DACs ADCs and offers enhanced robustness against variation and noise and sufficient preci- sion to achieve high efficiency and accurate inference. Additionally, DCiM architectures are inherently compatible with existing digital design flows, making them suitable candidates for LLM acceleration. Accelerators supporting sparsity. Among digital accelerators, the NVIDIA sparse tensor core [23] exploited fixed 2:4 sparsity of weights in DNNs. VEGETA [14] presented flexible N:M sparsity support for weights, and S2TA [20] extended support for the same for both weights and activations, albeit with significant overheads. [36] proposed hierarchically structured sparsity to represent diverse sparsity patterns in DNNs. Flexible architectures like [25] can poten- tially accelerate any N:M ratio exploiting unstructured sparsity, yet often suffer from unbalanced critical path delay. However, support for flexible structured sparsity in DCiM accelerators has been largely unexplored. The majority of sparse DCiM accelerators have either focused on bit-level sparsity [7], [42] or unstructured sparsity [31], [44]. Apart from these, a few works [35], [40] introduced fixed 1:2 sparsity support in DCiMs. However, challenges and potential for deploying flexible N:M sparsity in DCiM are yet to be unveiled. III. MOTIVATION A. Distribution of Outliers in LLMs OWL [38] presented a non-uniform layer-wise N:M sparsity with a fixed M and a different N for different layers with N being proportional to the layer outlier ratio. That is, layers with a high outlier ratio would have a high N value. However, varying N with a fixed M value, results in suboptimal sparse representation freedom in each layer. We argue that a predetermined M value when assigning different N:M sparsity patterns ignores another key metric in LLM weights, namely the outlier distribution (D). For a tensor l, we compute Dl by a proxy metric that measures the summation of the pairwise distance between the outliers averaged over the number of outliers. In Eq. 1, dist(.) measures the pair-wise L1-distance between two outliers, where nC2 is the total number of outlier pairs in a layer with n outliers. We then compute the normalized outlier distribution (ND) through the min-max normalization across all the layers as shown in Eq. 1. NDl (Dl min(D)) max(D) , where Dl PnC2 1 dist(ok, oj) nC2 (1) where D represents the set of Ds for all the layers. To understand the variance of ND over different models, in Figure 2(a) we plot the average ND averaged over all the layers for different models (including VLMs). In specific, we observe that models like BERT, OPT have a relatively high average ND, indicating that outliers are sparsely distributed within these models. On the contrary, models like LLaMA3-8B and OpenFlamingo-9B possess a significantly low value of average ND, implying that outliers are largely clustered together. Case 1 Outlier Presence: Low Outlier Distribution: Sparse Optimal N:M 1:4 Case 2 Outlier Presence: High Outlier Distribution: Sparse Optimal N:M 2:4 Case 3 Outlier Presence: High Outlier Distribution: Clustered Adjacent Optimal N:M 4:8 Inlier Value Outlier Value LEGEND Fig. 3: Example of efficient N:M assignment based on outlier presence and distribution in different situations. Furthermore, even within the same model, different layers depict varied normalized outlier distributions. Figure 2(b) demonstrates the diverse ND values for different layers of a LLaMA3-8B. We hypothesize that while choosing the N:M sparsity patterns to different layers, it is important to choose the optimal M value for each layer. If a layer has largely clustered outliers (for example: layer 15 of LLaMA3-8B), the pruning algorithm should favor a larger M value when assigning an N:M sparsity pattern. This will help to (a) minimize the possibility of pruning outliers within a block of M and (b) have a higher degree of flexibility in deciding which features to prune. We demonstrate the usefulness of such N:M selection in yielding a better perplexity score in Table I compared to fixed M. B. Challenges of Flexible Structured Sparsity in DCiMs Sparse digital accelerators [14], [23] have significant multiplexer overhead within each PE to support flexible N:M sparsity. However, this strategy is not directly applicable to DCiM-based accelerators due to various reasons: Area overhead. The key benefit of DCiM architectures is their high memory density [32], [35]. Typical memory cells in DCiM employ 6T or 12T SRAM cells [10]. However, a single 8:1 multiplexer for flexible N:8 support requires over 42 transistors [35] increasing the area overhead by more than 3 and undermining the compactness and efficiency of the DCiM architectures. Rigid crossbar structure. Digital accelerators using weight- stationary dataflow stream up to M activations into each PE [14], enabling PE-level selection. In contrast, DCiM architectures are restricted by the two-bit line (BL and BL) structure of SRAM cells, allowing only two activations to be streamed into a DCiM macro. Bit-serial processing. DCiM architectures employ bit-serial oper- ations to maximize energy efficiency, where each SRAM bit-cell stores a single weight bit and computes partial products with serially streamed activation bits. Von-Neumann accelerators, by contrast, use parallel multipliers adders and store metadata alongside weights in each PE to manage activation selection. For N:8 sparsity, each N non-zero element requires 3 bits of metadata. In DCiMs, this would necessitate more metadata bits per SRAM cell than weight bits, complicating implementation. These challenges highlight the need for a novel approach to enable flexible N:M sparsity tailored for DCiMs. IV. FLOW N:M SPARSITY In this section, we first formalize outlier identification and then describe FLOW s methodology for optimal N:M assignment. A. Outlier Identification In this work, we target N:M sparsity for weights [38] and focus our discussion on weight outliers. Let us assume an LLM weight tensor W R(Cin Cout) with input activations X R(T K Cin). Here, T, K are the number of tokens and sequence length, respectively. Inspired by prior works [33], we assign an importance score to each weight element Wij, determined as IW Wij Xj 2 with Xj 2 being the L2 norm of the input activations connected to the weight element. IW acts as a measure to identify the outliers based on both weight and associated activation values [33]. Upon obtaining this score, we calculate the mean (µ) and standard deviation (σ) of the weight importance scores for each layer [38]. Subsequently, we categorize all weights exceeding the threshold τ σ as outliers and the rest as inliers. We empirically identify τ 3 or τ 5 provide the best results. B. FLOW Methodology For an L-layer LLM, with a target sparsity ratio of S, the layer- wise sparsity ratios are assigned and handled by two lists N [N 1, N 2, ..., N L] and M [M 1, M 2, ..., M L]. For a layer l, the sparsity pattern is N l:M l, the sparsity ratio is Sl (M l N l) (M l) and S (Pl L l 1 Sl) L. Following the process outlined in IV-A, we obtain the outlier fraction in each layer to get O [O1, O2, ..., OL], where Ol identifies the normalized outlier fraction in a layer l [38]. Intuitively, if a layer has a larger Ol, the number of non-zeros in a block of M l elements should be higher (i.e., N l ). In specific, N l Ol. In Figure 3, examples with higher presence of outliers have larger values of N. FLOW takes the normalized outlier distribution of each layer (NDl) into account to identify the optimal M l. In specific, for each layer weight tensor, we partition it into B non-overlapping blocks of dimension (128 128). Within a block b, we measure Dl b by calculating the pairwise L1 distance between all the outlier pairs following Equation 1. We obtain Dl as PB b 1 Dl b, and finally get NDl via normalizing the Dl values following Equation 1. A higher NDl indicates a farther distance between outliers within a layer, implying that they are sparsely distributed (Case 1 in Figure 3), whereas smaller NDl indicates a large number of closely located outliers (Case 3 in Figure 3). Intuitively, if a layer has many closely located outliers, a higher value of M l is preferred, providing more freedom while performing weight pruning and preventing inadvertent pruning of outliers. In specific, M l (1 NDl). Automated layer-wise N, M allocation. Given the relations of N l, M l, we formulate the problem of assigning layer-wise N:M values as an integer linear programming (ILP) problem: argmin L X l 1 α N l k Ol β M l h (1 NDl) (2) s.t. 1 L L X l 1 Sl S, 0 Sl 1, l {1, . . . , L} 1 N l M l, N l {1, 2, 4, 8}, M l {2, 4, 8} where α, β are the weighing factors to optimize N and M values, respectively. k and h are the hyper-parameters to keep the terms Ol and NDl in the same ballpark range as N l and M l choices, respectively. We empirically choose their values to be (α, β, k, h) (1, 4, 8, 8). This formulation aims to assign values to N l and M l for each layer while minimizing deviations from desired characteristics, all while maintaining the target sparsity (Equation 2). N l is aligned with the normalized outlier count Ol, and M l is aligned with the outlier distribution (1 NDl). It is important to note that FLOW can be employed to explore any combination of N and M. However, to ensure hardware efficiency, we restrict N and M choices to powers of two and a maximum value of 8 for this work (see N, M choice set mentioned in Equation 2). V. FLEXCIM ACCELERATOR Overview. Figure 4(a) provides an overview of the FlexCiM architec- ture. FlexCiM extends an existing X Y 8 [35] SRAM-based DCiM bit-cell macro supporting fixed 1:2 sparsity, where (X, Y ) represent the crossbar array dimensions with the memory word size being 8-bits. FlexCiM can be employed to accelerate any layer that can be represented as a GEMM [14] operation. It partitions the existing iAct Serializer WL Driver (b) 8b iAct[15:0] Row 0 Row 1 Row X P-2 Row X P-1 Adder Tree To PSum Buffer 8b 9b 2b 1b 17b Distribution Unit Merging Unit PSum Buffer PSum Buffer PSum Buffer PSum Buffer Sub-Macro 0 Sub-Macro 2 Sub-Macro 3 Y (a) Sub-Macro 1 FlexCiM Global Controller iAct[1:0] 2b 1b 1b 1b WL WL WL BL (c) Column controller Fig. 4: (a) FlexCiM overview with a partition size P 4; (b) Organization of a single column of a FlexCiM sub-macro; (c) Memory cell structure. macro into P components (named as sub-macros), each of dimension X P Y 8. To seamlessly support diverse N:M sparsity patterns with this architecture, we introduce two novel hardware components, namely the distribution and the merging units to orchestrate the working of the P sub-macros. To enable the bit-serial computation, a local input activation (iAct) buffer feeds the activations to the distribution unit (detailed later) at a rate of X 8-bit activations per cycle, which is serialized via the iAct serializer unit in each sub- macro [10], [32]. We employ a two-tier control unit comprising a global controller and column-wise controller to generate control signals that manage pipelining across rows and columns of sub- macros and configure multiplexer select signals. In our demonstrated implementation, we assume X 128,Y 32 for the DCiM bit-cell macro, with partition size of P 4, resulting in an 8Kb sub-macro of dimensions 32 32 8. Each sub-macro column is divided into 32 banks to enable parallel MAC operation. A. Sub-Macro Architecture Memory cell. Figure 4(c), presents a memory word in FlexCiM, with each word composed of 8 bit-cells. To abstract the circuit complexity and demonstrate our proposal to be orthogonal to any SRAM technology, we follow [17] and implement a 28nm, standard cell, latch-based memory structurally resembling a 6T SRAM cell [35]. Each memory cell has two bit-lines BL, BL, shared by all cells in a row, and a word-line (WL), shared per column. All memory cells in a column share a control signal to enable the compute units, namely EN COL. Inspired by [35], we implement a 2:1 multiplexer that is shared by a memory word to select one of the two iActs (via control signal (isel) streamed along the two bit-lines to enable 1:2 structured sparsity as a baseline case. A bit-serial multiplier within each SRAM cell (implemented as NOR gate) performs the multiplication between the selected iAct and stored weight. The memory cell has two modes of operation: memory (MS) and compute (C) mode. Standard read and write operations are performed in MS mode. The WL is activated to access all cells in a column, with BLs used to read write data. C mode is enabled for all cells in a column only when WL 0 and the corresponding EN COL 1, allowing the compute units to perform MAC operations. Memory column. Figure 4(b) illustrates the organization of a column in a sub-macro. Each column comprises 32 rows of 8-bit memory words and a column-wise adder tree. Each memory word performs bit-wise multiplication with the streamed iAct bits (MSB first), which are then accumulated column-wise by a 32-input adder tree and directed to the partial sum (PSum) accumulator. Partial sum accumulator. Since DCiMs typically perform bit-serial arithmetic, the MACs from each column-wise adder tree must be bit shifted based on the bit significance of the streamed iAct and subse- quently accumulated to compute the final partial sum. The complete operation can be expressed as, PSumfinal Pi 7 i 0 2i P(iAct[i] W). Column-wise controller. Each sub-macro possesses a controller that is dedicated to generating the isel signal for all the 2:1 multiplexers in each column and enabling column pipelining (see V-C) via the EN COL signal. B. Hardware Extensions for Flexible N:M Sparsity N:M sparsity storage format. In FlexCiM all sparse weight tensors are encoded in compressed sparse column (CSC) format [14]. Each column is stored as a list consisting of all the nonzero values, and the coordinates of the N non-zero values within a block of size M are stored as the corresponding metadata (see Figure 5). We introduce two key hardware blocks, the distribution unit and merging unit, that enable flexible N:M structured sparsity together. Distribution unit. The distribution unit is responsible for efficiently feeding iActs to the sub-macros based on the N and M values. As depicted in Figure 4, it is composed of P P : 1 multiplexers that serve each of the P sub-macros (P 4 in our implementation) with a bit- width of 16 bits (packing two 8-bit iActs) per input line. A distribution unit is shared by spatially identical rows of all sub-macros. For example, row 0 of all P sub-macros shares one distribution unit. The M value indicates the number of iActs to be distributed by each multiplexer in the distribution unit. The N value of a sparsity pattern identifies the number of sub-macros that are aggregated together to process the same block M. In particular, for N 1, multiple multiplexers will have the same set of inputs. Still, the distribution unit will select the appropriate input line based on the metadata (see V-D for a demonstration). The distribution unit abstracts the complexity of supporting large multiplexers in the memory cell by efficiently selecting the correct set of two activations to be fed to the memory array. We demonstrate in V-C that by efficiently performing row and column pipelining, we require only 32 distribution units serving all rows of a column of all sub-macros every clock cycle. Merging unit. The merging unit is responsible for accumulating the partial sums that are individually computed by each column of each sub-macro to generate the final partial sum in our partitioned scheme. To enable this, the merging unit comprises a P-input adder tree that reads its inputs from the partial sum accumulator s buffer. C. Pipelining Scheme Following on-chip buffers employed in comparable real-world systems [8], [32], we design the local iAct buffer with a bandwidth of 1024 bits cycle (128 8-bit activations). For dense LLM inference, 128 iActs are streamed to a column, enabling parallel MAC operations by all rows in a column of all sub-macros. However, for sparse inference with the highest sparsity i.e., 1:8 in our implementation, each row requires 8 iActs to choose from, which translates to 1024 iActs required by each column. Considering the upper limit of memory bandwidth, during 1:8 inference, only 4 rows of a column can perform parallel MAC operations. Similarly, 8 rows for 1:4 and 16 rows for 1:2. Due to the bit-serial nature of DCiMs, a MAC operation takes 8 cycles. Therefore, we overlap computation with memory access via row-pipelining. The number of row pipeline stages ( stages) is equal to 32 ( of rows grouped). For example, for the 1:8 case, each set of 4 rows in a column activated via EN COL are fed with iActs in a single cycle, and the remaining 28 rows are fed iActs in the subsequent 8 clock cycles in sets of 4, resulting in stages 8. Every stages cycle, the EN COL of the adjacent column is activated, and similarly, iActs are fed to all rows in the column. Once all columns have been fed with iActs, EN COL of the first column is reactivated, and the process is repeated. We perform 1:4 Sparsity Pattern 1 iActs1x32 2 3 30 31 32 WT2x32 1 0 0 0 0 2 0 0 0 0 0 16 15 0 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 0 2 2 3 1 2 2 0 1 2 3 3 1 0 1 1 Weights encoded in CSC format 1 2 9 10 3 4 11 12 7 8 15 16 5 6 13 14 1 2 3 4 1 2 1 2 0 5 6 7 8 7 8 1 7 8 9 10 1112 9 10 0 1314 1516 1516 0 23 24 9 10 25 26 15 16 29 30 17 18 0 1 0 1 1 1 0 1 EN_COL 0 1 2 9 10 3 4 11 12 7 8 15 16 5 6 13 14 1 2 7 8 23 24 9 10 25 26 15 16 29 30 17 18 0 1 0 1 1 1 0 1 EN_COL 1 Row 0 Distribution Unit 3 4 0 19 20 0 7 8 1 23 24 1 11 12 0 25 26 1 13 14 0 29 30 1 Global Controller 0 2 2 3 2 2 0 1 2 1 1 2 3 4 5 4:8 Sparsity Pattern 1 iActs1x16 2 3 14 15 16 WT2x16 1 9 2 10 3 0 0 0 0 0 4 11 0 0 0 12 1 9 2 10 3 11 4 12 5 13 6 14 7 15 8 16 0 0 1 1 2 5 5 7 1 2 3 3 4 5 5 7 Weights encoded in CSC format 1 9 5 13 2 10 6 14 4 12 8 16 3 11 7 15 1 2 3 4 1 2 1 2 0 1 2 0 1 2 3 4 1 5 6 2 11 12 3 4 13 14 5 6 13 14 9 10 0 1 1 1 0 0 1 1 EN_COL 0 EN_COL 1 Row 0 Distribution Unit Global Controller 0 0 1 1 5 5 7 2 2 1 1 2 3 4 5 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 Sub-Macro 0 Sub-Macro 1 Sub-Macro 2 Sub-Macro 3 Sub-Macro 0 Sub-Macro 1 Sub-Macro 2 Sub-Macro 3 1 9 5 13 2 10 6 14 4 12 8 16 3 11 7 15 1 2 1 2 11 12 3 4 13 14 5 6 13 14 9 10 0 1 1 1 0 0 1 1 4 1 2 0 11 0 1 2 1 11 12 1 5 6 0 13 1 7 8 0 15 16 1 12 14 Metadata Metadata 1.63x lower latency than dense inference ! 1.42x lower latency than dense inference ! Fig. 5: Example of FlexCiM running 1:4 and 4:8 sparsity patterns. column-pipelining to reduce the overhead of the distribution unit because, without column pipelining, each column would require a dedicated distribution unit, which would be prohibitively expensive for memory-centric designs. D. Walkthrough Example To demonstrate FlexCiM enabling inference with diverse N:M patterns, we explain through two simple examples in Figure 5. The N of an N:M pattern identifies the number of sub-macros that work together to process a block of size M. Similarly, M specifies the number of iActs handled by each multiplexer in the distribution unit. For the 4:8 pattern, each set of four non-zero values are mapped to spatially identical locations in four sub-macros. For the 1:4 pattern, each sub-macro works independently on a block of size M, with each multiplexer receiving a separate set of 4 iActs. Note that the same distribution units employed for iActs can be used to map the weights to each of the sub-macros. 1 The global controller receives the CSC metadata (in green) of the mapped weights and generates corresponding select signals to control the multiplexers. The LSB of the metadata is always reserved for the 2:1 multiplexer in the sub-macro. The remaining bits are employed for the multiplexers in the distribution unit. For example, for M 8, the metadata requires 3 bits, where the top 2 bits are for the distribution unit multiplexers. 2 The distribution unit for each row (only row 0 TABLE I: WikiText2 perplexity of pruning methods for different LLM model families at different target sparsity ratios. LLaMA-2 [34] LLaMA-3 [22] Mixtral [15] Mamba [12] Method N:M 7B 13B 70B 8B 70B 8x7B 130M 1.4B Baseline Dense 5.47 4.83 3.31 6.13 2.85 3.84 20.04 10.42 Target Sparsity: 50 Magnitude 4:8 16.87 9.59 6.83 9.16 5.84 7.91 58.63 43.24 SparseGPT [9] 4:8 8.59 7.18 5.02 8.51 4.34 7.15 29.67 14.98 Wanda [33] 4:8 8.01 6.55 4.68 8.07 4.15 6.98 40.36 21.36 OWL [38] Mixed N:8 7.92 6.08 4.55 8.02 3.95 6.57 27.28 14.07 FLOW (Ours) Mixed N:M 7.04 5.73 3.97 7.03 3.46 5.96 24.48 12.96 Target Sparsity: 60 Magnitude 3:8 54.59 48.96 49.51 52.38 57.63 56.76 1e20 1e14 SparseGPT [9] 3:8 44.68 42.69 39.57 45.09 35.27 41.37 54.83 31.57 Wanda [33] 3:8 43.26 40.24 38.97 44.93 33.78 39.86 1268 1107 OWL [38] Mixed N:8 20.68 18.68 18.57 16.26 11.23 14.58 31.56 18.67 FLOW (Ours) Mixed N:M 18.96 16.53 17.95 15.24 10.89 13.83 28.61 15.37 Target Sparsity: Unconstrained OWL [38] Mixed N:8 (51.3 ) 7.98 6.25 4.87 8.36 4.09 6.87 27.35 14.03 FLOW (Ours) Mixed N:M (58.5 ) 7.13 5.95 4.57 8.03 3.87 6.35 25.12 13.08 unit shown in the figure) in column 0 selects the appropriate input line and directs it to the corresponding sub-macros. 3 The first column of each of the sub-macros receives the selected iActs. These are then serialized and sent along the bit-lines. 4 The 2:1 multiplexer selects the appropriate iAct (highlighted in dark blue) from the bit-lines to be multiplied with the stored weights. 5 After stages cycles, column 1 receives the iActs and the process is repeated. The merging unit performs final partial sum accumulation by reading from the PSum buffers of each column as they are calculated. Notably, in the 1:2 base case and dense inference, no selection occurs in the distribution units. Furthermore, for dense inference, the same iActs are streamed along both bit-lines, and Isel is a don t-care. VI. RESULTS AND DISCUSSIONS A. Experimental Setup Models and datasets. We evaluate transformer-based LLMs and VLMs [2], [15], [22], [34], and also include Mamba-based SSMs [12] for algorithm benchmarking. For pruning, we use 256 samples from PILE [11]. Models are compared by perplexity (PPL) on WikiText2 [21] and accuracy on downstream tasks [3], [4], [6], [30], [41]. Algorithm implementation. We implement FLOW in PyTorch. All experiments are conducted using a single NVIDIA H100 GPU. The runtime for the complete process of FLOW is less than 25 minutes, even for the largest model in our evaluation (LLaMA3 70B). Algorithm baselines. We employ standard magnitude-based pruning as the naive baseline, and three SoTA pruning techniques, namely SparseGPT [9], Wanda [33], and OWL [38] to compare with FLOW. Accelerator implementation. The FlexCiM accelerator is imple- mented in Verilog RTL and synthesized, placed-and-routed using Synopsys Design Compiler and Cadence Innovus, respectively, using TSMC 28nm technology library. In our experiments, we observed the RTL implementation of DCiM with optimized relative placement of logic cells and memory, results in PPA numbers similar to the case where logic is embedded within the memory cells (hardening of the memory macro). We chose the RTL for ease of experimentation, flexibility, and fair comparison with prior work. FlexCiM achieves a peak clock frequency of 1 GHz. Following [32], [35], we employ a two-level memory hierarchy: a level-1 SRAM that feeds data directly to the accelerator and a level-2 global shared SRAM that loads data from an off-chip DRAM. We assume all model parameters fit within the global SRAM for simplicity [32]. We use CACTI to estimate the area and power of level-1 and level-2 SRAMs. For end-to-end performance and energy metrics, we design a cycle-accurate simulator based on DNNWeaver [29], following prior works [27]. Accelerator baselines. We compare FlexCiM against SoTA sparse digital accelerator VEGETA [14], 1:2 structured sparse DCiM-based accelerator (SDP) and dense systolic array and dense DCiM. To ensure a fair comparison, we implement configurations of all accel- erators so that they achieve iso-throughput. All digital accelerators are implemented with 64 64 array configuration to achieve the (a) (b) Fig. 6: (a) Accuracy ( ) comparison across 4 zero-shot tasks with 60 target N:M sparsity across different LLMs and VLMs; (b) Ablation study on the impact of different N:M patterns. 1 0.61 1.15 0.58 1 0.73 1.18 0.64 1 0.57 1.19 0.55 0 0.2 0.4 0.6 0.8 1 1.2 Dense SA DCiM VEGETA SDP FlexCiM Dense SA DCiM VEGETA SDP FlexCiM Dense SA DCiM VEGETA SDP FlexCiM LLaMA2-7B LLaMA3-70B LLaVa2-7B Normalized Latency Fig. 7: Normalized performance comparison between different dense and sparse accelerators for different models. same peak throughput as DCiM-based designs with 128 32 8 configuration. All designs are scaled to 28nm using DeepScale [28]. B. FLOW: Results and Analysis Perplexity benchmark. In Table I, we compare FLOW with the baselines for different target sparsity ratios. Magnitude, SparseGPT, and Wanda require static determination of the N:M pattern. We therefore assign 4:8 and 3:8 as the sparsity pattern for 50 and 60 target sparsity, respectively. Since OWL can explore different values of N for a fixed M, we fix M 8. Assigning M smaller than 8 for these methods results in poor performance. FLOW significantly outperforms the baselines, at all target sparsity ratios, achieving up to 18 perplexity improvement (PPL ) over OWL, SparseGPT and Wanda. The benefit of FLOW is evident at higher target sparsity ratios, demonstrating that the higher sparse representational freedom offered by FLOW is instrumental in achieving better performance. Furthermore, we compare the performance of a FLOW-generated model with that generated via OWL for unconstrained target sparsity, where the framework has the freedom to choose the best N:M pattern for a layer that least impacts model performance. Notably, not only does FLOW result in a higher compression (1.14times), but it also achieves a lower PPL than OWL across all evaluated models. Zero-shot performance benchmark. We evaluated the zero-shot performance of the pruned models in Figure 6(a) at a target sparsity ratio of 60 . In specific, we performed this evaluation with both LLMs and VLMs to study the generalization of FLOW across modalities. Notably, FLOW outperforms all other baselines across all tasks with an accuracy improvement of up to 36 . FLOW ablation. We evaluate the impact of different N:M patterns on the perplexity with a LLaMA3-8B model in Figure 6(b) for target sparsity of 50 . To demonstrate the impact of static N:M choices, we performed ablations with two N:M patterns, namely 2:4 and 4:8, where the weights are pruned based on their importance scores. We find that the model pruned with 2:4 sparsity yields a significant increase in perplexity over the dense baseline. 4:8, though it improves the performance, still falls significantly behind the baseline dense. However, enabling flexibility for N, i.e., N:4 {1:4, 2:4, and 4:4 (dense)} alleviates some of the perplexity degradation from a static N:M. More importantly, enabling flexibility for both N and M as proposed in FLOW i.e., N:M {1:2, 1:4, 2:4, 1:8, 2:8, 4:8, 8:8 (dense)}, we achieve the lowest perplexity that is closest to the dense baseline. This shows the non-uniformity of LLM layers to sparsity that can be catered to only with a flexible selection of N and M. TABLE II: Comparison of FlexCiM with baselines at 28nm . , in a column identify the presence and absence of a feature, respectively. Architecture Component (Area (mm2)) Total Area (mm2) Flexible N:M Flexibility Overhead Compute Density (Peak TOPS mm2) VEGETA [14] Input Output buffers (0.27) 3.28 15.4 2.36 Weight buffer (0.18) PE array (2.46) Sparsity support (0.38) SDP [35] Input Output buffers (0.27) 0.98 3.5 7.65 Weight buffer (0) DCiM array (0.68) Sparsity support (0.024) FlexCiM (Ours) Input Output buffers (0.27) 1.03 5.9 7.28 Weight buffer (0) DCiM array (0.72) Sparsity support (0.043) C. FlexCiM: Results and Analysis Area comparison. In Table II, we compare the accelerator area breakdown of VEGETA and SDP with FlexCiM. All designs have the same buffer configurations. SDP and FlexCiM have zero weight buffer area because these designs replace the weight memory and compute array of digital accelerators with DCiM macros. FlexCiM and SDP have significantly lower area than VEGETA due to the high memory density of DCiM-based designs. In specific, FlexCiM, despite supporting flexible N:M sparsity compared to SDP s fixed 1:2 sparsity, has a modest compute area overhead of 5.9 and similar compute density. Furthermore, FlexCiM s partitioned architecture reduces the long chain of column-wise adder trees in SDP by having multiple smaller adder trees across sub- macros. Performance comparison. In Figure 7, we conduct a performance comparison across different models pruned via FLOW. We report normalized latency numbers, normalized to a dense systolic array (SA) DCiM. Since we assume iso-peak-throughput accelerators, the dense SA and DCiM have same performance. Since SDP supports fixed 1:2 sparsity or dense, it has a higher latency than dense accelerators because FLOW assigns 1:2 pattern to very few layers, and in SDP, all other N:M patterns are treated as dense. FlexCiM achieves a significantly lower latency up to 1.72 than the baseline accelerators. This is due to the efficient acceleration of diverse N:M patterns and row- column-pipelining scheme. Energy comparison. In Figure 1, we compare the normalized energy comparison of different accelerators running various models pruned via FLOW. We report energy breakdown in terms of local memory and global memory access, where local memory access measures energy from the Level-1 SRAM for DCiM designs and weight activation buffer for digital accelerators. The global memory access measures the energy consumption of data access from the shared SRAM. Since off-chip memory access will be approximately similar for DCiM or digital accelerators, we do not explicitly highlight it. Following [32], we assume all model parameters fit in the global SRAM and consider DRAM access as out of scope for this work. FlexCiM has the lowest energy consumption across different models. Particularly, the DCiM-based architecture significantly reduces the energy contribution from local memory accesses. Furthermore, the bit-serial computation and efficient flexible N:M acceleration of FlexCiM also contribute to lower compute energy. FlexCiM has up to 1.5 lower energy consumption compared to all baselines across different models. VII. CONCLUSION We introduced a novel N:M sparsity selection method dubbed as FLOW. We identify that outlier distribution in a layer contribute to their non-uniformity and tolerances to different sparsity ratios. FLOW enables the identification of optimal N and M values for each layer from a diverse set by accounting for both the presence and distribution of outliers. To accelerate such pruned models, we then proposed a DCiM-based accelerator, FlexCiM. FlexCiM enables acceleration of diverse N:M patterns through a partitioned architecture, where the different DCiM macros are dynamically aggregated by a distribution and merging unit to support different N:M patterns. Extensive exper- iments show the efficacy of FLOW over existing alternatives with an accuracy improvement of up to 36 , while FlexCiM delivers up to 1.75 lower inference latency. ACKNOWLEDGMENTS This work was supported in part by CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. The authors would also like to thank the anonymous reviewers for their valuable feedback and suggestions, which helped improve the quality of this paper. REFERENCES [1] A. Agrawal, A. Agarwal, N. Kedia, J. Mohan, S. Kundu, N. Kwatra, R. Ramjee, and A. Tumanov, Metron: Holistic performance evaluation framework for llm inference systems, arXiv preprint:2407.07000, 2024. [2] A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre, S. Sagawa et al., Openflamingo: An open-source framework for training large autoregressive vision-language models, arXiv preprint arXiv:2308.01390, 2023. [3] Y. Bisk, R. Zellers, J. Gao, Y. Choi et al., Piqa: Reasoning about physical commonsense in natural language, in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 7432 7439. [4] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll ar, and C. L. Zitnick, Microsoft coco captions: Data collection and evaluation server, arXiv preprint arXiv:1504.00325, 2015. [5] Z. Chen, X. Chen, and J. Gu, 15.3 a 65nm 3t dynamic analog ram- based computing-in-memory macro and cnn accelerator with retention enhancement, adaptive analog sparsity and 44tops w system energy efficiency, in 2021 IEEE International Solid-State Circuits Conference (ISSCC), vol. 64. IEEE, 2021, pp. 240 242. [6] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, Boolq: Exploring the surprising difficulty of natural yes no questions, arXiv preprint arXiv:1905.10044, 2019. [7] C. Duan et al., Towards efficient sram-pim architecture de- sign by exploiting unstructured bit-level sparsity, arXiv preprint arXiv:2404.09497, 2024. [8] F. Farshchi, Q. Huang, and H. Yun, Integrating nvidia deep learning accelerator (nvdla) with risc-v soc on firesim, in 2019 2nd Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2). IEEE, 2019, pp. 21 25. [9] E. Frantar and D. Alistarh, SparseGPT: Massive language models can be accurately pruned in one-shot, 2023. [10] H. Fujiwara et al., A 5-nm 254-tops w 221-tops mm 2 fully-digital computing-in-memory macro supporting wide-range dynamic-voltage- frequency scaling and simultaneous mac and write operations, in 2022 IEEE International Solid-State Circuits Conference (ISSCC), vol. 65. IEEE, 2022, pp. 1 3. [11] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al., The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027, 2020. [12] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, arXiv preprint arXiv:2312.00752, 2023. [13] J.-W. Jang et al., Sparsity-aware and re-configurable npu architecture for samsung flagship mobile soc, in ISCA, 2021, pp. 15 28. [14] G. Jeong, S. Damani, A. R. Bambhaniya, E. Qin, C. J. Hughes, S. Subramoney, H. Kim, and T. Krishna, Vegeta: Vertically-integrated extensions for sparse dense gemm tile acceleration on cpus, in 2023 IEEE International Symposium on High-Performance Computer Archi- tecture (HPCA). IEEE, 2023, pp. 259 272. [15] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bam- ford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al., Mixtral of experts, arXiv preprint arXiv:2401.04088, 2024. [16] H. Kim et al., Colonnade: A reconfigurable sram-based digital bit- serial compute-in-memory macro for processing neural networks, IEEE Journal of Solid-State Circuits, vol. 56, no. 7, pp. 2221 2233, 2021. [17] D. Li, T. Yamasaki, A. Mani, A. T. Do, N. Chen, and B. Wang, Laxor: A bit-accurate bnn accelerator with latch-xor logic for local computing, in 2023 IEEE ACM International Symposium on Low Power Electronics and Design (ISLPED). IEEE, 2023, pp. 1 6. [18] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, Awq: Activation-aware weight quanti- zation for on-device llm compression and acceleration, Proceedings of Machine Learning and Systems, vol. 6, pp. 87 100, 2024. [19] Q. Liu, B. Gao, P. Yao, D. Wu, J. Chen, Y. Pang, W. Zhang, Y. Liao, C.-X. Xue, W.-H. Chen et al., 33.2 a fully integrated analog reram based 78.4 tops w compute-in-memory chip with fully parallel mac computing, in 2020 IEEE International Solid-State Circuits Conference- (ISSCC). IEEE, 2020, pp. 500 502. [20] Z.-G. Liu, P. N. Whatmough, Y. Zhu, and M. Mattina, S2ta: Exploiting structured sparsity for energy-efficient mobile cnn acceleration, in 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022, pp. 573 586. [21] S. Merity, N. S. Keskar, and R. Socher, An analysis of neural language modeling at multiple scales, arXiv preprint arXiv:1803.08240, 2018. [22] A. Meta, Introducing meta llama 3: The most capable openly available llm to date, Meta AI, 2024. [23] A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu, and P. Micikevicius, Accelerating sparse deep neural networks, arXiv preprint arXiv:2104.08378, 2021. [24] A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan, B. Khailany, J. Emer, S. W. Keckler, and W. J. Dally, Scnn: An accelerator for compressed-sparse convolutional neural networks, ACM SIGARCH computer architecture news, vol. 45, no. 2, pp. 27 40, 2017. [25] A. Raha, D. A. Mathaikutty, S. K. Ghosh, and S. Kundu, FlexNN: A dataflow-aware flexible deep learning accelerator for energy-efficient edge devices, arXiv preprint:2403.09026, 2024. [26] A. Ramachandran, S. Kundu, and T. Krishna, MicroScopiQ: Acceler- ating foundational models through outlier-aware microscaling quantiza- tion, arXiv preprint arXiv:2411.05282, 2024. [27] A. Ramachandran, Z. Wan, G. Jeong, J. Gustafson, and T. Krishna, Algorithm-hardware co-design of distribution-aware logarithmic-posit encodings for efficient dnn inference, arXiv:2403.05465, 2024. [28] S. Sarangi and B. Baas, Deepscaletool: A tool for the accurate esti- mation of technology scaling in the deep-submicron era, in 2021 IEEE ISCAS. IEEE, 2021, pp. 1 5. [29] H. Sharma, J. Park, E. Amaro, B. Thwaites, P. Kotha, A. Gupta, J. K. Kim, A. Mishra, and H. Esmaeilzadeh, Dnnweaver: From high-level deep network models to fpga acceleration, in the Workshop on Cognitive Architectures, 2016. [30] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, Towards vqa models that can read, in Proceedings of the IEEE CVF conference on computer vision and pattern recognition, 2019, pp. 8317 8326. [31] A. Sridharan, F. Zhang, J.-S. Seo, and D. Fan, Sp-imc: A sparsity aware in-memory-computing macro in 28nm cmos with configurable sparse representation for highly sparse dnn workloads, in 2024 IEEE Custom Integrated Circuits Conference (CICC). IEEE, 2024, pp. 1 2. [32] H. E. Sumbul, J.-s. Seo, D. H. Morris, and E. Beigne, A fully-digital and row-pipelined compute-in-memory neural network accelerator with soc-level benchmarking for ar vr applications, IEEE Micro, 2023. [33] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, A simple and effective pruning approach for large language models, ICLR, 2024. [34] H. Touvron et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint:2307.09288, 2023. [35] F. Tu, Y. Wang, L. Liang, Y. Ding, L. Liu, S. Wei, S. Yin, and Y. Xie, Sdp: Co-designing algorithm, dataflow, and architecture for in-sram sparse nn acceleration, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 42, no. 1, pp. 109 121, 2022. [36] Y. N. Wu, P.-A. Tsai, S. Muralidharan, A. Parashar, V. Sze, and J. Emer, Highlight: Efficient and flexible dnn acceleration with hierarchical structured sparsity, in Proceedings of the 56th Annual IEEE ACM International Symposium on Microarchitecture, 2023, pp. 1106 1120. [37] L. Yin, S. Liu, A. Jaiswal, S. Kundu, and Z. Wang, A task-centric angle of llm pre-trained weights through sparsity, ICML, 2024. [38] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, G. Li, A. Jaiswal, M. Pechenizkiy, Y. Liang et al., Outlier weighed layerwise sparsity: A missing secret sauce for pruning llms to high sparsity, ICML, 2024. [39] S. Yu, H. Jiang, S. Huang, X. Peng, and A. Lu, Compute-in-memory chips for deep learning: Recent trends and prospects, IEEE circuits and systems magazine, vol. 21, no. 3, pp. 31 56, 2021. [40] J. Yue et al., 15.2 a 2.75-to-75.9 tops w computing-in-memory nn processor supporting set-associate block-wise zero skipping and ping- pong cim with simultaneous computation and weight updating, in 2021 IEEE International Solid-State Circuits Conference (ISSCC), vol. 64. IEEE, 2021, pp. 238 240. [41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, Hel- laswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [42] Y. Zhan, W.-H. Yu, K.-F. Un, R. P. Martins, and P.-I. Mak, A 28-nm 18.7 tops mm2 89.4-to-234.6 tops w 8b single-finger edram compute- in-memory macro with bit-wise sparsity aware and kernel-wise weight update refresh, IEEE Journal of Solid-State Circuits, 2024. [43] Y. Zhang, L. Zhao, M. Lin, Y. Sun, Y. Yao, X. Han, J. Tanner, S. Liu, and R. Ji, Dynamic sparse no training: Training-free fine-tuning for sparse llms, arXiv preprint arXiv:2310.08915, 2023. [44] B. Zhong, M. Wang, C. Zhang, Y. Mai, X. Li, and Z. Yu, A digital sram computing-in-memory design utilizing activation unstructured sparsity for high-efficient dnn inference, in 2023 IEEE Computer Society Annual Symposium on VLSI (ISVLSI). IEEE, 2023, pp. 1 6.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nAccelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator Akshat Ramachandran 1, Souvik Kundu2, Arnab Raha3, Shamik Kundu3, Deepak K. Mathaikutty3, Tushar Krishna1 1Georgia Institute of Technology, USA, 2Intel Labs, USA, 3Intel Corporation, USA Corresponding email: Abstract Large language model (LLM) pruning with fixed N:M structured sparsity significantly limits the expressivity of the sparse model, yielding sub-optimal performance. On the contrary, support for more than one N:M pattern to provide sparse representational freedom yields a costly overhead in the hardware. To mitigate these challenges for LLMs, we first present a flexible layer-wise outlier-density-aware N:M sparsity (FLOW) selection method. FLOW enables the identification of optimal layer-wise N and M values (from a given range) by simultaneously accounting for the presence and distribution of outliers, allowing a higher degree of representational freedom. To deploy the sparse models with such N:M flexibility, we then present a flexible low overhead, digital compute- in-memory architecture (FlexCiM). FlexCiM enables support for diverse sparsity patterns by partitioning a digital CiM (DCiM) macro into smaller sub-macros which are adaptively aggregated and disaggregated through distribution and merging mechanisms for different values of N and M. Extensive experiments on both transformer-based and recurrence-based state space foundation models (SSMs) demonstrate FLOW to outperform existing alternatives with an accuracy improvement of up to 36 , while FlexCiM delivers up to 1.75 lower inference latency and 1.5 lower energy consumption compared to existing sparse accelerators. Code is available at: I. INTRODUCTION To reduce the colossal size of large language models (LLMs) and enable their efficient deployment on resource-constrained devices, post-training pruning has emerged as an effective model compression method [9], [33], [37]. It reduces the memory footprint of the pre- trained LLMs by removing ineffectual model parameters, at the granularity of individual weights (unstructured) or blocks of weights (structured), and storing sparse tensors in a compressed format (CSR CSC) [14].\n\n--- Segment 2 ---\nINTRODUCTION To reduce the colossal size of large language models (LLMs) and enable their efficient deployment on resource-constrained devices, post-training pruning has emerged as an effective model compression method [9], [33], [37]. It reduces the memory footprint of the pre- trained LLMs by removing ineffectual model parameters, at the granularity of individual weights (unstructured) or blocks of weights (structured), and storing sparse tensors in a compressed format (CSR CSC) [14]. Notably, model pruning may yield compute acceler- ation via skipping ineffectual computations associated with the zero- valued weight activation. However, traditional weight pruning often requires fine-tuning, which becomes exceedingly compute-heavy for LLMs. Furthermore, this often requires the model to yield structured pruned weights, which can cause a high accuracy drop compared to the models pruned via an unstructured approach. To mitigate the dilemma of accuracy vs. compute efficiency, structured N:M sparsity [13], [38] has emerged as a popular solution that can yield models with compute acceleration benefits while maintaining good accuracy. In specific, many of the existing commodity accelerators have enabled sparse N:M acceleration for fixed N:M values [23]. Therefore, this work focuses on N:M structured sparsity for efficient LLM inference. To enable post-training pruning and mitigate fine-tuning cost, SparseGPT [9] proposed to employ row-Hessian information to rank the importance of pre-trained weights and prune the least important ones. SparseGPT still required high compute owing to its iterative computation of second-order information (Hessian) for each layer. Wanda [33] avoided the computationally expensive second-order details by introducing a pruning metric that considers both the weight magnitudes and their corresponding input activations. Considering the performance benefits of structured N:M sparsity, both these works [9], [33] extended their approach to support a fixed and pre-determined Work done during an internship at Intel.\n\n--- Segment 3 ---\nWanda [33] avoided the computationally expensive second-order details by introducing a pruning metric that considers both the weight magnitudes and their corresponding input activations. Considering the performance benefits of structured N:M sparsity, both these works [9], [33] extended their approach to support a fixed and pre-determined Work done during an internship at Intel. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Dense SA DCiM VEGETA SDP FlexCiM Dense SA DCiM VEGETA SDP FlexCiM Dense SA DCiM VEGETA SDP FlexCiM LLaMA2-7B LLaMA3-70B LLaVa2-7B Normalized Energy Compute Local Memory Access Global Memory Access 0.79 0.73 0.63 0.61 0.81 0.72 0.78 0.69 0.52 Fig. 1: Normalized energy comparison between several digital and DCiM accelerators and the proposed FlexCiM across different models for flexible N:M inference acceleration. N:M pattern for all the layers. However, only recently [38] researchers have determined that due to the presence of a large number of outliers1 in LLMs, different layers are inherently heterogeneous and exhibit different tolerances to pruning. Based on this, outlier weighed layer-wise sparsity (OWL) [38] was proposed. OWL differs from prior techniques in its assignment of different sparsity ratios for different layers through allocations of different N values in the N:M pattern with fixed M. However, this is still sub-optimal and has limited representational freedom due to their static selection of M value across all the layers. Moreover, existing hardware accelerators cost significant overhead to employ different values of N and or M patterns in a single architecture, which was beyond the scope of [38] to address. It is well established that the decode stage of LLM inference is memory bound [1], [26], with weights loading constituting a sig- nificant bottleneck. Traditional Von-Neumann accelerators consume significant energy and time when moving weights from on-chip buffers to processing elements (PEs).\n\n--- Segment 4 ---\nIt is well established that the decode stage of LLM inference is memory bound [1], [26], with weights loading constituting a sig- nificant bottleneck. Traditional Von-Neumann accelerators consume significant energy and time when moving weights from on-chip buffers to processing elements (PEs). This makes existing traditional accelerators with flexible structured sparsity support [14], [36] poor candidates to accelerate N:M sparse LLMs for inference. An al- ternative computing paradigm, namely, digital compute-in-memory (DCiM) offers an efficient solution to the above challenge by moving the compute within the memory arrays. This mitigates the memory access bottleneck. However, despite the advantages of DCiM over traditional digital accelerators, there has been limited exploration of flexible structured sparsity support for DCiM-based architectures. This can be associated with the rigid crossbar structure of memory arrays and the infeasibility of transferring the flexible structured sparsity techniques employed for digital accelerators to DCiM ( III). Our contributions. We present two significant contributions toward addressing the above-mentioned algorithmic and hardware limita- tions. Specifically, we present a pruning algorithm that can assign different N:M sparsity with a higher degree of freedom and then present the first DCiM accelerator to support such patterns with minimal overhead, as summarized below. 1Scalars with larger magnitude compared to the rest of the tensor. arXiv:2504.14365v1 [cs.LG] 19 Apr 2025 We first explore the efficient and optimal assignment of the N:M sparsity pattern to each layer of an LLM. Specifically, we identify that compared to static N or M, having the luxury to choose both N and M allows us to yield better sparse representational freedom necessary to maintain improved model accuracy. To this end, we propose FLOW flexible layer-wise outlier-density-aware N:M spar- sity selection method. FLOW is based on a key insight that both the presence of outliers and their distribution in different layers con- tributes to the layer heterogeneity, impacting their tolerance towards model pruning. We leverage these two characteristics presence and distribution of outliers to assign diverse N:M values with the ability to determine the optimal N and M for a layer simultaneously.\n\n--- Segment 5 ---\nFLOW is based on a key insight that both the presence of outliers and their distribution in different layers con- tributes to the layer heterogeneity, impacting their tolerance towards model pruning. We leverage these two characteristics presence and distribution of outliers to assign diverse N:M values with the ability to determine the optimal N and M for a layer simultaneously. Towards deploying the flexible N:M sparse LLMs in DCiM, we first analyze its challenges in directly incorporating techniques employed by digital accelerators such as [14], [36], particularly the infeasibility of incorporating large multiplexers within each memory cell of DCiM due to the rigid crossbar structure of the memory array. We then introduce a novel DCiM-based accelerator with flex- ible structured sparsity support FlexCiM. It decomposes an existing DCiM macro into multiple partitions called sub-macros (partitioned along the row dimension) and introduces two new units distribution and merging units. These units coordinate the mapping of weights to each sub-macro and broadcasting of input activations (iActs) to support diverse N:M sparsity with minimal overheads. We conduct extensive experiments with foundation models based on both transformer and state space model (SSM) primitives. Specifically, experiments across various LLMs and VLMs demon- strate FLOW outperforming SoTA pruning techniques by achieving up to 36 better accuracy at high sparsity. Our accelerator, FlexCiM, on the other hand, can yield up to 1.75 lower inference latency with 1.5 lower energy (see Figure 1) compared to the SoTA sparse accelerators with a minimal area overhead of 6 . II. BACKGROUND AND RELATED WORKS LLM pruning with N:M Sparsity. N:M sparsity retains N non- zero elements within a fine-grained block of consecutive M elements. Existing LLM pruning works, including SparseGPT [9] and Wanda [33], when extended to structured N:M sparsity, were limited in their exploration to a fixed N:M pattern for all layers. More recently, [38] highlighted the importance of a heterogeneous sparsity budget for different LLM layers. Specifically, it proposed a non-uniform layer- wise N:M sparsity scheme with different N values for different layers determined based on the number of outliers in each layer.\n\n--- Segment 6 ---\nMore recently, [38] highlighted the importance of a heterogeneous sparsity budget for different LLM layers. Specifically, it proposed a non-uniform layer- wise N:M sparsity scheme with different N values for different layers determined based on the number of outliers in each layer. However, unlike existing methods [9], [33], [37], [38], [43], in this work, to yield more optimally pruned models, we present a higher degree of sparse representational freedom by identifying both the optimal N and M values of an N:M pattern. DCiM accelerators. Conventional digital accelerators [24], [26], [27] employ compute engines separate from memory (buffers) to handle LLM inference, often leading to memory access bottlenecks, increasing the memory access cost [32]. Furthermore, the decode stage of LLM inference is primarily memory-bounded, making weight activation loading a significant bottleneck [18]. In light of these, CiM architectures [10], [32], [39] have been propelled into the mainstream. In CiM, the processing elements and weight local memory (SRAM, ReRAM, DRAM, etc.) are merged into a single macro [16], [35], effectively reducing on-chip memory access, yield- ing accelerated computation. Early work on CiM focused on analog CiM architectures [5], [19], embedding analog multiply-accumulate (MAC) units within memory arrays. While analog CiM architectures demonstrate high energy efficiency, they have various limitations, including vulnerability to noise process variations, limited bit pre- cision, and significant ADC DAC overheads, collectively restricting 0.61 0.65 0.52 0.63 0.41 0.42 0.57 0.48 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 1 3 4 7 15 18 22 26 27 Normalized D Layer Index 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 BERT-large OPT-6.7B LLaMA2-13B LLaMA3-8B OF-9B VILA-7B Average Normalized D Model (a) (b) Fig.\n\n--- Segment 7 ---\nEarly work on CiM focused on analog CiM architectures [5], [19], embedding analog multiply-accumulate (MAC) units within memory arrays. While analog CiM architectures demonstrate high energy efficiency, they have various limitations, including vulnerability to noise process variations, limited bit pre- cision, and significant ADC DAC overheads, collectively restricting 0.61 0.65 0.52 0.63 0.41 0.42 0.57 0.48 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 1 3 4 7 15 18 22 26 27 Normalized D Layer Index 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 BERT-large OPT-6.7B LLaMA2-13B LLaMA3-8B OF-9B VILA-7B Average Normalized D Model (a) (b) Fig. 2: (a) Outlier distribution measured by pairwise L1 distance between outliers (normalized to BERT-large) across different models. (b) Intra-model variations in outlier distribution across different layers of a LLaMA3-8B model. their scalable adoption. Recently, DCiM architectures have gained considerable traction, with various industry accelerators [10], [32], [35] adopting these macros to embed digital MAC within memory directly. This eliminates the need for costly DACs ADCs and offers enhanced robustness against variation and noise and sufficient preci- sion to achieve high efficiency and accurate inference. Additionally, DCiM architectures are inherently compatible with existing digital design flows, making them suitable candidates for LLM acceleration. Accelerators supporting sparsity. Among digital accelerators, the NVIDIA sparse tensor core [23] exploited fixed 2:4 sparsity of weights in DNNs. VEGETA [14] presented flexible N:M sparsity support for weights, and S2TA [20] extended support for the same for both weights and activations, albeit with significant overheads. [36] proposed hierarchically structured sparsity to represent diverse sparsity patterns in DNNs. Flexible architectures like [25] can poten- tially accelerate any N:M ratio exploiting unstructured sparsity, yet often suffer from unbalanced critical path delay.\n\n--- Segment 8 ---\n[36] proposed hierarchically structured sparsity to represent diverse sparsity patterns in DNNs. Flexible architectures like [25] can poten- tially accelerate any N:M ratio exploiting unstructured sparsity, yet often suffer from unbalanced critical path delay. However, support for flexible structured sparsity in DCiM accelerators has been largely unexplored. The majority of sparse DCiM accelerators have either focused on bit-level sparsity [7], [42] or unstructured sparsity [31], [44]. Apart from these, a few works [35], [40] introduced fixed 1:2 sparsity support in DCiMs. However, challenges and potential for deploying flexible N:M sparsity in DCiM are yet to be unveiled. III. MOTIVATION A. Distribution of Outliers in LLMs OWL [38] presented a non-uniform layer-wise N:M sparsity with a fixed M and a different N for different layers with N being proportional to the layer outlier ratio. That is, layers with a high outlier ratio would have a high N value. However, varying N with a fixed M value, results in suboptimal sparse representation freedom in each layer. We argue that a predetermined M value when assigning different N:M sparsity patterns ignores another key metric in LLM weights, namely the outlier distribution (D). For a tensor l, we compute Dl by a proxy metric that measures the summation of the pairwise distance between the outliers averaged over the number of outliers. In Eq. 1, dist(.) measures the pair-wise L1-distance between two outliers, where nC2 is the total number of outlier pairs in a layer with n outliers. We then compute the normalized outlier distribution (ND) through the min-max normalization across all the layers as shown in Eq. 1. NDl (Dl min(D)) max(D) , where Dl PnC2 1 dist(ok, oj) nC2 (1) where D represents the set of Ds for all the layers. To understand the variance of ND over different models, in Figure 2(a) we plot the average ND averaged over all the layers for different models (including VLMs).\n\n--- Segment 9 ---\nNDl (Dl min(D)) max(D) , where Dl PnC2 1 dist(ok, oj) nC2 (1) where D represents the set of Ds for all the layers. To understand the variance of ND over different models, in Figure 2(a) we plot the average ND averaged over all the layers for different models (including VLMs). In specific, we observe that models like BERT, OPT have a relatively high average ND, indicating that outliers are sparsely distributed within these models. On the contrary, models like LLaMA3-8B and OpenFlamingo-9B possess a significantly low value of average ND, implying that outliers are largely clustered together. Case 1 Outlier Presence: Low Outlier Distribution: Sparse Optimal N:M 1:4 Case 2 Outlier Presence: High Outlier Distribution: Sparse Optimal N:M 2:4 Case 3 Outlier Presence: High Outlier Distribution: Clustered Adjacent Optimal N:M 4:8 Inlier Value Outlier Value LEGEND Fig. 3: Example of efficient N:M assignment based on outlier presence and distribution in different situations. Furthermore, even within the same model, different layers depict varied normalized outlier distributions. Figure 2(b) demonstrates the diverse ND values for different layers of a LLaMA3-8B. We hypothesize that while choosing the N:M sparsity patterns to different layers, it is important to choose the optimal M value for each layer. If a layer has largely clustered outliers (for example: layer 15 of LLaMA3-8B), the pruning algorithm should favor a larger M value when assigning an N:M sparsity pattern. This will help to (a) minimize the possibility of pruning outliers within a block of M and (b) have a higher degree of flexibility in deciding which features to prune. We demonstrate the usefulness of such N:M selection in yielding a better perplexity score in Table I compared to fixed M. B. Challenges of Flexible Structured Sparsity in DCiMs Sparse digital accelerators [14], [23] have significant multiplexer overhead within each PE to support flexible N:M sparsity. However, this strategy is not directly applicable to DCiM-based accelerators due to various reasons: Area overhead.\n\n--- Segment 10 ---\nChallenges of Flexible Structured Sparsity in DCiMs Sparse digital accelerators [14], [23] have significant multiplexer overhead within each PE to support flexible N:M sparsity. However, this strategy is not directly applicable to DCiM-based accelerators due to various reasons: Area overhead. The key benefit of DCiM architectures is their high memory density [32], [35]. Typical memory cells in DCiM employ 6T or 12T SRAM cells [10]. However, a single 8:1 multiplexer for flexible N:8 support requires over 42 transistors [35] increasing the area overhead by more than 3 and undermining the compactness and efficiency of the DCiM architectures. Rigid crossbar structure. Digital accelerators using weight- stationary dataflow stream up to M activations into each PE [14], enabling PE-level selection. In contrast, DCiM architectures are restricted by the two-bit line (BL and BL) structure of SRAM cells, allowing only two activations to be streamed into a DCiM macro. Bit-serial processing. DCiM architectures employ bit-serial oper- ations to maximize energy efficiency, where each SRAM bit-cell stores a single weight bit and computes partial products with serially streamed activation bits. Von-Neumann accelerators, by contrast, use parallel multipliers adders and store metadata alongside weights in each PE to manage activation selection. For N:8 sparsity, each N non-zero element requires 3 bits of metadata. In DCiMs, this would necessitate more metadata bits per SRAM cell than weight bits, complicating implementation. These challenges highlight the need for a novel approach to enable flexible N:M sparsity tailored for DCiMs. IV. FLOW N:M SPARSITY In this section, we first formalize outlier identification and then describe FLOW s methodology for optimal N:M assignment. A. Outlier Identification In this work, we target N:M sparsity for weights [38] and focus our discussion on weight outliers. Let us assume an LLM weight tensor W R(Cin Cout) with input activations X R(T K Cin). Here, T, K are the number of tokens and sequence length, respectively.\n\n--- Segment 11 ---\nLet us assume an LLM weight tensor W R(Cin Cout) with input activations X R(T K Cin). Here, T, K are the number of tokens and sequence length, respectively. Inspired by prior works [33], we assign an importance score to each weight element Wij, determined as IW Wij Xj 2 with Xj 2 being the L2 norm of the input activations connected to the weight element. IW acts as a measure to identify the outliers based on both weight and associated activation values [33]. Upon obtaining this score, we calculate the mean (µ) and standard deviation (σ) of the weight importance scores for each layer [38]. Subsequently, we categorize all weights exceeding the threshold τ σ as outliers and the rest as inliers. We empirically identify τ 3 or τ 5 provide the best results. B. FLOW Methodology For an L-layer LLM, with a target sparsity ratio of S, the layer- wise sparsity ratios are assigned and handled by two lists N [N 1, N 2, ..., N L] and M [M 1, M 2, ..., M L]. For a layer l, the sparsity pattern is N l:M l, the sparsity ratio is Sl (M l N l) (M l) and S (Pl L l 1 Sl) L. Following the process outlined in IV-A, we obtain the outlier fraction in each layer to get O [O1, O2, ..., OL], where Ol identifies the normalized outlier fraction in a layer l [38]. Intuitively, if a layer has a larger Ol, the number of non-zeros in a block of M l elements should be higher (i.e., N l ). In specific, N l Ol. In Figure 3, examples with higher presence of outliers have larger values of N. FLOW takes the normalized outlier distribution of each layer (NDl) into account to identify the optimal M l. In specific, for each layer weight tensor, we partition it into B non-overlapping blocks of dimension (128 128). Within a block b, we measure Dl b by calculating the pairwise L1 distance between all the outlier pairs following Equation 1.\n\n--- Segment 12 ---\nIn Figure 3, examples with higher presence of outliers have larger values of N. FLOW takes the normalized outlier distribution of each layer (NDl) into account to identify the optimal M l. In specific, for each layer weight tensor, we partition it into B non-overlapping blocks of dimension (128 128). Within a block b, we measure Dl b by calculating the pairwise L1 distance between all the outlier pairs following Equation 1. We obtain Dl as PB b 1 Dl b, and finally get NDl via normalizing the Dl values following Equation 1. A higher NDl indicates a farther distance between outliers within a layer, implying that they are sparsely distributed (Case 1 in Figure 3), whereas smaller NDl indicates a large number of closely located outliers (Case 3 in Figure 3). Intuitively, if a layer has many closely located outliers, a higher value of M l is preferred, providing more freedom while performing weight pruning and preventing inadvertent pruning of outliers. In specific, M l (1 NDl). Automated layer-wise N, M allocation. Given the relations of N l, M l, we formulate the problem of assigning layer-wise N:M values as an integer linear programming (ILP) problem: argmin L X l 1 α N l k Ol β M l h (1 NDl) (2) s.t. 1 L L X l 1 Sl S, 0 Sl 1, l {1, . . . , L} 1 N l M l, N l {1, 2, 4, 8}, M l {2, 4, 8} where α, β are the weighing factors to optimize N and M values, respectively. k and h are the hyper-parameters to keep the terms Ol and NDl in the same ballpark range as N l and M l choices, respectively. We empirically choose their values to be (α, β, k, h) (1, 4, 8, 8). This formulation aims to assign values to N l and M l for each layer while minimizing deviations from desired characteristics, all while maintaining the target sparsity (Equation 2). N l is aligned with the normalized outlier count Ol, and M l is aligned with the outlier distribution (1 NDl).\n\n--- Segment 13 ---\nThis formulation aims to assign values to N l and M l for each layer while minimizing deviations from desired characteristics, all while maintaining the target sparsity (Equation 2). N l is aligned with the normalized outlier count Ol, and M l is aligned with the outlier distribution (1 NDl). It is important to note that FLOW can be employed to explore any combination of N and M. However, to ensure hardware efficiency, we restrict N and M choices to powers of two and a maximum value of 8 for this work (see N, M choice set mentioned in Equation 2). V. FLEXCIM ACCELERATOR Overview. Figure 4(a) provides an overview of the FlexCiM architec- ture. FlexCiM extends an existing X Y 8 [35] SRAM-based DCiM bit-cell macro supporting fixed 1:2 sparsity, where (X, Y ) represent the crossbar array dimensions with the memory word size being 8-bits. FlexCiM can be employed to accelerate any layer that can be represented as a GEMM [14] operation. It partitions the existing iAct Serializer WL Driver (b) 8b iAct[15:0] Row 0 Row 1 Row X P-2 Row X P-1 Adder Tree To PSum Buffer 8b 9b 2b 1b 17b Distribution Unit Merging Unit PSum Buffer PSum Buffer PSum Buffer PSum Buffer Sub-Macro 0 Sub-Macro 2 Sub-Macro 3 Y (a) Sub-Macro 1 FlexCiM Global Controller iAct[1:0] 2b 1b 1b 1b WL WL WL BL (c) Column controller Fig. 4: (a) FlexCiM overview with a partition size P 4; (b) Organization of a single column of a FlexCiM sub-macro; (c) Memory cell structure. macro into P components (named as sub-macros), each of dimension X P Y 8. To seamlessly support diverse N:M sparsity patterns with this architecture, we introduce two novel hardware components, namely the distribution and the merging units to orchestrate the working of the P sub-macros.\n\n--- Segment 14 ---\nmacro into P components (named as sub-macros), each of dimension X P Y 8. To seamlessly support diverse N:M sparsity patterns with this architecture, we introduce two novel hardware components, namely the distribution and the merging units to orchestrate the working of the P sub-macros. To enable the bit-serial computation, a local input activation (iAct) buffer feeds the activations to the distribution unit (detailed later) at a rate of X 8-bit activations per cycle, which is serialized via the iAct serializer unit in each sub- macro [10], [32]. We employ a two-tier control unit comprising a global controller and column-wise controller to generate control signals that manage pipelining across rows and columns of sub- macros and configure multiplexer select signals. In our demonstrated implementation, we assume X 128,Y 32 for the DCiM bit-cell macro, with partition size of P 4, resulting in an 8Kb sub-macro of dimensions 32 32 8. Each sub-macro column is divided into 32 banks to enable parallel MAC operation. A. Sub-Macro Architecture Memory cell. Figure 4(c), presents a memory word in FlexCiM, with each word composed of 8 bit-cells. To abstract the circuit complexity and demonstrate our proposal to be orthogonal to any SRAM technology, we follow [17] and implement a 28nm, standard cell, latch-based memory structurally resembling a 6T SRAM cell [35]. Each memory cell has two bit-lines BL, BL, shared by all cells in a row, and a word-line (WL), shared per column. All memory cells in a column share a control signal to enable the compute units, namely EN COL. Inspired by [35], we implement a 2:1 multiplexer that is shared by a memory word to select one of the two iActs (via control signal (isel) streamed along the two bit-lines to enable 1:2 structured sparsity as a baseline case. A bit-serial multiplier within each SRAM cell (implemented as NOR gate) performs the multiplication between the selected iAct and stored weight. The memory cell has two modes of operation: memory (MS) and compute (C) mode. Standard read and write operations are performed in MS mode.\n\n--- Segment 15 ---\nThe memory cell has two modes of operation: memory (MS) and compute (C) mode. Standard read and write operations are performed in MS mode. The WL is activated to access all cells in a column, with BLs used to read write data. C mode is enabled for all cells in a column only when WL 0 and the corresponding EN COL 1, allowing the compute units to perform MAC operations. Memory column. Figure 4(b) illustrates the organization of a column in a sub-macro. Each column comprises 32 rows of 8-bit memory words and a column-wise adder tree. Each memory word performs bit-wise multiplication with the streamed iAct bits (MSB first), which are then accumulated column-wise by a 32-input adder tree and directed to the partial sum (PSum) accumulator. Partial sum accumulator. Since DCiMs typically perform bit-serial arithmetic, the MACs from each column-wise adder tree must be bit shifted based on the bit significance of the streamed iAct and subse- quently accumulated to compute the final partial sum. The complete operation can be expressed as, PSumfinal Pi 7 i 0 2i P(iAct[i] W). Column-wise controller. Each sub-macro possesses a controller that is dedicated to generating the isel signal for all the 2:1 multiplexers in each column and enabling column pipelining (see V-C) via the EN COL signal. B. Hardware Extensions for Flexible N:M Sparsity N:M sparsity storage format. In FlexCiM all sparse weight tensors are encoded in compressed sparse column (CSC) format [14]. Each column is stored as a list consisting of all the nonzero values, and the coordinates of the N non-zero values within a block of size M are stored as the corresponding metadata (see Figure 5). We introduce two key hardware blocks, the distribution unit and merging unit, that enable flexible N:M structured sparsity together. Distribution unit. The distribution unit is responsible for efficiently feeding iActs to the sub-macros based on the N and M values. As depicted in Figure 4, it is composed of P P : 1 multiplexers that serve each of the P sub-macros (P 4 in our implementation) with a bit- width of 16 bits (packing two 8-bit iActs) per input line.\n\n--- Segment 16 ---\nThe distribution unit is responsible for efficiently feeding iActs to the sub-macros based on the N and M values. As depicted in Figure 4, it is composed of P P : 1 multiplexers that serve each of the P sub-macros (P 4 in our implementation) with a bit- width of 16 bits (packing two 8-bit iActs) per input line. A distribution unit is shared by spatially identical rows of all sub-macros. For example, row 0 of all P sub-macros shares one distribution unit. The M value indicates the number of iActs to be distributed by each multiplexer in the distribution unit. The N value of a sparsity pattern identifies the number of sub-macros that are aggregated together to process the same block M. In particular, for N 1, multiple multiplexers will have the same set of inputs. Still, the distribution unit will select the appropriate input line based on the metadata (see V-D for a demonstration). The distribution unit abstracts the complexity of supporting large multiplexers in the memory cell by efficiently selecting the correct set of two activations to be fed to the memory array. We demonstrate in V-C that by efficiently performing row and column pipelining, we require only 32 distribution units serving all rows of a column of all sub-macros every clock cycle. Merging unit. The merging unit is responsible for accumulating the partial sums that are individually computed by each column of each sub-macro to generate the final partial sum in our partitioned scheme. To enable this, the merging unit comprises a P-input adder tree that reads its inputs from the partial sum accumulator s buffer. C. Pipelining Scheme Following on-chip buffers employed in comparable real-world systems [8], [32], we design the local iAct buffer with a bandwidth of 1024 bits cycle (128 8-bit activations). For dense LLM inference, 128 iActs are streamed to a column, enabling parallel MAC operations by all rows in a column of all sub-macros. However, for sparse inference with the highest sparsity i.e., 1:8 in our implementation, each row requires 8 iActs to choose from, which translates to 1024 iActs required by each column. Considering the upper limit of memory bandwidth, during 1:8 inference, only 4 rows of a column can perform parallel MAC operations.\n\n--- Segment 17 ---\nHowever, for sparse inference with the highest sparsity i.e., 1:8 in our implementation, each row requires 8 iActs to choose from, which translates to 1024 iActs required by each column. Considering the upper limit of memory bandwidth, during 1:8 inference, only 4 rows of a column can perform parallel MAC operations. Similarly, 8 rows for 1:4 and 16 rows for 1:2. Due to the bit-serial nature of DCiMs, a MAC operation takes 8 cycles. Therefore, we overlap computation with memory access via row-pipelining. The number of row pipeline stages ( stages) is equal to 32 ( of rows grouped). For example, for the 1:8 case, each set of 4 rows in a column activated via EN COL are fed with iActs in a single cycle, and the remaining 28 rows are fed iActs in the subsequent 8 clock cycles in sets of 4, resulting in stages 8. Every stages cycle, the EN COL of the adjacent column is activated, and similarly, iActs are fed to all rows in the column. Once all columns have been fed with iActs, EN COL of the first column is reactivated, and the process is repeated.\n\n--- Segment 18 ---\nEvery stages cycle, the EN COL of the adjacent column is activated, and similarly, iActs are fed to all rows in the column. Once all columns have been fed with iActs, EN COL of the first column is reactivated, and the process is repeated. We perform 1:4 Sparsity Pattern 1 iActs1x32 2 3 30 31 32 WT2x32 1 0 0 0 0 2 0 0 0 0 0 16 15 0 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 0 2 2 3 1 2 2 0 1 2 3 3 1 0 1 1 Weights encoded in CSC format 1 2 9 10 3 4 11 12 7 8 15 16 5 6 13 14 1 2 3 4 1 2 1 2 0 5 6 7 8 7 8 1 7 8 9 10 1112 9 10 0 1314 1516 1516 0 23 24 9 10 25 26 15 16 29 30 17 18 0 1 0 1 1 1 0 1 EN_COL 0 1 2 9 10 3 4 11 12 7 8 15 16 5 6 13 14 1 2 7 8 23 24 9 10 25 26 15 16 29 30 17 18 0 1 0 1 1 1 0 1 EN_COL 1 Row 0 Distribution Unit 3 4 0 19 20 0 7 8 1 23 24 1 11 12 0 25 26 1 13 14 0 29 30 1 Global Controller 0 2 2 3 2 2 0 1 2 1 1 2 3 4 5 4:8 Sparsity Pattern 1 iActs1x16 2 3 14 15 16 WT2x16 1 9 2 10 3 0 0 0 0 0 4 11 0 0 0 12 1 9 2 10 3 11 4 12 5 13 6 14 7 15 8 16 0 0 1 1 2 5 5 7 1 2 3 3 4 5 5 7 Weights encoded in CSC format 1 9 5 13 2 10 6 14 4 12 8 16 3 11 7 15 1 2 3 4 1 2 1 2 0 1 2 0 1 2 3 4 1 5 6 2 11 12 3 4 13 14 5 6 13 14 9 10 0 1 1 1 0 0 1 1 EN_COL 0 EN_COL 1 Row 0 Distribution Unit Global Controller 0 0 1 1 5 5 7 2 2 1 1 2 3 4 5 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 Sub-Macro 0 Sub-Macro 1 Sub-Macro 2 Sub-Macro 3 Sub-Macro 0 Sub-Macro 1 Sub-Macro 2 Sub-Macro 3 1 9 5 13 2 10 6 14 4 12 8 16 3 11 7 15 1 2 1 2 11 12 3 4 13 14 5 6 13 14 9 10 0 1 1 1 0 0 1 1 4 1 2 0 11 0 1 2 1 11 12 1 5 6 0 13 1 7 8 0 15 16 1 12 14 Metadata Metadata 1.63x lower latency than dense inference !\n\n--- Segment 19 ---\nOnce all columns have been fed with iActs, EN COL of the first column is reactivated, and the process is repeated. We perform 1:4 Sparsity Pattern 1 iActs1x32 2 3 30 31 32 WT2x32 1 0 0 0 0 2 0 0 0 0 0 16 15 0 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 0 2 2 3 1 2 2 0 1 2 3 3 1 0 1 1 Weights encoded in CSC format 1 2 9 10 3 4 11 12 7 8 15 16 5 6 13 14 1 2 3 4 1 2 1 2 0 5 6 7 8 7 8 1 7 8 9 10 1112 9 10 0 1314 1516 1516 0 23 24 9 10 25 26 15 16 29 30 17 18 0 1 0 1 1 1 0 1 EN_COL 0 1 2 9 10 3 4 11 12 7 8 15 16 5 6 13 14 1 2 7 8 23 24 9 10 25 26 15 16 29 30 17 18 0 1 0 1 1 1 0 1 EN_COL 1 Row 0 Distribution Unit 3 4 0 19 20 0 7 8 1 23 24 1 11 12 0 25 26 1 13 14 0 29 30 1 Global Controller 0 2 2 3 2 2 0 1 2 1 1 2 3 4 5 4:8 Sparsity Pattern 1 iActs1x16 2 3 14 15 16 WT2x16 1 9 2 10 3 0 0 0 0 0 4 11 0 0 0 12 1 9 2 10 3 11 4 12 5 13 6 14 7 15 8 16 0 0 1 1 2 5 5 7 1 2 3 3 4 5 5 7 Weights encoded in CSC format 1 9 5 13 2 10 6 14 4 12 8 16 3 11 7 15 1 2 3 4 1 2 1 2 0 1 2 0 1 2 3 4 1 5 6 2 11 12 3 4 13 14 5 6 13 14 9 10 0 1 1 1 0 0 1 1 EN_COL 0 EN_COL 1 Row 0 Distribution Unit Global Controller 0 0 1 1 5 5 7 2 2 1 1 2 3 4 5 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 Sub-Macro 0 Sub-Macro 1 Sub-Macro 2 Sub-Macro 3 Sub-Macro 0 Sub-Macro 1 Sub-Macro 2 Sub-Macro 3 1 9 5 13 2 10 6 14 4 12 8 16 3 11 7 15 1 2 1 2 11 12 3 4 13 14 5 6 13 14 9 10 0 1 1 1 0 0 1 1 4 1 2 0 11 0 1 2 1 11 12 1 5 6 0 13 1 7 8 0 15 16 1 12 14 Metadata Metadata 1.63x lower latency than dense inference ! 1.42x lower latency than dense inference !\n\n--- Segment 20 ---\nWe perform 1:4 Sparsity Pattern 1 iActs1x32 2 3 30 31 32 WT2x32 1 0 0 0 0 2 0 0 0 0 0 16 15 0 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 0 2 2 3 1 2 2 0 1 2 3 3 1 0 1 1 Weights encoded in CSC format 1 2 9 10 3 4 11 12 7 8 15 16 5 6 13 14 1 2 3 4 1 2 1 2 0 5 6 7 8 7 8 1 7 8 9 10 1112 9 10 0 1314 1516 1516 0 23 24 9 10 25 26 15 16 29 30 17 18 0 1 0 1 1 1 0 1 EN_COL 0 1 2 9 10 3 4 11 12 7 8 15 16 5 6 13 14 1 2 7 8 23 24 9 10 25 26 15 16 29 30 17 18 0 1 0 1 1 1 0 1 EN_COL 1 Row 0 Distribution Unit 3 4 0 19 20 0 7 8 1 23 24 1 11 12 0 25 26 1 13 14 0 29 30 1 Global Controller 0 2 2 3 2 2 0 1 2 1 1 2 3 4 5 4:8 Sparsity Pattern 1 iActs1x16 2 3 14 15 16 WT2x16 1 9 2 10 3 0 0 0 0 0 4 11 0 0 0 12 1 9 2 10 3 11 4 12 5 13 6 14 7 15 8 16 0 0 1 1 2 5 5 7 1 2 3 3 4 5 5 7 Weights encoded in CSC format 1 9 5 13 2 10 6 14 4 12 8 16 3 11 7 15 1 2 3 4 1 2 1 2 0 1 2 0 1 2 3 4 1 5 6 2 11 12 3 4 13 14 5 6 13 14 9 10 0 1 1 1 0 0 1 1 EN_COL 0 EN_COL 1 Row 0 Distribution Unit Global Controller 0 0 1 1 5 5 7 2 2 1 1 2 3 4 5 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 Sub-Macro 0 Sub-Macro 1 Sub-Macro 2 Sub-Macro 3 Sub-Macro 0 Sub-Macro 1 Sub-Macro 2 Sub-Macro 3 1 9 5 13 2 10 6 14 4 12 8 16 3 11 7 15 1 2 1 2 11 12 3 4 13 14 5 6 13 14 9 10 0 1 1 1 0 0 1 1 4 1 2 0 11 0 1 2 1 11 12 1 5 6 0 13 1 7 8 0 15 16 1 12 14 Metadata Metadata 1.63x lower latency than dense inference ! 1.42x lower latency than dense inference ! Fig.\n\n--- Segment 21 ---\n1.42x lower latency than dense inference ! Fig. 5: Example of FlexCiM running 1:4 and 4:8 sparsity patterns. column-pipelining to reduce the overhead of the distribution unit because, without column pipelining, each column would require a dedicated distribution unit, which would be prohibitively expensive for memory-centric designs. D. Walkthrough Example To demonstrate FlexCiM enabling inference with diverse N:M patterns, we explain through two simple examples in Figure 5. The N of an N:M pattern identifies the number of sub-macros that work together to process a block of size M. Similarly, M specifies the number of iActs handled by each multiplexer in the distribution unit. For the 4:8 pattern, each set of four non-zero values are mapped to spatially identical locations in four sub-macros. For the 1:4 pattern, each sub-macro works independently on a block of size M, with each multiplexer receiving a separate set of 4 iActs. Note that the same distribution units employed for iActs can be used to map the weights to each of the sub-macros. 1 The global controller receives the CSC metadata (in green) of the mapped weights and generates corresponding select signals to control the multiplexers. The LSB of the metadata is always reserved for the 2:1 multiplexer in the sub-macro. The remaining bits are employed for the multiplexers in the distribution unit. For example, for M 8, the metadata requires 3 bits, where the top 2 bits are for the distribution unit multiplexers. 2 The distribution unit for each row (only row 0 TABLE I: WikiText2 perplexity of pruning methods for different LLM model families at different target sparsity ratios.\n\n--- Segment 22 ---\nFor example, for M 8, the metadata requires 3 bits, where the top 2 bits are for the distribution unit multiplexers. 2 The distribution unit for each row (only row 0 TABLE I: WikiText2 perplexity of pruning methods for different LLM model families at different target sparsity ratios. LLaMA-2 [34] LLaMA-3 [22] Mixtral [15] Mamba [12] Method N:M 7B 13B 70B 8B 70B 8x7B 130M 1.4B Baseline Dense 5.47 4.83 3.31 6.13 2.85 3.84 20.04 10.42 Target Sparsity: 50 Magnitude 4:8 16.87 9.59 6.83 9.16 5.84 7.91 58.63 43.24 SparseGPT [9] 4:8 8.59 7.18 5.02 8.51 4.34 7.15 29.67 14.98 Wanda [33] 4:8 8.01 6.55 4.68 8.07 4.15 6.98 40.36 21.36 OWL [38] Mixed N:8 7.92 6.08 4.55 8.02 3.95 6.57 27.28 14.07 FLOW (Ours) Mixed N:M 7.04 5.73 3.97 7.03 3.46 5.96 24.48 12.96 Target Sparsity: 60 Magnitude 3:8 54.59 48.96 49.51 52.38 57.63 56.76 1e20 1e14 SparseGPT [9] 3:8 44.68 42.69 39.57 45.09 35.27 41.37 54.83 31.57 Wanda [33] 3:8 43.26 40.24 38.97 44.93 33.78 39.86 1268 1107 OWL [38] Mixed N:8 20.68 18.68 18.57 16.26 11.23 14.58 31.56 18.67 FLOW (Ours) Mixed N:M 18.96 16.53 17.95 15.24 10.89 13.83 28.61 15.37 Target Sparsity: Unconstrained OWL [38] Mixed N:8 (51.3 ) 7.98 6.25 4.87 8.36 4.09 6.87 27.35 14.03 FLOW (Ours) Mixed N:M (58.5 ) 7.13 5.95 4.57 8.03 3.87 6.35 25.12 13.08 unit shown in the figure) in column 0 selects the appropriate input line and directs it to the corresponding sub-macros.\n\n--- Segment 23 ---\n2 The distribution unit for each row (only row 0 TABLE I: WikiText2 perplexity of pruning methods for different LLM model families at different target sparsity ratios. LLaMA-2 [34] LLaMA-3 [22] Mixtral [15] Mamba [12] Method N:M 7B 13B 70B 8B 70B 8x7B 130M 1.4B Baseline Dense 5.47 4.83 3.31 6.13 2.85 3.84 20.04 10.42 Target Sparsity: 50 Magnitude 4:8 16.87 9.59 6.83 9.16 5.84 7.91 58.63 43.24 SparseGPT [9] 4:8 8.59 7.18 5.02 8.51 4.34 7.15 29.67 14.98 Wanda [33] 4:8 8.01 6.55 4.68 8.07 4.15 6.98 40.36 21.36 OWL [38] Mixed N:8 7.92 6.08 4.55 8.02 3.95 6.57 27.28 14.07 FLOW (Ours) Mixed N:M 7.04 5.73 3.97 7.03 3.46 5.96 24.48 12.96 Target Sparsity: 60 Magnitude 3:8 54.59 48.96 49.51 52.38 57.63 56.76 1e20 1e14 SparseGPT [9] 3:8 44.68 42.69 39.57 45.09 35.27 41.37 54.83 31.57 Wanda [33] 3:8 43.26 40.24 38.97 44.93 33.78 39.86 1268 1107 OWL [38] Mixed N:8 20.68 18.68 18.57 16.26 11.23 14.58 31.56 18.67 FLOW (Ours) Mixed N:M 18.96 16.53 17.95 15.24 10.89 13.83 28.61 15.37 Target Sparsity: Unconstrained OWL [38] Mixed N:8 (51.3 ) 7.98 6.25 4.87 8.36 4.09 6.87 27.35 14.03 FLOW (Ours) Mixed N:M (58.5 ) 7.13 5.95 4.57 8.03 3.87 6.35 25.12 13.08 unit shown in the figure) in column 0 selects the appropriate input line and directs it to the corresponding sub-macros. 3 The first column of each of the sub-macros receives the selected iActs.\n\n--- Segment 24 ---\nLLaMA-2 [34] LLaMA-3 [22] Mixtral [15] Mamba [12] Method N:M 7B 13B 70B 8B 70B 8x7B 130M 1.4B Baseline Dense 5.47 4.83 3.31 6.13 2.85 3.84 20.04 10.42 Target Sparsity: 50 Magnitude 4:8 16.87 9.59 6.83 9.16 5.84 7.91 58.63 43.24 SparseGPT [9] 4:8 8.59 7.18 5.02 8.51 4.34 7.15 29.67 14.98 Wanda [33] 4:8 8.01 6.55 4.68 8.07 4.15 6.98 40.36 21.36 OWL [38] Mixed N:8 7.92 6.08 4.55 8.02 3.95 6.57 27.28 14.07 FLOW (Ours) Mixed N:M 7.04 5.73 3.97 7.03 3.46 5.96 24.48 12.96 Target Sparsity: 60 Magnitude 3:8 54.59 48.96 49.51 52.38 57.63 56.76 1e20 1e14 SparseGPT [9] 3:8 44.68 42.69 39.57 45.09 35.27 41.37 54.83 31.57 Wanda [33] 3:8 43.26 40.24 38.97 44.93 33.78 39.86 1268 1107 OWL [38] Mixed N:8 20.68 18.68 18.57 16.26 11.23 14.58 31.56 18.67 FLOW (Ours) Mixed N:M 18.96 16.53 17.95 15.24 10.89 13.83 28.61 15.37 Target Sparsity: Unconstrained OWL [38] Mixed N:8 (51.3 ) 7.98 6.25 4.87 8.36 4.09 6.87 27.35 14.03 FLOW (Ours) Mixed N:M (58.5 ) 7.13 5.95 4.57 8.03 3.87 6.35 25.12 13.08 unit shown in the figure) in column 0 selects the appropriate input line and directs it to the corresponding sub-macros. 3 The first column of each of the sub-macros receives the selected iActs. These are then serialized and sent along the bit-lines.\n\n--- Segment 25 ---\n3 The first column of each of the sub-macros receives the selected iActs. These are then serialized and sent along the bit-lines. 4 The 2:1 multiplexer selects the appropriate iAct (highlighted in dark blue) from the bit-lines to be multiplied with the stored weights. 5 After stages cycles, column 1 receives the iActs and the process is repeated. The merging unit performs final partial sum accumulation by reading from the PSum buffers of each column as they are calculated. Notably, in the 1:2 base case and dense inference, no selection occurs in the distribution units. Furthermore, for dense inference, the same iActs are streamed along both bit-lines, and Isel is a don t-care. VI. RESULTS AND DISCUSSIONS A. Experimental Setup Models and datasets. We evaluate transformer-based LLMs and VLMs [2], [15], [22], [34], and also include Mamba-based SSMs [12] for algorithm benchmarking. For pruning, we use 256 samples from PILE [11]. Models are compared by perplexity (PPL) on WikiText2 [21] and accuracy on downstream tasks [3], [4], [6], [30], [41]. Algorithm implementation. We implement FLOW in PyTorch. All experiments are conducted using a single NVIDIA H100 GPU. The runtime for the complete process of FLOW is less than 25 minutes, even for the largest model in our evaluation (LLaMA3 70B). Algorithm baselines. We employ standard magnitude-based pruning as the naive baseline, and three SoTA pruning techniques, namely SparseGPT [9], Wanda [33], and OWL [38] to compare with FLOW. Accelerator implementation. The FlexCiM accelerator is imple- mented in Verilog RTL and synthesized, placed-and-routed using Synopsys Design Compiler and Cadence Innovus, respectively, using TSMC 28nm technology library. In our experiments, we observed the RTL implementation of DCiM with optimized relative placement of logic cells and memory, results in PPA numbers similar to the case where logic is embedded within the memory cells (hardening of the memory macro). We chose the RTL for ease of experimentation, flexibility, and fair comparison with prior work.\n\n--- Segment 26 ---\nIn our experiments, we observed the RTL implementation of DCiM with optimized relative placement of logic cells and memory, results in PPA numbers similar to the case where logic is embedded within the memory cells (hardening of the memory macro). We chose the RTL for ease of experimentation, flexibility, and fair comparison with prior work. FlexCiM achieves a peak clock frequency of 1 GHz. Following [32], [35], we employ a two-level memory hierarchy: a level-1 SRAM that feeds data directly to the accelerator and a level-2 global shared SRAM that loads data from an off-chip DRAM. We assume all model parameters fit within the global SRAM for simplicity [32]. We use CACTI to estimate the area and power of level-1 and level-2 SRAMs. For end-to-end performance and energy metrics, we design a cycle-accurate simulator based on DNNWeaver [29], following prior works [27]. Accelerator baselines. We compare FlexCiM against SoTA sparse digital accelerator VEGETA [14], 1:2 structured sparse DCiM-based accelerator (SDP) and dense systolic array and dense DCiM. To ensure a fair comparison, we implement configurations of all accel- erators so that they achieve iso-throughput. All digital accelerators are implemented with 64 64 array configuration to achieve the (a) (b) Fig. 6: (a) Accuracy ( ) comparison across 4 zero-shot tasks with 60 target N:M sparsity across different LLMs and VLMs; (b) Ablation study on the impact of different N:M patterns. 1 0.61 1.15 0.58 1 0.73 1.18 0.64 1 0.57 1.19 0.55 0 0.2 0.4 0.6 0.8 1 1.2 Dense SA DCiM VEGETA SDP FlexCiM Dense SA DCiM VEGETA SDP FlexCiM Dense SA DCiM VEGETA SDP FlexCiM LLaMA2-7B LLaMA3-70B LLaVa2-7B Normalized Latency Fig. 7: Normalized performance comparison between different dense and sparse accelerators for different models. same peak throughput as DCiM-based designs with 128 32 8 configuration.\n\n--- Segment 27 ---\n7: Normalized performance comparison between different dense and sparse accelerators for different models. same peak throughput as DCiM-based designs with 128 32 8 configuration. All designs are scaled to 28nm using DeepScale [28]. B. FLOW: Results and Analysis Perplexity benchmark. In Table I, we compare FLOW with the baselines for different target sparsity ratios. Magnitude, SparseGPT, and Wanda require static determination of the N:M pattern. We therefore assign 4:8 and 3:8 as the sparsity pattern for 50 and 60 target sparsity, respectively. Since OWL can explore different values of N for a fixed M, we fix M 8. Assigning M smaller than 8 for these methods results in poor performance. FLOW significantly outperforms the baselines, at all target sparsity ratios, achieving up to 18 perplexity improvement (PPL ) over OWL, SparseGPT and Wanda. The benefit of FLOW is evident at higher target sparsity ratios, demonstrating that the higher sparse representational freedom offered by FLOW is instrumental in achieving better performance. Furthermore, we compare the performance of a FLOW-generated model with that generated via OWL for unconstrained target sparsity, where the framework has the freedom to choose the best N:M pattern for a layer that least impacts model performance. Notably, not only does FLOW result in a higher compression (1.14times), but it also achieves a lower PPL than OWL across all evaluated models. Zero-shot performance benchmark. We evaluated the zero-shot performance of the pruned models in Figure 6(a) at a target sparsity ratio of 60 . In specific, we performed this evaluation with both LLMs and VLMs to study the generalization of FLOW across modalities. Notably, FLOW outperforms all other baselines across all tasks with an accuracy improvement of up to 36 . FLOW ablation. We evaluate the impact of different N:M patterns on the perplexity with a LLaMA3-8B model in Figure 6(b) for target sparsity of 50 . To demonstrate the impact of static N:M choices, we performed ablations with two N:M patterns, namely 2:4 and 4:8, where the weights are pruned based on their importance scores.\n\n--- Segment 28 ---\nWe evaluate the impact of different N:M patterns on the perplexity with a LLaMA3-8B model in Figure 6(b) for target sparsity of 50 . To demonstrate the impact of static N:M choices, we performed ablations with two N:M patterns, namely 2:4 and 4:8, where the weights are pruned based on their importance scores. We find that the model pruned with 2:4 sparsity yields a significant increase in perplexity over the dense baseline. 4:8, though it improves the performance, still falls significantly behind the baseline dense. However, enabling flexibility for N, i.e., N:4 {1:4, 2:4, and 4:4 (dense)} alleviates some of the perplexity degradation from a static N:M. More importantly, enabling flexibility for both N and M as proposed in FLOW i.e., N:M {1:2, 1:4, 2:4, 1:8, 2:8, 4:8, 8:8 (dense)}, we achieve the lowest perplexity that is closest to the dense baseline. This shows the non-uniformity of LLM layers to sparsity that can be catered to only with a flexible selection of N and M. TABLE II: Comparison of FlexCiM with baselines at 28nm . , in a column identify the presence and absence of a feature, respectively. Architecture Component (Area (mm2)) Total Area (mm2) Flexible N:M Flexibility Overhead Compute Density (Peak TOPS mm2) VEGETA [14] Input Output buffers (0.27) 3.28 15.4 2.36 Weight buffer (0.18) PE array (2.46) Sparsity support (0.38) SDP [35] Input Output buffers (0.27) 0.98 3.5 7.65 Weight buffer (0) DCiM array (0.68) Sparsity support (0.024) FlexCiM (Ours) Input Output buffers (0.27) 1.03 5.9 7.28 Weight buffer (0) DCiM array (0.72) Sparsity support (0.043) C. FlexCiM: Results and Analysis Area comparison.\n\n--- Segment 29 ---\n, in a column identify the presence and absence of a feature, respectively. Architecture Component (Area (mm2)) Total Area (mm2) Flexible N:M Flexibility Overhead Compute Density (Peak TOPS mm2) VEGETA [14] Input Output buffers (0.27) 3.28 15.4 2.36 Weight buffer (0.18) PE array (2.46) Sparsity support (0.38) SDP [35] Input Output buffers (0.27) 0.98 3.5 7.65 Weight buffer (0) DCiM array (0.68) Sparsity support (0.024) FlexCiM (Ours) Input Output buffers (0.27) 1.03 5.9 7.28 Weight buffer (0) DCiM array (0.72) Sparsity support (0.043) C. FlexCiM: Results and Analysis Area comparison. In Table II, we compare the accelerator area breakdown of VEGETA and SDP with FlexCiM. All designs have the same buffer configurations. SDP and FlexCiM have zero weight buffer area because these designs replace the weight memory and compute array of digital accelerators with DCiM macros. FlexCiM and SDP have significantly lower area than VEGETA due to the high memory density of DCiM-based designs. In specific, FlexCiM, despite supporting flexible N:M sparsity compared to SDP s fixed 1:2 sparsity, has a modest compute area overhead of 5.9 and similar compute density. Furthermore, FlexCiM s partitioned architecture reduces the long chain of column-wise adder trees in SDP by having multiple smaller adder trees across sub- macros. Performance comparison. In Figure 7, we conduct a performance comparison across different models pruned via FLOW. We report normalized latency numbers, normalized to a dense systolic array (SA) DCiM. Since we assume iso-peak-throughput accelerators, the dense SA and DCiM have same performance. Since SDP supports fixed 1:2 sparsity or dense, it has a higher latency than dense accelerators because FLOW assigns 1:2 pattern to very few layers, and in SDP, all other N:M patterns are treated as dense.\n\n--- Segment 30 ---\nSince we assume iso-peak-throughput accelerators, the dense SA and DCiM have same performance. Since SDP supports fixed 1:2 sparsity or dense, it has a higher latency than dense accelerators because FLOW assigns 1:2 pattern to very few layers, and in SDP, all other N:M patterns are treated as dense. FlexCiM achieves a significantly lower latency up to 1.72 than the baseline accelerators. This is due to the efficient acceleration of diverse N:M patterns and row- column-pipelining scheme. Energy comparison. In Figure 1, we compare the normalized energy comparison of different accelerators running various models pruned via FLOW. We report energy breakdown in terms of local memory and global memory access, where local memory access measures energy from the Level-1 SRAM for DCiM designs and weight activation buffer for digital accelerators. The global memory access measures the energy consumption of data access from the shared SRAM. Since off-chip memory access will be approximately similar for DCiM or digital accelerators, we do not explicitly highlight it. Following [32], we assume all model parameters fit in the global SRAM and consider DRAM access as out of scope for this work. FlexCiM has the lowest energy consumption across different models. Particularly, the DCiM-based architecture significantly reduces the energy contribution from local memory accesses. Furthermore, the bit-serial computation and efficient flexible N:M acceleration of FlexCiM also contribute to lower compute energy. FlexCiM has up to 1.5 lower energy consumption compared to all baselines across different models. VII. CONCLUSION We introduced a novel N:M sparsity selection method dubbed as FLOW. We identify that outlier distribution in a layer contribute to their non-uniformity and tolerances to different sparsity ratios. FLOW enables the identification of optimal N and M values for each layer from a diverse set by accounting for both the presence and distribution of outliers. To accelerate such pruned models, we then proposed a DCiM-based accelerator, FlexCiM. FlexCiM enables acceleration of diverse N:M patterns through a partitioned architecture, where the different DCiM macros are dynamically aggregated by a distribution and merging unit to support different N:M patterns.\n\n--- Segment 31 ---\nTo accelerate such pruned models, we then proposed a DCiM-based accelerator, FlexCiM. FlexCiM enables acceleration of diverse N:M patterns through a partitioned architecture, where the different DCiM macros are dynamically aggregated by a distribution and merging unit to support different N:M patterns. Extensive exper- iments show the efficacy of FLOW over existing alternatives with an accuracy improvement of up to 36 , while FlexCiM delivers up to 1.75 lower inference latency. ACKNOWLEDGMENTS This work was supported in part by CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. The authors would also like to thank the anonymous reviewers for their valuable feedback and suggestions, which helped improve the quality of this paper. REFERENCES [1] A. Agrawal, A. Agarwal, N. Kedia, J. Mohan, S. Kundu, N. Kwatra, R. Ramjee, and A. Tumanov, Metron: Holistic performance evaluation framework for llm inference systems, arXiv preprint:2407.07000, 2024. [2] A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Gadre, S. Sagawa et al., Openflamingo: An open-source framework for training large autoregressive vision-language models, arXiv preprint arXiv:2308.01390, 2023. [3] Y. Bisk, R. Zellers, J. Gao, Y. Choi et al., Piqa: Reasoning about physical commonsense in natural language, in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 05, 2020, pp. 7432 7439. [4] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll ar, and C. L. Zitnick, Microsoft coco captions: Data collection and evaluation server, arXiv preprint arXiv:1504.00325, 2015.\n\n--- Segment 32 ---\n[4] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll ar, and C. L. Zitnick, Microsoft coco captions: Data collection and evaluation server, arXiv preprint arXiv:1504.00325, 2015. [5] Z. Chen, X. Chen, and J. Gu, 15.3 a 65nm 3t dynamic analog ram- based computing-in-memory macro and cnn accelerator with retention enhancement, adaptive analog sparsity and 44tops w system energy efficiency, in 2021 IEEE International Solid-State Circuits Conference (ISSCC), vol. 64. IEEE, 2021, pp. 240 242. [6] C. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova, Boolq: Exploring the surprising difficulty of natural yes no questions, arXiv preprint arXiv:1905.10044, 2019. [7] C. Duan et al., Towards efficient sram-pim architecture de- sign by exploiting unstructured bit-level sparsity, arXiv preprint arXiv:2404.09497, 2024. [8] F. Farshchi, Q. Huang, and H. Yun, Integrating nvidia deep learning accelerator (nvdla) with risc-v soc on firesim, in 2019 2nd Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2). IEEE, 2019, pp. 21 25. [9] E. Frantar and D. Alistarh, SparseGPT: Massive language models can be accurately pruned in one-shot, 2023. [10] H. Fujiwara et al., A 5-nm 254-tops w 221-tops mm 2 fully-digital computing-in-memory macro supporting wide-range dynamic-voltage- frequency scaling and simultaneous mac and write operations, in 2022 IEEE International Solid-State Circuits Conference (ISSCC), vol. 65. IEEE, 2022, pp. 1 3.\n\n--- Segment 33 ---\nIEEE, 2022, pp. 1 3. [11] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al., The pile: An 800gb dataset of diverse text for language modeling, arXiv preprint arXiv:2101.00027, 2020. [12] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, arXiv preprint arXiv:2312.00752, 2023. [13] J.-W. Jang et al., Sparsity-aware and re-configurable npu architecture for samsung flagship mobile soc, in ISCA, 2021, pp. 15 28. [14] G. Jeong, S. Damani, A. R. Bambhaniya, E. Qin, C. J. Hughes, S. Subramoney, H. Kim, and T. Krishna, Vegeta: Vertically-integrated extensions for sparse dense gemm tile acceleration on cpus, in 2023 IEEE International Symposium on High-Performance Computer Archi- tecture (HPCA). IEEE, 2023, pp. 259 272. [15] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bam- ford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand et al., Mixtral of experts, arXiv preprint arXiv:2401.04088, 2024. [16] H. Kim et al., Colonnade: A reconfigurable sram-based digital bit- serial compute-in-memory macro for processing neural networks, IEEE Journal of Solid-State Circuits, vol. 56, no. 7, pp. 2221 2233, 2021.\n\n--- Segment 34 ---\n7, pp. 2221 2233, 2021. [17] D. Li, T. Yamasaki, A. Mani, A. T. Do, N. Chen, and B. Wang, Laxor: A bit-accurate bnn accelerator with latch-xor logic for local computing, in 2023 IEEE ACM International Symposium on Low Power Electronics and Design (ISLPED). IEEE, 2023, pp. 1 6. [18] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang, C. Gan, and S. Han, Awq: Activation-aware weight quanti- zation for on-device llm compression and acceleration, Proceedings of Machine Learning and Systems, vol. 6, pp. 87 100, 2024. [19] Q. Liu, B. Gao, P. Yao, D. Wu, J. Chen, Y. Pang, W. Zhang, Y. Liao, C.-X. Xue, W.-H. Chen et al., 33.2 a fully integrated analog reram based 78.4 tops w compute-in-memory chip with fully parallel mac computing, in 2020 IEEE International Solid-State Circuits Conference- (ISSCC). IEEE, 2020, pp. 500 502. [20] Z.-G. Liu, P. N. Whatmough, Y. Zhu, and M. Mattina, S2ta: Exploiting structured sparsity for energy-efficient mobile cnn acceleration, in 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022, pp. 573 586. [21] S. Merity, N. S. Keskar, and R. Socher, An analysis of neural language modeling at multiple scales, arXiv preprint arXiv:1803.08240, 2018. [22] A. Meta, Introducing meta llama 3: The most capable openly available llm to date, Meta AI, 2024. [23] A. Mishra, J. A. Latorre, J.\n\n--- Segment 35 ---\n[23] A. Mishra, J. A. Latorre, J. Pool, D. Stosic, D. Stosic, G. Venkatesh, C. Yu, and P. Micikevicius, Accelerating sparse deep neural networks, arXiv preprint arXiv:2104.08378, 2021. [24] A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan, B. Khailany, J. Emer, S. W. Keckler, and W. J. Dally, Scnn: An accelerator for compressed-sparse convolutional neural networks, ACM SIGARCH computer architecture news, vol. 45, no. 2, pp. 27 40, 2017. [25] A. Raha, D. A. Mathaikutty, S. K. Ghosh, and S. Kundu, FlexNN: A dataflow-aware flexible deep learning accelerator for energy-efficient edge devices, arXiv preprint:2403.09026, 2024. [26] A. Ramachandran, S. Kundu, and T. Krishna, MicroScopiQ: Acceler- ating foundational models through outlier-aware microscaling quantiza- tion, arXiv preprint arXiv:2411.05282, 2024. [27] A. Ramachandran, Z. Wan, G. Jeong, J. Gustafson, and T. Krishna, Algorithm-hardware co-design of distribution-aware logarithmic-posit encodings for efficient dnn inference, arXiv:2403.05465, 2024. [28] S. Sarangi and B. Baas, Deepscaletool: A tool for the accurate esti- mation of technology scaling in the deep-submicron era, in 2021 IEEE ISCAS. IEEE, 2021, pp. 1 5. [29] H. Sharma, J.\n\n--- Segment 36 ---\n1 5. [29] H. Sharma, J. Park, E. Amaro, B. Thwaites, P. Kotha, A. Gupta, J. K. Kim, A. Mishra, and H. Esmaeilzadeh, Dnnweaver: From high-level deep network models to fpga acceleration, in the Workshop on Cognitive Architectures, 2016. [30] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, Towards vqa models that can read, in Proceedings of the IEEE CVF conference on computer vision and pattern recognition, 2019, pp. 8317 8326. [31] A. Sridharan, F. Zhang, J.-S. Seo, and D. Fan, Sp-imc: A sparsity aware in-memory-computing macro in 28nm cmos with configurable sparse representation for highly sparse dnn workloads, in 2024 IEEE Custom Integrated Circuits Conference (CICC). IEEE, 2024, pp. 1 2. [32] H. E. Sumbul, J.-s. Seo, D. H. Morris, and E. Beigne, A fully-digital and row-pipelined compute-in-memory neural network accelerator with soc-level benchmarking for ar vr applications, IEEE Micro, 2023. [33] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, A simple and effective pruning approach for large language models, ICLR, 2024. [34] H. Touvron et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint:2307.09288, 2023. [35] F. Tu, Y. Wang, L. Liang, Y. Ding, L. Liu, S. Wei, S. Yin, and Y. Xie, Sdp: Co-designing algorithm, dataflow, and architecture for in-sram sparse nn acceleration, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 42, no. 1, pp. 109 121, 2022. [36] Y. N. Wu, P.-A.\n\n--- Segment 37 ---\n109 121, 2022. [36] Y. N. Wu, P.-A. Tsai, S. Muralidharan, A. Parashar, V. Sze, and J. Emer, Highlight: Efficient and flexible dnn acceleration with hierarchical structured sparsity, in Proceedings of the 56th Annual IEEE ACM International Symposium on Microarchitecture, 2023, pp. 1106 1120. [37] L. Yin, S. Liu, A. Jaiswal, S. Kundu, and Z. Wang, A task-centric angle of llm pre-trained weights through sparsity, ICML, 2024. [38] L. Yin, Y. Wu, Z. Zhang, C.-Y. Hsieh, Y. Wang, Y. Jia, G. Li, A. Jaiswal, M. Pechenizkiy, Y. Liang et al., Outlier weighed layerwise sparsity: A missing secret sauce for pruning llms to high sparsity, ICML, 2024. [39] S. Yu, H. Jiang, S. Huang, X. Peng, and A. Lu, Compute-in-memory chips for deep learning: Recent trends and prospects, IEEE circuits and systems magazine, vol. 21, no. 3, pp. 31 56, 2021. [40] J. Yue et al., 15.2 a 2.75-to-75.9 tops w computing-in-memory nn processor supporting set-associate block-wise zero skipping and ping- pong cim with simultaneous computation and weight updating, in 2021 IEEE International Solid-State Circuits Conference (ISSCC), vol. 64. IEEE, 2021, pp. 238 240. [41] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, Hel- laswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [42] Y. Zhan, W.-H. Yu, K.-F. Un, R. P. Martins, and P.-I.\n\n--- Segment 38 ---\narXiv preprint arXiv:1905.07830, 2019. [42] Y. Zhan, W.-H. Yu, K.-F. Un, R. P. Martins, and P.-I. Mak, A 28-nm 18.7 tops mm2 89.4-to-234.6 tops w 8b single-finger edram compute- in-memory macro with bit-wise sparsity aware and kernel-wise weight update refresh, IEEE Journal of Solid-State Circuits, 2024. [43] Y. Zhang, L. Zhao, M. Lin, Y. Sun, Y. Yao, X. Han, J. Tanner, S. Liu, and R. Ji, Dynamic sparse no training: Training-free fine-tuning for sparse llms, arXiv preprint arXiv:2310.08915, 2023. [44] B. Zhong, M. Wang, C. Zhang, Y. Mai, X. Li, and Z. Yu, A digital sram computing-in-memory design utilizing activation unstructured sparsity for high-efficient dnn inference, in 2023 IEEE Computer Society Annual Symposium on VLSI (ISVLSI). IEEE, 2023, pp. 1 6.\n\n