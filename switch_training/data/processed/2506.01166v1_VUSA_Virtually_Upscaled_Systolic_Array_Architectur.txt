=== ORIGINAL PDF: 2506.01166v1_VUSA_Virtually_Upscaled_Systolic_Array_Architectur.pdf ===\n\nRaw text length: 32383 characters\nCleaned text length: 32002 characters\nNumber of segments: 19\n\n=== CLEANED TEXT ===\n\nVUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration Shereef Helal NXP Semiconductors Munich, Germany Alberto Garcia-Ortiz University of Bremen Bremen, Germany Lennart Bamberg NXP Semiconductors Hamburg, Germany Abstract Leveraging high degrees of unstructured sparsity is a promising approach to enhance the efficiency of deep neural network (DNN) accelerators particularly important for emerg- ing Edge-AI applications. We introduce VUSA, a systolic-array architecture that virtually grows based on the present sparsity to perform larger matrix multiplications with the same number of physical multiply-accumulate (MAC) units. The proposed architecture achieves saving by 37 and 68 in area and power efficiency, respectively, at the same peak-performance, compared to a baseline systolic array architecture in a commercial 16-nm technology. Still, the proposed architecture supports acceleration for any DNN with any sparsity even no sparsity at all. Thus, the proposed architecture is application-independent, making it viable for general-purpose AI acceleration. Index Terms Systolic Array, Unstructured Sparsity, AI Ac- celeration, Edge AI, Computer Architecture I. INTRODUCTION Over recent years, Artificial Intelligence (AI) has emerged as a revolutionary new technology, spreading across different industries and enhancing various aspects of our daily lives. The deployment of AI is not only confined to powerful data- center machines, but is increasingly demanded in resource- constrained embedded devices, a concept known as Edge AI. Deep Neural Network (DNN) architectures are the backbone of state-of-the-art AI applications to perform numerous tasks, such as image processing, speech recognition, natural language processing (NLP), and more [1]. However, DNNs have high computational demands, posing a significant challenge when deploying them in real-world applications. This challenge is even greater for Edge AI applications in resource-constrained environments. Domain-specific hardware accelerators are widely used for Edge AI applications to improve the efficiency of DNN infer- ence [2], [3], [4]. Due to its scalability and programmability, a systolic array is one of the most widely used architectures for DNN acceleration. The scalability and programmability come from a regular and simple structure that allows efficient matrix- by-matrix multiplications. Such matrix multiplications account for the largest fraction of the computational complexity of modern AI applications. However, the regularity of the systolic array also brings drawbacks such as poor utilization for large array dimensions (i.e., large number of multiply-accumulate (MAC) blocks) and no exploitation of unstructured sparsity. Weight sparsity refers to the number of zero-valued pa- rameters in a DNN. The computations associated with those parameters can be skipped without affecting the final results, thereby improving power efficiency, latency, and memory usage. Most accelerators can only exploit structured sparsity, which limits the amount of usable sparsity to 50 and has poor accuracy. In contrast, modern DNN applications can be trained to achieve much higher sparsity levels exceeding 90 without any loss in accuracy when sparsity is uncon- strained (i.e., unstructured) [5]. Thus, extending regular and scalable accelerator architectures, such as systolic arrays, to support this form of sparsity is a topic of great research interest in both academia [6] and industry [7], [8]. This work extends the standard weight-stationary systolic array architecture to exploit unstructured sparsity as much as possible while maintaining high efficiency for dense (i.e., non- sparse) execution. Specifically, we modify the architecture so that the systolic array can virtually expand to a larger size in the presence of weight sparsity. This expansion increases com- putational speed to match that of a more hardware-expensive array. Nevertheless, the simplicity of the hardware architecture and programmability is preserved, maintaining the key advan- tages of systolic arrays in terms of scalability and general- purpose applicability. Experimental results in a commercial 16-nm technology demonstrate that the proposed technique improves the area and power consumption by 37 and 68 , respectively, while maintaining the same peak performance. In the proposed architecture, the maximum amount of ex- ploitable weight sparsity is defined by an integer architectural parameter the maximum virtual growth. While the standard systolic array size is determined by the number of rows and columns, the proposed virtually upscaled systolic array (VUSA) is characterized by the number of rows, columns, and virtual columns. This approach preserves the generic and flexible nature of systolic arrays while expanding the design space to better exploit sparsity. To the best of the MOCAST 2025 Paper Preprint 2025 IEEE arXiv:2506.01166v1 [cs.AR] 1 Jun 2025 4 5 6 7 8 9 1 2 3 𝐼31 𝐼21 𝐼11 𝐼32 𝐼22 𝐼12 𝐼33 𝐼23 𝐼13 𝑤1 𝑤2 𝑤3 𝑤4 𝑤5 𝑤6 𝑤7 𝑤8 𝑤9 𝐼11 𝐼12 𝐼13 𝐼21 𝐼22 𝐼23 𝐼31 𝐼32 𝐼33 . 𝑤1 𝑤4 𝑤7 𝑤2 𝑤5 𝑤8 𝑤3 𝑤6 𝑤9 (1) (2) Fig. 1. Systolic array with weight-stationary data flow. authors knowledge, this is the first systolic array architecture that introduces a new dimensionality to effectively leverage unstructured weight sparsity in a weight-stationary systolic array without compromising scalability or programmability. The rest of the paper is structured as follows. Section II outlines the background. Subsequently, in Section III, we outline the architecture of the proposed VUSA. A quantitative, theoretical analysis of the expected gain of the proposed technique is presented in Section IV. Section V includes experimental results for a commercial 16-nm technology and real-life DNN applications. Finally, a comparison to related work and a conclusion are drawn. II. BACKGROUND A. Systolic Arrays The structure of a systolic array is shown in Figure 1. Systolic arrays are used to accelerate tensor operations by decomposing them into matrix-by-matrix multiplications (in combination with data arrangements by other components). The architecture is made up of multiple processing elements (PEs), performing MAC operations, between which data auto- matically flows from the left to the right and top to the bottom. Such a data-driven architecture heavily reduces the memory accesses per compute, enabling scaling to compute capabilities in the high TOPS regime [9]. Common data flows in systolic arrays are: Weight Stationary (WS), Input Stationary (IS), and Output Stationary (OS). The name of the dataflow indicates which parameter is stationary to the PEs during a processing sequence, while the other two are flowing from left to right and top to bottom [10]. This work focuses on WS dataflow as it enables to exploit unstructured weight sparsity while keeping the flow of data regular. An example WS matrix multiplication is illustrated in Figure 1. First, the weights are loaded into the PEs. The weights remain stationary to the respective PEs until the computations are done. Afterwards, the inputs are fed to the PEs from the left and multiplied by the weights to generate partial sums. Each processing element adds the partial sum to the accumulator value from the top input. To the right output the left input is forwarded as is, while to the bottom the updated accumulator value is passed. Multiplications of matrices larger than the systolic array size are decomposed into multiple smaller pieces that fit the array size. For this purpose, the resulting accumulator values (i.e., the results flowing out at the bottom) from the previous step are used as the starting accumulator values (i.e., feed to the array at the top). Matrices smaller than the systolic array size lead to an under utilization of the systolic array. B. Unstructured Weight Sparsity The term sparsity describes the high degree of zero in neural network activations data and weights. Pruning weights is the primary source of high sparsity; A technique that removes low-magnitude weights with the smallest impact on the model output. If pruning is done iteratively during training, the accuracy can be preserved or even enhanced (reduced over-fitting), while achieving sparsity factors above 90 [5]. However, dense accelerators such as standard systolic arrays cannot work with the compressed, pruned weights due to the absence of regular data flow. Thus, processing unstructured sparse weight tensors on such architectures, implies a large degree of zero-values in the weight matrices [3]. In this case, the effective operation of the large fraction of MAC blocks with a stationary zero-weight becomes a pass-through of both accumulator and input value (i.e., a NoOP). III. PROPOSED ARCHITECTURE Our goal is to extend the systolic-architecture to be capable of working with any weight matrix in which the total number of non-zero weights does not exceed the number of MAC units. In this case, each non-zero weight is loaded to one MAC, ignoring zero weights since they do not contribute to the final result. The challenge that this destroys the regularity of the data-flow of inputs and activations is addressed by extending the architecture as follows. A. Processing Sparse Weights in PEs The operation of a weight stationary PE is distinguished here in two different operating states: 1) Non-Sparse: stationary weight is non-zero: In this case, the full logic inside the PE depicted in Figure 2 is required for correct processing. 2) Sparse: stationary weight is zero: In this case, no data- path blocks are required. Only the pipeline stages for the cycle-by-cycle data-flow are required. Figure 2 high- lights in different colors the parts that can be removed for sparse processing (red) versus what must be kept to keep the data-flow regular (blue). B. Separating Processing from Data-Flow Elements Inspired by the fact that no compute element is actually needed when the weight is zero, we decompose the PE into two components: the MAC unit (i.e., processing components), and the pipeline stages (data-flow components). The resulting two blocks are shown in Figure 3. Sparse Processing Element (SPE): A block containing only pipeline registers, required for sparse as well as non- sparse processing MAC unit: A block that can be optionally connected to an SPE, required only for non-zero weights. left_in_reg top_in_reg 0 accumulat or_reg X left_in top_in bottom_out right_out Fig. 2. Example architecture of a PE. Parts marked in red can be removed when the stationary weight is zero. left_in_reg accumulat or_reg left_in acc_in acc_out right_out weight_reg X left_in weight_in weight_out acc_in acc_out SPE MAC Fig. 3. Decomposing the PE to a data-flow and a compute block. C. Virtual Upscaled Systolic Array (VUSA) As MAC units are not required for zero weights, they can be instantiated in a lower dimension than the maximum virtual systolic array size. Based on that, a sparse N M systolic array with N A MAC units is proposed, where N is the number of rows, M is the number of columns, and A is the number of MAC units per row. Thus, each row has M SPEs but only A number of MAC units where 1 A M. If a weight is zero, then only an SPE is sufficient in that position. Otherwise, a MAC unit is connected to the SPE in that position for the needed arithmetic computation. If the number of non-zero weights per row is less than or equal to the number of available MAC units per row (A), then the sparse systolic array virtually grows to an N M systolic array. Otherwise, a smaller window of weights is selected until the sparsity condition is satisfied, starting with an N (M 1) window, then N (M 2), and so on down to N A, at which the conditions are guaranteed to be satisfied. Thus, still any matrix-matrix multiplication can still be mapped even in the absence of sparsity. In detail, no sparsity just implies slower processing as we need to break down larger matrix-by-matrix multiplications into sub-blocks of size N A. A conceptual illustration of a single row of the VUSA is shown in Figure 4. Each MAC unit has a configurable connec- tion to multiple SPEs. Depending on the positions of non-zero weights in each row, the MAC units are connected to SPEs where computations involving non-zero weights are required. The connections to different MAC units can be dynamically configured such that changing positions of non-zero weights SPE_0 SPE_1 SPE_2 SPE_M-3 SPE_M-2 SPE_M-1 MAC_0 MAC_1 MAC_A-1 MAC_A-2 .. .. Fig. 4. Single row of the proposed VUSA architecture. SPE_0 SPE_1 SPE_2 SPE_3 SPE_5 MAC_0 MAC_1 MAC_2 SPE_4 Fig. 5. Row instance of the proposed VUSA for M 6 and A 3. can be handled per job. Still, an all-to-all connection between SPEs and MAC units is not needed. Each MAC unit only needs to be connected to one out of M A 1 directly adjacent SPEs to handle all potential zero-distributions. Let us label the SPEs with indexes i [0, ..., M 1] and the MACs with j [0, ..., A 1]. Consider now MAC number r, associated with SPE number t, MAC r can be mapped only to SPEs [r, ..., r M A]. In other words, an input-output shifter of up to M A positions per MAC in one direction is sufficient. Thereby, wiring is minimized which in turn reduces negative impacts on timing and power. In fact, experimental results for a commercial 16-nm technology (cf. Section V) show that even for M A the critical timing path and the power bottleneck is the MAC unit and not the MAC-to-SPE multiplexing. For example, applying this design to a sparse row with M 6 and A 3 requires to connect each MAC to four SPEs. This is illustrated in Figure 5. This proposed architecture can handle unstructured sparsity because the position on non-zero weights is not restricted. It achieves direct savings in resources by using the same number of MAC units to conduct matrix multiplications of a higher dimension. Since MAC units are not connected in positions where weights are zero, a natural multiplier gating [11], [12] for sparse weights is achieved. In this case, the gating applies not only to the multiplier but to the full MAC unit. The sparse row is the building block of VUSA which can be stacked vertically or horizontally to implement VUSAs with arbitrary dimensions. IV. THEORETICAL ANALYSIS In this section, we statistically assess the theoretical per- formance gains of the proposed architecture as a function of the sparsity. Truly unstructured sparsity implies that there is no correlation of sparsity between the individual weight positions. Thus, the sparsity probability of each weight is the same with no extra cross-correlation (i.e., inter-dependencies between the sparsity of different weights). In other words, the number of zero-terms follow a Binomial distribution. The maximum gain of the VUSA is achieved when the array dimension virtually grows to N M. Under the above assumption of no cross-correlation, the probability of this occurring depends only on the probability that a weight is non-sparse, denoted here as P1. For a VUSA, defined by tuple (N, M, A), the probability that the VUSA virtually grows to a standard N M systolic array, Psparse standard, can be derived as follows: 1) The gain probability of a single row (Pgain row) is computed which is the probability that the number of non-zero weights in the set of M is less than or equal to the number of available MAC units per row, A. This can be calculated by summing up the probabilities that the number of non-zero weights is exactly one of the numbers in the range [0:A]. The probability of getting exactly i number of non-zero weights is denoted as Pi,ones. Based on that, Pgain row can be calculated as: Pgain row A X i 0 Pi,ones (1) 2) The gain of the VUSA is achieved when the sparsity conditions are met for all rows. Hence, Psparse standard can be derived as: Psparse standard (Pgain row)N A X i 0 Pi,ones !N (2) 3) Pi,ones can be calculated as: Pi,ones M i P1 i(1 P1)M i (3) 4) From Equations 2 and 3, the formula for Psparse standard becomes: Psparse standard A X i 0 M i P1 i(1 P1)M i !N (4) If desired, a simpler analytical expression can be drawn using the regularized incomplete beta function, the CDF of the binomial, but the explicit result as a sum is more intuitive. Analogously, one can express the probability to have a virtual growth in the array size to another M , smaller than the theoretical maximum M defined by the hardware config- uration, but larger than A. The probability to map a given problem to a N A array (no virtual extension) is 1, as this is always possible, independent of the weights. By applying the derived mathematical model to a VUSA with N 3, M 6, and A 3, the probability to be able to virtually grow to 3 6, 3 5, 3 4 is calculated based on the sparsity rate, P0 1 P1. The results are shown in Figure 6. It can be observed that when sparsity is higher than 90 , the probability to virtually grow to 3 6 (i.e., 2 speed-up) is close to 1. Even for sparsity rates as low as 60 , the success rate for the maximum gain is above 50 . Even at dramatically low sparsity of around 30 , the success rate to grow to a 3 4 array (i.e., 4 3 speed-up) is still above 50 . Only in the theoretical worst case of no exploitable sparsity (3 3 window), there is no performance gain. On a real full AI application, composed of many different matrix multiplication, the VUSA performs 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 P(sparse standard) 𝑃0 3x6 3x5 3x4 3x3 Fig. 6. Growth probability of VUSA with N 3, M 6, and A 3 TABLE I SYNTHESIS RESULTS IN A COMMERCIAL 16-NM TECHNOLOGY AT 1 GHz Design MACs Area (Normalized) Power (Normalized) Standard 3 3 9 0.69 0.86 Standard 3 4 12 0.91 1.15 Standard 3 5 15 1.14 1.41 Standard 3 6 18 1.37 1.68 VUSA 3 6 9 1 1 with a combination of all virtual growth possibilities based on the weight sparsity of the current computation. This behavior is analyzed by means of experimental results next. V. EXPERIMENTAL RESULTS In this section, a VUSA with N 3, A 3, M 6 (i.e., 9 MAC units that can virtually grow to 18) is evaluated against varying systolic array sizes for real benchmark applications. The standard systolic array dimensions to be investigated are: 3 3, 3 4, 3 5, and 3 6. A. Physical-Design The different architectures are physical-aware (to consider interconnects) synthesized in a commercial 16-nm technology at the same target frequency of 1GHz. Table I shows the resulting silicon area and power consumption for all designs normalized to the values of the proposed VUSA architecture. The area footprints of standard 3 6 and 3 5 systolic arrays are 37 and 14 , larger than the proposed VUSA architecture, respectively. On the other end, the areas of standard 3 4 and 3 3 arrays are 9 and 31 smaller, respectively. In terms of power consumption, a standard 3 6, 3 5, and 3 4 array are 68 , 41 , and 15 , more costly than the proposed solution, respectively. A standard 3 3 array has a 14 lower power consumption. Overall, there is a clear advantage in both area and power for the 3 6 and 3 5 cases. For the 3 4 case, there are gains in power but with a small area overhead. Compared to a standard 3 3 array, there are area and power overheads. Thus, to be an efficient solution on a combined power-performance- area metric, our proposed technique must dominantly work at least as 3 4 virtual systolic array for a given application. 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 3x6: 100 3x6: 50 , 3x3: 50 Fig. 7. Example 50 sparsity distribution B. Effect of Sparsity on Performance The number of zero weights and their positions in the different layers of a given application determines the number of jobs to be executed on the VUSA structure. If the weights of a given load are almost all zeros, the VUSA virtually grows to a 3 6 array, implying a significant gain over the standard architecture. If on the other hand all weights are non-zero, then the VUSA acts only as a 3 3 and thus performs worse than a standard systolic array due to the power and area overhead outlined above. Not only the number of zeros but also their distribution is relevant. Consider a load with 50 sparsity, as depicted in Figure 7. If the zeros are distributed approximately evenly, then the VUSA can process the entire problem with a 3 6 virtual array. On the other hand, if the zeros are correlated in their positions, then the VUSA processes only 50 of the jobs with a 3 6 window, while the other 50 is processed with a 3 3 window. On a real application with unstructured sparsity, a combination of all cases typically occurs. C. Methodology to Analyze Full DNN Models The evaluation of full applications is performed in two steps: first, a cycle-accurate simulation is performed using SCALE- Sim [10] to obtain the number of required cycles to run DNN models on standard systolic arrays of dimensions from 3 3 to 3 6. The second step is obtaining pre-trained models and using the values of the pre-trained weights to determine the sparsity conditions of the weights (number of zeros and their distribution). Based on the weight filters, the number of different windows of sizes from 3 3 to 3 6 required by the VUSA is determined, which can be used to calculate the percentages of the VUSA performing as 3 6, 3 5, 3 4, and 3 3 systolic arrays. Finally, those percentages combined with the simulation results are used to calculate the performance of the VUSA compared to the reference architecture. D. Efficiency on real DNN Models After computing the number of cycles to execute the target model on different designs following the described method- ology in Section V-C, the execution time and performance is calculated for standard Edge-AI benchmarks. Afterwards, the area efficiency (performance per area unit), power efficiency (performance per power unit), and total energy consumption are calculated. For the following experiments, pre-trained models with unstructured sparsity from SparseZoo [13] are deployed. TABLE II PERFORMANCE ON RESNET-18 PRUNED AT 85 Design Load split Number of cycles Time at 1 GHz(ms) Performance (GOP s) Standard 3 3 5.13 1.75 108 174.70 8.84 Standard 3 4 4.13 1.30 108 130.03 11.88 Standard 3 5 3.89 1.06 108 106.20 14.55 Standard 3 6 86.85 8.98 107 89.81 17.21 VUSA 3 6 9.65 107 96.46 16.02 Design Performance Area (Normalized) Performance Power (Normalized) Energy (Normalized) Standard 3 3 1.02 1.00 1.00 Standard 3 4 1.03 1.01 0.99 Standard 3 5 1.01 1.00 1.00 Standard 3 6 1 1 1 VUSA 3x6 1.27 1.56 0.64 TABLE III RESULTS ON MOBILENETV1 PRUNED AT 75 Design Load split Number of cycles Time at 1 GHz(ms) Performance (GOP s) Standard 3 3 12.38 6.95 107 69.50 8.19 Standard 3 4 11.44 5.34 107 53.43 10.65 Standard 3 5 7.54 4.44 107 44.39 12.82 Standard 3 6 68.64 3.82 107 38.16 14.91 VUSA 3 6 4.43 107 44.25 12.86 Design Performance Area (Normalized) Performance Power (Normalized) Energy (Normalized) Standard 3 3 1.09 1.07 0.94 Standard 3 4 1.07 1.04 0.96 Standard 3 5 1.03 1.02 0.98 Standard 3 6 1 1 1 VUSA 3 6 1.18 1.45 0.69 1) ResNet: The first evaluated model is ResNet-18 [14] pruned by 85 . The evaluation results are depicted in Table II. Overall, VUSA performs as a standard 3 6 for 86.85 of the model load. Thus, it achieves a 10 higher performance than a standard 3 5 array with an additional 14 area gain and 41 power savings, showing a clear advantage of the proposed technique. In general, the proposed technique outperforms the standard array in any shape in all efficiency metrics by about 25 in area and 55 in power. 2) MobileNetV1: The second model used for evaluation is MobileNetV1 [15] pruned at 75 . MobileNets are designed for resource-constrained edge devices and thus are particularly relevant. Also, they are harder to prune, hence the sparsity rate is lower. The MobileNet results are depicted in Table III. Due to the lower sparsity, here the VUSA performs as a standard 3 6 for 68.85 of the model load. Thus, it achieves a similar performance as a standard 3 5 array at an additional 14 area gain and 41 power savings. This shows the advantage of the proposed technique also for highly optimized Edge-AI model architectures with less unstructured sparsity. 0 0,2 0,4 0,6 0,8 1 1,2 1,4 10 30 45 55 85 95 Performance Area (Normalized) Sparsity Standard VUSA Fig. 8. Area efficiency at different pruning rates. 0 0,2 0,4 0,6 0,8 1 1,2 1,4 1,6 1,8 10 30 45 55 85 95 Performance Power (Normalized) Sparsity Standard VUSA Fig. 9. Power efficiency at different pruning rates. Again, the proposed technique outperforms the standard array in any shape in all efficiency metrics here, by about 10 15 in area and 40 in power. E. Pruning Effect on Efficiency The ratio of jobs the VUSA computes as a 3 6, 3 5, 3 4, and 3 3 array determine the gains of the proposed technique. The weight sparsity is the major parameter defining these ratios. In this subsection, the area and power efficiencies are calculated for models pruned at different percentages to estimate the turning point at which using the VUSA is no longer beneficial over a standard systolic array. The obtained results for the area efficiency are shown in Figure 8. For a model pruned at 95 , there is a 36 improvement for the VUSA compared to a standard 3 6 systolic array, whereas for a base model without any pruning, there is a reduction of 28 . For a model pruned at 55 , the area efficiency is slightly higher for the VUSA. The power efficiency is shown in Figure 9. For a 95 pruning rate, there is a 67 improvement for the VUSA compared to a standard 3 6 systolic array. For a base model without any pruning, there is a degradation by 11 . For a model pruned at 30 , the power efficiency is slightly lower for the VUSA. In summary, from a 30 pruning rate, there are gains in power efficiency. At a 55 pruning rate, there are gains in area efficiency as well. The stronger the pruning, the bigger the gains. It can also be observed that the VUSA exhibits great potential to improve power efficiency by 68 . VI. RELATED WORK In several commercial and academic AI accelerators, sys- tolic arrays are used to accelerate matrix multiplications which account for the largest fraction of the compute in DNN appli- cations [2], [9]. In the traditional systolic array, sparsity (i.e., zero-valued parameter or data) cannot be exploited. Several extensions to systolic arrays have been proposed that allow to exploit sparsity if it follows pre-defined structures (e.g., [4], [16]). The drawback is a reduction in the maximum achievable sparsity and or accuracy. Other published work aims at exploiting unstructured spar- sity (e.g., [6], [11], [12], [17]), on either activations or weights like the technique presented in this paper. These reference architectures all introduce an area overhead to exploit un- structured weight sparsity. In contrast, our proposed VUSA achieves area savings by reducing the number of physical MAC units required for performing computations and intro- duces a simplified PE that processes sparse weights. Moreover, our proposed technique preserves the general scalability and programming model of the widely spread systolic array, which enables compatibility with existing tools and frameworks. VII. CONCLUSION In this work, we introduced VUSA, a novel systolic array structure that can exploit unstructured sparsity by virtually growing to a larger size with the same number of physical MAC units. Experimental results for a commercial 16-nm technology and real-life DNN application show that our tech- nique can improve the area and power efficiencies by around 55 and 25 , respectively, while additionally improving performance by 10 compared to the baseline systolic array architecture. Still, the proposed architecture supports any type of DNNs, even without any sparsity. ACKNOWLEDGMENTS This research is funded by the German Federal Ministry of Education and Research within the project GreenEdge-FuE , funding no. 16ME0517K. This result is part of the IPCEI ME CT and is funded by the European Union Next Generation EU, the German Federal Ministry for Economic Affairs and Climate Action, the Bavar- ian Ministry of Economic Affairs, Regional Development and Energy, the Free State of Saxony with the help of tax revenue based on the budget approved by the Saxon State parliament and the Free and Hanseatic City of Hamburg. REFERENCES [1] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, A survey of convolutional neural networks: analysis, applications, and prospects, IEEE transac- tions on neural networks and learning systems, 2021. [2] N. P. Jouppi et al., In-datacenter performance analysis of a tensor pro- cessing unit, in Proceedings of the 44th annual international symposium on computer architecture, 2017, pp. 1 12. [3] M. Soltaniyeh, R. P. Martin, and S. Nagarakatte, An accelerator for sparse convolutional neural networks leveraging systolic general matrix- matrix multiplication, ACM Transactions on Architecture and Code Optimization (TACO), vol. 19, no. 3, pp. 1 26, 2022. [4] W. Sun et al., Sense: Model-hardware codesign for accelerating sparse cnns on systolic arrays, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 31, no. 4, pp. 470 483, 2023. [5] S. Han, H. Mao, and W. J. Dally, Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, arXiv preprint arXiv:1510.00149, 2015. [6] S. Han et al., Eie: Efficient inference engine on compressed deep neural network, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 243 254, 2016. [7] L. Bamberg, A. Pourtaherian, L. Waeijen, A. Chahar, and O. Moreira, Synapse compression for event-based convolutional-neural-network ac- celerators, IEEE Transactions on Parallel and Distributed Systems, vol. 34, no. 4, pp. 1227 1240, 2023. [8] L. Bamberg, A. Najafi, and A. Garcia-Ortiz, Exploiting neural-network statistics for low-power DNN inference, IEEE Open Journal of Circuits and Systems, 2024. [9] H.-T. Kung, Why systolic architectures? IEEE computer, vol. 15, no. 1, pp. 37 46, 1982. [10] A. Samajdar, Y. Zhu, P. Whatmough, M. Mattina, and T. Kr- ishna, Scale-sim: Systolic cnn accelerator simulator, arXiv preprint arXiv:1811.02883, 2018. [11] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and A. Moshovos, Cnvlutin: Ineffectual-neuron-free deep neural network computing, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 1 13, 2016. [12] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze, Eyeriss: An energy- efficient reconfigurable accelerator for deep convolutional neural net- works, IEEE journal of solid-state circuits, vol. 52, no. 1, pp. 127 138, 2016. [13] NeuralMagic, SparseZoo: Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes, accessed: 2023-12-21. [Online]. Available: neuralmagic.com [14] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770 778. [15] A. G. Howard et al., Mobilenets: Efficient convolutional neural net- works for mobile vision applications, arXiv preprint arXiv:1704.04861, 2017. [16] Z.-G. Liu, P. N. Whatmough, Y. Zhu, and M. Mattina, S2ta: Exploiting structured sparsity for energy-efficient mobile cnn acceleration, in 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022, pp. 573 586. [17] A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan, B. Khailany, J. Emer, S. W. Keckler, and W. J. Dally, Scnn: An accelerator for compressed-sparse convolutional neural networks, ACM SIGARCH computer architecture news, vol. 45, no. 2, pp. 27 40, 2017.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nVUSA: Virtually Upscaled Systolic Array Architecture to Exploit Unstructured Sparsity in AI Acceleration Shereef Helal NXP Semiconductors Munich, Germany Alberto Garcia-Ortiz University of Bremen Bremen, Germany Lennart Bamberg NXP Semiconductors Hamburg, Germany Abstract Leveraging high degrees of unstructured sparsity is a promising approach to enhance the efficiency of deep neural network (DNN) accelerators particularly important for emerg- ing Edge-AI applications. We introduce VUSA, a systolic-array architecture that virtually grows based on the present sparsity to perform larger matrix multiplications with the same number of physical multiply-accumulate (MAC) units. The proposed architecture achieves saving by 37 and 68 in area and power efficiency, respectively, at the same peak-performance, compared to a baseline systolic array architecture in a commercial 16-nm technology. Still, the proposed architecture supports acceleration for any DNN with any sparsity even no sparsity at all. Thus, the proposed architecture is application-independent, making it viable for general-purpose AI acceleration. Index Terms Systolic Array, Unstructured Sparsity, AI Ac- celeration, Edge AI, Computer Architecture I. INTRODUCTION Over recent years, Artificial Intelligence (AI) has emerged as a revolutionary new technology, spreading across different industries and enhancing various aspects of our daily lives. The deployment of AI is not only confined to powerful data- center machines, but is increasingly demanded in resource- constrained embedded devices, a concept known as Edge AI. Deep Neural Network (DNN) architectures are the backbone of state-of-the-art AI applications to perform numerous tasks, such as image processing, speech recognition, natural language processing (NLP), and more [1]. However, DNNs have high computational demands, posing a significant challenge when deploying them in real-world applications. This challenge is even greater for Edge AI applications in resource-constrained environments. Domain-specific hardware accelerators are widely used for Edge AI applications to improve the efficiency of DNN infer- ence [2], [3], [4]. Due to its scalability and programmability, a systolic array is one of the most widely used architectures for DNN acceleration. The scalability and programmability come from a regular and simple structure that allows efficient matrix- by-matrix multiplications.\n\n--- Segment 2 ---\nDue to its scalability and programmability, a systolic array is one of the most widely used architectures for DNN acceleration. The scalability and programmability come from a regular and simple structure that allows efficient matrix- by-matrix multiplications. Such matrix multiplications account for the largest fraction of the computational complexity of modern AI applications. However, the regularity of the systolic array also brings drawbacks such as poor utilization for large array dimensions (i.e., large number of multiply-accumulate (MAC) blocks) and no exploitation of unstructured sparsity. Weight sparsity refers to the number of zero-valued pa- rameters in a DNN. The computations associated with those parameters can be skipped without affecting the final results, thereby improving power efficiency, latency, and memory usage. Most accelerators can only exploit structured sparsity, which limits the amount of usable sparsity to 50 and has poor accuracy. In contrast, modern DNN applications can be trained to achieve much higher sparsity levels exceeding 90 without any loss in accuracy when sparsity is uncon- strained (i.e., unstructured) [5]. Thus, extending regular and scalable accelerator architectures, such as systolic arrays, to support this form of sparsity is a topic of great research interest in both academia [6] and industry [7], [8]. This work extends the standard weight-stationary systolic array architecture to exploit unstructured sparsity as much as possible while maintaining high efficiency for dense (i.e., non- sparse) execution. Specifically, we modify the architecture so that the systolic array can virtually expand to a larger size in the presence of weight sparsity. This expansion increases com- putational speed to match that of a more hardware-expensive array. Nevertheless, the simplicity of the hardware architecture and programmability is preserved, maintaining the key advan- tages of systolic arrays in terms of scalability and general- purpose applicability. Experimental results in a commercial 16-nm technology demonstrate that the proposed technique improves the area and power consumption by 37 and 68 , respectively, while maintaining the same peak performance. In the proposed architecture, the maximum amount of ex- ploitable weight sparsity is defined by an integer architectural parameter the maximum virtual growth.\n\n--- Segment 3 ---\nExperimental results in a commercial 16-nm technology demonstrate that the proposed technique improves the area and power consumption by 37 and 68 , respectively, while maintaining the same peak performance. In the proposed architecture, the maximum amount of ex- ploitable weight sparsity is defined by an integer architectural parameter the maximum virtual growth. While the standard systolic array size is determined by the number of rows and columns, the proposed virtually upscaled systolic array (VUSA) is characterized by the number of rows, columns, and virtual columns. This approach preserves the generic and flexible nature of systolic arrays while expanding the design space to better exploit sparsity. To the best of the MOCAST 2025 Paper Preprint 2025 IEEE arXiv:2506.01166v1 [cs.AR] 1 Jun 2025 4 5 6 7 8 9 1 2 3 𝐼31 𝐼21 𝐼11 𝐼32 𝐼22 𝐼12 𝐼33 𝐼23 𝐼13 𝑤1 𝑤2 𝑤3 𝑤4 𝑤5 𝑤6 𝑤7 𝑤8 𝑤9 𝐼11 𝐼12 𝐼13 𝐼21 𝐼22 𝐼23 𝐼31 𝐼32 𝐼33 . 𝑤1 𝑤4 𝑤7 𝑤2 𝑤5 𝑤8 𝑤3 𝑤6 𝑤9 (1) (2) Fig. 1. Systolic array with weight-stationary data flow. authors knowledge, this is the first systolic array architecture that introduces a new dimensionality to effectively leverage unstructured weight sparsity in a weight-stationary systolic array without compromising scalability or programmability. The rest of the paper is structured as follows. Section II outlines the background. Subsequently, in Section III, we outline the architecture of the proposed VUSA. A quantitative, theoretical analysis of the expected gain of the proposed technique is presented in Section IV. Section V includes experimental results for a commercial 16-nm technology and real-life DNN applications.\n\n--- Segment 4 ---\nA quantitative, theoretical analysis of the expected gain of the proposed technique is presented in Section IV. Section V includes experimental results for a commercial 16-nm technology and real-life DNN applications. Finally, a comparison to related work and a conclusion are drawn. II. BACKGROUND A. Systolic Arrays The structure of a systolic array is shown in Figure 1. Systolic arrays are used to accelerate tensor operations by decomposing them into matrix-by-matrix multiplications (in combination with data arrangements by other components). The architecture is made up of multiple processing elements (PEs), performing MAC operations, between which data auto- matically flows from the left to the right and top to the bottom. Such a data-driven architecture heavily reduces the memory accesses per compute, enabling scaling to compute capabilities in the high TOPS regime [9]. Common data flows in systolic arrays are: Weight Stationary (WS), Input Stationary (IS), and Output Stationary (OS). The name of the dataflow indicates which parameter is stationary to the PEs during a processing sequence, while the other two are flowing from left to right and top to bottom [10]. This work focuses on WS dataflow as it enables to exploit unstructured weight sparsity while keeping the flow of data regular. An example WS matrix multiplication is illustrated in Figure 1. First, the weights are loaded into the PEs. The weights remain stationary to the respective PEs until the computations are done. Afterwards, the inputs are fed to the PEs from the left and multiplied by the weights to generate partial sums. Each processing element adds the partial sum to the accumulator value from the top input. To the right output the left input is forwarded as is, while to the bottom the updated accumulator value is passed. Multiplications of matrices larger than the systolic array size are decomposed into multiple smaller pieces that fit the array size. For this purpose, the resulting accumulator values (i.e., the results flowing out at the bottom) from the previous step are used as the starting accumulator values (i.e., feed to the array at the top). Matrices smaller than the systolic array size lead to an under utilization of the systolic array. B. Unstructured Weight Sparsity The term sparsity describes the high degree of zero in neural network activations data and weights.\n\n--- Segment 5 ---\nMatrices smaller than the systolic array size lead to an under utilization of the systolic array. B. Unstructured Weight Sparsity The term sparsity describes the high degree of zero in neural network activations data and weights. Pruning weights is the primary source of high sparsity; A technique that removes low-magnitude weights with the smallest impact on the model output. If pruning is done iteratively during training, the accuracy can be preserved or even enhanced (reduced over-fitting), while achieving sparsity factors above 90 [5]. However, dense accelerators such as standard systolic arrays cannot work with the compressed, pruned weights due to the absence of regular data flow. Thus, processing unstructured sparse weight tensors on such architectures, implies a large degree of zero-values in the weight matrices [3]. In this case, the effective operation of the large fraction of MAC blocks with a stationary zero-weight becomes a pass-through of both accumulator and input value (i.e., a NoOP). III. PROPOSED ARCHITECTURE Our goal is to extend the systolic-architecture to be capable of working with any weight matrix in which the total number of non-zero weights does not exceed the number of MAC units. In this case, each non-zero weight is loaded to one MAC, ignoring zero weights since they do not contribute to the final result. The challenge that this destroys the regularity of the data-flow of inputs and activations is addressed by extending the architecture as follows. A. Processing Sparse Weights in PEs The operation of a weight stationary PE is distinguished here in two different operating states: 1) Non-Sparse: stationary weight is non-zero: In this case, the full logic inside the PE depicted in Figure 2 is required for correct processing. 2) Sparse: stationary weight is zero: In this case, no data- path blocks are required. Only the pipeline stages for the cycle-by-cycle data-flow are required. Figure 2 high- lights in different colors the parts that can be removed for sparse processing (red) versus what must be kept to keep the data-flow regular (blue). B.\n\n--- Segment 6 ---\nFigure 2 high- lights in different colors the parts that can be removed for sparse processing (red) versus what must be kept to keep the data-flow regular (blue). B. Separating Processing from Data-Flow Elements Inspired by the fact that no compute element is actually needed when the weight is zero, we decompose the PE into two components: the MAC unit (i.e., processing components), and the pipeline stages (data-flow components). The resulting two blocks are shown in Figure 3. Sparse Processing Element (SPE): A block containing only pipeline registers, required for sparse as well as non- sparse processing MAC unit: A block that can be optionally connected to an SPE, required only for non-zero weights. left_in_reg top_in_reg 0 accumulat or_reg X left_in top_in bottom_out right_out Fig. 2. Example architecture of a PE. Parts marked in red can be removed when the stationary weight is zero. left_in_reg accumulat or_reg left_in acc_in acc_out right_out weight_reg X left_in weight_in weight_out acc_in acc_out SPE MAC Fig. 3. Decomposing the PE to a data-flow and a compute block. C. Virtual Upscaled Systolic Array (VUSA) As MAC units are not required for zero weights, they can be instantiated in a lower dimension than the maximum virtual systolic array size. Based on that, a sparse N M systolic array with N A MAC units is proposed, where N is the number of rows, M is the number of columns, and A is the number of MAC units per row. Thus, each row has M SPEs but only A number of MAC units where 1 A M. If a weight is zero, then only an SPE is sufficient in that position. Otherwise, a MAC unit is connected to the SPE in that position for the needed arithmetic computation. If the number of non-zero weights per row is less than or equal to the number of available MAC units per row (A), then the sparse systolic array virtually grows to an N M systolic array.\n\n--- Segment 7 ---\nOtherwise, a MAC unit is connected to the SPE in that position for the needed arithmetic computation. If the number of non-zero weights per row is less than or equal to the number of available MAC units per row (A), then the sparse systolic array virtually grows to an N M systolic array. Otherwise, a smaller window of weights is selected until the sparsity condition is satisfied, starting with an N (M 1) window, then N (M 2), and so on down to N A, at which the conditions are guaranteed to be satisfied. Thus, still any matrix-matrix multiplication can still be mapped even in the absence of sparsity. In detail, no sparsity just implies slower processing as we need to break down larger matrix-by-matrix multiplications into sub-blocks of size N A. A conceptual illustration of a single row of the VUSA is shown in Figure 4. Each MAC unit has a configurable connec- tion to multiple SPEs. Depending on the positions of non-zero weights in each row, the MAC units are connected to SPEs where computations involving non-zero weights are required. The connections to different MAC units can be dynamically configured such that changing positions of non-zero weights SPE_0 SPE_1 SPE_2 SPE_M-3 SPE_M-2 SPE_M-1 MAC_0 MAC_1 MAC_A-1 MAC_A-2 .. .. Fig. 4. Single row of the proposed VUSA architecture. SPE_0 SPE_1 SPE_2 SPE_3 SPE_5 MAC_0 MAC_1 MAC_2 SPE_4 Fig. 5. Row instance of the proposed VUSA for M 6 and A 3. can be handled per job. Still, an all-to-all connection between SPEs and MAC units is not needed. Each MAC unit only needs to be connected to one out of M A 1 directly adjacent SPEs to handle all potential zero-distributions. Let us label the SPEs with indexes i [0, ..., M 1] and the MACs with j [0, ..., A 1]. Consider now MAC number r, associated with SPE number t, MAC r can be mapped only to SPEs [r, ..., r M A].\n\n--- Segment 8 ---\nLet us label the SPEs with indexes i [0, ..., M 1] and the MACs with j [0, ..., A 1]. Consider now MAC number r, associated with SPE number t, MAC r can be mapped only to SPEs [r, ..., r M A]. In other words, an input-output shifter of up to M A positions per MAC in one direction is sufficient. Thereby, wiring is minimized which in turn reduces negative impacts on timing and power. In fact, experimental results for a commercial 16-nm technology (cf. Section V) show that even for M A the critical timing path and the power bottleneck is the MAC unit and not the MAC-to-SPE multiplexing. For example, applying this design to a sparse row with M 6 and A 3 requires to connect each MAC to four SPEs. This is illustrated in Figure 5. This proposed architecture can handle unstructured sparsity because the position on non-zero weights is not restricted. It achieves direct savings in resources by using the same number of MAC units to conduct matrix multiplications of a higher dimension. Since MAC units are not connected in positions where weights are zero, a natural multiplier gating [11], [12] for sparse weights is achieved. In this case, the gating applies not only to the multiplier but to the full MAC unit. The sparse row is the building block of VUSA which can be stacked vertically or horizontally to implement VUSAs with arbitrary dimensions. IV. THEORETICAL ANALYSIS In this section, we statistically assess the theoretical per- formance gains of the proposed architecture as a function of the sparsity. Truly unstructured sparsity implies that there is no correlation of sparsity between the individual weight positions. Thus, the sparsity probability of each weight is the same with no extra cross-correlation (i.e., inter-dependencies between the sparsity of different weights). In other words, the number of zero-terms follow a Binomial distribution. The maximum gain of the VUSA is achieved when the array dimension virtually grows to N M. Under the above assumption of no cross-correlation, the probability of this occurring depends only on the probability that a weight is non-sparse, denoted here as P1.\n\n--- Segment 9 ---\nIn other words, the number of zero-terms follow a Binomial distribution. The maximum gain of the VUSA is achieved when the array dimension virtually grows to N M. Under the above assumption of no cross-correlation, the probability of this occurring depends only on the probability that a weight is non-sparse, denoted here as P1. For a VUSA, defined by tuple (N, M, A), the probability that the VUSA virtually grows to a standard N M systolic array, Psparse standard, can be derived as follows: 1) The gain probability of a single row (Pgain row) is computed which is the probability that the number of non-zero weights in the set of M is less than or equal to the number of available MAC units per row, A. This can be calculated by summing up the probabilities that the number of non-zero weights is exactly one of the numbers in the range [0:A]. The probability of getting exactly i number of non-zero weights is denoted as Pi,ones. Based on that, Pgain row can be calculated as: Pgain row A X i 0 Pi,ones (1) 2) The gain of the VUSA is achieved when the sparsity conditions are met for all rows. Hence, Psparse standard can be derived as: Psparse standard (Pgain row)N A X i 0 Pi,ones !N (2) 3) Pi,ones can be calculated as: Pi,ones M i P1 i(1 P1)M i (3) 4) From Equations 2 and 3, the formula for Psparse standard becomes: Psparse standard A X i 0 M i P1 i(1 P1)M i !N (4) If desired, a simpler analytical expression can be drawn using the regularized incomplete beta function, the CDF of the binomial, but the explicit result as a sum is more intuitive. Analogously, one can express the probability to have a virtual growth in the array size to another M , smaller than the theoretical maximum M defined by the hardware config- uration, but larger than A. The probability to map a given problem to a N A array (no virtual extension) is 1, as this is always possible, independent of the weights.\n\n--- Segment 10 ---\nAnalogously, one can express the probability to have a virtual growth in the array size to another M , smaller than the theoretical maximum M defined by the hardware config- uration, but larger than A. The probability to map a given problem to a N A array (no virtual extension) is 1, as this is always possible, independent of the weights. By applying the derived mathematical model to a VUSA with N 3, M 6, and A 3, the probability to be able to virtually grow to 3 6, 3 5, 3 4 is calculated based on the sparsity rate, P0 1 P1. The results are shown in Figure 6. It can be observed that when sparsity is higher than 90 , the probability to virtually grow to 3 6 (i.e., 2 speed-up) is close to 1. Even for sparsity rates as low as 60 , the success rate for the maximum gain is above 50 . Even at dramatically low sparsity of around 30 , the success rate to grow to a 3 4 array (i.e., 4 3 speed-up) is still above 50 . Only in the theoretical worst case of no exploitable sparsity (3 3 window), there is no performance gain. On a real full AI application, composed of many different matrix multiplication, the VUSA performs 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 P(sparse standard) 𝑃0 3x6 3x5 3x4 3x3 Fig. 6. Growth probability of VUSA with N 3, M 6, and A 3 TABLE I SYNTHESIS RESULTS IN A COMMERCIAL 16-NM TECHNOLOGY AT 1 GHz Design MACs Area (Normalized) Power (Normalized) Standard 3 3 9 0.69 0.86 Standard 3 4 12 0.91 1.15 Standard 3 5 15 1.14 1.41 Standard 3 6 18 1.37 1.68 VUSA 3 6 9 1 1 with a combination of all virtual growth possibilities based on the weight sparsity of the current computation. This behavior is analyzed by means of experimental results next.\n\n--- Segment 11 ---\nGrowth probability of VUSA with N 3, M 6, and A 3 TABLE I SYNTHESIS RESULTS IN A COMMERCIAL 16-NM TECHNOLOGY AT 1 GHz Design MACs Area (Normalized) Power (Normalized) Standard 3 3 9 0.69 0.86 Standard 3 4 12 0.91 1.15 Standard 3 5 15 1.14 1.41 Standard 3 6 18 1.37 1.68 VUSA 3 6 9 1 1 with a combination of all virtual growth possibilities based on the weight sparsity of the current computation. This behavior is analyzed by means of experimental results next. V. EXPERIMENTAL RESULTS In this section, a VUSA with N 3, A 3, M 6 (i.e., 9 MAC units that can virtually grow to 18) is evaluated against varying systolic array sizes for real benchmark applications. The standard systolic array dimensions to be investigated are: 3 3, 3 4, 3 5, and 3 6. A. Physical-Design The different architectures are physical-aware (to consider interconnects) synthesized in a commercial 16-nm technology at the same target frequency of 1GHz. Table I shows the resulting silicon area and power consumption for all designs normalized to the values of the proposed VUSA architecture. The area footprints of standard 3 6 and 3 5 systolic arrays are 37 and 14 , larger than the proposed VUSA architecture, respectively. On the other end, the areas of standard 3 4 and 3 3 arrays are 9 and 31 smaller, respectively. In terms of power consumption, a standard 3 6, 3 5, and 3 4 array are 68 , 41 , and 15 , more costly than the proposed solution, respectively. A standard 3 3 array has a 14 lower power consumption. Overall, there is a clear advantage in both area and power for the 3 6 and 3 5 cases. For the 3 4 case, there are gains in power but with a small area overhead. Compared to a standard 3 3 array, there are area and power overheads. Thus, to be an efficient solution on a combined power-performance- area metric, our proposed technique must dominantly work at least as 3 4 virtual systolic array for a given application.\n\n--- Segment 12 ---\nCompared to a standard 3 3 array, there are area and power overheads. Thus, to be an efficient solution on a combined power-performance- area metric, our proposed technique must dominantly work at least as 3 4 virtual systolic array for a given application. 1 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 3x6: 100 3x6: 50 , 3x3: 50 Fig. 7. Example 50 sparsity distribution B. Effect of Sparsity on Performance The number of zero weights and their positions in the different layers of a given application determines the number of jobs to be executed on the VUSA structure. If the weights of a given load are almost all zeros, the VUSA virtually grows to a 3 6 array, implying a significant gain over the standard architecture. If on the other hand all weights are non-zero, then the VUSA acts only as a 3 3 and thus performs worse than a standard systolic array due to the power and area overhead outlined above. Not only the number of zeros but also their distribution is relevant. Consider a load with 50 sparsity, as depicted in Figure 7. If the zeros are distributed approximately evenly, then the VUSA can process the entire problem with a 3 6 virtual array. On the other hand, if the zeros are correlated in their positions, then the VUSA processes only 50 of the jobs with a 3 6 window, while the other 50 is processed with a 3 3 window. On a real application with unstructured sparsity, a combination of all cases typically occurs.\n\n--- Segment 13 ---\nOn the other hand, if the zeros are correlated in their positions, then the VUSA processes only 50 of the jobs with a 3 6 window, while the other 50 is processed with a 3 3 window. On a real application with unstructured sparsity, a combination of all cases typically occurs. C. Methodology to Analyze Full DNN Models The evaluation of full applications is performed in two steps: first, a cycle-accurate simulation is performed using SCALE- Sim [10] to obtain the number of required cycles to run DNN models on standard systolic arrays of dimensions from 3 3 to 3 6. The second step is obtaining pre-trained models and using the values of the pre-trained weights to determine the sparsity conditions of the weights (number of zeros and their distribution). Based on the weight filters, the number of different windows of sizes from 3 3 to 3 6 required by the VUSA is determined, which can be used to calculate the percentages of the VUSA performing as 3 6, 3 5, 3 4, and 3 3 systolic arrays. Finally, those percentages combined with the simulation results are used to calculate the performance of the VUSA compared to the reference architecture. D. Efficiency on real DNN Models After computing the number of cycles to execute the target model on different designs following the described method- ology in Section V-C, the execution time and performance is calculated for standard Edge-AI benchmarks. Afterwards, the area efficiency (performance per area unit), power efficiency (performance per power unit), and total energy consumption are calculated. For the following experiments, pre-trained models with unstructured sparsity from SparseZoo [13] are deployed.\n\n--- Segment 14 ---\nAfterwards, the area efficiency (performance per area unit), power efficiency (performance per power unit), and total energy consumption are calculated. For the following experiments, pre-trained models with unstructured sparsity from SparseZoo [13] are deployed. TABLE II PERFORMANCE ON RESNET-18 PRUNED AT 85 Design Load split Number of cycles Time at 1 GHz(ms) Performance (GOP s) Standard 3 3 5.13 1.75 108 174.70 8.84 Standard 3 4 4.13 1.30 108 130.03 11.88 Standard 3 5 3.89 1.06 108 106.20 14.55 Standard 3 6 86.85 8.98 107 89.81 17.21 VUSA 3 6 9.65 107 96.46 16.02 Design Performance Area (Normalized) Performance Power (Normalized) Energy (Normalized) Standard 3 3 1.02 1.00 1.00 Standard 3 4 1.03 1.01 0.99 Standard 3 5 1.01 1.00 1.00 Standard 3 6 1 1 1 VUSA 3x6 1.27 1.56 0.64 TABLE III RESULTS ON MOBILENETV1 PRUNED AT 75 Design Load split Number of cycles Time at 1 GHz(ms) Performance (GOP s) Standard 3 3 12.38 6.95 107 69.50 8.19 Standard 3 4 11.44 5.34 107 53.43 10.65 Standard 3 5 7.54 4.44 107 44.39 12.82 Standard 3 6 68.64 3.82 107 38.16 14.91 VUSA 3 6 4.43 107 44.25 12.86 Design Performance Area (Normalized) Performance Power (Normalized) Energy (Normalized) Standard 3 3 1.09 1.07 0.94 Standard 3 4 1.07 1.04 0.96 Standard 3 5 1.03 1.02 0.98 Standard 3 6 1 1 1 VUSA 3 6 1.18 1.45 0.69 1) ResNet: The first evaluated model is ResNet-18 [14] pruned by 85 . The evaluation results are depicted in Table II. Overall, VUSA performs as a standard 3 6 for 86.85 of the model load.\n\n--- Segment 15 ---\nThe evaluation results are depicted in Table II. Overall, VUSA performs as a standard 3 6 for 86.85 of the model load. Thus, it achieves a 10 higher performance than a standard 3 5 array with an additional 14 area gain and 41 power savings, showing a clear advantage of the proposed technique. In general, the proposed technique outperforms the standard array in any shape in all efficiency metrics by about 25 in area and 55 in power. 2) MobileNetV1: The second model used for evaluation is MobileNetV1 [15] pruned at 75 . MobileNets are designed for resource-constrained edge devices and thus are particularly relevant. Also, they are harder to prune, hence the sparsity rate is lower. The MobileNet results are depicted in Table III. Due to the lower sparsity, here the VUSA performs as a standard 3 6 for 68.85 of the model load. Thus, it achieves a similar performance as a standard 3 5 array at an additional 14 area gain and 41 power savings. This shows the advantage of the proposed technique also for highly optimized Edge-AI model architectures with less unstructured sparsity. 0 0,2 0,4 0,6 0,8 1 1,2 1,4 10 30 45 55 85 95 Performance Area (Normalized) Sparsity Standard VUSA Fig. 8. Area efficiency at different pruning rates. 0 0,2 0,4 0,6 0,8 1 1,2 1,4 1,6 1,8 10 30 45 55 85 95 Performance Power (Normalized) Sparsity Standard VUSA Fig. 9. Power efficiency at different pruning rates. Again, the proposed technique outperforms the standard array in any shape in all efficiency metrics here, by about 10 15 in area and 40 in power. E. Pruning Effect on Efficiency The ratio of jobs the VUSA computes as a 3 6, 3 5, 3 4, and 3 3 array determine the gains of the proposed technique. The weight sparsity is the major parameter defining these ratios. In this subsection, the area and power efficiencies are calculated for models pruned at different percentages to estimate the turning point at which using the VUSA is no longer beneficial over a standard systolic array. The obtained results for the area efficiency are shown in Figure 8.\n\n--- Segment 16 ---\nIn this subsection, the area and power efficiencies are calculated for models pruned at different percentages to estimate the turning point at which using the VUSA is no longer beneficial over a standard systolic array. The obtained results for the area efficiency are shown in Figure 8. For a model pruned at 95 , there is a 36 improvement for the VUSA compared to a standard 3 6 systolic array, whereas for a base model without any pruning, there is a reduction of 28 . For a model pruned at 55 , the area efficiency is slightly higher for the VUSA. The power efficiency is shown in Figure 9. For a 95 pruning rate, there is a 67 improvement for the VUSA compared to a standard 3 6 systolic array. For a base model without any pruning, there is a degradation by 11 . For a model pruned at 30 , the power efficiency is slightly lower for the VUSA. In summary, from a 30 pruning rate, there are gains in power efficiency. At a 55 pruning rate, there are gains in area efficiency as well. The stronger the pruning, the bigger the gains. It can also be observed that the VUSA exhibits great potential to improve power efficiency by 68 . VI. RELATED WORK In several commercial and academic AI accelerators, sys- tolic arrays are used to accelerate matrix multiplications which account for the largest fraction of the compute in DNN appli- cations [2], [9]. In the traditional systolic array, sparsity (i.e., zero-valued parameter or data) cannot be exploited. Several extensions to systolic arrays have been proposed that allow to exploit sparsity if it follows pre-defined structures (e.g., [4], [16]). The drawback is a reduction in the maximum achievable sparsity and or accuracy. Other published work aims at exploiting unstructured spar- sity (e.g., [6], [11], [12], [17]), on either activations or weights like the technique presented in this paper. These reference architectures all introduce an area overhead to exploit un- structured weight sparsity. In contrast, our proposed VUSA achieves area savings by reducing the number of physical MAC units required for performing computations and intro- duces a simplified PE that processes sparse weights.\n\n--- Segment 17 ---\nThese reference architectures all introduce an area overhead to exploit un- structured weight sparsity. In contrast, our proposed VUSA achieves area savings by reducing the number of physical MAC units required for performing computations and intro- duces a simplified PE that processes sparse weights. Moreover, our proposed technique preserves the general scalability and programming model of the widely spread systolic array, which enables compatibility with existing tools and frameworks. VII. CONCLUSION In this work, we introduced VUSA, a novel systolic array structure that can exploit unstructured sparsity by virtually growing to a larger size with the same number of physical MAC units. Experimental results for a commercial 16-nm technology and real-life DNN application show that our tech- nique can improve the area and power efficiencies by around 55 and 25 , respectively, while additionally improving performance by 10 compared to the baseline systolic array architecture. Still, the proposed architecture supports any type of DNNs, even without any sparsity. ACKNOWLEDGMENTS This research is funded by the German Federal Ministry of Education and Research within the project GreenEdge-FuE , funding no. 16ME0517K. This result is part of the IPCEI ME CT and is funded by the European Union Next Generation EU, the German Federal Ministry for Economic Affairs and Climate Action, the Bavar- ian Ministry of Economic Affairs, Regional Development and Energy, the Free State of Saxony with the help of tax revenue based on the budget approved by the Saxon State parliament and the Free and Hanseatic City of Hamburg. REFERENCES [1] Z. Li, F. Liu, W. Yang, S. Peng, and J. Zhou, A survey of convolutional neural networks: analysis, applications, and prospects, IEEE transac- tions on neural networks and learning systems, 2021. [2] N. P. Jouppi et al., In-datacenter performance analysis of a tensor pro- cessing unit, in Proceedings of the 44th annual international symposium on computer architecture, 2017, pp. 1 12. [3] M. Soltaniyeh, R. P. Martin, and S. Nagarakatte, An accelerator for sparse convolutional neural networks leveraging systolic general matrix- matrix multiplication, ACM Transactions on Architecture and Code Optimization (TACO), vol. 19, no. 3, pp.\n\n--- Segment 18 ---\n19, no. 3, pp. 1 26, 2022. [4] W. Sun et al., Sense: Model-hardware codesign for accelerating sparse cnns on systolic arrays, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 31, no. 4, pp. 470 483, 2023. [5] S. Han, H. Mao, and W. J. Dally, Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, arXiv preprint arXiv:1510.00149, 2015. [6] S. Han et al., Eie: Efficient inference engine on compressed deep neural network, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 243 254, 2016. [7] L. Bamberg, A. Pourtaherian, L. Waeijen, A. Chahar, and O. Moreira, Synapse compression for event-based convolutional-neural-network ac- celerators, IEEE Transactions on Parallel and Distributed Systems, vol. 34, no. 4, pp. 1227 1240, 2023. [8] L. Bamberg, A. Najafi, and A. Garcia-Ortiz, Exploiting neural-network statistics for low-power DNN inference, IEEE Open Journal of Circuits and Systems, 2024. [9] H.-T. Kung, Why systolic architectures? IEEE computer, vol. 15, no. 1, pp. 37 46, 1982. [10] A. Samajdar, Y. Zhu, P. Whatmough, M. Mattina, and T. Kr- ishna, Scale-sim: Systolic cnn accelerator simulator, arXiv preprint arXiv:1811.02883, 2018. [11] J. Albericio, P. Judd, T. Hetherington, T. Aamodt, N. E. Jerger, and A. Moshovos, Cnvlutin: Ineffectual-neuron-free deep neural network computing, ACM SIGARCH Computer Architecture News, vol. 44, no. 3, pp. 1 13, 2016.\n\n--- Segment 19 ---\n3, pp. 1 13, 2016. [12] Y.-H. Chen, T. Krishna, J. S. Emer, and V. Sze, Eyeriss: An energy- efficient reconfigurable accelerator for deep convolutional neural net- works, IEEE journal of solid-state circuits, vol. 52, no. 1, pp. 127 138, 2016. [13] NeuralMagic, SparseZoo: Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes, accessed: 2023-12-21. [Online]. Available: neuralmagic.com [14] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770 778. [15] A. G. Howard et al., Mobilenets: Efficient convolutional neural net- works for mobile vision applications, arXiv preprint arXiv:1704.04861, 2017. [16] Z.-G. Liu, P. N. Whatmough, Y. Zhu, and M. Mattina, S2ta: Exploiting structured sparsity for energy-efficient mobile cnn acceleration, in 2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 2022, pp. 573 586. [17] A. Parashar, M. Rhu, A. Mukkara, A. Puglielli, R. Venkatesan, B. Khailany, J. Emer, S. W. Keckler, and W. J. Dally, Scnn: An accelerator for compressed-sparse convolutional neural networks, ACM SIGARCH computer architecture news, vol. 45, no. 2, pp. 27 40, 2017.\n\n