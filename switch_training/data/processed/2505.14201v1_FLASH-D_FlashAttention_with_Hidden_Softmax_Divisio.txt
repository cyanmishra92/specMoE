=== ORIGINAL PDF: 2505.14201v1_FLASH-D_FlashAttention_with_Hidden_Softmax_Divisio.pdf ===\n\nRaw text length: 37455 characters\nCleaned text length: 36999 characters\nNumber of segments: 23\n\n=== CLEANED TEXT ===\n\nFLASH-D: FlashAttention with Hidden Softmax Division Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos Integrated Circuits Lab, Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece Abstract The transformer s attention mechanism has revolu- tionized AI and machine learning, with its efficient computation being crucial to its performance. However, calculating attention involves matrix operations interspersed with softmax rescaling, which inherently slows down computation and requires pro- cessing the entire input sequence. Building on online softmax computation, FlashAttention integrates softmax calculation with matrix arithmetic, enabling tiled computation independent of sequence length. While optimized for GPUs, FlashAttention s simplicity makes it amenable to direct hardware acceleration. This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a mathematically equivalent, yet simplified, formulation that achieves: (a) hiding softmax division within other non- linear function evaluations; (b) inherently numerically stable computation of exponentials, eliminating the need for maximum value subtraction; and (c) a reduction in computational cost without introducing numerical approximations to the FlashAtten- tion kernel. Importantly, the essential FlashAttention properties that facilitate efficient tiled implementation are fully preserved. Hardware implementation results at 28nm demonstrate that this proposed formulation achieves a 22.8 reduction in area and a 20.3 reduction in power, on average, compared to state-of- the-art parallel hardware architectures without any performance penalty. Index Terms AI Hardware Accelerators, Transformers, FlashAttention, Energy Efficiency I. INTRODUCTION Current state-of-the-art ML and AI systems are character- ized by deep learning models with billions of parameters, achieving human-level performance in tasks like image recog- nition [1] and natural language processing [2]. A key innova- tion driving this progress is the attention mechanism [3], which allows models to focus on relevant parts of the input data. This has led to breakthroughs like transformer networks and large language models that are revolutionizing how machines understand and interact with the world. The increasing demand for processing long sequences in transformer models has exposed the complexity of the at- tention mechanism [4]. This computational burden, makes inference (and training) prohibitively expensive for extended context lengths. Traditional attention calculates pairwise sim- ilarities between all tokens, leading to a massive number of operations and memory traffic. To reduce the number of operations either by approximating the full attention matrix or by selectively focusing on the most relevant parts of the input sequence, techniques such as sparse attention [5], linear attention [6], and low-rank attention [7] have been explored, each balancing accuracy and computational efficiency. Early attention hardware accelerators optimize which attention-relevant matrices, such as key, value and queries, remain stationary in local SRAMs and which are streamed to compute the attention scores [8], [9], [10]. If the accelerator cannot buffer and compute intermediate results for the entire sequence length, these intermediate results are written to back memory, which negatively impacts performance. To decouple the accelerator s computational resources and local memory from the sequence length, the designs of [11], [12] optimize key components of attention such as matrix arithmetic and softmax function evaluation. In addition to data-flow and computation reordering optimizations, other techniques exploit token similarity [13], [14] to skip unnecessary computation and to reduce latency and power consumption. In-memory computation [15] has also been explored for optimizing com- putation of attention. In parallel, FlashAttention [16], [17], [18], originally de- veloped for GPUs, has emerged as a powerful technique for accelerating attention. By leveraging tiling and fusing online softmax computation with matrix arithmetic, it enables parallel execution and reduces memory traffic. These features allow FlashAttention to lower execution latency and simplify the processing of long sequences without compromising accuracy. In this work, we aim at preserving the favorable IO prop- erties of FlashAttention and simplify its core computation kernel. To do this, we rewrite the forward pass of FlashAtten- tion kernel used in transformer inference in a mathematical equivalent form, called FLASH-D, enabling several favorable computational features: The softmax division is hidden within the reformulated non-linear function evaluations. This eliminates the need for explicit division, either during computation [16] or as a post-processing step when following the lazy softmax approach [11], [18]. Without altering the algorithm or introducing any approx- imation, softmax is replaced by an equivalent sigmoid function of attention score differences that is inherently numerically stable. This transformation improves paral- lelism and removes the requirement to scale inputs by the maximum attention score, which is necessary otherwise for numerically stable softmax computation [19]. New possibilities emerge for reducing multiplications or memory accesses while preserving the core features of FlashAttention, which enable tiled computation to enhance locality and decrease memory traffic [16], [17]. To assess the efficiency of FLASH-D for designing FlashAttention-based hardware accelerators, we implemented the optimized FlashAttention2 kernel and FLASH-D using a fully unrolled systolic architecture, in 28nm ASIC technology. Experimental results show that the proposed approach achieves significant area and power savings, ranging from 20 28 and 16 27 , respectively, for various hidden dimension sizes arXiv:2505.14201v1 [cs.LG] 20 May 2025 and floating-point number formats when compared to parallel hardware implementation of the kernel of FlashAttention2 state-of-the-art algorithm under the same performance. II. FLASHATTENTION-BASED HARDWARE ACCELERATORS Attention is a fundamental concept in machine learning, particularly in deep learning models that utilize the transformer architecture [20], [21]. It guides transformer models to take into account only relevant context: A user query is compared to a set of key vectors leading to an attention score matrix which is used to retrieve the information the user requested from a set of value vectors. In practice, attention is applied across multiple heads in parallel [3], allowing the model to comprehend more complex relationships. Without loss of generality, we limit our descriptions to single-head attention. A. Attention Kernel For a query q and a set of key and value vectors K k1, . . . , kN and V v1, . . . , vN attention is defined as si dot( q, ki) fi esi P j esj Attn( q, K, V) X i fi vi The attention score si represents the similarity between a given query and the ith key vector computed via a dot product. To identify the most relevant tokens, the softmax function is applied to all scores corresponding to the same query vector. Softmax first exponentiates each score and then divides it by the sum of all exponentials. The final output is obtained by multiplying the attention scores of a query by all value vectors, where the contribution of each value to the output is determined by its corresponding attention score normalized via softmax. Exponentiating scores can cause infinite values that would affect the final result. To prevent this, safe softmax subtracts the maximum score from all scores, thus avoiding overflow while keeping its core properties fi esi max P j esj max. B. Baseline FlashAttention Algorithm Algorithmically, attention calculation faces one major bot- tleneck. The softmax operation should be applied across the entire sequence length. This requirement reduces the extent to which attention can be parallelized or limits the application of attention to shorter sequence lengths [22]. To overcome this limitation, FlashAttention [16], driven by the online calculation of softmax function [23], reorganized computations involved in attention kernel and enabled arbitrary tiling that reduced also memory traffic. The forward pass of baseline FlashAttentionm, which is the focus of this work, is shown in Alg. 1 using vector-oriented operations. An equivalent block-based definition can be found in [16]. At each iteration, the dot product of the query vector and a key vector yields a similarity score, denoted as si. Subsequently, mi holds the current maximum similarity score, while ℓi incrementally accumulates the sum of the expo- nentials of each si minus the present maximum score. The multiplication by emi 1 mi in the calculation of ℓi adjusts the prior maximum value used whenever the current maximum mi differs from the previous maximum mi 1. Similarly, the output vector oi is updated by adding the new value vector vi Algorithm 1 FlashAttention 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: mi max(mi 1, si) 5: ℓi ℓi 1emi 1 mi esi mi 6: oi oi 1 ℓi 1emi 1 mi ℓi vi esi mi ℓi 7: end for 8: Attn( q, K, V) oN 9: end for weighted by its softmax importance, to the adjusted preceding output vector, oi 1. The final attention vector for one query vector is returned in oN. C. Flash Attention with Lazy Softmax Instead of calculating the exponential sum and division before the multiplication with the value vectors, lazy softmax architectures [11], [13] accumulate the weighted sum and the exponent sum in parallel and perform division as a final step. To avoid the expensive division operation other approaches compute softmax in the logarithmic domain [12], [24]. si dot( q, ki) fi esi max Attn( q, K, V) P i fi vi P j esj max This postponed division approach has been adopted by FlashAttention2 algorithm [17] that is shown in Alg. 2. Algorithm 2 FlashAttention2: Lazy Softmax Division 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: mi max(mi 1, si) 5: ℓi ℓi 1emi 1 mi esi mi 6: oi oi 1emi 1 mi viesi mi 7: end for 8: Attn( q, K, V) oN ℓN 9: end for FlashAttention2 keeps the same basic principle of operation of baseline FlashAttention but removes all internal division operations (line 6 of Alg. 1) and replaces them with one final vector division at the end (line 8 of Alg. 2). In this way, it leads to a simpler design that can be more easily parallelized and implemented directly in hardware. The FlashAttention2 kernel shown in Alg. 2 involves two for loops that can be unrolled to enhance parallelism and computational throughput. While unrolling the inner loop is a valid option, it still maintains the serial dependency across the three variables that constitute attention s internal state, namely mi, ℓi, and oi. A more scalable approach is to unroll the outer loop, allowing the FlashAttention2 kernel to process multiple query vectors in parallel within the same blocks of key and value vectors. In this case, the internal state is kept independently for each query vector, thus eliminating serial dependencies. Fig. 1. A parallel hardware architecture for FlashAttention2 kernel for multiple preloaded query vectors. Fig. 1 depicts the parallel hardware structure that corre- sponds to the outer-loop unrolling architecture of FlashAt- tention2. In this setup, a block of query vectors is loaded locally, while the key vectors are sequentially provided to each block for computing the dot products [25], [26]. This results in the determination of a new maximum value and an updated running sum of exponents. The value vectors are streamed into the accelerator, allowing the output vectors for different query vectors to be updated simultaneously. After all key and value vectors are processed, a final division operation is performed to compute the attention for each query vector. The process concludes once all query vectors have been processed. FlashAttention2 eliminates the need for a full softmax hardware unit, as the exponentiations and final division are computed independently. Several methods have been proposed for performing these two non-linear operations in hardware. These approaches include piece-wise linear approximations following range reduction [19], logarithmic quantization-based transformations [27], and other approximations [28] that con- vert exponential functions into simpler powers of two, allow- ing for shift-and-add algorithms. III. FLASHATTENTION WITH HIDDEN SOFTMAX DIVISION The two algorithmic variants of FlashAttention perform identical computations. Their sole difference lies in the timing of the softmax division. In baseline FlashAttention, the di- vision is executed incrementally during output accumulation, whereas in FlashAttention2, it is rescheduled to the end of the computation. In this work, we aim at rewriting baseline FlashAttention to effectively hide the softmax division within non-linear function evaluations, without resorting to any approximations. By doing so, we intend to introduce a novel, yet equivalent, algorithmic variant of FlashAttention that facilitates energy- efficient hardware accelerators. We accomplish this transfor- mation in two steps. A. Redefine output recursion as a weighted contribution of value vectors First, we need to rewrite the recursive output computation of baseline FlashAttention. From line 5 of Alg. 1 we know that the output is computed as follows: oi oi 1 ℓi 1emi 1 mi ℓi vi esi mi ℓi (1) From the definition of ℓi ℓi 1emi 1 mi esi mi in line 4 of Alg. 1 we get that: ℓi 1emi 1 mi ℓi esi mi Replacing this term to the output recursion (1) we get that oi oi 1 ℓi esi mi ℓi vi esi mi ℓi oi 1 1 esi mi ℓi vi esi mi ℓi (2) Setting wi esi mi ℓi (3) and replacing it in (2) we get an equivalent, but simplified form, for the recursive computation of the output in baseline FlashAttention: oi oi 1(1 wi) viwi (4) This newly rewritten form of (1) reveals that, effectively, the output is a weighted sum of the previously accumulated output, oi 1, and the new value vector, vi. The contribution of each part is determined by the value of wi which, according to (3), corresponds to an incrementally formed softmax since it includes the exponent of each attention score divided by the running sum of exponents accumulated in ℓi. Thus, by definition, wi is always positive and less than 1. B. Recursive weight computation Next, we aim to establish a recursive relation that links the current weight, wi to the weight from the previous iteration, wi 1 and the corresponding attention scores. Our goal is to use the planned recursive relation exclusively, replacing the recursive computation of ℓi and mi, as well. To do so, based on the definition of weight wi in (3), we can equivalently write that wi 1 esi 1 mi 1 ℓi 1. Solving for ℓi and ℓi 1, respectively, we get that: ℓi esi mi wi ℓi 1 esi 1 mi 1 wi 1 (5) Replacing ℓi and ℓi 1 of (5) in the recursive equation used to compute the sum of exponents ℓi ℓi 1emi 1 mi esi mi (repeated from line 4 of Alg. 1) we get that: esi mi wi esi 1 mi 1 wi 1 emi 1 mi esi mi (6) Simple algebraic manipulation allows us to remove the con- tribution of mi 1 leading to: esi mi wi esi 1 mi wi 1 esi mi (7) The term e mi is common to all terms in (7) and it can be simplified leading to: esi wi esi 1 wi 1 esi (8) At this point, we have expressed wi recursively, depending only on the previous weight wi 1 and neighbor attention scores si and si 1 of the same query vector with respect to two consecutive key vectors. Furthermore, the need to compute the running maximum attention score is also removed. In Section III-C, we will show that this has no negative effects on the numerical stability of computing wi. To clarify the structure of wi in (8), we first factor out esi and simplify both sides. This transforms (8) into 1 wi esi 1 si wi 1 1 rearranging the above fraction yields wi wi 1 wi 1 esi 1 si . (9) According to (3), wi is always positive. Therefore, we can replace wi 1 in (9) with eln wi 1 and rewrite it as follows: wi eln wi 1 eln wi 1 esi 1 si 1 1 esi 1 si ln wi 1 1 1 e (si si 1 ln wi 1) (10) Eq. (10) reveals that weight wi is the result of applying sigmoid function σ(x) 1 (1 e x) to the difference of attention scores si si 1 achieved by the same query vector for consecutive key vectors incremented (skewed) by the natural logarithm of previous weight wi 1. In other words, wi σ(si si 1 ln wi 1) (11) C. FLASH-D: Hiding Softmax Division in FlashAttetion The sigmoid-based computation of the newly defined weights in (11) and the revised recursive computation of the output derived in (4) facilitates a more compact reformulation of the FlashAttention kernel that is shown in Alg. 3. For each attention score si, a new weight wi is computed based on previous attention score si 1 and weight wi 1, that in turn determines how much the new value vector vi would affect the accumulation at the output oi. In the first iteration, the weight is set equal to 1. The output is gradually formed following the FlashAttention paradigm. For instance, following the evolution of the inner loop of Alg. 3, w1 is set to 1 that leads to o1 v1. The first value vector v1 will be weighted by its corresponding exponential in the next iteration. Specifically, in the next iteration, w2 1 (1 e (s2 s1)) es2 (es1 es2), which gives 1 w2 es1 (es1 es2). These coefficients lead to an incremental output o2 es1 v1 (es1 es2) es2 v2 (es1 es2). As the iterations progress each vi is multiplied to the appro- priate exponential, while the sum of exponentials is growing including more terms esi at the denominator of each fraction. Computation is completed in Nth step with oN, when all key and value vectors have been processed. Algorithm 3 FLASH-D: Division hidden in sigmoid function 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: if i 1 then 5: wi σ(si si 1 ln wi 1) 6: else 7: wi 1 8: end if 9: oi oi 1(1 wi) vi wi 10: end for 11: Attn( q, K, V) oN 12: end for The proposed Alg.3 is a one-to-one equivalent of the base- line FlashAttention (Alg.1), derived through mathematical reformulation without introducing any approximations at any stage. The incremental division by the sum of exponents in the baseline FlashAttention is effectively hidden within the sigmoid function, thereby merging it with the correspond- ing exponential function evaluations. Even without reducing each attention score si by the max- imum attention score across all key vectors, the numerical stability of FLASH-D is guaranteed. This is because we can completely avoid situations that would lead to exponential overflow and instead return the default correct values for wi. In fact, the cases that could cause exponential overflows correspond to a range of attention score differences si si 1, where computing the weight function becomes meaningless. Fig. 2. Weight wi function for various values of consecutive attention score differences si si 1. The four weight graphs correspond to four different values of the previous weight wi 1. To clarify this argument we plot in Fig. 2 weight wi for different attention score differences si si 1. The graphs correspond to four distinct values of the previous weight. Since wi is derived using the sigmoid function, its dynamic range is always between 0 and 1. The leftmost graph represents wi 1 0.99, which closely follows the standard sigmoid function, as ln wi 1 is effectively zero. As wi 1 decreases, the weight function shifts to the right. In all cases, it is evident that when si si 1 falls outside the range [-6,11], there is no need to compute the weight function explicitly. In such cases, wi will be very close to either 0 or 1, allowing it to be set directly to the smallest or largest values within (0, 1) by default. Consequently, for values of si si 1 outside the range [-6,11], the exponential within the sigmoid function is skipped, thus preventing any overflow condition. Additionally, in such cases, output update can be sim- plified. When si si 1 6, wi approaches 0, mean- ing the output vector oi remains unchanged (see line 9 of Alg. 3), eliminating the need of loading of vi and performing any for vector multiplication or addition. Conversely, when si si 1 11, wi approaches 1 by default, causing the output vector to effectively forget previous value contributions and update only with the new value vector vi. In this scenario, multiplication and addition operations are also bypassed. IV. HARDWARE ARCHITECTURE OF FLASH-D Flash-D preserves all the characteristics of FlashAttention variants while embedding the softmax division within the sigmoid non-linear function, ensuring numerical stability is maintained. This transformation greatly simplifies the hard- ware implementation of FLASH-D. A. Overall Organization As illustrated in Fig.3, the hardware design of FLASH- D follows the same overall architecture of FlashAttention2 (Fig.1). Both approaches share the same dataflow, processing multiple query vectors in parallel in an unrolled manner, driven by the same key and value vectors. The primary differences lie in three key areas. First, the division at the output of FlashAttention2 kernel is not required in Flash-D. While division is neither removed nor approximated, it is implemented incrementally during the computation of each weight wi for each query vector. Second, the running sum-of-exponents ℓi and the maximum attention score mi used in FlashAttention2, as shown in Fig. 1, are eliminated. The running sum-of-exponents ℓi is implicitly embedded in the computation of each weight (see Eq. (3)), so it does not need to be explicitly computed. Additionally, the maximum value is no longer necessary, as numerical stability is maintained by ensuring that the attention score difference remains within the active region of [-6,11], where the sigmoid function can be effectively computed. Third, the output computation module in the FLASH-D hardware requires one vector adder, one subtractor, and one multiplier, compared to the two multipliers and one adder found in FlashAttention2. This hardware simplification, where one multiplier is replaced by a subtractor, is made possible by the new weighted definition of the output recursion (line 9 of Alg. 3), which can be equivalently written as follows: oi oi 1(1 wi) vi wi oi 1 ( vi oi 1) wi (12) B. Non-linear function evaluation in FLASH-D The non-linear function evaluations in FLASH-D involve the sigmoid and natural logarithm functions, both of which are well-researched and have various hardware implementations available. Without focusing on any specific simplifications, this work implements both functions using standard piece-wise linear (PWL) approximations. This design choice is well justified for both functions. The sigmoid function has a well-defined structure and output range, Fig. 3. A parallel hardware architecture for FLASH-D kernel for multiple preloaded query vectors. making it a natural fit for the PWL approach. Furthermore, in FLASH-D the input dynamic range is constrained to [-6, 11], so no computation is required outside this range. The natural logarithm function is used solely to compute the natural logarithm of the previous weight, i.e., ln wi 1, which has a narrow input dynamic range of (0,1). As a result, there is no need to compute a generic logarithm; instead, we require one that consistently returns a negative result that follows the value of the previous weight. Again, the PWL approach proves to be a suitable choice for this case. In both cases, we approximated the functions using 8 line segments. The coefficients of each segment are produced via pwlf a Python-based PWL-fit optimization library [29]. V. EVALUATION The experimental evaluation has two parts. First, we high- light the area and power benefits provided by the hardware im- plementation of FLASH-D (Fig. 3) compared to the optimized hardware implementation of the FlashAttention2 kernel shown in Fig. 1. Second, our goal is to quantify how often the weight and output update can be skipped in real LLM applications using FLASH-D as the forward pass without affecting the outcome of FlashAttention. A. Hardware complexity To evaluate the hardware complexity of the two designs, we implemented the main block shown in the foreground of Figs. 1 and 3 for various hidden dimension sizes (d) and for two reduced-precision floating-point formats: BFloat16 [30] and FP8-E4M3 [31]. In practice, the total cost of the unrolled hardware serving multiple query vectors in parallel will be the cost for one query multiplied by the number of parallel query vectors served. The query vectors are preloaded separately in the architecture, while the key and value vectors are loaded and broadcasted to all parallel blocks. For both FLASH-D and FlashAttention2 kernels, we assume that we can read from local memories in each cycle one key and one value vector of d elements each. Fig. 4. The hardware area at 28 nm for FLASH-D and FlashAttention2 kernel for computing attention of a single query using BFloat16 and FP8- E4M3 floating-point formats, across different hidden dimension lengths. Fig. 5. The average power for FLASH-D and FlashAttention2 kernel for computing attention of a single query using BFloat16 and FP8-E4M3 floating- point formats, across different hidden dimension lengths. Memory and IO power is not included since it is identical to both designs. Both hardware blocks were implemented in C 1 and synthesized into Verilog using Catapult HLS with a 28-nm standard-cell library. For verifying the correctness of the C code, we integrated it to llama2.c [32] and we received exactly the same replies as the original implementation for all examined queries. Both designs operate at the same pipelined latency with a clock frequency of 500 MHz. Latency depends on the size of the hidden dimension, requiring 8, 10, and 12 cy- cles for d {16, 64, 256} elements, respectively. Verilog was synthesized using the Cadence digital implementation flow, while power consumption was estimated with the PowerPro power analysis and optimization tool. The reported power consumption represents the average power measured after executing attention kernels for various Large Language Models and benchmarks from PromptBench. Figs. 4 and 5 show the area and power of the proposed FLASH-D hardware and the parallel hardware implementation of the FlashAttention2 computation kernel, for the two ex- amined reduced precision floating-point formats and different sizes of the hidden dimension. Power estimation excludes memory power and focuses solely on the average power consumption of the computation kernel. The memory power in both cases is expected to be identical, as both approaches implement the same FlashAttention algorithm using the same computation order and data flows. The difference lies solely on how the computation kernel is executed internally. As shown in Fig. 4, FLASH-D reduces the hardware area by more than 20 in all examined cases. These savings are a direct result of the restructured FlashAttention kernel. 1Publicly available on TABLE I PERCENTAGE OF SKIPPED OUTPUT UPDATES DURING INFERENCE ON DIFFERENT NLP BENCHMARKS LLM Benchmarks CSQA GSM8K QASC MMLU Date Object Tracking Microsoft 0.8 1.7 2.2 2 1.5 2 Phi-3-mini-4k-instruct DeepSeek 2.5 2.0 2.2 2.7 2.4 2.8 Qwen-1.5B Meta 1.8 1.6 2.6 2.3 1.6 2.3 Llama-3.1-1B Google 1.2 0.5 0.51 1.4 0.8 0.83 Gemma2-2B Standalone division is eliminated and fused within the sigmoid PWL function evaluation, effectively merging the exponential and division operations. One vector multiplier is saved in the output update module, and the sum-of-exponents and maximum logic are entirely removed. The reduction in hardware complexity also improves power consumption, which is reduced by more than 16 on average across all examined cases. Power savings are expected to in- crease further, as the computation-skipping criterion developed in FLASH-D would save additional memory power, which has not been quantified in the presented analysis. B. How often output update can be simplified To quantify how often the attention score differences fall outside the range [-6,11] described in Section III-C, and thereby simplify the output update in FLASH-D (line 9 in Alg.3), we implemented the FLASH-D kernel in Python and integrated it into the forward pass of various contemporary LLM models available on HuggingFace [33]. We then per- formed inference on these LLMs using the utilities and bench- marks provided by Microsoft s PromptBench workflow [34]. The results for each case are summarized in Table I. In all cases, there is small percentage of cases that output update can be simplified with either keeping the previous computed output or loading the new value vector without any further calculations. This percentage, even if small, is always a win scenario and does not represent any tradeoff across energy savings vs application-level performance. In the future, we plan to replace this pessimistic and static range check with an adaptive criterion that includes both the range of attention score differences and the value of the previous weight to decide when output computation can be simplified. VI. CONCLUSIONS In this work, our target was to simplify the computational kernel of FlashAttention by hiding softmax division inside sigmoid non-linear function evaluation and reduce unneces- sary multiplications in output computation. This reformulation achieved these goals without compromising numerical stability and without negatively affecting the favorable IO and memory- access properties of FlashAttention algorithm. The proposed approach reduces the area and power of the hardware compu- tational kernel by 22.8 and 20.3 , on average, respectively, when compared to the parallel hardware implementation of FlashAttention2 state-of-the-art algorithm. Any other approach that simplifies attention mechanism is orthogonal to FLASH-D and can be applied for increasing efficiency further. REFERENCES [1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [2] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Intern. Conf. on Neural Information Processing Systems (NIPS), 2017, p. 6000 6010. [4] I. Beltagy, M. E. Peters, and A. Cohan, Longformer: The long- document transformer, arXiv preprint arXiv:2004.05150, 2020. [5] R. Child, S. Gray, A. Radford, and I. Sutskever, Generating long sequences with sparse transformers, arXiv preprint arXiv:1904.10509, 2019. [6] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, Transformers are rnns: Fast autoregressive transformers with linear attention, in Intern. conference on machine learning, 2020, pp. 5156 5165. [7] N. Tang, M. Fu, K. Zhu, and J. Wu, Low-rank attention side-tuning for parameter-efficient fine-tuning, arXiv preprint arXiv:2402.04009, 2024. [8] T. J. Ham, S. J. Jung, S. Kim, Y. H. Oh, Y. Park, Y. Song, J.-H. Park, S. Lee, K. Park, J. W. Lee, and D.-K. Jeong, A3: Accelerating attention mechanisms in neural networks with approximation, in IEEE Intern. Symp. on High-Performance Computer Architecture (HPCA), 2020, p. 328 341. [9] B. Keller, R. Venkatesan, S. Dai, S. G. Tell, B. Zimmer, C. Sakr, W. J. Dally, C. T. Gray, and B. Khailany, A 95.6-TOPS W deep learning inference accelerator with per-vector scaled 4-bit quantization in 5 nm, IEEE Journal of Solid-State Circuits, vol. 58, no. 4, p. 1129 1141, 2023. [10] S. Lu, M. Wang, S. Liang, J. Lin, and Z. Wang, Hardware accelerator for multi-head attention and position-wise feed-forward in the trans- former, in IEEE Intern. System-on-Chip Conference (SOCC), 2020, pp. 84 89. [11] H. Jang, J. Kim, J.-E. Jo, J. Lee, and J. Kim, Mnnfast: a fast and scalable system architecture for memory-augmented neural networks, in Intern. Symp. on Computer Architecture (ISCA), 2019, p. 250 263. [12] Z. Wang, G. Wang, and G. He, COSA plus: Enhanced co-operative systolic arrays for attention mechanism in transformers, IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 44, no. 2, p. 723 736, 2025. [13] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung, and J. W. Lee, ELSA: Hardware-software co-design for efficient, lightweight self- attention mechanism in neural networks, in Intern. Symp. on Computer Architecture (ISCA), 2021, p. 692 705. [14] Z. Song, C. Qi, Y. Yao, P. Zhou, Y. Zi, N. Wang, and X. Liang, TSAcc: An efficient tempo-spatial similarity aware accelerator for attention acceleration, in ACM IEEE Design Automation Conference, 2024. [15] S. Sridharan, J. R. Stevens, K. Roy, and A. Raghunathan, X-former: In-memory acceleration of transformers, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 31, no. 8, pp. 1223 1233, 2023. [16] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R e, Flashattention: Fast and memory-efficient exact attention with IO-awareness, Advances in neural information processing systems, vol. 35, pp. 16 344 16 359, 2022. [17] T. Dao, Flashattention-2: Faster attention with better parallelism and work partitioning, arXiv preprint arXiv:2307.08691, 2023. [18] M. N. Rabe and C. Staats, Self-attention does not need O(n2) memory, arXiv preprint arXiv:2112.05682, 2021. [19] N. A. Koca, A. T. Do, and C.-H. Chang, Hardware-efficient softmax approximation for self-attention networks, in Intern. Symp. on Circuits and Systems (ISCAS), 2023, p. 1 5. [20] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, p. 9, 2019. [21] A. Hatamizadeh, G. Heinrich, H. Yin, A. Tao, J. M. Alvarez, J. Kautz, and P. Molchanov, Fastervit: Fast vision transformers with hierarchical attention, in Intern. Conf. on Learning Representations (ICLR), 2024. [22] S. Liu, G. Tao, Y. Zou, D. Chow, Z. Fan, K. Lei, B. Pan, D. Sylvester, G. Kielian, and M. Saligane, Consmax: Hardware-friendly alternative softmax with learnable parameters, arXiv preprint arXiv:2402.10930, 2024. [23] M. Milakov and N. Gimelshein, Online normalizer calculation for softmax, arXiv preprint arXiv:1805.02867, 2018. [24] T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y. Yang, M. Donato, V. Sanh, P. Whatmough, A. M. Rush, D. Brooks, and G.-Y. Wei, Edge- BERT: sentence-level energy optimizations for latency-aware multi-task NLP inference, in Intern. Symposium on Microarchitecture (MICRO), 2021, p. 830 844. [25] K. Alexandridis and G. Dimitrakopoulos, Online alignment and ad- dition in multiterm floating-point adders, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 33, no. 4, pp. 1182 1186, 2025. [26] D. Filippas, C. Nicopoulos, and G. Dimitrakopoulos, Templatized fused vector floating-point dot product for high-level synthesis, Journal of Low Power Electronics and Applications, vol. 12, no. 4, p. 56, 2022. [27] W. Wang, S. Zhou, W. Sun, P. Sun, and Y. Liu, SOLE: hardware- software co-design of softmax and layernorm for efficient transformer inference, in IEEE ACM Intern. Conference on Computer Aided Design (ICCAD), 2023, p. 1 9. [28] G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino, A. Nannarelli, M. Re, and S. Span o, A pseudo-softmax function for hardware-based high speed image classification, Scientific Reports, vol. 11, 2021. [29] C. F. Jekel and G. Venter, pwlf: A Python Library for Fitting 1D Continuous Piecewise Linear Functions, 2019. [30] D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen et al., A study of bfloat16 for deep learning training, arXiv preprint arXiv:1905.12322, 2019. [31] P. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisen- thwaite, S. Ha, A. Heinecke, P. Judd, J. Kamalu et al., Fp8 formats for deep learning, arXiv preprint arXiv:2209.05433, 2022. [32] A. Karpathy, llama2.c, 2023. [33] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, Transformers: State-of-the-art natural language processing, in Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020, pp. 38 45. [34] K. Zhu, Q. Zhao, H. Chen, J. Wang, and X. Xie, Promptbench: A unified library for evaluation of large language models, arXiv preprint arXiv:2312.07910, 2023.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nFLASH-D: FlashAttention with Hidden Softmax Division Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos Integrated Circuits Lab, Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece Abstract The transformer s attention mechanism has revolu- tionized AI and machine learning, with its efficient computation being crucial to its performance. However, calculating attention involves matrix operations interspersed with softmax rescaling, which inherently slows down computation and requires pro- cessing the entire input sequence. Building on online softmax computation, FlashAttention integrates softmax calculation with matrix arithmetic, enabling tiled computation independent of sequence length. While optimized for GPUs, FlashAttention s simplicity makes it amenable to direct hardware acceleration. This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a mathematically equivalent, yet simplified, formulation that achieves: (a) hiding softmax division within other non- linear function evaluations; (b) inherently numerically stable computation of exponentials, eliminating the need for maximum value subtraction; and (c) a reduction in computational cost without introducing numerical approximations to the FlashAtten- tion kernel. Importantly, the essential FlashAttention properties that facilitate efficient tiled implementation are fully preserved. Hardware implementation results at 28nm demonstrate that this proposed formulation achieves a 22.8 reduction in area and a 20.3 reduction in power, on average, compared to state-of- the-art parallel hardware architectures without any performance penalty. Index Terms AI Hardware Accelerators, Transformers, FlashAttention, Energy Efficiency I. INTRODUCTION Current state-of-the-art ML and AI systems are character- ized by deep learning models with billions of parameters, achieving human-level performance in tasks like image recog- nition [1] and natural language processing [2]. A key innova- tion driving this progress is the attention mechanism [3], which allows models to focus on relevant parts of the input data. This has led to breakthroughs like transformer networks and large language models that are revolutionizing how machines understand and interact with the world. The increasing demand for processing long sequences in transformer models has exposed the complexity of the at- tention mechanism [4]. This computational burden, makes inference (and training) prohibitively expensive for extended context lengths.\n\n--- Segment 2 ---\nThe increasing demand for processing long sequences in transformer models has exposed the complexity of the at- tention mechanism [4]. This computational burden, makes inference (and training) prohibitively expensive for extended context lengths. Traditional attention calculates pairwise sim- ilarities between all tokens, leading to a massive number of operations and memory traffic. To reduce the number of operations either by approximating the full attention matrix or by selectively focusing on the most relevant parts of the input sequence, techniques such as sparse attention [5], linear attention [6], and low-rank attention [7] have been explored, each balancing accuracy and computational efficiency. Early attention hardware accelerators optimize which attention-relevant matrices, such as key, value and queries, remain stationary in local SRAMs and which are streamed to compute the attention scores [8], [9], [10]. If the accelerator cannot buffer and compute intermediate results for the entire sequence length, these intermediate results are written to back memory, which negatively impacts performance. To decouple the accelerator s computational resources and local memory from the sequence length, the designs of [11], [12] optimize key components of attention such as matrix arithmetic and softmax function evaluation. In addition to data-flow and computation reordering optimizations, other techniques exploit token similarity [13], [14] to skip unnecessary computation and to reduce latency and power consumption. In-memory computation [15] has also been explored for optimizing com- putation of attention. In parallel, FlashAttention [16], [17], [18], originally de- veloped for GPUs, has emerged as a powerful technique for accelerating attention. By leveraging tiling and fusing online softmax computation with matrix arithmetic, it enables parallel execution and reduces memory traffic. These features allow FlashAttention to lower execution latency and simplify the processing of long sequences without compromising accuracy. In this work, we aim at preserving the favorable IO prop- erties of FlashAttention and simplify its core computation kernel. To do this, we rewrite the forward pass of FlashAtten- tion kernel used in transformer inference in a mathematical equivalent form, called FLASH-D, enabling several favorable computational features: The softmax division is hidden within the reformulated non-linear function evaluations. This eliminates the need for explicit division, either during computation [16] or as a post-processing step when following the lazy softmax approach [11], [18].\n\n--- Segment 3 ---\nTo do this, we rewrite the forward pass of FlashAtten- tion kernel used in transformer inference in a mathematical equivalent form, called FLASH-D, enabling several favorable computational features: The softmax division is hidden within the reformulated non-linear function evaluations. This eliminates the need for explicit division, either during computation [16] or as a post-processing step when following the lazy softmax approach [11], [18]. Without altering the algorithm or introducing any approx- imation, softmax is replaced by an equivalent sigmoid function of attention score differences that is inherently numerically stable. This transformation improves paral- lelism and removes the requirement to scale inputs by the maximum attention score, which is necessary otherwise for numerically stable softmax computation [19]. New possibilities emerge for reducing multiplications or memory accesses while preserving the core features of FlashAttention, which enable tiled computation to enhance locality and decrease memory traffic [16], [17]. To assess the efficiency of FLASH-D for designing FlashAttention-based hardware accelerators, we implemented the optimized FlashAttention2 kernel and FLASH-D using a fully unrolled systolic architecture, in 28nm ASIC technology. Experimental results show that the proposed approach achieves significant area and power savings, ranging from 20 28 and 16 27 , respectively, for various hidden dimension sizes arXiv:2505.14201v1 [cs.LG] 20 May 2025 and floating-point number formats when compared to parallel hardware implementation of the kernel of FlashAttention2 state-of-the-art algorithm under the same performance. II. FLASHATTENTION-BASED HARDWARE ACCELERATORS Attention is a fundamental concept in machine learning, particularly in deep learning models that utilize the transformer architecture [20], [21]. It guides transformer models to take into account only relevant context: A user query is compared to a set of key vectors leading to an attention score matrix which is used to retrieve the information the user requested from a set of value vectors. In practice, attention is applied across multiple heads in parallel [3], allowing the model to comprehend more complex relationships. Without loss of generality, we limit our descriptions to single-head attention. A. Attention Kernel For a query q and a set of key and value vectors K k1, . . . , kN and V v1, . . .\n\n--- Segment 4 ---\n. . , vN attention is defined as si dot( q, ki) fi esi P j esj Attn( q, K, V) X i fi vi The attention score si represents the similarity between a given query and the ith key vector computed via a dot product. To identify the most relevant tokens, the softmax function is applied to all scores corresponding to the same query vector. Softmax first exponentiates each score and then divides it by the sum of all exponentials. The final output is obtained by multiplying the attention scores of a query by all value vectors, where the contribution of each value to the output is determined by its corresponding attention score normalized via softmax. Exponentiating scores can cause infinite values that would affect the final result. To prevent this, safe softmax subtracts the maximum score from all scores, thus avoiding overflow while keeping its core properties fi esi max P j esj max. B. Baseline FlashAttention Algorithm Algorithmically, attention calculation faces one major bot- tleneck. The softmax operation should be applied across the entire sequence length. This requirement reduces the extent to which attention can be parallelized or limits the application of attention to shorter sequence lengths [22]. To overcome this limitation, FlashAttention [16], driven by the online calculation of softmax function [23], reorganized computations involved in attention kernel and enabled arbitrary tiling that reduced also memory traffic. The forward pass of baseline FlashAttentionm, which is the focus of this work, is shown in Alg. 1 using vector-oriented operations. An equivalent block-based definition can be found in [16]. At each iteration, the dot product of the query vector and a key vector yields a similarity score, denoted as si. Subsequently, mi holds the current maximum similarity score, while ℓi incrementally accumulates the sum of the expo- nentials of each si minus the present maximum score. The multiplication by emi 1 mi in the calculation of ℓi adjusts the prior maximum value used whenever the current maximum mi differs from the previous maximum mi 1.\n\n--- Segment 5 ---\nSubsequently, mi holds the current maximum similarity score, while ℓi incrementally accumulates the sum of the expo- nentials of each si minus the present maximum score. The multiplication by emi 1 mi in the calculation of ℓi adjusts the prior maximum value used whenever the current maximum mi differs from the previous maximum mi 1. Similarly, the output vector oi is updated by adding the new value vector vi Algorithm 1 FlashAttention 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: mi max(mi 1, si) 5: ℓi ℓi 1emi 1 mi esi mi 6: oi oi 1 ℓi 1emi 1 mi ℓi vi esi mi ℓi 7: end for 8: Attn( q, K, V) oN 9: end for weighted by its softmax importance, to the adjusted preceding output vector, oi 1. The final attention vector for one query vector is returned in oN. C. Flash Attention with Lazy Softmax Instead of calculating the exponential sum and division before the multiplication with the value vectors, lazy softmax architectures [11], [13] accumulate the weighted sum and the exponent sum in parallel and perform division as a final step. To avoid the expensive division operation other approaches compute softmax in the logarithmic domain [12], [24]. si dot( q, ki) fi esi max Attn( q, K, V) P i fi vi P j esj max This postponed division approach has been adopted by FlashAttention2 algorithm [17] that is shown in Alg. 2. Algorithm 2 FlashAttention2: Lazy Softmax Division 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: mi max(mi 1, si) 5: ℓi ℓi 1emi 1 mi esi mi 6: oi oi 1emi 1 mi viesi mi 7: end for 8: Attn( q, K, V) oN ℓN 9: end for FlashAttention2 keeps the same basic principle of operation of baseline FlashAttention but removes all internal division operations (line 6 of Alg.\n\n--- Segment 6 ---\n2. Algorithm 2 FlashAttention2: Lazy Softmax Division 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: mi max(mi 1, si) 5: ℓi ℓi 1emi 1 mi esi mi 6: oi oi 1emi 1 mi viesi mi 7: end for 8: Attn( q, K, V) oN ℓN 9: end for FlashAttention2 keeps the same basic principle of operation of baseline FlashAttention but removes all internal division operations (line 6 of Alg. 1) and replaces them with one final vector division at the end (line 8 of Alg. 2). In this way, it leads to a simpler design that can be more easily parallelized and implemented directly in hardware. The FlashAttention2 kernel shown in Alg. 2 involves two for loops that can be unrolled to enhance parallelism and computational throughput. While unrolling the inner loop is a valid option, it still maintains the serial dependency across the three variables that constitute attention s internal state, namely mi, ℓi, and oi. A more scalable approach is to unroll the outer loop, allowing the FlashAttention2 kernel to process multiple query vectors in parallel within the same blocks of key and value vectors. In this case, the internal state is kept independently for each query vector, thus eliminating serial dependencies. Fig. 1. A parallel hardware architecture for FlashAttention2 kernel for multiple preloaded query vectors. Fig. 1 depicts the parallel hardware structure that corre- sponds to the outer-loop unrolling architecture of FlashAt- tention2. In this setup, a block of query vectors is loaded locally, while the key vectors are sequentially provided to each block for computing the dot products [25], [26]. This results in the determination of a new maximum value and an updated running sum of exponents. The value vectors are streamed into the accelerator, allowing the output vectors for different query vectors to be updated simultaneously. After all key and value vectors are processed, a final division operation is performed to compute the attention for each query vector. The process concludes once all query vectors have been processed. FlashAttention2 eliminates the need for a full softmax hardware unit, as the exponentiations and final division are computed independently.\n\n--- Segment 7 ---\nThe process concludes once all query vectors have been processed. FlashAttention2 eliminates the need for a full softmax hardware unit, as the exponentiations and final division are computed independently. Several methods have been proposed for performing these two non-linear operations in hardware. These approaches include piece-wise linear approximations following range reduction [19], logarithmic quantization-based transformations [27], and other approximations [28] that con- vert exponential functions into simpler powers of two, allow- ing for shift-and-add algorithms. III. FLASHATTENTION WITH HIDDEN SOFTMAX DIVISION The two algorithmic variants of FlashAttention perform identical computations. Their sole difference lies in the timing of the softmax division. In baseline FlashAttention, the di- vision is executed incrementally during output accumulation, whereas in FlashAttention2, it is rescheduled to the end of the computation. In this work, we aim at rewriting baseline FlashAttention to effectively hide the softmax division within non-linear function evaluations, without resorting to any approximations. By doing so, we intend to introduce a novel, yet equivalent, algorithmic variant of FlashAttention that facilitates energy- efficient hardware accelerators. We accomplish this transfor- mation in two steps. A. Redefine output recursion as a weighted contribution of value vectors First, we need to rewrite the recursive output computation of baseline FlashAttention. From line 5 of Alg. 1 we know that the output is computed as follows: oi oi 1 ℓi 1emi 1 mi ℓi vi esi mi ℓi (1) From the definition of ℓi ℓi 1emi 1 mi esi mi in line 4 of Alg.\n\n--- Segment 8 ---\nFrom line 5 of Alg. 1 we know that the output is computed as follows: oi oi 1 ℓi 1emi 1 mi ℓi vi esi mi ℓi (1) From the definition of ℓi ℓi 1emi 1 mi esi mi in line 4 of Alg. 1 we get that: ℓi 1emi 1 mi ℓi esi mi Replacing this term to the output recursion (1) we get that oi oi 1 ℓi esi mi ℓi vi esi mi ℓi oi 1 1 esi mi ℓi vi esi mi ℓi (2) Setting wi esi mi ℓi (3) and replacing it in (2) we get an equivalent, but simplified form, for the recursive computation of the output in baseline FlashAttention: oi oi 1(1 wi) viwi (4) This newly rewritten form of (1) reveals that, effectively, the output is a weighted sum of the previously accumulated output, oi 1, and the new value vector, vi. The contribution of each part is determined by the value of wi which, according to (3), corresponds to an incrementally formed softmax since it includes the exponent of each attention score divided by the running sum of exponents accumulated in ℓi. Thus, by definition, wi is always positive and less than 1. B. Recursive weight computation Next, we aim to establish a recursive relation that links the current weight, wi to the weight from the previous iteration, wi 1 and the corresponding attention scores. Our goal is to use the planned recursive relation exclusively, replacing the recursive computation of ℓi and mi, as well. To do so, based on the definition of weight wi in (3), we can equivalently write that wi 1 esi 1 mi 1 ℓi 1.\n\n--- Segment 9 ---\nOur goal is to use the planned recursive relation exclusively, replacing the recursive computation of ℓi and mi, as well. To do so, based on the definition of weight wi in (3), we can equivalently write that wi 1 esi 1 mi 1 ℓi 1. Solving for ℓi and ℓi 1, respectively, we get that: ℓi esi mi wi ℓi 1 esi 1 mi 1 wi 1 (5) Replacing ℓi and ℓi 1 of (5) in the recursive equation used to compute the sum of exponents ℓi ℓi 1emi 1 mi esi mi (repeated from line 4 of Alg. 1) we get that: esi mi wi esi 1 mi 1 wi 1 emi 1 mi esi mi (6) Simple algebraic manipulation allows us to remove the con- tribution of mi 1 leading to: esi mi wi esi 1 mi wi 1 esi mi (7) The term e mi is common to all terms in (7) and it can be simplified leading to: esi wi esi 1 wi 1 esi (8) At this point, we have expressed wi recursively, depending only on the previous weight wi 1 and neighbor attention scores si and si 1 of the same query vector with respect to two consecutive key vectors. Furthermore, the need to compute the running maximum attention score is also removed. In Section III-C, we will show that this has no negative effects on the numerical stability of computing wi. To clarify the structure of wi in (8), we first factor out esi and simplify both sides. This transforms (8) into 1 wi esi 1 si wi 1 1 rearranging the above fraction yields wi wi 1 wi 1 esi 1 si . (9) According to (3), wi is always positive. Therefore, we can replace wi 1 in (9) with eln wi 1 and rewrite it as follows: wi eln wi 1 eln wi 1 esi 1 si 1 1 esi 1 si ln wi 1 1 1 e (si si 1 ln wi 1) (10) Eq.\n\n--- Segment 10 ---\n(9) According to (3), wi is always positive. Therefore, we can replace wi 1 in (9) with eln wi 1 and rewrite it as follows: wi eln wi 1 eln wi 1 esi 1 si 1 1 esi 1 si ln wi 1 1 1 e (si si 1 ln wi 1) (10) Eq. (10) reveals that weight wi is the result of applying sigmoid function σ(x) 1 (1 e x) to the difference of attention scores si si 1 achieved by the same query vector for consecutive key vectors incremented (skewed) by the natural logarithm of previous weight wi 1. In other words, wi σ(si si 1 ln wi 1) (11) C. FLASH-D: Hiding Softmax Division in FlashAttetion The sigmoid-based computation of the newly defined weights in (11) and the revised recursive computation of the output derived in (4) facilitates a more compact reformulation of the FlashAttention kernel that is shown in Alg. 3. For each attention score si, a new weight wi is computed based on previous attention score si 1 and weight wi 1, that in turn determines how much the new value vector vi would affect the accumulation at the output oi. In the first iteration, the weight is set equal to 1. The output is gradually formed following the FlashAttention paradigm. For instance, following the evolution of the inner loop of Alg. 3, w1 is set to 1 that leads to o1 v1. The first value vector v1 will be weighted by its corresponding exponential in the next iteration. Specifically, in the next iteration, w2 1 (1 e (s2 s1)) es2 (es1 es2), which gives 1 w2 es1 (es1 es2). These coefficients lead to an incremental output o2 es1 v1 (es1 es2) es2 v2 (es1 es2). As the iterations progress each vi is multiplied to the appro- priate exponential, while the sum of exponentials is growing including more terms esi at the denominator of each fraction. Computation is completed in Nth step with oN, when all key and value vectors have been processed.\n\n--- Segment 11 ---\nAs the iterations progress each vi is multiplied to the appro- priate exponential, while the sum of exponentials is growing including more terms esi at the denominator of each fraction. Computation is completed in Nth step with oN, when all key and value vectors have been processed. Algorithm 3 FLASH-D: Division hidden in sigmoid function 1: for each query q do 2: for i 1 : N do 3: si dot( q, ki) 4: if i 1 then 5: wi σ(si si 1 ln wi 1) 6: else 7: wi 1 8: end if 9: oi oi 1(1 wi) vi wi 10: end for 11: Attn( q, K, V) oN 12: end for The proposed Alg.3 is a one-to-one equivalent of the base- line FlashAttention (Alg.1), derived through mathematical reformulation without introducing any approximations at any stage. The incremental division by the sum of exponents in the baseline FlashAttention is effectively hidden within the sigmoid function, thereby merging it with the correspond- ing exponential function evaluations. Even without reducing each attention score si by the max- imum attention score across all key vectors, the numerical stability of FLASH-D is guaranteed. This is because we can completely avoid situations that would lead to exponential overflow and instead return the default correct values for wi. In fact, the cases that could cause exponential overflows correspond to a range of attention score differences si si 1, where computing the weight function becomes meaningless. Fig. 2. Weight wi function for various values of consecutive attention score differences si si 1. The four weight graphs correspond to four different values of the previous weight wi 1. To clarify this argument we plot in Fig. 2 weight wi for different attention score differences si si 1. The graphs correspond to four distinct values of the previous weight. Since wi is derived using the sigmoid function, its dynamic range is always between 0 and 1. The leftmost graph represents wi 1 0.99, which closely follows the standard sigmoid function, as ln wi 1 is effectively zero. As wi 1 decreases, the weight function shifts to the right. In all cases, it is evident that when si si 1 falls outside the range [-6,11], there is no need to compute the weight function explicitly.\n\n--- Segment 12 ---\nAs wi 1 decreases, the weight function shifts to the right. In all cases, it is evident that when si si 1 falls outside the range [-6,11], there is no need to compute the weight function explicitly. In such cases, wi will be very close to either 0 or 1, allowing it to be set directly to the smallest or largest values within (0, 1) by default. Consequently, for values of si si 1 outside the range [-6,11], the exponential within the sigmoid function is skipped, thus preventing any overflow condition. Additionally, in such cases, output update can be sim- plified. When si si 1 6, wi approaches 0, mean- ing the output vector oi remains unchanged (see line 9 of Alg. 3), eliminating the need of loading of vi and performing any for vector multiplication or addition. Conversely, when si si 1 11, wi approaches 1 by default, causing the output vector to effectively forget previous value contributions and update only with the new value vector vi. In this scenario, multiplication and addition operations are also bypassed. IV. HARDWARE ARCHITECTURE OF FLASH-D Flash-D preserves all the characteristics of FlashAttention variants while embedding the softmax division within the sigmoid non-linear function, ensuring numerical stability is maintained. This transformation greatly simplifies the hard- ware implementation of FLASH-D. A. Overall Organization As illustrated in Fig.3, the hardware design of FLASH- D follows the same overall architecture of FlashAttention2 (Fig.1). Both approaches share the same dataflow, processing multiple query vectors in parallel in an unrolled manner, driven by the same key and value vectors. The primary differences lie in three key areas. First, the division at the output of FlashAttention2 kernel is not required in Flash-D. While division is neither removed nor approximated, it is implemented incrementally during the computation of each weight wi for each query vector. Second, the running sum-of-exponents ℓi and the maximum attention score mi used in FlashAttention2, as shown in Fig. 1, are eliminated. The running sum-of-exponents ℓi is implicitly embedded in the computation of each weight (see Eq. (3)), so it does not need to be explicitly computed.\n\n--- Segment 13 ---\nThe running sum-of-exponents ℓi is implicitly embedded in the computation of each weight (see Eq. (3)), so it does not need to be explicitly computed. Additionally, the maximum value is no longer necessary, as numerical stability is maintained by ensuring that the attention score difference remains within the active region of [-6,11], where the sigmoid function can be effectively computed. Third, the output computation module in the FLASH-D hardware requires one vector adder, one subtractor, and one multiplier, compared to the two multipliers and one adder found in FlashAttention2. This hardware simplification, where one multiplier is replaced by a subtractor, is made possible by the new weighted definition of the output recursion (line 9 of Alg. 3), which can be equivalently written as follows: oi oi 1(1 wi) vi wi oi 1 ( vi oi 1) wi (12) B. Non-linear function evaluation in FLASH-D The non-linear function evaluations in FLASH-D involve the sigmoid and natural logarithm functions, both of which are well-researched and have various hardware implementations available. Without focusing on any specific simplifications, this work implements both functions using standard piece-wise linear (PWL) approximations. This design choice is well justified for both functions. The sigmoid function has a well-defined structure and output range, Fig. 3. A parallel hardware architecture for FLASH-D kernel for multiple preloaded query vectors. making it a natural fit for the PWL approach. Furthermore, in FLASH-D the input dynamic range is constrained to [-6, 11], so no computation is required outside this range. The natural logarithm function is used solely to compute the natural logarithm of the previous weight, i.e., ln wi 1, which has a narrow input dynamic range of (0,1). As a result, there is no need to compute a generic logarithm; instead, we require one that consistently returns a negative result that follows the value of the previous weight. Again, the PWL approach proves to be a suitable choice for this case. In both cases, we approximated the functions using 8 line segments. The coefficients of each segment are produced via pwlf a Python-based PWL-fit optimization library [29].\n\n--- Segment 14 ---\nIn both cases, we approximated the functions using 8 line segments. The coefficients of each segment are produced via pwlf a Python-based PWL-fit optimization library [29]. V. EVALUATION The experimental evaluation has two parts. First, we high- light the area and power benefits provided by the hardware im- plementation of FLASH-D (Fig. 3) compared to the optimized hardware implementation of the FlashAttention2 kernel shown in Fig. 1. Second, our goal is to quantify how often the weight and output update can be skipped in real LLM applications using FLASH-D as the forward pass without affecting the outcome of FlashAttention. A. Hardware complexity To evaluate the hardware complexity of the two designs, we implemented the main block shown in the foreground of Figs. 1 and 3 for various hidden dimension sizes (d) and for two reduced-precision floating-point formats: BFloat16 [30] and FP8-E4M3 [31]. In practice, the total cost of the unrolled hardware serving multiple query vectors in parallel will be the cost for one query multiplied by the number of parallel query vectors served. The query vectors are preloaded separately in the architecture, while the key and value vectors are loaded and broadcasted to all parallel blocks. For both FLASH-D and FlashAttention2 kernels, we assume that we can read from local memories in each cycle one key and one value vector of d elements each. Fig. 4. The hardware area at 28 nm for FLASH-D and FlashAttention2 kernel for computing attention of a single query using BFloat16 and FP8- E4M3 floating-point formats, across different hidden dimension lengths. Fig. 5. The average power for FLASH-D and FlashAttention2 kernel for computing attention of a single query using BFloat16 and FP8-E4M3 floating- point formats, across different hidden dimension lengths. Memory and IO power is not included since it is identical to both designs. Both hardware blocks were implemented in C 1 and synthesized into Verilog using Catapult HLS with a 28-nm standard-cell library. For verifying the correctness of the C code, we integrated it to llama2.c [32] and we received exactly the same replies as the original implementation for all examined queries.\n\n--- Segment 15 ---\nBoth hardware blocks were implemented in C 1 and synthesized into Verilog using Catapult HLS with a 28-nm standard-cell library. For verifying the correctness of the C code, we integrated it to llama2.c [32] and we received exactly the same replies as the original implementation for all examined queries. Both designs operate at the same pipelined latency with a clock frequency of 500 MHz. Latency depends on the size of the hidden dimension, requiring 8, 10, and 12 cy- cles for d {16, 64, 256} elements, respectively. Verilog was synthesized using the Cadence digital implementation flow, while power consumption was estimated with the PowerPro power analysis and optimization tool. The reported power consumption represents the average power measured after executing attention kernels for various Large Language Models and benchmarks from PromptBench. Figs. 4 and 5 show the area and power of the proposed FLASH-D hardware and the parallel hardware implementation of the FlashAttention2 computation kernel, for the two ex- amined reduced precision floating-point formats and different sizes of the hidden dimension. Power estimation excludes memory power and focuses solely on the average power consumption of the computation kernel. The memory power in both cases is expected to be identical, as both approaches implement the same FlashAttention algorithm using the same computation order and data flows. The difference lies solely on how the computation kernel is executed internally. As shown in Fig. 4, FLASH-D reduces the hardware area by more than 20 in all examined cases. These savings are a direct result of the restructured FlashAttention kernel.\n\n--- Segment 16 ---\n4, FLASH-D reduces the hardware area by more than 20 in all examined cases. These savings are a direct result of the restructured FlashAttention kernel. 1Publicly available on TABLE I PERCENTAGE OF SKIPPED OUTPUT UPDATES DURING INFERENCE ON DIFFERENT NLP BENCHMARKS LLM Benchmarks CSQA GSM8K QASC MMLU Date Object Tracking Microsoft 0.8 1.7 2.2 2 1.5 2 Phi-3-mini-4k-instruct DeepSeek 2.5 2.0 2.2 2.7 2.4 2.8 Qwen-1.5B Meta 1.8 1.6 2.6 2.3 1.6 2.3 Llama-3.1-1B Google 1.2 0.5 0.51 1.4 0.8 0.83 Gemma2-2B Standalone division is eliminated and fused within the sigmoid PWL function evaluation, effectively merging the exponential and division operations. One vector multiplier is saved in the output update module, and the sum-of-exponents and maximum logic are entirely removed. The reduction in hardware complexity also improves power consumption, which is reduced by more than 16 on average across all examined cases. Power savings are expected to in- crease further, as the computation-skipping criterion developed in FLASH-D would save additional memory power, which has not been quantified in the presented analysis. B. How often output update can be simplified To quantify how often the attention score differences fall outside the range [-6,11] described in Section III-C, and thereby simplify the output update in FLASH-D (line 9 in Alg.3), we implemented the FLASH-D kernel in Python and integrated it into the forward pass of various contemporary LLM models available on HuggingFace [33]. We then per- formed inference on these LLMs using the utilities and bench- marks provided by Microsoft s PromptBench workflow [34]. The results for each case are summarized in Table I. In all cases, there is small percentage of cases that output update can be simplified with either keeping the previous computed output or loading the new value vector without any further calculations. This percentage, even if small, is always a win scenario and does not represent any tradeoff across energy savings vs application-level performance.\n\n--- Segment 17 ---\nIn all cases, there is small percentage of cases that output update can be simplified with either keeping the previous computed output or loading the new value vector without any further calculations. This percentage, even if small, is always a win scenario and does not represent any tradeoff across energy savings vs application-level performance. In the future, we plan to replace this pessimistic and static range check with an adaptive criterion that includes both the range of attention score differences and the value of the previous weight to decide when output computation can be simplified. VI. CONCLUSIONS In this work, our target was to simplify the computational kernel of FlashAttention by hiding softmax division inside sigmoid non-linear function evaluation and reduce unneces- sary multiplications in output computation. This reformulation achieved these goals without compromising numerical stability and without negatively affecting the favorable IO and memory- access properties of FlashAttention algorithm. The proposed approach reduces the area and power of the hardware compu- tational kernel by 22.8 and 20.3 , on average, respectively, when compared to the parallel hardware implementation of FlashAttention2 state-of-the-art algorithm. Any other approach that simplifies attention mechanism is orthogonal to FLASH-D and can be applied for increasing efficiency further. REFERENCES [1] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. [2] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Intern.\n\n--- Segment 18 ---\nSong, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in Intern. Conf. on Neural Information Processing Systems (NIPS), 2017, p. 6000 6010. [4] I. Beltagy, M. E. Peters, and A. Cohan, Longformer: The long- document transformer, arXiv preprint arXiv:2004.05150, 2020. [5] R. Child, S. Gray, A. Radford, and I. Sutskever, Generating long sequences with sparse transformers, arXiv preprint arXiv:1904.10509, 2019. [6] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, Transformers are rnns: Fast autoregressive transformers with linear attention, in Intern. conference on machine learning, 2020, pp. 5156 5165. [7] N. Tang, M. Fu, K. Zhu, and J. Wu, Low-rank attention side-tuning for parameter-efficient fine-tuning, arXiv preprint arXiv:2402.04009, 2024. [8] T. J. Ham, S. J. Jung, S. Kim, Y. H. Oh, Y. Park, Y. Song, J.-H. Park, S. Lee, K. Park, J. W. Lee, and D.-K. Jeong, A3: Accelerating attention mechanisms in neural networks with approximation, in IEEE Intern. Symp. on High-Performance Computer Architecture (HPCA), 2020, p. 328 341.\n\n--- Segment 19 ---\nSymp. on High-Performance Computer Architecture (HPCA), 2020, p. 328 341. [9] B. Keller, R. Venkatesan, S. Dai, S. G. Tell, B. Zimmer, C. Sakr, W. J. Dally, C. T. Gray, and B. Khailany, A 95.6-TOPS W deep learning inference accelerator with per-vector scaled 4-bit quantization in 5 nm, IEEE Journal of Solid-State Circuits, vol. 58, no. 4, p. 1129 1141, 2023. [10] S. Lu, M. Wang, S. Liang, J. Lin, and Z. Wang, Hardware accelerator for multi-head attention and position-wise feed-forward in the trans- former, in IEEE Intern. System-on-Chip Conference (SOCC), 2020, pp. 84 89. [11] H. Jang, J. Kim, J.-E. Jo, J. Lee, and J. Kim, Mnnfast: a fast and scalable system architecture for memory-augmented neural networks, in Intern. Symp. on Computer Architecture (ISCA), 2019, p. 250 263. [12] Z. Wang, G. Wang, and G. He, COSA plus: Enhanced co-operative systolic arrays for attention mechanism in transformers, IEEE Trans. on Computer-Aided Design of Integrated Circuits and Systems (TCAD), vol. 44, no. 2, p. 723 736, 2025. [13] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung, and J. W. Lee, ELSA: Hardware-software co-design for efficient, lightweight self- attention mechanism in neural networks, in Intern. Symp. on Computer Architecture (ISCA), 2021, p. 692 705. [14] Z. Song, C. Qi, Y. Yao, P. Zhou, Y. Zi, N. Wang, and X. Liang, TSAcc: An efficient tempo-spatial similarity aware accelerator for attention acceleration, in ACM IEEE Design Automation Conference, 2024.\n\n--- Segment 20 ---\n[14] Z. Song, C. Qi, Y. Yao, P. Zhou, Y. Zi, N. Wang, and X. Liang, TSAcc: An efficient tempo-spatial similarity aware accelerator for attention acceleration, in ACM IEEE Design Automation Conference, 2024. [15] S. Sridharan, J. R. Stevens, K. Roy, and A. Raghunathan, X-former: In-memory acceleration of transformers, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 31, no. 8, pp. 1223 1233, 2023. [16] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R e, Flashattention: Fast and memory-efficient exact attention with IO-awareness, Advances in neural information processing systems, vol. 35, pp. 16 344 16 359, 2022. [17] T. Dao, Flashattention-2: Faster attention with better parallelism and work partitioning, arXiv preprint arXiv:2307.08691, 2023. [18] M. N. Rabe and C. Staats, Self-attention does not need O(n2) memory, arXiv preprint arXiv:2112.05682, 2021. [19] N. A. Koca, A. T. Do, and C.-H. Chang, Hardware-efficient softmax approximation for self-attention networks, in Intern. Symp. on Circuits and Systems (ISCAS), 2023, p. 1 5. [20] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, p. 9, 2019. [21] A. Hatamizadeh, G. Heinrich, H. Yin, A. Tao, J. M. Alvarez, J. Kautz, and P. Molchanov, Fastervit: Fast vision transformers with hierarchical attention, in Intern. Conf. on Learning Representations (ICLR), 2024. [22] S. Liu, G. Tao, Y. Zou, D. Chow, Z.\n\n--- Segment 21 ---\non Learning Representations (ICLR), 2024. [22] S. Liu, G. Tao, Y. Zou, D. Chow, Z. Fan, K. Lei, B. Pan, D. Sylvester, G. Kielian, and M. Saligane, Consmax: Hardware-friendly alternative softmax with learnable parameters, arXiv preprint arXiv:2402.10930, 2024. [23] M. Milakov and N. Gimelshein, Online normalizer calculation for softmax, arXiv preprint arXiv:1805.02867, 2018. [24] T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y. Yang, M. Donato, V. Sanh, P. Whatmough, A. M. Rush, D. Brooks, and G.-Y. Wei, Edge- BERT: sentence-level energy optimizations for latency-aware multi-task NLP inference, in Intern. Symposium on Microarchitecture (MICRO), 2021, p. 830 844. [25] K. Alexandridis and G. Dimitrakopoulos, Online alignment and ad- dition in multiterm floating-point adders, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 33, no. 4, pp. 1182 1186, 2025. [26] D. Filippas, C. Nicopoulos, and G. Dimitrakopoulos, Templatized fused vector floating-point dot product for high-level synthesis, Journal of Low Power Electronics and Applications, vol. 12, no. 4, p. 56, 2022. [27] W. Wang, S. Zhou, W. Sun, P. Sun, and Y. Liu, SOLE: hardware- software co-design of softmax and layernorm for efficient transformer inference, in IEEE ACM Intern. Conference on Computer Aided Design (ICCAD), 2023, p. 1 9.\n\n--- Segment 22 ---\n[27] W. Wang, S. Zhou, W. Sun, P. Sun, and Y. Liu, SOLE: hardware- software co-design of softmax and layernorm for efficient transformer inference, in IEEE ACM Intern. Conference on Computer Aided Design (ICCAD), 2023, p. 1 9. [28] G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino, A. Nannarelli, M. Re, and S. Span o, A pseudo-softmax function for hardware-based high speed image classification, Scientific Reports, vol. 11, 2021. [29] C. F. Jekel and G. Venter, pwlf: A Python Library for Fitting 1D Continuous Piecewise Linear Functions, 2019. [30] D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen et al., A study of bfloat16 for deep learning training, arXiv preprint arXiv:1905.12322, 2019. [31] P. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisen- thwaite, S. Ha, A. Heinecke, P. Judd, J. Kamalu et al., Fp8 formats for deep learning, arXiv preprint arXiv:2209.05433, 2022. [32] A. Karpathy, llama2.c, 2023.\n\n--- Segment 23 ---\n[31] P. Micikevicius, D. Stosic, N. Burgess, M. Cornea, P. Dubey, R. Grisen- thwaite, S. Ha, A. Heinecke, P. Judd, J. Kamalu et al., Fp8 formats for deep learning, arXiv preprint arXiv:2209.05433, 2022. [32] A. Karpathy, llama2.c, 2023. [33] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, Transformers: State-of-the-art natural language processing, in Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2020, pp. 38 45. [34] K. Zhu, Q. Zhao, H. Chen, J. Wang, and X. Xie, Promptbench: A unified library for evaluation of large language models, arXiv preprint arXiv:2312.07910, 2023.\n\n