=== ORIGINAL PDF: 2506.04667v2_FlashDMoE_Fast_Distributed_MoE_in_a_Single_Kernel.pdf ===\n\nRaw text length: 72361 characters\nCleaned text length: 70290 characters\nNumber of segments: 46\n\n=== CLEANED TEXT ===\n\narXiv:2506.04667v2 [cs.DC] 9 Jun 2025 FlashDMoE: Fast Distributed MoE in a Single Kernel Osayamen Jonathan Aimuyo Cornell University Byungsoo Oh Cornell University Rachee Singh Cornell University Abstract The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, thus offering a scalable path to training massive neural networks. However, existing implementations suffer from low GPU utilization, significant latency overhead, and a fundamental inability to leverage task locality, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashDMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a single persistent GPU kernel. FlashDMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Unlike existing work, FlashDMoE obviates bulk-synchronous collectives for one-sided, device-initiated, inter-GPU (R)DMA transfers, thus unlocking payload efficiency, where we elim- inate bloated or redundant network payloads in sparsely activated layers. When evaluated on an 8-H100 GPU node with MoE models having up to 128 experts and 16K token sequences, FlashDMoE achieves up to 9 higher GPU utilization, 6 lower latency, 5.7 higher throughput, and 4 better overlap efficiency compared to state-of-the-art baselines despite using FP32 while baselines use FP16. FlashD- MoE shows that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML. FlashDMoE Comet FasterMoE Megatron DeepEP 0 20 40 60 80 100 Average SM Util ( ) 93.17 42.31 9.67 59.11 13.55 Forward Pass E 64 k 2 2 A100s is better (a) GPU SM Utilization 4K 8K 16K Number of Tokens 0 10 20 30 40 Runtime (ms) 2.1 3.7 7.0 10.3 21.3 45.3 9.9 18.9 36.9 10.0 18.9 36.9 Forward Latency E 32 k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (b) Scaling Tokens 2 4 8 of H100s 0 5 10 15 20 25 Runtime (ms) 3.3 3.4 3.7 10.3 12.6 21.3 4.2 16.6 18.9 4.4 9.0 18.9 Forward Latency T 8K E 32 k 2 is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (c) Weak Scaling across GPUs 8 16 32 64 128 Number of Experts 0 10 20 30 40 50 60 Runtime (s) Forward Latency T 16K k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (d) Across Experts Figure 1: FlashDMoE performance. Correspondence to Preprint. Under review. 1 Introduction Attention Attention T! T" O! O" All-to-All (Dispatch) E! E" E E Attention T! T" O! O" FFN Attention Gate T! T" O! O" Gate Gate E! E" E E All-to-All (Combine) (a) Transformer (b) MoE (c) Distributed MoE Figure 2: Transformer blocks (a) without MoE, (b) with MoE, and (c) with distributed MoE and expert parallelism. T, E, and O represent input tokens, experts, and output activations, respectively. State-of-the-art large language models (LLMs), including DeepSeek-v3 [1], LLama4 [2], DBRX [3] and Snowflake Arctic [4], have adopted the Mixture-of-Experts (MoE) architec- ture for its computational efficiency and strong performance across many tasks. The traditional Transformer block consists of a self-attention module followed by a dense feed-forward net- work (FFN) [5]. In contrast, MoE architectures replace this single FFN with identically sized FFNs, otherwise known as experts, (Figure 2(b)). A trainable neural network, known as a gate function, sparsely activates these experts by dy- namically routing input tokens to selected ex- perts at runtime. This increase in model param- eters (more FFNs) improves model quality with- out a corresponding increase in computational cost. Communication overheads in MoE. As MoE model sizes grow, GPU memory constraints prevent hosting all experts on a single device. The standard practice is to distribute experts across multiple GPUs using expert parallelism (EP), which requires token routing via many-to-many communication in AllToAll or AllGather [1, 4, 3, 6]. Another round of said many-to-many communication is also necessary for restoring the permuted tokens processed by experts to their original order within the sequence. Existing work has observed these communication operations taking 68 of total runtime [7, 8], during which the GPU is completely idle, unless the implementation explicitly overlaps with computation. This form of pipelining is challenging to achieve efficiently because it requires asynchronous GPU-driven communication and kernel fusion to maximize the overlap efficiency. Typically, inter-GPU communication APIs available in frameworks like PyTorch are not of this kind but instead are CPU-driven [9]. Kernel launch overheads in MoE. The efficacy of communication overlap is further limited by the overhead of launching many kernels from the CPU. Specifically, existing implementations [10 13] require launching a large number of kernels per a single layer pass (see Table 1). Frequent kernel launches negatively affect performance by: (1) creating non-deterministic kernel start times across GPUs, exacerbating straggler issues; (2) introducing unnecessary synchronization points, causing GPUs to wait on peers or the CPU before proceeding; and (3) incurring repeated global memory round trips at kernel boundaries. Although CUDA graphs [14] can partially mitigate the first issue in static workloads, they are incompatible with MoE s dynamic expert routing patterns. Addressing the remaining issues requires novel solutions, which we provide in this work through complete kernel fusion and asynchronous device-initiated communication. Attn CPU GPU NIC Gate A2A (Dispatch) E A2A (Combine) Kernel launch GPU kernel execution Remote communication Attn CPU GPU NIC Gate A2A (Dispatch) E A2A (Combine) Attn CPU GPU NIC Gate E Ours: FlashDMoE E E Gate A2A Expert A2A Expert Scale Expert Baseline MoE FlashDMoE Fused Kernel Time (milliseconds) 5 10 15 No Overlap Some Overlap Single Kernel Figure 3: Comparing FlashDMoE with state-of-the-art techniques that either do not overlap commu- nication and computation (left, top) or do some overlap (left, middle). FlashDMoE is a persistent kernel that fuses all computation and communication of the MoE operator (left, bottom). FlashDMoE implements device-initiated computation (gate, expert FFN, scale) and communication tasks (right). 2 Table 1: Kernel Fusion Comparison. Our method is the first to fully fuse the DMoE layer into a single GPU kernel. We report GPU operations from profiling with Nsight Systems. We count within a single layer (Gate Dispatch Expert Combine) on 2 A100s, where each GPU has 32 experts. Works Launched GPU Ops FlashDMoE 1 COMET [11] 33 Megatron-LM CUTLASS [12, 15] 85 Megatron-LM TE [12, 15] 261 Megatron-LM DeepEP [1] 432 DeepSpeedMoE [10] 550 1.1 Our Contributions: DMoE in a single kernel To overcome these fundamental inefficiencies in state-of-the-art MoE models, we develop FlashD- MoE, where we integrate all DMoE computation and communication tasks into a single persistent GPU kernel i.e., a kernel that remains active for the entirety of the MoE operator (Figure 3 bottom left). Instead of multiple kernel launches coordinated by the CPU, FlashDMoE requires launching only one kernel, significantly reducing the involvement of the CPU. Within the fused kernel, FlashDMoE implements a reactive programming model to achieve fine-grained parallelism and loosely coupled, non-blocking execution among tens of thousands of GPU threads. In-kernel Block scheduling and Tile parallelism. FlashDMoE implements tile-level parallelism, meaning it partitions input token matrices into smaller, independent units called tiles, which are processed by blocks but managed (scheduled and constructed) by warps. We specialize every thread block, except one, as processors to perform compute. In addition, we designate a dedicated Operating System (OS) block (4 warps) to perform administrative tasks of (1) scheduling computational work to processors (scheduler), and (2) decoding computational tasks from messages received from other GPUs (subscriber). This design allows FlashDMoE to dynamically assign tasks to GPU blocks based on readiness, ensuring that no GPU SM remains idle throughout the lifetime of the DMoE operator. FlashDMoE selects tile dimensions to maximize GPU arithmetic intensity while still benefitting from a high-degree of parallelism. Asynchronous and payload-efficient communication. By redesigning the MoE operator from the ground up, FlashDMoE resolves fundamental inefficiencies inherent in the conventional MoE execution pipeline. One notable inefficiency is token padding during communication. To simplify programming complexity and due to symmetry constraints of collective communication APIs, existing implementations have to zero-pad token payloads to match predefined buffer sizes. This occurs when tokens are asymmetrically routed to experts, resulting in GPUs receiving much less than the expected capacity. However, these null payloads waste communication bandwidth, bloat data transfer latency and may lead to unnecessary computations on null matrices in some implementations. FlashDMoE introduces payload-efficient communication by sending non-padded tokens only to GPUs with actively selected experts, conserving both communication and computational resources. Technical challenges. Realizing the single-kernel design of FlashDMoE required solving several technical challenges to achieve high performance: (1) lightweight computational dependency man- agement; (2) navigating optimal SM occupancy configurations; (3) implementing in-device BLAS operations; (4) minimizing inter- and intra-device synchronization overheads; (5) implementing transfer-awareness by leveraging DMA over Unified Virtual Addressing (UVA) when available. In addressing these challenges, FlashDMoE s design presents a radical departure from traditional synchronous AllToAll collectives, where GPUs exhibit significant idle time during layer execution. For device-initiated communication, FlashDMoE uses NVSHMEM [16] to establish a global address space across all GPUs to achieve the aforementioned Direct Memory Access (DMA) or Remote DMA (RDMA) communication. For in-device BLAS, FlashDMoE develops custom high-performance GEMM operations via CUTLASS [17]. Results. We evaluate FlashDMoE across multiple GPUs split across multiple nodes. Our evaluations show that FlashDMoE achieves 6 latency speedup, 9 higher GPU utilization, 4 better weak scaling efficiency and 5.7 increased throughput compared to state-of-the-art implementations. We 3 project these performance gains becoming even better in multi-node scenarios, where inter-node communication occurs using lower bandwidth inter-node links (e.g., RDMA, Infiniband). 2 Motivation 2.1 Synchronous Communication and Stragglers Figure 4: Overlapped Schedule (bottom) showing how idle time from the sequential schedule (top) is repurposed for computation. FlashDMoE implements the overlapped schedule. AlltoAll or AllGather communication as currently used in MoE frameworks is a synchronous collective operation, whose completion requires the participation of all involved GPUs. Here, disparities in processing speeds or kernel scheduling among GPUs induce a straggler effect detrimental (Figure 4) to (1) the collective operation s performance and (2) E2E performance, as stalled GPUs cannot proceed to downstream dependent or independent tasks until the collective terminates. Specifically, as shown in Figure 15, for distributed training of a 1.3B GPT-3 MoE model across 32 A100 GPUs, we see P95 communication performance degradation of 1.32X when compared to the mean actual kernel time from Figure 15b. This performance reduction is rather tame as the underlying hardware is a supercomputer well-tuned against software jitter [18]. However, we observe a more severe p95 performance loss of 11X in a single-node Virtual Machine (VM). In line with prior HPC works [19, 20], we argue that obviating the inherent barrier in this synchronous collective communication would allow GPUs to repurpose this observed idle time for downstream computation as depicted in Figure 4. Table 2: Straggler Delay within Synchronous All-to-All communication. We capture the distribution of delay induced by stragglers across many steps. Let Actual Time ta denote the fastest kernel execution time across all GPUs, and Total Time t be the maximum recorded step time. We define Delay as the maximum difference between t and ta. Note Delay is idle time. For the 1x8 V100, we profile 1750 steps and 600 steps for the 8x4 A100. See Figure 15 for the raw distribution. System Nodes GPUs Median p95 Commercial VM (V100) 1 8 3.1x 11.4x Supercomputer (A100) 8 32 1.09x 1.32x 4 2.2 Kernel launch overhead. Comet FasterMoE Megatron DeepEP 0 20 40 60 80 100 Average SM Util ( ) 42.31 9.67 59.11 13.55 Forward Pass E 64 k 2 2 A100s is better (a) GPU SM Utilization across baselines MoE Iteration [6.557 ms] FlashDMoE DeepEP (b) Kernel Launch overhead (CUDA API row) Figure 5: 5a shows GPU utilization averaged across 100 MoE forward passes on 2 NVLinked A100s with 300 GB s unidrectional bandwidth. Despite the high-bandwidth interconnect, we observe up to 90 idle time, which we attribute to kernel launch gaps and non-overlapping communication. We compare the kernel launch overheads between FlashDMoE and existing baselines. Table 1 shows the number of kernel launches during a single forward pass: FlashDMoE launches exactly one persistent kernel, while the baselines launch up to 550 short-lived kernels to perform the same computation. Figure 5 provides a visual comparison using CUDA API traces captured by NSight Systems, illustrating the difference between FlashDMoE and DeepEP. DeepEP exhibits numerous small CUDA API calls, with frequent stalls between individual operators, leading to increased GPU idle time (Figure 5a). In contrast, FlashDMoE maintains high GPU utilization by avoiding launch overhead and synchronization gaps achieving 93.17 GPU utilization compared to 14 for DeepEP. See 4 for experimental results and A for a discussion of related work. 3 Fused MoE Kernel Design Block n-1 FusedGate Packet Encode and Dispatch Subscriber Block n-2 Block 1 Block 0 ... ... Warp Processor 1 Processor 0 Processor n-1 ... Scheduler warp 0 warp 1 warp 2 warp 3 OS Decoded Tasks Schedule Task t to Processor p Remote Packets Send Tile to GPU x Send Tile to GPU y Send Tile to GPU z Figure 6: FlashDMoE Fused Kernel Modern distributed MoE systems suffer from two limitations: (1) frequent many-to-many (AlltoAll or AllGather) collectives on the critical path, and (2) significant overhead from repeated kernel launches. We address these in FlashDMoE, a fully fused MoE operator implemented as a single persistent GPU kernel. Unlike previous approaches [11, 1, 10, 12, 21, 8, 22 26], FlashDMoE is the first solution to implement a completely fused Distributed MoE kernel, eliminating kernel launch overhead entirely by requiring only a single kernel launch (see Table 1). 5 Algorithm 1: FlashDMoE Distributed MoE Fused Kernel Input: A, O RS H, X RE H D, N 1 begin 2 Tϕ, Gϕ FusedGate(A) 3 if blockId 1 N then 4 Dispatch(Tϕ, A) 5 processor::start() 6 else 7 if warpID 0 then 8 scheduler::start() 9 else 10 subscriber::start(Tϕ, Gϕ, O, X) 11 end if 12 end if 13 end Figure 7: DMoE Functional Dependencies Expressed as a Chain of Actor Interactions. We denote Sb, Sh, and P as the Subscriber, Scheduler and Processor actors, respectively. For any actor a {Sb, Sb, P}, ai identifies an actor on GPU i. We define Dj i as the operator, where GPU j dispatches packets of tiles to GPU i, This diagram expresses task dependencies at the granularity of a tile, namely GEMM0, GEMM1, combine and communication produce an output tile. Notifications occur as signals propagated through shared memory (subscriber scheduler) or global memory (scheduler processor or inter-GPU communication). Note one-sided inter-GPU transfers (packet or single tile) are coupled with a signal to notify Sj b on the receiving GPU j of the message s delivery. Actor-based model. The design of FlashDMoE is based on the actor model of concurrent compu- tation [27 29]. We implement this model by specializing GPU thread blocks and warps into three distinct actor roles: (1) Processor ( E.1), (2) Subscriber ( E.3), and (3) Scheduler( E.2). The Processor performs compute (GEMMs and element-wise operations) and tile communication. We use CUTLASS [17] as the underlying infrastructure for high-performance BLAS routines and NVSH- MEM for kernel-initiated communication [16]. The Subscriber and Scheduler perform administrative functions. Specifically, the Scheduler assigns computational tasks to available thread blocks. Our key innovation is making the Scheduler both multithreaded, enabling high scheduling throughput, and work-conserving, ensuring consistently high GPU SM utilization. On the other hand, the Subscriber decodes tile packets from peer GPUs to task descriptors ( 3.1). Of the N thread blocks on a GPU, we specialize N 1 to adopt the Processor role. We specialize the last block as the Operating System (OS). Within this block, we specialize three warps for the Subscriber role and one warp for the Scheduler role. This split of thread blocks across actors is intentional: our goal is to use few resources for administrative tasks while reserving bulk of the resources for performing MoE computation tasks. Figure 6 summarizes the FlashDMoE architecture and its constituent actors, while Algorithm 1 gives a very close translation of the system in code. Note that A RS H is the input token matrix; O RS H the output matrix; and X RE H D is a 3-D tensor of expert weights, where E denotes the number of local experts for the executing GPU, H is the embedding dimension, D is the FFN intermediate dimension and S is the sequence length. Tϕ R2 E C is a routing table data structure, where Tϕ (e, c) (i, w) indicates that token i at slot c dispatches to expert e. w is the combine weight (Equation 2) and C is expert capacity. The tuple structure of Tϕ is an implementation detail. Gϕ RS E captures the affinity scores produced by the gate (Equation 3). Inter-actor interactions in FlashDMoE. FlashDMoE decomposes MoE computation and com- munication at the granularity of a tile, a statically sized partition of a tensor, to achieve parallel execution and efficient overlap of tasks. Each tile maps to a discrete unit of work encapsulated by a task descriptor. The Subscriber decodes these task descriptors from the remote tile packets it receives. Concurrently, the Scheduler receives notifications about available tasks and dispatches them for execution to Processor actors that perform computations defined by these tasks, namely 6 Global Memory Shared Memory Shared Memory Global Memory Registers Thread Block 80 GB 46 KB Block 1 KB thread Global Memory Shared Memory Registers Device Thread Block Shared Memory Figure 8: GPU Memory Hierarchy. The inverted pyramid (left) shows the load store access la- tency [30 32]. The table above outlines the capacity for different memory tiers (for A100 GPUs). The shared memory and register capacity are static configurations for FlashDMoE. The right figure shows accessibility scopes: on-chip registers are scoped to a thread; on-chip shared memory is visible to all threads in a block; and off-chip global memory is accessible by all threads on device. the feed-forward network (FFN) and expert-combine operations. Figure 7 show the chain of actor interactions, demonstrating how FlashDMoE enforces DMoE functional dependencies. Determining tile dimensions in FlashDMoE. Selecting appropriate tile dimensions in FlashDMoE is crucial to ensure efficient GPU utilization. An undersized tile underutilizes the GPU, while excessively large tiles create register pressure, causing performance-degrading register spills to local memory. After careful parameter sweeps, we choose tile dimensions of (128, 64). Our key insights are: increasing tile width significantly raises the register usage per thread, potentially triggering costly spills; increasing tile height without adjusting thread count increases workload per thread, harming performance. Raising the thread count per block beyond our fixed value of 128 threads reduces the number of concurrent blocks, negatively affecting SM occupancy. Larger thread-block sizes also increase overhead from intra-block synchronization (__syncthreads() barriers), further degrading performance. Thus, our chosen tile dimensions balance register usage, shared-memory constraints, and GPU occupancy to deliver optimal performance. 3.1 Task Abstraction for Computation Computational operators. The FFN operator is a standard position-wise feed-forward network widely used in Transformer architectures [5], composed of two linear transformations separated by a nonlinear activation ϕ (e.g., GELU or ReLU): FFN(x) W2 ϕ(xW1 b1) b2 (1) Here, W1 and W2 represent learnable weight matrices, and b1 and b2 are biases. The expert-combine operation, used in architectures like GShard [33] and DeepSeek [1], merges outputs from multiple experts by computing a weighted combination based on their affinity scores: Ci k X j 1 gi,e (2) hi k X j 1 gi,e Ci hk i (3) In these equations, i 0, S 1 represents an input token index, e Ei,k identifies the k-th expert selected for token i, and gi,e is the affinity score indicating how relevant expert e is for token i. Unified task abstraction. We unify the FFN and combine operations under a common abstraction called a task. Tasks provide a uniform interface for communicating tile-level work among Subscribers, Schedulers, and Processors. Formally, a task descriptor t T is defined as a tuple: t (M, , ϕ) 7 where M is a set of metadata (e.g., device ID, tile index), is a binary tensor operation (specifically, matrix multiplication or Hadamard product ), and ϕ is an element-wise activation function (e.g., ReLU or identity). We define a task t operating on input tensors A, B, D, producing output tensor C, as follows: Ft(A, B, C, D) : C ϕ (A t B D) (4) The operator t (instantiated from ) may behave differently depending on the task metadata M, and the result of A t B is accumulated into D. We provide an example of task metadata in D. In practice, we implement each task defined by Equation 4 as a single fused __device__ decorated function which the Processor (Algorithm 2) invokes at runtime. Fusion for t entails applying ϕ and the succeeding addition operation to registers storing the results of the binary operator t. To illustrate its flexibility, we show how the FFN and expert-combine operations can be expressed using this task framework. Note that we omit the matrix multiplication symbol ( ) for simplicity. Also, ϕ1 can be any activation function, while ϕ2 is the identity function. The FFN is expressed as: t1 (M, , ϕ1), t2 (M, , ϕ2), Ft1(A, B1, C1, D1) : C1 ϕ1 (AB1 D1) , Ft2(C1, B2, C2, D2) : C2 ϕ2 (C1B2 D2) . Whereas, the expert-combine operation is formalized as: t3 (M, , ϕ2), Ft3(A, S, C, C) : C ϕ2 (A S C) . 3.2 Symmetric Tensor Layout for Inter-GPU Communication Exper t Par allel Gr oup B0 B1 B0 B1 R0 R1 P0 P1 E0 E1 P0 E2 E3 P1 E0 E1 E2 E3 E0 E1 E0 E1 E0 E1 E0 E1 E0 E1 E2 E3 B0 B1 B0 B1 R0 R1 P0 P1 E0 E1 E2 E3 E2 E3 E2 E3 E2 E3 E2 E3 E0 E1 E2 E3 P0 View P1 View (a) Symmetric Tensor Layout across 2 Expert-parallel Processes. P1 R0 B0 E2 P1 R1 B1 E2 P0 R1 B0 E2 P0 R0 B1 E2 2.) P0 stages the outgoing message to P1 before remote transfer 3.) P1 receives 4.) P1 processes 5.) P1 stages 6.) P0 receives Start 1.) P0 intends to Dispatch to E2 on P1 P1 R1 B1 E2 P0 R0 B1 E2 2.) P1 receives 4.) P0 receives Start 1.) P0 intends to Dispatch to E2 on P1 3.) P1 processes (b) State machine for DMA (top) and RDMA (bottom) communication. Within a single GPU device, the actors in FlashDMoE communicate through the GPU s memory subsystem (see Figure 8). Specifically, the Scheduler and Subscriber actors exchange data via fast shared memory, while other actor pairs communicate through global memory. For communication across multiple devices, FlashDMoE uses device-initiated communication, leveraging the one-sided PGAS (Partitioned Global Address Space) programming model [34]. However, achieving scalable and correct one-sided memory accesses in PGAS without costly synchronization is a known challenge [1, 35]. We address this challenge with a provably correct and scalable solution: a symmetric tensor layout L, supporting fully non-blocking memory accesses. We define L as: L RP R B E C H 8 where: P is the expert parallel world size, R identifies communication rounds (i.e., two rounds, one for token dispatch and one for combine), B is number of staging buffers, E is the number of local experts, C is the upscaled expert capacity ( 3.2.1) and H is the token embedding dimension. Our core insight to enable non-blocking communication is temporal buffering. Specifically, we overprovision memory for the underlying token matrix by at least 2 r times, where r is the number of communication rounds in the dependency graph, and the factor of 2 accounts for separate buffers for incoming and outgoing data within each communication round. For MoE models, we have 2 r 4. This modest increase in memory usage eliminates the need for synchronization during one-sided data transfers. Figure 9b illustrates how cells within this symmetric tensor layout are indexed and used for Direct Memory Access (DMA) and Remote DMA (RDMA) operations. As Theorem 3.1 reinforces, this indexing scheme over L is the underlying mechanism that allows for fully non-blocking accesses eliding synchronization because all accesses are write conflict-free. See C for the proof. Theorem 3.1. The symmetric tensor layout L is write-write conflict-free. To construct L, we start from the original token buffer T RS H, where S is the sequence length and H is the token embedding dimension. We first reorganize the sequence dimension S into three sub-dimensions representing the expert capacity (C), local expert slots (E), and the expert parallel world size (W), st: C E W C E S , where S S and E EW In the typical case of uniform expert distribution (illustrated in Figure 9a), we have S S and E EW , where EW is the total number of experts in the model. Thus, the size of the token buffer is Size(T) S H. In Figure 9a, each cell labeled Ei (with i {0, . . . , 3}) is a matrix of size (C, H). Extending prior work [33, 11], we introduce additional temporal dimensions R (communication rounds) and B (staging buffers). Each communication round has two fixed staging slots: one for outgoing tokens and another for incoming tokens. Each slot, indexed by dimension P, forms a tensor of shape (S , H). Therefore, the tensor size Size(L) is generally at least four times the original token buffer size, becoming exactly four times larger in the case of uniform expert distribution. Empirically, we find: Size(L) 4 Size(T) 3.2.1 In-place Padding for Payload Efficiency Due to the dynamic and uneven distribution of tokens in MoE dispatch [36], GPUs commonly receive fewer tokens than their predefined expert capacity. Current MoE frameworks [10] typically pad these buffers with null tokens before computation, unnecessarily increasing communication payloads and degrading performance. In contrast, we propose in-place padding, performing padding directly within the local symmetric tensor buffers and thus eliminating excess network communication. As we show in Figure 9a as a reference, each cell Ei is sized according to the expert capacity C. We further align this capacity to ensure divisibility by the tile block size bM 128, guaranteeing safe and aligned memory reads by Processor threads consuming remote tokens. This in-place padding strategy slightly increases the memory footprint of L, as described below: Size(L) ( 4 Size(T), S E bM 4 bM E S Size(T), otherwise 4 Experiments We implement ( G) and evaluate FlashDMoE and evaluate across five metrics: Forward Latency ( 4.2), GPU Utilization ( 4.3), Overlap Efficiency ( 4.4), Throughput ( 4.5), and Expert Scalability ( 4.6). We run experiments on a server with 8 NVIDIA H100 80G GPUs interconnected via NVLink, 125 GB of RAM, and 20 vCPUs. We used PyTorch 2.6.0, CUDA 12.8, and Ubuntu 22.04. All experiments use MoE transformer models configured with 16 attention heads, an embedding dimension of 2048, and an FFN intermediate size of 2048. We apply Distributed Data Parallelism (DDP) and Expert Parallelism for all experiments. We execute only the forward pass over a single MoE layer and measure the average runtime of 32 passes after 32 warmup passes. We use top-2 routing with a capacity factor of 1.0. We compare FlashDMoE against several state-of-the-art MoE 9 systems: (1) Comet [11], (2) FasterMoE [13], (3) Megatron-CUTLASS [37], and (4) Megatron- TE: Megatron-LM with Transformer Engine [38]. Comet relies on cudaMemcpyPeerAsync [39], while FasterMoE and Megatron-LM use NCCL exclusively for communication. We also evaluate FlashDMoE on a multi-node environment and discuss our findings in F. 4.1 Desiderata In our experiments, we observe Comet exhibiting anomalously bad performance values at 8 GPUs, so we exclude their results from evaluations at 8 GPUs and only include for results at 4 GPUs. Note we evaluate FlashDMoE using FP32 precision whereas all baselines use FP16. We do so because (1) no baseline supports FP32 and (2) time constraints prevent us from tuning our system to peak performance at FP16. Most importantly, this precision discrepancy disadvantages FlashDMoE by doubling the communication and computation precision, making our results a conservative lower bound. Yet, as we show in the succeeding sections, FlashDMoE outperforms all baselines. 4.2 Forward Latency 4K 8K 16K Number of Tokens 0 5 10 15 20 25 Runtime (ms) 2.1 3.4 6.2 3.0 5.4 10.0 7.9 12.6 22.4 4.9 16.6 17.4 5.1 9.0 17.2 Forward Latency E 32 k 2 4 H100s is better FlashDMoE Comet FasterMoE Megatron-CUTLASS Megatron-TE (a) 4 H100s 4K 8K 16K Number of Tokens 0 10 20 30 40 Runtime (ms) 2.1 3.7 7.0 10.3 21.3 45.3 9.9 18.9 36.9 10.0 18.9 36.9 Forward Latency E 32 k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (b) 8 H100s Figure 10: Forward Latency as the Number of Tokens per GPU increases. We first measure the forward latency of FlashDMoE across different sequence lengths on both 4 and 8 GPU setups (Figure 10). FlashDMoE consistently outperforms all baselines, with especially notable improvements at longer sequence lengths. On 4 GPUs, it achieves up to 4.6x speedup over Megatron-TE at 16K tokens, and 2.6x over FasterMoE. The gains are even more pronounced at 8 GPUs where FlashDMoE maintains low latency, exhibiting up to 6.4x speedup over baselines that degrade steeply due to increasing communication costs as token buffers increase proportionally. These results highlight FlashDMoE s ability to scale token throughput without suffering from the communication penalties that plague other implementations. 4.3 GPU Utilization To quantify GPU efficiency, we measure Streaming Multiprocessor (SM) utilization during the forward pass (Figure 11). FlashDMoE achieves 93.17 average SM utilization, over 9x higher than FasterMoE (9.67 ), 6.8x higher than DeepEP Megatron-LM (13.55 ) 4x higher than Megatron-TE (59.11 ), and 2.2x higher than Comet (42.31 ). This improvement stems from our fully fused kernel architecture and fine-grained pipelining of compute and communication tasks. By eliminating idle gaps due to kernel launches and enabling in-kernel task scheduling, FlashDMoE ensures SMs remain busy with productive work throughout execution. 4.4 Overlap Efficiency We evaluate the extent to which FlashDMoE overlaps communication and computation by measuring weak scaling efficiency as the number of GPUs increases (Figure 12b). We note that most baselines fail to execute at a single GPU, hence why we use 2 GPUs as the reference point. We observe that Megatron-CUTLASS and Megatron-TE degrade significantly, with overlap efficiency dropping below 0.5 at 4 GPUs. FlashDMoE gives up to 3.88x and 4x higher efficiency at 4 and 8 GPUs, respectively. Figure 12a further illuminates this efficiency, as FlashDMoE shows stable forward latency growth, 10 FlashDMoE Comet FasterMoE Megatron DeepEP 0 20 40 60 80 100 Average SM Util ( ) 93.17 42.31 9.67 59.11 13.55 Forward Pass E 64 k 2 2 A100s is better Figure 11: Comparison of SM utilization, defined as the ratio of cycles in which SMs have at least one warp in flight to the total number of cycles [40]. Values represent the average SM utilization over 100 iterations. All experiments use T 8K and E 64 on two A100s 2 4 8 of H100s 0 5 10 15 20 25 Runtime (ms) 3.3 3.4 3.7 10.3 12.6 21.3 4.2 16.6 18.9 4.4 9.0 18.9 Forward Latency T 8K E 32 k 2 is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (a) Latency as Number of GPUs increases. 2 4 8 of H100s 0 20 40 60 80 100 Overlap Efficiency ( ) 100 97 90 100 82 48 100 26 22 100 49 24 Forward T 8K E 32 k 2 is better Flash FM M-C M-TE (b) Weak scaling efficiency. Figure 12: Forward Latency as the Number of Tokens per GPU increases. We define Overlap Efficiency Oe to be Oe T(2) T(NG), where T(NG) is the latency at NG GPUs and T(2) is the latency at 2 GPUs. whereas baselines Megatron-CUTLASS and Megatron-TE experience approximately linear latency amplification while FasterMoE exhibits sublinear scaling. We attribute this suboptimal performance to straggler effects and exposed communication. In contrast, FlashDMoE demonstrates uniform latency as expected since the workload per GPU is fixed in this weak scaling experiment. These results further corroborate that FlashDMoE s actor-based design and asynchronous data movement achieve near-ideal overlap, even at scale. 4.5 Throughput Throughput, measured in tokens per second (MTokens s), reflects end-to-end system efficiency. As shown in Figure 13, FlashDMoE scales linearly with GPU count, reaching 17.7 MTokens s at 8 GPUs. This is over 5.7x higher than FasterMoE and 4.9x higher than Megatron-TE and Megatron- CUTLASS. Notably, these results are achieved despite FlashDMoE operating entirely in FP32, while baselines use FP16. This indicates that FlashDMoE s design eliminates throughput bottlenecks not by exploiting lower precision, but by maximizing hardware utilization and eliminating host-driven inefficiencies. 4.6 Expert Scalability We analyze how FlashDMoE scales with increasing number of experts at fixed sequence length (T 16K). Note that for the discussed plots, the number of experts on the x-axis is the total number across all GPUs. Each GPU gets 1 8th of this value. As seen in Figure14, FlashDMoE maintains low, uniform latency, as desired, even as the number of experts grows from 8 to 128. In contrast, 11 2 4 8 of H100s 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Throughput (MTps) 4.90 9.50 17.70 1.60 2.60 3.10 3.90 1.90 3.60 3.68 3.60 3.60 Forward Pass T 8K E 32 k 2 is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE Figure 13: Throughput as the amount of GPUs increases. We compute throughput as T NG latency, where NG is the number of GPUs. 8 16 32 64 128 Number of Experts 0 5 10 15 20 25 30 35 Runtime (s) Forward Latency T 16K k 2 4 H100s is better FlashDMoE Comet FasterMoE Megatron-CUTLASS Megatron-TE (a) 4 H100s 8 16 32 64 128 Number of Experts 0 10 20 30 40 50 60 Runtime (s) Forward Latency T 16K k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (b) 8 H100s Figure 14: Forward Latency as the Number of experts increases. baselines exhibit superlinear latency increases due to increased kernel launch overheads. FlashDMoE outperforms these baselines by up to 4X at 4 H100s and 6.6X at 8 H100s, both at 128 experts. FlashDMoE s payload-efficient communication and scheduler-driven in-kernel dispatching allow it to sustain expert parallelism without incurring the communication and orchestration penalties seen in other systems. These results reinforce FlashDMoE s scalability for ultra-sparse MoE configurations. 4.7 Memory Overhead We measure the GPU memory required for the symmetric tensor L and runtime bookkeeping state of FlashDMoE. Memory overhead depends primarily on the tile size, expert capacity (EC), and the number of experts (E). Table 3 summarizes memory overhead under various configurations, confirming that FlashDMoE maintains a modest and predictable memory footprint. 5 Limitations and Future Work Despite the performance gains and architectural innovations of FlashDMoE, there are several limita- tions worth acknowledging both practical and conceptual that open the door to future research. Programming complexity. Developing fully fused, persistent kernels is a non-trivial engineering task. While FlashDMoE proves the feasibility and benefit of such kernels, their construction demands deep expertise in GPU architectures, synchronization and distributed protocols, and memory hierarchies. This high barrier to entry limits adoption. Future work may consider compiler-level abstractions or DSLs to democratize this technique. FP16 support and shared memory access patterns. Although modern GPUs natively support half-precision computation, adapting FLASHDMOE to FP16 is non-trivial for 12 Table 3: Memory overhead of FlashDMoE (tile size bM 128), Size(T) Tokens 4KB). Tokens Experts EC max(bM, EC) Bookkeeping (MB) Size(L) (MB) Total (MB) 4K 16 256 256 64.57 64.00 128.57 4K 32 128 128 64.55 64.00 128.55 4K 64 64 128 128.90 128.01 256.91 4K 128 32 128 257.96 256.02 513.98 8K 16 512 512 128.95 128.01 256.95 8K 32 256 256 128.90 128.01 256.91 8K 64 128 128 128.90 128.01 256.91 8K 128 64 128 258.15 256.02 514.17 16K 16 1024 1024 257.89 256.02 513.90 16K 32 512 512 257.79 256.02 513.81 16K 64 256 256 257.80 256.02 513.81 16K 128 128 128 258.53 256.02 514.54 the Processor s computational operators. Specifically, our manually tuned swizzle shared memory layouts are not the most efficient template parameters for CUTLASS Collective Mainloop operator which we use to implement our in-device GEMMs. This suboptimal configuration degrades memory throughput as shown in H. Overcoming this for Ampere GPUs and below would require careful investigation of optimal layouts, but for Hopper GPUs and above, we anticipate using the builder interface that CUTLASS provides in our future improvements. Lack of backward pass and training support. While this work focuses on inference, enabling training requires fusing backward computation and gradient communication into the kernel. Supporting this entails non-trivial changes to both memory bookkeeping and task descriptor definitions. Nevertheless, it remains an exciting direction for extending this system to fully support end-to-end training. 6 Conclusion This work introduces FlashDMoE, the first system to fuse the entire Mixture-of-Experts (MoE) operator into a single, persistent GPU kernel. We show that prevailing MoE implementations suffer from two critical inefficiencies: (1) CPU-managed synchronous communication that leads to underutilized interconnects and (2) fragmented execution via multiple GPU kernels, introducing overhead and synchronization delays. In contrast, FlashDMoE embraces a model of GPU autonomy by embedding computation, com- munication, and scheduling within a unified kernel. It leverages actor-style concurrency, warp specialization, and asynchronous (R)DMA to achieve fine-grained communication computation overlap. Our evaluation demonstrates up to 6 speedup over state-of-the-art systems, up to 9 improved GPU utilization, and 5.7 increased throughput for Distributed MoE. FlashDMoE challenges the dominant execution paradigms in distributed deep learning and presents a compelling template for building future GPU-native systems. While several limitations remain, programming complexity and lack of FP16 support, this work lays the groundwork for a new era of in-kernel distributed computation. Future systems may build upon this foundation to enable kernel fusion for entire training pipelines, ushering in a design shift from CPU orchestration to fully autonomous GPU execution. 7 Acknowledgements This research is supported by NSF Award 2444537 and ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. This work also used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231 using NERSC award ASCR-ERCAP0030076. We acknowledge and thank Dr. Giulia Guidi for providing access to these NERSC supercomputing resources. 13 References [1] DeepSeek-AI. Deepseek-v3 technical report, 2025. URL 19437. [2] Meta AI. The llama 4 herd: The beginning of a new era of natively multimodal ai innovation, 2025. URL [3] Mosaic Research. Introducing dbrx: A new state-of-the-art open llm, 2024. URL https: www.databricks.com blog introducing-dbrx-new-state-art-open-llm. [4] Snowflake AI Research. Snowflake arctic: The best llm for enterprise ai effi- ciently intelligent, truly open, 2024. URL arctic-open-efficient-foundation-language-models-snowflake . [5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Ad- vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [6] Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, and Abhinav Bhatele. A hybrid tensor-expert-data parallelism approach to optimize mixture-of- experts training. In Proceedings of the 37th ACM International Conference on Supercomputing, ICS 23, page 203 214, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400700569. doi: 10.1145 3577193.3593704. URL 3577193.3593704. [7] Juncai Liu, Jessie Hui Wang, and Yimin Jiang. Janus: A unified distributed training framework for sparse mixture-of-experts models. In Proceedings of the ACM SIGCOMM 2023 Conference, ACM SIGCOMM 23, page 486 498, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702365. doi: 10.1145 3603269.3604869. URL 10.1145 3603269.3604869. [8] Chenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, and Yida Wang. Lancet: Accelerating mixture-of-experts training via whole graph computation-communication overlapping. In P. Gib- bons, G. Pekhimenko, and C. De Sa, editors, Proceedings of Machine Learning and Systems, volume 6, pages 74 86, 2024. URL paper 2024 file 339caf45a6fa281cae8adc6465343464-Paper-Conference.pdf. [9] NVIDIA Collective Communications Library (NCCL). nccl. [10] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture- of-experts inference and training to power next-generation AI scale. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18332 18346. PMLR, 17 23 Jul 2022. URL https: proceedings.mlr.press v162 rajbhandari22a.html. [11] Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, and Xin Liu. Comet: Fine- grained computation-communication overlapping for mixture-of-experts. In MLSys 25. URL [12] NVIDIA. Megatron-lm, 2025. URL readme-ov-file. v0.11.0. [13] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP 22), pages 120 134, 2022. 14 [14] Michael Wendt and Joshua Wyatt. Getting started with CUDA graphs. nvidia.com blog cuda-graphs , 2019. Accessed: 2024-05-15. [15] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384421. doi: 10.1145 3458817.3476209. URL [16] NVIDIA. Nvidia openshmem library (nvshmem), 2025. URL nvshmem api index.html. v3.2.5. [17] Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan Yan, Jack Kosaian, Mark Hoemmen, Haicheng Wu, Andrew Kerr, Matt Nicely, Duane Merrill, Dustyn Blasig, Fengqi Qiao, Piotr Majcher, Paul Springer, Markus Hohnerbach, Jin Wang, and Manish Gupta. CUTLASS, 2025. URL [18] NERSC. Network - NERSC Documentation. network , 2025. [Accessed 23-05-2025]. [19] C. Bell, D. Bonachea, R. Nishtala, and K. Yelick. Optimizing bandwidth limited problems using one-sided communication and overlap. In Proceedings 20th IEEE International Parallel Distributed Processing Symposium, pages 10 pp. , 2006. doi: 10.1109 IPDPS.2006.1639320. [20] Yuxin Chen, Benjamin Brock, Serban Porumbescu, Aydin Buluc, Katherine Yelick, and John Owens. Atos: A task-parallel gpu scheduler for graph analytics. In Proceedings of the 51st International Conference on Parallel Processing, ICPP 22, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450397339. doi: 10.1145 3545008.3545056. URL [21] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, HoYuen Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive mixture-of-experts at scale. In D. Song, M. Carbin, and T. Chen, editors, Proceedings of Machine Learning and Systems, volume 5, pages 269 287. Curan, 2023. URL file 5616d34cf8ff73942cfd5aa922842556-Paper-mlsys2023.pdf. [22] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained mod- els. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP 22, page 120 134, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392044. doi: 10.1145 3503221.3508418. URL [23] Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Gang Cao, and Bin Cui. Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement. Proc. ACM Manag. Data, 1(1), May 2023. doi: 10.1145 3588964. URL [24] Shaohuai Shi, Xinglin Pan, Qiang Wang, Chengjian Liu, Xiaozhe Ren, Zhongzhe Hu, Yu Yang, Bo Li, and Xiaowen Chu. Schemoe: An extensible mixture-of-experts distributed training system with tasks scheduling. In Proceedings of the Nineteenth European Conference on Computer Systems, EuroSys 24, page 236 249, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704376. doi: 10.1145 3627703.3650083. URL [25] Hulin Wang, Yaqi Xia, Donglin Yang, Xiaobo Zhou, and Dazhao Cheng. Harnessing inter-gpu shared memory for seamless moe communication-computation fusion. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, PPoPP 25, page 170 182, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400714436. doi: 10.1145 3710848.3710868. URL 3710848.3710868. 15 [26] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neu- ral Information Processing Systems, volume 35, pages 16344 16359. Curran Associates, Inc., 2022. URL 67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf. [27] Gul A. Agha. Actors: A model of concurrent computation in distributed systems. Technical report, 1985. MIT Artificial Intelligence Laboratory Technical Reports. [28] Carl Hewitt, Peter Bishop, and Richard Steiger. A universal modular actor formalism for artificial intelligence. IJCAI 73, page 235 245, San Francisco, CA, USA, 1973. Morgan Kaufmann Publishers Inc. [29] Irene Greif. SEMANTICS OF COMMUNICATING PARALLEL PROCESSES. PhD thesis, Massachusetts Institute of Technology, 1975. [30] Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Qiang Wang, and Xiaowen Chu. Benchmarking and Dissecting the Nvidia Hopper GPU Architecture . In 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pages 656 667, Los Alamitos, CA, USA, May 2024. IEEE Computer Society. doi: 10.1109 IPDPS57955.2024.00064. URL https: doi.ieeecomputersociety.org 10.1109 IPDPS57955.2024.00064. [31] Hamdy Abdelkhalik, Yehia Arafa, Nandakishore Santhi, and Abdel-Hameed Badawy. Demysti- fying the nvidia ampere architecture through microbenchmarking and instruction-level analysis, 2022. URL [32] NVIDIA. Ptx isa: Version 8.7, 2025. URL isa_8.7.pdf. [33] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with con- ditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL [34] Katherine Yelick, Dan Bonachea, Wei-Yu Chen, Phillip Colella, Kaushik Datta, Jason Duell, Susan L. Graham, Paul Hargrove, Paul Hilfinger, Parry Husbands, Costin Iancu, Amir Kamil, Rajesh Nishtala, Jimmy Su, Michael Welcome, and Tong Wen. Productivity and performance using partitioned global address space languages. In Proceedings of the 2007 International Workshop on Parallel Symbolic Computation, PASCO 07, page 24 32, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595937414. doi: 10.1145 1278177. 1278183. URL [35] Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Hao- jie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, and Xin Liu. Triton-distributed: Programming overlapping kernels on distributed ai systems with the triton compiler, 2025. URL [36] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models, 2024. URL [37] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 15, 2021. [38] NVIDIA. Transformer engine, . URL [39] Bytedance. Flux s overlap performance is worse than non-overlap [4xrtx 4090], 2025. URL issuecomment-2822823236. 16 [40] NVIDIA. NVIDIA Nsight Systems Metrics, . URL com nsight-systems UserGuide index.html?highlight SM 2520active available-metrics. [41] Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, and Olli Saarikivi. Breaking the computation and communication abstraction barrier in distributed machine learning workloads. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 22), pages 402 416, 2022. [42] Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivasa Murthy, Marcello Maggioni, Qiao Zhang, et al. Overlap communication with dependent computation via decomposition in large deep learning models. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 22), pages 93 106, 2022. [43] Chang Chen, Xiuhong Li, Qianchao Zhu, Jiangfei Duan, Peng Sun, Xingcheng Zhang, and Chao Yang. Centauri: Enabling efficient scheduling for communication-computation overlap in large model training via communication partitioning. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 24), pages 178 191, 2024. [44] Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, and Matthew D Sinclair. T3: Transparent tracking triggering for fine-grained overlap of compute collectives. In Pro- ceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 24), pages 1146 1164, 2024. [45] Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, et al. {MegaScale}: Scaling large language model training to more than 10,000 {GPUs}. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), pages 745 760, 2024. [46] Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu Qiao, and Yiran Zhong. CO2: Efficient distributed training with full communication-computation overlap. In The Twelfth International Conference on Learning Representations (ICLR 24), 2024. [47] Kshiteej Mahajan, Ching-Hsiang Chu, Srinivas Sridharan, and Aditya Akella. Better together: Jointly optimizing {ML} collective scheduling and execution planning using {SYNDICATE}. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), pages 809 824, 2023. [48] Hulin Wang, Yaqi Xia, Donglin Yang, Xiaobo Zhou, and Dazhao Cheng. Harnessing inter-gpu shared memory for seamless moe communication-computation fusion. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, pages 170 182, 2025. [49] Kishore Punniyamurthy, Khaled Hamidouche, and Bradford M Beckmann. Optimizing dis- tributed ml communication with fused computation-collective operations. In SC24: Interna- tional Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 17, 2024. [50] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, et al. Tutel: Adaptive mixture-of-experts at scale. Proceedings of Machine Learning and Systems (MLSys 23), 5:269 287, 2023. [51] Chenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, and Yida Wang. Lancet: Accelerating mixture-of-experts training via whole graph computation-communication overlapping. In Proceedings of Machine Learning and Systems (MLSys 24), pages 74 86, 2024. [52] OpenFabrics Interfaces Working Group. fi_cxi. v1.21.0 man fi_cxi.7.html, 2025. [Accessed 23-05-2025]. 17 NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper s contributions and scope? Answer: [Yes] 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] 6. Experimental setting details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All reported results in the evaluation section were obtained as the average of 150 executions. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics Answer: [Yes] 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] 18 Justification: We do not foresee immediate social or ethical impacts, but we acknowledge that increased compute efficiency could amplify access to large-scale models, which raises general considerations around prevalent issues such as environmental cost of training, and responsible downstream use. We recommend that users of our system consider these factors when integrating it into broader ML applications. 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval review based on the requirements of your country or institution) were obtained? Answer: [NA] 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] 19 A Related Work Computation-Communication Overlap and Kernel Fusion. To reduce the communication over- heads of synchronization in distributed DNN training, many research efforts have been focused on increasing the overlap of computation and communication. For generic Transformer-based models without MoE layers, many works [41 49] have provided insights and techniques to partition and schedule computation and communication operations, aimed at finer-grained overlapping. To address the challenges posed by AllToAll communication and expert parallelism in MoE training, Tutel [50] and FasterMoE [13] overlap AllToAll with expert computation. Lancet [51] additionally enables both non-MoE computation in forward pass and weight gradient computation in backward pass to be overlapped with AllToAll. Despite overlapping, the performance of these approaches is limited in practice due to blocking synchronous collective communication with barriers. In contrast, FlashD- MoE fundamentally eliminates these inefficiencies with asynchronous, device-initiated data transfers overlapped with tiled computation all within a single kernel. FlashDMoE further differentiates itself from SOTA works like COMET [11] and DeepEP [1], which also use this form of kernel-initiated communication but at a coarse-grained granularity and without complete kernel fusion. B Motivation Plots 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Delay (ms) 0.0 0.2 0.4 0.6 0.8 1.0 0.28ms median 0.19ms P95 0.64ms ECDF GPT-3 MoE 32x1.3B All-to-All Straggler Effect: 8x4 A100 (a) ECDF 0 100 200 300 400 500 600 All-to-All Steps 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Time (ms) max delay 3.22 ms avg actual 1.99ms Raw Distribution Total Time Actual Time GPT-3 MoE 32x1.3B All-to-All Straggler Effect: 8x4 A100 (b) Raw Distribution 0 5 10 15 20 25 Delay (ms) 0.0 0.2 0.4 0.6 0.8 1.0 1.17ms median 0.57ms P95 2.82ms ECDF GPT-3 MoE 8x350M All-to-All Straggler Effect: 1x8 V100 (c) ECDF 0 250 500 750 1000 1250 1500 1750 All-to-All Steps 0 5 10 15 20 25 Time (ms) max delay 27.90 ms avg actual 0.27ms Raw Distribution Total Time Actual Time GPT-3 MoE 8x350M All-to-All Straggler Effect: 1x8 V100 (d) Raw Distribution Figure 15: Straggler effect of synchronous AllToAll. M N A100 or V100 denotes N GPUs within a node across M nodes. Every GPU communicates with every other GPU per AllToAll step. We capture the distribution of delay induced by stragglers across many steps. Actual Time ta denotes the fastest kernel execution time across all GPUs, conversely Total Time t is the maximum recorded step time, while Delay is the maximum difference between t and ta. Note Delay is idle time. C Proof of Theorem 3.1 We begin with two necessary definitions vital to the proof. 20 Definition C.1. Define a write as w(ps, pt, i), where ps is the source process and i is an ordered tuple indicating the index coordinates for L residing on the target process pt. A write-write conflict occurs when there exist at least two distinct, un-synchronized, concurrent writes w1(ps1, pt1, i1) and w2(ps2, pt2, i2), such that pt1 pt2 and index coordinates i1 i2 but ps1 ps2 Definition C.2. For any source process ps, a valid index coordinate i (p , r, b, e, c) satisfies the following: 1. For inter-device writes, it must hold that p ps and b 1. Note this also applies to self-looping writes w(pt, pt, i). 2. For any write w(ps, pt, i), if b 0, then ps pt. This rule describes intra-device staging writes. We restate Theorem 3.1 and outline its proof below. Theorem C.1. The symmetric tensor layout L is write-write conflict-free. Proof. As is the case for typical physical implementations, assume that each index coordinate i maps to a distinct memory segment in L. Next, we show by contradiction that no write-write conflicts can exist when accessing L using valid i. For simplicity, we only include the index coordinates when describing a write. Assume that there exist at least two writes w1(ps1, pt1, i1), w2(ps2, pt2, i2) with pt1 pt2 and valid destination coordinates i1, i2, where i1 i2 lexicographically and both are unpacked below. i1 (p1, r1, b1, e1, c1), i1 (p2, r2, b2, e2, c2) Note that for the message staging state, even though i1 i2 the resultant memory segments reside in different physical buffers resident in ps1 and ps2 respectively. Therefore, for this state, there are no conflicts as intra-process writes always have distinct cj coordinates, where j {0, C 1}. For inter-process transfers, we have two cases. Case 1: ps1 ps2 Here, w1 and w2 are identical operations. This contradicts the definition of a conflict, which requires that ps1 ps2. In practice, such repeat writes never even occur. Case 2: ps1 ps2 To ensure validity for i1 and i2, it is the case that p1 ps1 and p2 ps2. However, this implies that i1 i2 yielding a contradiction as desired. 21 D Task Implementation def i ne GEMMs 2 st r uct __al i gn__( 16) Task { const byt e aDat a; ar r ay const byt e , GEMMs bDat a; ar r ay byt e , GEMMs cDat a; ar r ay const byt e , GEMMs dDat a; byt e r cDat a; ui nt 64_t f l ags; ui nt M; ui nt syncI dx; ui nt t i l eI dx; ui nt bat chI dx; ui nt peer I dx; ui nt exper t I dx; ui nt i sPeer Remot e; TaskType t askType; ui nt 16_t t i l eSi ze; Pad t i l l 128- byt e cache l i ne ui nt paddi ng[ 6] { } ; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Figure 16: Task Struct. TaskType {GEMM0, GEMM1, Combine} 22 E Actors E.1 Processor Algorithm 2: Processor Actor: executed by a block 1 begin 2 tQ GetTQ() 3 signal 0 4 shared memory variables 5 task {} 6 interrupt False 7 complete False 8 while interrupt False do 9 if warpId 0 then 10 if threadId 0 then 11 awaitTaskFromScheduler(interrupt, signal) 12 FencedNotifyRQ(ready) 13 end if 14 syncwarp() 15 warpReadTQ(tQ, signal, task) 16 end if 17 syncthreads() 18 if interrupt False then 19 switch task.Type do 20 case GEMM0 do 21 fused GEMM, epilogue and async tile staging 22 fGET(GEMM0, task) 23 if threadId 0 then 24 complete NotifyTileCompletion() 25 end if 26 syncthreads() 27 if complete True then 28 NotifySchedulerNextGEMM(tQ) 29 end if 30 end case 31 case GEMM1 do 32 fused GEMM, epilogue and async tile transfer 33 fGET(GEMM1, task) 34 end case 35 case Combine do 36 combine(task) 37 end case 38 end switch 39 end if 40 end while 41 end 23 E.2 Scheduler Algorithm 3: Scheduler Actor: executed by one warp 1 begin 2 scheduled 0 3 tTB 0 4 tqState {} 5 pTDB GetProcessorDoorbell() 6 sTDB GetSubscriberDoorbell() 7 taskBound GetTaskBound() 8 tTB AtomicLoad(taskBound) 9 circular buffer ready queue 10 rQ {} 11 Populate ready queue with Processor ids 12 PopulateRQ(rQ) 13 while scheduled tTB do 14 lt 0 15 do in parallel 16 Sweep doorbells and populate observed task counts into tqState 17 Aggregate locally observed task counts into lt 18 end 19 qS, taskTally 0 20 qS is the inclusive output 21 WarpInclusiveSum(lt, qS, tasktally) 22 while tasktally 0 do 23 Repopulate rQ with ready processor ids 24 do in parallel 25 Starting at rQ[qS], signal processors about task indices from tqState 26 end 27 end while 28 if threadId 0 then 29 tTB AtomicLoad(taskBound) 30 end if 31 tTB WarpBroadcast(tTB) 32 end while 33 InterruptSubscribers() 34 InterruptProcessors() 35 end 24 E.3 Subscriber Algorithm 4: Subscriber Actor: executed by three warps Input: Tϕ R2 E C, Gϕ RS E O RS H, X RE H D 1 begin 2 interrupt GetSharedInterrupt() 3 flags GetSymmetricFlags() 4 tQ GetTQ() 5 Predefined upper bound on the number of tasks. 6 We modulate this value to the actual task count computed 7 dispatch signals received from peer GPUs 8 taskBound GetTaskBound() 9 while AtomicLoad(interrupt) False do 10 dispatch flags 11 do in parallel 12 Visit dispatch flags 13 Atomically retrieve signal 14 if Signal is set and flag is not visited then 15 Mark visited 16 SelfCorrectTaskBound(taskBound, Signal) 17 Enforce memory consistency before consuming packet 18 Decode packet into a set of GEMM0 task descriptors using X 19 Write task descriptors to tQ 20 Notify Scheduler of decoded tasks 21 end if 22 end 23 Advance flags by number of dispatch flags length 24 Atomically retrieve signal 25 combine signals 26 do in parallel 27 Visit combine flags: one per tile 28 if Signal is set and flag is not visited then 29 Mark visited 30 Enforce memory consistency before consuming packet 31 Decode packet into a set of combine task descriptors using Tϕ, Gϕ, O 32 Write task descriptors to tQ 33 Notify Scheduler of decoded tasks 34 end if 35 end 36 end while 37 end 25 F Multi-Node Evaluation F.1 Setup In this experiment, we seek to evaluate FlashDMoE in the multi-node setting. We use 4 nodes, where each node comprises 4 A100 GPUs fully interconnected via NVLink. Across nodes, each GPU uses a single NIC providing 25 GB s of bandwidth. We set the number of experts to be 16 and assign each GPU to host only one, so the number of local experts is 1. Note that we define MIV formally as follows: MIV Tokens Experts local_experts precision hidden_size 2 nrg where nrg is the number of remote peers and the multiplicative factor of 2 accounts for communication rounds (dispatch and combine). nrg 12 for this experiment. F.2 Results 128 256 512 1024 2048 4096 8192 Tokens 1.2 1.3 1.4 1.5 1.6 1.7 Latency (ms) Network Failure Region Latency vs Number of Tokens Latency (ms) Network Failure Threshold (MIV 12 MB) 0.75 1.50 3.00 6.00 12.00 24.00 48.00 Maximal Incast Volume (MB) Figure 17: Multi-node Latency evaluation. Embbeding dimension is 1024 and FFN intermediate size is 4096. We define Maximal Incast Volume (MIV) as the worst case upper bound for data volume that a NIC receives in a single incast occurence. We observe a sublinear increase in latency as we scale the number of tokens. However, we observe at Tokens 2048, that the application fails to terminate due to failure to receive expectant messages. We hypothesize this failure to be due to buffer overflow at the networking hardware layer as is common for applications that generate many and large messages [18] like our system. We note that this failure is addressable by tuning hardware configurations [52] but we consider this exploration as an exercise orthogonal to this work. 26 G Implementation Table 4: Implementation metrics of FlashDMoE. Metric Value Total lines of code (CUDA C ) 6820 Kernel stack frame size 0 B Spill stores (per thread) 0 Spill loads (per thread) 0 Shared memory usage (per block) 46 KB Registers per thread 255 Max active blocks per SM 2 Compilation time 53 seconds Binary size 29 MB H FP16 Memory Throughput (a) Memory subsystem throughput for FP16 (b) Memory subsystem throughput for FP32 Figure 18: Here, we report the total A100 memory throughput for both FP16 (top) and FP32 (bottom) variants of FlashDMoE. Notably, the FP16 implementation issues approximately 2 more shared memory instructions compared to its FP32 counterpart under identical workloads. We attribute this inefficiency to suboptimal shared memory layouts in FlashDMoE when operating on half-precision data. While this bottleneck is addressable through improved layout strategies, we leave its resolution to future work due to time constraints. 27\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.04667v2 [cs.DC] 9 Jun 2025 FlashDMoE: Fast Distributed MoE in a Single Kernel Osayamen Jonathan Aimuyo Cornell University Byungsoo Oh Cornell University Rachee Singh Cornell University Abstract The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, thus offering a scalable path to training massive neural networks. However, existing implementations suffer from low GPU utilization, significant latency overhead, and a fundamental inability to leverage task locality, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashDMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a single persistent GPU kernel. FlashDMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Unlike existing work, FlashDMoE obviates bulk-synchronous collectives for one-sided, device-initiated, inter-GPU (R)DMA transfers, thus unlocking payload efficiency, where we elim- inate bloated or redundant network payloads in sparsely activated layers. When evaluated on an 8-H100 GPU node with MoE models having up to 128 experts and 16K token sequences, FlashDMoE achieves up to 9 higher GPU utilization, 6 lower latency, 5.7 higher throughput, and 4 better overlap efficiency compared to state-of-the-art baselines despite using FP32 while baselines use FP16. FlashD- MoE shows that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML.\n\n--- Segment 2 ---\nWhen evaluated on an 8-H100 GPU node with MoE models having up to 128 experts and 16K token sequences, FlashDMoE achieves up to 9 higher GPU utilization, 6 lower latency, 5.7 higher throughput, and 4 better overlap efficiency compared to state-of-the-art baselines despite using FP32 while baselines use FP16. FlashD- MoE shows that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML. FlashDMoE Comet FasterMoE Megatron DeepEP 0 20 40 60 80 100 Average SM Util ( ) 93.17 42.31 9.67 59.11 13.55 Forward Pass E 64 k 2 2 A100s is better (a) GPU SM Utilization 4K 8K 16K Number of Tokens 0 10 20 30 40 Runtime (ms) 2.1 3.7 7.0 10.3 21.3 45.3 9.9 18.9 36.9 10.0 18.9 36.9 Forward Latency E 32 k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (b) Scaling Tokens 2 4 8 of H100s 0 5 10 15 20 25 Runtime (ms) 3.3 3.4 3.7 10.3 12.6 21.3 4.2 16.6 18.9 4.4 9.0 18.9 Forward Latency T 8K E 32 k 2 is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (c) Weak Scaling across GPUs 8 16 32 64 128 Number of Experts 0 10 20 30 40 50 60 Runtime (s) Forward Latency T 16K k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (d) Across Experts Figure 1: FlashDMoE performance. Correspondence to Preprint. Under review. 1 Introduction Attention Attention T! T" O! O" All-to-All (Dispatch) E! E" E E Attention T! T" O! O" FFN Attention Gate T! T" O! O" Gate Gate E!\n\n--- Segment 3 ---\nT" O! O" Gate Gate E! E" E E All-to-All (Combine) (a) Transformer (b) MoE (c) Distributed MoE Figure 2: Transformer blocks (a) without MoE, (b) with MoE, and (c) with distributed MoE and expert parallelism. T, E, and O represent input tokens, experts, and output activations, respectively. State-of-the-art large language models (LLMs), including DeepSeek-v3 [1], LLama4 [2], DBRX [3] and Snowflake Arctic [4], have adopted the Mixture-of-Experts (MoE) architec- ture for its computational efficiency and strong performance across many tasks. The traditional Transformer block consists of a self-attention module followed by a dense feed-forward net- work (FFN) [5]. In contrast, MoE architectures replace this single FFN with identically sized FFNs, otherwise known as experts, (Figure 2(b)). A trainable neural network, known as a gate function, sparsely activates these experts by dy- namically routing input tokens to selected ex- perts at runtime. This increase in model param- eters (more FFNs) improves model quality with- out a corresponding increase in computational cost. Communication overheads in MoE. As MoE model sizes grow, GPU memory constraints prevent hosting all experts on a single device. The standard practice is to distribute experts across multiple GPUs using expert parallelism (EP), which requires token routing via many-to-many communication in AllToAll or AllGather [1, 4, 3, 6]. Another round of said many-to-many communication is also necessary for restoring the permuted tokens processed by experts to their original order within the sequence. Existing work has observed these communication operations taking 68 of total runtime [7, 8], during which the GPU is completely idle, unless the implementation explicitly overlaps with computation. This form of pipelining is challenging to achieve efficiently because it requires asynchronous GPU-driven communication and kernel fusion to maximize the overlap efficiency. Typically, inter-GPU communication APIs available in frameworks like PyTorch are not of this kind but instead are CPU-driven [9]. Kernel launch overheads in MoE.\n\n--- Segment 4 ---\nTypically, inter-GPU communication APIs available in frameworks like PyTorch are not of this kind but instead are CPU-driven [9]. Kernel launch overheads in MoE. The efficacy of communication overlap is further limited by the overhead of launching many kernels from the CPU. Specifically, existing implementations [10 13] require launching a large number of kernels per a single layer pass (see Table 1). Frequent kernel launches negatively affect performance by: (1) creating non-deterministic kernel start times across GPUs, exacerbating straggler issues; (2) introducing unnecessary synchronization points, causing GPUs to wait on peers or the CPU before proceeding; and (3) incurring repeated global memory round trips at kernel boundaries. Although CUDA graphs [14] can partially mitigate the first issue in static workloads, they are incompatible with MoE s dynamic expert routing patterns. Addressing the remaining issues requires novel solutions, which we provide in this work through complete kernel fusion and asynchronous device-initiated communication. Attn CPU GPU NIC Gate A2A (Dispatch) E A2A (Combine) Kernel launch GPU kernel execution Remote communication Attn CPU GPU NIC Gate A2A (Dispatch) E A2A (Combine) Attn CPU GPU NIC Gate E Ours: FlashDMoE E E Gate A2A Expert A2A Expert Scale Expert Baseline MoE FlashDMoE Fused Kernel Time (milliseconds) 5 10 15 No Overlap Some Overlap Single Kernel Figure 3: Comparing FlashDMoE with state-of-the-art techniques that either do not overlap commu- nication and computation (left, top) or do some overlap (left, middle). FlashDMoE is a persistent kernel that fuses all computation and communication of the MoE operator (left, bottom). FlashDMoE implements device-initiated computation (gate, expert FFN, scale) and communication tasks (right). 2 Table 1: Kernel Fusion Comparison. Our method is the first to fully fuse the DMoE layer into a single GPU kernel. We report GPU operations from profiling with Nsight Systems. We count within a single layer (Gate Dispatch Expert Combine) on 2 A100s, where each GPU has 32 experts.\n\n--- Segment 5 ---\nWe report GPU operations from profiling with Nsight Systems. We count within a single layer (Gate Dispatch Expert Combine) on 2 A100s, where each GPU has 32 experts. Works Launched GPU Ops FlashDMoE 1 COMET [11] 33 Megatron-LM CUTLASS [12, 15] 85 Megatron-LM TE [12, 15] 261 Megatron-LM DeepEP [1] 432 DeepSpeedMoE [10] 550 1.1 Our Contributions: DMoE in a single kernel To overcome these fundamental inefficiencies in state-of-the-art MoE models, we develop FlashD- MoE, where we integrate all DMoE computation and communication tasks into a single persistent GPU kernel i.e., a kernel that remains active for the entirety of the MoE operator (Figure 3 bottom left). Instead of multiple kernel launches coordinated by the CPU, FlashDMoE requires launching only one kernel, significantly reducing the involvement of the CPU. Within the fused kernel, FlashDMoE implements a reactive programming model to achieve fine-grained parallelism and loosely coupled, non-blocking execution among tens of thousands of GPU threads. In-kernel Block scheduling and Tile parallelism. FlashDMoE implements tile-level parallelism, meaning it partitions input token matrices into smaller, independent units called tiles, which are processed by blocks but managed (scheduled and constructed) by warps. We specialize every thread block, except one, as processors to perform compute. In addition, we designate a dedicated Operating System (OS) block (4 warps) to perform administrative tasks of (1) scheduling computational work to processors (scheduler), and (2) decoding computational tasks from messages received from other GPUs (subscriber). This design allows FlashDMoE to dynamically assign tasks to GPU blocks based on readiness, ensuring that no GPU SM remains idle throughout the lifetime of the DMoE operator. FlashDMoE selects tile dimensions to maximize GPU arithmetic intensity while still benefitting from a high-degree of parallelism. Asynchronous and payload-efficient communication. By redesigning the MoE operator from the ground up, FlashDMoE resolves fundamental inefficiencies inherent in the conventional MoE execution pipeline. One notable inefficiency is token padding during communication.\n\n--- Segment 6 ---\nBy redesigning the MoE operator from the ground up, FlashDMoE resolves fundamental inefficiencies inherent in the conventional MoE execution pipeline. One notable inefficiency is token padding during communication. To simplify programming complexity and due to symmetry constraints of collective communication APIs, existing implementations have to zero-pad token payloads to match predefined buffer sizes. This occurs when tokens are asymmetrically routed to experts, resulting in GPUs receiving much less than the expected capacity. However, these null payloads waste communication bandwidth, bloat data transfer latency and may lead to unnecessary computations on null matrices in some implementations. FlashDMoE introduces payload-efficient communication by sending non-padded tokens only to GPUs with actively selected experts, conserving both communication and computational resources. Technical challenges. Realizing the single-kernel design of FlashDMoE required solving several technical challenges to achieve high performance: (1) lightweight computational dependency man- agement; (2) navigating optimal SM occupancy configurations; (3) implementing in-device BLAS operations; (4) minimizing inter- and intra-device synchronization overheads; (5) implementing transfer-awareness by leveraging DMA over Unified Virtual Addressing (UVA) when available. In addressing these challenges, FlashDMoE s design presents a radical departure from traditional synchronous AllToAll collectives, where GPUs exhibit significant idle time during layer execution. For device-initiated communication, FlashDMoE uses NVSHMEM [16] to establish a global address space across all GPUs to achieve the aforementioned Direct Memory Access (DMA) or Remote DMA (RDMA) communication. For in-device BLAS, FlashDMoE develops custom high-performance GEMM operations via CUTLASS [17]. Results. We evaluate FlashDMoE across multiple GPUs split across multiple nodes. Our evaluations show that FlashDMoE achieves 6 latency speedup, 9 higher GPU utilization, 4 better weak scaling efficiency and 5.7 increased throughput compared to state-of-the-art implementations. We 3 project these performance gains becoming even better in multi-node scenarios, where inter-node communication occurs using lower bandwidth inter-node links (e.g., RDMA, Infiniband). 2 Motivation 2.1 Synchronous Communication and Stragglers Figure 4: Overlapped Schedule (bottom) showing how idle time from the sequential schedule (top) is repurposed for computation.\n\n--- Segment 7 ---\nWe 3 project these performance gains becoming even better in multi-node scenarios, where inter-node communication occurs using lower bandwidth inter-node links (e.g., RDMA, Infiniband). 2 Motivation 2.1 Synchronous Communication and Stragglers Figure 4: Overlapped Schedule (bottom) showing how idle time from the sequential schedule (top) is repurposed for computation. FlashDMoE implements the overlapped schedule. AlltoAll or AllGather communication as currently used in MoE frameworks is a synchronous collective operation, whose completion requires the participation of all involved GPUs. Here, disparities in processing speeds or kernel scheduling among GPUs induce a straggler effect detrimental (Figure 4) to (1) the collective operation s performance and (2) E2E performance, as stalled GPUs cannot proceed to downstream dependent or independent tasks until the collective terminates. Specifically, as shown in Figure 15, for distributed training of a 1.3B GPT-3 MoE model across 32 A100 GPUs, we see P95 communication performance degradation of 1.32X when compared to the mean actual kernel time from Figure 15b. This performance reduction is rather tame as the underlying hardware is a supercomputer well-tuned against software jitter [18]. However, we observe a more severe p95 performance loss of 11X in a single-node Virtual Machine (VM). In line with prior HPC works [19, 20], we argue that obviating the inherent barrier in this synchronous collective communication would allow GPUs to repurpose this observed idle time for downstream computation as depicted in Figure 4. Table 2: Straggler Delay within Synchronous All-to-All communication. We capture the distribution of delay induced by stragglers across many steps. Let Actual Time ta denote the fastest kernel execution time across all GPUs, and Total Time t be the maximum recorded step time. We define Delay as the maximum difference between t and ta. Note Delay is idle time. For the 1x8 V100, we profile 1750 steps and 600 steps for the 8x4 A100. See Figure 15 for the raw distribution. System Nodes GPUs Median p95 Commercial VM (V100) 1 8 3.1x 11.4x Supercomputer (A100) 8 32 1.09x 1.32x 4 2.2 Kernel launch overhead.\n\n--- Segment 8 ---\nSee Figure 15 for the raw distribution. System Nodes GPUs Median p95 Commercial VM (V100) 1 8 3.1x 11.4x Supercomputer (A100) 8 32 1.09x 1.32x 4 2.2 Kernel launch overhead. Comet FasterMoE Megatron DeepEP 0 20 40 60 80 100 Average SM Util ( ) 42.31 9.67 59.11 13.55 Forward Pass E 64 k 2 2 A100s is better (a) GPU SM Utilization across baselines MoE Iteration [6.557 ms] FlashDMoE DeepEP (b) Kernel Launch overhead (CUDA API row) Figure 5: 5a shows GPU utilization averaged across 100 MoE forward passes on 2 NVLinked A100s with 300 GB s unidrectional bandwidth. Despite the high-bandwidth interconnect, we observe up to 90 idle time, which we attribute to kernel launch gaps and non-overlapping communication. We compare the kernel launch overheads between FlashDMoE and existing baselines. Table 1 shows the number of kernel launches during a single forward pass: FlashDMoE launches exactly one persistent kernel, while the baselines launch up to 550 short-lived kernels to perform the same computation. Figure 5 provides a visual comparison using CUDA API traces captured by NSight Systems, illustrating the difference between FlashDMoE and DeepEP. DeepEP exhibits numerous small CUDA API calls, with frequent stalls between individual operators, leading to increased GPU idle time (Figure 5a). In contrast, FlashDMoE maintains high GPU utilization by avoiding launch overhead and synchronization gaps achieving 93.17 GPU utilization compared to 14 for DeepEP. See 4 for experimental results and A for a discussion of related work. 3 Fused MoE Kernel Design Block n-1 FusedGate Packet Encode and Dispatch Subscriber Block n-2 Block 1 Block 0 ... ...\n\n--- Segment 9 ---\nSee 4 for experimental results and A for a discussion of related work. 3 Fused MoE Kernel Design Block n-1 FusedGate Packet Encode and Dispatch Subscriber Block n-2 Block 1 Block 0 ... ... Warp Processor 1 Processor 0 Processor n-1 ... Scheduler warp 0 warp 1 warp 2 warp 3 OS Decoded Tasks Schedule Task t to Processor p Remote Packets Send Tile to GPU x Send Tile to GPU y Send Tile to GPU z Figure 6: FlashDMoE Fused Kernel Modern distributed MoE systems suffer from two limitations: (1) frequent many-to-many (AlltoAll or AllGather) collectives on the critical path, and (2) significant overhead from repeated kernel launches. We address these in FlashDMoE, a fully fused MoE operator implemented as a single persistent GPU kernel. Unlike previous approaches [11, 1, 10, 12, 21, 8, 22 26], FlashDMoE is the first solution to implement a completely fused Distributed MoE kernel, eliminating kernel launch overhead entirely by requiring only a single kernel launch (see Table 1). 5 Algorithm 1: FlashDMoE Distributed MoE Fused Kernel Input: A, O RS H, X RE H D, N 1 begin 2 Tϕ, Gϕ FusedGate(A) 3 if blockId 1 N then 4 Dispatch(Tϕ, A) 5 processor::start() 6 else 7 if warpID 0 then 8 scheduler::start() 9 else 10 subscriber::start(Tϕ, Gϕ, O, X) 11 end if 12 end if 13 end Figure 7: DMoE Functional Dependencies Expressed as a Chain of Actor Interactions. We denote Sb, Sh, and P as the Subscriber, Scheduler and Processor actors, respectively. For any actor a {Sb, Sb, P}, ai identifies an actor on GPU i. We define Dj i as the operator, where GPU j dispatches packets of tiles to GPU i, This diagram expresses task dependencies at the granularity of a tile, namely GEMM0, GEMM1, combine and communication produce an output tile. Notifications occur as signals propagated through shared memory (subscriber scheduler) or global memory (scheduler processor or inter-GPU communication).\n\n--- Segment 10 ---\nWe define Dj i as the operator, where GPU j dispatches packets of tiles to GPU i, This diagram expresses task dependencies at the granularity of a tile, namely GEMM0, GEMM1, combine and communication produce an output tile. Notifications occur as signals propagated through shared memory (subscriber scheduler) or global memory (scheduler processor or inter-GPU communication). Note one-sided inter-GPU transfers (packet or single tile) are coupled with a signal to notify Sj b on the receiving GPU j of the message s delivery. Actor-based model. The design of FlashDMoE is based on the actor model of concurrent compu- tation [27 29]. We implement this model by specializing GPU thread blocks and warps into three distinct actor roles: (1) Processor ( E.1), (2) Subscriber ( E.3), and (3) Scheduler( E.2). The Processor performs compute (GEMMs and element-wise operations) and tile communication. We use CUTLASS [17] as the underlying infrastructure for high-performance BLAS routines and NVSH- MEM for kernel-initiated communication [16]. The Subscriber and Scheduler perform administrative functions. Specifically, the Scheduler assigns computational tasks to available thread blocks. Our key innovation is making the Scheduler both multithreaded, enabling high scheduling throughput, and work-conserving, ensuring consistently high GPU SM utilization. On the other hand, the Subscriber decodes tile packets from peer GPUs to task descriptors ( 3.1). Of the N thread blocks on a GPU, we specialize N 1 to adopt the Processor role. We specialize the last block as the Operating System (OS). Within this block, we specialize three warps for the Subscriber role and one warp for the Scheduler role. This split of thread blocks across actors is intentional: our goal is to use few resources for administrative tasks while reserving bulk of the resources for performing MoE computation tasks. Figure 6 summarizes the FlashDMoE architecture and its constituent actors, while Algorithm 1 gives a very close translation of the system in code.\n\n--- Segment 11 ---\nThis split of thread blocks across actors is intentional: our goal is to use few resources for administrative tasks while reserving bulk of the resources for performing MoE computation tasks. Figure 6 summarizes the FlashDMoE architecture and its constituent actors, while Algorithm 1 gives a very close translation of the system in code. Note that A RS H is the input token matrix; O RS H the output matrix; and X RE H D is a 3-D tensor of expert weights, where E denotes the number of local experts for the executing GPU, H is the embedding dimension, D is the FFN intermediate dimension and S is the sequence length. Tϕ R2 E C is a routing table data structure, where Tϕ (e, c) (i, w) indicates that token i at slot c dispatches to expert e. w is the combine weight (Equation 2) and C is expert capacity. The tuple structure of Tϕ is an implementation detail. Gϕ RS E captures the affinity scores produced by the gate (Equation 3). Inter-actor interactions in FlashDMoE. FlashDMoE decomposes MoE computation and com- munication at the granularity of a tile, a statically sized partition of a tensor, to achieve parallel execution and efficient overlap of tasks. Each tile maps to a discrete unit of work encapsulated by a task descriptor. The Subscriber decodes these task descriptors from the remote tile packets it receives. Concurrently, the Scheduler receives notifications about available tasks and dispatches them for execution to Processor actors that perform computations defined by these tasks, namely 6 Global Memory Shared Memory Shared Memory Global Memory Registers Thread Block 80 GB 46 KB Block 1 KB thread Global Memory Shared Memory Registers Device Thread Block Shared Memory Figure 8: GPU Memory Hierarchy. The inverted pyramid (left) shows the load store access la- tency [30 32]. The table above outlines the capacity for different memory tiers (for A100 GPUs). The shared memory and register capacity are static configurations for FlashDMoE. The right figure shows accessibility scopes: on-chip registers are scoped to a thread; on-chip shared memory is visible to all threads in a block; and off-chip global memory is accessible by all threads on device. the feed-forward network (FFN) and expert-combine operations.\n\n--- Segment 12 ---\nThe right figure shows accessibility scopes: on-chip registers are scoped to a thread; on-chip shared memory is visible to all threads in a block; and off-chip global memory is accessible by all threads on device. the feed-forward network (FFN) and expert-combine operations. Figure 7 show the chain of actor interactions, demonstrating how FlashDMoE enforces DMoE functional dependencies. Determining tile dimensions in FlashDMoE. Selecting appropriate tile dimensions in FlashDMoE is crucial to ensure efficient GPU utilization. An undersized tile underutilizes the GPU, while excessively large tiles create register pressure, causing performance-degrading register spills to local memory. After careful parameter sweeps, we choose tile dimensions of (128, 64). Our key insights are: increasing tile width significantly raises the register usage per thread, potentially triggering costly spills; increasing tile height without adjusting thread count increases workload per thread, harming performance. Raising the thread count per block beyond our fixed value of 128 threads reduces the number of concurrent blocks, negatively affecting SM occupancy. Larger thread-block sizes also increase overhead from intra-block synchronization (__syncthreads() barriers), further degrading performance. Thus, our chosen tile dimensions balance register usage, shared-memory constraints, and GPU occupancy to deliver optimal performance. 3.1 Task Abstraction for Computation Computational operators. The FFN operator is a standard position-wise feed-forward network widely used in Transformer architectures [5], composed of two linear transformations separated by a nonlinear activation ϕ (e.g., GELU or ReLU): FFN(x) W2 ϕ(xW1 b1) b2 (1) Here, W1 and W2 represent learnable weight matrices, and b1 and b2 are biases. The expert-combine operation, used in architectures like GShard [33] and DeepSeek [1], merges outputs from multiple experts by computing a weighted combination based on their affinity scores: Ci k X j 1 gi,e (2) hi k X j 1 gi,e Ci hk i (3) In these equations, i 0, S 1 represents an input token index, e Ei,k identifies the k-th expert selected for token i, and gi,e is the affinity score indicating how relevant expert e is for token i. Unified task abstraction.\n\n--- Segment 13 ---\nThe expert-combine operation, used in architectures like GShard [33] and DeepSeek [1], merges outputs from multiple experts by computing a weighted combination based on their affinity scores: Ci k X j 1 gi,e (2) hi k X j 1 gi,e Ci hk i (3) In these equations, i 0, S 1 represents an input token index, e Ei,k identifies the k-th expert selected for token i, and gi,e is the affinity score indicating how relevant expert e is for token i. Unified task abstraction. We unify the FFN and combine operations under a common abstraction called a task. Tasks provide a uniform interface for communicating tile-level work among Subscribers, Schedulers, and Processors. Formally, a task descriptor t T is defined as a tuple: t (M, , ϕ) 7 where M is a set of metadata (e.g., device ID, tile index), is a binary tensor operation (specifically, matrix multiplication or Hadamard product ), and ϕ is an element-wise activation function (e.g., ReLU or identity). We define a task t operating on input tensors A, B, D, producing output tensor C, as follows: Ft(A, B, C, D) : C ϕ (A t B D) (4) The operator t (instantiated from ) may behave differently depending on the task metadata M, and the result of A t B is accumulated into D. We provide an example of task metadata in D. In practice, we implement each task defined by Equation 4 as a single fused __device__ decorated function which the Processor (Algorithm 2) invokes at runtime. Fusion for t entails applying ϕ and the succeeding addition operation to registers storing the results of the binary operator t. To illustrate its flexibility, we show how the FFN and expert-combine operations can be expressed using this task framework. Note that we omit the matrix multiplication symbol ( ) for simplicity. Also, ϕ1 can be any activation function, while ϕ2 is the identity function.\n\n--- Segment 14 ---\nNote that we omit the matrix multiplication symbol ( ) for simplicity. Also, ϕ1 can be any activation function, while ϕ2 is the identity function. The FFN is expressed as: t1 (M, , ϕ1), t2 (M, , ϕ2), Ft1(A, B1, C1, D1) : C1 ϕ1 (AB1 D1) , Ft2(C1, B2, C2, D2) : C2 ϕ2 (C1B2 D2) . Whereas, the expert-combine operation is formalized as: t3 (M, , ϕ2), Ft3(A, S, C, C) : C ϕ2 (A S C) . 3.2 Symmetric Tensor Layout for Inter-GPU Communication Exper t Par allel Gr oup B0 B1 B0 B1 R0 R1 P0 P1 E0 E1 P0 E2 E3 P1 E0 E1 E2 E3 E0 E1 E0 E1 E0 E1 E0 E1 E0 E1 E2 E3 B0 B1 B0 B1 R0 R1 P0 P1 E0 E1 E2 E3 E2 E3 E2 E3 E2 E3 E2 E3 E0 E1 E2 E3 P0 View P1 View (a) Symmetric Tensor Layout across 2 Expert-parallel Processes. P1 R0 B0 E2 P1 R1 B1 E2 P0 R1 B0 E2 P0 R0 B1 E2 2.) P0 stages the outgoing message to P1 before remote transfer 3.) P1 receives 4.) P1 processes 5.) P1 stages 6.) P0 receives Start 1.) P0 intends to Dispatch to E2 on P1 P1 R1 B1 E2 P0 R0 B1 E2 2.) P1 receives 4.) P0 receives Start 1.) P0 intends to Dispatch to E2 on P1 3.) P1 processes (b) State machine for DMA (top) and RDMA (bottom) communication. Within a single GPU device, the actors in FlashDMoE communicate through the GPU s memory subsystem (see Figure 8).\n\n--- Segment 15 ---\nP1 processes (b) State machine for DMA (top) and RDMA (bottom) communication. Within a single GPU device, the actors in FlashDMoE communicate through the GPU s memory subsystem (see Figure 8). Specifically, the Scheduler and Subscriber actors exchange data via fast shared memory, while other actor pairs communicate through global memory. For communication across multiple devices, FlashDMoE uses device-initiated communication, leveraging the one-sided PGAS (Partitioned Global Address Space) programming model [34]. However, achieving scalable and correct one-sided memory accesses in PGAS without costly synchronization is a known challenge [1, 35]. We address this challenge with a provably correct and scalable solution: a symmetric tensor layout L, supporting fully non-blocking memory accesses. We define L as: L RP R B E C H 8 where: P is the expert parallel world size, R identifies communication rounds (i.e., two rounds, one for token dispatch and one for combine), B is number of staging buffers, E is the number of local experts, C is the upscaled expert capacity ( 3.2.1) and H is the token embedding dimension. Our core insight to enable non-blocking communication is temporal buffering. Specifically, we overprovision memory for the underlying token matrix by at least 2 r times, where r is the number of communication rounds in the dependency graph, and the factor of 2 accounts for separate buffers for incoming and outgoing data within each communication round. For MoE models, we have 2 r 4. This modest increase in memory usage eliminates the need for synchronization during one-sided data transfers. Figure 9b illustrates how cells within this symmetric tensor layout are indexed and used for Direct Memory Access (DMA) and Remote DMA (RDMA) operations. As Theorem 3.1 reinforces, this indexing scheme over L is the underlying mechanism that allows for fully non-blocking accesses eliding synchronization because all accesses are write conflict-free. See C for the proof. Theorem 3.1. The symmetric tensor layout L is write-write conflict-free. To construct L, we start from the original token buffer T RS H, where S is the sequence length and H is the token embedding dimension.\n\n--- Segment 16 ---\nThe symmetric tensor layout L is write-write conflict-free. To construct L, we start from the original token buffer T RS H, where S is the sequence length and H is the token embedding dimension. We first reorganize the sequence dimension S into three sub-dimensions representing the expert capacity (C), local expert slots (E), and the expert parallel world size (W), st: C E W C E S , where S S and E EW In the typical case of uniform expert distribution (illustrated in Figure 9a), we have S S and E EW , where EW is the total number of experts in the model. Thus, the size of the token buffer is Size(T) S H. In Figure 9a, each cell labeled Ei (with i {0, . . . , 3}) is a matrix of size (C, H). Extending prior work [33, 11], we introduce additional temporal dimensions R (communication rounds) and B (staging buffers). Each communication round has two fixed staging slots: one for outgoing tokens and another for incoming tokens. Each slot, indexed by dimension P, forms a tensor of shape (S , H). Therefore, the tensor size Size(L) is generally at least four times the original token buffer size, becoming exactly four times larger in the case of uniform expert distribution. Empirically, we find: Size(L) 4 Size(T) 3.2.1 In-place Padding for Payload Efficiency Due to the dynamic and uneven distribution of tokens in MoE dispatch [36], GPUs commonly receive fewer tokens than their predefined expert capacity. Current MoE frameworks [10] typically pad these buffers with null tokens before computation, unnecessarily increasing communication payloads and degrading performance. In contrast, we propose in-place padding, performing padding directly within the local symmetric tensor buffers and thus eliminating excess network communication. As we show in Figure 9a as a reference, each cell Ei is sized according to the expert capacity C. We further align this capacity to ensure divisibility by the tile block size bM 128, guaranteeing safe and aligned memory reads by Processor threads consuming remote tokens.\n\n--- Segment 17 ---\nIn contrast, we propose in-place padding, performing padding directly within the local symmetric tensor buffers and thus eliminating excess network communication. As we show in Figure 9a as a reference, each cell Ei is sized according to the expert capacity C. We further align this capacity to ensure divisibility by the tile block size bM 128, guaranteeing safe and aligned memory reads by Processor threads consuming remote tokens. This in-place padding strategy slightly increases the memory footprint of L, as described below: Size(L) ( 4 Size(T), S E bM 4 bM E S Size(T), otherwise 4 Experiments We implement ( G) and evaluate FlashDMoE and evaluate across five metrics: Forward Latency ( 4.2), GPU Utilization ( 4.3), Overlap Efficiency ( 4.4), Throughput ( 4.5), and Expert Scalability ( 4.6). We run experiments on a server with 8 NVIDIA H100 80G GPUs interconnected via NVLink, 125 GB of RAM, and 20 vCPUs. We used PyTorch 2.6.0, CUDA 12.8, and Ubuntu 22.04. All experiments use MoE transformer models configured with 16 attention heads, an embedding dimension of 2048, and an FFN intermediate size of 2048. We apply Distributed Data Parallelism (DDP) and Expert Parallelism for all experiments. We execute only the forward pass over a single MoE layer and measure the average runtime of 32 passes after 32 warmup passes. We use top-2 routing with a capacity factor of 1.0. We compare FlashDMoE against several state-of-the-art MoE 9 systems: (1) Comet [11], (2) FasterMoE [13], (3) Megatron-CUTLASS [37], and (4) Megatron- TE: Megatron-LM with Transformer Engine [38]. Comet relies on cudaMemcpyPeerAsync [39], while FasterMoE and Megatron-LM use NCCL exclusively for communication. We also evaluate FlashDMoE on a multi-node environment and discuss our findings in F. 4.1 Desiderata In our experiments, we observe Comet exhibiting anomalously bad performance values at 8 GPUs, so we exclude their results from evaluations at 8 GPUs and only include for results at 4 GPUs.\n\n--- Segment 18 ---\nComet relies on cudaMemcpyPeerAsync [39], while FasterMoE and Megatron-LM use NCCL exclusively for communication. We also evaluate FlashDMoE on a multi-node environment and discuss our findings in F. 4.1 Desiderata In our experiments, we observe Comet exhibiting anomalously bad performance values at 8 GPUs, so we exclude their results from evaluations at 8 GPUs and only include for results at 4 GPUs. Note we evaluate FlashDMoE using FP32 precision whereas all baselines use FP16. We do so because (1) no baseline supports FP32 and (2) time constraints prevent us from tuning our system to peak performance at FP16. Most importantly, this precision discrepancy disadvantages FlashDMoE by doubling the communication and computation precision, making our results a conservative lower bound. Yet, as we show in the succeeding sections, FlashDMoE outperforms all baselines. 4.2 Forward Latency 4K 8K 16K Number of Tokens 0 5 10 15 20 25 Runtime (ms) 2.1 3.4 6.2 3.0 5.4 10.0 7.9 12.6 22.4 4.9 16.6 17.4 5.1 9.0 17.2 Forward Latency E 32 k 2 4 H100s is better FlashDMoE Comet FasterMoE Megatron-CUTLASS Megatron-TE (a) 4 H100s 4K 8K 16K Number of Tokens 0 10 20 30 40 Runtime (ms) 2.1 3.7 7.0 10.3 21.3 45.3 9.9 18.9 36.9 10.0 18.9 36.9 Forward Latency E 32 k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (b) 8 H100s Figure 10: Forward Latency as the Number of Tokens per GPU increases. We first measure the forward latency of FlashDMoE across different sequence lengths on both 4 and 8 GPU setups (Figure 10). FlashDMoE consistently outperforms all baselines, with especially notable improvements at longer sequence lengths. On 4 GPUs, it achieves up to 4.6x speedup over Megatron-TE at 16K tokens, and 2.6x over FasterMoE.\n\n--- Segment 19 ---\nFlashDMoE consistently outperforms all baselines, with especially notable improvements at longer sequence lengths. On 4 GPUs, it achieves up to 4.6x speedup over Megatron-TE at 16K tokens, and 2.6x over FasterMoE. The gains are even more pronounced at 8 GPUs where FlashDMoE maintains low latency, exhibiting up to 6.4x speedup over baselines that degrade steeply due to increasing communication costs as token buffers increase proportionally. These results highlight FlashDMoE s ability to scale token throughput without suffering from the communication penalties that plague other implementations. 4.3 GPU Utilization To quantify GPU efficiency, we measure Streaming Multiprocessor (SM) utilization during the forward pass (Figure 11). FlashDMoE achieves 93.17 average SM utilization, over 9x higher than FasterMoE (9.67 ), 6.8x higher than DeepEP Megatron-LM (13.55 ) 4x higher than Megatron-TE (59.11 ), and 2.2x higher than Comet (42.31 ). This improvement stems from our fully fused kernel architecture and fine-grained pipelining of compute and communication tasks. By eliminating idle gaps due to kernel launches and enabling in-kernel task scheduling, FlashDMoE ensures SMs remain busy with productive work throughout execution. 4.4 Overlap Efficiency We evaluate the extent to which FlashDMoE overlaps communication and computation by measuring weak scaling efficiency as the number of GPUs increases (Figure 12b). We note that most baselines fail to execute at a single GPU, hence why we use 2 GPUs as the reference point. We observe that Megatron-CUTLASS and Megatron-TE degrade significantly, with overlap efficiency dropping below 0.5 at 4 GPUs. FlashDMoE gives up to 3.88x and 4x higher efficiency at 4 and 8 GPUs, respectively. Figure 12a further illuminates this efficiency, as FlashDMoE shows stable forward latency growth, 10 FlashDMoE Comet FasterMoE Megatron DeepEP 0 20 40 60 80 100 Average SM Util ( ) 93.17 42.31 9.67 59.11 13.55 Forward Pass E 64 k 2 2 A100s is better Figure 11: Comparison of SM utilization, defined as the ratio of cycles in which SMs have at least one warp in flight to the total number of cycles [40].\n\n--- Segment 20 ---\nFlashDMoE gives up to 3.88x and 4x higher efficiency at 4 and 8 GPUs, respectively. Figure 12a further illuminates this efficiency, as FlashDMoE shows stable forward latency growth, 10 FlashDMoE Comet FasterMoE Megatron DeepEP 0 20 40 60 80 100 Average SM Util ( ) 93.17 42.31 9.67 59.11 13.55 Forward Pass E 64 k 2 2 A100s is better Figure 11: Comparison of SM utilization, defined as the ratio of cycles in which SMs have at least one warp in flight to the total number of cycles [40]. Values represent the average SM utilization over 100 iterations. All experiments use T 8K and E 64 on two A100s 2 4 8 of H100s 0 5 10 15 20 25 Runtime (ms) 3.3 3.4 3.7 10.3 12.6 21.3 4.2 16.6 18.9 4.4 9.0 18.9 Forward Latency T 8K E 32 k 2 is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (a) Latency as Number of GPUs increases. 2 4 8 of H100s 0 20 40 60 80 100 Overlap Efficiency ( ) 100 97 90 100 82 48 100 26 22 100 49 24 Forward T 8K E 32 k 2 is better Flash FM M-C M-TE (b) Weak scaling efficiency. Figure 12: Forward Latency as the Number of Tokens per GPU increases. We define Overlap Efficiency Oe to be Oe T(2) T(NG), where T(NG) is the latency at NG GPUs and T(2) is the latency at 2 GPUs. whereas baselines Megatron-CUTLASS and Megatron-TE experience approximately linear latency amplification while FasterMoE exhibits sublinear scaling. We attribute this suboptimal performance to straggler effects and exposed communication. In contrast, FlashDMoE demonstrates uniform latency as expected since the workload per GPU is fixed in this weak scaling experiment. These results further corroborate that FlashDMoE s actor-based design and asynchronous data movement achieve near-ideal overlap, even at scale. 4.5 Throughput Throughput, measured in tokens per second (MTokens s), reflects end-to-end system efficiency.\n\n--- Segment 21 ---\nThese results further corroborate that FlashDMoE s actor-based design and asynchronous data movement achieve near-ideal overlap, even at scale. 4.5 Throughput Throughput, measured in tokens per second (MTokens s), reflects end-to-end system efficiency. As shown in Figure 13, FlashDMoE scales linearly with GPU count, reaching 17.7 MTokens s at 8 GPUs. This is over 5.7x higher than FasterMoE and 4.9x higher than Megatron-TE and Megatron- CUTLASS. Notably, these results are achieved despite FlashDMoE operating entirely in FP32, while baselines use FP16. This indicates that FlashDMoE s design eliminates throughput bottlenecks not by exploiting lower precision, but by maximizing hardware utilization and eliminating host-driven inefficiencies. 4.6 Expert Scalability We analyze how FlashDMoE scales with increasing number of experts at fixed sequence length (T 16K). Note that for the discussed plots, the number of experts on the x-axis is the total number across all GPUs. Each GPU gets 1 8th of this value. As seen in Figure14, FlashDMoE maintains low, uniform latency, as desired, even as the number of experts grows from 8 to 128. In contrast, 11 2 4 8 of H100s 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Throughput (MTps) 4.90 9.50 17.70 1.60 2.60 3.10 3.90 1.90 3.60 3.68 3.60 3.60 Forward Pass T 8K E 32 k 2 is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE Figure 13: Throughput as the amount of GPUs increases. We compute throughput as T NG latency, where NG is the number of GPUs.\n\n--- Segment 22 ---\nIn contrast, 11 2 4 8 of H100s 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Throughput (MTps) 4.90 9.50 17.70 1.60 2.60 3.10 3.90 1.90 3.60 3.68 3.60 3.60 Forward Pass T 8K E 32 k 2 is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE Figure 13: Throughput as the amount of GPUs increases. We compute throughput as T NG latency, where NG is the number of GPUs. 8 16 32 64 128 Number of Experts 0 5 10 15 20 25 30 35 Runtime (s) Forward Latency T 16K k 2 4 H100s is better FlashDMoE Comet FasterMoE Megatron-CUTLASS Megatron-TE (a) 4 H100s 8 16 32 64 128 Number of Experts 0 10 20 30 40 50 60 Runtime (s) Forward Latency T 16K k 2 8 H100s is better FlashDMoE FasterMoE Megatron-CUTLASS Megatron-TE (b) 8 H100s Figure 14: Forward Latency as the Number of experts increases. baselines exhibit superlinear latency increases due to increased kernel launch overheads. FlashDMoE outperforms these baselines by up to 4X at 4 H100s and 6.6X at 8 H100s, both at 128 experts. FlashDMoE s payload-efficient communication and scheduler-driven in-kernel dispatching allow it to sustain expert parallelism without incurring the communication and orchestration penalties seen in other systems. These results reinforce FlashDMoE s scalability for ultra-sparse MoE configurations. 4.7 Memory Overhead We measure the GPU memory required for the symmetric tensor L and runtime bookkeeping state of FlashDMoE. Memory overhead depends primarily on the tile size, expert capacity (EC), and the number of experts (E). Table 3 summarizes memory overhead under various configurations, confirming that FlashDMoE maintains a modest and predictable memory footprint. 5 Limitations and Future Work Despite the performance gains and architectural innovations of FlashDMoE, there are several limita- tions worth acknowledging both practical and conceptual that open the door to future research. Programming complexity.\n\n--- Segment 23 ---\n5 Limitations and Future Work Despite the performance gains and architectural innovations of FlashDMoE, there are several limita- tions worth acknowledging both practical and conceptual that open the door to future research. Programming complexity. Developing fully fused, persistent kernels is a non-trivial engineering task. While FlashDMoE proves the feasibility and benefit of such kernels, their construction demands deep expertise in GPU architectures, synchronization and distributed protocols, and memory hierarchies. This high barrier to entry limits adoption. Future work may consider compiler-level abstractions or DSLs to democratize this technique. FP16 support and shared memory access patterns. Although modern GPUs natively support half-precision computation, adapting FLASHDMOE to FP16 is non-trivial for 12 Table 3: Memory overhead of FlashDMoE (tile size bM 128), Size(T) Tokens 4KB). Tokens Experts EC max(bM, EC) Bookkeeping (MB) Size(L) (MB) Total (MB) 4K 16 256 256 64.57 64.00 128.57 4K 32 128 128 64.55 64.00 128.55 4K 64 64 128 128.90 128.01 256.91 4K 128 32 128 257.96 256.02 513.98 8K 16 512 512 128.95 128.01 256.95 8K 32 256 256 128.90 128.01 256.91 8K 64 128 128 128.90 128.01 256.91 8K 128 64 128 258.15 256.02 514.17 16K 16 1024 1024 257.89 256.02 513.90 16K 32 512 512 257.79 256.02 513.81 16K 64 256 256 257.80 256.02 513.81 16K 128 128 128 258.53 256.02 514.54 the Processor s computational operators. Specifically, our manually tuned swizzle shared memory layouts are not the most efficient template parameters for CUTLASS Collective Mainloop operator which we use to implement our in-device GEMMs. This suboptimal configuration degrades memory throughput as shown in H. Overcoming this for Ampere GPUs and below would require careful investigation of optimal layouts, but for Hopper GPUs and above, we anticipate using the builder interface that CUTLASS provides in our future improvements. Lack of backward pass and training support.\n\n--- Segment 24 ---\nThis suboptimal configuration degrades memory throughput as shown in H. Overcoming this for Ampere GPUs and below would require careful investigation of optimal layouts, but for Hopper GPUs and above, we anticipate using the builder interface that CUTLASS provides in our future improvements. Lack of backward pass and training support. While this work focuses on inference, enabling training requires fusing backward computation and gradient communication into the kernel. Supporting this entails non-trivial changes to both memory bookkeeping and task descriptor definitions. Nevertheless, it remains an exciting direction for extending this system to fully support end-to-end training. 6 Conclusion This work introduces FlashDMoE, the first system to fuse the entire Mixture-of-Experts (MoE) operator into a single, persistent GPU kernel. We show that prevailing MoE implementations suffer from two critical inefficiencies: (1) CPU-managed synchronous communication that leads to underutilized interconnects and (2) fragmented execution via multiple GPU kernels, introducing overhead and synchronization delays. In contrast, FlashDMoE embraces a model of GPU autonomy by embedding computation, com- munication, and scheduling within a unified kernel. It leverages actor-style concurrency, warp specialization, and asynchronous (R)DMA to achieve fine-grained communication computation overlap. Our evaluation demonstrates up to 6 speedup over state-of-the-art systems, up to 9 improved GPU utilization, and 5.7 increased throughput for Distributed MoE. FlashDMoE challenges the dominant execution paradigms in distributed deep learning and presents a compelling template for building future GPU-native systems. While several limitations remain, programming complexity and lack of FP16 support, this work lays the groundwork for a new era of in-kernel distributed computation. Future systems may build upon this foundation to enable kernel fusion for entire training pipelines, ushering in a design shift from CPU orchestration to fully autonomous GPU execution. 7 Acknowledgements This research is supported by NSF Award 2444537 and ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. This work also used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No.\n\n--- Segment 25 ---\n7 Acknowledgements This research is supported by NSF Award 2444537 and ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. This work also used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231 using NERSC award ASCR-ERCAP0030076. We acknowledge and thank Dr. Giulia Guidi for providing access to these NERSC supercomputing resources. 13 References [1] DeepSeek-AI. Deepseek-v3 technical report, 2025. URL 19437. [2] Meta AI. The llama 4 herd: The beginning of a new era of natively multimodal ai innovation, 2025. URL [3] Mosaic Research. Introducing dbrx: A new state-of-the-art open llm, 2024. URL https: www.databricks.com blog introducing-dbrx-new-state-art-open-llm. [4] Snowflake AI Research. Snowflake arctic: The best llm for enterprise ai effi- ciently intelligent, truly open, 2024. URL arctic-open-efficient-foundation-language-models-snowflake . [5] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Ad- vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. [6] Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, and Abhinav Bhatele. A hybrid tensor-expert-data parallelism approach to optimize mixture-of- experts training.\n\n--- Segment 26 ---\n[6] Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, and Abhinav Bhatele. A hybrid tensor-expert-data parallelism approach to optimize mixture-of- experts training. In Proceedings of the 37th ACM International Conference on Supercomputing, ICS 23, page 203 214, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400700569. doi: 10.1145 3577193.3593704. URL 3577193.3593704. [7] Juncai Liu, Jessie Hui Wang, and Yimin Jiang. Janus: A unified distributed training framework for sparse mixture-of-experts models. In Proceedings of the ACM SIGCOMM 2023 Conference, ACM SIGCOMM 23, page 486 498, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702365. doi: 10.1145 3603269.3604869. URL 10.1145 3603269.3604869. [8] Chenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, and Yida Wang. Lancet: Accelerating mixture-of-experts training via whole graph computation-communication overlapping. In P. Gib- bons, G. Pekhimenko, and C. De Sa, editors, Proceedings of Machine Learning and Systems, volume 6, pages 74 86, 2024. URL paper 2024 file 339caf45a6fa281cae8adc6465343464-Paper-Conference.pdf. [9] NVIDIA Collective Communications Library (NCCL). nccl. [10] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. DeepSpeed-MoE: Advancing mixture- of-experts inference and training to power next-generation AI scale. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18332 18346. PMLR, 17 23 Jul 2022.\n\n--- Segment 27 ---\nIn Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 18332 18346. PMLR, 17 23 Jul 2022. URL https: proceedings.mlr.press v162 rajbhandari22a.html. [11] Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, and Xin Liu. Comet: Fine- grained computation-communication overlapping for mixture-of-experts. In MLSys 25. URL [12] NVIDIA. Megatron-lm, 2025. URL readme-ov-file. v0.11.0. [13] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP 22), pages 120 134, 2022. 14 [14] Michael Wendt and Joshua Wyatt. Getting started with CUDA graphs. nvidia.com blog cuda-graphs , 2019. Accessed: 2024-05-15. [15] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384421. doi: 10.1145 3458817.3476209. URL [16] NVIDIA. Nvidia openshmem library (nvshmem), 2025. URL nvshmem api index.html. v3.2.5.\n\n--- Segment 28 ---\nURL nvshmem api index.html. v3.2.5. [17] Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan Yan, Jack Kosaian, Mark Hoemmen, Haicheng Wu, Andrew Kerr, Matt Nicely, Duane Merrill, Dustyn Blasig, Fengqi Qiao, Piotr Majcher, Paul Springer, Markus Hohnerbach, Jin Wang, and Manish Gupta. CUTLASS, 2025. URL [18] NERSC. Network - NERSC Documentation. network , 2025. [Accessed 23-05-2025]. [19] C. Bell, D. Bonachea, R. Nishtala, and K. Yelick. Optimizing bandwidth limited problems using one-sided communication and overlap. In Proceedings 20th IEEE International Parallel Distributed Processing Symposium, pages 10 pp. , 2006. doi: 10.1109 IPDPS.2006.1639320. [20] Yuxin Chen, Benjamin Brock, Serban Porumbescu, Aydin Buluc, Katherine Yelick, and John Owens. Atos: A task-parallel gpu scheduler for graph analytics. In Proceedings of the 51st International Conference on Parallel Processing, ICPP 22, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450397339. doi: 10.1145 3545008.3545056. URL [21] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, HoYuen Chau, Peng Cheng, Fan Yang, Mao Yang, and Yongqiang Xiong. Tutel: Adaptive mixture-of-experts at scale. In D. Song, M. Carbin, and T. Chen, editors, Proceedings of Machine Learning and Systems, volume 5, pages 269 287. Curan, 2023. URL file 5616d34cf8ff73942cfd5aa922842556-Paper-mlsys2023.pdf.\n\n--- Segment 29 ---\nCuran, 2023. URL file 5616d34cf8ff73942cfd5aa922842556-Paper-mlsys2023.pdf. [22] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained mod- els. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP 22, page 120 134, New York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392044. doi: 10.1145 3503221.3508418. URL [23] Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Gang Cao, and Bin Cui. Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement. Proc. ACM Manag. Data, 1(1), May 2023. doi: 10.1145 3588964. URL [24] Shaohuai Shi, Xinglin Pan, Qiang Wang, Chengjian Liu, Xiaozhe Ren, Zhongzhe Hu, Yu Yang, Bo Li, and Xiaowen Chu. Schemoe: An extensible mixture-of-experts distributed training system with tasks scheduling. In Proceedings of the Nineteenth European Conference on Computer Systems, EuroSys 24, page 236 249, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704376. doi: 10.1145 3627703.3650083. URL [25] Hulin Wang, Yaqi Xia, Donglin Yang, Xiaobo Zhou, and Dazhao Cheng. Harnessing inter-gpu shared memory for seamless moe communication-computation fusion. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, PPoPP 25, page 170 182, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400714436. doi: 10.1145 3710848.3710868.\n\n--- Segment 30 ---\nAssociation for Computing Machinery. ISBN 9798400714436. doi: 10.1145 3710848.3710868. URL 3710848.3710868. 15 [26] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Advances in Neu- ral Information Processing Systems, volume 35, pages 16344 16359. Curran Associates, Inc., 2022. URL 67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf. [27] Gul A. Agha. Actors: A model of concurrent computation in distributed systems. Technical report, 1985. MIT Artificial Intelligence Laboratory Technical Reports. [28] Carl Hewitt, Peter Bishop, and Richard Steiger. A universal modular actor formalism for artificial intelligence. IJCAI 73, page 235 245, San Francisco, CA, USA, 1973. Morgan Kaufmann Publishers Inc. [29] Irene Greif. SEMANTICS OF COMMUNICATING PARALLEL PROCESSES. PhD thesis, Massachusetts Institute of Technology, 1975. [30] Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Qiang Wang, and Xiaowen Chu. Benchmarking and Dissecting the Nvidia Hopper GPU Architecture . In 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pages 656 667, Los Alamitos, CA, USA, May 2024. IEEE Computer Society. doi: 10.1109 IPDPS57955.2024.00064. URL https: doi.ieeecomputersociety.org 10.1109 IPDPS57955.2024.00064. [31] Hamdy Abdelkhalik, Yehia Arafa, Nandakishore Santhi, and Abdel-Hameed Badawy. Demysti- fying the nvidia ampere architecture through microbenchmarking and instruction-level analysis, 2022. URL [32] NVIDIA. Ptx isa: Version 8.7, 2025. URL isa_8.7.pdf.\n\n--- Segment 31 ---\nPtx isa: Version 8.7, 2025. URL isa_8.7.pdf. [33] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with con- ditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL [34] Katherine Yelick, Dan Bonachea, Wei-Yu Chen, Phillip Colella, Kaushik Datta, Jason Duell, Susan L. Graham, Paul Hargrove, Paul Hilfinger, Parry Husbands, Costin Iancu, Amir Kamil, Rajesh Nishtala, Jimmy Su, Michael Welcome, and Tong Wen. Productivity and performance using partitioned global address space languages. In Proceedings of the 2007 International Workshop on Parallel Symbolic Computation, PASCO 07, page 24 32, New York, NY, USA, 2007. Association for Computing Machinery. ISBN 9781595937414. doi: 10.1145 1278177. 1278183. URL [35] Size Zheng, Wenlei Bao, Qi Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi Li, Hao- jie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang Lu, Yun Liang, Jidong Zhai, and Xin Liu. Triton-distributed: Programming overlapping kernels on distributed ai systems with the triton compiler, 2025. URL [36] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models, 2024.\n\n--- Segment 32 ---\nURL [36] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge. Blackmamba: Mixture of experts for state-space models, 2024. URL [37] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 15, 2021. [38] NVIDIA. Transformer engine, . URL [39] Bytedance. Flux s overlap performance is worse than non-overlap [4xrtx 4090], 2025. URL issuecomment-2822823236. 16 [40] NVIDIA. NVIDIA Nsight Systems Metrics, . URL com nsight-systems UserGuide index.html?highlight SM 2520active available-metrics. [41] Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, and Olli Saarikivi. Breaking the computation and communication abstraction barrier in distributed machine learning workloads. In Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 22), pages 402 416, 2022. [42] Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivasa Murthy, Marcello Maggioni, Qiao Zhang, et al. Overlap communication with dependent computation via decomposition in large deep learning models. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 22), pages 93 106, 2022. [43] Chang Chen, Xiuhong Li, Qianchao Zhu, Jiangfei Duan, Peng Sun, Xingcheng Zhang, and Chao Yang.\n\n--- Segment 33 ---\nIn Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 22), pages 93 106, 2022. [43] Chang Chen, Xiuhong Li, Qianchao Zhu, Jiangfei Duan, Peng Sun, Xingcheng Zhang, and Chao Yang. Centauri: Enabling efficient scheduling for communication-computation overlap in large model training via communication partitioning. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 24), pages 178 191, 2024. [44] Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, and Matthew D Sinclair. T3: Transparent tracking triggering for fine-grained overlap of compute collectives. In Pro- ceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 24), pages 1146 1164, 2024. [45] Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, et al. {MegaScale}: Scaling large language model training to more than 10,000 {GPUs}. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), pages 745 760, 2024. [46] Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu Qiao, and Yiran Zhong. CO2: Efficient distributed training with full communication-computation overlap. In The Twelfth International Conference on Learning Representations (ICLR 24), 2024. [47] Kshiteej Mahajan, Ching-Hsiang Chu, Srinivas Sridharan, and Aditya Akella. Better together: Jointly optimizing {ML} collective scheduling and execution planning using {SYNDICATE}. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), pages 809 824, 2023. [48] Hulin Wang, Yaqi Xia, Donglin Yang, Xiaobo Zhou, and Dazhao Cheng.\n\n--- Segment 34 ---\nIn 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), pages 809 824, 2023. [48] Hulin Wang, Yaqi Xia, Donglin Yang, Xiaobo Zhou, and Dazhao Cheng. Harnessing inter-gpu shared memory for seamless moe communication-computation fusion. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, pages 170 182, 2025. [49] Kishore Punniyamurthy, Khaled Hamidouche, and Bradford M Beckmann. Optimizing dis- tributed ml communication with fused computation-collective operations. In SC24: Interna- tional Conference for High Performance Computing, Networking, Storage and Analysis, pages 1 17, 2024. [50] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, et al. Tutel: Adaptive mixture-of-experts at scale. Proceedings of Machine Learning and Systems (MLSys 23), 5:269 287, 2023. [51] Chenyu Jiang, Ye Tian, Zhen Jia, Shuai Zheng, Chuan Wu, and Yida Wang. Lancet: Accelerating mixture-of-experts training via whole graph computation-communication overlapping. In Proceedings of Machine Learning and Systems (MLSys 24), pages 74 86, 2024. [52] OpenFabrics Interfaces Working Group. fi_cxi. v1.21.0 man fi_cxi.7.html, 2025. [Accessed 23-05-2025]. 17 NeurIPS Paper Checklist 1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the paper s contributions and scope? Answer: [Yes] 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] 4.\n\n--- Segment 35 ---\nTheory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main ex- perimental results of the paper to the extent that it affects the main claims and or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instruc- tions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] 6. Experimental setting details Question: Does the paper specify all the training and test details (e.g., data splits, hyper- parameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All reported results in the evaluation section were obtained as the average of 150 executions. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the com- puter resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics Answer: [Yes] 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] 18 Justification: We do not foresee immediate social or ethical impacts, but we acknowledge that increased compute efficiency could amplify access to large-scale models, which raises general considerations around prevalent issues such as environmental cost of training, and responsible downstream use. We recommend that users of our system consider these factors when integrating it into broader ML applications. 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] 12.\n\n--- Segment 36 ---\nSafeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval review based on the requirements of your country or institution) were obtained? Answer: [NA] 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] 19 A Related Work Computation-Communication Overlap and Kernel Fusion. To reduce the communication over- heads of synchronization in distributed DNN training, many research efforts have been focused on increasing the overlap of computation and communication. For generic Transformer-based models without MoE layers, many works [41 49] have provided insights and techniques to partition and schedule computation and communication operations, aimed at finer-grained overlapping. To address the challenges posed by AllToAll communication and expert parallelism in MoE training, Tutel [50] and FasterMoE [13] overlap AllToAll with expert computation.\n\n--- Segment 37 ---\nFor generic Transformer-based models without MoE layers, many works [41 49] have provided insights and techniques to partition and schedule computation and communication operations, aimed at finer-grained overlapping. To address the challenges posed by AllToAll communication and expert parallelism in MoE training, Tutel [50] and FasterMoE [13] overlap AllToAll with expert computation. Lancet [51] additionally enables both non-MoE computation in forward pass and weight gradient computation in backward pass to be overlapped with AllToAll. Despite overlapping, the performance of these approaches is limited in practice due to blocking synchronous collective communication with barriers. In contrast, FlashD- MoE fundamentally eliminates these inefficiencies with asynchronous, device-initiated data transfers overlapped with tiled computation all within a single kernel. FlashDMoE further differentiates itself from SOTA works like COMET [11] and DeepEP [1], which also use this form of kernel-initiated communication but at a coarse-grained granularity and without complete kernel fusion.\n\n--- Segment 38 ---\nIn contrast, FlashD- MoE fundamentally eliminates these inefficiencies with asynchronous, device-initiated data transfers overlapped with tiled computation all within a single kernel. FlashDMoE further differentiates itself from SOTA works like COMET [11] and DeepEP [1], which also use this form of kernel-initiated communication but at a coarse-grained granularity and without complete kernel fusion. B Motivation Plots 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Delay (ms) 0.0 0.2 0.4 0.6 0.8 1.0 0.28ms median 0.19ms P95 0.64ms ECDF GPT-3 MoE 32x1.3B All-to-All Straggler Effect: 8x4 A100 (a) ECDF 0 100 200 300 400 500 600 All-to-All Steps 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Time (ms) max delay 3.22 ms avg actual 1.99ms Raw Distribution Total Time Actual Time GPT-3 MoE 32x1.3B All-to-All Straggler Effect: 8x4 A100 (b) Raw Distribution 0 5 10 15 20 25 Delay (ms) 0.0 0.2 0.4 0.6 0.8 1.0 1.17ms median 0.57ms P95 2.82ms ECDF GPT-3 MoE 8x350M All-to-All Straggler Effect: 1x8 V100 (c) ECDF 0 250 500 750 1000 1250 1500 1750 All-to-All Steps 0 5 10 15 20 25 Time (ms) max delay 27.90 ms avg actual 0.27ms Raw Distribution Total Time Actual Time GPT-3 MoE 8x350M All-to-All Straggler Effect: 1x8 V100 (d) Raw Distribution Figure 15: Straggler effect of synchronous AllToAll. M N A100 or V100 denotes N GPUs within a node across M nodes. Every GPU communicates with every other GPU per AllToAll step. We capture the distribution of delay induced by stragglers across many steps.\n\n--- Segment 39 ---\nEvery GPU communicates with every other GPU per AllToAll step. We capture the distribution of delay induced by stragglers across many steps. Actual Time ta denotes the fastest kernel execution time across all GPUs, conversely Total Time t is the maximum recorded step time, while Delay is the maximum difference between t and ta. Note Delay is idle time. C Proof of Theorem 3.1 We begin with two necessary definitions vital to the proof. 20 Definition C.1. Define a write as w(ps, pt, i), where ps is the source process and i is an ordered tuple indicating the index coordinates for L residing on the target process pt. A write-write conflict occurs when there exist at least two distinct, un-synchronized, concurrent writes w1(ps1, pt1, i1) and w2(ps2, pt2, i2), such that pt1 pt2 and index coordinates i1 i2 but ps1 ps2 Definition C.2. For any source process ps, a valid index coordinate i (p , r, b, e, c) satisfies the following: 1. For inter-device writes, it must hold that p ps and b 1. Note this also applies to self-looping writes w(pt, pt, i). 2. For any write w(ps, pt, i), if b 0, then ps pt. This rule describes intra-device staging writes. We restate Theorem 3.1 and outline its proof below. Theorem C.1. The symmetric tensor layout L is write-write conflict-free. Proof. As is the case for typical physical implementations, assume that each index coordinate i maps to a distinct memory segment in L. Next, we show by contradiction that no write-write conflicts can exist when accessing L using valid i. For simplicity, we only include the index coordinates when describing a write. Assume that there exist at least two writes w1(ps1, pt1, i1), w2(ps2, pt2, i2) with pt1 pt2 and valid destination coordinates i1, i2, where i1 i2 lexicographically and both are unpacked below.\n\n--- Segment 40 ---\nFor simplicity, we only include the index coordinates when describing a write. Assume that there exist at least two writes w1(ps1, pt1, i1), w2(ps2, pt2, i2) with pt1 pt2 and valid destination coordinates i1, i2, where i1 i2 lexicographically and both are unpacked below. i1 (p1, r1, b1, e1, c1), i1 (p2, r2, b2, e2, c2) Note that for the message staging state, even though i1 i2 the resultant memory segments reside in different physical buffers resident in ps1 and ps2 respectively. Therefore, for this state, there are no conflicts as intra-process writes always have distinct cj coordinates, where j {0, C 1}. For inter-process transfers, we have two cases. Case 1: ps1 ps2 Here, w1 and w2 are identical operations. This contradicts the definition of a conflict, which requires that ps1 ps2. In practice, such repeat writes never even occur. Case 2: ps1 ps2 To ensure validity for i1 and i2, it is the case that p1 ps1 and p2 ps2. However, this implies that i1 i2 yielding a contradiction as desired.\n\n--- Segment 41 ---\nCase 2: ps1 ps2 To ensure validity for i1 and i2, it is the case that p1 ps1 and p2 ps2. However, this implies that i1 i2 yielding a contradiction as desired. 21 D Task Implementation def i ne GEMMs 2 st r uct __al i gn__( 16) Task { const byt e aDat a; ar r ay const byt e , GEMMs bDat a; ar r ay byt e , GEMMs cDat a; ar r ay const byt e , GEMMs dDat a; byt e r cDat a; ui nt 64_t f l ags; ui nt M; ui nt syncI dx; ui nt t i l eI dx; ui nt bat chI dx; ui nt peer I dx; ui nt exper t I dx; ui nt i sPeer Remot e; TaskType t askType; ui nt 16_t t i l eSi ze; Pad t i l l 128- byt e cache l i ne ui nt paddi ng[ 6] { } ; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Figure 16: Task Struct.\n\n--- Segment 42 ---\nHowever, this implies that i1 i2 yielding a contradiction as desired. 21 D Task Implementation def i ne GEMMs 2 st r uct __al i gn__( 16) Task { const byt e aDat a; ar r ay const byt e , GEMMs bDat a; ar r ay byt e , GEMMs cDat a; ar r ay const byt e , GEMMs dDat a; byt e r cDat a; ui nt 64_t f l ags; ui nt M; ui nt syncI dx; ui nt t i l eI dx; ui nt bat chI dx; ui nt peer I dx; ui nt exper t I dx; ui nt i sPeer Remot e; TaskType t askType; ui nt 16_t t i l eSi ze; Pad t i l l 128- byt e cache l i ne ui nt paddi ng[ 6] { } ; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Figure 16: Task Struct. TaskType {GEMM0, GEMM1, Combine} 22 E Actors E.1 Processor Algorithm 2: Processor Actor: executed by a block 1 begin 2 tQ GetTQ() 3 signal 0 4 shared memory variables 5 task {} 6 interrupt False 7 complete False 8 while interrupt False do 9 if warpId 0 then 10 if threadId 0 then 11 awaitTaskFromScheduler(interrupt, signal) 12 FencedNotifyRQ(ready) 13 end if 14 syncwarp() 15 warpReadTQ(tQ, signal, task) 16 end if 17 syncthreads() 18 if interrupt False then 19 switch task.Type do 20 case GEMM0 do 21 fused GEMM, epilogue and async tile staging 22 fGET(GEMM0, task) 23 if threadId 0 then 24 complete NotifyTileCompletion() 25 end if 26 syncthreads() 27 if complete True then 28 NotifySchedulerNextGEMM(tQ) 29 end if 30 end case 31 case GEMM1 do 32 fused GEMM, epilogue and async tile transfer 33 fGET(GEMM1, task) 34 end case 35 case Combine do 36 combine(task) 37 end case 38 end switch 39 end if 40 end while 41 end 23 E.2 Scheduler Algorithm 3: Scheduler Actor: executed by one warp 1 begin 2 scheduled 0 3 tTB 0 4 tqState {} 5 pTDB GetProcessorDoorbell() 6 sTDB GetSubscriberDoorbell() 7 taskBound GetTaskBound() 8 tTB AtomicLoad(taskBound) 9 circular buffer ready queue 10 rQ {} 11 Populate ready queue with Processor ids 12 PopulateRQ(rQ) 13 while scheduled tTB do 14 lt 0 15 do in parallel 16 Sweep doorbells and populate observed task counts into tqState 17 Aggregate locally observed task counts into lt 18 end 19 qS, taskTally 0 20 qS is the inclusive output 21 WarpInclusiveSum(lt, qS, tasktally) 22 while tasktally 0 do 23 Repopulate rQ with ready processor ids 24 do in parallel 25 Starting at rQ[qS], signal processors about task indices from tqState 26 end 27 end while 28 if threadId 0 then 29 tTB AtomicLoad(taskBound) 30 end if 31 tTB WarpBroadcast(tTB) 32 end while 33 InterruptSubscribers() 34 InterruptProcessors() 35 end 24 E.3 Subscriber Algorithm 4: Subscriber Actor: executed by three warps Input: Tϕ R2 E C, Gϕ RS E O RS H, X RE H D 1 begin 2 interrupt GetSharedInterrupt() 3 flags GetSymmetricFlags() 4 tQ GetTQ() 5 Predefined upper bound on the number of tasks.\n\n--- Segment 43 ---\n21 D Task Implementation def i ne GEMMs 2 st r uct __al i gn__( 16) Task { const byt e aDat a; ar r ay const byt e , GEMMs bDat a; ar r ay byt e , GEMMs cDat a; ar r ay const byt e , GEMMs dDat a; byt e r cDat a; ui nt 64_t f l ags; ui nt M; ui nt syncI dx; ui nt t i l eI dx; ui nt bat chI dx; ui nt peer I dx; ui nt exper t I dx; ui nt i sPeer Remot e; TaskType t askType; ui nt 16_t t i l eSi ze; Pad t i l l 128- byt e cache l i ne ui nt paddi ng[ 6] { } ; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Figure 16: Task Struct. TaskType {GEMM0, GEMM1, Combine} 22 E Actors E.1 Processor Algorithm 2: Processor Actor: executed by a block 1 begin 2 tQ GetTQ() 3 signal 0 4 shared memory variables 5 task {} 6 interrupt False 7 complete False 8 while interrupt False do 9 if warpId 0 then 10 if threadId 0 then 11 awaitTaskFromScheduler(interrupt, signal) 12 FencedNotifyRQ(ready) 13 end if 14 syncwarp() 15 warpReadTQ(tQ, signal, task) 16 end if 17 syncthreads() 18 if interrupt False then 19 switch task.Type do 20 case GEMM0 do 21 fused GEMM, epilogue and async tile staging 22 fGET(GEMM0, task) 23 if threadId 0 then 24 complete NotifyTileCompletion() 25 end if 26 syncthreads() 27 if complete True then 28 NotifySchedulerNextGEMM(tQ) 29 end if 30 end case 31 case GEMM1 do 32 fused GEMM, epilogue and async tile transfer 33 fGET(GEMM1, task) 34 end case 35 case Combine do 36 combine(task) 37 end case 38 end switch 39 end if 40 end while 41 end 23 E.2 Scheduler Algorithm 3: Scheduler Actor: executed by one warp 1 begin 2 scheduled 0 3 tTB 0 4 tqState {} 5 pTDB GetProcessorDoorbell() 6 sTDB GetSubscriberDoorbell() 7 taskBound GetTaskBound() 8 tTB AtomicLoad(taskBound) 9 circular buffer ready queue 10 rQ {} 11 Populate ready queue with Processor ids 12 PopulateRQ(rQ) 13 while scheduled tTB do 14 lt 0 15 do in parallel 16 Sweep doorbells and populate observed task counts into tqState 17 Aggregate locally observed task counts into lt 18 end 19 qS, taskTally 0 20 qS is the inclusive output 21 WarpInclusiveSum(lt, qS, tasktally) 22 while tasktally 0 do 23 Repopulate rQ with ready processor ids 24 do in parallel 25 Starting at rQ[qS], signal processors about task indices from tqState 26 end 27 end while 28 if threadId 0 then 29 tTB AtomicLoad(taskBound) 30 end if 31 tTB WarpBroadcast(tTB) 32 end while 33 InterruptSubscribers() 34 InterruptProcessors() 35 end 24 E.3 Subscriber Algorithm 4: Subscriber Actor: executed by three warps Input: Tϕ R2 E C, Gϕ RS E O RS H, X RE H D 1 begin 2 interrupt GetSharedInterrupt() 3 flags GetSymmetricFlags() 4 tQ GetTQ() 5 Predefined upper bound on the number of tasks. 6 We modulate this value to the actual task count computed 7 dispatch signals received from peer GPUs 8 taskBound GetTaskBound() 9 while AtomicLoad(interrupt) False do 10 dispatch flags 11 do in parallel 12 Visit dispatch flags 13 Atomically retrieve signal 14 if Signal is set and flag is not visited then 15 Mark visited 16 SelfCorrectTaskBound(taskBound, Signal) 17 Enforce memory consistency before consuming packet 18 Decode packet into a set of GEMM0 task descriptors using X 19 Write task descriptors to tQ 20 Notify Scheduler of decoded tasks 21 end if 22 end 23 Advance flags by number of dispatch flags length 24 Atomically retrieve signal 25 combine signals 26 do in parallel 27 Visit combine flags: one per tile 28 if Signal is set and flag is not visited then 29 Mark visited 30 Enforce memory consistency before consuming packet 31 Decode packet into a set of combine task descriptors using Tϕ, Gϕ, O 32 Write task descriptors to tQ 33 Notify Scheduler of decoded tasks 34 end if 35 end 36 end while 37 end 25 F Multi-Node Evaluation F.1 Setup In this experiment, we seek to evaluate FlashDMoE in the multi-node setting.\n\n--- Segment 44 ---\nTaskType {GEMM0, GEMM1, Combine} 22 E Actors E.1 Processor Algorithm 2: Processor Actor: executed by a block 1 begin 2 tQ GetTQ() 3 signal 0 4 shared memory variables 5 task {} 6 interrupt False 7 complete False 8 while interrupt False do 9 if warpId 0 then 10 if threadId 0 then 11 awaitTaskFromScheduler(interrupt, signal) 12 FencedNotifyRQ(ready) 13 end if 14 syncwarp() 15 warpReadTQ(tQ, signal, task) 16 end if 17 syncthreads() 18 if interrupt False then 19 switch task.Type do 20 case GEMM0 do 21 fused GEMM, epilogue and async tile staging 22 fGET(GEMM0, task) 23 if threadId 0 then 24 complete NotifyTileCompletion() 25 end if 26 syncthreads() 27 if complete True then 28 NotifySchedulerNextGEMM(tQ) 29 end if 30 end case 31 case GEMM1 do 32 fused GEMM, epilogue and async tile transfer 33 fGET(GEMM1, task) 34 end case 35 case Combine do 36 combine(task) 37 end case 38 end switch 39 end if 40 end while 41 end 23 E.2 Scheduler Algorithm 3: Scheduler Actor: executed by one warp 1 begin 2 scheduled 0 3 tTB 0 4 tqState {} 5 pTDB GetProcessorDoorbell() 6 sTDB GetSubscriberDoorbell() 7 taskBound GetTaskBound() 8 tTB AtomicLoad(taskBound) 9 circular buffer ready queue 10 rQ {} 11 Populate ready queue with Processor ids 12 PopulateRQ(rQ) 13 while scheduled tTB do 14 lt 0 15 do in parallel 16 Sweep doorbells and populate observed task counts into tqState 17 Aggregate locally observed task counts into lt 18 end 19 qS, taskTally 0 20 qS is the inclusive output 21 WarpInclusiveSum(lt, qS, tasktally) 22 while tasktally 0 do 23 Repopulate rQ with ready processor ids 24 do in parallel 25 Starting at rQ[qS], signal processors about task indices from tqState 26 end 27 end while 28 if threadId 0 then 29 tTB AtomicLoad(taskBound) 30 end if 31 tTB WarpBroadcast(tTB) 32 end while 33 InterruptSubscribers() 34 InterruptProcessors() 35 end 24 E.3 Subscriber Algorithm 4: Subscriber Actor: executed by three warps Input: Tϕ R2 E C, Gϕ RS E O RS H, X RE H D 1 begin 2 interrupt GetSharedInterrupt() 3 flags GetSymmetricFlags() 4 tQ GetTQ() 5 Predefined upper bound on the number of tasks. 6 We modulate this value to the actual task count computed 7 dispatch signals received from peer GPUs 8 taskBound GetTaskBound() 9 while AtomicLoad(interrupt) False do 10 dispatch flags 11 do in parallel 12 Visit dispatch flags 13 Atomically retrieve signal 14 if Signal is set and flag is not visited then 15 Mark visited 16 SelfCorrectTaskBound(taskBound, Signal) 17 Enforce memory consistency before consuming packet 18 Decode packet into a set of GEMM0 task descriptors using X 19 Write task descriptors to tQ 20 Notify Scheduler of decoded tasks 21 end if 22 end 23 Advance flags by number of dispatch flags length 24 Atomically retrieve signal 25 combine signals 26 do in parallel 27 Visit combine flags: one per tile 28 if Signal is set and flag is not visited then 29 Mark visited 30 Enforce memory consistency before consuming packet 31 Decode packet into a set of combine task descriptors using Tϕ, Gϕ, O 32 Write task descriptors to tQ 33 Notify Scheduler of decoded tasks 34 end if 35 end 36 end while 37 end 25 F Multi-Node Evaluation F.1 Setup In this experiment, we seek to evaluate FlashDMoE in the multi-node setting. We use 4 nodes, where each node comprises 4 A100 GPUs fully interconnected via NVLink.\n\n--- Segment 45 ---\n6 We modulate this value to the actual task count computed 7 dispatch signals received from peer GPUs 8 taskBound GetTaskBound() 9 while AtomicLoad(interrupt) False do 10 dispatch flags 11 do in parallel 12 Visit dispatch flags 13 Atomically retrieve signal 14 if Signal is set and flag is not visited then 15 Mark visited 16 SelfCorrectTaskBound(taskBound, Signal) 17 Enforce memory consistency before consuming packet 18 Decode packet into a set of GEMM0 task descriptors using X 19 Write task descriptors to tQ 20 Notify Scheduler of decoded tasks 21 end if 22 end 23 Advance flags by number of dispatch flags length 24 Atomically retrieve signal 25 combine signals 26 do in parallel 27 Visit combine flags: one per tile 28 if Signal is set and flag is not visited then 29 Mark visited 30 Enforce memory consistency before consuming packet 31 Decode packet into a set of combine task descriptors using Tϕ, Gϕ, O 32 Write task descriptors to tQ 33 Notify Scheduler of decoded tasks 34 end if 35 end 36 end while 37 end 25 F Multi-Node Evaluation F.1 Setup In this experiment, we seek to evaluate FlashDMoE in the multi-node setting. We use 4 nodes, where each node comprises 4 A100 GPUs fully interconnected via NVLink. Across nodes, each GPU uses a single NIC providing 25 GB s of bandwidth. We set the number of experts to be 16 and assign each GPU to host only one, so the number of local experts is 1. Note that we define MIV formally as follows: MIV Tokens Experts local_experts precision hidden_size 2 nrg where nrg is the number of remote peers and the multiplicative factor of 2 accounts for communication rounds (dispatch and combine). nrg 12 for this experiment. F.2 Results 128 256 512 1024 2048 4096 8192 Tokens 1.2 1.3 1.4 1.5 1.6 1.7 Latency (ms) Network Failure Region Latency vs Number of Tokens Latency (ms) Network Failure Threshold (MIV 12 MB) 0.75 1.50 3.00 6.00 12.00 24.00 48.00 Maximal Incast Volume (MB) Figure 17: Multi-node Latency evaluation. Embbeding dimension is 1024 and FFN intermediate size is 4096.\n\n--- Segment 46 ---\nF.2 Results 128 256 512 1024 2048 4096 8192 Tokens 1.2 1.3 1.4 1.5 1.6 1.7 Latency (ms) Network Failure Region Latency vs Number of Tokens Latency (ms) Network Failure Threshold (MIV 12 MB) 0.75 1.50 3.00 6.00 12.00 24.00 48.00 Maximal Incast Volume (MB) Figure 17: Multi-node Latency evaluation. Embbeding dimension is 1024 and FFN intermediate size is 4096. We define Maximal Incast Volume (MIV) as the worst case upper bound for data volume that a NIC receives in a single incast occurence. We observe a sublinear increase in latency as we scale the number of tokens. However, we observe at Tokens 2048, that the application fails to terminate due to failure to receive expectant messages. We hypothesize this failure to be due to buffer overflow at the networking hardware layer as is common for applications that generate many and large messages [18] like our system. We note that this failure is addressable by tuning hardware configurations [52] but we consider this exploration as an exercise orthogonal to this work. 26 G Implementation Table 4: Implementation metrics of FlashDMoE. Metric Value Total lines of code (CUDA C ) 6820 Kernel stack frame size 0 B Spill stores (per thread) 0 Spill loads (per thread) 0 Shared memory usage (per block) 46 KB Registers per thread 255 Max active blocks per SM 2 Compilation time 53 seconds Binary size 29 MB H FP16 Memory Throughput (a) Memory subsystem throughput for FP16 (b) Memory subsystem throughput for FP32 Figure 18: Here, we report the total A100 memory throughput for both FP16 (top) and FP32 (bottom) variants of FlashDMoE. Notably, the FP16 implementation issues approximately 2 more shared memory instructions compared to its FP32 counterpart under identical workloads. We attribute this inefficiency to suboptimal shared memory layouts in FlashDMoE when operating on half-precision data. While this bottleneck is addressable through improved layout strategies, we leave its resolution to future work due to time constraints. 27\n\n