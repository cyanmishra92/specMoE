=== ORIGINAL PDF: 2504.20401v1_Nonlinear_Computation_with_Linear_Optics_via_Sourc.pdf ===\n\nRaw text length: 53713 characters\nCleaned text length: 53271 characters\nNumber of segments: 40\n\n=== CLEANED TEXT ===\n\nNonlinear Computation with Linear Optics via Source-Position Encoding Nick Richardson 1 Cyrill B osch 1 Ryan P. Adams 1 Abstract Optical computing systems provide an alternate hardware model which appears to be aligned with the demands of neural network workloads. How- ever, the challenge of implementing energy effi- cient nonlinearities in optics a key requirement for realizing neural networks is a conspicuous missing link. In this work we introduce a novel method to achieve nonlinear computation in fully linear media. Our method can operate at low power and requires only the ability to drive the optical system at a data-dependent spatial position. Leveraging this positional encoding, we formulate a fully automated, topology-optimization-based hardware design framework for extremely special- ized optical neural networks, drawing on modern advancements in optimization and machine learn- ing. We evaluate our optical designs on machine learning classification tasks: demonstrating sig- nificant improvements over linear methods, and competitive performance when compared to stan- dard artificial neural networks. 1. Introduction Modern machine learning systems demand an unprece- dented scale of resources to drive application performance (Radford et al., 2019; Brown et al., 2020). Sustaining these efforts requires infrastructure developments, or improve- ments in efficiency; most likely both. The resulting em- phasis on domain-specific hardware (Jouppi et al., 2017), coincident with the rampant popularity of neural networks for machine learning applications, has sparked a resurgence of interest in alternative hardware platforms for neural net- work workloads (Schuman et al., 2022). Computing with optics is particularly attractive, since opti- cal signals are well-aligned with parallel, high-bandwidth- requiring dataflows, like those found in many neural net- works (Sui et al., 2020). The linear scattering phenomena in- 1Department of Computer Science, Princeton Univer- sity. Correspondence to: herent to electromagnetic energy propagation is amenable to efficient implementations of the multiply-accumulate-based kernels which dominate neural network workloads (Ivanov et al., 2021). All that said, a critical unsolved problem is achieving efficient nonlinearities in optical hardware. Conventional materials-based optical nonlinearities typi- cally require some combination of exotic materials, difficult fabrication procedures, or high-power light sources which negate the efficiency gained through more-or-less passive linear wave propagation (Zuo et al., 2019; Feldmann et al., 2021; Keyes, 1985). An alternative is to give up on an all- optical design, and compromise with optoelectronic trans- duction; essentially executing linear computational kernels in the optical domain, and nonlinearities in standard elec- tronics (Hughes et al., 2018a; George et al., 2019). This strategy seems to use both media in their ideal operating regime, but much like analog electronic neural networks, these systems are ultimately gated by fundamental ineffi- ciencies involved in transducing signals between the two physical media (Rekhi et al., 2019). As a result, the past year has seen several works concerned with developing low-power methods for nonlinear compu- tation using linear media (Xia et al., 2024; Li et al., 2024; Wanjura Marquardt, 2024; Yildirim et al., 2024; Mo- meni et al., 2023); largely with an emphasis on machine learning applications. A discussion on prior art in inverse design for computational electromagnetics, and background on nonlinear optics are provided in Appendix A.3.3 and Appendix A.3.1. We discuss recent efforts toward nonlinear computation with linear optics, and its relationship to our own work in Appendix A.3.2. In this work, we contribute a novel method for nonlinear computation using low power, fully linear optics: source positional encoding. We exploit the fact that the relationship between the measured fields and the position of the source is nonlinear, and encode the data directly in the source position. Then, we optimize the spatial distribution of material such that the source-position-to-measured-field approximates a desired transfer function (e.g., a classifier). By harnessing a custom, differentiable full-wave field solver in concert with topology optimization for design of the optical hardware, we avoid analytic simplifications of the physics beyond discretization of Maxwell s equations (a source of error 1 arXiv:2504.20401v1 [physics.optics] 29 Apr 2025 Nonlinear Computation with Linear Optics via Source-Position Encoding which is well understood and largely controllable). In Section 2.1, we detail the fundamental physics underlying our position-based encoding method. Section 2.2 describes a proposed system design (see Figure 2) for an optical clas- sifier leveraging positional encoding as a nonlinearity. Sec- tion 2.3 describes the inverse problem associated with the hardware design; which is addressed with topology opti- mization and differentiable simulation. Section 3 presents experimental results across an array of test problems, with an emphasis on ablations verifying the effectiveness of our optical nonlinearity over fully linear methods. Finally, Sec- tion 4 offers limitations of our method and promising future directions. 2. Method 2.1. Source positional encoding Consider Maxwell s equations1 in the frequency domain, denoting the electric field with E, and the source with J . The resultant wave equation over a domain Ω R3 and subject to Dirichlet boundary conditions can be written as µ 1 r (z) E(z, ω) ω2ϵr(z)E(z, ω) jωJ (z, ω) (1) z Ω, E(z, ω) 0 z Ω. (2) The medium is modeled as isotropic and lossless, i.e. the relative permeability µr and relative permittivity ϵr are real- valued scalars. The corresponding generalized eigensystem is: µ 1 r (z) E(z, ω) ω2ϵr(z)E(z, ω). (3) We further define the following inner product: E(z, ω), E (z, ω) ϵr : Z Ω E (z, ω)ϵr(z)E (z, ω) dz. (4) The wave operator M : µ 1 r is self-adjoint with respect to this inner product. Therefore, by the spectral theorem (Hermite, 1855; Cauchy, 1829), M has a discrete spectrum with eigenvalue-eigenfunction pairs (wi, Ei). Sup- pose the eigenfunctions are chosen to be orthonormal with respect to the inner product (4), i.e. Ei(z), Ej(z) ϵr δij, such that {Ej} j 1 forms an orthonormal basis. Then it follows that any field E(z, ω) satisfying (1) can be decomposed as, E(z, ω) X i ci(ω)Ei(z). (5) Inserting this decomposition into (1), using (3) and comput- ing the inner product (4) with the eigenfunction Em results 1Our notation, and a brief review of the relevant electromagnet- ics is reviewed in Appendix A.2. i JZBLWyECOYDkiPMbSbJkr29Y3dPCEfAv2JjoYitv8POf MmuUITHw83pthZl4QC6N6347K6tr6xubua389s7u3n7h4LCho0QxrLNIRKoVgEbBJdYNwJbsUIA4HNYHQ9ZuPqDSP5IMZx iHMJC8zxkYK3ULx3dgUHEQ9MYuUzxI5 nrRLbkz0GXiZaRIMtS6ha9OL2JiNIwAVq3PTc2fgrKcCZwku8kGmNgIxhg21IJIWo nZ0 oWdW6dF pGxJQ2fq74kUQq3HYWA7QzBDvehNxf 8dmL6l37KZwYlGy qJ8IaiI6zYL2uEJmxNgSYIrbWykbgJmI9F5G4K3 PIyaZRLXq VUuS8Xq1dZHDlyQk7JOfHIBamSW1IjdcJISp7JK3lznpwX5935mLeuONnMEfkD5 MHd eV1Q latexit Material Distribution Modes Ei E1 E3 E6 Figure 1. Three material distributions (vacuum with ϵr 1, and two arbitrarily chosen distributions, with ϵr 3 in blue regions and ϵr 1 in white regions; µr 1 for all of them) and the 1st, 3rd and 6th corresponding eigenmodes. Clearly, the mode profiles can be quite distinct from the basic freespace modes (and one another) by varying the material distribution. Complex mate- rial distributions give rise to complex modes, implying a highly nonlinear relation between source position and measured field. in (Gilbert, 1971; Ulrich et al., 2022): cm(ω)ω2 m ω2cm(ω) jω Em(r) J (r, ω) ϵ . (6) The coefficients can be written explicitly as cm(ω) jω ω2m ω2 Em(z), J (z, ω) ϵr. (7) For simplicity, consider the source to be a monochromatic point source at location zs Ωwith frequency ωs and unit amplitude, i.e. J (z, ω) δ(z zs)δ(ω ωs). Then ci jωs ω2 i ω2s E i (zs). The electric field at a measurement location zR is therefore given by E(zR) X i jωs ω2 i ω2s E i (zs)Ei(zR). (8) This can be interpreted as the statement that the field mea- sured at zR (a fixed location) is a linear combination of modes Ei whose amplitudes depend on (i) the deviation between source and mode frequency, and (ii) the relative spatial placement of the source and the mode amplitude profile. Concretely, consider an eigenfunction Em, and suppose it 2 Nonlinear Computation with Linear Optics via Source-Position Encoding Class Label C1 Ck . . . P(C x, ω) x Rd 3 aKHgrmA " AB83icbVDLSgNBEJz1GeMr6tHLYBA8hV2R6DHoRW8RzAOyS5id9CZDZh M9IphyW948aCI V3 Gm3 jbLIHTSxoKq6e7yEyk02va3tbK6tr6xWdoqb s7u1XDg7bOk4VhxaPZay6PtMgRQtFCihmyh goS h49vcr zCEqLOHrASQJeyIaRCARnaCTXRXjC7C5KUpz2K1W7Zs9Al4lTkCop0OxXvtxBzNMQIuSa d1z7AS9jCkUXMK07KYaEsbHbAg9QyMWgvay2c1TemqUAQ1iZSpCOlN T2Qs1HoS qYzZDjSi14u uf1Ugyu vEzkL0HE54uCVFKMaR4AHQgFHOXEMaVMLdSPmKcTQxlU0IzuLy6R9XnPqtfr9RbVxXcRIsfkhJwRh1 ySBrklTdIinCTkmbySNyu1Xqx362PeumIVM0fkD6zPH7h6kiY latexit Input Source Geometry Simulation 4 bt20O2vpg4PHeDPzgpQzpR3n2yptbG5t75R3K3v7B4dH1eOTjkoySdGjCU9kLyAKORPoaY59lKJA4doPJ7dzvPqFULBGPepqiH5ORYBGjRBvJu2PIw2G15tSdBex14hakBgXaw rXIExoFqPQlBOl q6Taj8nUjPKcVYZApTQidkhH1DBYlR fni2Jl9YZTQjhJpSmh7of6eyEms1DQOTGdM9FitenPxP6 f6ejaz5lIM42CLhdFGbd1Ys8 t0Mm kWo NYRQycytNh0TSag2 VRMCO7qy uk06i7zXrzoVFr3RxlOEMzuESXLiCFtxDGzygwOAZXuHNEtaL9W59LFtLVjFzCn9gf4AqC2OmQ latexit Field Positional Encoder SiO2 Freespace Dipole Source (Inactive) Dipole Source (Active) Positional Encoding Decoder ωd Rd (k k) b s(x) A(ωg) (Section 2.3) (Section 2.2.1) O rvF3Z2d3br5oHhx0Zp4LQNol5Lo lpSziLYVU5x2E0Fx6HP64E u8 rDIxWSxdG9mibUC EoYgEjWGlrYFbrd5TkiFzLtc7PBmbNtuy50Co4BdSgUGtgfvWHMUlDGinCsZQ9x06Ul2GhGOF0VumnkiaYTPCI9jRGOKTSy aLz9CpdoYoiIV kUJz9 dEhkMp6GvO0OsxnK5lpv 1XqpCi69jEVJqmhEFh8FKUcqRnkKaMiEPptPNWAimN4VkTEWmCidVUWH4CyfvAod13IaVuPWrTWvijKcAwnUAcHLqAJN9CNhBI4Rle4c14Ml6Md Nj0Voyipkj CPj8wc6TJGE latexit (Section 2.2.4) QUFR1093lRYJrsO1va25 YXFpObOSXV1b39jMbW3XdBgryqo0FKFqeEQzwSWrAgfBGpFiJPAEq3v9i6Ffv2dK81DewiBirYB0Jfc5JWCkdu7osuAGBHqenzykZ9iFHgPS7hxgl0s8drzkJr1L sbjAdO4n7Zzebtoj4BniTMheTRBpZ37dDshjQMmgQqidOxI2glRAGngqVZN9YsIrRPuqxpqCRmTysZPZfifaN0sB8qUxLwSP09kZBA60Hgmc7huXraG4r ec0Y NWwmUA5N0vMiPB YQD5PCHa4YBTEwhFDFza2Y9ogiFEyeWROCM 3yLKkdFp1SsXR9nC fT LIoF20hwrIQSeojK5QBVURY oGb2iN vJerHerY9x65w1mdlBf2B9 QBI0aGI latexit D(x; ωd) Rk k R(e(x; ωg)) N T0zGxmLju sLi0nFtZvdJhrDhUeChDdeMyDVIEUEGBEm4iBcx3JVy7ndOef30HSoswuMRuBHWftQLhCc7QSI3cec1n2Ha9BNKtEb1PD2kN24Cs0dqmR3SkH5vISL5NdpxUj41sN3J5u2D3Qf8SZ0jyZIhyI dUa4Y89iFALpnWVceOsJ4whYJLSLO1WEPEeIe1oGpowHzQ9aR cko3jdKkXqjMC5D21fGJhPlad3XJHsr6t9eT zPq8boHdQTEUQxQsAH3mxpBjSXn 0KRwlF1DGFfC7Ep5mynG0bScNSU4v0 S652C06xULzYy5dOh nVkyDrZIFvEIfukRM5ImVQIJw kmbyRd vRerU rM9BdMIazqyRH7C vgEHFKqg latexit e(x; ωg) A(ωg) 1s(x) Softmax( ˆf(x; ω)) 1v x82 cJFto4oGBwzn3MvecIJHCoOt O2vrG5tb24Wd4u7e sFh6ei4ZeJUM95ksYx1J6CGS6F4EwVK3k0p1EgeTsY38z89hPXRsTqAScJ9yM6VCIUjKVWncpJin2S2W34s5BVomXkzLkaPRLX71BzNKIK2SGtP13AT9jGoUTPJpsZcanlA2pkPetVTRiBs m187JedWGZAw1vYpJHP190ZGI2MmUWAnI4ojs zNxP 8borhlZ8J ZRNxRYfhakGJNZdDIQmjOUE0so08LeStiIasrQFlS0JXjLkVdJq1rxapXafbVcv87rKMApnMEFeHAJdbiFBjSBwSM8wyu8ObHz4rw7H4vRNSfOYE cD5 ANXj1A latexit Output x1 W bYBFclZki1WXRjcsK9gGdoWTSTBuayYQkI5ahv HGhSJu Rl3 o2ZdhbaeiBwOde7skJWfauO63s7a sbm1Xdop7 7tHxWjo47OkVoW2S8ET1QqwpZ4K2DTOc9qSiOA457YaT29zvPlKlWSIezFTSIMYj wSJGsLGS78fYjMoe5oN6oNK1a25c6BV4hWkCgVag8qXP0xIGlNhCMda9z1XmiDyjDC6azsp5pKTCZ4RPuWChxTHWTzDN0bpUhihJlnzBorv7eyHCs9TQO7WSeUS97ufif109NdB1kTMjUEWh6KUI5Og vA0ZIoSw6eWYKYzYrIGCtMjK2pbEvwlr 8Sjr1mteoNe4vq82bo4SnMIZXIAHV9CEO2hBGwhIeIZXeHNS58V5dz4Wo2tOsXMCf B8 gAvdpHM latexit x2 d uBovgqiQi1WXRjcsK9gFNKJPJpB06mYSZiVhCf8ONC0Xc jPu BsnaRbaemDgcM693DPHTzhT2ra rcra sbmVnW7trO7t39QPzqTiVhHZJzGM58LGinAna1UxzOkgkxZHPad f3uZ 5FKxWLxoGcJ9SI8 FixkBGsjuW6E9cQPs6f5KBjVG3bTLoBWiVOSBpTojOpfbhCTNKJCE46VGjp2or0MS80Ip OamyqaYDLFYzo0VOCIKi8rMs RmVECFMbSPKFRof7eyHCk1CzyzWSeUS17ufifN0x1eO1lTCSpoIsDoUpRzpG eQEoYJISzWeGYCKZyYrIBEtMtKmpZkpwlr 8SnoXTafVbN1fNto3ZR1VOIFTOAcHrqANd9CBLhBI4Ble4c1KrRfr3fpYjFascucY sD6 AF7PpH latexit xd Figure 2. The input data x is used to determine which subset of the array of sources are active (illustrated in red). This collection of active sources is assembled into an aggregate electromagnetic source b : s(x). Below the aggregate source field we illustrate the geometry (hardware) A(θg), which does not depend on the data but is optimized via a gradient method. The source and geometry are simulated in a differentiable field solver, resulting in the field shown. At a collection of fixed locations (shown as white rectangular regions imposed on the field), the energy is summed and re-mapped via a data-dependent linear decoding D(x; θd). In a classification application, the result is normalized and interpreted as the readout probabilities of the system (see rightmost). is non-degenerate (the multiplicity of ωm is 1), with source frequency ωs close to its associated eigenfrequency ωm. Then the field is dominated by this particular mode: E(zR) jωs ω2m ω2s E m(zs)Em(zR). (9) Holding zR constant, and varying zs, the measured field follows the functional form of Em; which is nonlinear. In freespace, Em is sinusoidal (Figure 1, left column), but with general heterogeneous media, Em can have a nearly arbitrarily complicated functional form (Figure 1 second and third column from the left). By choosing the distribution of material in the domain through optimization, we can achieve targeted mode profiles, in turn realizing a desired input to output map zs 7 E(zR). In the general case, multiple modes whose frequencies are close to the source frequency will be excited, and the relation zs to E(zR) will be a linear combination of the modes. 2.2. Proposed Implementation While positional encoding is a generic method to exploit linear systems for nonlinear computation, here we consider a concrete application and describe a proposed hardware design for an optical classifier, illustrated in Figure 2. The proposed system is functionally comprised of four stages: encoding, optical dataflow, readout, and decoding. 2.2.1. SOURCE ENCODING Consider the source encoding system illustrated on the left in Figure 2, with a regular array of sources of size d k. While any sufficiently local source can be chosen, here we model a generic dipole emitter, i.e. a cylindrical source with common driving frequency. We logically partition the source array by row, so that the ith row corresponds with the ith entry of the input x Rd. A straightforward method for positional encoding is to uniformly quantize xi to k distinct values (i.e., log(k) bits of precision); and then power only the source in row i corresponding with the quantized value. That is, for any input x, precisely one source is active per row of the source array. 2.2.2. OPTICAL DATAFLOW Given the source s(x) derived from the positional encoding methodology above, executing the forward pass of the optical neural network is equivalent to simply driving the material design with the source, i.e. the computation is physically realized by the resultant wave propagation. We denote the material distribution obtained from optimiza- tion as µ r and ϵ r. Then the wave operator associated with the optimized hardware design is given by: A(z, ω; µ r, ϵ r) : j( µ 1 r (z) ω2ϵ r(z)) ω , (10) 3 Nonlinear Computation with Linear Optics via Source-Position Encoding B Spline Polygonal Mesh Neural Implicit Field p1 p2 6 9LBbBU0lUqseiF48V7Qe0oWy2m3bpZhN2J0IJ QlePCji1V kzX jts1Bqw8GHu NMDMvSKQw6LpfTmFldW19o7hZ2tre2d0r7x 0TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfjm5nfuTaiFg94CThfkS HSoSCUbTSfdI 75crbtWdg wlXk4qkKPRL3 2BjFLI6QSWpM13MT9DOqUTDJp6VeanhC2ZgOedSRSNu Gx 6pScWGVAwljbUkjm6s JjEbGTKLAdkYUR2bZm4n ed0Uwys EypJkSu2WBSmkmBMZn TgdC coZxYQpkW9lbCRlRThjadkg3BW375L2mdVb1atXZ3Ualf53EU4QiO4RQ8uIQ63EIDmsBgCE wAq OdJ6dN d90Vpw8plD AXn4xsGo2l latexit p3 N xtL0KgoA " AB6nicbVBNS8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ Q lePCji1V kzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLVCahG wSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj hQ8pAzaqz0kPQv WKW3XnIKvEy0kFcjT65a eIG ZphNIwQbXuem5i Iwqw5nAamXakwoG9Mhdi2VNELtZ NTp TMKgMSxsqWNGSu p7IaKT1JApsZ0TN SC97M E r5ua8NrPuExSg5ItFoWpICYms7 JgCtkRkwsoUxeythI6oMzadkg3BW35lbQuql6tWr u rNRv8jiKcAKncA4eXEd7qABTWAwhGd4hTdHOC Ou OxaC04 cwx IHz QMHno2m latexit p4 p5 t 5HjtW vbc " AB6nicbVBNS8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ Q lePCji1V kzX jts1BWx8MPN6bYWZekEh0HW ncLa sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajh UijeRIGSdxLNaRI3g7GtzO cS1EbF6xEnC YgOlQgFo2ilh6Rf65crbtWdg6wSLycVyNHol796g5 ilEVfIJDWm67kJ hnVKJjk01IvNTyhbEyHvGupohE3fjY dUrOrDIgYaxtKSRz9fdERiNjJlFgOyOK I7PszcT vG6K4bWfCZWkyBVbLApTSTAms7 JQGjOUE4soUwLeythI6opQ5tOyYbgLb 8SloXVa9Wrd 1fVuo3eRxFOIFTOAcPrqAOd9CAJjAYwjO8wpsjnRfn3flYtBacfOY sD5 AEKpo2o latexit p6 p7 p8 h QNVP4ZOd5jbO2PstxWOtxc " ACK3icbVDLSgMxFM34rPU16tJN sAiuyoxIdSOUunGpYKvQKUMmvW2DmcyQ3BHLMP jxl9xoQsfuPU TB IrwOBwzn3JjknSqUw6Hmvzszs3PzCYmpvLyurbubmy2TJpDk2 eyERfRcyAFAqaKFDCVaqBxZGEy j6ZORf3oA2IlEXOEyhE7O Ej3B GVopdBsBDgBZmAcxwFnMj8tCnpMgwj6QuWpVbW4LWga jToJmgsU zQA1f3yQrfiVb0x6F iT0mFTHEWuo 2Jp7FoJBLZkzb91Ls5Eyj4B KcpAZSBm Zn1oW6pYDKaTj7MWdNcqXdpLtD0K6Vj9vpGz2JhHNnJ USLz2xuJ 3ntDHtHnVyoNENQfPJQL5MUEzoqjnaFBo5yaAnjWti U j5gmnG09ZtCf7vyH9Ja7 q16q184NKvTGto0S2yQ7ZIz45JHVySs 5Ik3ByRx7IM3lx7p0n5815n4zONOdLfIDzscn 1eoxw latex it ωH !p1 . . . pn " ωH { !v1 . . . vn " , !ε1 . . . εm " } v1 v2 v3 L TgJBEOzF IL9ehlIjHxRHYNQY9ELx4xyiOBDZkdGpgwO7uZmSUhGz7BiweN8eoXefNvHGAPClbSaWqO91dQSy4Nq7eQ2Nre2d K7hb39g8Oj4vFJU0eJYthgkYhU O6AaBZfYMNwIbMcKaRgIbAXju7nfmqDSPJPZhqjH9Kh5APOqLHS46RX6RVLbtldgKwTLyMlyFDvFb 6 YglIUrDBNW647mx8VOqDGcCZ4VuojGmbEyH2LFU0hC1ny5 OnZELq TJIFK2pCEL9fdESkOtp2FgO0NqRnrVm4v eZ3EDG78lMs4MSjZctEgEcREZP436XOFzIipJZQpbm8lbEQVZcamU7AheKsvr5PmVdmrlqsPlVLtNosjD2dwDpf gwTXU4B7q0AGQ3iGV3hzhPivDsfy9ack82cwh84nz8Qwo2s latexit v4 N S8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Ae2oWy2k3bpZhN2N0IJ RdePCji1X jzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLV CahGwSU2DTcCO4lCGgUC28H4dua3n1BpHsHM0nQj hQ8pAzaqz02MNEcxHLvtcvV9yqOwdZJV5OKpCj0S9 9QYxSyOUhgmqdzE NnVBnOBE5LvVRjQtmYDrFrqaQ Raj bXzwlZ1YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIZVsCN7y6ukdVH1atXa WlfpPHUYQ TOIVz8OAK6nAHDWgCAwnP8ApvjnZenHfnY9FacPKZY gD5 MHe6OQzw latexit ω1 N S8NAEJ34WetX1aOXxSJ4KkmR6rHoxWMF 4FtKJvtpF262YTdjVBC 4UXD4p49d9489 4bXPQ1gcDj dmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJsFrHq BFSj4BKbhuBnUQhjQKB7WB8O PbT6g0j WDmSToR3QoecgZNVZ67GiuYhlv9ovld2KOwdZJV5OypCj0S9QYxSyOUhgmqdzE NnVBnOBE6LvVRjQtmYDrFrqaQ Raj bXzwl51YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIRVtCN7y6ukVa14tUrt rJcv8njKMA pnMEFeHAFdbiDBjSBgYRneIU3RzsvzrvzsWhdc KZE gD5 MHfSeQ0A latexit ω2 ω3 ω4 ω5 ω6 ω7 ω8 ω9 ωH : ωNN Figure 3. Hardware parameterization instances. On the left, we show B spline patches whose interior determine freespace pores amidst a homogeneous dielectric (e.g., SiO2). The parameters θH modulate the shape of the pores through the spline control points p1, . . . , pn. In the center, a triangular mesh is parameterized by a collection of n vertices v1, . . . , vn, and permittivities ϵ1, . . . , ϵm within the interior of each triangle in the mesh. In principle, the intermediate permittivities could be quantized into several discrete values to minimize the number of materials required. On the right, a neural implicit field parameterizes the distribution of material via its zero sublevel set. Details of this implicit parameterization are in (cref appendix). and the field is given by the solution to (1), i.e. E A 1s(x). 2.2.3. READOUT We measure the intensity of the electric field in small volumes centered at locations {zk}K k 1, for instance with avalanche photodiodes. This gives rise to K non-negative scalar read outs. In our device model, we denote this opera- tion with R(E) R(A 1s(x)) RK 0. 2.2.4. DECODING Before normalizing R(E) to a probability vector, we execute a decoder, D, with parameters θd that are fit during training and fixed at inference time. The decoder is a strictly linear map which accounts for the fact that the association between readout location zk and class label is chosen arbitrarily at initialization. Concretely, this provides a form of invariance; if we were to permute the class labels arbitrarily, applying that same permutation to the rows of D(x; θd) would keep our predictions fixed. While the linear restriction could be relaxed, here it is cru- cial to certify that the nonlinearity is a result of the source position encoding, and not the decoder. In summary, the aggregate system can be modeled by the following map: f(x; ω, µ, ϵ, θ) : D(x; θd)R(A(ω; µ, ϵ) 1s(x)) (11) 2.3. In Silico Inverse Design The system model above defines a family of classifiers pa- rameterized by (θd, µ, ϵ). Training a classifier amounts to making a particular choice of these parameters, which we obtain by constructing a differentiable computational model for Equation (11) and using gradient based optimization. We discretize the wave operator A A Cn2 n2 and the source s(x) b(x) Cn2 on a two-dimensional rectilinear grid of size n n using the finite difference frequency domain (FDFD) method (see Appendix A.2 for details). Then, we use topology optimization in tandem with a custom differentiable simulator to formulate a gradient- based co-design methodology for the material distribution and the decoder parameters. 2.3.1. TOPOLOGY OPTIMIZATION We formulate the problem of material distribution via level set topology optimization (Van Dijk et al., 2013). The idea is to frame the choice of a particular spatial distribution of material as an optimization problem, whose objective, in this context, is a given function of the electromagnetic fields which depends on the material distribution through the wave 4 Nonlinear Computation with Linear Optics via Source-Position Encoding Dataset Quantization (bits) Linear Model Optical Network ANN (MLP) MNIST2 8 46.8 91.7 82.6 MNIST 24 73.0 94.4 93.1 MNIST 64 83.3 95.2 95.6 Table 1. Experimental results for benchmark datasets. The linear model is fit numerically using the same quantized inputs as used in the optical neural network. The optical networks optimize the topology in a design space of 500 500, the results cited here are from the neural field parameterization. Additional training and experiment details can be found in Appendix A.4 operator. Given a rectangular domain Ω R2 and a function h : R2 Rℓ7 R parameterized by θg Rℓ, we define an implicit topology as the zero sublevel set of the parametric function h, that is: T : {x x Ω, h(x; θg) 0}. (12) The set T can be interpreted as the subset of the design area in which a target material of non-unit refractive index (e.g., SiO2) is to be placed, where the remainder of the area Ω\ T contains some given background substrate material (e.g., freespace, or a dielectric). See the lower right-hand side of Figure 3 for an illustration of h (shown as the surface on the far right), and T (shaded in blue). We discretize the domain, associating the material permit- tivity permeability values to each grid cell. To determine these values, we use the standard approach in level-set topol- ogy optimization: for each grid cell we compute the area of intersection with T. Then the material properties asso- ciated with each grid cell are proportional to the area of intersection. While this leads to some intermediate permit- tivities permeabilities, these are restricted to the boundary of the topology T. In the general approach described here, the objective is highly nonconvex and nondifferentiable with respect to the material distribution, so a variety of relaxation approaches are employed. The usual approach to computing the area of intersection is to smooth the boundary of the topology using a heuristic (e.g., a relaxed Heaviside). Instead, we compute the area of intersection using Fiber Monte Carlo (FMC) (Richardson et al., 2024). In FMC, a collection of line segements or fibers are sam- pled from Ω, and the fibers are used analogously to points in simple Monte Carlo to derive an unbiased estimator of the area of intersection. This estimator is differentiable even with an exact Heaviside; obviating the need for heuris- tic smoothing approaches, and resulting in far more stable optimization in our experiments. Using FMC also enables the use of highly flexible and struc- tured parameterizations of the topology. For instance, the center design of Figure 3 illustrates that we can directly optimize a triangular mesh topology with respect to both the vertices and the permittivities associated with each tri- angle constituting the mesh directly. In this context, the parameters θg correspond directly with the vertex positions and triangle permittivities, not the parameters of an implicit topology. The outer designs illustrate implicit topologies derived from B-splines (leftmost), and a neural network im- plicit field (rightmost), in which θg correspond with spline control points, and neural network parameters, respectively. Advanced physical layout and fabrication often requires the designer to arrange a collection of strict geometric primitives (e.g., rectangles or other simple polygons), which can be natively supported in our inverse design scheme. 2.3.2. DIFFERENTIABLE SIMULATION As we describe in detail in Appendix A.3.3, there exist a variety of special purpose field solvers exploiting the ad- joint method dating back decades (Georgieva et al., 2002). There also exist a handful of more general purpose full- wave solvers (Hughes et al., 2018b; 2019) using early research-grade automatic differentiation tools (Maclaurin et al., 2015), and several excellent modern special-purpose tools for specific electromagnetics applications (Laporte et al., 2019). We developed a generic differentiable field solver in Jax (Bradbury et al., 2018), with support for hardware accel- eration on GPU, custom sparse numerical linear algebra primitives with correct sparse vector-Jacobian product im- plementations, batched simulations (varying the data but holding the hardware wave operator fixed), and JIT com- piled simulation to amortize overhead involved in multiple successive simulations. 2.3.3. INVERSE DESIGN OF OPTICAL CLASSIFIER The aggregate forward computational model with parame- ters θ : (θd, θg) of Equation (11) is denoted with: ˆf(x; θ) : D(x; θd)R(A(θg) 1b(x)). (13) 5 Nonlinear Computation with Linear Optics via Source-Position Encoding In multiclass classification, we are given a collection of data D : {(xi, ci)}N i 1 comprised of inputs xi with labels ci. Then we aim to locally solve the following optimization problem, minimizeθ N X i 1 L(ci, Softmax( ˆf(xi; θ))), (14) with an appropriate objective (e.g., cross-entropy) L. We use gradient based optimization (specifically, Adam (Kingma Ba, 2014)), exploiting FMC-based topology op- timization and differentiable simulation to obtain the deriva- tives with respect to the material distribution parameters θg, and conventional automatic differentiation for the deriva- tives of the decoder parameters θd with respect to the objec- tive. 3. Experiments Our experimental setup is the inverse design of a two dimen- sional optical classifier using a regular array of cylindrical (i.e., dipole) sources for our positional encoding (see Fig- ure 2). Like prior work (Khoram et al., 2019; Wanjura Marquardt, 2024), we consider multi-class classification over the MNIST dataset, which is comprised of 28 28 grayscale images at 8-bit precision, each of which belongs to one of 10 distinct classes. 3.1. Encoding We first linearly project each input x R784 to a p-vector x Rp, with values of p 4, 8, 16 evaluated in our ex- periments. The linear projection is derived from principal components analysis (PCA). Then, for each of p dimensions comprising x, we uniformly quantize the entry into q bins, reducing the precision of the entry to log2 q bits. In our experiments, we use q 4, 8, 16. These configurations of (p, q) correspond with each image encoded with 8, 24, and 64 bits. Physically, this encoding corresponds with a p q array of identical cylindrical sources laid out within the design area. For any given x, the pth row of the source array has precisely one source activated, corresponding with the bin that index is quantized to. 3.2. Ablation For each quantization configuration we also fit a purely numerical linear model ˆf( x; W) W x (to the same quan- tized data), and a standard deep artificial neural network (an MLP), to contextualize the expected performance without using positional encoding; the results are shown in Table 1. Our optical neural networks significantly improve classifi- cation performance compared to a linear model (95.2 for the optical network compared to 83.3 for the linear model, 1 gdDX120 oNI lgxkn6Ed0IHnIGTVWur 9aTyWym7FnYEsEy8nZch81 9fszSCKVhgmrd9dzE BlVhjOBk2Iv1ZhQNqID7FoqaYTaz2arTsipVfokjJV90pCZ nsio5HW4yiwyYiaoV70puJ Xjc14YWfcZmkBiWbfxSmgpiYTO8mfa6QGTG2hDLF7a6EDamizNh2irYEb HkZdKqVrxapXZXLdcv8zoKcAwncAYenEMdrqEBTWAwgGd4hTdHOC Ou Mxj64 cwR IHz QPfLo2K latexit MLP Linear Validation Accuracy vs. Iteration Validation Accuracy [ ] Iteration Figure 4. Validation accuracy (as a percentage) against training iteration for the 64 bits image quantization configuration, shown enveloped by 2 standard deviations. The MLP and linear model best validation accuracies are illustrated as dashed horizontal lines in orange and green respectively. Topology vs. Iteration Mesh Bspline Neural Field Iteration Figure 5. Evolution of design topology over the course of opti- mization. The left-most column shows the initialization of the topology (before optimization), the right-most column shows the final topology (after optimization) and the center column illustrates the topology at an intermediate iteration. in the highest-precision regime), and achieve performance commensurate to the deep artificial neural network. The artificial neural network is out-performed by the optical network at low precision (see Figure 4), but it is important to note that we use a basic MLP; fitting an optimized, state of the art convolutional neural network (CNN) would almost certainly outperform our optical networks. We also show examples of how the topology evolves over the course of 6 Nonlinear Computation with Linear Optics via Source-Position Encoding training optimization in Figure 5. Additional details on ex- periments and optimization can be found in Appendix A.4. 4. Limitations and Future Work 4.1. Limitations A limitation of positional encoding is the fact that the nonlin- earities hold only elementwise with respect to the data. With the simpler tasks often used for evaluation in this space, this does not present a major impediment, but more complicated applications often require nonlinear mixtures of variables. Future work should investigate whether this limitation can be overcome by introducing a collection of layers of our compute elements, interconnected with nonlinear optical routers. A practical bottleneck of optimization in our bottom-up physics based approach is differentiable simulation. In the frequency domain, simulation scales (roughly) as O(n3) with an input domain of size n n, so scaling to larger designs likely requires reducing calls to the full field solver, possibly using queries to surrogate models or model reduction-based systems (Cozad et al., 2014). 4.2. Future Work Equation (8) makes explicit a nonlinear relationship be- tween both the source position and the measured fields, and between the source frequency and the measured fields. A follow-on work will explore this frequency-based nonlin- earity and describe its implications and relationship to the position-based nonlinearity introduced in this work. Future physical layer inverse design tools could attempt to directly utilize an FMC-based topology optimization ap- proach to design highly structured geometric layouts like those of a modern digital (or analog) CMOS electronic design, with appropriate modifications to the simulation framework. 4.3. Conclusion We contribute a novel method for low-power nonlinear com- putation in fully linear media. The proposed source posi- tional encoding is straightforward, requiring only the ability to drive the system at distinct locations. We also develop a fully automated inverse design system for extremely specialized optical computing elements. Fur- ther development of automated hardware design is critical to practically reaping the benefits of hardware specializa- tion moving forward. Both positional encoding and auto- mated hardware design generalizes well beyond optics, and may prove to be significant within a variety of computing paradigms. 4.4. Acknowledgements The authors would like to thank Jake Schaefer and Eric Blow for helpful feedback on the manuscript. N.R. was partially supported by the National Science Foun- dation (NSF): OAC-2118201. C.B. was supported by the Swiss National Science Foun- dation (SNSF) through a Postdoc.Mobility fellowship (P500PT 217673 1). References Balanis, C. A. Advanced Engineering Electromagnetics. John Wiley Sons, 2012. Bayer, E. and Schaack, G. Two-photon Absorption of CaF2: Eu2 . Physica Status Solidi (b), 41(2):827 835, 1970. Boyd, R. W., Gaeta, A. L., and Giese, E. Nonlinear Optics. In Springer Handbook of Atomic, Molecular, and Optical Physics, pp. 1097 1110. Springer, 2008. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: Compos- able Transformations of Python NumPy programs, 2018. URL Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language Models are Few-shot Learners. Advances in neural information processing systems, 33: 1877 1901, 2020. Cauchy, A. Sur l equation a l aide de Laquelle on D etermine les Inegalites s Eculaires des Mouvements des Planetes. vol. 9. O-euvres Completes (II eme S erie), 1829. Christiansen, R. E. and Sigmund, O. Inverse Design in Photonics by Topology Optimization: a Tutorial. JOSA B, 38(2):496 509, 2021. Cozad, A., Sahinidis, N. V., and Miller, D. C. Learning Sur- rogate Models for Simulation-based Optimization. AIChE Journal, 60(6):2211 2227, 2014. Feldmann, J., Youngblood, N., Karpov, M., Gehring, H., Li, X., Stappers, M., Le Gallo, M., Fu, X., Lukashchuk, A., Raja, A. S., et al. Parallel Convolutional Processing using an Integrated Photonic Tensor Core. Nature, 589(7840): 52 58, 2021. George, J. K., Mehrabian, A., Amin, R., Meng, J., De Lima, T. F., Tait, A. N., Shastri, B. J., El-Ghazawi, T., Prucnal, P. R., and Sorger, V. J. Neuromorphic Photonics with Electro-absorption Modulators. Optics express, 27(4): 5181 5191, 2019. 7 Nonlinear Computation with Linear Optics via Source-Position Encoding Georgieva, N. K., Glavic, S., Bakr, M. H., and Bandler, J. W. Feasible Adjoint Sensitivity Technique for EM Design Optimization. IEEE transactions on microwave theory and techniques, 50(12):2751 2758, 2002. Gilbert, F. Excitation of the Normal Modes of the Earth by Earthquake Sources. Geophysical Journal International, 22(2):223 226, 1971. G oppert, M. Uber Elementarakte mit zwei Quanten- spr ungen. PhD thesis, Doctoral thesis, Universit at zu G ottingen, 1930. Hermite, C. Remarque sur un Th eor eme de M. Cauchy. Comptes rendus hebdomadaires des s eances de l Acad emie des sciences, 41:181 183, 1855. Hughes, T. W., Minkov, M., Shi, Y., and Fan, S. Training of Photonic Neural Networks through in Situ Backpropaga- tion and Gradient Measurement. Optica, 5(7):864 871, 2018a. Hughes, T. W., Minkov, M., Williamson, I. A. D., and Fan, S. Adjoint Method and Inverse Design for Non- linear Nanophotonic Devices. ACS Photonics, 5(12): 4781 4787, 2018b. doi: 10.1021 acsphotonics.8b01522. Hughes, T. W., Williamson, I. A., Minkov, M., and Fan, S. Forward-Mode Differentiation of Maxwell s Equations. ACS Photonics, 6(11):3010 3016, 2019. Ivanov, A., Dryden, N., Ben-Nun, T., Li, S., and Hoefler, T. Data Movement is All You Need: A Case Study on Optimizing Transformers. Proceedings of Machine Learning and Systems, 3:711 732, 2021. Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of the 44th annual inter- national symposium on computer architecture, pp. 1 12, 2017. Karahan, E. A., Liu, Z., and Sengupta, K. Deep-learning- based Inverse-designed Millimeter-wave Passives and Power Amplifiers. IEEE Journal of Solid-State Circuits, 58(11):3074 3088, 2023. Keyes, R. W. Optical Logic-in the Light of Computer Tech- nology. Optica Acta: International Journal of Optics, 32 (5):525 535, 1985. Khoram, E., Chen, A., Liu, D., Ying, L., Wang, Q., Yuan, M., and Yu, Z. Nanophotonic Media for Artificial Neural Inference. Photonics Research, 7(8):823 827, 2019. Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980, 2014. Laporte, F., Dambre, J., and Bienstman, P. Highly Paral- lel Simulation and Optimization of Photonic Circuits in Time and Frequency Domain based on the Deep-learning Framework Pytorch. Scientific reports, 9(1):5918, 2019. Li, C., Xu, C., Gui, C., and Fox, M. D. Distance Regular- ized Level Set Evolution and its Application to Image Segmentation. IEEE transactions on image processing, 19(12):3243 3254, 2010. Li, Y., Li, J., and Ozcan, A. Nonlinear Encoding in Diffrac- tive Information Processing using Linear Optical Materi- als. Light: Science Applications, 13(1):173, 2024. Maclaurin, D., Duvenaud, D., and Adams, R. P. Autograd: Effortless Gradients in NumPy. In ICML 2015 AutoML workshop, volume 238, 2015. Molesky, S., Lin, Z., Piggott, A. Y., Jin, W., Vuckovi c, J., and Rodriguez, A. W. Inverse Design in Nanophotonics. Nature Photonics, 12(11):659 670, 2018. Momeni, A., Rahmani, B., Mall ejac, M., Del Hougne, P., and Fleury, R. Backpropagation-free Training of Deep Physical Neural Networks. Science, 382(6676):1297 1303, 2023. Pontryagin, Semenovich, L., Mishchenko, E., Boltyanskii, V., and Gamkrelidze, R. The Mathematical Theory of Optimal Processes. Interscience, 1962. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9, 2019. Rekhi, A. S., Zimmer, B., Nedovic, N., Liu, N., Venkatesan, R., Wang, M., Khailany, B., Dally, W. J., and Gray, C. T. Analog mixed-signal hardware error modeling for deep learning inference. In Proceedings of the 56th Annual Design Automation Conference 2019, pp. 1 6, 2019. Richardson, N., Oktay, D., Ovadia, Y., Bowden, J. C., and Adams, R. P. Fiber Monte Carlo. In The Twelfth International Conference on Learning Representations, 2024. URL id sP1tCl2QBk. Sauter, F. Uber das verhalten eines elektrons im homo- genen elektrischen feld nach der relativistischen theorie diracs. Zeitschrift f ur Physik, 69(11):742 764, 1931. doi: 10.1007 BF01339461. URL 1007 BF01339461. Schuman, C. D., Kulkarni, S. R., Parsa, M., Mitchell, J. P., Date, P., and Kay, B. Opportunities for Neuromorphic Computing Algorithms and Applications. Nature Com- putational Science, 2(1):10 19, 2022. 8 Nonlinear Computation with Linear Optics via Source-Position Encoding So, S., Badloe, T., Noh, J., Bravo-Abad, J., and Rho, J. Deep Learning Enabled Inverse Design in Nanophotonics. Nanophotonics, 9(5):1041 1057, 2020. Sui, X., Wu, Q., Liu, J., Chen, Q., and Gu, G. A Review of Optical Neural Networks. IEEE Access, 8:70773 70783, 2020. Ulrich, I. E., Boehm, C., Zunino, A., B osch, C., and Ficht- ner, A. Diffuse Ultrasound Computed Tomography. The Journal of the Acoustical Society of America, 151(6): 3654 3668, 2022. Van Dijk, N. P., Maute, K., Langelaar, M., and Van Keulen, F. Level-set Methods for Structural Topology Optimiza- tion: a Review. Structural and Multidisciplinary Opti- mization, 48(3):437 472, 2013. Wanjura, C. C. and Marquardt, F. Fully Nonlinear Neuro- morphic Computing with Linear Wave Scattering. Nature Physics, 20(9):1434 1440, 2024. Xia, F., Kim, K., Eliezer, Y., Han, S., Shaughnessy, L., Gi- gan, S., and Cao, H. Nonlinear Optical Encoding Enabled by Recurrent Linear Scattering. Nature Photonics, pp. 1 9, 2024. Yildirim, M., Dinc, N. U., Oguz, I., Psaltis, D., and Moser, C. Nonlinear Processing with Linear Optics. Nature Photonics, pp. 1 7, 2024. Zuo, Y., Li, B., Zhao, Y., Jiang, Y., Chen, Y.-C., Chen, P., Jo, G.-B., Liu, J., and Du, S. All-optical Neural Network with Nonlinear Activation Functions. Optica, 6(9):1132 1137, 2019. 9 Nonlinear Computation with Linear Optics via Source-Position Encoding A. Appendix A.1. Notation R 0 The set of nonnegative real numbers {x R x 0}. R 0 The set of positive real numbers {x R x 0}. T R 0 An alias for the nonnegative reals used to suggest a temporal coordinate. X Boundary of the set X, i.e., the set difference of its closure against its interior. A.2. Electromagnetics We follow the notation given in (Balanis, 2012), which is briefly discussed here and summarized at the conclusion of this section. Importantly, we work in the frequency domain, meaning we assume that the fields are harmonic functions of their temporal argument. We use boldface font to denote the complex-valued form of a given field quantity, and script font for the associated time-dependent field quantity. These representations are related through a complex sinuisoidal term ejωt where the frequency ω in units of [radians s] is implicit (but assumed fixed across all quantities) and j denotes the imaginary unit, as is standard in electrical engineering. E : R3 T 7 R3 The time-harmonic electric field. If the complex electric field is denoted E : R3 7 C3, then E(x, t) Re(ejωtE(x)). Units [V m]. H : R3 T 7 R3 The time-harmonic magnetic field. If the complex magnetic field is denoted H : R3 7 C3, then H(x, t) Re(ejωtH(x)). Units [A m]. D : R3 T 7 R3 The time-harmonic electric flux density. If the complex electric flux density is denoted D : R3 7 C3, then D(x, t) Re(ejωtD(x)). Units [C m2]. B : R3 T 7 R3 The time-harmonic magnetic flux density. If the complex magnetic flux density is denoted B : R3 7 C3, then B(x, t) Re(ejωtB(x)). Units [W m2]. J : R3 T 7 R3 The time-harmonic electric current density. If the complex electric current density is denoted B : R3 7 C3, then B(x, t) Re(ejωtB(x)). Units [W m2]. Q : R3 T 7 R The time-harmonic electric charge density. If the complex electric charge density is denoted Q : R3 7 C, then Q(x, t) Re(ejωtQ(x)). Units [C m3]. To denote the material parameters which appear in the constitutive relations, we use the following scalar functions of position. ϵ : R3 7 C The scalar, complex-valued electric permittivity conductivity. The real part Re(ϵ) is the electric permittivity and the imaginary part Im(ϵ) σ is the electric conductivity. µ : R3 7 R The scalar, real-valued magnetic permeability. This choice follows from our material assumptions, which are standard in many computational electromagnetics applications. We assume that all materials in our simulations are linear, isotropic, and non-dispersive. In electromagnetics a material is linear if the constitutive parameters (i.e., the value of (ϵ, µ) at any point in the simulation domain) do not depend on the value of the field intensities ( E 2 2, H 2 2). A material is isotropic if its constitutive parameters do not depend on the directions (1 E E, 1 H H) of the applied fields. Finally, a material is non-dispersive if its constitutive parameters do not depend on the frequency ω of excitation. In the general case, the constitutive parameters relate the fields through convolution, but in the frequency domain (and 10 Nonlinear Computation with Linear Optics via Source-Position Encoding representing the constitutive parameters as scalar values), we can simply write D ϵE (15) B µH. (16) We use ϵ0, µ0 to denote the permittivity and permeability of the vacuum, respectively. We denote the relative permittivity ϵr ϵ ϵ0 and the relative permeability µr µ µ0. We sometimes refer to the relative permittivity as the dielectric constant. Finally, we use k ω µϵ to denote the wavenumber, which is related to the wavelength via k 2π λ for wavelength λ. In vacuum, we have k0 ω c0, where c0 is the speed of light in vacuum. A.3. Additional Discussion of Related Work A.3.1. NONLINEAR OPTICS Nonlinear optical effects were first predicted in the early 1930s (G oppert, 1930) and then observed in the early 1960s at Bell Labs (Bayer Schaack, 1970). Subsequently, a wide body of work has explored nonlinear interactions with respect to intensity, frequency, phase, and polarization (Boyd et al., 2008). Nonlinearity is observed in all media (including the vacuum) above some finite field intensity (Sauter, 1931). Unfortunately, it is generally observed only at the extremes of power density, near the breakdown voltage for most materials. At a fundamental level, this is a result of the fact that photons (being bosonic particles) do not obey the Pauli exclusion principle. This means that in most conditions in free space, the lack of interaction between photons precludes nonlinear optical effects. By convention, the term nonlinear optics definitionally implies what we call here materials-derived nonlinear effects (Boyd et al., 2008). From a physical perspective, the view of nonlinear optics is correct; although in some technical sense one could make non-contradictory statements in the study of vacuum optical nonlinearities, practically speaking the science and engineering of optical nonlinearities is that science concerned with the nonlinear response of a material to an applied optical field. The nonlinearity we pursue in this work (and has been the major focus of the recent work in optical neural networks), is a logical nonlinearity associated with implicitly defined mathematical functions whose values are determined by the physical states of the system at hand. That is, the physical signals with which we associate logical symbols interact linearly, but our logical symbols are related nonlinearly. A.3.2. NONLINEARITIES IN OPTICAL NEURAL NETWORKS With respect to optical neural networks, the pursuit of a mechanism for practical, low-power nonlinearity is a major program. (Xia et al., 2024) leverage a multiple-scattering cavity to implement a randomized reservoir neural network which is used for feature learning on downstream tasks like classification and compression. Notably, is not a trainable neural network, in the sense that the network does not have parameters which can be fit through optimization, but instead can serve as a fixed unsupervised pre-processing embedding layer for a downstream model application. Our work differs in our explicit treatment of the optimization of the optical computing element, via topology optimization of the underlying material distribution. (Wanjura Marquardt, 2024) introduce an alternative methodology for nonlinearities. The discrete form of Maxwell s equations in the frequency domain can be cast as a certain linear system Ae b. (Wanjura Marquardt, 2024) essentially observe that a nonlinearity can be supported via matrix inversion, i.e., for x R, it follows that f(x) : A(x) 1b can be chosen in a form such that f is nonlinear in x. This is something like a structural nonlinearity in machine learning terminology, and an effective application of the underlying physics to derive a nonlinearity. On the other hand, we achieve a nonlinearity in the source term b above, and use the structural parameters in A ahead of time during training to learn effective electromagnetic mode profiles which can be exploited at inference time by b. (Li et al., 2024) study what could be called the general case of the work presented in (Xia et al., 2024), providing an analysis of the generic technique of data repitition-based strategies. The core argument is that the repitition within diffractive volumes (i.e., the optical cavity used in (Xia et al., 2024)) cannot strictly serve as optical implementations of most conventional neural network layers, but are still useful in task specific contexts. (Yildirim et al., 2024) introduce a technique which is similar to (Xia et al., 2024) but utilizes multiple modulating planes where data is repeatedly embedded. 11 Nonlinear Computation with Linear Optics via Source-Position Encoding A.3.3. INVERSE DESIGN IN PHOTONICS Inverse design of electromagnetic devices is widely studied (Molesky et al., 2018; Christiansen Sigmund, 2021; Georgieva et al., 2002), ranging from fairly application specific microwave circuit parameter optimization (Georgieva et al., 2002) to flexible structural optimization with full-wave field solvers (Hughes et al., 2018b; 2019). We use the adjoint method, which amounts to implicitly differentiating through our full-wave field solver to compute derivatives with respect to the geometry and material properties of our design. The adjoint method dates back to the 1960s (Pontryagin et al., 1962). In photonics, the use of modern computational methods (e.g., optimization, machine learning and data fitting), has drawn much interest in the design of flexible, complex, and unintuitive electromagnetic system designs across a variety of applications (Karahan et al., 2023; So et al., 2020). These efforts are in the direction of extreme specialization, leveraging the enormity of modern computing resources at our disposal to manage the complexity of low-level physical device design, whereas human designers are at least apparently obligated to employ abstraction and modularity to succeed in the design of complex systems. Our automated design methodology also subscribes to the theme of extreme specialization, but our use of topology optimization specifically is most similar to (Khoram et al., 2019). That said, (Khoram et al., 2019) depend on a material- derived nonlinearity (i.e., the electrical response of a hypothetical optical saturable absorber), and a discrete smoothed level set topology optimization method drawn from the computer vision literature (Li et al., 2010). Our method obviates material-derived nonlinearities and an exact topology optimization method, which we can differentiate through using Fiber Monte Carlo (Richardson et al., 2024). A.4. Additional Experimental Details A.4.1. DOES CODESIGN MATTER? During optimization, we track the root mean square (RMS) of the gradient of the objective with respect to (a) the topology parameters and (b) the decoder parameters, in an attempt to understand the relative contributions to the designs. In general (but with some exceptions), we find that the topology derivatives tend to be between 1 and 3 orders of magnitude larger in RMS than the decoder derivatives; to first order then, the interpretation is that the classification accuracy of the system is around 10-1000x more sensitive to changes in the topology than that of the decoder parameters. However, it should not be inferred from this quantitative result that the decoder is not critical to the overall system performance. In fact, optimizing without using the decoder at all degrades the system performance significantly. Our interpretation of the relatively smaller decoder gradient magnitude follows the apparent role of the decoder as a permuting map to account for the arbitrary assignment of class labels to output measurement locations. Then, after initialization, optimizing the decoder amounts to merely breaking symmetry in its weights to achieve a better class label to output region assignment. From that point, the absolute magnitude of any given decoder weight has minimal effect, since only their relative values are responsible for the label output region re-mapping. Optimizing only the decoder but not the topology results in significantly worse performance (around 10 reduction in classification accuracy). But, the fact that the decoder-only performance is significantly better than random indicates that even randomly initialized topologies can be adapted for use as effective classifiers with an appropriately optimized linear decoder; similar to the use of multiple-scattering devices as reservoir networks. A.4.2. COMPUTING RESOURCES All experiments were carried out on a single node Linux server with a 32 CPUs and an Nvidia RTX 3080Ti GPU. For efficiency, we precompute all possible source combinations (derived from the finite number of quantized values) and store them in an in-memory cache, and the wave operator (less the contribution from the topology) is precomputed and stored as a sparse matrix in memory. We execute batched simulations with a batch size of 32, as we determined that batch sizes larger than this only marginally improved performance. We use separate optimizers for the topology and decoder parameters, both Adam optimizers with separate step sizes depending on the topology parameterization used (for implicitly formulated B-splines and neural fields, we use αg 0.001, whereas direct mesh parameterization used αg 0.01). 12\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nNonlinear Computation with Linear Optics via Source-Position Encoding Nick Richardson 1 Cyrill B osch 1 Ryan P. Adams 1 Abstract Optical computing systems provide an alternate hardware model which appears to be aligned with the demands of neural network workloads. How- ever, the challenge of implementing energy effi- cient nonlinearities in optics a key requirement for realizing neural networks is a conspicuous missing link. In this work we introduce a novel method to achieve nonlinear computation in fully linear media. Our method can operate at low power and requires only the ability to drive the optical system at a data-dependent spatial position. Leveraging this positional encoding, we formulate a fully automated, topology-optimization-based hardware design framework for extremely special- ized optical neural networks, drawing on modern advancements in optimization and machine learn- ing. We evaluate our optical designs on machine learning classification tasks: demonstrating sig- nificant improvements over linear methods, and competitive performance when compared to stan- dard artificial neural networks. 1. Introduction Modern machine learning systems demand an unprece- dented scale of resources to drive application performance (Radford et al., 2019; Brown et al., 2020). Sustaining these efforts requires infrastructure developments, or improve- ments in efficiency; most likely both. The resulting em- phasis on domain-specific hardware (Jouppi et al., 2017), coincident with the rampant popularity of neural networks for machine learning applications, has sparked a resurgence of interest in alternative hardware platforms for neural net- work workloads (Schuman et al., 2022). Computing with optics is particularly attractive, since opti- cal signals are well-aligned with parallel, high-bandwidth- requiring dataflows, like those found in many neural net- works (Sui et al., 2020). The linear scattering phenomena in- 1Department of Computer Science, Princeton Univer- sity. Correspondence to: herent to electromagnetic energy propagation is amenable to efficient implementations of the multiply-accumulate-based kernels which dominate neural network workloads (Ivanov et al., 2021). All that said, a critical unsolved problem is achieving efficient nonlinearities in optical hardware.\n\n--- Segment 2 ---\nCorrespondence to: herent to electromagnetic energy propagation is amenable to efficient implementations of the multiply-accumulate-based kernels which dominate neural network workloads (Ivanov et al., 2021). All that said, a critical unsolved problem is achieving efficient nonlinearities in optical hardware. Conventional materials-based optical nonlinearities typi- cally require some combination of exotic materials, difficult fabrication procedures, or high-power light sources which negate the efficiency gained through more-or-less passive linear wave propagation (Zuo et al., 2019; Feldmann et al., 2021; Keyes, 1985). An alternative is to give up on an all- optical design, and compromise with optoelectronic trans- duction; essentially executing linear computational kernels in the optical domain, and nonlinearities in standard elec- tronics (Hughes et al., 2018a; George et al., 2019). This strategy seems to use both media in their ideal operating regime, but much like analog electronic neural networks, these systems are ultimately gated by fundamental ineffi- ciencies involved in transducing signals between the two physical media (Rekhi et al., 2019). As a result, the past year has seen several works concerned with developing low-power methods for nonlinear compu- tation using linear media (Xia et al., 2024; Li et al., 2024; Wanjura Marquardt, 2024; Yildirim et al., 2024; Mo- meni et al., 2023); largely with an emphasis on machine learning applications. A discussion on prior art in inverse design for computational electromagnetics, and background on nonlinear optics are provided in Appendix A.3.3 and Appendix A.3.1. We discuss recent efforts toward nonlinear computation with linear optics, and its relationship to our own work in Appendix A.3.2. In this work, we contribute a novel method for nonlinear computation using low power, fully linear optics: source positional encoding. We exploit the fact that the relationship between the measured fields and the position of the source is nonlinear, and encode the data directly in the source position. Then, we optimize the spatial distribution of material such that the source-position-to-measured-field approximates a desired transfer function (e.g., a classifier).\n\n--- Segment 3 ---\nWe exploit the fact that the relationship between the measured fields and the position of the source is nonlinear, and encode the data directly in the source position. Then, we optimize the spatial distribution of material such that the source-position-to-measured-field approximates a desired transfer function (e.g., a classifier). By harnessing a custom, differentiable full-wave field solver in concert with topology optimization for design of the optical hardware, we avoid analytic simplifications of the physics beyond discretization of Maxwell s equations (a source of error 1 arXiv:2504.20401v1 [physics.optics] 29 Apr 2025 Nonlinear Computation with Linear Optics via Source-Position Encoding which is well understood and largely controllable). In Section 2.1, we detail the fundamental physics underlying our position-based encoding method. Section 2.2 describes a proposed system design (see Figure 2) for an optical clas- sifier leveraging positional encoding as a nonlinearity. Sec- tion 2.3 describes the inverse problem associated with the hardware design; which is addressed with topology opti- mization and differentiable simulation. Section 3 presents experimental results across an array of test problems, with an emphasis on ablations verifying the effectiveness of our optical nonlinearity over fully linear methods. Finally, Sec- tion 4 offers limitations of our method and promising future directions. 2. Method 2.1. Source positional encoding Consider Maxwell s equations1 in the frequency domain, denoting the electric field with E, and the source with J . The resultant wave equation over a domain Ω R3 and subject to Dirichlet boundary conditions can be written as µ 1 r (z) E(z, ω) ω2ϵr(z)E(z, ω) jωJ (z, ω) (1) z Ω, E(z, ω) 0 z Ω. (2) The medium is modeled as isotropic and lossless, i.e. the relative permeability µr and relative permittivity ϵr are real- valued scalars. The corresponding generalized eigensystem is: µ 1 r (z) E(z, ω) ω2ϵr(z)E(z, ω).\n\n--- Segment 4 ---\nthe relative permeability µr and relative permittivity ϵr are real- valued scalars. The corresponding generalized eigensystem is: µ 1 r (z) E(z, ω) ω2ϵr(z)E(z, ω). (3) We further define the following inner product: E(z, ω), E (z, ω) ϵr : Z Ω E (z, ω)ϵr(z)E (z, ω) dz. (4) The wave operator M : µ 1 r is self-adjoint with respect to this inner product. Therefore, by the spectral theorem (Hermite, 1855; Cauchy, 1829), M has a discrete spectrum with eigenvalue-eigenfunction pairs (wi, Ei). Sup- pose the eigenfunctions are chosen to be orthonormal with respect to the inner product (4), i.e. Ei(z), Ej(z) ϵr δij, such that {Ej} j 1 forms an orthonormal basis. Then it follows that any field E(z, ω) satisfying (1) can be decomposed as, E(z, ω) X i ci(ω)Ei(z). (5) Inserting this decomposition into (1), using (3) and comput- ing the inner product (4) with the eigenfunction Em results 1Our notation, and a brief review of the relevant electromagnet- ics is reviewed in Appendix A.2.\n\n--- Segment 5 ---\nThen it follows that any field E(z, ω) satisfying (1) can be decomposed as, E(z, ω) X i ci(ω)Ei(z). (5) Inserting this decomposition into (1), using (3) and comput- ing the inner product (4) with the eigenfunction Em results 1Our notation, and a brief review of the relevant electromagnet- ics is reviewed in Appendix A.2. i JZBLWyECOYDkiPMbSbJkr29Y3dPCEfAv2JjoYitv8POf MmuUITHw83pthZl4QC6N6347K6tr6xubua389s7u3n7h4LCho0QxrLNIRKoVgEbBJdYNwJbsUIA4HNYHQ9ZuPqDSP5IMZx iHMJC8zxkYK3ULx3dgUHEQ9MYuUzxI5 nrRLbkz0GXiZaRIMtS6ha9OL2JiNIwAVq3PTc2fgrKcCZwku8kGmNgIxhg21IJIWo nZ0 oWdW6dF pGxJQ2fq74kUQq3HYWA7QzBDvehNxf 8dmL6l37KZwYlGy qJ8IaiI6zYL2uEJmxNgSYIrbWykbgJmI9F5G4K3 PIyaZRLXq VUuS8Xq1dZHDlyQk7JOfHIBamSW1IjdcJISp7JK3lznpwX5935mLeuONnMEfkD5 MHd eV1Q latexit Material Distribution Modes Ei E1 E3 E6 Figure 1. Three material distributions (vacuum with ϵr 1, and two arbitrarily chosen distributions, with ϵr 3 in blue regions and ϵr 1 in white regions; µr 1 for all of them) and the 1st, 3rd and 6th corresponding eigenmodes.\n\n--- Segment 6 ---\ni JZBLWyECOYDkiPMbSbJkr29Y3dPCEfAv2JjoYitv8POf MmuUITHw83pthZl4QC6N6347K6tr6xubua389s7u3n7h4LCho0QxrLNIRKoVgEbBJdYNwJbsUIA4HNYHQ9ZuPqDSP5IMZx iHMJC8zxkYK3ULx3dgUHEQ9MYuUzxI5 nrRLbkz0GXiZaRIMtS6ha9OL2JiNIwAVq3PTc2fgrKcCZwku8kGmNgIxhg21IJIWo nZ0 oWdW6dF pGxJQ2fq74kUQq3HYWA7QzBDvehNxf 8dmL6l37KZwYlGy qJ8IaiI6zYL2uEJmxNgSYIrbWykbgJmI9F5G4K3 PIyaZRLXq VUuS8Xq1dZHDlyQk7JOfHIBamSW1IjdcJISp7JK3lznpwX5935mLeuONnMEfkD5 MHd eV1Q latexit Material Distribution Modes Ei E1 E3 E6 Figure 1. Three material distributions (vacuum with ϵr 1, and two arbitrarily chosen distributions, with ϵr 3 in blue regions and ϵr 1 in white regions; µr 1 for all of them) and the 1st, 3rd and 6th corresponding eigenmodes. Clearly, the mode profiles can be quite distinct from the basic freespace modes (and one another) by varying the material distribution. Complex mate- rial distributions give rise to complex modes, implying a highly nonlinear relation between source position and measured field. in (Gilbert, 1971; Ulrich et al., 2022): cm(ω)ω2 m ω2cm(ω) jω Em(r) J (r, ω) ϵ .\n\n--- Segment 7 ---\nComplex mate- rial distributions give rise to complex modes, implying a highly nonlinear relation between source position and measured field. in (Gilbert, 1971; Ulrich et al., 2022): cm(ω)ω2 m ω2cm(ω) jω Em(r) J (r, ω) ϵ . (6) The coefficients can be written explicitly as cm(ω) jω ω2m ω2 Em(z), J (z, ω) ϵr. (7) For simplicity, consider the source to be a monochromatic point source at location zs Ωwith frequency ωs and unit amplitude, i.e. J (z, ω) δ(z zs)δ(ω ωs). Then ci jωs ω2 i ω2s E i (zs). The electric field at a measurement location zR is therefore given by E(zR) X i jωs ω2 i ω2s E i (zs)Ei(zR). (8) This can be interpreted as the statement that the field mea- sured at zR (a fixed location) is a linear combination of modes Ei whose amplitudes depend on (i) the deviation between source and mode frequency, and (ii) the relative spatial placement of the source and the mode amplitude profile. Concretely, consider an eigenfunction Em, and suppose it 2 Nonlinear Computation with Linear Optics via Source-Position Encoding Class Label C1 Ck . . .\n\n--- Segment 8 ---\n. . P(C x, ω) x Rd 3 aKHgrmA " AB83icbVDLSgNBEJz1GeMr6tHLYBA8hV2R6DHoRW8RzAOyS5id9CZDZh M9IphyW948aCI V3 Gm3 jbLIHTSxoKq6e7yEyk02va3tbK6tr6xWdoqb s7u1XDg7bOk4VhxaPZay6PtMgRQtFCihmyh goS h49vcr zCEqLOHrASQJeyIaRCARnaCTXRXjC7C5KUpz2K1W7Zs9Al4lTkCop0OxXvtxBzNMQIuSa d1z7AS9jCkUXMK07KYaEsbHbAg9QyMWgvay2c1TemqUAQ1iZSpCOlN T2Qs1HoS qYzZDjSi14u uf1Ugyu vEzkL0HE54uCVFKMaR4AHQgFHOXEMaVMLdSPmKcTQxlU0IzuLy6R9XnPqtfr9RbVxXcRIsfkhJwRh1 ySBrklTdIinCTkmbySNyu1Xqx362PeumIVM0fkD6zPH7h6kiY latexit Input Source Geometry Simulation 4 bt20O2vpg4PHeDPzgpQzpR3n2yptbG5t75R3K3v7B4dH1eOTjkoySdGjCU9kLyAKORPoaY59lKJA4doPJ7dzvPqFULBGPepqiH5ORYBGjRBvJu2PIw2G15tSdBex14hakBgXaw rXIExoFqPQlBOl q6Taj8nUjPKcVYZApTQidkhH1DBYlR fni2Jl9YZTQjhJpSmh7of6eyEms1DQOTGdM9FitenPxP6 f6ejaz5lIM42CLhdFGbd1Ys8 t0Mm kWo NYRQycytNh0TSag2 VRMCO7qy uk06i7zXrzoVFr3RxlOEMzuESXLiCFtxDGzygwOAZXuHNEtaL9W59LFtLVjFzCn9gf4AqC2OmQ latexit Field Positional Encoder SiO2 Freespace Dipole Source (Inactive) Dipole Source (Active) Positional Encoding Decoder ωd Rd (k k) b s(x) A(ωg) (Section 2.3) (Section 2.2.1) O rvF3Z2d3br5oHhx0Zp4LQNol5Lo lpSziLYVU5x2E0Fx6HP64E u8 rDIxWSxdG9mibUC EoYgEjWGlrYFbrd5TkiFzLtc7PBmbNtuy50Co4BdSgUGtgfvWHMUlDGinCsZQ9x06Ul2GhGOF0VumnkiaYTPCI9jRGOKTSy aLz9CpdoYoiIV kUJz9 dEhkMp6GvO0OsxnK5lpv 1XqpCi69jEVJqmhEFh8FKUcqRnkKaMiEPptPNWAimN4VkTEWmCidVUWH4CyfvAod13IaVuPWrTWvijKcAwnUAcHLqAJN9CNhBI4Rle4c14Ml6Md Nj0Voyipkj CPj8wc6TJGE latexit (Section 2.2.4) QUFR1093lRYJrsO1va25 YXFpObOSXV1b39jMbW3XdBgryqo0FKFqeEQzwSWrAgfBGpFiJPAEq3v9i6Ffv2dK81DewiBirYB0Jfc5JWCkdu7osuAGBHqenzykZ9iFHgPS7hxgl0s8drzkJr1L sbjAdO4n7Zzebtoj4BniTMheTRBpZ37dDshjQMmgQqidOxI2glRAGngqVZN9YsIrRPuqxpqCRmTysZPZfifaN0sB8qUxLwSP09kZBA60Hgmc7huXraG4r ec0Y NWwmUA5N0vMiPB YQD5PCHa4YBTEwhFDFza2Y9ogiFEyeWROCM 3yLKkdFp1SsXR9nC fT LIoF20hwrIQSeojK5QBVURY oGb2iN vJerHerY9x65w1mdlBf2B9 QBI0aGI latexit D(x; ωd) Rk k R(e(x; ωg)) N T0zGxmLju sLi0nFtZvdJhrDhUeChDdeMyDVIEUEGBEm4iBcx3JVy7ndOef30HSoswuMRuBHWftQLhCc7QSI3cec1n2Ha9BNKtEb1PD2kN24Cs0dqmR3SkH5vISL5NdpxUj41sN3J5u2D3Qf8SZ0jyZIhyI dUa4Y89iFALpnWVceOsJ4whYJLSLO1WEPEeIe1oGpowHzQ9aR cko3jdKkXqjMC5D21fGJhPlad3XJHsr6t9eT zPq8boHdQTEUQxQsAH3mxpBjSXn 0KRwlF1DGFfC7Ep5mynG0bScNSU4v0 S652C06xULzYy5dOh nVkyDrZIFvEIfukRM5ImVQIJw kmbyRd vRerU rM9BdMIazqyRH7C vgEHFKqg latexit e(x; ωg) A(ωg) 1s(x) Softmax( ˆf(x; ω)) 1v x82 cJFto4oGBwzn3MvecIJHCoOt O2vrG5tb24Wd4u7e sFh6ei4ZeJUM95ksYx1J6CGS6F4EwVK3k0p1EgeTsY38z89hPXRsTqAScJ9yM6VCIUjKVWncpJin2S2W34s5BVomXkzLkaPRLX71BzNKIK2SGtP13AT9jGoUTPJpsZcanlA2pkPetVTRiBs m187JedWGZAw1vYpJHP190ZGI2MmUWAnI4ojs zNxP 8borhlZ8J ZRNxRYfhakGJNZdDIQmjOUE0so08LeStiIasrQFlS0JXjLkVdJq1rxapXafbVcv87rKMApnMEFeHAJdbiFBjSBwSM8wyu8ObHz4rw7H4vRNSfOYE cD5 ANXj1A latexit Output x1 W bYBFclZki1WXRjcsK9gGdoWTSTBuayYQkI5ahv HGhSJu Rl3 o2ZdhbaeiBwOde7skJWfauO63s7a sbm1Xdop7 7tHxWjo47OkVoW2S8ET1QqwpZ4K2DTOc9qSiOA457YaT29zvPlKlWSIezFTSIMYj wSJGsLGS78fYjMoe5oN6oNK1a25c6BV4hWkCgVag8qXP0xIGlNhCMda9z1XmiDyjDC6azsp5pKTCZ4RPuWChxTHWTzDN0bpUhihJlnzBorv7eyHCs9TQO7WSeUS97ufif109NdB1kTMjUEWh6KUI5Og vA0ZIoSw6eWYKYzYrIGCtMjK2pbEvwlr 8Sjr1mteoNe4vq82bo4SnMIZXIAHV9CEO2hBGwhIeIZXeHNS58V5dz4Wo2tOsXMCf B8 gAvdpHM latexit x2 d uBovgqiQi1WXRjcsK9gFNKJPJpB06mYSZiVhCf8ONC0Xc jPu BsnaRbaemDgcM693DPHTzhT2ra rcra sbmVnW7trO7t39QPzqTiVhHZJzGM58LGinAna1UxzOkgkxZHPad f3uZ 5FKxWLxoGcJ9SI8 FixkBGsjuW6E9cQPs6f5KBjVG3bTLoBWiVOSBpTojOpfbhCTNKJCE46VGjp2or0MS80Ip OamyqaYDLFYzo0VOCIKi8rMs RmVECFMbSPKFRof7eyHCk1CzyzWSeUS17ufifN0x1eO1lTCSpoIsDoUpRzpG eQEoYJISzWeGYCKZyYrIBEtMtKmpZkpwlr 8SnoXTafVbN1fNto3ZR1VOIFTOAcHrqANd9CBLhBI4Ble4c1KrRfr3fpYjFascucY sD6 AF7PpH latexit xd Figure 2.\n\n--- Segment 9 ---\n. P(C x, ω) x Rd 3 aKHgrmA " AB83icbVDLSgNBEJz1GeMr6tHLYBA8hV2R6DHoRW8RzAOyS5id9CZDZh M9IphyW948aCI V3 Gm3 jbLIHTSxoKq6e7yEyk02va3tbK6tr6xWdoqb s7u1XDg7bOk4VhxaPZay6PtMgRQtFCihmyh goS h49vcr zCEqLOHrASQJeyIaRCARnaCTXRXjC7C5KUpz2K1W7Zs9Al4lTkCop0OxXvtxBzNMQIuSa d1z7AS9jCkUXMK07KYaEsbHbAg9QyMWgvay2c1TemqUAQ1iZSpCOlN T2Qs1HoS qYzZDjSi14u uf1Ugyu vEzkL0HE54uCVFKMaR4AHQgFHOXEMaVMLdSPmKcTQxlU0IzuLy6R9XnPqtfr9RbVxXcRIsfkhJwRh1 ySBrklTdIinCTkmbySNyu1Xqx362PeumIVM0fkD6zPH7h6kiY latexit Input Source Geometry Simulation 4 bt20O2vpg4PHeDPzgpQzpR3n2yptbG5t75R3K3v7B4dH1eOTjkoySdGjCU9kLyAKORPoaY59lKJA4doPJ7dzvPqFULBGPepqiH5ORYBGjRBvJu2PIw2G15tSdBex14hakBgXaw rXIExoFqPQlBOl q6Taj8nUjPKcVYZApTQidkhH1DBYlR fni2Jl9YZTQjhJpSmh7of6eyEms1DQOTGdM9FitenPxP6 f6ejaz5lIM42CLhdFGbd1Ys8 t0Mm kWo NYRQycytNh0TSag2 VRMCO7qy uk06i7zXrzoVFr3RxlOEMzuESXLiCFtxDGzygwOAZXuHNEtaL9W59LFtLVjFzCn9gf4AqC2OmQ latexit Field Positional Encoder SiO2 Freespace Dipole Source (Inactive) Dipole Source (Active) Positional Encoding Decoder ωd Rd (k k) b s(x) A(ωg) (Section 2.3) (Section 2.2.1) O rvF3Z2d3br5oHhx0Zp4LQNol5Lo lpSziLYVU5x2E0Fx6HP64E u8 rDIxWSxdG9mibUC EoYgEjWGlrYFbrd5TkiFzLtc7PBmbNtuy50Co4BdSgUGtgfvWHMUlDGinCsZQ9x06Ul2GhGOF0VumnkiaYTPCI9jRGOKTSy aLz9CpdoYoiIV kUJz9 dEhkMp6GvO0OsxnK5lpv 1XqpCi69jEVJqmhEFh8FKUcqRnkKaMiEPptPNWAimN4VkTEWmCidVUWH4CyfvAod13IaVuPWrTWvijKcAwnUAcHLqAJN9CNhBI4Rle4c14Ml6Md Nj0Voyipkj CPj8wc6TJGE latexit (Section 2.2.4) QUFR1093lRYJrsO1va25 YXFpObOSXV1b39jMbW3XdBgryqo0FKFqeEQzwSWrAgfBGpFiJPAEq3v9i6Ffv2dK81DewiBirYB0Jfc5JWCkdu7osuAGBHqenzykZ9iFHgPS7hxgl0s8drzkJr1L sbjAdO4n7Zzebtoj4BniTMheTRBpZ37dDshjQMmgQqidOxI2glRAGngqVZN9YsIrRPuqxpqCRmTysZPZfifaN0sB8qUxLwSP09kZBA60Hgmc7huXraG4r ec0Y NWwmUA5N0vMiPB YQD5PCHa4YBTEwhFDFza2Y9ogiFEyeWROCM 3yLKkdFp1SsXR9nC fT LIoF20hwrIQSeojK5QBVURY oGb2iN vJerHerY9x65w1mdlBf2B9 QBI0aGI latexit D(x; ωd) Rk k R(e(x; ωg)) N T0zGxmLju sLi0nFtZvdJhrDhUeChDdeMyDVIEUEGBEm4iBcx3JVy7ndOef30HSoswuMRuBHWftQLhCc7QSI3cec1n2Ha9BNKtEb1PD2kN24Cs0dqmR3SkH5vISL5NdpxUj41sN3J5u2D3Qf8SZ0jyZIhyI dUa4Y89iFALpnWVceOsJ4whYJLSLO1WEPEeIe1oGpowHzQ9aR cko3jdKkXqjMC5D21fGJhPlad3XJHsr6t9eT zPq8boHdQTEUQxQsAH3mxpBjSXn 0KRwlF1DGFfC7Ep5mynG0bScNSU4v0 S652C06xULzYy5dOh nVkyDrZIFvEIfukRM5ImVQIJw kmbyRd vRerU rM9BdMIazqyRH7C vgEHFKqg latexit e(x; ωg) A(ωg) 1s(x) Softmax( ˆf(x; ω)) 1v x82 cJFto4oGBwzn3MvecIJHCoOt O2vrG5tb24Wd4u7e sFh6ei4ZeJUM95ksYx1J6CGS6F4EwVK3k0p1EgeTsY38z89hPXRsTqAScJ9yM6VCIUjKVWncpJin2S2W34s5BVomXkzLkaPRLX71BzNKIK2SGtP13AT9jGoUTPJpsZcanlA2pkPetVTRiBs m187JedWGZAw1vYpJHP190ZGI2MmUWAnI4ojs zNxP 8borhlZ8J ZRNxRYfhakGJNZdDIQmjOUE0so08LeStiIasrQFlS0JXjLkVdJq1rxapXafbVcv87rKMApnMEFeHAJdbiFBjSBwSM8wyu8ObHz4rw7H4vRNSfOYE cD5 ANXj1A latexit Output x1 W bYBFclZki1WXRjcsK9gGdoWTSTBuayYQkI5ahv HGhSJu Rl3 o2ZdhbaeiBwOde7skJWfauO63s7a sbm1Xdop7 7tHxWjo47OkVoW2S8ET1QqwpZ4K2DTOc9qSiOA457YaT29zvPlKlWSIezFTSIMYj wSJGsLGS78fYjMoe5oN6oNK1a25c6BV4hWkCgVag8qXP0xIGlNhCMda9z1XmiDyjDC6azsp5pKTCZ4RPuWChxTHWTzDN0bpUhihJlnzBorv7eyHCs9TQO7WSeUS97ufif109NdB1kTMjUEWh6KUI5Og vA0ZIoSw6eWYKYzYrIGCtMjK2pbEvwlr 8Sjr1mteoNe4vq82bo4SnMIZXIAHV9CEO2hBGwhIeIZXeHNS58V5dz4Wo2tOsXMCf B8 gAvdpHM latexit x2 d uBovgqiQi1WXRjcsK9gFNKJPJpB06mYSZiVhCf8ONC0Xc jPu BsnaRbaemDgcM693DPHTzhT2ra rcra sbmVnW7trO7t39QPzqTiVhHZJzGM58LGinAna1UxzOkgkxZHPad f3uZ 5FKxWLxoGcJ9SI8 FixkBGsjuW6E9cQPs6f5KBjVG3bTLoBWiVOSBpTojOpfbhCTNKJCE46VGjp2or0MS80Ip OamyqaYDLFYzo0VOCIKi8rMs RmVECFMbSPKFRof7eyHCk1CzyzWSeUS17ufifN0x1eO1lTCSpoIsDoUpRzpG eQEoYJISzWeGYCKZyYrIBEtMtKmpZkpwlr 8SnoXTafVbN1fNto3ZR1VOIFTOAcHrqANd9CBLhBI4Ble4c1KrRfr3fpYjFascucY sD6 AF7PpH latexit xd Figure 2. The input data x is used to determine which subset of the array of sources are active (illustrated in red).\n\n--- Segment 10 ---\nP(C x, ω) x Rd 3 aKHgrmA " AB83icbVDLSgNBEJz1GeMr6tHLYBA8hV2R6DHoRW8RzAOyS5id9CZDZh M9IphyW948aCI V3 Gm3 jbLIHTSxoKq6e7yEyk02va3tbK6tr6xWdoqb s7u1XDg7bOk4VhxaPZay6PtMgRQtFCihmyh goS h49vcr zCEqLOHrASQJeyIaRCARnaCTXRXjC7C5KUpz2K1W7Zs9Al4lTkCop0OxXvtxBzNMQIuSa d1z7AS9jCkUXMK07KYaEsbHbAg9QyMWgvay2c1TemqUAQ1iZSpCOlN T2Qs1HoS qYzZDjSi14u uf1Ugyu vEzkL0HE54uCVFKMaR4AHQgFHOXEMaVMLdSPmKcTQxlU0IzuLy6R9XnPqtfr9RbVxXcRIsfkhJwRh1 ySBrklTdIinCTkmbySNyu1Xqx362PeumIVM0fkD6zPH7h6kiY latexit Input Source Geometry Simulation 4 bt20O2vpg4PHeDPzgpQzpR3n2yptbG5t75R3K3v7B4dH1eOTjkoySdGjCU9kLyAKORPoaY59lKJA4doPJ7dzvPqFULBGPepqiH5ORYBGjRBvJu2PIw2G15tSdBex14hakBgXaw rXIExoFqPQlBOl q6Taj8nUjPKcVYZApTQidkhH1DBYlR fni2Jl9YZTQjhJpSmh7of6eyEms1DQOTGdM9FitenPxP6 f6ejaz5lIM42CLhdFGbd1Ys8 t0Mm kWo NYRQycytNh0TSag2 VRMCO7qy uk06i7zXrzoVFr3RxlOEMzuESXLiCFtxDGzygwOAZXuHNEtaL9W59LFtLVjFzCn9gf4AqC2OmQ latexit Field Positional Encoder SiO2 Freespace Dipole Source (Inactive) Dipole Source (Active) Positional Encoding Decoder ωd Rd (k k) b s(x) A(ωg) (Section 2.3) (Section 2.2.1) O rvF3Z2d3br5oHhx0Zp4LQNol5Lo lpSziLYVU5x2E0Fx6HP64E u8 rDIxWSxdG9mibUC EoYgEjWGlrYFbrd5TkiFzLtc7PBmbNtuy50Co4BdSgUGtgfvWHMUlDGinCsZQ9x06Ul2GhGOF0VumnkiaYTPCI9jRGOKTSy aLz9CpdoYoiIV kUJz9 dEhkMp6GvO0OsxnK5lpv 1XqpCi69jEVJqmhEFh8FKUcqRnkKaMiEPptPNWAimN4VkTEWmCidVUWH4CyfvAod13IaVuPWrTWvijKcAwnUAcHLqAJN9CNhBI4Rle4c14Ml6Md Nj0Voyipkj CPj8wc6TJGE latexit (Section 2.2.4) QUFR1093lRYJrsO1va25 YXFpObOSXV1b39jMbW3XdBgryqo0FKFqeEQzwSWrAgfBGpFiJPAEq3v9i6Ffv2dK81DewiBirYB0Jfc5JWCkdu7osuAGBHqenzykZ9iFHgPS7hxgl0s8drzkJr1L sbjAdO4n7Zzebtoj4BniTMheTRBpZ37dDshjQMmgQqidOxI2glRAGngqVZN9YsIrRPuqxpqCRmTysZPZfifaN0sB8qUxLwSP09kZBA60Hgmc7huXraG4r ec0Y NWwmUA5N0vMiPB YQD5PCHa4YBTEwhFDFza2Y9ogiFEyeWROCM 3yLKkdFp1SsXR9nC fT LIoF20hwrIQSeojK5QBVURY oGb2iN vJerHerY9x65w1mdlBf2B9 QBI0aGI latexit D(x; ωd) Rk k R(e(x; ωg)) N T0zGxmLju sLi0nFtZvdJhrDhUeChDdeMyDVIEUEGBEm4iBcx3JVy7ndOef30HSoswuMRuBHWftQLhCc7QSI3cec1n2Ha9BNKtEb1PD2kN24Cs0dqmR3SkH5vISL5NdpxUj41sN3J5u2D3Qf8SZ0jyZIhyI dUa4Y89iFALpnWVceOsJ4whYJLSLO1WEPEeIe1oGpowHzQ9aR cko3jdKkXqjMC5D21fGJhPlad3XJHsr6t9eT zPq8boHdQTEUQxQsAH3mxpBjSXn 0KRwlF1DGFfC7Ep5mynG0bScNSU4v0 S652C06xULzYy5dOh nVkyDrZIFvEIfukRM5ImVQIJw kmbyRd vRerU rM9BdMIazqyRH7C vgEHFKqg latexit e(x; ωg) A(ωg) 1s(x) Softmax( ˆf(x; ω)) 1v x82 cJFto4oGBwzn3MvecIJHCoOt O2vrG5tb24Wd4u7e sFh6ei4ZeJUM95ksYx1J6CGS6F4EwVK3k0p1EgeTsY38z89hPXRsTqAScJ9yM6VCIUjKVWncpJin2S2W34s5BVomXkzLkaPRLX71BzNKIK2SGtP13AT9jGoUTPJpsZcanlA2pkPetVTRiBs m187JedWGZAw1vYpJHP190ZGI2MmUWAnI4ojs zNxP 8borhlZ8J ZRNxRYfhakGJNZdDIQmjOUE0so08LeStiIasrQFlS0JXjLkVdJq1rxapXafbVcv87rKMApnMEFeHAJdbiFBjSBwSM8wyu8ObHz4rw7H4vRNSfOYE cD5 ANXj1A latexit Output x1 W bYBFclZki1WXRjcsK9gGdoWTSTBuayYQkI5ahv HGhSJu Rl3 o2ZdhbaeiBwOde7skJWfauO63s7a sbm1Xdop7 7tHxWjo47OkVoW2S8ET1QqwpZ4K2DTOc9qSiOA457YaT29zvPlKlWSIezFTSIMYj wSJGsLGS78fYjMoe5oN6oNK1a25c6BV4hWkCgVag8qXP0xIGlNhCMda9z1XmiDyjDC6azsp5pKTCZ4RPuWChxTHWTzDN0bpUhihJlnzBorv7eyHCs9TQO7WSeUS97ufif109NdB1kTMjUEWh6KUI5Og vA0ZIoSw6eWYKYzYrIGCtMjK2pbEvwlr 8Sjr1mteoNe4vq82bo4SnMIZXIAHV9CEO2hBGwhIeIZXeHNS58V5dz4Wo2tOsXMCf B8 gAvdpHM latexit x2 d uBovgqiQi1WXRjcsK9gFNKJPJpB06mYSZiVhCf8ONC0Xc jPu BsnaRbaemDgcM693DPHTzhT2ra rcra sbmVnW7trO7t39QPzqTiVhHZJzGM58LGinAna1UxzOkgkxZHPad f3uZ 5FKxWLxoGcJ9SI8 FixkBGsjuW6E9cQPs6f5KBjVG3bTLoBWiVOSBpTojOpfbhCTNKJCE46VGjp2or0MS80Ip OamyqaYDLFYzo0VOCIKi8rMs RmVECFMbSPKFRof7eyHCk1CzyzWSeUS17ufifN0x1eO1lTCSpoIsDoUpRzpG eQEoYJISzWeGYCKZyYrIBEtMtKmpZkpwlr 8SnoXTafVbN1fNto3ZR1VOIFTOAcHrqANd9CBLhBI4Ble4c1KrRfr3fpYjFascucY sD6 AF7PpH latexit xd Figure 2. The input data x is used to determine which subset of the array of sources are active (illustrated in red). This collection of active sources is assembled into an aggregate electromagnetic source b : s(x).\n\n--- Segment 11 ---\nThe input data x is used to determine which subset of the array of sources are active (illustrated in red). This collection of active sources is assembled into an aggregate electromagnetic source b : s(x). Below the aggregate source field we illustrate the geometry (hardware) A(θg), which does not depend on the data but is optimized via a gradient method. The source and geometry are simulated in a differentiable field solver, resulting in the field shown. At a collection of fixed locations (shown as white rectangular regions imposed on the field), the energy is summed and re-mapped via a data-dependent linear decoding D(x; θd). In a classification application, the result is normalized and interpreted as the readout probabilities of the system (see rightmost). is non-degenerate (the multiplicity of ωm is 1), with source frequency ωs close to its associated eigenfrequency ωm. Then the field is dominated by this particular mode: E(zR) jωs ω2m ω2s E m(zs)Em(zR). (9) Holding zR constant, and varying zs, the measured field follows the functional form of Em; which is nonlinear. In freespace, Em is sinusoidal (Figure 1, left column), but with general heterogeneous media, Em can have a nearly arbitrarily complicated functional form (Figure 1 second and third column from the left). By choosing the distribution of material in the domain through optimization, we can achieve targeted mode profiles, in turn realizing a desired input to output map zs 7 E(zR). In the general case, multiple modes whose frequencies are close to the source frequency will be excited, and the relation zs to E(zR) will be a linear combination of the modes. 2.2. Proposed Implementation While positional encoding is a generic method to exploit linear systems for nonlinear computation, here we consider a concrete application and describe a proposed hardware design for an optical classifier, illustrated in Figure 2. The proposed system is functionally comprised of four stages: encoding, optical dataflow, readout, and decoding. 2.2.1.\n\n--- Segment 12 ---\nThe proposed system is functionally comprised of four stages: encoding, optical dataflow, readout, and decoding. 2.2.1. SOURCE ENCODING Consider the source encoding system illustrated on the left in Figure 2, with a regular array of sources of size d k. While any sufficiently local source can be chosen, here we model a generic dipole emitter, i.e. a cylindrical source with common driving frequency. We logically partition the source array by row, so that the ith row corresponds with the ith entry of the input x Rd. A straightforward method for positional encoding is to uniformly quantize xi to k distinct values (i.e., log(k) bits of precision); and then power only the source in row i corresponding with the quantized value. That is, for any input x, precisely one source is active per row of the source array. 2.2.2. OPTICAL DATAFLOW Given the source s(x) derived from the positional encoding methodology above, executing the forward pass of the optical neural network is equivalent to simply driving the material design with the source, i.e. the computation is physically realized by the resultant wave propagation.\n\n--- Segment 13 ---\nOPTICAL DATAFLOW Given the source s(x) derived from the positional encoding methodology above, executing the forward pass of the optical neural network is equivalent to simply driving the material design with the source, i.e. the computation is physically realized by the resultant wave propagation. We denote the material distribution obtained from optimiza- tion as µ r and ϵ r. Then the wave operator associated with the optimized hardware design is given by: A(z, ω; µ r, ϵ r) : j( µ 1 r (z) ω2ϵ r(z)) ω , (10) 3 Nonlinear Computation with Linear Optics via Source-Position Encoding B Spline Polygonal Mesh Neural Implicit Field p1 p2 6 9LBbBU0lUqseiF48V7Qe0oWy2m3bpZhN2J0IJ QlePCji1V kzX jts1Bqw8GHu NMDMvSKQw6LpfTmFldW19o7hZ2tre2d0r7x 0TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfjm5nfuTaiFg94CThfkS HSoSCUbTSfdI 75crbtWdg wlXk4qkKPRL3 2BjFLI6QSWpM13MT9DOqUTDJp6VeanhC2ZgOedSRSNu Gx 6pScWGVAwljbUkjm6s JjEbGTKLAdkYUR2bZm4n ed0Uwys EypJkSu2WBSmkmBMZn TgdC coZxYQpkW9lbCRlRThjadkg3BW375L2mdVb1atXZ3Ualf53EU4QiO4RQ8uIQ63EIDmsBgCE wAq OdJ6dN d90Vpw8plD AXn4xsGo2l latexit p3 N xtL0KgoA " AB6nicbVBNS8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ Q lePCji1V kzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLVCahG wSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj hQ8pAzaqz0kPQv WKW3XnIKvEy0kFcjT65a eIG ZphNIwQbXuem5i Iwqw5nAamXakwoG9Mhdi2VNELtZ NTp TMKgMSxsqWNGSu p7IaKT1JApsZ0TN SC97M E r5ua8NrPuExSg5ItFoWpICYms7 JgCtkRkwsoUxeythI6oMzadkg3BW35lbQuql6tWr u rNRv8jiKcAKncA4eXEd7qABTWAwhGd4hTdHOC Ou OxaC04 cwx IHz QMHno2m latexit p4 p5 t 5HjtW vbc " AB6nicbVBNS8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ Q lePCji1V kzX jts1BWx8MPN6bYWZekEh0HW ncLa sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajh UijeRIGSdxLNaRI3g7GtzO cS1EbF6xEnC YgOlQgFo2ilh6Rf65crbtWdg6wSLycVyNHol796g5 ilEVfIJDWm67kJ hnVKJjk01IvNTyhbEyHvGupohE3fjY dUrOrDIgYaxtKSRz9fdERiNjJlFgOyOK I7PszcT vG6K4bWfCZWkyBVbLApTSTAms7 JQGjOUE4soUwLeythI6opQ5tOyYbgLb 8SloXVa9Wrd 1fVuo3eRxFOIFTOAcPrqAOd9CAJjAYwjO8wpsjnRfn3flYtBacfOY sD5 AEKpo2o latexit p6 p7 p8 h QNVP4ZOd5jbO2PstxWOtxc " ACK3icbVDLSgMxFM34rPU16tJN sAiuyoxIdSOUunGpYKvQKUMmvW2DmcyQ3BHLMP jxl9xoQsfuPU TB IrwOBwzn3JjknSqUw6Hmvzszs3PzCYmpvLyurbubmy2TJpDk2 eyERfRcyAFAqaKFDCVaqBxZGEy j6ZORf3oA2IlEXOEyhE7O Ej3B GVopdBsBDgBZmAcxwFnMj8tCnpMgwj6QuWpVbW4LWga jToJmgsU zQA1f3yQrfiVb0x6F iT0mFTHEWuo 2Jp7FoJBLZkzb91Ls5Eyj4B KcpAZSBm Zn1oW6pYDKaTj7MWdNcqXdpLtD0K6Vj9vpGz2JhHNnJ USLz2xuJ 3ntDHtHnVyoNENQfPJQL5MUEzoqjnaFBo5yaAnjWti U j5gmnG09ZtCf7vyH9Ja7 q16q184NKvTGto0S2yQ7ZIz45JHVySs 5Ik3ByRx7IM3lx7p0n5815n4zONOdLfIDzscn 1eoxw latex it ωH !p1 .\n\n--- Segment 14 ---\nthe computation is physically realized by the resultant wave propagation. We denote the material distribution obtained from optimiza- tion as µ r and ϵ r. Then the wave operator associated with the optimized hardware design is given by: A(z, ω; µ r, ϵ r) : j( µ 1 r (z) ω2ϵ r(z)) ω , (10) 3 Nonlinear Computation with Linear Optics via Source-Position Encoding B Spline Polygonal Mesh Neural Implicit Field p1 p2 6 9LBbBU0lUqseiF48V7Qe0oWy2m3bpZhN2J0IJ QlePCji1V kzX jts1Bqw8GHu NMDMvSKQw6LpfTmFldW19o7hZ2tre2d0r7x 0TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfjm5nfuTaiFg94CThfkS HSoSCUbTSfdI 75crbtWdg wlXk4qkKPRL3 2BjFLI6QSWpM13MT9DOqUTDJp6VeanhC2ZgOedSRSNu Gx 6pScWGVAwljbUkjm6s JjEbGTKLAdkYUR2bZm4n ed0Uwys EypJkSu2WBSmkmBMZn TgdC coZxYQpkW9lbCRlRThjadkg3BW375L2mdVb1atXZ3Ualf53EU4QiO4RQ8uIQ63EIDmsBgCE wAq OdJ6dN d90Vpw8plD AXn4xsGo2l latexit p3 N xtL0KgoA " AB6nicbVBNS8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ Q lePCji1V kzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLVCahG wSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj hQ8pAzaqz0kPQv WKW3XnIKvEy0kFcjT65a eIG ZphNIwQbXuem5i Iwqw5nAamXakwoG9Mhdi2VNELtZ NTp TMKgMSxsqWNGSu p7IaKT1JApsZ0TN SC97M E r5ua8NrPuExSg5ItFoWpICYms7 JgCtkRkwsoUxeythI6oMzadkg3BW35lbQuql6tWr u rNRv8jiKcAKncA4eXEd7qABTWAwhGd4hTdHOC Ou OxaC04 cwx IHz QMHno2m latexit p4 p5 t 5HjtW vbc " AB6nicbVBNS8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ Q lePCji1V kzX jts1BWx8MPN6bYWZekEh0HW ncLa sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajh UijeRIGSdxLNaRI3g7GtzO cS1EbF6xEnC YgOlQgFo2ilh6Rf65crbtWdg6wSLycVyNHol796g5 ilEVfIJDWm67kJ hnVKJjk01IvNTyhbEyHvGupohE3fjY dUrOrDIgYaxtKSRz9fdERiNjJlFgOyOK I7PszcT vG6K4bWfCZWkyBVbLApTSTAms7 JQGjOUE4soUwLeythI6opQ5tOyYbgLb 8SloXVa9Wrd 1fVuo3eRxFOIFTOAcPrqAOd9CAJjAYwjO8wpsjnRfn3flYtBacfOY sD5 AEKpo2o latexit p6 p7 p8 h QNVP4ZOd5jbO2PstxWOtxc " ACK3icbVDLSgMxFM34rPU16tJN sAiuyoxIdSOUunGpYKvQKUMmvW2DmcyQ3BHLMP jxl9xoQsfuPU TB IrwOBwzn3JjknSqUw6Hmvzszs3PzCYmpvLyurbubmy2TJpDk2 eyERfRcyAFAqaKFDCVaqBxZGEy j6ZORf3oA2IlEXOEyhE7O Ej3B GVopdBsBDgBZmAcxwFnMj8tCnpMgwj6QuWpVbW4LWga jToJmgsU zQA1f3yQrfiVb0x6F iT0mFTHEWuo 2Jp7FoJBLZkzb91Ls5Eyj4B KcpAZSBm Zn1oW6pYDKaTj7MWdNcqXdpLtD0K6Vj9vpGz2JhHNnJ USLz2xuJ 3ntDHtHnVyoNENQfPJQL5MUEzoqjnaFBo5yaAnjWti U j5gmnG09ZtCf7vyH9Ja7 q16q184NKvTGto0S2yQ7ZIz45JHVySs 5Ik3ByRx7IM3lx7p0n5815n4zONOdLfIDzscn 1eoxw latex it ωH !p1 . .\n\n--- Segment 15 ---\nWe denote the material distribution obtained from optimiza- tion as µ r and ϵ r. Then the wave operator associated with the optimized hardware design is given by: A(z, ω; µ r, ϵ r) : j( µ 1 r (z) ω2ϵ r(z)) ω , (10) 3 Nonlinear Computation with Linear Optics via Source-Position Encoding B Spline Polygonal Mesh Neural Implicit Field p1 p2 6 9LBbBU0lUqseiF48V7Qe0oWy2m3bpZhN2J0IJ QlePCji1V kzX jts1Bqw8GHu NMDMvSKQw6LpfTmFldW19o7hZ2tre2d0r7x 0TJxqxpslrHuBNRwKRvokDJO4nmNAokbwfjm5nfuTaiFg94CThfkS HSoSCUbTSfdI 75crbtWdg wlXk4qkKPRL3 2BjFLI6QSWpM13MT9DOqUTDJp6VeanhC2ZgOedSRSNu Gx 6pScWGVAwljbUkjm6s JjEbGTKLAdkYUR2bZm4n ed0Uwys EypJkSu2WBSmkmBMZn TgdC coZxYQpkW9lbCRlRThjadkg3BW375L2mdVb1atXZ3Ualf53EU4QiO4RQ8uIQ63EIDmsBgCE wAq OdJ6dN d90Vpw8plD AXn4xsGo2l latexit p3 N xtL0KgoA " AB6nicbVBNS8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Qe0oWy2k3bpZhN2N0IJ Q lePCji1V kzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLVCahG wSU2DTcCO4lCGgUC28H4dua3n1BpHstHM0nQj hQ8pAzaqz0kPQv WKW3XnIKvEy0kFcjT65a eIG ZphNIwQbXuem5i Iwqw5nAamXakwoG9Mhdi2VNELtZ NTp TMKgMSxsqWNGSu p7IaKT1JApsZ0TN SC97M E r5ua8NrPuExSg5ItFoWpICYms7 JgCtkRkwsoUxeythI6oMzadkg3BW35lbQuql6tWr u rNRv8jiKcAKncA4eXEd7qABTWAwhGd4hTdHOC Ou OxaC04 cwx IHz QMHno2m latexit p4 p5 t 5HjtW vbc " AB6nicbVBNS8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Qe0oWy2m3bpZhN2J0IJ Q lePCji1V kzX jts1BWx8MPN6bYWZekEh0HW ncLa sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajh UijeRIGSdxLNaRI3g7GtzO cS1EbF6xEnC YgOlQgFo2ilh6Rf65crbtWdg6wSLycVyNHol796g5 ilEVfIJDWm67kJ hnVKJjk01IvNTyhbEyHvGupohE3fjY dUrOrDIgYaxtKSRz9fdERiNjJlFgOyOK I7PszcT vG6K4bWfCZWkyBVbLApTSTAms7 JQGjOUE4soUwLeythI6opQ5tOyYbgLb 8SloXVa9Wrd 1fVuo3eRxFOIFTOAcPrqAOd9CAJjAYwjO8wpsjnRfn3flYtBacfOY sD5 AEKpo2o latexit p6 p7 p8 h QNVP4ZOd5jbO2PstxWOtxc " ACK3icbVDLSgMxFM34rPU16tJN sAiuyoxIdSOUunGpYKvQKUMmvW2DmcyQ3BHLMP jxl9xoQsfuPU TB IrwOBwzn3JjknSqUw6Hmvzszs3PzCYmpvLyurbubmy2TJpDk2 eyERfRcyAFAqaKFDCVaqBxZGEy j6ZORf3oA2IlEXOEyhE7O Ej3B GVopdBsBDgBZmAcxwFnMj8tCnpMgwj6QuWpVbW4LWga jToJmgsU zQA1f3yQrfiVb0x6F iT0mFTHEWuo 2Jp7FoJBLZkzb91Ls5Eyj4B KcpAZSBm Zn1oW6pYDKaTj7MWdNcqXdpLtD0K6Vj9vpGz2JhHNnJ USLz2xuJ 3ntDHtHnVyoNENQfPJQL5MUEzoqjnaFBo5yaAnjWti U j5gmnG09ZtCf7vyH9Ja7 q16q184NKvTGto0S2yQ7ZIz45JHVySs 5Ik3ByRx7IM3lx7p0n5815n4zONOdLfIDzscn 1eoxw latex it ωH !p1 . . .\n\n--- Segment 16 ---\n. . εm " } v1 v2 v3 L TgJBEOzF IL9ehlIjHxRHYNQY9ELx4xyiOBDZkdGpgwO7uZmSUhGz7BiweN8eoXefNvHGAPClbSaWqO91dQSy4Nq7eQ2Nre2d K7hb39g8Oj4vFJU0eJYthgkYhU O6AaBZfYMNwIbMcKaRgIbAXju7nfmqDSPJPZhqjH9Kh5APOqLHS46RX6RVLbtldgKwTLyMlyFDvFb 6 YglIUrDBNW647mx8VOqDGcCZ4VuojGmbEyH2LFU0hC1ny5 OnZELq TJIFK2pCEL9fdESkOtp2FgO0NqRnrVm4v eZ3EDG78lMs4MSjZctEgEcREZP436XOFzIipJZQpbm8lbEQVZcamU7AheKsvr5PmVdmrlqsPlVLtNosjD2dwDpf gwTXU4B7q0AGQ3iGV3hzhPivDsfy9ack82cwh84nz8Qwo2s latexit v4 N S8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Ae2oWy2k3bpZhN2N0IJ RdePCji1X jzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLV CahGwSU2DTcCO4lCGgUC28H4dua3n1BpHsHM0nQj hQ8pAzaqz02MNEcxHLvtcvV9yqOwdZJV5OKpCj0S9 9QYxSyOUhgmqdzE NnVBnOBE5LvVRjQtmYDrFrqaQ Raj bXzwlZ1YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIZVsCN7y6ukdVH1atXa WlfpPHUYQ TOIVz8OAK6nAHDWgCAwnP8ApvjnZenHfnY9FacPKZY gD5 MHe6OQzw latexit ω1 N S8NAEJ34WetX1aOXxSJ4KkmR6rHoxWMF 4FtKJvtpF262YTdjVBC 4UXD4p49d9489 4bXPQ1gcDj dmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJsFrHq BFSj4BKbhuBnUQhjQKB7WB8O PbT6g0j WDmSToR3QoecgZNVZ67GiuYhlv9ovld2KOwdZJV5OypCj0S9QYxSyOUhgmqdzE NnVBnOBE6LvVRjQtmYDrFrqaQ Raj bXzwl51YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIRVtCN7y6ukVa14tUrt rJcv8njKMA pnMEFeHAFdbiDBjSBgYRneIU3RzsvzrvzsWhdc KZE gD5 MHfSeQ0A latexit ω2 ω3 ω4 ω5 ω6 ω7 ω8 ω9 ωH : ωNN Figure 3.\n\n--- Segment 17 ---\n. εm " } v1 v2 v3 L TgJBEOzF IL9ehlIjHxRHYNQY9ELx4xyiOBDZkdGpgwO7uZmSUhGz7BiweN8eoXefNvHGAPClbSaWqO91dQSy4Nq7eQ2Nre2d K7hb39g8Oj4vFJU0eJYthgkYhU O6AaBZfYMNwIbMcKaRgIbAXju7nfmqDSPJPZhqjH9Kh5APOqLHS46RX6RVLbtldgKwTLyMlyFDvFb 6 YglIUrDBNW647mx8VOqDGcCZ4VuojGmbEyH2LFU0hC1ny5 OnZELq TJIFK2pCEL9fdESkOtp2FgO0NqRnrVm4v eZ3EDG78lMs4MSjZctEgEcREZP436XOFzIipJZQpbm8lbEQVZcamU7AheKsvr5PmVdmrlqsPlVLtNosjD2dwDpf gwTXU4B7q0AGQ3iGV3hzhPivDsfy9ack82cwh84nz8Qwo2s latexit v4 N S8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Ae2oWy2k3bpZhN2N0IJ RdePCji1X jzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLV CahGwSU2DTcCO4lCGgUC28H4dua3n1BpHsHM0nQj hQ8pAzaqz02MNEcxHLvtcvV9yqOwdZJV5OKpCj0S9 9QYxSyOUhgmqdzE NnVBnOBE5LvVRjQtmYDrFrqaQ Raj bXzwlZ1YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIZVsCN7y6ukdVH1atXa WlfpPHUYQ TOIVz8OAK6nAHDWgCAwnP8ApvjnZenHfnY9FacPKZY gD5 MHe6OQzw latexit ω1 N S8NAEJ34WetX1aOXxSJ4KkmR6rHoxWMF 4FtKJvtpF262YTdjVBC 4UXD4p49d9489 4bXPQ1gcDj dmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJsFrHq BFSj4BKbhuBnUQhjQKB7WB8O PbT6g0j WDmSToR3QoecgZNVZ67GiuYhlv9ovld2KOwdZJV5OypCj0S9QYxSyOUhgmqdzE NnVBnOBE6LvVRjQtmYDrFrqaQ Raj bXzwl51YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIRVtCN7y6ukVa14tUrt rJcv8njKMA pnMEFeHAFdbiDBjSBgYRneIU3RzsvzrvzsWhdc KZE gD5 MHfSeQ0A latexit ω2 ω3 ω4 ω5 ω6 ω7 ω8 ω9 ωH : ωNN Figure 3. Hardware parameterization instances.\n\n--- Segment 18 ---\nεm " } v1 v2 v3 L TgJBEOzF IL9ehlIjHxRHYNQY9ELx4xyiOBDZkdGpgwO7uZmSUhGz7BiweN8eoXefNvHGAPClbSaWqO91dQSy4Nq7eQ2Nre2d K7hb39g8Oj4vFJU0eJYthgkYhU O6AaBZfYMNwIbMcKaRgIbAXju7nfmqDSPJPZhqjH9Kh5APOqLHS46RX6RVLbtldgKwTLyMlyFDvFb 6 YglIUrDBNW647mx8VOqDGcCZ4VuojGmbEyH2LFU0hC1ny5 OnZELq TJIFK2pCEL9fdESkOtp2FgO0NqRnrVm4v eZ3EDG78lMs4MSjZctEgEcREZP436XOFzIipJZQpbm8lbEQVZcamU7AheKsvr5PmVdmrlqsPlVLtNosjD2dwDpf gwTXU4B7q0AGQ3iGV3hzhPivDsfy9ack82cwh84nz8Qwo2s latexit v4 N S8NAEJ3Ur1q qh69LBbBU0lEqseiF48V7Ae2oWy2k3bpZhN2N0IJ RdePCji1X jzX jts1BWx8MPN6bYWZekAiujet O4W19Y3NreJ2aWd3b gfHjU0nGqGDZLGLV CahGwSU2DTcCO4lCGgUC28H4dua3n1BpHsHM0nQj hQ8pAzaqz02MNEcxHLvtcvV9yqOwdZJV5OKpCj0S9 9QYxSyOUhgmqdzE NnVBnOBE5LvVRjQtmYDrFrqaQ Raj bXzwlZ1YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIZVsCN7y6ukdVH1atXa WlfpPHUYQ TOIVz8OAK6nAHDWgCAwnP8ApvjnZenHfnY9FacPKZY gD5 MHe6OQzw latexit ω1 N S8NAEJ34WetX1aOXxSJ4KkmR6rHoxWMF 4FtKJvtpF262YTdjVBC 4UXD4p49d9489 4bXPQ1gcDj dmJkXJIJr47rfztr6xubWdmGnuLu3f3BYOjpu6ThVDJsFrHq BFSj4BKbhuBnUQhjQKB7WB8O PbT6g0j WDmSToR3QoecgZNVZ67GiuYhlv9ovld2KOwdZJV5OypCj0S9QYxSyOUhgmqdzE NnVBnOBE6LvVRjQtmYDrFrqaQ Raj bXzwl51YZkDBWtqQhc X3REYjrSdRYDsjakZ62ZuJ 3nd1ITXfsZlkhqUbLEoTAUxMZm9TwZcITNiYglitbCRtRZmxIRVtCN7y6ukVa14tUrt rJcv8njKMA pnMEFeHAFdbiDBjSBgYRneIU3RzsvzrvzsWhdc KZE gD5 MHfSeQ0A latexit ω2 ω3 ω4 ω5 ω6 ω7 ω8 ω9 ωH : ωNN Figure 3. Hardware parameterization instances. On the left, we show B spline patches whose interior determine freespace pores amidst a homogeneous dielectric (e.g., SiO2).\n\n--- Segment 19 ---\nHardware parameterization instances. On the left, we show B spline patches whose interior determine freespace pores amidst a homogeneous dielectric (e.g., SiO2). The parameters θH modulate the shape of the pores through the spline control points p1, . . . , pn. In the center, a triangular mesh is parameterized by a collection of n vertices v1, . . . , vn, and permittivities ϵ1, . . . , ϵm within the interior of each triangle in the mesh. In principle, the intermediate permittivities could be quantized into several discrete values to minimize the number of materials required. On the right, a neural implicit field parameterizes the distribution of material via its zero sublevel set. Details of this implicit parameterization are in (cref appendix). and the field is given by the solution to (1), i.e. E A 1s(x). 2.2.3. READOUT We measure the intensity of the electric field in small volumes centered at locations {zk}K k 1, for instance with avalanche photodiodes. This gives rise to K non-negative scalar read outs. In our device model, we denote this opera- tion with R(E) R(A 1s(x)) RK 0. 2.2.4. DECODING Before normalizing R(E) to a probability vector, we execute a decoder, D, with parameters θd that are fit during training and fixed at inference time. The decoder is a strictly linear map which accounts for the fact that the association between readout location zk and class label is chosen arbitrarily at initialization. Concretely, this provides a form of invariance; if we were to permute the class labels arbitrarily, applying that same permutation to the rows of D(x; θd) would keep our predictions fixed. While the linear restriction could be relaxed, here it is cru- cial to certify that the nonlinearity is a result of the source position encoding, and not the decoder. In summary, the aggregate system can be modeled by the following map: f(x; ω, µ, ϵ, θ) : D(x; θd)R(A(ω; µ, ϵ) 1s(x)) (11) 2.3.\n\n--- Segment 20 ---\nWhile the linear restriction could be relaxed, here it is cru- cial to certify that the nonlinearity is a result of the source position encoding, and not the decoder. In summary, the aggregate system can be modeled by the following map: f(x; ω, µ, ϵ, θ) : D(x; θd)R(A(ω; µ, ϵ) 1s(x)) (11) 2.3. In Silico Inverse Design The system model above defines a family of classifiers pa- rameterized by (θd, µ, ϵ). Training a classifier amounts to making a particular choice of these parameters, which we obtain by constructing a differentiable computational model for Equation (11) and using gradient based optimization. We discretize the wave operator A A Cn2 n2 and the source s(x) b(x) Cn2 on a two-dimensional rectilinear grid of size n n using the finite difference frequency domain (FDFD) method (see Appendix A.2 for details). Then, we use topology optimization in tandem with a custom differentiable simulator to formulate a gradient- based co-design methodology for the material distribution and the decoder parameters. 2.3.1. TOPOLOGY OPTIMIZATION We formulate the problem of material distribution via level set topology optimization (Van Dijk et al., 2013). The idea is to frame the choice of a particular spatial distribution of material as an optimization problem, whose objective, in this context, is a given function of the electromagnetic fields which depends on the material distribution through the wave 4 Nonlinear Computation with Linear Optics via Source-Position Encoding Dataset Quantization (bits) Linear Model Optical Network ANN (MLP) MNIST2 8 46.8 91.7 82.6 MNIST 24 73.0 94.4 93.1 MNIST 64 83.3 95.2 95.6 Table 1. Experimental results for benchmark datasets. The linear model is fit numerically using the same quantized inputs as used in the optical neural network. The optical networks optimize the topology in a design space of 500 500, the results cited here are from the neural field parameterization. Additional training and experiment details can be found in Appendix A.4 operator.\n\n--- Segment 21 ---\nThe optical networks optimize the topology in a design space of 500 500, the results cited here are from the neural field parameterization. Additional training and experiment details can be found in Appendix A.4 operator. Given a rectangular domain Ω R2 and a function h : R2 Rℓ7 R parameterized by θg Rℓ, we define an implicit topology as the zero sublevel set of the parametric function h, that is: T : {x x Ω, h(x; θg) 0}. (12) The set T can be interpreted as the subset of the design area in which a target material of non-unit refractive index (e.g., SiO2) is to be placed, where the remainder of the area Ω\ T contains some given background substrate material (e.g., freespace, or a dielectric). See the lower right-hand side of Figure 3 for an illustration of h (shown as the surface on the far right), and T (shaded in blue). We discretize the domain, associating the material permit- tivity permeability values to each grid cell. To determine these values, we use the standard approach in level-set topol- ogy optimization: for each grid cell we compute the area of intersection with T. Then the material properties asso- ciated with each grid cell are proportional to the area of intersection. While this leads to some intermediate permit- tivities permeabilities, these are restricted to the boundary of the topology T. In the general approach described here, the objective is highly nonconvex and nondifferentiable with respect to the material distribution, so a variety of relaxation approaches are employed. The usual approach to computing the area of intersection is to smooth the boundary of the topology using a heuristic (e.g., a relaxed Heaviside). Instead, we compute the area of intersection using Fiber Monte Carlo (FMC) (Richardson et al., 2024). In FMC, a collection of line segements or fibers are sam- pled from Ω, and the fibers are used analogously to points in simple Monte Carlo to derive an unbiased estimator of the area of intersection.\n\n--- Segment 22 ---\nInstead, we compute the area of intersection using Fiber Monte Carlo (FMC) (Richardson et al., 2024). In FMC, a collection of line segements or fibers are sam- pled from Ω, and the fibers are used analogously to points in simple Monte Carlo to derive an unbiased estimator of the area of intersection. This estimator is differentiable even with an exact Heaviside; obviating the need for heuris- tic smoothing approaches, and resulting in far more stable optimization in our experiments. Using FMC also enables the use of highly flexible and struc- tured parameterizations of the topology. For instance, the center design of Figure 3 illustrates that we can directly optimize a triangular mesh topology with respect to both the vertices and the permittivities associated with each tri- angle constituting the mesh directly. In this context, the parameters θg correspond directly with the vertex positions and triangle permittivities, not the parameters of an implicit topology. The outer designs illustrate implicit topologies derived from B-splines (leftmost), and a neural network im- plicit field (rightmost), in which θg correspond with spline control points, and neural network parameters, respectively. Advanced physical layout and fabrication often requires the designer to arrange a collection of strict geometric primitives (e.g., rectangles or other simple polygons), which can be natively supported in our inverse design scheme. 2.3.2. DIFFERENTIABLE SIMULATION As we describe in detail in Appendix A.3.3, there exist a variety of special purpose field solvers exploiting the ad- joint method dating back decades (Georgieva et al., 2002). There also exist a handful of more general purpose full- wave solvers (Hughes et al., 2018b; 2019) using early research-grade automatic differentiation tools (Maclaurin et al., 2015), and several excellent modern special-purpose tools for specific electromagnetics applications (Laporte et al., 2019).\n\n--- Segment 23 ---\nDIFFERENTIABLE SIMULATION As we describe in detail in Appendix A.3.3, there exist a variety of special purpose field solvers exploiting the ad- joint method dating back decades (Georgieva et al., 2002). There also exist a handful of more general purpose full- wave solvers (Hughes et al., 2018b; 2019) using early research-grade automatic differentiation tools (Maclaurin et al., 2015), and several excellent modern special-purpose tools for specific electromagnetics applications (Laporte et al., 2019). We developed a generic differentiable field solver in Jax (Bradbury et al., 2018), with support for hardware accel- eration on GPU, custom sparse numerical linear algebra primitives with correct sparse vector-Jacobian product im- plementations, batched simulations (varying the data but holding the hardware wave operator fixed), and JIT com- piled simulation to amortize overhead involved in multiple successive simulations. 2.3.3. INVERSE DESIGN OF OPTICAL CLASSIFIER The aggregate forward computational model with parame- ters θ : (θd, θg) of Equation (11) is denoted with: ˆf(x; θ) : D(x; θd)R(A(θg) 1b(x)). (13) 5 Nonlinear Computation with Linear Optics via Source-Position Encoding In multiclass classification, we are given a collection of data D : {(xi, ci)}N i 1 comprised of inputs xi with labels ci. Then we aim to locally solve the following optimization problem, minimizeθ N X i 1 L(ci, Softmax( ˆf(xi; θ))), (14) with an appropriate objective (e.g., cross-entropy) L. We use gradient based optimization (specifically, Adam (Kingma Ba, 2014)), exploiting FMC-based topology op- timization and differentiable simulation to obtain the deriva- tives with respect to the material distribution parameters θg, and conventional automatic differentiation for the deriva- tives of the decoder parameters θd with respect to the objec- tive. 3.\n\n--- Segment 24 ---\nThen we aim to locally solve the following optimization problem, minimizeθ N X i 1 L(ci, Softmax( ˆf(xi; θ))), (14) with an appropriate objective (e.g., cross-entropy) L. We use gradient based optimization (specifically, Adam (Kingma Ba, 2014)), exploiting FMC-based topology op- timization and differentiable simulation to obtain the deriva- tives with respect to the material distribution parameters θg, and conventional automatic differentiation for the deriva- tives of the decoder parameters θd with respect to the objec- tive. 3. Experiments Our experimental setup is the inverse design of a two dimen- sional optical classifier using a regular array of cylindrical (i.e., dipole) sources for our positional encoding (see Fig- ure 2). Like prior work (Khoram et al., 2019; Wanjura Marquardt, 2024), we consider multi-class classification over the MNIST dataset, which is comprised of 28 28 grayscale images at 8-bit precision, each of which belongs to one of 10 distinct classes. 3.1. Encoding We first linearly project each input x R784 to a p-vector x Rp, with values of p 4, 8, 16 evaluated in our ex- periments. The linear projection is derived from principal components analysis (PCA). Then, for each of p dimensions comprising x, we uniformly quantize the entry into q bins, reducing the precision of the entry to log2 q bits. In our experiments, we use q 4, 8, 16. These configurations of (p, q) correspond with each image encoded with 8, 24, and 64 bits. Physically, this encoding corresponds with a p q array of identical cylindrical sources laid out within the design area. For any given x, the pth row of the source array has precisely one source activated, corresponding with the bin that index is quantized to. 3.2. Ablation For each quantization configuration we also fit a purely numerical linear model ˆf( x; W) W x (to the same quan- tized data), and a standard deep artificial neural network (an MLP), to contextualize the expected performance without using positional encoding; the results are shown in Table 1.\n\n--- Segment 25 ---\n3.2. Ablation For each quantization configuration we also fit a purely numerical linear model ˆf( x; W) W x (to the same quan- tized data), and a standard deep artificial neural network (an MLP), to contextualize the expected performance without using positional encoding; the results are shown in Table 1. Our optical neural networks significantly improve classifi- cation performance compared to a linear model (95.2 for the optical network compared to 83.3 for the linear model, 1 gdDX120 oNI lgxkn6Ed0IHnIGTVWur 9aTyWym7FnYEsEy8nZch81 9fszSCKVhgmrd9dzE BlVhjOBk2Iv1ZhQNqID7FoqaYTaz2arTsipVfokjJV90pCZ nsio5HW4yiwyYiaoV70puJ Xjc14YWfcZmkBiWbfxSmgpiYTO8mfa6QGTG2hDLF7a6EDamizNh2irYEb HkZdKqVrxapXZXLdcv8zoKcAwncAYenEMdrqEBTWAwgGd4hTdHOC Ou Mxj64 cwR IHz QPfLo2K latexit MLP Linear Validation Accuracy vs. Iteration Validation Accuracy [ ] Iteration Figure 4. Validation accuracy (as a percentage) against training iteration for the 64 bits image quantization configuration, shown enveloped by 2 standard deviations. The MLP and linear model best validation accuracies are illustrated as dashed horizontal lines in orange and green respectively. Topology vs. Iteration Mesh Bspline Neural Field Iteration Figure 5. Evolution of design topology over the course of opti- mization. The left-most column shows the initialization of the topology (before optimization), the right-most column shows the final topology (after optimization) and the center column illustrates the topology at an intermediate iteration. in the highest-precision regime), and achieve performance commensurate to the deep artificial neural network.\n\n--- Segment 26 ---\nThe left-most column shows the initialization of the topology (before optimization), the right-most column shows the final topology (after optimization) and the center column illustrates the topology at an intermediate iteration. in the highest-precision regime), and achieve performance commensurate to the deep artificial neural network. The artificial neural network is out-performed by the optical network at low precision (see Figure 4), but it is important to note that we use a basic MLP; fitting an optimized, state of the art convolutional neural network (CNN) would almost certainly outperform our optical networks. We also show examples of how the topology evolves over the course of 6 Nonlinear Computation with Linear Optics via Source-Position Encoding training optimization in Figure 5. Additional details on ex- periments and optimization can be found in Appendix A.4. 4. Limitations and Future Work 4.1. Limitations A limitation of positional encoding is the fact that the nonlin- earities hold only elementwise with respect to the data. With the simpler tasks often used for evaluation in this space, this does not present a major impediment, but more complicated applications often require nonlinear mixtures of variables. Future work should investigate whether this limitation can be overcome by introducing a collection of layers of our compute elements, interconnected with nonlinear optical routers. A practical bottleneck of optimization in our bottom-up physics based approach is differentiable simulation. In the frequency domain, simulation scales (roughly) as O(n3) with an input domain of size n n, so scaling to larger designs likely requires reducing calls to the full field solver, possibly using queries to surrogate models or model reduction-based systems (Cozad et al., 2014). 4.2. Future Work Equation (8) makes explicit a nonlinear relationship be- tween both the source position and the measured fields, and between the source frequency and the measured fields. A follow-on work will explore this frequency-based nonlin- earity and describe its implications and relationship to the position-based nonlinearity introduced in this work. Future physical layer inverse design tools could attempt to directly utilize an FMC-based topology optimization ap- proach to design highly structured geometric layouts like those of a modern digital (or analog) CMOS electronic design, with appropriate modifications to the simulation framework. 4.3.\n\n--- Segment 27 ---\nFuture physical layer inverse design tools could attempt to directly utilize an FMC-based topology optimization ap- proach to design highly structured geometric layouts like those of a modern digital (or analog) CMOS electronic design, with appropriate modifications to the simulation framework. 4.3. Conclusion We contribute a novel method for low-power nonlinear com- putation in fully linear media. The proposed source posi- tional encoding is straightforward, requiring only the ability to drive the system at distinct locations. We also develop a fully automated inverse design system for extremely specialized optical computing elements. Fur- ther development of automated hardware design is critical to practically reaping the benefits of hardware specializa- tion moving forward. Both positional encoding and auto- mated hardware design generalizes well beyond optics, and may prove to be significant within a variety of computing paradigms. 4.4. Acknowledgements The authors would like to thank Jake Schaefer and Eric Blow for helpful feedback on the manuscript. N.R. was partially supported by the National Science Foun- dation (NSF): OAC-2118201. C.B. was supported by the Swiss National Science Foun- dation (SNSF) through a Postdoc.Mobility fellowship (P500PT 217673 1). References Balanis, C. A. Advanced Engineering Electromagnetics. John Wiley Sons, 2012. Bayer, E. and Schaack, G. Two-photon Absorption of CaF2: Eu2 . Physica Status Solidi (b), 41(2):827 835, 1970. Boyd, R. W., Gaeta, A. L., and Giese, E. Nonlinear Optics. In Springer Handbook of Atomic, Molecular, and Optical Physics, pp. 1097 1110. Springer, 2008. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: Compos- able Transformations of Python NumPy programs, 2018.\n\n--- Segment 28 ---\nSpringer, 2008. Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: Compos- able Transformations of Python NumPy programs, 2018. URL Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language Models are Few-shot Learners. Advances in neural information processing systems, 33: 1877 1901, 2020. Cauchy, A. Sur l equation a l aide de Laquelle on D etermine les Inegalites s Eculaires des Mouvements des Planetes. vol. 9. O-euvres Completes (II eme S erie), 1829. Christiansen, R. E. and Sigmund, O. Inverse Design in Photonics by Topology Optimization: a Tutorial. JOSA B, 38(2):496 509, 2021. Cozad, A., Sahinidis, N. V., and Miller, D. C. Learning Sur- rogate Models for Simulation-based Optimization. AIChE Journal, 60(6):2211 2227, 2014. Feldmann, J., Youngblood, N., Karpov, M., Gehring, H., Li, X., Stappers, M., Le Gallo, M., Fu, X., Lukashchuk, A., Raja, A. S., et al. Parallel Convolutional Processing using an Integrated Photonic Tensor Core. Nature, 589(7840): 52 58, 2021. George, J. K., Mehrabian, A., Amin, R., Meng, J., De Lima, T. F., Tait, A. N., Shastri, B. J., El-Ghazawi, T., Prucnal, P. R., and Sorger, V. J. Neuromorphic Photonics with Electro-absorption Modulators. Optics express, 27(4): 5181 5191, 2019.\n\n--- Segment 29 ---\nGeorge, J. K., Mehrabian, A., Amin, R., Meng, J., De Lima, T. F., Tait, A. N., Shastri, B. J., El-Ghazawi, T., Prucnal, P. R., and Sorger, V. J. Neuromorphic Photonics with Electro-absorption Modulators. Optics express, 27(4): 5181 5191, 2019. 7 Nonlinear Computation with Linear Optics via Source-Position Encoding Georgieva, N. K., Glavic, S., Bakr, M. H., and Bandler, J. W. Feasible Adjoint Sensitivity Technique for EM Design Optimization. IEEE transactions on microwave theory and techniques, 50(12):2751 2758, 2002. Gilbert, F. Excitation of the Normal Modes of the Earth by Earthquake Sources. Geophysical Journal International, 22(2):223 226, 1971. G oppert, M. Uber Elementarakte mit zwei Quanten- spr ungen. PhD thesis, Doctoral thesis, Universit at zu G ottingen, 1930. Hermite, C. Remarque sur un Th eor eme de M. Cauchy. Comptes rendus hebdomadaires des s eances de l Acad emie des sciences, 41:181 183, 1855. Hughes, T. W., Minkov, M., Shi, Y., and Fan, S. Training of Photonic Neural Networks through in Situ Backpropaga- tion and Gradient Measurement. Optica, 5(7):864 871, 2018a. Hughes, T. W., Minkov, M., Williamson, I. A. D., and Fan, S. Adjoint Method and Inverse Design for Non- linear Nanophotonic Devices. ACS Photonics, 5(12): 4781 4787, 2018b. doi: 10.1021 acsphotonics.8b01522. Hughes, T. W., Williamson, I. A., Minkov, M., and Fan, S. Forward-Mode Differentiation of Maxwell s Equations. ACS Photonics, 6(11):3010 3016, 2019.\n\n--- Segment 30 ---\nA., Minkov, M., and Fan, S. Forward-Mode Differentiation of Maxwell s Equations. ACS Photonics, 6(11):3010 3016, 2019. Ivanov, A., Dryden, N., Ben-Nun, T., Li, S., and Hoefler, T. Data Movement is All You Need: A Case Study on Optimizing Transformers. Proceedings of Machine Learning and Systems, 3:711 732, 2021. Jouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Bhatia, S., Boden, N., Borchers, A., et al. In-datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of the 44th annual inter- national symposium on computer architecture, pp. 1 12, 2017. Karahan, E. A., Liu, Z., and Sengupta, K. Deep-learning- based Inverse-designed Millimeter-wave Passives and Power Amplifiers. IEEE Journal of Solid-State Circuits, 58(11):3074 3088, 2023. Keyes, R. W. Optical Logic-in the Light of Computer Tech- nology. Optica Acta: International Journal of Optics, 32 (5):525 535, 1985. Khoram, E., Chen, A., Liu, D., Ying, L., Wang, Q., Yuan, M., and Yu, Z. Nanophotonic Media for Artificial Neural Inference. Photonics Research, 7(8):823 827, 2019. Kingma, D. P. and Ba, J. Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980, 2014. Laporte, F., Dambre, J., and Bienstman, P. Highly Paral- lel Simulation and Optimization of Photonic Circuits in Time and Frequency Domain based on the Deep-learning Framework Pytorch. Scientific reports, 9(1):5918, 2019. Li, C., Xu, C., Gui, C., and Fox, M. D. Distance Regular- ized Level Set Evolution and its Application to Image Segmentation.\n\n--- Segment 31 ---\nScientific reports, 9(1):5918, 2019. Li, C., Xu, C., Gui, C., and Fox, M. D. Distance Regular- ized Level Set Evolution and its Application to Image Segmentation. IEEE transactions on image processing, 19(12):3243 3254, 2010. Li, Y., Li, J., and Ozcan, A. Nonlinear Encoding in Diffrac- tive Information Processing using Linear Optical Materi- als. Light: Science Applications, 13(1):173, 2024. Maclaurin, D., Duvenaud, D., and Adams, R. P. Autograd: Effortless Gradients in NumPy. In ICML 2015 AutoML workshop, volume 238, 2015. Molesky, S., Lin, Z., Piggott, A. Y., Jin, W., Vuckovi c, J., and Rodriguez, A. W. Inverse Design in Nanophotonics. Nature Photonics, 12(11):659 670, 2018. Momeni, A., Rahmani, B., Mall ejac, M., Del Hougne, P., and Fleury, R. Backpropagation-free Training of Deep Physical Neural Networks. Science, 382(6676):1297 1303, 2023. Pontryagin, Semenovich, L., Mishchenko, E., Boltyanskii, V., and Gamkrelidze, R. The Mathematical Theory of Optimal Processes. Interscience, 1962. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9, 2019. Rekhi, A. S., Zimmer, B., Nedovic, N., Liu, N., Venkatesan, R., Wang, M., Khailany, B., Dally, W. J., and Gray, C. T. Analog mixed-signal hardware error modeling for deep learning inference. In Proceedings of the 56th Annual Design Automation Conference 2019, pp. 1 6, 2019.\n\n--- Segment 32 ---\nIn Proceedings of the 56th Annual Design Automation Conference 2019, pp. 1 6, 2019. Richardson, N., Oktay, D., Ovadia, Y., Bowden, J. C., and Adams, R. P. Fiber Monte Carlo. In The Twelfth International Conference on Learning Representations, 2024. URL id sP1tCl2QBk. Sauter, F. Uber das verhalten eines elektrons im homo- genen elektrischen feld nach der relativistischen theorie diracs. Zeitschrift f ur Physik, 69(11):742 764, 1931. doi: 10.1007 BF01339461. URL 1007 BF01339461. Schuman, C. D., Kulkarni, S. R., Parsa, M., Mitchell, J. P., Date, P., and Kay, B. Opportunities for Neuromorphic Computing Algorithms and Applications. Nature Com- putational Science, 2(1):10 19, 2022. 8 Nonlinear Computation with Linear Optics via Source-Position Encoding So, S., Badloe, T., Noh, J., Bravo-Abad, J., and Rho, J. Deep Learning Enabled Inverse Design in Nanophotonics. Nanophotonics, 9(5):1041 1057, 2020. Sui, X., Wu, Q., Liu, J., Chen, Q., and Gu, G. A Review of Optical Neural Networks. IEEE Access, 8:70773 70783, 2020. Ulrich, I. E., Boehm, C., Zunino, A., B osch, C., and Ficht- ner, A. Diffuse Ultrasound Computed Tomography. The Journal of the Acoustical Society of America, 151(6): 3654 3668, 2022. Van Dijk, N. P., Maute, K., Langelaar, M., and Van Keulen, F. Level-set Methods for Structural Topology Optimiza- tion: a Review. Structural and Multidisciplinary Opti- mization, 48(3):437 472, 2013. Wanjura, C. C. and Marquardt, F. Fully Nonlinear Neuro- morphic Computing with Linear Wave Scattering.\n\n--- Segment 33 ---\nStructural and Multidisciplinary Opti- mization, 48(3):437 472, 2013. Wanjura, C. C. and Marquardt, F. Fully Nonlinear Neuro- morphic Computing with Linear Wave Scattering. Nature Physics, 20(9):1434 1440, 2024. Xia, F., Kim, K., Eliezer, Y., Han, S., Shaughnessy, L., Gi- gan, S., and Cao, H. Nonlinear Optical Encoding Enabled by Recurrent Linear Scattering. Nature Photonics, pp. 1 9, 2024. Yildirim, M., Dinc, N. U., Oguz, I., Psaltis, D., and Moser, C. Nonlinear Processing with Linear Optics. Nature Photonics, pp. 1 7, 2024. Zuo, Y., Li, B., Zhao, Y., Jiang, Y., Chen, Y.-C., Chen, P., Jo, G.-B., Liu, J., and Du, S. All-optical Neural Network with Nonlinear Activation Functions. Optica, 6(9):1132 1137, 2019. 9 Nonlinear Computation with Linear Optics via Source-Position Encoding A. Appendix A.1. Notation R 0 The set of nonnegative real numbers {x R x 0}. R 0 The set of positive real numbers {x R x 0}. T R 0 An alias for the nonnegative reals used to suggest a temporal coordinate. X Boundary of the set X, i.e., the set difference of its closure against its interior. A.2. Electromagnetics We follow the notation given in (Balanis, 2012), which is briefly discussed here and summarized at the conclusion of this section. Importantly, we work in the frequency domain, meaning we assume that the fields are harmonic functions of their temporal argument. We use boldface font to denote the complex-valued form of a given field quantity, and script font for the associated time-dependent field quantity. These representations are related through a complex sinuisoidal term ejωt where the frequency ω in units of [radians s] is implicit (but assumed fixed across all quantities) and j denotes the imaginary unit, as is standard in electrical engineering. E : R3 T 7 R3 The time-harmonic electric field.\n\n--- Segment 34 ---\nThese representations are related through a complex sinuisoidal term ejωt where the frequency ω in units of [radians s] is implicit (but assumed fixed across all quantities) and j denotes the imaginary unit, as is standard in electrical engineering. E : R3 T 7 R3 The time-harmonic electric field. If the complex electric field is denoted E : R3 7 C3, then E(x, t) Re(ejωtE(x)). Units [V m]. H : R3 T 7 R3 The time-harmonic magnetic field. If the complex magnetic field is denoted H : R3 7 C3, then H(x, t) Re(ejωtH(x)). Units [A m]. D : R3 T 7 R3 The time-harmonic electric flux density. If the complex electric flux density is denoted D : R3 7 C3, then D(x, t) Re(ejωtD(x)). Units [C m2]. B : R3 T 7 R3 The time-harmonic magnetic flux density. If the complex magnetic flux density is denoted B : R3 7 C3, then B(x, t) Re(ejωtB(x)). Units [W m2]. J : R3 T 7 R3 The time-harmonic electric current density. If the complex electric current density is denoted B : R3 7 C3, then B(x, t) Re(ejωtB(x)). Units [W m2]. Q : R3 T 7 R The time-harmonic electric charge density. If the complex electric charge density is denoted Q : R3 7 C, then Q(x, t) Re(ejωtQ(x)). Units [C m3]. To denote the material parameters which appear in the constitutive relations, we use the following scalar functions of position. ϵ : R3 7 C The scalar, complex-valued electric permittivity conductivity. The real part Re(ϵ) is the electric permittivity and the imaginary part Im(ϵ) σ is the electric conductivity. µ : R3 7 R The scalar, real-valued magnetic permeability.\n\n--- Segment 35 ---\nThe real part Re(ϵ) is the electric permittivity and the imaginary part Im(ϵ) σ is the electric conductivity. µ : R3 7 R The scalar, real-valued magnetic permeability. This choice follows from our material assumptions, which are standard in many computational electromagnetics applications. We assume that all materials in our simulations are linear, isotropic, and non-dispersive. In electromagnetics a material is linear if the constitutive parameters (i.e., the value of (ϵ, µ) at any point in the simulation domain) do not depend on the value of the field intensities ( E 2 2, H 2 2). A material is isotropic if its constitutive parameters do not depend on the directions (1 E E, 1 H H) of the applied fields. Finally, a material is non-dispersive if its constitutive parameters do not depend on the frequency ω of excitation. In the general case, the constitutive parameters relate the fields through convolution, but in the frequency domain (and 10 Nonlinear Computation with Linear Optics via Source-Position Encoding representing the constitutive parameters as scalar values), we can simply write D ϵE (15) B µH. (16) We use ϵ0, µ0 to denote the permittivity and permeability of the vacuum, respectively. We denote the relative permittivity ϵr ϵ ϵ0 and the relative permeability µr µ µ0. We sometimes refer to the relative permittivity as the dielectric constant. Finally, we use k ω µϵ to denote the wavenumber, which is related to the wavelength via k 2π λ for wavelength λ. In vacuum, we have k0 ω c0, where c0 is the speed of light in vacuum. A.3. Additional Discussion of Related Work A.3.1. NONLINEAR OPTICS Nonlinear optical effects were first predicted in the early 1930s (G oppert, 1930) and then observed in the early 1960s at Bell Labs (Bayer Schaack, 1970). Subsequently, a wide body of work has explored nonlinear interactions with respect to intensity, frequency, phase, and polarization (Boyd et al., 2008).\n\n--- Segment 36 ---\nNONLINEAR OPTICS Nonlinear optical effects were first predicted in the early 1930s (G oppert, 1930) and then observed in the early 1960s at Bell Labs (Bayer Schaack, 1970). Subsequently, a wide body of work has explored nonlinear interactions with respect to intensity, frequency, phase, and polarization (Boyd et al., 2008). Nonlinearity is observed in all media (including the vacuum) above some finite field intensity (Sauter, 1931). Unfortunately, it is generally observed only at the extremes of power density, near the breakdown voltage for most materials. At a fundamental level, this is a result of the fact that photons (being bosonic particles) do not obey the Pauli exclusion principle. This means that in most conditions in free space, the lack of interaction between photons precludes nonlinear optical effects. By convention, the term nonlinear optics definitionally implies what we call here materials-derived nonlinear effects (Boyd et al., 2008). From a physical perspective, the view of nonlinear optics is correct; although in some technical sense one could make non-contradictory statements in the study of vacuum optical nonlinearities, practically speaking the science and engineering of optical nonlinearities is that science concerned with the nonlinear response of a material to an applied optical field. The nonlinearity we pursue in this work (and has been the major focus of the recent work in optical neural networks), is a logical nonlinearity associated with implicitly defined mathematical functions whose values are determined by the physical states of the system at hand. That is, the physical signals with which we associate logical symbols interact linearly, but our logical symbols are related nonlinearly. A.3.2. NONLINEARITIES IN OPTICAL NEURAL NETWORKS With respect to optical neural networks, the pursuit of a mechanism for practical, low-power nonlinearity is a major program. (Xia et al., 2024) leverage a multiple-scattering cavity to implement a randomized reservoir neural network which is used for feature learning on downstream tasks like classification and compression. Notably, is not a trainable neural network, in the sense that the network does not have parameters which can be fit through optimization, but instead can serve as a fixed unsupervised pre-processing embedding layer for a downstream model application.\n\n--- Segment 37 ---\n(Xia et al., 2024) leverage a multiple-scattering cavity to implement a randomized reservoir neural network which is used for feature learning on downstream tasks like classification and compression. Notably, is not a trainable neural network, in the sense that the network does not have parameters which can be fit through optimization, but instead can serve as a fixed unsupervised pre-processing embedding layer for a downstream model application. Our work differs in our explicit treatment of the optimization of the optical computing element, via topology optimization of the underlying material distribution. (Wanjura Marquardt, 2024) introduce an alternative methodology for nonlinearities. The discrete form of Maxwell s equations in the frequency domain can be cast as a certain linear system Ae b. (Wanjura Marquardt, 2024) essentially observe that a nonlinearity can be supported via matrix inversion, i.e., for x R, it follows that f(x) : A(x) 1b can be chosen in a form such that f is nonlinear in x. This is something like a structural nonlinearity in machine learning terminology, and an effective application of the underlying physics to derive a nonlinearity. On the other hand, we achieve a nonlinearity in the source term b above, and use the structural parameters in A ahead of time during training to learn effective electromagnetic mode profiles which can be exploited at inference time by b. (Li et al., 2024) study what could be called the general case of the work presented in (Xia et al., 2024), providing an analysis of the generic technique of data repitition-based strategies. The core argument is that the repitition within diffractive volumes (i.e., the optical cavity used in (Xia et al., 2024)) cannot strictly serve as optical implementations of most conventional neural network layers, but are still useful in task specific contexts. (Yildirim et al., 2024) introduce a technique which is similar to (Xia et al., 2024) but utilizes multiple modulating planes where data is repeatedly embedded. 11 Nonlinear Computation with Linear Optics via Source-Position Encoding A.3.3.\n\n--- Segment 38 ---\n(Yildirim et al., 2024) introduce a technique which is similar to (Xia et al., 2024) but utilizes multiple modulating planes where data is repeatedly embedded. 11 Nonlinear Computation with Linear Optics via Source-Position Encoding A.3.3. INVERSE DESIGN IN PHOTONICS Inverse design of electromagnetic devices is widely studied (Molesky et al., 2018; Christiansen Sigmund, 2021; Georgieva et al., 2002), ranging from fairly application specific microwave circuit parameter optimization (Georgieva et al., 2002) to flexible structural optimization with full-wave field solvers (Hughes et al., 2018b; 2019). We use the adjoint method, which amounts to implicitly differentiating through our full-wave field solver to compute derivatives with respect to the geometry and material properties of our design. The adjoint method dates back to the 1960s (Pontryagin et al., 1962). In photonics, the use of modern computational methods (e.g., optimization, machine learning and data fitting), has drawn much interest in the design of flexible, complex, and unintuitive electromagnetic system designs across a variety of applications (Karahan et al., 2023; So et al., 2020). These efforts are in the direction of extreme specialization, leveraging the enormity of modern computing resources at our disposal to manage the complexity of low-level physical device design, whereas human designers are at least apparently obligated to employ abstraction and modularity to succeed in the design of complex systems. Our automated design methodology also subscribes to the theme of extreme specialization, but our use of topology optimization specifically is most similar to (Khoram et al., 2019). That said, (Khoram et al., 2019) depend on a material- derived nonlinearity (i.e., the electrical response of a hypothetical optical saturable absorber), and a discrete smoothed level set topology optimization method drawn from the computer vision literature (Li et al., 2010). Our method obviates material-derived nonlinearities and an exact topology optimization method, which we can differentiate through using Fiber Monte Carlo (Richardson et al., 2024). A.4. Additional Experimental Details A.4.1. DOES CODESIGN MATTER?\n\n--- Segment 39 ---\nAdditional Experimental Details A.4.1. DOES CODESIGN MATTER? During optimization, we track the root mean square (RMS) of the gradient of the objective with respect to (a) the topology parameters and (b) the decoder parameters, in an attempt to understand the relative contributions to the designs. In general (but with some exceptions), we find that the topology derivatives tend to be between 1 and 3 orders of magnitude larger in RMS than the decoder derivatives; to first order then, the interpretation is that the classification accuracy of the system is around 10-1000x more sensitive to changes in the topology than that of the decoder parameters. However, it should not be inferred from this quantitative result that the decoder is not critical to the overall system performance. In fact, optimizing without using the decoder at all degrades the system performance significantly. Our interpretation of the relatively smaller decoder gradient magnitude follows the apparent role of the decoder as a permuting map to account for the arbitrary assignment of class labels to output measurement locations. Then, after initialization, optimizing the decoder amounts to merely breaking symmetry in its weights to achieve a better class label to output region assignment. From that point, the absolute magnitude of any given decoder weight has minimal effect, since only their relative values are responsible for the label output region re-mapping. Optimizing only the decoder but not the topology results in significantly worse performance (around 10 reduction in classification accuracy). But, the fact that the decoder-only performance is significantly better than random indicates that even randomly initialized topologies can be adapted for use as effective classifiers with an appropriately optimized linear decoder; similar to the use of multiple-scattering devices as reservoir networks. A.4.2. COMPUTING RESOURCES All experiments were carried out on a single node Linux server with a 32 CPUs and an Nvidia RTX 3080Ti GPU. For efficiency, we precompute all possible source combinations (derived from the finite number of quantized values) and store them in an in-memory cache, and the wave operator (less the contribution from the topology) is precomputed and stored as a sparse matrix in memory. We execute batched simulations with a batch size of 32, as we determined that batch sizes larger than this only marginally improved performance.\n\n--- Segment 40 ---\nFor efficiency, we precompute all possible source combinations (derived from the finite number of quantized values) and store them in an in-memory cache, and the wave operator (less the contribution from the topology) is precomputed and stored as a sparse matrix in memory. We execute batched simulations with a batch size of 32, as we determined that batch sizes larger than this only marginally improved performance. We use separate optimizers for the topology and decoder parameters, both Adam optimizers with separate step sizes depending on the topology parameterization used (for implicitly formulated B-splines and neural fields, we use αg 0.001, whereas direct mesh parameterization used αg 0.01). 12\n\n