=== ORIGINAL PDF: 2502.13921v2_Exploring_Code_Language_Models_for_Automated_HLS-b.pdf ===\n\nRaw text length: 42588 characters\nCleaned text length: 41615 characters\nNumber of segments: 25\n\n=== CLEANED TEXT ===\n\nExploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis Jiahao Gai2, Hao (Mark) Chen1, Zhican Wang3, Hongyu Zhou4, Wanru Zhao2, Nicholas Lane 2, Hongxiang Fan1 2 1Imperial College London, 2University of Cambridge, 3Shanghai Jiao Tong University, 4University of Sydney Email: ABSTRACT Recent advances in code generation have illuminated the potential of employing large language models (LLMs) for general-purpose programming languages such as Python and C , opening new opportunities for automating software development and enhanc- ing programmer productivity. The potential of LLMs in software programming has sparked significant interest in exploring auto- mated hardware generation and automation. Although preliminary endeavors have been made to adopt LLMs in generating hardware description languages (HDLs) such as Verilog and SystemVerilog, several challenges persist in this direction. First, the volume of available HDL training data is substantially smaller compared to that for software programming languages. Second, the pre-trained LLMs, mainly tailored for software code, tend to produce HDL designs that are more error-prone. Third, the generation of HDL requires a significantly higher number of tokens compared to soft- ware programming, leading to inefficiencies in cost and energy consumption. To tackle these challenges, this paper explores lever- aging LLMs to generate High-Level Synthesis (HLS)-based hard- ware design. Although code generation for domain-specific pro- gramming languages is not new in the literature, we aim to provide experimental results, insights, benchmarks, and evaluation infras- tructure to investigate the suitability of HLS over low-level HDLs for LLM-assisted hardware design generation. To achieve this, we first finetune pre-trained models for HLS-based hardware genera- tion, using a collected dataset with text prompts and corresponding reference HLS designs. An LLM-assisted framework is then pro- posed to automate end-to-end hardware code generation, which also investigates the impact of chain-of-thought and feedback loops promoting techniques on HLS- design generation. Comprehensive experiments demonstrate the effectiveness of our methods. Lim- ited by the timeframe of this research, we plan to evaluate more advanced reasoning models in the future. ACM Reference Format: Jiahao Gai, Hao (Mark) Chen, Zhican Wang, Hongyu Zhou, Wanru Zhao, Nicholas Lane, Hongxiang Fan. 2025. Exploring Code Language Models for Automated HLS-based Hardware Generation. In Proceedings of Asia and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan 2025 Copyright held by the owner author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0635-6 25 01 60.89 60.40 86.94 64.71 48.92 53.89 1.33 0 20 40 60 80 100 Volume Size GB Gap Volume Size KB 1E 0 1E 1 1E 2 1E 3 1E 4 1E 5 1E 6 1E 7 1E 8 CodeParrot RTLLM (a) Starcoder Dataset (b) CodeParrot vs RTLLM Gap 40.52 2.26 1E4 Figure 1: Comparison of data availability between HDLs and other software programming languages. South Pacific Design Automation Conference (ASP-DAC 25). ACM, New York, NY, USA, 8 pages. 1 INTRODUCTION In the field of Generative AI (GenAI), significant strides have been made in producing complex and creative content across various domains such as text [4], image [2, 23], and video [35]. Among various GenAI technologies, large language models (LLMs) have emerged as particularly influential techniques in the realm of nat- ural language processing [33]. This great capability of LLMs also raises intensive industrial interests in leveraging these models in automated code generation, as evidenced by GitHub Copilot [5] and DeepMind s AlphaCode [15]. Meanwhile, over 50 pre-trained mod- els and more than 170 programming language datasets have been published in the past few years [32]. Although significant progress has been made in this direction, most of these works mainly fo- cus on software code generation , and the potential of LLM for hardware design generations has not been fully exploited. The promises of LLM-assisted software programming has led to several recent attempts to explore automated code generation for hardware description languages (HDLs) such as Verilog and Sys- temVerilog [14, 18, 20, 25]. Although multiple datasets, pre-trained models, and code infrastructures have been introduced, several key challenges reamin in LLM-assisted hardware design generation: The data availability of HLD designs for LLM training and finetuning. Figure 1 compares the volume of training samples for software programming languages versus HDLs. For instance, the general-purpose code dataset StarCoder [14], as presented in Figure 1 (a), shows that the available amount of HDL designs is less than 1 of those for C . Similar trends can also be ob- served in specialized datasets, such as RTLLM [20] for Verilog and CodeParrot [27] for Python. Figure 1(b) indicates that the dataset size of RTLLM is less than 1 of that for CodeParrot. Therefore, This paper mainly focuses on text-to-code generation. arXiv:2502.13921v2 [cs.LG] 5 Mar 2025 ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan Gai et al. the quantity of training data available for hardware design is significantly lower than for software programming languages. Inability of utilizing learned knowledge from pre-trained coding LLMs. Pre-trained coding LLMs are primarily trained on software programming languages, which differ significantly in semantics and syntax from HDLs. Therefore, the knowledge ac- quired during pre-training cannot be directly applied to hardware code generation, compounding the data scarcity issue. Cost of HDL generation using LLMs. Figure 2 illustrates the number of tokens required for generating identical hardware designs using High-Level Synthesis (HLS) versus HDL. It shows that HDL implementations require approximately 3 4 times more tokens than HLS designs, making HLS-based design gener- ation a more sustainable solution considering the latency, energy, and monetary costs associated with LLM inference. To address the aforementioned issues, this paper proposes an LLM-assisted framework for generating HLS-based hardware de- signs. By crawling HLS designs from open-source Github reposito- ries, we collect a dataset to facilitate the fine-tuning of pre-trained LLM for the downstream HLS code generation. The benefits of generating HLS code are two folds: i) Given that HLS shares main semantics and syntax with C C , the coding knowledge acquired during the pre-training phase of the LLMs can be effectively utilized for hardware design. This compatibility also reduces the learning curve and dataset requirements for fine-tuning, as the additional knowledge needed for HLS is less than for traditional HDL coding. ii) The number of tokens required to generate HLS code is lower compared to HDLs, rendering our approach more cost-effective and energy-efficient than previous methodologies. To further improve the quality of the generated designs, the framework incorporates debugging feedback loops and a chain-of-thought enhancement mechanism, systematically integrating detected bugs back into the input for iterative refinement. Overall, our contributions can be summarized as follows: Finetuning pre-trained code language models for HLS-based hard- ware generation, using a collected dataset with over 40, 000 data entries, each containing a text prompt and the corresponding HLS-based hardware design (Section 3). Developing a framework that automatically produces HLS de- signs from input prompts, with an end-to-end evaluation of the syntax and functionality correctness (Section 4.1). Integrating multiple optimization techniques such as feedback loops and chain-of-thought techniques, which improve the pass rate for syntax and functionality (Section 4.2 Section 4.3). 2 BACKGROUND AND RELATED WORK 2.1 LLM-Assisted Software Engineering Based on the modality of inputs and outputs, language models for software engineering can be categorized into several downstream code tasks [32] such as text-to-code (code generation synthesis [16]), code-to-text (code summarization [10]), and code-to-pattern pro- cessing (defect detection [21]). This paper focuses on code gener- ation that aims at producing code from natural language descrip- tions prompts. To facilitate the development of code generation This paper focues on C-based HLS. static void compute_mult_16bit( hls::stream uint16_t inStream1, hls::stream uint16_t inStream2, hls::stream uint16_t outStream, int vSize) { execute: for (int i 0; i vSize; i ) { pragma HLS LOOP_TRIPCOUNT min size max size outStream (inStream1.read() inStream2.read()); } } HLS-based design module multi_16bit( input clk, input rst_n, input start, input [15:0] ain, input [15:0] bin, output [31:0] yout, output done ); reg [15:0] areg; reg [15:0] breg; reg [31:0] yout_r; reg done_r; reg [4:0] i; always (posedge clk or negedge rst_n) if (!rst_n) i 5'd0; else if (start i 5'd17) i i 1'b1; else if (!start) i 5'd0; always (posedge clk or negedge rst_n) if (!rst_n) done_r 1'b0; else if (i 5'd16) done_r 1'b1; else if (i 5'd17) done_r 1'b0; assign done done_r; always (posedge clk or negedge rst_n) begin if (!rst_n) begin areg 16'h0000; breg 16'h0000; yout_r 32'h00000000; end else if (start) begin if (i 5'd0) begin areg ain; breg bin; end else if (i 5'd0 i 5'd17) begin if (areg[i-1]) yout_r yout_r ({16'h0000, breg} (i-1)); end end end assign yout yout_r; endmodule Verilog-based design 0 20 40 60 80 100 HLS Verilog normalized token usage Token Comparison Figure 2: HLS-based and Verilog-based programs. with language models, various datasets, approaches, and pre-trained models have been introduced over the past decade. Due to the lack of model capability, the early-stage methods [11, 16] of code generation mainly focus on a few programming lan- guages such as Python or Java. Subsequently, with the increasing computational power, larger datasets are introduced to train models for multiple general-purpose programming languages. CodeXGLUE [19] presents a comprehensive code dataset consisting of different code tasks such as clone detection and code repair. HumanEval [5] dataset together with the code model CodeX marks as a milestone by using pre-trained LLMs for code generation. The promising capability shown by CodeX sparks significant academic and industrial in- terests in developing LLM-assisted code generation. Larger code datasets, such as StarCoder [14] and CodeParrot [27], and LLMs, including Code-LLaMA [22] and CodeFuse [7], are open-sourced in this community. However, most of these recent efforts focus on software programming languages. 2.2 Automated Hardware Design Generation Building on the success of LLM-assisted software programming, recent studies have explored using language models for automated hardware generation. Since the data is the key to training LLMs for hardware code generation, multiple HDL datasets have been introduced recently. Thakur et al. present Verigen [25] dataset that contains 17 hardware designs with 0.3K lines of HDL code. To in- crease the diversity of hardware designs for training and evaluation, Lu et al. open-source a larger benchmark consisting of 30 designs with more than 2.5K lines. Sourced from HDLBits , Verilogeval [18] introduces larger datasets with 156 problems. These open-sourced datasets provide public benchmarks for text-to-HDL generation. The evaluation metrics of LLM-assisted hardware code genera- tion focus on three aspects: i) syntax, ii) functionality, and iii) qual- ity. In previous literature [5, 18], syntax correctness and functional- ity are measured using which represents whether any of 𝑘generated code samples can pass the syntax check of synthesis tools or functional unit tests. The quality usually is de- fined as power, performance, and area of the generated hardware, collectively reflect the capability of the code generation methods. Aiming at improving these metrics, existing approaches [18, 20, 25] fine-tune pre-trained LLMs on the downstream task with opti- mized sampling schemes. These LLMs are mainly pre-trained using Exploring Code Language Models for Automated HLS-based Hardware Generation ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan Template of Design Point Design Description: High level description of the design details. Instruction Prompt: Specify coding language and requirements. Reference Design: Canonical HLS program. Figure 3: Template of design points. software programming languages. RTLFixer [26] introduces an auto- mated framework that adopts retrieval-augmented generation [12] and ReAct prompting [31] to enable LLM-assisted debugging for RTL designs. LLM-VeriPPA [1] enhances the code generation of RTL using a two-stage refinement process to progressively improve syntax, functionality, and hardware performance. However, these approaches focus on low-level hardware languages instead of HLS. In this work, we take the first step to investigate the HLS code gen- eration with LLM. Since HLS shares similar semantics and syntax with programming languages commonly used during LLM pre- training, this paper explores whether HLS is better than low-level hardware languages for automated hardware design generation. Although code generation with feedback and CoT prompting is not new in the literature of coding language models, our experi- mental results, insights, benchmark, and evaluation infrastructure specific to LLM-assisted HLS design offer valuable contributions to the future development of automated hardware generation. 3 HLS GENERATION BENCHMARK 3.1 Format of Design Points Following the practices of Python benchmark HumanEval [5] and Verilog dataset VerilogEval [18], each design point has three com- ponents: i) user instruction prompts, ii) design descriptions and iii) reference designs. Figure 3 shows the standardized format template, with each data point stored as a JSONL following the Alpaca format. The default user instruction prompt is Generate HLS code with the following instructions:, which can be enhanced using the chain-of- thought (COT) prompting technique as detailed in Section 4.2. 3.2 Dataset Collection We collect 52 HLS-based designs from open-source repositories, including HLSyn [3] and ML4Accel . These designs are split into training and testing sets at a 4:1 ratio and fall into five categories: Matrix and Linear Algebra Operations: Includes sparse matrix- vector multiplications, dense matrix-matrix multiplication, array transformation and stencil computations. Scientific Simulations: Methods for solving physical and math- ematical problems such as heat distribution and electromagnetic simulations. Statistical Computations: Calculations of statistical metrics from datasets. Iterative Methods: Techniques for solving equations using iter- ative approaches. Other Computational Kernels: Specialized computational tasks like molecular dynamics and interactions, encryption algorithms, and optical flow. Each of these 52 designs is associated with different combinations of programs such as PIPELINE, PARALLEL and TILE. We filter out the HLS programs that are invalid, resulting in a collection of over 42, 000 HLS programs. The whole dataset is split into training and test sets for supervised fine-tuning and evaluation, respectively. 3.3 Generation of Design Description Given that the dataset encompasses over 42,000 HLS programs, man- ually generating design descriptions for each program is both labor- intensive and time-consuming. Inspired by both HumanEval [5] and Verilog, we utilize ChatGPT (version 3.5 and 4) to automate the creation of design descriptions for the datasets. We append each HLS program with this base prompt when utilizing ChatGPT to generate the corresponding design descriptions. Both the reference design and its generated description are stored in JSON format, ad- hering to the structure outlined in Section 3.1. This method ensures streamlined and consistent documentation of design descriptions across the dataset. Following the practice of [18], we provide two versions of prompts for each HLS program in the test set: Machine- Gen and HumanRefine. The MachineGen version comprises prompts and instructions generated by GPT-based models without human modifications. In contrast, the HumanRefine includes manually re- fined prompts to ensure more concise and human-like instructions. 3.4 Assessment Infrastructure We provide evaluation infrastructure for both syntax and function- ality. For syntax verification, we use the GCC compiler with the "-fsyntax-only" option. It allows us to verify the syntax without the overhead of compiling the code, thereby enhancing time and space efficiency. Regarding functionality correctness, we design unit tests tailored for each example in the test dataset. Each test case is associated with its corresponding source_file . To achieve this goal, we modified the original test JSONL file to add another attribute source_file . The testing process involves executing both the generated code and the original source code to compare their outputs. For instance, if the outputs from both codes consist of ma- trices, we conduct a targeted comparison. This is done by selecting specific positions within the matrices from both outputs at random and verifying if they match. 4 AUTOMATED HARDWARE GENERATION 4.1 Framework Overview An overview of our proposed framework is depicted in Figure 4. The framework comprises two main stages: i) model fine-tuning and ii) iterative code generation. The final output is an HLS-based program that can be synthesized into the corresponding hardware design. While focused on Vivado-HLS, our framework is adaptable to any HLS language with appropriate datasets. In the model fine-tuning stage, our framework initiates by re- trieving coding LLMs from open-source repositories, such as Code- Llama and Start-Coder . Then, supervised fine-tuning is conducted on these pre-trained models using the HLS training data (Sec- tion 3.2). We adopt axolotl to perform the fine-tuning, allowing ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan Gai et al. HLS Source Code Diverse High-quality Code Kernels Corresponding Prompts Filter Code-to-Text LLM Data: x Label: y Proposed Dataset Split Training Dataset Test Dataset Pre-trained Text-to- Code LLM Supervised Fine-tuning Test Chain- of- Thought Completed Codes Syntax Check P O Feedback with Located Errors Functionality Check Feedback with Functional Defects O P HLS-based programs ① Model Fine-Tuning ②Iterative Code Generation Dataset Preparation Fine-tuned Text-to- Code LLM Unit Tests Generated HLS Code Vivado Synthesis Input Prompt Design Description: XXXXXX. Instruction Prompt: XXXXX. Compiler (Basic Syntax Check) Figure 4: An overview of our proposed framework. Chain-of-Thought Prompt for Generating HLS Design Instruction Prompt: Let's think step by step. First, Consider the characteristics of FPGA. Second, Determine the program structure. Third, Write code logic. Fourth, Consider data types and interfaces. Figure 5: Chain-of-thought prompts for HLS generation. for customization of models and training parameters, such as learn- ing rate, batch size, and epoch number to fit specific scenarios and available resources. In the second stage, we employ the fine-tuned LLM for iterative code generation. The process begins with initial inputs consisting of user instruction prompts and design descriptions. To enhance the quality of the generated HLS designs, we incorporate a chain- of-thought optimization technique (Section 4.2) into the instruction prompts. The code generation then proceeds with a feedback loop (Section 4.3) that iteratively improves the correctness of the HLS de- signs. This iterative process continues until a refined HLS program is generated as the final output. Users can specify the number of iterations, providing the flexibility to navigate this trade-off accord- ing to their specific needs, with more iterations typically yielding higher quality at increased runtime expense. 4.2 Chain-of-Thought Previous studies indicate that the quality of generated content is significantly influenced by the instructional prompt [34]. The Chain-of-Thought (CoT) [29] technique has proven simple and effective for enhancing the performance of LLMs across a wide range of tasks, including arithmetic, commonsense, and symbolic reasoning [29]. Although initially, COT yielded a modest 0.82 point increase in the metric in code generation, this improvement was substantially augmented through structured prompting [13]. In this paper, we investigate the effect of CoT in generating HLS-based hardware designs. Figure 5 illustrates the CoT prompt structured for HLS code generation. The prompt guides a systematic approach through sev- eral targeted steps: understanding FPGA characteristics, defining program structure, developing logic, selecting data types and inter- faces, before finalizing the code. This structured process ensures thorough consideration of each key aspect to optimize the HLS code generation. 4.3 Two-Step Feedback Loops Code generation with feedback loop has shown promising results in previous literature [1, 17, 24, 28]. This paper investigates its impact on HLS code generation using a two-step feedback loop tailored for automated hardware generation, focusing on HLS-related feed- back. At each iteration, the framework evaluates the syntax and functional correctness of the generated HLS program. The located syntax error and functional defects are then fed back into the input prompts as additional information for subsequent code generation. In the first step, syntax feedback is provided using the GCC compiler with the -fsyntax-only option, as specified in 3.4. It captures the syntax errors without fully compiling the code. It al- lows rapid identification including the types and locations, and precise error mapping for targeted corrections. If the syntax check passes, our framework proceeds to the second step by executing predefined unit tests to compare the outputs of the generated and the original source code. Functional defects are recorded and added to the prompts for the next iteration. This two-step feedback loop continues for user-specified iterations, providing flexibility to bal- ance the trade-off between design quality and runtime cost. 5 EVALUATION 5.1 Experimental Setup In our evaluation, we adopt Code-Llama-7B as the pre-trained model for fine-tuning, employing the low-rank-adaption (QLoRA) [6, 9] technique for faster training and lower memory consumption. Key configurations include loading the model in 8-bit, a sequence length of 4096, sample packing, and padding to sequence length. We set the warmup steps to 100, with a gradient accumulation of 4 steps, Exploring Code Language Models for Automated HLS-based Hardware Generation ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan a micro-batch size of 4, and an inference batch size of 2. For both syntax and functionality checks, we measure accuracy as metrics. In the ablation study from Section 5.2 to Section 5.6, we adopt MachineGen for evaluation. Experiments are conducted on a server with four NVIDIA L20 GPUs (48 GB each), an 80 vCPU Intel Xeon Platinum 8457C, and 100GB of RAM. This setup ensures sufficient computational power and memory to handle the intensive demands of fine-tuning and inference efficiently, especially for long data sequences in the feedback loop experiment. 5.2 Effect of Supervised Finetuning Our first ablation study investigates the effect of the model fine- tuning. We evaluated the performance based on both syntax and functionality checks. As shown in Figure 6(a), the results demon- strate that the finetuning dramatically increases syntax correctness from 54.85 to 88.44 . More importantly, the impact of finetuning is even more pronounced in the functionality evaluation, where the non-finetuned model failed to achieve any correct functional- ity test, but the accuracy is improved to 53.20 in the finetuned model. These enhancements highlight the critical role of finetuning in producing not only syntactically correct but also functionally viable codes, which demonstrates the benefits of finetuning LLMs for hardware design in the HLS code generation task. 5.3 Effect of Chain-of-Thought Prompting To assess the effect of the chain-of-thought (CoT) technique, we perform both syntax and functionality evaluation on the fine-tuned model with and without the use of CoT. As indicated in Figure 6(b), incorporating CoT leads to a noticeable improvement in both met- rics. Specifically, syntax correctness increases from 88.44 to 94.33 , and functionality score rises from 53.20 to 61.45 . The result demonstrates the effectiveness of CoT in enhancing the reasoning capability, thereby improving its overall performance. 5.4 Effect of Feedback Loops Our two-step feedback loop provides both syntax and functionality feedback. We evaluate the impact of these feedback loops with dif- ferent numbers of iterations, ranging from 0 to 2.The results, shown in Figure Figure 7 and Figure 8, indicate that both syntax and func- tionality feedback loops significantly improve model performance, especially when combined with COT prompting. The initial feed- back loop yields substantial accuracy improvements in both syntax correctness and functionality evaluation, though the second loop shows diminishing returns.Syntax feedback loops enhance both syntax correctness and functionality performance, suggesting that iterative refinement is particularly effective for complex tasks. Sim- ilarly, functionality feedback loops not only improve functionality checks but also boost syntax accuracy, indicating that enhance- ments in functional understanding contribute to better syntactic performance. 5.5 Time Cost and Hardware Performance Figure 9 shows the time cost for generating 120 data entries under different conditions, measuring the impact of CoT and feedback loops. Without a feedback loop, CoT significantly reduces the time. 54.85 88.44 0 53.20 0.00 20.00 40.00 60.00 80.00 100.00 w o Finetune with Finetune accuracy synax check functionality check 88.44 94.33 53.20 61.45 0.00 20.00 40.00 60.00 80.00 100.00 w o COT with COT accuracy synax check functionality check (a) Effect of fine-tuning (b) Effect of chain-of-thought prompting Figure 6: Effect of fine-tuning and chain-of-thought. 50.00 60.00 70.00 80.00 90.00 100.00 w o feedback syntax feedback (max 1 loop) syntax feedback (max 2 loops) accuracy syntax check w o COT syntax check with COT functionality check w o COT functionality check with COT Figure 7: Effect of syntax feedback loop. 50.00 60.00 70.00 80.00 90.00 100.00 w o feedback functionality feedback (max 1 loop) functionality feedback (max 2 loops) accuracy syntax check w o COT syntax check with COT functionality check w o COT functionality check with COT Figure 8: Effect of functionality feedback loop. Adding a syntax feedback loop increases the time, but CoT continues to notably decrease the duration. The functionality feedback loop is the most time-consuming, though CoT still provides a notable re- duction, albeit less dramatic. This demonstrates CoT s effectiveness in reducing operational times across varying complexities. For the test set, we evaluate the latency and resource consump- tion of the generated HLS designs using a Xilinx VCU118 as our target FPGA, with a clock frequency of 200MHz and Xilinx Vi- vado 2020.1 for synthesis. As shown in Table 1, all HLS designs demonstrate reasonable performance, with BRAM usage consis- tently remained at zero due to the design scale. 5.6 Effect of Task Complexity We analyze the effects of code complexity on the performance of fine-tuning our language model with CoT prompting and tested without the use of any feedback loops during inference. We catego- rize MachineGen into three classes according to their code complex- ity: easy, medium, and difficult. The results shown in the Table 2 indicates a clear trend: as the complexity of the generated code increases, both syntax and functionality correctness rates decline. ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan Gai et al. 7 9 11 5 6 9 0 2 4 6 8 10 12 w o feedback loop syntax loop (max 1) functionality loop (max 1) Inference Time averaged by120 Data Points (second) w o COT with COT Figure 9: Time cost of code generation. Table 1: Latency and resource usage of LLM-generated de- signs synthesized on a VCU118 FPGA. Latency (ms) LUTs Registers DSP48s BRAMs Available - 1182240 2364480 6840 4320 ellpack 0.304 1011 1079 11 0 syrk 21.537 1371 1621 19 0 syr2k 40.626 1572 1771 19 0 stencil2d 1.368 287 123 3 0 trmm-opt 15.889 1262 1239 11 0 stencil3d 21.537 1173 1271 20 0 symm 24.601 1495 1777 19 0 symm-opt 16.153 1361 1608 19 0 symm-opt-medium 579.0 2223 2245 22 0 This outcome could be attributed to several factors. First, more complex code inherently presents more challenges in maintaining syntactic integrity and functional accuracy. Second, the absence of feedback loops in the inference phase may have limited the model s ability to self-correct emerging errors in more complicated code generations. Table 2: Performance across different complexity levels. Test Set Syntax Check Functionality Easy 96.67 63.33 Medium 96.67 53.33 Difficult 90 53.33 5.7 Analysis of MachineGen and HumanRefine Table 3: Performance on MachineGen and HumanRefine. Test Set Syntax Check Functionality Check MachineGen 93.83 62.24 HumanRefine 47.29 21.36 As shown in Table 3, this section compares the performance of our model on MachineGen and HumanRefine test sets. Our findings reveal that the performance on the HumanRefine is significantly lower than on the MachineGen. This disparity suggests that the model is more adept at handling machine-generated prompts. The primary reasons for this are: the model s training data bias to- wards machine-generated prompts, the increased complexity and nuanced nature of human-generated prompts, and the conciseness and clarity of human-generated prompts that often omit repetitive or explicit details found in machine-generated prompts, making it harder for the model to generate syntactically and functionally correct code. 5.8 Thoughts, Insights, and Limitations 1. HLS versus HDL for AI-assisted code generation: The se- lection of programming language for hardware code generation should mainly depend on two factors: Quality of Generated Hardware Design: The evaluation of hard- ware design s quality includes syntax correctness, functionality, and hardware performance. Since HLS shares similar semantics and syntax with programming languages commonly used during LLM pre-training, this work demonstrates that the LLM-assisted code generation for HLS has the potential to achieve high syntax and functional correctness in hardware designs. While this work does not leverage hardware performance as feedback for design generation, it identifies this aspect as a key direction for future research and enhancements. Runtime Cost of Hardware Generation: Although HLS-based de- signs typically require fewer tokens compared to HDL during the code generation phase suggesting potentially lower costs the overall runtime costs associated with HLS synthesis must also be considered. A more comprehensive quantitative comparison of these runtime costs is planned for our future work. 2. Input instructions and datasets are crucial: The fine-tuning of pre-trained LLMs on HLS dataset can bring a significant improve- ment in the design quality, echoing findings from previous studies on Verilog code generation [25]. Additionally, during our evaluation, we found that employing simple CoT prompting largely improves hardware design quality. This result contrasts with the application of CoT in general-purpose programming languages, where a spe- cialized form of CoT is necessary [13]. Therefore, future efforts for further enhancement can focus on collecting high-quality datasets and exploring better refinement of input prompts. 3. Limitations: At the time of this research, more advanced rea- soning models, such as DeepSeek-R1 [8], were not available for evaluation. Additionally, test-time scaling approaches [30] could be incorporated to further enhance performance in the future. More- over, we observe that the diversity of hardware designs in the benchmark is limited, which may impact the generalizability of our findings. We intend to address these limitations in our future work. 6 CONCLUSION This paper explores automating hardware generation with code language models and High-Level Synthesis (HLS). We aim to inves- tigate the suitability of HLS over low-level hardware description languages for hardware design generation. To facilitate this, we propose benchmarks and code infrastructures for evaluating LLM- assisted HLS design generation. Our experimental findings reveal that, with the integration of advanced optimizations such as feed- back loops and chain-of-thought techniques, LLM-assisted HLS code generation shows substantial promise in designing complex hardware with high levels of syntax and functional correctness. Exploring Code Language Models for Automated HLS-based Hardware Generation ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan REFERENCES [1] Anonymous Authors. 2024. LLM-VeriPPA: Power, Performance, and Area-aware Verilog Code Generation and Refinement with Large Language Models. https: openreview.net pdf?id nZL6S0b-HcI [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In IEEE CVF Conference on Computer Vision and Pattern Recognition. 18208 18218. [3] Yunsheng Bai, Atefeh Sohrabizadeh, Zongyue Qin, Ziniu Hu, Yizhou Sun, and Ja- son Cong. 2023. Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs. Advances in Neural Information Processing Systems 36 (2023), 45288 45299. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877 1901. [5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024). [7] Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, et al. 2023. Codefuse-13b: A pretrained multi-lingual code large language model. arXiv preprint arXiv:2310.06266 (2023). [8] Daya Guo et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [10] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Sum- marizing source code using a neural attention model. In 54th Annual Meeting of the Association for Computational Linguistics 2016. Association for Computational Linguistics, 2073 2083. [11] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Map- ping language to code in programmatic context. arXiv preprint arXiv:1808.09588 (2018). [12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459 9474. [13] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023. Structured chain-of-thought prompt- ing for code generation. arXiv preprint arXiv:2305.06599 (2023). [14] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023). [15] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092 1097. [16] Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočisk y, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent predictor networks for code generation. arXiv preprint arXiv:1603.06744 (2016). [17] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. Rltf: Reinforcement learning from unit test feedback. arXiv preprint arXiv:2307.04349 (2023). [18] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. 2023. Ver- ilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 1 8. [19] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro- sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021). [20] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. 2024. RTLLM: An open-source benchmark for design rtl generation with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 722 727. [21] Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, and Premkumar Devanbu. 2016. On the" naturalness" of buggy code. In Proceedings of the 38th International Conference on Software Engineering. 428 439. [22] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao- qing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023). [23] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. 2022. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 4 (2022), 4713 4726. [24] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023). [25] Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan- Gavitt, Ramesh Karri, and Siddharth Garg. 2023. Verigen: A large language model for verilog code generation. ACM Transactions on Design Automation of Electronic Systems (2023). [26] YunDa Tsai, Mingjie Liu, and Haoxing Ren. 2023. Rtlfixer: Automatically fixing rtl syntax errors with large language models. arXiv preprint arXiv:2311.16543 (2023). [27] Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language processing with transformers. " O Reilly Media, Inc.". [28] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu. 2022. Compilable neural code generation with compiler feedback. arXiv preprint arXiv:2203.05132 (2022). [29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 24824 24837. [30] Sean Welleck et al. 2024. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838 (2024). [31] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022). [32] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the perspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989 (2023). [33] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [34] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning. PMLR, 12697 12706. [35] Zangwei Zheng, Xiangyu Peng, and Yang You. 2024. Open-Sora: Democratizing Efficient Video Production for All.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nExploring Code Language Models for Automated HLS-based Hardware Generation: Benchmark, Infrastructure and Analysis Jiahao Gai2, Hao (Mark) Chen1, Zhican Wang3, Hongyu Zhou4, Wanru Zhao2, Nicholas Lane 2, Hongxiang Fan1 2 1Imperial College London, 2University of Cambridge, 3Shanghai Jiao Tong University, 4University of Sydney Email: ABSTRACT Recent advances in code generation have illuminated the potential of employing large language models (LLMs) for general-purpose programming languages such as Python and C , opening new opportunities for automating software development and enhanc- ing programmer productivity. The potential of LLMs in software programming has sparked significant interest in exploring auto- mated hardware generation and automation. Although preliminary endeavors have been made to adopt LLMs in generating hardware description languages (HDLs) such as Verilog and SystemVerilog, several challenges persist in this direction. First, the volume of available HDL training data is substantially smaller compared to that for software programming languages. Second, the pre-trained LLMs, mainly tailored for software code, tend to produce HDL designs that are more error-prone. Third, the generation of HDL requires a significantly higher number of tokens compared to soft- ware programming, leading to inefficiencies in cost and energy consumption. To tackle these challenges, this paper explores lever- aging LLMs to generate High-Level Synthesis (HLS)-based hard- ware design. Although code generation for domain-specific pro- gramming languages is not new in the literature, we aim to provide experimental results, insights, benchmarks, and evaluation infras- tructure to investigate the suitability of HLS over low-level HDLs for LLM-assisted hardware design generation. To achieve this, we first finetune pre-trained models for HLS-based hardware genera- tion, using a collected dataset with text prompts and corresponding reference HLS designs. An LLM-assisted framework is then pro- posed to automate end-to-end hardware code generation, which also investigates the impact of chain-of-thought and feedback loops promoting techniques on HLS- design generation. Comprehensive experiments demonstrate the effectiveness of our methods. Lim- ited by the timeframe of this research, we plan to evaluate more advanced reasoning models in the future.\n\n--- Segment 2 ---\nComprehensive experiments demonstrate the effectiveness of our methods. Lim- ited by the timeframe of this research, we plan to evaluate more advanced reasoning models in the future. ACM Reference Format: Jiahao Gai, Hao (Mark) Chen, Zhican Wang, Hongyu Zhou, Wanru Zhao, Nicholas Lane, Hongxiang Fan. 2025. Exploring Code Language Models for Automated HLS-based Hardware Generation. In Proceedings of Asia and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan 2025 Copyright held by the owner author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0635-6 25 01 60.89 60.40 86.94 64.71 48.92 53.89 1.33 0 20 40 60 80 100 Volume Size GB Gap Volume Size KB 1E 0 1E 1 1E 2 1E 3 1E 4 1E 5 1E 6 1E 7 1E 8 CodeParrot RTLLM (a) Starcoder Dataset (b) CodeParrot vs RTLLM Gap 40.52 2.26 1E4 Figure 1: Comparison of data availability between HDLs and other software programming languages. South Pacific Design Automation Conference (ASP-DAC 25). ACM, New York, NY, USA, 8 pages. 1 INTRODUCTION In the field of Generative AI (GenAI), significant strides have been made in producing complex and creative content across various domains such as text [4], image [2, 23], and video [35]. Among various GenAI technologies, large language models (LLMs) have emerged as particularly influential techniques in the realm of nat- ural language processing [33].\n\n--- Segment 3 ---\n1 INTRODUCTION In the field of Generative AI (GenAI), significant strides have been made in producing complex and creative content across various domains such as text [4], image [2, 23], and video [35]. Among various GenAI technologies, large language models (LLMs) have emerged as particularly influential techniques in the realm of nat- ural language processing [33]. This great capability of LLMs also raises intensive industrial interests in leveraging these models in automated code generation, as evidenced by GitHub Copilot [5] and DeepMind s AlphaCode [15]. Meanwhile, over 50 pre-trained mod- els and more than 170 programming language datasets have been published in the past few years [32]. Although significant progress has been made in this direction, most of these works mainly fo- cus on software code generation , and the potential of LLM for hardware design generations has not been fully exploited. The promises of LLM-assisted software programming has led to several recent attempts to explore automated code generation for hardware description languages (HDLs) such as Verilog and Sys- temVerilog [14, 18, 20, 25]. Although multiple datasets, pre-trained models, and code infrastructures have been introduced, several key challenges reamin in LLM-assisted hardware design generation: The data availability of HLD designs for LLM training and finetuning. Figure 1 compares the volume of training samples for software programming languages versus HDLs. For instance, the general-purpose code dataset StarCoder [14], as presented in Figure 1 (a), shows that the available amount of HDL designs is less than 1 of those for C . Similar trends can also be ob- served in specialized datasets, such as RTLLM [20] for Verilog and CodeParrot [27] for Python. Figure 1(b) indicates that the dataset size of RTLLM is less than 1 of that for CodeParrot. Therefore, This paper mainly focuses on text-to-code generation. arXiv:2502.13921v2 [cs.LG] 5 Mar 2025 ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan Gai et al. the quantity of training data available for hardware design is significantly lower than for software programming languages. Inability of utilizing learned knowledge from pre-trained coding LLMs.\n\n--- Segment 4 ---\nthe quantity of training data available for hardware design is significantly lower than for software programming languages. Inability of utilizing learned knowledge from pre-trained coding LLMs. Pre-trained coding LLMs are primarily trained on software programming languages, which differ significantly in semantics and syntax from HDLs. Therefore, the knowledge ac- quired during pre-training cannot be directly applied to hardware code generation, compounding the data scarcity issue. Cost of HDL generation using LLMs. Figure 2 illustrates the number of tokens required for generating identical hardware designs using High-Level Synthesis (HLS) versus HDL. It shows that HDL implementations require approximately 3 4 times more tokens than HLS designs, making HLS-based design gener- ation a more sustainable solution considering the latency, energy, and monetary costs associated with LLM inference. To address the aforementioned issues, this paper proposes an LLM-assisted framework for generating HLS-based hardware de- signs. By crawling HLS designs from open-source Github reposito- ries, we collect a dataset to facilitate the fine-tuning of pre-trained LLM for the downstream HLS code generation. The benefits of generating HLS code are two folds: i) Given that HLS shares main semantics and syntax with C C , the coding knowledge acquired during the pre-training phase of the LLMs can be effectively utilized for hardware design. This compatibility also reduces the learning curve and dataset requirements for fine-tuning, as the additional knowledge needed for HLS is less than for traditional HDL coding. ii) The number of tokens required to generate HLS code is lower compared to HDLs, rendering our approach more cost-effective and energy-efficient than previous methodologies. To further improve the quality of the generated designs, the framework incorporates debugging feedback loops and a chain-of-thought enhancement mechanism, systematically integrating detected bugs back into the input for iterative refinement. Overall, our contributions can be summarized as follows: Finetuning pre-trained code language models for HLS-based hard- ware generation, using a collected dataset with over 40, 000 data entries, each containing a text prompt and the corresponding HLS-based hardware design (Section 3). Developing a framework that automatically produces HLS de- signs from input prompts, with an end-to-end evaluation of the syntax and functionality correctness (Section 4.1).\n\n--- Segment 5 ---\nOverall, our contributions can be summarized as follows: Finetuning pre-trained code language models for HLS-based hard- ware generation, using a collected dataset with over 40, 000 data entries, each containing a text prompt and the corresponding HLS-based hardware design (Section 3). Developing a framework that automatically produces HLS de- signs from input prompts, with an end-to-end evaluation of the syntax and functionality correctness (Section 4.1). Integrating multiple optimization techniques such as feedback loops and chain-of-thought techniques, which improve the pass rate for syntax and functionality (Section 4.2 Section 4.3). 2 BACKGROUND AND RELATED WORK 2.1 LLM-Assisted Software Engineering Based on the modality of inputs and outputs, language models for software engineering can be categorized into several downstream code tasks [32] such as text-to-code (code generation synthesis [16]), code-to-text (code summarization [10]), and code-to-pattern pro- cessing (defect detection [21]). This paper focuses on code gener- ation that aims at producing code from natural language descrip- tions prompts. To facilitate the development of code generation This paper focues on C-based HLS.\n\n--- Segment 6 ---\nThis paper focuses on code gener- ation that aims at producing code from natural language descrip- tions prompts. To facilitate the development of code generation This paper focues on C-based HLS. static void compute_mult_16bit( hls::stream uint16_t inStream1, hls::stream uint16_t inStream2, hls::stream uint16_t outStream, int vSize) { execute: for (int i 0; i vSize; i ) { pragma HLS LOOP_TRIPCOUNT min size max size outStream (inStream1.read() inStream2.read()); } } HLS-based design module multi_16bit( input clk, input rst_n, input start, input [15:0] ain, input [15:0] bin, output [31:0] yout, output done ); reg [15:0] areg; reg [15:0] breg; reg [31:0] yout_r; reg done_r; reg [4:0] i; always (posedge clk or negedge rst_n) if (!rst_n) i 5'd0; else if (start i 5'd17) i i 1'b1; else if (!start) i 5'd0; always (posedge clk or negedge rst_n) if (!rst_n) done_r 1'b0; else if (i 5'd16) done_r 1'b1; else if (i 5'd17) done_r 1'b0; assign done done_r; always (posedge clk or negedge rst_n) begin if (!rst_n) begin areg 16'h0000; breg 16'h0000; yout_r 32'h00000000; end else if (start) begin if (i 5'd0) begin areg ain; breg bin; end else if (i 5'd0 i 5'd17) begin if (areg[i-1]) yout_r yout_r ({16'h0000, breg} (i-1)); end end end assign yout yout_r; endmodule Verilog-based design 0 20 40 60 80 100 HLS Verilog normalized token usage Token Comparison Figure 2: HLS-based and Verilog-based programs.\n\n--- Segment 7 ---\nTo facilitate the development of code generation This paper focues on C-based HLS. static void compute_mult_16bit( hls::stream uint16_t inStream1, hls::stream uint16_t inStream2, hls::stream uint16_t outStream, int vSize) { execute: for (int i 0; i vSize; i ) { pragma HLS LOOP_TRIPCOUNT min size max size outStream (inStream1.read() inStream2.read()); } } HLS-based design module multi_16bit( input clk, input rst_n, input start, input [15:0] ain, input [15:0] bin, output [31:0] yout, output done ); reg [15:0] areg; reg [15:0] breg; reg [31:0] yout_r; reg done_r; reg [4:0] i; always (posedge clk or negedge rst_n) if (!rst_n) i 5'd0; else if (start i 5'd17) i i 1'b1; else if (!start) i 5'd0; always (posedge clk or negedge rst_n) if (!rst_n) done_r 1'b0; else if (i 5'd16) done_r 1'b1; else if (i 5'd17) done_r 1'b0; assign done done_r; always (posedge clk or negedge rst_n) begin if (!rst_n) begin areg 16'h0000; breg 16'h0000; yout_r 32'h00000000; end else if (start) begin if (i 5'd0) begin areg ain; breg bin; end else if (i 5'd0 i 5'd17) begin if (areg[i-1]) yout_r yout_r ({16'h0000, breg} (i-1)); end end end assign yout yout_r; endmodule Verilog-based design 0 20 40 60 80 100 HLS Verilog normalized token usage Token Comparison Figure 2: HLS-based and Verilog-based programs. with language models, various datasets, approaches, and pre-trained models have been introduced over the past decade.\n\n--- Segment 8 ---\nstatic void compute_mult_16bit( hls::stream uint16_t inStream1, hls::stream uint16_t inStream2, hls::stream uint16_t outStream, int vSize) { execute: for (int i 0; i vSize; i ) { pragma HLS LOOP_TRIPCOUNT min size max size outStream (inStream1.read() inStream2.read()); } } HLS-based design module multi_16bit( input clk, input rst_n, input start, input [15:0] ain, input [15:0] bin, output [31:0] yout, output done ); reg [15:0] areg; reg [15:0] breg; reg [31:0] yout_r; reg done_r; reg [4:0] i; always (posedge clk or negedge rst_n) if (!rst_n) i 5'd0; else if (start i 5'd17) i i 1'b1; else if (!start) i 5'd0; always (posedge clk or negedge rst_n) if (!rst_n) done_r 1'b0; else if (i 5'd16) done_r 1'b1; else if (i 5'd17) done_r 1'b0; assign done done_r; always (posedge clk or negedge rst_n) begin if (!rst_n) begin areg 16'h0000; breg 16'h0000; yout_r 32'h00000000; end else if (start) begin if (i 5'd0) begin areg ain; breg bin; end else if (i 5'd0 i 5'd17) begin if (areg[i-1]) yout_r yout_r ({16'h0000, breg} (i-1)); end end end assign yout yout_r; endmodule Verilog-based design 0 20 40 60 80 100 HLS Verilog normalized token usage Token Comparison Figure 2: HLS-based and Verilog-based programs. with language models, various datasets, approaches, and pre-trained models have been introduced over the past decade. Due to the lack of model capability, the early-stage methods [11, 16] of code generation mainly focus on a few programming lan- guages such as Python or Java.\n\n--- Segment 9 ---\nwith language models, various datasets, approaches, and pre-trained models have been introduced over the past decade. Due to the lack of model capability, the early-stage methods [11, 16] of code generation mainly focus on a few programming lan- guages such as Python or Java. Subsequently, with the increasing computational power, larger datasets are introduced to train models for multiple general-purpose programming languages. CodeXGLUE [19] presents a comprehensive code dataset consisting of different code tasks such as clone detection and code repair. HumanEval [5] dataset together with the code model CodeX marks as a milestone by using pre-trained LLMs for code generation. The promising capability shown by CodeX sparks significant academic and industrial in- terests in developing LLM-assisted code generation. Larger code datasets, such as StarCoder [14] and CodeParrot [27], and LLMs, including Code-LLaMA [22] and CodeFuse [7], are open-sourced in this community. However, most of these recent efforts focus on software programming languages. 2.2 Automated Hardware Design Generation Building on the success of LLM-assisted software programming, recent studies have explored using language models for automated hardware generation. Since the data is the key to training LLMs for hardware code generation, multiple HDL datasets have been introduced recently. Thakur et al. present Verigen [25] dataset that contains 17 hardware designs with 0.3K lines of HDL code. To in- crease the diversity of hardware designs for training and evaluation, Lu et al. open-source a larger benchmark consisting of 30 designs with more than 2.5K lines. Sourced from HDLBits , Verilogeval [18] introduces larger datasets with 156 problems. These open-sourced datasets provide public benchmarks for text-to-HDL generation. The evaluation metrics of LLM-assisted hardware code genera- tion focus on three aspects: i) syntax, ii) functionality, and iii) qual- ity. In previous literature [5, 18], syntax correctness and functional- ity are measured using which represents whether any of 𝑘generated code samples can pass the syntax check of synthesis tools or functional unit tests. The quality usually is de- fined as power, performance, and area of the generated hardware, collectively reflect the capability of the code generation methods.\n\n--- Segment 10 ---\nIn previous literature [5, 18], syntax correctness and functional- ity are measured using which represents whether any of 𝑘generated code samples can pass the syntax check of synthesis tools or functional unit tests. The quality usually is de- fined as power, performance, and area of the generated hardware, collectively reflect the capability of the code generation methods. Aiming at improving these metrics, existing approaches [18, 20, 25] fine-tune pre-trained LLMs on the downstream task with opti- mized sampling schemes. These LLMs are mainly pre-trained using Exploring Code Language Models for Automated HLS-based Hardware Generation ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan Template of Design Point Design Description: High level description of the design details. Instruction Prompt: Specify coding language and requirements. Reference Design: Canonical HLS program. Figure 3: Template of design points. software programming languages. RTLFixer [26] introduces an auto- mated framework that adopts retrieval-augmented generation [12] and ReAct prompting [31] to enable LLM-assisted debugging for RTL designs. LLM-VeriPPA [1] enhances the code generation of RTL using a two-stage refinement process to progressively improve syntax, functionality, and hardware performance. However, these approaches focus on low-level hardware languages instead of HLS. In this work, we take the first step to investigate the HLS code gen- eration with LLM. Since HLS shares similar semantics and syntax with programming languages commonly used during LLM pre- training, this paper explores whether HLS is better than low-level hardware languages for automated hardware design generation. Although code generation with feedback and CoT prompting is not new in the literature of coding language models, our experi- mental results, insights, benchmark, and evaluation infrastructure specific to LLM-assisted HLS design offer valuable contributions to the future development of automated hardware generation. 3 HLS GENERATION BENCHMARK 3.1 Format of Design Points Following the practices of Python benchmark HumanEval [5] and Verilog dataset VerilogEval [18], each design point has three com- ponents: i) user instruction prompts, ii) design descriptions and iii) reference designs.\n\n--- Segment 11 ---\nAlthough code generation with feedback and CoT prompting is not new in the literature of coding language models, our experi- mental results, insights, benchmark, and evaluation infrastructure specific to LLM-assisted HLS design offer valuable contributions to the future development of automated hardware generation. 3 HLS GENERATION BENCHMARK 3.1 Format of Design Points Following the practices of Python benchmark HumanEval [5] and Verilog dataset VerilogEval [18], each design point has three com- ponents: i) user instruction prompts, ii) design descriptions and iii) reference designs. Figure 3 shows the standardized format template, with each data point stored as a JSONL following the Alpaca format. The default user instruction prompt is Generate HLS code with the following instructions:, which can be enhanced using the chain-of- thought (COT) prompting technique as detailed in Section 4.2. 3.2 Dataset Collection We collect 52 HLS-based designs from open-source repositories, including HLSyn [3] and ML4Accel . These designs are split into training and testing sets at a 4:1 ratio and fall into five categories: Matrix and Linear Algebra Operations: Includes sparse matrix- vector multiplications, dense matrix-matrix multiplication, array transformation and stencil computations. Scientific Simulations: Methods for solving physical and math- ematical problems such as heat distribution and electromagnetic simulations. Statistical Computations: Calculations of statistical metrics from datasets. Iterative Methods: Techniques for solving equations using iter- ative approaches. Other Computational Kernels: Specialized computational tasks like molecular dynamics and interactions, encryption algorithms, and optical flow. Each of these 52 designs is associated with different combinations of programs such as PIPELINE, PARALLEL and TILE. We filter out the HLS programs that are invalid, resulting in a collection of over 42, 000 HLS programs. The whole dataset is split into training and test sets for supervised fine-tuning and evaluation, respectively. 3.3 Generation of Design Description Given that the dataset encompasses over 42,000 HLS programs, man- ually generating design descriptions for each program is both labor- intensive and time-consuming. Inspired by both HumanEval [5] and Verilog, we utilize ChatGPT (version 3.5 and 4) to automate the creation of design descriptions for the datasets.\n\n--- Segment 12 ---\n3.3 Generation of Design Description Given that the dataset encompasses over 42,000 HLS programs, man- ually generating design descriptions for each program is both labor- intensive and time-consuming. Inspired by both HumanEval [5] and Verilog, we utilize ChatGPT (version 3.5 and 4) to automate the creation of design descriptions for the datasets. We append each HLS program with this base prompt when utilizing ChatGPT to generate the corresponding design descriptions. Both the reference design and its generated description are stored in JSON format, ad- hering to the structure outlined in Section 3.1. This method ensures streamlined and consistent documentation of design descriptions across the dataset. Following the practice of [18], we provide two versions of prompts for each HLS program in the test set: Machine- Gen and HumanRefine. The MachineGen version comprises prompts and instructions generated by GPT-based models without human modifications. In contrast, the HumanRefine includes manually re- fined prompts to ensure more concise and human-like instructions. 3.4 Assessment Infrastructure We provide evaluation infrastructure for both syntax and function- ality. For syntax verification, we use the GCC compiler with the "-fsyntax-only" option. It allows us to verify the syntax without the overhead of compiling the code, thereby enhancing time and space efficiency. Regarding functionality correctness, we design unit tests tailored for each example in the test dataset. Each test case is associated with its corresponding source_file . To achieve this goal, we modified the original test JSONL file to add another attribute source_file . The testing process involves executing both the generated code and the original source code to compare their outputs. For instance, if the outputs from both codes consist of ma- trices, we conduct a targeted comparison. This is done by selecting specific positions within the matrices from both outputs at random and verifying if they match. 4 AUTOMATED HARDWARE GENERATION 4.1 Framework Overview An overview of our proposed framework is depicted in Figure 4. The framework comprises two main stages: i) model fine-tuning and ii) iterative code generation. The final output is an HLS-based program that can be synthesized into the corresponding hardware design. While focused on Vivado-HLS, our framework is adaptable to any HLS language with appropriate datasets.\n\n--- Segment 13 ---\nThe final output is an HLS-based program that can be synthesized into the corresponding hardware design. While focused on Vivado-HLS, our framework is adaptable to any HLS language with appropriate datasets. In the model fine-tuning stage, our framework initiates by re- trieving coding LLMs from open-source repositories, such as Code- Llama and Start-Coder . Then, supervised fine-tuning is conducted on these pre-trained models using the HLS training data (Sec- tion 3.2). We adopt axolotl to perform the fine-tuning, allowing ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan Gai et al. HLS Source Code Diverse High-quality Code Kernels Corresponding Prompts Filter Code-to-Text LLM Data: x Label: y Proposed Dataset Split Training Dataset Test Dataset Pre-trained Text-to- Code LLM Supervised Fine-tuning Test Chain- of- Thought Completed Codes Syntax Check P O Feedback with Located Errors Functionality Check Feedback with Functional Defects O P HLS-based programs ① Model Fine-Tuning ②Iterative Code Generation Dataset Preparation Fine-tuned Text-to- Code LLM Unit Tests Generated HLS Code Vivado Synthesis Input Prompt Design Description: XXXXXX. Instruction Prompt: XXXXX. Compiler (Basic Syntax Check) Figure 4: An overview of our proposed framework. Chain-of-Thought Prompt for Generating HLS Design Instruction Prompt: Let's think step by step. First, Consider the characteristics of FPGA. Second, Determine the program structure. Third, Write code logic. Fourth, Consider data types and interfaces. Figure 5: Chain-of-thought prompts for HLS generation. for customization of models and training parameters, such as learn- ing rate, batch size, and epoch number to fit specific scenarios and available resources. In the second stage, we employ the fine-tuned LLM for iterative code generation. The process begins with initial inputs consisting of user instruction prompts and design descriptions. To enhance the quality of the generated HLS designs, we incorporate a chain- of-thought optimization technique (Section 4.2) into the instruction prompts.\n\n--- Segment 14 ---\nThe process begins with initial inputs consisting of user instruction prompts and design descriptions. To enhance the quality of the generated HLS designs, we incorporate a chain- of-thought optimization technique (Section 4.2) into the instruction prompts. The code generation then proceeds with a feedback loop (Section 4.3) that iteratively improves the correctness of the HLS de- signs. This iterative process continues until a refined HLS program is generated as the final output. Users can specify the number of iterations, providing the flexibility to navigate this trade-off accord- ing to their specific needs, with more iterations typically yielding higher quality at increased runtime expense. 4.2 Chain-of-Thought Previous studies indicate that the quality of generated content is significantly influenced by the instructional prompt [34]. The Chain-of-Thought (CoT) [29] technique has proven simple and effective for enhancing the performance of LLMs across a wide range of tasks, including arithmetic, commonsense, and symbolic reasoning [29]. Although initially, COT yielded a modest 0.82 point increase in the metric in code generation, this improvement was substantially augmented through structured prompting [13]. In this paper, we investigate the effect of CoT in generating HLS-based hardware designs. Figure 5 illustrates the CoT prompt structured for HLS code generation. The prompt guides a systematic approach through sev- eral targeted steps: understanding FPGA characteristics, defining program structure, developing logic, selecting data types and inter- faces, before finalizing the code. This structured process ensures thorough consideration of each key aspect to optimize the HLS code generation. 4.3 Two-Step Feedback Loops Code generation with feedback loop has shown promising results in previous literature [1, 17, 24, 28]. This paper investigates its impact on HLS code generation using a two-step feedback loop tailored for automated hardware generation, focusing on HLS-related feed- back. At each iteration, the framework evaluates the syntax and functional correctness of the generated HLS program. The located syntax error and functional defects are then fed back into the input prompts as additional information for subsequent code generation. In the first step, syntax feedback is provided using the GCC compiler with the -fsyntax-only option, as specified in 3.4. It captures the syntax errors without fully compiling the code. It al- lows rapid identification including the types and locations, and precise error mapping for targeted corrections.\n\n--- Segment 15 ---\nIt captures the syntax errors without fully compiling the code. It al- lows rapid identification including the types and locations, and precise error mapping for targeted corrections. If the syntax check passes, our framework proceeds to the second step by executing predefined unit tests to compare the outputs of the generated and the original source code. Functional defects are recorded and added to the prompts for the next iteration. This two-step feedback loop continues for user-specified iterations, providing flexibility to bal- ance the trade-off between design quality and runtime cost. 5 EVALUATION 5.1 Experimental Setup In our evaluation, we adopt Code-Llama-7B as the pre-trained model for fine-tuning, employing the low-rank-adaption (QLoRA) [6, 9] technique for faster training and lower memory consumption. Key configurations include loading the model in 8-bit, a sequence length of 4096, sample packing, and padding to sequence length. We set the warmup steps to 100, with a gradient accumulation of 4 steps, Exploring Code Language Models for Automated HLS-based Hardware Generation ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan a micro-batch size of 4, and an inference batch size of 2. For both syntax and functionality checks, we measure accuracy as metrics. In the ablation study from Section 5.2 to Section 5.6, we adopt MachineGen for evaluation. Experiments are conducted on a server with four NVIDIA L20 GPUs (48 GB each), an 80 vCPU Intel Xeon Platinum 8457C, and 100GB of RAM. This setup ensures sufficient computational power and memory to handle the intensive demands of fine-tuning and inference efficiently, especially for long data sequences in the feedback loop experiment. 5.2 Effect of Supervised Finetuning Our first ablation study investigates the effect of the model fine- tuning. We evaluated the performance based on both syntax and functionality checks. As shown in Figure 6(a), the results demon- strate that the finetuning dramatically increases syntax correctness from 54.85 to 88.44 . More importantly, the impact of finetuning is even more pronounced in the functionality evaluation, where the non-finetuned model failed to achieve any correct functional- ity test, but the accuracy is improved to 53.20 in the finetuned model.\n\n--- Segment 16 ---\nAs shown in Figure 6(a), the results demon- strate that the finetuning dramatically increases syntax correctness from 54.85 to 88.44 . More importantly, the impact of finetuning is even more pronounced in the functionality evaluation, where the non-finetuned model failed to achieve any correct functional- ity test, but the accuracy is improved to 53.20 in the finetuned model. These enhancements highlight the critical role of finetuning in producing not only syntactically correct but also functionally viable codes, which demonstrates the benefits of finetuning LLMs for hardware design in the HLS code generation task. 5.3 Effect of Chain-of-Thought Prompting To assess the effect of the chain-of-thought (CoT) technique, we perform both syntax and functionality evaluation on the fine-tuned model with and without the use of CoT. As indicated in Figure 6(b), incorporating CoT leads to a noticeable improvement in both met- rics. Specifically, syntax correctness increases from 88.44 to 94.33 , and functionality score rises from 53.20 to 61.45 . The result demonstrates the effectiveness of CoT in enhancing the reasoning capability, thereby improving its overall performance. 5.4 Effect of Feedback Loops Our two-step feedback loop provides both syntax and functionality feedback. We evaluate the impact of these feedback loops with dif- ferent numbers of iterations, ranging from 0 to 2.The results, shown in Figure Figure 7 and Figure 8, indicate that both syntax and func- tionality feedback loops significantly improve model performance, especially when combined with COT prompting. The initial feed- back loop yields substantial accuracy improvements in both syntax correctness and functionality evaluation, though the second loop shows diminishing returns.Syntax feedback loops enhance both syntax correctness and functionality performance, suggesting that iterative refinement is particularly effective for complex tasks. Sim- ilarly, functionality feedback loops not only improve functionality checks but also boost syntax accuracy, indicating that enhance- ments in functional understanding contribute to better syntactic performance. 5.5 Time Cost and Hardware Performance Figure 9 shows the time cost for generating 120 data entries under different conditions, measuring the impact of CoT and feedback loops. Without a feedback loop, CoT significantly reduces the time.\n\n--- Segment 17 ---\n5.5 Time Cost and Hardware Performance Figure 9 shows the time cost for generating 120 data entries under different conditions, measuring the impact of CoT and feedback loops. Without a feedback loop, CoT significantly reduces the time. 54.85 88.44 0 53.20 0.00 20.00 40.00 60.00 80.00 100.00 w o Finetune with Finetune accuracy synax check functionality check 88.44 94.33 53.20 61.45 0.00 20.00 40.00 60.00 80.00 100.00 w o COT with COT accuracy synax check functionality check (a) Effect of fine-tuning (b) Effect of chain-of-thought prompting Figure 6: Effect of fine-tuning and chain-of-thought. 50.00 60.00 70.00 80.00 90.00 100.00 w o feedback syntax feedback (max 1 loop) syntax feedback (max 2 loops) accuracy syntax check w o COT syntax check with COT functionality check w o COT functionality check with COT Figure 7: Effect of syntax feedback loop. 50.00 60.00 70.00 80.00 90.00 100.00 w o feedback functionality feedback (max 1 loop) functionality feedback (max 2 loops) accuracy syntax check w o COT syntax check with COT functionality check w o COT functionality check with COT Figure 8: Effect of functionality feedback loop. Adding a syntax feedback loop increases the time, but CoT continues to notably decrease the duration. The functionality feedback loop is the most time-consuming, though CoT still provides a notable re- duction, albeit less dramatic. This demonstrates CoT s effectiveness in reducing operational times across varying complexities. For the test set, we evaluate the latency and resource consump- tion of the generated HLS designs using a Xilinx VCU118 as our target FPGA, with a clock frequency of 200MHz and Xilinx Vi- vado 2020.1 for synthesis. As shown in Table 1, all HLS designs demonstrate reasonable performance, with BRAM usage consis- tently remained at zero due to the design scale. 5.6 Effect of Task Complexity We analyze the effects of code complexity on the performance of fine-tuning our language model with CoT prompting and tested without the use of any feedback loops during inference.\n\n--- Segment 18 ---\nAs shown in Table 1, all HLS designs demonstrate reasonable performance, with BRAM usage consis- tently remained at zero due to the design scale. 5.6 Effect of Task Complexity We analyze the effects of code complexity on the performance of fine-tuning our language model with CoT prompting and tested without the use of any feedback loops during inference. We catego- rize MachineGen into three classes according to their code complex- ity: easy, medium, and difficult. The results shown in the Table 2 indicates a clear trend: as the complexity of the generated code increases, both syntax and functionality correctness rates decline. ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan Gai et al. 7 9 11 5 6 9 0 2 4 6 8 10 12 w o feedback loop syntax loop (max 1) functionality loop (max 1) Inference Time averaged by120 Data Points (second) w o COT with COT Figure 9: Time cost of code generation. Table 1: Latency and resource usage of LLM-generated de- signs synthesized on a VCU118 FPGA. Latency (ms) LUTs Registers DSP48s BRAMs Available - 1182240 2364480 6840 4320 ellpack 0.304 1011 1079 11 0 syrk 21.537 1371 1621 19 0 syr2k 40.626 1572 1771 19 0 stencil2d 1.368 287 123 3 0 trmm-opt 15.889 1262 1239 11 0 stencil3d 21.537 1173 1271 20 0 symm 24.601 1495 1777 19 0 symm-opt 16.153 1361 1608 19 0 symm-opt-medium 579.0 2223 2245 22 0 This outcome could be attributed to several factors. First, more complex code inherently presents more challenges in maintaining syntactic integrity and functional accuracy. Second, the absence of feedback loops in the inference phase may have limited the model s ability to self-correct emerging errors in more complicated code generations. Table 2: Performance across different complexity levels. Test Set Syntax Check Functionality Easy 96.67 63.33 Medium 96.67 53.33 Difficult 90 53.33 5.7 Analysis of MachineGen and HumanRefine Table 3: Performance on MachineGen and HumanRefine.\n\n--- Segment 19 ---\nTable 2: Performance across different complexity levels. Test Set Syntax Check Functionality Easy 96.67 63.33 Medium 96.67 53.33 Difficult 90 53.33 5.7 Analysis of MachineGen and HumanRefine Table 3: Performance on MachineGen and HumanRefine. Test Set Syntax Check Functionality Check MachineGen 93.83 62.24 HumanRefine 47.29 21.36 As shown in Table 3, this section compares the performance of our model on MachineGen and HumanRefine test sets. Our findings reveal that the performance on the HumanRefine is significantly lower than on the MachineGen. This disparity suggests that the model is more adept at handling machine-generated prompts. The primary reasons for this are: the model s training data bias to- wards machine-generated prompts, the increased complexity and nuanced nature of human-generated prompts, and the conciseness and clarity of human-generated prompts that often omit repetitive or explicit details found in machine-generated prompts, making it harder for the model to generate syntactically and functionally correct code. 5.8 Thoughts, Insights, and Limitations 1. HLS versus HDL for AI-assisted code generation: The se- lection of programming language for hardware code generation should mainly depend on two factors: Quality of Generated Hardware Design: The evaluation of hard- ware design s quality includes syntax correctness, functionality, and hardware performance. Since HLS shares similar semantics and syntax with programming languages commonly used during LLM pre-training, this work demonstrates that the LLM-assisted code generation for HLS has the potential to achieve high syntax and functional correctness in hardware designs. While this work does not leverage hardware performance as feedback for design generation, it identifies this aspect as a key direction for future research and enhancements. Runtime Cost of Hardware Generation: Although HLS-based de- signs typically require fewer tokens compared to HDL during the code generation phase suggesting potentially lower costs the overall runtime costs associated with HLS synthesis must also be considered. A more comprehensive quantitative comparison of these runtime costs is planned for our future work. 2. Input instructions and datasets are crucial: The fine-tuning of pre-trained LLMs on HLS dataset can bring a significant improve- ment in the design quality, echoing findings from previous studies on Verilog code generation [25]. Additionally, during our evaluation, we found that employing simple CoT prompting largely improves hardware design quality.\n\n--- Segment 20 ---\nInput instructions and datasets are crucial: The fine-tuning of pre-trained LLMs on HLS dataset can bring a significant improve- ment in the design quality, echoing findings from previous studies on Verilog code generation [25]. Additionally, during our evaluation, we found that employing simple CoT prompting largely improves hardware design quality. This result contrasts with the application of CoT in general-purpose programming languages, where a spe- cialized form of CoT is necessary [13]. Therefore, future efforts for further enhancement can focus on collecting high-quality datasets and exploring better refinement of input prompts. 3. Limitations: At the time of this research, more advanced rea- soning models, such as DeepSeek-R1 [8], were not available for evaluation. Additionally, test-time scaling approaches [30] could be incorporated to further enhance performance in the future. More- over, we observe that the diversity of hardware designs in the benchmark is limited, which may impact the generalizability of our findings. We intend to address these limitations in our future work. 6 CONCLUSION This paper explores automating hardware generation with code language models and High-Level Synthesis (HLS). We aim to inves- tigate the suitability of HLS over low-level hardware description languages for hardware design generation. To facilitate this, we propose benchmarks and code infrastructures for evaluating LLM- assisted HLS design generation. Our experimental findings reveal that, with the integration of advanced optimizations such as feed- back loops and chain-of-thought techniques, LLM-assisted HLS code generation shows substantial promise in designing complex hardware with high levels of syntax and functional correctness. Exploring Code Language Models for Automated HLS-based Hardware Generation ASP-DAC 25, January 20 23, 2025, Tokyo Odaiba Miraikan, Japan REFERENCES [1] Anonymous Authors. 2024. LLM-VeriPPA: Power, Performance, and Area-aware Verilog Code Generation and Refinement with Large Language Models. https: openreview.net pdf?id nZL6S0b-HcI [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In IEEE CVF Conference on Computer Vision and Pattern Recognition. 18208 18218.\n\n--- Segment 21 ---\nIn IEEE CVF Conference on Computer Vision and Pattern Recognition. 18208 18218. [3] Yunsheng Bai, Atefeh Sohrabizadeh, Zongyue Qin, Ziniu Hu, Yizhou Sun, and Ja- son Cong. 2023. Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs. Advances in Neural Information Processing Systems 36 (2023), 45288 45299. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877 1901. [5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024). [7] Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, Hongwei Chen, Liang Chen, et al. 2023. Codefuse-13b: A pretrained multi-lingual code large language model. arXiv preprint arXiv:2310.06266 (2023). [8] Daya Guo et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.\n\n--- Segment 22 ---\n[9] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [10] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Sum- marizing source code using a neural attention model. In 54th Annual Meeting of the Association for Computational Linguistics 2016. Association for Computational Linguistics, 2073 2083. [11] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2018. Map- ping language to code in programmatic context. arXiv preprint arXiv:1808.09588 (2018). [12] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459 9474. [13] Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023. Structured chain-of-thought prompt- ing for code generation. arXiv preprint arXiv:2305.06599 (2023). [14] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161 (2023). [15] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022.\n\n--- Segment 23 ---\n[15] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science 378, 6624 (2022), 1092 1097. [16] Wang Ling, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočisk y, Andrew Senior, Fumin Wang, and Phil Blunsom. 2016. Latent predictor networks for code generation. arXiv preprint arXiv:1603.06744 (2016). [17] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. 2023. Rltf: Reinforcement learning from unit test feedback. arXiv preprint arXiv:2307.04349 (2023). [18] Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing Ren. 2023. Ver- ilogeval: Evaluating large language models for verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 1 8. [19] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambro- sio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. 2021. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664 (2021). [20] Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. 2024. RTLLM: An open-source benchmark for design rtl generation with large language model. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 722 727. [21] Baishakhi Ray, Vincent Hellendoorn, Saheel Godhane, Zhaopeng Tu, Alberto Bacchelli, and Premkumar Devanbu. 2016. On the" naturalness" of buggy code.\n\n--- Segment 24 ---\n2016. On the" naturalness" of buggy code. In Proceedings of the 38th International Conference on Software Engineering. 428 439. [22] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiao- qing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023). [23] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. 2022. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 4 (2022), 4713 4726. [24] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K Reddy. 2023. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816 (2023). [25] Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan- Gavitt, Ramesh Karri, and Siddharth Garg. 2023. Verigen: A large language model for verilog code generation. ACM Transactions on Design Automation of Electronic Systems (2023). [26] YunDa Tsai, Mingjie Liu, and Haoxing Ren. 2023. Rtlfixer: Automatically fixing rtl syntax errors with large language models. arXiv preprint arXiv:2311.16543 (2023). [27] Lewis Tunstall, Leandro Von Werra, and Thomas Wolf. 2022. Natural language processing with transformers. " O Reilly Media, Inc.". [28] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, and Qun Liu. 2022. Compilable neural code generation with compiler feedback. arXiv preprint arXiv:2203.05132 (2022).\n\n--- Segment 25 ---\nCompilable neural code generation with compiler feedback. arXiv preprint arXiv:2203.05132 (2022). [29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 24824 24837. [30] Sean Welleck et al. 2024. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838 (2024). [31] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629 (2022). [32] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi Gong, Hang Yu, Jianguo Li, and Rui Wang. 2023. Unifying the perspectives of nlp and software engineering: A survey on language models for code. arXiv preprint arXiv:2311.07989 (2023). [33] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [34] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning. PMLR, 12697 12706. [35] Zangwei Zheng, Xiangyu Peng, and Yang You. 2024. Open-Sora: Democratizing Efficient Video Production for All.\n\n