=== ORIGINAL PDF: 2505.08599v1_MINIMALIST_switched-capacitor_circuits_for_efficie.pdf ===\n\nRaw text length: 26881 characters\nCleaned text length: 26727 characters\nNumber of segments: 17\n\n=== CLEANED TEXT ===\n\nMINIMALIST: switched-capacitor circuits for efficient in-memory computation of gated recurrent units Sebastian Billaudelle , Laura Kriener , Filippo Moro, Tristan Torchet, Melika Payvand Institute of Neuroinformatics, University of Zürich and ETH Zürich contributed equally This preprint represents an early and preliminary version of the final manuscript and will be updated regularly. Recurrent neural networks (RNNs) have been a long-stand- ing candidate for processing of temporal sequence data, especially in memory-constrained systems that one may find in embedded edge computing environments. Recent advances in training paradigms have now inspired new generations of efficient RNNs. We introduce a streamlined and hardware-compatible architecture based on minimal gated recurrent units (GRUs), and an accompanying effi- cient mixed-signal hardware implementation of the model. The proposed design leverages switched-capacitor circuits not only for in-memory-computing (IMC), but also for the gated state updates. The mixed-signal cores rely solely on commodity circuits consisting of metal capacitors, trans- mission gates, and a clocked comparator, thus greatly facilitating scaling and transfer to other technology nodes. We benchmark the performance of our architecture on time series data, introducing all constraints required for a direct mapping to the hardware system. The direct compat- ibility is verified in mixed-signal simulations, reproducing data recorded from the software-only network model. 1 Introduction Modeling temporal signals is a core challenge in AI, with applications ranging from speech and language to sensory processing and control. RNNs, particularly gated variants like long short-term memories (LSTMs) (Hochreiter 1997) and GRUs (Cho et al. 2014), emerged as the natural choice for such tasks due to their ability to maintain an internal memory and process inputs se- quentially. They offered a principled way to capture tem- poral dependencies across varying time scales. However, the advent of the Transformer architecture (Vaswani 2017) marked a shift, its ability to train in parallel across time steps led to significant efficiency gains and performance improvements. Despite this, the quadratic complexity of the attention mechanism in Transformers remains a bottleneck for deployment in edge and low- power settings. To address this, newer RNN variants such as the minGRU (Feng et al. 2024) have been developed to support parallel training while preserving the constant-time, local-state inference characteristic of classical RNNs. These algorithmic simplifications now also open the door to efficient hardware implementa- tions for edge computing scenarios. One of the key principles behind the design of effi- cient machine learning accelerators is the reduction of data movement (Bavikadi et al. 2020). This typically manifests itself in IMC, the collocation of memory and computing elements especially for matrix-vector multiplications, as found for example in the linear pro- jections between neural network layers. In this space, analog and mixed-signal implementations typically out- perform digital solutions in terms of energy efficiency, especially in a low-precision regime (Sun et al. 2023). This advantage is, however, often diminished by a high complexity and energy footprint of peripheral circuits. A successful mixed-signal implementation must thus attempt to reduce the frequency and resolution of con- verting between the analog and digital domains, simplify operations to mostly linear and hardware-amenable arithmetics, and reduce the impact of data movement. RNNs, in particular, often require rather complex state update arithmetics and involve a dense recurrent projec- tion of hidden states within a layer. This induces high bandwidth requirements and is typically not compatible to the low-resolution regime where analog implementa- tions perform best. Here, we present a streamlined RNN architecture that addresses above challenges. It adopts the diagonal-only recurrent projections of contemporary models (Orvieto et al. 2023, Feng et al. 2024), reduces inter-layer commu- nication by enforcing binary output activations, and reduces the complexity of gated state updates by resort- ing to simplified internal activation functions. Alongside the resulting model, we also introduce a highly effi- cient mixed-signal implementation thereof. It relies on switched-capacitor circuits to realize both the matrix- vector multiplications involved in the linear projections between layers and the recurrent state updates. Tran- sitions between the analog and digital domains are reduced to a bare minimum and the design can refrain from continuously biased analog circuits such as voltage buffers. The design only relies on metal-oxide-metal (MOM) capacitors, transmission gates, static random-ac- cess memory (SRAM) bitcells, and a simple comparator circuit, thus enabling straight-forward scaling and an optimal transfer across technology nodes. Both architecture and mixed-signal implementation are the result of a stringent co-design process, which allowed to optimally match circuits to the underlying RNN functions. And vice versa, the simplifications to the network model were directly informed by constraints 1 𝒙 GRU block GRU block GRU block 𝒚 𝜎ℎ 𝜎ℎ 𝜎ℎ 𝑾𝑧𝒙 𝒃𝑧 𝑾ℎ𝒙 𝒃ℎ 𝒛 𝒉 (1 𝒛) 𝒉𝑡 1 𝜎𝑧 𝜎 ℎ Figure 1: The MINIMALIST architecture consists of a feedforward network of simplified GRU blocks, inter- leaved with binary activation functions. imposed by the microelectronic implementation. The resulting design, to the best of our knowledge the first switched-capacitor implementation of a contemporary gated RNN architecture, thus enables high performance on temporal sequence classification tasks while promis- ing significant efficiency gains compared to the state of the art, which is dominated by purely digital designs (Conti et al. 2018, Paulin et al. 2021, Chen et al. 2024). 2 A hardware-amenable GRU-based architecture Our architecture builds on the minGRU model presented by Feng et al. (2024), which describes units with a recur- rent internal state 𝒉𝑡 𝒛𝑡 𝒉𝑡 (1 𝒛𝑡) 𝒉𝑡 1 (1) which persists across time steps 𝑡 and is only partially overwritten with a candidate state ℎ based on the gate 𝑧. The latter two are derived from the units input 𝑥 through linear projections 𝒉𝑡 𝑾ℎ 𝒙𝑡 𝒃ℎ , (2) 𝒛𝑡 𝜎𝑧(𝑾𝑧 𝒙𝑡 𝒃𝑧) , (3) with 𝜎 representing a sigmoidal activation function. In contrast to the original GRUs, minGRUs drop the explicit dependency on the previous hidden state ℎ𝑡 1 when cal- culating the gate and proposal states, allowing the appli- cation of the highly performant parallel scan algorithm during training. Removing cross-neuron dependencies in the state update as indicated by the element-wise Hadamard product in Equation 1 also reduces the recurrent computation to fully local information. This lays an important foundation for an efficient hardware implementation. Based on the minGRU model family, we derive a hardware-amenable architecture. For this purpose, we introduce a number of additional simplifications and constraints. Architecture To streamline the overall architecture, we refrain from relying on skip-connections or channel-mixing, and instead rely on a simple feed-forward architecture stack- ing GRU blocks as time mixing units, as shown in Figure 1. Quantization Most machine learning models rely on floating point numbers to store and compute with their parameters, i.e. neural network weights and biases. Moving to fixed- point or plain integer representations, in contrast, can in many cases dramatically increase the computational efficiency and reduce the memory overhead. Thus, effi- cient hardware implementations including most IMC architectures typically rely on quantized weights for parameter storage as well as computation (Verma et al. 2019). IMC implementations often push this to the extreme by resorting to low precision representations and even binary weight parameters. This allows them to reduce circuit complexity and optimized resource utilization. Higher precision can often be recovered through time-multiplexing or the aggregation of weight or synapse circuits into larger units with a then increased overall precision (Verma et al. 2019). Considering our mixed-signal implementation, we opt for a drastic reduction in weight and bias resolution. Weights are thus quantized to 2 b and biases to 6 b values. In the proposed system, the internal states are represented as analog voltages, and they thus remain unquantized. Binary output activations We adopt binary output activations to reduce the com- munication bandwidth between layers and to simplify the multiply-operation in the linear projections between GRU blocks. To that end, we rely on a Heaviside step function for the output activation function, i.e., 𝜎ℎ(𝒉𝑡) Θ(𝒉𝑡) . (4) These binary activations allow a sparse, event-based communication of on and off transitions between layers, in turn reducing the routing fabric s complexity and energy footprint. Simplified gating To avoid costly arithmetics, we replace the sigmoidal activation function resulting in 𝑧 by a hard sigmoid, i.e. a piece-wise linear function 2 𝑥 ℎ 𝑧 ℎ time step 0 ℎ 0.40 ℎ 0.00 𝑧 0.57 ADC 4 7 ℎ ℎ 0.22 𝑧 1 ℎ 0.77 ℎ 0.22 𝑧 0.28 ADC 2 7 ℎ ℎ 0.38 𝑧 2 ℎ 0.18 ℎ 0.38 𝑧 0.71 ADC 5 7 ℎ ℎ 0.23 𝑧 A ℎ ℎ 𝑧 𝑆ℎ' 1 𝑆ℎ' 2 𝑆ℎ'' 1 𝑆ℎ'' 2 𝑤ℎ 2 b SRAM 𝑆𝑧 1 𝑆𝑧 2 𝑤𝑧 2 b SRAM 𝑉11 0.7 V 𝑉10 0.5 V 𝑉01 0.3 V 𝑉00 0.1 V 𝑉0 0.4 V e.g. 𝑥𝑖 {0, 1} B Figure 2: The MINIMALIST cores perform both IMC operations and recurrent state updates through switched capacitor circuits. A The cores interleave the weights for the gating (𝑧) and hidden state candidates ( ℎ). Each synapse features three capacitors: one to represent the hidden state ℎ, a second one to calculate ℎ, and a third for calculating 𝑧. The first two capacitors swap their roles according to the value of 𝑧, as shown for exemplary activations across three time steps. B More detailed schematic of the switching scheme, including the circuits representing the input activation 𝑥𝑖 of a row, represented by a binary value. Weights 𝑤ℎ and 𝑤𝑧 are stored in local 2 b SRAM cells, which determine the potential to sample from through turning on one of the 4 switches. 𝜎𝑧(𝑥) { { { { {0 if 𝑥 3 , 1 if 𝑥 3 , 𝑥 6 1 2 otherwise. (5) The result is then quantized to 6 b. 3 Circuit implementation The MINIMALIST architecture encompasses multiple stacked GRU layers, each connected through feedfor- ward projections. Depending on their dimensionality, these GRU blocks can be mapped to one or multiple cores, which are connected through an event-based routing fabric. The following paragraphs introduce the design of those cores, namely the switched-capacitor IMC and state update circuitry. 3.1 Mixed-signal computing cores The computing cores capture the functionality of a GRU block and the subsequent application of the output acti- vation function 𝜎ℎ. To that end, they first calculate the gating variable 𝑧 as well as the new proposal state ℎ that both result from linear input projections through IMC. They also implement the subsequent state update mechanism. 3.1.1 Switched-capacitor-based IMC input projections MINIMALIST realizes the linear input projections repre- senting 𝑾h and 𝑾z through switched-capacitor IMC matrices. The two resulting matrix-vector multiplica- tions share the same input vector 𝑥, and can thus be merged into a single matrix as indicated in Figure 2 A. Each GRU circuit is thus connected to a column of ℎ and 𝑧 synapses, each. An ℎ synapse fulfills two distinct roles: it participates in the respective matrix-vector multiplication, but is also responsible for maintaining the previous hidden state ℎ𝑡 1. It thus features two identical capacitors. At each point in time, one of them holds the previous hidden state ℎ and is involved in the state update calculations (ref. Section 3.1.3). The second is available to calculate the new candidate state ℎ through IMC (Figure 2 B). In general, capacitor-based IMC solutions profit from the comparably accurate matching of metal-based capacitor structures even when relying only on parasitic fringe 3 capacitances to achieve state-of-the-art accuracy at a minimal energy budget and a compact silicon footprint (Bankman et al. 2018, Valavi et al. 2019). Capacitive IMC can be achieved through either charge redistribution or charge sharing strategies. Both are compatible with multi-bit multiply-accumulate (MAC) operations and, for that purpose, typically rely on segmented capacitors for a fractional control over the charge and thus the effective weight. As the presented architecture relies on the sampling capacitors also for maintaining and updat- ing of the internal GRU states ℎ𝑡, the sampling nodes must possess a constant and known capacitance. Multi- bit weights thus have to rely on multiple voltage levels to modulate the charge. We opted for a charge sharing paradigm and realized a 2 b weight resolution by allowing each synapse to choose among four distinct, equidistant voltages, 𝑉𝑤, 𝑤 {00, 01, 10, 11}, based on the locally stored weight (2b SRAM cell, refer to Figure 2 B). 𝑉0 1 2(𝑉00 𝑉11), a fifth potential representing zero activations, is chosen at an intermediate voltage, thus resulting in two positive and two negative weight values, although this might be adapted also on a per-layer basis to better represent the weight statistics of a given network. Calculating the element-wise product of binary input activation 𝑥𝑖 and weight 𝑤𝑗𝑖 at the intersection of row 𝑖 and column 𝑗 involves both the synapse itself and the row-wise driver circuitry (Figure 2 B): Presenting 𝑥𝑖 1 connects the four shared horizontal lines to the weight potentials 𝑉𝑤. In case of 𝑥𝑖 0, they are clamped to 𝑉0. The synapse then choses to sample from one of those four lines according to the locally stored weights. To compute both 𝑧 and ℎ, the respective sampling capacitors are first pre-charged via 𝑆 1 to the weight potentials corresponding to the weight values stored in local SRAM. In a second phase, the capacitors within a column are shorted via switches 𝑆 2. As a result, they share their charge and the potential settles towards 𝑽𝑧, ℎ 𝑉𝑤(𝑾𝑧, ℎ) 𝒙𝑡 1 dim(𝒙𝑡) , (6) representing the means of the weighted input activa- tions and thus implementing the desired linear projec- tions. 3.1.2 Digitization of 𝑧 The computation of state updates further relies on switching of capacitors based on the value of 𝑉𝑧 (ref. Section 3.1.3). To that end, 𝑧 has to be known in the dig- ital domain, which we address with a 6 b successive-ap- proximation register (SAR) analog-to-digital converter (ADC) (Figure 3). We can directly apply the activation function 𝜎𝑧, a hard sigmoid, by restricting the dynamic range of the sample m share m 2𝑚 1 dim(𝒙) ADC s[n-1] s[1] s[0] A sample[m-1:0] share[m-1:0] slope s[n-1:0] offset SAR phase B 𝑉𝑧 0 63 digitized z 0.2 0.3 0.4 0.5 0.6 𝑉𝑧 V 0 63 digitized z slope offset C Figure 3: The ADC transfer characteristics can be tuned by controlling the capacitive load represented by the IMC array or by using the capacitive DAC to induce a constant bias. This allows mapping of a wide range of 𝜎𝑧 activation functions, also on a per-layer or per- unit basis. A Schematic of a SAR ADC channel and a column of 𝑧 synapses. The sharing switches can be controlled to connect or disconnect a variable number of synapses to the ADC s input, thus allowing to tune the ratio of capacitances and in turn the slope of the activation function. B Timing of the sampling and sharing phases calculating 𝑧, but also of the digitization through successive approximation and, importantly, the pre-charging of the ADC s capacitor array to induce an offset. C Mixed-signal simulation results showing ADC characteristics as a function of the slope and offset parameters. ADC. A limited range, typically caused by parasitic capacitances reducing the voltage swing of the ADC s capacitor array, poses a challenge in many applications. Here, we deliberately exploit this effect by keeping the sampling capacitors 𝐶𝑧 𝟙 connected during the digitiza- tion phase (Figure 3 A). Segmenting the IMC matrix into groups with a binary scaling enables granular control over switches 𝑆𝑧 2. This allows to disconnect parts of the 4 IMC sampling capacitors after charge sharing, inducing control over the ratio 𝐶ADC 𝐶IMC and thus the ADC s dynamic range (Figure 3 B). Therefore, the circuits can be ideally matched to the layer-specific slope of 𝜎𝑧 (Figure 3 C). For a constant bias on 𝑧, we can rely on the ADC s capacitive digital-to-analog converter (DAC) to generate an offset on the sampled potential: During the sampling phase, the capacitor array is pre-set to a 6 b offset be- fore then starting the successive approximation with the initial configuration (s[5:0] 0b100000). This allows shifting of the ADC s transfer characteristics by half of the dynamic range towards both positive and negative voltages (Figure 3 C). 3.1.3 State update through charge sharing The state update itself is, again, implemented through charge sharing. Within a column, each synapse con- tributes one sampling capacitor to represent the previ- ous hidden state ℎ𝑡 1 represented as 𝑉ℎ on the total capacitance 𝐶ℎ, while the other capacitor is used to calculate ℎ according to the IMC scheme introduced be- fore, now present as 𝑉 ℎ on the merged capacitance 𝐶 ℎ. Updating ℎ simply involves mixing the charge between 𝐶ℎ and 𝐶 ℎ with a weighting determined by 𝑧, the 6 b digital representation of 𝜎𝑧(𝑉𝑧). For that purpose, the circuits, again, rely on a segmented IMC matrix and thus granular control over 𝑆ℎ 2 . The number of swapped capacitors is simply propor- tional to 𝑧. When 𝑧 0, the capacitor bank representing ℎ remains untouched, when 𝑧 1, all capacitors are ex- changed and thus fully carry over ℎ. Intermediate values of 𝑧 result in a proportional mixing of the two state variables. The process of updating ℎ is thus equivalent to swapping sampling capacitors between the two output lines ℎ and ℎ, effectively reassigning their role to either represent the previous hidden state or to be available for calculating the next ℎ (Figure 2 A). Crucially, this scheme does not require buffering of the internal states and simply redistributes charge between the capacitors. This reduced the overall energy footprint but also design complexity by restricting itself only to capacitors and switches. 3.1.4 Output activations The GRUs output activations, represented by the Heav- iside step function, are applied by reusing the ADC s comparator circuit. A bias on ℎ can be subsumed in the comparator s reference potential, which is generated through the ADC s capacitive DAC. 3.2 Implementation For the IMC arrays and the state-update circuits, the full-custom computing cores only rely on commodity circuits, such as transmission gates, SRAM bitcells, PyTorch circuit 2.5 0.0 2.5 𝑧 0.0 1.0 𝑧 2.5 0.0 2.5 ℎ 0 20 40 60 80 time steps 2.5 0.0 2.5 ℎ 0.35 0.40 0.45 𝑉 𝑧 V 0 32 63 𝑧 0.35 0.40 0.45 𝑉 ℎ V 0.35 0.40 0.45 𝑉ℎ V Figure 4: Comparison of activations recorded from a software implementation of the model and a mixed-sig- nal simulation set up with equivalent weights and biases. The traces stem from a random unit within a network trained on the sequential MNIST dataset. transmission gates, and a simple comparator used in the ADC design. This allows an optimal scaling and transfer across technology nodes. We opted to implement the MINIMALIST architecture in Globalfoundries 22 nm FD-SOI process, and can fully rely on their dense core transistor offerings. 4 Results We verified the MINIMALIST circuits in mixed-signal simulations using Cadence Spectre AMS Designer. To that end, we extracted weights, biases, and input activa- tions from a model implemented and trained in PyTorch, and the circuit simulation was set up accordingly. Fig- ure 4 compares the resulting activations on 𝑧, ℎ, and ℎ between the original software and the circuit implemen- tation. 4.1 Network performance Additionally, we have evaluated the performance im- pact of quantization and adaptations to the network architectures necessary for hardware deployment. In Figure 5 we compare the performance of three networks on the sequential MNIST dataset. All three share the same number of layers and GRU-blocks per layer (1-64-64-64-64-10) and with that include the same num- ber of trainable parameters. The baseline network is trained in full 32 b floating-point precision, uses the same activation functions as described in the original publication (Feng et al. 2024) and achieves a test accu- racy of 98.1 . When quantizing the weights to 2 b integers, the biases to 6 b and binarizing 𝜎ℎ, while keep- 5 full quantized HW-compatible 90 92 94 96 98 100 test accuracy 0.0 0.2 0.4 0.6 0.8 1.0 1.2 parameter memory Mb 𝑊 f32 i2 i2 𝑏 f32 i6 i6 𝑧 f32 f32 i6 𝜎ℎ 𝜎𝑧 𝜎 ℎ 𝟙 Figure 5: Performance of our model on the sequential MNIST dataset (mean and standard deviation across 10 seeds). We trained three models: The first one relied on the original activation functions and a floating point representation of weights and biases as well as internal activations. The second was restricted to quantized weights and biases as well as 1 b output activations. The third model was fully compatible to the hardware constraints and to that end, also included a quantized hard sigmoid activation function on 𝑧. ing the internal GRU-states and activation functions the same, we incur a performance penalty of 0.4 at a ten-fold reduction of parameter memory. This, however, requires the extension of the network training to a multi- stage process of 4 gradual phases of quantization-aware training. For hardware compatibility it is additionally required to eliminate the actiavation function on 𝜎 ℎ, exchange 𝜎𝑧 with a hard-sigmoid and to quantize the gating variable 𝑧 to 6 b integers. With this the network reaches a final test accuracy of 96.9 . 4.2 Energy efficiency The energy expenditure of the mixed-signal computing cores is dominated by the repeated charging and dis- charging of the sampling capacitors, as well as the tog- gling of the switches. Considering a network spanning 4 cores with 64 rows and 64 columns each, we estimate the energy to be bounded by 169 pJ per time step. Here, we assume all switches to toggle the worst case sce- nario corresponding to a constant 𝑧 1. Our estimate, however, does not yet include the SAR ADC (with a total DAC capacitance far below the IMC capacitance), the event routing (with relatively sparse 1 b activations), the digital control logic, and clock distribution. 5 Discussion In this manuscript, we have introduced the MINIMAL- IST architecture based on simplified GRUs. The network incorporates constraints on its weight quantization and activation functions to make it amenable for a mixed- signal circuit implementation. The implementation re- lies on switched-capacitor circuits to implement the lin- ear weight projections as well as recurrent state update circuit, and is free of continuously biased analog blocks. Besides the comparator used within the SAR ADC, it only consists of SRAM bitcells, transmission gates, and MOM capacitors, and can thus be optimally scaled and transferred across technology nodes. To the best of our knowledge, the presented architecture and circuits rep- resent the first switched-capacitor implementation of a contemporary RNN architecture. We have verified the architecture on the sequential MNIST dataset and compared the circuit dynamics to data recorded from the software model in a mixed- signal simulation. Tentative energy estimates promise significant efficiency gains compared to the state of the art (Giraldo and Verhelst 2018, Ankit et al. 2019, Zhao et al. 2019), but more elaborate estimates and analyses are required for a fair comparison. 6 Bibliography Ankit A, Hajj IE, Chalamalasetti SR, et al (2019) PUMA: A programmable ultra-efficient memristor- based accelerator for machine learning inference. In: Proceedings of the twenty-fourth international con- ference on architectural support for programming languages and operating systems. pp 715 731 Bankman D, Yang L, Moons B, et al (2018) An Always- On 3.8μJ 86 CIFAR-10 mixed-signal binary CNN processor with all memory on chip in 28-nm CMOS. IEEE Journal of Solid-State Circuits 54:158 172 Bavikadi S, Sutradhar PR, Khasawneh KN, et al (2020) A review of in-memory computing architectures for machine learning applications. In: Proceedings of the 2020 on Great Lakes Symposium on VLSI. pp 89 94 Chen J, Jun S-W, Hong S, et al (2024) Eciton: Very low-power recurrent neural network accelerator for real-time inference at the edge. ACM Transactions on Reconfigurable Technology and Systems 17:1 25 Cho K, Van Merriënboer B, Gulcehre C, et al (2014) Learning phrase representations using RNN en- coder-decoder for statistical machine translation. arXiv preprint arXiv:14061078 Conti F, Cavigelli L, Paulin G, et al (2018) Chipmunk: A systolically scalable 0.9 mm 2, 3.08 Gop s mW 1.2 mW accelerator for near-sensor recurrent neural network inference. In: 2018 IEEE Custom Integrated Circuits Conference (CICC). pp 1 4 6 Feng L, Tung F, Ahmed MO, et al (2024) Were RNNs all we needed?. arXiv preprint arXiv:241001201 Giraldo JSP, Verhelst M (2018) Laika: A 5μW program- mable LSTM accelerator for always-on keyword spotting in 65nm CMOS. In: ESSCIRC 2018-IEEE 44th European Solid State Circuits Conference (ESS- CIRC). pp 166 169 Hochreiter S (1997) Long Short-term Memory. Neural Computation MIT-Press Orvieto A, Smith SL, Gu A, et al (2023) Resurrecting recurrent neural networks for long sequences. In: International Conference on Machine Learning. pp 26670 26698 Paulin G, Conti F, Cavigelli L, Benini L (2021) Vau da muntanialas: Energy-efficient multi-die scalable ac- celeration of RNN inference. IEEE Transactions on Circuits and Systems I: Regular Papers 69:244 257 Sun J, Houshmand P, Verhelst M (2023) Analog or dig- ital in-memory computing? benchmarking through quantitative modeling. In: 2023 IEEE ACM Inter- national Conference on Computer Aided Design (ICCAD). pp 1 9 Valavi H, Ramadge PJ, Nestler E, Verma N (2019) A 64- tile 2.4-Mb in-memory-computing CNN accelerator employing charge-domain compute. IEEE Journal of Solid-State Circuits 54:1789 1799 Vaswani A (2017) Attention is all you need. Advances in Neural Information Processing Systems Verma N, Jia H, Valavi H, et al (2019) In-memory com- puting: Advances and prospects. IEEE Solid-State Circuits Magazine 11:43 55 Zhao Z, Srivastava A, Peng L, Chen Q (2019) Long short- term memory network design for analog computing. ACM Journal on Emerging Technologies in Comput- ing Systems (JETC) 15:1 27 7\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nMINIMALIST: switched-capacitor circuits for efficient in-memory computation of gated recurrent units Sebastian Billaudelle , Laura Kriener , Filippo Moro, Tristan Torchet, Melika Payvand Institute of Neuroinformatics, University of Zürich and ETH Zürich contributed equally This preprint represents an early and preliminary version of the final manuscript and will be updated regularly. Recurrent neural networks (RNNs) have been a long-stand- ing candidate for processing of temporal sequence data, especially in memory-constrained systems that one may find in embedded edge computing environments. Recent advances in training paradigms have now inspired new generations of efficient RNNs. We introduce a streamlined and hardware-compatible architecture based on minimal gated recurrent units (GRUs), and an accompanying effi- cient mixed-signal hardware implementation of the model. The proposed design leverages switched-capacitor circuits not only for in-memory-computing (IMC), but also for the gated state updates. The mixed-signal cores rely solely on commodity circuits consisting of metal capacitors, trans- mission gates, and a clocked comparator, thus greatly facilitating scaling and transfer to other technology nodes. We benchmark the performance of our architecture on time series data, introducing all constraints required for a direct mapping to the hardware system. The direct compat- ibility is verified in mixed-signal simulations, reproducing data recorded from the software-only network model. 1 Introduction Modeling temporal signals is a core challenge in AI, with applications ranging from speech and language to sensory processing and control. RNNs, particularly gated variants like long short-term memories (LSTMs) (Hochreiter 1997) and GRUs (Cho et al. 2014), emerged as the natural choice for such tasks due to their ability to maintain an internal memory and process inputs se- quentially. They offered a principled way to capture tem- poral dependencies across varying time scales. However, the advent of the Transformer architecture (Vaswani 2017) marked a shift, its ability to train in parallel across time steps led to significant efficiency gains and performance improvements. Despite this, the quadratic complexity of the attention mechanism in Transformers remains a bottleneck for deployment in edge and low- power settings. To address this, newer RNN variants such as the minGRU (Feng et al.\n\n--- Segment 2 ---\nDespite this, the quadratic complexity of the attention mechanism in Transformers remains a bottleneck for deployment in edge and low- power settings. To address this, newer RNN variants such as the minGRU (Feng et al. 2024) have been developed to support parallel training while preserving the constant-time, local-state inference characteristic of classical RNNs. These algorithmic simplifications now also open the door to efficient hardware implementa- tions for edge computing scenarios. One of the key principles behind the design of effi- cient machine learning accelerators is the reduction of data movement (Bavikadi et al. 2020). This typically manifests itself in IMC, the collocation of memory and computing elements especially for matrix-vector multiplications, as found for example in the linear pro- jections between neural network layers. In this space, analog and mixed-signal implementations typically out- perform digital solutions in terms of energy efficiency, especially in a low-precision regime (Sun et al. 2023). This advantage is, however, often diminished by a high complexity and energy footprint of peripheral circuits. A successful mixed-signal implementation must thus attempt to reduce the frequency and resolution of con- verting between the analog and digital domains, simplify operations to mostly linear and hardware-amenable arithmetics, and reduce the impact of data movement. RNNs, in particular, often require rather complex state update arithmetics and involve a dense recurrent projec- tion of hidden states within a layer. This induces high bandwidth requirements and is typically not compatible to the low-resolution regime where analog implementa- tions perform best. Here, we present a streamlined RNN architecture that addresses above challenges. It adopts the diagonal-only recurrent projections of contemporary models (Orvieto et al. 2023, Feng et al. 2024), reduces inter-layer commu- nication by enforcing binary output activations, and reduces the complexity of gated state updates by resort- ing to simplified internal activation functions. Alongside the resulting model, we also introduce a highly effi- cient mixed-signal implementation thereof. It relies on switched-capacitor circuits to realize both the matrix- vector multiplications involved in the linear projections between layers and the recurrent state updates.\n\n--- Segment 3 ---\nAlongside the resulting model, we also introduce a highly effi- cient mixed-signal implementation thereof. It relies on switched-capacitor circuits to realize both the matrix- vector multiplications involved in the linear projections between layers and the recurrent state updates. Tran- sitions between the analog and digital domains are reduced to a bare minimum and the design can refrain from continuously biased analog circuits such as voltage buffers. The design only relies on metal-oxide-metal (MOM) capacitors, transmission gates, static random-ac- cess memory (SRAM) bitcells, and a simple comparator circuit, thus enabling straight-forward scaling and an optimal transfer across technology nodes. Both architecture and mixed-signal implementation are the result of a stringent co-design process, which allowed to optimally match circuits to the underlying RNN functions. And vice versa, the simplifications to the network model were directly informed by constraints 1 𝒙 GRU block GRU block GRU block 𝒚 𝜎ℎ 𝜎ℎ 𝜎ℎ 𝑾𝑧𝒙 𝒃𝑧 𝑾ℎ𝒙 𝒃ℎ 𝒛 𝒉 (1 𝒛) 𝒉𝑡 1 𝜎𝑧 𝜎 ℎ Figure 1: The MINIMALIST architecture consists of a feedforward network of simplified GRU blocks, inter- leaved with binary activation functions. imposed by the microelectronic implementation. The resulting design, to the best of our knowledge the first switched-capacitor implementation of a contemporary gated RNN architecture, thus enables high performance on temporal sequence classification tasks while promis- ing significant efficiency gains compared to the state of the art, which is dominated by purely digital designs (Conti et al. 2018, Paulin et al. 2021, Chen et al. 2024). 2 A hardware-amenable GRU-based architecture Our architecture builds on the minGRU model presented by Feng et al.\n\n--- Segment 4 ---\n2024). 2 A hardware-amenable GRU-based architecture Our architecture builds on the minGRU model presented by Feng et al. (2024), which describes units with a recur- rent internal state 𝒉𝑡 𝒛𝑡 𝒉𝑡 (1 𝒛𝑡) 𝒉𝑡 1 (1) which persists across time steps 𝑡 and is only partially overwritten with a candidate state ℎ based on the gate 𝑧. The latter two are derived from the units input 𝑥 through linear projections 𝒉𝑡 𝑾ℎ 𝒙𝑡 𝒃ℎ , (2) 𝒛𝑡 𝜎𝑧(𝑾𝑧 𝒙𝑡 𝒃𝑧) , (3) with 𝜎 representing a sigmoidal activation function. In contrast to the original GRUs, minGRUs drop the explicit dependency on the previous hidden state ℎ𝑡 1 when cal- culating the gate and proposal states, allowing the appli- cation of the highly performant parallel scan algorithm during training. Removing cross-neuron dependencies in the state update as indicated by the element-wise Hadamard product in Equation 1 also reduces the recurrent computation to fully local information. This lays an important foundation for an efficient hardware implementation. Based on the minGRU model family, we derive a hardware-amenable architecture. For this purpose, we introduce a number of additional simplifications and constraints. Architecture To streamline the overall architecture, we refrain from relying on skip-connections or channel-mixing, and instead rely on a simple feed-forward architecture stack- ing GRU blocks as time mixing units, as shown in Figure 1. Quantization Most machine learning models rely on floating point numbers to store and compute with their parameters, i.e. neural network weights and biases. Moving to fixed- point or plain integer representations, in contrast, can in many cases dramatically increase the computational efficiency and reduce the memory overhead. Thus, effi- cient hardware implementations including most IMC architectures typically rely on quantized weights for parameter storage as well as computation (Verma et al. 2019).\n\n--- Segment 5 ---\nThus, effi- cient hardware implementations including most IMC architectures typically rely on quantized weights for parameter storage as well as computation (Verma et al. 2019). IMC implementations often push this to the extreme by resorting to low precision representations and even binary weight parameters. This allows them to reduce circuit complexity and optimized resource utilization. Higher precision can often be recovered through time-multiplexing or the aggregation of weight or synapse circuits into larger units with a then increased overall precision (Verma et al. 2019). Considering our mixed-signal implementation, we opt for a drastic reduction in weight and bias resolution. Weights are thus quantized to 2 b and biases to 6 b values. In the proposed system, the internal states are represented as analog voltages, and they thus remain unquantized. Binary output activations We adopt binary output activations to reduce the com- munication bandwidth between layers and to simplify the multiply-operation in the linear projections between GRU blocks. To that end, we rely on a Heaviside step function for the output activation function, i.e., 𝜎ℎ(𝒉𝑡) Θ(𝒉𝑡) . (4) These binary activations allow a sparse, event-based communication of on and off transitions between layers, in turn reducing the routing fabric s complexity and energy footprint. Simplified gating To avoid costly arithmetics, we replace the sigmoidal activation function resulting in 𝑧 by a hard sigmoid, i.e.\n\n--- Segment 6 ---\n(4) These binary activations allow a sparse, event-based communication of on and off transitions between layers, in turn reducing the routing fabric s complexity and energy footprint. Simplified gating To avoid costly arithmetics, we replace the sigmoidal activation function resulting in 𝑧 by a hard sigmoid, i.e. a piece-wise linear function 2 𝑥 ℎ 𝑧 ℎ time step 0 ℎ 0.40 ℎ 0.00 𝑧 0.57 ADC 4 7 ℎ ℎ 0.22 𝑧 1 ℎ 0.77 ℎ 0.22 𝑧 0.28 ADC 2 7 ℎ ℎ 0.38 𝑧 2 ℎ 0.18 ℎ 0.38 𝑧 0.71 ADC 5 7 ℎ ℎ 0.23 𝑧 A ℎ ℎ 𝑧 𝑆ℎ' 1 𝑆ℎ' 2 𝑆ℎ'' 1 𝑆ℎ'' 2 𝑤ℎ 2 b SRAM 𝑆𝑧 1 𝑆𝑧 2 𝑤𝑧 2 b SRAM 𝑉11 0.7 V 𝑉10 0.5 V 𝑉01 0.3 V 𝑉00 0.1 V 𝑉0 0.4 V e.g. 𝑥𝑖 {0, 1} B Figure 2: The MINIMALIST cores perform both IMC operations and recurrent state updates through switched capacitor circuits. A The cores interleave the weights for the gating (𝑧) and hidden state candidates ( ℎ). Each synapse features three capacitors: one to represent the hidden state ℎ, a second one to calculate ℎ, and a third for calculating 𝑧. The first two capacitors swap their roles according to the value of 𝑧, as shown for exemplary activations across three time steps. B More detailed schematic of the switching scheme, including the circuits representing the input activation 𝑥𝑖 of a row, represented by a binary value.\n\n--- Segment 7 ---\nThe first two capacitors swap their roles according to the value of 𝑧, as shown for exemplary activations across three time steps. B More detailed schematic of the switching scheme, including the circuits representing the input activation 𝑥𝑖 of a row, represented by a binary value. Weights 𝑤ℎ and 𝑤𝑧 are stored in local 2 b SRAM cells, which determine the potential to sample from through turning on one of the 4 switches. 𝜎𝑧(𝑥) { { { { {0 if 𝑥 3 , 1 if 𝑥 3 , 𝑥 6 1 2 otherwise. (5) The result is then quantized to 6 b. 3 Circuit implementation The MINIMALIST architecture encompasses multiple stacked GRU layers, each connected through feedfor- ward projections. Depending on their dimensionality, these GRU blocks can be mapped to one or multiple cores, which are connected through an event-based routing fabric. The following paragraphs introduce the design of those cores, namely the switched-capacitor IMC and state update circuitry. 3.1 Mixed-signal computing cores The computing cores capture the functionality of a GRU block and the subsequent application of the output acti- vation function 𝜎ℎ. To that end, they first calculate the gating variable 𝑧 as well as the new proposal state ℎ that both result from linear input projections through IMC. They also implement the subsequent state update mechanism. 3.1.1 Switched-capacitor-based IMC input projections MINIMALIST realizes the linear input projections repre- senting 𝑾h and 𝑾z through switched-capacitor IMC matrices. The two resulting matrix-vector multiplica- tions share the same input vector 𝑥, and can thus be merged into a single matrix as indicated in Figure 2 A. Each GRU circuit is thus connected to a column of ℎ and 𝑧 synapses, each. An ℎ synapse fulfills two distinct roles: it participates in the respective matrix-vector multiplication, but is also responsible for maintaining the previous hidden state ℎ𝑡 1. It thus features two identical capacitors.\n\n--- Segment 8 ---\nAn ℎ synapse fulfills two distinct roles: it participates in the respective matrix-vector multiplication, but is also responsible for maintaining the previous hidden state ℎ𝑡 1. It thus features two identical capacitors. At each point in time, one of them holds the previous hidden state ℎ and is involved in the state update calculations (ref. Section 3.1.3). The second is available to calculate the new candidate state ℎ through IMC (Figure 2 B). In general, capacitor-based IMC solutions profit from the comparably accurate matching of metal-based capacitor structures even when relying only on parasitic fringe 3 capacitances to achieve state-of-the-art accuracy at a minimal energy budget and a compact silicon footprint (Bankman et al. 2018, Valavi et al. 2019). Capacitive IMC can be achieved through either charge redistribution or charge sharing strategies. Both are compatible with multi-bit multiply-accumulate (MAC) operations and, for that purpose, typically rely on segmented capacitors for a fractional control over the charge and thus the effective weight. As the presented architecture relies on the sampling capacitors also for maintaining and updat- ing of the internal GRU states ℎ𝑡, the sampling nodes must possess a constant and known capacitance. Multi- bit weights thus have to rely on multiple voltage levels to modulate the charge. We opted for a charge sharing paradigm and realized a 2 b weight resolution by allowing each synapse to choose among four distinct, equidistant voltages, 𝑉𝑤, 𝑤 {00, 01, 10, 11}, based on the locally stored weight (2b SRAM cell, refer to Figure 2 B). 𝑉0 1 2(𝑉00 𝑉11), a fifth potential representing zero activations, is chosen at an intermediate voltage, thus resulting in two positive and two negative weight values, although this might be adapted also on a per-layer basis to better represent the weight statistics of a given network.\n\n--- Segment 9 ---\nWe opted for a charge sharing paradigm and realized a 2 b weight resolution by allowing each synapse to choose among four distinct, equidistant voltages, 𝑉𝑤, 𝑤 {00, 01, 10, 11}, based on the locally stored weight (2b SRAM cell, refer to Figure 2 B). 𝑉0 1 2(𝑉00 𝑉11), a fifth potential representing zero activations, is chosen at an intermediate voltage, thus resulting in two positive and two negative weight values, although this might be adapted also on a per-layer basis to better represent the weight statistics of a given network. Calculating the element-wise product of binary input activation 𝑥𝑖 and weight 𝑤𝑗𝑖 at the intersection of row 𝑖 and column 𝑗 involves both the synapse itself and the row-wise driver circuitry (Figure 2 B): Presenting 𝑥𝑖 1 connects the four shared horizontal lines to the weight potentials 𝑉𝑤. In case of 𝑥𝑖 0, they are clamped to 𝑉0. The synapse then choses to sample from one of those four lines according to the locally stored weights. To compute both 𝑧 and ℎ, the respective sampling capacitors are first pre-charged via 𝑆 1 to the weight potentials corresponding to the weight values stored in local SRAM. In a second phase, the capacitors within a column are shorted via switches 𝑆 2. As a result, they share their charge and the potential settles towards 𝑽𝑧, ℎ 𝑉𝑤(𝑾𝑧, ℎ) 𝒙𝑡 1 dim(𝒙𝑡) , (6) representing the means of the weighted input activa- tions and thus implementing the desired linear projec- tions. 3.1.2 Digitization of 𝑧 The computation of state updates further relies on switching of capacitors based on the value of 𝑉𝑧 (ref. Section 3.1.3).\n\n--- Segment 10 ---\n3.1.2 Digitization of 𝑧 The computation of state updates further relies on switching of capacitors based on the value of 𝑉𝑧 (ref. Section 3.1.3). To that end, 𝑧 has to be known in the dig- ital domain, which we address with a 6 b successive-ap- proximation register (SAR) analog-to-digital converter (ADC) (Figure 3). We can directly apply the activation function 𝜎𝑧, a hard sigmoid, by restricting the dynamic range of the sample m share m 2𝑚 1 dim(𝒙) ADC s[n-1] s[1] s[0] A sample[m-1:0] share[m-1:0] slope s[n-1:0] offset SAR phase B 𝑉𝑧 0 63 digitized z 0.2 0.3 0.4 0.5 0.6 𝑉𝑧 V 0 63 digitized z slope offset C Figure 3: The ADC transfer characteristics can be tuned by controlling the capacitive load represented by the IMC array or by using the capacitive DAC to induce a constant bias. This allows mapping of a wide range of 𝜎𝑧 activation functions, also on a per-layer or per- unit basis. A Schematic of a SAR ADC channel and a column of 𝑧 synapses. The sharing switches can be controlled to connect or disconnect a variable number of synapses to the ADC s input, thus allowing to tune the ratio of capacitances and in turn the slope of the activation function. B Timing of the sampling and sharing phases calculating 𝑧, but also of the digitization through successive approximation and, importantly, the pre-charging of the ADC s capacitor array to induce an offset. C Mixed-signal simulation results showing ADC characteristics as a function of the slope and offset parameters. ADC. A limited range, typically caused by parasitic capacitances reducing the voltage swing of the ADC s capacitor array, poses a challenge in many applications. Here, we deliberately exploit this effect by keeping the sampling capacitors 𝐶𝑧 𝟙 connected during the digitiza- tion phase (Figure 3 A).\n\n--- Segment 11 ---\nA limited range, typically caused by parasitic capacitances reducing the voltage swing of the ADC s capacitor array, poses a challenge in many applications. Here, we deliberately exploit this effect by keeping the sampling capacitors 𝐶𝑧 𝟙 connected during the digitiza- tion phase (Figure 3 A). Segmenting the IMC matrix into groups with a binary scaling enables granular control over switches 𝑆𝑧 2. This allows to disconnect parts of the 4 IMC sampling capacitors after charge sharing, inducing control over the ratio 𝐶ADC 𝐶IMC and thus the ADC s dynamic range (Figure 3 B). Therefore, the circuits can be ideally matched to the layer-specific slope of 𝜎𝑧 (Figure 3 C). For a constant bias on 𝑧, we can rely on the ADC s capacitive digital-to-analog converter (DAC) to generate an offset on the sampled potential: During the sampling phase, the capacitor array is pre-set to a 6 b offset be- fore then starting the successive approximation with the initial configuration (s[5:0] 0b100000). This allows shifting of the ADC s transfer characteristics by half of the dynamic range towards both positive and negative voltages (Figure 3 C). 3.1.3 State update through charge sharing The state update itself is, again, implemented through charge sharing. Within a column, each synapse con- tributes one sampling capacitor to represent the previ- ous hidden state ℎ𝑡 1 represented as 𝑉ℎ on the total capacitance 𝐶ℎ, while the other capacitor is used to calculate ℎ according to the IMC scheme introduced be- fore, now present as 𝑉 ℎ on the merged capacitance 𝐶 ℎ. Updating ℎ simply involves mixing the charge between 𝐶ℎ and 𝐶 ℎ with a weighting determined by 𝑧, the 6 b digital representation of 𝜎𝑧(𝑉𝑧). For that purpose, the circuits, again, rely on a segmented IMC matrix and thus granular control over 𝑆ℎ 2 .\n\n--- Segment 12 ---\nWithin a column, each synapse con- tributes one sampling capacitor to represent the previ- ous hidden state ℎ𝑡 1 represented as 𝑉ℎ on the total capacitance 𝐶ℎ, while the other capacitor is used to calculate ℎ according to the IMC scheme introduced be- fore, now present as 𝑉 ℎ on the merged capacitance 𝐶 ℎ. Updating ℎ simply involves mixing the charge between 𝐶ℎ and 𝐶 ℎ with a weighting determined by 𝑧, the 6 b digital representation of 𝜎𝑧(𝑉𝑧). For that purpose, the circuits, again, rely on a segmented IMC matrix and thus granular control over 𝑆ℎ 2 . The number of swapped capacitors is simply propor- tional to 𝑧. When 𝑧 0, the capacitor bank representing ℎ remains untouched, when 𝑧 1, all capacitors are ex- changed and thus fully carry over ℎ. Intermediate values of 𝑧 result in a proportional mixing of the two state variables. The process of updating ℎ is thus equivalent to swapping sampling capacitors between the two output lines ℎ and ℎ, effectively reassigning their role to either represent the previous hidden state or to be available for calculating the next ℎ (Figure 2 A). Crucially, this scheme does not require buffering of the internal states and simply redistributes charge between the capacitors. This reduced the overall energy footprint but also design complexity by restricting itself only to capacitors and switches. 3.1.4 Output activations The GRUs output activations, represented by the Heav- iside step function, are applied by reusing the ADC s comparator circuit. A bias on ℎ can be subsumed in the comparator s reference potential, which is generated through the ADC s capacitive DAC.\n\n--- Segment 13 ---\n3.1.4 Output activations The GRUs output activations, represented by the Heav- iside step function, are applied by reusing the ADC s comparator circuit. A bias on ℎ can be subsumed in the comparator s reference potential, which is generated through the ADC s capacitive DAC. 3.2 Implementation For the IMC arrays and the state-update circuits, the full-custom computing cores only rely on commodity circuits, such as transmission gates, SRAM bitcells, PyTorch circuit 2.5 0.0 2.5 𝑧 0.0 1.0 𝑧 2.5 0.0 2.5 ℎ 0 20 40 60 80 time steps 2.5 0.0 2.5 ℎ 0.35 0.40 0.45 𝑉 𝑧 V 0 32 63 𝑧 0.35 0.40 0.45 𝑉 ℎ V 0.35 0.40 0.45 𝑉ℎ V Figure 4: Comparison of activations recorded from a software implementation of the model and a mixed-sig- nal simulation set up with equivalent weights and biases. The traces stem from a random unit within a network trained on the sequential MNIST dataset. transmission gates, and a simple comparator used in the ADC design. This allows an optimal scaling and transfer across technology nodes. We opted to implement the MINIMALIST architecture in Globalfoundries 22 nm FD-SOI process, and can fully rely on their dense core transistor offerings. 4 Results We verified the MINIMALIST circuits in mixed-signal simulations using Cadence Spectre AMS Designer. To that end, we extracted weights, biases, and input activa- tions from a model implemented and trained in PyTorch, and the circuit simulation was set up accordingly. Fig- ure 4 compares the resulting activations on 𝑧, ℎ, and ℎ between the original software and the circuit implemen- tation. 4.1 Network performance Additionally, we have evaluated the performance im- pact of quantization and adaptations to the network architectures necessary for hardware deployment. In Figure 5 we compare the performance of three networks on the sequential MNIST dataset.\n\n--- Segment 14 ---\n4.1 Network performance Additionally, we have evaluated the performance im- pact of quantization and adaptations to the network architectures necessary for hardware deployment. In Figure 5 we compare the performance of three networks on the sequential MNIST dataset. All three share the same number of layers and GRU-blocks per layer (1-64-64-64-64-10) and with that include the same num- ber of trainable parameters. The baseline network is trained in full 32 b floating-point precision, uses the same activation functions as described in the original publication (Feng et al. 2024) and achieves a test accu- racy of 98.1 . When quantizing the weights to 2 b integers, the biases to 6 b and binarizing 𝜎ℎ, while keep- 5 full quantized HW-compatible 90 92 94 96 98 100 test accuracy 0.0 0.2 0.4 0.6 0.8 1.0 1.2 parameter memory Mb 𝑊 f32 i2 i2 𝑏 f32 i6 i6 𝑧 f32 f32 i6 𝜎ℎ 𝜎𝑧 𝜎 ℎ 𝟙 Figure 5: Performance of our model on the sequential MNIST dataset (mean and standard deviation across 10 seeds). We trained three models: The first one relied on the original activation functions and a floating point representation of weights and biases as well as internal activations. The second was restricted to quantized weights and biases as well as 1 b output activations. The third model was fully compatible to the hardware constraints and to that end, also included a quantized hard sigmoid activation function on 𝑧. ing the internal GRU-states and activation functions the same, we incur a performance penalty of 0.4 at a ten-fold reduction of parameter memory. This, however, requires the extension of the network training to a multi- stage process of 4 gradual phases of quantization-aware training. For hardware compatibility it is additionally required to eliminate the actiavation function on 𝜎 ℎ, exchange 𝜎𝑧 with a hard-sigmoid and to quantize the gating variable 𝑧 to 6 b integers. With this the network reaches a final test accuracy of 96.9 .\n\n--- Segment 15 ---\nFor hardware compatibility it is additionally required to eliminate the actiavation function on 𝜎 ℎ, exchange 𝜎𝑧 with a hard-sigmoid and to quantize the gating variable 𝑧 to 6 b integers. With this the network reaches a final test accuracy of 96.9 . 4.2 Energy efficiency The energy expenditure of the mixed-signal computing cores is dominated by the repeated charging and dis- charging of the sampling capacitors, as well as the tog- gling of the switches. Considering a network spanning 4 cores with 64 rows and 64 columns each, we estimate the energy to be bounded by 169 pJ per time step. Here, we assume all switches to toggle the worst case sce- nario corresponding to a constant 𝑧 1. Our estimate, however, does not yet include the SAR ADC (with a total DAC capacitance far below the IMC capacitance), the event routing (with relatively sparse 1 b activations), the digital control logic, and clock distribution. 5 Discussion In this manuscript, we have introduced the MINIMAL- IST architecture based on simplified GRUs. The network incorporates constraints on its weight quantization and activation functions to make it amenable for a mixed- signal circuit implementation. The implementation re- lies on switched-capacitor circuits to implement the lin- ear weight projections as well as recurrent state update circuit, and is free of continuously biased analog blocks. Besides the comparator used within the SAR ADC, it only consists of SRAM bitcells, transmission gates, and MOM capacitors, and can thus be optimally scaled and transferred across technology nodes. To the best of our knowledge, the presented architecture and circuits rep- resent the first switched-capacitor implementation of a contemporary RNN architecture. We have verified the architecture on the sequential MNIST dataset and compared the circuit dynamics to data recorded from the software model in a mixed- signal simulation. Tentative energy estimates promise significant efficiency gains compared to the state of the art (Giraldo and Verhelst 2018, Ankit et al. 2019, Zhao et al. 2019), but more elaborate estimates and analyses are required for a fair comparison. 6 Bibliography Ankit A, Hajj IE, Chalamalasetti SR, et al (2019) PUMA: A programmable ultra-efficient memristor- based accelerator for machine learning inference.\n\n--- Segment 16 ---\n2019), but more elaborate estimates and analyses are required for a fair comparison. 6 Bibliography Ankit A, Hajj IE, Chalamalasetti SR, et al (2019) PUMA: A programmable ultra-efficient memristor- based accelerator for machine learning inference. In: Proceedings of the twenty-fourth international con- ference on architectural support for programming languages and operating systems. pp 715 731 Bankman D, Yang L, Moons B, et al (2018) An Always- On 3.8μJ 86 CIFAR-10 mixed-signal binary CNN processor with all memory on chip in 28-nm CMOS. IEEE Journal of Solid-State Circuits 54:158 172 Bavikadi S, Sutradhar PR, Khasawneh KN, et al (2020) A review of in-memory computing architectures for machine learning applications. In: Proceedings of the 2020 on Great Lakes Symposium on VLSI. pp 89 94 Chen J, Jun S-W, Hong S, et al (2024) Eciton: Very low-power recurrent neural network accelerator for real-time inference at the edge. ACM Transactions on Reconfigurable Technology and Systems 17:1 25 Cho K, Van Merriënboer B, Gulcehre C, et al (2014) Learning phrase representations using RNN en- coder-decoder for statistical machine translation. arXiv preprint arXiv:14061078 Conti F, Cavigelli L, Paulin G, et al (2018) Chipmunk: A systolically scalable 0.9 mm 2, 3.08 Gop s mW 1.2 mW accelerator for near-sensor recurrent neural network inference. In: 2018 IEEE Custom Integrated Circuits Conference (CICC). pp 1 4 6 Feng L, Tung F, Ahmed MO, et al (2024) Were RNNs all we needed?. arXiv preprint arXiv:241001201 Giraldo JSP, Verhelst M (2018) Laika: A 5μW program- mable LSTM accelerator for always-on keyword spotting in 65nm CMOS. In: ESSCIRC 2018-IEEE 44th European Solid State Circuits Conference (ESS- CIRC). pp 166 169 Hochreiter S (1997) Long Short-term Memory.\n\n--- Segment 17 ---\nIn: ESSCIRC 2018-IEEE 44th European Solid State Circuits Conference (ESS- CIRC). pp 166 169 Hochreiter S (1997) Long Short-term Memory. Neural Computation MIT-Press Orvieto A, Smith SL, Gu A, et al (2023) Resurrecting recurrent neural networks for long sequences. In: International Conference on Machine Learning. pp 26670 26698 Paulin G, Conti F, Cavigelli L, Benini L (2021) Vau da muntanialas: Energy-efficient multi-die scalable ac- celeration of RNN inference. IEEE Transactions on Circuits and Systems I: Regular Papers 69:244 257 Sun J, Houshmand P, Verhelst M (2023) Analog or dig- ital in-memory computing? benchmarking through quantitative modeling. In: 2023 IEEE ACM Inter- national Conference on Computer Aided Design (ICCAD). pp 1 9 Valavi H, Ramadge PJ, Nestler E, Verma N (2019) A 64- tile 2.4-Mb in-memory-computing CNN accelerator employing charge-domain compute. IEEE Journal of Solid-State Circuits 54:1789 1799 Vaswani A (2017) Attention is all you need. Advances in Neural Information Processing Systems Verma N, Jia H, Valavi H, et al (2019) In-memory com- puting: Advances and prospects. IEEE Solid-State Circuits Magazine 11:43 55 Zhao Z, Srivastava A, Peng L, Chen Q (2019) Long short- term memory network design for analog computing. ACM Journal on Emerging Technologies in Comput- ing Systems (JETC) 15:1 27 7\n\n