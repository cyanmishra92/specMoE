=== ORIGINAL PDF: 2506.20810v1_FINN-GL_Generalized_Mixed-Precision_Extensions_for.pdf ===\n\nRaw text length: 52671 characters\nCleaned text length: 52178 characters\nNumber of segments: 32\n\n=== CLEANED TEXT ===\n\narXiv:2506.20810v1 [cs.LG] 25 Jun 2025 FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs Shashwat Khandelwal , Jakoba Petri-Koenig , Thomas B. Preu√üer , Michaela Blott , Shanker Shreejith Reconfigurable Computing Systems Lab, Electronic Electrical Engineering Trinity College Dublin, Ireland, Email: {khandels, Advanced Micro Devices (AMD) Research, Dublin, Ireland Email: {Jakoba.Petri-Koenig, thomas.preusser, Abstract Recurrent neural networks (RNNs), particularly LSTMs, are effective for time-series tasks like sentiment analysis and short-term stock prediction. However, their computational complexity poses challenges for real-time deployment in resource constrained environments. While FPGAs offer a promising plat- form for energy-efficient AI acceleration, existing tools mainly target feed-forward networks, and LSTM acceleration typically requires full custom implementation. In this paper, we address this gap by leveraging the open-source and extensible FINN framework to enable the generalized deployment of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open Neural Network Exchange (ONNX) specification to model the recurrent nature of LSTM computations, enabling support for mixed quantisation within them and functional verification of LSTM-based models. Furthermore, we introduce custom transformations within the FINN compiler to map the quantised ONNX computation graph to hardware blocks from the HLS kernel library of the FINN compiler and Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM model for a mid-price stock prediction task using the widely used dataset and generating a corresponding hardware IP of the model using our flow, targeting the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator through our flow achieves a balance between performance (latency) and resource consumption, while matching (or bettering) inference accuracy of state-of-the-art models with reduced precision. We believe that the generalisable nature of the proposed flow will pave the way for resource-efficient RNN accelerator designs on FPGAs. Index Terms Brevitas, Field Programmable Gate Arrays, FINN, HFT, ONNX, Quantised LSTMs, RNNs I. INTRODUCTION Time-series predictions are increasingly gaining recognition for their ability to allow stakeholders to dynamically optimise resource allocation in real-time, in applications such as en- ergy forecasting and stock market trend prediction, among others. The generalisability of machine learning (ML) models, coupled with the increasing availability of high-quality train- ing data, has led to significant improvements in prediction accuracy compared to traditional hand-tuned algorithms [1]. However, the real-time nature of these applications necessitates deployment on hardware accelerators to meet latency and per- formance requirements. Field-programmable gate arrays (FP- GAs) have emerged as a preferred choice for accelerating ML models in scenarios where energy efficiency, high throughput, and runtime reconfigurability are desirable. Over the past five years, several frameworks have been developed to streamline the prototyping and deployment of ML models, including con- volutional neural networks (CNNs) and unconventional feed- forward architectures (such as, ResNets, Inception nets and U- Nets), by leveraging quantisation techniques to optimise their Vitis, Vivado, PYNQ and combinations thereof are trademarks of Advanced Micro Devices, Inc. Matmul Conv Feedback connections Feed Forward Neural Networks Recurrent Neural Networks Act Matmul Conv Act Matmul Conv Act Matmul Conv Act Concat Matmul Conv Act Inputs Outputs Matmul Conv Act Matmul Conv Act Matmul Conv Act Matmul Conv Act Inputs Matmul Conv Act Matmul Conv Act Concat : ElMul ElAdd State Updates Computed States Fig. 1: The figure contrasts the information flows of con- ventional feed-forward vs. recurrent neural networks. While in feed-forward networks, data flows acyclically from input to output, recurrent neural networks utilise the feedback of previously computed state to track sequential dependencies. hardware performance [2], [3]. Despite these advancements, a significant gap remains in the deployment of recurrent neural networks (RNNs), such as long short-term memory (LSTM) networks and gated recurrent units (GRUs). RNNs are ideally suited for time-series data processing, of- ten outperforming feed-forward networks in extracting mean- ingful features and achieving higher predictive accuracy [4]. Unlike feed-forward architectures, RNNs compute hidden states from a sequence of inputs using recurrent or feedback connections, making their computational graphs inherently sequential. Figure 1 contrasts the information flows in feed- forward networks compared to RNNs. This capability makes them particularly well-suited for time-series applications, such as energy consumption forecasting in power grids and stock price prediction in high-frequency trading (HFT). Such appli- cations often demand an energy-efficient, low-latency acceler- ation of LSTM models on platforms such as FPGAs, where custom operators, precisions and sparsity can be exploited during the design and implementation phase. However, the recurrent dataflow limits the ability of existing frameworks to support efficient RNN deployments. Popular neural network representation formats, such as Open Neural Network Exchange (ONNX), simplify deploy- ment by providing pre-packaged LSTM nodes that abstract away the recurrence [5]. While this abstraction facilitates standardisation and interoperability, it restricts customisation of the internal data flow, particularly for applying fine-grained quantisation or optimising specific hardware implementations. Consequently, (a) ONNX s limited support for custom op- erations with feedback connections poses a challenge for development, and (b) the sequential nature of RNNs inherently limits parallelisation in FPGA-based implementations [6]. Hence, existing toolchains have seen limited generalisable extensions to support models with recurrent connections. On the contrary, developing a standalone toolchain for generalized RNN deployment on FPGAs will likely see limited adoption. A more practical approach is the extension of established open-source frameworks to support RNNs. This work aims at addressing this challenge by enhancing the RNN deployment capabilities within widely-used FPGA-based ML frameworks, paving the way for efficient, scalable, and energy-conscious solutions for real-time time-series applications. In this paper, we extend the widely used open-source FINN framework to support the generalized development and deployment of LSTM networks on FPGAs. Figure 2 illus- trates our contributions in relation with the end-to-end FINN flow. Through these contributions, we enable the efficient and flexible FPGA-based deployment of LSTMs, effectively addressing the challenges posed by their recurrent structure and complex computations. Our specific contributions in this work are outlined below: ONNX Representation: Development of an ONNX representation for cus- tom LSTM models, utilising the Scan operator to capture the recurrent nature of LSTM computations. The implementation is open-sourced as part of the qonnx repository, providing a reusable and extensible framework for quantised LSTM representations. This representation integrates LSTM layers seam- lessly with standard neural network layers, enabling the functional software simulation of LSTM-based networks. This representation supports mixed quantisation within an LSTM layer, broadening the scope for efficient model deployment. FINN Compiler Transformations: Introduction of new transformations within the FINN compiler to map LSTM computations to hardware blocks available in the HLS kernel library of the compiler (finn-hlslib). Key transformations include: Conversion of quantised activation functions (sig- moid and tanh) into representations compatible with existing FINN hardware blocks, ensuring efficient hardware execution. Enhancements to network optimisation transfor- mations (e.g., Streamlining [8]) to improve the hardware deployment of the LSTM compute graph, reducing resource usage and latency. HLS Layer Development: Construction of hardware for LSTM layers using building blocks from finn-hlslib, enabling a fully customisable implementation in terms of data types, dimensions, and other architectural parameters. This design also enhances parallelism, improving compu- tational efficiency while maintaining reusability for various FPGA deployments. Enabling the generation of LSTM networks as In- tellectual Property (IP) blocks for FPGA designs, facilitating broader usability. The implementation of this layer is open sourced as part of the FINN-GLSTM-Hw repository. Deployment: Demonstrating the seamless integration of an LSTM IP block within a neural network that includes con- volutional and dense layers, enabling mid-price stock prediction in HFT scenarios. This work bridges a critical gap in the deployment of LSTMs on FPGAs, providing a comprehensive methodology for leveraging the power of RNNs in hardware-accelerated environments. The rest of the paper is organised as follows. Section II discusses different LSTM implementations proposed in the literature as well as background on limit order books and the reasoning for extending FINN. Section III discusses the development stages of the implementation. Section IV dis- cusses the case study on integrating the quantised-ConvLSTM model for the mid-price stock prediction use case for HFT environments. We conclude the paper in Section V. II. BACKGROUND AND RELATED WORK A. Recurrent Neural Networks RNNs were introduced in the 1990s primarily for se- quence prediction tasks, such as speech recognition and natural language processing. Their key advantage is the ability to maintain hidden states that capture information from previous inputs [9]. This ability to model tempora l dependencies makes them especially useful in applications where context and patterns evolve over time. Despite their advantages, RNNs were challenging to train due to issues such as the vanishing gradient problem, which were later addressed with advanced architectures like LSTMs and GRUs. LSTMs and GRUs have since found widespread applications in many sequence prediction tasks. The compute equations (gates and states) for state computations in LSTMs are shown below: ft œÉ(Wfxt Ufht 1 bf) (1) it œÉ(Wixt Uiht 1 bi) (2) Ct tanh(Wcxt Ucht 1 bc) (3) Ct ft Ct 1 it Ct (4) ot œÉ(Woxt Uoht 1 bo) (5) ht ot tanh(Ct) (6) Network description: Pytorch Brevitas Frontend Streamline HW Conversion Folding Compiler Transformations Synthesis with Vivado and Vitis for different targets Backend Customised hardware solution with runtime environment Deployment ONNX representation Model development : QLSTM layer QLSTM computation encapsulated in scan operator FINN-ONNX LSTM specific quantised activations to thresholding layers New and modified transformations for QLSTM Reusable and efficient HW building block for QLSTM QLSTM IP integration with standard FINN layers Fig. 2: This figure illustrates the different stages in the overall deployment workflow for generalized quantised LSTM layer (with mixed quantisation support) based models as an extension of the FINN framework for FPGAs. This figure is inspired by Figure 6 in [7]. where œÉ represents the sigmoid activation function, tanh is the hyperbolic tangent activation, and denotes element-wise multiplication. Due to the computationally intensive nature of these operations, LSTMs are often accelerated using bespoke hardware implementations on FPGAs to support real-time applications, as discussed in the next subsection. B. LSTMs on FPGAs Multiple FPGA accelerators and frameworks with RNN support have been proposed in the research literature. Khoda et al. [10] extend the hls4ml framework by adding support for LSTM deployments on FPGAs. They showcase three example implementations with varied LSTM layer complexity. Que et al. [11] present a hardware architecture to address data dependency issues in LSTM computations along with a new blocking and batching strategy that enhances the weight reuse in LSTM operations. Their approach minimises external memory access, optimising the system performance for large LSTM-based models on devices with limited on- chip memory capacity. F-LSTM [12] is a framework for deploying LSTM-based models on FPGAs using the CPUs to carry out pre- and post-processing. The authors test their implementation on sentiment analysis and achieve improve- ments over a reference GPU implementation. Li et al [13] propose an efficient sparse LSTM accelerator on embedded FPGAs with bandwidth-oriented pruning for reducing the on-chip memory demand. They train and test their LSTM model on the TIMIT dataset demonstrating an optimised deployment on the PYNQ-Z1 FPGA platform. The C-LSTM accelerator [14] leverages a structured compression technique (as opposed to random pruning) to reduce model size while avoiding the introduction of irregularities in computation and memory accesses. Rybalkin et al. [6] propose FINN-L, an extension of FINN to deploy parametrisable quantised LSTMs on FPGAs. They implement a Bi-LSTM model for OCR recognition tasks on the ZCU104 FPGA platform. This has, however, remained a custom implementation not integrated with the general FINN workflow. Ribes et al. [15] propose solving the deployment constraints posed by stacked LSTM models on FPGA by altering their computational structure optimising memory requirements. Ioannou and Fahmy [16] propose a flexible overlay architecture for LSTMs on hybrid FPGAs. They propose a streaming dataflow arrangement ex- ploiting the capabilities of DSP blocks while also mitigating external memory overheads. While the above implementations introduce novel deployment architectures for LSTMs, they do not provide a generalized mixed precision development and deployment workflow, limiting the broader adoption of efficient LSTM models on FPGAs. C. FINN FINN facilitates the rapid development and deployment of quantised neural networks (QNNs) on both edge and datacen- ter FPGAs [2]. Widely adopted by the open-source research community, it has evolved into an efficient toolchain for acceleration of quantised machine learning models. A distin- guishing feature of this framework is its use of a thresholding operator to implement quantised activation functions. This operator maps input values to integers in the interval [0, n] by comparing the input to a set of threshold values and returning an integer value that specifies the number of threshold values that the input is greater than or equal to. By adjusting the spacing between threshold values, this operator can model any monotonically increasing activation function, such as, ReLU, sigmoid, and tanh. The efficient, hardware-friendly implementation of the latter two relying on comparisons rather than the explicit computation of exponentials is key for an efficient implementation of LSTMs. Figure 3 illustrates how to operator models the tanh activation at three different INT quantisation levels. We demonstrate that our extensions to the FINN framework encompass all essential components, including FINN compiler transformations that map the LSTM compute graph to pre-built hardware building blocks, along with support for converting custom operators such as mapping sigmoid and tanh acti- vations to comparison based thresholding operations available through finn-hlslib. These components enable the construction of hardware-accelerated LSTM layers within the framework, allowing the seamless integration into larger models. A key advantage of quantisation using FINN is that all LSTM layer weights can be stored in the on-chip memory of the FPGA, enhancing performance and addressing limitations encountered in other implementations [13]. As a result, we chose to extend this framework to develop a generalized deployment pipeline for recurrent neural networks. D. Limit Order Books A limit order book (LOB) is a record of pending limit orders maintained by an exchange. Each limit order is an instruction to buy or sell a security at a specified price or better. When limit orders are placed, they are recorded at the exchange by updating the order book to track them. Orders are executed when the market reaches or exceeds their specified price levels [17]. LOBs represent investors sentiments and, hence, can be used to predict the change in stock prices for certain time horizons. With the availability of public LOB datasets, multiple machine-learning stock price prediction models have been proposed in the literature. These architectures range from 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Inputs (x) 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Activation (y) tanh(x) (Original) INT2 (4 levels) INT4 (16 levels) INT6 (64 levels) Fig. 3: The figure illustrates how the Multithresholding oper- ator approximates the tanh activation function over a defined input range for INT2, INT4 and INT6 bitwidths. traditional machine learning techniques like support vector machines (SVMs) to deep learning architectures like CNNs, LSTMs, ConvLSTMs and attention-based architectures [1], [18] [22]. The authors in the above papers utilise the general- isable nature of deep learning architectures for trend prediction with high accuracies. However, the majority of the papers fail to report their deployment latency numbers to evaluate the performance of their models in HFT environments where the time interval between two events can be in the order of nano- and microseconds [1]. III. DEVELOPMENT STAGES In this section, we outline the different stages in LSTM development, from the front-end ONNX representation to mapping LSTM computations onto the hardware building blocks available in FINN. A. Frontend: QLSTM QCDQ representation ONNX is an open standard designed to represent machine learning models. It defines a comprehensive set of operators (like Matmul, ReLU), which serve as the fundamental building blocks for constructing machine and deep learning models. ONNX aims to provide a unified format that fosters interoper- ability, enabling seamless integration and interaction between various ML tools and frameworks [5]. The ONNX represen- tation also serves as a foundation for various ML hardware development frameworks, such as FINN. Additionally, ONNX supports modeling quantisation operators, such as Quantize- Linear and DequantizeLinear, enabling the representation of QNNs with standard ONNX operators. However, ONNX traditionally supports operators designed for forward computation and lacks extensive support for recurrent computations. ONNX recently introduced the Scan operator to facilitate modeling of recurrent connections [23]. This operator processes one or more input tensors iteratively to generate zero or more output tensors. It is inspired by general recurrences and functional programming constructs, such as scan, fold, map, and zip. During each iteration, the inputs to the operator consist of the current values of its state variables and the specific element of the input tensor being processed. Its Scan Op : Body LSTM computation executed in each iteration (h_init, c_init) X1 X2 X3 Scan Inputs : Initial hidden and cell states processing inputs X tanh X X tanh ct-1 ht-1 ct ht c1 c2 c3 h1 h2 h3 Scan Outputs : Concatenated hidden states Last hidden state Sequence size 3 Inp-1 (t 0) Inp-2 (t 1) Inp-3 (t 2) h1 - t 1 h2 - t 2 c1 - t 1 c2 - t 2 Fig. 4: The figure illustrates how the Scan operator replicates the recurrent hidden state computations in an LSTM layer. The yellow, red, and blue arrows highlight the pathways for input data processing and state updates at each time step for every input in the sequence. outputs comprise the updated state variables and one or more output tensors. The computation performed in each iteration is defined by the operator s body attribute that encodes a computational graph. The operator has unique properties that differentiate it from other ONNX compute nodes: It allows the state variable to be updated after each iteration, enabling the updated state to be used in the processing of the next input. It processes inputs sequentially, either row by row or column by column, continuously updating the hidden state for each input while storing the hidden states at each step. We utilise this container operator to encapsulate the LSTM compute graph within its body, effectively modelling the recurrent computational structure of the LSTM layer. The quantised ONNX implementation of the layer is constructed using the QuantizeLinear, Clip, and DequantizeLinear oper- ators, forming a QCDQ representation that exactly follows the QuantLSTM implementation in Brevitas (a framework for training quantised neural networks [24]). To verify its functional correctness, we compare our quantised LSTM layer implementation against this QuantLSTM layer. Figure 4 illus- trates the internal workings of the Scan operator in the context of the LSTM compute graph over three time steps. The primary advantage of developing this QCDQ-based implementation using the Scan ONNX operator is that it provides precise control over the bit-widths of all 11 internal quantisers within the QuantLSTM layer [25]. This capability directly facilitates the rapid development and prototyping of mixed-precision quantised LSTM-based models, enabling fine- grained exploration of trade-offs between model accuracy and hardware efficiency. In contrast, the standard LSTM operator in ONNX does not expose the internal computational structure of the node, making such customisations infeasible. B. FINN: Compiler Transformations After functionally verifying the QCDQ LSTM layer im- plemented with the Scan operator, the QCDQ computation graph should be transformed to a set of pre-built hardware building blocks from the finn-hlslib kernel library. At the time of this project, the finn-hlslib kernels were limited to supporting integer-only compute. However, the quantised graph contains floating-point operations coming from scale and bias operations of the quantisers. To achieve a functionally equivalent integer-only compute graph, we leverage FINN s streamlining process, which moves floating-point operations towards thresholding layers and merges them by updating the threshold values [8]. To achieve this, the QCDQ graph is trans- formed to present quantisers as thresholding layers utilising the quantised ONNX (QONNX) format. This allows us to reuse existing conversions and transformations available within the FINN flow and adapting them when required to support the quantised recurrent representations. We adopted the QCDQ-to- QONNX conversion because at the time of this work Brevitas does not support direct functional QONNX export for QLSTM layers. Instead, we used Brevitas solely to extract weights and quantisation parameters, constructing a custom QONNX graph from the corresponding QCDQ nodes. QONNX project extends ONNX framework by introducing custom quantisation operators (Quant, BipolarQuant, Trunc) to support arbitrary- precision uniform quantisation [26]. The conversion from QCDQ is achieved by applying the ConvertQCDQtoQONNX transformation to the graph. This transformation converts the QCDQ operators into Quant nodes, as shown below: QuantizeLinear Clip DequantizeLinear Quant (7) The QONNX LSTM graph is subsequently converted to the FINN-ONNX representation for further processing through the FINN flow. This conversion is performed using the ConvertQONNXtoFINNONNX transformation, which re- places QONNX operators and activations with the matmul, multithreshold operators, as shown below: Tanh Quant Multithreshold Mul Add (8) Finally, the LSTM computation graph also includes sig- moid and tanh activations, which are not directly sup- ported in the above transformation. However, since they are both monotonically increasing, we model these using threshold operators. We developed functions that will generate thresholds for tanh and sigmoid activations utilised in the ConvertQONNXtoFINNONNX tranformation to achieve the the result shown in eq 8. Subsequently the graph is functionally verified and passed to downstream layers for further stream- lining. At this stage, the primary goal is to minimise floating- point operations in the computation graph (the Mul and Add nodes introduced in the previous step) and eliminate operators by fusing absorbing them wherever possible to reduce the resource usage of the design. We try to achieve this by applying pre-existing transformations to the graph wherever possible and developing transformations (new or adaptations of existing FINN transformations) for unique compute patterns that are not fully absorbed (streamlined). Table I describes all the transformations used in the quantised LSTM graph streamlining process. The transformations we developed re- arrange the computations in the graph to position the floating- point operators close to the next multi-thresholding operator, allowing it to be absorbed by it. Figure 5 illustrates examples where our transformations allow the floating point operations to be absorbed into the nearest multi-thresholding operator. These transformations allow the LSTM graph to be fully streamlined, where all computations can be represented by (hardware) library functions in FINN and can hence be com- posed into hardware using the FINN-HLS backend. Figure 6 TABLE I: The table shows the list of pre-existing transfor- mations used to transform LSTM compute graph along with new modified transformations. Pre-existing transformations New Modified Transformations MoveAddPastMul() MoveScalarMulPastMatMul() MoveScalarAddPastMatMul() MoveLinearPastEltwiseMul() MoveAddPastMul() AbsorbMulIntoMultiThreshold() CollapseRepeatedAdd Mul() AbsorbsignbiasintoMultithrehsold() AbsorbAddIntoMultiThreshold() RoundAndClipThresholds() MoveScalarMulPastMatMul() : Added support for moving scalar Mul past each consumer Matmul in the fork. MoveLinearPastEltwiseMul() : Added support for moving join Mul's past element wise mul operator AbsorbMulIntoMultithreshold() : Added suppport to absorb scalar mul into multiple multithreshold nodes Fig. 5: The figure illustrates examples of which transforma- tions in the FINN compiler were modified and the result they achieved. summarises the transformation process to obtain the final streamlined graph. C. QLSTM HLS Backend The backend flow maps the QCDQ representation to a functionally equivalent graph consisting of hardware blocks that implements the operators transformations. This produces a quantised LSTM (Q-LSTM) computation graph with the following operators: Threshold, MatMul, Elementwise Add, and Elementwise Mul. As before, we leverage pre- existing hardware blocks available in finn-hlslib which were developed for mapping feed-forward networks (using Vitis HLS) to map the forward computation path of LSTM layers. In the case of recurrent (loopback) connections, we utilise a QCDQ QONNX FINN-ONNX Streamlined ONNX Nodes: QuantizeLinear Clip DequantizeLinear MatMul ReLU, Sigmoid, Tanh Nodes: Quant MatMul ReLU, Sigmoid, Tanh Nodes: Multithreshold Scalar Mul Add ElementwiseMul Add MatMul Nodes: Multithreshold ElementwiseMul Add MatMul Note : Comprises of floating point operations Note : Floating point operations eliminated Fig. 6: The figure demonstrates the compute nodes each representation comprises of in the process of obtaining the final streamlined quantised LSTM compute graph. standard for construct to map these to hardware blocks, whose bounds are determined by the input sequence length of the model. This loop structure introduces sequential dependencies in the LSTM computation graph and imposes a limitation on parallelisation (through unrolling). A longer input sequence length can increase the initiation interval (II) of the loop, which could be a constraint for real-time applications. Utilising the building blocks enables us to design a generic hardware layer with customisable parameters such as data types, input size, hidden layer dimensions, and sequence length. This allows for simple integration in the FINN hardware generation flow and provides a flexible interface for arbitrary settings. Once the design is complete, we synthesise the accelerator and export it as an independent IP block. D. QLSTM layer hardware testing After generating the independent LSTM IP block using Vitis HLS, we integrate it with an AXI-DMA-based subsystem in the Vivado IP Integrator to build the bitstream, which is then tested on the FPGA. For integration and functional verification, as well as benchmarking, we follow the three-part tutorial1, which demonstrates the complete integration process. The PYNQ APIs are used to drive the IP with input data and retrieve the output, allowing for functional verification of the hardware operation. IV. INTEGRATION CASE STUDY In this section, we present a case study for accelerating an LSTM-based model on an FPGA using our extensions for FINN, for predicting stock price trends using the openly available LOB dataset (normalised version). Our model design closely follows the architecture of the most successful ap- proach reported in the literature for this dataset [1]. The details of the case study, including design choices and implementation aspects, are provided in the following subsections. A. Model development 1) ConvLSTM-HFT classification model: We replicate the ConvLSTM architecture of DeepLOB [1] for our case, and evaluate the effectiveness of the proposed FINN extensions and the integration with standard FINN flow. DeepLOB utilises a series of convolutional blocks to extract features from a segment of order-book data, which is then passed through an LSTM layer to capture the time-series dependencies within the features. A final dense layer is employed for classification 1 3344 of stock price trend prediction. In our case, we use two convolutional blocks to extract features from a block of 100 order-book entries, followed by an LSTM layer for time-series learning, and a dense layer at the output for classification. The key distinction between our implementation and DeepLOB lies in the use of inception blocks; DeepLOB incorporates inception blocks in its convolutional layers, while our model does not. Our experiments indicate that omitting inception blocks does not result in a significant loss of accuracy, as shown in the evaluation subsection. Each convolutional block in our model uses three Conv2D layers. The first block utilizes filters of sizes 64, 32, and 32, with a higher number of filters in the initial layer to maximise feature extraction. The second block employs filters of sizes 64, 16, and 4, designed to extract and preserve the most salient features for subsequent processing by the LSTM layer. To balance effective feature extraction with computational effi- ciency, the first layer in each convolutional block is configured with a stride of 2, facilitating dimensionality reduction and reducing the complexity of the subsequent LSTM layer. All convolutional layers utilise a (3 3) kernel size and are fol- lowed by a Batch Normalization layer and a ReLU activation function to enhance training stability. The LSTM layer that follows the convolutional blocks is configured with 64 hidden units, an input size of 40, and a sequence length of 25. The classification stage consists of two fully connected layers with 256 and 3 neurons, respectively, where the final layer corre- sponds to the output classes. The complete model comprises of approximately 141K trainable parameters. The final model configuration was derived through an iterative process aimed at balancing performance and model complexity, to achieve good model quantisation performance and resoure efficiency during the final hardware implementation. This exploration involved several key design decisions, including the selection of the number of convolutional blocks, the number of filters, filter sizes, and strides within each block (to optimise the input size for the LSTM layer), the number of hidden units in the LSTM layer, and the configuration of the final classification module such as the number of dense layers and the number of neurons in each layer. 2) Quantising the ConvLSTM model: Once the final con- figuration of the floating-point model was established, based on test accuracy metrics, we proceeded to quantise the model using a W8A6 (8-bit weight, 6-bit activation) scheme with the Brevitas library [24]. While the standard Q-Conv and Q- Linear layers in Brevitas allow quantisation of the weights, the proposed Q-LSTM layer offers additional flexibility due to its internal computation structure, enabling both weight quantisation and the selection of quantisation levels for the different activation quantisers within the layer. In our imple- mentation, we apply uniform quantisation scheme (W8A6) to the LSTM layer as well as the convolutional and dense layers. This configuration was chosen after systematically reducing the activation bit-width and observing the point at which a significant degradation in model accuracy occurred. The next subsection provides detailed descriptions of the training process for both the floating-point and quantised models. B. Dataset and Training 1) Dataset: We utilise the FI-2010 dataset, which captured LOB data for five stocks traded over a 9-day period on TABLE II: The table describes the LOB features comprised in the FI-2010 dataset. Feature Set Description u1 {Paski, Vaski, Pbidi, Vbidi}n i 1 10-level LOB data u2 {(Paski Pbidi), (Paski Pbidi) 2}n i 1 Spread and Mid-price u3 {Paskn Pask1, ..., Pbidi 1 Pbidi }n 1 i 1 Price differences u4 1 n Pn i 1 Paski, 1 n Pn i 1 Pbidi, 1 n Pn i 1 Vaski, 1 n Pn i 1 Vbidi Price and Volume means u5 Pn i 1(Paski Pbidi), Pn i 1(Vaski Vbidi) Accumulated differences u6 {dPaski dt, dPbidi dt, dVaski dt, dVbidi dt}n i 1 Price and Volume derivation u7 {Œª1 t, Œª2 t, Œª3 t, Œª4 t, Œª5 t, Œª6 t} Average intensity per type u8 1Œª1 t Œª1 T , 1Œª2 t Œª2 T , ..., 1Œª6 t Œª6 T Relative intensity comparison u9 {dŒª1 dt, dŒª2 dt, dŒª3 dt, dŒª4 dt, dŒª5 dt, dŒª6 dt} Limit activity acceleration the Helsinki Stock Exchange [19] to train the quantised- ConvLSTM model. Each record comprises 144 features and five labels that corresponds to future price movements over prediction horizons of 1, 2, 3, 5, and 10 timesteps (each timestep being 192 ms apart). The labels classify the stock s movement direction as downward (0), stationary (1), or upward (2) after each horizon k. The input features capture a comprehensive view of the order book, including bid ask prices, bid ask volumes, their derivatives, and other metrics that represent a 10-level order book structure. A detailed breakdown of these features in the dataset is provided in Table II. Although the original dataset is not publicly available, three normalized versions z-score, min-max scaling, and decimal precision normalisation have been made available for research purposes. Out of the three, we use the z-score normalised version of the dataset which has widely adopted in the literature for predicting the trends of the stocks in the dataset [1]. 2) Training: The z-score version of the dataset uses nor- malised floating-point features, which were quantised to INT8 precision for training the Q-ConvLSTM model. From the dataset, we used the first seven days of data for training and validation purposes with 90 10 split. We used the remaining two days of data for testing the model, following the same training approach presented in [1]. Quantising features into lower integer ranges ( INT8) significantly hindered model convergence, as the narrow range caused many similar feature values to map to the same integer, diminishing useful informa- tion. We started with the training of the floating-point model on the dataset to achieve a baseline model with acceptable accuracy. We then used the trained floating-point model as a starting point to train and fine-tune the quantised version of the model, improving convergence by 10 compared to training the quantised version of the model from scratch. In all the training flows, the model with the lowest validation loss was chosen for testing. Both the floating-point and quantised models were trained for 100 epochs with early stopping mechanisms that triggers an exit from loop if no decrease of validation loss was observed for 20 consecutive epochs. We used the cross-entropy loss function and the Adam optimiser with a learning rate of 0.001 and a batch size of 16 for training the floating point model. To train the quantised model, we used the same hyperparameters as the floating point version except the batch size, which was set to 32 to reduce the training time. Post training, we observed that the quantised model suffered from an accuracy drop of 1.5 . With further fine-tuning, where we trained this model further using a faster (0.01) learning rate, the model achieved 1 higher accuracy than the floating-point version, and was passed through to the hardware implementation phase. Table III shows the comparison of the DeepLOB, ConvLSTM and Q-ConvLSTM model trained on the FI-2010 dataset for different accuracy metrics in detail. C. Experimental Evaluation 1) Hardware generation and deployment: The trained model is compiled using the proposed extensions integrated into the FINN flow to generate a dataflow accelerator IP (mvau width 36 and target fps 1000) and evaluate the effectiveness of our proposed extensions and transformations. We chose a Zynq Ultrascale platform as the target (XCZU7EV device on the ZCU104 development board) mimicking an edge deployment scenario, with the tools successfully synthesizing the IP for a clock frequency of 150 MHz. The IP was subsequently integrated using the IPI flow to evaluate hardware performance. From our testing, we determine the batch-1 processing latency for the quantised ConvLSTM network to be 4.3 ms, excluding any data move- ment overheads, identical to the cosimulation results observed in Vitis HLS. For the specific application of trend prediction, the average interval between consecutive events in the dataset is approximately 192 ms [1]. Thus, the network processes data significantly faster than the required decision-making window of ten time steps ( 2 seconds), providing enough time for stakeholders to analyse and respond to market changes and make informed decisions. Higher operating performance can be achieved if the network is further unrolled, targeting a datacentre class FPGA (such as the Alveo), and by replicating the LSTM cell (reducing the sequential dependency), at the cost of higher resource consumption. For the Zynq deploy- ment, the resource utilisation of our design is summarized in Table V. The model uses approximately 49 of LUTs, 13 of flip-flops (FFs), and 15 of DSP blocks, leaving room for additional network unfolding to achieve further latency reduction, if required. More importantly, we were able to determine that the proposed mapping flow maintains a one- to-one mapping between the trained Brevitas model and the generated hardware, demonstrating the functional effectiveness of the applied graph transformations. 2) Accuracy: To further assess the accuracy of the gen- erated Q-ConvLSTM IP, we evaluate its performance using precision, recall, and F1-score metrics for each of the three trend prediction labels: downward, stationary, and upward. We compare our quantised implementation to both the floating- point ConvLSTM model (in PyTorch) and the state-of-the-art DeepLOB model to determine the impact of quantisation on its classification performance. Table III presents this comparative analysis, illustrating that the generated quantised IP maintains an F1-score comparable to its floating-point counterpart and the DeepLOB across all three trend categories. Table IV summarises the comparative evaluation of Q- ConvLSTM IP generated by the proposed flow relative to existing approaches in the literature, in terms of inference accuracy. We replicated the DeepLOB model and indepen- dently reproduced the results for our comparison, following the same training pipeline specified in the original DeepLOB paper [1]. We also use macro F1-score as the primary eval- uation metric, as done by approaches in the literature, to mitigate the risk of inflated accuracy metrics that could result from the inherent class imbalance in the dataset. Our results show that the generated Q-ConvLSTM IP through our FINN extensions demonstrates superior prediction performance over several widely used financial prediction models. Specifically, it outperforms: SVM [18] by 42 , MLP [18] by 29 , TABLE III: Performance comparison of DeepLOB, ConvLSTM and Q-ConvLSTM model for the three different labels in the FI-2010 dataset. Class Metric DeepLOB ( ) ConvLSTM ( ) Q-ConvLSTM ( ) Downward-0 Precision 76.50 75.20 76.66 Recall 76.68 77.26 78.44 F1-Score 76.59 76.22 77.54 Stationary-1 Precision 82.92 81.66 83.25 Recall 77.95 71.23 73.50 F1-Score 80.36 76.09 78.07 Upward-2 Precision 74.69 72.36 73.53 Recall 78.77 79.06 79.93 F1-Score 76.68 75.56 76.60 TABLE IV: Comparison of the proposed Q-ConvLSTM model against the proposed state-of-the-art HFT models in the re- search literature. Model Accuracy ( ) Precision ( ) Recall ( ) F1 ( ) Prediction Horizon k 10 SVM [18] - 39.62 44.92 35.88 MLP [18] - 47.81 60.78 48.27 CNN-I [21] - 50.98 65.54 55.21 LSTM [18] - 60.77 75.92 66.33 CNN-II [22] - 56.00 45.00 44.00 T-BoF [27] 66.50 44.60 68.30 44.70 B(TABL) [20] 78.91 68.04 71.21 69.20 C(TABL) [20] 84.70 76.95 78.44 77.63 DeepLOB [1] 77.78 78.04 77.80 77.88 Q-ConvLSTM 77.37 77.81 77.29 77.40 CNN-I [21] by 23 , LSTM [18] by 11 , CNN-II [22] by 33 , B(TABL) [20] by 8 . The Q-ConvLSTM IP achieves performance equivalent to the C(TABL) [20] and DeepLOB [1] models, both of which incorporate sequential processing layers in their architectures. This result is particularly note- worthy as it demonstrates that quantised LSTM models can match the performance of floating-point counterparts, and that these quantised variants can be efficiently mapped to resource constrained devices using our flow. D. Comparison against other frameworks A comparable approach for mapping LSTM models to FPGAs is available through the hls4ml flow [10]. While both methods offer a generalised flow for mapping an LSTM accelerator on FPGA, our proposed flow using FINN offers a few key advantages. A key distinction is that our approach constructs a quantised ONNX representation of the LSTM compute graph, modeling its functionality using the Scan operator. This provides full control over all quantisation pa- rameters, enabling mixed-precision quantisation for the LSTM layer an aspect that, to our knowledge, is not available using the hls4ml flow. Additionally, our approach effectively leverages existing FINN compiler transformations and introduces few new ones to generate a computation graph that is entirely free of floating- point operations. This enables primary computations within the graph to rely solely on integer arithmetic to achieve significant resource savings during the hardware generation phase, allowing seamless deployment on resource-constrained FPGAs, as demonstrated in our integration case study. In contrast, hls4ml retains floating point operators in the compute graph, and has primarily targeted LSTM deployments on large FPGA fabrics. The use of fixed-point datatypes with fractional TABLE V: The resource consumption of the Q-ConvLSTM accelerator on the ZCU104 FPGA. Q-ConvLSTM LUTs FFs LUTRAM BRAMs DSPs Conv Network 65091 36398 4596 96 252 LSTM Network 49439 22601 6631 31.5 13 Overall 101489 58999 11227 127.5 265 49.6 12.8 11 40.8 15.2 bits in hls4ml is effective in certain cases at the expense of increased resource consumption. Also, to reduce the II, hls4ml proposes a non-static imple- mentation of LSTM layers, where multiple LSTM compute units are instantiated to match the input sequence length. This design allows computed states to be passed efficiently to subsequent stages, enabling new inputs to be processed every clock cycle and achieving an II of 1. However, this approach results in high resource utilisation, which may limit scalability for complex models with a large number of intermediate states. While our approach does not consider this implementation scheme at present, we recognize its potential and see value in exploring its integration with our resource-efficient design methodology. Finally, both frameworks share the broader goal of providing a parameterisable hardware layer to streamline LSTM deploy- ment. However, while our approach prioritises optimisation for constrained FPGA devices by balancing performance with efficient resource utilisation, the hls4ml approach prioritises latency minimisation at the expense of high resource usage. V. CONCLUSION AND FUTURE WORK In this paper, we extend the FINN framework by developing support for the generalized deployment of recurrent neural networks (using LSTMs as a working example) on FPGAs. We create a complete end-to-end framework and introduce the Scan operator from ONNX to enable modelling of mixed- precision quantised LSTM layers within FINN. Additionally, we design and modify FINN compiler transformations to optimise its compute graph and efficiently map operations to hardware blocks available in the FINN backend, while sup- porting arbitrary integer datatypes, dimensions and sequence lengths. We further present a case study for stock prediction using a model architecture that uses quantised LSTM layers with convolutional and dense layers (quantised ConvLSTM model). The quantised ConvLSTM model compiled to an accelerator IP core that can be deployed on FPGAs using our extended FINN flow. We show that our flow can generate a resource-efficient hardware accelerator IP of the model that meets the performance requirements even when mapped on an edge-class hybrid FPGA (Zynq Ultrascale XCZU7EV), and offers competitive inference performance when evaluated using the FI-2010 dataset for mid-price stock prediction. In the future, we aim to explore optimisations to reduce the latency introduced by the sequential layers in the LSTM flow, taking inspiration from the hls4ml approach (using non- static implementations for LSTM layers), and to extend the deployment pipeline to support other RNN models such as gated recurrent units (GRUs). VI. ACKNOWLEDGEMENTS I would like to thank Alessandro Pappalardo and Dr. Ya- man Umuroglu for their valuable insights and the engaging brainstorming sessions throughout the course of this project. REFERENCES [1] Z. Zhang, S. Zohren, and S. Roberts, Deeplob: Deep convolutional neural networks for limit order books, IEEE Transactions on Signal Processing, vol. 67, no. 11, pp. 3001 3012, 2019. [2] M. Blott, T. B. Preu√üer, N. J. Fraser, G. Gambardella, K. O brien, Y. Umuroglu, M. Leeser, and K. Vissers, FINN-R: An end-to-end deep- learning framework for fast exploration of quantized neural networks, ACM Transactions on Reconfigurable Technology and Systems (TRETS), vol. 11, no. 3, pp. 1 23, 2018. [3] J. Duarte et al., Fast inference of deep neural networks in FPGAs for particle physics, JINST, vol. 13, no. 07, p. P07027, 2018. [4] L. P. Joseph, R. C. Deo, R. Prasad, S. Salcedo-Sanz, N. Raj, and J. Soar, Near real-time wind speed forecast model with bidirectional LSTM networks, Renewable Energy, vol. 204, pp. 39 58, 2023. [5] ONNX, [6] V. Rybalkin, A. Pappalardo, M. M. Ghaffar, G. Gambardella, N. Wehn, and M. Blott, FINN-L: Library extensions and design trade-off analysis for variable precision LSTM networks on FPGAs, in 2018 28th international conference on field programmable logic and applications (FPL), pp. 89 897, IEEE, 2018. [7] T. Alonso, L. Petrica, M. Ruiz, J. Petri-Koenig, Y. Umuroglu, I. Stame- los, E. Koromilas, M. Blott, and K. Vissers, Elastic-df: Scaling perfor- mance of dnn inference in fpga clouds through automatic partitioning, ACM Trans. Reconfigurable Technol. Syst., vol. 15, Dec. 2021. [8] Y. Umuroglu and M. Jahre, Streamlined deployment for quantized neural networks, arXiv preprint arXiv:1709.04060, 2017. [9] T. Robinson, M. Hochberg, and S. Renals, The use of recurrent neural networks in continuous speech recognition, in Automatic Speech and Speaker Recognition: Advanced Topics, pp. 233 258, Springer, 1996. [10] E. E. Khoda, D. Rankin, R. T. de Lima, P. Harris, S. Hauck, S.-C. Hsu, M. Kagan, V. Loncar, C. Paikara, R. Rao, et al., Ultra-low latency recurrent neural network inference on FPGAs for physics applications with hls4ml, Machine Learning: Science and Technology, vol. 4, no. 2, p. 025004, 2023. [11] Z. Que, Y. Zhu, H. Fan, J. Meng, X. Niu, and W. Luk, Mapping large lstms to fpgas with weight reuse, Journal of Signal Processing Systems, vol. 92, pp. 965 979, 2020. [12] Liang, Bushun and Wang, Siye and Huang, Yeqin and Liu, Yiling and Ma, Linpeng, F-LSTM: FPGA-based heterogeneous computing framework for deploying LSTM-based algorithms, Electronics, vol. 12, no. 5, p. 1139, 2023. [13] S. Li, S. Zhu, X. Luo, T. Luo, and W. Liu, An Efficient Sparse LSTM Accelerator on Embedded FPGAs with Bandwidth-Oriented Pruning, in 2023 33rd International Conference on Field-Programmable Logic and Applications (FPL), pp. 42 48, 2023. [14] S. Wang, Z. Li, C. Ding, B. Yuan, Q. Qiu, Y. Wang, and Y. Liang, C-LSTM: Enabling efficient LSTM using structured compression tech- niques on FPGAs, in Proceedings of the 2018 ACM SIGDA Inter- national Symposium on Field-Programmable Gate Arrays, pp. 11 20, 2018. [15] S. Ribes, P. Trancoso, I. Sourdis, and C.-S. Bouganis, Mapping multiple LSTM models on FPGAs, in 2020 International Conference on Field- Programmable Technology (ICFPT), pp. 1 9, IEEE, 2020. [16] L. Ioannou and S. A. Fahmy, Streaming Overlay Architecture for Lightweight LSTM Computation on FPGA SoCs, ACM Transactions on Reconfigurable Technology and Systems, vol. 16, no. 1, pp. 1 26, 2022. [17] Investopedia, 2022. [18] Tsantekidis, Avraam and Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros, Using deep learning to detect price change indications in financial markets, in 25th European Signal Processing Conference (EUSIPCO), pp. 2511 2515, 2017. [19] A. Ntakaris, M. Magris, J. Kanniainen, M. Gabbouj, and A. Iosifidis, Benchmark dataset for mid-price forecasting of limit order book data with machine learning methods, Journal of Forecasting, vol. 37, no. 8, pp. 852 866, 2018. [20] Tran, Dat Thanh and Iosifidis, Alexandros and Kanniainen, Juho and Gabbouj, Moncef, Temporal attention-augmented bilinear network for financial time-series data analysis, IEEE transactions on neural net- works and learning systems, vol. 30, no. 5, pp. 1407 1418, 2018. [21] Tsantekidis, Avraam and Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros, Forecasting stock prices from the limit order book using convolutional neural networks, in 2017 IEEE 19th conference on business informatics (CBI), vol. 1, pp. 7 12, IEEE, 2017. [22] A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosifidis, Using deep learning for price prediction by exploiting stationary limit order book features, Applied Soft Computing, vol. 93, p. 106401, 2020. [23] ONNX, scan, 2018. [24] A. Pappalardo, Xilinx brevitas, 2021. [25] A. Brevitas, Quantlstm - brevitas tutorials, 2024. [26] A. Pappalardo, Y. Umuroglu, M. Blott, J. Mitrevski, B. Hawks, N. Tran, V. Loncar, S. Summers, H. Borras, J. Muhizi, et al., Qonnx: Repre- senting arbitrary-precision quantized neural networks, arXiv preprint arXiv:2206.07527, 2022. [27] N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosifidis, Temporal bag-of-features learning for predicting mid price movements using high frequency limit order book data, IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 4, no. 6, pp. 774 785, 2018.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.20810v1 [cs.LG] 25 Jun 2025 FINN-GL: Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs Shashwat Khandelwal , Jakoba Petri-Koenig , Thomas B. Preu√üer , Michaela Blott , Shanker Shreejith Reconfigurable Computing Systems Lab, Electronic Electrical Engineering Trinity College Dublin, Ireland, Email: {khandels, Advanced Micro Devices (AMD) Research, Dublin, Ireland Email: {Jakoba.Petri-Koenig, thomas.preusser, Abstract Recurrent neural networks (RNNs), particularly LSTMs, are effective for time-series tasks like sentiment analysis and short-term stock prediction. However, their computational complexity poses challenges for real-time deployment in resource constrained environments. While FPGAs offer a promising plat- form for energy-efficient AI acceleration, existing tools mainly target feed-forward networks, and LSTM acceleration typically requires full custom implementation. In this paper, we address this gap by leveraging the open-source and extensible FINN framework to enable the generalized deployment of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open Neural Network Exchange (ONNX) specification to model the recurrent nature of LSTM computations, enabling support for mixed quantisation within them and functional verification of LSTM-based models. Furthermore, we introduce custom transformations within the FINN compiler to map the quantised ONNX computation graph to hardware blocks from the HLS kernel library of the FINN compiler and Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM model for a mid-price stock prediction task using the widely used dataset and generating a corresponding hardware IP of the model using our flow, targeting the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator through our flow achieves a balance between performance (latency) and resource consumption, while matching (or bettering) inference accuracy of state-of-the-art models with reduced precision. We believe that the generalisable nature of the proposed flow will pave the way for resource-efficient RNN accelerator designs on FPGAs.\n\n--- Segment 2 ---\nWe show that the generated quantised ConvLSTM accelerator through our flow achieves a balance between performance (latency) and resource consumption, while matching (or bettering) inference accuracy of state-of-the-art models with reduced precision. We believe that the generalisable nature of the proposed flow will pave the way for resource-efficient RNN accelerator designs on FPGAs. Index Terms Brevitas, Field Programmable Gate Arrays, FINN, HFT, ONNX, Quantised LSTMs, RNNs I. INTRODUCTION Time-series predictions are increasingly gaining recognition for their ability to allow stakeholders to dynamically optimise resource allocation in real-time, in applications such as en- ergy forecasting and stock market trend prediction, among others. The generalisability of machine learning (ML) models, coupled with the increasing availability of high-quality train- ing data, has led to significant improvements in prediction accuracy compared to traditional hand-tuned algorithms [1]. However, the real-time nature of these applications necessitates deployment on hardware accelerators to meet latency and per- formance requirements. Field-programmable gate arrays (FP- GAs) have emerged as a preferred choice for accelerating ML models in scenarios where energy efficiency, high throughput, and runtime reconfigurability are desirable. Over the past five years, several frameworks have been developed to streamline the prototyping and deployment of ML models, including con- volutional neural networks (CNNs) and unconventional feed- forward architectures (such as, ResNets, Inception nets and U- Nets), by leveraging quantisation techniques to optimise their Vitis, Vivado, PYNQ and combinations thereof are trademarks of Advanced Micro Devices, Inc. Matmul Conv Feedback connections Feed Forward Neural Networks Recurrent Neural Networks Act Matmul Conv Act Matmul Conv Act Matmul Conv Act Concat Matmul Conv Act Inputs Outputs Matmul Conv Act Matmul Conv Act Matmul Conv Act Matmul Conv Act Inputs Matmul Conv Act Matmul Conv Act Concat : ElMul ElAdd State Updates Computed States Fig. 1: The figure contrasts the information flows of con- ventional feed-forward vs. recurrent neural networks.\n\n--- Segment 3 ---\nOver the past five years, several frameworks have been developed to streamline the prototyping and deployment of ML models, including con- volutional neural networks (CNNs) and unconventional feed- forward architectures (such as, ResNets, Inception nets and U- Nets), by leveraging quantisation techniques to optimise their Vitis, Vivado, PYNQ and combinations thereof are trademarks of Advanced Micro Devices, Inc. Matmul Conv Feedback connections Feed Forward Neural Networks Recurrent Neural Networks Act Matmul Conv Act Matmul Conv Act Matmul Conv Act Concat Matmul Conv Act Inputs Outputs Matmul Conv Act Matmul Conv Act Matmul Conv Act Matmul Conv Act Inputs Matmul Conv Act Matmul Conv Act Concat : ElMul ElAdd State Updates Computed States Fig. 1: The figure contrasts the information flows of con- ventional feed-forward vs. recurrent neural networks. While in feed-forward networks, data flows acyclically from input to output, recurrent neural networks utilise the feedback of previously computed state to track sequential dependencies. hardware performance [2], [3]. Despite these advancements, a significant gap remains in the deployment of recurrent neural networks (RNNs), such as long short-term memory (LSTM) networks and gated recurrent units (GRUs). RNNs are ideally suited for time-series data processing, of- ten outperforming feed-forward networks in extracting mean- ingful features and achieving higher predictive accuracy [4]. Unlike feed-forward architectures, RNNs compute hidden states from a sequence of inputs using recurrent or feedback connections, making their computational graphs inherently sequential. Figure 1 contrasts the information flows in feed- forward networks compared to RNNs. This capability makes them particularly well-suited for time-series applications, such as energy consumption forecasting in power grids and stock price prediction in high-frequency trading (HFT). Such appli- cations often demand an energy-efficient, low-latency acceler- ation of LSTM models on platforms such as FPGAs, where custom operators, precisions and sparsity can be exploited during the design and implementation phase. However, the recurrent dataflow limits the ability of existing frameworks to support efficient RNN deployments.\n\n--- Segment 4 ---\nSuch appli- cations often demand an energy-efficient, low-latency acceler- ation of LSTM models on platforms such as FPGAs, where custom operators, precisions and sparsity can be exploited during the design and implementation phase. However, the recurrent dataflow limits the ability of existing frameworks to support efficient RNN deployments. Popular neural network representation formats, such as Open Neural Network Exchange (ONNX), simplify deploy- ment by providing pre-packaged LSTM nodes that abstract away the recurrence [5]. While this abstraction facilitates standardisation and interoperability, it restricts customisation of the internal data flow, particularly for applying fine-grained quantisation or optimising specific hardware implementations. Consequently, (a) ONNX s limited support for custom op- erations with feedback connections poses a challenge for development, and (b) the sequential nature of RNNs inherently limits parallelisation in FPGA-based implementations [6]. Hence, existing toolchains have seen limited generalisable extensions to support models with recurrent connections. On the contrary, developing a standalone toolchain for generalized RNN deployment on FPGAs will likely see limited adoption. A more practical approach is the extension of established open-source frameworks to support RNNs. This work aims at addressing this challenge by enhancing the RNN deployment capabilities within widely-used FPGA-based ML frameworks, paving the way for efficient, scalable, and energy-conscious solutions for real-time time-series applications. In this paper, we extend the widely used open-source FINN framework to support the generalized development and deployment of LSTM networks on FPGAs. Figure 2 illus- trates our contributions in relation with the end-to-end FINN flow. Through these contributions, we enable the efficient and flexible FPGA-based deployment of LSTMs, effectively addressing the challenges posed by their recurrent structure and complex computations. Our specific contributions in this work are outlined below: ONNX Representation: Development of an ONNX representation for cus- tom LSTM models, utilising the Scan operator to capture the recurrent nature of LSTM computations. The implementation is open-sourced as part of the qonnx repository, providing a reusable and extensible framework for quantised LSTM representations.\n\n--- Segment 5 ---\nOur specific contributions in this work are outlined below: ONNX Representation: Development of an ONNX representation for cus- tom LSTM models, utilising the Scan operator to capture the recurrent nature of LSTM computations. The implementation is open-sourced as part of the qonnx repository, providing a reusable and extensible framework for quantised LSTM representations. This representation integrates LSTM layers seam- lessly with standard neural network layers, enabling the functional software simulation of LSTM-based networks. This representation supports mixed quantisation within an LSTM layer, broadening the scope for efficient model deployment. FINN Compiler Transformations: Introduction of new transformations within the FINN compiler to map LSTM computations to hardware blocks available in the HLS kernel library of the compiler (finn-hlslib). Key transformations include: Conversion of quantised activation functions (sig- moid and tanh) into representations compatible with existing FINN hardware blocks, ensuring efficient hardware execution. Enhancements to network optimisation transfor- mations (e.g., Streamlining [8]) to improve the hardware deployment of the LSTM compute graph, reducing resource usage and latency. HLS Layer Development: Construction of hardware for LSTM layers using building blocks from finn-hlslib, enabling a fully customisable implementation in terms of data types, dimensions, and other architectural parameters. This design also enhances parallelism, improving compu- tational efficiency while maintaining reusability for various FPGA deployments. Enabling the generation of LSTM networks as In- tellectual Property (IP) blocks for FPGA designs, facilitating broader usability. The implementation of this layer is open sourced as part of the FINN-GLSTM-Hw repository. Deployment: Demonstrating the seamless integration of an LSTM IP block within a neural network that includes con- volutional and dense layers, enabling mid-price stock prediction in HFT scenarios. This work bridges a critical gap in the deployment of LSTMs on FPGAs, providing a comprehensive methodology for leveraging the power of RNNs in hardware-accelerated environments. The rest of the paper is organised as follows. Section II discusses different LSTM implementations proposed in the literature as well as background on limit order books and the reasoning for extending FINN.\n\n--- Segment 6 ---\nThe rest of the paper is organised as follows. Section II discusses different LSTM implementations proposed in the literature as well as background on limit order books and the reasoning for extending FINN. Section III discusses the development stages of the implementation. Section IV dis- cusses the case study on integrating the quantised-ConvLSTM model for the mid-price stock prediction use case for HFT environments. We conclude the paper in Section V. II. BACKGROUND AND RELATED WORK A. Recurrent Neural Networks RNNs were introduced in the 1990s primarily for se- quence prediction tasks, such as speech recognition and natural language processing. Their key advantage is the ability to maintain hidden states that capture information from previous inputs [9]. This ability to model tempora l dependencies makes them especially useful in applications where context and patterns evolve over time. Despite their advantages, RNNs were challenging to train due to issues such as the vanishing gradient problem, which were later addressed with advanced architectures like LSTMs and GRUs. LSTMs and GRUs have since found widespread applications in many sequence prediction tasks. The compute equations (gates and states) for state computations in LSTMs are shown below: ft œÉ(Wfxt Ufht 1 bf) (1) it œÉ(Wixt Uiht 1 bi) (2) Ct tanh(Wcxt Ucht 1 bc) (3) Ct ft Ct 1 it Ct (4) ot œÉ(Woxt Uoht 1 bo) (5) ht ot tanh(Ct) (6) Network description: Pytorch Brevitas Frontend Streamline HW Conversion Folding Compiler Transformations Synthesis with Vivado and Vitis for different targets Backend Customised hardware solution with runtime environment Deployment ONNX representation Model development : QLSTM layer QLSTM computation encapsulated in scan operator FINN-ONNX LSTM specific quantised activations to thresholding layers New and modified transformations for QLSTM Reusable and efficient HW building block for QLSTM QLSTM IP integration with standard FINN layers Fig. 2: This figure illustrates the different stages in the overall deployment workflow for generalized quantised LSTM layer (with mixed quantisation support) based models as an extension of the FINN framework for FPGAs. This figure is inspired by Figure 6 in [7].\n\n--- Segment 7 ---\n2: This figure illustrates the different stages in the overall deployment workflow for generalized quantised LSTM layer (with mixed quantisation support) based models as an extension of the FINN framework for FPGAs. This figure is inspired by Figure 6 in [7]. where œÉ represents the sigmoid activation function, tanh is the hyperbolic tangent activation, and denotes element-wise multiplication. Due to the computationally intensive nature of these operations, LSTMs are often accelerated using bespoke hardware implementations on FPGAs to support real-time applications, as discussed in the next subsection. B. LSTMs on FPGAs Multiple FPGA accelerators and frameworks with RNN support have been proposed in the research literature. Khoda et al. [10] extend the hls4ml framework by adding support for LSTM deployments on FPGAs. They showcase three example implementations with varied LSTM layer complexity. Que et al. [11] present a hardware architecture to address data dependency issues in LSTM computations along with a new blocking and batching strategy that enhances the weight reuse in LSTM operations. Their approach minimises external memory access, optimising the system performance for large LSTM-based models on devices with limited on- chip memory capacity. F-LSTM [12] is a framework for deploying LSTM-based models on FPGAs using the CPUs to carry out pre- and post-processing. The authors test their implementation on sentiment analysis and achieve improve- ments over a reference GPU implementation. Li et al [13] propose an efficient sparse LSTM accelerator on embedded FPGAs with bandwidth-oriented pruning for reducing the on-chip memory demand. They train and test their LSTM model on the TIMIT dataset demonstrating an optimised deployment on the PYNQ-Z1 FPGA platform. The C-LSTM accelerator [14] leverages a structured compression technique (as opposed to random pruning) to reduce model size while avoiding the introduction of irregularities in computation and memory accesses. Rybalkin et al. [6] propose FINN-L, an extension of FINN to deploy parametrisable quantised LSTMs on FPGAs. They implement a Bi-LSTM model for OCR recognition tasks on the ZCU104 FPGA platform.\n\n--- Segment 8 ---\n[6] propose FINN-L, an extension of FINN to deploy parametrisable quantised LSTMs on FPGAs. They implement a Bi-LSTM model for OCR recognition tasks on the ZCU104 FPGA platform. This has, however, remained a custom implementation not integrated with the general FINN workflow. Ribes et al. [15] propose solving the deployment constraints posed by stacked LSTM models on FPGA by altering their computational structure optimising memory requirements. Ioannou and Fahmy [16] propose a flexible overlay architecture for LSTMs on hybrid FPGAs. They propose a streaming dataflow arrangement ex- ploiting the capabilities of DSP blocks while also mitigating external memory overheads. While the above implementations introduce novel deployment architectures for LSTMs, they do not provide a generalized mixed precision development and deployment workflow, limiting the broader adoption of efficient LSTM models on FPGAs. C. FINN FINN facilitates the rapid development and deployment of quantised neural networks (QNNs) on both edge and datacen- ter FPGAs [2]. Widely adopted by the open-source research community, it has evolved into an efficient toolchain for acceleration of quantised machine learning models. A distin- guishing feature of this framework is its use of a thresholding operator to implement quantised activation functions. This operator maps input values to integers in the interval [0, n] by comparing the input to a set of threshold values and returning an integer value that specifies the number of threshold values that the input is greater than or equal to. By adjusting the spacing between threshold values, this operator can model any monotonically increasing activation function, such as, ReLU, sigmoid, and tanh. The efficient, hardware-friendly implementation of the latter two relying on comparisons rather than the explicit computation of exponentials is key for an efficient implementation of LSTMs. Figure 3 illustrates how to operator models the tanh activation at three different INT quantisation levels. We demonstrate that our extensions to the FINN framework encompass all essential components, including FINN compiler transformations that map the LSTM compute graph to pre-built hardware building blocks, along with support for converting custom operators such as mapping sigmoid and tanh acti- vations to comparison based thresholding operations available through finn-hlslib.\n\n--- Segment 9 ---\nFigure 3 illustrates how to operator models the tanh activation at three different INT quantisation levels. We demonstrate that our extensions to the FINN framework encompass all essential components, including FINN compiler transformations that map the LSTM compute graph to pre-built hardware building blocks, along with support for converting custom operators such as mapping sigmoid and tanh acti- vations to comparison based thresholding operations available through finn-hlslib. These components enable the construction of hardware-accelerated LSTM layers within the framework, allowing the seamless integration into larger models. A key advantage of quantisation using FINN is that all LSTM layer weights can be stored in the on-chip memory of the FPGA, enhancing performance and addressing limitations encountered in other implementations [13]. As a result, we chose to extend this framework to develop a generalized deployment pipeline for recurrent neural networks. D. Limit Order Books A limit order book (LOB) is a record of pending limit orders maintained by an exchange. Each limit order is an instruction to buy or sell a security at a specified price or better. When limit orders are placed, they are recorded at the exchange by updating the order book to track them. Orders are executed when the market reaches or exceeds their specified price levels [17]. LOBs represent investors sentiments and, hence, can be used to predict the change in stock prices for certain time horizons. With the availability of public LOB datasets, multiple machine-learning stock price prediction models have been proposed in the literature. These architectures range from 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Inputs (x) 1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 Activation (y) tanh(x) (Original) INT2 (4 levels) INT4 (16 levels) INT6 (64 levels) Fig. 3: The figure illustrates how the Multithresholding oper- ator approximates the tanh activation function over a defined input range for INT2, INT4 and INT6 bitwidths. traditional machine learning techniques like support vector machines (SVMs) to deep learning architectures like CNNs, LSTMs, ConvLSTMs and attention-based architectures [1], [18] [22].\n\n--- Segment 10 ---\n3: The figure illustrates how the Multithresholding oper- ator approximates the tanh activation function over a defined input range for INT2, INT4 and INT6 bitwidths. traditional machine learning techniques like support vector machines (SVMs) to deep learning architectures like CNNs, LSTMs, ConvLSTMs and attention-based architectures [1], [18] [22]. The authors in the above papers utilise the general- isable nature of deep learning architectures for trend prediction with high accuracies. However, the majority of the papers fail to report their deployment latency numbers to evaluate the performance of their models in HFT environments where the time interval between two events can be in the order of nano- and microseconds [1]. III. DEVELOPMENT STAGES In this section, we outline the different stages in LSTM development, from the front-end ONNX representation to mapping LSTM computations onto the hardware building blocks available in FINN. A. Frontend: QLSTM QCDQ representation ONNX is an open standard designed to represent machine learning models. It defines a comprehensive set of operators (like Matmul, ReLU), which serve as the fundamental building blocks for constructing machine and deep learning models. ONNX aims to provide a unified format that fosters interoper- ability, enabling seamless integration and interaction between various ML tools and frameworks [5]. The ONNX represen- tation also serves as a foundation for various ML hardware development frameworks, such as FINN. Additionally, ONNX supports modeling quantisation operators, such as Quantize- Linear and DequantizeLinear, enabling the representation of QNNs with standard ONNX operators. However, ONNX traditionally supports operators designed for forward computation and lacks extensive support for recurrent computations. ONNX recently introduced the Scan operator to facilitate modeling of recurrent connections [23]. This operator processes one or more input tensors iteratively to generate zero or more output tensors. It is inspired by general recurrences and functional programming constructs, such as scan, fold, map, and zip. During each iteration, the inputs to the operator consist of the current values of its state variables and the specific element of the input tensor being processed.\n\n--- Segment 11 ---\nIt is inspired by general recurrences and functional programming constructs, such as scan, fold, map, and zip. During each iteration, the inputs to the operator consist of the current values of its state variables and the specific element of the input tensor being processed. Its Scan Op : Body LSTM computation executed in each iteration (h_init, c_init) X1 X2 X3 Scan Inputs : Initial hidden and cell states processing inputs X tanh X X tanh ct-1 ht-1 ct ht c1 c2 c3 h1 h2 h3 Scan Outputs : Concatenated hidden states Last hidden state Sequence size 3 Inp-1 (t 0) Inp-2 (t 1) Inp-3 (t 2) h1 - t 1 h2 - t 2 c1 - t 1 c2 - t 2 Fig. 4: The figure illustrates how the Scan operator replicates the recurrent hidden state computations in an LSTM layer. The yellow, red, and blue arrows highlight the pathways for input data processing and state updates at each time step for every input in the sequence. outputs comprise the updated state variables and one or more output tensors. The computation performed in each iteration is defined by the operator s body attribute that encodes a computational graph. The operator has unique properties that differentiate it from other ONNX compute nodes: It allows the state variable to be updated after each iteration, enabling the updated state to be used in the processing of the next input. It processes inputs sequentially, either row by row or column by column, continuously updating the hidden state for each input while storing the hidden states at each step. We utilise this container operator to encapsulate the LSTM compute graph within its body, effectively modelling the recurrent computational structure of the LSTM layer. The quantised ONNX implementation of the layer is constructed using the QuantizeLinear, Clip, and DequantizeLinear oper- ators, forming a QCDQ representation that exactly follows the QuantLSTM implementation in Brevitas (a framework for training quantised neural networks [24]). To verify its functional correctness, we compare our quantised LSTM layer implementation against this QuantLSTM layer. Figure 4 illus- trates the internal workings of the Scan operator in the context of the LSTM compute graph over three time steps.\n\n--- Segment 12 ---\nTo verify its functional correctness, we compare our quantised LSTM layer implementation against this QuantLSTM layer. Figure 4 illus- trates the internal workings of the Scan operator in the context of the LSTM compute graph over three time steps. The primary advantage of developing this QCDQ-based implementation using the Scan ONNX operator is that it provides precise control over the bit-widths of all 11 internal quantisers within the QuantLSTM layer [25]. This capability directly facilitates the rapid development and prototyping of mixed-precision quantised LSTM-based models, enabling fine- grained exploration of trade-offs between model accuracy and hardware efficiency. In contrast, the standard LSTM operator in ONNX does not expose the internal computational structure of the node, making such customisations infeasible. B. FINN: Compiler Transformations After functionally verifying the QCDQ LSTM layer im- plemented with the Scan operator, the QCDQ computation graph should be transformed to a set of pre-built hardware building blocks from the finn-hlslib kernel library. At the time of this project, the finn-hlslib kernels were limited to supporting integer-only compute. However, the quantised graph contains floating-point operations coming from scale and bias operations of the quantisers. To achieve a functionally equivalent integer-only compute graph, we leverage FINN s streamlining process, which moves floating-point operations towards thresholding layers and merges them by updating the threshold values [8]. To achieve this, the QCDQ graph is trans- formed to present quantisers as thresholding layers utilising the quantised ONNX (QONNX) format. This allows us to reuse existing conversions and transformations available within the FINN flow and adapting them when required to support the quantised recurrent representations. We adopted the QCDQ-to- QONNX conversion because at the time of this work Brevitas does not support direct functional QONNX export for QLSTM layers. Instead, we used Brevitas solely to extract weights and quantisation parameters, constructing a custom QONNX graph from the corresponding QCDQ nodes. QONNX project extends ONNX framework by introducing custom quantisation operators (Quant, BipolarQuant, Trunc) to support arbitrary- precision uniform quantisation [26].\n\n--- Segment 13 ---\nInstead, we used Brevitas solely to extract weights and quantisation parameters, constructing a custom QONNX graph from the corresponding QCDQ nodes. QONNX project extends ONNX framework by introducing custom quantisation operators (Quant, BipolarQuant, Trunc) to support arbitrary- precision uniform quantisation [26]. The conversion from QCDQ is achieved by applying the ConvertQCDQtoQONNX transformation to the graph. This transformation converts the QCDQ operators into Quant nodes, as shown below: QuantizeLinear Clip DequantizeLinear Quant (7) The QONNX LSTM graph is subsequently converted to the FINN-ONNX representation for further processing through the FINN flow. This conversion is performed using the ConvertQONNXtoFINNONNX transformation, which re- places QONNX operators and activations with the matmul, multithreshold operators, as shown below: Tanh Quant Multithreshold Mul Add (8) Finally, the LSTM computation graph also includes sig- moid and tanh activations, which are not directly sup- ported in the above transformation. However, since they are both monotonically increasing, we model these using threshold operators. We developed functions that will generate thresholds for tanh and sigmoid activations utilised in the ConvertQONNXtoFINNONNX tranformation to achieve the the result shown in eq 8. Subsequently the graph is functionally verified and passed to downstream layers for further stream- lining. At this stage, the primary goal is to minimise floating- point operations in the computation graph (the Mul and Add nodes introduced in the previous step) and eliminate operators by fusing absorbing them wherever possible to reduce the resource usage of the design. We try to achieve this by applying pre-existing transformations to the graph wherever possible and developing transformations (new or adaptations of existing FINN transformations) for unique compute patterns that are not fully absorbed (streamlined). Table I describes all the transformations used in the quantised LSTM graph streamlining process. The transformations we developed re- arrange the computations in the graph to position the floating- point operators close to the next multi-thresholding operator, allowing it to be absorbed by it. Figure 5 illustrates examples where our transformations allow the floating point operations to be absorbed into the nearest multi-thresholding operator.\n\n--- Segment 14 ---\nThe transformations we developed re- arrange the computations in the graph to position the floating- point operators close to the next multi-thresholding operator, allowing it to be absorbed by it. Figure 5 illustrates examples where our transformations allow the floating point operations to be absorbed into the nearest multi-thresholding operator. These transformations allow the LSTM graph to be fully streamlined, where all computations can be represented by (hardware) library functions in FINN and can hence be com- posed into hardware using the FINN-HLS backend. Figure 6 TABLE I: The table shows the list of pre-existing transfor- mations used to transform LSTM compute graph along with new modified transformations. Pre-existing transformations New Modified Transformations MoveAddPastMul() MoveScalarMulPastMatMul() MoveScalarAddPastMatMul() MoveLinearPastEltwiseMul() MoveAddPastMul() AbsorbMulIntoMultiThreshold() CollapseRepeatedAdd Mul() AbsorbsignbiasintoMultithrehsold() AbsorbAddIntoMultiThreshold() RoundAndClipThresholds() MoveScalarMulPastMatMul() : Added support for moving scalar Mul past each consumer Matmul in the fork. MoveLinearPastEltwiseMul() : Added support for moving join Mul's past element wise mul operator AbsorbMulIntoMultithreshold() : Added suppport to absorb scalar mul into multiple multithreshold nodes Fig. 5: The figure illustrates examples of which transforma- tions in the FINN compiler were modified and the result they achieved. summarises the transformation process to obtain the final streamlined graph. C. QLSTM HLS Backend The backend flow maps the QCDQ representation to a functionally equivalent graph consisting of hardware blocks that implements the operators transformations. This produces a quantised LSTM (Q-LSTM) computation graph with the following operators: Threshold, MatMul, Elementwise Add, and Elementwise Mul. As before, we leverage pre- existing hardware blocks available in finn-hlslib which were developed for mapping feed-forward networks (using Vitis HLS) to map the forward computation path of LSTM layers.\n\n--- Segment 15 ---\nThis produces a quantised LSTM (Q-LSTM) computation graph with the following operators: Threshold, MatMul, Elementwise Add, and Elementwise Mul. As before, we leverage pre- existing hardware blocks available in finn-hlslib which were developed for mapping feed-forward networks (using Vitis HLS) to map the forward computation path of LSTM layers. In the case of recurrent (loopback) connections, we utilise a QCDQ QONNX FINN-ONNX Streamlined ONNX Nodes: QuantizeLinear Clip DequantizeLinear MatMul ReLU, Sigmoid, Tanh Nodes: Quant MatMul ReLU, Sigmoid, Tanh Nodes: Multithreshold Scalar Mul Add ElementwiseMul Add MatMul Nodes: Multithreshold ElementwiseMul Add MatMul Note : Comprises of floating point operations Note : Floating point operations eliminated Fig. 6: The figure demonstrates the compute nodes each representation comprises of in the process of obtaining the final streamlined quantised LSTM compute graph. standard for construct to map these to hardware blocks, whose bounds are determined by the input sequence length of the model. This loop structure introduces sequential dependencies in the LSTM computation graph and imposes a limitation on parallelisation (through unrolling). A longer input sequence length can increase the initiation interval (II) of the loop, which could be a constraint for real-time applications. Utilising the building blocks enables us to design a generic hardware layer with customisable parameters such as data types, input size, hidden layer dimensions, and sequence length. This allows for simple integration in the FINN hardware generation flow and provides a flexible interface for arbitrary settings. Once the design is complete, we synthesise the accelerator and export it as an independent IP block. D. QLSTM layer hardware testing After generating the independent LSTM IP block using Vitis HLS, we integrate it with an AXI-DMA-based subsystem in the Vivado IP Integrator to build the bitstream, which is then tested on the FPGA. For integration and functional verification, as well as benchmarking, we follow the three-part tutorial1, which demonstrates the complete integration process. The PYNQ APIs are used to drive the IP with input data and retrieve the output, allowing for functional verification of the hardware operation.\n\n--- Segment 16 ---\nFor integration and functional verification, as well as benchmarking, we follow the three-part tutorial1, which demonstrates the complete integration process. The PYNQ APIs are used to drive the IP with input data and retrieve the output, allowing for functional verification of the hardware operation. IV. INTEGRATION CASE STUDY In this section, we present a case study for accelerating an LSTM-based model on an FPGA using our extensions for FINN, for predicting stock price trends using the openly available LOB dataset (normalised version). Our model design closely follows the architecture of the most successful ap- proach reported in the literature for this dataset [1]. The details of the case study, including design choices and implementation aspects, are provided in the following subsections. A. Model development 1) ConvLSTM-HFT classification model: We replicate the ConvLSTM architecture of DeepLOB [1] for our case, and evaluate the effectiveness of the proposed FINN extensions and the integration with standard FINN flow. DeepLOB utilises a series of convolutional blocks to extract features from a segment of order-book data, which is then passed through an LSTM layer to capture the time-series dependencies within the features. A final dense layer is employed for classification 1 3344 of stock price trend prediction. In our case, we use two convolutional blocks to extract features from a block of 100 order-book entries, followed by an LSTM layer for time-series learning, and a dense layer at the output for classification. The key distinction between our implementation and DeepLOB lies in the use of inception blocks; DeepLOB incorporates inception blocks in its convolutional layers, while our model does not. Our experiments indicate that omitting inception blocks does not result in a significant loss of accuracy, as shown in the evaluation subsection. Each convolutional block in our model uses three Conv2D layers. The first block utilizes filters of sizes 64, 32, and 32, with a higher number of filters in the initial layer to maximise feature extraction. The second block employs filters of sizes 64, 16, and 4, designed to extract and preserve the most salient features for subsequent processing by the LSTM layer.\n\n--- Segment 17 ---\nThe first block utilizes filters of sizes 64, 32, and 32, with a higher number of filters in the initial layer to maximise feature extraction. The second block employs filters of sizes 64, 16, and 4, designed to extract and preserve the most salient features for subsequent processing by the LSTM layer. To balance effective feature extraction with computational effi- ciency, the first layer in each convolutional block is configured with a stride of 2, facilitating dimensionality reduction and reducing the complexity of the subsequent LSTM layer. All convolutional layers utilise a (3 3) kernel size and are fol- lowed by a Batch Normalization layer and a ReLU activation function to enhance training stability. The LSTM layer that follows the convolutional blocks is configured with 64 hidden units, an input size of 40, and a sequence length of 25. The classification stage consists of two fully connected layers with 256 and 3 neurons, respectively, where the final layer corre- sponds to the output classes. The complete model comprises of approximately 141K trainable parameters. The final model configuration was derived through an iterative process aimed at balancing performance and model complexity, to achieve good model quantisation performance and resoure efficiency during the final hardware implementation. This exploration involved several key design decisions, including the selection of the number of convolutional blocks, the number of filters, filter sizes, and strides within each block (to optimise the input size for the LSTM layer), the number of hidden units in the LSTM layer, and the configuration of the final classification module such as the number of dense layers and the number of neurons in each layer. 2) Quantising the ConvLSTM model: Once the final con- figuration of the floating-point model was established, based on test accuracy metrics, we proceeded to quantise the model using a W8A6 (8-bit weight, 6-bit activation) scheme with the Brevitas library [24]. While the standard Q-Conv and Q- Linear layers in Brevitas allow quantisation of the weights, the proposed Q-LSTM layer offers additional flexibility due to its internal computation structure, enabling both weight quantisation and the selection of quantisation levels for the different activation quantisers within the layer.\n\n--- Segment 18 ---\n2) Quantising the ConvLSTM model: Once the final con- figuration of the floating-point model was established, based on test accuracy metrics, we proceeded to quantise the model using a W8A6 (8-bit weight, 6-bit activation) scheme with the Brevitas library [24]. While the standard Q-Conv and Q- Linear layers in Brevitas allow quantisation of the weights, the proposed Q-LSTM layer offers additional flexibility due to its internal computation structure, enabling both weight quantisation and the selection of quantisation levels for the different activation quantisers within the layer. In our imple- mentation, we apply uniform quantisation scheme (W8A6) to the LSTM layer as well as the convolutional and dense layers. This configuration was chosen after systematically reducing the activation bit-width and observing the point at which a significant degradation in model accuracy occurred. The next subsection provides detailed descriptions of the training process for both the floating-point and quantised models. B. Dataset and Training 1) Dataset: We utilise the FI-2010 dataset, which captured LOB data for five stocks traded over a 9-day period on TABLE II: The table describes the LOB features comprised in the FI-2010 dataset.\n\n--- Segment 19 ---\nThe next subsection provides detailed descriptions of the training process for both the floating-point and quantised models. B. Dataset and Training 1) Dataset: We utilise the FI-2010 dataset, which captured LOB data for five stocks traded over a 9-day period on TABLE II: The table describes the LOB features comprised in the FI-2010 dataset. Feature Set Description u1 {Paski, Vaski, Pbidi, Vbidi}n i 1 10-level LOB data u2 {(Paski Pbidi), (Paski Pbidi) 2}n i 1 Spread and Mid-price u3 {Paskn Pask1, ..., Pbidi 1 Pbidi }n 1 i 1 Price differences u4 1 n Pn i 1 Paski, 1 n Pn i 1 Pbidi, 1 n Pn i 1 Vaski, 1 n Pn i 1 Vbidi Price and Volume means u5 Pn i 1(Paski Pbidi), Pn i 1(Vaski Vbidi) Accumulated differences u6 {dPaski dt, dPbidi dt, dVaski dt, dVbidi dt}n i 1 Price and Volume derivation u7 {Œª1 t, Œª2 t, Œª3 t, Œª4 t, Œª5 t, Œª6 t} Average intensity per type u8 1Œª1 t Œª1 T , 1Œª2 t Œª2 T , ..., 1Œª6 t Œª6 T Relative intensity comparison u9 {dŒª1 dt, dŒª2 dt, dŒª3 dt, dŒª4 dt, dŒª5 dt, dŒª6 dt} Limit activity acceleration the Helsinki Stock Exchange [19] to train the quantised- ConvLSTM model. Each record comprises 144 features and five labels that corresponds to future price movements over prediction horizons of 1, 2, 3, 5, and 10 timesteps (each timestep being 192 ms apart).\n\n--- Segment 20 ---\nFeature Set Description u1 {Paski, Vaski, Pbidi, Vbidi}n i 1 10-level LOB data u2 {(Paski Pbidi), (Paski Pbidi) 2}n i 1 Spread and Mid-price u3 {Paskn Pask1, ..., Pbidi 1 Pbidi }n 1 i 1 Price differences u4 1 n Pn i 1 Paski, 1 n Pn i 1 Pbidi, 1 n Pn i 1 Vaski, 1 n Pn i 1 Vbidi Price and Volume means u5 Pn i 1(Paski Pbidi), Pn i 1(Vaski Vbidi) Accumulated differences u6 {dPaski dt, dPbidi dt, dVaski dt, dVbidi dt}n i 1 Price and Volume derivation u7 {Œª1 t, Œª2 t, Œª3 t, Œª4 t, Œª5 t, Œª6 t} Average intensity per type u8 1Œª1 t Œª1 T , 1Œª2 t Œª2 T , ..., 1Œª6 t Œª6 T Relative intensity comparison u9 {dŒª1 dt, dŒª2 dt, dŒª3 dt, dŒª4 dt, dŒª5 dt, dŒª6 dt} Limit activity acceleration the Helsinki Stock Exchange [19] to train the quantised- ConvLSTM model. Each record comprises 144 features and five labels that corresponds to future price movements over prediction horizons of 1, 2, 3, 5, and 10 timesteps (each timestep being 192 ms apart). The labels classify the stock s movement direction as downward (0), stationary (1), or upward (2) after each horizon k. The input features capture a comprehensive view of the order book, including bid ask prices, bid ask volumes, their derivatives, and other metrics that represent a 10-level order book structure. A detailed breakdown of these features in the dataset is provided in Table II. Although the original dataset is not publicly available, three normalized versions z-score, min-max scaling, and decimal precision normalisation have been made available for research purposes.\n\n--- Segment 21 ---\nA detailed breakdown of these features in the dataset is provided in Table II. Although the original dataset is not publicly available, three normalized versions z-score, min-max scaling, and decimal precision normalisation have been made available for research purposes. Out of the three, we use the z-score normalised version of the dataset which has widely adopted in the literature for predicting the trends of the stocks in the dataset [1]. 2) Training: The z-score version of the dataset uses nor- malised floating-point features, which were quantised to INT8 precision for training the Q-ConvLSTM model. From the dataset, we used the first seven days of data for training and validation purposes with 90 10 split. We used the remaining two days of data for testing the model, following the same training approach presented in [1]. Quantising features into lower integer ranges ( INT8) significantly hindered model convergence, as the narrow range caused many similar feature values to map to the same integer, diminishing useful informa- tion. We started with the training of the floating-point model on the dataset to achieve a baseline model with acceptable accuracy. We then used the trained floating-point model as a starting point to train and fine-tune the quantised version of the model, improving convergence by 10 compared to training the quantised version of the model from scratch. In all the training flows, the model with the lowest validation loss was chosen for testing. Both the floating-point and quantised models were trained for 100 epochs with early stopping mechanisms that triggers an exit from loop if no decrease of validation loss was observed for 20 consecutive epochs. We used the cross-entropy loss function and the Adam optimiser with a learning rate of 0.001 and a batch size of 16 for training the floating point model. To train the quantised model, we used the same hyperparameters as the floating point version except the batch size, which was set to 32 to reduce the training time. Post training, we observed that the quantised model suffered from an accuracy drop of 1.5 . With further fine-tuning, where we trained this model further using a faster (0.01) learning rate, the model achieved 1 higher accuracy than the floating-point version, and was passed through to the hardware implementation phase.\n\n--- Segment 22 ---\nPost training, we observed that the quantised model suffered from an accuracy drop of 1.5 . With further fine-tuning, where we trained this model further using a faster (0.01) learning rate, the model achieved 1 higher accuracy than the floating-point version, and was passed through to the hardware implementation phase. Table III shows the comparison of the DeepLOB, ConvLSTM and Q-ConvLSTM model trained on the FI-2010 dataset for different accuracy metrics in detail. C. Experimental Evaluation 1) Hardware generation and deployment: The trained model is compiled using the proposed extensions integrated into the FINN flow to generate a dataflow accelerator IP (mvau width 36 and target fps 1000) and evaluate the effectiveness of our proposed extensions and transformations. We chose a Zynq Ultrascale platform as the target (XCZU7EV device on the ZCU104 development board) mimicking an edge deployment scenario, with the tools successfully synthesizing the IP for a clock frequency of 150 MHz. The IP was subsequently integrated using the IPI flow to evaluate hardware performance. From our testing, we determine the batch-1 processing latency for the quantised ConvLSTM network to be 4.3 ms, excluding any data move- ment overheads, identical to the cosimulation results observed in Vitis HLS. For the specific application of trend prediction, the average interval between consecutive events in the dataset is approximately 192 ms [1]. Thus, the network processes data significantly faster than the required decision-making window of ten time steps ( 2 seconds), providing enough time for stakeholders to analyse and respond to market changes and make informed decisions. Higher operating performance can be achieved if the network is further unrolled, targeting a datacentre class FPGA (such as the Alveo), and by replicating the LSTM cell (reducing the sequential dependency), at the cost of higher resource consumption. For the Zynq deploy- ment, the resource utilisation of our design is summarized in Table V. The model uses approximately 49 of LUTs, 13 of flip-flops (FFs), and 15 of DSP blocks, leaving room for additional network unfolding to achieve further latency reduction, if required.\n\n--- Segment 23 ---\nHigher operating performance can be achieved if the network is further unrolled, targeting a datacentre class FPGA (such as the Alveo), and by replicating the LSTM cell (reducing the sequential dependency), at the cost of higher resource consumption. For the Zynq deploy- ment, the resource utilisation of our design is summarized in Table V. The model uses approximately 49 of LUTs, 13 of flip-flops (FFs), and 15 of DSP blocks, leaving room for additional network unfolding to achieve further latency reduction, if required. More importantly, we were able to determine that the proposed mapping flow maintains a one- to-one mapping between the trained Brevitas model and the generated hardware, demonstrating the functional effectiveness of the applied graph transformations. 2) Accuracy: To further assess the accuracy of the gen- erated Q-ConvLSTM IP, we evaluate its performance using precision, recall, and F1-score metrics for each of the three trend prediction labels: downward, stationary, and upward. We compare our quantised implementation to both the floating- point ConvLSTM model (in PyTorch) and the state-of-the-art DeepLOB model to determine the impact of quantisation on its classification performance. Table III presents this comparative analysis, illustrating that the generated quantised IP maintains an F1-score comparable to its floating-point counterpart and the DeepLOB across all three trend categories. Table IV summarises the comparative evaluation of Q- ConvLSTM IP generated by the proposed flow relative to existing approaches in the literature, in terms of inference accuracy. We replicated the DeepLOB model and indepen- dently reproduced the results for our comparison, following the same training pipeline specified in the original DeepLOB paper [1]. We also use macro F1-score as the primary eval- uation metric, as done by approaches in the literature, to mitigate the risk of inflated accuracy metrics that could result from the inherent class imbalance in the dataset. Our results show that the generated Q-ConvLSTM IP through our FINN extensions demonstrates superior prediction performance over several widely used financial prediction models.\n\n--- Segment 24 ---\nWe also use macro F1-score as the primary eval- uation metric, as done by approaches in the literature, to mitigate the risk of inflated accuracy metrics that could result from the inherent class imbalance in the dataset. Our results show that the generated Q-ConvLSTM IP through our FINN extensions demonstrates superior prediction performance over several widely used financial prediction models. Specifically, it outperforms: SVM [18] by 42 , MLP [18] by 29 , TABLE III: Performance comparison of DeepLOB, ConvLSTM and Q-ConvLSTM model for the three different labels in the FI-2010 dataset. Class Metric DeepLOB ( ) ConvLSTM ( ) Q-ConvLSTM ( ) Downward-0 Precision 76.50 75.20 76.66 Recall 76.68 77.26 78.44 F1-Score 76.59 76.22 77.54 Stationary-1 Precision 82.92 81.66 83.25 Recall 77.95 71.23 73.50 F1-Score 80.36 76.09 78.07 Upward-2 Precision 74.69 72.36 73.53 Recall 78.77 79.06 79.93 F1-Score 76.68 75.56 76.60 TABLE IV: Comparison of the proposed Q-ConvLSTM model against the proposed state-of-the-art HFT models in the re- search literature.\n\n--- Segment 25 ---\nSpecifically, it outperforms: SVM [18] by 42 , MLP [18] by 29 , TABLE III: Performance comparison of DeepLOB, ConvLSTM and Q-ConvLSTM model for the three different labels in the FI-2010 dataset. Class Metric DeepLOB ( ) ConvLSTM ( ) Q-ConvLSTM ( ) Downward-0 Precision 76.50 75.20 76.66 Recall 76.68 77.26 78.44 F1-Score 76.59 76.22 77.54 Stationary-1 Precision 82.92 81.66 83.25 Recall 77.95 71.23 73.50 F1-Score 80.36 76.09 78.07 Upward-2 Precision 74.69 72.36 73.53 Recall 78.77 79.06 79.93 F1-Score 76.68 75.56 76.60 TABLE IV: Comparison of the proposed Q-ConvLSTM model against the proposed state-of-the-art HFT models in the re- search literature. Model Accuracy ( ) Precision ( ) Recall ( ) F1 ( ) Prediction Horizon k 10 SVM [18] - 39.62 44.92 35.88 MLP [18] - 47.81 60.78 48.27 CNN-I [21] - 50.98 65.54 55.21 LSTM [18] - 60.77 75.92 66.33 CNN-II [22] - 56.00 45.00 44.00 T-BoF [27] 66.50 44.60 68.30 44.70 B(TABL) [20] 78.91 68.04 71.21 69.20 C(TABL) [20] 84.70 76.95 78.44 77.63 DeepLOB [1] 77.78 78.04 77.80 77.88 Q-ConvLSTM 77.37 77.81 77.29 77.40 CNN-I [21] by 23 , LSTM [18] by 11 , CNN-II [22] by 33 , B(TABL) [20] by 8 . The Q-ConvLSTM IP achieves performance equivalent to the C(TABL) [20] and DeepLOB [1] models, both of which incorporate sequential processing layers in their architectures.\n\n--- Segment 26 ---\nModel Accuracy ( ) Precision ( ) Recall ( ) F1 ( ) Prediction Horizon k 10 SVM [18] - 39.62 44.92 35.88 MLP [18] - 47.81 60.78 48.27 CNN-I [21] - 50.98 65.54 55.21 LSTM [18] - 60.77 75.92 66.33 CNN-II [22] - 56.00 45.00 44.00 T-BoF [27] 66.50 44.60 68.30 44.70 B(TABL) [20] 78.91 68.04 71.21 69.20 C(TABL) [20] 84.70 76.95 78.44 77.63 DeepLOB [1] 77.78 78.04 77.80 77.88 Q-ConvLSTM 77.37 77.81 77.29 77.40 CNN-I [21] by 23 , LSTM [18] by 11 , CNN-II [22] by 33 , B(TABL) [20] by 8 . The Q-ConvLSTM IP achieves performance equivalent to the C(TABL) [20] and DeepLOB [1] models, both of which incorporate sequential processing layers in their architectures. This result is particularly note- worthy as it demonstrates that quantised LSTM models can match the performance of floating-point counterparts, and that these quantised variants can be efficiently mapped to resource constrained devices using our flow. D. Comparison against other frameworks A comparable approach for mapping LSTM models to FPGAs is available through the hls4ml flow [10]. While both methods offer a generalised flow for mapping an LSTM accelerator on FPGA, our proposed flow using FINN offers a few key advantages. A key distinction is that our approach constructs a quantised ONNX representation of the LSTM compute graph, modeling its functionality using the Scan operator. This provides full control over all quantisation pa- rameters, enabling mixed-precision quantisation for the LSTM layer an aspect that, to our knowledge, is not available using the hls4ml flow. Additionally, our approach effectively leverages existing FINN compiler transformations and introduces few new ones to generate a computation graph that is entirely free of floating- point operations.\n\n--- Segment 27 ---\nThis provides full control over all quantisation pa- rameters, enabling mixed-precision quantisation for the LSTM layer an aspect that, to our knowledge, is not available using the hls4ml flow. Additionally, our approach effectively leverages existing FINN compiler transformations and introduces few new ones to generate a computation graph that is entirely free of floating- point operations. This enables primary computations within the graph to rely solely on integer arithmetic to achieve significant resource savings during the hardware generation phase, allowing seamless deployment on resource-constrained FPGAs, as demonstrated in our integration case study. In contrast, hls4ml retains floating point operators in the compute graph, and has primarily targeted LSTM deployments on large FPGA fabrics. The use of fixed-point datatypes with fractional TABLE V: The resource consumption of the Q-ConvLSTM accelerator on the ZCU104 FPGA. Q-ConvLSTM LUTs FFs LUTRAM BRAMs DSPs Conv Network 65091 36398 4596 96 252 LSTM Network 49439 22601 6631 31.5 13 Overall 101489 58999 11227 127.5 265 49.6 12.8 11 40.8 15.2 bits in hls4ml is effective in certain cases at the expense of increased resource consumption. Also, to reduce the II, hls4ml proposes a non-static imple- mentation of LSTM layers, where multiple LSTM compute units are instantiated to match the input sequence length. This design allows computed states to be passed efficiently to subsequent stages, enabling new inputs to be processed every clock cycle and achieving an II of 1. However, this approach results in high resource utilisation, which may limit scalability for complex models with a large number of intermediate states. While our approach does not consider this implementation scheme at present, we recognize its potential and see value in exploring its integration with our resource-efficient design methodology. Finally, both frameworks share the broader goal of providing a parameterisable hardware layer to streamline LSTM deploy- ment. However, while our approach prioritises optimisation for constrained FPGA devices by balancing performance with efficient resource utilisation, the hls4ml approach prioritises latency minimisation at the expense of high resource usage.\n\n--- Segment 28 ---\nFinally, both frameworks share the broader goal of providing a parameterisable hardware layer to streamline LSTM deploy- ment. However, while our approach prioritises optimisation for constrained FPGA devices by balancing performance with efficient resource utilisation, the hls4ml approach prioritises latency minimisation at the expense of high resource usage. V. CONCLUSION AND FUTURE WORK In this paper, we extend the FINN framework by developing support for the generalized deployment of recurrent neural networks (using LSTMs as a working example) on FPGAs. We create a complete end-to-end framework and introduce the Scan operator from ONNX to enable modelling of mixed- precision quantised LSTM layers within FINN. Additionally, we design and modify FINN compiler transformations to optimise its compute graph and efficiently map operations to hardware blocks available in the FINN backend, while sup- porting arbitrary integer datatypes, dimensions and sequence lengths. We further present a case study for stock prediction using a model architecture that uses quantised LSTM layers with convolutional and dense layers (quantised ConvLSTM model). The quantised ConvLSTM model compiled to an accelerator IP core that can be deployed on FPGAs using our extended FINN flow. We show that our flow can generate a resource-efficient hardware accelerator IP of the model that meets the performance requirements even when mapped on an edge-class hybrid FPGA (Zynq Ultrascale XCZU7EV), and offers competitive inference performance when evaluated using the FI-2010 dataset for mid-price stock prediction. In the future, we aim to explore optimisations to reduce the latency introduced by the sequential layers in the LSTM flow, taking inspiration from the hls4ml approach (using non- static implementations for LSTM layers), and to extend the deployment pipeline to support other RNN models such as gated recurrent units (GRUs). VI. ACKNOWLEDGEMENTS I would like to thank Alessandro Pappalardo and Dr. Ya- man Umuroglu for their valuable insights and the engaging brainstorming sessions throughout the course of this project. REFERENCES [1] Z. Zhang, S. Zohren, and S. Roberts, Deeplob: Deep convolutional neural networks for limit order books, IEEE Transactions on Signal Processing, vol. 67, no. 11, pp.\n\n--- Segment 29 ---\n67, no. 11, pp. 3001 3012, 2019. [2] M. Blott, T. B. Preu√üer, N. J. Fraser, G. Gambardella, K. O brien, Y. Umuroglu, M. Leeser, and K. Vissers, FINN-R: An end-to-end deep- learning framework for fast exploration of quantized neural networks, ACM Transactions on Reconfigurable Technology and Systems (TRETS), vol. 11, no. 3, pp. 1 23, 2018. [3] J. Duarte et al., Fast inference of deep neural networks in FPGAs for particle physics, JINST, vol. 13, no. 07, p. P07027, 2018. [4] L. P. Joseph, R. C. Deo, R. Prasad, S. Salcedo-Sanz, N. Raj, and J. Soar, Near real-time wind speed forecast model with bidirectional LSTM networks, Renewable Energy, vol. 204, pp. 39 58, 2023. [5] ONNX, [6] V. Rybalkin, A. Pappalardo, M. M. Ghaffar, G. Gambardella, N. Wehn, and M. Blott, FINN-L: Library extensions and design trade-off analysis for variable precision LSTM networks on FPGAs, in 2018 28th international conference on field programmable logic and applications (FPL), pp. 89 897, IEEE, 2018. [7] T. Alonso, L. Petrica, M. Ruiz, J. Petri-Koenig, Y. Umuroglu, I. Stame- los, E. Koromilas, M. Blott, and K. Vissers, Elastic-df: Scaling perfor- mance of dnn inference in fpga clouds through automatic partitioning, ACM Trans. Reconfigurable Technol. Syst., vol. 15, Dec. 2021. [8] Y. Umuroglu and M. Jahre, Streamlined deployment for quantized neural networks, arXiv preprint arXiv:1709.04060, 2017.\n\n--- Segment 30 ---\n15, Dec. 2021. [8] Y. Umuroglu and M. Jahre, Streamlined deployment for quantized neural networks, arXiv preprint arXiv:1709.04060, 2017. [9] T. Robinson, M. Hochberg, and S. Renals, The use of recurrent neural networks in continuous speech recognition, in Automatic Speech and Speaker Recognition: Advanced Topics, pp. 233 258, Springer, 1996. [10] E. E. Khoda, D. Rankin, R. T. de Lima, P. Harris, S. Hauck, S.-C. Hsu, M. Kagan, V. Loncar, C. Paikara, R. Rao, et al., Ultra-low latency recurrent neural network inference on FPGAs for physics applications with hls4ml, Machine Learning: Science and Technology, vol. 4, no. 2, p. 025004, 2023. [11] Z. Que, Y. Zhu, H. Fan, J. Meng, X. Niu, and W. Luk, Mapping large lstms to fpgas with weight reuse, Journal of Signal Processing Systems, vol. 92, pp. 965 979, 2020. [12] Liang, Bushun and Wang, Siye and Huang, Yeqin and Liu, Yiling and Ma, Linpeng, F-LSTM: FPGA-based heterogeneous computing framework for deploying LSTM-based algorithms, Electronics, vol. 12, no. 5, p. 1139, 2023. [13] S. Li, S. Zhu, X. Luo, T. Luo, and W. Liu, An Efficient Sparse LSTM Accelerator on Embedded FPGAs with Bandwidth-Oriented Pruning, in 2023 33rd International Conference on Field-Programmable Logic and Applications (FPL), pp. 42 48, 2023. [14] S. Wang, Z. Li, C. Ding, B. Yuan, Q. Qiu, Y. Wang, and Y. Liang, C-LSTM: Enabling efficient LSTM using structured compression tech- niques on FPGAs, in Proceedings of the 2018 ACM SIGDA Inter- national Symposium on Field-Programmable Gate Arrays, pp. 11 20, 2018.\n\n--- Segment 31 ---\n[14] S. Wang, Z. Li, C. Ding, B. Yuan, Q. Qiu, Y. Wang, and Y. Liang, C-LSTM: Enabling efficient LSTM using structured compression tech- niques on FPGAs, in Proceedings of the 2018 ACM SIGDA Inter- national Symposium on Field-Programmable Gate Arrays, pp. 11 20, 2018. [15] S. Ribes, P. Trancoso, I. Sourdis, and C.-S. Bouganis, Mapping multiple LSTM models on FPGAs, in 2020 International Conference on Field- Programmable Technology (ICFPT), pp. 1 9, IEEE, 2020. [16] L. Ioannou and S. A. Fahmy, Streaming Overlay Architecture for Lightweight LSTM Computation on FPGA SoCs, ACM Transactions on Reconfigurable Technology and Systems, vol. 16, no. 1, pp. 1 26, 2022. [17] Investopedia, 2022. [18] Tsantekidis, Avraam and Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros, Using deep learning to detect price change indications in financial markets, in 25th European Signal Processing Conference (EUSIPCO), pp. 2511 2515, 2017. [19] A. Ntakaris, M. Magris, J. Kanniainen, M. Gabbouj, and A. Iosifidis, Benchmark dataset for mid-price forecasting of limit order book data with machine learning methods, Journal of Forecasting, vol. 37, no. 8, pp. 852 866, 2018. [20] Tran, Dat Thanh and Iosifidis, Alexandros and Kanniainen, Juho and Gabbouj, Moncef, Temporal attention-augmented bilinear network for financial time-series data analysis, IEEE transactions on neural net- works and learning systems, vol. 30, no. 5, pp. 1407 1418, 2018.\n\n--- Segment 32 ---\n5, pp. 1407 1418, 2018. [21] Tsantekidis, Avraam and Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros, Forecasting stock prices from the limit order book using convolutional neural networks, in 2017 IEEE 19th conference on business informatics (CBI), vol. 1, pp. 7 12, IEEE, 2017. [22] A. Tsantekidis, N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosifidis, Using deep learning for price prediction by exploiting stationary limit order book features, Applied Soft Computing, vol. 93, p. 106401, 2020. [23] ONNX, scan, 2018. [24] A. Pappalardo, Xilinx brevitas, 2021. [25] A. Brevitas, Quantlstm - brevitas tutorials, 2024. [26] A. Pappalardo, Y. Umuroglu, M. Blott, J. Mitrevski, B. Hawks, N. Tran, V. Loncar, S. Summers, H. Borras, J. Muhizi, et al., Qonnx: Repre- senting arbitrary-precision quantized neural networks, arXiv preprint arXiv:2206.07527, 2022. [27] N. Passalis, A. Tefas, J. Kanniainen, M. Gabbouj, and A. Iosifidis, Temporal bag-of-features learning for predicting mid price movements using high frequency limit order book data, IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 4, no. 6, pp. 774 785, 2018.\n\n