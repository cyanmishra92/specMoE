=== ORIGINAL PDF: 2506.21371v1_MAx-DNN_Multi-Level_Arithmetic_Approximation_for_E.pdf ===\n\nRaw text length: 23015 characters\nCleaned text length: 22830 characters\nNumber of segments: 13\n\n=== CLEANED TEXT ===\n\narXiv:2506.21371v1 [cs.LG] 26 Jun 2025 Presented at the 13th IEEE Latin America Symposium on Circuits and System (LASCAS), 01-04 03 2022, Puerto Varas, CL. This is the authors version of the paper. The final published version is posted on IEEE Xplore. DOI: MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators Vasileios Leon , Georgios Makris , Sotirios Xydis , Kiamal Pekmestzi , Dimitrios Soudris National Technical University of Athens, School of Electrical and Computer Engineering, 15780 Athens, Greece Harokopio University of Athens, Department of Informatics and Telematics, 17778 Athens, Greece Abstract Nowadays, the rapid growth of Deep Neural Net- work (DNN) architectures has established them as the defacto approach for providing advanced Machine Learning tasks with excellent accuracy. Targeting low-power DNN computing, this paper examines the interplay of fine-grained error resilience of DNN workloads in collaboration with hardware approximation techniques, to achieve higher levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate multipliers, we systemat- ically explore their fine-grained distribution across the network according to our layer-, filter-, and kernel-level approaches, and examine their impact on accuracy and energy. We use the ResNet- 8 model on the CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers up to 54 energy gains in exchange for up to 4 accuracy loss, compared to the baseline quantized model, while it provides 2 energy gains with better accuracy versus the state-of-the-art DNN approximations. Index Terms Approximate Computing, Inexact Multipliers, ASIC, Deep Neural Networks, ResNet, CIFAR-10, TensorFlow. I. INTRODUCTION The proliferation of demanding workloads in the Digital Signal Processing (DSP) and Artificial Intelligence (AI) do- mains marks a new era in the design of integrated circuits and embedded systems. Towards high-performance computing within a limited power envelope, the research community is examining alternative design strategies. In this direction, Approximate Computing is an emerging design paradigm [1], which exploits the error resilience to trade accuracy for gains in power, energy, area, and or latency. In the field of inexact hardware, approximation techniques are applied at different levels, i.e., from circuits [2] [5] to accelerators [6] [10]. The intrinsic error resilience and increased computational demands of DNNs, which are a state-of-the-art AI approach to tackle Computer Vision tasks (e.g., image classification, object detection, pose estimation), constitutes them as promising candidates for approximate computing. Typical approximation techniques on DNNs involve inexact arithmetic operators [7], low-bit numerical formats [11], weight quantization [12], and neuron connection pruning [13]. Approximate hardware acce- lerators for DNN inferencing [7] [9] have gained momentum due to their attractive performance-per-power ratio. Moreover, the ASIC FPGA technology facilitates approximate computing by allowing low-level optimizations and custom datapath bit- widths, contrary to the general-purpose GPU CPU solutions. The design of approximate DNN accelerators imposes se- veral challenges. As discussed in [7], one of the research goals is to avoid retraining for reducing the accuracy loss. The reason is twofold: (i) proprietary datasets models may not be available, (ii) the increased training time due the emulation of the hardware approximations and the use of custom ap- proximate arithmetic operators. Another important challenge in approximate DNNs is the approximation localization, i.e., where to insert approximations, in order to maximize the energy efficiency while keeping accuracy in acceptable levels. In this paper, we introduce the MAx-DNN framework to ad- dress the aforementioned design challenges, by examining ap- proximation at different levels of the DNN architecture, i.e., in network s layers, layer s filters, or filter s kernels. Considering that the majority of the computations in DNNs are multiply- accumulate [9], we focus on optimizing the multiplications. MAx-DNN adopts ALWANN [7] for generating approximate DNN hardware accelerators without retraining, and extends it with our approximation localization, which is based on the ROUP multiplication library [2]. Our design space is larger compared to that of the original ALWANN framework, which introduces the same inexact multipliers in each convolutional layer (multipliers may differ among layers). Interestingly, the proposed approximation approaches assign different approxi- mate ROUP multipliers either in each convolutional layer, fil- ter, or kernel. To evaluate our approximations, we employ the quantized ResNet-8 network, trained on the CIFAR-10 dataset, and use standard-cell ASIC technology with the TSMC 45- nm library. At first, we examine the sensitivity of the DNN layers to approximations in both standalone and combined fashion, and then, we perform an extensive design space exploration and accuracy energy Pareto analysis to extract the most prominent configurations of our approaches. The results show that, compared to the quantized model, our designs deliver up to 54 energy gains in exchange for an accuracy loss of 4 , while providing 2 energy efficiency versus the default ALWANN approximations. The paper contribution lies in proposing an improved fine-grained method to approximate the DNN multiplications, and quantifying the results from various approximation approaches and configurations. The remainder of the paper is organized as follows. Section II introduces the ALWANN framework [7]. Section III defines our approximation space. Section IV reports the experimental evaluation. Finally, Section V draws the conclusions. 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. New Approx. Approaches Model Constructor Learning Network Build HDL Models C C Models ROUP Library ALWANN AxConv2D Extension Synopsys DC RTL Synthesis Layer-Level Filter-Level Kernel-Level1 Kernel-Level2 Initial Network Representation (Uniform Structure) ALWANN [7] TSMC 45-nm Final AxNNs Fig. 1. The MAx-DNN toolflow architecture, extension of ALWANN [7]. II. THE ALWANN APPROXIMATION FRAMEWORK The ALWANN framework [7] takes the following inputs: (i) the trained (frozen) network model in protobuf format, (ii) a library of approximate multipliers, and (iii) architecture constraints for the hardware accelerator (e.g., pipelined or power-gated mode, number of approximate units). It assumes accurate addition and approximate multiplication, as well as one approximation type per convolutional layer. Moreover, to improve the accuracy without re-training, the network weights are tuned updated according to the multipliers properties. The approximate networks, labeled as AxNNs, are modified versions of the initial model and satisfy the user constraints for the architecture of the accelerator. To enable ALWANN, TensorFlow is extended to support approximate quantized layers by creating a new operator, which replaces the conventional QuantizedConv2D layers with AxConv2D layers. This operator allows to specify which approximate multipliers to employ via the AxMult(str) para- meter (a C C model of the multipliers is necessary), and optionally use the weight tuning feature via AxTune(bool). The frozen model is processed by the TensorFlow transform graph tool, which inserts the AxConv2D layers, and then, the Pareto- optimal AxNNs are extracted by the NSGA-II algorithm. In this paper, we equip ALWANN with new approximation approaches regarding the distribution of the approximate mul- tiplications across the network. The toolflow of the extended framework version is illustrated in Fig. 1. In comparison with ALWANN, we modify the AxConv2D TensorFlow operator to support our approximation approaches at three distinct levels (layer, filter, kernel) and also employ the state-of-the-art ROUP library of approximate multipliers [2]. III. DEFINING THE APPROXIMATION SPACE In this section, we discuss the proposed approximation space, explaining how we distribute the approximate multipli- ers in each layer, filter, or kernel of the DNN. For our analysis, we assume the typical DNN architecture: each convolutional layer includes N filters, with each one consisting of M convolution kernels to process the M input channels and output a feature map (N output feature maps are generated in total). The approximations are not applied uniformly across the examined level, i.e., each layer filter kernel is assigned a different ROUP multiplier. Finally, we present the ROUP multiplier family [2] and the model for estimating energy. A. LLAM: Layer-Level Approximate Multiplication The first approach, illustrated in Fig. 2a, aims to insert approximations with different strength in the network. Specifi- cally, we perform all the multiplications of each convolutional layer with a ROUP multiplier. This approach is the conven- tional one, where the approximations are distributed across the network by assigning an approximate multiplier, either the same (uniform distribution) or different (non-uniform distribution), to each convolutional layer. B. FLAM: Filter-Level Approximate Multiplication Similarly to LLAM, our second approach, illustrated in Fig. 2b, creates filters with different approximation for each convolutional layer of the DNN. Namely, we use different ROUP multipliers in each filter of the layer, contrary to the first approach, where all the filters of a layer have the same ROUP multiplier. This approach is implemented by creating groups of filters and assigning them the ROUP multipliers. C. KLAM: Kernel-Level Approximate Multiplication In the third approximation approach, we proceed deeper in the DNN architecture, and approximate the multiplications separately for each convolutional kernel. As a result, the convolutions of each filter are performed with different ROUP multipliers. Fig. 2c illustrates the three proposed flavors of this approach: the channel flavor, where all the multiplications corresponding to the kernel are performed with the same mul- tiplier, and the row column flavor, where different multipliers are employed for each row column of the kernel. D. KLMS: Kernel-Level Multiplication Skip Our last approach, illustrated in Fig. 2d, skips some of the multiplications of each convolution based on the weight values. More specifically, we calculate the mean value of all the kernel weights in each layer, and perform only the multiplications corresponding to the weights that are near it. Assuming a mean µ and a standard deviation σ, we select to perform only the multiplications with the weights belonging either in the interval [µ σ, µ σ] or [µ 2σ, µ 2σ]. E. The ROUP Approximate Multipliers The ROUP family of approximate multipliers [2] is based on the radix-4 encoding, and applies two orthogonal approxi- mation techniques, i.e., Asymmetric Rounding and Perforation. In this hybrid approximation, Perforation omits the generation of least-significant partial products, and Asymmetric Rounding rounds each remaining partial product to a different bit-width, depending on its significance in the partial product matrix. The ROUP multiplication of two N-bit 2 s-complement numbers is performed by accumulating the non-perforated, rounded partial products, i.e., Pj, which are generated based on the radix-4 encoding [2]: ROUP(A, B) N 2 1 X j P Pj4j N 2 1 X j P Ar j bR4 j 4j (1) ROUP Conv Layer 1 Conv Layer 2 Conv Layer 3 ... M1 M2 M3 Conv Layer 4 Conv Layer 5 Conv Layer 6 Conv Layer 7 (a) LLAM ROUP Filter 1 Filter 2 Filter 3 ... M1 M2 M3 Filter 4 Filter 5 Filter 6 Filter 7 Conv Layer i (b) FLAM ROUP ... M1 M2 M3 w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 Filter j (c) KLAM w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 Filter j w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 (d) KLMS Fig. 2. The proposed non-uniform approximation approaches at different levels: (a) layer-level, (b) filter-level (c) kernel-level approximate multiplication, and (d) kernel-level multiplication skip. The parameter P denotes the number of successive least- significant partial products that are perforated. For each re- maining partial product Pj, the input operand A is rounded to its r least-significant bit (r differs per product) as follows: Ar j aN 1aN 2 ar 2 s ar 1 (2) Bit-level manipulations in Ar j facilitate the process of round- ing, as the above addition is absorbed in the calculations of the radix-4 correction terms, and the carry propagation is avoided. Finally, bR4 j is the conventional radix-4 logic function of the encoding signals of B, with an extra XOR gate involving ar 1, which is imposed by the optimizations to avoid the addition. The ROUP multipliers provide a large design space to explore various error energy trade-offs, as the approximations are tuned by two independent parameters. In the comparative evaluation of [2], the ROUP family formed the error energy Pareto front with increased resolution, outperforming several state-of-the-art multipliers. In comparison with the multipliers from the EvoApprox8b library [5], which is used in ALWANN, ROUP provides more approximation configurations, especially for larger bit-widths. As a result, its error scaling is more dense, while in terms of energy, several ROUP configurations outperform the EvoApprox8b multipliers by up to 15 . F. Energy Model for Approximate DNNs The energy consumption of the approximate DNN accelera- tors is estimated based on the implementation of the approxi- mate multipliers. Targeting standard-cell ASIC technology, the multipliers are implemented on the TSMC 45-nm library using industrial-strength tools, i.e., Synopsys Design Compiler for synthesis and Synopsys PrimeTime for measuring power. Similarly to ALWANN [7], we calculate the energy of each layer based on the number of multiplications and the energy per multiplier, i.e., mult avg mult energy. To estimate the energy of the entire DNN, we accumulate the individual layer energies. IV. EXPERIMENTAL EVALUATION To evaluate our approach, we employ the ResNet-8 DNN and the open-source ALWANN framework [7], which is based on TensorFlow 1.14. ResNet-8 is trained with quantization on the CIFAR-10 dataset, achieving 83 classification accuracy. Layer-wise study of error resilience: At first, we examine the error sensitivity of the convolutional layers, in an effort to understand which layers are offered for approximations. For this experiment, we pick three ROUP multipliers with different approximation strength, i.e., low , medium , and high , labeled as ROUPL, ROUPM, and ROUPH, respectively. Fig. 3a illustrates the accuracy scaling when using these multi- pliers only in the m-th convolutional layer (m 1, 2, ...7). Regardless of the approximation strength, it is shown that approximating one of the first layers results in remarkable accuracy loss (m 0 shows the baseline model with 83 accuracy). This is more evident in the ROUPH configurations, where significant computation errors are inserted. In this case, when approximating one of the layers 4 7, the accuracy loss is decreased and stabilized around 8 . Fig. 3b, depicts the accuracy scaling when approximating the first m layers. As expected, the accuracy loss is increased with respect to the number of approximate layers, however, we again notice the error resilience of the last layers. Specifically, the accuracy loss is slowing down when extending the approximation after the 4-th layer. Another important outcome from this exploration is the negligible accuracy loss of the ROUPL and ROUPM configurations, regardless of which and how many layers are approximated, as these multipliers provide significant energy gains compared to their accurate design, i.e., around 10 and 20 , respectively. 0 1 2 3 4 5 6 7 0.55 0.65 0.75 0.85 Approximate Layer Accuracy ROUPL ROUPM ROUPH (a) 0 1 2 3 4 5 6 7 0.25 0.45 0.65 0.85 Succes. Approximate Layers Accuracy ROUPL ROUPM ROUPH (b) Fig. 3. The scaling of ResNet-8 accuracy w.r.t. the layers approximated: (a) one layer per occasion (e.g., only the 5th), and (b) successive layers (e.g., layers 1 to 5). Layer 0 denotes the baseline quantized model. 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 Accuracy Loss Energy (mJ) Layer-Level (LLAM) Filter-Level (FLAM) Kernel-Level (KLAM KLMS) Fig. 4. The energy consumption and accuracy loss of the approximate ResNet- 8 hardware accelerators, implemented with the proposed approaches. Exploration of approximation space: Subsequently, we perform an extensive design space exploration on the proposed approaches, involving various ROUP multipliers, to extract their most prominent configurations in terms of accuracy and energy. For each approach, several multiplication replacements and combinations are examined. In Fig. 4, we present all the configurations that deliver at least 75 classification accuracy, i.e., up to 8 accuracy loss compared to the quantized model. As shown in the upper left segment of the plot, multiple configurations deliver negligible accuracy loss compared to the quantized model, which ranges from 0.02 to 1 , while improving the energy efficiency due to using the ROUP mul- tipliers. Therefore, our approaches can satisfy near-zero accu- racy loss with more energy-efficient computing. Regarding the efficiency of each approach, the Pareto front is formed almost exclusively from the kernel- and filter-level configurations. For the same accuracy loss, these configurations provide better energy than the conventional layer-level approach, which has to sacrifice a large amount of accuracy, i.e., more than 40 , to deliver this energy efficiency. Comparison to ALWANN framework: Table I com- pares the Pareto-front configurations with ALWANN networks employing the EvoApprox8b multipliers [5]. The proposed designs deliver better accuracy, as the average loss of the EvoApprox8b configurations is 23 , while in terms of energy, they provide 2 gains. This comparison highlights the advantage of studying the approximations at a lower DNN level, i.e., filter or kernel. Namely, the fine-grained use of multipliers with different approximation strength outperforms the conventional approximation of all the layer multiplications. Lessons Learnt: According to our multi-level design space exploration and evaluation, we experimentally prove that: (i) the first convolutional layers are more sensitive in appro- ximations, i.e., less error resilient, than the final ones. (ii) the approximation of multipliers based on a non-uniform fine-grained filter kernel-level approach delivers better accu- racy than the coarse-grained layer-level approximation. V. CONCLUSION AND FUTURE WORK In this work, we presented the MAx-DNN framework to explore the efficiency of multi-level arithmetic approximation in DNN hardware accelerators. MAx-DNN extends prior-art TABLE I COMPARISON OF APPROXIMATE RESNET-8 HARDWARE ACCELERATORS Approximation Approach Config. Energy Gain Accuracy Loss1 Proposed ROUP [2] FLAM-3clas. 2 1 1 49 18 FLAM-3clas. 2 2 1 52 20 KLAM-chan. 1 0 1 46 17 KLAM-chan. 2 0 2 53 21 KLAM-chan. 1 1 2 50 19 KLAM-chan. 2 1 2 54 21 KLAM-row 2 1 1 50 19 KLAM-row 2 1 2 52 20 Uniform Evo [5] Evo mul8 2AC 23 20 Evo mul8u 2HH 23 23 Evo mul8u NGR 32 23 Evo mul8u ZFB 39 23 Evo mul8u 7C1 20 24 1 It is reported compared to the full accurate model. The baseline quantized model, where we apply our approximations, already has an accuracy loss of 17 . design approaches considering approximate multiplier hete- rogeneity, not only to each DNN layer, but also to each filter and kernel. The results show that our most prominent configurations achieve up to 54 energy reduction and 4 accuracy loss, while providing 2 gains versus the straightfor- ward approximation with EvoApprox8b multipliers. Our future work includes evaluation on different DNN types, and study of the error propagation among the DNN nodes. REFERENCES [1] S. Mittal, A Survey of Techniques for Approximate Computing, ACM Computing Surveys, vol. 48, no. 4, Mar. 2016. [2] V. Leon et al., Cooperative Arithmetic-Aware Approximation Tech- niques for Energy-Efficient Multipliers, in Design Automation Confer- ence (DAC), 2019, pp. 1 6. [3] D. Hernandez-Araya et al., AUGER: A Tool for Generating Approximate Arithmetic Circuits, in IEEE Latin American Symposium on Circuits Systems (LASCAS), 2020, pp. 1 4. [4] V. Leon et al., Approximate Hybrid High Radix Encoding for Energy- Efficient Inexact Multipliers, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 26, no. 3, pp. 421 430, Mar. 2018. [5] V. Mrazek et al., EvoApprox8b: Library of Approximate Adders and Multipliers for Circuit Design and Benchmarking of Approximation Meth- ods, in Design, Automation and Test in Europe Conference (DATE), 2017, pp. 258 261. [6] V. Leon et al., Exploiting the Potential of Approximate Arithmetic in DSP AI Hardware Accelerators, in Int l. Conference on Field Programmable Logic and Applications (FPL), 2021, pp. 1 2. [7] V. Mrazek et al., ALWANN: Automatic Layer-Wise Approximation of Deep Neural Network Accelerators without Retraining, in Int l. Confer- ence on Computer-Aided Design (ICCAD), 2019, pp. 1 8. [8] G. Lentaris et al., Combining Arithmetic Approximation Techniques for Improved CNN Circuit Design, in Int l. Conference on Electronics, Circuits and Systems (ICECS), 2020, pp. 1 4. [9] S. Venkataramani et al., Efficient AI System Design With Cross-Layer Approximate Computing, Proceedings of the IEEE, vol. 108, no. 12, pp. 2232 2250, Dec. 2020. [10] V. Leon et al., ApproxQAM: High-Order QAM Demodulation Circuits with Approximate Arithmetic, in Int l. Conference on Modern Circuits and Systems Technologies (MOCAST), 2021, pp. 1 5. [11] U. K oster et al., Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks, in Int l. Conference on Neural Infor- mation Processing Systems (NIPS), 2017, pp. 1740 1750. [12] J. Wu et al., Quantized Convolutional Neural Networks for Mobile De- vices, in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 4820 4828. [13] S. Anwar et al., Structured Pruning of Deep Convolutional Neural Net- works, ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 13, no. 3, Feb. 2017.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.21371v1 [cs.LG] 26 Jun 2025 Presented at the 13th IEEE Latin America Symposium on Circuits and System (LASCAS), 01-04 03 2022, Puerto Varas, CL. This is the authors version of the paper. The final published version is posted on IEEE Xplore. DOI: MAx-DNN: Multi-Level Arithmetic Approximation for Energy-Efficient DNN Hardware Accelerators Vasileios Leon , Georgios Makris , Sotirios Xydis , Kiamal Pekmestzi , Dimitrios Soudris National Technical University of Athens, School of Electrical and Computer Engineering, 15780 Athens, Greece Harokopio University of Athens, Department of Informatics and Telematics, 17778 Athens, Greece Abstract Nowadays, the rapid growth of Deep Neural Net- work (DNN) architectures has established them as the defacto approach for providing advanced Machine Learning tasks with excellent accuracy. Targeting low-power DNN computing, this paper examines the interplay of fine-grained error resilience of DNN workloads in collaboration with hardware approximation techniques, to achieve higher levels of energy efficiency. Utilizing the state-of-the-art ROUP approximate multipliers, we systemat- ically explore their fine-grained distribution across the network according to our layer-, filter-, and kernel-level approaches, and examine their impact on accuracy and energy. We use the ResNet- 8 model on the CIFAR-10 dataset to evaluate our approximations. The proposed solution delivers up to 54 energy gains in exchange for up to 4 accuracy loss, compared to the baseline quantized model, while it provides 2 energy gains with better accuracy versus the state-of-the-art DNN approximations. Index Terms Approximate Computing, Inexact Multipliers, ASIC, Deep Neural Networks, ResNet, CIFAR-10, TensorFlow. I. INTRODUCTION The proliferation of demanding workloads in the Digital Signal Processing (DSP) and Artificial Intelligence (AI) do- mains marks a new era in the design of integrated circuits and embedded systems. Towards high-performance computing within a limited power envelope, the research community is examining alternative design strategies.\n\n--- Segment 2 ---\nINTRODUCTION The proliferation of demanding workloads in the Digital Signal Processing (DSP) and Artificial Intelligence (AI) do- mains marks a new era in the design of integrated circuits and embedded systems. Towards high-performance computing within a limited power envelope, the research community is examining alternative design strategies. In this direction, Approximate Computing is an emerging design paradigm [1], which exploits the error resilience to trade accuracy for gains in power, energy, area, and or latency. In the field of inexact hardware, approximation techniques are applied at different levels, i.e., from circuits [2] [5] to accelerators [6] [10]. The intrinsic error resilience and increased computational demands of DNNs, which are a state-of-the-art AI approach to tackle Computer Vision tasks (e.g., image classification, object detection, pose estimation), constitutes them as promising candidates for approximate computing. Typical approximation techniques on DNNs involve inexact arithmetic operators [7], low-bit numerical formats [11], weight quantization [12], and neuron connection pruning [13]. Approximate hardware acce- lerators for DNN inferencing [7] [9] have gained momentum due to their attractive performance-per-power ratio. Moreover, the ASIC FPGA technology facilitates approximate computing by allowing low-level optimizations and custom datapath bit- widths, contrary to the general-purpose GPU CPU solutions. The design of approximate DNN accelerators imposes se- veral challenges. As discussed in [7], one of the research goals is to avoid retraining for reducing the accuracy loss. The reason is twofold: (i) proprietary datasets models may not be available, (ii) the increased training time due the emulation of the hardware approximations and the use of custom ap- proximate arithmetic operators. Another important challenge in approximate DNNs is the approximation localization, i.e., where to insert approximations, in order to maximize the energy efficiency while keeping accuracy in acceptable levels. In this paper, we introduce the MAx-DNN framework to ad- dress the aforementioned design challenges, by examining ap- proximation at different levels of the DNN architecture, i.e., in network s layers, layer s filters, or filter s kernels.\n\n--- Segment 3 ---\nAnother important challenge in approximate DNNs is the approximation localization, i.e., where to insert approximations, in order to maximize the energy efficiency while keeping accuracy in acceptable levels. In this paper, we introduce the MAx-DNN framework to ad- dress the aforementioned design challenges, by examining ap- proximation at different levels of the DNN architecture, i.e., in network s layers, layer s filters, or filter s kernels. Considering that the majority of the computations in DNNs are multiply- accumulate [9], we focus on optimizing the multiplications. MAx-DNN adopts ALWANN [7] for generating approximate DNN hardware accelerators without retraining, and extends it with our approximation localization, which is based on the ROUP multiplication library [2]. Our design space is larger compared to that of the original ALWANN framework, which introduces the same inexact multipliers in each convolutional layer (multipliers may differ among layers). Interestingly, the proposed approximation approaches assign different approxi- mate ROUP multipliers either in each convolutional layer, fil- ter, or kernel. To evaluate our approximations, we employ the quantized ResNet-8 network, trained on the CIFAR-10 dataset, and use standard-cell ASIC technology with the TSMC 45- nm library. At first, we examine the sensitivity of the DNN layers to approximations in both standalone and combined fashion, and then, we perform an extensive design space exploration and accuracy energy Pareto analysis to extract the most prominent configurations of our approaches. The results show that, compared to the quantized model, our designs deliver up to 54 energy gains in exchange for an accuracy loss of 4 , while providing 2 energy efficiency versus the default ALWANN approximations. The paper contribution lies in proposing an improved fine-grained method to approximate the DNN multiplications, and quantifying the results from various approximation approaches and configurations. The remainder of the paper is organized as follows. Section II introduces the ALWANN framework [7]. Section III defines our approximation space. Section IV reports the experimental evaluation. Finally, Section V draws the conclusions. 2022 IEEE. Personal use of this material is permitted.\n\n--- Segment 4 ---\n2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. New Approx. Approaches Model Constructor Learning Network Build HDL Models C C Models ROUP Library ALWANN AxConv2D Extension Synopsys DC RTL Synthesis Layer-Level Filter-Level Kernel-Level1 Kernel-Level2 Initial Network Representation (Uniform Structure) ALWANN [7] TSMC 45-nm Final AxNNs Fig. 1. The MAx-DNN toolflow architecture, extension of ALWANN [7]. II. THE ALWANN APPROXIMATION FRAMEWORK The ALWANN framework [7] takes the following inputs: (i) the trained (frozen) network model in protobuf format, (ii) a library of approximate multipliers, and (iii) architecture constraints for the hardware accelerator (e.g., pipelined or power-gated mode, number of approximate units). It assumes accurate addition and approximate multiplication, as well as one approximation type per convolutional layer. Moreover, to improve the accuracy without re-training, the network weights are tuned updated according to the multipliers properties. The approximate networks, labeled as AxNNs, are modified versions of the initial model and satisfy the user constraints for the architecture of the accelerator. To enable ALWANN, TensorFlow is extended to support approximate quantized layers by creating a new operator, which replaces the conventional QuantizedConv2D layers with AxConv2D layers. This operator allows to specify which approximate multipliers to employ via the AxMult(str) para- meter (a C C model of the multipliers is necessary), and optionally use the weight tuning feature via AxTune(bool). The frozen model is processed by the TensorFlow transform graph tool, which inserts the AxConv2D layers, and then, the Pareto- optimal AxNNs are extracted by the NSGA-II algorithm. In this paper, we equip ALWANN with new approximation approaches regarding the distribution of the approximate mul- tiplications across the network.\n\n--- Segment 5 ---\nThe frozen model is processed by the TensorFlow transform graph tool, which inserts the AxConv2D layers, and then, the Pareto- optimal AxNNs are extracted by the NSGA-II algorithm. In this paper, we equip ALWANN with new approximation approaches regarding the distribution of the approximate mul- tiplications across the network. The toolflow of the extended framework version is illustrated in Fig. 1. In comparison with ALWANN, we modify the AxConv2D TensorFlow operator to support our approximation approaches at three distinct levels (layer, filter, kernel) and also employ the state-of-the-art ROUP library of approximate multipliers [2]. III. DEFINING THE APPROXIMATION SPACE In this section, we discuss the proposed approximation space, explaining how we distribute the approximate multipli- ers in each layer, filter, or kernel of the DNN. For our analysis, we assume the typical DNN architecture: each convolutional layer includes N filters, with each one consisting of M convolution kernels to process the M input channels and output a feature map (N output feature maps are generated in total). The approximations are not applied uniformly across the examined level, i.e., each layer filter kernel is assigned a different ROUP multiplier. Finally, we present the ROUP multiplier family [2] and the model for estimating energy. A. LLAM: Layer-Level Approximate Multiplication The first approach, illustrated in Fig. 2a, aims to insert approximations with different strength in the network. Specifi- cally, we perform all the multiplications of each convolutional layer with a ROUP multiplier. This approach is the conven- tional one, where the approximations are distributed across the network by assigning an approximate multiplier, either the same (uniform distribution) or different (non-uniform distribution), to each convolutional layer. B. FLAM: Filter-Level Approximate Multiplication Similarly to LLAM, our second approach, illustrated in Fig. 2b, creates filters with different approximation for each convolutional layer of the DNN. Namely, we use different ROUP multipliers in each filter of the layer, contrary to the first approach, where all the filters of a layer have the same ROUP multiplier.\n\n--- Segment 6 ---\n2b, creates filters with different approximation for each convolutional layer of the DNN. Namely, we use different ROUP multipliers in each filter of the layer, contrary to the first approach, where all the filters of a layer have the same ROUP multiplier. This approach is implemented by creating groups of filters and assigning them the ROUP multipliers. C. KLAM: Kernel-Level Approximate Multiplication In the third approximation approach, we proceed deeper in the DNN architecture, and approximate the multiplications separately for each convolutional kernel. As a result, the convolutions of each filter are performed with different ROUP multipliers. Fig. 2c illustrates the three proposed flavors of this approach: the channel flavor, where all the multiplications corresponding to the kernel are performed with the same mul- tiplier, and the row column flavor, where different multipliers are employed for each row column of the kernel. D. KLMS: Kernel-Level Multiplication Skip Our last approach, illustrated in Fig. 2d, skips some of the multiplications of each convolution based on the weight values. More specifically, we calculate the mean value of all the kernel weights in each layer, and perform only the multiplications corresponding to the weights that are near it. Assuming a mean µ and a standard deviation σ, we select to perform only the multiplications with the weights belonging either in the interval [µ σ, µ σ] or [µ 2σ, µ 2σ]. E. The ROUP Approximate Multipliers The ROUP family of approximate multipliers [2] is based on the radix-4 encoding, and applies two orthogonal approxi- mation techniques, i.e., Asymmetric Rounding and Perforation. In this hybrid approximation, Perforation omits the generation of least-significant partial products, and Asymmetric Rounding rounds each remaining partial product to a different bit-width, depending on its significance in the partial product matrix.\n\n--- Segment 7 ---\nE. The ROUP Approximate Multipliers The ROUP family of approximate multipliers [2] is based on the radix-4 encoding, and applies two orthogonal approxi- mation techniques, i.e., Asymmetric Rounding and Perforation. In this hybrid approximation, Perforation omits the generation of least-significant partial products, and Asymmetric Rounding rounds each remaining partial product to a different bit-width, depending on its significance in the partial product matrix. The ROUP multiplication of two N-bit 2 s-complement numbers is performed by accumulating the non-perforated, rounded partial products, i.e., Pj, which are generated based on the radix-4 encoding [2]: ROUP(A, B) N 2 1 X j P Pj4j N 2 1 X j P Ar j bR4 j 4j (1) ROUP Conv Layer 1 Conv Layer 2 Conv Layer 3 ... M1 M2 M3 Conv Layer 4 Conv Layer 5 Conv Layer 6 Conv Layer 7 (a) LLAM ROUP Filter 1 Filter 2 Filter 3 ... M1 M2 M3 Filter 4 Filter 5 Filter 6 Filter 7 Conv Layer i (b) FLAM ROUP ... M1 M2 M3 w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 Filter j (c) KLAM w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 Filter j w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 w1 w2 w3 w4 w5 w6 w7 w8 w9 (d) KLMS Fig. 2. The proposed non-uniform approximation approaches at different levels: (a) layer-level, (b) filter-level (c) kernel-level approximate multiplication, and (d) kernel-level multiplication skip.\n\n--- Segment 8 ---\n2. The proposed non-uniform approximation approaches at different levels: (a) layer-level, (b) filter-level (c) kernel-level approximate multiplication, and (d) kernel-level multiplication skip. The parameter P denotes the number of successive least- significant partial products that are perforated. For each re- maining partial product Pj, the input operand A is rounded to its r least-significant bit (r differs per product) as follows: Ar j aN 1aN 2 ar 2 s ar 1 (2) Bit-level manipulations in Ar j facilitate the process of round- ing, as the above addition is absorbed in the calculations of the radix-4 correction terms, and the carry propagation is avoided. Finally, bR4 j is the conventional radix-4 logic function of the encoding signals of B, with an extra XOR gate involving ar 1, which is imposed by the optimizations to avoid the addition. The ROUP multipliers provide a large design space to explore various error energy trade-offs, as the approximations are tuned by two independent parameters. In the comparative evaluation of [2], the ROUP family formed the error energy Pareto front with increased resolution, outperforming several state-of-the-art multipliers. In comparison with the multipliers from the EvoApprox8b library [5], which is used in ALWANN, ROUP provides more approximation configurations, especially for larger bit-widths. As a result, its error scaling is more dense, while in terms of energy, several ROUP configurations outperform the EvoApprox8b multipliers by up to 15 . F. Energy Model for Approximate DNNs The energy consumption of the approximate DNN accelera- tors is estimated based on the implementation of the approxi- mate multipliers. Targeting standard-cell ASIC technology, the multipliers are implemented on the TSMC 45-nm library using industrial-strength tools, i.e., Synopsys Design Compiler for synthesis and Synopsys PrimeTime for measuring power. Similarly to ALWANN [7], we calculate the energy of each layer based on the number of multiplications and the energy per multiplier, i.e., mult avg mult energy. To estimate the energy of the entire DNN, we accumulate the individual layer energies. IV.\n\n--- Segment 9 ---\nTo estimate the energy of the entire DNN, we accumulate the individual layer energies. IV. EXPERIMENTAL EVALUATION To evaluate our approach, we employ the ResNet-8 DNN and the open-source ALWANN framework [7], which is based on TensorFlow 1.14. ResNet-8 is trained with quantization on the CIFAR-10 dataset, achieving 83 classification accuracy. Layer-wise study of error resilience: At first, we examine the error sensitivity of the convolutional layers, in an effort to understand which layers are offered for approximations. For this experiment, we pick three ROUP multipliers with different approximation strength, i.e., low , medium , and high , labeled as ROUPL, ROUPM, and ROUPH, respectively. Fig. 3a illustrates the accuracy scaling when using these multi- pliers only in the m-th convolutional layer (m 1, 2, ...7). Regardless of the approximation strength, it is shown that approximating one of the first layers results in remarkable accuracy loss (m 0 shows the baseline model with 83 accuracy). This is more evident in the ROUPH configurations, where significant computation errors are inserted. In this case, when approximating one of the layers 4 7, the accuracy loss is decreased and stabilized around 8 . Fig. 3b, depicts the accuracy scaling when approximating the first m layers. As expected, the accuracy loss is increased with respect to the number of approximate layers, however, we again notice the error resilience of the last layers. Specifically, the accuracy loss is slowing down when extending the approximation after the 4-th layer. Another important outcome from this exploration is the negligible accuracy loss of the ROUPL and ROUPM configurations, regardless of which and how many layers are approximated, as these multipliers provide significant energy gains compared to their accurate design, i.e., around 10 and 20 , respectively. 0 1 2 3 4 5 6 7 0.55 0.65 0.75 0.85 Approximate Layer Accuracy ROUPL ROUPM ROUPH (a) 0 1 2 3 4 5 6 7 0.25 0.45 0.65 0.85 Succes. Approximate Layers Accuracy ROUPL ROUPM ROUPH (b) Fig. 3. The scaling of ResNet-8 accuracy w.r.t.\n\n--- Segment 10 ---\n3. The scaling of ResNet-8 accuracy w.r.t. the layers approximated: (a) one layer per occasion (e.g., only the 5th), and (b) successive layers (e.g., layers 1 to 5). Layer 0 denotes the baseline quantized model. 0.17 0.18 0.19 0.20 0.21 0.22 0.23 0.24 0.25 3.2 3.4 3.6 3.8 4.0 4.2 4.4 4.6 Accuracy Loss Energy (mJ) Layer-Level (LLAM) Filter-Level (FLAM) Kernel-Level (KLAM KLMS) Fig. 4. The energy consumption and accuracy loss of the approximate ResNet- 8 hardware accelerators, implemented with the proposed approaches. Exploration of approximation space: Subsequently, we perform an extensive design space exploration on the proposed approaches, involving various ROUP multipliers, to extract their most prominent configurations in terms of accuracy and energy. For each approach, several multiplication replacements and combinations are examined. In Fig. 4, we present all the configurations that deliver at least 75 classification accuracy, i.e., up to 8 accuracy loss compared to the quantized model. As shown in the upper left segment of the plot, multiple configurations deliver negligible accuracy loss compared to the quantized model, which ranges from 0.02 to 1 , while improving the energy efficiency due to using the ROUP mul- tipliers. Therefore, our approaches can satisfy near-zero accu- racy loss with more energy-efficient computing. Regarding the efficiency of each approach, the Pareto front is formed almost exclusively from the kernel- and filter-level configurations. For the same accuracy loss, these configurations provide better energy than the conventional layer-level approach, which has to sacrifice a large amount of accuracy, i.e., more than 40 , to deliver this energy efficiency. Comparison to ALWANN framework: Table I com- pares the Pareto-front configurations with ALWANN networks employing the EvoApprox8b multipliers [5]. The proposed designs deliver better accuracy, as the average loss of the EvoApprox8b configurations is 23 , while in terms of energy, they provide 2 gains. This comparison highlights the advantage of studying the approximations at a lower DNN level, i.e., filter or kernel.\n\n--- Segment 11 ---\nThe proposed designs deliver better accuracy, as the average loss of the EvoApprox8b configurations is 23 , while in terms of energy, they provide 2 gains. This comparison highlights the advantage of studying the approximations at a lower DNN level, i.e., filter or kernel. Namely, the fine-grained use of multipliers with different approximation strength outperforms the conventional approximation of all the layer multiplications. Lessons Learnt: According to our multi-level design space exploration and evaluation, we experimentally prove that: (i) the first convolutional layers are more sensitive in appro- ximations, i.e., less error resilient, than the final ones. (ii) the approximation of multipliers based on a non-uniform fine-grained filter kernel-level approach delivers better accu- racy than the coarse-grained layer-level approximation. V. CONCLUSION AND FUTURE WORK In this work, we presented the MAx-DNN framework to explore the efficiency of multi-level arithmetic approximation in DNN hardware accelerators. MAx-DNN extends prior-art TABLE I COMPARISON OF APPROXIMATE RESNET-8 HARDWARE ACCELERATORS Approximation Approach Config. Energy Gain Accuracy Loss1 Proposed ROUP [2] FLAM-3clas. 2 1 1 49 18 FLAM-3clas. 2 2 1 52 20 KLAM-chan. 1 0 1 46 17 KLAM-chan. 2 0 2 53 21 KLAM-chan. 1 1 2 50 19 KLAM-chan. 2 1 2 54 21 KLAM-row 2 1 1 50 19 KLAM-row 2 1 2 52 20 Uniform Evo [5] Evo mul8 2AC 23 20 Evo mul8u 2HH 23 23 Evo mul8u NGR 32 23 Evo mul8u ZFB 39 23 Evo mul8u 7C1 20 24 1 It is reported compared to the full accurate model. The baseline quantized model, where we apply our approximations, already has an accuracy loss of 17 . design approaches considering approximate multiplier hete- rogeneity, not only to each DNN layer, but also to each filter and kernel.\n\n--- Segment 12 ---\nThe baseline quantized model, where we apply our approximations, already has an accuracy loss of 17 . design approaches considering approximate multiplier hete- rogeneity, not only to each DNN layer, but also to each filter and kernel. The results show that our most prominent configurations achieve up to 54 energy reduction and 4 accuracy loss, while providing 2 gains versus the straightfor- ward approximation with EvoApprox8b multipliers. Our future work includes evaluation on different DNN types, and study of the error propagation among the DNN nodes. REFERENCES [1] S. Mittal, A Survey of Techniques for Approximate Computing, ACM Computing Surveys, vol. 48, no. 4, Mar. 2016. [2] V. Leon et al., Cooperative Arithmetic-Aware Approximation Tech- niques for Energy-Efficient Multipliers, in Design Automation Confer- ence (DAC), 2019, pp. 1 6. [3] D. Hernandez-Araya et al., AUGER: A Tool for Generating Approximate Arithmetic Circuits, in IEEE Latin American Symposium on Circuits Systems (LASCAS), 2020, pp. 1 4. [4] V. Leon et al., Approximate Hybrid High Radix Encoding for Energy- Efficient Inexact Multipliers, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 26, no. 3, pp. 421 430, Mar. 2018. [5] V. Mrazek et al., EvoApprox8b: Library of Approximate Adders and Multipliers for Circuit Design and Benchmarking of Approximation Meth- ods, in Design, Automation and Test in Europe Conference (DATE), 2017, pp. 258 261. [6] V. Leon et al., Exploiting the Potential of Approximate Arithmetic in DSP AI Hardware Accelerators, in Int l. Conference on Field Programmable Logic and Applications (FPL), 2021, pp. 1 2. [7] V. Mrazek et al., ALWANN: Automatic Layer-Wise Approximation of Deep Neural Network Accelerators without Retraining, in Int l. Confer- ence on Computer-Aided Design (ICCAD), 2019, pp. 1 8.\n\n--- Segment 13 ---\n[7] V. Mrazek et al., ALWANN: Automatic Layer-Wise Approximation of Deep Neural Network Accelerators without Retraining, in Int l. Confer- ence on Computer-Aided Design (ICCAD), 2019, pp. 1 8. [8] G. Lentaris et al., Combining Arithmetic Approximation Techniques for Improved CNN Circuit Design, in Int l. Conference on Electronics, Circuits and Systems (ICECS), 2020, pp. 1 4. [9] S. Venkataramani et al., Efficient AI System Design With Cross-Layer Approximate Computing, Proceedings of the IEEE, vol. 108, no. 12, pp. 2232 2250, Dec. 2020. [10] V. Leon et al., ApproxQAM: High-Order QAM Demodulation Circuits with Approximate Arithmetic, in Int l. Conference on Modern Circuits and Systems Technologies (MOCAST), 2021, pp. 1 5. [11] U. K oster et al., Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks, in Int l. Conference on Neural Infor- mation Processing Systems (NIPS), 2017, pp. 1740 1750. [12] J. Wu et al., Quantized Convolutional Neural Networks for Mobile De- vices, in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 4820 4828. [13] S. Anwar et al., Structured Pruning of Deep Convolutional Neural Net- works, ACM Journal on Emerging Technologies in Computing Systems (JETC), vol. 13, no. 3, Feb. 2017.\n\n