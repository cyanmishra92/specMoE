=== ORIGINAL PDF: 2503.08823v2_ResBench_Benchmarking_LLM-Generated_FPGA_Designs_w.pdf ===\n\nRaw text length: 52628 characters\nCleaned text length: 51774 characters\nNumber of segments: 35\n\n=== CLEANED TEXT ===\n\nResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness Ce Guo Imperial College London United Kingdom Tong Zhao Imperial College London United Kingdom Abstract Field-Programmable Gate Arrays (FPGAs) are widely used in mod- ern hardware design, yet writing Hardware Description Language (HDL) code for FPGA implementation remains a complex and time- consuming task. Large Language Models (LLMs) have emerged as a promising tool for HDL generation, but existing benchmarks for LLM-based code generation primarily focus on functional correct- ness while overlooking hardware resource usage. Furthermore, cur- rent benchmarks offer limited diversity and do not fully represent the wide range of real-world FPGA applications. To address these shortcomings, we introduce ResBench, the first resource-focused benchmark explicitly designed to distinguish between resource- optimized and inefficient LLM-generated HDL code. ResBench con- sists of 56 problems across 12 categories, covering applications from finite state machines to financial computing. Our open-source evaluation framework automatically tests LLMs by generating Ver- ilog code, verifying correctness, and measuring resource usage. The experiments, which primarily analyze Lookup Table (LUT) usage, re- veal significant differences among LLMs, demonstrating ResBench s capability to identify models that generate more resource-optimized FPGA designs. CCS Concepts Hardware Board- and system-level test; Reconfigurable logic applications; Functional verification; Physical verification; Software and its engineering Source code generation. Keywords Large Language Models (LLMs), Hardware Description Languages (HDLs), Verilog Code Generation, FPGA Resource Utilization, Au- tomated Benchmarking, Empirical Evaluation of LLMs ACM Reference Format: Ce Guo and Tong Zhao. 2025. ResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness. In Proceedings of International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies (HEART 25). ACM, New York, NY, USA, 10 pages. XXXXXXX Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from HEART 25, Kumamoto, Japan 2025 Copyright held by the owner author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X 2018 06 1 Introduction Field-Programmable Gate Arrays (FPGAs) are widely used in re- configurable computing, providing flexible and high-performance hardware implementations for applications such as artificial intel- ligence acceleration, financial computing, and embedded systems. However, FPGA development traditionally requires manual cod- ing in hardware description languages (HDLs), a process that is both time-consuming and prone to errors. Large Language Models (LLMs) have recently shown potential in automating HDL genera- tion, offering a way to improve FPGA design productivity. While studies such as VeriGen [32] and RTLLM [23] have exam- ined the feasibility of generating Verilog with LLMs, most existing benchmarks focus primarily on functional correctness while pay- ing little attention to resource constraints, which is an essential consideration in FPGA design. FPGA designs are subject to strict hardware resource constraints. Even when HDL code passes functional correctness tests, its effi- ciency in utilizing these resources can vary significantly depending on how logic is optimized and mapped to FPGA hardware. In recon- figurable computing, resource-aware optimizations play a crucial role in determining whether a design can be practically deployed and the degree of parallelism achievable. However, existing bench- marks for LLM-generated hardware designs typically evaluate the generated code based only on syntax and functional correctness, overlooking resource-related issues. To address this limitation, we introduce ResBench, the first FPGA-resource-focused benchmark specifically designed to eval- uate LLM-generated designs based on resource usage. Unlike pre- vious benchmarks that focus primarily on syntax and functional correctness, ResBench highlights how well LLMs generate Verilog code optimized for FPGA resource utilization. Our key contribu- tions are: A resource-focused benchmark featuring 56 problems across 12 categories, covering real-world FPGA workloads such as combinational logic, state machines, AI accelerators, and financial computing applications. (Section 3) An open-source automated evaluation framework that per- forms LLM querying, functional correctness testing, FPGA synthesis, and resource measurement1. The framework auto- matically generates Verilog code using LLMs and evaluates its correctness and resource usage. (Section 4) A detailed study of nine LLMs, comparing their performance in functional correctness and FPGA resource usage. The results reveal substantial differences in how various models generate resource-conscious designs. (Section 5) 1Code repository: arXiv:2503.08823v2 [cs.AR] 21 Mar 2025 HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao By integrating FPGA resource awareness into benchmarking, ResBench provides a practical evaluation of LLM-generated HDL. This benchmark establishes a foundation for advancing AI-driven FPGA design, encouraging the development of more resource- efficient models optimized for FPGAs. 2 Background This section explores the evolution of large language models (LLMs) from general code generation to hardware design generation using hardware description languages (HDLs). We examine their capa- bilities and limitations, as well as existing benchmarks for LLM- generated hardware design. 2.1 Code-specialized and HDL-specialized LLMs Language models have seen significant advancements, particularly with the introduction of Transformers [36]. Large-scale pre-trained models such as BERT [8], the GPT series [5, 28, 44], and PaLM 2 [7] have expanded their capabilities across various tasks, including code generation. Code-specialized LLMs are models designed to generate com- puter programs based on human-language prompts. Surveys such as [16, 39, 45] review the latest techniques and model architectures developed for this purpose. In general, a code-specialized LLM can be created using two different approaches. The first approach trains models from scratch on large-scale open-source code datasets spanning multiple programming languages, as demonstrated by CodeGen [26], InCoder [9], and StarCoder [19]. These code-first LLMs excel at code completion and multi-language generation. However, this method demands extensive computational resources and high-quality datasets. Additionally, these models may strug- gle with instruction-driven tasks due to their limited ability to process human language instructions. The second approach fine-tunes general-purpose LLMs for coding tasks, as seen in Codex [6], which is derived from GPT-3, and Code Llama [30], which builds on Meta s Llama. Fine-tuning incorporates the linguistic knowledge of general LLMs, allowing them to handle natural language prompts while maintaining strong coding capabilities. This method is more computationally efficient than training from scratch, but it may lack the precision of code-first models in under- standing programming language syntax. While significant research has explored the use of language mod- els for general software code generation, the application of LLMs to HDL code has not received comparable attention. Existing work on HDL code generation primarily focuses on improving general- purpose or software-code-specialized LLMs. In particular, these studies aim to improve these LLMs understanding of hardware description tasks by training them on HDL datasets and bench- marking frameworks. Notable efforts in this area include VeriGen, MEV-LLM, and AutoVCoder. VeriGen [33] fine-tunes CodeGen (2B, 6B, 16B) on a Verilog dataset collected from GitHub repositories and textbooks. It employs supervised fine-tuning with problem-code pairs, validating functional correctness using a benchmark problem set and problems from HDLBits tutorials [31]. MEV-LLM [25] trains on 31,104 source code files from GitHub, labeled by GPT-3.5, to fine-tune CodeGen (2B, 6B, 16B) and GEMMA (2B, 7B) models. This approach yields an improve- ment of up to 23.9 in the metric [6] over VeriGen. AutoVCoder [10] also uses Verilog code from GitHub, fil- tering high-quality samples with ChatGPT-3.5. It applies a two-stage fine-tuning process to improve generalization, with final evaluation conducted on a real-world benchmark. Beyond direct fine-tuning, reinforcement learning approaches like Golden Code Feedback is used to refine models iteratively using user feedback [40]. Similarly, multi-modal techniques such as VGV [41] integrate circuit diagrams with textual data during training, allowing models to understand spatial and parallel aspects of circuit design. 2.2 Benchmarks for LLM-Generated Software The research community has recognized the need for standardized benchmarks to rigorously evaluate LLM-generated code in terms of design correctness. Most existing benchmarks for code generation are tailored for software development rather than hardware design. For instance, HumanEval [6] and MBPP (Mostly Basic Python Problems) [20] are widely used to evaluate LLMs for software code generation. HumanEval consists of 164 Python programming tasks, each with a function signature and a set of test cases to validate correct- ness. While relatively small in scope, this benchmark is carefully designed, making it well-suited for quick functional correctness evaluations of Python-based code generation. Several extensions have expanded its coverage, including HumanEval [20], which increases the number of test cases by 80 times, and HumanEvalPack [24], which extends the benchmark to six programming languages. Similarly, MBPP comprises approximately 974 short Python pro- gramming tasks, each including a task description prompt, a code solution, and three automated test cases. It emphasizes both correct- ness and clarity. Its enhanced version, MBPP [20], refines flawed implementations and expands the number of test cases to improve robustness. While HumanEval and MBPP provide fundamental benchmarks, they primarily focus on entry-level programming tasks and do not always reflect the complexity of real-world software develop- ment. To address this limitation, benchmarks with more intricate problem sets have been introduced. For example, DSP-1000 [18] con- tains 1,000 science-related programming tasks from seven Python libraries, covering a diverse set of topics and incorporating multi- criteria evaluation metrics to provide a more realistic assessment of code generation models. 2.3 Benchmarks for LLM-Generated Hardware Existing benchmarks for hardware design generation in HDL are often based on experiences and lessons learned in software code benchmarking, particularly in designing diverse problem sets and developing automated evaluation frameworks. Similar to their soft- ware counterparts, performance metrics for hardware design gen- eration frequently focus on design correctness measures such as However, benchmarking hardware designs typically in- volves simulation-based hardware verification [27]. ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 1: Benchmarks for LLM HDL Generation Benchmark Size PL Type Features VerilogEval [21] 156 Verilog Verilog code generation tasks Covers a wide range of tasks from simple combinational cir- cuits to finite state machines; includes automatic functional correctness testing. HDLEval [17] 100 Multiple Language-agnostic HDL evaluation Evaluates LLMs across multiple HDLs using standardized test- benches and formal verification; categorizes problems into combinational and pipelined tests. PyHDL-Eval [4] 168 Python-embedded DSLs Specification-to-RTL tasks Focuses on Python-embedded DSLs for hardware design; in- cludes Verilog reference solutions and testbenches; evaluates LLMs ability to handle specification-to-RTL translations. RTLLM [22] 50 Verilog, VHDL, Chisel Design RTL generation Supports evaluation across multiple HDL formats; spans var- ious design complexities and scales; includes an automated evaluation framework. VHDL-Eval [37] 202 VHDL VHDL code generation tasks Aggregates translated Verilog problems and publicly available VHDL problems; utilizes self-verifying testbenches for func- tional correctness validation. GenBen [38] 351 Verilog Fundamental hardware de- sign and debugging tasks Evaluates synthesizability, power consumption, area utiliza- tion, and timing performance to ensure real-world applicabil- ity. Unlike software benchmarks, which evaluate correctness by di- rectly executing code, hardware designs require dedicated testbench scripts to simulate hardware behavior and validate functionality. A typical evaluation process involves querying the LLM for HDL code, executing the generated HDL within a hardware simulation environment, and comparing the outputs to determine correctness. Some benchmarks have emerged to address these challenges, as presented in Table 1. Among them, VerilogEval, HDLEval, and PyHDL-Eval are widely recognized. VerilogEval [21] is a widely adopted benchmark for evaluat- ing LLMs in Verilog code generation [1, 10, 43]. It consists of 156 problems taken from the HDLBits tutorial website [31], covering a range of Verilog tasks from combinational circuits to finite state machines. The framework automates functional correctness testing by comparing the simulation outputs of generated designs against predefined golden so- lutions. HDLEval [17] follows a language-agnostic approach. In par- ticular, this benchmark allows the same set of problems, formulated in plain English, to be evaluated across different HDLs. The benchmark consists of 100 problems systemati- cally categorized into combinational and pipelined designs, covering fundamental hardware components such as logic gates, arithmetic operations, and pipelined processing units. A prominent feature of this benchmark is the use of formal verification instead of unit tests. This feature ensures that the generated HDL code is functionally correct and maintains logical equivalence with reference implementations. PyHDL-Eval [4] is a framework for evaluating LLMs on specification-to-Register Transfer Level (RTL) tasks within Python-embedded domain-specific languages (DSLs). It in- cludes 168 problems across 19 subclasses, covering combi- national logic and sequential logic. The evaluation process involves executing the generated code in Python-embedded HDLs (e.g., PyMTL3, PyRTL) and measuring functional cor- rectness based on pass rates. 2.4 Challenges in LLM Benchmarking for FPGA Design Despite advancements in LLM-driven Verilog generation, existing models primarily focus on producing syntactically and functionally correct HDL but fail to address critical hardware constraints essen- tial for FPGA deployment, such as resource efficiency, timing con- straints, and power consumption. Unlike ASIC design, FPGA-based development demands careful consideration of resource usage, in- cluding lookup tables (LUTs), flip-flops (FFs), block RAM (BRAM), and digital signal processing (DSP) blocks. However, current LLMs for Verilog generation lack an understanding of FPGA-specific re- quirements, often producing designs that are functionally correct but inefficient and impractical for real-world FPGA deployment. The inability to evaluate and optimize LLM-generated Verilog for FPGA resource constraints highlights the need for advancing resource-aware Verilog generation and motivates this study. To establish LLMs as a practical solution for HDL automation, it is essential to equip them with a deeper understanding of FPGA de- sign constraints. Achieving this requires developing new training datasets, designing robust evaluation frameworks, and refining LLM training strategies to enhance their capability in hardware-aware code generation. This paper focuses on constructing a benchmark that provides an evaluation of LLMs performance in generating HDLs that are both functionally correct and optimized for FPGA- specific resource constraints. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao 3 Design of ResBench This section presents the design of ResBench, outlining its guiding principles and structured problem set for evaluating LLM-generated Verilog code. Additionally, we compare ResBench with existing benchmarks for LLM-generated HDL code. 3.1 Design Principles and Benchmark Problems ResBench is designed to evaluate LLM-generated Verilog across a di- verse range of FPGA applications, with a primary focus on resource optimization awareness. The benchmark consists of 56 problems categorized into 12 domains, each representing a key area of FPGA applications. The problems in ResBench are carefully structured to evaluate both functional correctness and resource efficiency. The benchmark covers a wide range of FPGA design tasks, from fundamental digital logic and mathematical computation to more complex, application-driven domains such as machine learning, cryptography, and financial computing. By spanning these diverse categories, ResBench ensures that LLMs are tested on both low- level design problems and high-level algorithmic implementations for real-world applications. The design of ResBench is guided by two key principles: The Principle of Resource Usage Differentiation aims to highlight differences in how LLMs optimize FPGA resource usage. The benchmark includes problems that allow multiple resource-aware optimization strategies and require mathe- matical transformations. This approach makes it possible to distinguish between models that generate resource-efficient Verilog and those that do not. The Principle of FPGA Application Diversity recog- nizes the wide range of FPGA applications, particularly in computational acceleration and edge computing. ResBench spans various domains, including financial computing, cli- mate modeling, and signal processing, allowing LLMs to be tested across a broad set of real-world FPGA workloads. Table 2 provides an overview of the problem categories along with representative examples. The problems in the benchmark are designed to align with the two guiding principles. Specifically, ResBench addresses the principles as follows: ResBench addresses resource usage differentiation by in- troducing problems that require optimization techniques beyond simple code-level improvements. Fig. 1 illustrates this with a polynomial evaluation problem from the bench- mark. Algebraic simplification in this case enables a more efficient Verilog implementation using fewer LUTs. LLMs that fail to apply this optimization generate designs with excessive LUT consumption. ResBench addresses FPGA application diversity by cover- ing both foundational and application-driven workloads. Foundational problems include combinational logic, finite state machines, arithmetic operations, pipelining, polyno- mial evaluations, and mathematical functions. Application- driven problems span machine learning, encryption, and financial computing, emphasizing the role of FPGAs in AI acceleration, security, and high-speed data processing. By adhering to these principles, the benchmark evaluates not only an LLM s ability to generate syntactically correct Verilog code but also its capability to produce hardware-efficient designs suited for FPGA deployment. 3.2 Comparison with Existing Benchmarks Table 3 highlights key differences between ResBench and existing HDL benchmarks. The differences are particularly significant in FPGA resource optimization awareness and problem diversity: FPGA resource optimization awareness. Most existing bench- marks, such as VerilogEval, HDLEval, and GenBen, focus primarily on functional correctness and HDL syntax quality but do not explicitly account for FPGA resource usage. Con- sequently, these benchmarks cannot distinguish between functionally correct designs that differ significantly in hard- ware resource utilization. In contrast, ResBench introduces optimization-aware problems specifically designed to expose variations in resource usage. This enables a more practi- cal comparison of LLMs based on their ability to generate resource-efficient designs for FPGAs. Problem diversity. Existing benchmarks primarily focus on fundamental HDL constructs such as basic logic, state ma- chines, and arithmetic operations, with limited diversity in FPGA applications. For example, VerilogEval emphasizes control logic and arithmetic, while HDLEval mainly evalu- ates digital circuits and state machines. In contrast, ResBench encompasses a significantly broader range of FPGA appli- cations, including machine learning, encryption, financial computing, and physics-based modeling. These domains rep- resent real-world FPGA workloads where resource efficiency is crucial for minimizing device cost and maximizing paral- lelism. By incorporating a diverse set of tasks, ResBench pro- vides a more comprehensive evaluation of LLM-generated HDL in practical FPGA design scenarios. 4 Evaluation Framework for ResBench To evaluate LLM-generated designs for FPGA design with Res- Bench, we implement a structured framework that examines both functional correctness and hardware efficiency. We build the software for the evaluation framework based on the lessons learned in testing LLM-based software code generation. The benchmarks for LLM-based software share a common eval- uation framework aimed at assessing whether generated code is both syntactically valid and functionally correct. In particular, each benchmark generally includes four key components: (1) Prompts, which can be presented as a natural language de- scription [11, 35] or both description and function signature [6, 20], guiding the model on what to generate. (2) A reference solution, which serves as the correct implemen- tation for comparison. (3) Test cases, which are predefined inputs and expected outputs used to validate correctness (4) Performance metrics, like [6] and Code Similar- ity Scores [29], which estimates how effectively an LLM- generated solution satisfies the given problem constraints. A widely used metric is which measures the likelihood ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 2: Summary of benchmark categories, including the number of problems and representative examples. Category Problems Example Problems Combinational Logic 8 parity_8bit, mux4to1, bin_to_gray Finite State Machines 4 fsm_3state, traffic_light, elevator_controller Mathematical Functions 5 int_sqrt, fibonacci, mod_exp Basic Arithmetic Operations 5 add_8bit, mult_4bit, abs_diff Bitwise and Logical Operations 4 bitwise_ops, left_shift, rotate_left Pipelining 5 pipelined_adder, pipelined_multiplier, pipelined_fir Polynomial Evaluation 5 (洧논 2)2 (洧논 2)2 (洧논 2)2, (洧녩 洧녪)2 (洧녩 洧녪)2 Machine Learning 5 matrix_vector_mult, relu, mse_loss Financial Computing 4 compound_interest, present_value, currency_converter Encryption 3 caesar_cipher, modular_add_cipher, feistel_cipher Physics 4 free_fall_distance, kinetic_energy, wavelength Climate 4 carbon_footprint, heat_index, air_quality_index Total 56 (a) Design Generated by Qwen-2.5 (213 LUTs) (b) Design Generated by GPT-4 (0 LUT 1 DSP) Figure 1: Benchmark example illustrating HDL optimization capability using the expression (洧녩 洧녪)2 (洧녩 洧녪)2. (a) Qwen-2.5 computes the full expression directly, leading to high LUT usage. (b) GPT-4 simplifies the expression to 4洧녩洧녪, significantly reducing resource usage by using a single DSP unit instead of LUTs. This example demonstrates ResBench s ability to differentiate LLMs based on resource optimization. Table 3: Comparison of Benchmarks for LLM HDL Evaluation Benchmark Year Hardware Optimization Problem Diversity Awareness VerilogEval [33] 2023 General HDL No Logic, FSMs, arithmetic HDLEval [17] 2024 General HDL No Digital circuits, control logic PyHDL-Eval [4] 2024 General HDL No Python-based HDL, small designs RTLLM [23] 2024 General HDL No RTL, bus protocols, DSP VHDL-Eval [37] 2024 General HDL No VHDL logic, sequential circuits GenBen [38] 2024 General HDL No Application-driven tasks ResBench (This paper) 2025 FPGA Resource Usage Optimizations 56 problems across 12 domains that at least one of the top-k generated solutions passes all test cases. Different from the evaluation of software code generation, our framework uses the resource usage count as a key metric to quantify the quality of resource-oriented optimization. The framework also HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao Figure 2: Overview of the software workflow. The process begins with Verilog generation using an LLM, followed by functional verification through testbenches. Functionally correct designs undergo FPGA synthesis to extract resource usage metrics, and the framework compiles performance reports comparing functional correctness and resource usage. uses an automated benchmarking system that automates Verilog code generation, functional correctness testing, FPGA synthesis, and resource usage extraction. The open-source software for ResBench automates the evalu- ation of LLM-generated Verilog, systematically measuring both functional correctness and FPGA resource usage with minimal manual intervention. An overview of the software s workflow is shown in Fig. 2. The software accepts a user-specified LLM and generates Ver- ilog solutions based on structured problem definitions. It produces detailed evaluation reports, indicating whether each design passes synthesis and functional correctness checks, along with a resource utilization summary. By automating the full evaluation pipeline, the framework facilitates large-scale benchmarking and comparative studies of different LLMs for HDL code generation. To maintain consistency, each problem follows a structured for- mat consisting of three components: a natural language problem description in plain English, a Verilog module header, and a pre- defined testbench. The problem description specifies the expected input-output format and functional constraints, ensuring that LLM- generated code aligns with real-world design requirements. The module header provides a consistent Verilog interface with de- fined input and output signals but leaves the internal logic for the LLM to fill. The testbench validates functional correctness through simulation by applying predefined test cases in the testbench and comparing outputs against a manually verified reference solution. The evaluation framework follows a structured process to evalu- ate LLM-generated designs. The evaluation of an LLM on a bench- mark problem consists of the following three steps: (1) The framework queries the selected LLM to generate multi- ple Verilog code snippets for a given problem. These gener- ated snippets are stored in text format along with references to their corresponding problem descriptions. Functional cor- rectness is then verified using predefined testbenches. De- signs that pass all test cases proceed to FPGA synthesis, while those that fail have their errors recorded for further analysis. (2) FPGA synthesis is performed to determine resource usage metrics such as LUT count, DSP utilization, and register count. For designs that fail synthesis, the resource count is set to , ensuring a consistent comparison framework. (3) The framework generates structured reports summarizing pass rates, synthesis success rates, and resource usage statis- tics. Users can visualize model performance through auto- matically generated comparisons of functional correctness and resource usage. By following this structured evaluation process, the framework provides a fully automated benchmarking solution that evaluates LLM-generated Verilog across all benchmark problems, focusing on both design correctness and resource usage. ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan 5 Evaluation This section presents our evaluation of LLM-generated FPGA de- signs using ResBench. The evaluation focuses not only on functional correctness but also on FPGA resource usage. 5.1 Experimental Setup and Metrics Our experiments evaluate the capability of LLMs to generate both funtionally correct and resource-efficient Verilog code by examining the number of functionally correct designs. Also, we examine how well different models optimize FPGA resources. We run the software proposed in Section 4 with all benchmark problems. For each LLM-generated design, we compile the test- bench and simulate it to verify the functional correctness of the generated Verilog modules. If the simulation confirms that the de- sign is correct, we use the Vivado synthesis tool to generate a resource report and assess resource usage. To evaluate functional correctness, we generate the same number of designs from each LLM and count how many designs pass for each problem. In cases where two LLMs produce the same number of passing designs, we break the tie by considering the number of designs that pass synthesis but fail the functional correctness test. We do not use the metric, which is commonly applied to LLM-generated software code, because we intend to distinguish designs that fail hardware synthesis from those that successfully synthesize but do not meet functional correctness requirements. In this study, we quantify the capability of resource optimization by minimizing the LUT count. LUTs serve as the primary logic resource for implementing combinational operations and small memory elements. While FPGAs also provide other resources, these tend to be application-specific. For instance, DSPs and BRAMs are crucial for arithmetic-intensive and memory-heavy designs but are not universally required across all FPGA applications. In contrast, LUTs are a fundamental component in nearly every design, making them a consistent and reliable metric for evaluating different HDL implementations. Each LLM-generated design 洧녬洧녰is evaluated based on its LUT usage. If a design successfully passes both synthesis and functional correctness testing, its LUT count is recorded as LUT(洧녬洧녰). Oth- erwise, it is assigned to indicate that the design is either non- synthesizable or functionally incorrect: LUT(洧녬洧녰) ( LUT count, if 洧녬洧녰is synthesizable and correct , otherwise (1) LUTmin min (LUT(洧녬0), LUT(洧녬1), . . . , LUT(洧녬洧녵 1)) . (2) By using for failed designs, our approach naturally excludes non-functional implementations. This setting maintains computa- tional consistency and eliminates the need for explicit filtering in the evaluation of Equation 2. Note that while this study focuses on minimizing LUT usage, our framework is capable of extracting and analyzing other resource metrics with a different optimization objective. For correctness testing and resource usage evaluation, we use Vivado 2023.1 for simulation, synthesis, and analysis. The hardware implementation is targeted at the programmable logic section of the AMD Zynq 7000 XC7Z020CLG400-1 SoC, operating at its default clock frequency. While design correctness remains independent of the chosen tool, the LUT count is influenced by Vivado s synthesis capabilities and the type of LUTs on the target device. However, we expect the impact of FPGA software choice on relative resource efficiency to be small. In particular, for a given problem, the HDL designs with the smallest LUT count will likely stay unchanged even when evaluated with different FPGA tools. For all the evaluated LLMs, we set the temperature parameter to 1.5 to encourage high diversity of the HDL code for each problem. The evaluation includes three types of models: general-purpose LLMs, code-specialized LLMs, and HDL-specialized LLMs. The general-purpose models we evaluate include GPT-3.5 [44], GPT-4o [13], GPT-4 [2], GPT-o1-mini [14], Llama3.1-450B [34], Qwen-Max [3], and Qwen-Plus [42]. The evaluated code-specialized models include Qwen2.5-Coder-32B-Instruct [12] and Codestral [15]. We also evaluate VeriGen [33], an HDL-specialized model. However, during the evaluation, VeriGen failed to generate legitimate Verilog code for all problems. As a result, we omit its results from further discussion. 5.2 Functional Correctness Table 4 provides detailed pass counts across 12 categories of prob- lems, with 15 designs generated for each problem. Each table cell follows the format: pass synthesis OK but incorrect design synthesis error. For example, if a category contains 5 problems, each LLM generates a total of 75 solutions (5 problems 15 gen- erated designs per problem), and the sum of the three numbers in each cell corresponds to this total. In this table, we also include the number of wins, which represents the number of categories in which each LLM achieved the highest pass count. The results show that GPT-o1-mini is the leading model, achiev- ing the highest pass counts in most categories. This suggests that reasoning-optimized models have an advantage in Verilog code generation. This is potentially because its reasoning capabilities contribute to more accurate outputs. Table 4 shows a notable observation in finite state machines, mathematical functions, pipelining. The generated code can of- ten pass synthesis but fail to function correctly. This observation suggests that while LLMs grasp basic syntax, they struggle with complex functional logic. In contrast, for more intricate problems with complex contexts, such as mathematical functions and finan- cial computing, LLMs tend to produce syntactically incorrect code, reflecting challenges in understanding and reasoning within these contexts. The results show that for every problem there is at least one model providing correct solutions. However, in categories such as pipelining, financial computing, and encryption, LLMs tend to un- derperform and show higher variability. For example, GPT-3.5 pro- duced no passing solutions in pipelining, but LLaMA 3.1 achieved good results. A similar pattern is observed in the mathematical functions category. These results highlight the importance of eval- uating both functional correctness and resource optimization in complex design scenarios. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao Table 4: Design correctness of LLMs in generating Verilog code across different categories Cell format: pass synthesis OK but incorrect design synthesis error GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B Combinational Logic 112 5 3 117 3 0 120 0 0 118 1 1 115 2 3 117 2 1 109 1 10 112 2 6 120 0 0 Finite State Machines 23 15 22 32 22 6 31 24 5 39 18 3 31 24 5 34 26 0 27 23 10 39 10 11 36 6 18 Mathematical Functions 13 19 43 6 39 30 36 10 29 46 24 5 7 6 62 26 27 22 20 26 29 5 8 62 0 3 72 Basic Arithmetic Ops 37 2 36 63 8 4 66 9 0 68 4 3 43 2 30 38 22 15 27 13 35 54 6 15 62 13 0 Bitwise Logic Ops 35 0 25 55 0 5 58 2 0 59 0 1 52 0 8 47 0 13 33 11 16 36 0 24 55 0 5 Pipelining 0 59 16 11 54 10 26 49 0 15 38 22 7 38 30 15 32 28 16 26 33 21 31 23 6 56 13 Polynomial Evaluation 19 3 53 69 0 6 74 1 0 68 5 2 58 6 11 55 2 18 28 5 42 65 7 3 69 6 0 Machine Learning 31 3 41 60 8 7 60 13 2 73 1 1 45 28 2 63 12 0 61 12 2 57 2 16 64 8 3 Financial Computing 9 23 28 21 22 17 29 13 18 20 20 20 11 21 28 28 15 17 15 12 33 16 7 37 17 23 20 Encryption 30 0 15 30 2 13 25 20 0 30 0 15 26 0 19 25 9 11 30 1 14 30 0 15 30 0 15 Physics 45 3 12 57 0 3 53 4 3 54 5 1 41 11 8 49 7 4 40 17 3 38 15 7 55 2 3 Climate 8 15 37 21 30 9 41 11 8 41 15 4 24 23 13 38 19 3 19 31 10 32 14 14 28 19 13 Number of wins 0 2 4 5 0 0 0 1 1 5.3 Resource Usage Table 5 presents the LUT usage results for the benchmark problems. In this table, problems where all LLMs yield identical LUT usage are excluded for brevity. The cell with the lowest resource usage in each category is highlighted in bold. The number of wins is determined by counting these highlighted cells. The results suggest that different LLMs have varying levels of optimization capability within our benchmark framework. This highlights how the benchmark problems reveal differences in LLMs ability to optimize resource usage. Moreover, we have the following observations based on the results: (1) GPT-o1-mini leads with 19 wins, significantly outperform- ing the runner-up, GPT-4, which achieves 12 wins. This indicates that GPT-o1-mini generates Verilog designs with lower resource usage in most problems. Its strong perfor- mance suggests that advanced reasoning capabilities may enhance its understanding of problem requirements, the Ver- ilog language, and the complexities of hardware design. In contrast, code-specialized LLMs, while demonstrating high accuracy in producing functionally correct Verilog, may lack the reasoning depth needed to optimize designs effectively for FPGA constraints. This difference highlights that gen- erating syntactically correct HDL alone is insufficient for producing resource-efficient hardware, as true optimization demands a deeper understanding of both design require- ments and FPGA-specific constraints. (2) GPT-3.5-Turbo, Qwen2.5-Coder, and Codestral demonstrate the weakest resource optimization ability, achieving only 7, 7, and 8 wins, respectively. The poor resource optimiza- tion of GPT-3.5-turbo is potentially due to its model size and lack of updates. Qwen2.5-Coder and Codestral, the two code-specialized models in our evaluation, also struggle with resource optimization. One possible explanation is that these models are primarily trained on software code rather than HDL, which may limit their ability to account for FPGA re- source constraints when generating and optimizing Verilog. Additionally, key optimization techniques, such as mathemat- ical simplifications, are unlikely to be picked up effectively from software code data. (3) The observed variations in resource usage across different problem types confirm that our benchmarks can lead to diver- gent hardware resource usage for Verilog designs generated by different LLMs. For simple tasks such as combinational logic and basic arithmetic operations, the differences tend to be less significant. This is likely because the training data of the models include well-established reference solution for these problems. However, for more complex problems, our benchmark problems lead to significantly greater divergence in LUT usage. This suggests that our benchmark problems effectively evaluate the ability of LLMs to optimize resource usage beyond learned patterns. Considering both functional correctness and resource usage, we find that GPT-o1-mini achieves the highest performance in both aspects, while code-specialized models, including Qwen2.5-Coder and Codestral, perform the worst. 6 Conclusion and Future Work LLMs provide a promising solution for automating HDL generation. However, most current benchmarks focus mainly on functional correctness while overlooking FPGA resource constraints. This lack of attention on FPGA resource efficiency underscores the need for resource-aware benchmarks to better evaluate LLM-generated HDL for real-world FPGA deployment. Additionally, current bench- marks lack problem diversity, limiting their effectiveness in evalu- ating real-world FPGA applications. To address these limitations, we introduce ResBench, the first resource-centric benchmark for LLM-generated HDL. ResBench features 56 problems spanning 12 categories. The benchmark problems are designed to expose LLMs ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 5: LUTmin for each LLM across categories GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B fsm 3state 1 0 0 0 0 0 0 0 0 traffic light 1 1 2 0 0 2 3 2 elevator controller 3 3 2 2 2 2 2 2 2 vending machine 1 1 2 1 2 1 1 2 1 int sqrt 68 177 64 229 173 fibonacci 56 1 56 56 56 mod exp 4466 4669 1911 1678 power 79 93 93 93 93 93 log2 int 10 20 12 abs diff 12 12 14 12 12 12 12 12 modulo op 82 82 82 82 111 left shift 10 10 10 10 10 12 12 10 10 pipelined adder 0 16 0 0 15 pipelined multiplier 77 70 56 70 pipelined max finder 0 24 0 24 24 24 24 24 洧논3 3洧논2 3洧논 1 49 49 0 91 0 91 0 91 49 (洧논 2)2 (洧논 2)2 (洧논 2)2 64 33 96 11 108 108 26 18 33 (洧녩 洧녪)2 (洧녩 洧녪)2 0 213 59 16 213 16 16 16 relu 8 8 8 8 8 16 8 8 16 mse loss 216 64 64 216 64 216 64 64 compound interest 13060 10135 10135 52950 9247 10135 52950 currency converter 0 0 25 0 free fall distance 6 6 64 6 6 64 67 64 6 kinetic energy 70 70 54 54 54 54 54 54 54 potential energy 6 6 84 0 6 6 6 6 6 carbon footprint 174 121 110 92 121 121 110 110 110 heat index 16 16 201 16 195 16 124 201 201 air quality index 128 104 104 116 128 128 Number of wins 7 12 10 19 11 10 9 7 8 ability to generate Verilog designs optimized for FPGA resource usage. While ResBench is not explicitly designed to emphasize combi- national logic and arithmetic operations, the current problem set naturally includes a high proportion of such designs. Future work will expand the benchmark to include more sequential designs, such as pipelined architectures and state-driven circuits. Additionally, although the current evaluation focuses on Verilog, our framework is designed to support multiple HDLs. Future efforts will extend support to VHDL and high-level synthesis (HLS) tools. Acknowledgments We thank the anonymous reviewers for their valuable feedback and suggestions. The support of the United Kingdom EPSRC (grant number UKRI256, EP V028251 1, EP N031768 1, EP S030069 1, and EP X036006 1), Intel, and AMD is gratefully acknowledged. AI Usage Statement: This work involves the use of generative AI in multiple aspects. The methodology presented in this paper focuses on evaluating the ability of AI models to generate HDL code. As such, all experimental results are based on Verilog designs produced by LLMs. For writing, ChatGPT-4 and Llama 3.1 were used to refine phrasing, improve clarity, and proofread the text. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao References [1] Abdelatty, M., Ma, J., and Reda, S. MetRex: A benchmark for Verilog code metric reasoning using llms. arXiv preprint arXiv:2411.03471 (2024). [2] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [3] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023). [4] Batten, C., Pinckney, N., Liu, M., Ren, H., and Khailany, B. PyHDL-Eval: An LLM evaluation framework for hardware design using python-embedded dsls. In Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD (2024), pp. 1 17. [5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877 1901. [6] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [7] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research 24, 240 (2023), 1 113. [8] Devlin, J. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [9] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, W.-t., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999 (2022). [10] Gao, M., Zhao, J., Lin, Z., Ding, W., Hou, X., Feng, Y., Li, C., and Guo, M. AutoVCoder: A systematic framework for automated Verilog code generation using llms. arXiv preprint arXiv:2407.18333 (2024). [11] Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 (2021). [12] Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186 (2024). [13] Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. GPT-4o system card. arXiv preprint arXiv:2410.21276 (2024). [14] Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. OpenAI O1 system card. arXiv preprint arXiv:2412.16720 (2024). [15] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [16] Jiang, J., Wang, F., Shen, J., Kim, S., and Kim, S. A survey on large language models for code generation. arXiv preprint arXiv:2406.00515 (2024). [17] Kashanaki, F. R., Zakharov, M., and Renau, J. HDLEval benchmarking LLMs for multiple HDLs. In 2024 IEEE LLM Aided Design Workshop (LAD) (2024), IEEE, pp. 1 5. [18] Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T. DS-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning (2023), PMLR, pp. 18319 18345. [19] Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. StarCoder: May the source be with you! arXiv preprint arXiv:2305.06161 (2023). [20] Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by ChatGPT really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems 36 (2024). [21] Liu, M., Pinckney, N., Khailany, B., and Ren, H. Verilogeval: Evaluating large language models for Verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD) (2023), IEEE, pp. 1 8. [22] Liu, S., Lu, Y., Fang, W., Li, M., and Xie, Z. OpenLLM-RTL: Open dataset and benchmark for llm-aided design RTL generation. In IEEE ACM International Conference on Computer-Aided Design (ICCAD) (2024). [23] Lu, Y., Liu, S., Zhang, Q., and Xie, Z. RTLLM: An open-source benchmark for design RTL generation with large language models. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC) (2024), IEEE, pp. 722 727. [24] Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., Von Werra, L., and Longpre, S. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124 (2023). [25] Nadimi, B., and Zheng, H. A multi-expert large language model architecture for Verilog code generation. arXiv preprint arXiv:2404.08029 (2024). [26] Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. CodeGen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022). [27] Qiu, R., Zhang, G. L., Drechsler, R., Schlichtmann, U., and Li, B. Autobench: Automatic testbench generation and evaluation using LLMs for HDL design. In Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD (2024), pp. 1 10. [28] Radford, A. Improving language understanding by generative pre-training, 2018. [29] Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., and Ma, S. CodeBLEU: A method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020). [30] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., et al. Code Llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023). [31] Tan, A. HDLBits: Digital circuits exercises, 2017. Accessed: 2025. [32] Thakur, S., Ahmad, B., Fan, Z., Pearce, H., Tan, B., Karri, R., Dolan-Gavitt, B., and Garg, S. Benchmarking large language models for automated Verilog RTL code generation. In 2023 Design, Automation Test in Europe Conference Exhibition (DATE) (2023), IEEE, pp. 1 6. [33] Thakur, S., Ahmad, B., Pearce, H., Tan, B., Dolan-Gavitt, B., Karri, R., and Garg, S. VeriGen: A large language model for Verilog code generation. ACM Transactions on Design Automation of Electronic Systems 29, 3 (2024), 1 31. [34] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi칟re, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [35] Uniyal, M., Singh, M., Verbruggen, G., Gulwani, S., and Le, V. One-to-many testing for code generation from (just) natural language. In Findings of the Association for Computational Linguistics: EMNLP 2024 (2024), pp. 15397 15402. [36] Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems (2017). [37] Vijayaraghavan, P., Shi, L., Ambrogio, S., Mackin, C., Nitsure, A., Beymer, D., and Degan, E. VHDL-Eval: A framework for evaluating large language models in VHDL code generation. arXiv preprint arXiv:2406.04379 (2024). [38] Wan, G.-W., yubo, W., Wong, S., jingyi zhang, Xing, M., jiang, Z., Guan, N., ying wang, Xu, N., Xu, Q., and Wang, X. Genben:a genarative benchmark for LLM-aided design, 2025. [39] Wang, J., and Chen, Y. A review on code generation with LLMs: Application and evaluation. In 2023 IEEE International Conference on Medical Artificial Intelligence (MedAI) (2023), IEEE, pp. 284 289. [40] Wang, N., Yao, B., Zhou, J., Wang, X., Jiang, Z., and Guan, N. Large lan- guage model for Verilog generation with golden code feedback. arXiv preprint arXiv:2407.18271 (2024). [41] Wong, S.-Z., Wan, G.-W., Liu, D., and Wang, X. VGV: Verilog generation using visual capabilities of multi-modal large language models. In 2024 IEEE LLM Aided Design Workshop (LAD) (2024), IEEE, pp. 1 5. [42] Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115 (2024). [43] Yang, Y., Teng, F., Liu, P., Qi, M., Lv, C., Li, J., Zhang, X., and He, Z. Haven: Hallucination-mitigated llm for Verilog code generation aligned with HDL engi- neers. arXiv preprint arXiv:2501.04908 (2025). [44] Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou, Z., Gong, C., Shen, Y., et al. A comprehensive capability analysis of GPT-3 and GPT-3.5 series models. arXiv preprint arXiv:2303.10420 (2023). [45] Zheng, Z., Ning, K., Wang, Y., Zhang, J., Zheng, D., Ye, M., and Chen, J. A survey of large language models for code: Evolution, benchmarking, and future trends. arXiv preprint arXiv:2311.10372 (2023).\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness Ce Guo Imperial College London United Kingdom Tong Zhao Imperial College London United Kingdom Abstract Field-Programmable Gate Arrays (FPGAs) are widely used in mod- ern hardware design, yet writing Hardware Description Language (HDL) code for FPGA implementation remains a complex and time- consuming task. Large Language Models (LLMs) have emerged as a promising tool for HDL generation, but existing benchmarks for LLM-based code generation primarily focus on functional correct- ness while overlooking hardware resource usage. Furthermore, cur- rent benchmarks offer limited diversity and do not fully represent the wide range of real-world FPGA applications. To address these shortcomings, we introduce ResBench, the first resource-focused benchmark explicitly designed to distinguish between resource- optimized and inefficient LLM-generated HDL code. ResBench con- sists of 56 problems across 12 categories, covering applications from finite state machines to financial computing. Our open-source evaluation framework automatically tests LLMs by generating Ver- ilog code, verifying correctness, and measuring resource usage. The experiments, which primarily analyze Lookup Table (LUT) usage, re- veal significant differences among LLMs, demonstrating ResBench s capability to identify models that generate more resource-optimized FPGA designs. CCS Concepts Hardware Board- and system-level test; Reconfigurable logic applications; Functional verification; Physical verification; Software and its engineering Source code generation. Keywords Large Language Models (LLMs), Hardware Description Languages (HDLs), Verilog Code Generation, FPGA Resource Utilization, Au- tomated Benchmarking, Empirical Evaluation of LLMs ACM Reference Format: Ce Guo and Tong Zhao. 2025. ResBench: Benchmarking LLM-Generated FPGA Designs with Resource Awareness. In Proceedings of International Symposium on Highly Efficient Accelerators and Reconfigurable Technologies (HEART 25). ACM, New York, NY, USA, 10 pages. XXXXXXX Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\n\n--- Segment 2 ---\nXXXXXXX Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from HEART 25, Kumamoto, Japan 2025 Copyright held by the owner author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X 2018 06 1 Introduction Field-Programmable Gate Arrays (FPGAs) are widely used in re- configurable computing, providing flexible and high-performance hardware implementations for applications such as artificial intel- ligence acceleration, financial computing, and embedded systems. However, FPGA development traditionally requires manual cod- ing in hardware description languages (HDLs), a process that is both time-consuming and prone to errors. Large Language Models (LLMs) have recently shown potential in automating HDL genera- tion, offering a way to improve FPGA design productivity. While studies such as VeriGen [32] and RTLLM [23] have exam- ined the feasibility of generating Verilog with LLMs, most existing benchmarks focus primarily on functional correctness while pay- ing little attention to resource constraints, which is an essential consideration in FPGA design. FPGA designs are subject to strict hardware resource constraints. Even when HDL code passes functional correctness tests, its effi- ciency in utilizing these resources can vary significantly depending on how logic is optimized and mapped to FPGA hardware. In recon- figurable computing, resource-aware optimizations play a crucial role in determining whether a design can be practically deployed and the degree of parallelism achievable. However, existing bench- marks for LLM-generated hardware designs typically evaluate the generated code based only on syntax and functional correctness, overlooking resource-related issues. To address this limitation, we introduce ResBench, the first FPGA-resource-focused benchmark specifically designed to eval- uate LLM-generated designs based on resource usage.\n\n--- Segment 3 ---\nHowever, existing bench- marks for LLM-generated hardware designs typically evaluate the generated code based only on syntax and functional correctness, overlooking resource-related issues. To address this limitation, we introduce ResBench, the first FPGA-resource-focused benchmark specifically designed to eval- uate LLM-generated designs based on resource usage. Unlike pre- vious benchmarks that focus primarily on syntax and functional correctness, ResBench highlights how well LLMs generate Verilog code optimized for FPGA resource utilization. Our key contribu- tions are: A resource-focused benchmark featuring 56 problems across 12 categories, covering real-world FPGA workloads such as combinational logic, state machines, AI accelerators, and financial computing applications. (Section 3) An open-source automated evaluation framework that per- forms LLM querying, functional correctness testing, FPGA synthesis, and resource measurement1. The framework auto- matically generates Verilog code using LLMs and evaluates its correctness and resource usage. (Section 4) A detailed study of nine LLMs, comparing their performance in functional correctness and FPGA resource usage. The results reveal substantial differences in how various models generate resource-conscious designs. (Section 5) 1Code repository: arXiv:2503.08823v2 [cs.AR] 21 Mar 2025 HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao By integrating FPGA resource awareness into benchmarking, ResBench provides a practical evaluation of LLM-generated HDL. This benchmark establishes a foundation for advancing AI-driven FPGA design, encouraging the development of more resource- efficient models optimized for FPGAs. 2 Background This section explores the evolution of large language models (LLMs) from general code generation to hardware design generation using hardware description languages (HDLs). We examine their capa- bilities and limitations, as well as existing benchmarks for LLM- generated hardware design. 2.1 Code-specialized and HDL-specialized LLMs Language models have seen significant advancements, particularly with the introduction of Transformers [36]. Large-scale pre-trained models such as BERT [8], the GPT series [5, 28, 44], and PaLM 2 [7] have expanded their capabilities across various tasks, including code generation.\n\n--- Segment 4 ---\n2.1 Code-specialized and HDL-specialized LLMs Language models have seen significant advancements, particularly with the introduction of Transformers [36]. Large-scale pre-trained models such as BERT [8], the GPT series [5, 28, 44], and PaLM 2 [7] have expanded their capabilities across various tasks, including code generation. Code-specialized LLMs are models designed to generate com- puter programs based on human-language prompts. Surveys such as [16, 39, 45] review the latest techniques and model architectures developed for this purpose. In general, a code-specialized LLM can be created using two different approaches. The first approach trains models from scratch on large-scale open-source code datasets spanning multiple programming languages, as demonstrated by CodeGen [26], InCoder [9], and StarCoder [19]. These code-first LLMs excel at code completion and multi-language generation. However, this method demands extensive computational resources and high-quality datasets. Additionally, these models may strug- gle with instruction-driven tasks due to their limited ability to process human language instructions. The second approach fine-tunes general-purpose LLMs for coding tasks, as seen in Codex [6], which is derived from GPT-3, and Code Llama [30], which builds on Meta s Llama. Fine-tuning incorporates the linguistic knowledge of general LLMs, allowing them to handle natural language prompts while maintaining strong coding capabilities. This method is more computationally efficient than training from scratch, but it may lack the precision of code-first models in under- standing programming language syntax. While significant research has explored the use of language mod- els for general software code generation, the application of LLMs to HDL code has not received comparable attention. Existing work on HDL code generation primarily focuses on improving general- purpose or software-code-specialized LLMs. In particular, these studies aim to improve these LLMs understanding of hardware description tasks by training them on HDL datasets and bench- marking frameworks. Notable efforts in this area include VeriGen, MEV-LLM, and AutoVCoder. VeriGen [33] fine-tunes CodeGen (2B, 6B, 16B) on a Verilog dataset collected from GitHub repositories and textbooks.\n\n--- Segment 5 ---\nNotable efforts in this area include VeriGen, MEV-LLM, and AutoVCoder. VeriGen [33] fine-tunes CodeGen (2B, 6B, 16B) on a Verilog dataset collected from GitHub repositories and textbooks. It employs supervised fine-tuning with problem-code pairs, validating functional correctness using a benchmark problem set and problems from HDLBits tutorials [31]. MEV-LLM [25] trains on 31,104 source code files from GitHub, labeled by GPT-3.5, to fine-tune CodeGen (2B, 6B, 16B) and GEMMA (2B, 7B) models. This approach yields an improve- ment of up to 23.9 in the metric [6] over VeriGen. AutoVCoder [10] also uses Verilog code from GitHub, fil- tering high-quality samples with ChatGPT-3.5. It applies a two-stage fine-tuning process to improve generalization, with final evaluation conducted on a real-world benchmark. Beyond direct fine-tuning, reinforcement learning approaches like Golden Code Feedback is used to refine models iteratively using user feedback [40]. Similarly, multi-modal techniques such as VGV [41] integrate circuit diagrams with textual data during training, allowing models to understand spatial and parallel aspects of circuit design. 2.2 Benchmarks for LLM-Generated Software The research community has recognized the need for standardized benchmarks to rigorously evaluate LLM-generated code in terms of design correctness. Most existing benchmarks for code generation are tailored for software development rather than hardware design. For instance, HumanEval [6] and MBPP (Mostly Basic Python Problems) [20] are widely used to evaluate LLMs for software code generation. HumanEval consists of 164 Python programming tasks, each with a function signature and a set of test cases to validate correct- ness. While relatively small in scope, this benchmark is carefully designed, making it well-suited for quick functional correctness evaluations of Python-based code generation. Several extensions have expanded its coverage, including HumanEval [20], which increases the number of test cases by 80 times, and HumanEvalPack [24], which extends the benchmark to six programming languages.\n\n--- Segment 6 ---\nWhile relatively small in scope, this benchmark is carefully designed, making it well-suited for quick functional correctness evaluations of Python-based code generation. Several extensions have expanded its coverage, including HumanEval [20], which increases the number of test cases by 80 times, and HumanEvalPack [24], which extends the benchmark to six programming languages. Similarly, MBPP comprises approximately 974 short Python pro- gramming tasks, each including a task description prompt, a code solution, and three automated test cases. It emphasizes both correct- ness and clarity. Its enhanced version, MBPP [20], refines flawed implementations and expands the number of test cases to improve robustness. While HumanEval and MBPP provide fundamental benchmarks, they primarily focus on entry-level programming tasks and do not always reflect the complexity of real-world software develop- ment. To address this limitation, benchmarks with more intricate problem sets have been introduced. For example, DSP-1000 [18] con- tains 1,000 science-related programming tasks from seven Python libraries, covering a diverse set of topics and incorporating multi- criteria evaluation metrics to provide a more realistic assessment of code generation models. 2.3 Benchmarks for LLM-Generated Hardware Existing benchmarks for hardware design generation in HDL are often based on experiences and lessons learned in software code benchmarking, particularly in designing diverse problem sets and developing automated evaluation frameworks. Similar to their soft- ware counterparts, performance metrics for hardware design gen- eration frequently focus on design correctness measures such as However, benchmarking hardware designs typically in- volves simulation-based hardware verification [27]. ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 1: Benchmarks for LLM HDL Generation Benchmark Size PL Type Features VerilogEval [21] 156 Verilog Verilog code generation tasks Covers a wide range of tasks from simple combinational cir- cuits to finite state machines; includes automatic functional correctness testing. HDLEval [17] 100 Multiple Language-agnostic HDL evaluation Evaluates LLMs across multiple HDLs using standardized test- benches and formal verification; categorizes problems into combinational and pipelined tests.\n\n--- Segment 7 ---\nResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 1: Benchmarks for LLM HDL Generation Benchmark Size PL Type Features VerilogEval [21] 156 Verilog Verilog code generation tasks Covers a wide range of tasks from simple combinational cir- cuits to finite state machines; includes automatic functional correctness testing. HDLEval [17] 100 Multiple Language-agnostic HDL evaluation Evaluates LLMs across multiple HDLs using standardized test- benches and formal verification; categorizes problems into combinational and pipelined tests. PyHDL-Eval [4] 168 Python-embedded DSLs Specification-to-RTL tasks Focuses on Python-embedded DSLs for hardware design; in- cludes Verilog reference solutions and testbenches; evaluates LLMs ability to handle specification-to-RTL translations. RTLLM [22] 50 Verilog, VHDL, Chisel Design RTL generation Supports evaluation across multiple HDL formats; spans var- ious design complexities and scales; includes an automated evaluation framework. VHDL-Eval [37] 202 VHDL VHDL code generation tasks Aggregates translated Verilog problems and publicly available VHDL problems; utilizes self-verifying testbenches for func- tional correctness validation. GenBen [38] 351 Verilog Fundamental hardware de- sign and debugging tasks Evaluates synthesizability, power consumption, area utiliza- tion, and timing performance to ensure real-world applicabil- ity. Unlike software benchmarks, which evaluate correctness by di- rectly executing code, hardware designs require dedicated testbench scripts to simulate hardware behavior and validate functionality. A typical evaluation process involves querying the LLM for HDL code, executing the generated HDL within a hardware simulation environment, and comparing the outputs to determine correctness. Some benchmarks have emerged to address these challenges, as presented in Table 1. Among them, VerilogEval, HDLEval, and PyHDL-Eval are widely recognized. VerilogEval [21] is a widely adopted benchmark for evaluat- ing LLMs in Verilog code generation [1, 10, 43].\n\n--- Segment 8 ---\nAmong them, VerilogEval, HDLEval, and PyHDL-Eval are widely recognized. VerilogEval [21] is a widely adopted benchmark for evaluat- ing LLMs in Verilog code generation [1, 10, 43]. It consists of 156 problems taken from the HDLBits tutorial website [31], covering a range of Verilog tasks from combinational circuits to finite state machines. The framework automates functional correctness testing by comparing the simulation outputs of generated designs against predefined golden so- lutions. HDLEval [17] follows a language-agnostic approach. In par- ticular, this benchmark allows the same set of problems, formulated in plain English, to be evaluated across different HDLs. The benchmark consists of 100 problems systemati- cally categorized into combinational and pipelined designs, covering fundamental hardware components such as logic gates, arithmetic operations, and pipelined processing units. A prominent feature of this benchmark is the use of formal verification instead of unit tests. This feature ensures that the generated HDL code is functionally correct and maintains logical equivalence with reference implementations. PyHDL-Eval [4] is a framework for evaluating LLMs on specification-to-Register Transfer Level (RTL) tasks within Python-embedded domain-specific languages (DSLs). It in- cludes 168 problems across 19 subclasses, covering combi- national logic and sequential logic. The evaluation process involves executing the generated code in Python-embedded HDLs (e.g., PyMTL3, PyRTL) and measuring functional cor- rectness based on pass rates. 2.4 Challenges in LLM Benchmarking for FPGA Design Despite advancements in LLM-driven Verilog generation, existing models primarily focus on producing syntactically and functionally correct HDL but fail to address critical hardware constraints essen- tial for FPGA deployment, such as resource efficiency, timing con- straints, and power consumption. Unlike ASIC design, FPGA-based development demands careful consideration of resource usage, in- cluding lookup tables (LUTs), flip-flops (FFs), block RAM (BRAM), and digital signal processing (DSP) blocks.\n\n--- Segment 9 ---\n2.4 Challenges in LLM Benchmarking for FPGA Design Despite advancements in LLM-driven Verilog generation, existing models primarily focus on producing syntactically and functionally correct HDL but fail to address critical hardware constraints essen- tial for FPGA deployment, such as resource efficiency, timing con- straints, and power consumption. Unlike ASIC design, FPGA-based development demands careful consideration of resource usage, in- cluding lookup tables (LUTs), flip-flops (FFs), block RAM (BRAM), and digital signal processing (DSP) blocks. However, current LLMs for Verilog generation lack an understanding of FPGA-specific re- quirements, often producing designs that are functionally correct but inefficient and impractical for real-world FPGA deployment. The inability to evaluate and optimize LLM-generated Verilog for FPGA resource constraints highlights the need for advancing resource-aware Verilog generation and motivates this study. To establish LLMs as a practical solution for HDL automation, it is essential to equip them with a deeper understanding of FPGA de- sign constraints. Achieving this requires developing new training datasets, designing robust evaluation frameworks, and refining LLM training strategies to enhance their capability in hardware-aware code generation. This paper focuses on constructing a benchmark that provides an evaluation of LLMs performance in generating HDLs that are both functionally correct and optimized for FPGA- specific resource constraints. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao 3 Design of ResBench This section presents the design of ResBench, outlining its guiding principles and structured problem set for evaluating LLM-generated Verilog code. Additionally, we compare ResBench with existing benchmarks for LLM-generated HDL code. 3.1 Design Principles and Benchmark Problems ResBench is designed to evaluate LLM-generated Verilog across a di- verse range of FPGA applications, with a primary focus on resource optimization awareness. The benchmark consists of 56 problems categorized into 12 domains, each representing a key area of FPGA applications. The problems in ResBench are carefully structured to evaluate both functional correctness and resource efficiency. The benchmark covers a wide range of FPGA design tasks, from fundamental digital logic and mathematical computation to more complex, application-driven domains such as machine learning, cryptography, and financial computing.\n\n--- Segment 10 ---\nThe problems in ResBench are carefully structured to evaluate both functional correctness and resource efficiency. The benchmark covers a wide range of FPGA design tasks, from fundamental digital logic and mathematical computation to more complex, application-driven domains such as machine learning, cryptography, and financial computing. By spanning these diverse categories, ResBench ensures that LLMs are tested on both low- level design problems and high-level algorithmic implementations for real-world applications. The design of ResBench is guided by two key principles: The Principle of Resource Usage Differentiation aims to highlight differences in how LLMs optimize FPGA resource usage. The benchmark includes problems that allow multiple resource-aware optimization strategies and require mathe- matical transformations. This approach makes it possible to distinguish between models that generate resource-efficient Verilog and those that do not. The Principle of FPGA Application Diversity recog- nizes the wide range of FPGA applications, particularly in computational acceleration and edge computing. ResBench spans various domains, including financial computing, cli- mate modeling, and signal processing, allowing LLMs to be tested across a broad set of real-world FPGA workloads. Table 2 provides an overview of the problem categories along with representative examples. The problems in the benchmark are designed to align with the two guiding principles. Specifically, ResBench addresses the principles as follows: ResBench addresses resource usage differentiation by in- troducing problems that require optimization techniques beyond simple code-level improvements. Fig. 1 illustrates this with a polynomial evaluation problem from the bench- mark. Algebraic simplification in this case enables a more efficient Verilog implementation using fewer LUTs. LLMs that fail to apply this optimization generate designs with excessive LUT consumption. ResBench addresses FPGA application diversity by cover- ing both foundational and application-driven workloads. Foundational problems include combinational logic, finite state machines, arithmetic operations, pipelining, polyno- mial evaluations, and mathematical functions. Application- driven problems span machine learning, encryption, and financial computing, emphasizing the role of FPGAs in AI acceleration, security, and high-speed data processing. By adhering to these principles, the benchmark evaluates not only an LLM s ability to generate syntactically correct Verilog code but also its capability to produce hardware-efficient designs suited for FPGA deployment.\n\n--- Segment 11 ---\nApplication- driven problems span machine learning, encryption, and financial computing, emphasizing the role of FPGAs in AI acceleration, security, and high-speed data processing. By adhering to these principles, the benchmark evaluates not only an LLM s ability to generate syntactically correct Verilog code but also its capability to produce hardware-efficient designs suited for FPGA deployment. 3.2 Comparison with Existing Benchmarks Table 3 highlights key differences between ResBench and existing HDL benchmarks. The differences are particularly significant in FPGA resource optimization awareness and problem diversity: FPGA resource optimization awareness. Most existing bench- marks, such as VerilogEval, HDLEval, and GenBen, focus primarily on functional correctness and HDL syntax quality but do not explicitly account for FPGA resource usage. Con- sequently, these benchmarks cannot distinguish between functionally correct designs that differ significantly in hard- ware resource utilization. In contrast, ResBench introduces optimization-aware problems specifically designed to expose variations in resource usage. This enables a more practi- cal comparison of LLMs based on their ability to generate resource-efficient designs for FPGAs. Problem diversity. Existing benchmarks primarily focus on fundamental HDL constructs such as basic logic, state ma- chines, and arithmetic operations, with limited diversity in FPGA applications. For example, VerilogEval emphasizes control logic and arithmetic, while HDLEval mainly evalu- ates digital circuits and state machines. In contrast, ResBench encompasses a significantly broader range of FPGA appli- cations, including machine learning, encryption, financial computing, and physics-based modeling. These domains rep- resent real-world FPGA workloads where resource efficiency is crucial for minimizing device cost and maximizing paral- lelism. By incorporating a diverse set of tasks, ResBench pro- vides a more comprehensive evaluation of LLM-generated HDL in practical FPGA design scenarios. 4 Evaluation Framework for ResBench To evaluate LLM-generated designs for FPGA design with Res- Bench, we implement a structured framework that examines both functional correctness and hardware efficiency. We build the software for the evaluation framework based on the lessons learned in testing LLM-based software code generation. The benchmarks for LLM-based software share a common eval- uation framework aimed at assessing whether generated code is both syntactically valid and functionally correct.\n\n--- Segment 12 ---\nWe build the software for the evaluation framework based on the lessons learned in testing LLM-based software code generation. The benchmarks for LLM-based software share a common eval- uation framework aimed at assessing whether generated code is both syntactically valid and functionally correct. In particular, each benchmark generally includes four key components: (1) Prompts, which can be presented as a natural language de- scription [11, 35] or both description and function signature [6, 20], guiding the model on what to generate. (2) A reference solution, which serves as the correct implemen- tation for comparison. (3) Test cases, which are predefined inputs and expected outputs used to validate correctness (4) Performance metrics, like [6] and Code Similar- ity Scores [29], which estimates how effectively an LLM- generated solution satisfies the given problem constraints. A widely used metric is which measures the likelihood ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 2: Summary of benchmark categories, including the number of problems and representative examples.\n\n--- Segment 13 ---\n(3) Test cases, which are predefined inputs and expected outputs used to validate correctness (4) Performance metrics, like [6] and Code Similar- ity Scores [29], which estimates how effectively an LLM- generated solution satisfies the given problem constraints. A widely used metric is which measures the likelihood ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 2: Summary of benchmark categories, including the number of problems and representative examples. Category Problems Example Problems Combinational Logic 8 parity_8bit, mux4to1, bin_to_gray Finite State Machines 4 fsm_3state, traffic_light, elevator_controller Mathematical Functions 5 int_sqrt, fibonacci, mod_exp Basic Arithmetic Operations 5 add_8bit, mult_4bit, abs_diff Bitwise and Logical Operations 4 bitwise_ops, left_shift, rotate_left Pipelining 5 pipelined_adder, pipelined_multiplier, pipelined_fir Polynomial Evaluation 5 (洧논 2)2 (洧논 2)2 (洧논 2)2, (洧녩 洧녪)2 (洧녩 洧녪)2 Machine Learning 5 matrix_vector_mult, relu, mse_loss Financial Computing 4 compound_interest, present_value, currency_converter Encryption 3 caesar_cipher, modular_add_cipher, feistel_cipher Physics 4 free_fall_distance, kinetic_energy, wavelength Climate 4 carbon_footprint, heat_index, air_quality_index Total 56 (a) Design Generated by Qwen-2.5 (213 LUTs) (b) Design Generated by GPT-4 (0 LUT 1 DSP) Figure 1: Benchmark example illustrating HDL optimization capability using the expression (洧녩 洧녪)2 (洧녩 洧녪)2. (a) Qwen-2.5 computes the full expression directly, leading to high LUT usage. (b) GPT-4 simplifies the expression to 4洧녩洧녪, significantly reducing resource usage by using a single DSP unit instead of LUTs.\n\n--- Segment 14 ---\n(a) Qwen-2.5 computes the full expression directly, leading to high LUT usage. (b) GPT-4 simplifies the expression to 4洧녩洧녪, significantly reducing resource usage by using a single DSP unit instead of LUTs. This example demonstrates ResBench s ability to differentiate LLMs based on resource optimization. Table 3: Comparison of Benchmarks for LLM HDL Evaluation Benchmark Year Hardware Optimization Problem Diversity Awareness VerilogEval [33] 2023 General HDL No Logic, FSMs, arithmetic HDLEval [17] 2024 General HDL No Digital circuits, control logic PyHDL-Eval [4] 2024 General HDL No Python-based HDL, small designs RTLLM [23] 2024 General HDL No RTL, bus protocols, DSP VHDL-Eval [37] 2024 General HDL No VHDL logic, sequential circuits GenBen [38] 2024 General HDL No Application-driven tasks ResBench (This paper) 2025 FPGA Resource Usage Optimizations 56 problems across 12 domains that at least one of the top-k generated solutions passes all test cases. Different from the evaluation of software code generation, our framework uses the resource usage count as a key metric to quantify the quality of resource-oriented optimization. The framework also HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao Figure 2: Overview of the software workflow. The process begins with Verilog generation using an LLM, followed by functional verification through testbenches. Functionally correct designs undergo FPGA synthesis to extract resource usage metrics, and the framework compiles performance reports comparing functional correctness and resource usage. uses an automated benchmarking system that automates Verilog code generation, functional correctness testing, FPGA synthesis, and resource usage extraction. The open-source software for ResBench automates the evalu- ation of LLM-generated Verilog, systematically measuring both functional correctness and FPGA resource usage with minimal manual intervention. An overview of the software s workflow is shown in Fig. 2. The software accepts a user-specified LLM and generates Ver- ilog solutions based on structured problem definitions. It produces detailed evaluation reports, indicating whether each design passes synthesis and functional correctness checks, along with a resource utilization summary.\n\n--- Segment 15 ---\nThe software accepts a user-specified LLM and generates Ver- ilog solutions based on structured problem definitions. It produces detailed evaluation reports, indicating whether each design passes synthesis and functional correctness checks, along with a resource utilization summary. By automating the full evaluation pipeline, the framework facilitates large-scale benchmarking and comparative studies of different LLMs for HDL code generation. To maintain consistency, each problem follows a structured for- mat consisting of three components: a natural language problem description in plain English, a Verilog module header, and a pre- defined testbench. The problem description specifies the expected input-output format and functional constraints, ensuring that LLM- generated code aligns with real-world design requirements. The module header provides a consistent Verilog interface with de- fined input and output signals but leaves the internal logic for the LLM to fill. The testbench validates functional correctness through simulation by applying predefined test cases in the testbench and comparing outputs against a manually verified reference solution. The evaluation framework follows a structured process to evalu- ate LLM-generated designs. The evaluation of an LLM on a bench- mark problem consists of the following three steps: (1) The framework queries the selected LLM to generate multi- ple Verilog code snippets for a given problem. These gener- ated snippets are stored in text format along with references to their corresponding problem descriptions. Functional cor- rectness is then verified using predefined testbenches. De- signs that pass all test cases proceed to FPGA synthesis, while those that fail have their errors recorded for further analysis. (2) FPGA synthesis is performed to determine resource usage metrics such as LUT count, DSP utilization, and register count. For designs that fail synthesis, the resource count is set to , ensuring a consistent comparison framework. (3) The framework generates structured reports summarizing pass rates, synthesis success rates, and resource usage statis- tics. Users can visualize model performance through auto- matically generated comparisons of functional correctness and resource usage. By following this structured evaluation process, the framework provides a fully automated benchmarking solution that evaluates LLM-generated Verilog across all benchmark problems, focusing on both design correctness and resource usage.\n\n--- Segment 16 ---\nUsers can visualize model performance through auto- matically generated comparisons of functional correctness and resource usage. By following this structured evaluation process, the framework provides a fully automated benchmarking solution that evaluates LLM-generated Verilog across all benchmark problems, focusing on both design correctness and resource usage. ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan 5 Evaluation This section presents our evaluation of LLM-generated FPGA de- signs using ResBench. The evaluation focuses not only on functional correctness but also on FPGA resource usage. 5.1 Experimental Setup and Metrics Our experiments evaluate the capability of LLMs to generate both funtionally correct and resource-efficient Verilog code by examining the number of functionally correct designs. Also, we examine how well different models optimize FPGA resources. We run the software proposed in Section 4 with all benchmark problems. For each LLM-generated design, we compile the test- bench and simulate it to verify the functional correctness of the generated Verilog modules. If the simulation confirms that the de- sign is correct, we use the Vivado synthesis tool to generate a resource report and assess resource usage. To evaluate functional correctness, we generate the same number of designs from each LLM and count how many designs pass for each problem. In cases where two LLMs produce the same number of passing designs, we break the tie by considering the number of designs that pass synthesis but fail the functional correctness test. We do not use the metric, which is commonly applied to LLM-generated software code, because we intend to distinguish designs that fail hardware synthesis from those that successfully synthesize but do not meet functional correctness requirements. In this study, we quantify the capability of resource optimization by minimizing the LUT count. LUTs serve as the primary logic resource for implementing combinational operations and small memory elements. While FPGAs also provide other resources, these tend to be application-specific. For instance, DSPs and BRAMs are crucial for arithmetic-intensive and memory-heavy designs but are not universally required across all FPGA applications. In contrast, LUTs are a fundamental component in nearly every design, making them a consistent and reliable metric for evaluating different HDL implementations. Each LLM-generated design 洧녬洧녰is evaluated based on its LUT usage.\n\n--- Segment 17 ---\nIn contrast, LUTs are a fundamental component in nearly every design, making them a consistent and reliable metric for evaluating different HDL implementations. Each LLM-generated design 洧녬洧녰is evaluated based on its LUT usage. If a design successfully passes both synthesis and functional correctness testing, its LUT count is recorded as LUT(洧녬洧녰). Oth- erwise, it is assigned to indicate that the design is either non- synthesizable or functionally incorrect: LUT(洧녬洧녰) ( LUT count, if 洧녬洧녰is synthesizable and correct , otherwise (1) LUTmin min (LUT(洧녬0), LUT(洧녬1), . . . , LUT(洧녬洧녵 1)) . (2) By using for failed designs, our approach naturally excludes non-functional implementations. This setting maintains computa- tional consistency and eliminates the need for explicit filtering in the evaluation of Equation 2. Note that while this study focuses on minimizing LUT usage, our framework is capable of extracting and analyzing other resource metrics with a different optimization objective. For correctness testing and resource usage evaluation, we use Vivado 2023.1 for simulation, synthesis, and analysis. The hardware implementation is targeted at the programmable logic section of the AMD Zynq 7000 XC7Z020CLG400-1 SoC, operating at its default clock frequency. While design correctness remains independent of the chosen tool, the LUT count is influenced by Vivado s synthesis capabilities and the type of LUTs on the target device. However, we expect the impact of FPGA software choice on relative resource efficiency to be small. In particular, for a given problem, the HDL designs with the smallest LUT count will likely stay unchanged even when evaluated with different FPGA tools. For all the evaluated LLMs, we set the temperature parameter to 1.5 to encourage high diversity of the HDL code for each problem. The evaluation includes three types of models: general-purpose LLMs, code-specialized LLMs, and HDL-specialized LLMs.\n\n--- Segment 18 ---\nFor all the evaluated LLMs, we set the temperature parameter to 1.5 to encourage high diversity of the HDL code for each problem. The evaluation includes three types of models: general-purpose LLMs, code-specialized LLMs, and HDL-specialized LLMs. The general-purpose models we evaluate include GPT-3.5 [44], GPT-4o [13], GPT-4 [2], GPT-o1-mini [14], Llama3.1-450B [34], Qwen-Max [3], and Qwen-Plus [42]. The evaluated code-specialized models include Qwen2.5-Coder-32B-Instruct [12] and Codestral [15]. We also evaluate VeriGen [33], an HDL-specialized model. However, during the evaluation, VeriGen failed to generate legitimate Verilog code for all problems. As a result, we omit its results from further discussion. 5.2 Functional Correctness Table 4 provides detailed pass counts across 12 categories of prob- lems, with 15 designs generated for each problem. Each table cell follows the format: pass synthesis OK but incorrect design synthesis error. For example, if a category contains 5 problems, each LLM generates a total of 75 solutions (5 problems 15 gen- erated designs per problem), and the sum of the three numbers in each cell corresponds to this total. In this table, we also include the number of wins, which represents the number of categories in which each LLM achieved the highest pass count. The results show that GPT-o1-mini is the leading model, achiev- ing the highest pass counts in most categories. This suggests that reasoning-optimized models have an advantage in Verilog code generation. This is potentially because its reasoning capabilities contribute to more accurate outputs. Table 4 shows a notable observation in finite state machines, mathematical functions, pipelining. The generated code can of- ten pass synthesis but fail to function correctly. This observation suggests that while LLMs grasp basic syntax, they struggle with complex functional logic. In contrast, for more intricate problems with complex contexts, such as mathematical functions and finan- cial computing, LLMs tend to produce syntactically incorrect code, reflecting challenges in understanding and reasoning within these contexts. The results show that for every problem there is at least one model providing correct solutions.\n\n--- Segment 19 ---\nIn contrast, for more intricate problems with complex contexts, such as mathematical functions and finan- cial computing, LLMs tend to produce syntactically incorrect code, reflecting challenges in understanding and reasoning within these contexts. The results show that for every problem there is at least one model providing correct solutions. However, in categories such as pipelining, financial computing, and encryption, LLMs tend to un- derperform and show higher variability. For example, GPT-3.5 pro- duced no passing solutions in pipelining, but LLaMA 3.1 achieved good results. A similar pattern is observed in the mathematical functions category. These results highlight the importance of eval- uating both functional correctness and resource optimization in complex design scenarios.\n\n--- Segment 20 ---\nA similar pattern is observed in the mathematical functions category. These results highlight the importance of eval- uating both functional correctness and resource optimization in complex design scenarios. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao Table 4: Design correctness of LLMs in generating Verilog code across different categories Cell format: pass synthesis OK but incorrect design synthesis error GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B Combinational Logic 112 5 3 117 3 0 120 0 0 118 1 1 115 2 3 117 2 1 109 1 10 112 2 6 120 0 0 Finite State Machines 23 15 22 32 22 6 31 24 5 39 18 3 31 24 5 34 26 0 27 23 10 39 10 11 36 6 18 Mathematical Functions 13 19 43 6 39 30 36 10 29 46 24 5 7 6 62 26 27 22 20 26 29 5 8 62 0 3 72 Basic Arithmetic Ops 37 2 36 63 8 4 66 9 0 68 4 3 43 2 30 38 22 15 27 13 35 54 6 15 62 13 0 Bitwise Logic Ops 35 0 25 55 0 5 58 2 0 59 0 1 52 0 8 47 0 13 33 11 16 36 0 24 55 0 5 Pipelining 0 59 16 11 54 10 26 49 0 15 38 22 7 38 30 15 32 28 16 26 33 21 31 23 6 56 13 Polynomial Evaluation 19 3 53 69 0 6 74 1 0 68 5 2 58 6 11 55 2 18 28 5 42 65 7 3 69 6 0 Machine Learning 31 3 41 60 8 7 60 13 2 73 1 1 45 28 2 63 12 0 61 12 2 57 2 16 64 8 3 Financial Computing 9 23 28 21 22 17 29 13 18 20 20 20 11 21 28 28 15 17 15 12 33 16 7 37 17 23 20 Encryption 30 0 15 30 2 13 25 20 0 30 0 15 26 0 19 25 9 11 30 1 14 30 0 15 30 0 15 Physics 45 3 12 57 0 3 53 4 3 54 5 1 41 11 8 49 7 4 40 17 3 38 15 7 55 2 3 Climate 8 15 37 21 30 9 41 11 8 41 15 4 24 23 13 38 19 3 19 31 10 32 14 14 28 19 13 Number of wins 0 2 4 5 0 0 0 1 1 5.3 Resource Usage Table 5 presents the LUT usage results for the benchmark problems.\n\n--- Segment 21 ---\nThese results highlight the importance of eval- uating both functional correctness and resource optimization in complex design scenarios. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao Table 4: Design correctness of LLMs in generating Verilog code across different categories Cell format: pass synthesis OK but incorrect design synthesis error GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B Combinational Logic 112 5 3 117 3 0 120 0 0 118 1 1 115 2 3 117 2 1 109 1 10 112 2 6 120 0 0 Finite State Machines 23 15 22 32 22 6 31 24 5 39 18 3 31 24 5 34 26 0 27 23 10 39 10 11 36 6 18 Mathematical Functions 13 19 43 6 39 30 36 10 29 46 24 5 7 6 62 26 27 22 20 26 29 5 8 62 0 3 72 Basic Arithmetic Ops 37 2 36 63 8 4 66 9 0 68 4 3 43 2 30 38 22 15 27 13 35 54 6 15 62 13 0 Bitwise Logic Ops 35 0 25 55 0 5 58 2 0 59 0 1 52 0 8 47 0 13 33 11 16 36 0 24 55 0 5 Pipelining 0 59 16 11 54 10 26 49 0 15 38 22 7 38 30 15 32 28 16 26 33 21 31 23 6 56 13 Polynomial Evaluation 19 3 53 69 0 6 74 1 0 68 5 2 58 6 11 55 2 18 28 5 42 65 7 3 69 6 0 Machine Learning 31 3 41 60 8 7 60 13 2 73 1 1 45 28 2 63 12 0 61 12 2 57 2 16 64 8 3 Financial Computing 9 23 28 21 22 17 29 13 18 20 20 20 11 21 28 28 15 17 15 12 33 16 7 37 17 23 20 Encryption 30 0 15 30 2 13 25 20 0 30 0 15 26 0 19 25 9 11 30 1 14 30 0 15 30 0 15 Physics 45 3 12 57 0 3 53 4 3 54 5 1 41 11 8 49 7 4 40 17 3 38 15 7 55 2 3 Climate 8 15 37 21 30 9 41 11 8 41 15 4 24 23 13 38 19 3 19 31 10 32 14 14 28 19 13 Number of wins 0 2 4 5 0 0 0 1 1 5.3 Resource Usage Table 5 presents the LUT usage results for the benchmark problems. In this table, problems where all LLMs yield identical LUT usage are excluded for brevity.\n\n--- Segment 22 ---\nHEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao Table 4: Design correctness of LLMs in generating Verilog code across different categories Cell format: pass synthesis OK but incorrect design synthesis error GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B Combinational Logic 112 5 3 117 3 0 120 0 0 118 1 1 115 2 3 117 2 1 109 1 10 112 2 6 120 0 0 Finite State Machines 23 15 22 32 22 6 31 24 5 39 18 3 31 24 5 34 26 0 27 23 10 39 10 11 36 6 18 Mathematical Functions 13 19 43 6 39 30 36 10 29 46 24 5 7 6 62 26 27 22 20 26 29 5 8 62 0 3 72 Basic Arithmetic Ops 37 2 36 63 8 4 66 9 0 68 4 3 43 2 30 38 22 15 27 13 35 54 6 15 62 13 0 Bitwise Logic Ops 35 0 25 55 0 5 58 2 0 59 0 1 52 0 8 47 0 13 33 11 16 36 0 24 55 0 5 Pipelining 0 59 16 11 54 10 26 49 0 15 38 22 7 38 30 15 32 28 16 26 33 21 31 23 6 56 13 Polynomial Evaluation 19 3 53 69 0 6 74 1 0 68 5 2 58 6 11 55 2 18 28 5 42 65 7 3 69 6 0 Machine Learning 31 3 41 60 8 7 60 13 2 73 1 1 45 28 2 63 12 0 61 12 2 57 2 16 64 8 3 Financial Computing 9 23 28 21 22 17 29 13 18 20 20 20 11 21 28 28 15 17 15 12 33 16 7 37 17 23 20 Encryption 30 0 15 30 2 13 25 20 0 30 0 15 26 0 19 25 9 11 30 1 14 30 0 15 30 0 15 Physics 45 3 12 57 0 3 53 4 3 54 5 1 41 11 8 49 7 4 40 17 3 38 15 7 55 2 3 Climate 8 15 37 21 30 9 41 11 8 41 15 4 24 23 13 38 19 3 19 31 10 32 14 14 28 19 13 Number of wins 0 2 4 5 0 0 0 1 1 5.3 Resource Usage Table 5 presents the LUT usage results for the benchmark problems. In this table, problems where all LLMs yield identical LUT usage are excluded for brevity. The cell with the lowest resource usage in each category is highlighted in bold.\n\n--- Segment 23 ---\nIn this table, problems where all LLMs yield identical LUT usage are excluded for brevity. The cell with the lowest resource usage in each category is highlighted in bold. The number of wins is determined by counting these highlighted cells. The results suggest that different LLMs have varying levels of optimization capability within our benchmark framework. This highlights how the benchmark problems reveal differences in LLMs ability to optimize resource usage. Moreover, we have the following observations based on the results: (1) GPT-o1-mini leads with 19 wins, significantly outperform- ing the runner-up, GPT-4, which achieves 12 wins. This indicates that GPT-o1-mini generates Verilog designs with lower resource usage in most problems. Its strong perfor- mance suggests that advanced reasoning capabilities may enhance its understanding of problem requirements, the Ver- ilog language, and the complexities of hardware design. In contrast, code-specialized LLMs, while demonstrating high accuracy in producing functionally correct Verilog, may lack the reasoning depth needed to optimize designs effectively for FPGA constraints. This difference highlights that gen- erating syntactically correct HDL alone is insufficient for producing resource-efficient hardware, as true optimization demands a deeper understanding of both design require- ments and FPGA-specific constraints. (2) GPT-3.5-Turbo, Qwen2.5-Coder, and Codestral demonstrate the weakest resource optimization ability, achieving only 7, 7, and 8 wins, respectively. The poor resource optimiza- tion of GPT-3.5-turbo is potentially due to its model size and lack of updates. Qwen2.5-Coder and Codestral, the two code-specialized models in our evaluation, also struggle with resource optimization. One possible explanation is that these models are primarily trained on software code rather than HDL, which may limit their ability to account for FPGA re- source constraints when generating and optimizing Verilog. Additionally, key optimization techniques, such as mathemat- ical simplifications, are unlikely to be picked up effectively from software code data. (3) The observed variations in resource usage across different problem types confirm that our benchmarks can lead to diver- gent hardware resource usage for Verilog designs generated by different LLMs. For simple tasks such as combinational logic and basic arithmetic operations, the differences tend to be less significant.\n\n--- Segment 24 ---\n(3) The observed variations in resource usage across different problem types confirm that our benchmarks can lead to diver- gent hardware resource usage for Verilog designs generated by different LLMs. For simple tasks such as combinational logic and basic arithmetic operations, the differences tend to be less significant. This is likely because the training data of the models include well-established reference solution for these problems. However, for more complex problems, our benchmark problems lead to significantly greater divergence in LUT usage. This suggests that our benchmark problems effectively evaluate the ability of LLMs to optimize resource usage beyond learned patterns. Considering both functional correctness and resource usage, we find that GPT-o1-mini achieves the highest performance in both aspects, while code-specialized models, including Qwen2.5-Coder and Codestral, perform the worst. 6 Conclusion and Future Work LLMs provide a promising solution for automating HDL generation. However, most current benchmarks focus mainly on functional correctness while overlooking FPGA resource constraints. This lack of attention on FPGA resource efficiency underscores the need for resource-aware benchmarks to better evaluate LLM-generated HDL for real-world FPGA deployment. Additionally, current bench- marks lack problem diversity, limiting their effectiveness in evalu- ating real-world FPGA applications. To address these limitations, we introduce ResBench, the first resource-centric benchmark for LLM-generated HDL. ResBench features 56 problems spanning 12 categories.\n\n--- Segment 25 ---\nTo address these limitations, we introduce ResBench, the first resource-centric benchmark for LLM-generated HDL. ResBench features 56 problems spanning 12 categories. The benchmark problems are designed to expose LLMs ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 5: LUTmin for each LLM across categories GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B fsm 3state 1 0 0 0 0 0 0 0 0 traffic light 1 1 2 0 0 2 3 2 elevator controller 3 3 2 2 2 2 2 2 2 vending machine 1 1 2 1 2 1 1 2 1 int sqrt 68 177 64 229 173 fibonacci 56 1 56 56 56 mod exp 4466 4669 1911 1678 power 79 93 93 93 93 93 log2 int 10 20 12 abs diff 12 12 14 12 12 12 12 12 modulo op 82 82 82 82 111 left shift 10 10 10 10 10 12 12 10 10 pipelined adder 0 16 0 0 15 pipelined multiplier 77 70 56 70 pipelined max finder 0 24 0 24 24 24 24 24 洧논3 3洧논2 3洧논 1 49 49 0 91 0 91 0 91 49 (洧논 2)2 (洧논 2)2 (洧논 2)2 64 33 96 11 108 108 26 18 33 (洧녩 洧녪)2 (洧녩 洧녪)2 0 213 59 16 213 16 16 16 relu 8 8 8 8 8 16 8 8 16 mse loss 216 64 64 216 64 216 64 64 compound interest 13060 10135 10135 52950 9247 10135 52950 currency converter 0 0 25 0 free fall distance 6 6 64 6 6 64 67 64 6 kinetic energy 70 70 54 54 54 54 54 54 54 potential energy 6 6 84 0 6 6 6 6 6 carbon footprint 174 121 110 92 121 121 110 110 110 heat index 16 16 201 16 195 16 124 201 201 air quality index 128 104 104 116 128 128 Number of wins 7 12 10 19 11 10 9 7 8 ability to generate Verilog designs optimized for FPGA resource usage.\n\n--- Segment 26 ---\nResBench features 56 problems spanning 12 categories. The benchmark problems are designed to expose LLMs ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 5: LUTmin for each LLM across categories GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B fsm 3state 1 0 0 0 0 0 0 0 0 traffic light 1 1 2 0 0 2 3 2 elevator controller 3 3 2 2 2 2 2 2 2 vending machine 1 1 2 1 2 1 1 2 1 int sqrt 68 177 64 229 173 fibonacci 56 1 56 56 56 mod exp 4466 4669 1911 1678 power 79 93 93 93 93 93 log2 int 10 20 12 abs diff 12 12 14 12 12 12 12 12 modulo op 82 82 82 82 111 left shift 10 10 10 10 10 12 12 10 10 pipelined adder 0 16 0 0 15 pipelined multiplier 77 70 56 70 pipelined max finder 0 24 0 24 24 24 24 24 洧논3 3洧논2 3洧논 1 49 49 0 91 0 91 0 91 49 (洧논 2)2 (洧논 2)2 (洧논 2)2 64 33 96 11 108 108 26 18 33 (洧녩 洧녪)2 (洧녩 洧녪)2 0 213 59 16 213 16 16 16 relu 8 8 8 8 8 16 8 8 16 mse loss 216 64 64 216 64 216 64 64 compound interest 13060 10135 10135 52950 9247 10135 52950 currency converter 0 0 25 0 free fall distance 6 6 64 6 6 64 67 64 6 kinetic energy 70 70 54 54 54 54 54 54 54 potential energy 6 6 84 0 6 6 6 6 6 carbon footprint 174 121 110 92 121 121 110 110 110 heat index 16 16 201 16 195 16 124 201 201 air quality index 128 104 104 116 128 128 Number of wins 7 12 10 19 11 10 9 7 8 ability to generate Verilog designs optimized for FPGA resource usage. While ResBench is not explicitly designed to emphasize combi- national logic and arithmetic operations, the current problem set naturally includes a high proportion of such designs.\n\n--- Segment 27 ---\nThe benchmark problems are designed to expose LLMs ResBench: A Resource-Aware Benchmark for LLM-Generated FPGA Designs HEART 25, May 26 28, 2025, Kumamoto, Japan Table 5: LUTmin for each LLM across categories GPT-3.5 GPT-4 GPT-4o GPT-o1 Llama3.1 Qwen-max Qwen-plus Qwen2.5-coder Codestral turbo mini 405B 32B fsm 3state 1 0 0 0 0 0 0 0 0 traffic light 1 1 2 0 0 2 3 2 elevator controller 3 3 2 2 2 2 2 2 2 vending machine 1 1 2 1 2 1 1 2 1 int sqrt 68 177 64 229 173 fibonacci 56 1 56 56 56 mod exp 4466 4669 1911 1678 power 79 93 93 93 93 93 log2 int 10 20 12 abs diff 12 12 14 12 12 12 12 12 modulo op 82 82 82 82 111 left shift 10 10 10 10 10 12 12 10 10 pipelined adder 0 16 0 0 15 pipelined multiplier 77 70 56 70 pipelined max finder 0 24 0 24 24 24 24 24 洧논3 3洧논2 3洧논 1 49 49 0 91 0 91 0 91 49 (洧논 2)2 (洧논 2)2 (洧논 2)2 64 33 96 11 108 108 26 18 33 (洧녩 洧녪)2 (洧녩 洧녪)2 0 213 59 16 213 16 16 16 relu 8 8 8 8 8 16 8 8 16 mse loss 216 64 64 216 64 216 64 64 compound interest 13060 10135 10135 52950 9247 10135 52950 currency converter 0 0 25 0 free fall distance 6 6 64 6 6 64 67 64 6 kinetic energy 70 70 54 54 54 54 54 54 54 potential energy 6 6 84 0 6 6 6 6 6 carbon footprint 174 121 110 92 121 121 110 110 110 heat index 16 16 201 16 195 16 124 201 201 air quality index 128 104 104 116 128 128 Number of wins 7 12 10 19 11 10 9 7 8 ability to generate Verilog designs optimized for FPGA resource usage. While ResBench is not explicitly designed to emphasize combi- national logic and arithmetic operations, the current problem set naturally includes a high proportion of such designs. Future work will expand the benchmark to include more sequential designs, such as pipelined architectures and state-driven circuits.\n\n--- Segment 28 ---\nWhile ResBench is not explicitly designed to emphasize combi- national logic and arithmetic operations, the current problem set naturally includes a high proportion of such designs. Future work will expand the benchmark to include more sequential designs, such as pipelined architectures and state-driven circuits. Additionally, although the current evaluation focuses on Verilog, our framework is designed to support multiple HDLs. Future efforts will extend support to VHDL and high-level synthesis (HLS) tools. Acknowledgments We thank the anonymous reviewers for their valuable feedback and suggestions. The support of the United Kingdom EPSRC (grant number UKRI256, EP V028251 1, EP N031768 1, EP S030069 1, and EP X036006 1), Intel, and AMD is gratefully acknowledged. AI Usage Statement: This work involves the use of generative AI in multiple aspects. The methodology presented in this paper focuses on evaluating the ability of AI models to generate HDL code. As such, all experimental results are based on Verilog designs produced by LLMs. For writing, ChatGPT-4 and Llama 3.1 were used to refine phrasing, improve clarity, and proofread the text. HEART 25, May 26 28, 2025, Kumamoto, Japan C. Guo and T. Zhao References [1] Abdelatty, M., Ma, J., and Reda, S. MetRex: A benchmark for Verilog code metric reasoning using llms. arXiv preprint arXiv:2411.03471 (2024). [2] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [3] Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023).\n\n--- Segment 29 ---\nQwen technical report. arXiv preprint arXiv:2309.16609 (2023). [4] Batten, C., Pinckney, N., Liu, M., Ren, H., and Khailany, B. PyHDL-Eval: An LLM evaluation framework for hardware design using python-embedded dsls. In Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD (2024), pp. 1 17. [5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877 1901. [6] Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). [7] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research 24, 240 (2023), 1 113. [8] Devlin, J. BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [9] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, W.-t., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999 (2022).\n\n--- Segment 30 ---\n[9] Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, W.-t., Zettlemoyer, L., and Lewis, M. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999 (2022). [10] Gao, M., Zhao, J., Lin, Z., Ding, W., Hou, X., Feng, Y., Li, C., and Guo, M. AutoVCoder: A systematic framework for automated Verilog code generation using llms. arXiv preprint arXiv:2407.18333 (2024). [11] Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874 (2021). [12] Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186 (2024). [13] Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. GPT-4o system card. arXiv preprint arXiv:2410.21276 (2024). [14] Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. OpenAI O1 system card. arXiv preprint arXiv:2412.16720 (2024).\n\n--- Segment 31 ---\nOpenAI O1 system card. arXiv preprint arXiv:2412.16720 (2024). [15] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [16] Jiang, J., Wang, F., Shen, J., Kim, S., and Kim, S. A survey on large language models for code generation. arXiv preprint arXiv:2406.00515 (2024). [17] Kashanaki, F. R., Zakharov, M., and Renau, J. HDLEval benchmarking LLMs for multiple HDLs. In 2024 IEEE LLM Aided Design Workshop (LAD) (2024), IEEE, pp. 1 5. [18] Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T. DS-1000: A natural and reliable benchmark for data science code generation. In International Conference on Machine Learning (2023), PMLR, pp. 18319 18345. [19] Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. StarCoder: May the source be with you! arXiv preprint arXiv:2305.06161 (2023). [20] Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by ChatGPT really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems 36 (2024). [21] Liu, M., Pinckney, N., Khailany, B., and Ren, H. Verilogeval: Evaluating large language models for Verilog code generation.\n\n--- Segment 32 ---\nAdvances in Neural Information Processing Systems 36 (2024). [21] Liu, M., Pinckney, N., Khailany, B., and Ren, H. Verilogeval: Evaluating large language models for Verilog code generation. In 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD) (2023), IEEE, pp. 1 8. [22] Liu, S., Lu, Y., Fang, W., Li, M., and Xie, Z. OpenLLM-RTL: Open dataset and benchmark for llm-aided design RTL generation. In IEEE ACM International Conference on Computer-Aided Design (ICCAD) (2024). [23] Lu, Y., Liu, S., Zhang, Q., and Xie, Z. RTLLM: An open-source benchmark for design RTL generation with large language models. In 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC) (2024), IEEE, pp. 722 727. [24] Muennighoff, N., Liu, Q., Zebaze, A., Zheng, Q., Hui, B., Zhuo, T. Y., Singh, S., Tang, X., Von Werra, L., and Longpre, S. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124 (2023). [25] Nadimi, B., and Zheng, H. A multi-expert large language model architecture for Verilog code generation. arXiv preprint arXiv:2404.08029 (2024). [26] Nijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., and Xiong, C. CodeGen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474 (2022). [27] Qiu, R., Zhang, G. L., Drechsler, R., Schlichtmann, U., and Li, B. Autobench: Automatic testbench generation and evaluation using LLMs for HDL design. In Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD (2024), pp.\n\n--- Segment 33 ---\n[27] Qiu, R., Zhang, G. L., Drechsler, R., Schlichtmann, U., and Li, B. Autobench: Automatic testbench generation and evaluation using LLMs for HDL design. In Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD (2024), pp. 1 10. [28] Radford, A. Improving language understanding by generative pre-training, 2018. [29] Ren, S., Guo, D., Lu, S., Zhou, L., Liu, S., Tang, D., Sundaresan, N., Zhou, M., Blanco, A., and Ma, S. CodeBLEU: A method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297 (2020). [30] Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., et al. Code Llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023). [31] Tan, A. HDLBits: Digital circuits exercises, 2017. Accessed: 2025. [32] Thakur, S., Ahmad, B., Fan, Z., Pearce, H., Tan, B., Karri, R., Dolan-Gavitt, B., and Garg, S. Benchmarking large language models for automated Verilog RTL code generation. In 2023 Design, Automation Test in Europe Conference Exhibition (DATE) (2023), IEEE, pp. 1 6. [33] Thakur, S., Ahmad, B., Pearce, H., Tan, B., Dolan-Gavitt, B., Karri, R., and Garg, S. VeriGen: A large language model for Verilog code generation. ACM Transactions on Design Automation of Electronic Systems 29, 3 (2024), 1 31.\n\n--- Segment 34 ---\n[33] Thakur, S., Ahmad, B., Pearce, H., Tan, B., Dolan-Gavitt, B., Karri, R., and Garg, S. VeriGen: A large language model for Verilog code generation. ACM Transactions on Design Automation of Electronic Systems 29, 3 (2024), 1 31. [34] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi칟re, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [35] Uniyal, M., Singh, M., Verbruggen, G., Gulwani, S., and Le, V. One-to-many testing for code generation from (just) natural language. In Findings of the Association for Computational Linguistics: EMNLP 2024 (2024), pp. 15397 15402. [36] Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems (2017). [37] Vijayaraghavan, P., Shi, L., Ambrogio, S., Mackin, C., Nitsure, A., Beymer, D., and Degan, E. VHDL-Eval: A framework for evaluating large language models in VHDL code generation. arXiv preprint arXiv:2406.04379 (2024). [38] Wan, G.-W., yubo, W., Wong, S., jingyi zhang, Xing, M., jiang, Z., Guan, N., ying wang, Xu, N., Xu, Q., and Wang, X. Genben:a genarative benchmark for LLM-aided design, 2025. [39] Wang, J., and Chen, Y. A review on code generation with LLMs: Application and evaluation. In 2023 IEEE International Conference on Medical Artificial Intelligence (MedAI) (2023), IEEE, pp. 284 289.\n\n--- Segment 35 ---\nIn 2023 IEEE International Conference on Medical Artificial Intelligence (MedAI) (2023), IEEE, pp. 284 289. [40] Wang, N., Yao, B., Zhou, J., Wang, X., Jiang, Z., and Guan, N. Large lan- guage model for Verilog generation with golden code feedback. arXiv preprint arXiv:2407.18271 (2024). [41] Wong, S.-Z., Wan, G.-W., Liu, D., and Wang, X. VGV: Verilog generation using visual capabilities of multi-modal large language models. In 2024 IEEE LLM Aided Design Workshop (LAD) (2024), IEEE, pp. 1 5. [42] Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115 (2024). [43] Yang, Y., Teng, F., Liu, P., Qi, M., Lv, C., Li, J., Zhang, X., and He, Z. Haven: Hallucination-mitigated llm for Verilog code generation aligned with HDL engi- neers. arXiv preprint arXiv:2501.04908 (2025). [44] Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou, Z., Gong, C., Shen, Y., et al. A comprehensive capability analysis of GPT-3 and GPT-3.5 series models. arXiv preprint arXiv:2303.10420 (2023). [45] Zheng, Z., Ning, K., Wang, Y., Zhang, J., Zheng, D., Ye, M., and Chen, J. A survey of large language models for code: Evolution, benchmarking, and future trends. arXiv preprint arXiv:2311.10372 (2023).\n\n