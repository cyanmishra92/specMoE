=== ORIGINAL PDF: 2503.13572v3_VeriContaminated_Assessing_LLM-Driven_Verilog_Codi.pdf ===\n\nRaw text length: 41151 characters\nCleaned text length: 40193 characters\nNumber of segments: 30\n\n=== CLEANED TEXT ===\n\narXiv:2503.13572v3 [cs.AR] 12 Jun 2025 VeriContaminated: Assessing LLM-Driven Verilog Coding for Data Contamination Zeng Wang , Minghao Shao , Jitendra Bhandari , Likhitha Mankali , Ramesh Karri , Ozgur Sinanoglu , Muhammad Shafique , Johann Knechtel NYU Tandon School of Engineering, USA NYU Abu Dhabi, UAE Email:{zw3464, shao.minghao, jb7410, likhitha.mankali, rkarri, ozgursin, muhammad.shafique, Abstract Large Language Models (LLMs) have revolutionized code generation, achieving exceptional results on various estab- lished benchmarking frameworks. However, concerns about data contamination where benchmark data inadvertently leaks into pre-training or fine-tuning datasets raise questions about the validity of these evaluations. While this issue is known, limiting the industrial adoption of LLM-driven software engineering, hardware coding has received little to no attention regarding these risks. For the first time, we analyze state-of-the-art (SOTA) evaluation frameworks for Verilog code generation (VerilogEval and RTLLM), using established methods for contamination detection (CCD and Min-K Prob). We cover SOTA commercial and open-source LLMs (CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned models (RTLCoder and Verigen). Our study confirms that data contamination is a critical concern. We explore mitigations and the resulting trade- offs for code quality vs fairness (i.e., reducing contamination toward unbiased benchmarking). Index Terms LLMs, Hardware Design, Data Contamination I. INTRODUCTION Large Language Models (LLMs) like GPT-4 [1] and Gem- ini [2] have exhibited remarkable capabilities in compre- hending text semantics and generating code across different programming languages. However, a critical challenge for evaluating these models is data contamination, which occurs when benchmarks inadvertently include samples from the model s pre-training corpus [3]. Such a scenario would unfairly inflate performance estimates and undermine the reliability of comparative assessments. With impressive performance metrics reported for many LLMs, the undisclosed nature of the pre- training datasets, common even among open-source models [4], raises significant concerns for such contamination [5] [8]. LLMs have also shown considerable promise in the domain of hardware design [10] [20]. However, as in the software domain, data contamination poses a challenge when establish- ing LLMs for hardware designs. The performance and utility of these domain-specific LLMs depend on the availability of large, high-quality datasets tailored to the complexities of hardware design. Given the relative sparsity of such data, it is easy to see that evaluation benchmarks may (inadvertently) include some of the same samples that were part of the pre-training data. Figure 1 confirms this hypothesis through Authors contributed equally to this research. Top-1 Score 80 Top-1 Score 90 Top-1 Score 100 Top-1 Score 80 Top-1 Score 90 Top-1 Score 100 0.00 20.00 40.00 60.00 RTLLM Benchmark VerlogEval Benchmark RTLCoder Verigen Fig. 1: Top-1 scores for Verilog Abstract Syntax Tree (AST) similarity, powered by Dolos [9], between golden solutions and closest designs in training datasets. RTLCoder and Verigen show significant similarities to the RTLLM and VerilogEval benchmarks, suggesting severe data contamination. assessment of code similarity for the state-of-the-art (SOTA) datasets of Verigen [14] and RTLCoder [21] against the SOTA benchmarks VerilogEval [11] and RTLLM [13]. Such large overlaps between training and testing data are highly likely to induce data contamination and thereby skew benchmarking results. Addressing this challenge is crucial for fair ranking of LLMs for hardware design. We address this challenge for the first time. We study data contamination for many SOTA LLMs (CodeGen2.5 [22], Minitron 4b [23], Mistral 7b [24], phi-4 mini [25], LLaMA- {1,2,3.1} [4], [26], [27], GPT-{2,3.5,4o} [28] [30], Deepseek- Coder [31], and CodeQwen 1.5 [32]), across different datasets before and after fine-tuning. For benchmarking, we use the SOTA tools VerilogEval and RTLLM. Key contributions of our work are the following. 1) Our analysis of LLM data contamination across Verilo- gEval and RTLLM benchmarks shows near 100 rates in GPT-3.5 and GPT-4o, but lower in older models. 2) We analyze detection methods and illustrate how contam- ination trends vary across different experimental settings. 3) We study the trade-off between code quality and contami- nation mitigation. That evaluation shows the potential of mitigating hardware code contamination. II. BACKGROUND A. LLMs for Hardware Design LLMs have demonstrated impressive capabilities in code generation [1], [4], which has extended interest toward their application in hardware design. LLMs have been tailored for a variety of tasks, including Verilog code generation [11] [14], assertion generation [15], [16], testbench generation [17], [18], and scripting for electronic design automation [19], [20]. For example, in [14], researchers fine-tuned CodeGen- 16B [33] using a comprehensive training corpus of Verilog codes sourced from GitHub and textbooks. ChipNemo [20] leverages LLaMA2 [4] as a foundation, refining it using public datasets and NVIDIA s proprietary designs. RTLCoder [21] constructs instruction-code pairs using GPT to generate training data from a curated pool of keywords and source codes. Prompt-engineering strategies have been introduced in [34] [36] to enhance code generation performance. To evaluate these capabilities, frameworks like VerilogEval [11] and RTLLM [13] have been developed, which assess the functional and syntactic correctness of Verilog code produced by these models. B. Data Contamination in LLMs Data contamination refers to cases where test data have been included in the model s training data [5] [8]. Such a scenario, be it inadvertently or on purpose, leads to the models performing exceptionally well on the leaked test data. It has been shown that data contamination grows rapidly through time for various LLM models [37], especially in ChatGPT [38]. Detection: A challenge for detection of data contamination is the magnitude of pre-training data, which renders full disclosure or cross-verification impractical, as the resources required to audit every data point are prohibitive [4], [39]. Still, practical means for detection exist, as outlined next. [40] proposed the identification of potential contamination at the instance level, further using this information to assess wider contamination at the partition level to identify data contamination within LLMs. [41] leverages the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities to detect data contamination. [4] reported a significant performance gap for LLaMA-2 70B on clean vs dirty sets of benchmarks. [42] investigated the impact of contamination on the performance of GPT-3.5 in for text-to-SQL code-generation. [43] proposed contamination detection CDD via analyzing the peak token-level edit distance distributions of LLMs. [44] introduced Min-K Prob, evaluating the k tokens with minimum probabilities, based on hypothesis that an unseen example is likely to contain a few outlier words with low probabilities. Mitigation: Established approaches for mitigation seek to either dynamically generate new test data or withold reference test data, as outlined next. TED [43] excludes peakedness and removes duplication to restore uncontaminated inferrences. [45] proposed dynamic and time-sensitive test construction. [46] proposed private benchmarking, a solution where test datasets are private and models are evaluated without revealing the test data to the model. Similarly, [47] proposed a comprehensive and contamination-free evaluation of LLMs for coding, which collects new problems over time from contests and other sources. [48] studied data contamination of popular code generation benchmarks and quantified their overlap with pre- training corpus through surface- and semantic-level matching. III. EVALUATION A. Experiment Setup Our investigation has two phases: (1) evaluating contamina- tion across multiple foundation models using benchmarks to establish baseline Verilog code contamination levels and (2) deliberately contaminating a clean model to assess TED s [43] mitigation efficacy. All experiments used an Nvidia A100 GPU (80GB) with CUDA 12.2. This section details the methodology. Models: We use widely recognized baseline models with different sizes and sources. We pick CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5 for evaluation [49], [50]. This selection encompasses models ranging from earlier itera- tions to state-of-the-art LLM families, balancing commercial and open-source offerings. Fine-tuning Setup: To establish a fair setting for contam- ination assessment and to enable controllable mitigation, we simulate data contamination for LLaMA-3.1-8B. We specifi- cally chose this model due to its moderate contamination rate (Section III-C). We fine-tuned two separate instances on distinct training datasets: one using the 55M RTLCoder [21] dataset within its native training framework and another using the 78M filtered Verigen [14], [51] via the Alpaca [52] library. After experimenting with various hyperparameter configurations, we determined that epoch 3 and learning rate (lr) 1e 5 with the Adam optimizer produced the highest contamination rate, creating optimal conditions for observing contamination effects. For inference, we used temperature (temp) 0.8, top-p 0.95, and maximum context length 2048. Evaluation Setup: We evaluate model performance using RTLLM [13] and VerilogEval [11] benchmark sets. We will analyze the trade-off between contamination mitigation and Verilog generation accuracy. Using the setup in [43] [44], we ran 50 sample inferences per model for each problem in VerilogEval and RTLLM for CDD and Min-K Prob (K 20) evaluations. For fine-tuned models, we used the inferred samples to evaluate functionality of changes during TED. B. Metrics To evaluate contamination in current test benchmarks, we define contamination rates as the proportion of contaminated problems within the problem set. We identify contaminated problems using two methods, CDD [43] and Min-K Prob [44], and by varying parameters described in Section III-C. These approaches define contamination differently, providing complementary insights. CDD uses token-level edit distance to identify repetition across inference distributions, while Min-K Prob assesses contamination by examining rare token probabilities, with higher values indicating contamination. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (a) RTLLM evaluation for different α values. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (b) VerilogEval evaluation for different α values. Fig. 2: Model contamination evaluation using CCD. Parameter variations reveal contamination rate patterns. We analyze the impact of data contamination on model performance in Verilog generation in Section III-D. For this analysis, we employ the metric to evaluate the accuracy of the generated Verilog code. Additionally, we assess the impact of the mitigation algorithm TED [43] on model accuracy. In Section VI, we further evaluate TED s effectiveness by comparing results before and after removing the samples most likely to be contaminated during inference. C. Contamination Evaluation This section evaluates contamination using two detection methods, CDD and Min-K Prob. We experiment with RTLLM and VerilogEval by varying threshold values. Our goal is to reveal data memorization and generalization, and to contrast commercial and open-source models. Fig. 2 compares contamination rates detected by CDD across different α values, introduced in [43] as a similarity threshold. As α increases, contamination rates rise, indicating stricter detection criteria. Commercial models such as GPT-3.5 and GPT-4o exhibit higher initial contamination rates approaching 100 suggesting higher memorization or weaker generaliza- tion. In contrast, open-source models like GPT-2 and LlaMA 1 maintain lower contamination rates. Interestingly, the small model scale of phi-4 mini does not result in lower contamination 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate min_20_prob_acc_list codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (a) RTLLM evaluation for different threshold values. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate min_20_prob_acc_list codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (b) VerilogEval evaluation for different threshold values. Fig. 3: Model contamination evaluation using Min-K prob. rates. RTLLM s more detailed prompts correlate with lower contamination rates, indicating stronger generalization under the same threshold settings. Both benchmarks exceed a 90 contamination rate at α 0.9. VerilogEval presents higher overall contamination, suggesting its evaluation likely relies on data memorized during these models pre-training. In short, CDD effectively evaluates RTL-based contamination at lower α. Results demonstrate that earlier open-source models appear less prone to contamination, while commercial models display higher contamination rates under the same α value. Fig. 3 illustrates contamination rates detected by Min-K Prob across thresholds (T) ranging from 0.0 to 1.0, as in [44]. Higher thresholds impose stricter criteria for identifying model contamination, which is reflected in the figure by an overall decline in contamination rates as thresholds increase. As before, GPT-3.5 and GPT-4o exhibit contamination spikes, contrasting with earlier models, such as GPT-2 and LLaMA 1. Min-K Prob analysis shows that RTLLM has high contamination peaks exceeding 0.8 in some cases while VerilogEval plateaus near 0.6. Since Min-K Prob relies on single-inference probability distribution, the detailed prompts in RTLLM may cause models to generate tokens with higher log probabilities, potentially explaining the higher contamination rates for the RTLLM benchmark compared to VerilogEval. Model CDD ( ) Min-K Prob ( ) Pass Rate ( ) RTLLM VerilogEval RTLLM VerilogEval RTLLM VerilogEval CodeGen2.5 14.00 1.28 4.00 5.77 2.00 2.56 Minitron 4b 38.00 2.56 0.00 0.00 2.00 1.28 Mistral 7b v0.2 30.00 10.90 24.00 21.15 2.00 1.28 phi-4 mini 54.00 22.43 14.00 10.90 10.00 4.49 LLaMA 1 14.00 2.56 0.00 0.64 0.00 1.28 LLaMA 2 24.00 17.95 84.00 67.95 0.00 3.85 LLaMA 3.1 8b 22.00 33.33 84.00 52.56 20.00 9.62 GPT-2 4.00 6.41 0.00 0.00 0.00 0.00 GPT-3.5 100.00 98.72 100.00 97.44 32.00 36.54 GPT-4o 100.00 100.00 98.00 99.36 44.00 60.26 DeepSeek-Coder 58.00 26.92 8.00 9.62 42.00 19.87 CodeQwen 1.5 78.00 25.64 10.00 4.49 26.00 17.95 Contamination: High ( 75 ) Mid (25-75 ) Low ( 25 ) Pass Rate: High ( 50 ) Mid (20-50 ) Low ( 20 ) TABLE I: Model accuracy and contamination rate analysis across metrics with CDD α 0.05 and Min-K Prob T 0.55. These results highlight methodological discrepancies in contamination detection: Min-K Prob and CDD yield divergent contamination estimates, underscoring how detection frameworks influence observed outcomes. The variability suggests that contamination metrics are sensitive to RTL design characteristics and its promptings, emphasizing the need for RTL-aware interpretation of such evaluations. D. Impact of Contamination on Accuracy Table I presents the performance of models in terms of functional correctness, accuracy, and contamination rate. The evaluations were conducted using the default setup from [43] with a CDD α of 0.05 and a Min-K Prob threshold of 0.55. From the CDD contamination rate results on VerilogEval and RTLLM, we observe that GPT-3.5 and GPT-4o exhibit nearly 100 contamination, which explains their superior performance in Verilog code generation. In contrast, DeepSeek-Coder and CodeQwen 1.5 have lower contamination rates, corresponding to their reduced performance in Verilog generation. phi-4 mini has a comparable functionality accuracy yet imposes only a small model size. Models such as CodeGen2.5, LLaMA 1, and GPT-2 show even lower CDD values, aligning with their significantly weaker performance. However, despite LLaMA 2 having a similar CDD value as LLaMA 3.1, its is noticeably lower than that of LLaMA 3.1, indicating CDD does not work well for LLaMA 2. Mistral 7b and Minitron 4b have a medium contamination rate but lower performance in Min-K Prob captures the contamination rates of GPT-3.5 and GPT-4o due to their high log probability of generating previously seen designs. However, it is weak when distinguishing contamination levels among other models, making it less reliable for fine-grained comparisons. As shown in Figure 3, it illustrates the decreasing trend of contamination rates as the threshold value increases. IV. CONTAMINATION CASE STUDIES Here, we present two contamination case studies using benchmark tasks from VerilogEval and RTLLM datasets. Each task has three parts: (1) a natural language prompt describing the hardware design objective, (2) a syntactic Verilog code template with functional gaps, and (3) a reference implementation serving as the ground truth. In each case study, we analyze a contaminated inference output from the fine-tuned LLaMA 3.1 model alongside a clean code sample generated by baseline LLaMA 3.1. By contrasting their structural and functional alignment with the ground truth, this analysis highlights differences in code quality due to contamination. In Case 1, we see clear differences between code reflecting memorized patterns and code generated more autonomously. The contaminated output faithfully reproduces the memorized shift register design, using identical bit concatenation syntax ({shift reg[2:0], in}) and correct index-based assignment (shift reg[3]), thereby satisfying the ground-truth functionality. In contrast, the uncontaminated version demonstrates incorrect implementation through reverse-order bit concatenation ({in, shift reg[3:1]}) and a inappropriate multi-bit output port assignment (wire [3:0] out), causing the syntactic errors. The contaminated code adheres to standard practices by handling resets and indexing signals in a conventional manner. Mean- while, the uncontaminated version s reversed shift direction highlight its lack of internalized shift register patterns. Even if contamination improves the code generation ability, the evaluation on this test turns out to be unfair. Case 1: VerilogEval Contamination Example Task: m2014 q4k Prompt: Implement a shift reg- ister with four D flops. Reset is active-low synchronous re- settable. 1 module TopModule ( 2 input clk, 3 input resetn, 4 input in, 5 output out 6 ); Reference Code 1 module RefModule (input clk, input resetn, input in, output out); 2 reg [3:0] sr; 3 always (posedge clk) begin 4 if ( resetn) 5 sr 0; 6 else 7 sr {sr[2:0], in}; 8 end 9 assign out sr[3]; 10 endmodule Uncontaminated Output 1 module TopModule ( 2 input clk, 3 input resetn, 4 input in, 5 output out 6 ); 7 reg [3:0] shift_reg; 8 wire [3:0] out; 9 10 always (posedge clk or negedge resetn) 11 if (!resetn) 12 shift_reg 4 b0; 13 else 14 shift_reg {in, shift_reg[3:1]}; 15 16 assign out shift_reg; 17 endmodule Contaminated Output 1 module TopModule ( 2 input clk, 3 input resetn, 4 input in, 5 output out 6 ); 7 reg [3:0] shift_reg; 8 always (posedge clk) begin 9 if (resetn 0) begin 10 shift_reg 4 b0; 11 end else begin 12 shift_reg {shift_reg[2:0], in}; 13 end 14 end 15 assign out shift_reg[3]; 16 endmodule Case 2 highlights RTLLM s adder-8bit implementation, showcasing a clear divergence between its reasoning and the memorization of patterns. In this case, the contaminated output reproduces a memorized code by constructing adder 8bit hierarchically by leveraging its self-defined full adder module. In contrast, the clean version, relying solely on its internal understanding, attempts a direct approach using hardware- oriented bitwise operations and Boolean algebra to compute cout and sum. But it ultimately fails to propagate the carry and assign the sum correctly. While the contaminated variant achieves the correct implementation echoing the training syntax from fine-tuning it raises important questions about comprehension versus memorization. Case 2: RTLLM Contamination Example Task: adder-8bit Prompt: Please act as a professional verilog designer. Implement module of an 8-bit adder with multiple bit-level adders in combinational logic. Implementation: The module utilizes a series of bit-level adders (full adders) to perform the addition operation. Give me the complete code. 1 Module name: 2 adder_8bit 3 Input ports: 4 a[7:0]: 8-bit input operand A 5 b[7:0]: 8-bit input operand B; cin: Carry-in input. 6 Output ports: 7 sum[7:0]: 8-bit output representing the sum of A and B 8 cout: Carry-out output. Reference Code 1 module verified_adder_8bit(input [7:0] a, b, input cin, output [7:0] sum, output cout); 2 wire [8:0] c; 3 full_adder FA0 (.a(a[0]), .b(b[0]), .cin(cin), .sum(sum[0]), .cout(c[0])); 4 ...... Folded Implementations 5 full_adder FA7 (.a(a[7]), .b(b[7]), .cin(c[6]), .sum(sum[7]), .cout(c[7])); 6 assign cout c[7]; 7 endmodule 8 9 module full_adder (input a, b, cin, output sum, cout); 10 assign {cout, sum} a b cin; 11 endmodule Uncontaminated Output 1 module adder_8bit(input [7:0] a, b, input cin, output [7:0] sum, output cout); 2 adder_8bit 3 sum[7] a[7] ˆ b[7] ˆ cin; 4 cout (a[7] b[7] cin) (a[7] b[7] !cin) (a[7] cin !b[7]) (b[7] cin !a[7]); 5 sum[6:0] {a[6] b[6] cout, a[5] b[5] cout, a[4] b[4] cout, a[3] b[3] cout, a[2] b[2] cout, a[1] b[1] cout, a[0] b[0] cout}; 6 endmodule Contaminated Output 1 module adder_8bit(input [7:0] a, input [7:0] b, input cin, output [7:0] sum, output cout); 2 wire [7:0] carry; 3 wire [7:0] sum_temp; 4 full_adder fa0(a[0], b[0], cin, sum_temp[0], carry[0]); 5 ...... Folded Implementations 6 full_adder fa7(a[7], b[7], carry[6], sum_temp[7], cout); 7 assign sum sum_temp; 8 endmodule 9 10 module full_adder(input a, input b, input cin, output sum, output cout); 11 assign sum a ˆ b ˆ cin; 12 assign cout (a b) (a cin) (b cin); 13 endmodule 0.0 0.2 0.4 0.6 0.8 1.0 Value (0.0 - 1.0) with step 0.1 0 20 40 60 80 100 Contamination Rate ( ) CDD Contamination Rate on RTLLM 0.0 0.1 0.2 0.3 96 98 100 LLaMA 3.1-70B and 405B show similar RTLLM contamination LLaMA 3.1-8B LLaMA 3.1-70B LLaMA 3.1-405B 0.0 0.2 0.4 0.6 0.8 1.0 Value (0.0 - 1.0) with step 0.1 0 20 40 60 80 100 Contamination Rate ( ) CDD Contamination on VerilogEval 0.0 0.1 0.2 0.3 96 98 100 (1).LLaMA 3.1-70B and 405B show similar VerilogEval contamination. (2).70B slightly cleaner than 405B. (3).Both exceed 8B contamination. (4).Model size appears to affect contamination levels. Fig. 4: Contamination Rate using CDD on RTLLM and VerilogEval V. MODEL SCALE IMPACTS ON CONTAMINATION We investigate how model scale impacts contamination by comparing CDD contamination rates across LLaMA 3.1 models of varying sizes (8B, 70B and 405B). Figure 4 reveals both 70B and 405B models maintain nearly 100 contamination on both benchmarks at α 0, with 405B showing 2.5 higher initial contamination than 70B. In contrast, the 8B model exhibits only 20 initial contamination, requiring higher α values to reach the levels of larger models. This suggests larger models covering more training data exhibit higher contamination rates, indicating a direct relationship between model scale and contamination. Moreover, differentiating contamination levels between sufficiently large-scale models (i.e., 70B and 405B) requires stricter α values in CDD or more precise metrics. VI. MITIGATION EVALUATION Here, we evaluate TED [43] mitigation through controlled experiments with RTLCoder and Verigen datasets, measuring performance on RTLLM and VerilogEval benchmarks with adaptive threshold scaling. Our dual-arm approach isolates con- tamination effects through separate fine-tuning while simulating data leakage, enabling systematic assessment of mitigation effectiveness. 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Mitigation Ratio ( ) Diff: 58.00 Verigen gap: 58 at 20 versus RTLCoder Mitigation Rate on RTLLM RTLCoder Verigen 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Mitigation Ratio ( ) Diff: 27.56 (1).Max gap: 27.56 at 36 on VerilogEval. (2).Smaller than RTLLM. (3).Fewer task description clearly reduce gap size Mitigation Rate on VerilogEval Fig. 5: Mitigation Rate using TED on RTLLM and VerilogEval Figure 5 demonstrates how mitigation rates vary with top percentage exclusion thresholds (τ) across RTLLM and VerilogEval benchmarks, comparing RTLCoder and Verigen datasets. AST analysis in Figure 1 confirms RTLCoder exhibits higher contamination than Verigen. This is consistent with Figure 5, where Verigen-fine-tuned models achieve higher 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 2.07x S F avg. drop rate, steeper decline when 20 on RTLLM fine-tuned with RTLCoder Syntax Functional 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 1.90x S F avg. drop rate, steeper decline when 40 on RTLLM fine-tuned with RTLCoder 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 2.66x S F avg. drop rate, steeper decline when 20. Less contaminated dataset mitigates with lower on RTLLM fine-tuned with Verigen 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 2.38x S F avg. drop rate, steeper decline when 40 on RTLLM fine-tuned with Verigen Fig. 6: Impact of Pass Rate on TED Mitigation Following Contamination Simulation on LLaMA 3.1 by Varying τ for Top Exclusion on RTLLM. mitigation rates across most τ values for both benchmarks. These findings indicate that less contaminated datasets like Verigen respond more effectively to mitigations due to their distinctive distribution patterns, facilitating efficient suppression of memorized content during threshold-based filtering. Figure 6 shows the relation between filtering threshold τ and pass rate accuracy in TED s contamination mitigation on RTLLM. As τ increases, stricter data filtering is applied, causing the pass rate to decline. This drop reflects the removal of a larger portion of contaminated data during training or evaluation. The drop on syntax over functionality (i.e, S F average drop rate) is around 2 times, indicating syntax-level contamination is easier to mitigate. Lower τ values retain more data, preserving model performance. Higher thresholds enforce stricter filtering, making the approach adaptable to risk tolerances across deployment scenarios. Despite this inverse correlation, TED effectively mitigates hardware-level contamination. It maintains strong model performance even at higher τ values, especially at with τ 40. This resilience makes it suited for high-stakes environments where data integrity is essential. Adjusting τ allows balancing decontamination and predictive accuracy. Figure 7 illustrates the relation between the filtering threshold τ and pass rate accuracy in TED s application to VerilogEval. Accuracy of the Verigen-contaminated model decreases gradu- ally with higher τ, while the RTLCoder-contaminated model has a sharp decline at low τ values but stabilizes as τ grows. The results mirror those shown in Figure 6, with accuracy decreasing as τ increases, a pattern consistent with TED s successful removal of memorization-based correct inferences. However, two key differences emerge in the VerilogEval evaluation: 1) The baseline functionality accuracy is notably lower than in RTLLM, and 2) The accuracy curve exhibits greater irregularity, reflecting VerilogEval s higher problem 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Steeper decline when 20. Compared to RTLLM, limited details from VerilogEval make contamination easier to mitigate. on VerilogEval fine-tuned with RTLCoder Functional 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Smooth decline from initial. Accuracy dropped from 0 to 100. on VerilogEval fine-tuned with RTLCoder 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Steeper decline when 20. Less contaminated dataset mitigation with lower on VerilogEval fine-tuned with Verigen 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Smooth decline from initial. With same on same benchmark, accuracy drop patterns show similarity. on VerilogEval fine-tuned with Verigen Fig. 7: Impact of Pass Rate on TED Mitigation Following Contamination Simulation on LLaMA 3.1 by Varying τ for Top Exclusion on VerilogEval. complexity. This increased difficulty suggests that individual inference response exert a more substantial influence on overall pass rates in VerilogEval compared to RTLLM. Consequently, accuracy declines more steeply as τ increases, particularly evident when comparing the fine-tuned with RTLCoder results between the two benchmarks (Figure 6). By comparing the accuracy degradation in the RTLCoder and Verigen fine- tuned models, the Verigen model s accuracy declines sharply at lower values of τ, whereas the RTLCoder model continues to drop even at higher τ. Severe contamination requires a stricter mitigation threshold (i.e., τ in TED). VII. LIMITATION AND FUTURE WORK Despite the utility of contamination detectors, our findings of- fer insights from different techniques, highlighting the need for robust approaches tailored to hardware domain. While CDD and Min-K Prob perform well when detecting contamination in software, hardware-specific methods are lacking. Although TED mitigation reduces contamination, it can adversely affect Verilog code accuracy, suggesting hardware-focused mitigations that balance contamination control with functional performance are needed. Our experiments evaluated traditional foundation models and the emergening thinking models like DeepSeek- R1 [53], Claude 3.7 [54] and OpenAI-o3 [55]; one should explore how reasoning processes influence contamination evaluation and detection. Integrating model reasoning with contamination analysis could yield deeper insights. Current contamination evaluations focus on transformer-based models. Next-generation diffusion models (e.g., LLaDA [56]) warrant an investigation whether they exhibit similar contamination risks and require specialized detection and mitigations. VIII. CONCLUSION This study highlights data contamination in LLM-aided RTL generation, compromising the fairness of hardware evaluations. Using CDD and Min-K Prob, commercial models (e.g., GPT-3.5 and GPT-4o) exhibits higher contamination rates than open-source models. The TED mitigation reduces contamination with limited impact on RTL accuracy. Future work will expand benchmarks to other hardware languages and, via industry collaboration, establish reliable LLM-driven design tools. REFERENCES [1] OpenAI, GPT-4, Mar. 2023. Available: gpt-4 [2] G. Team et al., Gemini: a family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [3] S. Balloccu et al., Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms, arXiv preprint arXiv:2402.03927, 2024. [4] H. Touvron et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [5] I. Magar and R. Schwartz, Data contamination: From memorization to exploitation, arXiv preprint arXiv:2203.08242, 2022. [6] C. Xu et al., Benchmark data contamination of large language models: A survey, arXiv preprint arXiv:2406.04244, 2024. [7] S. Ishihara, Training data extraction from pre-trained language models: A survey, in Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), A. Ovalle et al., Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 260 275. Available: [8] H. Hu et al., Membership inference attacks on machine learning: A survey, ACM Comput. Surv., vol. 54, no. 11s, Sep. 2022. Available: [9] R. Maertens et al., Discovering and exploring cases of educational source code plagiarism with dolos, SoftwareX, vol. 26, p. 101755, 2024. [10] Z. Wang et al., Llms and the future of chip design: Unveiling security risks and building trust, in 2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI). IEEE, 2024, pp. 385 390. [11] M. Liu et al., Verilogeval: Evaluating large language models for verilog code generation, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 8. [12] S. Thakur et al., Autochip: Automating hdl generation using llm feedback, arXiv preprint arXiv:2311.04887, 2023. [13] Y. Lu et al., Rtllm: An open-source benchmark for design rtl generation with large language model, in 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 2024, pp. 722 727. [14] S. Thakur et al., Verigen: A large language model for verilog code generation, ACM TODAES, 2023. [15] R. Kande et al., Llm-assisted generation of hardware assertions, arXiv preprint arXiv:2306.14027, 2023. [16] W. Fang et al., Assertllm: Generating and evaluating hardware verifica- tion assertions from design specifications via multi-llms, arXiv preprint arXiv:2402.00386, 2024. [17] R. Qiu et al., Autobench: Automatic testbench generation and evaluation using llms for hdl design, in Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD, 2024, pp. 1 10. [18] J. Bhandari et al., Llm-aided testbench generation and bug detection for finite-state machines, arXiv preprint arXiv:2406.17132, 2024. [19] H. Wu et al., Chateda: A large language model powered autonomous agent for eda, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [20] M. Liu et al., Chipnemo: Domain-adapted llms for chip design, arXiv preprint arXiv:2311.00176, 2023. [21] S. Liu et al., Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, 2024. Available: [22] E. Nijkamp et al., Codegen2: Lessons for training llms on programming and natural languages, arXiv preprint arXiv:2305.02309, 2023. [23] S. Muralidharan et al., Compact language models via pruning and knowledge distillation, Advances in Neural Information Processing Systems, vol. 37, pp. 41 076 41 102, 2024. [24] A. Q. Jiang et al., Mistral 7b, 2023. [25] A. Abouelenin et al., Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras, 2025. [26] H. Touvron et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [27] Meta, Introducing Llama 3.1: Our most capable models to date, 2024, accessed: 2025-02-27. Available: meta-llama-3-1 [28] A. Radford et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [29] T. Brown et al., Language models are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020. [30] A. Hurst et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [31] D. Guo et al., Deepseek-coder: When the large language model meets programming the rise of code intelligence, arXiv preprint arXiv:2401.14196, 2024. [32] Qwen, Code with CodeQwen1.5, 2024, accessed: 2025-02-27. Available: [33] E. Nijkamp et al., Codegen: An open large language model for code with multi-turn program synthesis, 2023. Available: [34] J. Blocklove et al., Chip-chat: Challenges and opportunities in conversa- tional hardware design, in 2023 ACM IEEE 5th Workshop on Machine Learning for CAD (MLCAD). IEEE, Sep. 2023. [35] Y. Fu et al., Gpt4aigchip: Towards next-generation ai accelerator design automation via large language models, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 9. [36] K. Chang et al., Chipgpt: How far are we from natural language hardware design, arXiv preprint arXiv:2305.14019, 2023. [37] Y. Li et al., An open-source data contamination report for large language models, in Findings of the Association for Computational Linguistics: EMNLP 2024, Y. Al-Onaizan et al., Eds. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 528 541. Available: [38] R. Aiyappa et al., Can we trust the evaluation on ChatGPT? in Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), A. Ovalle et al., Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 47 54. Available: [39] A. Chowdhery et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research, vol. 24, no. 240, pp. 1 113, 2023. [40] S. Golchin and M. Surdeanu, Time travel in llms: Tracing data con- tamination in large language models, arXiv preprint arXiv:2308.08493, 2023. [41] K. Zhu et al., Dyval: Graph-informed dynamic evaluation of large language models, arXiv preprint arXiv:2309.17167, 2023. [42] F. Ranaldi et al., Investigating the impact of data contamination of large language models in text-to-sql translation, arXiv preprint arXiv:2402.08100, 2024. [43] Y. Dong et al., Generalization or memorization: Data contamination and trustworthy evaluation for large language models, arXiv preprint arXiv:2402.15938, 2024. [44] W. Shi et al., Detecting pretraining data from large language models, arXiv preprint arXiv:2310.16789, 2023. [45] Y. Li et al., Avoiding data contamination in language model evalua- tion: Dynamic test construction with latest materials, arXiv preprint arXiv:2312.12343, 2023. [46] N. Chandran et al., Private benchmarking to prevent contamina- tion and improve comparative evaluation of llms, arXiv preprint arXiv:2403.00393, 2024. [47] N. Jain et al., Livecodebench: Holistic and contamination free evaluation of large language models for code, arXiv preprint arXiv:2403.07974, 2024. [48] M. Riddell et al., Quantifying contamination in evaluating code genera- tion capabilities of language models, arXiv preprint arXiv:2403.04811, 2024. [49] Z. Pei et al., Betterv: Controlled verilog generation with discriminative guidance, arXiv preprint arXiv:2402.03375, 2024. [50] Y. Zhao et al., Codev: Empowering llms for verilog generation through multi-level summarization, arXiv preprint arXiv:2407.10424, 2024. [51] L. L. Mankali et al., Rtl-breaker: Assessing the security of llms against backdoor attacks on hdl code generation, arXiv preprint arXiv:2411.17569, 2024. [52] R. Taori et al., Stanford alpaca: An instruction-following llama model, alpaca, 2023. [53] DeepSeek-AI, Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Available: 12948 [54] Anthropic, Claude 3.7 Sonnet and Claude Code, 2025, accessed: 2025- 02-28. Available: [55] OpenAI, OpenAI O3 Mini, 2024, accessed: 2025-02-25. Available: [56] S. Nie et al., Large language diffusion models, arXiv preprint arXiv:2502.09992, 2025.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2503.13572v3 [cs.AR] 12 Jun 2025 VeriContaminated: Assessing LLM-Driven Verilog Coding for Data Contamination Zeng Wang , Minghao Shao , Jitendra Bhandari , Likhitha Mankali , Ramesh Karri , Ozgur Sinanoglu , Muhammad Shafique , Johann Knechtel NYU Tandon School of Engineering, USA NYU Abu Dhabi, UAE Email:{zw3464, shao.minghao, jb7410, likhitha.mankali, rkarri, ozgursin, muhammad.shafique, Abstract Large Language Models (LLMs) have revolutionized code generation, achieving exceptional results on various estab- lished benchmarking frameworks. However, concerns about data contamination where benchmark data inadvertently leaks into pre-training or fine-tuning datasets raise questions about the validity of these evaluations. While this issue is known, limiting the industrial adoption of LLM-driven software engineering, hardware coding has received little to no attention regarding these risks. For the first time, we analyze state-of-the-art (SOTA) evaluation frameworks for Verilog code generation (VerilogEval and RTLLM), using established methods for contamination detection (CCD and Min-K Prob). We cover SOTA commercial and open-source LLMs (CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5), in baseline and fine-tuned models (RTLCoder and Verigen). Our study confirms that data contamination is a critical concern. We explore mitigations and the resulting trade- offs for code quality vs fairness (i.e., reducing contamination toward unbiased benchmarking). Index Terms LLMs, Hardware Design, Data Contamination I. INTRODUCTION Large Language Models (LLMs) like GPT-4 [1] and Gem- ini [2] have exhibited remarkable capabilities in compre- hending text semantics and generating code across different programming languages.\n\n--- Segment 2 ---\nIndex Terms LLMs, Hardware Design, Data Contamination I. INTRODUCTION Large Language Models (LLMs) like GPT-4 [1] and Gem- ini [2] have exhibited remarkable capabilities in compre- hending text semantics and generating code across different programming languages. However, a critical challenge for evaluating these models is data contamination, which occurs when benchmarks inadvertently include samples from the model s pre-training corpus [3]. Such a scenario would unfairly inflate performance estimates and undermine the reliability of comparative assessments. With impressive performance metrics reported for many LLMs, the undisclosed nature of the pre- training datasets, common even among open-source models [4], raises significant concerns for such contamination [5] [8]. LLMs have also shown considerable promise in the domain of hardware design [10] [20]. However, as in the software domain, data contamination poses a challenge when establish- ing LLMs for hardware designs. The performance and utility of these domain-specific LLMs depend on the availability of large, high-quality datasets tailored to the complexities of hardware design. Given the relative sparsity of such data, it is easy to see that evaluation benchmarks may (inadvertently) include some of the same samples that were part of the pre-training data. Figure 1 confirms this hypothesis through Authors contributed equally to this research. Top-1 Score 80 Top-1 Score 90 Top-1 Score 100 Top-1 Score 80 Top-1 Score 90 Top-1 Score 100 0.00 20.00 40.00 60.00 RTLLM Benchmark VerlogEval Benchmark RTLCoder Verigen Fig. 1: Top-1 scores for Verilog Abstract Syntax Tree (AST) similarity, powered by Dolos [9], between golden solutions and closest designs in training datasets. RTLCoder and Verigen show significant similarities to the RTLLM and VerilogEval benchmarks, suggesting severe data contamination. assessment of code similarity for the state-of-the-art (SOTA) datasets of Verigen [14] and RTLCoder [21] against the SOTA benchmarks VerilogEval [11] and RTLLM [13]. Such large overlaps between training and testing data are highly likely to induce data contamination and thereby skew benchmarking results. Addressing this challenge is crucial for fair ranking of LLMs for hardware design. We address this challenge for the first time.\n\n--- Segment 3 ---\nAddressing this challenge is crucial for fair ranking of LLMs for hardware design. We address this challenge for the first time. We study data contamination for many SOTA LLMs (CodeGen2.5 [22], Minitron 4b [23], Mistral 7b [24], phi-4 mini [25], LLaMA- {1,2,3.1} [4], [26], [27], GPT-{2,3.5,4o} [28] [30], Deepseek- Coder [31], and CodeQwen 1.5 [32]), across different datasets before and after fine-tuning. For benchmarking, we use the SOTA tools VerilogEval and RTLLM. Key contributions of our work are the following. 1) Our analysis of LLM data contamination across Verilo- gEval and RTLLM benchmarks shows near 100 rates in GPT-3.5 and GPT-4o, but lower in older models. 2) We analyze detection methods and illustrate how contam- ination trends vary across different experimental settings. 3) We study the trade-off between code quality and contami- nation mitigation. That evaluation shows the potential of mitigating hardware code contamination. II. BACKGROUND A. LLMs for Hardware Design LLMs have demonstrated impressive capabilities in code generation [1], [4], which has extended interest toward their application in hardware design. LLMs have been tailored for a variety of tasks, including Verilog code generation [11] [14], assertion generation [15], [16], testbench generation [17], [18], and scripting for electronic design automation [19], [20]. For example, in [14], researchers fine-tuned CodeGen- 16B [33] using a comprehensive training corpus of Verilog codes sourced from GitHub and textbooks. ChipNemo [20] leverages LLaMA2 [4] as a foundation, refining it using public datasets and NVIDIA s proprietary designs. RTLCoder [21] constructs instruction-code pairs using GPT to generate training data from a curated pool of keywords and source codes. Prompt-engineering strategies have been introduced in [34] [36] to enhance code generation performance.\n\n--- Segment 4 ---\nRTLCoder [21] constructs instruction-code pairs using GPT to generate training data from a curated pool of keywords and source codes. Prompt-engineering strategies have been introduced in [34] [36] to enhance code generation performance. To evaluate these capabilities, frameworks like VerilogEval [11] and RTLLM [13] have been developed, which assess the functional and syntactic correctness of Verilog code produced by these models. B. Data Contamination in LLMs Data contamination refers to cases where test data have been included in the model s training data [5] [8]. Such a scenario, be it inadvertently or on purpose, leads to the models performing exceptionally well on the leaked test data. It has been shown that data contamination grows rapidly through time for various LLM models [37], especially in ChatGPT [38]. Detection: A challenge for detection of data contamination is the magnitude of pre-training data, which renders full disclosure or cross-verification impractical, as the resources required to audit every data point are prohibitive [4], [39]. Still, practical means for detection exist, as outlined next. [40] proposed the identification of potential contamination at the instance level, further using this information to assess wider contamination at the partition level to identify data contamination within LLMs. [41] leverages the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities to detect data contamination. [4] reported a significant performance gap for LLaMA-2 70B on clean vs dirty sets of benchmarks. [42] investigated the impact of contamination on the performance of GPT-3.5 in for text-to-SQL code-generation. [43] proposed contamination detection CDD via analyzing the peak token-level edit distance distributions of LLMs. [44] introduced Min-K Prob, evaluating the k tokens with minimum probabilities, based on hypothesis that an unseen example is likely to contain a few outlier words with low probabilities. Mitigation: Established approaches for mitigation seek to either dynamically generate new test data or withold reference test data, as outlined next. TED [43] excludes peakedness and removes duplication to restore uncontaminated inferrences. [45] proposed dynamic and time-sensitive test construction. [46] proposed private benchmarking, a solution where test datasets are private and models are evaluated without revealing the test data to the model.\n\n--- Segment 5 ---\n[45] proposed dynamic and time-sensitive test construction. [46] proposed private benchmarking, a solution where test datasets are private and models are evaluated without revealing the test data to the model. Similarly, [47] proposed a comprehensive and contamination-free evaluation of LLMs for coding, which collects new problems over time from contests and other sources. [48] studied data contamination of popular code generation benchmarks and quantified their overlap with pre- training corpus through surface- and semantic-level matching. III. EVALUATION A. Experiment Setup Our investigation has two phases: (1) evaluating contamina- tion across multiple foundation models using benchmarks to establish baseline Verilog code contamination levels and (2) deliberately contaminating a clean model to assess TED s [43] mitigation efficacy. All experiments used an Nvidia A100 GPU (80GB) with CUDA 12.2. This section details the methodology. Models: We use widely recognized baseline models with different sizes and sources. We pick CodeGen2.5, Minitron 4b, Mistral 7b, phi-4 mini, LLaMA-{1,2,3.1}, GPT-{2,3.5,4o}, Deepseek-Coder, and CodeQwen 1.5 for evaluation [49], [50]. This selection encompasses models ranging from earlier itera- tions to state-of-the-art LLM families, balancing commercial and open-source offerings. Fine-tuning Setup: To establish a fair setting for contam- ination assessment and to enable controllable mitigation, we simulate data contamination for LLaMA-3.1-8B. We specifi- cally chose this model due to its moderate contamination rate (Section III-C). We fine-tuned two separate instances on distinct training datasets: one using the 55M RTLCoder [21] dataset within its native training framework and another using the 78M filtered Verigen [14], [51] via the Alpaca [52] library. After experimenting with various hyperparameter configurations, we determined that epoch 3 and learning rate (lr) 1e 5 with the Adam optimizer produced the highest contamination rate, creating optimal conditions for observing contamination effects. For inference, we used temperature (temp) 0.8, top-p 0.95, and maximum context length 2048.\n\n--- Segment 6 ---\nAfter experimenting with various hyperparameter configurations, we determined that epoch 3 and learning rate (lr) 1e 5 with the Adam optimizer produced the highest contamination rate, creating optimal conditions for observing contamination effects. For inference, we used temperature (temp) 0.8, top-p 0.95, and maximum context length 2048. Evaluation Setup: We evaluate model performance using RTLLM [13] and VerilogEval [11] benchmark sets. We will analyze the trade-off between contamination mitigation and Verilog generation accuracy. Using the setup in [43] [44], we ran 50 sample inferences per model for each problem in VerilogEval and RTLLM for CDD and Min-K Prob (K 20) evaluations. For fine-tuned models, we used the inferred samples to evaluate functionality of changes during TED. B. Metrics To evaluate contamination in current test benchmarks, we define contamination rates as the proportion of contaminated problems within the problem set. We identify contaminated problems using two methods, CDD [43] and Min-K Prob [44], and by varying parameters described in Section III-C. These approaches define contamination differently, providing complementary insights. CDD uses token-level edit distance to identify repetition across inference distributions, while Min-K Prob assesses contamination by examining rare token probabilities, with higher values indicating contamination. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (a) RTLLM evaluation for different α values.\n\n--- Segment 7 ---\nCDD uses token-level edit distance to identify repetition across inference distributions, while Min-K Prob assesses contamination by examining rare token probabilities, with higher values indicating contamination. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (a) RTLLM evaluation for different α values. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (b) VerilogEval evaluation for different α values. Fig. 2: Model contamination evaluation using CCD. Parameter variations reveal contamination rate patterns. We analyze the impact of data contamination on model performance in Verilog generation in Section III-D. For this analysis, we employ the metric to evaluate the accuracy of the generated Verilog code.\n\n--- Segment 8 ---\nParameter variations reveal contamination rate patterns. We analyze the impact of data contamination on model performance in Verilog generation in Section III-D. For this analysis, we employ the metric to evaluate the accuracy of the generated Verilog code. Additionally, we assess the impact of the mitigation algorithm TED [43] on model accuracy. In Section VI, we further evaluate TED s effectiveness by comparing results before and after removing the samples most likely to be contaminated during inference. C. Contamination Evaluation This section evaluates contamination using two detection methods, CDD and Min-K Prob. We experiment with RTLLM and VerilogEval by varying threshold values. Our goal is to reveal data memorization and generalization, and to contrast commercial and open-source models. Fig. 2 compares contamination rates detected by CDD across different α values, introduced in [43] as a similarity threshold. As α increases, contamination rates rise, indicating stricter detection criteria. Commercial models such as GPT-3.5 and GPT-4o exhibit higher initial contamination rates approaching 100 suggesting higher memorization or weaker generaliza- tion. In contrast, open-source models like GPT-2 and LlaMA 1 maintain lower contamination rates. Interestingly, the small model scale of phi-4 mini does not result in lower contamination 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate min_20_prob_acc_list codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (a) RTLLM evaluation for different threshold values.\n\n--- Segment 9 ---\nIn contrast, open-source models like GPT-2 and LlaMA 1 maintain lower contamination rates. Interestingly, the small model scale of phi-4 mini does not result in lower contamination 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate min_20_prob_acc_list codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (a) RTLLM evaluation for different threshold values. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Threshold Value (0.0 - 1.0) with step 0.1 0.0 0.2 0.4 0.6 0.8 1.0 Contamination Rate min_20_prob_acc_list codegen25-7b-instruct_P llama-7b Llama-2-7b-hf Llama-3.1-8B-Instruct gpt2 gpt-3.5-turbo-0125 gpt-4o-2024-11-20 deepseek-coder-6.7b-instruct CodeQwen1.5-7B-Chat Minitron-4B-Base Mistral-7B-Instruct-v0.2 Phi-4-mini-instruct (b) VerilogEval evaluation for different threshold values. Fig. 3: Model contamination evaluation using Min-K prob. rates. RTLLM s more detailed prompts correlate with lower contamination rates, indicating stronger generalization under the same threshold settings.\n\n--- Segment 10 ---\nrates. RTLLM s more detailed prompts correlate with lower contamination rates, indicating stronger generalization under the same threshold settings. Both benchmarks exceed a 90 contamination rate at α 0.9. VerilogEval presents higher overall contamination, suggesting its evaluation likely relies on data memorized during these models pre-training. In short, CDD effectively evaluates RTL-based contamination at lower α. Results demonstrate that earlier open-source models appear less prone to contamination, while commercial models display higher contamination rates under the same α value. Fig. 3 illustrates contamination rates detected by Min-K Prob across thresholds (T) ranging from 0.0 to 1.0, as in [44]. Higher thresholds impose stricter criteria for identifying model contamination, which is reflected in the figure by an overall decline in contamination rates as thresholds increase. As before, GPT-3.5 and GPT-4o exhibit contamination spikes, contrasting with earlier models, such as GPT-2 and LLaMA 1. Min-K Prob analysis shows that RTLLM has high contamination peaks exceeding 0.8 in some cases while VerilogEval plateaus near 0.6. Since Min-K Prob relies on single-inference probability distribution, the detailed prompts in RTLLM may cause models to generate tokens with higher log probabilities, potentially explaining the higher contamination rates for the RTLLM benchmark compared to VerilogEval.\n\n--- Segment 11 ---\nMin-K Prob analysis shows that RTLLM has high contamination peaks exceeding 0.8 in some cases while VerilogEval plateaus near 0.6. Since Min-K Prob relies on single-inference probability distribution, the detailed prompts in RTLLM may cause models to generate tokens with higher log probabilities, potentially explaining the higher contamination rates for the RTLLM benchmark compared to VerilogEval. Model CDD ( ) Min-K Prob ( ) Pass Rate ( ) RTLLM VerilogEval RTLLM VerilogEval RTLLM VerilogEval CodeGen2.5 14.00 1.28 4.00 5.77 2.00 2.56 Minitron 4b 38.00 2.56 0.00 0.00 2.00 1.28 Mistral 7b v0.2 30.00 10.90 24.00 21.15 2.00 1.28 phi-4 mini 54.00 22.43 14.00 10.90 10.00 4.49 LLaMA 1 14.00 2.56 0.00 0.64 0.00 1.28 LLaMA 2 24.00 17.95 84.00 67.95 0.00 3.85 LLaMA 3.1 8b 22.00 33.33 84.00 52.56 20.00 9.62 GPT-2 4.00 6.41 0.00 0.00 0.00 0.00 GPT-3.5 100.00 98.72 100.00 97.44 32.00 36.54 GPT-4o 100.00 100.00 98.00 99.36 44.00 60.26 DeepSeek-Coder 58.00 26.92 8.00 9.62 42.00 19.87 CodeQwen 1.5 78.00 25.64 10.00 4.49 26.00 17.95 Contamination: High ( 75 ) Mid (25-75 ) Low ( 25 ) Pass Rate: High ( 50 ) Mid (20-50 ) Low ( 20 ) TABLE I: Model accuracy and contamination rate analysis across metrics with CDD α 0.05 and Min-K Prob T 0.55. These results highlight methodological discrepancies in contamination detection: Min-K Prob and CDD yield divergent contamination estimates, underscoring how detection frameworks influence observed outcomes.\n\n--- Segment 12 ---\nModel CDD ( ) Min-K Prob ( ) Pass Rate ( ) RTLLM VerilogEval RTLLM VerilogEval RTLLM VerilogEval CodeGen2.5 14.00 1.28 4.00 5.77 2.00 2.56 Minitron 4b 38.00 2.56 0.00 0.00 2.00 1.28 Mistral 7b v0.2 30.00 10.90 24.00 21.15 2.00 1.28 phi-4 mini 54.00 22.43 14.00 10.90 10.00 4.49 LLaMA 1 14.00 2.56 0.00 0.64 0.00 1.28 LLaMA 2 24.00 17.95 84.00 67.95 0.00 3.85 LLaMA 3.1 8b 22.00 33.33 84.00 52.56 20.00 9.62 GPT-2 4.00 6.41 0.00 0.00 0.00 0.00 GPT-3.5 100.00 98.72 100.00 97.44 32.00 36.54 GPT-4o 100.00 100.00 98.00 99.36 44.00 60.26 DeepSeek-Coder 58.00 26.92 8.00 9.62 42.00 19.87 CodeQwen 1.5 78.00 25.64 10.00 4.49 26.00 17.95 Contamination: High ( 75 ) Mid (25-75 ) Low ( 25 ) Pass Rate: High ( 50 ) Mid (20-50 ) Low ( 20 ) TABLE I: Model accuracy and contamination rate analysis across metrics with CDD α 0.05 and Min-K Prob T 0.55. These results highlight methodological discrepancies in contamination detection: Min-K Prob and CDD yield divergent contamination estimates, underscoring how detection frameworks influence observed outcomes. The variability suggests that contamination metrics are sensitive to RTL design characteristics and its promptings, emphasizing the need for RTL-aware interpretation of such evaluations. D. Impact of Contamination on Accuracy Table I presents the performance of models in terms of functional correctness, accuracy, and contamination rate.\n\n--- Segment 13 ---\nThe variability suggests that contamination metrics are sensitive to RTL design characteristics and its promptings, emphasizing the need for RTL-aware interpretation of such evaluations. D. Impact of Contamination on Accuracy Table I presents the performance of models in terms of functional correctness, accuracy, and contamination rate. The evaluations were conducted using the default setup from [43] with a CDD α of 0.05 and a Min-K Prob threshold of 0.55. From the CDD contamination rate results on VerilogEval and RTLLM, we observe that GPT-3.5 and GPT-4o exhibit nearly 100 contamination, which explains their superior performance in Verilog code generation. In contrast, DeepSeek-Coder and CodeQwen 1.5 have lower contamination rates, corresponding to their reduced performance in Verilog generation. phi-4 mini has a comparable functionality accuracy yet imposes only a small model size. Models such as CodeGen2.5, LLaMA 1, and GPT-2 show even lower CDD values, aligning with their significantly weaker performance. However, despite LLaMA 2 having a similar CDD value as LLaMA 3.1, its is noticeably lower than that of LLaMA 3.1, indicating CDD does not work well for LLaMA 2. Mistral 7b and Minitron 4b have a medium contamination rate but lower performance in Min-K Prob captures the contamination rates of GPT-3.5 and GPT-4o due to their high log probability of generating previously seen designs. However, it is weak when distinguishing contamination levels among other models, making it less reliable for fine-grained comparisons. As shown in Figure 3, it illustrates the decreasing trend of contamination rates as the threshold value increases. IV. CONTAMINATION CASE STUDIES Here, we present two contamination case studies using benchmark tasks from VerilogEval and RTLLM datasets. Each task has three parts: (1) a natural language prompt describing the hardware design objective, (2) a syntactic Verilog code template with functional gaps, and (3) a reference implementation serving as the ground truth. In each case study, we analyze a contaminated inference output from the fine-tuned LLaMA 3.1 model alongside a clean code sample generated by baseline LLaMA 3.1.\n\n--- Segment 14 ---\nEach task has three parts: (1) a natural language prompt describing the hardware design objective, (2) a syntactic Verilog code template with functional gaps, and (3) a reference implementation serving as the ground truth. In each case study, we analyze a contaminated inference output from the fine-tuned LLaMA 3.1 model alongside a clean code sample generated by baseline LLaMA 3.1. By contrasting their structural and functional alignment with the ground truth, this analysis highlights differences in code quality due to contamination. In Case 1, we see clear differences between code reflecting memorized patterns and code generated more autonomously. The contaminated output faithfully reproduces the memorized shift register design, using identical bit concatenation syntax ({shift reg[2:0], in}) and correct index-based assignment (shift reg[3]), thereby satisfying the ground-truth functionality. In contrast, the uncontaminated version demonstrates incorrect implementation through reverse-order bit concatenation ({in, shift reg[3:1]}) and a inappropriate multi-bit output port assignment (wire [3:0] out), causing the syntactic errors. The contaminated code adheres to standard practices by handling resets and indexing signals in a conventional manner. Mean- while, the uncontaminated version s reversed shift direction highlight its lack of internalized shift register patterns. Even if contamination improves the code generation ability, the evaluation on this test turns out to be unfair. Case 1: VerilogEval Contamination Example Task: m2014 q4k Prompt: Implement a shift reg- ister with four D flops. Reset is active-low synchronous re- settable.\n\n--- Segment 15 ---\nCase 1: VerilogEval Contamination Example Task: m2014 q4k Prompt: Implement a shift reg- ister with four D flops. Reset is active-low synchronous re- settable. 1 module TopModule ( 2 input clk, 3 input resetn, 4 input in, 5 output out 6 ); Reference Code 1 module RefModule (input clk, input resetn, input in, output out); 2 reg [3:0] sr; 3 always (posedge clk) begin 4 if ( resetn) 5 sr 0; 6 else 7 sr {sr[2:0], in}; 8 end 9 assign out sr[3]; 10 endmodule Uncontaminated Output 1 module TopModule ( 2 input clk, 3 input resetn, 4 input in, 5 output out 6 ); 7 reg [3:0] shift_reg; 8 wire [3:0] out; 9 10 always (posedge clk or negedge resetn) 11 if (!resetn) 12 shift_reg 4 b0; 13 else 14 shift_reg {in, shift_reg[3:1]}; 15 16 assign out shift_reg; 17 endmodule Contaminated Output 1 module TopModule ( 2 input clk, 3 input resetn, 4 input in, 5 output out 6 ); 7 reg [3:0] shift_reg; 8 always (posedge clk) begin 9 if (resetn 0) begin 10 shift_reg 4 b0; 11 end else begin 12 shift_reg {shift_reg[2:0], in}; 13 end 14 end 15 assign out shift_reg[3]; 16 endmodule Case 2 highlights RTLLM s adder-8bit implementation, showcasing a clear divergence between its reasoning and the memorization of patterns. In this case, the contaminated output reproduces a memorized code by constructing adder 8bit hierarchically by leveraging its self-defined full adder module. In contrast, the clean version, relying solely on its internal understanding, attempts a direct approach using hardware- oriented bitwise operations and Boolean algebra to compute cout and sum. But it ultimately fails to propagate the carry and assign the sum correctly. While the contaminated variant achieves the correct implementation echoing the training syntax from fine-tuning it raises important questions about comprehension versus memorization.\n\n--- Segment 16 ---\nBut it ultimately fails to propagate the carry and assign the sum correctly. While the contaminated variant achieves the correct implementation echoing the training syntax from fine-tuning it raises important questions about comprehension versus memorization. Case 2: RTLLM Contamination Example Task: adder-8bit Prompt: Please act as a professional verilog designer. Implement module of an 8-bit adder with multiple bit-level adders in combinational logic. Implementation: The module utilizes a series of bit-level adders (full adders) to perform the addition operation. Give me the complete code. 1 Module name: 2 adder_8bit 3 Input ports: 4 a[7:0]: 8-bit input operand A 5 b[7:0]: 8-bit input operand B; cin: Carry-in input. 6 Output ports: 7 sum[7:0]: 8-bit output representing the sum of A and B 8 cout: Carry-out output. Reference Code 1 module verified_adder_8bit(input [7:0] a, b, input cin, output [7:0] sum, output cout); 2 wire [8:0] c; 3 full_adder FA0 (.a(a[0]), .b(b[0]), .cin(cin), .sum(sum[0]), .cout(c[0])); 4 ......\n\n--- Segment 17 ---\n6 Output ports: 7 sum[7:0]: 8-bit output representing the sum of A and B 8 cout: Carry-out output. Reference Code 1 module verified_adder_8bit(input [7:0] a, b, input cin, output [7:0] sum, output cout); 2 wire [8:0] c; 3 full_adder FA0 (.a(a[0]), .b(b[0]), .cin(cin), .sum(sum[0]), .cout(c[0])); 4 ...... Folded Implementations 5 full_adder FA7 (.a(a[7]), .b(b[7]), .cin(c[6]), .sum(sum[7]), .cout(c[7])); 6 assign cout c[7]; 7 endmodule 8 9 module full_adder (input a, b, cin, output sum, cout); 10 assign {cout, sum} a b cin; 11 endmodule Uncontaminated Output 1 module adder_8bit(input [7:0] a, b, input cin, output [7:0] sum, output cout); 2 adder_8bit 3 sum[7] a[7] ˆ b[7] ˆ cin; 4 cout (a[7] b[7] cin) (a[7] b[7] !cin) (a[7] cin !b[7]) (b[7] cin !a[7]); 5 sum[6:0] {a[6] b[6] cout, a[5] b[5] cout, a[4] b[4] cout, a[3] b[3] cout, a[2] b[2] cout, a[1] b[1] cout, a[0] b[0] cout}; 6 endmodule Contaminated Output 1 module adder_8bit(input [7:0] a, input [7:0] b, input cin, output [7:0] sum, output cout); 2 wire [7:0] carry; 3 wire [7:0] sum_temp; 4 full_adder fa0(a[0], b[0], cin, sum_temp[0], carry[0]); 5 ......\n\n--- Segment 18 ---\nReference Code 1 module verified_adder_8bit(input [7:0] a, b, input cin, output [7:0] sum, output cout); 2 wire [8:0] c; 3 full_adder FA0 (.a(a[0]), .b(b[0]), .cin(cin), .sum(sum[0]), .cout(c[0])); 4 ...... Folded Implementations 5 full_adder FA7 (.a(a[7]), .b(b[7]), .cin(c[6]), .sum(sum[7]), .cout(c[7])); 6 assign cout c[7]; 7 endmodule 8 9 module full_adder (input a, b, cin, output sum, cout); 10 assign {cout, sum} a b cin; 11 endmodule Uncontaminated Output 1 module adder_8bit(input [7:0] a, b, input cin, output [7:0] sum, output cout); 2 adder_8bit 3 sum[7] a[7] ˆ b[7] ˆ cin; 4 cout (a[7] b[7] cin) (a[7] b[7] !cin) (a[7] cin !b[7]) (b[7] cin !a[7]); 5 sum[6:0] {a[6] b[6] cout, a[5] b[5] cout, a[4] b[4] cout, a[3] b[3] cout, a[2] b[2] cout, a[1] b[1] cout, a[0] b[0] cout}; 6 endmodule Contaminated Output 1 module adder_8bit(input [7:0] a, input [7:0] b, input cin, output [7:0] sum, output cout); 2 wire [7:0] carry; 3 wire [7:0] sum_temp; 4 full_adder fa0(a[0], b[0], cin, sum_temp[0], carry[0]); 5 ...... Folded Implementations 6 full_adder fa7(a[7], b[7], carry[6], sum_temp[7], cout); 7 assign sum sum_temp; 8 endmodule 9 10 module full_adder(input a, input b, input cin, output sum, output cout); 11 assign sum a ˆ b ˆ cin; 12 assign cout (a b) (a cin) (b cin); 13 endmodule 0.0 0.2 0.4 0.6 0.8 1.0 Value (0.0 - 1.0) with step 0.1 0 20 40 60 80 100 Contamination Rate ( ) CDD Contamination Rate on RTLLM 0.0 0.1 0.2 0.3 96 98 100 LLaMA 3.1-70B and 405B show similar RTLLM contamination LLaMA 3.1-8B LLaMA 3.1-70B LLaMA 3.1-405B 0.0 0.2 0.4 0.6 0.8 1.0 Value (0.0 - 1.0) with step 0.1 0 20 40 60 80 100 Contamination Rate ( ) CDD Contamination on VerilogEval 0.0 0.1 0.2 0.3 96 98 100 (1).LLaMA 3.1-70B and 405B show similar VerilogEval contamination.\n\n--- Segment 19 ---\nFolded Implementations 5 full_adder FA7 (.a(a[7]), .b(b[7]), .cin(c[6]), .sum(sum[7]), .cout(c[7])); 6 assign cout c[7]; 7 endmodule 8 9 module full_adder (input a, b, cin, output sum, cout); 10 assign {cout, sum} a b cin; 11 endmodule Uncontaminated Output 1 module adder_8bit(input [7:0] a, b, input cin, output [7:0] sum, output cout); 2 adder_8bit 3 sum[7] a[7] ˆ b[7] ˆ cin; 4 cout (a[7] b[7] cin) (a[7] b[7] !cin) (a[7] cin !b[7]) (b[7] cin !a[7]); 5 sum[6:0] {a[6] b[6] cout, a[5] b[5] cout, a[4] b[4] cout, a[3] b[3] cout, a[2] b[2] cout, a[1] b[1] cout, a[0] b[0] cout}; 6 endmodule Contaminated Output 1 module adder_8bit(input [7:0] a, input [7:0] b, input cin, output [7:0] sum, output cout); 2 wire [7:0] carry; 3 wire [7:0] sum_temp; 4 full_adder fa0(a[0], b[0], cin, sum_temp[0], carry[0]); 5 ...... Folded Implementations 6 full_adder fa7(a[7], b[7], carry[6], sum_temp[7], cout); 7 assign sum sum_temp; 8 endmodule 9 10 module full_adder(input a, input b, input cin, output sum, output cout); 11 assign sum a ˆ b ˆ cin; 12 assign cout (a b) (a cin) (b cin); 13 endmodule 0.0 0.2 0.4 0.6 0.8 1.0 Value (0.0 - 1.0) with step 0.1 0 20 40 60 80 100 Contamination Rate ( ) CDD Contamination Rate on RTLLM 0.0 0.1 0.2 0.3 96 98 100 LLaMA 3.1-70B and 405B show similar RTLLM contamination LLaMA 3.1-8B LLaMA 3.1-70B LLaMA 3.1-405B 0.0 0.2 0.4 0.6 0.8 1.0 Value (0.0 - 1.0) with step 0.1 0 20 40 60 80 100 Contamination Rate ( ) CDD Contamination on VerilogEval 0.0 0.1 0.2 0.3 96 98 100 (1).LLaMA 3.1-70B and 405B show similar VerilogEval contamination. (2).70B slightly cleaner than 405B.\n\n--- Segment 20 ---\nFolded Implementations 6 full_adder fa7(a[7], b[7], carry[6], sum_temp[7], cout); 7 assign sum sum_temp; 8 endmodule 9 10 module full_adder(input a, input b, input cin, output sum, output cout); 11 assign sum a ˆ b ˆ cin; 12 assign cout (a b) (a cin) (b cin); 13 endmodule 0.0 0.2 0.4 0.6 0.8 1.0 Value (0.0 - 1.0) with step 0.1 0 20 40 60 80 100 Contamination Rate ( ) CDD Contamination Rate on RTLLM 0.0 0.1 0.2 0.3 96 98 100 LLaMA 3.1-70B and 405B show similar RTLLM contamination LLaMA 3.1-8B LLaMA 3.1-70B LLaMA 3.1-405B 0.0 0.2 0.4 0.6 0.8 1.0 Value (0.0 - 1.0) with step 0.1 0 20 40 60 80 100 Contamination Rate ( ) CDD Contamination on VerilogEval 0.0 0.1 0.2 0.3 96 98 100 (1).LLaMA 3.1-70B and 405B show similar VerilogEval contamination. (2).70B slightly cleaner than 405B. (3).Both exceed 8B contamination. (4).Model size appears to affect contamination levels. Fig. 4: Contamination Rate using CDD on RTLLM and VerilogEval V. MODEL SCALE IMPACTS ON CONTAMINATION We investigate how model scale impacts contamination by comparing CDD contamination rates across LLaMA 3.1 models of varying sizes (8B, 70B and 405B). Figure 4 reveals both 70B and 405B models maintain nearly 100 contamination on both benchmarks at α 0, with 405B showing 2.5 higher initial contamination than 70B. In contrast, the 8B model exhibits only 20 initial contamination, requiring higher α values to reach the levels of larger models. This suggests larger models covering more training data exhibit higher contamination rates, indicating a direct relationship between model scale and contamination.\n\n--- Segment 21 ---\nIn contrast, the 8B model exhibits only 20 initial contamination, requiring higher α values to reach the levels of larger models. This suggests larger models covering more training data exhibit higher contamination rates, indicating a direct relationship between model scale and contamination. Moreover, differentiating contamination levels between sufficiently large-scale models (i.e., 70B and 405B) requires stricter α values in CDD or more precise metrics. VI. MITIGATION EVALUATION Here, we evaluate TED [43] mitigation through controlled experiments with RTLCoder and Verigen datasets, measuring performance on RTLLM and VerilogEval benchmarks with adaptive threshold scaling. Our dual-arm approach isolates con- tamination effects through separate fine-tuning while simulating data leakage, enabling systematic assessment of mitigation effectiveness. 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Mitigation Ratio ( ) Diff: 58.00 Verigen gap: 58 at 20 versus RTLCoder Mitigation Rate on RTLLM RTLCoder Verigen 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Mitigation Ratio ( ) Diff: 27.56 (1).Max gap: 27.56 at 36 on VerilogEval. (2).Smaller than RTLLM. (3).Fewer task description clearly reduce gap size Mitigation Rate on VerilogEval Fig. 5: Mitigation Rate using TED on RTLLM and VerilogEval Figure 5 demonstrates how mitigation rates vary with top percentage exclusion thresholds (τ) across RTLLM and VerilogEval benchmarks, comparing RTLCoder and Verigen datasets. AST analysis in Figure 1 confirms RTLCoder exhibits higher contamination than Verigen. This is consistent with Figure 5, where Verigen-fine-tuned models achieve higher 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 2.07x S F avg. drop rate, steeper decline when 20 on RTLLM fine-tuned with RTLCoder Syntax Functional 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 1.90x S F avg.\n\n--- Segment 22 ---\nThis is consistent with Figure 5, where Verigen-fine-tuned models achieve higher 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 2.07x S F avg. drop rate, steeper decline when 20 on RTLLM fine-tuned with RTLCoder Syntax Functional 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 1.90x S F avg. drop rate, steeper decline when 40 on RTLLM fine-tuned with RTLCoder 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 2.66x S F avg. drop rate, steeper decline when 20. Less contaminated dataset mitigates with lower on RTLLM fine-tuned with Verigen 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) 2.38x S F avg. drop rate, steeper decline when 40 on RTLLM fine-tuned with Verigen Fig. 6: Impact of Pass Rate on TED Mitigation Following Contamination Simulation on LLaMA 3.1 by Varying τ for Top Exclusion on RTLLM. mitigation rates across most τ values for both benchmarks. These findings indicate that less contaminated datasets like Verigen respond more effectively to mitigations due to their distinctive distribution patterns, facilitating efficient suppression of memorized content during threshold-based filtering. Figure 6 shows the relation between filtering threshold τ and pass rate accuracy in TED s contamination mitigation on RTLLM. As τ increases, stricter data filtering is applied, causing the pass rate to decline. This drop reflects the removal of a larger portion of contaminated data during training or evaluation. The drop on syntax over functionality (i.e, S F average drop rate) is around 2 times, indicating syntax-level contamination is easier to mitigate. Lower τ values retain more data, preserving model performance. Higher thresholds enforce stricter filtering, making the approach adaptable to risk tolerances across deployment scenarios. Despite this inverse correlation, TED effectively mitigates hardware-level contamination. It maintains strong model performance even at higher τ values, especially at with τ 40. This resilience makes it suited for high-stakes environments where data integrity is essential. Adjusting τ allows balancing decontamination and predictive accuracy.\n\n--- Segment 23 ---\nThis resilience makes it suited for high-stakes environments where data integrity is essential. Adjusting τ allows balancing decontamination and predictive accuracy. Figure 7 illustrates the relation between the filtering threshold τ and pass rate accuracy in TED s application to VerilogEval. Accuracy of the Verigen-contaminated model decreases gradu- ally with higher τ, while the RTLCoder-contaminated model has a sharp decline at low τ values but stabilizes as τ grows. The results mirror those shown in Figure 6, with accuracy decreasing as τ increases, a pattern consistent with TED s successful removal of memorization-based correct inferences. However, two key differences emerge in the VerilogEval evaluation: 1) The baseline functionality accuracy is notably lower than in RTLLM, and 2) The accuracy curve exhibits greater irregularity, reflecting VerilogEval s higher problem 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Steeper decline when 20. Compared to RTLLM, limited details from VerilogEval make contamination easier to mitigate. on VerilogEval fine-tuned with RTLCoder Functional 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Smooth decline from initial. Accuracy dropped from 0 to 100. on VerilogEval fine-tuned with RTLCoder 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Steeper decline when 20. Less contaminated dataset mitigation with lower on VerilogEval fine-tuned with Verigen 0 20 40 60 80 100 Top Percentage Exclusion ( ) 0 20 40 60 80 100 Pass Rate ( ) Smooth decline from initial. With same on same benchmark, accuracy drop patterns show similarity. on VerilogEval fine-tuned with Verigen Fig. 7: Impact of Pass Rate on TED Mitigation Following Contamination Simulation on LLaMA 3.1 by Varying τ for Top Exclusion on VerilogEval. complexity. This increased difficulty suggests that individual inference response exert a more substantial influence on overall pass rates in VerilogEval compared to RTLLM. Consequently, accuracy declines more steeply as τ increases, particularly evident when comparing the fine-tuned with RTLCoder results between the two benchmarks (Figure 6).\n\n--- Segment 24 ---\nThis increased difficulty suggests that individual inference response exert a more substantial influence on overall pass rates in VerilogEval compared to RTLLM. Consequently, accuracy declines more steeply as τ increases, particularly evident when comparing the fine-tuned with RTLCoder results between the two benchmarks (Figure 6). By comparing the accuracy degradation in the RTLCoder and Verigen fine- tuned models, the Verigen model s accuracy declines sharply at lower values of τ, whereas the RTLCoder model continues to drop even at higher τ. Severe contamination requires a stricter mitigation threshold (i.e., τ in TED). VII. LIMITATION AND FUTURE WORK Despite the utility of contamination detectors, our findings of- fer insights from different techniques, highlighting the need for robust approaches tailored to hardware domain. While CDD and Min-K Prob perform well when detecting contamination in software, hardware-specific methods are lacking. Although TED mitigation reduces contamination, it can adversely affect Verilog code accuracy, suggesting hardware-focused mitigations that balance contamination control with functional performance are needed. Our experiments evaluated traditional foundation models and the emergening thinking models like DeepSeek- R1 [53], Claude 3.7 [54] and OpenAI-o3 [55]; one should explore how reasoning processes influence contamination evaluation and detection. Integrating model reasoning with contamination analysis could yield deeper insights. Current contamination evaluations focus on transformer-based models. Next-generation diffusion models (e.g., LLaDA [56]) warrant an investigation whether they exhibit similar contamination risks and require specialized detection and mitigations. VIII. CONCLUSION This study highlights data contamination in LLM-aided RTL generation, compromising the fairness of hardware evaluations. Using CDD and Min-K Prob, commercial models (e.g., GPT-3.5 and GPT-4o) exhibits higher contamination rates than open-source models. The TED mitigation reduces contamination with limited impact on RTL accuracy. Future work will expand benchmarks to other hardware languages and, via industry collaboration, establish reliable LLM-driven design tools. REFERENCES [1] OpenAI, GPT-4, Mar. 2023. Available: gpt-4 [2] G. Team et al., Gemini: a family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023.\n\n--- Segment 25 ---\n2023. Available: gpt-4 [2] G. Team et al., Gemini: a family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [3] S. Balloccu et al., Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source llms, arXiv preprint arXiv:2402.03927, 2024. [4] H. Touvron et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [5] I. Magar and R. Schwartz, Data contamination: From memorization to exploitation, arXiv preprint arXiv:2203.08242, 2022. [6] C. Xu et al., Benchmark data contamination of large language models: A survey, arXiv preprint arXiv:2406.04244, 2024. [7] S. Ishihara, Training data extraction from pre-trained language models: A survey, in Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), A. Ovalle et al., Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 260 275. Available: [8] H. Hu et al., Membership inference attacks on machine learning: A survey, ACM Comput. Surv., vol. 54, no. 11s, Sep. 2022. Available: [9] R. Maertens et al., Discovering and exploring cases of educational source code plagiarism with dolos, SoftwareX, vol. 26, p. 101755, 2024. [10] Z. Wang et al., Llms and the future of chip design: Unveiling security risks and building trust, in 2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI). IEEE, 2024, pp. 385 390. [11] M. Liu et al., Verilogeval: Evaluating large language models for verilog code generation, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 8.\n\n--- Segment 26 ---\nIEEE, 2023, pp. 1 8. [12] S. Thakur et al., Autochip: Automating hdl generation using llm feedback, arXiv preprint arXiv:2311.04887, 2023. [13] Y. Lu et al., Rtllm: An open-source benchmark for design rtl generation with large language model, in 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 2024, pp. 722 727. [14] S. Thakur et al., Verigen: A large language model for verilog code generation, ACM TODAES, 2023. [15] R. Kande et al., Llm-assisted generation of hardware assertions, arXiv preprint arXiv:2306.14027, 2023. [16] W. Fang et al., Assertllm: Generating and evaluating hardware verifica- tion assertions from design specifications via multi-llms, arXiv preprint arXiv:2402.00386, 2024. [17] R. Qiu et al., Autobench: Automatic testbench generation and evaluation using llms for hdl design, in Proceedings of the 2024 ACM IEEE International Symposium on Machine Learning for CAD, 2024, pp. 1 10. [18] J. Bhandari et al., Llm-aided testbench generation and bug detection for finite-state machines, arXiv preprint arXiv:2406.17132, 2024. [19] H. Wu et al., Chateda: A large language model powered autonomous agent for eda, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [20] M. Liu et al., Chipnemo: Domain-adapted llms for chip design, arXiv preprint arXiv:2311.00176, 2023. [21] S. Liu et al., Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, 2024. Available: [22] E. Nijkamp et al., Codegen2: Lessons for training llms on programming and natural languages, arXiv preprint arXiv:2305.02309, 2023.\n\n--- Segment 27 ---\n[21] S. Liu et al., Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, 2024. Available: [22] E. Nijkamp et al., Codegen2: Lessons for training llms on programming and natural languages, arXiv preprint arXiv:2305.02309, 2023. [23] S. Muralidharan et al., Compact language models via pruning and knowledge distillation, Advances in Neural Information Processing Systems, vol. 37, pp. 41 076 41 102, 2024. [24] A. Q. Jiang et al., Mistral 7b, 2023. [25] A. Abouelenin et al., Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras, 2025. [26] H. Touvron et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [27] Meta, Introducing Llama 3.1: Our most capable models to date, 2024, accessed: 2025-02-27. Available: meta-llama-3-1 [28] A. Radford et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [29] T. Brown et al., Language models are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020. [30] A. Hurst et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [31] D. Guo et al., Deepseek-coder: When the large language model meets programming the rise of code intelligence, arXiv preprint arXiv:2401.14196, 2024. [32] Qwen, Code with CodeQwen1.5, 2024, accessed: 2025-02-27. Available: [33] E. Nijkamp et al., Codegen: An open large language model for code with multi-turn program synthesis, 2023.\n\n--- Segment 28 ---\n[32] Qwen, Code with CodeQwen1.5, 2024, accessed: 2025-02-27. Available: [33] E. Nijkamp et al., Codegen: An open large language model for code with multi-turn program synthesis, 2023. Available: [34] J. Blocklove et al., Chip-chat: Challenges and opportunities in conversa- tional hardware design, in 2023 ACM IEEE 5th Workshop on Machine Learning for CAD (MLCAD). IEEE, Sep. 2023. [35] Y. Fu et al., Gpt4aigchip: Towards next-generation ai accelerator design automation via large language models, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 9. [36] K. Chang et al., Chipgpt: How far are we from natural language hardware design, arXiv preprint arXiv:2305.14019, 2023. [37] Y. Li et al., An open-source data contamination report for large language models, in Findings of the Association for Computational Linguistics: EMNLP 2024, Y. Al-Onaizan et al., Eds. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 528 541. Available: [38] R. Aiyappa et al., Can we trust the evaluation on ChatGPT? in Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), A. Ovalle et al., Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 47 54. Available: [39] A. Chowdhery et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research, vol. 24, no. 240, pp. 1 113, 2023. [40] S. Golchin and M. Surdeanu, Time travel in llms: Tracing data con- tamination in large language models, arXiv preprint arXiv:2308.08493, 2023. [41] K. Zhu et al., Dyval: Graph-informed dynamic evaluation of large language models, arXiv preprint arXiv:2309.17167, 2023.\n\n--- Segment 29 ---\n[40] S. Golchin and M. Surdeanu, Time travel in llms: Tracing data con- tamination in large language models, arXiv preprint arXiv:2308.08493, 2023. [41] K. Zhu et al., Dyval: Graph-informed dynamic evaluation of large language models, arXiv preprint arXiv:2309.17167, 2023. [42] F. Ranaldi et al., Investigating the impact of data contamination of large language models in text-to-sql translation, arXiv preprint arXiv:2402.08100, 2024. [43] Y. Dong et al., Generalization or memorization: Data contamination and trustworthy evaluation for large language models, arXiv preprint arXiv:2402.15938, 2024. [44] W. Shi et al., Detecting pretraining data from large language models, arXiv preprint arXiv:2310.16789, 2023. [45] Y. Li et al., Avoiding data contamination in language model evalua- tion: Dynamic test construction with latest materials, arXiv preprint arXiv:2312.12343, 2023. [46] N. Chandran et al., Private benchmarking to prevent contamina- tion and improve comparative evaluation of llms, arXiv preprint arXiv:2403.00393, 2024. [47] N. Jain et al., Livecodebench: Holistic and contamination free evaluation of large language models for code, arXiv preprint arXiv:2403.07974, 2024. [48] M. Riddell et al., Quantifying contamination in evaluating code genera- tion capabilities of language models, arXiv preprint arXiv:2403.04811, 2024. [49] Z. Pei et al., Betterv: Controlled verilog generation with discriminative guidance, arXiv preprint arXiv:2402.03375, 2024. [50] Y. Zhao et al., Codev: Empowering llms for verilog generation through multi-level summarization, arXiv preprint arXiv:2407.10424, 2024.\n\n--- Segment 30 ---\n[49] Z. Pei et al., Betterv: Controlled verilog generation with discriminative guidance, arXiv preprint arXiv:2402.03375, 2024. [50] Y. Zhao et al., Codev: Empowering llms for verilog generation through multi-level summarization, arXiv preprint arXiv:2407.10424, 2024. [51] L. L. Mankali et al., Rtl-breaker: Assessing the security of llms against backdoor attacks on hdl code generation, arXiv preprint arXiv:2411.17569, 2024. [52] R. Taori et al., Stanford alpaca: An instruction-following llama model, alpaca, 2023. [53] DeepSeek-AI, Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Available: 12948 [54] Anthropic, Claude 3.7 Sonnet and Claude Code, 2025, accessed: 2025- 02-28. Available: [55] OpenAI, OpenAI O3 Mini, 2024, accessed: 2025-02-25. Available: [56] S. Nie et al., Large language diffusion models, arXiv preprint arXiv:2502.09992, 2025.\n\n