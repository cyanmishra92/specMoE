=== ORIGINAL PDF: 2504.11981v1_Hardware-Friendly_Delayed-Feedback_Reservoir_for_M.pdf ===\n\nRaw text length: 54071 characters\nCleaned text length: 53627 characters\nNumber of segments: 38\n\n=== CLEANED TEXT ===\n\nHardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification SOSEI IKEDA, Kyoto University, Japan HIROMITSU AWANO, Kyoto University, Japan TAKASHI SATO, Kyoto University, Japan Reservoir computing is attracting attention as a machine learning technique for edge computing. In time- series classification tasks, the number of features obtained using a reservoir depends on the length of the input series. Therefore, the features must be converted to a constant-length intermediate representation (IR), such that they can be processed by an output layer. Existing conversion methods involve computationally expensive matrix inversion that significantly increases the circuit size and requires processing power when implemented in hardware. In this paper, we propose a simple but effective IR, namely dot-product-based reservoir representation (DPRR), for reservoir computing based on the dot product of data features. Additionally, we propose a hardware-friendly delayed feedback reservoir (DFR) consisting of a nonlinear element and delayed feedback loop with DPRR. The proposed DFR successfully classified multivariate time series data that has been considered particularly difficult to implement efficiently in hardware. In contrast to conventional DFR models that require analog circuits, the proposed model can be implemented in a fully digital manner suitable for high-level syntheses. A comparison with existing machine learning methods via field-programmable gate array implementation using 12 multivariate time-series classification tasks confirmed the superior accuracy and small circuit size of the proposed method. CCS Concepts: Computer systems organization Neural networks. Additional Key Words and Phrases: reservoir computing, delayed feedback reservoir (DFR), edge computing ACM Reference Format: Sosei Ikeda, Hiromitsu Awano, and Takashi Sato. 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification. ACM Trans. Embedd. Comput. Syst. 00, 0, Article 000 ( 0000), 20 pages. 1 INTRODUCTION Edge computing, which is gaining attention in today s Internet of Things (IoT) society, is a method of processing and exchanging information with other devices in the vicinity of data sources [15]. To reduce the energy consumption of edge devices, processing near data sources such as sensors is crucial. In general, data sources generate a large amount of time-series data that must be processed in real time. In addition, because processing is performed at the edge devices, they must operate with a low power consumption and be implemented using small hardware resources. Reservoir computing is a machine learning method that uses a reservoir. Input signals are transformed nonlinearly to a higher-dimensional space via a layer called a reservoir with fixed weights, and only the weights of the output layer are learned [10]. This method is considered to be particularly suitable for edge computing because it is capable of processing time-series data with its Authors addresses: Sosei Ikeda, Kyoto University, Kyoto, Japan, Hiromitsu Awano, Kyoto University, Kyoto, Japan, Takashi Sato, Kyoto University, Kyoto, Japan, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from 0000 Association for Computing Machinery. 1539-9087 0000 0-ART000 15.00 ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. arXiv:2504.11981v1 [cs.LG] 16 Apr 2025 000:2 Ikeda et al. recurrent structure that reflects past inputs. In addition, the reservoir layer does not require learning and can therefore be implemented in hardware utilizing a variety of physical phenomena [17]. The greatest obstacle in applying reservoir computing to real-world time-series classification tasks is the transformation of features obtained via the reservoir [7]. In general, the number of features obtained by the reservoir varies with the length of the input data series, thereby preventing the structure of the output layer from being fixed. Therefore, it is necessary to convert these features to an intermediate representation (IR) that is independent of the data length, such that they can be processed using an output layer with a fixed structure. Thus far, no established method has been proposed for this conversion. For echo state networks (ESNs) [9] that are another variant of reservoir computing, several transformation methods have been proposed [4, 12]. However, those used in existing studies exhibited less classification accuracy than existing neural-network-based classification methods, especially in multivariate time-series classification tasks [5]. The reservoir model space (RMS) [5] is an example of IR that facilitates the classification of multivariate time-series data and improves the classification accuracy. However, converting features to RMS involves the inversion of a large matrix, which requires a substantial amount of hardware resources and computation time. In this paper, we propose a new feature transformation method that achieves both high accuracy and small circuit size. A dot-product-based reservoir representation (DPRR) is devised by focusing on the convolution of the time development of the features. We evaluated the effectiveness of applying DPRR to a delayed feedback reservoir (DFR) [2], which is a method of reservoir computing that enables a particularly compact construct consisting of only one nonlinear element and a delayed feedback loop. The combination of DPRR with DFR demonstrated excellent classification accuracy on multivariate time-series classification tasks and the smallest circuit size compared to existing machine learning classifiers based on neural networks. The important contributions of this work can be summarized as follows: (1) A novel IR called the dot-product-based reservoir representation (DPRR), which is computa- tionally efficient and effective for improving the classification accuracy of multivariate time series data, is proposed. (2) A fully digital delayed feedback reservoir (DFR) model with DPRR is proposed. The accuracy of DFR on multivariate time-series classification tasks is thoroughly evaluated. The accuracy and hardware cost are compared with those of existing machine learning classifiers. (3) A construct of input-data masking that supports multivariate time-series inputs and sup- presses inference accuracy variation is defined. The remainder of this paper is organized as follows. In Section 2, the concept and operation of reservoir computing are explained. In Section 3, existing reservoir representations are briefly reviewed. In Section 4, a reservoir representation called DPRR and a new DFR model that uses DPRR are proposed. In Section 5, an evaluation of the effectiveness of DPRR and classification accuracy of the proposed DFR implemented on a field-programmable gate array (FPGA), are presented. Finally, in Section 6, we conclude the paper. 2 RESERVOIR COMPUTING This section explains the concept of reservoir computing. Subsequently, ESN and DFR that are typical implementations of reservoir computing are discussed. 2.1 Concept of Reservoir Computing Reservoir computing is a machine learning method comprising three major components or layers: input layer, reservoir, and output layer [10]. First, the input layer converts the input signal into ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:3 a format suitable for entering the reservoir layer. Subsequently, the reservoir transforms the signal into a high-dimensional nonlinear vector. Throughout this paper, we refer to this vector as the reservoir state and to its components as features. Finally, the output layer performs pattern recognition using a simple learning algorithm. The characteristics of each layer are defined by the weights that connect the nodes. The weights of the input layer and reservoir are fixed and unaltered, whereas that of the output layer is determined through training. In time-series processing, reservoir computing is useful for two main tasks: regression and classification [13]. In the regression tasks, one output is expected for each input. In classification tasks, a single output is expected from multiple time-series inputs. The number of features obtained by the reservoir layer depends on the length of the input series; thus, the features need to be converted into an intermediate representation of a fixed length, regardless of the length of the inputs, such that they can be processed at the output layer of a fixed size [7]. In this paper, the intermediate representation is called the reservoir representation. In the following, for the sake of simplicity, we assume that the processing tasks are classification tasks and consider a model that involves conversion to a reservoir representation in its operations. 2.2 Echo State Network (ESN) An ESN is a realization of reservoir computing. The reservoir can be considered as a recurrent neural network with fixed weights. Figure 1 depicts a conceptual diagram of the ESN. ğ’–(ğ‘˜) Input layer Reservoir Output layer ğ’š ğ’™(ğ‘˜) ğ‘Š ğ‘Š ğ‘Šout in Fig. 1. Conceptual diagram of ESN [17]. The reservoir in the ESN is a recurrent neural network with fixed weight ğ‘Š. First, the input time series is represented as a series of vectors ğ’–(ğ‘˜) Rğ‘ğ‘¢(ğ‘˜ 1, 2, . . . ,ğ‘‡). Each ğ’–(ğ‘˜) represents a vector of real numbers, ğ‘‡is the length of the series or number of sampling time steps, and ğ‘ğ‘¢denotes the number of variables. When the number of nodes in the reservoir is ğ‘ğ‘¥, the weights of the input layer are represented as a matrix ğ‘Šin with ğ‘ğ‘¥rows and ğ‘ğ‘¢columns. The weights of the reservoir are represented as a matrix ğ‘Šwith ğ‘ğ‘¥rows and ğ‘ğ‘¥columns, and the hyperbolic tangent function is used as an activation function. The time evolution of the reservoir state ğ’™(ğ‘˜) Rğ‘ğ‘¥is expressed as follows: ğ’™(ğ‘˜ 1) tanh(ğ‘Šinğ’–(ğ‘˜ 1) ğ‘Šğ’™(ğ‘˜)). (1) ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:4 Ikeda et al. Next, the reservoir representation ğ’“ Rğ‘ğ‘Ÿis obtained from the reservoir states ğ’™(1), ğ’™(2), . . . , ğ’™(ğ‘‡). In most cases, even within a dataset, the length of the input time series varies for each input data instance. For example, in spoken language classification, the lengths of the time series of the input corresponding to "1" and "14" differ because of the difference in the number of syllables. Accordingly, the length of the reservoir state ğ‘‡differs. Reservoir representations should be able to absorb these differences. Regardless of the length of the input series, a constant-length vector should always be produced. This is a strict requirement that comes from the fact that the output layer that converts the reservoir representation to output ğ’š, is fixed [7]. Here, the weight of the output layer, ğ‘Šout, is trained in advance for the use of the ESN, such that ğ’š ğ‘Šoutğ’“provides a good prediction. Here, ğ’š Rğ‘ğ‘¦is the output and ğ‘ğ‘¦is the number of classification classes. Note that the target output is given in a one-hot representation, with 1 for the correct label and 0 for the others. 2.3 Delayed Feedback Reservoir (DFR) The DFR is another realization of reservoir computing in which the reservoir consists of only one nonlinear element and a delayed feedback loop. Compared with other reservoir computing schemes, DFR is easier to implement as hardware because of its simple structure [17]. Figure 2 illustrates a conceptual diagram of DFR. ğœ‚ NL ğ‘¥(ğ‘¡) ğ‘¥(ğ‘¡-ğœ) ğ‘ğœƒ ğœ ğ‘¥ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğ›¾ Reservoir Output layer ğ‘—(ğ‘¡) Input layer ğ’–(ğ‘˜) ğ’š (Masking) Fig. 2. Conceptual diagram of DFR [2]. The reservoir in the DFR consists of a nonlinear element (NL) and a feedback loop with a total delay ğœ. The feedback loop consists of ğ‘ğ‘¥virtual nodes having equal time intervals ğœƒ. Most of the existing hardware implementations of the reservoir layer in DFR [2, 16] operate entirely using analog circuits. Therefore, the input ğ’–(ğ‘˜), typically represented as digital values such as the IEEE-754 float, is converted to an analog signal before passing to the reservoir layer. The analog output of the reservoir layer is then converted into a digital value. More specifically, after applying the masking process to the input time series ğ’–(ğ‘˜), it is converted to analog signal ğ‘—(ğ‘¡) (Fig. 2). This masking is a preprocessing step for the input signal, which is equivalent to multiplying the input by ğ‘Šin in the case of ESN. The purpose of masking is to modulate the input signal using the mask signal. Figure 3 illustrates an example of mask processing for a univariate time-series input. The input signal ğ‘–(ğ‘¡) is multiplied ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:5 ğ‘¢(ğ‘˜) ğ‘¢(ğ‘˜-1) ğ‘¢(ğ‘˜ 1) ğœ ğœƒ ğ‘–(ğ‘¡) 0 0 0 ğ‘¡ ğ‘¡ ğ‘¡ ğ‘—(ğ‘¡) ğ‘š(ğ‘¡) ğœ ğœƒ ğœ Fig. 3. Masking process (univariate input example for clarity). ğ‘–is the output of digital-to-analog conversion of the original digital input signal ğ‘¢. The signal ğ‘–maintains a constant value for each time duration of ğœ. The mask signal ğ‘šchanges at time intervals of ğœƒand has a period of ğœ. The input to the reservoir is given as ğ‘—(ğ‘¡) ğ‘–(ğ‘¡) ğ‘š(ğ‘¡). by the mask signal with a faster sample rate and amplitude of either 1 or 1. Note that the masking process for univariate inputs has been explained in several studies [2, 3]; however, that for the multivariate input case has not yet been adequately investigated. The masking process for a multivariate input signal is defined as follows. Letğ‘–1(ğ‘¡),ğ‘–2(ğ‘¡), . . . ,ğ‘–ğ‘ğ‘¢(ğ‘¡) be the multivariate input signals: ğ‘–1(ğ‘¡) ğ‘–2(ğ‘¡) ğ‘–ğ‘ğ‘¢(ğ‘¡) Âª ğ’–(ğ‘˜). (2) Here, the input ğ’–(ğ‘˜) remains the same for the time duration ğœ. The length of the mask patterns is equal to the virtual nodes ğ‘ğ‘¥in the DFR, which is designed such that the following relationship holds: ğ‘ğ‘¥ğœƒ ğœ. (3) Here, the delay between the nodes in the reservoir is ğœƒ. Therefore, the constant input during ğœis divided into ğ‘ğ‘¥equal intervals of ğœƒ, and each mask value is applied for the time interval of ğœƒ. The converted input ğ‘—(ğ‘¡) can be expressed as follows: ğ‘—(ğ‘¡) ğ‘ğ‘¢ ğ‘ 1 ğ‘šğ‘(ğ‘¡)ğ‘–ğ‘(ğ‘¡), (4) ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:6 Ikeda et al. where ğ‘š1(ğ‘¡),ğ‘š2(ğ‘¡), . . . ,ğ‘šğ‘ğ‘¢(ğ‘¡) are mask functions, each having a length of ğ‘ğ‘¥. Once ğ‘—(ğ‘¡) is obtained, ğ‘—(ğ‘¡) is multiplied by ğ›¾and added to the feedback signal in the reservoir. It is then entered into a one-input, one-output nonlinear (NL) device, whose output is multiplied by ğœ‚ and becomes ğ‘¥(ğ‘¡). Thereafter, ğ‘¥(ğ‘¡) is added to ğ›¾ğ‘—(ğ‘¡) through a feedback loop with a total delay time of ğœ. Virtual nodes with interval ğœƒare serially connected to form a feedback loop in the reservoir. These nodes operate as shift registers. The values stored in the virtual nodes are the features and they collectively form a vector with ğ‘ğ‘¥elements, which is the reservoir state. Therefore, the state that this reservoir holds in the delay elements is expressed as: ğ’™(ğ‘˜) [ğ‘¥(ğ‘˜ğœ ğœƒ),ğ‘¥(ğ‘˜ğœ 2ğœƒ), . . . ,ğ‘¥(ğ‘˜ğœ ğœ)]. (5) Most of the delayed feedback reservoirs use the Mackey-Glass model [11] as a nonlinear element[1, 2, 16], which is expressed as follows: d dğ‘¡ğ‘¥(ğ‘¡) ğ‘¥(ğ‘¡) ğœ‚[ğ‘¥(ğ‘¡ ğœ) ğ›¾ğ‘—(ğ‘¡)] 1 [ğ‘¥(ğ‘¡ ğœ) ğ›¾ğ‘—(ğ‘¡)]ğ‘, (6) where ğ‘is an adjustable parameter. There are two reasons why this model is often used [2]. First, it is easy to implement in analog electronic circuits. Second, the nonlinearity can be adjusted by changing ğ‘. The existing studies often use ğ‘ 7 for ease of hardware implementation. Next, the reservoir representation ğ’“ Rğ‘ğ‘Ÿis obtained for the reservoir states ğ’™(1), ğ’™(2), . . . , ğ’™(ğ‘‡) for use in the output layer. The construction of a reservoir representation is critically important because it significantly affects both the accuracy of the DFR and hardware resource usage. We discuss the implementation of the reservoir representation in the next section. The construct and function of the output layer are similar to those of ESN. The weight of the output layer, ğ‘Šout is learned in advance, as in ESN. 3 RESERVOIR REPRESENTATION The reservoir maps the input signal to a higher dimensional space [13]. Let the elements of ğ’™(ğ‘˜) (i.e., the features) be [ğ‘¥(ğ‘˜)1,ğ‘¥(ğ‘˜)2, . . . ,ğ‘¥(ğ‘˜)ğ‘ğ‘¥]. The reservoir then yields a total of ğ‘‡ ğ‘ğ‘¥features that are denoted by ğ‘¥(ğ‘˜)ğ‘›(ğ‘˜ 1, 2, . . . ,ğ‘‡; ğ‘› 1, 2, . . . , ğ‘ğ‘¥). As explained in the previous sections, because the length of the input series ğ‘‡may vary, so does the number of features. In this situation, the features cannot be processed with an output layer of a fixed size. Consequently, conversion to reservoir representation with a fixed length is required. The following subsections review the existing reservoir representations. 3.1 Last Reservoir State (LRS) In [12], LRS after all the input series were entered was used as the reservoir representation. The reservoir representation is then expressed as: ğ’“ ğ’™(ğ‘‡). (7) The underlying assumption of this method is that the reservoir cumulatively stores information about past inputs. Therefore, the LRS is considered to contain all information for the entire input. The number of dimensions available with this representation is ğ‘ğ‘¥and it does not depend on the series lengthğ‘‡. However, ğ‘ğ‘¥is relatively small (otherwise, the hardware cost increases). As a result, the prediction of reservoirs using this reservoir representation is less accurate than that using existing neural-network-based classifiers, particularly for multivariate time-series classification tasks [5]. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:7 3.2 Direct Reservoir State (DRS) In the DRS, the reservoir state at each sample time is used as a reservoir representation: ğ’“(ğ‘˜) ğ’™(ğ‘˜). (8) As the reservoir representation is available for every sample timeğ‘‡, the outputs,ğ’š(1),ğ’š(2), . . . ,ğ’š(ğ‘‡), are calculated using the output layer. The mode of the ğ‘‡-inferred labels is used as the output inference label. This representation is often used for time-series classification using DFR [2, 16]. Similar to the LRS, the number of features in DRS is ğ‘ğ‘¥. Owing to its low dimensionality, this method also yields less accurate results than existing neural-network-based classifiers, especially for multivariate time series classification tasks [5]. 3.3 Maximal Reservoir States (MRSs) In the MRSs,ğ‘‡max, which is the maximum series lengthğ‘‡of all possible inputs, is applied to all other inputs to equalize the feature length. ğ‘‡max ğ‘ğ‘¥features denoted by ğ‘¥(ğ‘˜)ğ‘›(ğ‘˜ 1, 2, . . . ,ğ‘‡max; ğ‘› 1, 2, . . . , ğ‘ğ‘¥) are then used as a reservoir representation [6]. In this method, there are several options for extending short data to the maximum length. The first option is to append zeros at the end of the input for which no value is given: ğ’–(ğ‘˜) 0 (ğ‘˜ ğ‘‡ 1,ğ‘‡ 2, . . . ,ğ‘‡max). (9) The other option is to append zeros for the reservoir state: ğ’™(ğ‘˜) 0 (ğ‘˜ ğ‘‡ 1,ğ‘‡ 2, . . . ,ğ‘‡max). (10) The greatest drawback of this representation is the difficulty in determining the necessary and sufficient ğ‘‡max value in advance. Setting ğ‘‡max to be too large would increase hardware resources. In addition, the zero-filling process of the input (Eq. (9)) requires additional processing until ğ‘‡max is reached, which reduces throughput, whereas the zero-filling required for the reservoir state (Eq. (10)) cannot add additional information. 3.4 Output Model Space (OMS) In the OMS, linear regression is first used to predict the input that is one time ahead of the reservoir state: ğ’–(ğ‘˜ 1) ğ‘…omsğ’™(ğ‘˜) ğ’“oms. (11) Here, ğ‘…oms and ğ’“oms are constant regardless of ğ‘˜and are subject to regression. ğ’™ (ğ‘˜) is defined as: ğ’™ (ğ‘˜) [ğ’™(ğ‘˜), 1]. (12) Eq. (11) can be rewritten as: ğ’–(ğ‘˜ 1) ğ‘… omsğ’™ (ğ‘˜). (13) Note that the dimension of ğ‘… oms Rğ‘ğ‘¢ (ğ‘ğ‘¥ 1) holds. Let ğ‘ˆ [ğ’–(1), ğ’–(2), . . . , ğ’–(ğ‘‡)] and (14) ğ‘‹ [ğ’™ (0), ğ’™ (1), . . . , ğ’™ (ğ‘‡ 1)]. (15) Then, ğ‘… oms may be ridge regressed using the Moore-Penrose pseudoinverse with ğ¸as the (ğ‘ğ‘¥ 1)th- order unit matrix and ğœ†as a parameter: ğ‘… oms ğ‘ˆ ğ‘‹ ğ‘‡(ğ‘‹ ğ‘‹ ğ‘‡ ğœ†ğ¸) 1. (16) OMS is a single-column rearrangement of ğ‘… oms [5, 7]: ğ’“ vec(ğ‘… oms). (17) ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:8 Ikeda et al. The derivation of OMS is based on the idea that there is a constant linear transformation from the reservoir state to the input that is one time ahead, because ESNs with a linear output layer are effective for time series prediction tasks [7]. In OMS, the number of features is ğ‘ğ‘¢ (ğ‘ğ‘¥ 1), which is independent of the series length ğ‘‡and shows better classification accuracy than LRS and MRS [5]. However, OMS involves the computation of a high-dimensional inverse matrix, which increases the circuit size when implemented in hardware. 3.5 Reservoir Model Space (RMS) In the RMS, linear regression is first performed to predict the reservoir state, one time ahead. ğ’™(ğ‘˜ 1) ğ‘…rmsğ’™(ğ‘˜) ğ’“rms. (18) Here, ğ‘…rms and ğ’“rms are constant regardless of ğ‘˜and are subjected to regression. Introducing ğ‘… rms with a definition similar to ğ‘… oms yields ğ’™(ğ‘˜ 1) ğ‘… rmsğ’™ (ğ‘˜). Again, similarly to the OMS, using ğ‘‹ [ğ’™(1), ğ’™(2), . . . , ğ’™(ğ‘‡)] and (19) ğ‘‹ [ğ’™ (0), ğ’™ (1), . . . , ğ’™ (ğ‘‡ 1)], (20) the ridge regression of Eq. (19) leads to ğ‘… rms ğ‘‹ ğ‘‹ ğ‘‡(ğ‘‹ ğ‘‹ ğ‘‡ ğœ†ğ¸) 1. (21) The RMS is a single-column reservoir representation, defined as follows [5]: ğ’“ vec(ğ‘… rms). (22) The RMS has ğ‘ğ‘¥ (ğ‘ğ‘¥ 1) features, which is larger than the OMS. As a result, the RMS exhibits even higher accuracy than the OMS [5] at the cost of calculating the inversion of an even higher- dimensional matrix than the OMS, which leads to a larger circuit size. 4 DELAYED FEEDBACK RESERVOIR WITH DOT-PRODUCT-BASED RESERVOIR REPRESENTATION 4.1 Dot-Product-based Reservoir Representation (DPRR) Here, we propose DPRR, which is a novel reservoir representation based on the dot product across reservoir states of different times. Both the OMS and RMS are effective for improving inference accuracy [5]. However, the calcula- tion of both reservoir representations involves the inversion of a large matrix that significantly increases the circuit size when implemented in hardware. The fundamental requirements of reservoir representation are as follows: 1) it should retain information in a high-dimensional space; 2) its size should be constant and independent of the time series; and 3) it should be easy to calculate. The vector of a node in the reservoir: ğ’™ğ‘› [ğ‘¥(1)ğ‘›,ğ‘¥(2)ğ‘›, . . . ,ğ‘¥(ğ‘‡)ğ‘›], (23) can be considered to be the time evolution of the node state. A new reservoir representation can be derived by taking the dot product of the vectors of two nodes: ğ’™ğ‘– ğ’™ğ‘— ğ‘‡ ğ‘˜ 1 ğ‘¥(ğ‘˜)ğ‘–ğ‘¥(ğ‘˜)ğ‘—. (ğ‘–, ğ‘— 1, 2, . . . , ğ‘ğ‘¥). (24) This yields ğ‘2 ğ‘¥features that are independent of the length of the series ğ‘‡. However, because ğ’™ğ‘– ğ’™ğ‘— ğ’™ğ‘— ğ’™ğ‘–holds, the actual number of features is reduced by approximately half. Furthermore, this representation completely loses information regarding the time variation in the reservoir state. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:9 To eliminate the loss of features while retaining the information concerning time variation, we utilize the time evolution of the features with samples of shifted time. ğ’™ğ‘› [ğ‘¥(1)ğ‘›,ğ‘¥(2)ğ‘›, . . . ,ğ‘¥(ğ‘‡)ğ‘›], (25) ğ’™ ğ‘› [ğ‘¥(0)ğ‘›,ğ‘¥(1)ğ‘›, . . . ,ğ‘¥(ğ‘‡ 1)ğ‘›]. (26) Taking the dot product of these vectors for two nodes yields: ğ’™ğ‘– ğ’™ ğ‘— ğ‘‡ ğ‘˜ 1 ğ‘¥(ğ‘˜)ğ‘–ğ‘¥(ğ‘˜ 1)ğ‘—. (ğ‘–, ğ‘— 1, 2, . . . , ğ‘ğ‘¥). (27) By shifting the dot product by one time step and considering all node combinations, we can obtain ğ‘2 ğ‘¥features, which are independent of the time series, while retaining the information of the temporal evolution. Note also that the dot product of the features at different times is not commutative. In addition, we added the reservoir state itself as a feature: ğ’™ğ‘– 1 ğ‘‡ ğ‘˜ 1 ğ‘¥(ğ‘˜)ğ‘–. (ğ‘– 1, 2, . . . , ğ‘ğ‘¥). (28) The above ğ‘ğ‘¥ (ğ‘ğ‘¥ 1) values were used as the proposed DPRR. These features are represented as vectors. Using ğ’™ (ğ‘˜) [ğ’™(ğ‘˜), 1], DPRR can be represented as follows: ğ’“ vec( ğ‘‡ ğ‘˜ 1 ğ’™(ğ‘˜)ğ’™ (ğ‘˜ 1)). (29) DPRR is expected to be implemented more efficiently than OMS or RMS because it can be calculated by using only matrix multiplications. 4.2 Fully Digital DFR for Hardware Implementation Here, we describe a DFR model that uses the proposed reservoir representation. In contrast to existing DFRs that use a mixture of digital and analog signals, our model operates entirely digitally making it particularly easy to implement in FPGAs. In addition, the proposed model incorporates the following: 1) the reservoir layer applies a lightweight nonlinear function without storing weights that require a large amount of memory; 2) the masking method suppresses the accuracy variation and supports multivariate time series inputs; and 3) the learning method of an output layer based on ridge regression with a constant term improves classification accuracy. Algorithms 1 and 2 show pseudo codes for the algorithms used in the nonlinear element part of the DFR. Algorithm 1 Calculate ğ‘“ Input: ğ‘¥, ğ‘— 1: ğ‘¡ (ğ‘¥ ğ›¾ ğ‘—) 2: return ğœ‚ğ‘¡ [1 ğ‘¡2] ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:10 Ikeda et al. Algorithm 2 Calculate MackeyGlass Input: ğ’™(ğ‘˜ 1) [ğ‘¥(ğ‘˜ 1)1,ğ‘¥(ğ‘˜ 1)2, . . . ,ğ‘¥(ğ‘˜ 1)ğ‘ğ‘¥] Input: ğ’‹(ğ‘˜) [ğ‘—(ğ‘˜)1, ğ‘—(ğ‘˜)2, . . . , ğ‘—(ğ‘˜)ğ‘ğ‘¥] 1: ğ‘¥(ğ‘˜)1 ğ‘¥(ğ‘˜ 1)ğ‘ğ‘¥e ğœƒ (1 e ğœƒ)ğ‘“(ğ‘¥(ğ‘˜ 1)1, ğ‘—(ğ‘˜)1) 2: for ğ‘› 2 to ğ‘do 3: ğ‘¥(ğ‘˜)ğ‘› ğ‘¥(ğ‘˜)ğ‘› 1e ğœƒ (1 e ğœƒ)ğ‘“(ğ‘¥(ğ‘˜ 1)ğ‘›, ğ‘—(ğ‘˜)ğ‘›) 4: end for 5: return ğ’™(ğ‘˜) Figure 4 shows a block diagram of the proposed fully digital DFR used in this study. ğ’–(1) ğ’–(2) ğ’–(ğ‘‡) ğ‘ğ‘¢ ğ‘ğ‘¢ ğ‘ğ‘¥ ğ‘€ ğ‘ğ‘¥ ğ’‹(ğ‘˜) ğ‘ğ‘Ÿ ğ’“ ğ‘ğ‘¦ ğ‘ğ‘Ÿ ğ‘Šout ğ’š ğ‘ğ‘¦ ğ‘¦ ğ‘ğ‘¥ ğ’™(ğ‘˜) ğ’™(ğ‘˜-1) ğ‘ğ‘¥ ğ‘˜ 1, 2, , ğ‘‡ Input layer (Masking) Feedback memory Dot-product-based reservoir representation (DPRR) Output layer 1 1 1 1 Nonlinear element Eq. (30) MackeyGlass Eq. (42) Eq. (29) Eq. (39), Eq. (40) Fig. 4. Circuit block diagram and dataflow of the proposed fully digital DFR with DPRR. The operation of "MackeyGlass" is defined in Algorithms 1 and 2. The signal after the masking process ğ’‹(ğ‘˜) is expressed as: [2] ğ’‹(ğ‘˜) ğ‘€ğ’–(ğ‘˜). (30) Here, ğ‘€is a masking matrix whose elements are either 1 or 1. The size of ğ‘€is ğ‘ğ‘¥ ğ‘ğ‘¢. The elements of ğ‘€can be determined randomly. However, through separate experiments, we found that this resulted in a larger variation in classification accuracy. To suppress this accuracy variation associated with the randomly determined elements, each column of ğ‘€is determined pseudo- randomly based on the maximum length sequence (m-sequence) [3]. The underlying assumption in this method is that it is desirable for the mask function to contain various combinations of consecutive values. For example, consider a sequence of two consecutive numbers in 01001 ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:11 this contains three combinations: 01 , 10 , and 00. We intended to maximize the variation in the combinations included in the sequence. Here, we explain how the mask functions are generated based on the m-sequence. This was briefly explained in [3] for the univariate case. We extend the procedure to multivariate time-series inputs. The proposed procedure is summarized as follows: (1) Choose a primitive polynomial to use. (2) Determine an initial value that has the same length as the degree ğ‘šof the primitive polyno- mial. (3) Calculate the m-sequence. (4) Insert a zero immediately after (ğ‘š 1) consecutive zeros. If there is no such location, we insert zero at the beginning. (5) At the end of the sequence, append the (ğ‘š 1) initial values with the trailing digit removed. (6) Replace 0 with 1 to obtain a column vector of ğ‘€. (7) For multivariate time-series inputs, generate column vectors of ğ‘€by the round rotation of the sequence derived above. We show an example of how a sequence can be generated by choosing a primitive polynomial ğº(ğ‘¥) ğ‘¥3 ğ‘¥ 1 of degree ğ‘š 3. In this case, we are interested in variations in the sequence of three consecutive numbers. Assuming the maximal length sequence to be (ğ‘ğ‘›), the recurrence relation (base 2) can be expressed as follows: ğ‘ğ‘› ğ‘ğ‘› 2 ğ‘ğ‘› 3. (31) When the given set of initial value is ğ‘0 0, ğ‘1 0, and ğ‘2 1, the m-sequence is derived as follows: (ğ‘ğ‘›) {0, 0, 1, 0, 1, 1, 1}. (32) Among the eight different sequences of three consecutive binary numbers, (ğ‘ğ‘›) contains all se- quences except 000 when considering wrap-around rotations. Without the rotation, (ğ‘ğ‘›) does not contain 110 and 100 . An operation is conducted to insert a zero immediately after (ğ‘š 1) consecutive zeros, which yields a 000 sequence, and ğ‘0, . . . ,ğ‘ğ‘š 2 at the end, which yields 110 and 100 . (ğ‘ ğ‘›) {0, 0, 0, 1, 0, 1, 1, 1, 0, 0}. (33) After replacing the 0 values with 1, the column vector of ğ‘€is represented as follows: { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1}. (34) Therefore, ğ‘ğ‘¥ 2ğ‘š ğ‘š 1 holds for primitive polynomials of degree ğ‘š. After masking, the reservoir state ğ’™(ğ‘˜) is obtained via the delayed feedback path. As a nonlinear function, the Mackey-Glass model in Eq. (6) is adopted. The parameter ğ‘ 2 is chosen in our fully digital implementation to reduce computational resources while retaining nonlinearity. The secod term in Eq. (6) can be rewritten as ğ‘“: ğ‘“(ğ‘¥,ğ‘¡) ğœ‚[ğ‘¥ ğ›¾ğ‘—] 1 [ğ‘¥ ğ›¾ğ‘—]ğ‘. (35) Solving the differential equation in Eq. (6) by assuming ğ‘“to be constant for a small time ğœƒ, which is the interval of virtual nodes, yields ğ‘¥(ğ‘¡): ğ‘¥(ğ‘¡) ğ‘¥0e ğ‘¡ (1 e ğ‘¡)ğ‘“(ğ‘¥(ğ‘¡ ğœ), ğ‘—(ğ‘¡)). (36) ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:12 Ikeda et al. Here, ğ‘¥0 is the initial value of ğ‘¥(ğ‘¡) at each ğœƒ[2]. Then, the value of the next virtual node of the node with ğ‘¥0 is represented by ğ‘¥(ğœƒ). All of these operations are carried out in digital domain. Let the components of ğ’™(ğ‘˜) and ğ’‹(ğ‘˜) be: ğ’™(ğ‘˜) [ğ‘¥(ğ‘˜)1,ğ‘¥(ğ‘˜)2, . . . ,ğ‘¥(ğ‘˜)ğ‘ğ‘¥], (37) ğ’‹(ğ‘˜) [ğ‘—(ğ‘˜)1, ğ‘—(ğ‘˜)2, . . . , ğ‘—(ğ‘˜)ğ‘ğ‘¥]. (38) Then, ğ’™(ğ‘˜) is derived recurrently as follows: ğ‘¥(ğ‘˜)1 ğ‘¥(ğ‘˜ 1)ğ‘ğ‘¥e ğœƒ (1 e ğœƒ)ğ‘“(ğ‘¥(ğ‘˜ 1)1, ğ‘—(ğ‘˜)1), (39) ğ‘¥(ğ‘˜)ğ‘› ğ‘¥(ğ‘˜)ğ‘› 1e ğœƒ (1 e ğœƒ)ğ‘“(ğ‘¥(ğ‘˜ 1)ğ‘›, ğ‘—(ğ‘˜)ğ‘›). (ğ‘› 2) (40) We use 0 as the initial values of the reservoir state. The proposed DPRR uses a time-shifted feature, wherein it is necessary to store not only ğ’™(ğ‘˜) but also ğ’™(ğ‘˜ 1). For this purpose, the memory required to store ğ’™(ğ‘˜) is duplicated. As ğœƒis constant, the values of e ğœƒand (1 e ğœƒ) are constants and they can therefore be precalculated and embedded in the logic circuit. The recurrent equation approach for solving differential equations is intended to balance the accuracy and computational complexity. In a previous study, either the Euler [1] or Heun [3] methods were used. The Euler method is relatively simple but may compromise the classification accuracy, whereas the Heun method is relatively complex and requires a larger circuit for hardware implementation. Next, the reservoir representation ğ’“is obtained using DPRR from the reservoir states. Finally, outputğ’šis obtained. First, a constant term is added to improve the classification accuracy: ğ’“ [ğ’“, 1]. (41) The weight matrix ğ‘Šout of size ğ‘ğ‘¦ (ğ‘ğ‘Ÿ 1) is used to compute ğ’š: ğ’š ğ‘Šoutğ’“ . (42) The largest value in the elements of ğ’šis used as the inference label. Ridge regression is used to determine ğ‘Šout. 5 EVALUATION In this section we evaluate the classification accuracy and required hardware resources of the proposed fully digital DFR with DPRR. First, a comparison with different reservoir representa- tions is made. Thereafter, we compare our results with those of other machine learning methods. Comparisons are made through both software and FPGA implementations, the former to evaluate classification performance and the latter to evaluate hardware resource usage. Finally, the tradeoffs between analog and full digital implementation are discussed. For the FPGA implementation, we used Xilinx Vitis HLS 2021.1 as the high-level synthesis tool and Zynq-7000 (xc7z020clg400-1) as the target FPGA board. The numbers are represented in IEEE-754 floating-point format, and the data paths were not pipelined. In practice, it may be possible to reduce the bit width of number representations and to improve throughput by pipelining. However, as it is difficult to ensure an equal level of optimizations across multiple machine learning methods, this setting was selected for fair comparison. A summary of the datasets used in the evaluation is listed in Table 1. Throughout the evaluation, we used the npz files available in [5]. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:13 Table 1. Summary of multivariate time series classification datasets (taken from [5]). Column ğ‘‰, ğ¶, Train, and Test respectively reports the input dimension, the output dimension, the number of training data, and that of testing data. ğ‘‡min and ğ‘‡max are the shortest and the longest length of the input series in the dataset, respectively. Dataset ğ‘‰ ğ¶ Train Test ğ‘‡min ğ‘‡max ARAB 13 10 6600 2200 4 93 AUS 22 95 1140 1425 45 136 CHAR 3 20 300 2558 109 205 CMU 62 2 29 29 127 580 ECG 2 2 100 100 39 152 JPVOW 12 9 270 370 7 29 KICK 62 2 16 10 274 841 LIB 2 15 180 180 45 45 NET 4 13 803 534 50 994 UWAV 3 8 200 427 315 315 WAF 6 2 298 896 104 198 WALK 62 2 28 16 128 1918 5.1 Effects of Reservoir Representation First, the effect of reservoir representation was studied. DFR with various reservoir representations was implemented in Python. The dataset used in this section was ARAB. Because different reservoir representations yield different value ranges, the hyperparameters ğ›¾ and ğœ‚must be adjusted to attain improved accuracy. Table 2 shows the hyperparameters used in this evaluation that were found through a grid search in the range {0.03, 0.1, 0.3, 1}. Here, MRS_upad and MRS_xpad are the maximal reservoir states with zero-filling for the input and reservoir states, respectively. Table 2. Parameters used in comparison with existing reservoir representation. The parameters ğœƒ 0.25, ğ›½ 0.01, and ğœ† 1 are commonly used. The dataset is ARAB. LRS DRS MRS MRS OMS RMS DPRR _upad _xpad ğ›¾ 0.03 0.3 0.1 0.03 0.03 0.03 0.03 ğœ‚ 1 0.1 0.1 1 1 1 1 Table 3 lists the classification accuracy for each reservoir representation. Regardless of the choice of ğ‘š, LRS and DRS were less accurate than the other reservoir represen- tations. Although the proposed DPRR is computationally efficient, it achieved equivalent or higher accuracy compared to OMS and RMS. The calculation of each reservoir representation transformation was implemented on an FPGA using a high-level synthesis tool to demonstrate that DPRR is computationally more efficient than OMS or RMS. Table 4 presents the logic synthesis results. Clearly, DPRR is more computationally efficient than OMS and RMS. When ğ‘ğ‘¥ 50, the number of FFs in DPRR is reduced to about 1 4 of those in OMS and in RMS, and the number of LUTs is about 1 5. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:14 Ikeda et al. Table 3. Classification accuracy for different reservoir representation. The dataset used is ARAB. Classification accuracy is in . ğ‘š 3, 4, 5, 6 corresponds to ğ‘ğ‘¥ 10, 19, 36, 69, respectively. LRS DRS MRS MRS OMS RMS DPRR ğ‘š _upad _xpad 3 44.9 18.2 89.3 89.1 90.9 85.4 88.5 4 52.6 22.0 93.0 92.6 95.3 95.0 96.5 5 54.1 26.0 93.5 93.2 96.2 96.0 97.5 6 60.1 38.1 94.3 94.7 96.0 98.0 98.0 Table 4. Synthesis results for different reservoir representation. Latency is in Cycles. Based on the ARAB dataset, the dimension of the input is 13 and the series length is 50. ğ‘ğ‘¥ 10 20 30 40 50 Latency 16,344 62,032 162,125 412,452 740,512 BRAM 8 8 11 21 37 OMS DSP 18 18 12 19 19 FF 7,822 10,941 15,422 12,529 14,325 LUT 8,631 11,080 13,490 11,522 12,761 Latency 14,474 63,649 170,557 435,699 789,574 BRAM 8 8 12 26 50 RMS DSP 19 19 12 20 19 FF 7,768 10,900 15,455 12,510 14,351 LUT 8,284 10,615 12,805 10,643 11,734 Latency 5,628 21,448 47,468 83,688 130,108 BRAM 2 2 2 5 9 DPRR DSP 5 5 5 5 5 FF 1,212 1,815 2,284 2,667 3,171 LUT 1,465 1,836 2,086 2,395 2,725 5.2 Comparison of Classification Accuracy with Existing Machine Learning Methods Next, we compared the accuracy of the proposed DFR with DPRR (DFR_DPRR), to those of the DFR using DRS (DFR_DRS) and other machine learning methods [8]. Comparison with DFR_DRS corresponds to comparison with existing studies of DFR [2, 16]. This comparison of accuracy is followed by a comparison of hardware usage. As can be seen from the comparison in Section 5.1, DFR with OMS or RMS has the same level of accuracy as DFR_DPRR, but is less computationally efficient than DFR_DPRR, therefore it is not included in the comparison. However, DFR_DRS is less accurate than DFR_DPRR, but is more computationally efficient, therefore it is worth evaluating. The DFR models were implemented using Python. The hyperparameters listed in Table 5 were used for each dataset. In some datasets, the length of the input series varied. For data with a shorter length ğ‘‡than the maximum series length ğ‘‡max, the inputs were filled with zeros when using methods other than DFRs. The machine learning methods that were compared were: (1) Multi layer perceptron (MLP) [19] (2) Fully convolutional neural network (FCN) [19] ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:15 Table 5. Parameters of the DFR used for comparision with existing machine learning methods. The number of nodes ğ‘ğ‘¥in the reservoir is 36 (ğ‘š 5). DFR_DRS DFR_DPRR Dataset ğ›¾ ğœ‚ ğœƒ ğ›½ ğ›¾ ğœ‚ ğœƒ ğ›½ ARAB 4 0.4 0.3 1e-2 0.04 1 0.3 1e-2 AUS 0.3 4 0.45 1e-1 0.05 1 0.25 1e-3 CHAR 0.2 1.5 0.4 1e-2 0.05 1 0.3 1e-4 CMU 0.02 0.1 0.2 1e-2 0.01 3 0.15 1e-1 ECG 1 1 0.15 1e-1 0.03 1 0.3 1e-5 JPVOW 0.1 1 0.2 1e-2 0.03 1 0.2 1e-1 KICK 3 3 0.15 1e-1 0.003 3 0.15 1e-1 LIB 0.3 1.5 0.25 1e-5 0.14 1 0.45 1e-3 NET 0.01 3 0.2 1e-2 0.025 3 0.2 1e-3 UWAV 0.03 1.5 0.15 1e-5 0.03 1 0.4 1e-2 WAF 1 1 0.15 1e-3 0.2 0.4 0.25 1e-2 WALK 0.01 1 0.15 1e-1 1 1 0.25 1e-2 (3) Residual network (ResNet) [19] (4) Encoder [14] (5) Multi channel deep convolutional neural network (MCDCNN) [21] (6) Time convolutional neural network (Time-CNN) [20] (7) Time warping invariant echo state network (TWIESN). [18] In [8], MCNN and t-LeNet were also evaluated. However, these two were inferior to the above methods in terms of Average Rank, which will be discussed later, and have been omitted for clarity of comparison. Table 6 summarizes the classification accuracies of all the aforementioned methods. The proposed method ranks second to the FCN in terms of average rank and consistently exhibits high accuracy on all datasets. In terms of the classification accuracy, the proposed method is comparable to FCN and outperforms most other machine learning methods. 5.3 Comparison of Hardware Resource Usage with Existing Machine Learning Methods The proposed model was implemented on an FPGA using a high-level synthesis tool to investigate its hardware resource usage. The Python code used in Section 5.1 for DFR and the Python code available in [8] for the other methods were rewritten in C and synthesized for the target FPGA. Table 7 presents the logic synthesis results. Figure 5 compares the number of block RAMs (BRAMs) required for the FPGA implementation. The smaller the numbers on both the horizontal and vertical axes indicate improved results (i.e., methods with results closer to the lower left achieves high accuracy with a smaller memory footprint). Methods based on DFR require a much smaller memory footprint than most machine- learning methods and ESN. Only Time-CNN with a very small number of filters in the convolution layers consumes approximately an equal number of BRAMs, but the accuracy of the proposed method is significantly better than that of Time-CNN. If DFR with OMS or RMS were plotted in the same figure, they would be located directly to the right of DFR_DPRR, i.e., similar accuracy while about 5x larger BRAM size. The number of BRAMs on the target FPGA board (Zynq-7000, xc7z020clg400-1) is 220, and thus the methods except DFR and Time-CNN cannot be implemented. Even if Zynq UltraScale ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:16 Ikeda et al. Table 6. Classification accuracy with other machine learning methods. Accuracy is in . The best and the second-best accuracies for each dataset is shown in bold and underlined respectively. The average accuracy is defined as the arithmetic mean of the accuracy. The average rank is defined as the arithmetic mean of the rank numbers being the most accurate method as 1. The data in the table, except for DFR, are from [8]. MLP FCN Res Encoder MCD Time- TWI DFR_ DFR_ Dataset Net CNN CNN ESN DRS DPRR ARAB 96.9 99.4 99.6 98.1 95.9 95.8 85.3 27.8 98.0 AUS 93.3 97.5 97.4 93.8 85.4 72.6 72.4 34.4 95.6 CHAR 96.9 99.0 99.0 97.1 93.8 96.0 92.0 33.9 96.2 CMU 60.0 100.0 99.7 98.3 51.4 97.6 89.3 93.1 100.0 ECG 74.8 87.2 86.7 87.2 50.0 84.1 73.7 67.0 88.0 JPVOW 97.6 99.3 99.2 97.6 94.4 95.6 96.5 62.4 97.8 KICK 61.0 54.0 51.0 61.0 56.0 62.0 67.0 60.0 80.0 LIB 78.0 96.4 95.4 78.3 65.1 63.7 79.4 30.0 78.3 NET 55.0 89.1 62.7 77.7 63.0 89.0 94.5 92.5 95.9 UWAV 90.1 93.4 92.6 90.8 84.5 85.9 75.4 26.2 86.2 WAF 89.4 98.2 98.9 98.6 65.8 94.8 94.9 90.2 99.0 WALK 70.0 100.0 100.0 100.0 45.0 100.0 94.4 100.0 100.0 Average Accuracy 80.3 92.8 90.2 89.9 70.9 86.4 84.6 59.8 92.9 Average Rank 5.92 2.25 3.08 3.42 7.58 5.50 5.92 7.08 2.50 Table 7. Synthesis results for different machine learning methods. Latency is in cycles. The dataset used is ARAB. Method Latency BRAM DSP FF LUT MLP 6,663,799 3,105 9 3,784 4,575 FCN 126,841,718 942 12 5,277 7,383 ResNet 227,560,415 1,951 87 34,673 37,984 Encoder 801,121,336 5,787 19 9,941 12,395 MCDCNN 16,328,669 8,547 62 11,909 14,882 Time-CNN 270,020 61 74 14,488 16,600 TWIESN 6,229,486 330 24 3,529 4,946 DFR_DRS 2,130 21 182 22,905 30,767 DFR_DPRR 84,532 57 65 12,083 14,152 MPSoC ZCU102, were used, in which the total number of available BRAMs is 892 (32.1 Mb), the other methods except for TWIESN still could not be implemented. Therefore, when comparing the hardware implementation efficiency using the power-delay product, which is equivalent to energy consumption, the BRAMs were excluded and the power consumption of other elements (i.e., LUT, FF, and DSP) was calculated using Xilinx power estimator (XPE). XPE version 2019.1.2 is used for Zynq-7000 as target. "Quick Estimate" was used to calculate the power and the setting was set as default except for "Design Utilization". Table 8 presents the summary of power estimation results. Figure 6 compares the power-delay product using the number of delay cycles obtained from the high-level synthesis tool and the power consumption without BRAM obtained using the power estimation tool. Similar to Fig. 5, the closer the results are to the lower left, the better is the method. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:17 Fig. 5. Comparison of memory usage. Each method is plotted using BRAM counts on the horizontal axis and the average rank calculated in Section 5.2 on the vertical axis Table 8. Power estimation results. Power is in W. For each method, the resource usage obtained from the high-level synthesis was entered in "Design Utilization". Note that these powers are independent of the number of BRAMs. Method CLOCK LOGIC DSP SUM MLP 0.102 0.095 0.004 0.201 FCN 0.111 0.110 0.005 0.226 ResNet 0.254 0.313 0.038 0.605 Encoder 0.137 0.143 0.008 0.288 MCDCNN 0.147 0.159 0.027 0.333 Time-CNN 0.160 0.172 0.033 0.365 TWIESN 0.102 0.096 0.011 0.209 DFR_DRS 0.202 0.253 0.080 0.535 DFR_DPRR 0.148 0.156 0.029 0.333 The proposed method, DFR_DPRR, operates with a power-delay product that is three orders of magnitude smaller than that of FCN but achieves nearly the same accuracy as FCN. The difference in power-delay product would become larger if the power consumption of BRAM is considered. Overall, the proposed method exhibited the best balance in terms of accuracy and hardware efficiency among the investigated methods. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:18 Ikeda et al. Fig. 6. Accuracy versus power-delay product. Each method is plotted using the product of power consumption except BRAM and latency in clock cycles. The power-delay product is on the horizontal axis, and the average rank is on the vertical axis. Table 9. Implementation results of proposed DFR_DPRR. The dataset used is ARAB. All 2200 testing data are processed. Power is the sum of the power of clocks, signals, logic, BRAM, and DSP. LUT 12,471 (23.44 ) LUTRAM 390 (2.24 ) FF 17,009 (15.99 ) BRAM 26 (18.57 ) DSP 67 (30.45 ) BUFG 1 (3.13 ) Clock 100 MHz Worst Negative Slack 0.566 ns Power 0.274 W Prediction Time (2200 time-series) 7.68 s Energy Consumption ( 1 time-series) 9.57E-04 J 5.4 Tradeoffs between Analog and Full Digital Implementation Analog and digital implementations exhibit pros and cons. Here, we discuss the tradeoff between the analog implementation of DFR [2, 16] and the proposed full digital implementation. An analog implementation of the reservoir would be beneficial in terms of power or energy. However, neither power consumption nor energy consumption was evaluated in existing studies [2, 16]. In general, analog implementation of DFRs require a DAC and ADC, and the non-linear ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:19 (such as Mackey-Glass) circuit should involve the design of an operational amplifier, making the overall circuit size larger. In addition, the design of such analog circuits and their components may require expertise and tuning, which can make it difficult to achieve maximum accuracy. The circuit design process can take a long time to complete. In contrast, the digital implementations, particularly the proposed fully digital DFR, are easy to adopt and more feasible with existing design tools. With digital design it is also easy to adjust the size and accuracy of the circuit by changing hyperparameters and data representations, such as input and output dimensions and the precision of fractional numbers. Furthermore, as our experiments in this section indicate, DFR_DPRR can achieve comparable or higher accuracy in comparison to the cutting-edge neural network-based methods as well as analog implementations. Finally, to allow future quantitative comparison of design tradeoffs among analog, digital, and other design styles, the implementation result of the proposed DFR_DPRR generated using Xilinx Vivado 2021.1 is summarized in Table 9. We confirmed that the proposed DFR_DPRR achieves exactly the same classification accuracy obtained in the Python simulations performed in Section 5.2. 6 CONCLUSION In this paper, we proposed a computationally efficient reservoir representation, namely DPRR, based on the dot product between features in the reservoir of shifted time. The proposed DPRR contributes to the implementation of accurate and compact hardware for reservoir-based classification. We also proposed a fully digital DFR using DPRR that is suitable for solving multivariate time-series classification tasks, which have been difficult to achieve high accuracy using reservoir computing methods. Through the FPGA implementation of the proposed DFR using DPRR, the proposed method was evaluated on 12 different multivariate time-series classification tasks. The DFR using DPRR outperformed conventional machine learning methods in terms of accuracy, despite its hardware resources in terms of power-delay product and memory usage being orders of magnitude smaller. The proposed model is therefore the most suitable among all investigated methods for solving multivariate time-series classification tasks via hardware. REFERENCES [1] Miquel L Alomar, Miguel C Soriano, Miguel Escalona-MorÃ¡n, Vincent Canals, Ingo Fischer, Claudio R Mirasso, and Jose L RossellÃ³. 2015. Digital implementation of a single dynamical node reservoir computer. IEEE Transactions on Circuits and Systems II: Express Briefs 62, 10 (2015), 977 981. [2] Lennert Appeltant, Miguel Cornelles Soriano, Guy Van der Sande, Jan Danckaert, Serge Massar, Joni Dambre, Benjamin Schrauwen, Claudio R Mirasso, and Ingo Fischer. 2011. Information processing using a single dynamical node as complex system. Nature Communications 2, 1 (2011), 1 6. [3] Lennert Appeltant, Guy Van der Sande, Jan Danckaert, and Ingo Fischer. 2014. Constructing optimized binary masks for reservoir computing with delay systems. Scientific Reports 4, 1 (2014), 1 5. [4] Witali Aswolinskiy, RenÃ© Felix Reinhart, and Jochen Steil. 2016. Time series classification in reservoir-and model-space: a comparison. In IAPR Workshop on Artificial Neural Networks in Pattern Recognition. 197 208. [5] Filippo Maria Bianchi, Simone Scardapane, Sigurd LÃ¸kse, and Robert Jenssen. 2020. Reservoir computing approaches for representation and classification of multivariate time series. IEEE Transactions on Neural Networks and Learning Systems 32, 5 (2020), 2169 2179. [6] JÃ©rÃ©mie Cabessa, Hugo Hernault, Heechang Kim, Yves Lamonato, and Yariv Z Levy. 2021. Efficient Text Classification with Echo State Networks. In International Joint Conference on Neural Networks. 1 8. [7] Huanhuan Chen, Fengzhen Tang, Peter Tino, and Xin Yao. 2013. Model-based kernel for efficient time series analysis. In Proceedings of ACM International Conference on Knowledge Discovery and Data Mining. 392 400. [8] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. 2019. Deep learning for time series classification: a review. Data Mining and Knowledge Discovery 33, 4 (2019), 917 963. [9] Herbert Jaeger. 2001. The echo state approach to analysing and training recurrent neural networks-with an erratum note. German National Research Center for Information Technology GMD Technical Report 148, 34 (2001), 13. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:20 Ikeda et al. [10] Mantas LukoÅ¡eviÄius and Herbert Jaeger. 2009. Reservoir computing approaches to recurrent neural network training. Computer Science Review 3, 3 (2009), 127 149. [11] Michael C Mackey and Leon Glass. 1977. Oscillation and chaos in physiological control systems. Science 197, 4300 (1977), 287 289. [12] Razvan Pascanu, Jack W Stokes, Hermineh Sanossian, Mady Marinescu, and Anil Thomas. 2015. Malware classification with recurrent networks. In IEEE International Conference on Acoustics, Speech and Signal Processing. 1916 1920. [13] Benjamin Schrauwen, David Verstraeten, and Jan Van Campenhout. 2007. An overview of reservoir computing: theory, applications and implementations. In Proceedings of European Symposium on Artificial Neural Networks. 471 482. [14] Joan SerrÃ , Santiago Pascual, and Alexandros Karatzoglou. 2018. Towards a Universal Neural Network Encoder for Time Series. In Current Challenges, New Trends and Applications. 120 129. [15] Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, and Lanyu Xu. 2016. Edge computing: Vision and challenges. IEEE Internet of Things Journal 3, 5 (2016), 637 646. [16] Miguel C Soriano, Silvia OrtÃ­n, Lars Keuninckx, Lennert Appeltant, Jan Danckaert, Luis Pesquera, and Guy Van der Sande. 2014. Delay-based reservoir computing: noise effects in a combined analog and digital implementation. IEEE Transactions on Neural Networks and Learning Systems 26, 2 (2014), 388 393. [17] Gouhei Tanaka, Toshiyuki Yamane, Jean Benoit HÃ©roux, Ryosho Nakane, Naoki Kanazawa, Seiji Takeda, Hidetoshi Numata, Daiju Nakano, and Akira Hirose. 2019. Recent advances in physical reservoir computing: A review. Neural Networks 115 (2019), 100 123. [18] Pattreeya Tanisaro and Gunther Heidemann. 2016. Time series classification using time warping invariant echo state networks. In IEEE International Conference on Machine Learning and Applications. 831 836. [19] Zhiguang Wang, Weizhong Yan, and Tim Oates. 2017. Time series classification from scratch with deep neural networks: A strong baseline. In International Joint Conference on Neural Networks. 1578 1585. [20] Bendong Zhao, Huanzhang Lu, Shangfeng Chen, Junliang Liu, and Dongya Wu. 2017. Convolutional neural networks for time series classification. Journal of Systems Engineering and Electronics 28, 1 (2017), 162 169. [21] Yi Zheng, Qi Liu, Enhong Chen, Yong Ge, and J Leon Zhao. 2014. Time series classification using multi-channels deep convolutional neural networks. In International Conference on Web-Age Information Management. 298 310. Received 00 January 0000; revised 00 January 0000; accepted 00 January 0000 ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nHardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification SOSEI IKEDA, Kyoto University, Japan HIROMITSU AWANO, Kyoto University, Japan TAKASHI SATO, Kyoto University, Japan Reservoir computing is attracting attention as a machine learning technique for edge computing. In time- series classification tasks, the number of features obtained using a reservoir depends on the length of the input series. Therefore, the features must be converted to a constant-length intermediate representation (IR), such that they can be processed by an output layer. Existing conversion methods involve computationally expensive matrix inversion that significantly increases the circuit size and requires processing power when implemented in hardware. In this paper, we propose a simple but effective IR, namely dot-product-based reservoir representation (DPRR), for reservoir computing based on the dot product of data features. Additionally, we propose a hardware-friendly delayed feedback reservoir (DFR) consisting of a nonlinear element and delayed feedback loop with DPRR. The proposed DFR successfully classified multivariate time series data that has been considered particularly difficult to implement efficiently in hardware. In contrast to conventional DFR models that require analog circuits, the proposed model can be implemented in a fully digital manner suitable for high-level syntheses. A comparison with existing machine learning methods via field-programmable gate array implementation using 12 multivariate time-series classification tasks confirmed the superior accuracy and small circuit size of the proposed method. CCS Concepts: Computer systems organization Neural networks. Additional Key Words and Phrases: reservoir computing, delayed feedback reservoir (DFR), edge computing ACM Reference Format: Sosei Ikeda, Hiromitsu Awano, and Takashi Sato. 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification. ACM Trans. Embedd. Comput. Syst. 00, 0, Article 000 ( 0000), 20 pages. 1 INTRODUCTION Edge computing, which is gaining attention in today s Internet of Things (IoT) society, is a method of processing and exchanging information with other devices in the vicinity of data sources [15]. To reduce the energy consumption of edge devices, processing near data sources such as sensors is crucial. In general, data sources generate a large amount of time-series data that must be processed in real time.\n\n--- Segment 2 ---\nTo reduce the energy consumption of edge devices, processing near data sources such as sensors is crucial. In general, data sources generate a large amount of time-series data that must be processed in real time. In addition, because processing is performed at the edge devices, they must operate with a low power consumption and be implemented using small hardware resources. Reservoir computing is a machine learning method that uses a reservoir. Input signals are transformed nonlinearly to a higher-dimensional space via a layer called a reservoir with fixed weights, and only the weights of the output layer are learned [10]. This method is considered to be particularly suitable for edge computing because it is capable of processing time-series data with its Authors addresses: Sosei Ikeda, Kyoto University, Kyoto, Japan, Hiromitsu Awano, Kyoto University, Kyoto, Japan, Takashi Sato, Kyoto University, Kyoto, Japan, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from 0000 Association for Computing Machinery. 1539-9087 0000 0-ART000 15.00 ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. arXiv:2504.11981v1 [cs.LG] 16 Apr 2025 000:2 Ikeda et al. recurrent structure that reflects past inputs. In addition, the reservoir layer does not require learning and can therefore be implemented in hardware utilizing a variety of physical phenomena [17]. The greatest obstacle in applying reservoir computing to real-world time-series classification tasks is the transformation of features obtained via the reservoir [7]. In general, the number of features obtained by the reservoir varies with the length of the input data series, thereby preventing the structure of the output layer from being fixed.\n\n--- Segment 3 ---\nThe greatest obstacle in applying reservoir computing to real-world time-series classification tasks is the transformation of features obtained via the reservoir [7]. In general, the number of features obtained by the reservoir varies with the length of the input data series, thereby preventing the structure of the output layer from being fixed. Therefore, it is necessary to convert these features to an intermediate representation (IR) that is independent of the data length, such that they can be processed using an output layer with a fixed structure. Thus far, no established method has been proposed for this conversion. For echo state networks (ESNs) [9] that are another variant of reservoir computing, several transformation methods have been proposed [4, 12]. However, those used in existing studies exhibited less classification accuracy than existing neural-network-based classification methods, especially in multivariate time-series classification tasks [5]. The reservoir model space (RMS) [5] is an example of IR that facilitates the classification of multivariate time-series data and improves the classification accuracy. However, converting features to RMS involves the inversion of a large matrix, which requires a substantial amount of hardware resources and computation time. In this paper, we propose a new feature transformation method that achieves both high accuracy and small circuit size. A dot-product-based reservoir representation (DPRR) is devised by focusing on the convolution of the time development of the features. We evaluated the effectiveness of applying DPRR to a delayed feedback reservoir (DFR) [2], which is a method of reservoir computing that enables a particularly compact construct consisting of only one nonlinear element and a delayed feedback loop. The combination of DPRR with DFR demonstrated excellent classification accuracy on multivariate time-series classification tasks and the smallest circuit size compared to existing machine learning classifiers based on neural networks. The important contributions of this work can be summarized as follows: (1) A novel IR called the dot-product-based reservoir representation (DPRR), which is computa- tionally efficient and effective for improving the classification accuracy of multivariate time series data, is proposed. (2) A fully digital delayed feedback reservoir (DFR) model with DPRR is proposed. The accuracy of DFR on multivariate time-series classification tasks is thoroughly evaluated. The accuracy and hardware cost are compared with those of existing machine learning classifiers.\n\n--- Segment 4 ---\nThe accuracy of DFR on multivariate time-series classification tasks is thoroughly evaluated. The accuracy and hardware cost are compared with those of existing machine learning classifiers. (3) A construct of input-data masking that supports multivariate time-series inputs and sup- presses inference accuracy variation is defined. The remainder of this paper is organized as follows. In Section 2, the concept and operation of reservoir computing are explained. In Section 3, existing reservoir representations are briefly reviewed. In Section 4, a reservoir representation called DPRR and a new DFR model that uses DPRR are proposed. In Section 5, an evaluation of the effectiveness of DPRR and classification accuracy of the proposed DFR implemented on a field-programmable gate array (FPGA), are presented. Finally, in Section 6, we conclude the paper. 2 RESERVOIR COMPUTING This section explains the concept of reservoir computing. Subsequently, ESN and DFR that are typical implementations of reservoir computing are discussed. 2.1 Concept of Reservoir Computing Reservoir computing is a machine learning method comprising three major components or layers: input layer, reservoir, and output layer [10]. First, the input layer converts the input signal into ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:3 a format suitable for entering the reservoir layer. Subsequently, the reservoir transforms the signal into a high-dimensional nonlinear vector. Throughout this paper, we refer to this vector as the reservoir state and to its components as features. Finally, the output layer performs pattern recognition using a simple learning algorithm. The characteristics of each layer are defined by the weights that connect the nodes. The weights of the input layer and reservoir are fixed and unaltered, whereas that of the output layer is determined through training. In time-series processing, reservoir computing is useful for two main tasks: regression and classification [13]. In the regression tasks, one output is expected for each input. In classification tasks, a single output is expected from multiple time-series inputs.\n\n--- Segment 5 ---\nIn the regression tasks, one output is expected for each input. In classification tasks, a single output is expected from multiple time-series inputs. The number of features obtained by the reservoir layer depends on the length of the input series; thus, the features need to be converted into an intermediate representation of a fixed length, regardless of the length of the inputs, such that they can be processed at the output layer of a fixed size [7]. In this paper, the intermediate representation is called the reservoir representation. In the following, for the sake of simplicity, we assume that the processing tasks are classification tasks and consider a model that involves conversion to a reservoir representation in its operations. 2.2 Echo State Network (ESN) An ESN is a realization of reservoir computing. The reservoir can be considered as a recurrent neural network with fixed weights. Figure 1 depicts a conceptual diagram of the ESN. ğ’–(ğ‘˜) Input layer Reservoir Output layer ğ’š ğ’™(ğ‘˜) ğ‘Š ğ‘Š ğ‘Šout in Fig. 1. Conceptual diagram of ESN [17]. The reservoir in the ESN is a recurrent neural network with fixed weight ğ‘Š. First, the input time series is represented as a series of vectors ğ’–(ğ‘˜) Rğ‘ğ‘¢(ğ‘˜ 1, 2, . . . ,ğ‘‡). Each ğ’–(ğ‘˜) represents a vector of real numbers, ğ‘‡is the length of the series or number of sampling time steps, and ğ‘ğ‘¢denotes the number of variables. When the number of nodes in the reservoir is ğ‘ğ‘¥, the weights of the input layer are represented as a matrix ğ‘Šin with ğ‘ğ‘¥rows and ğ‘ğ‘¢columns. The weights of the reservoir are represented as a matrix ğ‘Šwith ğ‘ğ‘¥rows and ğ‘ğ‘¥columns, and the hyperbolic tangent function is used as an activation function.\n\n--- Segment 6 ---\nWhen the number of nodes in the reservoir is ğ‘ğ‘¥, the weights of the input layer are represented as a matrix ğ‘Šin with ğ‘ğ‘¥rows and ğ‘ğ‘¢columns. The weights of the reservoir are represented as a matrix ğ‘Šwith ğ‘ğ‘¥rows and ğ‘ğ‘¥columns, and the hyperbolic tangent function is used as an activation function. The time evolution of the reservoir state ğ’™(ğ‘˜) Rğ‘ğ‘¥is expressed as follows: ğ’™(ğ‘˜ 1) tanh(ğ‘Šinğ’–(ğ‘˜ 1) ğ‘Šğ’™(ğ‘˜)). (1) ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:4 Ikeda et al. Next, the reservoir representation ğ’“ Rğ‘ğ‘Ÿis obtained from the reservoir states ğ’™(1), ğ’™(2), . . . , ğ’™(ğ‘‡). In most cases, even within a dataset, the length of the input time series varies for each input data instance. For example, in spoken language classification, the lengths of the time series of the input corresponding to "1" and "14" differ because of the difference in the number of syllables. Accordingly, the length of the reservoir state ğ‘‡differs. Reservoir representations should be able to absorb these differences. Regardless of the length of the input series, a constant-length vector should always be produced. This is a strict requirement that comes from the fact that the output layer that converts the reservoir representation to output ğ’š, is fixed [7]. Here, the weight of the output layer, ğ‘Šout, is trained in advance for the use of the ESN, such that ğ’š ğ‘Šoutğ’“provides a good prediction. Here, ğ’š Rğ‘ğ‘¦is the output and ğ‘ğ‘¦is the number of classification classes. Note that the target output is given in a one-hot representation, with 1 for the correct label and 0 for the others.\n\n--- Segment 7 ---\nHere, ğ’š Rğ‘ğ‘¦is the output and ğ‘ğ‘¦is the number of classification classes. Note that the target output is given in a one-hot representation, with 1 for the correct label and 0 for the others. 2.3 Delayed Feedback Reservoir (DFR) The DFR is another realization of reservoir computing in which the reservoir consists of only one nonlinear element and a delayed feedback loop. Compared with other reservoir computing schemes, DFR is easier to implement as hardware because of its simple structure [17]. Figure 2 illustrates a conceptual diagram of DFR. ğœ‚ NL ğ‘¥(ğ‘¡) ğ‘¥(ğ‘¡-ğœ) ğ‘ğœƒ ğœ ğ‘¥ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğœƒ ğ›¾ Reservoir Output layer ğ‘—(ğ‘¡) Input layer ğ’–(ğ‘˜) ğ’š (Masking) Fig. 2. Conceptual diagram of DFR [2]. The reservoir in the DFR consists of a nonlinear element (NL) and a feedback loop with a total delay ğœ. The feedback loop consists of ğ‘ğ‘¥virtual nodes having equal time intervals ğœƒ. Most of the existing hardware implementations of the reservoir layer in DFR [2, 16] operate entirely using analog circuits. Therefore, the input ğ’–(ğ‘˜), typically represented as digital values such as the IEEE-754 float, is converted to an analog signal before passing to the reservoir layer. The analog output of the reservoir layer is then converted into a digital value. More specifically, after applying the masking process to the input time series ğ’–(ğ‘˜), it is converted to analog signal ğ‘—(ğ‘¡) (Fig. 2). This masking is a preprocessing step for the input signal, which is equivalent to multiplying the input by ğ‘Šin in the case of ESN. The purpose of masking is to modulate the input signal using the mask signal. Figure 3 illustrates an example of mask processing for a univariate time-series input.\n\n--- Segment 8 ---\nThe purpose of masking is to modulate the input signal using the mask signal. Figure 3 illustrates an example of mask processing for a univariate time-series input. The input signal ğ‘–(ğ‘¡) is multiplied ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:5 ğ‘¢(ğ‘˜) ğ‘¢(ğ‘˜-1) ğ‘¢(ğ‘˜ 1) ğœ ğœƒ ğ‘–(ğ‘¡) 0 0 0 ğ‘¡ ğ‘¡ ğ‘¡ ğ‘—(ğ‘¡) ğ‘š(ğ‘¡) ğœ ğœƒ ğœ Fig. 3. Masking process (univariate input example for clarity). ğ‘–is the output of digital-to-analog conversion of the original digital input signal ğ‘¢. The signal ğ‘–maintains a constant value for each time duration of ğœ. The mask signal ğ‘šchanges at time intervals of ğœƒand has a period of ğœ. The input to the reservoir is given as ğ‘—(ğ‘¡) ğ‘–(ğ‘¡) ğ‘š(ğ‘¡). by the mask signal with a faster sample rate and amplitude of either 1 or 1. Note that the masking process for univariate inputs has been explained in several studies [2, 3]; however, that for the multivariate input case has not yet been adequately investigated. The masking process for a multivariate input signal is defined as follows. Letğ‘–1(ğ‘¡),ğ‘–2(ğ‘¡), . . . ,ğ‘–ğ‘ğ‘¢(ğ‘¡) be the multivariate input signals: ğ‘–1(ğ‘¡) ğ‘–2(ğ‘¡) ğ‘–ğ‘ğ‘¢(ğ‘¡) Âª ğ’–(ğ‘˜). (2) Here, the input ğ’–(ğ‘˜) remains the same for the time duration ğœ.\n\n--- Segment 9 ---\n,ğ‘–ğ‘ğ‘¢(ğ‘¡) be the multivariate input signals: ğ‘–1(ğ‘¡) ğ‘–2(ğ‘¡) ğ‘–ğ‘ğ‘¢(ğ‘¡) Âª ğ’–(ğ‘˜). (2) Here, the input ğ’–(ğ‘˜) remains the same for the time duration ğœ. The length of the mask patterns is equal to the virtual nodes ğ‘ğ‘¥in the DFR, which is designed such that the following relationship holds: ğ‘ğ‘¥ğœƒ ğœ. (3) Here, the delay between the nodes in the reservoir is ğœƒ. Therefore, the constant input during ğœis divided into ğ‘ğ‘¥equal intervals of ğœƒ, and each mask value is applied for the time interval of ğœƒ. The converted input ğ‘—(ğ‘¡) can be expressed as follows: ğ‘—(ğ‘¡) ğ‘ğ‘¢ ğ‘ 1 ğ‘šğ‘(ğ‘¡)ğ‘–ğ‘(ğ‘¡), (4) ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:6 Ikeda et al. where ğ‘š1(ğ‘¡),ğ‘š2(ğ‘¡), . . . ,ğ‘šğ‘ğ‘¢(ğ‘¡) are mask functions, each having a length of ğ‘ğ‘¥. Once ğ‘—(ğ‘¡) is obtained, ğ‘—(ğ‘¡) is multiplied by ğ›¾and added to the feedback signal in the reservoir. It is then entered into a one-input, one-output nonlinear (NL) device, whose output is multiplied by ğœ‚ and becomes ğ‘¥(ğ‘¡). Thereafter, ğ‘¥(ğ‘¡) is added to ğ›¾ğ‘—(ğ‘¡) through a feedback loop with a total delay time of ğœ.\n\n--- Segment 10 ---\nIt is then entered into a one-input, one-output nonlinear (NL) device, whose output is multiplied by ğœ‚ and becomes ğ‘¥(ğ‘¡). Thereafter, ğ‘¥(ğ‘¡) is added to ğ›¾ğ‘—(ğ‘¡) through a feedback loop with a total delay time of ğœ. Virtual nodes with interval ğœƒare serially connected to form a feedback loop in the reservoir. These nodes operate as shift registers. The values stored in the virtual nodes are the features and they collectively form a vector with ğ‘ğ‘¥elements, which is the reservoir state. Therefore, the state that this reservoir holds in the delay elements is expressed as: ğ’™(ğ‘˜) [ğ‘¥(ğ‘˜ğœ ğœƒ),ğ‘¥(ğ‘˜ğœ 2ğœƒ), . . . ,ğ‘¥(ğ‘˜ğœ ğœ)]. (5) Most of the delayed feedback reservoirs use the Mackey-Glass model [11] as a nonlinear element[1, 2, 16], which is expressed as follows: d dğ‘¡ğ‘¥(ğ‘¡) ğ‘¥(ğ‘¡) ğœ‚[ğ‘¥(ğ‘¡ ğœ) ğ›¾ğ‘—(ğ‘¡)] 1 [ğ‘¥(ğ‘¡ ğœ) ğ›¾ğ‘—(ğ‘¡)]ğ‘, (6) where ğ‘is an adjustable parameter. There are two reasons why this model is often used [2]. First, it is easy to implement in analog electronic circuits. Second, the nonlinearity can be adjusted by changing ğ‘. The existing studies often use ğ‘ 7 for ease of hardware implementation. Next, the reservoir representation ğ’“ Rğ‘ğ‘Ÿis obtained for the reservoir states ğ’™(1), ğ’™(2), . . . , ğ’™(ğ‘‡) for use in the output layer. The construction of a reservoir representation is critically important because it significantly affects both the accuracy of the DFR and hardware resource usage. We discuss the implementation of the reservoir representation in the next section.\n\n--- Segment 11 ---\nThe construction of a reservoir representation is critically important because it significantly affects both the accuracy of the DFR and hardware resource usage. We discuss the implementation of the reservoir representation in the next section. The construct and function of the output layer are similar to those of ESN. The weight of the output layer, ğ‘Šout is learned in advance, as in ESN. 3 RESERVOIR REPRESENTATION The reservoir maps the input signal to a higher dimensional space [13]. Let the elements of ğ’™(ğ‘˜) (i.e., the features) be [ğ‘¥(ğ‘˜)1,ğ‘¥(ğ‘˜)2, . . . ,ğ‘¥(ğ‘˜)ğ‘ğ‘¥]. The reservoir then yields a total of ğ‘‡ ğ‘ğ‘¥features that are denoted by ğ‘¥(ğ‘˜)ğ‘›(ğ‘˜ 1, 2, . . . ,ğ‘‡; ğ‘› 1, 2, . . . , ğ‘ğ‘¥). As explained in the previous sections, because the length of the input series ğ‘‡may vary, so does the number of features. In this situation, the features cannot be processed with an output layer of a fixed size. Consequently, conversion to reservoir representation with a fixed length is required. The following subsections review the existing reservoir representations. 3.1 Last Reservoir State (LRS) In [12], LRS after all the input series were entered was used as the reservoir representation. The reservoir representation is then expressed as: ğ’“ ğ’™(ğ‘‡). (7) The underlying assumption of this method is that the reservoir cumulatively stores information about past inputs. Therefore, the LRS is considered to contain all information for the entire input. The number of dimensions available with this representation is ğ‘ğ‘¥and it does not depend on the series lengthğ‘‡. However, ğ‘ğ‘¥is relatively small (otherwise, the hardware cost increases). As a result, the prediction of reservoirs using this reservoir representation is less accurate than that using existing neural-network-based classifiers, particularly for multivariate time-series classification tasks [5]. ACM Trans. Embedd. Comput. Syst., Vol. 00, No.\n\n--- Segment 12 ---\nSyst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:7 3.2 Direct Reservoir State (DRS) In the DRS, the reservoir state at each sample time is used as a reservoir representation: ğ’“(ğ‘˜) ğ’™(ğ‘˜). (8) As the reservoir representation is available for every sample timeğ‘‡, the outputs,ğ’š(1),ğ’š(2), . . . ,ğ’š(ğ‘‡), are calculated using the output layer. The mode of the ğ‘‡-inferred labels is used as the output inference label. This representation is often used for time-series classification using DFR [2, 16]. Similar to the LRS, the number of features in DRS is ğ‘ğ‘¥. Owing to its low dimensionality, this method also yields less accurate results than existing neural-network-based classifiers, especially for multivariate time series classification tasks [5]. 3.3 Maximal Reservoir States (MRSs) In the MRSs,ğ‘‡max, which is the maximum series lengthğ‘‡of all possible inputs, is applied to all other inputs to equalize the feature length. ğ‘‡max ğ‘ğ‘¥features denoted by ğ‘¥(ğ‘˜)ğ‘›(ğ‘˜ 1, 2, . . . ,ğ‘‡max; ğ‘› 1, 2, . . . , ğ‘ğ‘¥) are then used as a reservoir representation [6]. In this method, there are several options for extending short data to the maximum length. The first option is to append zeros at the end of the input for which no value is given: ğ’–(ğ‘˜) 0 (ğ‘˜ ğ‘‡ 1,ğ‘‡ 2, . . . ,ğ‘‡max). (9) The other option is to append zeros for the reservoir state: ğ’™(ğ‘˜) 0 (ğ‘˜ ğ‘‡ 1,ğ‘‡ 2, . . . ,ğ‘‡max).\n\n--- Segment 13 ---\n. ,ğ‘‡max). (10) The greatest drawback of this representation is the difficulty in determining the necessary and sufficient ğ‘‡max value in advance. Setting ğ‘‡max to be too large would increase hardware resources. In addition, the zero-filling process of the input (Eq. (9)) requires additional processing until ğ‘‡max is reached, which reduces throughput, whereas the zero-filling required for the reservoir state (Eq. (10)) cannot add additional information. 3.4 Output Model Space (OMS) In the OMS, linear regression is first used to predict the input that is one time ahead of the reservoir state: ğ’–(ğ‘˜ 1) ğ‘…omsğ’™(ğ‘˜) ğ’“oms. (11) Here, ğ‘…oms and ğ’“oms are constant regardless of ğ‘˜and are subject to regression. ğ’™ (ğ‘˜) is defined as: ğ’™ (ğ‘˜) [ğ’™(ğ‘˜), 1]. (12) Eq. (11) can be rewritten as: ğ’–(ğ‘˜ 1) ğ‘… omsğ’™ (ğ‘˜). (13) Note that the dimension of ğ‘… oms Rğ‘ğ‘¢ (ğ‘ğ‘¥ 1) holds. Let ğ‘ˆ [ğ’–(1), ğ’–(2), . . . , ğ’–(ğ‘‡)] and (14) ğ‘‹ [ğ’™ (0), ğ’™ (1), . . . , ğ’™ (ğ‘‡ 1)]. (15) Then, ğ‘… oms may be ridge regressed using the Moore-Penrose pseudoinverse with ğ¸as the (ğ‘ğ‘¥ 1)th- order unit matrix and ğœ†as a parameter: ğ‘… oms ğ‘ˆ ğ‘‹ ğ‘‡(ğ‘‹ ğ‘‹ ğ‘‡ ğœ†ğ¸) 1. (16) OMS is a single-column rearrangement of ğ‘… oms [5, 7]: ğ’“ vec(ğ‘… oms).\n\n--- Segment 14 ---\n(15) Then, ğ‘… oms may be ridge regressed using the Moore-Penrose pseudoinverse with ğ¸as the (ğ‘ğ‘¥ 1)th- order unit matrix and ğœ†as a parameter: ğ‘… oms ğ‘ˆ ğ‘‹ ğ‘‡(ğ‘‹ ğ‘‹ ğ‘‡ ğœ†ğ¸) 1. (16) OMS is a single-column rearrangement of ğ‘… oms [5, 7]: ğ’“ vec(ğ‘… oms). (17) ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:8 Ikeda et al. The derivation of OMS is based on the idea that there is a constant linear transformation from the reservoir state to the input that is one time ahead, because ESNs with a linear output layer are effective for time series prediction tasks [7]. In OMS, the number of features is ğ‘ğ‘¢ (ğ‘ğ‘¥ 1), which is independent of the series length ğ‘‡and shows better classification accuracy than LRS and MRS [5]. However, OMS involves the computation of a high-dimensional inverse matrix, which increases the circuit size when implemented in hardware. 3.5 Reservoir Model Space (RMS) In the RMS, linear regression is first performed to predict the reservoir state, one time ahead. ğ’™(ğ‘˜ 1) ğ‘…rmsğ’™(ğ‘˜) ğ’“rms. (18) Here, ğ‘…rms and ğ’“rms are constant regardless of ğ‘˜and are subjected to regression. Introducing ğ‘… rms with a definition similar to ğ‘… oms yields ğ’™(ğ‘˜ 1) ğ‘… rmsğ’™ (ğ‘˜). Again, similarly to the OMS, using ğ‘‹ [ğ’™(1), ğ’™(2), . . . , ğ’™(ğ‘‡)] and (19) ğ‘‹ [ğ’™ (0), ğ’™ (1), . . .\n\n--- Segment 15 ---\n. . , ğ’™ (ğ‘‡ 1)], (20) the ridge regression of Eq. (19) leads to ğ‘… rms ğ‘‹ ğ‘‹ ğ‘‡(ğ‘‹ ğ‘‹ ğ‘‡ ğœ†ğ¸) 1. (21) The RMS is a single-column reservoir representation, defined as follows [5]: ğ’“ vec(ğ‘… rms). (22) The RMS has ğ‘ğ‘¥ (ğ‘ğ‘¥ 1) features, which is larger than the OMS. As a result, the RMS exhibits even higher accuracy than the OMS [5] at the cost of calculating the inversion of an even higher- dimensional matrix than the OMS, which leads to a larger circuit size. 4 DELAYED FEEDBACK RESERVOIR WITH DOT-PRODUCT-BASED RESERVOIR REPRESENTATION 4.1 Dot-Product-based Reservoir Representation (DPRR) Here, we propose DPRR, which is a novel reservoir representation based on the dot product across reservoir states of different times. Both the OMS and RMS are effective for improving inference accuracy [5]. However, the calcula- tion of both reservoir representations involves the inversion of a large matrix that significantly increases the circuit size when implemented in hardware. The fundamental requirements of reservoir representation are as follows: 1) it should retain information in a high-dimensional space; 2) its size should be constant and independent of the time series; and 3) it should be easy to calculate. The vector of a node in the reservoir: ğ’™ğ‘› [ğ‘¥(1)ğ‘›,ğ‘¥(2)ğ‘›, . . . ,ğ‘¥(ğ‘‡)ğ‘›], (23) can be considered to be the time evolution of the node state. A new reservoir representation can be derived by taking the dot product of the vectors of two nodes: ğ’™ğ‘– ğ’™ğ‘— ğ‘‡ ğ‘˜ 1 ğ‘¥(ğ‘˜)ğ‘–ğ‘¥(ğ‘˜)ğ‘—. (ğ‘–, ğ‘— 1, 2, . . . , ğ‘ğ‘¥).\n\n--- Segment 16 ---\n. , ğ‘ğ‘¥). (24) This yields ğ‘2 ğ‘¥features that are independent of the length of the series ğ‘‡. However, because ğ’™ğ‘– ğ’™ğ‘— ğ’™ğ‘— ğ’™ğ‘–holds, the actual number of features is reduced by approximately half. Furthermore, this representation completely loses information regarding the time variation in the reservoir state. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:9 To eliminate the loss of features while retaining the information concerning time variation, we utilize the time evolution of the features with samples of shifted time. ğ’™ğ‘› [ğ‘¥(1)ğ‘›,ğ‘¥(2)ğ‘›, . . . ,ğ‘¥(ğ‘‡)ğ‘›], (25) ğ’™ ğ‘› [ğ‘¥(0)ğ‘›,ğ‘¥(1)ğ‘›, . . . ,ğ‘¥(ğ‘‡ 1)ğ‘›]. (26) Taking the dot product of these vectors for two nodes yields: ğ’™ğ‘– ğ’™ ğ‘— ğ‘‡ ğ‘˜ 1 ğ‘¥(ğ‘˜)ğ‘–ğ‘¥(ğ‘˜ 1)ğ‘—. (ğ‘–, ğ‘— 1, 2, . . . , ğ‘ğ‘¥). (27) By shifting the dot product by one time step and considering all node combinations, we can obtain ğ‘2 ğ‘¥features, which are independent of the time series, while retaining the information of the temporal evolution. Note also that the dot product of the features at different times is not commutative. In addition, we added the reservoir state itself as a feature: ğ’™ğ‘– 1 ğ‘‡ ğ‘˜ 1 ğ‘¥(ğ‘˜)ğ‘–. (ğ‘– 1, 2, . . . , ğ‘ğ‘¥).\n\n--- Segment 17 ---\n. , ğ‘ğ‘¥). (28) The above ğ‘ğ‘¥ (ğ‘ğ‘¥ 1) values were used as the proposed DPRR. These features are represented as vectors. Using ğ’™ (ğ‘˜) [ğ’™(ğ‘˜), 1], DPRR can be represented as follows: ğ’“ vec( ğ‘‡ ğ‘˜ 1 ğ’™(ğ‘˜)ğ’™ (ğ‘˜ 1)). (29) DPRR is expected to be implemented more efficiently than OMS or RMS because it can be calculated by using only matrix multiplications. 4.2 Fully Digital DFR for Hardware Implementation Here, we describe a DFR model that uses the proposed reservoir representation. In contrast to existing DFRs that use a mixture of digital and analog signals, our model operates entirely digitally making it particularly easy to implement in FPGAs. In addition, the proposed model incorporates the following: 1) the reservoir layer applies a lightweight nonlinear function without storing weights that require a large amount of memory; 2) the masking method suppresses the accuracy variation and supports multivariate time series inputs; and 3) the learning method of an output layer based on ridge regression with a constant term improves classification accuracy. Algorithms 1 and 2 show pseudo codes for the algorithms used in the nonlinear element part of the DFR. Algorithm 1 Calculate ğ‘“ Input: ğ‘¥, ğ‘— 1: ğ‘¡ (ğ‘¥ ğ›¾ ğ‘—) 2: return ğœ‚ğ‘¡ [1 ğ‘¡2] ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:10 Ikeda et al. Algorithm 2 Calculate MackeyGlass Input: ğ’™(ğ‘˜ 1) [ğ‘¥(ğ‘˜ 1)1,ğ‘¥(ğ‘˜ 1)2, . . . ,ğ‘¥(ğ‘˜ 1)ğ‘ğ‘¥] Input: ğ’‹(ğ‘˜) [ğ‘—(ğ‘˜)1, ğ‘—(ğ‘˜)2, . . .\n\n--- Segment 18 ---\n. . , ğ‘—(ğ‘˜)ğ‘ğ‘¥] 1: ğ‘¥(ğ‘˜)1 ğ‘¥(ğ‘˜ 1)ğ‘ğ‘¥e ğœƒ (1 e ğœƒ)ğ‘“(ğ‘¥(ğ‘˜ 1)1, ğ‘—(ğ‘˜)1) 2: for ğ‘› 2 to ğ‘do 3: ğ‘¥(ğ‘˜)ğ‘› ğ‘¥(ğ‘˜)ğ‘› 1e ğœƒ (1 e ğœƒ)ğ‘“(ğ‘¥(ğ‘˜ 1)ğ‘›, ğ‘—(ğ‘˜)ğ‘›) 4: end for 5: return ğ’™(ğ‘˜) Figure 4 shows a block diagram of the proposed fully digital DFR used in this study. ğ’–(1) ğ’–(2) ğ’–(ğ‘‡) ğ‘ğ‘¢ ğ‘ğ‘¢ ğ‘ğ‘¥ ğ‘€ ğ‘ğ‘¥ ğ’‹(ğ‘˜) ğ‘ğ‘Ÿ ğ’“ ğ‘ğ‘¦ ğ‘ğ‘Ÿ ğ‘Šout ğ’š ğ‘ğ‘¦ ğ‘¦ ğ‘ğ‘¥ ğ’™(ğ‘˜) ğ’™(ğ‘˜-1) ğ‘ğ‘¥ ğ‘˜ 1, 2, , ğ‘‡ Input layer (Masking) Feedback memory Dot-product-based reservoir representation (DPRR) Output layer 1 1 1 1 Nonlinear element Eq. (30) MackeyGlass Eq. (42) Eq. (29) Eq. (39), Eq. (40) Fig. 4. Circuit block diagram and dataflow of the proposed fully digital DFR with DPRR. The operation of "MackeyGlass" is defined in Algorithms 1 and 2.\n\n--- Segment 19 ---\nCircuit block diagram and dataflow of the proposed fully digital DFR with DPRR. The operation of "MackeyGlass" is defined in Algorithms 1 and 2. The signal after the masking process ğ’‹(ğ‘˜) is expressed as: [2] ğ’‹(ğ‘˜) ğ‘€ğ’–(ğ‘˜). (30) Here, ğ‘€is a masking matrix whose elements are either 1 or 1. The size of ğ‘€is ğ‘ğ‘¥ ğ‘ğ‘¢. The elements of ğ‘€can be determined randomly. However, through separate experiments, we found that this resulted in a larger variation in classification accuracy. To suppress this accuracy variation associated with the randomly determined elements, each column of ğ‘€is determined pseudo- randomly based on the maximum length sequence (m-sequence) [3]. The underlying assumption in this method is that it is desirable for the mask function to contain various combinations of consecutive values. For example, consider a sequence of two consecutive numbers in 01001 ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:11 this contains three combinations: 01 , 10 , and 00. We intended to maximize the variation in the combinations included in the sequence. Here, we explain how the mask functions are generated based on the m-sequence. This was briefly explained in [3] for the univariate case. We extend the procedure to multivariate time-series inputs. The proposed procedure is summarized as follows: (1) Choose a primitive polynomial to use. (2) Determine an initial value that has the same length as the degree ğ‘šof the primitive polyno- mial. (3) Calculate the m-sequence. (4) Insert a zero immediately after (ğ‘š 1) consecutive zeros. If there is no such location, we insert zero at the beginning. (5) At the end of the sequence, append the (ğ‘š 1) initial values with the trailing digit removed. (6) Replace 0 with 1 to obtain a column vector of ğ‘€.\n\n--- Segment 20 ---\n(5) At the end of the sequence, append the (ğ‘š 1) initial values with the trailing digit removed. (6) Replace 0 with 1 to obtain a column vector of ğ‘€. (7) For multivariate time-series inputs, generate column vectors of ğ‘€by the round rotation of the sequence derived above. We show an example of how a sequence can be generated by choosing a primitive polynomial ğº(ğ‘¥) ğ‘¥3 ğ‘¥ 1 of degree ğ‘š 3. In this case, we are interested in variations in the sequence of three consecutive numbers. Assuming the maximal length sequence to be (ğ‘ğ‘›), the recurrence relation (base 2) can be expressed as follows: ğ‘ğ‘› ğ‘ğ‘› 2 ğ‘ğ‘› 3. (31) When the given set of initial value is ğ‘0 0, ğ‘1 0, and ğ‘2 1, the m-sequence is derived as follows: (ğ‘ğ‘›) {0, 0, 1, 0, 1, 1, 1}. (32) Among the eight different sequences of three consecutive binary numbers, (ğ‘ğ‘›) contains all se- quences except 000 when considering wrap-around rotations. Without the rotation, (ğ‘ğ‘›) does not contain 110 and 100 . An operation is conducted to insert a zero immediately after (ğ‘š 1) consecutive zeros, which yields a 000 sequence, and ğ‘0, . . . ,ğ‘ğ‘š 2 at the end, which yields 110 and 100 . (ğ‘ ğ‘›) {0, 0, 0, 1, 0, 1, 1, 1, 0, 0}. (33) After replacing the 0 values with 1, the column vector of ğ‘€is represented as follows: { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1}. (34) Therefore, ğ‘ğ‘¥ 2ğ‘š ğ‘š 1 holds for primitive polynomials of degree ğ‘š. After masking, the reservoir state ğ’™(ğ‘˜) is obtained via the delayed feedback path.\n\n--- Segment 21 ---\n(34) Therefore, ğ‘ğ‘¥ 2ğ‘š ğ‘š 1 holds for primitive polynomials of degree ğ‘š. After masking, the reservoir state ğ’™(ğ‘˜) is obtained via the delayed feedback path. As a nonlinear function, the Mackey-Glass model in Eq. (6) is adopted. The parameter ğ‘ 2 is chosen in our fully digital implementation to reduce computational resources while retaining nonlinearity. The secod term in Eq. (6) can be rewritten as ğ‘“: ğ‘“(ğ‘¥,ğ‘¡) ğœ‚[ğ‘¥ ğ›¾ğ‘—] 1 [ğ‘¥ ğ›¾ğ‘—]ğ‘. (35) Solving the differential equation in Eq. (6) by assuming ğ‘“to be constant for a small time ğœƒ, which is the interval of virtual nodes, yields ğ‘¥(ğ‘¡): ğ‘¥(ğ‘¡) ğ‘¥0e ğ‘¡ (1 e ğ‘¡)ğ‘“(ğ‘¥(ğ‘¡ ğœ), ğ‘—(ğ‘¡)). (36) ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:12 Ikeda et al. Here, ğ‘¥0 is the initial value of ğ‘¥(ğ‘¡) at each ğœƒ[2]. Then, the value of the next virtual node of the node with ğ‘¥0 is represented by ğ‘¥(ğœƒ). All of these operations are carried out in digital domain. Let the components of ğ’™(ğ‘˜) and ğ’‹(ğ‘˜) be: ğ’™(ğ‘˜) [ğ‘¥(ğ‘˜)1,ğ‘¥(ğ‘˜)2, . . . ,ğ‘¥(ğ‘˜)ğ‘ğ‘¥], (37) ğ’‹(ğ‘˜) [ğ‘—(ğ‘˜)1, ğ‘—(ğ‘˜)2, . . .\n\n--- Segment 22 ---\n. . , ğ‘—(ğ‘˜)ğ‘ğ‘¥]. (38) Then, ğ’™(ğ‘˜) is derived recurrently as follows: ğ‘¥(ğ‘˜)1 ğ‘¥(ğ‘˜ 1)ğ‘ğ‘¥e ğœƒ (1 e ğœƒ)ğ‘“(ğ‘¥(ğ‘˜ 1)1, ğ‘—(ğ‘˜)1), (39) ğ‘¥(ğ‘˜)ğ‘› ğ‘¥(ğ‘˜)ğ‘› 1e ğœƒ (1 e ğœƒ)ğ‘“(ğ‘¥(ğ‘˜ 1)ğ‘›, ğ‘—(ğ‘˜)ğ‘›). (ğ‘› 2) (40) We use 0 as the initial values of the reservoir state. The proposed DPRR uses a time-shifted feature, wherein it is necessary to store not only ğ’™(ğ‘˜) but also ğ’™(ğ‘˜ 1). For this purpose, the memory required to store ğ’™(ğ‘˜) is duplicated. As ğœƒis constant, the values of e ğœƒand (1 e ğœƒ) are constants and they can therefore be precalculated and embedded in the logic circuit. The recurrent equation approach for solving differential equations is intended to balance the accuracy and computational complexity. In a previous study, either the Euler [1] or Heun [3] methods were used. The Euler method is relatively simple but may compromise the classification accuracy, whereas the Heun method is relatively complex and requires a larger circuit for hardware implementation. Next, the reservoir representation ğ’“is obtained using DPRR from the reservoir states. Finally, outputğ’šis obtained. First, a constant term is added to improve the classification accuracy: ğ’“ [ğ’“, 1]. (41) The weight matrix ğ‘Šout of size ğ‘ğ‘¦ (ğ‘ğ‘Ÿ 1) is used to compute ğ’š: ğ’š ğ‘Šoutğ’“ . (42) The largest value in the elements of ğ’šis used as the inference label.\n\n--- Segment 23 ---\n(41) The weight matrix ğ‘Šout of size ğ‘ğ‘¦ (ğ‘ğ‘Ÿ 1) is used to compute ğ’š: ğ’š ğ‘Šoutğ’“ . (42) The largest value in the elements of ğ’šis used as the inference label. Ridge regression is used to determine ğ‘Šout. 5 EVALUATION In this section we evaluate the classification accuracy and required hardware resources of the proposed fully digital DFR with DPRR. First, a comparison with different reservoir representa- tions is made. Thereafter, we compare our results with those of other machine learning methods. Comparisons are made through both software and FPGA implementations, the former to evaluate classification performance and the latter to evaluate hardware resource usage. Finally, the tradeoffs between analog and full digital implementation are discussed. For the FPGA implementation, we used Xilinx Vitis HLS 2021.1 as the high-level synthesis tool and Zynq-7000 (xc7z020clg400-1) as the target FPGA board. The numbers are represented in IEEE-754 floating-point format, and the data paths were not pipelined. In practice, it may be possible to reduce the bit width of number representations and to improve throughput by pipelining. However, as it is difficult to ensure an equal level of optimizations across multiple machine learning methods, this setting was selected for fair comparison. A summary of the datasets used in the evaluation is listed in Table 1. Throughout the evaluation, we used the npz files available in [5]. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:13 Table 1. Summary of multivariate time series classification datasets (taken from [5]). Column ğ‘‰, ğ¶, Train, and Test respectively reports the input dimension, the output dimension, the number of training data, and that of testing data. ğ‘‡min and ğ‘‡max are the shortest and the longest length of the input series in the dataset, respectively.\n\n--- Segment 24 ---\nColumn ğ‘‰, ğ¶, Train, and Test respectively reports the input dimension, the output dimension, the number of training data, and that of testing data. ğ‘‡min and ğ‘‡max are the shortest and the longest length of the input series in the dataset, respectively. Dataset ğ‘‰ ğ¶ Train Test ğ‘‡min ğ‘‡max ARAB 13 10 6600 2200 4 93 AUS 22 95 1140 1425 45 136 CHAR 3 20 300 2558 109 205 CMU 62 2 29 29 127 580 ECG 2 2 100 100 39 152 JPVOW 12 9 270 370 7 29 KICK 62 2 16 10 274 841 LIB 2 15 180 180 45 45 NET 4 13 803 534 50 994 UWAV 3 8 200 427 315 315 WAF 6 2 298 896 104 198 WALK 62 2 28 16 128 1918 5.1 Effects of Reservoir Representation First, the effect of reservoir representation was studied. DFR with various reservoir representations was implemented in Python. The dataset used in this section was ARAB. Because different reservoir representations yield different value ranges, the hyperparameters ğ›¾ and ğœ‚must be adjusted to attain improved accuracy. Table 2 shows the hyperparameters used in this evaluation that were found through a grid search in the range {0.03, 0.1, 0.3, 1}. Here, MRS_upad and MRS_xpad are the maximal reservoir states with zero-filling for the input and reservoir states, respectively. Table 2. Parameters used in comparison with existing reservoir representation. The parameters ğœƒ 0.25, ğ›½ 0.01, and ğœ† 1 are commonly used. The dataset is ARAB. LRS DRS MRS MRS OMS RMS DPRR _upad _xpad ğ›¾ 0.03 0.3 0.1 0.03 0.03 0.03 0.03 ğœ‚ 1 0.1 0.1 1 1 1 1 Table 3 lists the classification accuracy for each reservoir representation. Regardless of the choice of ğ‘š, LRS and DRS were less accurate than the other reservoir represen- tations. Although the proposed DPRR is computationally efficient, it achieved equivalent or higher accuracy compared to OMS and RMS.\n\n--- Segment 25 ---\nRegardless of the choice of ğ‘š, LRS and DRS were less accurate than the other reservoir represen- tations. Although the proposed DPRR is computationally efficient, it achieved equivalent or higher accuracy compared to OMS and RMS. The calculation of each reservoir representation transformation was implemented on an FPGA using a high-level synthesis tool to demonstrate that DPRR is computationally more efficient than OMS or RMS. Table 4 presents the logic synthesis results. Clearly, DPRR is more computationally efficient than OMS and RMS. When ğ‘ğ‘¥ 50, the number of FFs in DPRR is reduced to about 1 4 of those in OMS and in RMS, and the number of LUTs is about 1 5. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:14 Ikeda et al. Table 3. Classification accuracy for different reservoir representation. The dataset used is ARAB. Classification accuracy is in . ğ‘š 3, 4, 5, 6 corresponds to ğ‘ğ‘¥ 10, 19, 36, 69, respectively. LRS DRS MRS MRS OMS RMS DPRR ğ‘š _upad _xpad 3 44.9 18.2 89.3 89.1 90.9 85.4 88.5 4 52.6 22.0 93.0 92.6 95.3 95.0 96.5 5 54.1 26.0 93.5 93.2 96.2 96.0 97.5 6 60.1 38.1 94.3 94.7 96.0 98.0 98.0 Table 4. Synthesis results for different reservoir representation. Latency is in Cycles. Based on the ARAB dataset, the dimension of the input is 13 and the series length is 50.\n\n--- Segment 26 ---\nLatency is in Cycles. Based on the ARAB dataset, the dimension of the input is 13 and the series length is 50. ğ‘ğ‘¥ 10 20 30 40 50 Latency 16,344 62,032 162,125 412,452 740,512 BRAM 8 8 11 21 37 OMS DSP 18 18 12 19 19 FF 7,822 10,941 15,422 12,529 14,325 LUT 8,631 11,080 13,490 11,522 12,761 Latency 14,474 63,649 170,557 435,699 789,574 BRAM 8 8 12 26 50 RMS DSP 19 19 12 20 19 FF 7,768 10,900 15,455 12,510 14,351 LUT 8,284 10,615 12,805 10,643 11,734 Latency 5,628 21,448 47,468 83,688 130,108 BRAM 2 2 2 5 9 DPRR DSP 5 5 5 5 5 FF 1,212 1,815 2,284 2,667 3,171 LUT 1,465 1,836 2,086 2,395 2,725 5.2 Comparison of Classification Accuracy with Existing Machine Learning Methods Next, we compared the accuracy of the proposed DFR with DPRR (DFR_DPRR), to those of the DFR using DRS (DFR_DRS) and other machine learning methods [8]. Comparison with DFR_DRS corresponds to comparison with existing studies of DFR [2, 16]. This comparison of accuracy is followed by a comparison of hardware usage. As can be seen from the comparison in Section 5.1, DFR with OMS or RMS has the same level of accuracy as DFR_DPRR, but is less computationally efficient than DFR_DPRR, therefore it is not included in the comparison. However, DFR_DRS is less accurate than DFR_DPRR, but is more computationally efficient, therefore it is worth evaluating. The DFR models were implemented using Python. The hyperparameters listed in Table 5 were used for each dataset. In some datasets, the length of the input series varied. For data with a shorter length ğ‘‡than the maximum series length ğ‘‡max, the inputs were filled with zeros when using methods other than DFRs.\n\n--- Segment 27 ---\nIn some datasets, the length of the input series varied. For data with a shorter length ğ‘‡than the maximum series length ğ‘‡max, the inputs were filled with zeros when using methods other than DFRs. The machine learning methods that were compared were: (1) Multi layer perceptron (MLP) [19] (2) Fully convolutional neural network (FCN) [19] ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:15 Table 5. Parameters of the DFR used for comparision with existing machine learning methods. The number of nodes ğ‘ğ‘¥in the reservoir is 36 (ğ‘š 5).\n\n--- Segment 28 ---\nParameters of the DFR used for comparision with existing machine learning methods. The number of nodes ğ‘ğ‘¥in the reservoir is 36 (ğ‘š 5). DFR_DRS DFR_DPRR Dataset ğ›¾ ğœ‚ ğœƒ ğ›½ ğ›¾ ğœ‚ ğœƒ ğ›½ ARAB 4 0.4 0.3 1e-2 0.04 1 0.3 1e-2 AUS 0.3 4 0.45 1e-1 0.05 1 0.25 1e-3 CHAR 0.2 1.5 0.4 1e-2 0.05 1 0.3 1e-4 CMU 0.02 0.1 0.2 1e-2 0.01 3 0.15 1e-1 ECG 1 1 0.15 1e-1 0.03 1 0.3 1e-5 JPVOW 0.1 1 0.2 1e-2 0.03 1 0.2 1e-1 KICK 3 3 0.15 1e-1 0.003 3 0.15 1e-1 LIB 0.3 1.5 0.25 1e-5 0.14 1 0.45 1e-3 NET 0.01 3 0.2 1e-2 0.025 3 0.2 1e-3 UWAV 0.03 1.5 0.15 1e-5 0.03 1 0.4 1e-2 WAF 1 1 0.15 1e-3 0.2 0.4 0.25 1e-2 WALK 0.01 1 0.15 1e-1 1 1 0.25 1e-2 (3) Residual network (ResNet) [19] (4) Encoder [14] (5) Multi channel deep convolutional neural network (MCDCNN) [21] (6) Time convolutional neural network (Time-CNN) [20] (7) Time warping invariant echo state network (TWIESN). [18] In [8], MCNN and t-LeNet were also evaluated. However, these two were inferior to the above methods in terms of Average Rank, which will be discussed later, and have been omitted for clarity of comparison.\n\n--- Segment 29 ---\n[18] In [8], MCNN and t-LeNet were also evaluated. However, these two were inferior to the above methods in terms of Average Rank, which will be discussed later, and have been omitted for clarity of comparison. Table 6 summarizes the classification accuracies of all the aforementioned methods. The proposed method ranks second to the FCN in terms of average rank and consistently exhibits high accuracy on all datasets. In terms of the classification accuracy, the proposed method is comparable to FCN and outperforms most other machine learning methods. 5.3 Comparison of Hardware Resource Usage with Existing Machine Learning Methods The proposed model was implemented on an FPGA using a high-level synthesis tool to investigate its hardware resource usage. The Python code used in Section 5.1 for DFR and the Python code available in [8] for the other methods were rewritten in C and synthesized for the target FPGA. Table 7 presents the logic synthesis results. Figure 5 compares the number of block RAMs (BRAMs) required for the FPGA implementation. The smaller the numbers on both the horizontal and vertical axes indicate improved results (i.e., methods with results closer to the lower left achieves high accuracy with a smaller memory footprint). Methods based on DFR require a much smaller memory footprint than most machine- learning methods and ESN. Only Time-CNN with a very small number of filters in the convolution layers consumes approximately an equal number of BRAMs, but the accuracy of the proposed method is significantly better than that of Time-CNN. If DFR with OMS or RMS were plotted in the same figure, they would be located directly to the right of DFR_DPRR, i.e., similar accuracy while about 5x larger BRAM size. The number of BRAMs on the target FPGA board (Zynq-7000, xc7z020clg400-1) is 220, and thus the methods except DFR and Time-CNN cannot be implemented. Even if Zynq UltraScale ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:16 Ikeda et al. Table 6. Classification accuracy with other machine learning methods. Accuracy is in . The best and the second-best accuracies for each dataset is shown in bold and underlined respectively.\n\n--- Segment 30 ---\nAccuracy is in . The best and the second-best accuracies for each dataset is shown in bold and underlined respectively. The average accuracy is defined as the arithmetic mean of the accuracy. The average rank is defined as the arithmetic mean of the rank numbers being the most accurate method as 1. The data in the table, except for DFR, are from [8].\n\n--- Segment 31 ---\nThe average rank is defined as the arithmetic mean of the rank numbers being the most accurate method as 1. The data in the table, except for DFR, are from [8]. MLP FCN Res Encoder MCD Time- TWI DFR_ DFR_ Dataset Net CNN CNN ESN DRS DPRR ARAB 96.9 99.4 99.6 98.1 95.9 95.8 85.3 27.8 98.0 AUS 93.3 97.5 97.4 93.8 85.4 72.6 72.4 34.4 95.6 CHAR 96.9 99.0 99.0 97.1 93.8 96.0 92.0 33.9 96.2 CMU 60.0 100.0 99.7 98.3 51.4 97.6 89.3 93.1 100.0 ECG 74.8 87.2 86.7 87.2 50.0 84.1 73.7 67.0 88.0 JPVOW 97.6 99.3 99.2 97.6 94.4 95.6 96.5 62.4 97.8 KICK 61.0 54.0 51.0 61.0 56.0 62.0 67.0 60.0 80.0 LIB 78.0 96.4 95.4 78.3 65.1 63.7 79.4 30.0 78.3 NET 55.0 89.1 62.7 77.7 63.0 89.0 94.5 92.5 95.9 UWAV 90.1 93.4 92.6 90.8 84.5 85.9 75.4 26.2 86.2 WAF 89.4 98.2 98.9 98.6 65.8 94.8 94.9 90.2 99.0 WALK 70.0 100.0 100.0 100.0 45.0 100.0 94.4 100.0 100.0 Average Accuracy 80.3 92.8 90.2 89.9 70.9 86.4 84.6 59.8 92.9 Average Rank 5.92 2.25 3.08 3.42 7.58 5.50 5.92 7.08 2.50 Table 7. Synthesis results for different machine learning methods. Latency is in cycles. The dataset used is ARAB.\n\n--- Segment 32 ---\nLatency is in cycles. The dataset used is ARAB. Method Latency BRAM DSP FF LUT MLP 6,663,799 3,105 9 3,784 4,575 FCN 126,841,718 942 12 5,277 7,383 ResNet 227,560,415 1,951 87 34,673 37,984 Encoder 801,121,336 5,787 19 9,941 12,395 MCDCNN 16,328,669 8,547 62 11,909 14,882 Time-CNN 270,020 61 74 14,488 16,600 TWIESN 6,229,486 330 24 3,529 4,946 DFR_DRS 2,130 21 182 22,905 30,767 DFR_DPRR 84,532 57 65 12,083 14,152 MPSoC ZCU102, were used, in which the total number of available BRAMs is 892 (32.1 Mb), the other methods except for TWIESN still could not be implemented. Therefore, when comparing the hardware implementation efficiency using the power-delay product, which is equivalent to energy consumption, the BRAMs were excluded and the power consumption of other elements (i.e., LUT, FF, and DSP) was calculated using Xilinx power estimator (XPE). XPE version 2019.1.2 is used for Zynq-7000 as target. "Quick Estimate" was used to calculate the power and the setting was set as default except for "Design Utilization". Table 8 presents the summary of power estimation results. Figure 6 compares the power-delay product using the number of delay cycles obtained from the high-level synthesis tool and the power consumption without BRAM obtained using the power estimation tool. Similar to Fig. 5, the closer the results are to the lower left, the better is the method. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:17 Fig. 5. Comparison of memory usage. Each method is plotted using BRAM counts on the horizontal axis and the average rank calculated in Section 5.2 on the vertical axis Table 8. Power estimation results.\n\n--- Segment 33 ---\nEach method is plotted using BRAM counts on the horizontal axis and the average rank calculated in Section 5.2 on the vertical axis Table 8. Power estimation results. Power is in W. For each method, the resource usage obtained from the high-level synthesis was entered in "Design Utilization". Note that these powers are independent of the number of BRAMs. Method CLOCK LOGIC DSP SUM MLP 0.102 0.095 0.004 0.201 FCN 0.111 0.110 0.005 0.226 ResNet 0.254 0.313 0.038 0.605 Encoder 0.137 0.143 0.008 0.288 MCDCNN 0.147 0.159 0.027 0.333 Time-CNN 0.160 0.172 0.033 0.365 TWIESN 0.102 0.096 0.011 0.209 DFR_DRS 0.202 0.253 0.080 0.535 DFR_DPRR 0.148 0.156 0.029 0.333 The proposed method, DFR_DPRR, operates with a power-delay product that is three orders of magnitude smaller than that of FCN but achieves nearly the same accuracy as FCN. The difference in power-delay product would become larger if the power consumption of BRAM is considered. Overall, the proposed method exhibited the best balance in terms of accuracy and hardware efficiency among the investigated methods. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:18 Ikeda et al. Fig. 6. Accuracy versus power-delay product. Each method is plotted using the product of power consumption except BRAM and latency in clock cycles. The power-delay product is on the horizontal axis, and the average rank is on the vertical axis. Table 9. Implementation results of proposed DFR_DPRR. The dataset used is ARAB. All 2200 testing data are processed. Power is the sum of the power of clocks, signals, logic, BRAM, and DSP.\n\n--- Segment 34 ---\nAll 2200 testing data are processed. Power is the sum of the power of clocks, signals, logic, BRAM, and DSP. LUT 12,471 (23.44 ) LUTRAM 390 (2.24 ) FF 17,009 (15.99 ) BRAM 26 (18.57 ) DSP 67 (30.45 ) BUFG 1 (3.13 ) Clock 100 MHz Worst Negative Slack 0.566 ns Power 0.274 W Prediction Time (2200 time-series) 7.68 s Energy Consumption ( 1 time-series) 9.57E-04 J 5.4 Tradeoffs between Analog and Full Digital Implementation Analog and digital implementations exhibit pros and cons. Here, we discuss the tradeoff between the analog implementation of DFR [2, 16] and the proposed full digital implementation. An analog implementation of the reservoir would be beneficial in terms of power or energy. However, neither power consumption nor energy consumption was evaluated in existing studies [2, 16]. In general, analog implementation of DFRs require a DAC and ADC, and the non-linear ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. Hardware-Friendly Delayed Feedback Reservoir for Multivariate Time Series Classification 000:19 (such as Mackey-Glass) circuit should involve the design of an operational amplifier, making the overall circuit size larger. In addition, the design of such analog circuits and their components may require expertise and tuning, which can make it difficult to achieve maximum accuracy. The circuit design process can take a long time to complete. In contrast, the digital implementations, particularly the proposed fully digital DFR, are easy to adopt and more feasible with existing design tools. With digital design it is also easy to adjust the size and accuracy of the circuit by changing hyperparameters and data representations, such as input and output dimensions and the precision of fractional numbers. Furthermore, as our experiments in this section indicate, DFR_DPRR can achieve comparable or higher accuracy in comparison to the cutting-edge neural network-based methods as well as analog implementations. Finally, to allow future quantitative comparison of design tradeoffs among analog, digital, and other design styles, the implementation result of the proposed DFR_DPRR generated using Xilinx Vivado 2021.1 is summarized in Table 9.\n\n--- Segment 35 ---\nFurthermore, as our experiments in this section indicate, DFR_DPRR can achieve comparable or higher accuracy in comparison to the cutting-edge neural network-based methods as well as analog implementations. Finally, to allow future quantitative comparison of design tradeoffs among analog, digital, and other design styles, the implementation result of the proposed DFR_DPRR generated using Xilinx Vivado 2021.1 is summarized in Table 9. We confirmed that the proposed DFR_DPRR achieves exactly the same classification accuracy obtained in the Python simulations performed in Section 5.2. 6 CONCLUSION In this paper, we proposed a computationally efficient reservoir representation, namely DPRR, based on the dot product between features in the reservoir of shifted time. The proposed DPRR contributes to the implementation of accurate and compact hardware for reservoir-based classification. We also proposed a fully digital DFR using DPRR that is suitable for solving multivariate time-series classification tasks, which have been difficult to achieve high accuracy using reservoir computing methods. Through the FPGA implementation of the proposed DFR using DPRR, the proposed method was evaluated on 12 different multivariate time-series classification tasks. The DFR using DPRR outperformed conventional machine learning methods in terms of accuracy, despite its hardware resources in terms of power-delay product and memory usage being orders of magnitude smaller. The proposed model is therefore the most suitable among all investigated methods for solving multivariate time-series classification tasks via hardware. REFERENCES [1] Miquel L Alomar, Miguel C Soriano, Miguel Escalona-MorÃ¡n, Vincent Canals, Ingo Fischer, Claudio R Mirasso, and Jose L RossellÃ³. 2015. Digital implementation of a single dynamical node reservoir computer. IEEE Transactions on Circuits and Systems II: Express Briefs 62, 10 (2015), 977 981. [2] Lennert Appeltant, Miguel Cornelles Soriano, Guy Van der Sande, Jan Danckaert, Serge Massar, Joni Dambre, Benjamin Schrauwen, Claudio R Mirasso, and Ingo Fischer. 2011. Information processing using a single dynamical node as complex system. Nature Communications 2, 1 (2011), 1 6. [3] Lennert Appeltant, Guy Van der Sande, Jan Danckaert, and Ingo Fischer. 2014.\n\n--- Segment 36 ---\n[3] Lennert Appeltant, Guy Van der Sande, Jan Danckaert, and Ingo Fischer. 2014. Constructing optimized binary masks for reservoir computing with delay systems. Scientific Reports 4, 1 (2014), 1 5. [4] Witali Aswolinskiy, RenÃ© Felix Reinhart, and Jochen Steil. 2016. Time series classification in reservoir-and model-space: a comparison. In IAPR Workshop on Artificial Neural Networks in Pattern Recognition. 197 208. [5] Filippo Maria Bianchi, Simone Scardapane, Sigurd LÃ¸kse, and Robert Jenssen. 2020. Reservoir computing approaches for representation and classification of multivariate time series. IEEE Transactions on Neural Networks and Learning Systems 32, 5 (2020), 2169 2179. [6] JÃ©rÃ©mie Cabessa, Hugo Hernault, Heechang Kim, Yves Lamonato, and Yariv Z Levy. 2021. Efficient Text Classification with Echo State Networks. In International Joint Conference on Neural Networks. 1 8. [7] Huanhuan Chen, Fengzhen Tang, Peter Tino, and Xin Yao. 2013. Model-based kernel for efficient time series analysis. In Proceedings of ACM International Conference on Knowledge Discovery and Data Mining. 392 400. [8] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. 2019. Deep learning for time series classification: a review. Data Mining and Knowledge Discovery 33, 4 (2019), 917 963. [9] Herbert Jaeger. 2001. The echo state approach to analysing and training recurrent neural networks-with an erratum note. German National Research Center for Information Technology GMD Technical Report 148, 34 (2001), 13. ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000. 000:20 Ikeda et al. [10] Mantas LukoÅ¡eviÄius and Herbert Jaeger. 2009. Reservoir computing approaches to recurrent neural network training. Computer Science Review 3, 3 (2009), 127 149. [11] Michael C Mackey and Leon Glass. 1977. Oscillation and chaos in physiological control systems.\n\n--- Segment 37 ---\n1977. Oscillation and chaos in physiological control systems. Science 197, 4300 (1977), 287 289. [12] Razvan Pascanu, Jack W Stokes, Hermineh Sanossian, Mady Marinescu, and Anil Thomas. 2015. Malware classification with recurrent networks. In IEEE International Conference on Acoustics, Speech and Signal Processing. 1916 1920. [13] Benjamin Schrauwen, David Verstraeten, and Jan Van Campenhout. 2007. An overview of reservoir computing: theory, applications and implementations. In Proceedings of European Symposium on Artificial Neural Networks. 471 482. [14] Joan SerrÃ , Santiago Pascual, and Alexandros Karatzoglou. 2018. Towards a Universal Neural Network Encoder for Time Series. In Current Challenges, New Trends and Applications. 120 129. [15] Weisong Shi, Jie Cao, Quan Zhang, Youhuizi Li, and Lanyu Xu. 2016. Edge computing: Vision and challenges. IEEE Internet of Things Journal 3, 5 (2016), 637 646. [16] Miguel C Soriano, Silvia OrtÃ­n, Lars Keuninckx, Lennert Appeltant, Jan Danckaert, Luis Pesquera, and Guy Van der Sande. 2014. Delay-based reservoir computing: noise effects in a combined analog and digital implementation. IEEE Transactions on Neural Networks and Learning Systems 26, 2 (2014), 388 393. [17] Gouhei Tanaka, Toshiyuki Yamane, Jean Benoit HÃ©roux, Ryosho Nakane, Naoki Kanazawa, Seiji Takeda, Hidetoshi Numata, Daiju Nakano, and Akira Hirose. 2019. Recent advances in physical reservoir computing: A review. Neural Networks 115 (2019), 100 123. [18] Pattreeya Tanisaro and Gunther Heidemann. 2016. Time series classification using time warping invariant echo state networks. In IEEE International Conference on Machine Learning and Applications. 831 836. [19] Zhiguang Wang, Weizhong Yan, and Tim Oates. 2017. Time series classification from scratch with deep neural networks: A strong baseline. In International Joint Conference on Neural Networks. 1578 1585.\n\n--- Segment 38 ---\nIn International Joint Conference on Neural Networks. 1578 1585. [20] Bendong Zhao, Huanzhang Lu, Shangfeng Chen, Junliang Liu, and Dongya Wu. 2017. Convolutional neural networks for time series classification. Journal of Systems Engineering and Electronics 28, 1 (2017), 162 169. [21] Yi Zheng, Qi Liu, Enhong Chen, Yong Ge, and J Leon Zhao. 2014. Time series classification using multi-channels deep convolutional neural networks. In International Conference on Web-Age Information Management. 298 310. Received 00 January 0000; revised 00 January 0000; accepted 00 January 0000 ACM Trans. Embedd. Comput. Syst., Vol. 00, No. 0, Article 000. Publication date: 0000.\n\n