{
  "source": "HoloAR.pdf",
  "raw_length": 88035,
  "cleaned_length": 86828,
  "base_segments": 416,
  "augmented_segments": 832,
  "segments": [
    {
      "text": "Abstract \nHologram processing is the primary bottleneck and contributes to more than 50% of energy consumption in battery-operated aug- mented reality (AR) headsets. Thus, improving the computational efficiency of the holographic pipeline is critical. The objective of this paper is to maximize its energy efficiency without jeopardizing the hologram quality for AR applications.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "The objective of this paper is to maximize its energy efficiency without jeopardizing the hologram quality for AR applications. Towards this, we take the approach of analyzing the workloads to identify approximation op- portunities. We show that, by considering various parameters like region of interest and depth of view, we can approximate the ren- dering of the virtual object to minimize the amount of computation without affecting the user experience.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "We show that, by considering various parameters like region of interest and depth of view, we can approximate the ren- dering of the virtual object to minimize the amount of computation without affecting the user experience. Furthermore, by optimizing the software design flow, we propose  HoloAR , which intelligently renders the most important object in sight to the clearest detail, while approximating the computations for the others, thereby sig- nificantly reducing the amount of computation, saving energy, and gaining performance at the same time. We implement our design in an edge GPU platform to demonstrate the real-world applicability of our research.",
      "type": "sliding_window",
      "tokens": 141
    },
    {
      "text": "We implement our design in an edge GPU platform to demonstrate the real-world applicability of our research. Our experimental results show that, compared to the baseline,  HoloAR  achieves, on average, 2 . 7 ×  speedup and 73% energy savings.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "7 ×  speedup and 73% energy savings. CCS CONCEPTS \n•  Computing methodologies  → Ray tracing ;  •  Computer sys- tems organization  → Embedded software ;  •  Human-centered computing  → Visual analytics . ∗ Work was done as a student at Penn State.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "∗ Work was done as a student at Penn State. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MICRO ’21, October 18–22, 2021, Virtual Event, Greece © 2021 Association for Computing Machinery.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "MICRO ’21, October 18–22, 2021, Virtual Event, Greece © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8557-2/21/10...$15.00 https://doi.org/10.1145/3466752.3480056 \nKEYWORDS \nAugmented Reality, Holographic Processing, Approximation, Energy-efficiency \nACM Reference Format: Shulin Zhao, Haibo Zhang, Cyan S. Mishra, Sandeepa Bhuyan, Ziyu Ying, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. 2021.",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "2021. HoloAR: On-the-fly Optimization of 3D Holographic Processing for Aug- mented Reality. In  MICRO-54: 54th Annual IEEE/ACM International Sympo- sium on Microarchitecture (MICRO ’21), October 18–22, 2021, Virtual Event, Greece.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "In  MICRO-54: 54th Annual IEEE/ACM International Sympo- sium on Microarchitecture (MICRO ’21), October 18–22, 2021, Virtual Event, Greece. ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/3466752.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "https://doi.org/10.1145/3466752. 3480056 \n1 INTRODUCTION Augmented reality (AR) has gained recent traction in both the con- sumer and research communities, thanks to the advances in efficient and low power computing technologies, high-speed communica- tion, and specialized hardware platforms. These technologies have become an important part of our daily life, in the form of creative photography, content creation, gaming, online shopping, virtual touring, and educational and non-educational training, etc.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "These technologies have become an important part of our daily life, in the form of creative photography, content creation, gaming, online shopping, virtual touring, and educational and non-educational training, etc. For ex- ample, one of the earliest AR games, Pokémon GO (launched in July 2016), had a cumulative download of over 1 Billion, and gener- ated about $900 Million in revenue by late 2019 1 . Moreover, these AR infotainment applications have helped many of us through the recent global pandemic by bringing us the liveliness of the virtual outdoors, while we were confined to our homes, and more AR capa- ble mobile devices penetrating the market with cheaper price tags have made AR applications pervasive and made the virtual world easily accessible for users on the tip of their fingers.",
      "type": "sliding_window",
      "tokens": 182
    },
    {
      "text": "Moreover, these AR infotainment applications have helped many of us through the recent global pandemic by bringing us the liveliness of the virtual outdoors, while we were confined to our homes, and more AR capa- ble mobile devices penetrating the market with cheaper price tags have made AR applications pervasive and made the virtual world easily accessible for users on the tip of their fingers. However, even the state-of-the-art mobile devices with high bandwidth cannot meet the heavy compute and real-time demands of the AR applications, leading to very low quality of service (QoS) – in some cases as low as 1 frame per second (fps) [ 19 ,  54 ]. Further, \n1 To give a quantitative estimation of the popularity of the game, a Pokémon GO event at Safari Zone New Taipei City, Taiwan in October 2019 had a total of 327,000 attendees and they walked around 4.5 million kilometers to catch 50 Million Pokémons [5].",
      "type": "sliding_window",
      "tokens": 217
    },
    {
      "text": "Further, \n1 To give a quantitative estimation of the popularity of the game, a Pokémon GO event at Safari Zone New Taipei City, Taiwan in October 2019 had a total of 327,000 attendees and they walked around 4.5 million kilometers to catch 50 Million Pokémons [5]. 494 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. the limited battery capacity prevents users from enjoying their AR devices for extended periods of time.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "the limited battery capacity prevents users from enjoying their AR devices for extended periods of time. To meet the heavy compute demands of these applications, most of AR applications are run using high-end desktop/server-class GPUs [ 18 ,  55 ], or specialized hardware accelerators [ 35 ] on cloud platforms [ 16 ,  27 ]. However, since most of these applications are now running on low-power mobile devices, and frequent communi- cation of data to and from cloud via wireless medium is inefficient, optimization of an AR pipeline to maximize the compute and en- ergy efficiency, while providing adequate QoS, at an edge device is an architectural challenge.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "However, since most of these applications are now running on low-power mobile devices, and frequent communi- cation of data to and from cloud via wireless medium is inefficient, optimization of an AR pipeline to maximize the compute and en- ergy efficiency, while providing adequate QoS, at an edge device is an architectural challenge. Furthermore, existing AR headsets are typically equipped with multiple sensors for head orientation, eye tracking, motion detection, etc., to provide an interactive and life- like experience. These sensor inputs play a major role in deciding which portions of the 3D voxels need to be rendered for the user to view.",
      "type": "sliding_window",
      "tokens": 141
    },
    {
      "text": "These sensor inputs play a major role in deciding which portions of the 3D voxels need to be rendered for the user to view. However, even selective rendering of the portion of a scene, which is in the field of view (FoV) of the user, on a mobile device with limited compute and power budget is challenging [ 19 ,  52 ]. This calls for finding further opportunities for optimization.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "This calls for finding further opportunities for optimization. To under- stand the computing requirements in a typical AR pipeline consists of many stages (refer Sec. 2), we profiled a set of applications and found that the  hologram  processing is the primary bottleneck in terms of computation, energy consumption, and execution latency.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "2), we profiled a set of applications and found that the  hologram  processing is the primary bottleneck in terms of computation, energy consumption, and execution latency. The heavy compute demand of the hologram (re)construction has made this a promising candidate for acceleration, and prior works have tried to offload it to cloud [ 16 ,  27 ,  67 ] and specialized accelerators [35] to achieve high throughput, but doing so has led the communication with the edge device to be a major bottleneck. Others have proposed to design efficient and lightweight deep neu- ral networks (DNNs) to achieve high quality scene rendering at the edge device itself, but this requires model retraining/tuning for a particular user [ 33 ,  54 ].",
      "type": "sliding_window",
      "tokens": 169
    },
    {
      "text": "Others have proposed to design efficient and lightweight deep neu- ral networks (DNNs) to achieve high quality scene rendering at the edge device itself, but this requires model retraining/tuning for a particular user [ 33 ,  54 ]. Apart from the above works targeting all areas in a scene,  foveated rendering  techniques have been pro- posed to reduce image resolution in the peripheral area (typically beyond 135 °  vertically and 160 °  horizontally in human visual system (HVS)), while maintaining a normal/high quality only for 5 °  foveal vision [ 2 ,  22 ,  25 ,  47 ,  62 ]. Such differential resolution within an image can reduce computational costs without significantly impacting user experience [ 25 ,  47 ,  62 ].",
      "type": "sliding_window",
      "tokens": 173
    },
    {
      "text": "Such differential resolution within an image can reduce computational costs without significantly impacting user experience [ 25 ,  47 ,  62 ]. Further optimizations such as eye- dominance (i.e., HVS prefers scene perception from one eye over the other) and learning-based foveated rendering are orthogonal to this core idea and beyond the scope of this paper [24, 30]. Despite providing significant performance and energy-efficiency benefits, these prior works still miss out on even more selective rendering of viewed hologram images - beyond just the FoV and/or regions of the user’s focus.",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "Despite providing significant performance and energy-efficiency benefits, these prior works still miss out on even more selective rendering of viewed hologram images - beyond just the FoV and/or regions of the user’s focus. This is the primary motivation of this paper, where we explore trade-offs between hologram quality and processing costs. These trade-offs are not very straightforward due to the following  challenges : First,  among all of the inputs to the AR headset (shown later in Fig.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "These trade-offs are not very straightforward due to the following  challenges : First,  among all of the inputs to the AR headset (shown later in Fig. 1b), which one(s) are critical for holographic processing? Second,  which features of these inputs are salient and need more fine-grained computation, and which of them could be approximated without impacting the QoS?",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "Second,  which features of these inputs are salient and need more fine-grained computation, and which of them could be approximated without impacting the QoS? Third,  how do we make dynamic decisions of approximation based on the runtime conditions (e.g., user’s current pose and eye movements)? Towards this, we propose  HoloAR , an opportunistic and edge- friendly framework to speed up the AR holographic computation \n(a) An app.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Towards this, we propose  HoloAR , an opportunistic and edge- friendly framework to speed up the AR holographic computation \n(a) An app. (b) HW components [20, 29]. (c) SW pipeline [19, 50].",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "(c) SW pipeline [19, 50]. Figure 1: Main hardware components and software pipeline on a typical AR device. and improve its energy efficiency, with “approximation” as the core idea.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "and improve its energy efficiency, with “approximation” as the core idea. Starting from investigating and evaluating the existing foveated rendering techniques, this work further explores the entire design space for potential opportunities and optimizations unique in AR applications, for speedup as well as energy savings. The major  contributions  of this work can be summarized as follows: •  We first conduct a detailed characterization of a generic AR processing pipeline to identify the major bottlenecks in current state-of-the-art AR headsets, and set our optimization target as the  hologram computation .",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "The major  contributions  of this work can be summarized as follows: •  We first conduct a detailed characterization of a generic AR processing pipeline to identify the major bottlenecks in current state-of-the-art AR headsets, and set our optimization target as the  hologram computation . (Sec. 2.1) •  From two open-source AR datasets [ 1 ,  58 ], we identify two prop- erties in AR hologram applications:  spatio diversity for objects , and  temporal locality for the user (viewer) interests (i.e., user typ- ically focuses on one region within a short period of time) .",
      "type": "sliding_window",
      "tokens": 150
    },
    {
      "text": "2.1) •  From two open-source AR datasets [ 1 ,  58 ], we identify two prop- erties in AR hologram applications:  spatio diversity for objects , and  temporal locality for the user (viewer) interests (i.e., user typ- ically focuses on one region within a short period of time) . Such properties are leveraged as approximation opportunities to skip the “unimportant” portions of the hologram computation, based on user’s region of focus (known as foveated rendering), and object’s distance/size from the user. (Sec.",
      "type": "sliding_window",
      "tokens": 142
    },
    {
      "text": "(Sec. 2.2) •  To capture these two approximation opportunities from both the user and object perspectives, first, the prior  foveated ren- dering  idea (denoted as  Inter-Holo  design) has been imple- mented (in Sec. 4.3) and found to work well (in Sec.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "4.3) and found to work well (in Sec. 5) as in prior works [ 22 ,  25 ,  30 ,  47 ]. In this paper, we have gone be- yond foveated rendering ( Inter-Holo ), by proposing an optimiza- tion/approximation called  Intra-Holo , that complements the for- mer in boosting performance/energy efficiency.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "In this paper, we have gone be- yond foveated rendering ( Inter-Holo ), by proposing an optimiza- tion/approximation called  Intra-Holo , that complements the for- mer in boosting performance/energy efficiency. Such  Intra-Holo enhancement is ideally suited for holographic processing at the edge, without requiring additional hardware, cloud assistance, or machine learning. •  We implement both the designs on an edge GPU platform [ 36 ], without the need for any hardware modification.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "•  We implement both the designs on an edge GPU platform [ 36 ], without the need for any hardware modification. We evaluate these designs using the NVPROF tool [ 37 ] and hardware power management unit on the edge GPU platform [ 36 ]. Our exper- imental results reveal that,  HoloAR  provides 29% reduction in power consumption and 2 .",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Our exper- imental results reveal that,  HoloAR  provides 29% reduction in power consumption and 2 . 7 ×  speedup, which collectively trans- late to 73% total energy savings compared to the baseline setup (Sec. 5.3).",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "5.3). Finally, based on our findings, we discuss future direc- tions that may help one design custom hardware accelerators for AR holograms (Sec. 5.5).",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "5.5). 2 BACKGROUND AND MOTIVATION \nBefore diving deep into the problems and possible solutions asso- ciated with holographic processing, we first present the hardware \n495 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \nand software pipelines of a typical holographic AR application (in Fig. 1).",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "1). We further describe the existing holographic execution inefficiencies in the AR pipeline and potential opportunities for computation reduction. 2.1 AR Holographic Applications and Pipeline \nThe holographic display technique enables a large body of aug- mented applications in real life [ 14 ].",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "2.1 AR Holographic Applications and Pipeline \nThe holographic display technique enables a large body of aug- mented applications in real life [ 14 ]. One such application is illus- trated in Fig. 1a, where a physical car being driven on a highway is replaced by the corresponding virtual/augmented holographic car in a real-time fashion such that, instead of viewing the real cars, the AR user views the virtual ones.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "1a, where a physical car being driven on a highway is replaced by the corresponding virtual/augmented holographic car in a real-time fashion such that, instead of viewing the real cars, the AR user views the virtual ones. To implement such ap- plications, today’s AR headsets are usually equipped with various hardware components for sensing and processing, as depicted in Fig. 1b.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "1b. Specifically, the AR hardware has three major components: Sensor Inputs:  The AR headset receives the real-time information from both the surrounding environment and the user (viewer), with two types of sensing:  ① “world sensors”  to sense the physical surrounding the user is currently in, such as cameras for the RGB image and LiDAR/depth sensor for the depth or distance of the objects in front of the user, and  ② “user sensors”  to sample the behavior/status of the user, such as inertial measurement unit (IMU) sensors for head rotation, IR sensors for eye tracking, and controller for hand gesture. After sensing, the input samples are then buffered in the video buffer, waiting to be processed timely at the frame-rate.",
      "type": "sliding_window",
      "tokens": 167
    },
    {
      "text": "After sensing, the input samples are then buffered in the video buffer, waiting to be processed timely at the frame-rate. Processing Engines:  To efficiently handle the above two types of inputs, various computational resources have been integrated into AR SoCs, as shown in Fig. 1b, e.g., CPUs for generic processing, GPUs for graphics computing, vision processing units (VPUs) for rendering, and tensor processing units (TPUs) for learning infer- ences.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "1b, e.g., CPUs for generic processing, GPUs for graphics computing, vision processing units (VPUs) for rendering, and tensor processing units (TPUs) for learning infer- ences. Recently, state-of-the-art AR headsets such as HoloLens [ 31 ] have even been planning to integrate the holographic processing units (HPUs) for processing the information coming from all of the on-board sensors (currently under development) [32]. On-board Battery:  It is to be noted that all of the sensors and the processing engines mentioned above are  battery-backed , as shown in Fig.",
      "type": "sliding_window",
      "tokens": 149
    },
    {
      "text": "On-board Battery:  It is to be noted that all of the sensors and the processing engines mentioned above are  battery-backed , as shown in Fig. 1b. This is for enabling users to freely move around in a large area without the need of connecting with a power cable constantly.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "This is for enabling users to freely move around in a large area without the need of connecting with a power cable constantly. Hence, the power/energy efficiency is critical metrics in many AR use cases so that the battery lifetime can be sufficiently long. With these sensors and compute resources in place, an AR head- set executes a set of software tasks, either entirely or selectively based on the applications’ requirements [ 19 ].",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "With these sensors and compute resources in place, an AR head- set executes a set of software tasks, either entirely or selectively based on the applications’ requirements [ 19 ]. Without loss of gener- ality, a typical AR pipeline [ 19 ] is shown in Fig. 1c.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "1c. At a high level, this AR pipeline has three major stages:  ❶ Inputs  stage first collects the real-time information from all the on-board sensors such as IMU, IR, camera and depth image sensors. With these inputs,  ❷ Perception  stage understands the current surrounding environment such as pose estimation for head rotations/directions, eye tracking for pupil centers, and scene reconstruction for the current view analysis.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "With these inputs,  ❷ Perception  stage understands the current surrounding environment such as pose estimation for head rotations/directions, eye tracking for pupil centers, and scene reconstruction for the current view analysis. Finally,  ❸ Visual  stage combines the physical world with the virtual information (which is generated in real-time) together, and renders the final images (both the physical scene as well as the virtual frame augmented with it) for the user to view. We want to emphasize that, compared to virtual reality (VR), the AR video processing typically incurs additional computational \nTable 1: Ideal latency requirements [19].",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "We want to emphasize that, compared to virtual reality (VR), the AR video processing typically incurs additional computational \nTable 1: Ideal latency requirements [19]. Task Ideal Latency Algo. Pose Estimate \n33  ms Kimera [53] \nEye Track \n33  ms NVGaze [26] \nScene Reconstruct \n100  ms InfiniTAM [50] \nHologram 33  ms GSW [49, 63] \ntasks and interacts with more hardware resources [ 61 ].",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Pose Estimate \n33  ms Kimera [53] \nEye Track \n33  ms NVGaze [26] \nScene Reconstruct \n100  ms InfiniTAM [50] \nHologram 33  ms GSW [49, 63] \ntasks and interacts with more hardware resources [ 61 ]. Based on our measurements collected from a smartphone [ 60 ] running a sim- ple AR application [ 3 ], the processing performance can be lower than 0 . 5 fps, and the battery life can be as short as just 1 hour.",
      "type": "sliding_window",
      "tokens": 126
    },
    {
      "text": "5 fps, and the battery life can be as short as just 1 hour. This motivates us to investigate which component is the major perfor- mance and energy bottleneck, charging most of the “performance- and/or energy-taxes” from the battery-backed AR headsets. 2.2 Motivation \n2.2.1 What is the Major Bottleneck?",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "2.2 Motivation \n2.2.1 What is the Major Bottleneck? To identify the major performance bottlenecks in the current AR headsets, we characterized the execution latency of the software pipeline (discussed above in Fig. 1c) on a typical edge prototype [ 36 ] running a set of state-of-the-art AR-related tasks [ 19 ,  26 ,  49 ,  50 , 53 ], and compared the collected results against ideal execution latencies for the same set of tasks (i.e., the maximum latency within which the task needs to finish before its next invocation).",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "1c) on a typical edge prototype [ 36 ] running a set of state-of-the-art AR-related tasks [ 19 ,  26 ,  49 ,  50 , 53 ], and compared the collected results against ideal execution latencies for the same set of tasks (i.e., the maximum latency within which the task needs to finish before its next invocation). The ideal latencies and our collected latencies are given in Table 1 and Fig. 2, respectively, for an ILLIXR playground scenario [15, 19].",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "2, respectively, for an ILLIXR playground scenario [15, 19]. Comparing the ideal latencies with practical latencies, we make the following conclusions: In our practical setting,  Pose Estimation tracks user’s motion and viewing scene to estimate the current body pose [ 53 ], and it takes around 13 . 8 ms .",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "8 ms . Furthermore, estimating the user’s eye gaze,  Eye Track , requires the execution of a light-weight neural network that takes 4 . 4 ms  and achieves 2 .",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "4 ms  and achieves 2 . 06 °  of accuracy [ 26 ]. Thus, both of these two tasks are able to meet the performance re- quirements shown in Table 1.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "Thus, both of these two tasks are able to meet the performance re- quirements shown in Table 1. On the other hand,  Scene Reconstruct captures comprehensive consistent maps of environments from an RGB-D image, and consumes 120 ms  in the practical setting. Note that such maps are not necessarily required to be generated for each frame (typically computed once per two or three frames [ 28 ,  50 ], thus 67 − 100 ms  in Table 1); hence, we argue that the state-of-the-art InfiniTAM technique, which implements a framework for real-time depth fusion and learning of 3D scenes [ 50 ], is already close to the ideal case.",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "Note that such maps are not necessarily required to be generated for each frame (typically computed once per two or three frames [ 28 ,  50 ], thus 67 − 100 ms  in Table 1); hence, we argue that the state-of-the-art InfiniTAM technique, which implements a framework for real-time depth fusion and learning of 3D scenes [ 50 ], is already close to the ideal case. However,  Hologram , which takes depthmap, point-cloud, or light field as its input [ 18 ] 2   to create arbitrary 3D configurations of optical traps useful for capturing, moving and transforming mesoscopic objects freely in the world [ 4 ], takes as long as 341 . 7 ms on an edge GPU 3 .",
      "type": "sliding_window",
      "tokens": 174
    },
    {
      "text": "7 ms on an edge GPU 3 . This 10 ×  performance gap between the practical scenario and the ideal case (and the large amount of power/energy consumption this task makes) motivates us to focus on holographic processing in this paper, and explore the opportunities for improv- ing the hologram computational efficiency to speed up the overall AR application execution and reduce its energy consumption. 2 In this paper, we mainly use the popular depthmap input method.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "2 In this paper, we mainly use the popular depthmap input method. 3 Five iterations of the GSW algorithm [63] are profiled. 496 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "496 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 0.2 8.0 13.8 4.4 \n120.0 \n6.0 \n341.7 \n0 100 200 300 400 \nIMU/IR \nCamera \nPose \nEstimate \nEye Track \nScene \nReconstruct \nReproject \nHologram \nInputs Perception Visual \nLatency (ms) \nOur  focus \nFigure 2: A comparison of latency requirements results col- lected from our practical setting and ideal cases shown in Table 1. 2.2.2 What are the Prior Optimization Efforts?",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "2.2.2 What are the Prior Optimization Efforts? Targeting the compute-intensive holographic processing dis- cussed above,  foveated rendering  techniques have been previously proposed to approximate selective regions (i.e.,  peripheral vision ). In fact, prior research on HVS has shown that human eyes are able to observe beyond 135 °  vertically and 160 °  horizontally, but see fine details within an only around 5 °  central circle (i.e.,  foveal vision ).",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "In fact, prior research on HVS has shown that human eyes are able to observe beyond 135 °  vertically and 160 °  horizontally, but see fine details within an only around 5 °  central circle (i.e.,  foveal vision ). Motivated by such degradation of  peripheral visual acuity , foveated rendering  reduces computational costs for the peripheral region, and maintains high/normal resolution only for the foveal re- gion [ 2 ,  22 ,  24 ,  25 ,  30 ,  47 ,  62 ]. For instance, a real-time gaze-tracked foveated rendering system is proposed to yield performance and memory savings by avoiding shading up to 70% of the pixels for VR headsets [ 47 ].",
      "type": "sliding_window",
      "tokens": 171
    },
    {
      "text": "For instance, a real-time gaze-tracked foveated rendering system is proposed to yield performance and memory savings by avoiding shading up to 70% of the pixels for VR headsets [ 47 ]. Similarly, a prototype AR display also takes advan- tage of foveated rendering by tracking the user’s gaze and providing low-resolution images to the peripheral area to reduce computa- tion and improve display resolution [ 25 ]. More recently, another foveated rendering based CGH reconstruction technique has been proposed to accelerate calculations with negligible effect for the viewer [ 22 ].",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "More recently, another foveated rendering based CGH reconstruction technique has been proposed to accelerate calculations with negligible effect for the viewer [ 22 ]. We implemented such foveated rendering idea (de- noted as  Inter-Holo  design in Sec. 4.3) and found to reduce around 23% execution latency (in Sec.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "4.3) and found to reduce around 23% execution latency (in Sec. 5) for AR holograms. However, such performance gain from foveated rendering is still insufficient to close the 10 ×  gap discussed above.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "However, such performance gain from foveated rendering is still insufficient to close the 10 ×  gap discussed above. Thus, in this paper, we want to go beyond the prior foveated rendering for further optimizations, by investigating the potential opportunities which are unique to the AR use cases and may have been missed out before. 2.2.3 What are the Potential Opportunities?",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "2.2.3 What are the Potential Opportunities? Towards addressing this hologram bottleneck, various ap- proaches from both the software [ 33 ,  52 ,  54 ] and hardware [ 32 ,  35 ] sides have been proposed. These prior approaches either incorpo- rate additional memory for maintaining a lookup table for compu- tation reduction, or build an application-specific integrated circuit (ASIC) chip specifically for holographic processing, which is more power-efficient than generic processors.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "These prior approaches either incorpo- rate additional memory for maintaining a lookup table for compu- tation reduction, or build an application-specific integrated circuit (ASIC) chip specifically for holographic processing, which is more power-efficient than generic processors. While such approaches im- prove the hologram execution to some extent, they do not consider the unique features of the AR applications. Recall that, in the AR holographic application discussed above in Fig.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Recall that, in the AR holographic application discussed above in Fig. 1a and Section 2.1, there are two types of inputs to the holographic pipeline –  world sensors  for the physical objects (real cars in this case) in the world, and  user sensors  for the user behavior/state such as pose and eye movements (discussed in details in Sec. 3).",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "3). Therefore, in principle, \n0 1 2 3 \nbike \nbook \nbottle \ncamera \ncerealBox \nchair \ncup \nlaptop \nshoe \nDistance/Size (m) \nCam2ObjDist. ObjSize \n(a) Object study.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "ObjSize \n(a) Object study. 0 \n0.5 \n1 \n0 0.5 1 \nPupi Postion Y \nPupil Position X \n0 \n0.5 \n1 \n0 0.5 1 \nPupil Position Y \nPupil Position X \n0 \n0.5 \n1 \n0 0.5 1 \nPupil Position Y \nPupil Position X \nUser1: \nUser2: \nUser3: \n(b) User eye tracking study. Figure 3: Dataset study.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Figure 3: Dataset study. (a) Depthmap hologram algorithm. 0 200 400 600 800 1000 1200 \n1 2 4 8 16 32 64 \nExec.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "0 200 400 600 800 1000 1200 \n1 2 4 8 16 32 64 \nExec. Latency (ms) \n# Depth Planes \nForward Backward \n(b) Latency w/ num of depth planes. Figure 4: Depthmap hologram algorithm details.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Figure 4: Depthmap hologram algorithm details. more intuitive opportunities could exist in the AR application do- main, from  both  the object and user perspectives. To identify them, we studied two published AR datasets (Objectron [ 1 ] for objects shown in Fig.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "To identify them, we studied two published AR datasets (Objectron [ 1 ] for objects shown in Fig. 3a, and MPIIDEye [ 58 ] for users shown in Fig. 3b), and observed the following two properties in the AR holographic applications: Spatio Diversity for Objects:  Intuitively, objects which are far from the user and with small-sized shapes require less informa- tion to generate the virtual hologram than others (more details are provided in Sec.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "3b), and observed the following two properties in the AR holographic applications: Spatio Diversity for Objects:  Intuitively, objects which are far from the user and with small-sized shapes require less informa- tion to generate the virtual hologram than others (more details are provided in Sec. 3). Hence, the distance between the user and the objects ( Cam2ObjDist  shown in black color in Fig.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Hence, the distance between the user and the objects ( Cam2ObjDist  shown in black color in Fig. 3a), as well as the size of how the object seems/appears to the user ( ObjSize shown in red color in Fig. 3a) affect the amount of computations actually required to provide just enough yet necessary virtual holo- grams.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "3a) affect the amount of computations actually required to provide just enough yet necessary virtual holo- grams. For example, compared to the  chair  object in Fig. 3a, the bike  object is closer to the user, and also has a larger range/size ( size=farmost-nearest ); thus, more information is required to create the hologram for the  bike  for maintaining fairly good QoS than the  chair .",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "3a, the bike  object is closer to the user, and also has a larger range/size ( size=farmost-nearest ); thus, more information is required to create the hologram for the  bike  for maintaining fairly good QoS than the  chair . Therefore, one opportunity to reduce the amount of computation is to  approximate  the hologram processing based on the objects’ distances and sizes. Temporal Locality for the User Interests:  As also established by prior foveated rendering proposals, the foveal vision (or Region of Focus, RoF) is only a small region in the current scene and can be traced by eye tracking techniques [ 26 ].",
      "type": "sliding_window",
      "tokens": 147
    },
    {
      "text": "Temporal Locality for the User Interests:  As also established by prior foveated rendering proposals, the foveal vision (or Region of Focus, RoF) is only a small region in the current scene and can be traced by eye tracking techniques [ 26 ]. As can be observed from three users’ eye tracking shown in Fig. 3b, all focus only on a portion of the entire viewing window within a short period of time (10 seconds in this case).",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "3b, all focus only on a portion of the entire viewing window within a short period of time (10 seconds in this case). On the other hand, even when viewing the exact same scene, the RoF varies across users. For example,  User1 has similar interest as  User3 , whereas  User2  focuses more on the bottom left corner.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "For example,  User1 has similar interest as  User3 , whereas  User2  focuses more on the bottom left corner. Clearly, such temporal similarity for a particular user’s interests exposes another opportunity for leveraging prior \n497 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \nfoveated rendering in AR holograms, by reducing the amount of computation needed for the objects which are outside the RoF, thus only emphasizing on the processing of the objects which the user is currently focusing on. Driven by these observations, we next want to study the details of hologram with the goal of addressing two critical questions:  What are the problems in the current state-of-the-art hologram software and hardware?",
      "type": "sliding_window",
      "tokens": 190
    },
    {
      "text": "Driven by these observations, we next want to study the details of hologram with the goal of addressing two critical questions:  What are the problems in the current state-of-the-art hologram software and hardware? , and  How can we leverage “approximation opportunities” (based on the two observations above) to speed up hologram processing and save energy, while still maintaining a high QoS? 3 HOLOGRAPHIC PROCESSING STUDY \nTo leverage the opportunities in the holographic processing from a RGB-D (i.e., RGB and depth) image, we need to first understand the detailed execution of the entire hologram processing from both the algorithm and hardware perspectives.",
      "type": "sliding_window",
      "tokens": 157
    },
    {
      "text": "3 HOLOGRAPHIC PROCESSING STUDY \nTo leverage the opportunities in the holographic processing from a RGB-D (i.e., RGB and depth) image, we need to first understand the detailed execution of the entire hologram processing from both the algorithm and hardware perspectives. We illustrate the details of depthmap hologram processing in Fig. 4a and Algo.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "4a and Algo. 1 as two ma- jor steps (more details on the depthmap hologram algorithm can be found elsewhere [ 4 ,  18 ,  55 ,  63 ]). As shown in Fig.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "As shown in Fig. 4a, the depthmap input is first sliced into several planes ( M  depth planes in this case). With these depth planes, the first step,  Forward Propagation  (de- noted  ❶ in Fig.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "With these depth planes, the first step,  Forward Propagation  (de- noted  ❶ in Fig. 4a), is to overlay the  i th   plane on the propagation result of the previous 1 st   to  ( i  − 1 ) th   planes, and then propagate to the next  ( i  +  1 ) th   plane. Note from  Line#3  to  Line#5  in Algo.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Note from  Line#3  to  Line#5  in Algo. 1 that, such forward propagation is massively parallel at the depth plane level (across planes) as well as at the pixel level (within one plane). Each depth plane processes the forward-propagation from the hologram plane independently, and each pixel on a particular depth plane goes through the exact processing sequence ( HP2DP in  Line#5 ; more details can be found in [ 4 ,  18 ]).",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Each depth plane processes the forward-propagation from the hologram plane independently, and each pixel on a particular depth plane goes through the exact processing sequence ( HP2DP in  Line#5 ; more details can be found in [ 4 ,  18 ]). This makes hard- ware parallelization and pipelining easier on a block/tensor type of architecture such as GPUs. Note, however, that, this step also requires sequential barriers within each plane ( Line#6  synchro- nizes the threads in a warp/block for one depth plane) and across planes ( Line#7  synchronizes the results from all the depth planes, before moving forward to the second step).",
      "type": "sliding_window",
      "tokens": 157
    },
    {
      "text": "Note, however, that, this step also requires sequential barriers within each plane ( Line#6  synchro- nizes the threads in a warp/block for one depth plane) and across planes ( Line#7  synchronizes the results from all the depth planes, before moving forward to the second step). Hence, as we will show later in this section, such barriers sliced into the massive parallel execution can cause load imbalance and instruction stalls, which slow down the entire execution and impact performance. The sec- ond step,  Backward-Propagate  (denoted  ❷ in Fig.",
      "type": "sliding_window",
      "tokens": 135
    },
    {
      "text": "The sec- ond step,  Backward-Propagate  (denoted  ❷ in Fig. 4a), accumulates the results of each depth plane, backpropagates it to the hologram plane via the  DP2HP  procedure (in  Line#11 ), and generates the final hologram for this depthmap input. Like the first step, this step also involves synchronizations between planes (in  Line#12 ), which can again impact parallelization and slow down the entire execution.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Like the first step, this step also involves synchronizations between planes (in  Line#12 ), which can again impact parallelization and slow down the entire execution. Intuitively, the execution performance is mainly determined by the number of depth planes (the outer for-loop in the algorithm) as well as the number of pixels in each depth plane (the inner for-loop in the algorithm). To study how the number of depth planes affects the hologram performance, we profile the execution latency from a typical edge GPU device [ 36 ], generating holograms with different number of depth planes (assuming the same number of pixels in each plane), and the results are plotted in Fig.",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "To study how the number of depth planes affects the hologram performance, we profile the execution latency from a typical edge GPU device [ 36 ], generating holograms with different number of depth planes (assuming the same number of pixels in each plane), and the results are plotted in Fig. 4b. From this figure, one can observe the following: First, in general, these two steps take similar times to execute, due to the similar procedures they \nAlgorithm 1:  Depthmap Hologram Algorithm [4, 18].",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "From this figure, one can observe the following: First, in general, these two steps take similar times to execute, due to the similar procedures they \nAlgorithm 1:  Depthmap Hologram Algorithm [4, 18]. Input : M : Number of depth planes Input : DP [ i ] : Pixels in the  i th   depth plane Output: Holoдram : Generated hologram \n1  procedure  Depthmap _ Holoдram ( M ,  DP ) // main \n2 // Step-1: Forward-propagate \n3 for  i  in  [ 1 ,  M ]  do // planes in parallel \n4 for  p  in  DP[i]  do // pixels in parallel \n5 IntraPlane i  =  HP 2 DP ( i ,  p ) \n6 IntraBlockSync (IntraPlane[i]) \n7 Inter BlockSync () \n8 // Step-2: Backward-propagate \n9 for  i  in  [ 1 ,  M ]  do // in parallel \n10 for  p ′   in  IntraPlane[i]  do // in parallel \n11 Hologram[ p ′ ] +=  DP 2 HP ( i ,  p ′ ) \n12 Inter BlockSync () \n13 return  { Holoдram } \nemploy, as shown in Algo. 1.",
      "type": "sliding_window",
      "tokens": 302
    },
    {
      "text": "1. Second, by increasing the number of depth planes, it takes around 2 ×  latency to generate a hologram with 2 ×  number of depth planes. As also mentioned in Sec.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "As also mentioned in Sec. 2.1, the 16 depth planes required by most of the AR applications (typically 10 to 100 depth planes are sufficient) [ 19 ,  49 ] consume more than 300 ms , which is 10 ×  larger than the real-time (QoS) requirement. Thus, it can be concluded that, without any optimization, a state- of-the-art edge GPU is only able to compute for  <  4 depth planes in real-time [ 36 ].",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Thus, it can be concluded that, without any optimization, a state- of-the-art edge GPU is only able to compute for  <  4 depth planes in real-time [ 36 ]. These observations motivate us to investigate the reasons behind such low performance on GPU: is it because of the intrinsic software/algorithm characteristics, or is it primarily a hardware mapping issue? Towards this, we profiled the hologram processing on the edge GPU [ 36 ] using the NVPROF tool [ 37 ], and observed the follow- ing: First, the SM utilization for both the steps is very high, i.e., 74% for  Forward-Propagation  and 90% for  Backward-Propagation .",
      "type": "sliding_window",
      "tokens": 164
    },
    {
      "text": "Towards this, we profiled the hologram processing on the edge GPU [ 36 ] using the NVPROF tool [ 37 ], and observed the follow- ing: First, the SM utilization for both the steps is very high, i.e., 74% for  Forward-Propagation  and 90% for  Backward-Propagation . This is because the execution is massively parallel at the depth plane level as well as at the pixel level. Moreover, the L1 hit rate for both these steps is as high as 99%.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "Moreover, the L1 hit rate for both these steps is as high as 99%. Thus, GPU seems to be one of the reasonable hardware candidates for mapping the hologram application. Second, the four major reasons for instruction stalls in the  Forward-Propagation  step are: Data Request (21%), Execu- tion Dependency (19%), Instruction Fetch (15%), and Sync (10%), whereas in the  Backward-Propagation  step they are Read-only Loads (42%), Sync (24%), Data Request (16%), and Execution Dependency (6%).",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "Second, the four major reasons for instruction stalls in the  Forward-Propagation  step are: Data Request (21%), Execu- tion Dependency (19%), Instruction Fetch (15%), and Sync (10%), whereas in the  Backward-Propagation  step they are Read-only Loads (42%), Sync (24%), Data Request (16%), and Execution Dependency (6%). These stalls originate mainly from the inter-block and intra- block synchronizations required by the application, as discussed above when explaining Algo. 1.",
      "type": "sliding_window",
      "tokens": 126
    },
    {
      "text": "1. Because of this, recently, alternate hardware-based solutions have been proposed to improve the com- putational efficiency by replacing the expensive transcendental cal- culations with lookup table (LUT) based memoization [ 35 ], or miti- gating the data movement overheads by employing a customized buffer on-chip [ 32 ], or simply offloading computations to cloud then streaming back [ 16 ,  27 ,  67 ]. While such an approach improved the computational efficiency and reduced power consumption to some extent, rethinking the design of hologram software/hardware con- sidering the unique features of the AR holographic applications (as discussed in Sec.",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "While such an approach improved the computational efficiency and reduced power consumption to some extent, rethinking the design of hologram software/hardware con- sidering the unique features of the AR holographic applications (as discussed in Sec. 2.2) as well as the characteristics of the underlying hardware can potentially open up further opportunities. 498 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "498 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. Motivated by this, we next explore the entire design space for the AR holographic applications running on edge GPUs, and try to ex- ploit potential opportunities for reducing computations to improve both performance and energy efficiency in hologram processing. 4 PROPOSED STRATEGIES \nAs discussed in Sec.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "4 PROPOSED STRATEGIES \nAs discussed in Sec. 2.2, holographic processing dominates the la- tency and energy consumption in the AR video pipeline. Further, we also observed in Sec.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "Further, we also observed in Sec. 3 that, the main reason behind this is that the number of depth planes affects the number of synchronizations between parallel executions, and determines the amount of com- putation required to generate the holograms. Unlike prior works targeting at optimizing the efficiency of the hologram program- ming itself by proposing alternative hardware [ 32 ,  35 ], we primarily focus on exploring the intrinsic approximation opportunities (dis- cussed in Sec.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "Unlike prior works targeting at optimizing the efficiency of the hologram program- ming itself by proposing alternative hardware [ 32 ,  35 ], we primarily focus on exploring the intrinsic approximation opportunities (dis- cussed in Sec. 2.2.3) ignored in the current implementation of the AR applications, but can be embedded into the existing hardware such as GPUs, to speedup the holographic execution and improve power/energy efficiency with negligible quality loss. 4.1 Exploring the Entire Design Space in AR Hologram Processing \nExploring the entire design space for the AR hologram processing is a non-trivial task.",
      "type": "sliding_window",
      "tokens": 142
    },
    {
      "text": "4.1 Exploring the Entire Design Space in AR Hologram Processing \nExploring the entire design space for the AR hologram processing is a non-trivial task. First of all, a large number of sensor inputs are fed into the hologram pipeline (as shown in Fig. 1b), such as IMU sensors, eye tracking or IR sensors, hand motion sensors, RGB-D im- age sensors, etc.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "1b), such as IMU sensors, eye tracking or IR sensors, hand motion sensors, RGB-D im- age sensors, etc. To improve the hologram approximation, we need to first identify the set of inputs that affect the hologram computing the most. As discussed above in Sec.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "As discussed above in Sec. 3, both the user’s pose and the gaze position, as well as the targeted objects (intended to be re- placed by the virtual holograms) shape the hologram computation. Further, in many cases, these inputs are dynamically changing at the same frequency (e.g., the image sensors) as the frame-rate, which needs to be captured and updated at runtime, or even at a faster rate (e.g., the IMU and IR sensors).",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "Further, in many cases, these inputs are dynamically changing at the same frequency (e.g., the image sensors) as the frame-rate, which needs to be captured and updated at runtime, or even at a faster rate (e.g., the IMU and IR sensors). Thus, to systematically explore the potential opportunities of approximation in the AR hologram applications, we start by distinguishing between three fundamental scenarios, where the  objects ,  head pose , and  eye tracking  provide different opportunities, as depicted in Fig. 5.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "5. •  In the  Viewing-Window  scenario shown in Fig. 5a, only the soccer ball  object is located inside the viewing window in the current frame,  Frame-I , while  football  and  box  are not.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "5a, only the soccer ball  object is located inside the viewing window in the current frame,  Frame-I , while  football  and  box  are not. Thus, only the  soccer ball  hologram is required to be com- puted for this frame, and other two can be skipped. Similarly, for the next frame,  Frame-II , now the user lifts her head a bit, hence the corresponding viewing window changes from the previous one.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Similarly, for the next frame,  Frame-II , now the user lifts her head a bit, hence the corresponding viewing window changes from the previous one. Because of this, now the  football  is partially located in the viewing window, and requires computing (only for the bottom right part that is inside the viewing window). Note also that, since the  soccer ball  hologram has been already generated in Frame-I , we can skip its computation.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "Note also that, since the  soccer ball  hologram has been already generated in Frame-I , we can skip its computation. Again, the  box  object is still outside of the viewing window and thus, we do not need to compute its hologram. We use such a viewing-window based “sub-hologram” technique which has already been proposed in prior works (such as Sub-Hologram [ 52 ]) as the  Baseline  design.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "We use such a viewing-window based “sub-hologram” technique which has already been proposed in prior works (such as Sub-Hologram [ 52 ]) as the  Baseline  design. •  Apart from the viewing window, the dense or sparse hologram computing is also RoF-dependent, which is the main idea be- hind foveated rendering [ 25 ,  47 ,  62 ], as discussed in Sec. 1 and Sec.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "1 and Sec. 2.2.3. In the  Inter-Holo  scenario shown in Fig.",
      "type": "sliding_window",
      "tokens": 21
    },
    {
      "text": "In the  Inter-Holo  scenario shown in Fig. 5b, such a region of focus is just a subset of the entire viewing window, and thus contains a small number of objects that need to be computed with rich information (as it needs 16 depth planes). However, for the objects outside of the current RoF, since the user is not cur- rently focusing on them, a reasonable approximation would not affect the user experience that much (which implies we do not need 16 depth planes for all of them).",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "However, for the objects outside of the current RoF, since the user is not cur- rently focusing on them, a reasonable approximation would not affect the user experience that much (which implies we do not need 16 depth planes for all of them). For example, in  Frame-I  in Fig. 5b, the user is currently focusing on the  soccer ball ; mean- while the  football  is located outside of the RoF, hence, becomes a candidate for approximation.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "5b, the user is currently focusing on the  soccer ball ; mean- while the  football  is located outside of the RoF, hence, becomes a candidate for approximation. On the other hand, in  Frame-II , the user moves her eyes and changes the region of focus. Now, the  football  needs the full depth planes’ information, while the soccer ball  can be approximated.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "Now, the  football  needs the full depth planes’ information, while the soccer ball  can be approximated. To take advantage of this opportunity, the  football  object (which is inside the RoF in this example scenario) requires all of the 16 depth planes to compute its dense hologram, whereas the other objects (the  soccer ball in this case) can be approximated with a pre-defined sparse sam- pling factor (e.g.,   1 \n2 ; more details provided later in Algo. 2 and Sec.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "2 and Sec. 4.3). We leverage such foveated rendering idea in  Inter-Holo as our  Reference  design in this paper.",
      "type": "sliding_window",
      "tokens": 30
    },
    {
      "text": "We leverage such foveated rendering idea in  Inter-Holo as our  Reference  design in this paper. •  The above  Viewing-Window  and  Inter-Holo  proposals target at reducing the amount of computation for the holograms from the head orientation ( rotation ) and eye tracking ( up-down ) perspec- tives. As discussed in Fig.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "As discussed in Fig. 3a, another enabler for computation reduction is the relative distance between the camera/user and the objects, i.e.,  left-right . As one can observe from the  Intra- Holo  scenario shown in Fig.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "As one can observe from the  Intra- Holo  scenario shown in Fig. 5c, the  football  is larger than the soccer ball , and is located much closer to the user. Hence, even though both of them are located inside the RoF (and, of course, inside the viewing window), intuitively, the  soccer ball hologram does not need as much information as the  football hologram to compute.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "Hence, even though both of them are located inside the RoF (and, of course, inside the viewing window), intuitively, the  soccer ball hologram does not need as much information as the  football hologram to compute. Inspired by this observation, another level of approximation can be explored based on the relative camera- to-object distance as well as the object range/size. 4.2 HoloAR Overview \nDriven by the above discussion and the potential approximation opportunities presented by the  Inter-Holo  and  Intra-Holo  scenarios, we propose  HoloAR , a novel framework for holographic process- ing in AR applications to improve  both  the performance and en- ergy consumption of the hologram processing, without affecting user experience.",
      "type": "sliding_window",
      "tokens": 171
    },
    {
      "text": "4.2 HoloAR Overview \nDriven by the above discussion and the potential approximation opportunities presented by the  Inter-Holo  and  Intra-Holo  scenarios, we propose  HoloAR , a novel framework for holographic process- ing in AR applications to improve  both  the performance and en- ergy consumption of the hologram processing, without affecting user experience. HoloAR  aims to reduce the amount of hologram computations as much as possible by carefully approximating the hologram computing for select objects, while maintaining an ac- ceptable video quality. The overall design of our proposed  HoloAR framework is illustrated in Fig.",
      "type": "sliding_window",
      "tokens": 150
    },
    {
      "text": "The overall design of our proposed  HoloAR framework is illustrated in Fig. 6a. First,  HoloAR  utilizes the exist- ing viewing-window based technique [52] (denoted \na  ) to skip the hologram computations for the objects which are outside of the current viewing window, in a “just-in-time” fashion.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "First,  HoloAR  utilizes the exist- ing viewing-window based technique [52] (denoted \na  ) to skip the hologram computations for the objects which are outside of the current viewing window, in a “just-in-time” fashion. Next,  HoloAR employs the  Inter-Holo  scheme (denoted  b  ), to take advantage of the region of focus from analyzing the current eye tracking inputs and sparsely compute the objects outside the RoF. Finally,  HoloAR \n499 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n(a) Viewing-Window scenario [52].",
      "type": "sliding_window",
      "tokens": 174
    },
    {
      "text": "Finally,  HoloAR \n499 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n(a) Viewing-Window scenario [52]. (b) Inter-Holo scenario. (c) Intra-Holo scenario.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "(c) Intra-Holo scenario. Figure 5: Three opportunities for reducing hologram computation in an AR application. (a) HoloAR overview.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "(a) HoloAR overview. (b) The  Inter-Holo  and  Intra-Holo . Figure 6: The proposed  HoloAR  which includes  Inter-Holo leveraging foveated rendering, and  Intr-Holo  further ap- proximating holograms for far objects.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "Figure 6: The proposed  HoloAR  which includes  Inter-Holo leveraging foveated rendering, and  Intr-Holo  further ap- proximating holograms for far objects. uses the  Intra-Holo  scheme (denoted  c  ), to identify the number of depth planes required for a particular object by analyzing the rela- tive camera-to-object distance as well as the shape/size of the target object. Note that, both the  Inter-Holo  and the  Intra-Holo  schemes are complementary to each other, when both the eye tracking and pose estimation inputs are available at the same time.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "Note that, both the  Inter-Holo  and the  Intra-Holo  schemes are complementary to each other, when both the eye tracking and pose estimation inputs are available at the same time. Therefore, we also investigate a combined  Inter-Intra-Holo  scheme which com- bines both the schemes to further reduce the amount of hologram computations. We would like to emphasize that the proposed  HoloAR  framework can, in principle, work with any hardware platform.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "We would like to emphasize that the proposed  HoloAR  framework can, in principle, work with any hardware platform. As discussed later in Sec. 5, in this paper, we evaluate the performance and energy benefits of  HoloAR  by using an embedded GPU prototype for the edge AR headsets [ 36 ], and leave the hardware-software co- design based on FPGA-based acceleration for future work.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "5, in this paper, we evaluate the performance and energy benefits of  HoloAR  by using an embedded GPU prototype for the edge AR headsets [ 36 ], and leave the hardware-software co- design based on FPGA-based acceleration for future work. However, the architectural insights on how to co-design a next-generation accelerator that can accommodate our proposed  HoloAR  framework are discussed later in Sec. 5.5.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "5.5. 4.3 Inter-Holo Computation Optimization \nWe first answer how to deploy the previously proposed foveated rendering technique on AR holograms, by investigating how to leverage the temporal similarity when the user’s region of focus is only a part of the entire viewing window, as mentioned earlier in Sec. 2.2 (Fig.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "2.2 (Fig. 3b). To capture the current RoF, an additional eye track- ing step is introduced before the hologram computations, as shown in Fig.",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "To capture the current RoF, an additional eye track- ing step is introduced before the hologram computations, as shown in Fig. 6b \na  . This eye tracking step takes the current IR sensor im- ages as its input, and analyzes the user’s current gaze area as well as \nAlgorithm 2:  Inter-Holo algorithm.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "This eye tracking step takes the current IR sensor im- ages as its input, and analyzes the user’s current gaze area as well as \nAlgorithm 2:  Inter-Holo algorithm. Input : IRs : eye tracking sensors Input : Objs : set of virtual objects Input : α : inter-holo approximation factor,  α  ∈( 0 ,  1 ] Output: Holoдrams : Generated holograms \n1  procedure  Inter _ Holo ( IRs ,  Objs ,  α ) // main \n2 RoF =  EyeT rackinд ( IRs ) \n3 for  obj  in  Objs  do // View-Window only \n4 if  obj  in  RoF  then // inside of RoF \n5 Holograms[obj] = Algorithm1( 16 , obj) \n6 else // outside of RoF, thus approximate \n7 Holograms[obj] = Algorithm1( 16  ×  α , obj) \n8 return  { Holoдrams } \nthe viewing direction. Note that this additional eye tracking proce- dure needs to be invoked for each frame, in order to capture/reflect the current eye movements without causing nausea for the user.",
      "type": "sliding_window",
      "tokens": 304
    },
    {
      "text": "Note that this additional eye tracking proce- dure needs to be invoked for each frame, in order to capture/reflect the current eye movements without causing nausea for the user. As a result, eye tracking needs to incur minimum overhead, while providing a fairly good accuracy. Fortunately, there already exist a large body of techniques which can track the eye movements efficiently (e.g., see [ 26 ] and [ 12 ] and the references therein).",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Fortunately, there already exist a large body of techniques which can track the eye movements efficiently (e.g., see [ 26 ] and [ 12 ] and the references therein). In this work, we chose to use the NVGaze technique [ 26 ] to perform eye tracking for the  Inter-Holo  design due to two main reasons. First, it provides sufficient accuracy for the AR applications – as high as 2 .",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "First, it provides sufficient accuracy for the AR applications – as high as 2 . 06 °  accuracy for gaze shape/direction estimation across a wide field of view [ 26 ]. Second, its execution latency when running on our edge GPU prototype [ 36 ] is within 4 .",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Second, its execution latency when running on our edge GPU prototype [ 36 ] is within 4 . 5 ms , which contributes to less than 1% of the entire hologram processing pipeline latency. With the RoF attained from the eye tracking, the next question we need to answer is how to deploy the approximation opportuni- ties discussed above in Sec.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "With the RoF attained from the eye tracking, the next question we need to answer is how to deploy the approximation opportuni- ties discussed above in Sec. 2.2.3 on top of the existing hologram pipeline. As shown by  Line#5  and  Line#7  in Algo.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "As shown by  Line#5  and  Line#7  in Algo. 2, our proposal can actually reuse the original hologram execution engine without any architectural modifications or reprogramming. In fact, only one input argument, i.e., the number of depth planes, requires to be changed based on the approximation factor  α , when the object is outside of RoF.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "In fact, only one input argument, i.e., the number of depth planes, requires to be changed based on the approximation factor  α , when the object is outside of RoF. Here, we set  α  to 0 . 5, as our detailed profiling (discussed later in Sec.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "5, as our detailed profiling (discussed later in Sec. 5) indicates that setting  α  to this specific value brings significant energy savings while maintaining good hologram quality. We also present a sensitivity study on how en- ergy savings and performance vary with different approximation factors in Sec.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "We also present a sensitivity study on how en- ergy savings and performance vary with different approximation factors in Sec. 5.4. 500 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "500 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 4.4 Intra-Holo Computation Optimization \nAlgorithm 3:  Intra-Holo proposal algorithm. Input : Poses : pose sensors Input : Objs : set of virtual objects Output: Holoдrams : Generated holograms \n1  procedure  Intra _ Holo ( Poses ,  Objs ) // main \n2 Cam2ObjDists =  PoseEstimation ( Poses ) \n3 for  obj  in  Objs  do // approx.",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "Input : Poses : pose sensors Input : Objs : set of virtual objects Output: Holoдrams : Generated holograms \n1  procedure  Intra _ Holo ( Poses ,  Objs ) // main \n2 Cam2ObjDists =  PoseEstimation ( Poses ) \n3 for  obj  in  Objs  do // approx. based on dist. 4 β  =  approxFactors (cam2ObjDists[obj]) \n5 Holograms[obj] = Algorithm1( 16  ×  β , obj) \n6 return  { Holoдrams } \nIn the  Inter-Holo  design, the hologram computation can be ap- proximated by identifying the region of focus from eye tracking.",
      "type": "sliding_window",
      "tokens": 200
    },
    {
      "text": "4 β  =  approxFactors (cam2ObjDists[obj]) \n5 Holograms[obj] = Algorithm1( 16  ×  β , obj) \n6 return  { Holoдrams } \nIn the  Inter-Holo  design, the hologram computation can be ap- proximated by identifying the region of focus from eye tracking. However, the scope of this approximation opportunity might be limited due to the strict 16 depth planes requirement for all objects inside the RoF, regardless of their distance from the user. In fact, there may still be another level of opportunity for approximating the objects in long distance ( Intra-Holo , shown in Fig.",
      "type": "sliding_window",
      "tokens": 174
    },
    {
      "text": "In fact, there may still be another level of opportunity for approximating the objects in long distance ( Intra-Holo , shown in Fig. 5c). To lever- age this opportunity, we need to know where the user is located in the world and what the objects in the world look like [ 13 ,  19 ,  53 ,  59 ].",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "To lever- age this opportunity, we need to know where the user is located in the world and what the objects in the world look like [ 13 ,  19 ,  53 ,  59 ]. Next, we use one of the popular SLAM techniques, Kimera-VIO [ 53 ], to estimate the user’s pose and understand the relative positions of the objects and the user. As shown in Fig.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "As shown in Fig. 6b  b  , similar to the  Inter- Holo  pipeline, the additional pose estimation step also sits between the inputs and the original hologram processing, and thus has to be efficient without introducing much overhead. Our profiling on the edge GPU prototype [ 36 ] shows that Kimera-VIO takes, on av- erage, 13 .",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "Our profiling on the edge GPU prototype [ 36 ] shows that Kimera-VIO takes, on av- erage, 13 . 75 ms  latency to execute, which is less than 1% of the total hologram processing time. Therefore, the overhead introduced due to the additional pose estimation step is negligible compared to the baseline latency, thereby opening up opportunities for significant energy savings and performance speedup as demonstrated later in Sec.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "Therefore, the overhead introduced due to the additional pose estimation step is negligible compared to the baseline latency, thereby opening up opportunities for significant energy savings and performance speedup as demonstrated later in Sec. 5. With the help of the pose estimation, now the AR hologram pipeline has the knowledge about the range/size of each object as well as its relative distance from the user.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "With the help of the pose estimation, now the AR hologram pipeline has the knowledge about the range/size of each object as well as its relative distance from the user. Next, as shown in Algo. 3, for each of the objects, a corresponding approximation factor ( β ) can be determined based on these insights.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "3, for each of the objects, a corresponding approximation factor ( β ) can be determined based on these insights. Similarly, the original hologram engine can still be reused without any reprogramming, except for the first argument, i.e., the number of depth planes for this particular object, as shown in  Line#5  of Algo. 3.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "3. Inter-Intra-Holo:  It is to be noted that, when the user eye track- ing and pose estimation are available simultaneously for hologram processing, the  Inter-Holo  and  Intra-Holo  schemes can be both ap- plied to achieve maximum amount of energy savings and perfor- mance benefits. In this paper, we refer to this combined scheme as  Inter-Intra-Holo .",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "In this paper, we refer to this combined scheme as  Inter-Intra-Holo . In this scheme, we first identify the objects in- side/outside the RoF ( Inter-Holo ), and then approximate each of them based on its shape and distance ( Intra-Holo ). Note that since the other option – first  Intra-Holo , then  Inter-Holo  – is theoreti- cally identical to the proposed  Inter-Intra-Holo , we skip its detailed discussion due to space limitation.",
      "type": "sliding_window",
      "tokens": 126
    },
    {
      "text": "Note that since the other option – first  Intra-Holo , then  Inter-Holo  – is theoreti- cally identical to the proposed  Inter-Intra-Holo , we skip its detailed discussion due to space limitation. 4.5 Design and Implementation \nOptimization Choices:  Our main goal in this paper is to reduce the amount of hologram computation by appropriate approxima- tion, in order to speed up hologram processing, to satisfy the real- time requirement as well as to reduce the energy consumption and prolong the battery life of the AR device, while maintaining the QoS. Our proposal is fundamentally different from prior optimizations targeting various architectures or execution environments, such as customized hardware accelerators [ 35 ], cloud assistance [ 16 ,  27 ,  67 ], or neural network training/inferencing [ 33 ,  54 ].",
      "type": "sliding_window",
      "tokens": 192
    },
    {
      "text": "Our proposal is fundamentally different from prior optimizations targeting various architectures or execution environments, such as customized hardware accelerators [ 35 ], cloud assistance [ 16 ,  27 ,  67 ], or neural network training/inferencing [ 33 ,  54 ]. Note that, each of these prior efforts has its own limitations, e.g., expensive in-house implementation and fixed functionality without proper power gat- ing in accelerators [ 35 ]; requiring reliable network connections and expensive round-trip latency in cloud offloading [ 16 ,  27 ,  67 ]; and re-training of a new model for each application scenario and poten- tially for each user in neural networks [ 33 ,  54 ]. Thus, our proposal does not rely on any assistance from hardware accelerators, cloud platforms, or neural networks.",
      "type": "sliding_window",
      "tokens": 187
    },
    {
      "text": "Thus, our proposal does not rely on any assistance from hardware accelerators, cloud platforms, or neural networks. Instead, we focus exclusively on a typical edge GPU to execute the hologram, and present our three techniques, namely,  Inter-Holo  (as  Reference ),  Intra-Holo  and  Inter- Intra-Holo , which capture various approximation opportunities in the AR hologram applications to improve both performance and energy efficiency. Framework Prototype:  To prototype a real-life AR headset, a proper codebase and a hardware platform are essential.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "Framework Prototype:  To prototype a real-life AR headset, a proper codebase and a hardware platform are essential. For our codebase, we build our proposals on top of ILLIXR [ 19 ], which is the first open-source full-system extended reality testbed. IL- LIXR already contains several AR software components (some of them are shown in Fig.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "IL- LIXR already contains several AR software components (some of them are shown in Fig. 1c), including head tracking, IMU integra- tion, reprojection, and sound processing. On top of the ILLIXR codebase, we implemented three new components – eye tracking, pose estimation, and hologram processing.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "On top of the ILLIXR codebase, we implemented three new components – eye tracking, pose estimation, and hologram processing. Further, we mapped these AR software components to an edge GPU prototype [ 36 ], from which the power breakdown across different components such as SoC, memory, CPU, and GPU are measured through the on-board Texas Instruments INA 3221 voltage monitor IC hard- ware, and the performance of execution status is sampled by the Nvidia NVPROF [ 37 ] profiling tool, which enables the collection of a timeline of CUDA-related activities on both the CPU and GPU, including kernel execution, memory transfer, CUDA API calls and events/metrics for CUDA kernels. 5 EVALUATION We evaluate our proposed  HoloAR  design by comparing the execu- tion latency and total energy consumption with four different AR hologram setups.",
      "type": "sliding_window",
      "tokens": 204
    },
    {
      "text": "5 EVALUATION We evaluate our proposed  HoloAR  design by comparing the execu- tion latency and total energy consumption with four different AR hologram setups. In this section, we first describe our evaluation methodology, experimental platform, datasets, and measurement tools. Next, we analyze the results measured using these platforms.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "Next, we analyze the results measured using these platforms. After that, we show the general applicability of the proposed de- sign, and also present results from a sensitivity study that focuses on the quality-loss vs. energy-savings trade-offs. We conclude this section by outlining some research directions for implementing approximation-based accelerators for AR holograms.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "We conclude this section by outlining some research directions for implementing approximation-based accelerators for AR holograms. 501 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \nTable 2: Salient features of the six videos used in this study. No.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "No. Video #Frames #Obj/Frame Distance ObjSize 1 bike[38] 150k 1.1 2.08m 1.54m 2 book[39] 576k 1.5 0.64m 0.28m 3 bottle[40] 476k 1.1 0.47m 0.22m 4 cup[41] 546k 1.6 0.47m 0.16m 5 laptop[42] 485k 1.3 0.58m 0.38m 6 shoe[43] 557k 2.3 0.65m 0.21m \n5.1 AR Hologram Configurations \nWe evaluate the following five configurations of AR holo- gram processing to demonstrate the effectiveness of our proposed HoloAR : •  Baseline (Viewing-Window):  Similar to the recent viewing- window based sub-hologram optimization [ 52 ], we first obtain the field of view or the current viewing window from the user’s head orientation, and then skip the computations of the objects, which are outside the viewing window (i.e., only compute for the objects located inside) to save computations and energy. This software-based viewing window optimization is considered to be the state-of-the-art at an algorithm level, and we refer to it as Baseline  in this study.",
      "type": "sliding_window",
      "tokens": 273
    },
    {
      "text": "This software-based viewing window optimization is considered to be the state-of-the-art at an algorithm level, and we refer to it as Baseline  in this study. We evaluate this baseline by profiling its performance and energy consumption from a mobile GPU [36]. •  Inter-Holo:  We evaluate the  Inter-Holo  design on a mobile GPU [ 36 ] using a framework similar to the state-of-the-art IL- LIXR framework [ 19 ], with one additional eye tracking task (as shown in Fig.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "•  Inter-Holo:  We evaluate the  Inter-Holo  design on a mobile GPU [ 36 ] using a framework similar to the state-of-the-art IL- LIXR framework [ 19 ], with one additional eye tracking task (as shown in Fig. 6b \na  ) integrated into the existing pipeline to par- tially bypass the computations of holograms that are outside the focus area. Note that, this implementation is purely done in software, without any hardware modification.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "Note that, this implementation is purely done in software, without any hardware modification. •  Intra-Holo:  We evaluate our  Intra-Holo  design again on a mobile GPU as shown in Fig. 6b  b  .",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "6b  b  . This approach tries to reduce the amount of hologram computation by approximating each of the holograms based on the distance between the user and the object. •  Inter-Intra-Holo : The above two designs can be integrated to- gether into the original hologram pipeline, in either Inter-then- Intra or Intra-then-Inter fashion.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "•  Inter-Intra-Holo : The above two designs can be integrated to- gether into the original hologram pipeline, in either Inter-then- Intra or Intra-then-Inter fashion. In this paper, we chose the first one and denote this design as  Inter-Intra-Holo . •  HORN-8:  While hardware acceleration of hologram is not a goal of this work, to qualitatively compare our GPU-based design with hardware specific accelerators, we also discuss one of the most recent ASIC implementations, HORN-8 [ 35 ].",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "•  HORN-8:  While hardware acceleration of hologram is not a goal of this work, to qualitatively compare our GPU-based design with hardware specific accelerators, we also discuss one of the most recent ASIC implementations, HORN-8 [ 35 ]. Due to unavailabil- ity of its hardware implementation or datasheet, we estimate its power efficiency compared to the equivalent GPU SoC based on a published data [ 51 ]. With this estimation, we briefly discuss the performance and computational efficiency variations between this accelerator and our approach, and discuss takeaways that can help one to co-design a hardware accelerator targeting hologram processing.",
      "type": "sliding_window",
      "tokens": 143
    },
    {
      "text": "With this estimation, we briefly discuss the performance and computational efficiency variations between this accelerator and our approach, and discuss takeaways that can help one to co-design a hardware accelerator targeting hologram processing. 5.2 Experimental Platform and Datasets \nThe edge GPU platform used in this work consists of a 512- core Volta GPU, a 4Kp60 HEVC codec, 16GB LPDDR4x memory, 32GB eMMC storage, and a power management unit (PMU) that exposes the real-time power traces to users [ 36 ]. To ensemble the \nAR pipeline with generic state-of-the-art components shown in Fig.",
      "type": "sliding_window",
      "tokens": 149
    },
    {
      "text": "To ensemble the \nAR pipeline with generic state-of-the-art components shown in Fig. 1c, we implemented an open-source full-system extended real- ity testbed, ILLIXR [ 19 ], on the edge GPU platform [ 36 ], and built our  HoloAR  design on top of it. To collect performance metrics such as the streaming multiprocessor (SM) utilization, memory traffic, and CUDA kernel execution latency, we utilized the open-source Nvidia NVPROF tool [37] on the GPU platform.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "To collect performance metrics such as the streaming multiprocessor (SM) utilization, memory traffic, and CUDA kernel execution latency, we utilized the open-source Nvidia NVPROF tool [37] on the GPU platform. We use the published short object-centric Objectron [ 1 ] video dataset, which is accompanied by AR session metadata such as camera poses, as well as the object annotations such as position, orientation and dimension for nine categories of object videos 4 . The salient characteristics these videos are given in Tab.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "The salient characteristics these videos are given in Tab. 2. To re- place these real objects, we choose six virtual holograms (Sniper, Rock, Tree, Planet, Rabbit, and Dice holograms) from the Open- Holo depthmap database [ 45 ].",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "To re- place these real objects, we choose six virtual holograms (Sniper, Rock, Tree, Planet, Rabbit, and Dice holograms) from the Open- Holo depthmap database [ 45 ]. Note that the real-object and the corresponding virtual-hologram are randomly mapped because, theoretically, different mappings have no impact on the perfor- mance speedup and energy saving results (shown in Sec. 5.3).",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "5.3). 5.3 Experimental Results We present the power and energy consumption, as well as the execution latency of the hologram computation in Fig. 7, when processing the six videos listed in Tab.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "7, when processing the six videos listed in Tab. 2, with the first four config- urations described earlier in Sec. 5.1.",
      "type": "sliding_window",
      "tokens": 30
    },
    {
      "text": "5.1. We discuss the impact of our proposal on output/result quality, compared to the baseline design, later in Sec. 5.4.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "5.4. Power Consumption:  Overall, the  Inter-Holo  scheme consumes around 4 . 24 Watts , on average, when running on the edge GPU, which translates to 3 .",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "24 Watts , on average, when running on the edge GPU, which translates to 3 . 86% power reduction, compared to the baseline. In addition, our  Intra-Holo  scheme is more power efficient than  Inter- Holo , translating to 27 .",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "In addition, our  Intra-Holo  scheme is more power efficient than  Inter- Holo , translating to 27 . 72% power reduction with respect to the baseline. This indicates that the optimization scope of the distance- based  Intra-Holo  is larger than that of the RoF-based  Inter-Holo , which provides more sparsity in the hologram computing.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "This indicates that the optimization scope of the distance- based  Intra-Holo  is larger than that of the RoF-based  Inter-Holo , which provides more sparsity in the hologram computing. Finally, combination of the two schemes ( Inter-Intra-Holo ), results in 28 . 95% power reduction compared to the baseline.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "95% power reduction compared to the baseline. To better explain where the power benefits come from, we further breakdown the power consumption for the hologram processing into four parts: CPU (to handle sensor inputs, scheduling, kernel launch, etc. ), GPU (to execute hologram), Mem (for data accesses), and the SoC (the remaining hardware components, e.g., codec, network), with different number of depth planes (ranging from 2 to 16), as shown in Fig.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "), GPU (to execute hologram), Mem (for data accesses), and the SoC (the remaining hardware components, e.g., codec, network), with different number of depth planes (ranging from 2 to 16), as shown in Fig. 8a. One can observe from this figure that, when the number of depth planes is increased, the power consumptions of  SoC  and  CPU  do not change much, while, in contrast, both the GPU  and  Mem  consume more power.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "One can observe from this figure that, when the number of depth planes is increased, the power consumptions of  SoC  and  CPU  do not change much, while, in contrast, both the GPU  and  Mem  consume more power. This is due to the fact that, to process a denser hologram with more depth planes, additional GPU cores are scheduled to launch the per-plane CUDA kernel (as discussed in Algo. 1) with more holographic data accesses (fetched from the host-side memory).",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "1) with more holographic data accesses (fetched from the host-side memory). We next quantify how many depth planes can be reduced by our approximation scheme. Towards this, we plot, in Fig.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "Towards this, we plot, in Fig. 8b, the average number of depth planes (across our six videos), required by the four design alternatives (configurations). We see that, the number \n4 Due to space limitation, we chose six representative categories that cover diversity across multiple video parameters.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "We see that, the number \n4 Due to space limitation, we chose six representative categories that cover diversity across multiple video parameters. 502 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 4413.87 4243.51 3190.25 3135.99 \n0 1000 2000 3000 4000 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "4413.87 4243.51 3190.25 3135.99 \n0 1000 2000 3000 4000 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. baseline InterHolo IntraHolo InterIntraHolo \nAvg.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "baseline InterHolo IntraHolo InterIntraHolo \nAvg. Power (mW) \n(a) Avg. power (mW).",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "power (mW). 1006.01 \n0 500 \n1000 1500 2000 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. baseline InterHolo IntraHolo InterIntraHolo \nExec. Latency (ms) \nHoloCompute Overhead \n876.81 \n430.15 393.07 \n(b) Exec.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Latency (ms) \nHoloCompute Overhead \n876.81 \n430.15 393.07 \n(b) Exec. latency (ms). 4.48 \n0 2 4 6 8 10 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "4.48 \n0 2 4 6 8 10 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. baseline InterHolo IntraHolo InterIntraHolo \nEnergy (J) \nHoloCompute Overhead \n3.68 \n1.40 1.28 \n(c) Energy consumption (J).",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "baseline InterHolo IntraHolo InterIntraHolo \nEnergy (J) \nHoloCompute Overhead \n3.68 \n1.40 1.28 \n(c) Energy consumption (J). Figure 7: (a) Average power consumption, (b) execution latency, and (c) energy consumption with different configurations and video inputs. 0 \n2000 \n4000 \n6000 \n2 4 6 8 10 12 14 16 \nPower (mW) \n# Depth-planes \nCPU SoC GPU Mem \n(a) Power breakdown.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "0 \n2000 \n4000 \n6000 \n2 4 6 8 10 12 14 16 \nPower (mW) \n# Depth-planes \nCPU SoC GPU Mem \n(a) Power breakdown. 23.6 \n19.8 \n7.1 6.7 \n0 10 20 30 40 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. baseline InterHolo IntraHolo InterIntraHolo \nAvg. #depthPlanes \n(b) Avg.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "#depthPlanes \n(b) Avg. number of depth planes. Figure 8: (a): Profiling the power breakdown on the edge GPU prototype [36]; and (b): Average number of depth planes required for four design configurations.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "Figure 8: (a): Profiling the power breakdown on the edge GPU prototype [36]; and (b): Average number of depth planes required for four design configurations. of depth planes required by the  Inter-Holo  scheme is reduced from 23 . 6 to 19 .",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "1 and 6 . 7, by the  Intra-Holo and  Inter-Intra-Holo  schemes, respectively. The above observations from these two figures explain the power benefits of our proposed designs.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "The above observations from these two figures explain the power benefits of our proposed designs. Execution Latency:  Clearly, the reduction in the number of depth planes when using our approximation schemes can reduce the hologram execution latency as well. As shown in Fig.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "As shown in Fig. 7b, overall, the  Inter-Holo  scheme provides a 1 . 15 ×  speedup compared to the baseline.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "15 ×  speedup compared to the baseline. Further, a 2 . 42 ×  speedup is achieved when employing Intra-Holo  (with only 0 .",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "42 ×  speedup is achieved when employing Intra-Holo  (with only 0 . 44% overhead), and 2 . 68 ×  when employ- ing  Inter-Intra-Holo  (with only 0 .",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "68 ×  when employ- ing  Inter-Intra-Holo  (with only 0 . 14% overhead). Recall that the number of depth planes for each hologram object affects the ex- ecution latency dramatically as shown in Fig.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Recall that the number of depth planes for each hologram object affects the ex- ecution latency dramatically as shown in Fig. 4b; thus, these per- formance benefits come from the speedup brought by the reduced depth planes by approximation in our schemes. Another interest- ing observation is that,  Intra-Holo  saves more execution time than Inter-Holo .",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Another interest- ing observation is that,  Intra-Holo  saves more execution time than Inter-Holo . This is because the latter only approximates the objects outside of the current region of focus (still requiring full compute for the objects inside), whereas the scope of the former is much larger, i.e., including all the objects in the current viewing window and approximating each of them based on its location. In addition, from an individual video’s perspective, we further observe that the  shoe  video achieves the maximum performance benefits from our schemes (specifically, 23%, 69% and 73% latency reduction with Inter-Holo ,  Intra-Holo  and  Inter-Intra-Holo , respectively, compared to the baseline).",
      "type": "sliding_window",
      "tokens": 172
    },
    {
      "text": "In addition, from an individual video’s perspective, we further observe that the  shoe  video achieves the maximum performance benefits from our schemes (specifically, 23%, 69% and 73% latency reduction with Inter-Holo ,  Intra-Holo  and  Inter-Intra-Holo , respectively, compared to the baseline). In contrast, the  bike  video achieves the minimum speedup (4%, 34% and 36% in the same order). The reason behind this is that, as shown earlier in Tab.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "The reason behind this is that, as shown earlier in Tab. 2, the  bike  video usually has only one object per frame (1 . 1 on average), and also the ranges/sizes of the bikes are larger, compared to others.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "1 on average), and also the ranges/sizes of the bikes are larger, compared to others. Thus, chances for ap- proximating the objects outside the RoF (in  Inter-Holo ) and the objects which are relatively far-away from the user (in  Intra-Holo ) \nare limited. On the other hand, the  shoe  video frames typically contain more objects (2 .",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "On the other hand, the  shoe  video frames typically contain more objects (2 . 3 on average, as shown in Tab. 2), thereby gaining more opportunities to reduce the amount of computations for all the objects in the current frame.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "2), thereby gaining more opportunities to reduce the amount of computations for all the objects in the current frame. Energy Savings:  The above power and latency reductions pro- vided by  HoloAR  eventually translates to energy savings for the hologram processing. As shown in Fig.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "As shown in Fig. 7c, on average, the  Inter- Holo  scheme saves 18% energy compared to the baseline, and the Intra-Holo  scheme saves 70% energy. Finally, the energy saving achieved by the  Inter-Intra-Holo  scheme is about 73%, meaning that it only consumes 27% of the baseline energy.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "Finally, the energy saving achieved by the  Inter-Intra-Holo  scheme is about 73%, meaning that it only consumes 27% of the baseline energy. To put the energy-efficiency of our designs into perspective, we compared their energy consumption against the state-of-the-art HORN-8 hardware accelerator [ 35 ]. Due to the unavailability of the hardware RTL, we estimated its energy consumption based on the published characterization numbers from the Jetson GPU plat- form with the ZCU102 FPGA[ 64 ] (which is similar to the HORN-8 prototype) [ 51 ].",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "Due to the unavailability of the hardware RTL, we estimated its energy consumption based on the published characterization numbers from the Jetson GPU plat- form with the ZCU102 FPGA[ 64 ] (which is similar to the HORN-8 prototype) [ 51 ]. Because of the LUT memoization and power ef- ficiency optimizations [ 35 ], HORN-8 saves around 48% power 5 . However, HORN-8 does not explore the approximation opportuni- ties to speedup the hologram execution.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "However, HORN-8 does not explore the approximation opportuni- ties to speedup the hologram execution. Hence, as shown in Fig. 7c, our  HoloAR  design running on the edge GPU [ 36 ] still saves 25% more energy than the custom HORN-8 accelerator.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "7c, our  HoloAR  design running on the edge GPU [ 36 ] still saves 25% more energy than the custom HORN-8 accelerator. 5.4 Sensitivity Study \nImpact on Quality:  The prior  Inter-Holo  scheme captures the small-region of eye focus to approximate the hologram outside of RoF, and the proposed  Intra-Holo  takes advantage of the sparse computation required for the far objects to reduce the number of depth planes. To study how these approximation decisions affect the hologram video quality, we next want to reconstruct/render the hologram from our design based on the real-time eye move- ments and head orientations, and compare the quality of the recon- structed images against the baseline using the peak signal-to-noise ratio (PSNR) [ 21 ,  44 ] metric.",
      "type": "sliding_window",
      "tokens": 193
    },
    {
      "text": "To study how these approximation decisions affect the hologram video quality, we next want to reconstruct/render the hologram from our design based on the real-time eye move- ments and head orientations, and compare the quality of the recon- structed images against the baseline using the peak signal-to-noise ratio (PSNR) [ 21 ,  44 ] metric. Given the lack of the physical optical holographic displays (e.g., the prototype built in Tensor Holog- raphy project [ 54 ]), we numerically generate the reconstructed holographic images on top of the OpenHolo library [ 18 ]. Three demo examples of reconstructed images by OpenHolo are shown in Fig.",
      "type": "sliding_window",
      "tokens": 172
    },
    {
      "text": "Three demo examples of reconstructed images by OpenHolo are shown in Fig. 9: viewing a whole-hologram from different pupil positions in Fig. 9a; viewing an entire hologram (in Fig.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "9a; viewing an entire hologram (in Fig. 9b); and a partial \n5 The data is from our estimation based on [ 51 ], rather than real-hardware measurements. 503 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n(a) Viewing W-CGH from different eye-center positions.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "503 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n(a) Viewing W-CGH from different eye-center positions. Eye-center coordi- nates from left to right:  (0, 11mm) ,  (0, 12mm) ,  (0, 13mm) , and  (0, 14mm) . (b) Viewing W-CGH from different focal distances.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "(b) Viewing W-CGH from different focal distances. Focal distance from left to right:  0.3m ,  0.4m ,  0.5m , and  0.6m . (c) Viewing S-CGH from different focal distances.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "(c) Viewing S-CGH from different focal distances. Focal distance from left to right:  0.3m ,  0.4m ,  0.5m , and  0.6m \nFigure 9: A demo of viewing/rendering the virtual planet whole-hologram (W-CGH, generated from all of the depth planes, i.e., from  1-st  to  16-th ) or sub-hologram (S-CGH, generated from only a subset of the depth planes, from 9-th  to  12-th  in this case) with different configurations. (a): Viewing the W-CGH from different eye-center positions.",
      "type": "sliding_window",
      "tokens": 150
    },
    {
      "text": "(a): Viewing the W-CGH from different eye-center positions. (b) Viewing the W-CGH from different focal distances. (c): View- ing the S-CGH from different focal distances.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "(c): View- ing the S-CGH from different focal distances. 41.48 31.79 30.74 \n0 10 20 30 40 50 60 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. InterHolo IntraHolo InterIntraHolo \nPSNR \n(a) PSNR.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "InterHolo IntraHolo InterIntraHolo \nPSNR \n(a) PSNR. 0% \n20% \n40% \n60% \n80% \n100% \n0% 20%40%60%80%100% \norm. PSNR (%) \nEnergy Savings (%) \n(b) Trade-offs.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "PSNR (%) \nEnergy Savings (%) \n(b) Trade-offs. Figure 10: Sensitivity studies. hologram (in Fig.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "hologram (in Fig. 9c) from different distances. Compared to the baseline, we then report the averaged PSNR [ 21 ,  44 ] of the recon- structed images from the six videos in Fig.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Compared to the baseline, we then report the averaged PSNR [ 21 ,  44 ] of the recon- structed images from the six videos in Fig. 10a. It can be observed from this figure that, even with the most aggressive approximation introduced by  Inter-Intra-Holo , the video quality is still sufficient for most of the AR applications (30 .",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "It can be observed from this figure that, even with the most aggressive approximation introduced by  Inter-Intra-Holo , the video quality is still sufficient for most of the AR applications (30 . 7 on average) [57]. Further, to study how the tuned approximation (in Algo.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "Further, to study how the tuned approximation (in Algo. 2 and Algo. 3) affect the energy savings achieved, we report five design points in Fig.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "3) affect the energy savings achieved, we report five design points in Fig. 10b. This figure shows a clear pattern of trade-offs between more-energy-savings vs. more-quality-drop.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "This figure shows a clear pattern of trade-offs between more-energy-savings vs. more-quality-drop. Generality of  HoloAR :  Although the core idea of approximation seems to be general across many video domains, our proposal is not expected to work very well for all AR applications. Specifi- cally, there are two classes of applications that would probably achieve only limited benefits from our approach.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Specifi- cally, there are two classes of applications that would probably achieve only limited benefits from our approach. First, for the quality-critical applications such as AR surgery [ 56 ], ultra-high resolution/quality of holograms are typically required. In this case, offloading computations to a resource-rich cluster/cloud system would be a more reasonable design choice (instead of approximating on the edge).",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "In this case, offloading computations to a resource-rich cluster/cloud system would be a more reasonable design choice (instead of approximating on the edge). Second, for applications, which are motion-sensitive \nsuch as the spaceship simulation [ 34 ], the hologram computation process is required to complete faster, in order to correctly reflect the current user’s eye movement and head pose in real-time. The proposed  HoloAR  on the edge GPU cannot achieve such strict la- tency requirement, and can cause lagging, e.g., the eye could move to another area, while the hologram is still being computed for the previous focus region.",
      "type": "sliding_window",
      "tokens": 149
    },
    {
      "text": "The proposed  HoloAR  on the edge GPU cannot achieve such strict la- tency requirement, and can cause lagging, e.g., the eye could move to another area, while the hologram is still being computed for the previous focus region. We postpone optimizations for such applications to a future work. 5.5 Future Work \nDespite the hardware-agnostic nature of  HoloAR , it is still in- teresting to study how to deploy our idea on an ASIC hardware, and co-design the next-generation accelerator on edge for the AR hologram.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "5.5 Future Work \nDespite the hardware-agnostic nature of  HoloAR , it is still in- teresting to study how to deploy our idea on an ASIC hardware, and co-design the next-generation accelerator on edge for the AR hologram. Towards this, we plan to explore three critical questions in our future work: First, how many processing units (PUs) are required and just sufficient for most of the cases in a typical AR holographic application? To answer this, we plan to characterize the number of depth planes needed in various AR applications, and guide the optimal design choices (i.e., number of PUs, frequency, input and output buffer size, etc.)",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "To answer this, we plan to characterize the number of depth planes needed in various AR applications, and guide the optimal design choices (i.e., number of PUs, frequency, input and output buffer size, etc.) based on application require- ments, and evaluate both PSNR and user-experience metrics such as satisfaction and dizziness [ 66 ]. Second, How do we maintain high power efficiency of PUs during runtime?",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Second, How do we maintain high power efficiency of PUs during runtime? In some cases where a small amount of hologram computation required, not all of the PUs on-board are needed to be active. We plan to design and imple- ment a clock/power gating technology to switch off the un-utilized PUs and save power/energy.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "We plan to design and imple- ment a clock/power gating technology to switch off the un-utilized PUs and save power/energy. Third, how do we handle the corner cases, where more computational resources than that provided by the accelerator are required? Towards this, we plan to design a system-level scheduler which can efficiently partition the hologram tasks between the heterogeneous accelerator and original execution engines such as CPUs or GPUs.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "Towards this, we plan to design a system-level scheduler which can efficiently partition the hologram tasks between the heterogeneous accelerator and original execution engines such as CPUs or GPUs. 6 RELATED WORK \nIn this section, we summarize prior work related to different aspects of holographic processing. Optimizations in Holographic Processing:  Holographic pro- cessing has been optimized in various domains [ 33 ,  35 ,  52 ,  54 ], to improve power efficiency or execution performance.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "Optimizations in Holographic Processing:  Holographic pro- cessing has been optimized in various domains [ 33 ,  35 ,  52 ,  54 ], to improve power efficiency or execution performance. For exam- ple, HORN-8 [ 35 ] has proposed a special-purpose computer for electro-holography to reduce the power consumption and still de- liver a high frame rate (similar to that of a cloud GPU). From the software/algorithm perspective, a sub-hologram technique is pro- posed with a tracked viewing-window technology to tailor the holographic computation only for the necessary information in- side of the window [ 52 ].",
      "type": "sliding_window",
      "tokens": 148
    },
    {
      "text": "From the software/algorithm perspective, a sub-hologram technique is pro- posed with a tracked viewing-window technology to tailor the holographic computation only for the necessary information in- side of the window [ 52 ]. More recent efforts have attempted to combine holographic processing with neural network techniques. For instance, DeepHolo [ 33 ] proposes a binary-weighted computer- generated hologram model to recognize 3D objects.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "For instance, DeepHolo [ 33 ] proposes a binary-weighted computer- generated hologram model to recognize 3D objects. Furthermore, another convolution neural network (CNN) model is trained and deployed on mobile devices to synthesize a photorealistic colour 3D hologram from a single RGB-depth image in real time [ 54 ]. Apart from neural network techniques, foveated rendering is another promising performance optimization for reducing computational costs [ 2 ,  22 ,  24 ,  25 ,  30 ,  47 ,  62 ], as summarized in Sec.",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "Apart from neural network techniques, foveated rendering is another promising performance optimization for reducing computational costs [ 2 ,  22 ,  24 ,  25 ,  30 ,  47 ,  62 ], as summarized in Sec. 2.2.2. In this paper, the foveated rendering idea (denoted as  Inter-Holo  design) has been implemented (in Sec.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "In this paper, the foveated rendering idea (denoted as  Inter-Holo  design) has been implemented (in Sec. 4.3) and found to work well (in \n504 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. Sec.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "Sec. 5) as in prior works. Further, in this paper, we have gone beyond foveated rendering ( Inter-Holo ), by proposing an optimiza- tion/approximation called  Intra-Holo , that complements the former in boosting performance/energy efficiency.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Further, in this paper, we have gone beyond foveated rendering ( Inter-Holo ), by proposing an optimiza- tion/approximation called  Intra-Holo , that complements the former in boosting performance/energy efficiency. This enhancement is ideally suited for holographic processing at the edge, without re- quiring additional hardware, cloud assistance, or machine learning framework. Holographic Displays on AR:  Another large body of prior works focus on optimizing the holographic displays for the next- generation AR headsets [ 6 ,  17 ,  23 ].",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "Holographic Displays on AR:  Another large body of prior works focus on optimizing the holographic displays for the next- generation AR headsets [ 6 ,  17 ,  23 ]. For example, Michelson proposes a holographic display technology that optimizes image quality for emerging near-eye displays using two SLMs and camera-in-the- loop calibration [ 7 ]. Neural-Holography proposes an algorithmic hologram generation framework that uses camera-in-the-loop train- ing to achieve unprecedented image fidelity and real-time frame rates [ 48 ].",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "Neural-Holography proposes an algorithmic hologram generation framework that uses camera-in-the-loop train- ing to achieve unprecedented image fidelity and real-time frame rates [ 48 ]. OLAS proposes an overlap-add stereogram algorithm, which uses overlapping hogels to encode the view-dependent light- ing effects of a light field into a hologram, achieving better quality than other holographic stereograms [ 46 ]. These display quality optimizations are orthogonal to our approximation-based proposal, and our approach can be used along with such optimizations.",
      "type": "sliding_window",
      "tokens": 136
    },
    {
      "text": "These display quality optimizations are orthogonal to our approximation-based proposal, and our approach can be used along with such optimizations. Volumetric Video Streaming, Compression, and Other Opti- mizations:  Volumetric sensor inputs such as LiDAR have large vol- ume and require significant computational power and bandwidth to process/transmit. Targeting them, prior efforts have proposed to optimize their compression ratio, processing performance, and energy efficiency [ 8 – 10 ,  16 ,  27 ,  67 – 70 ].",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Targeting them, prior efforts have proposed to optimize their compression ratio, processing performance, and energy efficiency [ 8 – 10 ,  16 ,  27 ,  67 – 70 ]. For example, ASV lever- ages characteristics unique to stereo vision and proposes algorith- mic and computational optimizations to improve performance and energy-efficiency of “depth from stereo” [ 11 ]. Tigris proposes an algorithm-architecture co-design system specialized for point cloud registration, to improve real-time performance and energy effi- ciency for 3D perception applications [ 65 ].",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "Tigris proposes an algorithm-architecture co-design system specialized for point cloud registration, to improve real-time performance and energy effi- ciency for 3D perception applications [ 65 ]. To efficiently steam volumetric video to mobile devices, GROOT proposes a novel PD- Tree data structure and streams the volumetric videos at a 30fps frame rate with minimal memory usage and computation for de- coding [ 27 ]. Note, however, that none of these existing schemes target at reducing the amount of “unnecessary” computations in the AR holographic applications.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "Note, however, that none of these existing schemes target at reducing the amount of “unnecessary” computations in the AR holographic applications. In addition to the  Inter-Holo  de- sign, our proposed  Intra-Holo  technique focuses on computation approximation opportunities, and as such, it is orthogonal to these prior efforts. 7 CONCLUSION The extremely heavy computation in hologram processing hinders the growth of the 3D display applications on AR headsets.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "7 CONCLUSION The extremely heavy computation in hologram processing hinders the growth of the 3D display applications on AR headsets. Thus, prior efforts have proposed using accelerators or cloud for optimiz- ing the hologram computation. In contrast, this paper attempts to exploit available approximation opportunities unique in AR holo- graphic applications, and proposes a two-stage  HoloAR  scheme to speed up the execution and save energy.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "In contrast, this paper attempts to exploit available approximation opportunities unique in AR holo- graphic applications, and proposes a two-stage  HoloAR  scheme to speed up the execution and save energy. Specifically, we leverage the existing foveated rendering in  Inter-Holo  to track the user’s eye movements and approximate the holograms of the objects that are outside the user interest. We also propose  Intra-Holo  to further approximate each of the object holograms, by analyzing its cur- rent distance from the user.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "We also propose  Intra-Holo  to further approximate each of the object holograms, by analyzing its cur- rent distance from the user. Our experimental results show that, compared to the baseline,  HoloAR  achieves 2 . 7 ×  speedup and 73% \nenergy savings.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "7 ×  speedup and 73% \nenergy savings. We believe that the lessons learned from this work will help in designing next-generation hologram accelerators that can combine approximation as well as other optimizations such as tuning PU counts, frequency, and power gating for achieving the target performance and energy efficiency for edge devices. ACKNOWLEDGMENTS \nThis research is supported in part by NSF grants #1763681, #1629915, #1629129, #1317560, #1526750, #1714389, #1912495, and #1909004.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "ACKNOWLEDGMENTS \nThis research is supported in part by NSF grants #1763681, #1629915, #1629129, #1317560, #1526750, #1714389, #1912495, and #1909004. This work was also supported in part by CRISP, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. We would also like to thank Dr. Jack Sampson and Dr. Dinghao Wu for their feedback on this paper.",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "We would also like to thank Dr. Jack Sampson and Dr. Dinghao Wu for their feedback on this paper. REFERENCES \n[1]  Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, and Matthias Grundmann. 2020.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "2020. Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations. arXiv preprint arXiv:2012.09988  (2020).",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "arXiv preprint arXiv:2012.09988  (2020). [2]  Rachel Albert, Anjul Patney, David Luebke, and Joohwan Kim. 2017.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "2017. Latency Requirements for Foveated Rendering in Virtual Reality. ACM Trans.",
      "type": "sliding_window",
      "tokens": 23
    },
    {
      "text": "[3]  ARCore. 2020. Using Scene Viewer to Display Interactive 3D Models in AR from an Android App or Browser.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "Using Scene Viewer to Display Interactive 3D Models in AR from an Android App or Browser. \"https://developers.google.com/ar/develop/java/ scene-viewer\". [4]  Stephen A Benton and V Michael Bove Jr. 2008.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "[4]  Stephen A Benton and V Michael Bove Jr. 2008. Holographic Imaging . John Wiley & Sons.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "2020. Pokémon GO Revenue and Usage Statistics. \"https: //www.businessofapps.com/data/pokemon-go-statistics/\".",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "\"https: //www.businessofapps.com/data/pokemon-go-statistics/\". [6]  Chenliang Chang, Kiseung Bang, Gordon Wetzstein, Byoungho Lee, and Liang Gao. 2020.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "2020. Toward the Next-generation VR/AR Optics: A Review of Holographic Near-eye Displays from a Human-centric Perspective. Optica  (2020), 1563–1578.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "Optica  (2020), 1563–1578. [7]  Suyeon Choi, Jonghyun Kim, Yifan Peng, and Gordon Wetzstein. 2021.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "2021. Optimizing image quality for holographic near-eye displays with Michelson Holography. Optica  (2021), 143–146.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "Optica  (2021), 143–146. [8]  Yu Feng, Patrick Hansen, P. Whatmough, Guoyu Lu, and Yuhao Zhu. 2021.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "2021. A LiDAR-Guided Framework for Video Enhancement. ArXiv  (2021).",
      "type": "sliding_window",
      "tokens": 25
    },
    {
      "text": "ArXiv  (2021). [9]  Y. Feng, Shaoshan Liu, and Yuhao Zhu. 2020.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "2020. Real-Time Spatio-Temporal LiDAR Point Cloud Compression. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)  (2020), 10766–10773.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)  (2020), 10766–10773. [10]  Yu Feng, Boyuan Tian, Tiancheng Xu, Paul Whatmough, and Yuhao Zhu. 2020.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "2020. Mesorasi: Architecture Support for Point Cloud Analytics via Delayed- aggregation. In  Proceedings of the International Symposium on Microarchitecture (MICRO) .",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 1037–1050. [11]  Yu Feng, Paul Whatmough, and Yuhao Zhu.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "[11]  Yu Feng, Paul Whatmough, and Yuhao Zhu. 2019. ASV: Accelerated Stereo Vision System.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "ASV: Accelerated Stereo Vision System. In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 643–656.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "2020. Eye Tracking and Neuromarketing Research Made Easy. \"https://www.gazept.com/\".",
      "type": "sliding_window",
      "tokens": 25
    },
    {
      "text": "\"https://www.gazept.com/\". [13]  Patrick Geneva, Kevin Eckenhoff, Woosik Lee, Y. Yang, and Guoquan Huang. 2020.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "2020. OpenVINS: A Research Platform for Visual-Inertial Estimation. 2020 IEEE International Conference on Robotics and Automation (ICRA)  (2020), 4666–4672.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "2020 IEEE International Conference on Robotics and Automation (ICRA)  (2020), 4666–4672. [14]  Giorgia Lombardo. 2020.",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "2020. Meet the Humans of the Future: Holograms, Digital Humans, and Deep Fakes. \"https://medium.com/demagsign/meet-the-humans- of-the-future-holograms-digital-humans-and-deep-fakes-35024b881545\".",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "\"https://medium.com/demagsign/meet-the-humans- of-the-future-holograms-digital-humans-and-deep-fakes-35024b881545\". [15]  Stuart Golodetz, Michael Sapienza, Julien Valentin, Vibhav Vineet, Ming-Ming Cheng, Anurag Arnab, Victor Adrian Prisacariu, Olaf Kaehler, Carl Yuheng Ren, David W. Murray, Shahram Izadi, and Philip H.S. Torr.",
      "type": "sliding_window",
      "tokens": 145
    },
    {
      "text": "Torr. 2015. SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes.",
      "type": "sliding_window",
      "tokens": 27
    },
    {
      "text": "SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes. arXiv  (2015). [16]  Bo Han, Yu Liu, and Feng Qian.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "[16]  Bo Han, Yu Liu, and Feng Qian. 2020. ViVo: Visibility-aware Mobile Volumetric Video Streaming.",
      "type": "sliding_window",
      "tokens": 37
    },
    {
      "text": "ViVo: Visibility-aware Mobile Volumetric Video Streaming. In  Proceedings of the ACM/IEEE International Conference on Mobile Computing and Networking (MobiCom) . 1–13.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "1–13. [17]  Zehao He, Xiaomeng Sui, Guofan Jin, and Liangcai Cao. 2019.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "2019. Progress in Virtual Reality and Augmented Reality Based on Holographic Display. Appl.",
      "type": "sliding_window",
      "tokens": 19
    },
    {
      "text": "(2019), A74–A81. [18]  Jisoo Hong, Youngmin Kim, Hyunjoo Bae, and Sunghee Hong. 2020.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "2020. OpenHolo: Open Source Library for Hologram Generation, Reconstruction and Signal Pro- cessing. In  Imaging and Applied Optics Congress .",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "In  Imaging and Applied Optics Congress . Optical Society of America, HF3G.1. [19]  Muhammad Huzaifa, Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Finn Sinclair, Boyuan Tian, Hengzhi Yuan, Jeffrey Zhang, and Sarita V. Adve.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "[19]  Muhammad Huzaifa, Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Finn Sinclair, Boyuan Tian, Hengzhi Yuan, Jeffrey Zhang, and Sarita V. Adve. 2021. Exploring Extended Reality with ILLIXR: A new Playground for Architecture Research.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Exploring Extended Reality with ILLIXR: A new Playground for Architecture Research. arXiv:cs.DC/2004.04643 \n505 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n[20]  IFIXIT. 2020.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "2020. Magic Leap One Teardown. \"https://www.ifixit.com/Teardown/Magic+Leap+One+Teardown/112245\".",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "\"https://www.ifixit.com/Teardown/Magic+Leap+One+Teardown/112245\". [21]  NATIONAL INSTRUMENTS. 2019.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "2019. Peak Signal-to-Noise Ratio as an Image Quality Metric. \"https://www.ni.com/en-us/innovations/white-papers/11/peak- signal-to-noise-ratio-as-an-image-quality-metric.html\".",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "\"https://www.ni.com/en-us/innovations/white-papers/11/peak- signal-to-noise-ratio-as-an-image-quality-metric.html\". [22]  Yeon-Gyeong Ju and Jae-Hyeung Park. 2018.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "2018. Fast Generation of Mesh Based CGH in Head-Mounted Displays using Foveated Rendering Technique, In Imaging and Applied Optics 2018 (3D, AO, AIO, COSI, DH, IS, LACSEA, LS&C, MATH, pcAOP). Imaging and Applied Optics 2018 (3D, AO, AIO, COSI, DH, IS, LACSEA, LS&C, MATH, pcAOP) , DTu5F.6.",
      "type": "sliding_window",
      "tokens": 125
    },
    {
      "text": "Imaging and Applied Optics 2018 (3D, AO, AIO, COSI, DH, IS, LACSEA, LS&C, MATH, pcAOP) , DTu5F.6. [23]  Daniel K Nikolov, Sifan Ye, Sydney Dlhopolsky, Zhen Bai, Yuhao Zhu, and Jannick P Rolland. 2020.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "2020. Hyperion: A 3D Visualization Platform for Optical Design of Folded Systems. Frameless  2, 1 (2020), 21.",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "Frameless  2, 1 (2020), 21. [24]  Anton Kaplanyan, Anton Sochenov, Thomas Leimkühler, Mikhail Okunev, T. Goodall, and Gizem Rufo. 2019.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "2019. DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos. ACM Trans.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "(2019), 212:1–212:13. [25]  Jonghyun Kim, Youngmo Jeong, Michael Stengel, Kaan Akşit, Rachel Albert, Ben Boudaoud, Trey Greer, Joohwan Kim, Ward Lopes, Zander Majercik, Peter Shirley, Josef Spjut, Morgan McGuire, and David Luebke. 2019.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "2019. Foveated AR: Dynamically-Foveated Augmented Reality Display. ACM Trans.",
      "type": "sliding_window",
      "tokens": 24
    },
    {
      "text": "(2019). [26]  Joohwan Kim, Michael Stengel, Alexander Majercik, Shalini De Mello, David Dunn, Samuli Laine, Morgan McGuire, and David Luebke. 2019.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "2019. Nvgaze: An Anatomically-informed Dataset for Low-latency, Near-eye Gaze Estimation. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems .",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems . 1–12. [27]  Kyungjin Lee, Juheon Yi, Youngki Lee, Sunghyun Choi, and Young Min Kim.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "[27]  Kyungjin Lee, Juheon Yi, Youngki Lee, Sunghyun Choi, and Young Min Kim. 2020. GROOT: A Real-time Streaming System of High-fidelity Volumetric Videos.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "GROOT: A Real-time Streaming System of High-fidelity Volumetric Videos. In Proceedings of the ACM/IEEE International Conference on Mobile Computing and Networking (MobiCom) . 1–14.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "2008. Hybrid Feature Tracking and User Interaction for Markerless Augmented Reality. In  2008 IEEE Virtual Reality Conference .",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "In  2008 IEEE Virtual Reality Conference . 145–152. [29]  Magic Leap.",
      "type": "sliding_window",
      "tokens": 22
    },
    {
      "text": "[29]  Magic Leap. 2020. Magic Leap 1 is a Wearable Computer for Enterprise Produc- tivity.",
      "type": "sliding_window",
      "tokens": 30
    },
    {
      "text": "Magic Leap 1 is a Wearable Computer for Enterprise Produc- tivity. \"https://www.magicleap.com/en-us/magic-leap-1\". [30]  Xiaoxu Meng, Ruofei Du, and Amitabh Varshney.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "[30]  Xiaoxu Meng, Ruofei Du, and Amitabh Varshney. 2020. Eye-dominance-guided Foveated Rendering.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "Eye-dominance-guided Foveated Rendering. IEEE Transactions on Visualization and Computer Graphics (2020), 1972–1980. [31]  Microsoft.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "HoloLens 2 Tech Specs. \"https://www.microsoft.com/en- us/p/holoLens-2/91pnzzznzwcp/?activetab=pivot:techspecstab\". [32]  Microsoft Research Blog.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "[32]  Microsoft Research Blog. 2020. Second Version of HoloLens HPU will Incorporate AI Coprocessor for Implementing DNNs.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "Second Version of HoloLens HPU will Incorporate AI Coprocessor for Implementing DNNs. \"https://www.microsoft.com/en- us/research/blog/second-version-hololens-hpu-will-incorporate-ai- coprocessor-implementing-dnns/\". [33]  Naoya Muramatsu, Chun Wei Ooi, Yuta Itoh, and Yoichi Ochiai.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "[33]  Naoya Muramatsu, Chun Wei Ooi, Yuta Itoh, and Yoichi Ochiai. 2017. Deep- Holo: Recognizing 3D Objects Using a Binary-Weighted Computer-Generated Hologram.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Deep- Holo: Recognizing 3D Objects Using a Binary-Weighted Computer-Generated Hologram. In  SIGGRAPH Asia 2017 Posters . [34]  NASA.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "[34]  NASA. 2020. NASA at Home – Virtual Tours and Apps.",
      "type": "sliding_window",
      "tokens": 20
    },
    {
      "text": "NASA at Home – Virtual Tours and Apps. \"https://www.nasa.gov/ nasa-at-home-virtual-tours-and-augmented-reality\". [35]  Takashi Nishitsuji, Yota Yamamoto, Takashige Sugie, Takanori Akamatsu, Ryuji Hirayama, Hirotaka Nakayama, Takashi Kakue, Tomoyoshi Shimobaba, and Tomoyoshi Ito.",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "[35]  Takashi Nishitsuji, Yota Yamamoto, Takashige Sugie, Takanori Akamatsu, Ryuji Hirayama, Hirotaka Nakayama, Takashi Kakue, Tomoyoshi Shimobaba, and Tomoyoshi Ito. 2018. Special-purpose Computer HORN-8 for Phase-type Electro- holography.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Special-purpose Computer HORN-8 for Phase-type Electro- holography. Opt. Express  (2018), 26722–26733.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "2019. JETSON AGX XAVIER AND THE NEW ERA OF AUTONOMOUS MACHINES. \"http://info.nvidia.com/rs/156-OFN-742/images/Jetson_AGX_ Xavier_New_Era_Autonomous_Machines.pdf\".",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "\"http://info.nvidia.com/rs/156-OFN-742/images/Jetson_AGX_ Xavier_New_Era_Autonomous_Machines.pdf\". [37] Nvidia. 2020.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "2020. CUDA Toolkit Documentation: Nvprof. \"shorturl.at/zEFU5\".",
      "type": "sliding_window",
      "tokens": 28
    },
    {
      "text": "2020. Objectron Dataset Annotation: bike. \"https://github.com/google- research-datasets/Objectron/blob/master/index/bike_annotations\".",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "\"https://github.com/google- research-datasets/Objectron/blob/master/index/bike_annotations\". [39]  Objectron. 2020.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "2020. Objectron Dataset Annotation: book. \"https://github.com/ google-research-datasets/Objectron/blob/master/index/book_annotations\".",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "\"https://github.com/ google-research-datasets/Objectron/blob/master/index/book_annotations\". [40]  Objectron. 2020.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "2020. Objectron Dataset Annotation: bottle. \"https://github.com/ google-research-datasets/Objectron/blob/master/index/bottle_annotations\".",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "\"https://github.com/ google-research-datasets/Objectron/blob/master/index/bottle_annotations\". [41]  Objectron. 2020.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "2020. Objectron Dataset Annotation: cup. \"https://github.com/google- research-datasets/Objectron/blob/master/index/cup_annotations\".",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "\"https://github.com/google- research-datasets/Objectron/blob/master/index/cup_annotations\". [42]  Objectron. 2020.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "2020. Objectron Dataset Annotation: laptop. \"https://github.com/ google-research-datasets/Objectron/blob/master/index/laptop_annotations\".",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "\"https://github.com/ google-research-datasets/Objectron/blob/master/index/laptop_annotations\". [43]  Objectron. 2020.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "2020. Objectron Dataset Annotation: shoe. \"https://github.com/google- research-datasets/Objectron/blob/master/index/shoe_annotations\".",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "\"https://github.com/google- research-datasets/Objectron/blob/master/index/shoe_annotations\". [44]  OpenCV. 2019.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "2019. Similarity check (PNSR and SSIM) on the GPU. \"https://docs.opencv.org/2.4/doc/tutorials/gpu/gpu-basics-similarity/gpu- basics-similarity.html\".",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "\"https://docs.opencv.org/2.4/doc/tutorials/gpu/gpu-basics-similarity/gpu- basics-similarity.html\". [45] OpenHolo. 2020.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "2020. OpenHolo Database. \"http://openholo.org/database/depth\".",
      "type": "sliding_window",
      "tokens": 23
    },
    {
      "text": "\"http://openholo.org/database/depth\". [46]  Nitish Padmanaban, Yifan Peng, and Gordon Wetzstein. 2019.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "2019. Holographic Near- Eye Displays Based on Overlap-Add Stereograms. ACM Trans.",
      "type": "sliding_window",
      "tokens": 23
    },
    {
      "text": "(2019). [47]  Anjul Patney, Marco Salvi, Joohwan Kim, Anton Kaplanyan, Chris Wyman, Nir Benty, David Luebke, and Aaron Lefohn. 2016.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "2016. Towards Foveated Rendering for Gaze-Tracked Virtual Reality. ACM Trans.",
      "type": "sliding_window",
      "tokens": 25
    },
    {
      "text": "(2016). [48]  Yifan Peng, Suyeon Choi, Nitish Padmanaban, Jonghyun Kim, and Gordon Wet- zstein. 2020.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "2020. Neural Holography. In  ACM SIGGRAPH 2020 Emerging Technologies (SIGGRAPH ’20) .",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "In  ACM SIGGRAPH 2020 Emerging Technologies (SIGGRAPH ’20) . Association for Computing Machinery. [49]  Martin Persson, David Engström, and Mattias Goksör.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "[49]  Martin Persson, David Engström, and Mattias Goksör. 2011. Real-time Generation of Fully Optimized Holograms for Optical Trapping Applications.",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "Real-time Generation of Fully Optimized Holograms for Optical Trapping Applications. In  Optical Trapping and Optical Micromanipulation VIII , Vol. 8097. International Society for Optics and Photonics, 80971H.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "8097. International Society for Optics and Photonics, 80971H. [50]  Victor Adrian Prisacariu, Olaf Kähler, Stuart Golodetz, Michael Sapienza, Tom- maso Cavallari, Philip H. S. Torr, and David William Murray. 2017.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "2017. InfiniTAM v3: A Framework for Large-Scale 3D Reconstruction with Loop Closure. CoRR (2017).",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "CoRR (2017). [51]  Murad Qasaimeh, Kristof Denolf, Jack Lo, Kees A. Vissers, Joseph Zambreno, and Phillip H. Jones. 2019.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "2019. Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for Vision Kernels. In  15th IEEE International Conference on Embedded Software and Systems .",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "In  15th IEEE International Conference on Embedded Software and Systems . 1–8. [52]  Stephan Reichelt, Ralf Haussler, Norbert Leister, Gerald Futterer, Hagen Stolle, and Armin Schwerdtner.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "[52]  Stephan Reichelt, Ralf Haussler, Norbert Leister, Gerald Futterer, Hagen Stolle, and Armin Schwerdtner. 2010. Holographic 3-D Displays - Electro-holography Within the Grasp of Commercialization .",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "Holographic 3-D Displays - Electro-holography Within the Grasp of Commercialization . IntechOpen. shorturl.at/jmnpD [53]  Antoni Rosinol, Marcus Abate, Yun Chang, and Luca Carlone.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "shorturl.at/jmnpD [53]  Antoni Rosinol, Marcus Abate, Yun Chang, and Luca Carlone. 2020. Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping. In  IEEE Intl. Conf.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "Conf. on Robotics and Automation (ICRA) . [54]  Liang Shi, Beichen Li, Changil Kim, Petr Kellnhofer, and Wojciech Matusik.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "[54]  Liang Shi, Beichen Li, Changil Kim, Petr Kellnhofer, and Wojciech Matusik. 2021. Towards Real-time Photorealistic 3D Holography with Deep Neural Networks.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Towards Real-time Photorealistic 3D Holography with Deep Neural Networks. Nature  592 (2021). [55]  Tomoyoshi Shimobaba, Jiantong Weng, Takahiro Sakurai, Naohisa Okada, Takashi Nishitsuji, Naoki Takada, Atsushi Shiraki, Nobuyuki Masuda, and To- moyoshi Ito.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "[55]  Tomoyoshi Shimobaba, Jiantong Weng, Takahiro Sakurai, Naohisa Okada, Takashi Nishitsuji, Naoki Takada, Atsushi Shiraki, Nobuyuki Masuda, and To- moyoshi Ito. 2012. Computational Wave Optics Library for C++: CWO++ Library.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Computational Wave Optics Library for C++: CWO++ Library. Computer Physics Communications  (2012), 1124–1138. [56]  Jeffrey H. Shuhaiber.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "[56]  Jeffrey H. Shuhaiber. 2004. Augmented Reality in Surgery.",
      "type": "sliding_window",
      "tokens": 20
    },
    {
      "text": "Augmented Reality in Surgery. Archives of Surgery (2004), 170–174. [57]  Randall Shumaker and Lackey Stephanie.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "[57]  Randall Shumaker and Lackey Stephanie. 2014. Virtual, Augmented and Mixed Reality: Designing and Developing Augmented and Virtual Environments: 6th International Conference, VAMR 2014, Held as Part of HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014, Proceedings, Part I .",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "Virtual, Augmented and Mixed Reality: Designing and Developing Augmented and Virtual Environments: 6th International Conference, VAMR 2014, Held as Part of HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014, Proceedings, Part I . Vol. 8525.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "8525. Springer. [58]  Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "[58]  Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling. 2019. Privacy-Aware Eye Tracking Using Differential Privacy.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "Privacy-Aware Eye Tracking Using Differential Privacy. In  Proc. ACM Interna- tional Symposium on Eye Tracking Research and Applications (ETRA) .",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "ACM Interna- tional Symposium on Eye Tracking Research and Applications (ETRA) . 1–9. [59]  Stereolabs.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "[59]  Stereolabs. 2020. ZED Software Development Kit.",
      "type": "sliding_window",
      "tokens": 16
    },
    {
      "text": "ZED Software Development Kit. \"https://www.stereolabs.com/developers/release/\". [60]  techradar.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "Google Pixel 2 Review. \"https://www.techradar.com/reviews/ google-pixel-2-review\". [61]  Oren M Tepper, Hayeem L Rudy, Aaron Lefkowitz, Katie A Weimer, Shelby M Marks, Carrie S Stern, and Evan S Garfein.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "[61]  Oren M Tepper, Hayeem L Rudy, Aaron Lefkowitz, Katie A Weimer, Shelby M Marks, Carrie S Stern, and Evan S Garfein. 2017. Mixed Reality with HoloLens: Where Virtual Reality Meets Augmented Reality in the Operating Room.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Mixed Reality with HoloLens: Where Virtual Reality Meets Augmented Reality in the Operating Room. Plastic and reconstructive surgery  (2017), 1066–1070. [62]  Lingjie Wei and Yuji Sakamoto.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "[62]  Lingjie Wei and Yuji Sakamoto. 2019. Fast Calculation Method with Foveated Rendering for Computer-generated Holograms Using an Angle-changeable Ray- tracing Method.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "Fast Calculation Method with Foveated Rendering for Computer-generated Holograms Using an Angle-changeable Ray- tracing Method. Appl. Opt.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "Opt. (2019), A258–A266. [63]  Yang Wu, Jun Wang, Chun Chen, Chan-Juan Liu, Feng-Ming Jin, and Ni Chen.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "[63]  Yang Wu, Jun Wang, Chun Chen, Chan-Juan Liu, Feng-Ming Jin, and Ni Chen. 2021. Adaptive Weighted Gerchberg-Saxton Algorithm for Generation of Phase- only Hologram with Artifacts Suppression.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Adaptive Weighted Gerchberg-Saxton Algorithm for Generation of Phase- only Hologram with Artifacts Suppression. Opt. Express  (2021), 1412–1427.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "2020. Zynq UltraScale+ MPSoC ZCU102 Evaluation Kit. \"https://www.",
      "type": "sliding_window",
      "tokens": 27
    },
    {
      "text": "\"https://www. xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html\". [65]  Tiancheng Xu, Boyuan Tian, and Yuhao Zhu.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "[65]  Tiancheng Xu, Boyuan Tian, and Yuhao Zhu. 2019. Tigris: Architecture and Algorithms for 3D Perception in Point Clouds.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Tigris: Architecture and Algorithms for 3D Perception in Point Clouds. In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 629–642.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "629–642. [66]  Hiroshi Yoshikawa, Takeshi Yamaguchi, and Hiroki Uetake. 2016.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "2016. Image Quality Evaluation and Control of Computer-generated Holograms. In  Practical Hologra- phy XXX: Materials and Applications , Hans I. Bjelkhagen and V. Michael Bove Jr.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "In  Practical Hologra- phy XXX: Materials and Applications , Hans I. Bjelkhagen and V. Michael Bove Jr. (Eds.). International Society for Optics and Photonics, SPIE, 144 – 152.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "International Society for Optics and Photonics, SPIE, 144 – 152. [67]  Anlan Zhang, Chendong Wang, Bo Han, and Feng Qian. 2021.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "2021. Efficient Volu- metric Video Streaming Through Super Resolution. In  Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications .",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "In  Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications . 106–111. [68]  Haibo Zhang, Prasanna Venkatesh Rengasamy, Shulin Zhao, Nachiappan Chi- dambaram Nachiappan, Anand Sivasubramaniam, Mahmut T. Kandemir, Ravi Iyer, and Chita R. Das.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "[68]  Haibo Zhang, Prasanna Venkatesh Rengasamy, Shulin Zhao, Nachiappan Chi- dambaram Nachiappan, Anand Sivasubramaniam, Mahmut T. Kandemir, Ravi Iyer, and Chita R. Das. 2017. Race-to-Sleep + Content Caching + Display Caching: A Recipe for Energy-Efficient Video Streaming on Handhelds.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Race-to-Sleep + Content Caching + Display Caching: A Recipe for Energy-Efficient Video Streaming on Handhelds. In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 517–531.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "517–531. [69]  Haibo Zhang, Shulin Zhao, Ashutosh Pattnaik, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. 2019.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "2019. Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices. In  Proceedings of the International Symposium on Microarchitecture (MICRO) .",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 657–669. [70]  Shulin Zhao, Haibo Zhang, Sandeepa Bhuyan, Cyan Subhra Mishra, Ziyu Ying, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "[70]  Shulin Zhao, Haibo Zhang, Sandeepa Bhuyan, Cyan Subhra Mishra, Ziyu Ying, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. 2020. Déjà View: Spatio-Temporal Compute Reuse for Energy-Efficient 360 °  VR Video Streaming.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "Déjà View: Spatio-Temporal Compute Reuse for Energy-Efficient 360 °  VR Video Streaming. In  Proceedings of the International Symposium on Computer Architec- ture (ISCA) . 241–253.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Thus, improving the computational efficiency of the holographic pipeline is critical. The objective of this paper is to maximize its energy efficiency without jeopardizing the hologram quality for AR applications. Abstract \nHologram processing is the primary bottleneck and contributes to more than 50% of energy consumption in battery-operated aug- mented reality (AR) headsets.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Towards this, we take the approach of analyzing the workloads to identify approximation op- portunities. We show that, by considering various parameters like region of interest and depth of view, we can approximate the ren- dering of the virtual object to minimize the amount of computation without affecting the user experience. The objective of this paper is to maximize its energy efficiency without jeopardizing the hologram quality for AR applications.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "We show that, by considering various parameters like region of interest and depth of view, we can approximate the ren- dering of the virtual object to minimize the amount of computation without affecting the user experience. Furthermore, by optimizing the software design flow, we propose  HoloAR , which intelligently renders the most important object in sight to the clearest detail, while approximating the computations for the others, thereby sig- nificantly reducing the amount of computation, saving energy, and gaining performance at the same time. We implement our design in an edge GPU platform to demonstrate the real-world applicability of our research.",
      "type": "sliding_window_shuffled",
      "tokens": 141,
      "augmented": true
    },
    {
      "text": "We implement our design in an edge GPU platform to demonstrate the real-world applicability of our research. 7 ×  speedup and 73% energy savings. Our experimental results show that, compared to the baseline,  HoloAR  achieves, on average, 2 .",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "7 ×  speedup and 73% energy savings. CCS CONCEPTS \n•  Computing methodologies  → Ray tracing ;  •  Computer sys- tems organization  → Embedded software ;  •  Human-centered computing  → Visual analytics . ∗ Work was done as a student at Penn State.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "Copyrights for components of this work owned by others than ACM must be honored. ∗ Work was done as a student at Penn State. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "MICRO ’21, October 18–22, 2021, Virtual Event, Greece © 2021 Association for Computing Machinery. Request permissions from permissions@acm.org. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "MICRO ’21, October 18–22, 2021, Virtual Event, Greece © 2021 Association for Computing Machinery. ACM ISBN 978-1-4503-8557-2/21/10...$15.00 https://doi.org/10.1145/3466752.3480056 \nKEYWORDS \nAugmented Reality, Holographic Processing, Approximation, Energy-efficiency \nACM Reference Format: Shulin Zhao, Haibo Zhang, Cyan S. Mishra, Sandeepa Bhuyan, Ziyu Ying, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "2021. HoloAR: On-the-fly Optimization of 3D Holographic Processing for Aug- mented Reality. In  MICRO-54: 54th Annual IEEE/ACM International Sympo- sium on Microarchitecture (MICRO ’21), October 18–22, 2021, Virtual Event, Greece.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "https://doi.org/10.1145/3466752. ACM, New York, NY, USA, 13 pages. In  MICRO-54: 54th Annual IEEE/ACM International Sympo- sium on Microarchitecture (MICRO ’21), October 18–22, 2021, Virtual Event, Greece.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "3480056 \n1 INTRODUCTION Augmented reality (AR) has gained recent traction in both the con- sumer and research communities, thanks to the advances in efficient and low power computing technologies, high-speed communica- tion, and specialized hardware platforms. These technologies have become an important part of our daily life, in the form of creative photography, content creation, gaming, online shopping, virtual touring, and educational and non-educational training, etc. https://doi.org/10.1145/3466752.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "Moreover, these AR infotainment applications have helped many of us through the recent global pandemic by bringing us the liveliness of the virtual outdoors, while we were confined to our homes, and more AR capa- ble mobile devices penetrating the market with cheaper price tags have made AR applications pervasive and made the virtual world easily accessible for users on the tip of their fingers. For ex- ample, one of the earliest AR games, Pokémon GO (launched in July 2016), had a cumulative download of over 1 Billion, and gener- ated about $900 Million in revenue by late 2019 1 . These technologies have become an important part of our daily life, in the form of creative photography, content creation, gaming, online shopping, virtual touring, and educational and non-educational training, etc.",
      "type": "sliding_window_shuffled",
      "tokens": 182,
      "augmented": true
    },
    {
      "text": "Moreover, these AR infotainment applications have helped many of us through the recent global pandemic by bringing us the liveliness of the virtual outdoors, while we were confined to our homes, and more AR capa- ble mobile devices penetrating the market with cheaper price tags have made AR applications pervasive and made the virtual world easily accessible for users on the tip of their fingers. However, even the state-of-the-art mobile devices with high bandwidth cannot meet the heavy compute and real-time demands of the AR applications, leading to very low quality of service (QoS) – in some cases as low as 1 frame per second (fps) [ 19 ,  54 ]. Further, \n1 To give a quantitative estimation of the popularity of the game, a Pokémon GO event at Safari Zone New Taipei City, Taiwan in October 2019 had a total of 327,000 attendees and they walked around 4.5 million kilometers to catch 50 Million Pokémons [5].",
      "type": "sliding_window_shuffled",
      "tokens": 217,
      "augmented": true
    },
    {
      "text": "Further, \n1 To give a quantitative estimation of the popularity of the game, a Pokémon GO event at Safari Zone New Taipei City, Taiwan in October 2019 had a total of 327,000 attendees and they walked around 4.5 million kilometers to catch 50 Million Pokémons [5]. 494 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. the limited battery capacity prevents users from enjoying their AR devices for extended periods of time.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "To meet the heavy compute demands of these applications, most of AR applications are run using high-end desktop/server-class GPUs [ 18 ,  55 ], or specialized hardware accelerators [ 35 ] on cloud platforms [ 16 ,  27 ]. the limited battery capacity prevents users from enjoying their AR devices for extended periods of time. However, since most of these applications are now running on low-power mobile devices, and frequent communi- cation of data to and from cloud via wireless medium is inefficient, optimization of an AR pipeline to maximize the compute and en- ergy efficiency, while providing adequate QoS, at an edge device is an architectural challenge.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "However, since most of these applications are now running on low-power mobile devices, and frequent communi- cation of data to and from cloud via wireless medium is inefficient, optimization of an AR pipeline to maximize the compute and en- ergy efficiency, while providing adequate QoS, at an edge device is an architectural challenge. These sensor inputs play a major role in deciding which portions of the 3D voxels need to be rendered for the user to view. Furthermore, existing AR headsets are typically equipped with multiple sensors for head orientation, eye tracking, motion detection, etc., to provide an interactive and life- like experience.",
      "type": "sliding_window_shuffled",
      "tokens": 141,
      "augmented": true
    },
    {
      "text": "However, even selective rendering of the portion of a scene, which is in the field of view (FoV) of the user, on a mobile device with limited compute and power budget is challenging [ 19 ,  52 ]. This calls for finding further opportunities for optimization. These sensor inputs play a major role in deciding which portions of the 3D voxels need to be rendered for the user to view.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "This calls for finding further opportunities for optimization. 2), we profiled a set of applications and found that the  hologram  processing is the primary bottleneck in terms of computation, energy consumption, and execution latency. To under- stand the computing requirements in a typical AR pipeline consists of many stages (refer Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "The heavy compute demand of the hologram (re)construction has made this a promising candidate for acceleration, and prior works have tried to offload it to cloud [ 16 ,  27 ,  67 ] and specialized accelerators [35] to achieve high throughput, but doing so has led the communication with the edge device to be a major bottleneck. 2), we profiled a set of applications and found that the  hologram  processing is the primary bottleneck in terms of computation, energy consumption, and execution latency. Others have proposed to design efficient and lightweight deep neu- ral networks (DNNs) to achieve high quality scene rendering at the edge device itself, but this requires model retraining/tuning for a particular user [ 33 ,  54 ].",
      "type": "sliding_window_shuffled",
      "tokens": 169,
      "augmented": true
    },
    {
      "text": "Apart from the above works targeting all areas in a scene,  foveated rendering  techniques have been pro- posed to reduce image resolution in the peripheral area (typically beyond 135 °  vertically and 160 °  horizontally in human visual system (HVS)), while maintaining a normal/high quality only for 5 °  foveal vision [ 2 ,  22 ,  25 ,  47 ,  62 ]. Such differential resolution within an image can reduce computational costs without significantly impacting user experience [ 25 ,  47 ,  62 ]. Others have proposed to design efficient and lightweight deep neu- ral networks (DNNs) to achieve high quality scene rendering at the edge device itself, but this requires model retraining/tuning for a particular user [ 33 ,  54 ].",
      "type": "sliding_window_shuffled",
      "tokens": 173,
      "augmented": true
    },
    {
      "text": "Further optimizations such as eye- dominance (i.e., HVS prefers scene perception from one eye over the other) and learning-based foveated rendering are orthogonal to this core idea and beyond the scope of this paper [24, 30]. Such differential resolution within an image can reduce computational costs without significantly impacting user experience [ 25 ,  47 ,  62 ]. Despite providing significant performance and energy-efficiency benefits, these prior works still miss out on even more selective rendering of viewed hologram images - beyond just the FoV and/or regions of the user’s focus.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "This is the primary motivation of this paper, where we explore trade-offs between hologram quality and processing costs. Despite providing significant performance and energy-efficiency benefits, these prior works still miss out on even more selective rendering of viewed hologram images - beyond just the FoV and/or regions of the user’s focus. These trade-offs are not very straightforward due to the following  challenges : First,  among all of the inputs to the AR headset (shown later in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "These trade-offs are not very straightforward due to the following  challenges : First,  among all of the inputs to the AR headset (shown later in Fig. 1b), which one(s) are critical for holographic processing? Second,  which features of these inputs are salient and need more fine-grained computation, and which of them could be approximated without impacting the QoS?",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "Second,  which features of these inputs are salient and need more fine-grained computation, and which of them could be approximated without impacting the QoS? Towards this, we propose  HoloAR , an opportunistic and edge- friendly framework to speed up the AR holographic computation \n(a) An app. Third,  how do we make dynamic decisions of approximation based on the runtime conditions (e.g., user’s current pose and eye movements)?",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "Towards this, we propose  HoloAR , an opportunistic and edge- friendly framework to speed up the AR holographic computation \n(a) An app. (c) SW pipeline [19, 50]. (b) HW components [20, 29].",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "Figure 1: Main hardware components and software pipeline on a typical AR device. and improve its energy efficiency, with “approximation” as the core idea. (c) SW pipeline [19, 50].",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "The major  contributions  of this work can be summarized as follows: •  We first conduct a detailed characterization of a generic AR processing pipeline to identify the major bottlenecks in current state-of-the-art AR headsets, and set our optimization target as the  hologram computation . Starting from investigating and evaluating the existing foveated rendering techniques, this work further explores the entire design space for potential opportunities and optimizations unique in AR applications, for speedup as well as energy savings. and improve its energy efficiency, with “approximation” as the core idea.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "2.1) •  From two open-source AR datasets [ 1 ,  58 ], we identify two prop- erties in AR hologram applications:  spatio diversity for objects , and  temporal locality for the user (viewer) interests (i.e., user typ- ically focuses on one region within a short period of time) . The major  contributions  of this work can be summarized as follows: •  We first conduct a detailed characterization of a generic AR processing pipeline to identify the major bottlenecks in current state-of-the-art AR headsets, and set our optimization target as the  hologram computation . (Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 150,
      "augmented": true
    },
    {
      "text": "(Sec. Such properties are leveraged as approximation opportunities to skip the “unimportant” portions of the hologram computation, based on user’s region of focus (known as foveated rendering), and object’s distance/size from the user. 2.1) •  From two open-source AR datasets [ 1 ,  58 ], we identify two prop- erties in AR hologram applications:  spatio diversity for objects , and  temporal locality for the user (viewer) interests (i.e., user typ- ically focuses on one region within a short period of time) .",
      "type": "sliding_window_shuffled",
      "tokens": 142,
      "augmented": true
    },
    {
      "text": "(Sec. 4.3) and found to work well (in Sec. 2.2) •  To capture these two approximation opportunities from both the user and object perspectives, first, the prior  foveated ren- dering  idea (denoted as  Inter-Holo  design) has been imple- mented (in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "5) as in prior works [ 22 ,  25 ,  30 ,  47 ]. In this paper, we have gone be- yond foveated rendering ( Inter-Holo ), by proposing an optimiza- tion/approximation called  Intra-Holo , that complements the for- mer in boosting performance/energy efficiency. 4.3) and found to work well (in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Such  Intra-Holo enhancement is ideally suited for holographic processing at the edge, without requiring additional hardware, cloud assistance, or machine learning. •  We implement both the designs on an edge GPU platform [ 36 ], without the need for any hardware modification. In this paper, we have gone be- yond foveated rendering ( Inter-Holo ), by proposing an optimiza- tion/approximation called  Intra-Holo , that complements the for- mer in boosting performance/energy efficiency.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "We evaluate these designs using the NVPROF tool [ 37 ] and hardware power management unit on the edge GPU platform [ 36 ]. Our exper- imental results reveal that,  HoloAR  provides 29% reduction in power consumption and 2 . •  We implement both the designs on an edge GPU platform [ 36 ], without the need for any hardware modification.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "7 ×  speedup, which collectively trans- late to 73% total energy savings compared to the baseline setup (Sec. Our exper- imental results reveal that,  HoloAR  provides 29% reduction in power consumption and 2 . 5.3).",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "5.3). Finally, based on our findings, we discuss future direc- tions that may help one design custom hardware accelerators for AR holograms (Sec. 5.5).",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "1). 2 BACKGROUND AND MOTIVATION \nBefore diving deep into the problems and possible solutions asso- ciated with holographic processing, we first present the hardware \n495 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \nand software pipelines of a typical holographic AR application (in Fig. 5.5).",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "We further describe the existing holographic execution inefficiencies in the AR pipeline and potential opportunities for computation reduction. 2.1 AR Holographic Applications and Pipeline \nThe holographic display technique enables a large body of aug- mented applications in real life [ 14 ]. 1).",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "2.1 AR Holographic Applications and Pipeline \nThe holographic display technique enables a large body of aug- mented applications in real life [ 14 ]. 1a, where a physical car being driven on a highway is replaced by the corresponding virtual/augmented holographic car in a real-time fashion such that, instead of viewing the real cars, the AR user views the virtual ones. One such application is illus- trated in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "1b. To implement such ap- plications, today’s AR headsets are usually equipped with various hardware components for sensing and processing, as depicted in Fig. 1a, where a physical car being driven on a highway is replaced by the corresponding virtual/augmented holographic car in a real-time fashion such that, instead of viewing the real cars, the AR user views the virtual ones.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "After sensing, the input samples are then buffered in the video buffer, waiting to be processed timely at the frame-rate. 1b. Specifically, the AR hardware has three major components: Sensor Inputs:  The AR headset receives the real-time information from both the surrounding environment and the user (viewer), with two types of sensing:  ① “world sensors”  to sense the physical surrounding the user is currently in, such as cameras for the RGB image and LiDAR/depth sensor for the depth or distance of the objects in front of the user, and  ② “user sensors”  to sample the behavior/status of the user, such as inertial measurement unit (IMU) sensors for head rotation, IR sensors for eye tracking, and controller for hand gesture.",
      "type": "sliding_window_shuffled",
      "tokens": 167,
      "augmented": true
    },
    {
      "text": "Processing Engines:  To efficiently handle the above two types of inputs, various computational resources have been integrated into AR SoCs, as shown in Fig. After sensing, the input samples are then buffered in the video buffer, waiting to be processed timely at the frame-rate. 1b, e.g., CPUs for generic processing, GPUs for graphics computing, vision processing units (VPUs) for rendering, and tensor processing units (TPUs) for learning infer- ences.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "Recently, state-of-the-art AR headsets such as HoloLens [ 31 ] have even been planning to integrate the holographic processing units (HPUs) for processing the information coming from all of the on-board sensors (currently under development) [32]. 1b, e.g., CPUs for generic processing, GPUs for graphics computing, vision processing units (VPUs) for rendering, and tensor processing units (TPUs) for learning infer- ences. On-board Battery:  It is to be noted that all of the sensors and the processing engines mentioned above are  battery-backed , as shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 149,
      "augmented": true
    },
    {
      "text": "1b. On-board Battery:  It is to be noted that all of the sensors and the processing engines mentioned above are  battery-backed , as shown in Fig. This is for enabling users to freely move around in a large area without the need of connecting with a power cable constantly.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "With these sensors and compute resources in place, an AR head- set executes a set of software tasks, either entirely or selectively based on the applications’ requirements [ 19 ]. Hence, the power/energy efficiency is critical metrics in many AR use cases so that the battery lifetime can be sufficiently long. This is for enabling users to freely move around in a large area without the need of connecting with a power cable constantly.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "With these sensors and compute resources in place, an AR head- set executes a set of software tasks, either entirely or selectively based on the applications’ requirements [ 19 ]. Without loss of gener- ality, a typical AR pipeline [ 19 ] is shown in Fig. 1c.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "1c. With these inputs,  ❷ Perception  stage understands the current surrounding environment such as pose estimation for head rotations/directions, eye tracking for pupil centers, and scene reconstruction for the current view analysis. At a high level, this AR pipeline has three major stages:  ❶ Inputs  stage first collects the real-time information from all the on-board sensors such as IMU, IR, camera and depth image sensors.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Finally,  ❸ Visual  stage combines the physical world with the virtual information (which is generated in real-time) together, and renders the final images (both the physical scene as well as the virtual frame augmented with it) for the user to view. We want to emphasize that, compared to virtual reality (VR), the AR video processing typically incurs additional computational \nTable 1: Ideal latency requirements [19]. With these inputs,  ❷ Perception  stage understands the current surrounding environment such as pose estimation for head rotations/directions, eye tracking for pupil centers, and scene reconstruction for the current view analysis.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "Task Ideal Latency Algo. Pose Estimate \n33  ms Kimera [53] \nEye Track \n33  ms NVGaze [26] \nScene Reconstruct \n100  ms InfiniTAM [50] \nHologram 33  ms GSW [49, 63] \ntasks and interacts with more hardware resources [ 61 ]. We want to emphasize that, compared to virtual reality (VR), the AR video processing typically incurs additional computational \nTable 1: Ideal latency requirements [19].",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "Pose Estimate \n33  ms Kimera [53] \nEye Track \n33  ms NVGaze [26] \nScene Reconstruct \n100  ms InfiniTAM [50] \nHologram 33  ms GSW [49, 63] \ntasks and interacts with more hardware resources [ 61 ]. 5 fps, and the battery life can be as short as just 1 hour. Based on our measurements collected from a smartphone [ 60 ] running a sim- ple AR application [ 3 ], the processing performance can be lower than 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 126,
      "augmented": true
    },
    {
      "text": "This motivates us to investigate which component is the major perfor- mance and energy bottleneck, charging most of the “performance- and/or energy-taxes” from the battery-backed AR headsets. 2.2 Motivation \n2.2.1 What is the Major Bottleneck? 5 fps, and the battery life can be as short as just 1 hour.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "1c) on a typical edge prototype [ 36 ] running a set of state-of-the-art AR-related tasks [ 19 ,  26 ,  49 ,  50 , 53 ], and compared the collected results against ideal execution latencies for the same set of tasks (i.e., the maximum latency within which the task needs to finish before its next invocation). 2.2 Motivation \n2.2.1 What is the Major Bottleneck? To identify the major performance bottlenecks in the current AR headsets, we characterized the execution latency of the software pipeline (discussed above in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "The ideal latencies and our collected latencies are given in Table 1 and Fig. 2, respectively, for an ILLIXR playground scenario [15, 19]. 1c) on a typical edge prototype [ 36 ] running a set of state-of-the-art AR-related tasks [ 19 ,  26 ,  49 ,  50 , 53 ], and compared the collected results against ideal execution latencies for the same set of tasks (i.e., the maximum latency within which the task needs to finish before its next invocation).",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "Comparing the ideal latencies with practical latencies, we make the following conclusions: In our practical setting,  Pose Estimation tracks user’s motion and viewing scene to estimate the current body pose [ 53 ], and it takes around 13 . 2, respectively, for an ILLIXR playground scenario [15, 19]. 8 ms .",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Furthermore, estimating the user’s eye gaze,  Eye Track , requires the execution of a light-weight neural network that takes 4 . 8 ms . 4 ms  and achieves 2 .",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "06 °  of accuracy [ 26 ]. Thus, both of these two tasks are able to meet the performance re- quirements shown in Table 1. 4 ms  and achieves 2 .",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "On the other hand,  Scene Reconstruct captures comprehensive consistent maps of environments from an RGB-D image, and consumes 120 ms  in the practical setting. Thus, both of these two tasks are able to meet the performance re- quirements shown in Table 1. Note that such maps are not necessarily required to be generated for each frame (typically computed once per two or three frames [ 28 ,  50 ], thus 67 − 100 ms  in Table 1); hence, we argue that the state-of-the-art InfiniTAM technique, which implements a framework for real-time depth fusion and learning of 3D scenes [ 50 ], is already close to the ideal case.",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "7 ms on an edge GPU 3 . Note that such maps are not necessarily required to be generated for each frame (typically computed once per two or three frames [ 28 ,  50 ], thus 67 − 100 ms  in Table 1); hence, we argue that the state-of-the-art InfiniTAM technique, which implements a framework for real-time depth fusion and learning of 3D scenes [ 50 ], is already close to the ideal case. However,  Hologram , which takes depthmap, point-cloud, or light field as its input [ 18 ] 2   to create arbitrary 3D configurations of optical traps useful for capturing, moving and transforming mesoscopic objects freely in the world [ 4 ], takes as long as 341 .",
      "type": "sliding_window_shuffled",
      "tokens": 174,
      "augmented": true
    },
    {
      "text": "This 10 ×  performance gap between the practical scenario and the ideal case (and the large amount of power/energy consumption this task makes) motivates us to focus on holographic processing in this paper, and explore the opportunities for improv- ing the hologram computational efficiency to speed up the overall AR application execution and reduce its energy consumption. 2 In this paper, we mainly use the popular depthmap input method. 7 ms on an edge GPU 3 .",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "3 Five iterations of the GSW algorithm [63] are profiled. 496 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 2 In this paper, we mainly use the popular depthmap input method.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "496 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 0.2 8.0 13.8 4.4 \n120.0 \n6.0 \n341.7 \n0 100 200 300 400 \nIMU/IR \nCamera \nPose \nEstimate \nEye Track \nScene \nReconstruct \nReproject \nHologram \nInputs Perception Visual \nLatency (ms) \nOur  focus \nFigure 2: A comparison of latency requirements results col- lected from our practical setting and ideal cases shown in Table 1. 2.2.2 What are the Prior Optimization Efforts?",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "Targeting the compute-intensive holographic processing dis- cussed above,  foveated rendering  techniques have been previously proposed to approximate selective regions (i.e.,  peripheral vision ). In fact, prior research on HVS has shown that human eyes are able to observe beyond 135 °  vertically and 160 °  horizontally, but see fine details within an only around 5 °  central circle (i.e.,  foveal vision ). 2.2.2 What are the Prior Optimization Efforts?",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Motivated by such degradation of  peripheral visual acuity , foveated rendering  reduces computational costs for the peripheral region, and maintains high/normal resolution only for the foveal re- gion [ 2 ,  22 ,  24 ,  25 ,  30 ,  47 ,  62 ]. In fact, prior research on HVS has shown that human eyes are able to observe beyond 135 °  vertically and 160 °  horizontally, but see fine details within an only around 5 °  central circle (i.e.,  foveal vision ). For instance, a real-time gaze-tracked foveated rendering system is proposed to yield performance and memory savings by avoiding shading up to 70% of the pixels for VR headsets [ 47 ].",
      "type": "sliding_window_shuffled",
      "tokens": 171,
      "augmented": true
    },
    {
      "text": "More recently, another foveated rendering based CGH reconstruction technique has been proposed to accelerate calculations with negligible effect for the viewer [ 22 ]. For instance, a real-time gaze-tracked foveated rendering system is proposed to yield performance and memory savings by avoiding shading up to 70% of the pixels for VR headsets [ 47 ]. Similarly, a prototype AR display also takes advan- tage of foveated rendering by tracking the user’s gaze and providing low-resolution images to the peripheral area to reduce computa- tion and improve display resolution [ 25 ].",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "4.3) and found to reduce around 23% execution latency (in Sec. More recently, another foveated rendering based CGH reconstruction technique has been proposed to accelerate calculations with negligible effect for the viewer [ 22 ]. We implemented such foveated rendering idea (de- noted as  Inter-Holo  design in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "5) for AR holograms. 4.3) and found to reduce around 23% execution latency (in Sec. However, such performance gain from foveated rendering is still insufficient to close the 10 ×  gap discussed above.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "Thus, in this paper, we want to go beyond the prior foveated rendering for further optimizations, by investigating the potential opportunities which are unique to the AR use cases and may have been missed out before. 2.2.3 What are the Potential Opportunities? However, such performance gain from foveated rendering is still insufficient to close the 10 ×  gap discussed above.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "2.2.3 What are the Potential Opportunities? Towards addressing this hologram bottleneck, various ap- proaches from both the software [ 33 ,  52 ,  54 ] and hardware [ 32 ,  35 ] sides have been proposed. These prior approaches either incorpo- rate additional memory for maintaining a lookup table for compu- tation reduction, or build an application-specific integrated circuit (ASIC) chip specifically for holographic processing, which is more power-efficient than generic processors.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "Recall that, in the AR holographic application discussed above in Fig. While such approaches im- prove the hologram execution to some extent, they do not consider the unique features of the AR applications. These prior approaches either incorpo- rate additional memory for maintaining a lookup table for compu- tation reduction, or build an application-specific integrated circuit (ASIC) chip specifically for holographic processing, which is more power-efficient than generic processors.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Recall that, in the AR holographic application discussed above in Fig. 3). 1a and Section 2.1, there are two types of inputs to the holographic pipeline –  world sensors  for the physical objects (real cars in this case) in the world, and  user sensors  for the user behavior/state such as pose and eye movements (discussed in details in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "Therefore, in principle, \n0 1 2 3 \nbike \nbook \nbottle \ncamera \ncerealBox \nchair \ncup \nlaptop \nshoe \nDistance/Size (m) \nCam2ObjDist. ObjSize \n(a) Object study. 3).",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "Figure 3: Dataset study. 0 \n0.5 \n1 \n0 0.5 1 \nPupi Postion Y \nPupil Position X \n0 \n0.5 \n1 \n0 0.5 1 \nPupil Position Y \nPupil Position X \n0 \n0.5 \n1 \n0 0.5 1 \nPupil Position Y \nPupil Position X \nUser1: \nUser2: \nUser3: \n(b) User eye tracking study. ObjSize \n(a) Object study.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "(a) Depthmap hologram algorithm. 0 200 400 600 800 1000 1200 \n1 2 4 8 16 32 64 \nExec. Figure 3: Dataset study.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "0 200 400 600 800 1000 1200 \n1 2 4 8 16 32 64 \nExec. Latency (ms) \n# Depth Planes \nForward Backward \n(b) Latency w/ num of depth planes. Figure 4: Depthmap hologram algorithm details.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "more intuitive opportunities could exist in the AR application do- main, from  both  the object and user perspectives. Figure 4: Depthmap hologram algorithm details. To identify them, we studied two published AR datasets (Objectron [ 1 ] for objects shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "To identify them, we studied two published AR datasets (Objectron [ 1 ] for objects shown in Fig. 3b), and observed the following two properties in the AR holographic applications: Spatio Diversity for Objects:  Intuitively, objects which are far from the user and with small-sized shapes require less informa- tion to generate the virtual hologram than others (more details are provided in Sec. 3a, and MPIIDEye [ 58 ] for users shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "3). 3b), and observed the following two properties in the AR holographic applications: Spatio Diversity for Objects:  Intuitively, objects which are far from the user and with small-sized shapes require less informa- tion to generate the virtual hologram than others (more details are provided in Sec. Hence, the distance between the user and the objects ( Cam2ObjDist  shown in black color in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "3a) affect the amount of computations actually required to provide just enough yet necessary virtual holo- grams. 3a), as well as the size of how the object seems/appears to the user ( ObjSize shown in red color in Fig. Hence, the distance between the user and the objects ( Cam2ObjDist  shown in black color in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "3a, the bike  object is closer to the user, and also has a larger range/size ( size=farmost-nearest ); thus, more information is required to create the hologram for the  bike  for maintaining fairly good QoS than the  chair . For example, compared to the  chair  object in Fig. 3a) affect the amount of computations actually required to provide just enough yet necessary virtual holo- grams.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "Temporal Locality for the User Interests:  As also established by prior foveated rendering proposals, the foveal vision (or Region of Focus, RoF) is only a small region in the current scene and can be traced by eye tracking techniques [ 26 ]. Therefore, one opportunity to reduce the amount of computation is to  approximate  the hologram processing based on the objects’ distances and sizes. 3a, the bike  object is closer to the user, and also has a larger range/size ( size=farmost-nearest ); thus, more information is required to create the hologram for the  bike  for maintaining fairly good QoS than the  chair .",
      "type": "sliding_window_shuffled",
      "tokens": 147,
      "augmented": true
    },
    {
      "text": "As can be observed from three users’ eye tracking shown in Fig. 3b, all focus only on a portion of the entire viewing window within a short period of time (10 seconds in this case). Temporal Locality for the User Interests:  As also established by prior foveated rendering proposals, the foveal vision (or Region of Focus, RoF) is only a small region in the current scene and can be traced by eye tracking techniques [ 26 ].",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "3b, all focus only on a portion of the entire viewing window within a short period of time (10 seconds in this case). On the other hand, even when viewing the exact same scene, the RoF varies across users. For example,  User1 has similar interest as  User3 , whereas  User2  focuses more on the bottom left corner.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "Clearly, such temporal similarity for a particular user’s interests exposes another opportunity for leveraging prior \n497 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \nfoveated rendering in AR holograms, by reducing the amount of computation needed for the objects which are outside the RoF, thus only emphasizing on the processing of the objects which the user is currently focusing on. Driven by these observations, we next want to study the details of hologram with the goal of addressing two critical questions:  What are the problems in the current state-of-the-art hologram software and hardware? For example,  User1 has similar interest as  User3 , whereas  User2  focuses more on the bottom left corner.",
      "type": "sliding_window_shuffled",
      "tokens": 190,
      "augmented": true
    },
    {
      "text": ", and  How can we leverage “approximation opportunities” (based on the two observations above) to speed up hologram processing and save energy, while still maintaining a high QoS? Driven by these observations, we next want to study the details of hologram with the goal of addressing two critical questions:  What are the problems in the current state-of-the-art hologram software and hardware? 3 HOLOGRAPHIC PROCESSING STUDY \nTo leverage the opportunities in the holographic processing from a RGB-D (i.e., RGB and depth) image, we need to first understand the detailed execution of the entire hologram processing from both the algorithm and hardware perspectives.",
      "type": "sliding_window_shuffled",
      "tokens": 157,
      "augmented": true
    },
    {
      "text": "We illustrate the details of depthmap hologram processing in Fig. 4a and Algo. 3 HOLOGRAPHIC PROCESSING STUDY \nTo leverage the opportunities in the holographic processing from a RGB-D (i.e., RGB and depth) image, we need to first understand the detailed execution of the entire hologram processing from both the algorithm and hardware perspectives.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "1 as two ma- jor steps (more details on the depthmap hologram algorithm can be found elsewhere [ 4 ,  18 ,  55 ,  63 ]). 4a and Algo. As shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "With these depth planes, the first step,  Forward Propagation  (de- noted  ❶ in Fig. As shown in Fig. 4a, the depthmap input is first sliced into several planes ( M  depth planes in this case).",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "Note from  Line#3  to  Line#5  in Algo. 4a), is to overlay the  i th   plane on the propagation result of the previous 1 st   to  ( i  − 1 ) th   planes, and then propagate to the next  ( i  +  1 ) th   plane. With these depth planes, the first step,  Forward Propagation  (de- noted  ❶ in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "Note from  Line#3  to  Line#5  in Algo. Each depth plane processes the forward-propagation from the hologram plane independently, and each pixel on a particular depth plane goes through the exact processing sequence ( HP2DP in  Line#5 ; more details can be found in [ 4 ,  18 ]). 1 that, such forward propagation is massively parallel at the depth plane level (across planes) as well as at the pixel level (within one plane).",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "This makes hard- ware parallelization and pipelining easier on a block/tensor type of architecture such as GPUs. Note, however, that, this step also requires sequential barriers within each plane ( Line#6  synchro- nizes the threads in a warp/block for one depth plane) and across planes ( Line#7  synchronizes the results from all the depth planes, before moving forward to the second step). Each depth plane processes the forward-propagation from the hologram plane independently, and each pixel on a particular depth plane goes through the exact processing sequence ( HP2DP in  Line#5 ; more details can be found in [ 4 ,  18 ]).",
      "type": "sliding_window_shuffled",
      "tokens": 157,
      "augmented": true
    },
    {
      "text": "The sec- ond step,  Backward-Propagate  (denoted  ❷ in Fig. Note, however, that, this step also requires sequential barriers within each plane ( Line#6  synchro- nizes the threads in a warp/block for one depth plane) and across planes ( Line#7  synchronizes the results from all the depth planes, before moving forward to the second step). Hence, as we will show later in this section, such barriers sliced into the massive parallel execution can cause load imbalance and instruction stalls, which slow down the entire execution and impact performance.",
      "type": "sliding_window_shuffled",
      "tokens": 135,
      "augmented": true
    },
    {
      "text": "The sec- ond step,  Backward-Propagate  (denoted  ❷ in Fig. 4a), accumulates the results of each depth plane, backpropagates it to the hologram plane via the  DP2HP  procedure (in  Line#11 ), and generates the final hologram for this depthmap input. Like the first step, this step also involves synchronizations between planes (in  Line#12 ), which can again impact parallelization and slow down the entire execution.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "Like the first step, this step also involves synchronizations between planes (in  Line#12 ), which can again impact parallelization and slow down the entire execution. Intuitively, the execution performance is mainly determined by the number of depth planes (the outer for-loop in the algorithm) as well as the number of pixels in each depth plane (the inner for-loop in the algorithm). To study how the number of depth planes affects the hologram performance, we profile the execution latency from a typical edge GPU device [ 36 ], generating holograms with different number of depth planes (assuming the same number of pixels in each plane), and the results are plotted in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "From this figure, one can observe the following: First, in general, these two steps take similar times to execute, due to the similar procedures they \nAlgorithm 1:  Depthmap Hologram Algorithm [4, 18]. To study how the number of depth planes affects the hologram performance, we profile the execution latency from a typical edge GPU device [ 36 ], generating holograms with different number of depth planes (assuming the same number of pixels in each plane), and the results are plotted in Fig. 4b.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "From this figure, one can observe the following: First, in general, these two steps take similar times to execute, due to the similar procedures they \nAlgorithm 1:  Depthmap Hologram Algorithm [4, 18]. Input : M : Number of depth planes Input : DP [ i ] : Pixels in the  i th   depth plane Output: Holoдram : Generated hologram \n1  procedure  Depthmap _ Holoдram ( M ,  DP ) // main \n2 // Step-1: Forward-propagate \n3 for  i  in  [ 1 ,  M ]  do // planes in parallel \n4 for  p  in  DP[i]  do // pixels in parallel \n5 IntraPlane i  =  HP 2 DP ( i ,  p ) \n6 IntraBlockSync (IntraPlane[i]) \n7 Inter BlockSync () \n8 // Step-2: Backward-propagate \n9 for  i  in  [ 1 ,  M ]  do // in parallel \n10 for  p ′   in  IntraPlane[i]  do // in parallel \n11 Hologram[ p ′ ] +=  DP 2 HP ( i ,  p ′ ) \n12 Inter BlockSync () \n13 return  { Holoдram } \nemploy, as shown in Algo. 1.",
      "type": "sliding_window_shuffled",
      "tokens": 302,
      "augmented": true
    },
    {
      "text": "1. Second, by increasing the number of depth planes, it takes around 2 ×  latency to generate a hologram with 2 ×  number of depth planes. As also mentioned in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Thus, it can be concluded that, without any optimization, a state- of-the-art edge GPU is only able to compute for  <  4 depth planes in real-time [ 36 ]. 2.1, the 16 depth planes required by most of the AR applications (typically 10 to 100 depth planes are sufficient) [ 19 ,  49 ] consume more than 300 ms , which is 10 ×  larger than the real-time (QoS) requirement. As also mentioned in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "These observations motivate us to investigate the reasons behind such low performance on GPU: is it because of the intrinsic software/algorithm characteristics, or is it primarily a hardware mapping issue? Thus, it can be concluded that, without any optimization, a state- of-the-art edge GPU is only able to compute for  <  4 depth planes in real-time [ 36 ]. Towards this, we profiled the hologram processing on the edge GPU [ 36 ] using the NVPROF tool [ 37 ], and observed the follow- ing: First, the SM utilization for both the steps is very high, i.e., 74% for  Forward-Propagation  and 90% for  Backward-Propagation .",
      "type": "sliding_window_shuffled",
      "tokens": 164,
      "augmented": true
    },
    {
      "text": "Moreover, the L1 hit rate for both these steps is as high as 99%. Towards this, we profiled the hologram processing on the edge GPU [ 36 ] using the NVPROF tool [ 37 ], and observed the follow- ing: First, the SM utilization for both the steps is very high, i.e., 74% for  Forward-Propagation  and 90% for  Backward-Propagation . This is because the execution is massively parallel at the depth plane level as well as at the pixel level.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "Second, the four major reasons for instruction stalls in the  Forward-Propagation  step are: Data Request (21%), Execu- tion Dependency (19%), Instruction Fetch (15%), and Sync (10%), whereas in the  Backward-Propagation  step they are Read-only Loads (42%), Sync (24%), Data Request (16%), and Execution Dependency (6%). Moreover, the L1 hit rate for both these steps is as high as 99%. Thus, GPU seems to be one of the reasonable hardware candidates for mapping the hologram application.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "1. These stalls originate mainly from the inter-block and intra- block synchronizations required by the application, as discussed above when explaining Algo. Second, the four major reasons for instruction stalls in the  Forward-Propagation  step are: Data Request (21%), Execu- tion Dependency (19%), Instruction Fetch (15%), and Sync (10%), whereas in the  Backward-Propagation  step they are Read-only Loads (42%), Sync (24%), Data Request (16%), and Execution Dependency (6%).",
      "type": "sliding_window_shuffled",
      "tokens": 126,
      "augmented": true
    },
    {
      "text": "While such an approach improved the computational efficiency and reduced power consumption to some extent, rethinking the design of hologram software/hardware con- sidering the unique features of the AR holographic applications (as discussed in Sec. Because of this, recently, alternate hardware-based solutions have been proposed to improve the com- putational efficiency by replacing the expensive transcendental cal- culations with lookup table (LUT) based memoization [ 35 ], or miti- gating the data movement overheads by employing a customized buffer on-chip [ 32 ], or simply offloading computations to cloud then streaming back [ 16 ,  27 ,  67 ]. 1.",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "498 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 2.2) as well as the characteristics of the underlying hardware can potentially open up further opportunities. While such an approach improved the computational efficiency and reduced power consumption to some extent, rethinking the design of hologram software/hardware con- sidering the unique features of the AR holographic applications (as discussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "Motivated by this, we next explore the entire design space for the AR holographic applications running on edge GPUs, and try to ex- ploit potential opportunities for reducing computations to improve both performance and energy efficiency in hologram processing. 498 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 4 PROPOSED STRATEGIES \nAs discussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Further, we also observed in Sec. 2.2, holographic processing dominates the la- tency and energy consumption in the AR video pipeline. 4 PROPOSED STRATEGIES \nAs discussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "Further, we also observed in Sec. 3 that, the main reason behind this is that the number of depth planes affects the number of synchronizations between parallel executions, and determines the amount of com- putation required to generate the holograms. Unlike prior works targeting at optimizing the efficiency of the hologram program- ming itself by proposing alternative hardware [ 32 ,  35 ], we primarily focus on exploring the intrinsic approximation opportunities (dis- cussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "2.2.3) ignored in the current implementation of the AR applications, but can be embedded into the existing hardware such as GPUs, to speedup the holographic execution and improve power/energy efficiency with negligible quality loss. 4.1 Exploring the Entire Design Space in AR Hologram Processing \nExploring the entire design space for the AR hologram processing is a non-trivial task. Unlike prior works targeting at optimizing the efficiency of the hologram program- ming itself by proposing alternative hardware [ 32 ,  35 ], we primarily focus on exploring the intrinsic approximation opportunities (dis- cussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 142,
      "augmented": true
    },
    {
      "text": "First of all, a large number of sensor inputs are fed into the hologram pipeline (as shown in Fig. 4.1 Exploring the Entire Design Space in AR Hologram Processing \nExploring the entire design space for the AR hologram processing is a non-trivial task. 1b), such as IMU sensors, eye tracking or IR sensors, hand motion sensors, RGB-D im- age sensors, etc.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "As discussed above in Sec. 1b), such as IMU sensors, eye tracking or IR sensors, hand motion sensors, RGB-D im- age sensors, etc. To improve the hologram approximation, we need to first identify the set of inputs that affect the hologram computing the most.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "As discussed above in Sec. Further, in many cases, these inputs are dynamically changing at the same frequency (e.g., the image sensors) as the frame-rate, which needs to be captured and updated at runtime, or even at a faster rate (e.g., the IMU and IR sensors). 3, both the user’s pose and the gaze position, as well as the targeted objects (intended to be re- placed by the virtual holograms) shape the hologram computation.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Thus, to systematically explore the potential opportunities of approximation in the AR hologram applications, we start by distinguishing between three fundamental scenarios, where the  objects ,  head pose , and  eye tracking  provide different opportunities, as depicted in Fig. Further, in many cases, these inputs are dynamically changing at the same frequency (e.g., the image sensors) as the frame-rate, which needs to be captured and updated at runtime, or even at a faster rate (e.g., the IMU and IR sensors). 5.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "5a, only the soccer ball  object is located inside the viewing window in the current frame,  Frame-I , while  football  and  box  are not. 5. •  In the  Viewing-Window  scenario shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "5a, only the soccer ball  object is located inside the viewing window in the current frame,  Frame-I , while  football  and  box  are not. Thus, only the  soccer ball  hologram is required to be com- puted for this frame, and other two can be skipped. Similarly, for the next frame,  Frame-II , now the user lifts her head a bit, hence the corresponding viewing window changes from the previous one.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Similarly, for the next frame,  Frame-II , now the user lifts her head a bit, hence the corresponding viewing window changes from the previous one. Because of this, now the  football  is partially located in the viewing window, and requires computing (only for the bottom right part that is inside the viewing window). Note also that, since the  soccer ball  hologram has been already generated in Frame-I , we can skip its computation.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "Again, the  box  object is still outside of the viewing window and thus, we do not need to compute its hologram. We use such a viewing-window based “sub-hologram” technique which has already been proposed in prior works (such as Sub-Hologram [ 52 ]) as the  Baseline  design. Note also that, since the  soccer ball  hologram has been already generated in Frame-I , we can skip its computation.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "1 and Sec. •  Apart from the viewing window, the dense or sparse hologram computing is also RoF-dependent, which is the main idea be- hind foveated rendering [ 25 ,  47 ,  62 ], as discussed in Sec. We use such a viewing-window based “sub-hologram” technique which has already been proposed in prior works (such as Sub-Hologram [ 52 ]) as the  Baseline  design.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "In the  Inter-Holo  scenario shown in Fig. 2.2.3. 1 and Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 21,
      "augmented": true
    },
    {
      "text": "5b, such a region of focus is just a subset of the entire viewing window, and thus contains a small number of objects that need to be computed with rich information (as it needs 16 depth planes). However, for the objects outside of the current RoF, since the user is not cur- rently focusing on them, a reasonable approximation would not affect the user experience that much (which implies we do not need 16 depth planes for all of them). In the  Inter-Holo  scenario shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "For example, in  Frame-I  in Fig. 5b, the user is currently focusing on the  soccer ball ; mean- while the  football  is located outside of the RoF, hence, becomes a candidate for approximation. However, for the objects outside of the current RoF, since the user is not cur- rently focusing on them, a reasonable approximation would not affect the user experience that much (which implies we do not need 16 depth planes for all of them).",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "5b, the user is currently focusing on the  soccer ball ; mean- while the  football  is located outside of the RoF, hence, becomes a candidate for approximation. Now, the  football  needs the full depth planes’ information, while the soccer ball  can be approximated. On the other hand, in  Frame-II , the user moves her eyes and changes the region of focus.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "2 and Sec. Now, the  football  needs the full depth planes’ information, while the soccer ball  can be approximated. To take advantage of this opportunity, the  football  object (which is inside the RoF in this example scenario) requires all of the 16 depth planes to compute its dense hologram, whereas the other objects (the  soccer ball in this case) can be approximated with a pre-defined sparse sam- pling factor (e.g.,   1 \n2 ; more details provided later in Algo.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "4.3). We leverage such foveated rendering idea in  Inter-Holo as our  Reference  design in this paper. 2 and Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 30,
      "augmented": true
    },
    {
      "text": "We leverage such foveated rendering idea in  Inter-Holo as our  Reference  design in this paper. •  The above  Viewing-Window  and  Inter-Holo  proposals target at reducing the amount of computation for the holograms from the head orientation ( rotation ) and eye tracking ( up-down ) perspec- tives. As discussed in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "As discussed in Fig. As one can observe from the  Intra- Holo  scenario shown in Fig. 3a, another enabler for computation reduction is the relative distance between the camera/user and the objects, i.e.,  left-right .",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "5c, the  football  is larger than the soccer ball , and is located much closer to the user. As one can observe from the  Intra- Holo  scenario shown in Fig. Hence, even though both of them are located inside the RoF (and, of course, inside the viewing window), intuitively, the  soccer ball hologram does not need as much information as the  football hologram to compute.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "4.2 HoloAR Overview \nDriven by the above discussion and the potential approximation opportunities presented by the  Inter-Holo  and  Intra-Holo  scenarios, we propose  HoloAR , a novel framework for holographic process- ing in AR applications to improve  both  the performance and en- ergy consumption of the hologram processing, without affecting user experience. Inspired by this observation, another level of approximation can be explored based on the relative camera- to-object distance as well as the object range/size. Hence, even though both of them are located inside the RoF (and, of course, inside the viewing window), intuitively, the  soccer ball hologram does not need as much information as the  football hologram to compute.",
      "type": "sliding_window_shuffled",
      "tokens": 171,
      "augmented": true
    },
    {
      "text": "The overall design of our proposed  HoloAR framework is illustrated in Fig. 4.2 HoloAR Overview \nDriven by the above discussion and the potential approximation opportunities presented by the  Inter-Holo  and  Intra-Holo  scenarios, we propose  HoloAR , a novel framework for holographic process- ing in AR applications to improve  both  the performance and en- ergy consumption of the hologram processing, without affecting user experience. HoloAR  aims to reduce the amount of hologram computations as much as possible by carefully approximating the hologram computing for select objects, while maintaining an ac- ceptable video quality.",
      "type": "sliding_window_shuffled",
      "tokens": 150,
      "augmented": true
    },
    {
      "text": "The overall design of our proposed  HoloAR framework is illustrated in Fig. First,  HoloAR  utilizes the exist- ing viewing-window based technique [52] (denoted \na  ) to skip the hologram computations for the objects which are outside of the current viewing window, in a “just-in-time” fashion. 6a.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "Next,  HoloAR employs the  Inter-Holo  scheme (denoted  b  ), to take advantage of the region of focus from analyzing the current eye tracking inputs and sparsely compute the objects outside the RoF. First,  HoloAR  utilizes the exist- ing viewing-window based technique [52] (denoted \na  ) to skip the hologram computations for the objects which are outside of the current viewing window, in a “just-in-time” fashion. Finally,  HoloAR \n499 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n(a) Viewing-Window scenario [52].",
      "type": "sliding_window_shuffled",
      "tokens": 174,
      "augmented": true
    },
    {
      "text": "Finally,  HoloAR \n499 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n(a) Viewing-Window scenario [52]. (b) Inter-Holo scenario. (c) Intra-Holo scenario.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "(a) HoloAR overview. (c) Intra-Holo scenario. Figure 5: Three opportunities for reducing hologram computation in an AR application.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "(b) The  Inter-Holo  and  Intra-Holo . Figure 6: The proposed  HoloAR  which includes  Inter-Holo leveraging foveated rendering, and  Intr-Holo  further ap- proximating holograms for far objects. (a) HoloAR overview.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "uses the  Intra-Holo  scheme (denoted  c  ), to identify the number of depth planes required for a particular object by analyzing the rela- tive camera-to-object distance as well as the shape/size of the target object. Figure 6: The proposed  HoloAR  which includes  Inter-Holo leveraging foveated rendering, and  Intr-Holo  further ap- proximating holograms for far objects. Note that, both the  Inter-Holo  and the  Intra-Holo  schemes are complementary to each other, when both the eye tracking and pose estimation inputs are available at the same time.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "Therefore, we also investigate a combined  Inter-Intra-Holo  scheme which com- bines both the schemes to further reduce the amount of hologram computations. Note that, both the  Inter-Holo  and the  Intra-Holo  schemes are complementary to each other, when both the eye tracking and pose estimation inputs are available at the same time. We would like to emphasize that the proposed  HoloAR  framework can, in principle, work with any hardware platform.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "5, in this paper, we evaluate the performance and energy benefits of  HoloAR  by using an embedded GPU prototype for the edge AR headsets [ 36 ], and leave the hardware-software co- design based on FPGA-based acceleration for future work. We would like to emphasize that the proposed  HoloAR  framework can, in principle, work with any hardware platform. As discussed later in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "However, the architectural insights on how to co-design a next-generation accelerator that can accommodate our proposed  HoloAR  framework are discussed later in Sec. 5.5. 5, in this paper, we evaluate the performance and energy benefits of  HoloAR  by using an embedded GPU prototype for the edge AR headsets [ 36 ], and leave the hardware-software co- design based on FPGA-based acceleration for future work.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "5.5. 4.3 Inter-Holo Computation Optimization \nWe first answer how to deploy the previously proposed foveated rendering technique on AR holograms, by investigating how to leverage the temporal similarity when the user’s region of focus is only a part of the entire viewing window, as mentioned earlier in Sec. 2.2 (Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "2.2 (Fig. To capture the current RoF, an additional eye track- ing step is introduced before the hologram computations, as shown in Fig. 3b).",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "To capture the current RoF, an additional eye track- ing step is introduced before the hologram computations, as shown in Fig. This eye tracking step takes the current IR sensor im- ages as its input, and analyzes the user’s current gaze area as well as \nAlgorithm 2:  Inter-Holo algorithm. 6b \na  .",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "This eye tracking step takes the current IR sensor im- ages as its input, and analyzes the user’s current gaze area as well as \nAlgorithm 2:  Inter-Holo algorithm. Note that this additional eye tracking proce- dure needs to be invoked for each frame, in order to capture/reflect the current eye movements without causing nausea for the user. Input : IRs : eye tracking sensors Input : Objs : set of virtual objects Input : α : inter-holo approximation factor,  α  ∈( 0 ,  1 ] Output: Holoдrams : Generated holograms \n1  procedure  Inter _ Holo ( IRs ,  Objs ,  α ) // main \n2 RoF =  EyeT rackinд ( IRs ) \n3 for  obj  in  Objs  do // View-Window only \n4 if  obj  in  RoF  then // inside of RoF \n5 Holograms[obj] = Algorithm1( 16 , obj) \n6 else // outside of RoF, thus approximate \n7 Holograms[obj] = Algorithm1( 16  ×  α , obj) \n8 return  { Holoдrams } \nthe viewing direction.",
      "type": "sliding_window_shuffled",
      "tokens": 304,
      "augmented": true
    },
    {
      "text": "Note that this additional eye tracking proce- dure needs to be invoked for each frame, in order to capture/reflect the current eye movements without causing nausea for the user. Fortunately, there already exist a large body of techniques which can track the eye movements efficiently (e.g., see [ 26 ] and [ 12 ] and the references therein). As a result, eye tracking needs to incur minimum overhead, while providing a fairly good accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "In this work, we chose to use the NVGaze technique [ 26 ] to perform eye tracking for the  Inter-Holo  design due to two main reasons. Fortunately, there already exist a large body of techniques which can track the eye movements efficiently (e.g., see [ 26 ] and [ 12 ] and the references therein). First, it provides sufficient accuracy for the AR applications – as high as 2 .",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Second, its execution latency when running on our edge GPU prototype [ 36 ] is within 4 . First, it provides sufficient accuracy for the AR applications – as high as 2 . 06 °  accuracy for gaze shape/direction estimation across a wide field of view [ 26 ].",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Second, its execution latency when running on our edge GPU prototype [ 36 ] is within 4 . 5 ms , which contributes to less than 1% of the entire hologram processing pipeline latency. With the RoF attained from the eye tracking, the next question we need to answer is how to deploy the approximation opportuni- ties discussed above in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "2.2.3 on top of the existing hologram pipeline. As shown by  Line#5  and  Line#7  in Algo. With the RoF attained from the eye tracking, the next question we need to answer is how to deploy the approximation opportuni- ties discussed above in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "In fact, only one input argument, i.e., the number of depth planes, requires to be changed based on the approximation factor  α , when the object is outside of RoF. As shown by  Line#5  and  Line#7  in Algo. 2, our proposal can actually reuse the original hologram execution engine without any architectural modifications or reprogramming.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "In fact, only one input argument, i.e., the number of depth planes, requires to be changed based on the approximation factor  α , when the object is outside of RoF. Here, we set  α  to 0 . 5, as our detailed profiling (discussed later in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "5) indicates that setting  α  to this specific value brings significant energy savings while maintaining good hologram quality. We also present a sensitivity study on how en- ergy savings and performance vary with different approximation factors in Sec. 5, as our detailed profiling (discussed later in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "500 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 5.4. We also present a sensitivity study on how en- ergy savings and performance vary with different approximation factors in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "Input : Poses : pose sensors Input : Objs : set of virtual objects Output: Holoдrams : Generated holograms \n1  procedure  Intra _ Holo ( Poses ,  Objs ) // main \n2 Cam2ObjDists =  PoseEstimation ( Poses ) \n3 for  obj  in  Objs  do // approx. 500 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 4.4 Intra-Holo Computation Optimization \nAlgorithm 3:  Intra-Holo proposal algorithm.",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "Input : Poses : pose sensors Input : Objs : set of virtual objects Output: Holoдrams : Generated holograms \n1  procedure  Intra _ Holo ( Poses ,  Objs ) // main \n2 Cam2ObjDists =  PoseEstimation ( Poses ) \n3 for  obj  in  Objs  do // approx. 4 β  =  approxFactors (cam2ObjDists[obj]) \n5 Holograms[obj] = Algorithm1( 16  ×  β , obj) \n6 return  { Holoдrams } \nIn the  Inter-Holo  design, the hologram computation can be ap- proximated by identifying the region of focus from eye tracking. based on dist.",
      "type": "sliding_window_shuffled",
      "tokens": 200,
      "augmented": true
    },
    {
      "text": "4 β  =  approxFactors (cam2ObjDists[obj]) \n5 Holograms[obj] = Algorithm1( 16  ×  β , obj) \n6 return  { Holoдrams } \nIn the  Inter-Holo  design, the hologram computation can be ap- proximated by identifying the region of focus from eye tracking. However, the scope of this approximation opportunity might be limited due to the strict 16 depth planes requirement for all objects inside the RoF, regardless of their distance from the user. In fact, there may still be another level of opportunity for approximating the objects in long distance ( Intra-Holo , shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 174,
      "augmented": true
    },
    {
      "text": "5c). In fact, there may still be another level of opportunity for approximating the objects in long distance ( Intra-Holo , shown in Fig. To lever- age this opportunity, we need to know where the user is located in the world and what the objects in the world look like [ 13 ,  19 ,  53 ,  59 ].",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "Next, we use one of the popular SLAM techniques, Kimera-VIO [ 53 ], to estimate the user’s pose and understand the relative positions of the objects and the user. As shown in Fig. To lever- age this opportunity, we need to know where the user is located in the world and what the objects in the world look like [ 13 ,  19 ,  53 ,  59 ].",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "As shown in Fig. 6b  b  , similar to the  Inter- Holo  pipeline, the additional pose estimation step also sits between the inputs and the original hologram processing, and thus has to be efficient without introducing much overhead. Our profiling on the edge GPU prototype [ 36 ] shows that Kimera-VIO takes, on av- erage, 13 .",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "75 ms  latency to execute, which is less than 1% of the total hologram processing time. Therefore, the overhead introduced due to the additional pose estimation step is negligible compared to the baseline latency, thereby opening up opportunities for significant energy savings and performance speedup as demonstrated later in Sec. Our profiling on the edge GPU prototype [ 36 ] shows that Kimera-VIO takes, on av- erage, 13 .",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "5. With the help of the pose estimation, now the AR hologram pipeline has the knowledge about the range/size of each object as well as its relative distance from the user. Therefore, the overhead introduced due to the additional pose estimation step is negligible compared to the baseline latency, thereby opening up opportunities for significant energy savings and performance speedup as demonstrated later in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "3, for each of the objects, a corresponding approximation factor ( β ) can be determined based on these insights. With the help of the pose estimation, now the AR hologram pipeline has the knowledge about the range/size of each object as well as its relative distance from the user. Next, as shown in Algo.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "Similarly, the original hologram engine can still be reused without any reprogramming, except for the first argument, i.e., the number of depth planes for this particular object, as shown in  Line#5  of Algo. 3. 3, for each of the objects, a corresponding approximation factor ( β ) can be determined based on these insights.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Inter-Intra-Holo:  It is to be noted that, when the user eye track- ing and pose estimation are available simultaneously for hologram processing, the  Inter-Holo  and  Intra-Holo  schemes can be both ap- plied to achieve maximum amount of energy savings and perfor- mance benefits. In this paper, we refer to this combined scheme as  Inter-Intra-Holo . 3.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "In this paper, we refer to this combined scheme as  Inter-Intra-Holo . In this scheme, we first identify the objects in- side/outside the RoF ( Inter-Holo ), and then approximate each of them based on its shape and distance ( Intra-Holo ). Note that since the other option – first  Intra-Holo , then  Inter-Holo  – is theoreti- cally identical to the proposed  Inter-Intra-Holo , we skip its detailed discussion due to space limitation.",
      "type": "sliding_window_shuffled",
      "tokens": 126,
      "augmented": true
    },
    {
      "text": "Our proposal is fundamentally different from prior optimizations targeting various architectures or execution environments, such as customized hardware accelerators [ 35 ], cloud assistance [ 16 ,  27 ,  67 ], or neural network training/inferencing [ 33 ,  54 ]. Note that since the other option – first  Intra-Holo , then  Inter-Holo  – is theoreti- cally identical to the proposed  Inter-Intra-Holo , we skip its detailed discussion due to space limitation. 4.5 Design and Implementation \nOptimization Choices:  Our main goal in this paper is to reduce the amount of hologram computation by appropriate approxima- tion, in order to speed up hologram processing, to satisfy the real- time requirement as well as to reduce the energy consumption and prolong the battery life of the AR device, while maintaining the QoS.",
      "type": "sliding_window_shuffled",
      "tokens": 192,
      "augmented": true
    },
    {
      "text": "Our proposal is fundamentally different from prior optimizations targeting various architectures or execution environments, such as customized hardware accelerators [ 35 ], cloud assistance [ 16 ,  27 ,  67 ], or neural network training/inferencing [ 33 ,  54 ]. Thus, our proposal does not rely on any assistance from hardware accelerators, cloud platforms, or neural networks. Note that, each of these prior efforts has its own limitations, e.g., expensive in-house implementation and fixed functionality without proper power gat- ing in accelerators [ 35 ]; requiring reliable network connections and expensive round-trip latency in cloud offloading [ 16 ,  27 ,  67 ]; and re-training of a new model for each application scenario and poten- tially for each user in neural networks [ 33 ,  54 ].",
      "type": "sliding_window_shuffled",
      "tokens": 187,
      "augmented": true
    },
    {
      "text": "Instead, we focus exclusively on a typical edge GPU to execute the hologram, and present our three techniques, namely,  Inter-Holo  (as  Reference ),  Intra-Holo  and  Inter- Intra-Holo , which capture various approximation opportunities in the AR hologram applications to improve both performance and energy efficiency. Framework Prototype:  To prototype a real-life AR headset, a proper codebase and a hardware platform are essential. Thus, our proposal does not rely on any assistance from hardware accelerators, cloud platforms, or neural networks.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "IL- LIXR already contains several AR software components (some of them are shown in Fig. Framework Prototype:  To prototype a real-life AR headset, a proper codebase and a hardware platform are essential. For our codebase, we build our proposals on top of ILLIXR [ 19 ], which is the first open-source full-system extended reality testbed.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "1c), including head tracking, IMU integra- tion, reprojection, and sound processing. On top of the ILLIXR codebase, we implemented three new components – eye tracking, pose estimation, and hologram processing. IL- LIXR already contains several AR software components (some of them are shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "Further, we mapped these AR software components to an edge GPU prototype [ 36 ], from which the power breakdown across different components such as SoC, memory, CPU, and GPU are measured through the on-board Texas Instruments INA 3221 voltage monitor IC hard- ware, and the performance of execution status is sampled by the Nvidia NVPROF [ 37 ] profiling tool, which enables the collection of a timeline of CUDA-related activities on both the CPU and GPU, including kernel execution, memory transfer, CUDA API calls and events/metrics for CUDA kernels. 5 EVALUATION We evaluate our proposed  HoloAR  design by comparing the execu- tion latency and total energy consumption with four different AR hologram setups. On top of the ILLIXR codebase, we implemented three new components – eye tracking, pose estimation, and hologram processing.",
      "type": "sliding_window_shuffled",
      "tokens": 204,
      "augmented": true
    },
    {
      "text": "5 EVALUATION We evaluate our proposed  HoloAR  design by comparing the execu- tion latency and total energy consumption with four different AR hologram setups. In this section, we first describe our evaluation methodology, experimental platform, datasets, and measurement tools. Next, we analyze the results measured using these platforms.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "Next, we analyze the results measured using these platforms. We conclude this section by outlining some research directions for implementing approximation-based accelerators for AR holograms. After that, we show the general applicability of the proposed de- sign, and also present results from a sensitivity study that focuses on the quality-loss vs. energy-savings trade-offs.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "No. 501 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \nTable 2: Salient features of the six videos used in this study. We conclude this section by outlining some research directions for implementing approximation-based accelerators for AR holograms.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "This software-based viewing window optimization is considered to be the state-of-the-art at an algorithm level, and we refer to it as Baseline  in this study. Video #Frames #Obj/Frame Distance ObjSize 1 bike[38] 150k 1.1 2.08m 1.54m 2 book[39] 576k 1.5 0.64m 0.28m 3 bottle[40] 476k 1.1 0.47m 0.22m 4 cup[41] 546k 1.6 0.47m 0.16m 5 laptop[42] 485k 1.3 0.58m 0.38m 6 shoe[43] 557k 2.3 0.65m 0.21m \n5.1 AR Hologram Configurations \nWe evaluate the following five configurations of AR holo- gram processing to demonstrate the effectiveness of our proposed HoloAR : •  Baseline (Viewing-Window):  Similar to the recent viewing- window based sub-hologram optimization [ 52 ], we first obtain the field of view or the current viewing window from the user’s head orientation, and then skip the computations of the objects, which are outside the viewing window (i.e., only compute for the objects located inside) to save computations and energy. No.",
      "type": "sliding_window_shuffled",
      "tokens": 273,
      "augmented": true
    },
    {
      "text": "We evaluate this baseline by profiling its performance and energy consumption from a mobile GPU [36]. This software-based viewing window optimization is considered to be the state-of-the-art at an algorithm level, and we refer to it as Baseline  in this study. •  Inter-Holo:  We evaluate the  Inter-Holo  design on a mobile GPU [ 36 ] using a framework similar to the state-of-the-art IL- LIXR framework [ 19 ], with one additional eye tracking task (as shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "6b \na  ) integrated into the existing pipeline to par- tially bypass the computations of holograms that are outside the focus area. •  Inter-Holo:  We evaluate the  Inter-Holo  design on a mobile GPU [ 36 ] using a framework similar to the state-of-the-art IL- LIXR framework [ 19 ], with one additional eye tracking task (as shown in Fig. Note that, this implementation is purely done in software, without any hardware modification.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "•  Intra-Holo:  We evaluate our  Intra-Holo  design again on a mobile GPU as shown in Fig. Note that, this implementation is purely done in software, without any hardware modification. 6b  b  .",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "This approach tries to reduce the amount of hologram computation by approximating each of the holograms based on the distance between the user and the object. •  Inter-Intra-Holo : The above two designs can be integrated to- gether into the original hologram pipeline, in either Inter-then- Intra or Intra-then-Inter fashion. 6b  b  .",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "•  Inter-Intra-Holo : The above two designs can be integrated to- gether into the original hologram pipeline, in either Inter-then- Intra or Intra-then-Inter fashion. In this paper, we chose the first one and denote this design as  Inter-Intra-Holo . •  HORN-8:  While hardware acceleration of hologram is not a goal of this work, to qualitatively compare our GPU-based design with hardware specific accelerators, we also discuss one of the most recent ASIC implementations, HORN-8 [ 35 ].",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "With this estimation, we briefly discuss the performance and computational efficiency variations between this accelerator and our approach, and discuss takeaways that can help one to co-design a hardware accelerator targeting hologram processing. •  HORN-8:  While hardware acceleration of hologram is not a goal of this work, to qualitatively compare our GPU-based design with hardware specific accelerators, we also discuss one of the most recent ASIC implementations, HORN-8 [ 35 ]. Due to unavailabil- ity of its hardware implementation or datasheet, we estimate its power efficiency compared to the equivalent GPU SoC based on a published data [ 51 ].",
      "type": "sliding_window_shuffled",
      "tokens": 143,
      "augmented": true
    },
    {
      "text": "To ensemble the \nAR pipeline with generic state-of-the-art components shown in Fig. 5.2 Experimental Platform and Datasets \nThe edge GPU platform used in this work consists of a 512- core Volta GPU, a 4Kp60 HEVC codec, 16GB LPDDR4x memory, 32GB eMMC storage, and a power management unit (PMU) that exposes the real-time power traces to users [ 36 ]. With this estimation, we briefly discuss the performance and computational efficiency variations between this accelerator and our approach, and discuss takeaways that can help one to co-design a hardware accelerator targeting hologram processing.",
      "type": "sliding_window_shuffled",
      "tokens": 149,
      "augmented": true
    },
    {
      "text": "To ensemble the \nAR pipeline with generic state-of-the-art components shown in Fig. To collect performance metrics such as the streaming multiprocessor (SM) utilization, memory traffic, and CUDA kernel execution latency, we utilized the open-source Nvidia NVPROF tool [37] on the GPU platform. 1c, we implemented an open-source full-system extended real- ity testbed, ILLIXR [ 19 ], on the edge GPU platform [ 36 ], and built our  HoloAR  design on top of it.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "We use the published short object-centric Objectron [ 1 ] video dataset, which is accompanied by AR session metadata such as camera poses, as well as the object annotations such as position, orientation and dimension for nine categories of object videos 4 . The salient characteristics these videos are given in Tab. To collect performance metrics such as the streaming multiprocessor (SM) utilization, memory traffic, and CUDA kernel execution latency, we utilized the open-source Nvidia NVPROF tool [37] on the GPU platform.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "2. To re- place these real objects, we choose six virtual holograms (Sniper, Rock, Tree, Planet, Rabbit, and Dice holograms) from the Open- Holo depthmap database [ 45 ]. The salient characteristics these videos are given in Tab.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "Note that the real-object and the corresponding virtual-hologram are randomly mapped because, theoretically, different mappings have no impact on the perfor- mance speedup and energy saving results (shown in Sec. 5.3). To re- place these real objects, we choose six virtual holograms (Sniper, Rock, Tree, Planet, Rabbit, and Dice holograms) from the Open- Holo depthmap database [ 45 ].",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "7, when processing the six videos listed in Tab. 5.3 Experimental Results We present the power and energy consumption, as well as the execution latency of the hologram computation in Fig. 5.3).",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "7, when processing the six videos listed in Tab. 5.1. 2, with the first four config- urations described earlier in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 30,
      "augmented": true
    },
    {
      "text": "5.1. 5.4. We discuss the impact of our proposal on output/result quality, compared to the baseline design, later in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "Power Consumption:  Overall, the  Inter-Holo  scheme consumes around 4 . 5.4. 24 Watts , on average, when running on the edge GPU, which translates to 3 .",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "86% power reduction, compared to the baseline. In addition, our  Intra-Holo  scheme is more power efficient than  Inter- Holo , translating to 27 . 24 Watts , on average, when running on the edge GPU, which translates to 3 .",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "In addition, our  Intra-Holo  scheme is more power efficient than  Inter- Holo , translating to 27 . 72% power reduction with respect to the baseline. This indicates that the optimization scope of the distance- based  Intra-Holo  is larger than that of the RoF-based  Inter-Holo , which provides more sparsity in the hologram computing.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Finally, combination of the two schemes ( Inter-Intra-Holo ), results in 28 . This indicates that the optimization scope of the distance- based  Intra-Holo  is larger than that of the RoF-based  Inter-Holo , which provides more sparsity in the hologram computing. 95% power reduction compared to the baseline.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "To better explain where the power benefits come from, we further breakdown the power consumption for the hologram processing into four parts: CPU (to handle sensor inputs, scheduling, kernel launch, etc. 95% power reduction compared to the baseline. ), GPU (to execute hologram), Mem (for data accesses), and the SoC (the remaining hardware components, e.g., codec, network), with different number of depth planes (ranging from 2 to 16), as shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "), GPU (to execute hologram), Mem (for data accesses), and the SoC (the remaining hardware components, e.g., codec, network), with different number of depth planes (ranging from 2 to 16), as shown in Fig. One can observe from this figure that, when the number of depth planes is increased, the power consumptions of  SoC  and  CPU  do not change much, while, in contrast, both the GPU  and  Mem  consume more power. 8a.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "This is due to the fact that, to process a denser hologram with more depth planes, additional GPU cores are scheduled to launch the per-plane CUDA kernel (as discussed in Algo. 1) with more holographic data accesses (fetched from the host-side memory). One can observe from this figure that, when the number of depth planes is increased, the power consumptions of  SoC  and  CPU  do not change much, while, in contrast, both the GPU  and  Mem  consume more power.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "We next quantify how many depth planes can be reduced by our approximation scheme. Towards this, we plot, in Fig. 1) with more holographic data accesses (fetched from the host-side memory).",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "Towards this, we plot, in Fig. 8b, the average number of depth planes (across our six videos), required by the four design alternatives (configurations). We see that, the number \n4 Due to space limitation, we chose six representative categories that cover diversity across multiple video parameters.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "502 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al. 4413.87 4243.51 3190.25 3135.99 \n0 1000 2000 3000 4000 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. We see that, the number \n4 Due to space limitation, we chose six representative categories that cover diversity across multiple video parameters.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. 4413.87 4243.51 3190.25 3135.99 \n0 1000 2000 3000 4000 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. baseline InterHolo IntraHolo InterIntraHolo \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "power (mW). baseline InterHolo IntraHolo InterIntraHolo \nAvg. Power (mW) \n(a) Avg.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "1006.01 \n0 500 \n1000 1500 2000 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. power (mW).",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. baseline InterHolo IntraHolo InterIntraHolo \nExec. Latency (ms) \nHoloCompute Overhead \n876.81 \n430.15 393.07 \n(b) Exec.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "4.48 \n0 2 4 6 8 10 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. latency (ms). Latency (ms) \nHoloCompute Overhead \n876.81 \n430.15 393.07 \n(b) Exec.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. 4.48 \n0 2 4 6 8 10 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "baseline InterHolo IntraHolo InterIntraHolo \nEnergy (J) \nHoloCompute Overhead \n3.68 \n1.40 1.28 \n(c) Energy consumption (J). bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "baseline InterHolo IntraHolo InterIntraHolo \nEnergy (J) \nHoloCompute Overhead \n3.68 \n1.40 1.28 \n(c) Energy consumption (J). Figure 7: (a) Average power consumption, (b) execution latency, and (c) energy consumption with different configurations and video inputs. 0 \n2000 \n4000 \n6000 \n2 4 6 8 10 12 14 16 \nPower (mW) \n# Depth-planes \nCPU SoC GPU Mem \n(a) Power breakdown.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "23.6 \n19.8 \n7.1 6.7 \n0 10 20 30 40 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. 0 \n2000 \n4000 \n6000 \n2 4 6 8 10 12 14 16 \nPower (mW) \n# Depth-planes \nCPU SoC GPU Mem \n(a) Power breakdown. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. #depthPlanes \n(b) Avg. baseline InterHolo IntraHolo InterIntraHolo \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "number of depth planes. #depthPlanes \n(b) Avg. Figure 8: (a): Profiling the power breakdown on the edge GPU prototype [36]; and (b): Average number of depth planes required for four design configurations.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "of depth planes required by the  Inter-Holo  scheme is reduced from 23 . Figure 8: (a): Profiling the power breakdown on the edge GPU prototype [36]; and (b): Average number of depth planes required for four design configurations. 6 to 19 .",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "7, by the  Intra-Holo and  Inter-Intra-Holo  schemes, respectively. 1 and 6 . The above observations from these two figures explain the power benefits of our proposed designs.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Execution Latency:  Clearly, the reduction in the number of depth planes when using our approximation schemes can reduce the hologram execution latency as well. The above observations from these two figures explain the power benefits of our proposed designs. As shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "As shown in Fig. 7b, overall, the  Inter-Holo  scheme provides a 1 . 15 ×  speedup compared to the baseline.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "15 ×  speedup compared to the baseline. Further, a 2 . 42 ×  speedup is achieved when employing Intra-Holo  (with only 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "42 ×  speedup is achieved when employing Intra-Holo  (with only 0 . 44% overhead), and 2 . 68 ×  when employ- ing  Inter-Intra-Holo  (with only 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Recall that the number of depth planes for each hologram object affects the ex- ecution latency dramatically as shown in Fig. 68 ×  when employ- ing  Inter-Intra-Holo  (with only 0 . 14% overhead).",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "Another interest- ing observation is that,  Intra-Holo  saves more execution time than Inter-Holo . Recall that the number of depth planes for each hologram object affects the ex- ecution latency dramatically as shown in Fig. 4b; thus, these per- formance benefits come from the speedup brought by the reduced depth planes by approximation in our schemes.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "This is because the latter only approximates the objects outside of the current region of focus (still requiring full compute for the objects inside), whereas the scope of the former is much larger, i.e., including all the objects in the current viewing window and approximating each of them based on its location. Another interest- ing observation is that,  Intra-Holo  saves more execution time than Inter-Holo . In addition, from an individual video’s perspective, we further observe that the  shoe  video achieves the maximum performance benefits from our schemes (specifically, 23%, 69% and 73% latency reduction with Inter-Holo ,  Intra-Holo  and  Inter-Intra-Holo , respectively, compared to the baseline).",
      "type": "sliding_window_shuffled",
      "tokens": 172,
      "augmented": true
    },
    {
      "text": "In contrast, the  bike  video achieves the minimum speedup (4%, 34% and 36% in the same order). The reason behind this is that, as shown earlier in Tab. In addition, from an individual video’s perspective, we further observe that the  shoe  video achieves the maximum performance benefits from our schemes (specifically, 23%, 69% and 73% latency reduction with Inter-Holo ,  Intra-Holo  and  Inter-Intra-Holo , respectively, compared to the baseline).",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "The reason behind this is that, as shown earlier in Tab. 2, the  bike  video usually has only one object per frame (1 . 1 on average), and also the ranges/sizes of the bikes are larger, compared to others.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "On the other hand, the  shoe  video frames typically contain more objects (2 . Thus, chances for ap- proximating the objects outside the RoF (in  Inter-Holo ) and the objects which are relatively far-away from the user (in  Intra-Holo ) \nare limited. 1 on average), and also the ranges/sizes of the bikes are larger, compared to others.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "3 on average, as shown in Tab. 2), thereby gaining more opportunities to reduce the amount of computations for all the objects in the current frame. On the other hand, the  shoe  video frames typically contain more objects (2 .",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "Energy Savings:  The above power and latency reductions pro- vided by  HoloAR  eventually translates to energy savings for the hologram processing. As shown in Fig. 2), thereby gaining more opportunities to reduce the amount of computations for all the objects in the current frame.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "As shown in Fig. Finally, the energy saving achieved by the  Inter-Intra-Holo  scheme is about 73%, meaning that it only consumes 27% of the baseline energy. 7c, on average, the  Inter- Holo  scheme saves 18% energy compared to the baseline, and the Intra-Holo  scheme saves 70% energy.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "Due to the unavailability of the hardware RTL, we estimated its energy consumption based on the published characterization numbers from the Jetson GPU plat- form with the ZCU102 FPGA[ 64 ] (which is similar to the HORN-8 prototype) [ 51 ]. To put the energy-efficiency of our designs into perspective, we compared their energy consumption against the state-of-the-art HORN-8 hardware accelerator [ 35 ]. Finally, the energy saving achieved by the  Inter-Intra-Holo  scheme is about 73%, meaning that it only consumes 27% of the baseline energy.",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "Because of the LUT memoization and power ef- ficiency optimizations [ 35 ], HORN-8 saves around 48% power 5 . Due to the unavailability of the hardware RTL, we estimated its energy consumption based on the published characterization numbers from the Jetson GPU plat- form with the ZCU102 FPGA[ 64 ] (which is similar to the HORN-8 prototype) [ 51 ]. However, HORN-8 does not explore the approximation opportuni- ties to speedup the hologram execution.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "However, HORN-8 does not explore the approximation opportuni- ties to speedup the hologram execution. Hence, as shown in Fig. 7c, our  HoloAR  design running on the edge GPU [ 36 ] still saves 25% more energy than the custom HORN-8 accelerator.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "To study how these approximation decisions affect the hologram video quality, we next want to reconstruct/render the hologram from our design based on the real-time eye move- ments and head orientations, and compare the quality of the recon- structed images against the baseline using the peak signal-to-noise ratio (PSNR) [ 21 ,  44 ] metric. 7c, our  HoloAR  design running on the edge GPU [ 36 ] still saves 25% more energy than the custom HORN-8 accelerator. 5.4 Sensitivity Study \nImpact on Quality:  The prior  Inter-Holo  scheme captures the small-region of eye focus to approximate the hologram outside of RoF, and the proposed  Intra-Holo  takes advantage of the sparse computation required for the far objects to reduce the number of depth planes.",
      "type": "sliding_window_shuffled",
      "tokens": 193,
      "augmented": true
    },
    {
      "text": "Three demo examples of reconstructed images by OpenHolo are shown in Fig. To study how these approximation decisions affect the hologram video quality, we next want to reconstruct/render the hologram from our design based on the real-time eye move- ments and head orientations, and compare the quality of the recon- structed images against the baseline using the peak signal-to-noise ratio (PSNR) [ 21 ,  44 ] metric. Given the lack of the physical optical holographic displays (e.g., the prototype built in Tensor Holog- raphy project [ 54 ]), we numerically generate the reconstructed holographic images on top of the OpenHolo library [ 18 ].",
      "type": "sliding_window_shuffled",
      "tokens": 172,
      "augmented": true
    },
    {
      "text": "Three demo examples of reconstructed images by OpenHolo are shown in Fig. 9: viewing a whole-hologram from different pupil positions in Fig. 9a; viewing an entire hologram (in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "9b); and a partial \n5 The data is from our estimation based on [ 51 ], rather than real-hardware measurements. 503 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n(a) Viewing W-CGH from different eye-center positions. 9a; viewing an entire hologram (in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "(b) Viewing W-CGH from different focal distances. Eye-center coordi- nates from left to right:  (0, 11mm) ,  (0, 12mm) ,  (0, 13mm) , and  (0, 14mm) . 503 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n(a) Viewing W-CGH from different eye-center positions.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "(c) Viewing S-CGH from different focal distances. Focal distance from left to right:  0.3m ,  0.4m ,  0.5m , and  0.6m . (b) Viewing W-CGH from different focal distances.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "(a): Viewing the W-CGH from different eye-center positions. (c) Viewing S-CGH from different focal distances. Focal distance from left to right:  0.3m ,  0.4m ,  0.5m , and  0.6m \nFigure 9: A demo of viewing/rendering the virtual planet whole-hologram (W-CGH, generated from all of the depth planes, i.e., from  1-st  to  16-th ) or sub-hologram (S-CGH, generated from only a subset of the depth planes, from 9-th  to  12-th  in this case) with different configurations.",
      "type": "sliding_window_shuffled",
      "tokens": 150,
      "augmented": true
    },
    {
      "text": "(c): View- ing the S-CGH from different focal distances. (b) Viewing the W-CGH from different focal distances. (a): Viewing the W-CGH from different eye-center positions.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. (c): View- ing the S-CGH from different focal distances. 41.48 31.79 30.74 \n0 10 20 30 40 50 60 \nbike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "InterHolo IntraHolo InterIntraHolo \nPSNR \n(a) PSNR. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg. bike \nbook \nbottle \ncup \nlaptop \nshoe \nAvg.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "PSNR (%) \nEnergy Savings (%) \n(b) Trade-offs. 0% \n20% \n40% \n60% \n80% \n100% \n0% 20%40%60%80%100% \norm. InterHolo IntraHolo InterIntraHolo \nPSNR \n(a) PSNR.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "hologram (in Fig. PSNR (%) \nEnergy Savings (%) \n(b) Trade-offs. Figure 10: Sensitivity studies.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "hologram (in Fig. Compared to the baseline, we then report the averaged PSNR [ 21 ,  44 ] of the recon- structed images from the six videos in Fig. 9c) from different distances.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "It can be observed from this figure that, even with the most aggressive approximation introduced by  Inter-Intra-Holo , the video quality is still sufficient for most of the AR applications (30 . Compared to the baseline, we then report the averaged PSNR [ 21 ,  44 ] of the recon- structed images from the six videos in Fig. 10a.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "It can be observed from this figure that, even with the most aggressive approximation introduced by  Inter-Intra-Holo , the video quality is still sufficient for most of the AR applications (30 . Further, to study how the tuned approximation (in Algo. 7 on average) [57].",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "Further, to study how the tuned approximation (in Algo. 3) affect the energy savings achieved, we report five design points in Fig. 2 and Algo.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "This figure shows a clear pattern of trade-offs between more-energy-savings vs. more-quality-drop. 3) affect the energy savings achieved, we report five design points in Fig. 10b.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "Generality of  HoloAR :  Although the core idea of approximation seems to be general across many video domains, our proposal is not expected to work very well for all AR applications. This figure shows a clear pattern of trade-offs between more-energy-savings vs. more-quality-drop. Specifi- cally, there are two classes of applications that would probably achieve only limited benefits from our approach.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "First, for the quality-critical applications such as AR surgery [ 56 ], ultra-high resolution/quality of holograms are typically required. In this case, offloading computations to a resource-rich cluster/cloud system would be a more reasonable design choice (instead of approximating on the edge). Specifi- cally, there are two classes of applications that would probably achieve only limited benefits from our approach.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Second, for applications, which are motion-sensitive \nsuch as the spaceship simulation [ 34 ], the hologram computation process is required to complete faster, in order to correctly reflect the current user’s eye movement and head pose in real-time. In this case, offloading computations to a resource-rich cluster/cloud system would be a more reasonable design choice (instead of approximating on the edge). The proposed  HoloAR  on the edge GPU cannot achieve such strict la- tency requirement, and can cause lagging, e.g., the eye could move to another area, while the hologram is still being computed for the previous focus region.",
      "type": "sliding_window_shuffled",
      "tokens": 149,
      "augmented": true
    },
    {
      "text": "We postpone optimizations for such applications to a future work. The proposed  HoloAR  on the edge GPU cannot achieve such strict la- tency requirement, and can cause lagging, e.g., the eye could move to another area, while the hologram is still being computed for the previous focus region. 5.5 Future Work \nDespite the hardware-agnostic nature of  HoloAR , it is still in- teresting to study how to deploy our idea on an ASIC hardware, and co-design the next-generation accelerator on edge for the AR hologram.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "5.5 Future Work \nDespite the hardware-agnostic nature of  HoloAR , it is still in- teresting to study how to deploy our idea on an ASIC hardware, and co-design the next-generation accelerator on edge for the AR hologram. Towards this, we plan to explore three critical questions in our future work: First, how many processing units (PUs) are required and just sufficient for most of the cases in a typical AR holographic application? To answer this, we plan to characterize the number of depth planes needed in various AR applications, and guide the optimal design choices (i.e., number of PUs, frequency, input and output buffer size, etc.)",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "based on application require- ments, and evaluate both PSNR and user-experience metrics such as satisfaction and dizziness [ 66 ]. Second, How do we maintain high power efficiency of PUs during runtime? To answer this, we plan to characterize the number of depth planes needed in various AR applications, and guide the optimal design choices (i.e., number of PUs, frequency, input and output buffer size, etc.)",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Second, How do we maintain high power efficiency of PUs during runtime? In some cases where a small amount of hologram computation required, not all of the PUs on-board are needed to be active. We plan to design and imple- ment a clock/power gating technology to switch off the un-utilized PUs and save power/energy.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Third, how do we handle the corner cases, where more computational resources than that provided by the accelerator are required? We plan to design and imple- ment a clock/power gating technology to switch off the un-utilized PUs and save power/energy. Towards this, we plan to design a system-level scheduler which can efficiently partition the hologram tasks between the heterogeneous accelerator and original execution engines such as CPUs or GPUs.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "Towards this, we plan to design a system-level scheduler which can efficiently partition the hologram tasks between the heterogeneous accelerator and original execution engines such as CPUs or GPUs. 6 RELATED WORK \nIn this section, we summarize prior work related to different aspects of holographic processing. Optimizations in Holographic Processing:  Holographic pro- cessing has been optimized in various domains [ 33 ,  35 ,  52 ,  54 ], to improve power efficiency or execution performance.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "From the software/algorithm perspective, a sub-hologram technique is pro- posed with a tracked viewing-window technology to tailor the holographic computation only for the necessary information in- side of the window [ 52 ]. Optimizations in Holographic Processing:  Holographic pro- cessing has been optimized in various domains [ 33 ,  35 ,  52 ,  54 ], to improve power efficiency or execution performance. For exam- ple, HORN-8 [ 35 ] has proposed a special-purpose computer for electro-holography to reduce the power consumption and still de- liver a high frame rate (similar to that of a cloud GPU).",
      "type": "sliding_window_shuffled",
      "tokens": 148,
      "augmented": true
    },
    {
      "text": "For instance, DeepHolo [ 33 ] proposes a binary-weighted computer- generated hologram model to recognize 3D objects. From the software/algorithm perspective, a sub-hologram technique is pro- posed with a tracked viewing-window technology to tailor the holographic computation only for the necessary information in- side of the window [ 52 ]. More recent efforts have attempted to combine holographic processing with neural network techniques.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "For instance, DeepHolo [ 33 ] proposes a binary-weighted computer- generated hologram model to recognize 3D objects. Apart from neural network techniques, foveated rendering is another promising performance optimization for reducing computational costs [ 2 ,  22 ,  24 ,  25 ,  30 ,  47 ,  62 ], as summarized in Sec. Furthermore, another convolution neural network (CNN) model is trained and deployed on mobile devices to synthesize a photorealistic colour 3D hologram from a single RGB-depth image in real time [ 54 ].",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "2.2.2. In this paper, the foveated rendering idea (denoted as  Inter-Holo  design) has been implemented (in Sec. Apart from neural network techniques, foveated rendering is another promising performance optimization for reducing computational costs [ 2 ,  22 ,  24 ,  25 ,  30 ,  47 ,  62 ], as summarized in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "Sec. In this paper, the foveated rendering idea (denoted as  Inter-Holo  design) has been implemented (in Sec. 4.3) and found to work well (in \n504 \nMICRO ’21, October 18–22, 2021, Virtual Event, Greece Shulin and Haibo, et al.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "Further, in this paper, we have gone beyond foveated rendering ( Inter-Holo ), by proposing an optimiza- tion/approximation called  Intra-Holo , that complements the former in boosting performance/energy efficiency. Sec. 5) as in prior works.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "Holographic Displays on AR:  Another large body of prior works focus on optimizing the holographic displays for the next- generation AR headsets [ 6 ,  17 ,  23 ]. Further, in this paper, we have gone beyond foveated rendering ( Inter-Holo ), by proposing an optimiza- tion/approximation called  Intra-Holo , that complements the former in boosting performance/energy efficiency. This enhancement is ideally suited for holographic processing at the edge, without re- quiring additional hardware, cloud assistance, or machine learning framework.",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "For example, Michelson proposes a holographic display technology that optimizes image quality for emerging near-eye displays using two SLMs and camera-in-the- loop calibration [ 7 ]. Holographic Displays on AR:  Another large body of prior works focus on optimizing the holographic displays for the next- generation AR headsets [ 6 ,  17 ,  23 ]. Neural-Holography proposes an algorithmic hologram generation framework that uses camera-in-the-loop train- ing to achieve unprecedented image fidelity and real-time frame rates [ 48 ].",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "OLAS proposes an overlap-add stereogram algorithm, which uses overlapping hogels to encode the view-dependent light- ing effects of a light field into a hologram, achieving better quality than other holographic stereograms [ 46 ]. Neural-Holography proposes an algorithmic hologram generation framework that uses camera-in-the-loop train- ing to achieve unprecedented image fidelity and real-time frame rates [ 48 ]. These display quality optimizations are orthogonal to our approximation-based proposal, and our approach can be used along with such optimizations.",
      "type": "sliding_window_shuffled",
      "tokens": 136,
      "augmented": true
    },
    {
      "text": "Targeting them, prior efforts have proposed to optimize their compression ratio, processing performance, and energy efficiency [ 8 – 10 ,  16 ,  27 ,  67 – 70 ]. These display quality optimizations are orthogonal to our approximation-based proposal, and our approach can be used along with such optimizations. Volumetric Video Streaming, Compression, and Other Opti- mizations:  Volumetric sensor inputs such as LiDAR have large vol- ume and require significant computational power and bandwidth to process/transmit.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Targeting them, prior efforts have proposed to optimize their compression ratio, processing performance, and energy efficiency [ 8 – 10 ,  16 ,  27 ,  67 – 70 ]. For example, ASV lever- ages characteristics unique to stereo vision and proposes algorith- mic and computational optimizations to improve performance and energy-efficiency of “depth from stereo” [ 11 ]. Tigris proposes an algorithm-architecture co-design system specialized for point cloud registration, to improve real-time performance and energy effi- ciency for 3D perception applications [ 65 ].",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "To efficiently steam volumetric video to mobile devices, GROOT proposes a novel PD- Tree data structure and streams the volumetric videos at a 30fps frame rate with minimal memory usage and computation for de- coding [ 27 ]. Tigris proposes an algorithm-architecture co-design system specialized for point cloud registration, to improve real-time performance and energy effi- ciency for 3D perception applications [ 65 ]. Note, however, that none of these existing schemes target at reducing the amount of “unnecessary” computations in the AR holographic applications.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "In addition to the  Inter-Holo  de- sign, our proposed  Intra-Holo  technique focuses on computation approximation opportunities, and as such, it is orthogonal to these prior efforts. 7 CONCLUSION The extremely heavy computation in hologram processing hinders the growth of the 3D display applications on AR headsets. Note, however, that none of these existing schemes target at reducing the amount of “unnecessary” computations in the AR holographic applications.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Thus, prior efforts have proposed using accelerators or cloud for optimiz- ing the hologram computation. In contrast, this paper attempts to exploit available approximation opportunities unique in AR holo- graphic applications, and proposes a two-stage  HoloAR  scheme to speed up the execution and save energy. 7 CONCLUSION The extremely heavy computation in hologram processing hinders the growth of the 3D display applications on AR headsets.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "In contrast, this paper attempts to exploit available approximation opportunities unique in AR holo- graphic applications, and proposes a two-stage  HoloAR  scheme to speed up the execution and save energy. We also propose  Intra-Holo  to further approximate each of the object holograms, by analyzing its cur- rent distance from the user. Specifically, we leverage the existing foveated rendering in  Inter-Holo  to track the user’s eye movements and approximate the holograms of the objects that are outside the user interest.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "We also propose  Intra-Holo  to further approximate each of the object holograms, by analyzing its cur- rent distance from the user. 7 ×  speedup and 73% \nenergy savings. Our experimental results show that, compared to the baseline,  HoloAR  achieves 2 .",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "ACKNOWLEDGMENTS \nThis research is supported in part by NSF grants #1763681, #1629915, #1629129, #1317560, #1526750, #1714389, #1912495, and #1909004. We believe that the lessons learned from this work will help in designing next-generation hologram accelerators that can combine approximation as well as other optimizations such as tuning PU counts, frequency, and power gating for achieving the target performance and energy efficiency for edge devices. 7 ×  speedup and 73% \nenergy savings.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "We would also like to thank Dr. Jack Sampson and Dr. Dinghao Wu for their feedback on this paper. This work was also supported in part by CRISP, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. ACKNOWLEDGMENTS \nThis research is supported in part by NSF grants #1763681, #1629915, #1629129, #1317560, #1526750, #1714389, #1912495, and #1909004.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "We would also like to thank Dr. Jack Sampson and Dr. Dinghao Wu for their feedback on this paper. 2020. REFERENCES \n[1]  Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, and Matthias Grundmann.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "arXiv preprint arXiv:2012.09988  (2020). Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "[2]  Rachel Albert, Anjul Patney, David Luebke, and Joohwan Kim. arXiv preprint arXiv:2012.09988  (2020). 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "Latency Requirements for Foveated Rendering in Virtual Reality. ACM Trans. 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 23,
      "augmented": true
    },
    {
      "text": "Using Scene Viewer to Display Interactive 3D Models in AR from an Android App or Browser. 2020. [3]  ARCore.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "Using Scene Viewer to Display Interactive 3D Models in AR from an Android App or Browser. [4]  Stephen A Benton and V Michael Bove Jr. 2008. \"https://developers.google.com/ar/develop/java/ scene-viewer\".",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "John Wiley & Sons. Holographic Imaging . [4]  Stephen A Benton and V Michael Bove Jr. 2008.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "Pokémon GO Revenue and Usage Statistics. 2020. \"https: //www.businessofapps.com/data/pokemon-go-statistics/\".",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "\"https: //www.businessofapps.com/data/pokemon-go-statistics/\". [6]  Chenliang Chang, Kiseung Bang, Gordon Wetzstein, Byoungho Lee, and Liang Gao. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "Toward the Next-generation VR/AR Optics: A Review of Holographic Near-eye Displays from a Human-centric Perspective. 2020. Optica  (2020), 1563–1578.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "[7]  Suyeon Choi, Jonghyun Kim, Yifan Peng, and Gordon Wetzstein. Optica  (2020), 1563–1578. 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Optica  (2021), 143–146. Optimizing image quality for holographic near-eye displays with Michelson Holography. 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "2021. [8]  Yu Feng, Patrick Hansen, P. Whatmough, Guoyu Lu, and Yuhao Zhu. Optica  (2021), 143–146.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "ArXiv  (2021). 2021. A LiDAR-Guided Framework for Video Enhancement.",
      "type": "sliding_window_shuffled",
      "tokens": 25,
      "augmented": true
    },
    {
      "text": "[9]  Y. Feng, Shaoshan Liu, and Yuhao Zhu. ArXiv  (2021). 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "Real-Time Spatio-Temporal LiDAR Point Cloud Compression. 2020. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)  (2020), 10766–10773.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "[10]  Yu Feng, Boyuan Tian, Tiancheng Xu, Paul Whatmough, and Yuhao Zhu. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)  (2020), 10766–10773. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "2020. In  Proceedings of the International Symposium on Microarchitecture (MICRO) . Mesorasi: Architecture Support for Point Cloud Analytics via Delayed- aggregation.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 1037–1050. [11]  Yu Feng, Paul Whatmough, and Yuhao Zhu.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "[11]  Yu Feng, Paul Whatmough, and Yuhao Zhu. 2019. ASV: Accelerated Stereo Vision System.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "643–656. In  Proceedings of the International Symposium on Microarchitecture (MICRO) . ASV: Accelerated Stereo Vision System.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "2020. \"https://www.gazept.com/\". Eye Tracking and Neuromarketing Research Made Easy.",
      "type": "sliding_window_shuffled",
      "tokens": 25,
      "augmented": true
    },
    {
      "text": "\"https://www.gazept.com/\". [13]  Patrick Geneva, Kevin Eckenhoff, Woosik Lee, Y. Yang, and Guoquan Huang. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "OpenVINS: A Research Platform for Visual-Inertial Estimation. 2020. 2020 IEEE International Conference on Robotics and Automation (ICRA)  (2020), 4666–4672.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "[14]  Giorgia Lombardo. 2020 IEEE International Conference on Robotics and Automation (ICRA)  (2020), 4666–4672. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "\"https://medium.com/demagsign/meet-the-humans- of-the-future-holograms-digital-humans-and-deep-fakes-35024b881545\". Meet the Humans of the Future: Holograms, Digital Humans, and Deep Fakes. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "[15]  Stuart Golodetz, Michael Sapienza, Julien Valentin, Vibhav Vineet, Ming-Ming Cheng, Anurag Arnab, Victor Adrian Prisacariu, Olaf Kaehler, Carl Yuheng Ren, David W. Murray, Shahram Izadi, and Philip H.S. \"https://medium.com/demagsign/meet-the-humans- of-the-future-holograms-digital-humans-and-deep-fakes-35024b881545\". Torr.",
      "type": "sliding_window_shuffled",
      "tokens": 145,
      "augmented": true
    },
    {
      "text": "Torr. SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes. 2015.",
      "type": "sliding_window_shuffled",
      "tokens": 27,
      "augmented": true
    },
    {
      "text": "arXiv  (2015). SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes. [16]  Bo Han, Yu Liu, and Feng Qian.",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "2020. ViVo: Visibility-aware Mobile Volumetric Video Streaming. [16]  Bo Han, Yu Liu, and Feng Qian.",
      "type": "sliding_window_shuffled",
      "tokens": 37,
      "augmented": true
    },
    {
      "text": "1–13. In  Proceedings of the ACM/IEEE International Conference on Mobile Computing and Networking (MobiCom) . ViVo: Visibility-aware Mobile Volumetric Video Streaming.",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "2019. 1–13. [17]  Zehao He, Xiaomeng Sui, Guofan Jin, and Liangcai Cao.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "2019. Appl. Progress in Virtual Reality and Augmented Reality Based on Holographic Display.",
      "type": "sliding_window_shuffled",
      "tokens": 19,
      "augmented": true
    },
    {
      "text": "[18]  Jisoo Hong, Youngmin Kim, Hyunjoo Bae, and Sunghee Hong. 2020. (2019), A74–A81.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "2020. OpenHolo: Open Source Library for Hologram Generation, Reconstruction and Signal Pro- cessing. In  Imaging and Applied Optics Congress .",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "[19]  Muhammad Huzaifa, Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Finn Sinclair, Boyuan Tian, Hengzhi Yuan, Jeffrey Zhang, and Sarita V. Adve. In  Imaging and Applied Optics Congress . Optical Society of America, HF3G.1.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "2021. Exploring Extended Reality with ILLIXR: A new Playground for Architecture Research. [19]  Muhammad Huzaifa, Rishi Desai, Samuel Grayson, Xutao Jiang, Ying Jing, Jae Lee, Fang Lu, Yihan Pang, Joseph Ravichandran, Finn Sinclair, Boyuan Tian, Hengzhi Yuan, Jeffrey Zhang, and Sarita V. Adve.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Exploring Extended Reality with ILLIXR: A new Playground for Architecture Research. arXiv:cs.DC/2004.04643 \n505 \nHoloAR: On-the-fly Optimization of 3D Holographic Processing for Augmented Reality MICRO ’21, October 18–22, 2021, Virtual Event, Greece \n[20]  IFIXIT. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "\"https://www.ifixit.com/Teardown/Magic+Leap+One+Teardown/112245\". Magic Leap One Teardown. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "2019. [21]  NATIONAL INSTRUMENTS. \"https://www.ifixit.com/Teardown/Magic+Leap+One+Teardown/112245\".",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "\"https://www.ni.com/en-us/innovations/white-papers/11/peak- signal-to-noise-ratio-as-an-image-quality-metric.html\". 2019. Peak Signal-to-Noise Ratio as an Image Quality Metric.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "2018. \"https://www.ni.com/en-us/innovations/white-papers/11/peak- signal-to-noise-ratio-as-an-image-quality-metric.html\". [22]  Yeon-Gyeong Ju and Jae-Hyeung Park.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "2018. Fast Generation of Mesh Based CGH in Head-Mounted Displays using Foveated Rendering Technique, In Imaging and Applied Optics 2018 (3D, AO, AIO, COSI, DH, IS, LACSEA, LS&C, MATH, pcAOP). Imaging and Applied Optics 2018 (3D, AO, AIO, COSI, DH, IS, LACSEA, LS&C, MATH, pcAOP) , DTu5F.6.",
      "type": "sliding_window_shuffled",
      "tokens": 125,
      "augmented": true
    },
    {
      "text": "[23]  Daniel K Nikolov, Sifan Ye, Sydney Dlhopolsky, Zhen Bai, Yuhao Zhu, and Jannick P Rolland. Imaging and Applied Optics 2018 (3D, AO, AIO, COSI, DH, IS, LACSEA, LS&C, MATH, pcAOP) , DTu5F.6. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "2020. Hyperion: A 3D Visualization Platform for Optical Design of Folded Systems. Frameless  2, 1 (2020), 21.",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "Frameless  2, 1 (2020), 21. [24]  Anton Kaplanyan, Anton Sochenov, Thomas Leimkühler, Mikhail Okunev, T. Goodall, and Gizem Rufo. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "DeepFovea: Neural Reconstruction for Foveated Rendering and Video Compression using Learned Statistics of Natural Videos. 2019. ACM Trans.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "(2019), 212:1–212:13. [25]  Jonghyun Kim, Youngmo Jeong, Michael Stengel, Kaan Akşit, Rachel Albert, Ben Boudaoud, Trey Greer, Joohwan Kim, Ward Lopes, Zander Majercik, Peter Shirley, Josef Spjut, Morgan McGuire, and David Luebke. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "ACM Trans. 2019. Foveated AR: Dynamically-Foveated Augmented Reality Display.",
      "type": "sliding_window_shuffled",
      "tokens": 24,
      "augmented": true
    },
    {
      "text": "[26]  Joohwan Kim, Michael Stengel, Alexander Majercik, Shalini De Mello, David Dunn, Samuli Laine, Morgan McGuire, and David Luebke. (2019). 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "2019. Nvgaze: An Anatomically-informed Dataset for Low-latency, Near-eye Gaze Estimation. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems .",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "1–12. [27]  Kyungjin Lee, Juheon Yi, Youngki Lee, Sunghyun Choi, and Young Min Kim. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems .",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "GROOT: A Real-time Streaming System of High-fidelity Volumetric Videos. 2020. [27]  Kyungjin Lee, Juheon Yi, Youngki Lee, Sunghyun Choi, and Young Min Kim.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "In Proceedings of the ACM/IEEE International Conference on Mobile Computing and Networking (MobiCom) . 1–14. GROOT: A Real-time Streaming System of High-fidelity Volumetric Videos.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "In  2008 IEEE Virtual Reality Conference . 2008. Hybrid Feature Tracking and User Interaction for Markerless Augmented Reality.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "[29]  Magic Leap. 145–152. In  2008 IEEE Virtual Reality Conference .",
      "type": "sliding_window_shuffled",
      "tokens": 22,
      "augmented": true
    },
    {
      "text": "[29]  Magic Leap. 2020. Magic Leap 1 is a Wearable Computer for Enterprise Produc- tivity.",
      "type": "sliding_window_shuffled",
      "tokens": 30,
      "augmented": true
    },
    {
      "text": "[30]  Xiaoxu Meng, Ruofei Du, and Amitabh Varshney. Magic Leap 1 is a Wearable Computer for Enterprise Produc- tivity. \"https://www.magicleap.com/en-us/magic-leap-1\".",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "[30]  Xiaoxu Meng, Ruofei Du, and Amitabh Varshney. Eye-dominance-guided Foveated Rendering. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "Eye-dominance-guided Foveated Rendering. IEEE Transactions on Visualization and Computer Graphics (2020), 1972–1980. [31]  Microsoft.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "\"https://www.microsoft.com/en- us/p/holoLens-2/91pnzzznzwcp/?activetab=pivot:techspecstab\". HoloLens 2 Tech Specs. [32]  Microsoft Research Blog.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "Second Version of HoloLens HPU will Incorporate AI Coprocessor for Implementing DNNs. 2020. [32]  Microsoft Research Blog.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "[33]  Naoya Muramatsu, Chun Wei Ooi, Yuta Itoh, and Yoichi Ochiai. Second Version of HoloLens HPU will Incorporate AI Coprocessor for Implementing DNNs. \"https://www.microsoft.com/en- us/research/blog/second-version-hololens-hpu-will-incorporate-ai- coprocessor-implementing-dnns/\".",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "2017. [33]  Naoya Muramatsu, Chun Wei Ooi, Yuta Itoh, and Yoichi Ochiai. Deep- Holo: Recognizing 3D Objects Using a Binary-Weighted Computer-Generated Hologram.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "Deep- Holo: Recognizing 3D Objects Using a Binary-Weighted Computer-Generated Hologram. In  SIGGRAPH Asia 2017 Posters . [34]  NASA.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "2020. NASA at Home – Virtual Tours and Apps. [34]  NASA.",
      "type": "sliding_window_shuffled",
      "tokens": 20,
      "augmented": true
    },
    {
      "text": "[35]  Takashi Nishitsuji, Yota Yamamoto, Takashige Sugie, Takanori Akamatsu, Ryuji Hirayama, Hirotaka Nakayama, Takashi Kakue, Tomoyoshi Shimobaba, and Tomoyoshi Ito. \"https://www.nasa.gov/ nasa-at-home-virtual-tours-and-augmented-reality\". NASA at Home – Virtual Tours and Apps.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "Special-purpose Computer HORN-8 for Phase-type Electro- holography. [35]  Takashi Nishitsuji, Yota Yamamoto, Takashige Sugie, Takanori Akamatsu, Ryuji Hirayama, Hirotaka Nakayama, Takashi Kakue, Tomoyoshi Shimobaba, and Tomoyoshi Ito. 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Express  (2018), 26722–26733. Opt. Special-purpose Computer HORN-8 for Phase-type Electro- holography.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "JETSON AGX XAVIER AND THE NEW ERA OF AUTONOMOUS MACHINES. \"http://info.nvidia.com/rs/156-OFN-742/images/Jetson_AGX_ Xavier_New_Era_Autonomous_Machines.pdf\". 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "[37] Nvidia. 2020. \"http://info.nvidia.com/rs/156-OFN-742/images/Jetson_AGX_ Xavier_New_Era_Autonomous_Machines.pdf\".",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "2020. CUDA Toolkit Documentation: Nvprof. \"shorturl.at/zEFU5\".",
      "type": "sliding_window_shuffled",
      "tokens": 28,
      "augmented": true
    },
    {
      "text": "\"https://github.com/google- research-datasets/Objectron/blob/master/index/bike_annotations\". 2020. Objectron Dataset Annotation: bike.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "[39]  Objectron. 2020. \"https://github.com/google- research-datasets/Objectron/blob/master/index/bike_annotations\".",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "2020. Objectron Dataset Annotation: book. \"https://github.com/ google-research-datasets/Objectron/blob/master/index/book_annotations\".",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "2020. [40]  Objectron. \"https://github.com/ google-research-datasets/Objectron/blob/master/index/book_annotations\".",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "\"https://github.com/ google-research-datasets/Objectron/blob/master/index/bottle_annotations\". Objectron Dataset Annotation: bottle. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "2020. \"https://github.com/ google-research-datasets/Objectron/blob/master/index/bottle_annotations\". [41]  Objectron.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "2020. \"https://github.com/google- research-datasets/Objectron/blob/master/index/cup_annotations\". Objectron Dataset Annotation: cup.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "\"https://github.com/google- research-datasets/Objectron/blob/master/index/cup_annotations\". [42]  Objectron. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "2020. \"https://github.com/ google-research-datasets/Objectron/blob/master/index/laptop_annotations\". Objectron Dataset Annotation: laptop.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "2020. \"https://github.com/ google-research-datasets/Objectron/blob/master/index/laptop_annotations\". [43]  Objectron.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "2020. \"https://github.com/google- research-datasets/Objectron/blob/master/index/shoe_annotations\". Objectron Dataset Annotation: shoe.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "2019. \"https://github.com/google- research-datasets/Objectron/blob/master/index/shoe_annotations\". [44]  OpenCV.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "Similarity check (PNSR and SSIM) on the GPU. \"https://docs.opencv.org/2.4/doc/tutorials/gpu/gpu-basics-similarity/gpu- basics-similarity.html\". 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "[45] OpenHolo. \"https://docs.opencv.org/2.4/doc/tutorials/gpu/gpu-basics-similarity/gpu- basics-similarity.html\". 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "OpenHolo Database. \"http://openholo.org/database/depth\". 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 23,
      "augmented": true
    },
    {
      "text": "2019. [46]  Nitish Padmanaban, Yifan Peng, and Gordon Wetzstein. \"http://openholo.org/database/depth\".",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "Holographic Near- Eye Displays Based on Overlap-Add Stereograms. 2019. ACM Trans.",
      "type": "sliding_window_shuffled",
      "tokens": 23,
      "augmented": true
    },
    {
      "text": "2016. (2019). [47]  Anjul Patney, Marco Salvi, Joohwan Kim, Anton Kaplanyan, Chris Wyman, Nir Benty, David Luebke, and Aaron Lefohn.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "Towards Foveated Rendering for Gaze-Tracked Virtual Reality. ACM Trans. 2016.",
      "type": "sliding_window_shuffled",
      "tokens": 25,
      "augmented": true
    },
    {
      "text": "(2016). [48]  Yifan Peng, Suyeon Choi, Nitish Padmanaban, Jonghyun Kim, and Gordon Wet- zstein. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "2020. In  ACM SIGGRAPH 2020 Emerging Technologies (SIGGRAPH ’20) . Neural Holography.",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "[49]  Martin Persson, David Engström, and Mattias Goksör. In  ACM SIGGRAPH 2020 Emerging Technologies (SIGGRAPH ’20) . Association for Computing Machinery.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "[49]  Martin Persson, David Engström, and Mattias Goksör. Real-time Generation of Fully Optimized Holograms for Optical Trapping Applications. 2011.",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "8097. International Society for Optics and Photonics, 80971H. Real-time Generation of Fully Optimized Holograms for Optical Trapping Applications. In  Optical Trapping and Optical Micromanipulation VIII , Vol.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "8097. International Society for Optics and Photonics, 80971H. [50]  Victor Adrian Prisacariu, Olaf Kähler, Stuart Golodetz, Michael Sapienza, Tom- maso Cavallari, Philip H. S. Torr, and David William Murray. 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "2017. CoRR (2017). InfiniTAM v3: A Framework for Large-Scale 3D Reconstruction with Loop Closure.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "2019. CoRR (2017). [51]  Murad Qasaimeh, Kristof Denolf, Jack Lo, Kees A. Vissers, Joseph Zambreno, and Phillip H. Jones.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "Comparing Energy Efficiency of CPU, GPU and FPGA Implementations for Vision Kernels. In  15th IEEE International Conference on Embedded Software and Systems . 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "[52]  Stephan Reichelt, Ralf Haussler, Norbert Leister, Gerald Futterer, Hagen Stolle, and Armin Schwerdtner. In  15th IEEE International Conference on Embedded Software and Systems . 1–8.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "Holographic 3-D Displays - Electro-holography Within the Grasp of Commercialization . 2010. [52]  Stephan Reichelt, Ralf Haussler, Norbert Leister, Gerald Futterer, Hagen Stolle, and Armin Schwerdtner.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "shorturl.at/jmnpD [53]  Antoni Rosinol, Marcus Abate, Yun Chang, and Luca Carlone. IntechOpen. Holographic 3-D Displays - Electro-holography Within the Grasp of Commercialization .",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "2020. shorturl.at/jmnpD [53]  Antoni Rosinol, Marcus Abate, Yun Chang, and Luca Carlone. Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping. Conf. In  IEEE Intl.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "on Robotics and Automation (ICRA) . Conf. [54]  Liang Shi, Beichen Li, Changil Kim, Petr Kellnhofer, and Wojciech Matusik.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "2021. Towards Real-time Photorealistic 3D Holography with Deep Neural Networks. [54]  Liang Shi, Beichen Li, Changil Kim, Petr Kellnhofer, and Wojciech Matusik.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Towards Real-time Photorealistic 3D Holography with Deep Neural Networks. Nature  592 (2021). [55]  Tomoyoshi Shimobaba, Jiantong Weng, Takahiro Sakurai, Naohisa Okada, Takashi Nishitsuji, Naoki Takada, Atsushi Shiraki, Nobuyuki Masuda, and To- moyoshi Ito.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "2012. [55]  Tomoyoshi Shimobaba, Jiantong Weng, Takahiro Sakurai, Naohisa Okada, Takashi Nishitsuji, Naoki Takada, Atsushi Shiraki, Nobuyuki Masuda, and To- moyoshi Ito. Computational Wave Optics Library for C++: CWO++ Library.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Computational Wave Optics Library for C++: CWO++ Library. [56]  Jeffrey H. Shuhaiber. Computer Physics Communications  (2012), 1124–1138.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "2004. [56]  Jeffrey H. Shuhaiber. Augmented Reality in Surgery.",
      "type": "sliding_window_shuffled",
      "tokens": 20,
      "augmented": true
    },
    {
      "text": "Archives of Surgery (2004), 170–174. [57]  Randall Shumaker and Lackey Stephanie. Augmented Reality in Surgery.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "[57]  Randall Shumaker and Lackey Stephanie. Virtual, Augmented and Mixed Reality: Designing and Developing Augmented and Virtual Environments: 6th International Conference, VAMR 2014, Held as Part of HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014, Proceedings, Part I . 2014.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "8525. Virtual, Augmented and Mixed Reality: Designing and Developing Augmented and Virtual Environments: 6th International Conference, VAMR 2014, Held as Part of HCI International 2014, Heraklion, Crete, Greece, June 22-27, 2014, Proceedings, Part I . Vol.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "[58]  Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling. Springer. 8525.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "[58]  Julian Steil, Inken Hagestedt, Michael Xuelin Huang, and Andreas Bulling. Privacy-Aware Eye Tracking Using Differential Privacy. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "ACM Interna- tional Symposium on Eye Tracking Research and Applications (ETRA) . Privacy-Aware Eye Tracking Using Differential Privacy. In  Proc.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "[59]  Stereolabs. 1–9. ACM Interna- tional Symposium on Eye Tracking Research and Applications (ETRA) .",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "[59]  Stereolabs. 2020. ZED Software Development Kit.",
      "type": "sliding_window_shuffled",
      "tokens": 16,
      "augmented": true
    },
    {
      "text": "ZED Software Development Kit. [60]  techradar. \"https://www.stereolabs.com/developers/release/\".",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "Google Pixel 2 Review. [61]  Oren M Tepper, Hayeem L Rudy, Aaron Lefkowitz, Katie A Weimer, Shelby M Marks, Carrie S Stern, and Evan S Garfein. \"https://www.techradar.com/reviews/ google-pixel-2-review\".",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "2017. [61]  Oren M Tepper, Hayeem L Rudy, Aaron Lefkowitz, Katie A Weimer, Shelby M Marks, Carrie S Stern, and Evan S Garfein. Mixed Reality with HoloLens: Where Virtual Reality Meets Augmented Reality in the Operating Room.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Plastic and reconstructive surgery  (2017), 1066–1070. [62]  Lingjie Wei and Yuji Sakamoto. Mixed Reality with HoloLens: Where Virtual Reality Meets Augmented Reality in the Operating Room.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "2019. Fast Calculation Method with Foveated Rendering for Computer-generated Holograms Using an Angle-changeable Ray- tracing Method. [62]  Lingjie Wei and Yuji Sakamoto.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "Appl. Fast Calculation Method with Foveated Rendering for Computer-generated Holograms Using an Angle-changeable Ray- tracing Method. Opt.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "Opt. (2019), A258–A266. [63]  Yang Wu, Jun Wang, Chun Chen, Chan-Juan Liu, Feng-Ming Jin, and Ni Chen.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "2021. Adaptive Weighted Gerchberg-Saxton Algorithm for Generation of Phase- only Hologram with Artifacts Suppression. [63]  Yang Wu, Jun Wang, Chun Chen, Chan-Juan Liu, Feng-Ming Jin, and Ni Chen.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Express  (2021), 1412–1427. Adaptive Weighted Gerchberg-Saxton Algorithm for Generation of Phase- only Hologram with Artifacts Suppression. Opt.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "\"https://www. Zynq UltraScale+ MPSoC ZCU102 Evaluation Kit. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 27,
      "augmented": true
    },
    {
      "text": "\"https://www. xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html\". [65]  Tiancheng Xu, Boyuan Tian, and Yuhao Zhu.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "2019. [65]  Tiancheng Xu, Boyuan Tian, and Yuhao Zhu. Tigris: Architecture and Algorithms for 3D Perception in Point Clouds.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "629–642. Tigris: Architecture and Algorithms for 3D Perception in Point Clouds. In  Proceedings of the International Symposium on Microarchitecture (MICRO) .",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "2016. 629–642. [66]  Hiroshi Yoshikawa, Takeshi Yamaguchi, and Hiroki Uetake.",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "Image Quality Evaluation and Control of Computer-generated Holograms. 2016. In  Practical Hologra- phy XXX: Materials and Applications , Hans I. Bjelkhagen and V. Michael Bove Jr.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "(Eds.). International Society for Optics and Photonics, SPIE, 144 – 152. In  Practical Hologra- phy XXX: Materials and Applications , Hans I. Bjelkhagen and V. Michael Bove Jr.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "International Society for Optics and Photonics, SPIE, 144 – 152. 2021. [67]  Anlan Zhang, Chendong Wang, Bo Han, and Feng Qian.",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "Efficient Volu- metric Video Streaming Through Super Resolution. In  Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications . 2021.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "[68]  Haibo Zhang, Prasanna Venkatesh Rengasamy, Shulin Zhao, Nachiappan Chi- dambaram Nachiappan, Anand Sivasubramaniam, Mahmut T. Kandemir, Ravi Iyer, and Chita R. Das. In  Proceedings of the 22nd International Workshop on Mobile Computing Systems and Applications . 106–111.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "2017. Race-to-Sleep + Content Caching + Display Caching: A Recipe for Energy-Efficient Video Streaming on Handhelds. [68]  Haibo Zhang, Prasanna Venkatesh Rengasamy, Shulin Zhao, Nachiappan Chi- dambaram Nachiappan, Anand Sivasubramaniam, Mahmut T. Kandemir, Ravi Iyer, and Chita R. Das.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 517–531. Race-to-Sleep + Content Caching + Display Caching: A Recipe for Energy-Efficient Video Streaming on Handhelds.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "517–531. [69]  Haibo Zhang, Shulin Zhao, Ashutosh Pattnaik, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices. In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "[70]  Shulin Zhao, Haibo Zhang, Sandeepa Bhuyan, Cyan Subhra Mishra, Ziyu Ying, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. In  Proceedings of the International Symposium on Microarchitecture (MICRO) . 657–669.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Déjà View: Spatio-Temporal Compute Reuse for Energy-Efficient 360 °  VR Video Streaming. [70]  Shulin Zhao, Haibo Zhang, Sandeepa Bhuyan, Cyan Subhra Mishra, Ziyu Ying, Mahmut T. Kandemir, Anand Sivasubramaniam, and Chita R. Das. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "Déjà View: Spatio-Temporal Compute Reuse for Energy-Efficient 360 °  VR Video Streaming. 241–253. In  Proceedings of the International Symposium on Computer Architec- ture (ISCA) .",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    }
  ]
}