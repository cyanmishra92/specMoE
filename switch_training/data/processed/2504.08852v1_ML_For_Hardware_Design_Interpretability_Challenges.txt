=== ORIGINAL PDF: 2504.08852v1_ML_For_Hardware_Design_Interpretability_Challenges.pdf ===\n\nRaw text length: 64588 characters\nCleaned text length: 63102 characters\nNumber of segments: 38\n\n=== CLEANED TEXT ===\n\nML For Hardware Design Interpretability: Challenges and Opportunities Raymond Baartmans 1 Andrew Ensinger 1 Victor Agostinelli 1 Lizhong Chen 1 Abstract The increasing size and complexity of machine learning (ML) models have driven the growing need for custom hardware accelerators capable of efficiently supporting ML workloads. How- ever, the design of such accelerators remains a time-consuming process, heavily relying on engi- neers to manually ensure design interpretability through clear documentation and effective com- munication. Recent advances in large language models (LLMs) offer a promising opportunity to automate these design interpretability tasks, par- ticularly the generation of natural language de- scriptions for register-transfer level (RTL) code, what we refer to as RTL-to-NL tasks. In this paper, we examine how design interpretability, particularly in RTL-to-NL tasks, influences the efficiency of the hardware design process. We review existing work adapting LLMs for these tasks, highlight key challenges that remain un- addressed, including those related to data, com- putation, and model development, and identify opportunities to address them. By doing so, we aim to guide future research in leveraging ML to automate RTL-to-NL tasks and improve hardware design interpretability, thereby accelerating the hardware design process and meeting the increas- ing demand for custom hardware accelerators in machine learning and beyond. 1. Introduction A significant area of research and industry attention has fo- cused on the development of custom hardware accelerators to address critical challenges in the computational efficiency and scalability of machine learning (ML) applications. The increasing size and complexity of modern deep learning models have led to significantly higher computational de- mands for training and inference, a challenge that has be- 1School of Electrical Engineering and Computer Science, Ore- gon State University, Corvallis, Oregon, USA. Contact: {baartmar, ensingea, agostinv, Figure 1. The standard VLSI design flow consists of a front-end phase, where functionality is specified, implemented in RTL, and verified, followed by a back-end phase, where the design undergoes synthesis, physical design, and final preparation for fabrication. come especially pronounced with the rise of large language models (LLMs) in recent years (Radford et al., 2019; Brown et al., 2020; OpenAI, 2024). For example, training Ope- nAI s GPT-3 (Brown et al., 2020) required approximately 3.14 1023 floating-point operations (FLOPs), highlighting the immense computational resources needed for such large- scale models. While approaches such as model quantization (Fan et al., 2020; Dettmers et al., 2022; Tseng et al., 2024) and algorithmic optimization (Ainslie et al., 2023; Yang et al., 2024; Agostinelli et al., 2024; DeepSeek-AI, 2025) can drastically reduce resource consumption, just how much optimization can be achieved is limited by what the under- lying hardware is designed to support. One such example is BitNet (Wang et al., 2023b; Ma et al., 2024; Wang et al., 2024a), a ternary weight LLM which, although matching the performance to various full-precision LLMs, has lim- ited efficiency gains since ternary arithmetic is not well supported on general-purpose GPU hardware. In contrast, specialized hardware accelerators can enable such models to run more efficiently by eliminating unnecessary com- ponents from the chip, freeing up chip area to implement logic specifically designed for specialized functions like 1 arXiv:2504.08852v1 [cs.LG] 11 Apr 2025 ML For Hardware Design Interpretability: Challenges and Opportunities ternary arithmetic. This has created a growing demand for research and development of custom ML hardware (Chen et al., 2017b; Reuther et al., 2020), designed to efficiently handle the unique requirements of modern machine learning workloads. The increasing demand for custom hardware accelerators has also driven the need for more efficient workflows in hardware development. An overview of the hardware de- sign cycle, often referred to as the VLSI flow, is shown in Figure 1. During the frontend phase, a design specifi- cation and its various sub-components must be expressed in register-transfer level (RTL) logic using a hardware de- scription language (HDL) such as Verilog. While electronic design automation (EDA) tools now exist to automate many rote or repetitive aspects of HDL coding, the majority of ef- fort in design cycles is spent on integrating sub-components and verifying the correctness of the overall design (Chen et al., 2017a; Varambally Sehgal, 2020). These processes rely on design interpretability, where each sub-component must be clearly specified and communicated to designers, who must then ensure their RTL implementations are well- documented and accurately shared with other teams to en- sure proper integration and functionality. Given the increas- ing complexity and scale of modern hardware designs (Ernst, 2005; Ziegler et al., 2017; Gomes et al., 2022), providing clear natural language descriptions for every part of a de- sign is a slow and challenging task, especially for globally distributed engineering teams with many different linguistic backgrounds. In this paper, we primarily focus on RTL-to- NL tasks as a key aspect of design interpretability, where machine learning models are applied to generate natural language explanations of RTL code. Applying machine learning-based natural language process- ing (NLP) methods for design interpretability could lead to significant speed-ups across the hardware design cycle. Recently, large language models (LLMs) have demonstrated effective performance at both natural language (Hendrycks et al., 2021; Xiong et al., 2023; Saba, 2024; Zhang et al., 2024a) and coding-related tasks (Tarassow, 2023; Nijkamp et al., 2023; Bairi et al., 2024; Chen et al., 2024), including code documentation (Luo et al., 2024; Nam et al., 2024). As interest grows in adapting LLMs for hardware design tasks (Zhang et al., 2024b; Thakur et al., 2023; Kashanaki et al., 2024), they appear poised to become a critical tool for design interpretability and the automation of RTL-to-NL tasks. In this paper, we explore the challenges and opportunities of applying machine learning to hardware design interpretabil- ity and RTL-to-NL tasks. We highlight the critical impor- tance of design interpretability and RTL-to-NL tasks in chip design cycles (Section 2). We survey the limited existing research in this area and other related topics (Section 3). We discuss outstanding dataset, computational, and model de- velopment challenges that remain unaddressed by existing work (Section 4), and identify opportunities for addressing these challenges (Section 5). Finally, we include further discussion the long-term significance of this emerging area of research (Section 6). In doing so, we hope to guide future research in using ML to automate RTL-to-NL tasks and hardware design interpretability, which in turn can greatly accelerate hardware design processes. 2. Design Interpretability and RTL-to-NL Tasks Design interpretability (not to be confused with model in- terpretability) refers to the human-readability of hardware designs, ensuring that the RTL or other design representa- tions can be easily understood, communicated, and modified by engineers. In this section, we introduce the RTL-to-NL task, and highlight the importance of design interpretability across the design cycle. Afterwards, we provide a brief out- line of the infrastructure needed to apply machine learning to this problem. 2.1. RTL-to-NL Tasks Within the broader umbrella of design interpretability, we define RTL-to-NL tasks as tasks which require interpreting RTL designs and related artifacts (e.g., simulator logs, error messages) to produce a response in natural language. RTL designs are expressed using hardware description languages (HDLs) such as Verilog, which differ from traditional pro- gramming languages in that they model the structure of a digital circuit, including critical aspects like concurrency and timing. While it may seem initially unclear how such concepts could be accurately communicated in natural language, natural language is actually well-equipped for describing RTL. De- scribing timing information or concurrency in natural lan- guage can be accomplished by properly using phrases such as at the same time, meanwhile, or until. The fact that engineers routinely explain and teach RTL concepts in textbooks, documentation, and discussions using natural language underscores this. Simple examples of RTL-to-NL tasks would be writing doc- umentation for an undocumented RTL design, or answering a high-level question like How does this design work? A more complex version of an RTL-to-NL task might involve reasoning about specific behaviors within the design, such as answering, What control signals does this design use to ensure that data dependencies are resolved in the matrix multiplication unit? Figure 2 illustrates an example of an RTL-to-NL task, where a model (e.g., an LLM) generates a specification based on a given Verilog design. Unlike 2 ML For Hardware Design Interpretability: Challenges and Opportunities Figure 2. An example of a possible RTL-to-NL task, where the goal is to generate natural language documentation for an RTL design described in Verilog. traditional code-to-NL tasks, which describe a sequence of instructions to be executed on a CPU GPU, RTL-to-NL tasks are unique in multiple ways: 1. RTL represents hardware structure and behavior. While typical software code defines sequential algo- rithms, RTL (e.g., Verilog) directly determines the structure and behavior of a digital circuit. For example, generating a natural language description of a Verilog counter requires not only explaining its behavior (incre- menting a counter) but also describing the underlying hardware, such as the 4-bit register and adder that op- erate synchronously on each clock cycle. 2. Distinct syntactic structure of RTL. For example, while state-updating operations in software are typi- cally expressed as sequential function calls or object state modifications, in Verilog, logic is typically de- fined within always blocks. These blocks execute con- currently, independent of each other, and are triggered by specific sensitivity conditions, such as clock edges or signal changes. Syntactic structures like this must be handled separately than typical software code. 3. Understanding RTL requires foundational knowl- edge in digital logic. Reading RTL requires a funda- mental understanding of hardware design and digital logic concepts. As a result, LLMs for RTL-to-NL tasks must also possess foundational knowledge of these con- cepts. Figure 4 illustrates how typical LLMs, which are primarily trained on software code, struggle to answer basic questions that require an understanding of digital logic design. 2.2. Impact of Interpretability in the Design Cycle As hardware designs continue to grow in scale and modular- ity (Ernst, 2005; Ziegler et al., 2017; Gomes et al., 2022), understanding the vast range of interconnected subsystems has become increasingly challenging. In this context, RTL interpretability can have a significant impact on the speed of the design cycle. 2.2.1. LEGACY CODE AND IP INTEGRATION Understanding code written by others is often cited as one of the most challenging and time-consuming aspects of engi- neering projects (Sun et al., 2023b; Oliveira et al., 2025), and chip design is no exception. Modern chip designs now often consist of many smaller subcomponents (IP) developed by individual teams or external vendors (Kim et al., 2020; ipx, 2023). When integrating IP with missing or poor documen- tation, engineers may have to reverse-engineer the module s functionality and behavior, significantly slowing down the integration process. In contrast, having well-documented IP can greatly accelerate the integration process, as evidenced by the industry-wide adoption of standardized IP description formats (ipx, 2023). 2.2.2. VERIFICATION Verification, in which a design is tested for correctness, is the most time-consuming part of hardware development, occupying up to 70 of the entire chip design cycle (Varam- bally Sehgal, 2020). Surveys have found that debugging alone can take up 44 of this process (Mutschler, 2021), meaning that up to a third of the entire design cycle may be spent on RTL-to-NL tasks for debugging, such as read- ing RTL code, simulator logs, and other design artifacts for fault localization. Importantly, time spent in debug more than doubles the time spent on any other verification task, such as running simulations (21 ), testbench development (19 ), and test planning (13 ). Misunderstandings or lack of clarity in RTL documentation can further extend the time spent on debugging. The quicker engineers can access accu- rate RTL-to-NL translations, the more time they can spend resolving actual issues. 2.2.3. ELECTRONIC DESIGN AUTOMATION Electronic Design Automation (EDA) tool developers have long sought to automate as many chip design tasks as possi- ble. However, this pursuit of automation can come at the ex- pense of human readability and interpretability of the tool s outputs. High-Level Synthesis (HLS) is one such tool, offer- ing the potential to significantly reduce the need for manual RTL coding by automatically translating a functional specifi- cation written in C into RTL. However, these HLS tools pri- oritize performance over readability and maintainability, as shown in Figure 3. Variable names are often incredibly long 3 ML For Hardware Design Interpretability: Challenges and Opportunities Figure 3. An example of how hand-written Verilog may compare to the code generated by High-Level Synthesis tools. The HLS- generated code in the figure was generated by Bambu HLS (Fer- randi et al., 2021) as well as Catapult HLS (Siemens, 2024). and unintelligible, the design logic is hard to follow, and the tools do not optimize the organization of the code or de- sign hierarchy for better maintainability. Although this can cause challenges when designing for field-programmable gate arrays (FPGAs), it becomes an even greater problem when designing for large-scale fixed application-specific integrated circuits (ASICs). ASIC design flows, where the majority of designers primarily work at the RTL level, often need to meet much stricter performance, power, or area tar- gets. Since HLS tools do not guarantee optimal solutions, their generated RTL is less likely to be used in these flows, even as an initial RTL draft, because its lack of readabil- ity and RTL documentation makes any further hand-tuning nearly impossible. RTL-to-NL models could generate the necessary documentation and explanations to make the out- puts of automated design tools more understandable, thereby expanding the potential applications of these tools to accel- erate design cycles. 2.2.4. AI ALIGNMENT IN CHIP DESIGN With the rise of generative AI tools powered by large deep learning models, the need for design interpretability has become increasingly important. As these AI-driven tools become integrated into chip design workflows, maintaining human interpretability of their outputs is essential to ensure that generated designs align with engineers expectations. This is particularly crucial for preventing scenarios where an AI might introduce vulnerabilities into a design. 2.3. Needed Infrastructure These challenges highlight the importance of developing tools which can provide automated support for RTL-to-NL tasks and design interpretability. Traditional approaches, such as rule-based systems and static analysis tools (Ba- jwa Choudhary, 2006; Chiticariu et al., 2013; Xu Cai, 2021), rely on extensive manual rule engineering and often struggle to generalize beyond narrowly defined domains. In contrast, deep learning models particularly large language models (LLMs) have demonstrated strong generalization capabilities (Kang et al., 2024; Wang et al., 2024b; Liu et al., 2024d; OpenAI, 2024) and have outperformed rule-based methods in various code analysis tasks (Lu et al., 2024a; Yin et al., 2024). Reflecting this potential, NVIDIA s inter- nal studies on hardware design workflows have identified LLM-based assistants as a promising way to boost engi- neering productivity (Liu et al., 2024a). At the same time, the widespread adoption of standardized IP formats such as IP-Xact (ipx, 2023) signals a broader industry consensus on the importance of improving design interpretability to support more efficient and scalable hardware development cycles. However, adapting models for RTL-to-NL tasks requires dedicated ML infrastructure to support training and eval- uating models on RTL-to-NL tasks, including appropriate datasets, evaluation benchmarks, fine-tuning frameworks, and more. Despite growing interest in applying LLMs to hardware design, this supporting infrastructure remains un- derdeveloped. In the next section, we examine the existing efforts to address these gaps. 3. Current Work In this section, we survey the existing research on adapting LLMs for hardware design interpretability and RTL-to-NL tasks. We also briefly summarize relevant work on related topics. 3.1. LLM for RTL-to-NL Tasks 3.1.1. DEEPRTL DeepRTL (Liu et al., 2025) presents the first dataset and benchmark for assessing LLM performance in both Ver- ilog understanding and Verilog generation. Their approach leverages GPT-4, along with professional hardware engi- neers, to annotate a dataset of Verilog designs with natural language descriptions of their functionality. The annota- tion takes place at multiple levels of granularity; starting from line-level commenting and eventually reaching high- level block and module-level descriptions. To fine-tune LLMs on their dataset, a curriculum learning approach is employed, which organizes training into sequential phases for line-level, block-level, and module-level annotations. To 4 ML For Hardware Design Interpretability: Challenges and Opportunities evaluate their dataset, they construct a benchmark of 100 Verilog designs with expert-written descriptions, measuring the embedding similarity of model-generated descriptions against the human references. Additionally, they measure a GPT Score, utilizing GPT-4 to evaluate the quality of the generated description by providing a score between 0 and 1, based on the similarity to the reference description. 3.1.2. SPECLLM SpecLLM (Li et al., 2024a) compiles a dataset of hardware architecture specifications from various open-source hard- ware projects to fine-tune LLMs for generating specifica- tions of RTL designs. Although the dataset can be very useful in many scenarios, one drawback is that it primarily consists of architecture-level specifications, which do not necessarily capture RTL-level details. As a result, the im- provement in the model s understanding of RTL through this dataset may be limited, as it appears to focus more on architecture standards and specification formatting. The paper does not provide any quantitative evaluation to assess the model s performance after fine-tuning. 3.1.3. CHIPNEMO NVIDIA s ChipNeMo (Liu et al., 2024a) is a general- purpose hardware design assistant LLM, trained on a di- verse set of proprietary chip-design documents, including RTL code, specifications, and documentation. The model in- corporates techniques like domain-specific tokenization and retrieval-augmented generation (RAG) to enhance its effi- ciency. While the paper does not specifically evaluate its per- formance on RTL-to-NL tasks, its broad (albeit proprietary) training dataset and general-purpose design make it relevant for inclusion here. Furthermore, the evaluation benchmark details are limited, with much of the assessment relying on expert grading. 3.2. LLM for NL-to-RTL Tasks While the adaptation of LLMs for RTL-to-NL tasks and hardware design interpretability remains underexplored, adapting LLMs for NL-to-RTL tasks, such as Verilog generation, has been the dominant focus of prior work (Liu et al., 2023; Lu et al., 2023; Allam Shalan, 2024; Vun- garala et al., 2024; Zhang et al., 2024b; Thakur et al., 2023; Kashanaki et al., 2024; Sandal Akturk, 2024; DeLorenzo et al., 2024a;b). A number of works have released datasets and benchmarks to assess LLM-generated Verilog designs, including RTLCoder (Liu et al., 2024c), VerilogEval (Liu et al., 2023) (benchmark only), RTLLM (Lu et al., 2023), and more (Thakur et al., 2023; Chang et al., 2024; Zhang et al., 2024b). These works primarily draw upon open- source RTL designs from platforms like GitHub and HDL- Bits (HDLBits, 2024) to construct their datasets. Other areas of exploration include generating SystemVerilog ver- ification constructs (Sun et al., 2023a), designing in HLS (Huang et al., 2024b), EDA tool script generation (Liu et al., 2024a), and applying agent-based approaches to hardware design (Huang et al., 2024a). 3.3. LLM for Coding Outside of hardware design, a significant amount of work has focused on adapting LLMs for coding in more widely used programming languages. While it is beyond the scope of this paper to examine all approaches, one relevant tech- nique involves custom attention mechanisms that incorpo- rate abstract syntax trees (ASTs) and language-specific in- formation (Ahmad et al., 2020; Wang et al., 2021; Guo et al., 2021), which could similarly be extended for LLMs adapted for Verilog processing. Additionally, we discuss later in Section 4.4 why approaches to adapting LLMs for software coding may not necessarily translate to the realm of hardware design. 4. Challenges In this section, we examine outstanding challenges in adapt- ing LLMs for RTL-to-NL tasks which have yet to be fully addressed by existing work. Language Number of Files Java 155,000,000 C 104,000,000 Python 95,400,000 Go 24,100,000 Verilog 1,200,000 SystemVerilog 465,000 Table 1. The number of files on GitHub categorized by program- ming language, highlighting the disparity between HDLs and pop- ular software languages such as Java and Python. 4.1. Dataset Construction The most critical gap limiting research on adapting LLMs to RTL-to-NL tasks has been the absence of datasets of Verilog code with corresponding natural language labels, such as design specifications or question-answer pairs. Assembling such datasets is challenging due to the competitive nature of the industry and the smaller number of chip designers com- pared to software engineers, leading to a lack of open-source RTL designs to use for dataset construction. As shown in Table 4, the total number of Verilog and SystemVerilog files on GitHub is approximately 93 times smaller than the num- ber of Java files. Additionally, the time required to manually label data and the expertise needed for proper labeling fur- ther prevent the use of data annotation platforms (Wang, 5 ML For Hardware Design Interpretability: Challenges and Opportunities 2024), which could otherwise expedite the creation of these datasets. While some progress has been made in automating RTL dataset creation by using publically available RTL designs from GitHub (Thakur et al., 2023), many of the sources that provide reliably structured or labeled data, such as HDLBits (HDLBits, 2024), mainly feature basic problems that do not reflect a broader set of real-world designs. As such, recent works on NL-to-RTL (Liu et al., 2024c) and RTL-to-NL (Liu et al., 2025) tasks tend to heavily rely on synthetic data to compensate for the lack of high-quality, labeled examples. 4.2. Evaluating RTL-to-NL Accuracy While NL-to-RTL tasks can be evaluated by running LLM- generated code against human-written test suites to verify functional correctness, evaluating RTL-to-NL tasks is less straightforward. Natural language descriptions or specifi- cations of hardware must convey critical implementation details, where even minor omissions or inaccuracies can significantly distort meaning. Even when data is available, research on RTL-to-NL tasks still lacks robust approaches for accurately evaluating the quality of natural language documentation. Existing reference-based measures such as BLEU or ROUGE, which target lexical similarity, could give misleadingly high scores to critically different descrip- tions simply due to lexical overlap. Similarly, embedding similarity can compare if an LLM-generated description is semantically close (i.e., uses many semantically related words, like clock, reset, etc.), but cannot penalize omis- sions or vague descriptions that could lead to serious im- plementation mismatches. An alternative way to evaluate accuracy could be through expert grading, but this approach is difficult to scale due to the specialized knowledge re- quired and the inherent subjectivity of graders. Methods like GPT-scoring, such as the one used by (Liu et al., 2025), are also problematic, as pointwise scoring with LLMs is challenging to calibrate (Desai Durrett, 2020) and tends to be computationally expensive. 4.3. Computational Challenges Adapting LLMs to process RTL designs also presents a computational challenge, as representing hardware logic in RTL often requires substantially more lines of code and consequently, greater context length compared to describ- ing equivalent functionality in high-level languages like C. Table 2 shows that while algorithms are relatively simple to describe in C, their RTL implementations require signifi- cantly more code. To make this comparison, we retrieved corresponding C and Verilog implementations of some well- known algorithms from GitHub. Additionally, we ran the C implementations through Bambu HLS (Ferrandi et al., 2021) to generate RTL, which is shown in the Verilog (HLS) col- umn for comparison. We observe that hand-written Verilog implementations can require more than triple the number of tokens compared to their C counterparts. Additionally, the length of a single IP generated by HLS can exceed the limits of even commercially deployed models that are opti- mized for long contexts, such as LLaMa-3 (Grattafiori et al., 2024) (up to 128,000 tokens), GPT-4 (OpenAI, 2024) (up to 128,000 tokens), and Claude 3 (Anthropic, 2024) (up to 200,000 tokens). These context limitations make it clear that scaling to larger designs, which incorporate many different IPs, can quickly become computationally prohibitive. Algorithm Number of Tokens C Verilog Verilog (HLS) GEMM 252 18423 224289 Sobel 557 2031 39849 BellmanFord 1440 5156 47335 SHA256 2683 9290 260356 Table 2. The token counts for implementing a variety of algorithms in C and Verilog. The Verilog (HLS) column represents Verilog code generated by Bambu HLS from the corresponding C program. Token counts were obtained using the GPT-4 tokenizer (OpenAI, 2024). Moreover, LLMs are known to suffer performance degrada- tion when handling long context lengths (Li et al., 2024b; An et al., 2024; Laban et al., 2024), and exhibit a tendency to lose focus on middle-context information (Liu et al., 2024b). They also demonstrate a tendency to hallucinate , gener- ating outputs that may appear coherent but are nonsensical or inaccurate (Ji et al., 2023; Friel Sanyal, 2023). The effects of these properties can be particularly detrimental for interpreting RTL, where tracking the code s structure and dependencies is essential for correctly understanding the de- sign and ensuring accurate interpretation of its functionality. The current performance and reliability challenges of LLMs, particularly when dealing with long contexts, make research in adapting LLMs for RTL-to-NL tasks more challenging, as it would be difficult to demonstrate their applicability to real-world RTL development, where designs may span hundreds of thousands of tokens. Critically, many of the cur- rently existing benchmarks for both Verilog generation and Verilog understanding (Chang et al., 2024; Liu et al., 2025) do not include designs exceeding 2048 tokens in length. 4.4. Training Learning Challenges As we previously discuss in section 2.1 HDLs, such as Ver- ilog, are fundamentally different from traditional imperative programming languages, as they are used to describe the structure of digital circuits, rather than defining sequential behavior. This poses unique challenges for LLMs, includ- ing those optimized for coding (Rozi ere et al., 2024), as 6 ML For Hardware Design Interpretability: Challenges and Opportunities these models have been primarily trained on sequential pro- gramming languages. Such a model may misinterpret a design s concurrency, assuming two components execute sequentially in the order they appear in the code. RTL code also involves underlying digital design concepts that are rarely explicitly documented within the code itself but can be essential for understanding the design s perfor- mance and functionality. Critical path timing, circuit area, and the effects of propagation delays are crucial to accurate design analysis but are not explicitly represented in Verilog. In hardware design, different logic components exhibit vary- ing delay characteristics. For instance, an adder generally has higher delay than a basic AND gate due to the addi- tional gates involved in carry propagation. Now consider a scenario where a model trained on traditional software- oriented code is asked to document the critical path in a Verilog design. Without an understanding of component- specific delays, such a model might naively identify the longest dependency chain as the critical path, focusing solely on operation count. In doing so, it could miss a shorter path that includes higher-delay components, such as adders, which ultimately defines the actual critical path. Figure 4. We asked GPT4o-mini to evaluate the critical path of simple Verilog modules. This figure shows how the model will give incorrect or inconsistent answers. For this example, the path leading to f will have a longer critical path due to carry logic necessary for the adders. Figure 4 shows how GPT 4o-mini can make this exact mis- take when asked to identify the critical path in a simple Verilog design. In the figure, GPT selected the path lead- ing to e, likely because it contained more operations than the path leading to f, even though the latter has a longer critical path due to the carry logic involved in the adders. Importantly, RTL code that requires the most interpretation is often poorly organized, featuring inconsistent or poorly named variables that do not align with human-readable con- ventions. As a result, adapting LLMs for RTL-to-NL tasks requires training approaches that not only enable the model to learn the unique structure of the HDLs used to describe RTL designs and underlying hardware design concepts, but also enable it to interpret poorly organized RTL code that lacks meaningful semantic clues. 5. Research Opportunities The challenges outlined in the previous section present many opportunities where new research can help provide the crit- ical ML infrastructure outlined in Section 2.3. This in- frastructure could catalyze greater research participation towards adapting LLMs for RTL-to-NL tasks, creating new tools that can drastically improve the efficiency of hardware development. 5.1. Approaches for Dataset Construction Datasets are the foundational infrastructure needed to adapt LLMs for RTL-to-NL tasks. However, creating task-specific RTL-to-NL datasets is complex and time-consuming, par- ticularly when manual labeling is required, which makes it challenging for most research groups to undertake. There- fore, innovative approaches to automated dataset generation are essential. Recent works on NL-to-RTL (Liu et al., 2024c) and RTL-to-NL (Liu et al., 2025) tasks have started to ex- plore synthetic data as a solution to the lack of high-quality labeled examples. While synthetic data has proven effec- tive in these works, as well as in other domains (Figueira Vaz, 2022; Lu et al., 2024b; OpenAI, 2024), it is not guaranteed to be effective, and further exploration is needed to determine how such methods could be successfully ap- plied for RTL-to-NL tasks. One potential approach could integrate HLS or RTL synthesis tools with LLMs like GPT to automatically generate and label question-answer pairs on critical RTL design aspects, such as concurrency. Furthermore, there is still significant potential for community-driven efforts to create and expand RTL-to-NL datasets. The newly established IEEE International Confer- ence on LLM-Aided Design (ieee, 2025) features a datasets track, which could help bring together researchers interested in advancing RTL-to-NL datasets through collaboration. 5.2. New Approaches for Evaluation As discussed in Section 4.2, evaluating RTL-to-NL accuracy is a nontrivial problem where existing metrics are likely inadequate. One potential approach to evaluating LLM- generated explanations is to have another LLM regenerate the same RTL design based on the explanation, followed by logical equivalence checking (LEC) to compare the regen- erated design with the original. This approach is inspired by the concept of Round-Trip Correctness (Allamanis et al., 2024), where an LLM generates a natural language descrip- tion of a subset of code, then re-implements the code in the target language. In the case of RTL-to-NL, a separate LLM with adequate Verilog skills could be used to re-implement 7 ML For Hardware Design Interpretability: Challenges and Opportunities the design. This approach could provide a better signal of whether critical design details are accurately captured in natural language, reducing the reliance on human-written ground truth references. However, challenges remain, such as distinguishing between hallucinations coding errors and inaccuracies in the description. Another approach could explore distilling expert grading into a BERT model to ap- proximate expert judgment, similar to the COMET model used in machine translation (Rei et al., 2020). 5.3. Model Architectures for Handling Long Contexts of RTL Efficiently processing long context lengths is crucial for any LLM dealing with extensive RTL code. As outlined in Section 4.3, RTL designs often involve large context spans, making it challenging for models to capture dependencies across the entire design hierarchy. One approach to address- ing this challenge is modifying attention mechanisms to take advantage of the hierarchical and modular nature of RTL, similar to techniques used in code-specific transformer mod- els (Ahmad et al., 2020; Wang et al., 2021; Guo et al., 2021). These models incorporate structural information about the code directly into the attention function. Such modifica- tions could enhance recall over long contexts by focusing on relevant signal dependencies and module interactions, ultimately improving the model s ability to handle complex RTL designs. Additionally, RTL code often contains highly repetitive patterns that require less processing effort, such as the statement always (posedge clk) begin, which appears in every Verilog module involving sequential logic. While it breaks into six tokens using the GPT-4o tok- enizer, its semantic contribution is minimal. Alternative tokenization methods, like Byte-Latent Transformers (BLTs) (Pagnoni et al., 2024), adaptively allocate compute to more informative parts of the sequence, improving efficiency. A custom tokenizer tailored to RTL text data could also re- duce context length by condensing repetitive constructs into single tokens. For example, always (posedge clk) begin could be reduced to one token. NVIDIA s Chip- Nemo (Liu et al., 2024a) incorporated a custom tokenizer which demonstrated modest improvements in tokenization efficiency (1.6 to 3.3 ) on their internal datasets, though public data saw little to no gain. While not strictly a model architecture, domain-specific retrieval-augmented generation (RAG) techniques (Liu et al., 2024a) could also help alleviate the burden of long con- text lengths. By indexing RTL submodules and retrieving only relevant portions of a design during inference, RAG approaches could improve scalability and performance, par- ticularly when dealing with large IP libraries or system- on-chip (SoC) designs. Similarly, another strategy could involve using module embeddings, analogous to document embeddings, to capture the functionality of Verilog submod- ules in compact vector representations. This could reduce the need to include all submodule definitions in the context, allowing the LLM to operate at a higher level of abstraction. Code embeddings, a well-studied topic (Chen Monperrus, 2019), could be key in enabling this approach. 5.4. LLM-Aware EDA Tools As discussed in Section 2.1 and shown in Figure 3, the RTL code generated by EDA tools, such as HLS, often contains bloat and readability issues that can cause problems for both human designers and LLMs. Code generation tools that enhance readability by producing semantically meaningful variable names, structured hierarchies, and more intuitive representations could significantly improve both human understanding and make the code easier for LLMs to process. Developers of automation tools may also benefit from recog- nizing how the intermediate data their tools generate often discarded after fulfilling its primary function could serve as a rich source of training data for LLMs. HLS tools primar- ily generate Verilog but internally represent designs using graph-based abstractions that guide the scheduling of opera- tions, resolution of data dependencies, and optimization of the hardware datapath. These representations support the step-by-step transformation of functional algorithms into hardware implementations. For example, Bambu HLS (Fer- randi et al., 2021) provides a --print-dot option that lets users visualize these graphs, offering insights about the design that are not easily visible in the Verilog output. Traditionally, this intermediate data is discarded after com- pilation, but it could be repurposed to train LLMs, enabling them to reason through higher-level hardware design chal- lenges. This approach is aligned with Chain-of-Thought reasoning research (Wang et al., 2023a; Chu et al., 2024), where models enhance performance by breaking down com- plex problems into intermediate steps. 6. Further Discussion Given the ongoing uncertainty about how rapidly advancing AI systems will affect the workforce and human roles, we believe it is important to discuss the long-term relevance and potential impact of research on RTL-to-NL tasks and hardware design interpretability in this evolving landscape. 6.1. Would Fully Autonomous Hardware Design Eliminate the Need for Design Interpretability? One may argue that if AI and EDA tools eventually surpass human capabilities across all stages of the design cycle, then making these designs human-interpretable would become 8 ML For Hardware Design Interpretability: Challenges and Opportunities unnecessary. However, while advanced EDA tools have made significant strides, the problem sizes they address are so vast (Alazemi et al., 2018; Noonan, 2024; Zhong et al., 2023) that achieving globally optimal designs is intractable with current technology. Although deep learning-driven EDA tools have demonstrated substantial improvements in many areas (Hafen Critchlow, 2013; Mirhoseini et al., 2021; Goswami Bhatia, 2023), their models still face significant challenges in being able to produce formal guar- antees for correct optimal output, as verifying even sim- ple neural networks is shown to be NP-hard (Katz et al., 2017). As a result, human involvement, including RTL-to- NL tasks, will likely remain essential, even if it eventually serves primarily to provide oversight of AI-generated out- puts. Moreover, as discussed in Section 2.2.4, ensuring design interpretability will be crucial for aligning AI with human engineers intentions. 6.2. Is Design Interpretability a Human Problem? Another question to raise is that the problems addressed by RTL-to-NL tasks may be fundamentally human prob- lems and are best solved through better organizational and management strategies, rather then creating complex and ex- pensive ML solutions, which may do nothing but exacerbate existing problems. For example, knowledge silos across teams and poor documentation for IPs are problems that a design group or organization could resolve by enforcing stricter documentation practices. While organizational and management improvements are certainly important, this overlooks the limitations of such ap- proaches as hardware design projects grow in scale and com- plexity. As teams grow in size and individual engineers take on larger and more intricate subsystems, it becomes increas- ingly difficult for teams to enforce comprehensive documen- tation and rigorous standards without micromanagement. Additionally, manually writing high-quality documentation will always require significant man-hours, which can be alleviated through automation with LLMs (Dvivedi et al., 2024). As discussed in Section 2.1, LLMs offer notable advantages in generalizability and performance compared to methods like rule-based systems, making them a fitting and practical solution for automating design interpretability. 7. Conclusion The growing computational demands of modern ML models have highlighted the need for custom ML hardware. How- ever, we find that the hardware design cycle remains heavily dependent on engineers manually ensuring design inter- pretability through extensive documentation and commu- nication, creating a significant bottleneck. Leveraging ML to address this challenge presents a promising opportunity. In this paper, we examine existing research on this topic and identify the computational and learning challenges that remain unresolved. We then discuss how these challenges open new research avenues that future research in this area can address. Critically, progress in adapting LLMs for RTL- to-NL tasks and enhancing hardware design interpretability will not only streamline the hardware development process but could also contribute to broader advancements in ma- chine learning techniques with applications across multiple research domains. References Ieee standard for ip-xact, standard structure for packaging, integrating, and reusing ip within tool flows. IEEE Std 1685-2022 (Revision of IEEE Std 1685-2014), pp. 1 750, 2023. doi: 10.1109 IEEESTD.2023.10054520. Agostinelli, V., Hong, S., and Chen, L. Leapformer: Enabling linear transformers for autoregressive and si- multaneous tasks via learned proportions. In Forty- first International Conference on Machine Learning, 2024. URL id XhH1OKLANY. Ahmad, W., Chakraborty, S., Ray, B., and Chang, K.-W. A transformer-based approach for source code summa- rization. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pp. 4998 5007, Online, July 2020. Association for Compu- tational Linguistics. doi: 10.18653 v1 2020.acl-main. 449. URL acl-main.449 . Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. GQA: Training generalized multi-query transformer models from multi-head check- points. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https: openreview.net forum?id hmOwOZWzYE. Alazemi, F., AziziMazreah, A., Bose, B., and Chen, L. Routerless network-on-chip. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 492 503, 2018. doi: 10.1109 HPCA.2018. 00049. Allam, A. and Shalan, M. Rtl-repo: A benchmark for evaluating llms on large-scale rtl design projects, 2024. Allamanis, M., Panthaplackel, S., and Yin, P. Unsupervised evaluation of code llms with round-trip correctness, 2024. URL An, C., Zhang, J., Zhong, M., Li, L., Gong, S., Luo, Y., Xu, J., and Kong, L. Why does the effective context length of 9 ML For Hardware Design Interpretability: Challenges and Opportunities llms fall short?, 2024. URL abs 2410.18745. Anthropic. The claude 3 model family: Opus, son- net, haiku, 2024. URL 2303.08774. Bairi, R., Sonwane, A., Kanade, A., C., V. D., Iyer, A., Parthasarathy, S., Rajamani, S., Ashok, B., and Shet, S. Codeplan: Repository-level coding using llms and planning. Proc. ACM Softw. Eng., 1(FSE), July 2024. doi: 10.1145 3643757. URL 1145 3643757. Bajwa, I. and Choudhary, M. A rule based system for speech language context understanding. 23, 12 2006. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020. URL https: arxiv.org abs 2005.14165. Chang, K., Chen, Z., Zhou, Y., Zhu, W., kun wang, Xu, H., Li, C., Wang, M., Liang, S., Li, H., Han, Y., and Wang, Y. Natural language is not enough: Benchmarking multi- modal generative ai for verilog generation, 2024. URL Chen, J., Li, Z., Hu, X., and Xia, X. Nlperturbator: Studying the robustness of code llms to natural language variations, 2024. URL 2406.19783. Chen, W., Ray, S., Bhadra, J., Abadir, M., and Wang, L.-C. Challenges and trends in modern soc design verification. IEEE Design and Test, 34(5):7 22, 2017a. doi: 10.1109 MDAT.2017.2735383. Chen, Y.-H., Krishna, T., Emer, J. S., and Sze, V. Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. IEEE Journal of Solid- State Circuits, 52(1):127 138, 2017b. doi: 10.1109 JSSC. 2016.2616357. Chen, Z. and Monperrus, M. A literature study of embed- dings on source code. arXiv preprint arXiv:1904.03061, 2019. Chiticariu, L., Li, Y., and Reiss, F. R. Rule-based infor- mation extraction is dead! long live rule-based infor- mation extraction systems! In Yarowsky, D., Bald- win, T., Korhonen, A., Livescu, K., and Bethard, S. (eds.), Proceedings of the 2013 Conference on Empir- ical Methods in Natural Language Processing, pp. 827 832, Seattle, Washington, USA, October 2013. Asso- ciation for Computational Linguistics. URL https: aclanthology.org D13-1079 . Chu, Z., Chen, J., Chen, Q., Yu, W., He, T., Wang, H., Peng, W., Liu, M., Qin, B., and Liu, T. Navigate through enigmatic labyrinth a survey of chain of thought rea- soning: Advances, frontiers and future, 2024. URL DeepSeek-AI. Deepseek-r1: Incentivizing reasoning ca- pability in llms via reinforcement learning, 2025. URL DeLorenzo, M., Chowdhury, A. B., Gohil, V., Thakur, S., Karri, R., Garg, S., and Rajendran, J. Make every move count: Llm-based high-quality rtl code generation us- ing mcts, 2024a. URL 2402.03289. DeLorenzo, M., Gohil, V., and Rajendran, J. Creativeval: Evaluating creativity of llm-based hardware code gen- eration, 2024b. URL 2404.08806. Desai, S. and Durrett, G. Calibration of pre-trained trans- formers, 2020. URL 2003.07892. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022. URL 2208.07339. Dvivedi, S. S., Vijay, V., Pujari, S. L. R., Lodh, S., and Kumar, D. A comparative analysis of large lan- guage models for code documentation generation. In Proceedings of the 1st ACM International Conference on AI-Powered Software, AIware 2024, pp. 65 73, New York, NY, USA, 2024. Association for Comput- ing Machinery. ISBN 9798400706851. doi: 10.1145 3664646.3664765. URL 1145 3664646.3664765. Ernst, D. Limits to modularity: Reflections on re- cent developments in chip design. Industry and Innovation, 12(3):303 335, 2005. doi: 10.1080 13662710500195918. URL 1080 13662710500195918. Fan , A., Stock , P., , Graham, B., Grave, E., Gribonval, R., Jegou, H., and Joulin, A. Training with quantization noise for extreme model compression. 2020. 10 ML For Hardware Design Interpretability: Challenges and Opportunities Ferrandi, F., Castellana, V. G., Curzel, S., Fezzardi, P., Fiorito, M., Lattuada, M., Minutoli, M., Pilato, C., and Tumeo, A. Invited: Bambu: an open-source research framework for the high-level synthesis of complex ap- plications. In 2021 58th ACM IEEE Design Automa- tion Conference (DAC), pp. 1327 1330, 2021. doi: 10.1109 DAC18074.2021.9586110. Figueira, A. and Vaz, B. Survey on synthetic data genera- tion, evaluation methods and gans. Mathematics, 10(15), 2022. ISSN 2227-7390. doi: 10.3390 math10152733. URL 15 2733. Friel, R. and Sanyal, A. Chainpoll: A high efficacy method for llm hallucination detection, 2023. URL https: arxiv.org abs 2310.18344. Gomes, W., Koker, A., Stover, P., Ingerly, D., Siers, S., Venkataraman, S., Pelto, C., Shah, T., Rao, A., O Mahony, F., Karl, E., Cheney, L., Rajwani, I., Jain, H., Cortez, R., Chandrasekhar, A., Kanthi, B., and Koduri, R. Ponte vecchio: A multi-tile 3d stacked processor for exascale computing. In 2022 IEEE International Solid-State Cir- cuits Conference (ISSCC), volume 65, pp. 42 44, 2022. doi: 10.1109 ISSCC42614.2022.9731673. Goswami, P. and Bhatia, D. Application of machine learning in fpga eda tool development. IEEE Access, 11:109564 109580, 2023. doi: 10.1109 ACCESS.2023.3322358. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., and et. al. The llama 3 herd of models, 2024. URL org abs 2407.21783. Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., LIU, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S. K., Clement, C., Drain, D., Sundaresan, N., Yin, J., Jiang, D., and Zhou, M. Graphcode{bert}: Pre-training code representations with data flow. In International Conference on Learning Representations, 2021. URL id jLoC4ez43PZ. Hafen, R. and Critchlow, T. Eda and ml a perfect pair for large-scale data analysis. In 2013 IEEE Interna- tional Symposium on Parallel and Distributed Processing, Workshops and Phd Forum, pp. 1894 1898, 2013. doi: 10.1109 IPDPSW.2013.118. HDLBits. Hdlbits: Verilog practice, 2024. URL https: hdlbits.01xz.net wiki Main_Page. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https: openreview.net forum?id d7KBjmI3GmQ. Huang, H., Lin, Z., Wang, Z., Chen, X., Ding, K., and Zhao, J. Towards llm-powered verilog rtl assistant: Self-verification and self-correction. arXiv preprint arXiv:2406.00115, 2024a. Huang, Y., Wan, L. J., Ye, H., Jha, M., Wang, J., Li, Y., Zhang, X., and Chen, D. New solutions on llm accelera- tion, optimization, and application. In Proceedings of the 61st ACM IEEE Design Automation Conference, pp. 1 4, 2024b. ieee. Ieee international conference on llm-aided design, 2025. URL Ji, Z., Yu, T., Xu, Y., Lee, N., Ishii, E., and Fung, P. Towards mitigating LLM hallucination via self reflection. In Bouamor, H., Pino, J., and Bali, K. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1827 1843, Singa- pore, December 2023. Association for Computational Linguistics. doi: 10.18653 v1 2023.findings-emnlp. 123. URL findings-emnlp.123 . Kang, K., Setlur, A., Ghosh, D., Steinhardt, J., Tomlin, C., Levine, S., and Kumar, A. What do learning dynamics reveal about generalization in llm reasoning?, 2024. URL Kashanaki, F. R., Zakharov, M., and Renau, J. HDLEval: Benchmarking LLMs for Multiple HDLs. In The First IEEE International Workshop on LLM-Aided Design (IS- LAD), July 2024. Katz, G., Barrett, C., Dill, D. L., Julian, K., and Kochender- fer, M. J. Reluplex: An efficient smt solver for verifying deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, pp. 97 117. Springer, 2017. Kim, J., Murali, G., Park, H., Qin, E., Kwon, H., Chekuri, V. C. K., Rahman, N. M., Dasari, N., Singh, A. K., Lee, M., Torun, H. M., Roy, K., Swami- nathan, M., Mukhopadhyay, S., Krishna, T., and Lim, S. K. Architecture, chip, and package codesign flow for interposer-based 2.5-d chiplet integration enabling heterogeneous ip reuse. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 28:2424 2437, 2020. URL org CorpusID:225079697. 11 ML For Hardware Design Interpretability: Challenges and Opportunities Laban, P., Fabbri, A., Xiong, C., and Wu, C.-S. Summary of a haystack: A challenge to long-context LLMs and RAG systems. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empiri- cal Methods in Natural Language Processing, pp. 9885 9903, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653 v1 2024. emnlp-main.552. URL org 2024.emnlp-main.552 . Li, M., Fang, W., Zhang, Q., and Xie, Z. Specllm: Exploring generation and review of vlsi design specification with large language model, 2024a. URL org abs 2401.13266. Li, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. Long- context llms struggle with long in-context learning. CoRR, abs 2404.02060, 2024b. URL 10.48550 arXiv.2404.02060. Liu, M., Pinckney, N., Khailany, B., and Ren, H. Verilo- geval: Evaluating large language models for verilog code generation, 2023. URL 2309.07544. Liu, M., Ene, T.-D., Kirby, R., Cheng, C., Pinckney, N., Liang, R., Alben, J., Anand, H., and et. al. Chipnemo: Domain-adapted llms for chip design, 2024a. URL Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157 173, 2024b. doi: 10.1162 tacl a 00638. URL https: aclanthology.org 2024.tacl-1.9 . Liu, S., Fang, W., Lu, Y., Zhang, Q., Zhang, H., and Xie, Z. Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution. In 2024 IEEE LLM Aided Design Workshop (LAD), pp. 1 5. IEEE, 2024c. Liu, Y., Meng, Y., Wu, F., Peng, S., Yao, H., Guan, C., Tang, C., Ma, X., Wang, Z., and Zhu, W. Evaluating the generalization ability of quantized llms: Benchmark, analysis, and toolbox, 2024d. URL org abs 2406.12928. Liu, Y., Xu, C., Zhou, Y., Li, Z., and Xu, Q. Deeprtl: Bridging verilog understanding and generation with a unified representation model, 2025. URL https: arxiv.org abs 2502.15832. Lu, G., Ju, X., Chen, X., Pei, W., and Cai, Z. Grace: Empowering llm-based software vulnerability detection with graph structure and in-context learning. Journal of Systems and Software, 212:112031, 2024a. ISSN 0164- 1212. doi: URL science article pii S0164121224000748. Lu, Y., Liu, S., Zhang, Q., and Xie, Z. Rtllm: An open- source benchmark for design rtl generation with large language model, 2023. URL abs 2308.05345. Lu, Y., Shen, M., Wang, H., Wang, X., van Rechem, C., Fu, T., and Wei, W. Machine learning for synthetic data generation: A review, 2024b. URL org abs 2302.04062. Luo, Q., Ye, Y., Liang, S., Zhang, Z., Qin, Y., Lu, Y., Wu, Y., Cong, X., Lin, Y., Zhang, Y., et al. Repoagent: An llm-powered open-source framework for repository- level code documentation generation. arXiv preprint arXiv:2402.16667, 2024. Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., and Wei, F. The era of 1-bit llms: All large language models are in 1.58 bits, 2024. URL Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J. W., Songhori, E. M., Wang, S., Lee, Y.-J., Johnson, E., Pathak, O., Nazi, A., Pak, J., Tong, A., Srinivasa, K., Hang, W., Tuncer, E., Le, Q. V., Laudon, J., Ho, R., Carpenter, R., and Dean, J. A graph placement method- ology for fast chip design. Nature, 594:207 212, 2021. URL org CorpusID:235395490. Mutschler, A. Debug tops verification tasks, Oct 2021. URL debug-tops-verification-tasks . Nam, D., Macvean, A., Hellendoorn, V., Vasilescu, B., and Myers, B. Using an llm to help with code understand- ing. In Proceedings of the IEEE ACM 46th International Conference on Software Engineering, pp. 1 13, 2024. Nijkamp, E., Hayashi, H., Xiong, C., Savarese, S., and Zhou, Y. Codegen2: Lessons for training llms on programming and natural languages, 2023. URL org abs 2305.02309. Noonan, K. Next-generation eda tools reshape the future of chip design, Dec 2024. Oliveira, D., Santos, R., de Oliveira, B., Monperrus, M., Castor, F., and Madeiral, F. Understanding code under- standability improvements in code reviews. IEEE Trans- actions on Software Engineering, 51(1):14 37, 2025. doi: 10.1109 TSE.2024.3453783. 12 ML For Hardware Design Interpretability: Challenges and Opportunities OpenAI. Gpt-4 technical report, 2024. URL https: arxiv.org abs 2303.08774. Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J., Zettle- moyer, L., Ghosh, G., Lewis, M., Holtzman, A., and Iyer, S. Byte latent transformer: Patches scale bet- ter than tokens, 2024. URL abs 2412.09871. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Rei, R., Stewart, C., Farinha, A. C., and Lavie, A. Comet: A neural framework for mt evaluation. arXiv preprint arXiv:2009.09025, 2020. Reuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., and Kepner, J. Survey of machine learning ac- celerators. In 2020 IEEE High Performance Extreme Computing Conference (HPEC), pp. 1 12, 2020. doi: 10.1109 HPEC43674.2020.9286149. Rozi ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., D efossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2024. URL https: arxiv.org abs 2308.12950. Saba, W. S. Llms understanding of natural language revealed, 2024. URL 2407.19630. Sandal, S. and Akturk, I. Zero-shot rtl code generation with attention sink augmented large language models, 2024. URL Siemens. High-level synthesis and verification, 2024. Sun, C., Hahn, C., and Trippel, C. Towards improving verification productivity with circuit-aware translation of natural language to systemverilog assertions. In First International Workshop on Deep Learning-aided Verifi- cation, 2023a. Sun, W., Fang, C., Ge, Y., Hu, Y., Chen, Y., Zhang, Q., Ge, X., Liu, Y., and Chen, Z. A survey of source code search: A 3-dimensional perspective, 2023b. URL https: arxiv.org abs 2311.07107. Tarassow, A. The potential of llms for coding with low-resource and domain-specific programming lan- guages, 2023. URL 2307.13018. Thakur, S., Ahmad, B., Fan, Z., Pearce, H., Tan, B., Karri, R., Dolan-Gavitt, B., and Garg, S. Benchmark- ing large language models for automated verilog rtl code generation. 2023 Design, Automation and Test in Eu- rope Conference and Exhibition (DATE), 2023. doi: 10.23919 DATE56975.2023.10137086. URL https: par.nsf.gov biblio 10419705. Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa, C. Quip: even better llm quantization with hadamard incoherence and lattice codebooks. In Proceedings of the 41st International Conference on Machine Learning, ICML 24. JMLR.org, 2024. Varambally, B. S. and Sehgal, N. Optimising design ver- ification using machine learning: An open source solu- tion, 2020. URL 02453. Vungarala, D., Nazzal, M., Morsali, M., Zhang, C., Ghosh, A., Khreishah, A., and Angizi, S. Sa-ds: A dataset for large language model-driven ai accelerator design gener- ation, 2024. Wang, A. 10 best data annotation and data labeling tools in 2024: Basicai s blog, 2024. Wang, C., Xu, Y., Peng, Z., Zhang, C., Chen, B., Wang, X., Feng, L., and An, B. keqing: knowledge-based question answering is a nature chain-of-thought men- tor of llm, 2023a. URL 2401.00426. Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1- bit transformers for large language models, 2023b. URL Wang, H., Ma, S., and Wei, F. Bitnet a4.8: 4-bit activations for 1-bit llms, 2024a. URL abs 2411.04965. Wang, Y., Wang, W., Joty, S., and Hoi, S. C. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696 8708, Online and Punta Cana, Dominican Republic, November 2021. Association for Computa- tional Linguistics. doi: 10.18653 v1 2021.emnlp-main. 685. URL emnlp-main.685 . Wang, Y., Si, S., Li, D., Lukasik, M., Yu, F., Hsieh, C.-J., Dhillon, I. S., and Kumar, S. Two-stage llm fine-tuning with less specialization and more generalization, 2024b. URL 13 ML For Hardware Design Interpretability: Challenges and Opportunities Xiong, H., Bian, J., Yang, S., Zhang, X., Kong, L., and Zhang, D. Natural language based context modeling and reasoning for ubiquitous computing with large language models: A tutorial, 2023. URL org abs 2309.15074. Xu, X. and Cai, H. Ontology and rule-based natural language processing approach for interpreting textual reg- ulations on underground utility infrastructure. Advanced Engineering Informatics, 48:101288, 2021. ISSN 1474- 0346. doi: URL science article pii S1474034621000422. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient train- ing. In Forty-first International Conference on Machine Learning, 2024. URL forum?id ia5XvxFUJT. Yin, X., Ni, C., and Wang, S. Multitask-based evaluation of open-source llm on software vulnerability. IEEE Transac- tions on Software Engineering, 50(11):3071 3087, 2024. doi: 10.1109 TSE.2024.3470333. Zhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., and Hashimoto, T. B. Benchmarking large lan- guage models for news summarization. Transactions of the Association for Computational Linguistics, 12:39 57, 2024a. doi: 10.1162 tacl a 00632. URL https: aclanthology.org 2024.tacl-1.3 . Zhang, Y., Yu, Z., Fu, Y., Wan, C., and Lin, Y. C. MG- Verilog: multi-grained dataset towards enhanced llm- assisted verilog generation. In The First IEEE Interna- tional Workshop on LLM-Aided Design (LAD 24), 2024b. Zhong, R., Du, X., Kai, S., Tang, Z., Xu, S., Zhen, H.-L., Hao, J., Xu, Q., Yuan, M., and Yan, J. Llm4eda: Emerg- ing progress in large language models for electronic de- sign automation, 2023. URL abs 2401.12224. Ziegler, M. M., Bertran, R., Buyuktosunoglu, A., and Bose, P. Machine learning techniques for taming the complexity of modern hardware design. IBM Journal of Research and Development, 61(4 5):13:1 13:14, 2017. doi: 10. 1147 JRD.2017.2721699. 14\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nML For Hardware Design Interpretability: Challenges and Opportunities Raymond Baartmans 1 Andrew Ensinger 1 Victor Agostinelli 1 Lizhong Chen 1 Abstract The increasing size and complexity of machine learning (ML) models have driven the growing need for custom hardware accelerators capable of efficiently supporting ML workloads. How- ever, the design of such accelerators remains a time-consuming process, heavily relying on engi- neers to manually ensure design interpretability through clear documentation and effective com- munication. Recent advances in large language models (LLMs) offer a promising opportunity to automate these design interpretability tasks, par- ticularly the generation of natural language de- scriptions for register-transfer level (RTL) code, what we refer to as RTL-to-NL tasks. In this paper, we examine how design interpretability, particularly in RTL-to-NL tasks, influences the efficiency of the hardware design process. We review existing work adapting LLMs for these tasks, highlight key challenges that remain un- addressed, including those related to data, com- putation, and model development, and identify opportunities to address them. By doing so, we aim to guide future research in leveraging ML to automate RTL-to-NL tasks and improve hardware design interpretability, thereby accelerating the hardware design process and meeting the increas- ing demand for custom hardware accelerators in machine learning and beyond. 1. Introduction A significant area of research and industry attention has fo- cused on the development of custom hardware accelerators to address critical challenges in the computational efficiency and scalability of machine learning (ML) applications. The increasing size and complexity of modern deep learning models have led to significantly higher computational de- mands for training and inference, a challenge that has be- 1School of Electrical Engineering and Computer Science, Ore- gon State University, Corvallis, Oregon, USA. Contact: {baartmar, ensingea, agostinv, Figure 1. The standard VLSI design flow consists of a front-end phase, where functionality is specified, implemented in RTL, and verified, followed by a back-end phase, where the design undergoes synthesis, physical design, and final preparation for fabrication. come especially pronounced with the rise of large language models (LLMs) in recent years (Radford et al., 2019; Brown et al., 2020; OpenAI, 2024).\n\n--- Segment 2 ---\nThe standard VLSI design flow consists of a front-end phase, where functionality is specified, implemented in RTL, and verified, followed by a back-end phase, where the design undergoes synthesis, physical design, and final preparation for fabrication. come especially pronounced with the rise of large language models (LLMs) in recent years (Radford et al., 2019; Brown et al., 2020; OpenAI, 2024). For example, training Ope- nAI s GPT-3 (Brown et al., 2020) required approximately 3.14 1023 floating-point operations (FLOPs), highlighting the immense computational resources needed for such large- scale models. While approaches such as model quantization (Fan et al., 2020; Dettmers et al., 2022; Tseng et al., 2024) and algorithmic optimization (Ainslie et al., 2023; Yang et al., 2024; Agostinelli et al., 2024; DeepSeek-AI, 2025) can drastically reduce resource consumption, just how much optimization can be achieved is limited by what the under- lying hardware is designed to support. One such example is BitNet (Wang et al., 2023b; Ma et al., 2024; Wang et al., 2024a), a ternary weight LLM which, although matching the performance to various full-precision LLMs, has lim- ited efficiency gains since ternary arithmetic is not well supported on general-purpose GPU hardware. In contrast, specialized hardware accelerators can enable such models to run more efficiently by eliminating unnecessary com- ponents from the chip, freeing up chip area to implement logic specifically designed for specialized functions like 1 arXiv:2504.08852v1 [cs.LG] 11 Apr 2025 ML For Hardware Design Interpretability: Challenges and Opportunities ternary arithmetic. This has created a growing demand for research and development of custom ML hardware (Chen et al., 2017b; Reuther et al., 2020), designed to efficiently handle the unique requirements of modern machine learning workloads. The increasing demand for custom hardware accelerators has also driven the need for more efficient workflows in hardware development. An overview of the hardware de- sign cycle, often referred to as the VLSI flow, is shown in Figure 1.\n\n--- Segment 3 ---\nThe increasing demand for custom hardware accelerators has also driven the need for more efficient workflows in hardware development. An overview of the hardware de- sign cycle, often referred to as the VLSI flow, is shown in Figure 1. During the frontend phase, a design specifi- cation and its various sub-components must be expressed in register-transfer level (RTL) logic using a hardware de- scription language (HDL) such as Verilog. While electronic design automation (EDA) tools now exist to automate many rote or repetitive aspects of HDL coding, the majority of ef- fort in design cycles is spent on integrating sub-components and verifying the correctness of the overall design (Chen et al., 2017a; Varambally Sehgal, 2020). These processes rely on design interpretability, where each sub-component must be clearly specified and communicated to designers, who must then ensure their RTL implementations are well- documented and accurately shared with other teams to en- sure proper integration and functionality. Given the increas- ing complexity and scale of modern hardware designs (Ernst, 2005; Ziegler et al., 2017; Gomes et al., 2022), providing clear natural language descriptions for every part of a de- sign is a slow and challenging task, especially for globally distributed engineering teams with many different linguistic backgrounds. In this paper, we primarily focus on RTL-to- NL tasks as a key aspect of design interpretability, where machine learning models are applied to generate natural language explanations of RTL code. Applying machine learning-based natural language process- ing (NLP) methods for design interpretability could lead to significant speed-ups across the hardware design cycle. Recently, large language models (LLMs) have demonstrated effective performance at both natural language (Hendrycks et al., 2021; Xiong et al., 2023; Saba, 2024; Zhang et al., 2024a) and coding-related tasks (Tarassow, 2023; Nijkamp et al., 2023; Bairi et al., 2024; Chen et al., 2024), including code documentation (Luo et al., 2024; Nam et al., 2024).\n\n--- Segment 4 ---\nApplying machine learning-based natural language process- ing (NLP) methods for design interpretability could lead to significant speed-ups across the hardware design cycle. Recently, large language models (LLMs) have demonstrated effective performance at both natural language (Hendrycks et al., 2021; Xiong et al., 2023; Saba, 2024; Zhang et al., 2024a) and coding-related tasks (Tarassow, 2023; Nijkamp et al., 2023; Bairi et al., 2024; Chen et al., 2024), including code documentation (Luo et al., 2024; Nam et al., 2024). As interest grows in adapting LLMs for hardware design tasks (Zhang et al., 2024b; Thakur et al., 2023; Kashanaki et al., 2024), they appear poised to become a critical tool for design interpretability and the automation of RTL-to-NL tasks. In this paper, we explore the challenges and opportunities of applying machine learning to hardware design interpretabil- ity and RTL-to-NL tasks. We highlight the critical impor- tance of design interpretability and RTL-to-NL tasks in chip design cycles (Section 2). We survey the limited existing research in this area and other related topics (Section 3). We discuss outstanding dataset, computational, and model de- velopment challenges that remain unaddressed by existing work (Section 4), and identify opportunities for addressing these challenges (Section 5). Finally, we include further discussion the long-term significance of this emerging area of research (Section 6). In doing so, we hope to guide future research in using ML to automate RTL-to-NL tasks and hardware design interpretability, which in turn can greatly accelerate hardware design processes. 2. Design Interpretability and RTL-to-NL Tasks Design interpretability (not to be confused with model in- terpretability) refers to the human-readability of hardware designs, ensuring that the RTL or other design representa- tions can be easily understood, communicated, and modified by engineers. In this section, we introduce the RTL-to-NL task, and highlight the importance of design interpretability across the design cycle. Afterwards, we provide a brief out- line of the infrastructure needed to apply machine learning to this problem. 2.1.\n\n--- Segment 5 ---\nAfterwards, we provide a brief out- line of the infrastructure needed to apply machine learning to this problem. 2.1. RTL-to-NL Tasks Within the broader umbrella of design interpretability, we define RTL-to-NL tasks as tasks which require interpreting RTL designs and related artifacts (e.g., simulator logs, error messages) to produce a response in natural language. RTL designs are expressed using hardware description languages (HDLs) such as Verilog, which differ from traditional pro- gramming languages in that they model the structure of a digital circuit, including critical aspects like concurrency and timing. While it may seem initially unclear how such concepts could be accurately communicated in natural language, natural language is actually well-equipped for describing RTL. De- scribing timing information or concurrency in natural lan- guage can be accomplished by properly using phrases such as at the same time, meanwhile, or until. The fact that engineers routinely explain and teach RTL concepts in textbooks, documentation, and discussions using natural language underscores this. Simple examples of RTL-to-NL tasks would be writing doc- umentation for an undocumented RTL design, or answering a high-level question like How does this design work? A more complex version of an RTL-to-NL task might involve reasoning about specific behaviors within the design, such as answering, What control signals does this design use to ensure that data dependencies are resolved in the matrix multiplication unit? Figure 2 illustrates an example of an RTL-to-NL task, where a model (e.g., an LLM) generates a specification based on a given Verilog design. Unlike 2 ML For Hardware Design Interpretability: Challenges and Opportunities Figure 2. An example of a possible RTL-to-NL task, where the goal is to generate natural language documentation for an RTL design described in Verilog. traditional code-to-NL tasks, which describe a sequence of instructions to be executed on a CPU GPU, RTL-to-NL tasks are unique in multiple ways: 1. RTL represents hardware structure and behavior. While typical software code defines sequential algo- rithms, RTL (e.g., Verilog) directly determines the structure and behavior of a digital circuit.\n\n--- Segment 6 ---\nRTL represents hardware structure and behavior. While typical software code defines sequential algo- rithms, RTL (e.g., Verilog) directly determines the structure and behavior of a digital circuit. For example, generating a natural language description of a Verilog counter requires not only explaining its behavior (incre- menting a counter) but also describing the underlying hardware, such as the 4-bit register and adder that op- erate synchronously on each clock cycle. 2. Distinct syntactic structure of RTL. For example, while state-updating operations in software are typi- cally expressed as sequential function calls or object state modifications, in Verilog, logic is typically de- fined within always blocks. These blocks execute con- currently, independent of each other, and are triggered by specific sensitivity conditions, such as clock edges or signal changes. Syntactic structures like this must be handled separately than typical software code. 3. Understanding RTL requires foundational knowl- edge in digital logic. Reading RTL requires a funda- mental understanding of hardware design and digital logic concepts. As a result, LLMs for RTL-to-NL tasks must also possess foundational knowledge of these con- cepts. Figure 4 illustrates how typical LLMs, which are primarily trained on software code, struggle to answer basic questions that require an understanding of digital logic design. 2.2. Impact of Interpretability in the Design Cycle As hardware designs continue to grow in scale and modular- ity (Ernst, 2005; Ziegler et al., 2017; Gomes et al., 2022), understanding the vast range of interconnected subsystems has become increasingly challenging. In this context, RTL interpretability can have a significant impact on the speed of the design cycle. 2.2.1. LEGACY CODE AND IP INTEGRATION Understanding code written by others is often cited as one of the most challenging and time-consuming aspects of engi- neering projects (Sun et al., 2023b; Oliveira et al., 2025), and chip design is no exception. Modern chip designs now often consist of many smaller subcomponents (IP) developed by individual teams or external vendors (Kim et al., 2020; ipx, 2023).\n\n--- Segment 7 ---\nLEGACY CODE AND IP INTEGRATION Understanding code written by others is often cited as one of the most challenging and time-consuming aspects of engi- neering projects (Sun et al., 2023b; Oliveira et al., 2025), and chip design is no exception. Modern chip designs now often consist of many smaller subcomponents (IP) developed by individual teams or external vendors (Kim et al., 2020; ipx, 2023). When integrating IP with missing or poor documen- tation, engineers may have to reverse-engineer the module s functionality and behavior, significantly slowing down the integration process. In contrast, having well-documented IP can greatly accelerate the integration process, as evidenced by the industry-wide adoption of standardized IP description formats (ipx, 2023). 2.2.2. VERIFICATION Verification, in which a design is tested for correctness, is the most time-consuming part of hardware development, occupying up to 70 of the entire chip design cycle (Varam- bally Sehgal, 2020). Surveys have found that debugging alone can take up 44 of this process (Mutschler, 2021), meaning that up to a third of the entire design cycle may be spent on RTL-to-NL tasks for debugging, such as read- ing RTL code, simulator logs, and other design artifacts for fault localization. Importantly, time spent in debug more than doubles the time spent on any other verification task, such as running simulations (21 ), testbench development (19 ), and test planning (13 ). Misunderstandings or lack of clarity in RTL documentation can further extend the time spent on debugging. The quicker engineers can access accu- rate RTL-to-NL translations, the more time they can spend resolving actual issues. 2.2.3. ELECTRONIC DESIGN AUTOMATION Electronic Design Automation (EDA) tool developers have long sought to automate as many chip design tasks as possi- ble. However, this pursuit of automation can come at the ex- pense of human readability and interpretability of the tool s outputs. High-Level Synthesis (HLS) is one such tool, offer- ing the potential to significantly reduce the need for manual RTL coding by automatically translating a functional specifi- cation written in C into RTL.\n\n--- Segment 8 ---\nHowever, this pursuit of automation can come at the ex- pense of human readability and interpretability of the tool s outputs. High-Level Synthesis (HLS) is one such tool, offer- ing the potential to significantly reduce the need for manual RTL coding by automatically translating a functional specifi- cation written in C into RTL. However, these HLS tools pri- oritize performance over readability and maintainability, as shown in Figure 3. Variable names are often incredibly long 3 ML For Hardware Design Interpretability: Challenges and Opportunities Figure 3. An example of how hand-written Verilog may compare to the code generated by High-Level Synthesis tools. The HLS- generated code in the figure was generated by Bambu HLS (Fer- randi et al., 2021) as well as Catapult HLS (Siemens, 2024). and unintelligible, the design logic is hard to follow, and the tools do not optimize the organization of the code or de- sign hierarchy for better maintainability. Although this can cause challenges when designing for field-programmable gate arrays (FPGAs), it becomes an even greater problem when designing for large-scale fixed application-specific integrated circuits (ASICs). ASIC design flows, where the majority of designers primarily work at the RTL level, often need to meet much stricter performance, power, or area tar- gets. Since HLS tools do not guarantee optimal solutions, their generated RTL is less likely to be used in these flows, even as an initial RTL draft, because its lack of readabil- ity and RTL documentation makes any further hand-tuning nearly impossible. RTL-to-NL models could generate the necessary documentation and explanations to make the out- puts of automated design tools more understandable, thereby expanding the potential applications of these tools to accel- erate design cycles. 2.2.4. AI ALIGNMENT IN CHIP DESIGN With the rise of generative AI tools powered by large deep learning models, the need for design interpretability has become increasingly important. As these AI-driven tools become integrated into chip design workflows, maintaining human interpretability of their outputs is essential to ensure that generated designs align with engineers expectations. This is particularly crucial for preventing scenarios where an AI might introduce vulnerabilities into a design. 2.3.\n\n--- Segment 9 ---\nThis is particularly crucial for preventing scenarios where an AI might introduce vulnerabilities into a design. 2.3. Needed Infrastructure These challenges highlight the importance of developing tools which can provide automated support for RTL-to-NL tasks and design interpretability. Traditional approaches, such as rule-based systems and static analysis tools (Ba- jwa Choudhary, 2006; Chiticariu et al., 2013; Xu Cai, 2021), rely on extensive manual rule engineering and often struggle to generalize beyond narrowly defined domains. In contrast, deep learning models particularly large language models (LLMs) have demonstrated strong generalization capabilities (Kang et al., 2024; Wang et al., 2024b; Liu et al., 2024d; OpenAI, 2024) and have outperformed rule-based methods in various code analysis tasks (Lu et al., 2024a; Yin et al., 2024). Reflecting this potential, NVIDIA s inter- nal studies on hardware design workflows have identified LLM-based assistants as a promising way to boost engi- neering productivity (Liu et al., 2024a). At the same time, the widespread adoption of standardized IP formats such as IP-Xact (ipx, 2023) signals a broader industry consensus on the importance of improving design interpretability to support more efficient and scalable hardware development cycles. However, adapting models for RTL-to-NL tasks requires dedicated ML infrastructure to support training and eval- uating models on RTL-to-NL tasks, including appropriate datasets, evaluation benchmarks, fine-tuning frameworks, and more. Despite growing interest in applying LLMs to hardware design, this supporting infrastructure remains un- derdeveloped. In the next section, we examine the existing efforts to address these gaps. 3. Current Work In this section, we survey the existing research on adapting LLMs for hardware design interpretability and RTL-to-NL tasks. We also briefly summarize relevant work on related topics. 3.1. LLM for RTL-to-NL Tasks 3.1.1. DEEPRTL DeepRTL (Liu et al., 2025) presents the first dataset and benchmark for assessing LLM performance in both Ver- ilog understanding and Verilog generation. Their approach leverages GPT-4, along with professional hardware engi- neers, to annotate a dataset of Verilog designs with natural language descriptions of their functionality.\n\n--- Segment 10 ---\nDEEPRTL DeepRTL (Liu et al., 2025) presents the first dataset and benchmark for assessing LLM performance in both Ver- ilog understanding and Verilog generation. Their approach leverages GPT-4, along with professional hardware engi- neers, to annotate a dataset of Verilog designs with natural language descriptions of their functionality. The annota- tion takes place at multiple levels of granularity; starting from line-level commenting and eventually reaching high- level block and module-level descriptions. To fine-tune LLMs on their dataset, a curriculum learning approach is employed, which organizes training into sequential phases for line-level, block-level, and module-level annotations. To 4 ML For Hardware Design Interpretability: Challenges and Opportunities evaluate their dataset, they construct a benchmark of 100 Verilog designs with expert-written descriptions, measuring the embedding similarity of model-generated descriptions against the human references. Additionally, they measure a GPT Score, utilizing GPT-4 to evaluate the quality of the generated description by providing a score between 0 and 1, based on the similarity to the reference description. 3.1.2. SPECLLM SpecLLM (Li et al., 2024a) compiles a dataset of hardware architecture specifications from various open-source hard- ware projects to fine-tune LLMs for generating specifica- tions of RTL designs. Although the dataset can be very useful in many scenarios, one drawback is that it primarily consists of architecture-level specifications, which do not necessarily capture RTL-level details. As a result, the im- provement in the model s understanding of RTL through this dataset may be limited, as it appears to focus more on architecture standards and specification formatting. The paper does not provide any quantitative evaluation to assess the model s performance after fine-tuning. 3.1.3. CHIPNEMO NVIDIA s ChipNeMo (Liu et al., 2024a) is a general- purpose hardware design assistant LLM, trained on a di- verse set of proprietary chip-design documents, including RTL code, specifications, and documentation. The model in- corporates techniques like domain-specific tokenization and retrieval-augmented generation (RAG) to enhance its effi- ciency.\n\n--- Segment 11 ---\nCHIPNEMO NVIDIA s ChipNeMo (Liu et al., 2024a) is a general- purpose hardware design assistant LLM, trained on a di- verse set of proprietary chip-design documents, including RTL code, specifications, and documentation. The model in- corporates techniques like domain-specific tokenization and retrieval-augmented generation (RAG) to enhance its effi- ciency. While the paper does not specifically evaluate its per- formance on RTL-to-NL tasks, its broad (albeit proprietary) training dataset and general-purpose design make it relevant for inclusion here. Furthermore, the evaluation benchmark details are limited, with much of the assessment relying on expert grading. 3.2. LLM for NL-to-RTL Tasks While the adaptation of LLMs for RTL-to-NL tasks and hardware design interpretability remains underexplored, adapting LLMs for NL-to-RTL tasks, such as Verilog generation, has been the dominant focus of prior work (Liu et al., 2023; Lu et al., 2023; Allam Shalan, 2024; Vun- garala et al., 2024; Zhang et al., 2024b; Thakur et al., 2023; Kashanaki et al., 2024; Sandal Akturk, 2024; DeLorenzo et al., 2024a;b). A number of works have released datasets and benchmarks to assess LLM-generated Verilog designs, including RTLCoder (Liu et al., 2024c), VerilogEval (Liu et al., 2023) (benchmark only), RTLLM (Lu et al., 2023), and more (Thakur et al., 2023; Chang et al., 2024; Zhang et al., 2024b). These works primarily draw upon open- source RTL designs from platforms like GitHub and HDL- Bits (HDLBits, 2024) to construct their datasets. Other areas of exploration include generating SystemVerilog ver- ification constructs (Sun et al., 2023a), designing in HLS (Huang et al., 2024b), EDA tool script generation (Liu et al., 2024a), and applying agent-based approaches to hardware design (Huang et al., 2024a). 3.3.\n\n--- Segment 12 ---\nOther areas of exploration include generating SystemVerilog ver- ification constructs (Sun et al., 2023a), designing in HLS (Huang et al., 2024b), EDA tool script generation (Liu et al., 2024a), and applying agent-based approaches to hardware design (Huang et al., 2024a). 3.3. LLM for Coding Outside of hardware design, a significant amount of work has focused on adapting LLMs for coding in more widely used programming languages. While it is beyond the scope of this paper to examine all approaches, one relevant tech- nique involves custom attention mechanisms that incorpo- rate abstract syntax trees (ASTs) and language-specific in- formation (Ahmad et al., 2020; Wang et al., 2021; Guo et al., 2021), which could similarly be extended for LLMs adapted for Verilog processing. Additionally, we discuss later in Section 4.4 why approaches to adapting LLMs for software coding may not necessarily translate to the realm of hardware design. 4. Challenges In this section, we examine outstanding challenges in adapt- ing LLMs for RTL-to-NL tasks which have yet to be fully addressed by existing work. Language Number of Files Java 155,000,000 C 104,000,000 Python 95,400,000 Go 24,100,000 Verilog 1,200,000 SystemVerilog 465,000 Table 1. The number of files on GitHub categorized by program- ming language, highlighting the disparity between HDLs and pop- ular software languages such as Java and Python. 4.1. Dataset Construction The most critical gap limiting research on adapting LLMs to RTL-to-NL tasks has been the absence of datasets of Verilog code with corresponding natural language labels, such as design specifications or question-answer pairs. Assembling such datasets is challenging due to the competitive nature of the industry and the smaller number of chip designers com- pared to software engineers, leading to a lack of open-source RTL designs to use for dataset construction. As shown in Table 4, the total number of Verilog and SystemVerilog files on GitHub is approximately 93 times smaller than the num- ber of Java files.\n\n--- Segment 13 ---\nAssembling such datasets is challenging due to the competitive nature of the industry and the smaller number of chip designers com- pared to software engineers, leading to a lack of open-source RTL designs to use for dataset construction. As shown in Table 4, the total number of Verilog and SystemVerilog files on GitHub is approximately 93 times smaller than the num- ber of Java files. Additionally, the time required to manually label data and the expertise needed for proper labeling fur- ther prevent the use of data annotation platforms (Wang, 5 ML For Hardware Design Interpretability: Challenges and Opportunities 2024), which could otherwise expedite the creation of these datasets. While some progress has been made in automating RTL dataset creation by using publically available RTL designs from GitHub (Thakur et al., 2023), many of the sources that provide reliably structured or labeled data, such as HDLBits (HDLBits, 2024), mainly feature basic problems that do not reflect a broader set of real-world designs. As such, recent works on NL-to-RTL (Liu et al., 2024c) and RTL-to-NL (Liu et al., 2025) tasks tend to heavily rely on synthetic data to compensate for the lack of high-quality, labeled examples. 4.2. Evaluating RTL-to-NL Accuracy While NL-to-RTL tasks can be evaluated by running LLM- generated code against human-written test suites to verify functional correctness, evaluating RTL-to-NL tasks is less straightforward. Natural language descriptions or specifi- cations of hardware must convey critical implementation details, where even minor omissions or inaccuracies can significantly distort meaning. Even when data is available, research on RTL-to-NL tasks still lacks robust approaches for accurately evaluating the quality of natural language documentation. Existing reference-based measures such as BLEU or ROUGE, which target lexical similarity, could give misleadingly high scores to critically different descrip- tions simply due to lexical overlap. Similarly, embedding similarity can compare if an LLM-generated description is semantically close (i.e., uses many semantically related words, like clock, reset, etc. ), but cannot penalize omis- sions or vague descriptions that could lead to serious im- plementation mismatches.\n\n--- Segment 14 ---\nSimilarly, embedding similarity can compare if an LLM-generated description is semantically close (i.e., uses many semantically related words, like clock, reset, etc. ), but cannot penalize omis- sions or vague descriptions that could lead to serious im- plementation mismatches. An alternative way to evaluate accuracy could be through expert grading, but this approach is difficult to scale due to the specialized knowledge re- quired and the inherent subjectivity of graders. Methods like GPT-scoring, such as the one used by (Liu et al., 2025), are also problematic, as pointwise scoring with LLMs is challenging to calibrate (Desai Durrett, 2020) and tends to be computationally expensive. 4.3. Computational Challenges Adapting LLMs to process RTL designs also presents a computational challenge, as representing hardware logic in RTL often requires substantially more lines of code and consequently, greater context length compared to describ- ing equivalent functionality in high-level languages like C. Table 2 shows that while algorithms are relatively simple to describe in C, their RTL implementations require signifi- cantly more code. To make this comparison, we retrieved corresponding C and Verilog implementations of some well- known algorithms from GitHub. Additionally, we ran the C implementations through Bambu HLS (Ferrandi et al., 2021) to generate RTL, which is shown in the Verilog (HLS) col- umn for comparison. We observe that hand-written Verilog implementations can require more than triple the number of tokens compared to their C counterparts. Additionally, the length of a single IP generated by HLS can exceed the limits of even commercially deployed models that are opti- mized for long contexts, such as LLaMa-3 (Grattafiori et al., 2024) (up to 128,000 tokens), GPT-4 (OpenAI, 2024) (up to 128,000 tokens), and Claude 3 (Anthropic, 2024) (up to 200,000 tokens). These context limitations make it clear that scaling to larger designs, which incorporate many different IPs, can quickly become computationally prohibitive.\n\n--- Segment 15 ---\nAdditionally, the length of a single IP generated by HLS can exceed the limits of even commercially deployed models that are opti- mized for long contexts, such as LLaMa-3 (Grattafiori et al., 2024) (up to 128,000 tokens), GPT-4 (OpenAI, 2024) (up to 128,000 tokens), and Claude 3 (Anthropic, 2024) (up to 200,000 tokens). These context limitations make it clear that scaling to larger designs, which incorporate many different IPs, can quickly become computationally prohibitive. Algorithm Number of Tokens C Verilog Verilog (HLS) GEMM 252 18423 224289 Sobel 557 2031 39849 BellmanFord 1440 5156 47335 SHA256 2683 9290 260356 Table 2. The token counts for implementing a variety of algorithms in C and Verilog. The Verilog (HLS) column represents Verilog code generated by Bambu HLS from the corresponding C program. Token counts were obtained using the GPT-4 tokenizer (OpenAI, 2024). Moreover, LLMs are known to suffer performance degrada- tion when handling long context lengths (Li et al., 2024b; An et al., 2024; Laban et al., 2024), and exhibit a tendency to lose focus on middle-context information (Liu et al., 2024b). They also demonstrate a tendency to hallucinate , gener- ating outputs that may appear coherent but are nonsensical or inaccurate (Ji et al., 2023; Friel Sanyal, 2023). The effects of these properties can be particularly detrimental for interpreting RTL, where tracking the code s structure and dependencies is essential for correctly understanding the de- sign and ensuring accurate interpretation of its functionality. The current performance and reliability challenges of LLMs, particularly when dealing with long contexts, make research in adapting LLMs for RTL-to-NL tasks more challenging, as it would be difficult to demonstrate their applicability to real-world RTL development, where designs may span hundreds of thousands of tokens. Critically, many of the cur- rently existing benchmarks for both Verilog generation and Verilog understanding (Chang et al., 2024; Liu et al., 2025) do not include designs exceeding 2048 tokens in length. 4.4.\n\n--- Segment 16 ---\nCritically, many of the cur- rently existing benchmarks for both Verilog generation and Verilog understanding (Chang et al., 2024; Liu et al., 2025) do not include designs exceeding 2048 tokens in length. 4.4. Training Learning Challenges As we previously discuss in section 2.1 HDLs, such as Ver- ilog, are fundamentally different from traditional imperative programming languages, as they are used to describe the structure of digital circuits, rather than defining sequential behavior. This poses unique challenges for LLMs, includ- ing those optimized for coding (Rozi ere et al., 2024), as 6 ML For Hardware Design Interpretability: Challenges and Opportunities these models have been primarily trained on sequential pro- gramming languages. Such a model may misinterpret a design s concurrency, assuming two components execute sequentially in the order they appear in the code. RTL code also involves underlying digital design concepts that are rarely explicitly documented within the code itself but can be essential for understanding the design s perfor- mance and functionality. Critical path timing, circuit area, and the effects of propagation delays are crucial to accurate design analysis but are not explicitly represented in Verilog. In hardware design, different logic components exhibit vary- ing delay characteristics. For instance, an adder generally has higher delay than a basic AND gate due to the addi- tional gates involved in carry propagation. Now consider a scenario where a model trained on traditional software- oriented code is asked to document the critical path in a Verilog design. Without an understanding of component- specific delays, such a model might naively identify the longest dependency chain as the critical path, focusing solely on operation count. In doing so, it could miss a shorter path that includes higher-delay components, such as adders, which ultimately defines the actual critical path. Figure 4. We asked GPT4o-mini to evaluate the critical path of simple Verilog modules. This figure shows how the model will give incorrect or inconsistent answers. For this example, the path leading to f will have a longer critical path due to carry logic necessary for the adders. Figure 4 shows how GPT 4o-mini can make this exact mis- take when asked to identify the critical path in a simple Verilog design.\n\n--- Segment 17 ---\nFor this example, the path leading to f will have a longer critical path due to carry logic necessary for the adders. Figure 4 shows how GPT 4o-mini can make this exact mis- take when asked to identify the critical path in a simple Verilog design. In the figure, GPT selected the path lead- ing to e, likely because it contained more operations than the path leading to f, even though the latter has a longer critical path due to the carry logic involved in the adders. Importantly, RTL code that requires the most interpretation is often poorly organized, featuring inconsistent or poorly named variables that do not align with human-readable con- ventions. As a result, adapting LLMs for RTL-to-NL tasks requires training approaches that not only enable the model to learn the unique structure of the HDLs used to describe RTL designs and underlying hardware design concepts, but also enable it to interpret poorly organized RTL code that lacks meaningful semantic clues. 5. Research Opportunities The challenges outlined in the previous section present many opportunities where new research can help provide the crit- ical ML infrastructure outlined in Section 2.3. This in- frastructure could catalyze greater research participation towards adapting LLMs for RTL-to-NL tasks, creating new tools that can drastically improve the efficiency of hardware development. 5.1. Approaches for Dataset Construction Datasets are the foundational infrastructure needed to adapt LLMs for RTL-to-NL tasks. However, creating task-specific RTL-to-NL datasets is complex and time-consuming, par- ticularly when manual labeling is required, which makes it challenging for most research groups to undertake. There- fore, innovative approaches to automated dataset generation are essential. Recent works on NL-to-RTL (Liu et al., 2024c) and RTL-to-NL (Liu et al., 2025) tasks have started to ex- plore synthetic data as a solution to the lack of high-quality labeled examples. While synthetic data has proven effec- tive in these works, as well as in other domains (Figueira Vaz, 2022; Lu et al., 2024b; OpenAI, 2024), it is not guaranteed to be effective, and further exploration is needed to determine how such methods could be successfully ap- plied for RTL-to-NL tasks.\n\n--- Segment 18 ---\nRecent works on NL-to-RTL (Liu et al., 2024c) and RTL-to-NL (Liu et al., 2025) tasks have started to ex- plore synthetic data as a solution to the lack of high-quality labeled examples. While synthetic data has proven effec- tive in these works, as well as in other domains (Figueira Vaz, 2022; Lu et al., 2024b; OpenAI, 2024), it is not guaranteed to be effective, and further exploration is needed to determine how such methods could be successfully ap- plied for RTL-to-NL tasks. One potential approach could integrate HLS or RTL synthesis tools with LLMs like GPT to automatically generate and label question-answer pairs on critical RTL design aspects, such as concurrency. Furthermore, there is still significant potential for community-driven efforts to create and expand RTL-to-NL datasets. The newly established IEEE International Confer- ence on LLM-Aided Design (ieee, 2025) features a datasets track, which could help bring together researchers interested in advancing RTL-to-NL datasets through collaboration. 5.2. New Approaches for Evaluation As discussed in Section 4.2, evaluating RTL-to-NL accuracy is a nontrivial problem where existing metrics are likely inadequate. One potential approach to evaluating LLM- generated explanations is to have another LLM regenerate the same RTL design based on the explanation, followed by logical equivalence checking (LEC) to compare the regen- erated design with the original. This approach is inspired by the concept of Round-Trip Correctness (Allamanis et al., 2024), where an LLM generates a natural language descrip- tion of a subset of code, then re-implements the code in the target language. In the case of RTL-to-NL, a separate LLM with adequate Verilog skills could be used to re-implement 7 ML For Hardware Design Interpretability: Challenges and Opportunities the design. This approach could provide a better signal of whether critical design details are accurately captured in natural language, reducing the reliance on human-written ground truth references. However, challenges remain, such as distinguishing between hallucinations coding errors and inaccuracies in the description.\n\n--- Segment 19 ---\nThis approach could provide a better signal of whether critical design details are accurately captured in natural language, reducing the reliance on human-written ground truth references. However, challenges remain, such as distinguishing between hallucinations coding errors and inaccuracies in the description. Another approach could explore distilling expert grading into a BERT model to ap- proximate expert judgment, similar to the COMET model used in machine translation (Rei et al., 2020). 5.3. Model Architectures for Handling Long Contexts of RTL Efficiently processing long context lengths is crucial for any LLM dealing with extensive RTL code. As outlined in Section 4.3, RTL designs often involve large context spans, making it challenging for models to capture dependencies across the entire design hierarchy. One approach to address- ing this challenge is modifying attention mechanisms to take advantage of the hierarchical and modular nature of RTL, similar to techniques used in code-specific transformer mod- els (Ahmad et al., 2020; Wang et al., 2021; Guo et al., 2021). These models incorporate structural information about the code directly into the attention function. Such modifica- tions could enhance recall over long contexts by focusing on relevant signal dependencies and module interactions, ultimately improving the model s ability to handle complex RTL designs. Additionally, RTL code often contains highly repetitive patterns that require less processing effort, such as the statement always (posedge clk) begin, which appears in every Verilog module involving sequential logic. While it breaks into six tokens using the GPT-4o tok- enizer, its semantic contribution is minimal. Alternative tokenization methods, like Byte-Latent Transformers (BLTs) (Pagnoni et al., 2024), adaptively allocate compute to more informative parts of the sequence, improving efficiency. A custom tokenizer tailored to RTL text data could also re- duce context length by condensing repetitive constructs into single tokens. For example, always (posedge clk) begin could be reduced to one token. NVIDIA s Chip- Nemo (Liu et al., 2024a) incorporated a custom tokenizer which demonstrated modest improvements in tokenization efficiency (1.6 to 3.3 ) on their internal datasets, though public data saw little to no gain.\n\n--- Segment 20 ---\nFor example, always (posedge clk) begin could be reduced to one token. NVIDIA s Chip- Nemo (Liu et al., 2024a) incorporated a custom tokenizer which demonstrated modest improvements in tokenization efficiency (1.6 to 3.3 ) on their internal datasets, though public data saw little to no gain. While not strictly a model architecture, domain-specific retrieval-augmented generation (RAG) techniques (Liu et al., 2024a) could also help alleviate the burden of long con- text lengths. By indexing RTL submodules and retrieving only relevant portions of a design during inference, RAG approaches could improve scalability and performance, par- ticularly when dealing with large IP libraries or system- on-chip (SoC) designs. Similarly, another strategy could involve using module embeddings, analogous to document embeddings, to capture the functionality of Verilog submod- ules in compact vector representations. This could reduce the need to include all submodule definitions in the context, allowing the LLM to operate at a higher level of abstraction. Code embeddings, a well-studied topic (Chen Monperrus, 2019), could be key in enabling this approach. 5.4. LLM-Aware EDA Tools As discussed in Section 2.1 and shown in Figure 3, the RTL code generated by EDA tools, such as HLS, often contains bloat and readability issues that can cause problems for both human designers and LLMs. Code generation tools that enhance readability by producing semantically meaningful variable names, structured hierarchies, and more intuitive representations could significantly improve both human understanding and make the code easier for LLMs to process. Developers of automation tools may also benefit from recog- nizing how the intermediate data their tools generate often discarded after fulfilling its primary function could serve as a rich source of training data for LLMs. HLS tools primar- ily generate Verilog but internally represent designs using graph-based abstractions that guide the scheduling of opera- tions, resolution of data dependencies, and optimization of the hardware datapath. These representations support the step-by-step transformation of functional algorithms into hardware implementations.\n\n--- Segment 21 ---\nHLS tools primar- ily generate Verilog but internally represent designs using graph-based abstractions that guide the scheduling of opera- tions, resolution of data dependencies, and optimization of the hardware datapath. These representations support the step-by-step transformation of functional algorithms into hardware implementations. For example, Bambu HLS (Fer- randi et al., 2021) provides a --print-dot option that lets users visualize these graphs, offering insights about the design that are not easily visible in the Verilog output. Traditionally, this intermediate data is discarded after com- pilation, but it could be repurposed to train LLMs, enabling them to reason through higher-level hardware design chal- lenges. This approach is aligned with Chain-of-Thought reasoning research (Wang et al., 2023a; Chu et al., 2024), where models enhance performance by breaking down com- plex problems into intermediate steps. 6. Further Discussion Given the ongoing uncertainty about how rapidly advancing AI systems will affect the workforce and human roles, we believe it is important to discuss the long-term relevance and potential impact of research on RTL-to-NL tasks and hardware design interpretability in this evolving landscape. 6.1. Would Fully Autonomous Hardware Design Eliminate the Need for Design Interpretability? One may argue that if AI and EDA tools eventually surpass human capabilities across all stages of the design cycle, then making these designs human-interpretable would become 8 ML For Hardware Design Interpretability: Challenges and Opportunities unnecessary. However, while advanced EDA tools have made significant strides, the problem sizes they address are so vast (Alazemi et al., 2018; Noonan, 2024; Zhong et al., 2023) that achieving globally optimal designs is intractable with current technology. Although deep learning-driven EDA tools have demonstrated substantial improvements in many areas (Hafen Critchlow, 2013; Mirhoseini et al., 2021; Goswami Bhatia, 2023), their models still face significant challenges in being able to produce formal guar- antees for correct optimal output, as verifying even sim- ple neural networks is shown to be NP-hard (Katz et al., 2017).\n\n--- Segment 22 ---\nHowever, while advanced EDA tools have made significant strides, the problem sizes they address are so vast (Alazemi et al., 2018; Noonan, 2024; Zhong et al., 2023) that achieving globally optimal designs is intractable with current technology. Although deep learning-driven EDA tools have demonstrated substantial improvements in many areas (Hafen Critchlow, 2013; Mirhoseini et al., 2021; Goswami Bhatia, 2023), their models still face significant challenges in being able to produce formal guar- antees for correct optimal output, as verifying even sim- ple neural networks is shown to be NP-hard (Katz et al., 2017). As a result, human involvement, including RTL-to- NL tasks, will likely remain essential, even if it eventually serves primarily to provide oversight of AI-generated out- puts. Moreover, as discussed in Section 2.2.4, ensuring design interpretability will be crucial for aligning AI with human engineers intentions. 6.2. Is Design Interpretability a Human Problem? Another question to raise is that the problems addressed by RTL-to-NL tasks may be fundamentally human prob- lems and are best solved through better organizational and management strategies, rather then creating complex and ex- pensive ML solutions, which may do nothing but exacerbate existing problems. For example, knowledge silos across teams and poor documentation for IPs are problems that a design group or organization could resolve by enforcing stricter documentation practices. While organizational and management improvements are certainly important, this overlooks the limitations of such ap- proaches as hardware design projects grow in scale and com- plexity. As teams grow in size and individual engineers take on larger and more intricate subsystems, it becomes increas- ingly difficult for teams to enforce comprehensive documen- tation and rigorous standards without micromanagement. Additionally, manually writing high-quality documentation will always require significant man-hours, which can be alleviated through automation with LLMs (Dvivedi et al., 2024). As discussed in Section 2.1, LLMs offer notable advantages in generalizability and performance compared to methods like rule-based systems, making them a fitting and practical solution for automating design interpretability. 7. Conclusion The growing computational demands of modern ML models have highlighted the need for custom ML hardware.\n\n--- Segment 23 ---\n7. Conclusion The growing computational demands of modern ML models have highlighted the need for custom ML hardware. How- ever, we find that the hardware design cycle remains heavily dependent on engineers manually ensuring design inter- pretability through extensive documentation and commu- nication, creating a significant bottleneck. Leveraging ML to address this challenge presents a promising opportunity. In this paper, we examine existing research on this topic and identify the computational and learning challenges that remain unresolved. We then discuss how these challenges open new research avenues that future research in this area can address. Critically, progress in adapting LLMs for RTL- to-NL tasks and enhancing hardware design interpretability will not only streamline the hardware development process but could also contribute to broader advancements in ma- chine learning techniques with applications across multiple research domains. References Ieee standard for ip-xact, standard structure for packaging, integrating, and reusing ip within tool flows. IEEE Std 1685-2022 (Revision of IEEE Std 1685-2014), pp. 1 750, 2023. doi: 10.1109 IEEESTD.2023.10054520. Agostinelli, V., Hong, S., and Chen, L. Leapformer: Enabling linear transformers for autoregressive and si- multaneous tasks via learned proportions. In Forty- first International Conference on Machine Learning, 2024. URL id XhH1OKLANY. Ahmad, W., Chakraborty, S., Ray, B., and Chang, K.-W. A transformer-based approach for source code summa- rization. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds. ), Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pp. 4998 5007, Online, July 2020. Association for Compu- tational Linguistics. doi: 10.18653 v1 2020.acl-main. 449. URL acl-main.449 . Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. GQA: Training generalized multi-query transformer models from multi-head check- points.\n\n--- Segment 24 ---\nURL acl-main.449 . Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. GQA: Training generalized multi-query transformer models from multi-head check- points. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https: openreview.net forum?id hmOwOZWzYE. Alazemi, F., AziziMazreah, A., Bose, B., and Chen, L. Routerless network-on-chip. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 492 503, 2018. doi: 10.1109 HPCA.2018. 00049. Allam, A. and Shalan, M. Rtl-repo: A benchmark for evaluating llms on large-scale rtl design projects, 2024. Allamanis, M., Panthaplackel, S., and Yin, P. Unsupervised evaluation of code llms with round-trip correctness, 2024. URL An, C., Zhang, J., Zhong, M., Li, L., Gong, S., Luo, Y., Xu, J., and Kong, L. Why does the effective context length of 9 ML For Hardware Design Interpretability: Challenges and Opportunities llms fall short?, 2024. URL abs 2410.18745. Anthropic. The claude 3 model family: Opus, son- net, haiku, 2024. URL 2303.08774. Bairi, R., Sonwane, A., Kanade, A., C., V. D., Iyer, A., Parthasarathy, S., Rajamani, S., Ashok, B., and Shet, S. Codeplan: Repository-level coding using llms and planning. Proc. ACM Softw. Eng., 1(FSE), July 2024. doi: 10.1145 3643757. URL 1145 3643757. Bajwa, I. and Choudhary, M. A rule based system for speech language context understanding. 23, 12 2006.\n\n--- Segment 25 ---\nBajwa, I. and Choudhary, M. A rule based system for speech language context understanding. 23, 12 2006. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020. URL https: arxiv.org abs 2005.14165. Chang, K., Chen, Z., Zhou, Y., Zhu, W., kun wang, Xu, H., Li, C., Wang, M., Liang, S., Li, H., Han, Y., and Wang, Y. Natural language is not enough: Benchmarking multi- modal generative ai for verilog generation, 2024. URL Chen, J., Li, Z., Hu, X., and Xia, X. Nlperturbator: Studying the robustness of code llms to natural language variations, 2024. URL 2406.19783. Chen, W., Ray, S., Bhadra, J., Abadir, M., and Wang, L.-C. Challenges and trends in modern soc design verification. IEEE Design and Test, 34(5):7 22, 2017a. doi: 10.1109 MDAT.2017.2735383. Chen, Y.-H., Krishna, T., Emer, J. S., and Sze, V. Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks. IEEE Journal of Solid- State Circuits, 52(1):127 138, 2017b. doi: 10.1109 JSSC. 2016.2616357.\n\n--- Segment 26 ---\ndoi: 10.1109 JSSC. 2016.2616357. Chen, Z. and Monperrus, M. A literature study of embed- dings on source code. arXiv preprint arXiv:1904.03061, 2019. Chiticariu, L., Li, Y., and Reiss, F. R. Rule-based infor- mation extraction is dead! long live rule-based infor- mation extraction systems! In Yarowsky, D., Bald- win, T., Korhonen, A., Livescu, K., and Bethard, S. (eds. ), Proceedings of the 2013 Conference on Empir- ical Methods in Natural Language Processing, pp. 827 832, Seattle, Washington, USA, October 2013. Asso- ciation for Computational Linguistics. URL https: aclanthology.org D13-1079 . Chu, Z., Chen, J., Chen, Q., Yu, W., He, T., Wang, H., Peng, W., Liu, M., Qin, B., and Liu, T. Navigate through enigmatic labyrinth a survey of chain of thought rea- soning: Advances, frontiers and future, 2024. URL DeepSeek-AI. Deepseek-r1: Incentivizing reasoning ca- pability in llms via reinforcement learning, 2025. URL DeLorenzo, M., Chowdhury, A. B., Gohil, V., Thakur, S., Karri, R., Garg, S., and Rajendran, J. Make every move count: Llm-based high-quality rtl code generation us- ing mcts, 2024a. URL 2402.03289. DeLorenzo, M., Gohil, V., and Rajendran, J. Creativeval: Evaluating creativity of llm-based hardware code gen- eration, 2024b. URL 2404.08806. Desai, S. and Durrett, G. Calibration of pre-trained trans- formers, 2020. URL 2003.07892. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022. URL 2208.07339.\n\n--- Segment 27 ---\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Llm.int8(): 8-bit matrix multiplication for transformers at scale, 2022. URL 2208.07339. Dvivedi, S. S., Vijay, V., Pujari, S. L. R., Lodh, S., and Kumar, D. A comparative analysis of large lan- guage models for code documentation generation. In Proceedings of the 1st ACM International Conference on AI-Powered Software, AIware 2024, pp. 65 73, New York, NY, USA, 2024. Association for Comput- ing Machinery. ISBN 9798400706851. doi: 10.1145 3664646.3664765. URL 1145 3664646.3664765. Ernst, D. Limits to modularity: Reflections on re- cent developments in chip design. Industry and Innovation, 12(3):303 335, 2005. doi: 10.1080 13662710500195918. URL 1080 13662710500195918. Fan , A., Stock , P., , Graham, B., Grave, E., Gribonval, R., Jegou, H., and Joulin, A. Training with quantization noise for extreme model compression. 2020. 10 ML For Hardware Design Interpretability: Challenges and Opportunities Ferrandi, F., Castellana, V. G., Curzel, S., Fezzardi, P., Fiorito, M., Lattuada, M., Minutoli, M., Pilato, C., and Tumeo, A. Invited: Bambu: an open-source research framework for the high-level synthesis of complex ap- plications. In 2021 58th ACM IEEE Design Automa- tion Conference (DAC), pp. 1327 1330, 2021. doi: 10.1109 DAC18074.2021.9586110. Figueira, A. and Vaz, B. Survey on synthetic data genera- tion, evaluation methods and gans. Mathematics, 10(15), 2022. ISSN 2227-7390. doi: 10.3390 math10152733. URL 15 2733.\n\n--- Segment 28 ---\nISSN 2227-7390. doi: 10.3390 math10152733. URL 15 2733. Friel, R. and Sanyal, A. Chainpoll: A high efficacy method for llm hallucination detection, 2023. URL https: arxiv.org abs 2310.18344. Gomes, W., Koker, A., Stover, P., Ingerly, D., Siers, S., Venkataraman, S., Pelto, C., Shah, T., Rao, A., O Mahony, F., Karl, E., Cheney, L., Rajwani, I., Jain, H., Cortez, R., Chandrasekhar, A., Kanthi, B., and Koduri, R. Ponte vecchio: A multi-tile 3d stacked processor for exascale computing. In 2022 IEEE International Solid-State Cir- cuits Conference (ISSCC), volume 65, pp. 42 44, 2022. doi: 10.1109 ISSCC42614.2022.9731673. Goswami, P. and Bhatia, D. Application of machine learning in fpga eda tool development. IEEE Access, 11:109564 109580, 2023. doi: 10.1109 ACCESS.2023.3322358. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., and et. al. The llama 3 herd of models, 2024. URL org abs 2407.21783. Guo, D., Ren, S., Lu, S., Feng, Z., Tang, D., LIU, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S. K., Clement, C., Drain, D., Sundaresan, N., Yin, J., Jiang, D., and Zhou, M. Graphcode{bert}: Pre-training code representations with data flow. In International Conference on Learning Representations, 2021.\n\n--- Segment 29 ---\nGuo, D., Ren, S., Lu, S., Feng, Z., Tang, D., LIU, S., Zhou, L., Duan, N., Svyatkovskiy, A., Fu, S., Tufano, M., Deng, S. K., Clement, C., Drain, D., Sundaresan, N., Yin, J., Jiang, D., and Zhou, M. Graphcode{bert}: Pre-training code representations with data flow. In International Conference on Learning Representations, 2021. URL id jLoC4ez43PZ. Hafen, R. and Critchlow, T. Eda and ml a perfect pair for large-scale data analysis. In 2013 IEEE Interna- tional Symposium on Parallel and Distributed Processing, Workshops and Phd Forum, pp. 1894 1898, 2013. doi: 10.1109 IPDPSW.2013.118. HDLBits. Hdlbits: Verilog practice, 2024. URL https: hdlbits.01xz.net wiki Main_Page. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https: openreview.net forum?id d7KBjmI3GmQ. Huang, H., Lin, Z., Wang, Z., Chen, X., Ding, K., and Zhao, J. Towards llm-powered verilog rtl assistant: Self-verification and self-correction. arXiv preprint arXiv:2406.00115, 2024a. Huang, Y., Wan, L. J., Ye, H., Jha, M., Wang, J., Li, Y., Zhang, X., and Chen, D. New solutions on llm accelera- tion, optimization, and application. In Proceedings of the 61st ACM IEEE Design Automation Conference, pp. 1 4, 2024b. ieee. Ieee international conference on llm-aided design, 2025. URL Ji, Z., Yu, T., Xu, Y., Lee, N., Ishii, E., and Fung, P. Towards mitigating LLM hallucination via self reflection.\n\n--- Segment 30 ---\nIeee international conference on llm-aided design, 2025. URL Ji, Z., Yu, T., Xu, Y., Lee, N., Ishii, E., and Fung, P. Towards mitigating LLM hallucination via self reflection. In Bouamor, H., Pino, J., and Bali, K. (eds. ), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1827 1843, Singa- pore, December 2023. Association for Computational Linguistics. doi: 10.18653 v1 2023.findings-emnlp. 123. URL findings-emnlp.123 . Kang, K., Setlur, A., Ghosh, D., Steinhardt, J., Tomlin, C., Levine, S., and Kumar, A. What do learning dynamics reveal about generalization in llm reasoning?, 2024. URL Kashanaki, F. R., Zakharov, M., and Renau, J. HDLEval: Benchmarking LLMs for Multiple HDLs. In The First IEEE International Workshop on LLM-Aided Design (IS- LAD), July 2024. Katz, G., Barrett, C., Dill, D. L., Julian, K., and Kochender- fer, M. J. Reluplex: An efficient smt solver for verifying deep neural networks. In Computer Aided Verification: 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30, pp. 97 117. Springer, 2017. Kim, J., Murali, G., Park, H., Qin, E., Kwon, H., Chekuri, V. C. K., Rahman, N. M., Dasari, N., Singh, A. K., Lee, M., Torun, H. M., Roy, K., Swami- nathan, M., Mukhopadhyay, S., Krishna, T., and Lim, S. K. Architecture, chip, and package codesign flow for interposer-based 2.5-d chiplet integration enabling heterogeneous ip reuse. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 28:2424 2437, 2020. URL org CorpusID:225079697.\n\n--- Segment 31 ---\nIEEE Transactions on Very Large Scale Integration (VLSI) Systems, 28:2424 2437, 2020. URL org CorpusID:225079697. 11 ML For Hardware Design Interpretability: Challenges and Opportunities Laban, P., Fabbri, A., Xiong, C., and Wu, C.-S. Summary of a haystack: A challenge to long-context LLMs and RAG systems. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds. ), Proceedings of the 2024 Conference on Empiri- cal Methods in Natural Language Processing, pp. 9885 9903, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653 v1 2024. emnlp-main.552. URL org 2024.emnlp-main.552 . Li, M., Fang, W., Zhang, Q., and Xie, Z. Specllm: Exploring generation and review of vlsi design specification with large language model, 2024a. URL org abs 2401.13266. Li, T., Zhang, G., Do, Q. D., Yue, X., and Chen, W. Long- context llms struggle with long in-context learning. CoRR, abs 2404.02060, 2024b. URL 10.48550 arXiv.2404.02060. Liu, M., Pinckney, N., Khailany, B., and Ren, H. Verilo- geval: Evaluating large language models for verilog code generation, 2023. URL 2309.07544. Liu, M., Ene, T.-D., Kirby, R., Cheng, C., Pinckney, N., Liang, R., Alben, J., Anand, H., and et. al. Chipnemo: Domain-adapted llms for chip design, 2024a. URL Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157 173, 2024b. doi: 10.1162 tacl a 00638.\n\n--- Segment 32 ---\nTransactions of the Association for Computational Linguistics, 12:157 173, 2024b. doi: 10.1162 tacl a 00638. URL https: aclanthology.org 2024.tacl-1.9 . Liu, S., Fang, W., Lu, Y., Zhang, Q., Zhang, H., and Xie, Z. Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution. In 2024 IEEE LLM Aided Design Workshop (LAD), pp. 1 5. IEEE, 2024c. Liu, Y., Meng, Y., Wu, F., Peng, S., Yao, H., Guan, C., Tang, C., Ma, X., Wang, Z., and Zhu, W. Evaluating the generalization ability of quantized llms: Benchmark, analysis, and toolbox, 2024d. URL org abs 2406.12928. Liu, Y., Xu, C., Zhou, Y., Li, Z., and Xu, Q. Deeprtl: Bridging verilog understanding and generation with a unified representation model, 2025. URL https: arxiv.org abs 2502.15832. Lu, G., Ju, X., Chen, X., Pei, W., and Cai, Z. Grace: Empowering llm-based software vulnerability detection with graph structure and in-context learning. Journal of Systems and Software, 212:112031, 2024a. ISSN 0164- 1212. doi: URL science article pii S0164121224000748. Lu, Y., Liu, S., Zhang, Q., and Xie, Z. Rtllm: An open- source benchmark for design rtl generation with large language model, 2023. URL abs 2308.05345. Lu, Y., Shen, M., Wang, H., Wang, X., van Rechem, C., Fu, T., and Wei, W. Machine learning for synthetic data generation: A review, 2024b. URL org abs 2302.04062. Luo, Q., Ye, Y., Liang, S., Zhang, Z., Qin, Y., Lu, Y., Wu, Y., Cong, X., Lin, Y., Zhang, Y., et al.\n\n--- Segment 33 ---\nURL org abs 2302.04062. Luo, Q., Ye, Y., Liang, S., Zhang, Z., Qin, Y., Lu, Y., Wu, Y., Cong, X., Lin, Y., Zhang, Y., et al. Repoagent: An llm-powered open-source framework for repository- level code documentation generation. arXiv preprint arXiv:2402.16667, 2024. Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., and Wei, F. The era of 1-bit llms: All large language models are in 1.58 bits, 2024. URL Mirhoseini, A., Goldie, A., Yazgan, M., Jiang, J. W., Songhori, E. M., Wang, S., Lee, Y.-J., Johnson, E., Pathak, O., Nazi, A., Pak, J., Tong, A., Srinivasa, K., Hang, W., Tuncer, E., Le, Q. V., Laudon, J., Ho, R., Carpenter, R., and Dean, J. A graph placement method- ology for fast chip design. Nature, 594:207 212, 2021. URL org CorpusID:235395490. Mutschler, A. Debug tops verification tasks, Oct 2021. URL debug-tops-verification-tasks . Nam, D., Macvean, A., Hellendoorn, V., Vasilescu, B., and Myers, B. Using an llm to help with code understand- ing. In Proceedings of the IEEE ACM 46th International Conference on Software Engineering, pp. 1 13, 2024. Nijkamp, E., Hayashi, H., Xiong, C., Savarese, S., and Zhou, Y. Codegen2: Lessons for training llms on programming and natural languages, 2023. URL org abs 2305.02309. Noonan, K. Next-generation eda tools reshape the future of chip design, Dec 2024. Oliveira, D., Santos, R., de Oliveira, B., Monperrus, M., Castor, F., and Madeiral, F. Understanding code under- standability improvements in code reviews.\n\n--- Segment 34 ---\nNoonan, K. Next-generation eda tools reshape the future of chip design, Dec 2024. Oliveira, D., Santos, R., de Oliveira, B., Monperrus, M., Castor, F., and Madeiral, F. Understanding code under- standability improvements in code reviews. IEEE Trans- actions on Software Engineering, 51(1):14 37, 2025. doi: 10.1109 TSE.2024.3453783. 12 ML For Hardware Design Interpretability: Challenges and Opportunities OpenAI. Gpt-4 technical report, 2024. URL https: arxiv.org abs 2303.08774. Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J., Zettle- moyer, L., Ghosh, G., Lewis, M., Holtzman, A., and Iyer, S. Byte latent transformer: Patches scale bet- ter than tokens, 2024. URL abs 2412.09871. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Rei, R., Stewart, C., Farinha, A. C., and Lavie, A. Comet: A neural framework for mt evaluation. arXiv preprint arXiv:2009.09025, 2020. Reuther, A., Michaleas, P., Jones, M., Gadepally, V., Samsi, S., and Kepner, J. Survey of machine learning ac- celerators. In 2020 IEEE High Performance Extreme Computing Conference (HPEC), pp. 1 12, 2020. doi: 10.1109 HPEC43674.2020.9286149.\n\n--- Segment 35 ---\nIn 2020 IEEE High Performance Extreme Computing Conference (HPEC), pp. 1 12, 2020. doi: 10.1109 HPEC43674.2020.9286149. Rozi ere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., D efossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code, 2024. URL https: arxiv.org abs 2308.12950. Saba, W. S. Llms understanding of natural language revealed, 2024. URL 2407.19630. Sandal, S. and Akturk, I. Zero-shot rtl code generation with attention sink augmented large language models, 2024. URL Siemens. High-level synthesis and verification, 2024. Sun, C., Hahn, C., and Trippel, C. Towards improving verification productivity with circuit-aware translation of natural language to systemverilog assertions. In First International Workshop on Deep Learning-aided Verifi- cation, 2023a. Sun, W., Fang, C., Ge, Y., Hu, Y., Chen, Y., Zhang, Q., Ge, X., Liu, Y., and Chen, Z. A survey of source code search: A 3-dimensional perspective, 2023b. URL https: arxiv.org abs 2311.07107. Tarassow, A. The potential of llms for coding with low-resource and domain-specific programming lan- guages, 2023. URL 2307.13018. Thakur, S., Ahmad, B., Fan, Z., Pearce, H., Tan, B., Karri, R., Dolan-Gavitt, B., and Garg, S. Benchmark- ing large language models for automated verilog rtl code generation.\n\n--- Segment 36 ---\nURL 2307.13018. Thakur, S., Ahmad, B., Fan, Z., Pearce, H., Tan, B., Karri, R., Dolan-Gavitt, B., and Garg, S. Benchmark- ing large language models for automated verilog rtl code generation. 2023 Design, Automation and Test in Eu- rope Conference and Exhibition (DATE), 2023. doi: 10.23919 DATE56975.2023.10137086. URL https: par.nsf.gov biblio 10419705. Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and De Sa, C. Quip: even better llm quantization with hadamard incoherence and lattice codebooks. In Proceedings of the 41st International Conference on Machine Learning, ICML 24. JMLR.org, 2024. Varambally, B. S. and Sehgal, N. Optimising design ver- ification using machine learning: An open source solu- tion, 2020. URL 02453. Vungarala, D., Nazzal, M., Morsali, M., Zhang, C., Ghosh, A., Khreishah, A., and Angizi, S. Sa-ds: A dataset for large language model-driven ai accelerator design gener- ation, 2024. Wang, A. 10 best data annotation and data labeling tools in 2024: Basicai s blog, 2024. Wang, C., Xu, Y., Peng, Z., Zhang, C., Chen, B., Wang, X., Feng, L., and An, B. keqing: knowledge-based question answering is a nature chain-of-thought men- tor of llm, 2023a. URL 2401.00426. Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1- bit transformers for large language models, 2023b. URL Wang, H., Ma, S., and Wei, F. Bitnet a4.8: 4-bit activations for 1-bit llms, 2024a. URL abs 2411.04965.\n\n--- Segment 37 ---\nURL Wang, H., Ma, S., and Wei, F. Bitnet a4.8: 4-bit activations for 1-bit llms, 2024a. URL abs 2411.04965. Wang, Y., Wang, W., Joty, S., and Hoi, S. C. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds. ), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 8696 8708, Online and Punta Cana, Dominican Republic, November 2021. Association for Computa- tional Linguistics. doi: 10.18653 v1 2021.emnlp-main. 685. URL emnlp-main.685 . Wang, Y., Si, S., Li, D., Lukasik, M., Yu, F., Hsieh, C.-J., Dhillon, I. S., and Kumar, S. Two-stage llm fine-tuning with less specialization and more generalization, 2024b. URL 13 ML For Hardware Design Interpretability: Challenges and Opportunities Xiong, H., Bian, J., Yang, S., Zhang, X., Kong, L., and Zhang, D. Natural language based context modeling and reasoning for ubiquitous computing with large language models: A tutorial, 2023. URL org abs 2309.15074. Xu, X. and Cai, H. Ontology and rule-based natural language processing approach for interpreting textual reg- ulations on underground utility infrastructure. Advanced Engineering Informatics, 48:101288, 2021. ISSN 1474- 0346. doi: URL science article pii S1474034621000422. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient train- ing. In Forty-first International Conference on Machine Learning, 2024. URL forum?id ia5XvxFUJT. Yin, X., Ni, C., and Wang, S. Multitask-based evaluation of open-source llm on software vulnerability.\n\n--- Segment 38 ---\nURL forum?id ia5XvxFUJT. Yin, X., Ni, C., and Wang, S. Multitask-based evaluation of open-source llm on software vulnerability. IEEE Transac- tions on Software Engineering, 50(11):3071 3087, 2024. doi: 10.1109 TSE.2024.3470333. Zhang, T., Ladhak, F., Durmus, E., Liang, P., McKeown, K., and Hashimoto, T. B. Benchmarking large lan- guage models for news summarization. Transactions of the Association for Computational Linguistics, 12:39 57, 2024a. doi: 10.1162 tacl a 00632. URL https: aclanthology.org 2024.tacl-1.3 . Zhang, Y., Yu, Z., Fu, Y., Wan, C., and Lin, Y. C. MG- Verilog: multi-grained dataset towards enhanced llm- assisted verilog generation. In The First IEEE Interna- tional Workshop on LLM-Aided Design (LAD 24), 2024b. Zhong, R., Du, X., Kai, S., Tang, Z., Xu, S., Zhen, H.-L., Hao, J., Xu, Q., Yuan, M., and Yan, J. Llm4eda: Emerg- ing progress in large language models for electronic de- sign automation, 2023. URL abs 2401.12224. Ziegler, M. M., Bertran, R., Buyuktosunoglu, A., and Bose, P. Machine learning techniques for taming the complexity of modern hardware design. IBM Journal of Research and Development, 61(4 5):13:1 13:14, 2017. doi: 10. 1147 JRD.2017.2721699. 14\n\n