=== ORIGINAL PDF: 2505.13479v1_RTL_Graph-enhanced_LLM_for_RTL_Code_Generation.pdf ===\n\nRaw text length: 42656 characters\nCleaned text length: 41072 characters\nNumber of segments: 28\n\n=== CLEANED TEXT ===\n\narXiv:2505.13479v1 [cs.PL] 11 May 2025 RTL : Graph-enhanced LLM for RTL Code Generation Mohammad Akyash, Kimia Azar, Hadi Kamali Department of Electrical and Computer Engineering (ECE), University of Central Florida, Orlando, FL 32816, USA {mohammad.akyash, azar, Abstract As hardware design complexity escalates, there is an urgent need for advanced automation in electronic design au- tomation (EDA). Traditional register transfer level (RTL) design methods are manual, time-consuming, and prone to errors. While commercial (instruction-tuned) large language models (LLMs) shows promising performance for automation, they pose security and privacy concerns. Open-source models offer alternatives; however, they frequently fall short in quality correctness, largely due to limited, high-quality RTL code data essential for effective training and generalization. This paper proposes RTL , a first- of-its-kind LLM-assisted method for RTL code generation that utilizes graph representations of code structures to enhance the quality of generated code. By encoding RTL code into a textualized control flowgraphs (CFG) and data flow graphs (DFG), RTL captures the inherent hierarchy, dependencies, and relationships within the code. This structured graph-based approach enhances the context available to LLMs, enabling them to better understand and generate instructions. By focusing on data generation through graph representations, RTL addresses the limitations of previous approaches that rely solely on code and suffer from lack of diversity. Experimental results demonstrate that RTL outperforms state-of-the-art models fine-tuned for RTL generation, as evaluated using the VerilogEval benchmark s metric, as well as the RTLLM1.1 model, which highlight the effectiveness of graph-enhanced context in advanc- ing the capabilities of LLM-assisted RTL code generation1. Index Terms LLM, RTL Code Generation, Verilog, Graph. I. INTRODUCTION Large language models (LLMs) like GPT have shown exceptional capabilities in natural language processing (NLP) [2], driving interest in their applications beyond traditional NLP tasks, particularly code generation [3]. Commercial LLMs like Codex [4], AlphaCode [5], PaLM2 [6], and Claude [7], have significantly advanced software development by understanding and generating code2. While generating RTL code from natural language representation of design functionality architecture descriptions can boost efficiency in hardware development (by reducing the effort for manually RTL coding, testing, and verification), their effectiveness in hardware design, particularly for register transfer level (RTL) remains constrained due to several challenges: (i) the lack of reliable training dataset, (ii) limited understanding of concur- rent nature of RTL by LLMs, (iii) no consideration of resource constraining, etc. [9]. Recent advancements in commercial LLMs have demon- strated substantial improvements in RTL generation [10]. 1Dataset Model is available at [1]. 2It is due to the abundance of high-quality training data, well-defined patterns, and mature ecosystems in languages like C Python [8]. However, they continue to face significant challenges, particu- larly security and privacy concerns when dealing with security- critical and sensitive data (design) [11], [12]3. Hence, the trend shows a shift from commercial to open-source LLMs for RTL code generation, driven by the need for greater control, customization, and privacy in specialized applications (e.g., fine-tuning LLM for RTL generation, specialized in AI accelerators) [13], [14]. Fig. 1 shows a simplified top view of how fine-tuning and refinement processes adapt the open-source LLMs to become domain-specific, enabling it to effectively address hardware design automation challenges. Building on this general process of instruction-tuning, as shown in Fig. 1, recent studies have developed several new open-source LLMs fine-tuned specifically for RTL code gener- ation [8], [10], [15] [18]. Table I summarizes the key features of each one of these RTL-oriented fine-tuned LLMs. As shown, with no multi-modal support, these methods rely solely on unstructured text, and are particularly prone to halluci- nations due to a lack of structured context [19], increasing the risk of generating instructions that inaccurately reflect the intent or functionality of the original code. This is while the retrieval-augmented generation (RAG) has proved substantial improvement by incorporating additional relevant information, which enriches the LLM s understanding and improves output alignment [20]. Often, this information takes the form of structured data, such as graphs, and recent research has inves- tigated the ability of LLMs to understand and process these graph structures effectively [19], [21] [24]. These studies have demonstrated that LLMs can effectively capture and inter- pret relationships and patterns within structured data which represent complex dependencies and hierarchies. Despite the 3The fully closed nature of these commercial LLMs discourages their use on proprietary documents, design specifications, and data, as organizations are unwilling to risk the exposure of their confidential designs. Open-source RTL Designs Prompting for Specification Dataset of RTL Designs Corresponded (generated) Instructions: Specification, Func. Desc., Arch. Notes, etc. Open-source pre-trained LLM Instruction-tuning (fine-tuning on pairs of codes instructions) (on Commercial) (on Open-Source) Open-source fine-tuned LLM RTL Code generation (using tuned model) Testing (evaluation) Refinement Fig. 1: The overview of Fine-tuning and (Potential Directions of) Refinement using Open-Source LLMs for Specializing of RTL Code Generation. growing interest in applying LLMs to graph-based data, their potential for analyzing RTL code remains almost unexplored. As LLMs struggle with hardware design due to their limited understanding of RTL s concurrent nature, representing code as graphs can address this by capturing the hierarchy, depen- dencies, and relationships between components. This struc- tured encoding improves context awareness, enabling LLMs to produce outputs that are more accurate and aligned with the intended functionality of the design. To address this need, this paper introduces RTL , the first multi-modal graph- augmented fine-tuned LLM designed for enhanced RTL code generation. In RTL , a unique graph-based representation of designs will be incorporated as an supporting embedding during the instruction generation and fine-tuning phases, which improves both the model s functional and structural under- standing. The main contributions of RTL are as follows: (1) With an automated RTL-to-graph mechanism, which en- codes each training dataset entry into control flow graphs (CFG) and data flow graphs (DFG), we introduce a new fine- tuning mechanism that relies on instructions generated based not only on the RTL code itself but also on its corresponding CFG and DFG4. To the best of our knowledge, RTL is the first LLM-assisted RTL code generator to enhance LLM efficiency by combining multiple data formats. (2) In RTL , a 100K training dataset has been curated us- ing well-established open-source repositories such as GitHub, Bitbucket, and Opencores. The dataset entries are diverse, high-star rated, and contain critical keywords structs5 in RTL generation. This diversity is crucial for LLM fine-tuning that can avoid overfitting to specific patterns. Also, to improve code quality, an LLM-assisted refinement (pruning) has been imple- mented to refine and optimize the collected code samples. (3) We evaluated the latest foundational and advanced models, compared with our new model, highlighting that our multi- modal fine-tuning strategy sets a new benchmark in RTL coding. We also plan to make RTL a fully open-source model with its 100K instruction tuning dataset to support collaboration in EDA and chip design community. II. RELATED WORK A. LLM for RTL Code Generation While recent advancements in hardware design automation have shown the effectiveness of adapting LLMs for specialized tasks like EDA automation and optimization, e.g., scripting [26], error interpretation [27], assistant chatbot for design flow [28], etc., numerous efforts have been made to fine-tune and pre-train models for RTL (Verilog) code generation: 4While RTL code (text) is for syntactical and semantical perspective, its CFG and DFG is for structural perspective of the circuit. This is conceptually a cross-modality fusion, in which the model integrate insights from both the RTL code (syntax and semantic) and its graph structures (structure). 5Structs include (but not limited to) module, port, wire, reg, procedural blocks (e.g., always and initial), control flow constructs (e.g., if-else and case), instantiation, FSM, parameters and constants, generate constructs, arrays and memories, etc. (i) VeriGen [15] is an early attempt that compiled Verilog files from GitHub and textbooks for training dataset. De- spite assembling a substantial dataset, the lack of proper pre-processing and organization led to inconsistencies which causes the fine-tuned CodeGen model to often generate non- functional Verilog code with syntax errors. (ii) RTLCoder [10] is another early endeavor that has lever- aged GPT-3.5 for synthesizing code-instruction pairs by ex- tracting RTL-specific keywords (to overcome dataset scarsity and code generation quality). However, its dependence on GPT-3.5 s embedded knowledge limited code diversity. This limitation prompted subsequent efforts to enrich data diversity through augmentation techniques. (iii) OriGen [8] advanced RTL code generation by introducing code-to-code augmentation and a self-reflection mechanism. The former diversifies the dataset with semantically equivalent but syntactically varied Verilog code, while the latter uses compiler feedback to iteratively correct errors, addressing VeriGen s inconsistencies and improving code quality. (iv) BetterV [16], building upon OriGen s idea of augment- ing data and integrating feedback, extends the capabilities of Verilog code generation by introducing a controlled text generation framework tailored specifically for Verilog, drawing parallels with C programs to help LLMs better comprehend Verilog semantics. It employs generative discriminators to optimize the Verilog for Power, Performance, and Area (PPA) while also incorporating data augmentation techniques to address data scarcity issues. (v) AutoVCoder [17] focuses on addressing the limitation of diversity and domain-specific accuracy in RTL code gener- ation using a two-round fine-tuning process to boost LLM performance in Verilog code generation. AutoVCoder also incorporated a domain-specific RAG module to constructively enhance prompts, which improved the syntactic and functional correctness of the generated code. (vi) CodeV [25] leverages LLMs for code summarization rather than generation, shifting the focus from producing Verilog code from natural language to generating detailed descriptions from Verilog code. By processing 165K Ver- ilog modules from GitHub and fine-tuning with multi-level summarization, CodeV enhances training datasets with rich description-code pairs, ensuring both syntactic accuracy and semantic depth for high-quality Verilog representations. (vii) CraftRTL [18] introduces an approach that includes constructing correct-by-construction data, such as Karnaugh Maps, state-transition diagrams, and waveforms, which im- prove the ability of LLMs to interpret additional information for LLM fine-tuning. In addition, CraftRTL employs an auto- mated framework that uses LLMs to generate error reports at various training checkpoints. MAGE [29] enhances RTL generation using a multi-agent system with high-temperature sampling and Verilog-state checkpointing, but for evaluation, we focus on approaches that improve model performance through fine-tuning and data collection. While recent advancements in RTL code generation have shown promising improvements, they lack (i) capturing TABLE I: Top View of Existing LLM-assisted Studies for RTL Code Generation. Model Key Novelty Training Dataset, [Size] Fine-tuned Model Multi-modal HW Efficiency VeriGen [15] Fine-tuning on Dataset collected from GitHub and Textbooks Open-source, GitHub and Textbooks, [not listed] CodeGen-16B No None RTLCoder [10] GPT-3.5-based Code-Instruction Pair Synthesis Open-source, Synthesized, [27K] Mistral-7B DeepSeek-Coder-6.7b No None BetterV [16] Applying Controllable Text Generation w Discriminators for Engineering Optimization Closed-source, From internet, [not listed] CodeLlama-7B DeepSeek-Coder-6.7b-Instruct Code Qwen1.5-7B No Area Improvement OriGen [8] Code-to-code Augmentation, Self-reflection for Fixing Open-source, [222K] DeepSeek-Coder-7B No Iterative Functional Correctness Check AutoVCoder [17] Domain-specific RAG with Two-round LLM fine-tuning for Constructive Prompting Collected from Github, [not listed] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No (Text and Retrieval ) RAG-based Optimization CodeV [25] Multi-Level Summarization for Verilog Generation Close-source Github, [165K] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No Code Generation Improvement CraftRTL [18] Correct-by-construction data Synthetic and GitHub, [80.1K] Codellama-7B DeepSeek-Coder-6.7B Starcoder2-15B No Fine-tuning Correction RTL Structural-based Optimization (Graph Embedding for Instruction Tuning) Open-source, [200K] Codellama-7B Yes (Graph and Text) Structural Optimization, Area and Delay Improvement : This RAG is to identify the piece of data (RTL code). It has nothing to do with cross-modality understanding. the hierarchical structure of designs, (ii) comprehending data and control flow, and (iii) addressing the intrinsic concurrency of hardware designs. Prior NLP-based studies [19], [30], [31] have demonstrated the power of integrating structured knowledge to enhance reasoning and boost inter- pretability in LLMs. Inspired by these works, our proposed RTL incorporates graph-based knowledge of RTL design to bridge the gap between design abstraction and design phase. B. Graph Prompt Learning and Engineering Since the advent of LLMs, researchers have been exploring ways to embed graph data into the input of LLMs (as an embedding to use as in-context learning) to enable reasoning over graph-structured information [19], [22] [24], [31] [33]. Fatemi et al. [24] evaluated the encoding graph-structured data as text for LLMs. A key observation of this study is that LLM performance on graph reasoning tasks is highly sensitive to the chosen encoding method, the type of task, and the structure of the graph, hence emphasizing on selecting appropriate graph encoding techniques is paramount. Perozzi et al. [19] proposed a novel approach that leverages graph neural networks (GNNs) to encode data into embeddings instead of textualizing graphs. They introduced GraphToken, a parameter-efficient method explicitly designed to encode structured graph data for LLMs. GraphToken learns an encod- ing function that augments prompts with explicit structured information. Unlike GraphToken, GraphLLM [22] adopts an end-to-end approach, integrating graph learning models with LLMs. It employs a graph transformer to process graph structures directly, enhancing both accuracy and efficiency. Inspired by these approaches, we also leverage textualized graph representations to integrate graph data into LLMs. Specifically, we convert graph-structured data from RTL codes, such as CFG and DFG, into textual formats, which are then used alongside RTL codes to generate meaningful instructions. By providing both the graph and corresponding code, we enable LLMs to effectively reason about and generate complex hardware design instructions. Small to medium size V codes 100-300 lines Syntax-variant Semantic-variant High star (High quality) Code Refinement Generality, Diversity, Originality Remove similar modules Remove hierarchical w o instantiation Remove gate-level V codes Remove non-synthesizable codes Code Extract Graph Generation High-level graph generation Hierarchical connections Inter-module (Inter-structs) Inter-module data flow graph Inter-module control flow graph Intra-module control flow graph using PyVerilog Synthesizer LLM Textualizing Graph Graph embedding Textualized w NN-based featuring Semantic node attributes Timing constructs (sequential) Normalizing (name convention) Preserving state machines (FSM) Separated CFG and DFG Graph embedding Code embedding to GPT-4o Instruction Generation RTL dataset Graph-enhanced instruction Verilog code Sweeping dataset size from 5K (code, Graphed inst) to 20K (code, Graphed inst) Fine-tuned Llama3.1 for RTL Generation RTL VerilogEval for Verification Instruction-tuning on Llama3.1 7B Fig. 2: RTL Overview: Text and Graph Embedding for LLM Fine-Tuning for RTL Code Generation. TABLE II: Main Targeted Keyword in Dataset Collection in RTL Category Struct-based Keywords Structural Constructs module, endmodule, input, output, inout, wire, reg, assign, generate, endgenerate, parameter, localparam, always ( ), begin, end Sequential Logic always, always (posedge clk), always (negedge clk), posedge, negedge, if, else, case, default Combinational Logic assign, case, casex, casez, and, or, not, nand, nor, xor, xnor, mux, demux, generate, genvar Memory and Storage always (posedge clk or negedge reset), Flip-flop constructs (e.g., if(enable)), RAM, ROM, initial Data Path add, sub, mul, div, , , , -, , , , , ˆ, State Machines always (posedge clk), case, endcase, default, parameter, localparam, idle, current, next Hierarchy module instantiation, Dot (.port_name(xyz)) Category Context-based Keywords Arith Logic adder, subtractor, multiplier, divider, alu Sequential Logic dff, register, shift_register, counter Memory register_file, register_bank, fifo, cache Control Logic encoder, decoder, arbiter, bus_controller Data Transfer uart, spi, i2c, ethernet, input_buffer, output_buffer, parity, hamming, crc Signal Proc. filter, fft, dft, mac, cordic Interconnect axi, wishbone, apb, crossbar, bus_switch, bridge Clock pll, clock_divider, prescaler, timer, stopwatch III. PROPOSED MODEL: RTL Fig. 2 presents a top-down view of our proposed model, RTL , which is structured into five key steps: (1) RTL code collection, (2) RTL code refinement, (3) RTL CFG DFG generation, (4) instruction generation (based on RTL code and CFG DFG), (5) graph-enhanced instruction-tuning. These steps, each detailed below, collectively prepare the fine-tuned LLM for high-quality RTL code generation. A. Collecting RTL Dataset from Repositories To gather a high-quality RTL code dataset, i.e., from GitHub, Bitbucket, and Opencores, we targeted a list of most common keywords relevant to RTL design (see Table II. For each keyword, we generated 10 related sub-keywords to capture a wider range of code (maximizing diversity). This keyword expansion allowed us to cover more specific design cases, including real common use cases in hardware design. We filtered out data with less than 100 lines and more than 300 lines to maintain consistency and focus on small-sized designs, which balance complexity and make them ideal for realistic use cases and model training. To ensure that we selected the high quality and reliable RTL codes, we ranked GitHub repositories by their star count, as- suming that more popular repositories indicated by a higher number of stars likely contain well-maintained, reliable code. From these top-ranked repositories, we extracted RTL code files with high star counts, as this metric often correlates with quality and community validation. In addition to ranking by popularity, we filtered out test- benches and netlists, focusing exclusively on RTL (behavioral) design files. We also ensured that the collected RTL codes contained all modules with their needed decalaration (for hierarchical design). By considering these extra steps for RTL code collection, we create a clean, relevant dataset of RTL design code suitable for training purposes. B. RTL Code Refinement Using GPT While state-of-the-art studies focuses on using either machine-generated (LLM-based) or human-created RTL code for training datasets, in RTL , we employ a hybrid approach, where we collect RTL codes ( III-A) and refine them using the machine (here GPT-4o). The refinement process involved a structured prompt that guided GPT through several key steps to standardize and enhance each RTL module (making the RTL code consistent, syntactically correct, self-contained, and aligned with the requirements for effective model training): (i) Dependency Removal: For modules dependent on external files or submodules instantiated within the main module6, GPT embedded the logic of these dependencies directly within the code. External module instantiations were replaced by their corresponding internal logic, creating self-contained modules that no longer required external dependencies. (ii) Variable Definition and Initialization: In large RTL projects, libraries or headers often define global variables. However, when assembling sub-modules from these hierarchi- cal projects, these reference files may not always be included in the training dataset. So, GPT is called to set value to these undefined variables and functions (infers typical use cases or context-based values), creating self-contained, standalone modules for enhanced training utility. (ii) Syntax Check Correction and Synthesizability: GPT is in- voked to check7 correct syntactical structures to adhere to standard RTL (Verilog) syntax, addressing common elements such as operator usage, and supported constructs. Additionally, to validate the correctness of the collected and refined RTL codes, a basic synthesis flow was performed using Yosys [34]8. C. High-level Graph Generation from RTL Codes There are various methods to encode text into graphs for LLMs, such as using GNNs [19], and graph convolutional networks (GCNs) [33], and directly integrating them with LLMs. Some representations are particularly well-suited for LLMs as they balance structural accuracy with textual clarity [24]. In RTL , we employ textual descriptions of graphs because they offer better interpretability for LLMs, making it easier for the models to understand hierarchies. By potentially leveraging these descriptions as RAGs, we enable LLMs to tackle more complex reasoning tasks in auto-debugging, optimization, and verification. We follow these steps to generate textual graphs in RTL : (1) Flattening (embedding all modules into the main graph) that is for designs with instantiated modules (hierarchical). (2) DFG generation (module-level to I O-level) that is for data movement form inputs to outputs. 6While we exclude code dependencies on external sources (at III-A), GPT- based auto-completion is used to fix incomplete code fragments. 7Each sample has module and endmodule with a procedural block (e.g., assign, .instance(port) , while inputs (in inout ports) are connected to outputs (out inout ports). 8Codes with synthesis error are excluded from the further steps and training. TABLE III: RTL vs. base CodeLlama-7B-instruct and GPT-4. Evaluated Model no. of params Open-Source? VerilogEval [39] Only HumanEval k 1 k 5 k 10 GPT-4 N A 43.5 55.8 N A CodeLlama-based 7B 18.2 22.7 24.3 RTL 5K Trained 7B 23.7 28.2 30.7 RTL 10K Trained 26.2 30.1 33.3 RTL 15K Trained 28.2 32.6 34.6 RTL 20K Trained 29.4 34.6 37.1 RTL 50K Trained 41.3 47.1 50.5 RTL 100K Trained 54.3 60.8 65.2 RTL 200K Trained 59.9 68.8 72.1 : HumanEval ensures RTL evaluation aligns with real-world data [40]. (3) CFG generation (module-level to I O-level) that is for control signals form inputs to outputs. (4) Adding node attributes that includes type (gate, module, input, output, etc.) and function (arith, storage, logic, etc.). (5) Adding temporal behavior of nodes that determines the sequence of operation (based on sequencing9). It also includes feedback loops in sequential circuits (e.g., FSMs). Using these steps in RTL , we design all graphs at a high level, keeping details minimal to maintain a balance between abstraction and usability. Our primary focus is on defining graphs at the module level, avoiding unnecessary details that could overwhelm the model. This approach aligns closely with how hardware engineers conceptualize RTL code. D. Graph-enhanced Instruction Generation To create code-instruction pairs for fine-tuning in RTL , relying on in-context learning [36], the prompting includes both code snippets and their associated (textualized) graph- based representations. By integrating detailed information from both the code and its graphical representations, the LLM generates instructions that are more accurate, informative, and aligned with the actual functionality of the hardware module. The instructions encapsulate complex control flows and data interactions (from CFG and DFG) in a clear and concise manner. By using CFG and DFG as additional embeddings, the LLM enhances its understanding of critical component interactions and the intended purpose of various modules. This results in RTL code that is more detailed, precise, and less likely to include inaccuracies or hallucinations. E. Fine-Tuning over Code-Instruction Pairs At the end, once the pairs of RTL codes and (graph- enghanced) instructions are ready as our dataset, we finetune the base model (i.e., CodeLlama [37]) on this dataset10. IV. EXPERIMENTS To evaluate the performance of RTL , we fine-tune the CodeLlama-7B-Instruct as the targeted generative LLM. All experiments are conducted for 1 epoch (to avoid over-fitting, 9BMC engines can be used for iterations to collect sequencing [35]. 10While DeepSeek [38] could obtain superior outcomes as the base model, we opted for CodeLlama [37] to evaluate the genuine impact of incorporating graph structures during fine-tuning. as we observed over-fitting when using more epochs) us- ing PyTorch on NVIDIA L4 with the learning rate at 2e- 4. Additionally, for RTL code refinement, graph refinement, and instruction generation, GPT-4 has been engaged, costing approximately 84 per 1000 samples. To assess the RTL performance, we utilized two bench- marking frameworks: VerilogEval11 [39] and RTLLM [41]. Both benchmarks employ the widely recognized metric to evaluate the correctness of the generated code. To fine-tune the model for RTL code generation, we lever- age the LoRA (Low-Rank Adaptation) [42] technique (enhanc- ing RTL-oriented capabilities while maintaining performance). The optimization process employs the AdamW optimizer, configured with β1 0.9 and β2 0.99, and a cosine decay schedule for the learning rate. A warm-up phase is included with a ratio of 0.03, and training batch size is 2. We compared RTL against several existing models, in- cluding Verigen [15], RTLCoder [10], BetterV [16], Ori- gen [8], AutoVCoder [17] and CraftRTL [18]. Additionally, CodeLlama-7B-instruct was used as a baseline to assess the improvements made by RTL , while comparison with GPT-4 has been also explored (to show RTL efficiency). A. Comparison with the Base Models Table III shows the performance comparison between RTL , the base CodeLlama-7B-instruct model, and GPT-4. The results indicate that as the training dataset for RTL grows, the quality of the generated code consistently improves. Notably, RTL outperforms GPT-4 when trained on a 100K dataset. Note that for the VerilogEval benchmark, we prioritize HumanEval as it more accurately reflects real-world data [40]. B. Impact of data size The impact of dataset size on the accuracy of our model is illustrated in Figure 3. We trained the model using datasets collected up to 200K samples. This suggests that if we gather more data, we can achieve higher accuracy levels. The initial collected data (5K, 10K, and 15K) showed a gradual improve- ment in model accuracy, but with larger datasets, there is a clear trend of significant performance gains. The combination of high-quality data, augmenting instruction generation by graphs, and effective model training demonstrate the potential to achieve even higher accuracy levels with larger datasets. C. Comparison with the State-of-the-art Models Table IV presents a comprehensive comparison between the performance of RTL and other state-of-the-art models. While many competing models leverage both CodeLlama [37] and DeepSeek [38], all reported results are based on fine-tuned versions of CodeLlama-7B-Instruct to ensure a consistent and fair evaluation. The results demonstrate that, with an expanded training dataset, RTL outperforms these models, highlighting the critical role of multi-modal embedding, par- ticularly augmenting the embedding using CFGs and DFGs, in improving LLM-assisted RTL code generation. 11Built upon the revisited VerilogEval [40] is the percentage of problems solved within k attempts. 20 VerilogEval ( ) 30 40 50 60 70 80 0 20K 40K 60K 80K 100K 120K 140K 160K 180K 200K 0.1 0.2 0.3 0.4 0.5 Improve per 1K Training VerilogEval ( ) VerilogEval ( ) VerilogEval ( ) VerilogEval Improvement per 1K Dataset ( ) VerilogEval Improvement per 1K Dataset ( ) VerilogEval Improvement per 1K Dataset ( ) Training Dataset Size Fig. 3: Impact of dataset size on model accuracy. TABLE IV: Comparsion of RTL and state-of-the-art RTL Generators. Evaluated Model ps OSS? Tr Size VerilogEval [39] RTLLM 1.1 HumanEval ( ) Syn. Func. 1 5 10 GPT-4 N A - 43.5 55.8 N A 89.7 37.9 CodeLlama-7B-I 7B N A 18.2 22.7 24.3 62.1 10.3 VeriGen [15] 15B - 30.3 43.9 49.6 86.2 24.1 RTLCoder-DS [10] 6.7B 27K 41.6 50.1 53.4 93.1 48.3 BetterV-CQ [16] 7B - 46.1 53.7 58.2 N A N A OriGen [8] 7B 222K 54.4 60.1 64.2 N A 65.5 AutoVCoder-CQ [17] 7B - 48.5 55.9 N A 100 51.7 CodeV [25] 7B 165K 45.2 59.5 63.8 93.1 62.1 CraftRTL [18] 7B 80.1K 63.1 67.8 69.7 93.9 52.9 7B 50K 41.3 47.1 50.5 82.7 41.3 100K 54.3 60.8 65.2 86.2 44.8 200K 59.9 68.8 72.1 93.9 51.7 : Our focus is on works that involve both data collection and fine-tuning. D. Impact of Graph Utilization on Instruction Generation To evaluate the impact of graph-based augmentation on instruction generation quality in RTL , Table V compares models trained with and without textualized graph representa- tions. Training the model using instructions augmented with textualized graph representations results in a noticeable accu- racy improvement. Table V provides a representative ablation study based on a 5K dataset, showing up to a 5 increase in When scaled to a 100K dataset, the improvement grows significantly, reaching 18 , underscoring the value of graph augmentation for fine-tuning. E. A Simple Case Study: An ALU in RTL To gain deeper insights into the impact of graph-based in- struction generation, we performed an ablation study focusing on a RTL module for an arithmetic logic unit (ALU). The ALU takes two 4-bit inputs (A and B), performs a variety of arithmetic and logical operations based on a 3-bit control TABLE V: Ablation Study on Textualized Graph Rep. (TGR) 5K Datasets. Model Temp. VerilogEval [39] Model Temp. VerilogEval [39] 1 5 10 1 5 10 RTL w o TGR 0.6 22.4 23.7 25.6 RTL w TGR 0.6 23.7 27.5 29.4 RTL w o TGR 0.7 22.4 24.3 25.6 RTL w TGR 0.7 23.7 28.2 30.7 module ALU ( input [3:0] A, input [3:0] B, input [2:0] ALUOp, output reg [7:0] Result, output reg Zero); always ( ) begin case (ALUOp) 3'b000: Result A B; 3'b001: Result A - B; 3'b010: Result A B; 3'b011: Result A B; 3'b100: Result A B; default: Result 8 h00; endcase if (Result 8 h00) Zero 1; else Zero 0; end endmodule Instruction generated WITHOUT graph representation: Create a Verilog module for a 4-bit Arithmetic Logic Unit (ALU) named ALU. The module should have two 4-bit inputs (A and B) representing operands, a 3-bit input (ALUOp) to select the operation, an 8-bit output (Result) for the operation result, and a 1-bit output (Zero) as a flag indicating if the result is zero. Instruction generated WITH graph representation: Create the Verilog module, ALU, processes two 4-bit inputs (A and B) and performs one of five operations addition, subtraction, bitwise AND, bitwise OR, or multiplication based on the value of the 3-bit ALUOp signal. The computed result is stored in an 8-bit output register, Result. A case statement governs the operation selection: - ALUOp 000: Perform addition (A B). - ALUOp 001: Perform subtraction (A - B). - ALUOp 010: Perform bitwise AND (A B). - ALUOp 011: Perform bitwise OR (A B). - ALUOp 100: Perform multiplication (A B). For any other value of ALUOp, the Result is set to zero. A comparator checks if the Result is zero and sets the Zero flag accordingly (1 if the result is zero, otherwise 0). Fig. 4: Instructions w and w o CFG and DFG and corresponding RTL code. signal (ALUOp), and outputs an 8-bit result (Result) along with a zero flag (Zero). As in Fig. 4, For this study, we generated instructions using two different configurations: one that included graph-based representations, as additional input, and one that used only the RTL code with no graph. As shown, the instruction with the graph representation provides a more technically detailed and complete description compared to the one without it. This version systematically defines the functionality of the 4-bit ALU by explicitly mapping ALUOp values to specific operations (addition, subtraction, bitwise AND, OR, and multiplication) through a case state- ment, while also handling invalid ALUOp cases by setting the Result to zero. It also clarifies the generation of the Zero flag, which indicates whether the computed Result is zero, ensuring robustness in implementation. In contrast, the instruction generated without the graph representation lacks this detailed description of control signal dependencies and default behaviors which leads to ambiguity. The inclusion of graph representation enhances clarity, depicting control flow, data dependencies, and interactions more precisely, which is crucial for accurate RTL code implementation. In this specific example, the instruction generated with the both DFG and CFG representation closely aligns with the code, which can effectively capture the control flow and structural dependencies of the RTL module. By explicitly depicting the relationships between control signals and data, it provides a more structured understanding, which leads to better training outcomes for LLMs. V. CONCLUSION This paper introduces RTL , an fully open-source model leveraging LLMs for efficient RTL code generation. RTL is a first-of-its-kind that integrates both textual (RTL code) and graph-based representations (CFG and DFG in textualized formats) to generate high-quality instruction-code pairs for LLM fine-tuning for RTL generation purposes. By using multi- modal fine-tuning approach, RTL achieves remarkable per- formance, surpassing state-of-the-art competitive models. The experimental results show that its success rate exceeds 70 in VerilogEval and 90 in RTLLM benchmark, all while operating on a fine-tuned version of CodeLlama-7B-Instruct. REFERENCES [1] RTL Model Dataset - Graph-enhanced LLM for RTL Generation, 2025. [2] T. B. Brown et al., Language models are few-shot learners, 2020. [Online]. Available: [3] E. Nijkamp et al., Codegen: An open large language model for code with multi-turn program synthesis, 2023. [Online]. Available: [4] M. Chen et al., Evaluating large language models trained on code, 2021. [Online]. Available: [5] Y. Li et al., Competition-level code generation with alphacode, Science, vol. 378, no. 6624, p. 1092 1097, Dec. 2022. [Online]. Available: [6] R. Anil et al., Palm 2 technical report, 2023. [Online]. Available: [7] Anthropic, Claude: An ai assistant built by anthropic, anthropic.com, 2023. [8] F. Cui et al., Origen:enhancing rtl code generation with code- to-code augmentation and self-reflection, 2024. [Online]. Available: [9] M. Akyash et al., Simeval: Investigating the similarity obstacle in llm-based hardware code generation, in Proceedings of the 30th Asia and South Pacific Design Automation Conference, ser. ASPDAC 25. New York, NY, USA: Association for Computing Machinery, 2025, p. 1002 1007. [Online]. Available: 3697624 [10] S. Liu et al., Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, in 2024 IEEE International Workshop on LLM-Aided Design. IEEE, 2024. [11] N. Mashnoor et al., Llm-ift: Llm-powered information flow tracking for secure hardware, 2025. [Online]. Available: 2504.07015 [12] M. Akyash et al., Self-hwdebug: Automation of llm self-instructing for hardware security verification, in 2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2024, pp. 391 396. [13] Y. Fu et al., Gpt4aigchip: Towards next-generation ai accelerator design automation via large language models, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 9. [14] M. Akyash et al., Evolutionary large language models for hardware security: A comparative survey, in Great Lakes Symposium on VLSI (GLSVLSI), 2024, pp. 1 6. [15] S. Thakur et al., Verigen: A large language model for verilog code generation, 2023. [Online]. Available: [16] Z. Pei et al., Betterv: Controlled verilog generation with discriminative guidance, 2024. [Online]. Available: [17] M. Gao et al., Autovcoder: A systematic framework for automated verilog code generation using llms, arXiv preprint arXiv:2407.18333, 2024. [18] M. Liu et al., Craftrtl: High-quality synthetic data generation for verilog code models with correct-by-construction non-textual representations and targeted code repair, 2024. [Online]. Available: [19] B. Perozzi et al., Let your graph do the talking: Encoding structured data for llms, 2024. [Online]. Available: 05862 [20] K. Guu et al., Realm: Retrieval-augmented language model pre- training, 2020. [Online]. Available: [21] Z. Luo et al., Graphinstruct: Empowering large language models with graph understanding and reasoning capability, 2024. [Online]. Available: [22] Z. Chai et al., Graphllm: Boosting graph reasoning ability of large language model, 2023. [Online]. Available: 05845 [23] X. He et al., G-retriever: Retrieval-augmented generation for textual graph understanding and question answering, 2024. [Online]. Available: [24] B. Fatemi et al., Talk like a graph: Encoding graphs for large language models, 2023. [Online]. Available: [25] Y. Zhao et al., Codev: Empowering llms for verilog generation through multi-level summarization, 2024. [Online]. Available: https: arxiv.org abs 2407.10424 [26] M. Liu et al., Chipnemo: Domain-adapted llms for chip design, 2024. [Online]. Available: [27] K. Chang et al., Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework, in Proceedings of the 61st ACM IEEE Design Automation Conference, ser. DAC 24. New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: [28] H. Wu et al., Chateda: A large language model powered autonomous agent for eda, IEEE Transactions on Computer-Aided Design of Inte- grated Circuits and Systems, 2024. [29] Y. Zhao et al., Mage: A multi-agent engine for automated rtl code generation, 2024. [Online]. Available: [30] Q. Cai et al., Codegraph: Enhancing graph reasoning of llms with code, 2024. [Online]. Available: [31] L. Luo et al., Reasoning on graphs: Faithful and interpretable large language model reasoning, 2024. [Online]. Available: https: arxiv.org abs 2310.01061 [32] Z. Luo et al., Graphinstruct: Empowering large language models with graph understanding and reasoning capability, 2024. [Online]. Available: [33] H. Ren et al., Graph convolutional networks in language and vision: A survey, Knowledge-Based Systems, vol. 251, p. 109250, 2022. [34] C. Wolf et al., Yosys-a free verilog synthesis suite, in Proceedings of the 21st Austrian Workshop on Microelectronics (Austrochip), vol. 97, 2013. [35] E. Clarke et al., Bounded model checking using satisfiability solving, Formal methods in system design, vol. 19, pp. 7 34, 2001. [36] Q. Dong et al., A survey on in-context learning, arXiv preprint arXiv:2301.00234, 2022. [37] B. Rozi ere et al., Code llama: Open foundation models for code, 2024. [Online]. Available: [38] D. Guo et al., Deepseek-coder: When the large language model meets programming the rise of code intelligence, arXiv preprint arXiv:2401.14196, 2024. [39] M. Liu et al., Verilogeval: Evaluating large language models for verilog code generation, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), 2023, pp. 1 8. [40] N. Pinckney et al., Revisiting verilogeval: Newer llms, in-context learn- ing, and specification-to-rtl tasks, arXiv preprint arXiv:2408.11053, 2024. [41] Y. Lu et al., Rtllm: An open-source benchmark for design rtl generation with large language model, 2023. [Online]. Available: [42] E. J. Hu et al., Lora: Low-rank adaptation of large language models, 2021. [Online]. Available:\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2505.13479v1 [cs.PL] 11 May 2025 RTL : Graph-enhanced LLM for RTL Code Generation Mohammad Akyash, Kimia Azar, Hadi Kamali Department of Electrical and Computer Engineering (ECE), University of Central Florida, Orlando, FL 32816, USA {mohammad.akyash, azar, Abstract As hardware design complexity escalates, there is an urgent need for advanced automation in electronic design au- tomation (EDA). Traditional register transfer level (RTL) design methods are manual, time-consuming, and prone to errors. While commercial (instruction-tuned) large language models (LLMs) shows promising performance for automation, they pose security and privacy concerns. Open-source models offer alternatives; however, they frequently fall short in quality correctness, largely due to limited, high-quality RTL code data essential for effective training and generalization. This paper proposes RTL , a first- of-its-kind LLM-assisted method for RTL code generation that utilizes graph representations of code structures to enhance the quality of generated code. By encoding RTL code into a textualized control flowgraphs (CFG) and data flow graphs (DFG), RTL captures the inherent hierarchy, dependencies, and relationships within the code. This structured graph-based approach enhances the context available to LLMs, enabling them to better understand and generate instructions. By focusing on data generation through graph representations, RTL addresses the limitations of previous approaches that rely solely on code and suffer from lack of diversity. Experimental results demonstrate that RTL outperforms state-of-the-art models fine-tuned for RTL generation, as evaluated using the VerilogEval benchmark s metric, as well as the RTLLM1.1 model, which highlight the effectiveness of graph-enhanced context in advanc- ing the capabilities of LLM-assisted RTL code generation1. Index Terms LLM, RTL Code Generation, Verilog, Graph. I. INTRODUCTION Large language models (LLMs) like GPT have shown exceptional capabilities in natural language processing (NLP) [2], driving interest in their applications beyond traditional NLP tasks, particularly code generation [3].\n\n--- Segment 2 ---\nI. INTRODUCTION Large language models (LLMs) like GPT have shown exceptional capabilities in natural language processing (NLP) [2], driving interest in their applications beyond traditional NLP tasks, particularly code generation [3]. Commercial LLMs like Codex [4], AlphaCode [5], PaLM2 [6], and Claude [7], have significantly advanced software development by understanding and generating code2. While generating RTL code from natural language representation of design functionality architecture descriptions can boost efficiency in hardware development (by reducing the effort for manually RTL coding, testing, and verification), their effectiveness in hardware design, particularly for register transfer level (RTL) remains constrained due to several challenges: (i) the lack of reliable training dataset, (ii) limited understanding of concur- rent nature of RTL by LLMs, (iii) no consideration of resource constraining, etc. [9]. Recent advancements in commercial LLMs have demon- strated substantial improvements in RTL generation [10]. 1Dataset Model is available at [1]. 2It is due to the abundance of high-quality training data, well-defined patterns, and mature ecosystems in languages like C Python [8]. However, they continue to face significant challenges, particu- larly security and privacy concerns when dealing with security- critical and sensitive data (design) [11], [12]3. Hence, the trend shows a shift from commercial to open-source LLMs for RTL code generation, driven by the need for greater control, customization, and privacy in specialized applications (e.g., fine-tuning LLM for RTL generation, specialized in AI accelerators) [13], [14]. Fig. 1 shows a simplified top view of how fine-tuning and refinement processes adapt the open-source LLMs to become domain-specific, enabling it to effectively address hardware design automation challenges. Building on this general process of instruction-tuning, as shown in Fig. 1, recent studies have developed several new open-source LLMs fine-tuned specifically for RTL code gener- ation [8], [10], [15] [18]. Table I summarizes the key features of each one of these RTL-oriented fine-tuned LLMs.\n\n--- Segment 3 ---\n1, recent studies have developed several new open-source LLMs fine-tuned specifically for RTL code gener- ation [8], [10], [15] [18]. Table I summarizes the key features of each one of these RTL-oriented fine-tuned LLMs. As shown, with no multi-modal support, these methods rely solely on unstructured text, and are particularly prone to halluci- nations due to a lack of structured context [19], increasing the risk of generating instructions that inaccurately reflect the intent or functionality of the original code. This is while the retrieval-augmented generation (RAG) has proved substantial improvement by incorporating additional relevant information, which enriches the LLM s understanding and improves output alignment [20]. Often, this information takes the form of structured data, such as graphs, and recent research has inves- tigated the ability of LLMs to understand and process these graph structures effectively [19], [21] [24]. These studies have demonstrated that LLMs can effectively capture and inter- pret relationships and patterns within structured data which represent complex dependencies and hierarchies. Despite the 3The fully closed nature of these commercial LLMs discourages their use on proprietary documents, design specifications, and data, as organizations are unwilling to risk the exposure of their confidential designs. Open-source RTL Designs Prompting for Specification Dataset of RTL Designs Corresponded (generated) Instructions: Specification, Func. Desc., Arch. Notes, etc. Open-source pre-trained LLM Instruction-tuning (fine-tuning on pairs of codes instructions) (on Commercial) (on Open-Source) Open-source fine-tuned LLM RTL Code generation (using tuned model) Testing (evaluation) Refinement Fig. 1: The overview of Fine-tuning and (Potential Directions of) Refinement using Open-Source LLMs for Specializing of RTL Code Generation. growing interest in applying LLMs to graph-based data, their potential for analyzing RTL code remains almost unexplored. As LLMs struggle with hardware design due to their limited understanding of RTL s concurrent nature, representing code as graphs can address this by capturing the hierarchy, depen- dencies, and relationships between components. This struc- tured encoding improves context awareness, enabling LLMs to produce outputs that are more accurate and aligned with the intended functionality of the design.\n\n--- Segment 4 ---\nAs LLMs struggle with hardware design due to their limited understanding of RTL s concurrent nature, representing code as graphs can address this by capturing the hierarchy, depen- dencies, and relationships between components. This struc- tured encoding improves context awareness, enabling LLMs to produce outputs that are more accurate and aligned with the intended functionality of the design. To address this need, this paper introduces RTL , the first multi-modal graph- augmented fine-tuned LLM designed for enhanced RTL code generation. In RTL , a unique graph-based representation of designs will be incorporated as an supporting embedding during the instruction generation and fine-tuning phases, which improves both the model s functional and structural under- standing. The main contributions of RTL are as follows: (1) With an automated RTL-to-graph mechanism, which en- codes each training dataset entry into control flow graphs (CFG) and data flow graphs (DFG), we introduce a new fine- tuning mechanism that relies on instructions generated based not only on the RTL code itself but also on its corresponding CFG and DFG4. To the best of our knowledge, RTL is the first LLM-assisted RTL code generator to enhance LLM efficiency by combining multiple data formats. (2) In RTL , a 100K training dataset has been curated us- ing well-established open-source repositories such as GitHub, Bitbucket, and Opencores. The dataset entries are diverse, high-star rated, and contain critical keywords structs5 in RTL generation. This diversity is crucial for LLM fine-tuning that can avoid overfitting to specific patterns. Also, to improve code quality, an LLM-assisted refinement (pruning) has been imple- mented to refine and optimize the collected code samples. (3) We evaluated the latest foundational and advanced models, compared with our new model, highlighting that our multi- modal fine-tuning strategy sets a new benchmark in RTL coding. We also plan to make RTL a fully open-source model with its 100K instruction tuning dataset to support collaboration in EDA and chip design community. II.\n\n--- Segment 5 ---\nWe also plan to make RTL a fully open-source model with its 100K instruction tuning dataset to support collaboration in EDA and chip design community. II. RELATED WORK A. LLM for RTL Code Generation While recent advancements in hardware design automation have shown the effectiveness of adapting LLMs for specialized tasks like EDA automation and optimization, e.g., scripting [26], error interpretation [27], assistant chatbot for design flow [28], etc., numerous efforts have been made to fine-tune and pre-train models for RTL (Verilog) code generation: 4While RTL code (text) is for syntactical and semantical perspective, its CFG and DFG is for structural perspective of the circuit. This is conceptually a cross-modality fusion, in which the model integrate insights from both the RTL code (syntax and semantic) and its graph structures (structure). 5Structs include (but not limited to) module, port, wire, reg, procedural blocks (e.g., always and initial), control flow constructs (e.g., if-else and case), instantiation, FSM, parameters and constants, generate constructs, arrays and memories, etc. (i) VeriGen [15] is an early attempt that compiled Verilog files from GitHub and textbooks for training dataset. De- spite assembling a substantial dataset, the lack of proper pre-processing and organization led to inconsistencies which causes the fine-tuned CodeGen model to often generate non- functional Verilog code with syntax errors. (ii) RTLCoder [10] is another early endeavor that has lever- aged GPT-3.5 for synthesizing code-instruction pairs by ex- tracting RTL-specific keywords (to overcome dataset scarsity and code generation quality). However, its dependence on GPT-3.5 s embedded knowledge limited code diversity. This limitation prompted subsequent efforts to enrich data diversity through augmentation techniques. (iii) OriGen [8] advanced RTL code generation by introducing code-to-code augmentation and a self-reflection mechanism. The former diversifies the dataset with semantically equivalent but syntactically varied Verilog code, while the latter uses compiler feedback to iteratively correct errors, addressing VeriGen s inconsistencies and improving code quality.\n\n--- Segment 6 ---\n(iii) OriGen [8] advanced RTL code generation by introducing code-to-code augmentation and a self-reflection mechanism. The former diversifies the dataset with semantically equivalent but syntactically varied Verilog code, while the latter uses compiler feedback to iteratively correct errors, addressing VeriGen s inconsistencies and improving code quality. (iv) BetterV [16], building upon OriGen s idea of augment- ing data and integrating feedback, extends the capabilities of Verilog code generation by introducing a controlled text generation framework tailored specifically for Verilog, drawing parallels with C programs to help LLMs better comprehend Verilog semantics. It employs generative discriminators to optimize the Verilog for Power, Performance, and Area (PPA) while also incorporating data augmentation techniques to address data scarcity issues. (v) AutoVCoder [17] focuses on addressing the limitation of diversity and domain-specific accuracy in RTL code gener- ation using a two-round fine-tuning process to boost LLM performance in Verilog code generation. AutoVCoder also incorporated a domain-specific RAG module to constructively enhance prompts, which improved the syntactic and functional correctness of the generated code. (vi) CodeV [25] leverages LLMs for code summarization rather than generation, shifting the focus from producing Verilog code from natural language to generating detailed descriptions from Verilog code. By processing 165K Ver- ilog modules from GitHub and fine-tuning with multi-level summarization, CodeV enhances training datasets with rich description-code pairs, ensuring both syntactic accuracy and semantic depth for high-quality Verilog representations. (vii) CraftRTL [18] introduces an approach that includes constructing correct-by-construction data, such as Karnaugh Maps, state-transition diagrams, and waveforms, which im- prove the ability of LLMs to interpret additional information for LLM fine-tuning. In addition, CraftRTL employs an auto- mated framework that uses LLMs to generate error reports at various training checkpoints. MAGE [29] enhances RTL generation using a multi-agent system with high-temperature sampling and Verilog-state checkpointing, but for evaluation, we focus on approaches that improve model performance through fine-tuning and data collection.\n\n--- Segment 7 ---\nIn addition, CraftRTL employs an auto- mated framework that uses LLMs to generate error reports at various training checkpoints. MAGE [29] enhances RTL generation using a multi-agent system with high-temperature sampling and Verilog-state checkpointing, but for evaluation, we focus on approaches that improve model performance through fine-tuning and data collection. While recent advancements in RTL code generation have shown promising improvements, they lack (i) capturing TABLE I: Top View of Existing LLM-assisted Studies for RTL Code Generation.\n\n--- Segment 8 ---\nMAGE [29] enhances RTL generation using a multi-agent system with high-temperature sampling and Verilog-state checkpointing, but for evaluation, we focus on approaches that improve model performance through fine-tuning and data collection. While recent advancements in RTL code generation have shown promising improvements, they lack (i) capturing TABLE I: Top View of Existing LLM-assisted Studies for RTL Code Generation. Model Key Novelty Training Dataset, [Size] Fine-tuned Model Multi-modal HW Efficiency VeriGen [15] Fine-tuning on Dataset collected from GitHub and Textbooks Open-source, GitHub and Textbooks, [not listed] CodeGen-16B No None RTLCoder [10] GPT-3.5-based Code-Instruction Pair Synthesis Open-source, Synthesized, [27K] Mistral-7B DeepSeek-Coder-6.7b No None BetterV [16] Applying Controllable Text Generation w Discriminators for Engineering Optimization Closed-source, From internet, [not listed] CodeLlama-7B DeepSeek-Coder-6.7b-Instruct Code Qwen1.5-7B No Area Improvement OriGen [8] Code-to-code Augmentation, Self-reflection for Fixing Open-source, [222K] DeepSeek-Coder-7B No Iterative Functional Correctness Check AutoVCoder [17] Domain-specific RAG with Two-round LLM fine-tuning for Constructive Prompting Collected from Github, [not listed] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No (Text and Retrieval ) RAG-based Optimization CodeV [25] Multi-Level Summarization for Verilog Generation Close-source Github, [165K] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No Code Generation Improvement CraftRTL [18] Correct-by-construction data Synthetic and GitHub, [80.1K] Codellama-7B DeepSeek-Coder-6.7B Starcoder2-15B No Fine-tuning Correction RTL Structural-based Optimization (Graph Embedding for Instruction Tuning) Open-source, [200K] Codellama-7B Yes (Graph and Text) Structural Optimization, Area and Delay Improvement : This RAG is to identify the piece of data (RTL code).\n\n--- Segment 9 ---\nWhile recent advancements in RTL code generation have shown promising improvements, they lack (i) capturing TABLE I: Top View of Existing LLM-assisted Studies for RTL Code Generation. Model Key Novelty Training Dataset, [Size] Fine-tuned Model Multi-modal HW Efficiency VeriGen [15] Fine-tuning on Dataset collected from GitHub and Textbooks Open-source, GitHub and Textbooks, [not listed] CodeGen-16B No None RTLCoder [10] GPT-3.5-based Code-Instruction Pair Synthesis Open-source, Synthesized, [27K] Mistral-7B DeepSeek-Coder-6.7b No None BetterV [16] Applying Controllable Text Generation w Discriminators for Engineering Optimization Closed-source, From internet, [not listed] CodeLlama-7B DeepSeek-Coder-6.7b-Instruct Code Qwen1.5-7B No Area Improvement OriGen [8] Code-to-code Augmentation, Self-reflection for Fixing Open-source, [222K] DeepSeek-Coder-7B No Iterative Functional Correctness Check AutoVCoder [17] Domain-specific RAG with Two-round LLM fine-tuning for Constructive Prompting Collected from Github, [not listed] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No (Text and Retrieval ) RAG-based Optimization CodeV [25] Multi-Level Summarization for Verilog Generation Close-source Github, [165K] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No Code Generation Improvement CraftRTL [18] Correct-by-construction data Synthetic and GitHub, [80.1K] Codellama-7B DeepSeek-Coder-6.7B Starcoder2-15B No Fine-tuning Correction RTL Structural-based Optimization (Graph Embedding for Instruction Tuning) Open-source, [200K] Codellama-7B Yes (Graph and Text) Structural Optimization, Area and Delay Improvement : This RAG is to identify the piece of data (RTL code). It has nothing to do with cross-modality understanding.\n\n--- Segment 10 ---\nModel Key Novelty Training Dataset, [Size] Fine-tuned Model Multi-modal HW Efficiency VeriGen [15] Fine-tuning on Dataset collected from GitHub and Textbooks Open-source, GitHub and Textbooks, [not listed] CodeGen-16B No None RTLCoder [10] GPT-3.5-based Code-Instruction Pair Synthesis Open-source, Synthesized, [27K] Mistral-7B DeepSeek-Coder-6.7b No None BetterV [16] Applying Controllable Text Generation w Discriminators for Engineering Optimization Closed-source, From internet, [not listed] CodeLlama-7B DeepSeek-Coder-6.7b-Instruct Code Qwen1.5-7B No Area Improvement OriGen [8] Code-to-code Augmentation, Self-reflection for Fixing Open-source, [222K] DeepSeek-Coder-7B No Iterative Functional Correctness Check AutoVCoder [17] Domain-specific RAG with Two-round LLM fine-tuning for Constructive Prompting Collected from Github, [not listed] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No (Text and Retrieval ) RAG-based Optimization CodeV [25] Multi-Level Summarization for Verilog Generation Close-source Github, [165K] Codellama-7B DeepSeek-Coder-6.7B CodeQwen1.5-7B No Code Generation Improvement CraftRTL [18] Correct-by-construction data Synthetic and GitHub, [80.1K] Codellama-7B DeepSeek-Coder-6.7B Starcoder2-15B No Fine-tuning Correction RTL Structural-based Optimization (Graph Embedding for Instruction Tuning) Open-source, [200K] Codellama-7B Yes (Graph and Text) Structural Optimization, Area and Delay Improvement : This RAG is to identify the piece of data (RTL code). It has nothing to do with cross-modality understanding. the hierarchical structure of designs, (ii) comprehending data and control flow, and (iii) addressing the intrinsic concurrency of hardware designs.\n\n--- Segment 11 ---\nIt has nothing to do with cross-modality understanding. the hierarchical structure of designs, (ii) comprehending data and control flow, and (iii) addressing the intrinsic concurrency of hardware designs. Prior NLP-based studies [19], [30], [31] have demonstrated the power of integrating structured knowledge to enhance reasoning and boost inter- pretability in LLMs. Inspired by these works, our proposed RTL incorporates graph-based knowledge of RTL design to bridge the gap between design abstraction and design phase. B. Graph Prompt Learning and Engineering Since the advent of LLMs, researchers have been exploring ways to embed graph data into the input of LLMs (as an embedding to use as in-context learning) to enable reasoning over graph-structured information [19], [22] [24], [31] [33]. Fatemi et al. [24] evaluated the encoding graph-structured data as text for LLMs. A key observation of this study is that LLM performance on graph reasoning tasks is highly sensitive to the chosen encoding method, the type of task, and the structure of the graph, hence emphasizing on selecting appropriate graph encoding techniques is paramount. Perozzi et al. [19] proposed a novel approach that leverages graph neural networks (GNNs) to encode data into embeddings instead of textualizing graphs. They introduced GraphToken, a parameter-efficient method explicitly designed to encode structured graph data for LLMs. GraphToken learns an encod- ing function that augments prompts with explicit structured information. Unlike GraphToken, GraphLLM [22] adopts an end-to-end approach, integrating graph learning models with LLMs. It employs a graph transformer to process graph structures directly, enhancing both accuracy and efficiency. Inspired by these approaches, we also leverage textualized graph representations to integrate graph data into LLMs. Specifically, we convert graph-structured data from RTL codes, such as CFG and DFG, into textual formats, which are then used alongside RTL codes to generate meaningful instructions. By providing both the graph and corresponding code, we enable LLMs to effectively reason about and generate complex hardware design instructions.\n\n--- Segment 12 ---\nSpecifically, we convert graph-structured data from RTL codes, such as CFG and DFG, into textual formats, which are then used alongside RTL codes to generate meaningful instructions. By providing both the graph and corresponding code, we enable LLMs to effectively reason about and generate complex hardware design instructions. Small to medium size V codes 100-300 lines Syntax-variant Semantic-variant High star (High quality) Code Refinement Generality, Diversity, Originality Remove similar modules Remove hierarchical w o instantiation Remove gate-level V codes Remove non-synthesizable codes Code Extract Graph Generation High-level graph generation Hierarchical connections Inter-module (Inter-structs) Inter-module data flow graph Inter-module control flow graph Intra-module control flow graph using PyVerilog Synthesizer LLM Textualizing Graph Graph embedding Textualized w NN-based featuring Semantic node attributes Timing constructs (sequential) Normalizing (name convention) Preserving state machines (FSM) Separated CFG and DFG Graph embedding Code embedding to GPT-4o Instruction Generation RTL dataset Graph-enhanced instruction Verilog code Sweeping dataset size from 5K (code, Graphed inst) to 20K (code, Graphed inst) Fine-tuned Llama3.1 for RTL Generation RTL VerilogEval for Verification Instruction-tuning on Llama3.1 7B Fig. 2: RTL Overview: Text and Graph Embedding for LLM Fine-Tuning for RTL Code Generation.\n\n--- Segment 13 ---\nSmall to medium size V codes 100-300 lines Syntax-variant Semantic-variant High star (High quality) Code Refinement Generality, Diversity, Originality Remove similar modules Remove hierarchical w o instantiation Remove gate-level V codes Remove non-synthesizable codes Code Extract Graph Generation High-level graph generation Hierarchical connections Inter-module (Inter-structs) Inter-module data flow graph Inter-module control flow graph Intra-module control flow graph using PyVerilog Synthesizer LLM Textualizing Graph Graph embedding Textualized w NN-based featuring Semantic node attributes Timing constructs (sequential) Normalizing (name convention) Preserving state machines (FSM) Separated CFG and DFG Graph embedding Code embedding to GPT-4o Instruction Generation RTL dataset Graph-enhanced instruction Verilog code Sweeping dataset size from 5K (code, Graphed inst) to 20K (code, Graphed inst) Fine-tuned Llama3.1 for RTL Generation RTL VerilogEval for Verification Instruction-tuning on Llama3.1 7B Fig. 2: RTL Overview: Text and Graph Embedding for LLM Fine-Tuning for RTL Code Generation. TABLE II: Main Targeted Keyword in Dataset Collection in RTL Category Struct-based Keywords Structural Constructs module, endmodule, input, output, inout, wire, reg, assign, generate, endgenerate, parameter, localparam, always ( ), begin, end Sequential Logic always, always (posedge clk), always (negedge clk), posedge, negedge, if, else, case, default Combinational Logic assign, case, casex, casez, and, or, not, nand, nor, xor, xnor, mux, demux, generate, genvar Memory and Storage always (posedge clk or negedge reset), Flip-flop constructs (e.g., if(enable)), RAM, ROM, initial Data Path add, sub, mul, div, , , , -, , , , , ˆ, State Machines always (posedge clk), case, endcase, default, parameter, localparam, idle, current, next Hierarchy module instantiation, Dot (.port_name(xyz)) Category Context-based Keywords Arith Logic adder, subtractor, multiplier, divider, alu Sequential Logic dff, register, shift_register, counter Memory register_file, register_bank, fifo, cache Control Logic encoder, decoder, arbiter, bus_controller Data Transfer uart, spi, i2c, ethernet, input_buffer, output_buffer, parity, hamming, crc Signal Proc.\n\n--- Segment 14 ---\n2: RTL Overview: Text and Graph Embedding for LLM Fine-Tuning for RTL Code Generation. TABLE II: Main Targeted Keyword in Dataset Collection in RTL Category Struct-based Keywords Structural Constructs module, endmodule, input, output, inout, wire, reg, assign, generate, endgenerate, parameter, localparam, always ( ), begin, end Sequential Logic always, always (posedge clk), always (negedge clk), posedge, negedge, if, else, case, default Combinational Logic assign, case, casex, casez, and, or, not, nand, nor, xor, xnor, mux, demux, generate, genvar Memory and Storage always (posedge clk or negedge reset), Flip-flop constructs (e.g., if(enable)), RAM, ROM, initial Data Path add, sub, mul, div, , , , -, , , , , ˆ, State Machines always (posedge clk), case, endcase, default, parameter, localparam, idle, current, next Hierarchy module instantiation, Dot (.port_name(xyz)) Category Context-based Keywords Arith Logic adder, subtractor, multiplier, divider, alu Sequential Logic dff, register, shift_register, counter Memory register_file, register_bank, fifo, cache Control Logic encoder, decoder, arbiter, bus_controller Data Transfer uart, spi, i2c, ethernet, input_buffer, output_buffer, parity, hamming, crc Signal Proc. filter, fft, dft, mac, cordic Interconnect axi, wishbone, apb, crossbar, bus_switch, bridge Clock pll, clock_divider, prescaler, timer, stopwatch III. PROPOSED MODEL: RTL Fig. 2 presents a top-down view of our proposed model, RTL , which is structured into five key steps: (1) RTL code collection, (2) RTL code refinement, (3) RTL CFG DFG generation, (4) instruction generation (based on RTL code and CFG DFG), (5) graph-enhanced instruction-tuning.\n\n--- Segment 15 ---\nPROPOSED MODEL: RTL Fig. 2 presents a top-down view of our proposed model, RTL , which is structured into five key steps: (1) RTL code collection, (2) RTL code refinement, (3) RTL CFG DFG generation, (4) instruction generation (based on RTL code and CFG DFG), (5) graph-enhanced instruction-tuning. These steps, each detailed below, collectively prepare the fine-tuned LLM for high-quality RTL code generation. A. Collecting RTL Dataset from Repositories To gather a high-quality RTL code dataset, i.e., from GitHub, Bitbucket, and Opencores, we targeted a list of most common keywords relevant to RTL design (see Table II. For each keyword, we generated 10 related sub-keywords to capture a wider range of code (maximizing diversity). This keyword expansion allowed us to cover more specific design cases, including real common use cases in hardware design. We filtered out data with less than 100 lines and more than 300 lines to maintain consistency and focus on small-sized designs, which balance complexity and make them ideal for realistic use cases and model training. To ensure that we selected the high quality and reliable RTL codes, we ranked GitHub repositories by their star count, as- suming that more popular repositories indicated by a higher number of stars likely contain well-maintained, reliable code. From these top-ranked repositories, we extracted RTL code files with high star counts, as this metric often correlates with quality and community validation. In addition to ranking by popularity, we filtered out test- benches and netlists, focusing exclusively on RTL (behavioral) design files. We also ensured that the collected RTL codes contained all modules with their needed decalaration (for hierarchical design). By considering these extra steps for RTL code collection, we create a clean, relevant dataset of RTL design code suitable for training purposes. B. RTL Code Refinement Using GPT While state-of-the-art studies focuses on using either machine-generated (LLM-based) or human-created RTL code for training datasets, in RTL , we employ a hybrid approach, where we collect RTL codes ( III-A) and refine them using the machine (here GPT-4o).\n\n--- Segment 16 ---\nBy considering these extra steps for RTL code collection, we create a clean, relevant dataset of RTL design code suitable for training purposes. B. RTL Code Refinement Using GPT While state-of-the-art studies focuses on using either machine-generated (LLM-based) or human-created RTL code for training datasets, in RTL , we employ a hybrid approach, where we collect RTL codes ( III-A) and refine them using the machine (here GPT-4o). The refinement process involved a structured prompt that guided GPT through several key steps to standardize and enhance each RTL module (making the RTL code consistent, syntactically correct, self-contained, and aligned with the requirements for effective model training): (i) Dependency Removal: For modules dependent on external files or submodules instantiated within the main module6, GPT embedded the logic of these dependencies directly within the code. External module instantiations were replaced by their corresponding internal logic, creating self-contained modules that no longer required external dependencies. (ii) Variable Definition and Initialization: In large RTL projects, libraries or headers often define global variables. However, when assembling sub-modules from these hierarchi- cal projects, these reference files may not always be included in the training dataset. So, GPT is called to set value to these undefined variables and functions (infers typical use cases or context-based values), creating self-contained, standalone modules for enhanced training utility. (ii) Syntax Check Correction and Synthesizability: GPT is in- voked to check7 correct syntactical structures to adhere to standard RTL (Verilog) syntax, addressing common elements such as operator usage, and supported constructs. Additionally, to validate the correctness of the collected and refined RTL codes, a basic synthesis flow was performed using Yosys [34]8. C. High-level Graph Generation from RTL Codes There are various methods to encode text into graphs for LLMs, such as using GNNs [19], and graph convolutional networks (GCNs) [33], and directly integrating them with LLMs. Some representations are particularly well-suited for LLMs as they balance structural accuracy with textual clarity [24]. In RTL , we employ textual descriptions of graphs because they offer better interpretability for LLMs, making it easier for the models to understand hierarchies.\n\n--- Segment 17 ---\nSome representations are particularly well-suited for LLMs as they balance structural accuracy with textual clarity [24]. In RTL , we employ textual descriptions of graphs because they offer better interpretability for LLMs, making it easier for the models to understand hierarchies. By potentially leveraging these descriptions as RAGs, we enable LLMs to tackle more complex reasoning tasks in auto-debugging, optimization, and verification. We follow these steps to generate textual graphs in RTL : (1) Flattening (embedding all modules into the main graph) that is for designs with instantiated modules (hierarchical). (2) DFG generation (module-level to I O-level) that is for data movement form inputs to outputs. 6While we exclude code dependencies on external sources (at III-A), GPT- based auto-completion is used to fix incomplete code fragments. 7Each sample has module and endmodule with a procedural block (e.g., assign, .instance(port) , while inputs (in inout ports) are connected to outputs (out inout ports). 8Codes with synthesis error are excluded from the further steps and training. TABLE III: RTL vs. base CodeLlama-7B-instruct and GPT-4. Evaluated Model no. of params Open-Source? VerilogEval [39] Only HumanEval k 1 k 5 k 10 GPT-4 N A 43.5 55.8 N A CodeLlama-based 7B 18.2 22.7 24.3 RTL 5K Trained 7B 23.7 28.2 30.7 RTL 10K Trained 26.2 30.1 33.3 RTL 15K Trained 28.2 32.6 34.6 RTL 20K Trained 29.4 34.6 37.1 RTL 50K Trained 41.3 47.1 50.5 RTL 100K Trained 54.3 60.8 65.2 RTL 200K Trained 59.9 68.8 72.1 : HumanEval ensures RTL evaluation aligns with real-world data [40]. (3) CFG generation (module-level to I O-level) that is for control signals form inputs to outputs. (4) Adding node attributes that includes type (gate, module, input, output, etc.)\n\n--- Segment 18 ---\n(3) CFG generation (module-level to I O-level) that is for control signals form inputs to outputs. (4) Adding node attributes that includes type (gate, module, input, output, etc.) and function (arith, storage, logic, etc.). (5) Adding temporal behavior of nodes that determines the sequence of operation (based on sequencing9). It also includes feedback loops in sequential circuits (e.g., FSMs). Using these steps in RTL , we design all graphs at a high level, keeping details minimal to maintain a balance between abstraction and usability. Our primary focus is on defining graphs at the module level, avoiding unnecessary details that could overwhelm the model. This approach aligns closely with how hardware engineers conceptualize RTL code. D. Graph-enhanced Instruction Generation To create code-instruction pairs for fine-tuning in RTL , relying on in-context learning [36], the prompting includes both code snippets and their associated (textualized) graph- based representations. By integrating detailed information from both the code and its graphical representations, the LLM generates instructions that are more accurate, informative, and aligned with the actual functionality of the hardware module. The instructions encapsulate complex control flows and data interactions (from CFG and DFG) in a clear and concise manner. By using CFG and DFG as additional embeddings, the LLM enhances its understanding of critical component interactions and the intended purpose of various modules. This results in RTL code that is more detailed, precise, and less likely to include inaccuracies or hallucinations. E. Fine-Tuning over Code-Instruction Pairs At the end, once the pairs of RTL codes and (graph- enghanced) instructions are ready as our dataset, we finetune the base model (i.e., CodeLlama [37]) on this dataset10. IV. EXPERIMENTS To evaluate the performance of RTL , we fine-tune the CodeLlama-7B-Instruct as the targeted generative LLM. All experiments are conducted for 1 epoch (to avoid over-fitting, 9BMC engines can be used for iterations to collect sequencing [35]. 10While DeepSeek [38] could obtain superior outcomes as the base model, we opted for CodeLlama [37] to evaluate the genuine impact of incorporating graph structures during fine-tuning.\n\n--- Segment 19 ---\nAll experiments are conducted for 1 epoch (to avoid over-fitting, 9BMC engines can be used for iterations to collect sequencing [35]. 10While DeepSeek [38] could obtain superior outcomes as the base model, we opted for CodeLlama [37] to evaluate the genuine impact of incorporating graph structures during fine-tuning. as we observed over-fitting when using more epochs) us- ing PyTorch on NVIDIA L4 with the learning rate at 2e- 4. Additionally, for RTL code refinement, graph refinement, and instruction generation, GPT-4 has been engaged, costing approximately 84 per 1000 samples. To assess the RTL performance, we utilized two bench- marking frameworks: VerilogEval11 [39] and RTLLM [41]. Both benchmarks employ the widely recognized metric to evaluate the correctness of the generated code. To fine-tune the model for RTL code generation, we lever- age the LoRA (Low-Rank Adaptation) [42] technique (enhanc- ing RTL-oriented capabilities while maintaining performance). The optimization process employs the AdamW optimizer, configured with β1 0.9 and β2 0.99, and a cosine decay schedule for the learning rate. A warm-up phase is included with a ratio of 0.03, and training batch size is 2. We compared RTL against several existing models, in- cluding Verigen [15], RTLCoder [10], BetterV [16], Ori- gen [8], AutoVCoder [17] and CraftRTL [18]. Additionally, CodeLlama-7B-instruct was used as a baseline to assess the improvements made by RTL , while comparison with GPT-4 has been also explored (to show RTL efficiency). A. Comparison with the Base Models Table III shows the performance comparison between RTL , the base CodeLlama-7B-instruct model, and GPT-4. The results indicate that as the training dataset for RTL grows, the quality of the generated code consistently improves. Notably, RTL outperforms GPT-4 when trained on a 100K dataset. Note that for the VerilogEval benchmark, we prioritize HumanEval as it more accurately reflects real-world data [40]. B.\n\n--- Segment 20 ---\nNote that for the VerilogEval benchmark, we prioritize HumanEval as it more accurately reflects real-world data [40]. B. Impact of data size The impact of dataset size on the accuracy of our model is illustrated in Figure 3. We trained the model using datasets collected up to 200K samples. This suggests that if we gather more data, we can achieve higher accuracy levels. The initial collected data (5K, 10K, and 15K) showed a gradual improve- ment in model accuracy, but with larger datasets, there is a clear trend of significant performance gains. The combination of high-quality data, augmenting instruction generation by graphs, and effective model training demonstrate the potential to achieve even higher accuracy levels with larger datasets. C. Comparison with the State-of-the-art Models Table IV presents a comprehensive comparison between the performance of RTL and other state-of-the-art models. While many competing models leverage both CodeLlama [37] and DeepSeek [38], all reported results are based on fine-tuned versions of CodeLlama-7B-Instruct to ensure a consistent and fair evaluation. The results demonstrate that, with an expanded training dataset, RTL outperforms these models, highlighting the critical role of multi-modal embedding, par- ticularly augmenting the embedding using CFGs and DFGs, in improving LLM-assisted RTL code generation. 11Built upon the revisited VerilogEval [40] is the percentage of problems solved within k attempts. 20 VerilogEval ( ) 30 40 50 60 70 80 0 20K 40K 60K 80K 100K 120K 140K 160K 180K 200K 0.1 0.2 0.3 0.4 0.5 Improve per 1K Training VerilogEval ( ) VerilogEval ( ) VerilogEval ( ) VerilogEval Improvement per 1K Dataset ( ) VerilogEval Improvement per 1K Dataset ( ) VerilogEval Improvement per 1K Dataset ( ) Training Dataset Size Fig. 3: Impact of dataset size on model accuracy. TABLE IV: Comparsion of RTL and state-of-the-art RTL Generators. Evaluated Model ps OSS?\n\n--- Segment 21 ---\nTABLE IV: Comparsion of RTL and state-of-the-art RTL Generators. Evaluated Model ps OSS? Tr Size VerilogEval [39] RTLLM 1.1 HumanEval ( ) Syn. Func. 1 5 10 GPT-4 N A - 43.5 55.8 N A 89.7 37.9 CodeLlama-7B-I 7B N A 18.2 22.7 24.3 62.1 10.3 VeriGen [15] 15B - 30.3 43.9 49.6 86.2 24.1 RTLCoder-DS [10] 6.7B 27K 41.6 50.1 53.4 93.1 48.3 BetterV-CQ [16] 7B - 46.1 53.7 58.2 N A N A OriGen [8] 7B 222K 54.4 60.1 64.2 N A 65.5 AutoVCoder-CQ [17] 7B - 48.5 55.9 N A 100 51.7 CodeV [25] 7B 165K 45.2 59.5 63.8 93.1 62.1 CraftRTL [18] 7B 80.1K 63.1 67.8 69.7 93.9 52.9 7B 50K 41.3 47.1 50.5 82.7 41.3 100K 54.3 60.8 65.2 86.2 44.8 200K 59.9 68.8 72.1 93.9 51.7 : Our focus is on works that involve both data collection and fine-tuning. D. Impact of Graph Utilization on Instruction Generation To evaluate the impact of graph-based augmentation on instruction generation quality in RTL , Table V compares models trained with and without textualized graph representa- tions. Training the model using instructions augmented with textualized graph representations results in a noticeable accu- racy improvement. Table V provides a representative ablation study based on a 5K dataset, showing up to a 5 increase in When scaled to a 100K dataset, the improvement grows significantly, reaching 18 , underscoring the value of graph augmentation for fine-tuning.\n\n--- Segment 22 ---\nTraining the model using instructions augmented with textualized graph representations results in a noticeable accu- racy improvement. Table V provides a representative ablation study based on a 5K dataset, showing up to a 5 increase in When scaled to a 100K dataset, the improvement grows significantly, reaching 18 , underscoring the value of graph augmentation for fine-tuning. E. A Simple Case Study: An ALU in RTL To gain deeper insights into the impact of graph-based in- struction generation, we performed an ablation study focusing on a RTL module for an arithmetic logic unit (ALU). The ALU takes two 4-bit inputs (A and B), performs a variety of arithmetic and logical operations based on a 3-bit control TABLE V: Ablation Study on Textualized Graph Rep. (TGR) 5K Datasets. Model Temp. VerilogEval [39] Model Temp. VerilogEval [39] 1 5 10 1 5 10 RTL w o TGR 0.6 22.4 23.7 25.6 RTL w TGR 0.6 23.7 27.5 29.4 RTL w o TGR 0.7 22.4 24.3 25.6 RTL w TGR 0.7 23.7 28.2 30.7 module ALU ( input [3:0] A, input [3:0] B, input [2:0] ALUOp, output reg [7:0] Result, output reg Zero); always ( ) begin case (ALUOp) 3'b000: Result A B; 3'b001: Result A - B; 3'b010: Result A B; 3'b011: Result A B; 3'b100: Result A B; default: Result 8 h00; endcase if (Result 8 h00) Zero 1; else Zero 0; end endmodule Instruction generated WITHOUT graph representation: Create a Verilog module for a 4-bit Arithmetic Logic Unit (ALU) named ALU. The module should have two 4-bit inputs (A and B) representing operands, a 3-bit input (ALUOp) to select the operation, an 8-bit output (Result) for the operation result, and a 1-bit output (Zero) as a flag indicating if the result is zero.\n\n--- Segment 23 ---\nVerilogEval [39] 1 5 10 1 5 10 RTL w o TGR 0.6 22.4 23.7 25.6 RTL w TGR 0.6 23.7 27.5 29.4 RTL w o TGR 0.7 22.4 24.3 25.6 RTL w TGR 0.7 23.7 28.2 30.7 module ALU ( input [3:0] A, input [3:0] B, input [2:0] ALUOp, output reg [7:0] Result, output reg Zero); always ( ) begin case (ALUOp) 3'b000: Result A B; 3'b001: Result A - B; 3'b010: Result A B; 3'b011: Result A B; 3'b100: Result A B; default: Result 8 h00; endcase if (Result 8 h00) Zero 1; else Zero 0; end endmodule Instruction generated WITHOUT graph representation: Create a Verilog module for a 4-bit Arithmetic Logic Unit (ALU) named ALU. The module should have two 4-bit inputs (A and B) representing operands, a 3-bit input (ALUOp) to select the operation, an 8-bit output (Result) for the operation result, and a 1-bit output (Zero) as a flag indicating if the result is zero. Instruction generated WITH graph representation: Create the Verilog module, ALU, processes two 4-bit inputs (A and B) and performs one of five operations addition, subtraction, bitwise AND, bitwise OR, or multiplication based on the value of the 3-bit ALUOp signal. The computed result is stored in an 8-bit output register, Result. A case statement governs the operation selection: - ALUOp 000: Perform addition (A B). - ALUOp 001: Perform subtraction (A - B). - ALUOp 010: Perform bitwise AND (A B). - ALUOp 011: Perform bitwise OR (A B). - ALUOp 100: Perform multiplication (A B). For any other value of ALUOp, the Result is set to zero. A comparator checks if the Result is zero and sets the Zero flag accordingly (1 if the result is zero, otherwise 0). Fig.\n\n--- Segment 24 ---\nA comparator checks if the Result is zero and sets the Zero flag accordingly (1 if the result is zero, otherwise 0). Fig. 4: Instructions w and w o CFG and DFG and corresponding RTL code. signal (ALUOp), and outputs an 8-bit result (Result) along with a zero flag (Zero). As in Fig. 4, For this study, we generated instructions using two different configurations: one that included graph-based representations, as additional input, and one that used only the RTL code with no graph. As shown, the instruction with the graph representation provides a more technically detailed and complete description compared to the one without it. This version systematically defines the functionality of the 4-bit ALU by explicitly mapping ALUOp values to specific operations (addition, subtraction, bitwise AND, OR, and multiplication) through a case state- ment, while also handling invalid ALUOp cases by setting the Result to zero. It also clarifies the generation of the Zero flag, which indicates whether the computed Result is zero, ensuring robustness in implementation. In contrast, the instruction generated without the graph representation lacks this detailed description of control signal dependencies and default behaviors which leads to ambiguity. The inclusion of graph representation enhances clarity, depicting control flow, data dependencies, and interactions more precisely, which is crucial for accurate RTL code implementation. In this specific example, the instruction generated with the both DFG and CFG representation closely aligns with the code, which can effectively capture the control flow and structural dependencies of the RTL module. By explicitly depicting the relationships between control signals and data, it provides a more structured understanding, which leads to better training outcomes for LLMs. V. CONCLUSION This paper introduces RTL , an fully open-source model leveraging LLMs for efficient RTL code generation. RTL is a first-of-its-kind that integrates both textual (RTL code) and graph-based representations (CFG and DFG in textualized formats) to generate high-quality instruction-code pairs for LLM fine-tuning for RTL generation purposes. By using multi- modal fine-tuning approach, RTL achieves remarkable per- formance, surpassing state-of-the-art competitive models.\n\n--- Segment 25 ---\nRTL is a first-of-its-kind that integrates both textual (RTL code) and graph-based representations (CFG and DFG in textualized formats) to generate high-quality instruction-code pairs for LLM fine-tuning for RTL generation purposes. By using multi- modal fine-tuning approach, RTL achieves remarkable per- formance, surpassing state-of-the-art competitive models. The experimental results show that its success rate exceeds 70 in VerilogEval and 90 in RTLLM benchmark, all while operating on a fine-tuned version of CodeLlama-7B-Instruct. REFERENCES [1] RTL Model Dataset - Graph-enhanced LLM for RTL Generation, 2025. [2] T. B. Brown et al., Language models are few-shot learners, 2020. [Online]. Available: [3] E. Nijkamp et al., Codegen: An open large language model for code with multi-turn program synthesis, 2023. [Online]. Available: [4] M. Chen et al., Evaluating large language models trained on code, 2021. [Online]. Available: [5] Y. Li et al., Competition-level code generation with alphacode, Science, vol. 378, no. 6624, p. 1092 1097, Dec. 2022. [Online]. Available: [6] R. Anil et al., Palm 2 technical report, 2023. [Online]. Available: [7] Anthropic, Claude: An ai assistant built by anthropic, anthropic.com, 2023. [8] F. Cui et al., Origen:enhancing rtl code generation with code- to-code augmentation and self-reflection, 2024. [Online]. Available: [9] M. Akyash et al., Simeval: Investigating the similarity obstacle in llm-based hardware code generation, in Proceedings of the 30th Asia and South Pacific Design Automation Conference, ser. ASPDAC 25. New York, NY, USA: Association for Computing Machinery, 2025, p. 1002 1007. [Online].\n\n--- Segment 26 ---\nNew York, NY, USA: Association for Computing Machinery, 2025, p. 1002 1007. [Online]. Available: 3697624 [10] S. Liu et al., Rtlcoder: Outperforming gpt-3.5 in design rtl generation with our open-source dataset and lightweight solution, in 2024 IEEE International Workshop on LLM-Aided Design. IEEE, 2024. [11] N. Mashnoor et al., Llm-ift: Llm-powered information flow tracking for secure hardware, 2025. [Online]. Available: 2504.07015 [12] M. Akyash et al., Self-hwdebug: Automation of llm self-instructing for hardware security verification, in 2024 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2024, pp. 391 396. [13] Y. Fu et al., Gpt4aigchip: Towards next-generation ai accelerator design automation via large language models, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD). IEEE, 2023, pp. 1 9. [14] M. Akyash et al., Evolutionary large language models for hardware security: A comparative survey, in Great Lakes Symposium on VLSI (GLSVLSI), 2024, pp. 1 6. [15] S. Thakur et al., Verigen: A large language model for verilog code generation, 2023. [Online]. Available: [16] Z. Pei et al., Betterv: Controlled verilog generation with discriminative guidance, 2024. [Online]. Available: [17] M. Gao et al., Autovcoder: A systematic framework for automated verilog code generation using llms, arXiv preprint arXiv:2407.18333, 2024. [18] M. Liu et al., Craftrtl: High-quality synthetic data generation for verilog code models with correct-by-construction non-textual representations and targeted code repair, 2024. [Online]. Available: [19] B. Perozzi et al., Let your graph do the talking: Encoding structured data for llms, 2024. [Online].\n\n--- Segment 27 ---\nAvailable: [19] B. Perozzi et al., Let your graph do the talking: Encoding structured data for llms, 2024. [Online]. Available: 05862 [20] K. Guu et al., Realm: Retrieval-augmented language model pre- training, 2020. [Online]. Available: [21] Z. Luo et al., Graphinstruct: Empowering large language models with graph understanding and reasoning capability, 2024. [Online]. Available: [22] Z. Chai et al., Graphllm: Boosting graph reasoning ability of large language model, 2023. [Online]. Available: 05845 [23] X. He et al., G-retriever: Retrieval-augmented generation for textual graph understanding and question answering, 2024. [Online]. Available: [24] B. Fatemi et al., Talk like a graph: Encoding graphs for large language models, 2023. [Online]. Available: [25] Y. Zhao et al., Codev: Empowering llms for verilog generation through multi-level summarization, 2024. [Online]. Available: https: arxiv.org abs 2407.10424 [26] M. Liu et al., Chipnemo: Domain-adapted llms for chip design, 2024. [Online]. Available: [27] K. Chang et al., Data is all you need: Finetuning llms for chip design via an automated design-data augmentation framework, in Proceedings of the 61st ACM IEEE Design Automation Conference, ser. DAC 24. New York, NY, USA: Association for Computing Machinery, 2024. [Online]. Available: [28] H. Wu et al., Chateda: A large language model powered autonomous agent for eda, IEEE Transactions on Computer-Aided Design of Inte- grated Circuits and Systems, 2024. [29] Y. Zhao et al., Mage: A multi-agent engine for automated rtl code generation, 2024. [Online]. Available: [30] Q. Cai et al., Codegraph: Enhancing graph reasoning of llms with code, 2024. [Online]. Available: [31] L. Luo et al., Reasoning on graphs: Faithful and interpretable large language model reasoning, 2024. [Online].\n\n--- Segment 28 ---\nAvailable: [31] L. Luo et al., Reasoning on graphs: Faithful and interpretable large language model reasoning, 2024. [Online]. Available: https: arxiv.org abs 2310.01061 [32] Z. Luo et al., Graphinstruct: Empowering large language models with graph understanding and reasoning capability, 2024. [Online]. Available: [33] H. Ren et al., Graph convolutional networks in language and vision: A survey, Knowledge-Based Systems, vol. 251, p. 109250, 2022. [34] C. Wolf et al., Yosys-a free verilog synthesis suite, in Proceedings of the 21st Austrian Workshop on Microelectronics (Austrochip), vol. 97, 2013. [35] E. Clarke et al., Bounded model checking using satisfiability solving, Formal methods in system design, vol. 19, pp. 7 34, 2001. [36] Q. Dong et al., A survey on in-context learning, arXiv preprint arXiv:2301.00234, 2022. [37] B. Rozi ere et al., Code llama: Open foundation models for code, 2024. [Online]. Available: [38] D. Guo et al., Deepseek-coder: When the large language model meets programming the rise of code intelligence, arXiv preprint arXiv:2401.14196, 2024. [39] M. Liu et al., Verilogeval: Evaluating large language models for verilog code generation, in 2023 IEEE ACM International Conference on Computer Aided Design (ICCAD), 2023, pp. 1 8. [40] N. Pinckney et al., Revisiting verilogeval: Newer llms, in-context learn- ing, and specification-to-rtl tasks, arXiv preprint arXiv:2408.11053, 2024. [41] Y. Lu et al., Rtllm: An open-source benchmark for design rtl generation with large language model, 2023. [Online]. Available: [42] E. J. Hu et al., Lora: Low-rank adaptation of large language models, 2021. [Online]. Available:\n\n