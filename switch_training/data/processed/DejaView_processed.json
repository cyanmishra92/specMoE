{
  "source": "DejaView.pdf",
  "raw_length": 77296,
  "cleaned_length": 76665,
  "base_segments": 304,
  "augmented_segments": 609,
  "segments": [
    {
      "text": "Abstract —The emergence of virtual reality (VR) and aug- mented reality (AR) has revolutionized our lives by enabling a  360 °  artiﬁcial sensory stimulation across diverse domains, including, but not limited to, sports, media, healthcare, and gaming. Unlike the conventional planar video processing, where memory access is the main bottleneck, in  360 °  VR videos the compute is the primary bottleneck and contributes to more than 50%  energy consumption in battery-operated VR headsets. Thus, improving the computational efﬁciency of the video processing pipeline in a VR is critical.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "Thus, improving the computational efﬁciency of the video processing pipeline in a VR is critical. While prior efforts have attempted to address this problem through acceleration using a GPU or FPGA, none of them has analyzed the  360 °  VR pipeline to examine if there is any scope to optimize the computation with known techniques such as memoization. Thus, in this paper, we analyze the VR computation pipeline and observe that there is signiﬁcant scope to skip computations by leveraging the temporal and spatial locality in head orienta- tion and eye correlations, respectively, resulting in computation reduction and energy efﬁciency.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "Thus, in this paper, we analyze the VR computation pipeline and observe that there is signiﬁcant scope to skip computations by leveraging the temporal and spatial locality in head orienta- tion and eye correlations, respectively, resulting in computation reduction and energy efﬁciency. The proposed  D´ej`a View  design takes advantage of temporal reuse by memoizing head orientation and spatial reuse by establishing a relationship between left and right eye projection, and can be implemented either on a GPU or an FPGA. We propose both software modiﬁcations for existing compute pipeline and microarchitectural additions for further enhancement.",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "We propose both software modiﬁcations for existing compute pipeline and microarchitectural additions for further enhancement. We evaluate our design by implementing the software enhancements on an NVIDIA Jetson TX2 GPU board and our microarchitectural additions on a Xilinx Zynq-7000 FPGA model using ﬁve video workloads. Experimental results show that  D´ej`a View  can provide  34%  computation reduction and  17%  energy saving, compared to the state-of-the-art design.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "Experimental results show that  D´ej`a View  can provide  34%  computation reduction and  17%  energy saving, compared to the state-of-the-art design. Index Terms —Virtual Reality, Edge Computing, IoT,  360 ° Video Processing \nI. I NTRODUCTION \nRecent developments in technology, computing and com- munication have brought signiﬁcant changes to the lifestyle of common people by providing them access to increasingly sophisticated devices. Especially, VR and AR are now gaining traction because of their versatile nature of providing an immersive sensory experience, which is not possible with the conventional systems – especially in the domain of video streaming.",
      "type": "sliding_window",
      "tokens": 141
    },
    {
      "text": "Especially, VR and AR are now gaining traction because of their versatile nature of providing an immersive sensory experience, which is not possible with the conventional systems – especially in the domain of video streaming. They are emerging as one of the most important entertainment markets and Goldman Sachs predicts that, by 2025, around 79 million users will use online video streaming from the VR/AR ecosystem, resulting in a multi-billion dollar market [20], penetrating the ﬁelds of media streaming, VR gaming, education, medicine, communication and many more. Even today, more than 10 million users enjoy  360 ° videos \nusing Google Cardboard [10], Samsung Gear VR [44], and Oculus VR [8], to experience  360 ° video [7], art museum [9], live stadium [46], etc.",
      "type": "sliding_window",
      "tokens": 175
    },
    {
      "text": "Even today, more than 10 million users enjoy  360 ° videos \nusing Google Cardboard [10], Samsung Gear VR [44], and Oculus VR [8], to experience  360 ° video [7], art museum [9], live stadium [46], etc. The  360 ° videos are created by capturing scenes in all directions typically using omnidirectional cameras or a set of cameras. They are further encoded by the conventional video encoders, as if they are planar videos, for transmission efﬁciency.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "They are further encoded by the conventional video encoders, as if they are planar videos, for transmission efﬁciency. The video frames are transmitted to the users, who wear a portable VR headset (like Facebook Oculus or Google Cardboard), via  Youtube  or  Facebook 360  services [7], [61]. 360 ° video streaming creates an interactive and immersive environment by connecting the user and the video content; the users are allowed to move their heads’ orientation to enjoy the surroundings in all perspectives along with a 3D view, i.e., a different view for each of the eyes, and hence creating an illusion that the user is present at the scene rather than viewing it on a projected surface.",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "360 ° video streaming creates an interactive and immersive environment by connecting the user and the video content; the users are allowed to move their heads’ orientation to enjoy the surroundings in all perspectives along with a 3D view, i.e., a different view for each of the eyes, and hence creating an illusion that the user is present at the scene rather than viewing it on a projected surface. This immersive experience comes at the cost of additional computations - not only is the video being streamed,  the streaming itself changes with the head orientation. Moreover, streaming requires two projections for both the eyes.",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "Moreover, streaming requires two projections for both the eyes. As the 360 ° video is not in a planar format, the VR ecosystem converts it to a conformal 2D format by passing it through multiple stages of transformations. Thus, unlike planar videos, in  360 ° videos, speciﬁcally the projection computations for capturing the head movements and eye correlations, are sig- niﬁcantly computation-heavy, amounting to  59%  of the overall VR (headset) power budget.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Thus, unlike planar videos, in  360 ° videos, speciﬁcally the projection computations for capturing the head movements and eye correlations, are sig- niﬁcantly computation-heavy, amounting to  59%  of the overall VR (headset) power budget. Current head mounted VR devices use a GPU for this heavy computation. Since the head mounted VR devices are battery-backed, the computations that draw high power from the battery greatly hinder the experience of watching long  360 ° videos [39].",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Since the head mounted VR devices are battery-backed, the computations that draw high power from the battery greatly hinder the experience of watching long  360 ° videos [39]. This heavy computation has become an acceleration candi- date/target in previous works, by ofﬂoading the entire compu- tation, as is, to an accelerator (GPU [39], or FPGA [28]). However, prior works do not consider other avenues for optimizing the computation.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "However, prior works do not consider other avenues for optimizing the computation. In this context, this paper dives deep to understand the projection computation pipeline for ex- ploring available opportunities and optimizations for speedup as well as power savings. Since  head movement  and  cor- relations between the left and right eye projections  are the two critical components of the projection computation, we analyze and study them to explore possible opportunities to exploit these relations.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Since  head movement  and  cor- relations between the left and right eye projections  are the two critical components of the projection computation, we analyze and study them to explore possible opportunities to exploit these relations. Speciﬁcally, we analyze four scenarios, \n241 \n2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA) \n978-1-7281-4661-4/20/$31.00 ©2020 IEEE DOI 10.1109/ISCA45697.2020.00030 \namely,  InterFrame-IntraEye (EA) ,  IntraFrame-InterEye (AE) , IntraFrame-IntraEye (AA) , and  InterFrame-InterEye (EE) , that are critical in capturing the head movement and eye correlation for projection computation. Out of these four scenarios, we observe that  EA  computation for head orientation can be exploited for  temporal reuse/memoization  since there is little difference between two previous head orientations, and  AE computation for exploiting the correlation between both the eyes by  spatial reuse – correlating the coordinate relationship between both eyes .",
      "type": "sliding_window",
      "tokens": 250
    },
    {
      "text": "Out of these four scenarios, we observe that  EA  computation for head orientation can be exploited for  temporal reuse/memoization  since there is little difference between two previous head orientations, and  AE computation for exploiting the correlation between both the eyes by  spatial reuse – correlating the coordinate relationship between both eyes . Based on this observation, we develop computation optimization mechanisms for facilitating temporal reuse/memoization and spatial reuse that can be integrated with a VR projection computation pipeline to signiﬁcantly reduce energy consumption of the device. The proposed ar- chitecture is named  D´ej`a View , a play on the word D´ej`a vu, as it uses previous or  already seen  views.",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "The proposed ar- chitecture is named  D´ej`a View , a play on the word D´ej`a vu, as it uses previous or  already seen  views. To the best of our knowledge, this is the ﬁrst work that leverages head orientation and correlation between eyes to do efﬁcient memoization and in turn result in compute reduction in the VR video streaming domain. The major  contributions of the paper can be summarized as follows: \n•  From an open-source 360° VR video dataset [3],  we identify both temporal reuse and spatial locality that exists in user behavior .",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "The major  contributions of the paper can be summarized as follows: \n•  From an open-source 360° VR video dataset [3],  we identify both temporal reuse and spatial locality that exists in user behavior . We formally analyze the potential “input invari- ability” in the  projection computation  during  360 ° video streaming, which manifests in the head movement locality (temporal reuse) and the stationarity relationship between two eyes (spatial reuse). Such invariances are leveraged as reuse opportunities to reduce the compute-heavy projection computation.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "Such invariances are leveraged as reuse opportunities to reduce the compute-heavy projection computation. •  We design two complementary schemes to capture both temporal and spatial reuse opportunities. We propose a memoization scheme, called  EA , to capture recent head orientation data for temporal reuse, and for the spatial reuse, we design the  AE  scheme, which leverages the stationary relationship between two eyes to efﬁciently reduce the amount of projection computation.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "We propose a memoization scheme, called  EA , to capture recent head orientation data for temporal reuse, and for the spatial reuse, we design the  AE  scheme, which leverages the stationary relationship between two eyes to efﬁciently reduce the amount of projection computation. •  We implement both our schemes as a  software  enhancement to the existing compute pipeline in NVIDIA GPUs. To further exploit the energy efﬁciency, we also implement our hardware  prototype using an FPGA to evaluate the energy beneﬁts brought by the microarchitectural augmentations.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "To further exploit the energy efﬁciency, we also implement our hardware  prototype using an FPGA to evaluate the energy beneﬁts brought by the microarchitectural augmentations. Both the proposed software and hardware solutions are modular , and hence can be integrated to the existing pipeline with little change. •  We evaluate our integrated design, including both  EA  and AE , using an open-source 360° VR video dataset [3] with the traces of 20 users watching 5 different VR videos.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "•  We evaluate our integrated design, including both  EA  and AE , using an open-source 360° VR video dataset [3] with the traces of 20 users watching 5 different VR videos. Over- all, our experimental results show that, on an average,  D´ej`a View  can provide  54%  compute reduction, which translates to  28%  total energy savings compared to the baseline setup. Compared to a state-of-the-art scheme [28], our design provides  34%  reduction in projection computations, which translates to  17%  additional energy savings.",
      "type": "sliding_window",
      "tokens": 125
    },
    {
      "text": "Compared to a state-of-the-art scheme [28], our design provides  34%  reduction in projection computations, which translates to  17%  additional energy savings. Fig. 1: A  360 °  video processing pipeline on a battery-backed stereoscopic HMD with an Inertial Measurement Unit (IMU) and an SoC equipped with a GPU [28], [39].",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "1: A  360 °  video processing pipeline on a battery-backed stereoscopic HMD with an Inertial Measurement Unit (IMU) and an SoC equipped with a GPU [28], [39]. II. B ACKGROUND AND  M OTIVATION \nBefore getting into the details of the existing issues and possible solutions, we ﬁrst outline the computation pipeline of the state-of-the-art  360 ° VR streaming (Fig.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "B ACKGROUND AND  M OTIVATION \nBefore getting into the details of the existing issues and possible solutions, we ﬁrst outline the computation pipeline of the state-of-the-art  360 ° VR streaming (Fig. 1). Further, we describe the existing energy inefﬁciencies in processing  360 ° VR systems, to motivate our design for mitigating the com- putational inefﬁciencies by avoiding redundant computations.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "Further, we describe the existing energy inefﬁciencies in processing  360 ° VR systems, to motivate our design for mitigating the com- putational inefﬁciencies by avoiding redundant computations. A. 360 °  Video Streaming Pipeline \nThe key  difference  between a  360 ° VR video compared to a conventional 2D video is that the former provides content-rich immersive user experience.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "360 °  Video Streaming Pipeline \nThe key  difference  between a  360 ° VR video compared to a conventional 2D video is that the former provides content-rich immersive user experience. Wearing a head mounted display (HMD), a user navigates in a virtual world by  looking around , or  moving around  [55], to interact with the virtual world. As shown in Fig.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "As shown in Fig. 1, a typical VR HMD [39] has two major components: (i) an SoC with a video decoder, a GPU for processing projection computation, and a display controller, and (ii) a video buffer in DRAM for storing the decoded  360 ° frames and projected frames for both the left and right eyes. More speciﬁcally, the  360 ° video processing pipeline can be summarized as follows: Video Decoder:  The HMD receives encoded  360 ° video bitstream from the network (YouTube [61], Facebook-360 [7], etc).",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "More speciﬁcally, the  360 ° video processing pipeline can be summarized as follows: Video Decoder:  The HMD receives encoded  360 ° video bitstream from the network (YouTube [61], Facebook-360 [7], etc). Similar to 2D videos, the  360 ° video bitstreams are encoded in H.264/MPEG formats [19] for network efﬁciency. The next step is to decode the original frame from the bitstream, and today, this is mostly done using a hardware- based h264/MPEG decoder for more energy efﬁciency.",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "The next step is to decode the original frame from the bitstream, and today, this is mostly done using a hardware- based h264/MPEG decoder for more energy efﬁciency. After decoding, the  360 ° output frames are then buffered in the video buffer, waiting to be rendered. Projection:  Note that, the output frames from the decoder are still in the  spherical coordinate system .",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Projection:  Note that, the output frames from the decoder are still in the  spherical coordinate system . This is because, for encoding purpose, the original  360 ° videos are projected into the 2D plane (usually represented in 2D format such as Equirectangular [52], Cubemap [41], etc.). Therefore, unlike the 2D video processing where the display can directly read \n242 \n59% \n29% \n6%  6% \nCompute \nMemory Decode Display \n(a) Power breakdown.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Therefore, unlike the 2D video processing where the display can directly read \n242 \n59% \n29% \n6%  6% \nCompute \nMemory Decode Display \n(a) Power breakdown. (b) Overview of  360 °  video projection. (c) Head movement and pupillary distance as inputs.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "(c) Head movement and pupillary distance as inputs. Fig. 2: Overview of  360 °  video projection.",
      "type": "sliding_window",
      "tokens": 28
    },
    {
      "text": "2: Overview of  360 °  video projection. (a) Power breakdown consuming 3.4 Watts; (b) Projection pipeline taking head orientation and pupillary distance to compute projection matrices for both the eyes, which map  360 °  coordinates to 2D coordinates for generating stereoscopic frames. (c) Reusing projection matrices by exploiting relation between both eyes and fusing it with head orientation.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "(c) Reusing projection matrices by exploiting relation between both eyes and fusing it with head orientation. the video buffer to present the  decoded  frames, the  360 ° video frames require “additional rendering effort” to get displayed. More speciﬁcally, the rendering process is a projection from the  360 ° frame pixels’ 3D coordinates to the 2D frame pixels’ 2D coordinates on HMDs.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "More speciﬁcally, the rendering process is a projection from the  360 ° frame pixels’ 3D coordinates to the 2D frame pixels’ 2D coordinates on HMDs. The projection process considers two user-side aspects –  head orientation and pupillary dis- tance 1   – to render stereoscopic views or Field of View (FoV) frames for both eyes, towards the head direction. The head orientation is sensed by an inertial measurement unit (IMU) on the HMD as a triple [ Y aw ,  Pitch ,  Roll ] for projection computation 2 .",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "The head orientation is sensed by an inertial measurement unit (IMU) on the HMD as a triple [ Y aw ,  Pitch ,  Roll ] for projection computation 2 . For each frame, this computation is processed twice – one for left eye and the other for right eye – to reinforce users’ sense of depth. Display:  After the projection, the two generated FoV frames are stored in 2D format in the video buffer.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "Display:  After the projection, the two generated FoV frames are stored in 2D format in the video buffer. The display controller just needs to read them from DRAM to the screen. To summarize, compared to 2D video processing,  360 ° video processing incurs additional projection computation.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "To summarize, compared to 2D video processing,  360 ° video processing incurs additional projection computation. From our measurements collected from a smartphone [53] running a  360 ° VR application [11], even with extra computa- tion, the overall processing for rendering one  360 ° frame can be completed within 22  ms  on average (translating to 45 fps). However, since the whole computation/rendering process takes place on a battery-backed device [39], one needs to consider the “energy efﬁciency” of this computation, i.e., even though we can meet the performance requirements of such video, energy efﬁciency needs to be improved.",
      "type": "sliding_window",
      "tokens": 148
    },
    {
      "text": "However, since the whole computation/rendering process takes place on a battery-backed device [39], one needs to consider the “energy efﬁciency” of this computation, i.e., even though we can meet the performance requirements of such video, energy efﬁciency needs to be improved. B. Motivation \nTo understand the energy proﬁle in the current VR devices, we characterize the energy consumption of  360 ° video pro- cessing on a prototype [36] (conﬁgured similar to a com- mercial VR device [39], discussed in Sec.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "Motivation \nTo understand the energy proﬁle in the current VR devices, we characterize the energy consumption of  360 ° video pro- cessing on a prototype [36] (conﬁgured similar to a com- mercial VR device [39], discussed in Sec. V) in Fig. 2a.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "2a. Overall,  360 ° VR video processing consumes 3.4  Watts , which is  2 . 27 ×  the power compared to its planar counter- parts (1.5  Watts ).",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "27 ×  the power compared to its planar counter- parts (1.5  Watts ). We also observe that, unlike conventional planar video processing where  memory  is the main bottleneck ( 43% ), in  360 ° VR video processing,  compute  dominates the \n1 Pupillary distance is the distance, typically measured in millimeters, between the centers of the pupils of the eyes. 2 Yaw: vertical; Pitch: side-to-side; Roll: front-to-back.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "2 Yaw: vertical; Pitch: side-to-side; Roll: front-to-back. power consumption, constituting  59% . Previous studies have observed that the computation in  360 ° video processing is, mainly, the  projection transformation  [28].",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "Previous studies have observed that the computation in  360 ° video processing is, mainly, the  projection transformation  [28]. These observations motivate us to explore the potential opportunities for reducing the power/energy consumption in the  projection  stage. We illustrate the  360 ° video projection/projection transfor- mation 3   computation in Fig.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "We illustrate the  360 ° video projection/projection transfor- mation 3   computation in Fig. 2b. At a high level, we need two major inputs to generate the ﬁnal projected frames on the display.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "At a high level, we need two major inputs to generate the ﬁnal projected frames on the display. The ﬁrst input is from the user side, including the head orientation and pupillary distance. Since there is a small offset between the two eyes, the projection computation needs to cap- ture the pupillary distance to generate a separate view for each eye.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Since there is a small offset between the two eyes, the projection computation needs to cap- ture the pupillary distance to generate a separate view for each eye. Therefore, the output of the projection computation is  two projection matrices, each indicating the mapping between each coordinate in  360 ° video frame and a 2D coordinate on screen for the left and right eye. Note that, this projection process is quite compute-intensive.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Note that, this projection process is quite compute-intensive. Furthermore, in a typical VR headset, the above computation needs to repeat millions of times, for processing just one  360 ° frame. Our characterization indicates that, on average, around 2.3 GFLOPS is required for this projection transformation (details are discussed in Sec.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "Our characterization indicates that, on average, around 2.3 GFLOPS is required for this projection transformation (details are discussed in Sec. III). The second input is the decoded  360 ° frame that contains the pixel values.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "The second input is the decoded  360 ° frame that contains the pixel values. The decoded  360 ° frame is fed to the Projection Mapping stage, which uses the projection matrix, locates the coordinates in the decoded  360 ° frame, and moves their pixel values to the transformed 2D coordinates in the FoV frames. It is to be noted that, when a user’s head orientation is changed, the Projection Computation stage needs to  recompute the transformations to reﬂect the user’s head movement.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "It is to be noted that, when a user’s head orientation is changed, the Projection Computation stage needs to  recompute the transformations to reﬂect the user’s head movement. The VR headset allows users to freely move their heads and eyes at any time to any degree. Hence, the projection transformation computation has to be executed at every frame to reﬂect user movements in real-time, and the whole process is very compute intensive (36 times per second [3]) and power hungry.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "Hence, the projection transformation computation has to be executed at every frame to reﬂect user movements in real-time, and the whole process is very compute intensive (36 times per second [3]) and power hungry. However, it is too conservative to execute the projection transformation in a short period of time even for the same set of inputs. Intuitively, if the inputs of the transformation computation do not change, the output of the transformation will also be same.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "Intuitively, if the inputs of the transformation computation do not change, the output of the transformation will also be same. In fact, we observe the \n3 We use “projection transformation” and “ 360 ° video projection” inter- changeably. 243 \nFig.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "243 \nFig. 3: Detailed illustration of projection transformation. The projection transformation ﬁrst calculates the transformation matrix ( T  ), and then uses the transformation matrix to map each of the pixel coordinates to generate the projection matrices ( P ) for the FoV frames.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "The projection transformation ﬁrst calculates the transformation matrix ( T  ), and then uses the transformation matrix to map each of the pixel coordinates to generate the projection matrices ( P ) for the FoV frames. Then projection mapping stage uses this coordinate mapping ( P ) to move pixels from the original 360 frame  F 360  to the 2D frame  F . following two properties from a published  360 ° VR dataset [3], as shown in Fig.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "following two properties from a published  360 ° VR dataset [3], as shown in Fig. 2c: Head Orientation Proximity:  In a short period of time, the user’s head orientation is usually stable in a small space range (3D) or even still. In fact, from this dataset we have found that, the head orientation for users does not often change within around  150 ms  period of time.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "In fact, from this dataset we have found that, the head orientation for users does not often change within around  150 ms  period of time. Furthermore, even in cases where head orientation changes, the change is usually within a small range – in a few consecutive frames. Since two identical head orientations lead to the same projection matrices, one opportunity to reduce computation is to  memoize  a set of head orientations as well as their corresponding compute results.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Since two identical head orientations lead to the same projection matrices, one opportunity to reduce computation is to  memoize  a set of head orientations as well as their corresponding compute results. Vision Proximity:  It is to be emphasized that, even when the head orientation input for two computations are the same, the two eye coordinates can be different. Because of this, in current designs, the projection transformation is invoked  twice as it needs to generate two different transformation matrices for the left eye and the right eye.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "Because of this, in current designs, the projection transformation is invoked  twice as it needs to generate two different transformation matrices for the left eye and the right eye. On the other hand, the distance between the two eyes is small and is constant for a particular user 4 . The two transformation matrices are very “similar” as they inherit a relationship between them as a function of the small pupillary distance.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "The two transformation matrices are very “similar” as they inherit a relationship between them as a function of the small pupillary distance. Motivated by these observations, in the following sections, we explore and address two critical questions:  Can we identify the proximity in the projection computation? , and  Can we leverage this proximity to safely skip some computations to save energy?",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": ", and  Can we leverage this proximity to safely skip some computations to save energy? III. 360 ° V IDEO  P ROJECTION \nTo leverage the opportunities in the  360 ° video projection, we need to understand the execution of the entire projection processing in a  360 ° VR system.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "360 ° V IDEO  P ROJECTION \nTo leverage the opportunities in the  360 ° video projection, we need to understand the execution of the entire projection processing in a  360 ° VR system. We illustrate the details of  360 ° video projection in Fig. 3 as three stages (detailed background of this projection transformation can be found in [21], [27]).",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "3 as three stages (detailed background of this projection transformation can be found in [21], [27]). The ﬁrst stage,  Transformation  (denoted  a in Fig. 3, is to determine a transformation matrix by com- bining ﬁve different transforms.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "3, is to determine a transformation matrix by com- bining ﬁve different transforms. The second stage,  Projection Computation  (denoted  b  in Fig. 3), uses the transformation matrix and the 2D FoV coordinates for both eyes to obtain \n4 We used an averaged pupillary distance in our evaluations [27], [37].",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "3), uses the transformation matrix and the 2D FoV coordinates for both eyes to obtain \n4 We used an averaged pupillary distance in our evaluations [27], [37]. TABLE I: Projection Computation description. Label Description HO dependent?",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Label Description HO dependent? Known-time Eye-dependent? T 1 Rigid body No Compile-time Left = Right T 2 An eye’s view Yes Runtime Left = Right T 3 Eye adjusting No Compile-time Left  ̸ =  Right T 4 Perspective No Design-time Left = Right T 5 Viewport No Design-time Left = Right \ntheir corresponding  360 ° video frame coordinates.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "T 1 Rigid body No Compile-time Left = Right T 2 An eye’s view Yes Runtime Left = Right T 3 Eye adjusting No Compile-time Left  ̸ =  Right T 4 Perspective No Design-time Left = Right T 5 Viewport No Design-time Left = Right \ntheir corresponding  360 ° video frame coordinates. Finally, the third stage, i.e.,  Projection Mapping , uses the mapping results from the second stage and the  360 ° video frame to deduce the pixel values for 2D FoV frames (shown in  c  in Fig. 3) ,which can be projected to both eyes on the HMD.",
      "type": "sliding_window",
      "tokens": 147
    },
    {
      "text": "3) ,which can be projected to both eyes on the HMD. The  Transformation  stage, shown in Fig. 3  a  for compu- tation of the  Transformation Matrix , is used for projecting the 360 ° frame pixels onto the  2 D  FoV plane in the subsequent stages.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "3  a  for compu- tation of the  Transformation Matrix , is used for projecting the 360 ° frame pixels onto the  2 D  FoV plane in the subsequent stages. This matrix is calculated by applying ﬁve different transforms –  T 1 ,  T 2 ,  T 3 ,  T 4 , and  T 5  – in a serial fashion. •  T 1  serves as a  rigid body transformation  matrix which ap- plies 3D rotation ( Y aw, Pitch, Roll ) and translation so that, the objects do not get distorted.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "•  T 1  serves as a  rigid body transformation  matrix which ap- plies 3D rotation ( Y aw, Pitch, Roll ) and translation so that, the objects do not get distorted. Since this transformation does not depend on any of the sensor inputs, it can be pre- calculated at compile-time. •  T 2  gives us  eyes’ view ; i.e., this changes the virtual world’s coordinate frame to match the frame of the eye.",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "•  T 2  gives us  eyes’ view ; i.e., this changes the virtual world’s coordinate frame to match the frame of the eye. This requires knowledge of the head orientation or the direction of gaze, which can be read at runtime from the IMU sensors embedded in the VR headset. •  T 3  transforms the  360 ° coordinates from a  monocular view to a  stereoscopic view .",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "•  T 3  transforms the  360 ° coordinates from a  monocular view to a  stereoscopic view . Since each eye sees the same object differently , this transformation matrix is different for each eye to give the user a more realistic experience. •  T 4 , also known as the  perspective transformation  ma- trix, maps all  360 ° coordinates onto 2D coordinates.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "•  T 4 , also known as the  perspective transformation  ma- trix, maps all  360 ° coordinates onto 2D coordinates. This transformation depends only on the HMD characteristics, including, but not limited to, the display size and resolution, and hence, is known apriori (at design-time). •  T 5 , the last transformation to be applied, performs a view- port transformation 5 , bringing the projected points to the coordinates used to index the pixels on the HMD.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "•  T 5 , the last transformation to be applied, performs a view- port transformation 5 , bringing the projected points to the coordinates used to index the pixels on the HMD. As in the case of  T 4 , this transformation is also HMD design- dependent and is known at design-time. Note that, the  product  of these ﬁve transforms gives us the ﬁnal transformation matrices ( T L  and  T R ), which together convert the  3 D  coordinates of the  360 ° frame to the  2 D  coordinates suitable for HMD.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Note that, the  product  of these ﬁve transforms gives us the ﬁnal transformation matrices ( T L  and  T R ), which together convert the  3 D  coordinates of the  360 ° frame to the  2 D  coordinates suitable for HMD. Mathematically, the transformation matrix for each eye is shown in Equation 1. T L = T 5  ×  T 4  ×  T   L 3   ×  T 2   ×  T 1 T R = T 5  ×  T 4  ×  T   R 3   ×  T 2   ×  T 1 (1) \nThese ﬁve transforms are of dimension of  4 × 4  ( 3  dimensions for rotation;  1  for translation), thus producing  4  ×  4  T L  and T R  matrices [27].",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "T L = T 5  ×  T 4  ×  T   L 3   ×  T 2   ×  T 1 T R = T 5  ×  T 4  ×  T   R 3   ×  T 2   ×  T 1 (1) \nThese ﬁve transforms are of dimension of  4 × 4  ( 3  dimensions for rotation;  1  for translation), thus producing  4  ×  4  T L  and T R  matrices [27]. Note that, given an arbitrary FoV frame, these transformation matrices remain the same for all the pixel \n5 A Viewport Transformation is the process of transforming a 2D coordinate objects to device coordinates [27]. 244 \ncoordinates in that frame, thus are evaluated only  once  for that frame, and account for only  4 .",
      "type": "sliding_window",
      "tokens": 159
    },
    {
      "text": "244 \ncoordinates in that frame, thus are evaluated only  once  for that frame, and account for only  4 . 8 MFLOPS  without any optimization. In the  Projection Computation  stage (refer to  b  in Fig.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "In the  Projection Computation  stage (refer to  b  in Fig. 3), we use the transformation matrix ( T  ) to generate the mapping ( P ) between the  360 ° frame coordinates and the  2 D  FoV frame coordinates. At any instance, a user is only concerned about the FoV pixels in the entire  360 ° frame.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "At any instance, a user is only concerned about the FoV pixels in the entire  360 ° frame. So, instead of evaluating the mapping for all coordinates in the  360 ° frame, we only generate the mapping for those pixels which are within the user’s view. As the target  2 D  FoV coordinates are already known (VR screen dimensions), these mappings can be performed by multiplying the inverse of the transformation matrix ( T   − 1 ) with the  2 D  FoV coordinates ( V 2 D ), thus generating the corresponding  360 ° pixel coordinates ( P ), as shown in Equation 2.",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "As the target  2 D  FoV coordinates are already known (VR screen dimensions), these mappings can be performed by multiplying the inverse of the transformation matrix ( T   − 1 ) with the  2 D  FoV coordinates ( V 2 D ), thus generating the corresponding  360 ° pixel coordinates ( P ), as shown in Equation 2. P i L   =  T  − 1 L × V i 2 D ; ∀ i  ≤ num pixels \nP i R   =  T  − 1 R × V i 2 D ; ∀ i  ≤ num pixels (2) \nHere,  V 2 D  = [ q 0 , q 1 , q 2 , q 3 ] ⊤ represents the quaternion equiva- lent of the  2 D  FoV coordinates used for matrix multiplication with the inverse transformation matrix ( T   − 1 ). Note that, this operation, which is a matrix multiplication on each FoV pixel coordinate, can be quite compute intensive.",
      "type": "sliding_window",
      "tokens": 232
    },
    {
      "text": "Note that, this operation, which is a matrix multiplication on each FoV pixel coordinate, can be quite compute intensive. In fact, the number of pixels in the FoV is usually around 1 million, and the videos stream at a rate of 30 fps for an immersive experience. This amounts to about  2.3 GFLOPS , which represents a substantial amount of computation, given the limited compute capabilities and power in such edge devices.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "This amounts to about  2.3 GFLOPS , which represents a substantial amount of computation, given the limited compute capabilities and power in such edge devices. Note that, even though theoretically one represents the  360 ° frame coordinates as quaternions, in practice, they are typically represented using speciﬁc projection formats, e.g., equirectangular, cube map, equi-angular cubemap, pyramid format, etc. The details of these formats are in the purview of cartography and computer graphics domain, and hence we do not evaluate all of the aforementioned formats.",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "The details of these formats are in the purview of cartography and computer graphics domain, and hence we do not evaluate all of the aforementioned formats. In our evaluations and experiments, we used the equirectangular format [52], which is one of the most popular projection formats. The  Projection Mapping  stage (  c  in Fig.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "The  Projection Mapping  stage (  c  in Fig. 3) takes the projection matrices for both the eyes ( P L ,  P R ) of Equation 2 as well as the pixel values of the  360 ° video frame ( F 360 ), to obtain the  2 D  FoV frames ( F L  and  F R ), which can be further displayed on the HMD. This stage mostly comprises of memory operations, and thus is not a compute bottleneck.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "This stage mostly comprises of memory operations, and thus is not a compute bottleneck. Our discussion in this section is summarized in Tab. I.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "I. We have two important takeaways: •  Computation Dependence Chain: We note that there exists a data dependence from the  360 ° frame to generate the ﬁnal FoV frame, where  F  depends on  P , which in turn depends on  T  . This also determines the “order of computation”, which is ﬁrst  T  , then  P , and ﬁnally  F .",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "This also determines the “order of computation”, which is ﬁrst  T  , then  P , and ﬁnally  F . •  Input (in-)Variability:  It should be clear from the discus- sion above that  T 1 ,  T 3  T 4 , and  T 5  can be determined apriori. However,  T 2  can change at runtime, and if any element in  T 2 is changed, the transformation matrix needs re-computation \nFig.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "However,  T 2  can change at runtime, and if any element in  T 2 is changed, the transformation matrix needs re-computation \nFig. 4: InterFrame-IntraEye ( EA ) and IntraFrame-InterEye ( AE ) reuse opportunities. This example illustrates 3 consecutive frames processing, each of which consists of two projection matrices ( P L and  P R ) for both eyes.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "This example illustrates 3 consecutive frames processing, each of which consists of two projection matrices ( P L and  P R ) for both eyes. The  3 rd frame shares the same head orientation with the  1 st, thus can be optimized by  EA . Moreover, the reuse between both eyes is further optimized by  AE .",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Moreover, the reuse between both eyes is further optimized by  AE . along with  P  and  F , due to their dependencies. However, if  T 2  does not change across frames,  P  is identical to the previous frame.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "However, if  T 2  does not change across frames,  P  is identical to the previous frame. IV. R EDUCING  P ROJECTION  C OMPUTATION \nAs discussed in Sec.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "R EDUCING  P ROJECTION  C OMPUTATION \nAs discussed in Sec. II, computations dominate the energy consumption in  360 ° VR video processing. Further, we also observed in Sec.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "Further, we also observed in Sec. III that, the main reason behind this is that the compute is executed repeatedly both within a single frame (due to offset between the eyes) and across frames (due to changes in head orientation at runtime). Unlike prior works targeting at optimizing the efﬁciency of  each computation  [28], [57], we primarily focus on reducing  the amount of computation to be performed, by exploring the intrinsic “compute reuse opportunities” in  360 ° VR video processing.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "Unlike prior works targeting at optimizing the efﬁciency of  each computation  [28], [57], we primarily focus on reducing  the amount of computation to be performed, by exploring the intrinsic “compute reuse opportunities” in  360 ° VR video processing. A. Opportunities \nExploring and exploiting computation output reuse oppor- tunities is non-trivial in this context.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "Opportunities \nExploring and exploiting computation output reuse oppor- tunities is non-trivial in this context. First of all, the projec- tion transformation is multi-staged and is a composition of multiple mathematical operations, e.g., transformation matrix, projection computation, mapping, etc. As discussed in Sec.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "As discussed in Sec. III, the projection computation varies across  eyes  even for the same head orientation. Moreover, in many cases, computations are also sensor input-dependent such as the IMU data for determining the head orientation, which is updated across frames , at runtime.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "Moreover, in many cases, computations are also sensor input-dependent such as the IMU data for determining the head orientation, which is updated across frames , at runtime. Thus, to explore computation reuse op- portunities, we start by distinguishing between 4 complemen- tary opportunities –  InterFrame-IntraEye (EA) ,  IntraFrame- InterEye (AE) ,  IntraFrame-IntraEye (AA) , and  InterFrame- InterEye (EE) , using a represent example shown in Fig. 4.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "4. •  In  EA , as discussed in Sec. III, the transformation matrix ( T  ) is determined by the head orientation, which is sampled from the built-in IMU sensors.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "III, the transformation matrix ( T  ) is determined by the head orientation, which is sampled from the built-in IMU sensors. We observe that, if the head orientation does not change across two frames, the ﬁve transforms and the  360 ° coordinate inputs remain the same, thereby providing ample opportunities for directly  reusing \n245 \nthe compute results from the previous frame ( P 1 ), as shown in  a  in Fig. 4.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "4. •  AE  comes to play when there is a change in head orientation in consecutive frames, and we cannot enjoy the oppor- tunities in  EA . For such scenarios, due to the prevailing relationship between the left and right eye transformation matrices ( T L  and  T R ), we can further avail the spatial compute reuse opportunity shown in b in Fig.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "For such scenarios, due to the prevailing relationship between the left and right eye transformation matrices ( T L  and  T R ), we can further avail the spatial compute reuse opportunity shown in b in Fig. 4, by reconstructing the computation needed for one eye ( P R ) from the other ( P L ). •  In  AA , the input and output mapping are unique, that is, no two input coordinates in  360 ° frame map to the same coordinates in the 2D FoV frame, thereby eliminating any compute reuse scope.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "•  In  AA , the input and output mapping are unique, that is, no two input coordinates in  360 ° frame map to the same coordinates in the 2D FoV frame, thereby eliminating any compute reuse scope. Although, in principle, for computing the transformation for consecutive pixels, one can leverage data value similarity to reduce the computation, in this work we are not focusing on leveraging any such opportunity. •  EE  offers little chance of reuse, and can only be leveraged in rare occasions, where we have oracular knowledge of head movements.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "•  EE  offers little chance of reuse, and can only be leveraged in rare occasions, where we have oracular knowledge of head movements. Furthermore, in such cases of head movement, there is likely to be some reuse from inter-eye reusability within a frame, rather than inter-frame reusability. B.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "B. Why have  EA / AE  Opportunities been previously Ignored? Based on the above discussion, in this work, we focus on EA  and  AE  opportunities.",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "Based on the above discussion, in this work, we focus on EA  and  AE  opportunities. We are unaware of any existing implementation or research work that focus on compute reuse by leveraging across-frames and across-eyes memoization. In fact, the existing state-of-the-art software stack, such as GoogleVR-SDK [11], simply uses the IMU sensor inputs to calculate the updated transformation matrices, then passes them to the OpenGL [42] engine to process the projection computation, as shown in Fig.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "In fact, the existing state-of-the-art software stack, such as GoogleVR-SDK [11], simply uses the IMU sensor inputs to calculate the updated transformation matrices, then passes them to the OpenGL [42] engine to process the projection computation, as shown in Fig. 2b. We would also like to point that capturing these opportunities is not trivial and cannot be efﬁciently done by just optimizing the existing application layer and software stack.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "We would also like to point that capturing these opportunities is not trivial and cannot be efﬁciently done by just optimizing the existing application layer and software stack. We describe the underlying issues to address and emphasize the non-trivialities: •  To ease development efforts, state-of-the-art VR applica- tions reuse APIs provided by OpenGL [24], [42], and whenever a new frame is decoded, they always invoke the glDrawFrame  twice for both eyes (see line number  257  in googlevr-video360 application [12]). They do not seem to leverage the fact that the transformation matrices are unique for each head orientation and memoizing them will save re-calculating the transformation matrix ( T  ) as well as the projection matrix ( P ).",
      "type": "sliding_window",
      "tokens": 177
    },
    {
      "text": "They do not seem to leverage the fact that the transformation matrices are unique for each head orientation and memoizing them will save re-calculating the transformation matrix ( T  ) as well as the projection matrix ( P ). •  Even if they do realize such opportunities, the projection matrix ( P ) is very big ( ≈ 8 MB , details in Sec. IV-C), and one edge VR headset cannot afford to memoize them for all  possible head orientations.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "IV-C), and one edge VR headset cannot afford to memoize them for all  possible head orientations. To address this, we need to study the impact of head orientation on the computation and whether we can establish a relationship between the computation executed for both the eyes to get rid of any existing redundancies. All these are possible avenues for optimization and demand a detailed study of the computa- \ntion pipeline, the workloads, user behavior, etc., to ﬁnd a way to further improve the state-of-the-art.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "All these are possible avenues for optimization and demand a detailed study of the computa- \ntion pipeline, the workloads, user behavior, etc., to ﬁnd a way to further improve the state-of-the-art. •  Furthermore, a software-only approach may not give us the desired solution as some of these additional execution cycles, control and data path manipulations may need ar- chitectural support, especially to reduce memory and power overheads on edge VRs. Therefore, we believe that, achiev- ing beneﬁts by exploiting the  EA  and  AE  opportunities needs an extensive study and a careful design, especially from an architectural perspective, to maximize the beneﬁts.",
      "type": "sliding_window",
      "tokens": 152
    },
    {
      "text": "Therefore, we believe that, achiev- ing beneﬁts by exploiting the  EA  and  AE  opportunities needs an extensive study and a careful design, especially from an architectural perspective, to maximize the beneﬁts. Driven by the above discussion and the potential optimiza- tion opportunities presented by  EA  and  AE , we propose  D´ej`a View , an energy-efﬁcient design for  360 ° video streaming on VRs. As shown in Fig.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "As shown in Fig. 4,  D´ej`a View  leverages compute lo- cality to bypass computations and provides signiﬁcant energy savings, with the following two-step optimization strategy: \na  For each frame, if the head orientation remains the same, we take advantage of the  EA  opportunity. b  If exploiting the  EA  opportunity is not possible, we take advantage of the  AE  opportunity, by performing computation for only one eye (and construct the result for the other eye).",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "b  If exploiting the  EA  opportunity is not possible, we take advantage of the  AE  opportunity, by performing computation for only one eye (and construct the result for the other eye). C. InterFrame-IntraEye (EA) Computation Optimization \nWe plan to leverage the  EA  opportunity when the user’s head orientation does not change. Intuitively, as mentioned earlier in Sec.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Intuitively, as mentioned earlier in Sec. III (Fig. 3),  a  Transformation and  b  Projec- tion Computation remain unchanged.",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "3),  a  Transformation and  b  Projec- tion Computation remain unchanged. To understand all the contributing factors which affect computations, we further investigate the important inputs of the VR headset. This can help us identify and isolate proper memoization candidates for carefully tweaking our design decisions to maximize the reuse beneﬁts.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "This can help us identify and isolate proper memoization candidates for carefully tweaking our design decisions to maximize the reuse beneﬁts. Further, we also study the overheads introduced by our design modiﬁcations to perform a fair comparison with the state-of-the-art. What (features) to Memoize?",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "What (features) to Memoize? As discussed earlier, at any moment during VR video processing, the execution pipeline is not only impacted by the head orientation, but also by other features such as video frame rate, video types/semantics information, pixel values, user interactions. To better under- stand which of these are the best candidates ( features , using machine learning parlance) for memoization and whether they are sufﬁcient or not, we next discuss input parameters and their impact on the computation: •  Head orientation:  Any changes in this affect the matrix T 2  as discussed in Tab.",
      "type": "sliding_window",
      "tokens": 126
    },
    {
      "text": "To better under- stand which of these are the best candidates ( features , using machine learning parlance) for memoization and whether they are sufﬁcient or not, we next discuss input parameters and their impact on the computation: •  Head orientation:  Any changes in this affect the matrix T 2  as discussed in Tab. I, thus changing the transformation matrix  T  and eventually leading to  re-computation  of the most compute-intensive projection matrix  P . Thus, it is a critical feature in projection computation executions.",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "Thus, it is a critical feature in projection computation executions. •  Pixel values:  The pixel contents/values (denoted as  F  in Fig. 3) matter only during data transfer (from the input 360 ° frame to the framebuffer) in the projection mapping stage, after the coordinate mappings ( P  in Fig.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "3) matter only during data transfer (from the input 360 ° frame to the framebuffer) in the projection mapping stage, after the coordinate mappings ( P  in Fig. 3) are gener- ated. Potentially, content-based optimizations (e.g., content cache [63]) can beneﬁt the data transfer; however, they are not attractive candidates to leverage compute reuse, which is the major power-hungry stage (as shown in Fig.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Potentially, content-based optimizations (e.g., content cache [63]) can beneﬁt the data transfer; however, they are not attractive candidates to leverage compute reuse, which is the major power-hungry stage (as shown in Fig. 2a). In \n246 \nTABLE II: Video workloads.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "In \n246 \nTABLE II: Video workloads. No. Video Type (Cam movement/focus of attention direction) \nFrame Rate (fps) \n#Frames Bit Rate (kbps) \nV1 Rhinos [4] Stationary cam, no focus direction 30 3280 13462 \nV2 Timelapse [56] \nStationary cam, fast-moving objects, no focus direction \n30 2730 15581 \nV3 Rollercoaster [35] \nFast-moving cam hooked in front of a rollercoaster, uni-direction focus \n29.97 6194 16075 \nV4 Paris [51] \nStationary cam, smooth scene cuts, no focus direction \n59.94 14629 14268 \nV5 Elephants [5] Stationary cam, uni-direction focus 30 5510 16522 \nthis work, we are focusing on reusing computation results rather than reducing the content maintenance/transfer, and hence do not consider that optimization.",
      "type": "sliding_window",
      "tokens": 210
    },
    {
      "text": "Video Type (Cam movement/focus of attention direction) \nFrame Rate (fps) \n#Frames Bit Rate (kbps) \nV1 Rhinos [4] Stationary cam, no focus direction 30 3280 13462 \nV2 Timelapse [56] \nStationary cam, fast-moving objects, no focus direction \n30 2730 15581 \nV3 Rollercoaster [35] \nFast-moving cam hooked in front of a rollercoaster, uni-direction focus \n29.97 6194 16075 \nV4 Paris [51] \nStationary cam, smooth scene cuts, no focus direction \n59.94 14629 14268 \nV5 Elephants [5] Stationary cam, uni-direction focus 30 5510 16522 \nthis work, we are focusing on reusing computation results rather than reducing the content maintenance/transfer, and hence do not consider that optimization. •  Video meta-information:  This contains the additional infor- mation, such as frame rates, video semantics/types, etc., about the video inputs. This feature can only be used as an add-on, along with other inputs to further improve compute reuse scope.",
      "type": "sliding_window",
      "tokens": 259
    },
    {
      "text": "This feature can only be used as an add-on, along with other inputs to further improve compute reuse scope. For example, if the  360 ° video frame rate increases from the typical 30 fps to 60 fps, then one can potentially leverage this enhanced compute frequency in conjunction with the head orientation, to further expand the compute reuse window. Note however that, this meta- information is not on the data-dependence chain, and we do not consider it for memoization.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Note however that, this meta- information is not on the data-dependence chain, and we do not consider it for memoization. To summarize, among the above discussed features, we identify  head orientation  as the only suitable memoization candidate for boosting the compute reuse scope. Thus, we memoize both head orientation and its corresponding projec- tion matrix (i.e., projection computation results) in a memory buffer, namely,  P buff , and use the head orientation to index the address/pointer of that  P buff  stored in DRAM.",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "Thus, we memoize both head orientation and its corresponding projec- tion matrix (i.e., projection computation results) in a memory buffer, namely,  P buff , and use the head orientation to index the address/pointer of that  P buff  stored in DRAM. How Much to Memoize? The occupied DRAM size is mainly determined by  P buff .",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "The occupied DRAM size is mainly determined by  P buff . In fact, with a VR screen size of  1 ,  000 × 1 ,  000 , one  P buff  occupies  ≈ 8 MB  in DRAM. Since this puts a high demand on memory, one edge VR headset cannot afford to memoize for all possible head orientations.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Since this puts a high demand on memory, one edge VR headset cannot afford to memoize for all possible head orientations. Thus, we want to limit the number of  P buff  that we need to store. To address this, we need to carefully decide how much his- tory is to be memoized for leveraging computation reuse.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "To address this, we need to carefully decide how much his- tory is to be memoized for leveraging computation reuse. We performed a study on the VR video dataset [3] to investigate the head orientation traces of 20 users watching 5 widely- variant  360 ° VR videos (summarized in Tab. II).",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "II). Typically, the resolution of the IMU traces can be as high as 20 bits per ﬁeld [3], [28]. From the dataset, we report the average reuse distance, i.e., the average number of preceding frames with same head orientation to be memoized, and show it in Fig.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "From the dataset, we report the average reuse distance, i.e., the average number of preceding frames with same head orientation to be memoized, and show it in Fig. 5a. It can be concluded from these results that, memoizing the last  two  frames is sufﬁcient for most of the cases.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "It can be concluded from these results that, memoizing the last  two  frames is sufﬁcient for most of the cases. Memoizing more frames may not bring much additional beneﬁts because of the high sensitivity of the IMU sensors. Storing only  two \nhead orientations (in registers) and their associated  P buff  in the DRAM occupies only  ≈ 16 MB  memory space.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Storing only  two \nhead orientations (in registers) and their associated  P buff  in the DRAM occupies only  ≈ 16 MB  memory space. Further, we also observe that, the duration for which the head orientation does not change for three consecutive frames sums up to only  ≈ 28%  of the video runtime on average (refer to Fig. 5b), limiting the memoization opportunities to those instances.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "5b), limiting the memoization opportunities to those instances. Such low reuse ratio is expected because of the high sensitivity of the IMU sensors. However, a higher reuse ratio can be achieved by relaxing the precision of the IMU output.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "However, a higher reuse ratio can be achieved by relaxing the precision of the IMU output. Furthermore, we study the  V3  (i.e.,  Rollercoaster ) video and examine the trade-offs between (i) quantizing/approximat- ing the head orientation (thus compromising video quality) with more reuse, vs. (ii) maintaining the lossless video quality but with a lower reuse ratio, in Fig. 5c, to provide an intuitive comparison in different scenarios.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "5c, to provide an intuitive comparison in different scenarios. Here, we quantify the video quality with the popular Peak Signal-to-Noise Ratio (PSNR, normalized to the ground-truth; the higher, the better) [25], [40]. From Fig.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "From Fig. 5c, we can observe that, as the precision decreases from  4  (resolution is  0 . 0001 ) to  1  (resolution is  0 .",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "0001 ) to  1  (resolution is  0 . 1 ), the reuse ratio increases from  18%  to  92% ; however, the PSNR drops from  85%  to only  19% . This is because low precision leads to a mis-projection, which fails to reﬂect the current head orientation.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "This is because low precision leads to a mis-projection, which fails to reﬂect the current head orientation. In this work, we do not want to distort video quality and thus explore the ground-truth only. The Effect of EA:  With this  EA  memoization, once a new head orientation is received, we ﬁrst search it in the two head orientation registers.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "The Effect of EA:  With this  EA  memoization, once a new head orientation is received, we ﬁrst search it in the two head orientation registers. If there is a match, the associated  P buff will return the memory address of the saved  P  so that we can reuse  P  and skip the  entire  coordinate projection computation (refer to  a  in Fig. 4), with only  1%  overhead w.r.t.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "4), with only  1%  overhead w.r.t. baseline. If not, we have to execute the entire computation as in the baseline case.",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": "If not, we have to execute the entire computation as in the baseline case. As a result, by exploiting the  EA  scheme on the second frame, its compute energy consumption can be reduced to only  1%  of that consumed by  Baseline . D. IntraFrame-InterEye (AE) Computation Optimization \nIn  EA , the compute can be bypassed by reusing the pre- computed results, if the head orientation matches with any of the two previously memoized head orientations (stored in reg- isters).",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "D. IntraFrame-InterEye (AE) Computation Optimization \nIn  EA , the compute can be bypassed by reusing the pre- computed results, if the head orientation matches with any of the two previously memoized head orientations (stored in reg- isters). However, we also note that these opportunities might be limited owing to the “non-repetitive” user behavior. De- spite this variation, there may still be matches/recomputations within a frame between two eyes, i.e., IntraFrame-InterEye as shown in  b  in Fig.",
      "type": "sliding_window",
      "tokens": 143
    },
    {
      "text": "De- spite this variation, there may still be matches/recomputations within a frame between two eyes, i.e., IntraFrame-InterEye as shown in  b  in Fig. 4. To leverage this opportunity, we next study the coordinate projection results relationship between left-eye and right-eye.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "To leverage this opportunity, we next study the coordinate projection results relationship between left-eye and right-eye. If there exists a simple mechanism to describe the difference between the two projection matrices of the two eyes ( P L  and  P R ), one can simplify the computation from matrix  multiplications  to matrix  additions . Distance Vector Study:  Let us further look into the detailed mapping of a  360 ° frame (in equirectangular format) onto a 2 D  FoV frame in the Projection Mapping stage (refer  c  in Fig.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "Distance Vector Study:  Let us further look into the detailed mapping of a  360 ° frame (in equirectangular format) onto a 2 D  FoV frame in the Projection Mapping stage (refer  c  in Fig. 3), at a pixel granularity. The pixel rendered at  [ x 0 l   , y 0 l   ]  on the left VR screen is mapped from position  [( x 360 ) 0 l   ,  ( y 360 ) 0 l   ] on the equirectangular  360 ° frame, as shown in Fig.",
      "type": "sliding_window",
      "tokens": 142
    },
    {
      "text": "The pixel rendered at  [ x 0 l   , y 0 l   ]  on the left VR screen is mapped from position  [( x 360 ) 0 l   ,  ( y 360 ) 0 l   ] on the equirectangular  360 ° frame, as shown in Fig. 6a. Simi- larly, the pixel value rendered at  [ x 0 r , y 0 r ]  on the right VR screen \n247 \n0 \n2 \n4 \n6 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \nAvg.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "Simi- larly, the pixel value rendered at  [ x 0 r , y 0 r ]  on the right VR screen \n247 \n0 \n2 \n4 \n6 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \nAvg. Reuse distance (# frame) \nUsers \nRhinos Timelapse Rollercoaster Paris Elephants \n(a) Reuse distance is just  2  for most of cases \n28% \n0% \n20% \n40% \n60% \nReuse Ratio \nVideo (b) Reuse ratio \n0% 20% 40% 60% 80% 100% \nG 4 3 2 1 \nNorm. to Ground Truth(G) \nPrecision \nReuseRatio PSNR \n(c) Precision vs. reuse ratio tradeoffs \nFig.",
      "type": "sliding_window",
      "tokens": 165
    },
    {
      "text": "to Ground Truth(G) \nPrecision \nReuseRatio PSNR \n(c) Precision vs. reuse ratio tradeoffs \nFig. 5: In  EA , (a) shows that, on average, how many frame(s) from the current frame to a previous one with the same head orientation, as denoted as reuse distance. This indicates two memoization buffers are sufﬁcient.",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "This indicates two memoization buffers are sufﬁcient. (b) plots among all the head orientations, how many can be memoized by these two buffers. (c) illustrates the trade-off between the precision level and reuse ratio.",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "(c) illustrates the trade-off between the precision level and reuse ratio. (a) Distance vector. -5 5 15 25 \n1 225 449 673 897 1121 1345 1569 1793 2017 2241 \nDistance \nPixel ID \ndistanceY distanceX \n-10 \n-5 \n0 \n5 \n10 \n0 10 20 \nDistance-x \nDistance-y \n(b) (0.00, 1.57, -0.73) \n-20 0 20 40 60 \n1 202 403 604 805 1006 1207 1408 1609 1810 2011 2212 \nDistance \nPixel ID \ndistanceY distanceX \n-10 -5 0 5 10 \n0 20 40 60 \nDistance-x \nDistance-y \n(c) (0.52, 1.05, -0.73) \nFig.",
      "type": "sliding_window",
      "tokens": 158
    },
    {
      "text": "-5 5 15 25 \n1 225 449 673 897 1121 1345 1569 1793 2017 2241 \nDistance \nPixel ID \ndistanceY distanceX \n-10 \n-5 \n0 \n5 \n10 \n0 10 20 \nDistance-x \nDistance-y \n(b) (0.00, 1.57, -0.73) \n-20 0 20 40 60 \n1 202 403 604 805 1006 1207 1408 1609 1810 2011 2212 \nDistance \nPixel ID \ndistanceY distanceX \n-10 -5 0 5 10 \n0 20 40 60 \nDistance-x \nDistance-y \n(c) (0.52, 1.05, -0.73) \nFig. 6: In  AE , distance vector (a) patterns with two different head orientations  ( Y aw, Pitch, Roll )  in (b) and (c). is mapped from position  [( x 360 ) 0 r ,  ( y 360 ) 0 r ]  on the  360 ° frame.",
      "type": "sliding_window",
      "tokens": 212
    },
    {
      "text": "is mapped from position  [( x 360 ) 0 r ,  ( y 360 ) 0 r ]  on the  360 ° frame. To study the relationship between the coordinate projection re- sults between both eyes, for the  same  two coordinates  [ x i , y i ] on both  2 D  FoV frames, we determine the distance vector ( ⃗ d ) i   between their equirectangular counterparts, represented in Equation 3: \n( ⃗ d ) i   = [( x 360 ) i r   − ( x 360 ) i l ,  ( y 360 ) i r   − ( y 360 ) i l ] ⊤ (3) \nThe knowledge of distance vector   ⃗ d  is critical to explore the AE  opportunity, because if it was known apriori, then we only need to process the entire projection transformation to generate the mapping results ( P L ) for one eye, and then deduce the coordinate projection computation results for the other ( P R ) by simply adding   ⃗ d  with  P L . This encourages us to further study whether this distance vector changes with head orientation or not, and also whether it is invariant for any particular frame – if yes, then how?",
      "type": "sliding_window",
      "tokens": 299
    },
    {
      "text": "This encourages us to further study whether this distance vector changes with head orientation or not, and also whether it is invariant for any particular frame – if yes, then how? To investigate these questions, we examine how the distance vector varies within a same frame, with two different head orientations, shown in Fig. 6b and Fig.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "6b and Fig. 6c. On plotting the distance vectors for each row of the FoV frames, we observe a recurring  ellipse  pattern.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "On plotting the distance vectors for each row of the FoV frames, we observe a recurring  ellipse  pattern. The intuitive reason behind this  ellipse  pattern is related to the built-in features of the equirectangular format. Second, for different head orientations, their distance vectors plotted in Fig.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Second, for different head orientations, their distance vectors plotted in Fig. 6b and Fig. 6c retain the same ellipse behavior but different shapes.",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "6c retain the same ellipse behavior but different shapes. Furthermore, by exacting the x (or y) coordinate in the distance vector, the above  ellipse  pattern can be represented as  Δ x  =  a  ·  cos ( θ )  and  Δ y  =  b  ·  sin ( θ ) +  c , where  θ  ∈ [0 , π ] , and  a, b, c  vary with head orientation change but remain same \nfor each row in the same frame. Additionally, there are few pixel positions at the frame edges which can only be viewed by one eye (denoted as  exclusive ), which cannot be captured by the above pattern.",
      "type": "sliding_window",
      "tokens": 162
    },
    {
      "text": "Additionally, there are few pixel positions at the frame edges which can only be viewed by one eye (denoted as  exclusive ), which cannot be captured by the above pattern. These pixels amount to only  2 . 7%  of the entire FoV frame.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "7%  of the entire FoV frame. How to capture the pattern and utilize the pattern? Due to this inherent nature of compute, the pattern between left eye and right eye can be easily  captured  by proﬁling the distance vector for only the ﬁrst row on the screens:  Δ = \u0002 ⃗x 0 r   − ⃗x 0 l   , ⃗y 0 r   − ⃗y 0 l \u0003 , as shown in line number  2  in Algorithm 1.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "Due to this inherent nature of compute, the pattern between left eye and right eye can be easily  captured  by proﬁling the distance vector for only the ﬁrst row on the screens:  Δ = \u0002 ⃗x 0 r   − ⃗x 0 l   , ⃗y 0 r   − ⃗y 0 l \u0003 , as shown in line number  2  in Algorithm 1. With the learned pattern, the remaining  i th rows for the right- eye ( i  ∈ [1 , n  − 1] , where  n  is the  height  of the VR screen) can be  reconstructed  by using the projection computation results of the left-eye ( [ ⃗x i l , ⃗y i l ] ) and the pattern  Δ , as shown in line  6  in Algorithm 1. Note that, as discussed above, 2 .",
      "type": "sliding_window",
      "tokens": 198
    },
    {
      "text": "Note that, as discussed above, 2 . 7%  exclusive  pixel coordinates for the right-eye cannot be reconstructed by this algorithm. Therefore, only for this small number of pixel coordinates, the entire coordinate projection computations need to be processed.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Therefore, only for this small number of pixel coordinates, the entire coordinate projection computations need to be processed. Algorithm 1  Algorithm to capture and utilize the pattern  Δ \nInput:  [ ⃗x 0: n − 1 l , ⃗y 0: n − 1 l ] : left-eye projection (all rows) Input:  [ ⃗x 0 r , ⃗y 0 r ] : right-eye ﬁrst-row’s projection Output:  [ ⃗x 1: n − 1 r , ⃗y 1: n − 1 r ] : right-eye projection \n1:  procedure  C APTURE P ATTERN ( ⃗x 0 r ,  ⃗y 0 r ,  ⃗x 0 l   ,  ⃗y 0 l   ) \n2: [Δ x ,  Δ y ]  :=  [ ⃗x 0 r   − ⃗x 0 l   ,  ⃗y 0 r   − ⃗y 0 l   ] 3: return  Δ = [Δ x ,  Δ y ] \n4:  end procedure \n5:  procedure  U TILIZE P ATTERN ( ⃗x 1: n − 1 l ,  ⃗y 1: n − 1 l ,  Δ ) 6: [ ⃗x 1: n − 1 r ,  ⃗y 1: n − 1 r ]  :=  [ ⃗x 1: n − 1 l +  Δ x  ,  ⃗y 1: n − 1 l +  Δ y ] \n7: return  [ ⃗x 1: n − 1 r ,  ⃗y 1: n − 1 r ] 8:  end procedure \nThe Effect of AE:  with this  AE  optimization, for the right eye, the intensive projective transformation computations can now be short-circuited by light-weight  Add  operations. As a result, by exploiting the  AE  scheme on the ﬁrst frame in Fig.",
      "type": "sliding_window",
      "tokens": 494
    },
    {
      "text": "As a result, by exploiting the  AE  scheme on the ﬁrst frame in Fig. 4  b  , its compute energy can be reduced to only  62 . 35%  of that consumed by baseline.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "35%  of that consumed by baseline. 248 \nFig. 7: The proposed  EA  and  AE  design blocks implementation.",
      "type": "sliding_window",
      "tokens": 27
    },
    {
      "text": "7: The proposed  EA  and  AE  design blocks implementation. E. Design Considerations and Implementation \nWe designed both our schemes as modular and scalable additions to the existing pipeline (refer the  EA  and  AE  blocks shown in Fig. 7).",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "7). The  EA  block is placed before the original compute engine (OCE, e.g., GPU) to opportunistically bypass the projection computation. However, when  AE  cannot take advantage of memoization due to a head orientation change, then the compute is distributed across the OCE ( 51% ) and  AE block ( 49% ); to be precise, only the entire coordinates on the left screen and the ﬁrst row on the right-screen are processed by the OCE – the remaining rows on the right screen are reconstructed by the less power-hungry  AE  block.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "However, when  AE  cannot take advantage of memoization due to a head orientation change, then the compute is distributed across the OCE ( 51% ) and  AE block ( 49% ); to be precise, only the entire coordinates on the left screen and the ﬁrst row on the right-screen are processed by the OCE – the remaining rows on the right screen are reconstructed by the less power-hungry  AE  block. Implementation Details:  We abstract the  EA  and  AE  design blocks irrespective of the underlying hardware SoCs, and plot them in Fig. 7.",
      "type": "sliding_window",
      "tokens": 128
    },
    {
      "text": "7. In the  Baseline , the OCE takes the head orientation as its input, processes the entire projection transformation for each pixel coordinate in the FoV region, and then stores the compute results for both eyes in DRAM for the subsequent mapping stage from the  360 ° frame to framebuffer. In our  EA  design, the last two head orientations are  cached  in local SRAM ( Comp 1  and  Comp 2  in the  EA block) and their corresponding projection computation results are stored in DRAM ( P i − 2 buff   and  P i − 1 buff ).",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "In our  EA  design, the last two head orientations are  cached  in local SRAM ( Comp 1  and  Comp 2  in the  EA block) and their corresponding projection computation results are stored in DRAM ( P i − 2 buff   and  P i − 1 buff ). Once the current head orientation is received, the  EA  block ﬁrst compares it with the memoized  Comp 1  and  Comp 2 . If a match is detected, then the corresponding  P i − 2 buff   or  P i − 2 buff   buffer address pointer is directly returned.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "If a match is detected, then the corresponding  P i − 2 buff   or  P i − 2 buff   buffer address pointer is directly returned. If no match is found, the OCE is invoked for the entire left eye and only the ﬁrst row for the right eye, and then terminates by an external signal sent from our  AE  block, and bypasses the computation for rest rows. In the proposed AE  design, the  Δ  pattern buffer is ﬁrst initialized by subtract- ing  Result [1] .R  from  Result [1] .L , as shown in the  AE  block in Fig.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "In the proposed AE  design, the  Δ  pattern buffer is ﬁrst initialized by subtract- ing  Result [1] .R  from  Result [1] .L , as shown in the  AE  block in Fig. 7. After the pattern between left eye and right eye is captured, an external signal is propagated to the OCE to bypass the further original projection computations.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "After the pattern between left eye and right eye is captured, an external signal is propagated to the OCE to bypass the further original projection computations. Consequently, the projection computation results ( Result [2 :  n ] .R ) for the remaining rows of the right eye can be easily reconstructed by adding  Result [2 :  n ] .L  and the  Δ . We prototyped our proposed  EA  and  AE  design blocks using System Verilog in Xilinx Vivado 2019.2 [58], targeting the Xil- \ninx Zynq-7000 SoC ZC706 board running at 100MHz (same as state-of-the-art EVR [28]).",
      "type": "sliding_window",
      "tokens": 166
    },
    {
      "text": "We prototyped our proposed  EA  and  AE  design blocks using System Verilog in Xilinx Vivado 2019.2 [58], targeting the Xil- \ninx Zynq-7000 SoC ZC706 board running at 100MHz (same as state-of-the-art EVR [28]). The evaluation shows that our  EA and  AE  designs consume only  2 mW  and  65 mW , respectively, and are able to deliver around  100  fps, which is more than sufﬁcient for the current VR application requirements. V. E VALUATION \nWe compare our proposed  EA  and  AE  designs with six different VR streaming setups, by evaluating the computation and the total energy consumption.",
      "type": "sliding_window",
      "tokens": 163
    },
    {
      "text": "V. E VALUATION \nWe compare our proposed  EA  and  AE  designs with six different VR streaming setups, by evaluating the computation and the total energy consumption. In this section, we ﬁrst describe the experimental platforms, datasets and measurement tools used in this study. We then analyze the results measured using these platforms.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "We then analyze the results measured using these platforms. A. VR Design Conﬁgurations \nWe evaluate the following six conﬁgurations of VR stream- ing to demonstrate the effectiveness of D´ej`a View: •  Baseline  (SW):  We use a mobile GPU [36] to evaluate the baseline VR video streaming. This GPU is commonly used in contemporary VR devices (Oculus [39], Magic Leap [16], and GameFace [47], etc.).",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "This GPU is commonly used in contemporary VR devices (Oculus [39], Magic Leap [16], and GameFace [47], etc.). Note that, with this setup, the projection computation is triggered for each frame, and also per projection computation invocation includes computation of two projection matrices for the two eyes. •  PTU  (HW):  A recent optimized solution [28] utilizes a more energy-efﬁcient hardware accelerator, i.e., Projec- tive Transformation Unit ( PTU ), to process the compute- intensive projection operations.",
      "type": "sliding_window",
      "tokens": 126
    },
    {
      "text": "•  PTU  (HW):  A recent optimized solution [28] utilizes a more energy-efﬁcient hardware accelerator, i.e., Projec- tive Transformation Unit ( PTU ), to process the compute- intensive projection operations. This is the most recent VR design that uses an FPGA for accelerating the computation. We consider this design as the  state-of-the-art .",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "We consider this design as the  state-of-the-art . However, PTU  only optimizes the energy per compute through accel- eration, with exactly the “same amount of computations” as in the baseline design. In contrast, as explained earlier in Sec.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "In contrast, as explained earlier in Sec. IV, our design skips a huge amount of computations by exploiting the  EA  and  AE . •  EA  (SW) : We evaluate the  InterFrame, IntraEye ( EA ) design on a GPU, as shown in the  EA  block in Fig.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "•  EA  (SW) : We evaluate the  InterFrame, IntraEye ( EA ) design on a GPU, as shown in the  EA  block in Fig. 7. Note that, this implementation is purely done in software, without any hardware modiﬁcation.",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "Note that, this implementation is purely done in software, without any hardware modiﬁcation. •  AE  (SW) : We evaluate the  IntraFrame, InterEye ( AE ) design on a GPU, and bypasses the projection computation for the right-eye by  reconstructing  the results with a learned pattern, as shown in the  AE  block in Fig. 7.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "7. •  EA+AE  (SW) : The above two designs can be seamlessly integrated into the original SoC, with the  EA  block placed before the GPU and the  AE  block after the GPU. We denote this design combination as  EA+AE .",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "We denote this design combination as  EA+AE . •  PTU+EA+AE  (HW) : In addition to the GPU-based design, our proposed designs can also be integrated into any other hardware platforms, including the FPGA-based PTU [28]. The  PTU+EA+AE  implementation combines the  PTU  and our  EA+AE  optimizations together.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "The  PTU+EA+AE  implementation combines the  PTU  and our  EA+AE  optimizations together. B. Experimental Platforms and Datasets \nEvaluation Platforms:  The  Baseline  GPU platform described in Fig.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Experimental Platforms and Datasets \nEvaluation Platforms:  The  Baseline  GPU platform described in Fig. 8 consists of a 512-core Volta GPU, a 4Kp60 HEVC \n249 \ncodec, 16GB LPDDR4x memory, 32GB eMMC storage, and a power management unit (PMU) that exposes the real-time power traces to users. To evaluate our design implementation in hardware, we use an FPGA platform, which is the same as the state-of-the-art PTU [28], with a 100MHz system clock, onboard conﬁguration circuitry, 2x16MB Quad SPI Flash, 1GB DDR2 Component Memory, and also a hardware PMU.",
      "type": "sliding_window",
      "tokens": 164
    },
    {
      "text": "To evaluate our design implementation in hardware, we use an FPGA platform, which is the same as the state-of-the-art PTU [28], with a 100MHz system clock, onboard conﬁguration circuitry, 2x16MB Quad SPI Flash, 1GB DDR2 Component Memory, and also a hardware PMU. A full seat Vivado design suite [58], [59] is utilized to synthesize the design and report the power and timing numbers. We collect the display traces from a 5-inch (130 mm) 16:9 1080p (1920 × 1080) AMOLED display [54], which is similar to the Samsung Gear VR display [45].",
      "type": "sliding_window",
      "tokens": 154
    },
    {
      "text": "We collect the display traces from a 5-inch (130 mm) 16:9 1080p (1920 × 1080) AMOLED display [54], which is similar to the Samsung Gear VR display [45]. Fig. 8: Evaluation proto- type – Nvidia Jetson TX2 GPU board [36] (PMU: Power Management Unit).",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "8: Evaluation proto- type – Nvidia Jetson TX2 GPU board [36] (PMU: Power Management Unit). 360 °  VR Video Dataset:  We use the published 360° Head Movements Dataset [3], which includes head movement traces from 59 users viewing seven widely-variant 360° VR videos. 6 \nThe meta information of these VR videos are listed in Tab.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "6 \nThe meta information of these VR videos are listed in Tab. II. C. Experimental Results \nWe present and compare the energy consumption of the pro- jection computation and the cor- responding video quality impact, when running the ﬁve VR videos described in Tab.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "C. Experimental Results \nWe present and compare the energy consumption of the pro- jection computation and the cor- responding video quality impact, when running the ﬁve VR videos described in Tab. II, with the six design conﬁgurations discussed in Sec. V-A.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "V-A. These energy results are normalized w.r.t. the  Baseline  method.",
      "type": "sliding_window",
      "tokens": 23
    },
    {
      "text": "the  Baseline  method. Later, we show quality results compared to the baseline design. In addition, we also discuss the  our design’s versatility  on other 360 ° video representation formats.",
      "type": "sliding_window",
      "tokens": 40
    },
    {
      "text": "In addition, we also discuss the  our design’s versatility  on other 360 ° video representation formats. Energy Savings : Overall, our software implementation EA+AE on GPU can save 54% computation, which translates to 28% total energy savings, compared to the baseline. Com- pared to the state-of-the-art hardware-modiﬁed PTU, our soft- ware implementation can still provide 16% computation and 8% total energy savings.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Com- pared to the state-of-the-art hardware-modiﬁed PTU, our soft- ware implementation can still provide 16% computation and 8% total energy savings. Our FPGA results can further provide 18% more computation and 9% more total energy savings, compared to the state-of-the-art design. More speciﬁcally, for each of the ﬁve video inputs (shown in the x-axis in Fig.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "More speciﬁcally, for each of the ﬁve video inputs (shown in the x-axis in Fig. 9), we compare the compute energy consumption incurred by six schemes with left-eye and right-eye breakdown, and present the respective compute energy in the left y-axis Fig. 9, which is further translated to the total end-to-end energy savings shown in the right y-axis in Fig.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "9, which is further translated to the total end-to-end energy savings shown in the right y-axis in Fig. 9. From this ﬁgure, we observe that: •  Baseline:  In  Baseline , since there are no optimizations, the projection operations for both eyes consume equal energy (on GPU), i.e., each eye’s compute consumes  50%  energy.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "From this ﬁgure, we observe that: •  Baseline:  In  Baseline , since there are no optimizations, the projection operations for both eyes consume equal energy (on GPU), i.e., each eye’s compute consumes  50%  energy. •  EA:  With our proposed  EA  scheme, we fully exploit the temporal compute reuse  across frames with head orien- tations unchanged, with a negligible overhead ( 1%  extra \n6 Due to space limitation, here we only present 5 videos and 20 users. overhead, as discussed in Sec.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "overhead, as discussed in Sec. IV-C). In this scheme, one can observe from Fig.",
      "type": "sliding_window",
      "tokens": 23
    },
    {
      "text": "In this scheme, one can observe from Fig. 9 that, the compute consumes less energy than the  Baseline , i.e., only  72%  on average. This occurs as a result of reusing the memoized results which have been computed and stored previously, ranging from 21 .",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "This occurs as a result of reusing the memoized results which have been computed and stored previously, ranging from 21 . 63%  ( Rollercoaster  video) to  50 . 28%  ( Paris video).",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "28%  ( Paris video). By applying the  EA  optimization itself, on an average, the energy beneﬁt is translated to  14%  end-to-end energy savings, as shown on the right y-axis in Fig. 9.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "9. •  AE:  For those head orientations not memoized, we further exploited the  spatial compute reuse  across eyes within a frame. In the proposed  AE  scheme, one can observe that for the left-eye computation, the energy consumption is the same as in the  Baseline .",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "In the proposed  AE  scheme, one can observe that for the left-eye computation, the energy consumption is the same as in the  Baseline . Recall from the  AE  design logic in Fig. 7 that, the results of the left-eye are generated by the Original Compute Engine (GPU in this case) and fed into the  AE  block with the ﬁrst row for the right-eye, to store the pattern into the Delta Buffer.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "7 that, the results of the left-eye are generated by the Original Compute Engine (GPU in this case) and fed into the  AE  block with the ﬁrst row for the right-eye, to store the pattern into the Delta Buffer. After that, the computation for the right-eye can be easily reconstructed by the left- eye’s compute results and the pattern, which only consumes 13%  energy compared to the  Baseline . Therefore, as shown in Fig.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "Therefore, as shown in Fig. 9, our proposed  AE  optimization alone saves  37% compute energy compared to the  Baseline , translating to 19%  total energy saving. •  EA+AE:  With both  EA  and  AE  optimizations deployed, as shown in Fig.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "•  EA+AE:  With both  EA  and  AE  optimizations deployed, as shown in Fig. 9, on average, the left-eye compute consumes only  36%  energy w.r.t. the  Baseline , with only  10%  for the right-eye, translating to  28%  total energy saving.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "the  Baseline , with only  10%  for the right-eye, translating to  28%  total energy saving. •  PTU:  In the current state-of-the-art scheme, which is the hardware-based PTU [28], they explored the energy- efﬁcient hardware accelerator (namely, PTU) to replace power-hungry GPU. Due to this, one can observe from Fig.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Due to this, one can observe from Fig. 9 that, to execute the same amount of the projection computation, the  PTU  scheme consumes only  62%  of energy w.r.t. the  Baseline , which contributes to  20%  total energy saving.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "the  Baseline , which contributes to  20%  total energy saving. •  PTU+EA+AE:  Note that, our proposed  EA  and  AE  designs are “independent” of the underlying hardware used. As a result, they can also be deployed on top of the PTU-based SoC.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "As a result, they can also be deployed on top of the PTU-based SoC. This can be further asserted from Fig. 9, that only 28%  of the compute energy is consumed w.r.t.",
      "type": "sliding_window",
      "tokens": 51
    },
    {
      "text": "9, that only 28%  of the compute energy is consumed w.r.t. the  Baseline ( 22%  for the left-eye,  6%  for the right-eye), translating to a  37%  total energy saving. Impact on Quality : The proposed  AE  scheme captures the pattern between both the eyes with only the  1 st  row of the frame, and then uses the same pattern to  bypass  the projection computation for the remaining rows of the right eye.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Impact on Quality : The proposed  AE  scheme captures the pattern between both the eyes with only the  1 st  row of the frame, and then uses the same pattern to  bypass  the projection computation for the remaining rows of the right eye. Note that, as shown in Fig. 6b and 6c, the  i th-row’s pattern may not be exactly the same as the  j th-row.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "6b and 6c, the  i th-row’s pattern may not be exactly the same as the  j th-row. This is due to the ﬂoating- point hardware rounding (to ﬁnd the nearest-neighbor integer coordinates) and the transformation matrix’s various weights are dependent on the row numbers. To simplify our  AE  design, we simply reuse the pattern captured in the  1 st row, and do not consider the deeper information related to the row numbers.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "To simplify our  AE  design, we simply reuse the pattern captured in the  1 st row, and do not consider the deeper information related to the row numbers. To study how this decision affects the video quality, we report \n250 \n0% 20% 40% 60% 80% 100% \n0% 20% 40% 60% 80% 100% \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nRhinos Timelapse Rollercoaster Paris Elephants Avg. % Total Energy Saving \nCompute Energy  Consumption \nL \nR %TotalEnergySaving \nFig.",
      "type": "sliding_window",
      "tokens": 221
    },
    {
      "text": "% Total Energy Saving \nCompute Energy  Consumption \nL \nR %TotalEnergySaving \nFig. 9: Normalized energy consumption and savings with different conﬁgurations and video inputs. The left y-axis shows the compute energy consumption normalized to the compute energy consumption in Baseline (the lower, the better).",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "The left y-axis shows the compute energy consumption normalized to the compute energy consumption in Baseline (the lower, the better). The right y-axis shows the amount of energy savings compared to the end-to-end total energy consumption in Baseline (the higher, the better). 0 \n20 \n40 \n60 \n80 \nV1 V2 V3 V4 V5 Avg.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "0 \n20 \n40 \n60 \n80 \nV1 V2 V3 V4 V5 Avg. PSNR (dB) \n-10 \n-5 \n0 \n5 \n10 \n1 224 447 670 893 1116 1339 1562 1785 2008 2231 2454 2677 2900 3123 3346 3569 3792 \nDistance \nPixel ID \nDelta-y Delta-x \n(a): PSNR (b): Pattern in CubeMap format \nFig. 10: Sensitivity study.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "10: Sensitivity study. (a): Video quality metric (PSNR [25], [40]) across video inputs. (b): The pattern between left-eye and right-eye in the  front  face in Cube Mapping [41].",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "(b): The pattern between left-eye and right-eye in the  front  face in Cube Mapping [41]. the averaged PSNR [25], [40] of the ﬁve videos represented in Equirectangular format [52] in Fig. 10  a  .",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "10  a  . These results indicate that, although we ignore the row-number related information, the resulting PSNR is still sufﬁcient ( 47 . 71  on average) for VR video applications [26], [62].",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "71  on average) for VR video applications [26], [62]. General Applicability of D´ej`a View : The above discussion assumes that the Equirectangular format [52] is used to represent the  360 ° videos. We want to emphasize that our underlying ideas behind the proposed  EA  and  AE  (designed for the Equirectangular format) can work irrespective of the representation formats used [48].",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "We want to emphasize that our underlying ideas behind the proposed  EA  and  AE  (designed for the Equirectangular format) can work irrespective of the representation formats used [48]. For example, similar to the distance vector study in Fig. 6b and Fig.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "6b and Fig. 6c, we plot the distance pattern between both eyes of a  360 ° frame using the CubeMap format [41] in Fig. 10  b  .",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "10  b  . Clearly, a pattern (different from the  ellipse  observed with the Equirectangular format) exists in  Δ x  and  Δ y . Note that the pattern behavior also depends on the row numbers.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Note that the pattern behavior also depends on the row numbers. This again validates the quality impact discussed earlier. Putting together, these above observa- tions indicate that, with very little change (to capture the row- dependent information) in our original  AE  design, our idea is able to work with any representation format.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Putting together, these above observa- tions indicate that, with very little change (to capture the row- dependent information) in our original  AE  design, our idea is able to work with any representation format. This motivates us to target on further improving quality across video formats by capturing the information related to row numbers in future. VI.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "VI. R ELATED  W ORK \nOptimizations in Planar Video Streaming:  Pixel-similarity based optimizations [38], [57] have been exploited to improve \nperformance in 2D rendering. For example, ATW [38] is a post-render technique, which sits between rendering (our focus) and display.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "For example, ATW [38] is a post-render technique, which sits between rendering (our focus) and display. To reduce the impact by the long-latency from rendering, ATW either guesses the next head-orientation or only considers the rotation (no translation), then skews two already-rendered planar FoV frames to remove judders [43]. Note that, this computation still happens in planar-format, and remains the same between two-eyes for one frame.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "Note that, this computation still happens in planar-format, and remains the same between two-eyes for one frame. Targeting on ATW, PIMVR [57] proposed a 3D-stacked HMC to re- duce Motion-to-Photon latency and off-chip memory accesses. Motivated by the observation that the ATW transform matrix generated by rotation on a 2D image is shared by both eyes, PIMVR [57] calculated the transform matrix only once, and scheduled two tiles (one for left-eye one for right-eye) with the same coordinate to the same vault in HMC.",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "Motivated by the observation that the ATW transform matrix generated by rotation on a 2D image is shared by both eyes, PIMVR [57] calculated the transform matrix only once, and scheduled two tiles (one for left-eye one for right-eye) with the same coordinate to the same vault in HMC. However, in contrast to the  360 ° VR video streaming, ATW is in 2D planar format, and share the same compute results across eyes. These two characterizations indicate that such optimizations in the planar world are infeasible to be applied in 3D PT-rendering.",
      "type": "sliding_window",
      "tokens": 135
    },
    {
      "text": "These two characterizations indicate that such optimizations in the planar world are infeasible to be applied in 3D PT-rendering. Hardware assist on VRs:  Various energy-efﬁcient hardware modiﬁcations [28] have been proposed to reduce energy con- sumption in the VR domain. For example, PTU [28] uses a hardware-accelerated rendering unit (HAR) to mitigate energy- overheads due to on-device rendering.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "For example, PTU [28] uses a hardware-accelerated rendering unit (HAR) to mitigate energy- overheads due to on-device rendering. In this work, we pro- pose two optimizations, i.e.,  EA  and  AE , which can be coupled with the existing  360 ° video compute engine (without any hardware modiﬁcations), and are even more energy efﬁcient than the existing state-of-the-art  PTU  (discussed in Sec. V).",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "V). Moreover,  EA  and  AE  can further be integrated into  PTU  to save even more energy. Pixel Content Reuse on VRs:  Pixel value reuse has been well-studied in VRs [17], [22], [23], [29], [33], [50], [60], [66] to improve throughput and performance.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "Pixel Content Reuse on VRs:  Pixel value reuse has been well-studied in VRs [17], [22], [23], [29], [33], [50], [60], [66] to improve throughput and performance. For example, DeltaVR [29] adaptively reuses the redundant VR pixels across multiple VR frames to improve performance. These works focus on the pixel content reuse, which is the last stage ( Projection Mapping ) in the  360 ° video projection pipeline (discussed in Sec III).",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "These works focus on the pixel content reuse, which is the last stage ( Projection Mapping ) in the  360 ° video projection pipeline (discussed in Sec III). However, none of these existing schemes leverage reducing the large amounts of “redundant” computations in the preceding stage (projection computation). 251 \nOur proposed  EA  and  AE  designs focus on these intensive projection computations, and as such are orthogonal to these prior efforts.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "251 \nOur proposed  EA  and  AE  designs focus on these intensive projection computations, and as such are orthogonal to these prior efforts. In our future work, we would like to further explore the beneﬁts by incorporating them into our design. Head Orientation Prediction for  360 °  Video Streaming: To optimize both performance and energy, researchers have leveraged the powerful remote rendering engines on cloud to predict the next head orientation for the VR clients [2], [6], [18], [23], [30]–[32].",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "Head Orientation Prediction for  360 °  Video Streaming: To optimize both performance and energy, researchers have leveraged the powerful remote rendering engines on cloud to predict the next head orientation for the VR clients [2], [6], [18], [23], [30]–[32]. FlashBack maintains a storage cache of multiple versions of pre-rendered frames, which can be quickly indexed by head orientations [2]. In comparison, Semantic- Aware-Streaming (SAS) exploits the semantic information inherent in a VR video content to precisely predict users’ next head orientations [28].",
      "type": "sliding_window",
      "tokens": 135
    },
    {
      "text": "In comparison, Semantic- Aware-Streaming (SAS) exploits the semantic information inherent in a VR video content to precisely predict users’ next head orientations [28]. These optimizations rely on the powerful cloud with a high bandwidth access, which may not be always available. However, our work focuses on edge-side optimization, which can also be implemented as a comple- mentary add-on in such cloud-assisted systems.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "However, our work focuses on edge-side optimization, which can also be implemented as a comple- mentary add-on in such cloud-assisted systems. Energy Optimizations in Conventional Video Processing: In the existing planar video processing pipeline on mobile de- vices, prior works have looked at memory [1], [63], [64], dis- play [13]–[15], [34] and codec [49], [65], and identiﬁed ”mem- ory” as the major energy bottleneck. For example, AFBC [1] is proposed to efﬁciently compress video streams between the processing pipeline blocks.",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "For example, AFBC [1] is proposed to efﬁciently compress video streams between the processing pipeline blocks. MACH [63] integrates a display cache to reduce the amount of memory bandwidth. Although, these techniques can potentially save memory usage/energy for  360 ° VR videos, as discussed in earlier sections, due to inherent nature of  360 ° video processing, which introduces additional overheads for projection computation, we identify compute  to be the major energy bottleneck.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "Although, these techniques can potentially save memory usage/energy for  360 ° VR videos, as discussed in earlier sections, due to inherent nature of  360 ° video processing, which introduces additional overheads for projection computation, we identify compute  to be the major energy bottleneck. Hence, these memory optimizations are not applicable to reduce compute energy on  360 ° VR videos. VII.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "VII. C ONCLUDING  R EMARKS \n360° VR videos have become the next trend in entertain- ment media and soon will become an integral part of the technology inﬂuencing many application domains. However, unlike planar videos, the 360° VR video streaming demands signiﬁcantly more compute power from a battery-operated headset.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "However, unlike planar videos, the 360° VR video streaming demands signiﬁcantly more compute power from a battery-operated headset. Thus, prior research has proposed using accelerators for optimizing the computations. In contrast, this paper attempts to exploit available “re- dundancies” in computation by analyzing the VR projection computation pipeline.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "In contrast, this paper attempts to exploit available “re- dundancies” in computation by analyzing the VR projection computation pipeline. Speciﬁcally, we propose and evaluate in detail two pluggable schemes (for taking advantage of intrinsic temporal-spatial reuse), and prototype them as microarchitec- tural augmentations using FPGA. Our experimental results show 34% computation reduction and 17% energy savings, compared to the state-of-the-art [28].",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Our experimental results show 34% computation reduction and 17% energy savings, compared to the state-of-the-art [28]. In the future, we would also like to explore other opportunities to improve energy efﬁciency and tune the computation pipelines to cater more towards VR applications. We believe, given that the current VR devices are battery-backed, these kinds of energy savings and performance improvements will not only enable the users to experience longer videos, but also encourage both industry \nand academia to work further on improving the pipeline to make VR more pervasive and versatile.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "We believe, given that the current VR devices are battery-backed, these kinds of energy savings and performance improvements will not only enable the users to experience longer videos, but also encourage both industry \nand academia to work further on improving the pipeline to make VR more pervasive and versatile. A CKNOWLEDGMENT \nThis research is supported in part by NSF grants #1763681, #1629915, #1629129, #1317560, #1526750, #1714389, #1912495, and a DARPA/SRC JUMP grant. We would also like to thank Dr. Jack Sampson, Dr. Aasheesh Kolli and Dr. Timothy Zhu for their feedback on this paper.",
      "type": "sliding_window",
      "tokens": 161
    },
    {
      "text": "We would also like to thank Dr. Jack Sampson, Dr. Aasheesh Kolli and Dr. Timothy Zhu for their feedback on this paper. R EFERENCES \n[1] Arm Holdings, “Arm Frame Buffer Compression (AFBC).” ”https: //developer.arm.com/architectures/media-architectures/afbc”, 2019. [2] K. Boos, D. Chu, and E. Cuervo, “FlashBack: Immersive Virtual Reality on Mobile Devices via Rendering Memoization,” in  Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services , ser.",
      "type": "sliding_window",
      "tokens": 153
    },
    {
      "text": "[2] K. Boos, D. Chu, and E. Cuervo, “FlashBack: Immersive Virtual Reality on Mobile Devices via Rendering Memoization,” in  Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services , ser. MobiSys ’16, 2016, pp. 291–304.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "291–304. [3] X. Corbillon, F. De Simone, and G. Simon, “360-Degreee Video Head Movement Dataset,” in  Proceedings of the 8th ACM on Multimedia Systems Conference , 2017, pp. 199–204.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "199–204. [4] Discovery, “Caring for Rhinos: Discovery VR (360 Video).” ”https:// www.youtube.com/watch?v=7IWp875pCxQ”, 2019. [5] Discovery, “Elephants on the Brink.” ”https://www.youtube.com/watch?",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "[5] Discovery, “Elephants on the Brink.” ”https://www.youtube.com/watch? v=2bpICIClAIg”, 2019. [6] T. El-Ganainy and M. Hefeeda, “Streaming Virtual Reality Content,” CoRR , vol.",
      "type": "sliding_window",
      "tokens": 76
    },
    {
      "text": "[6] T. El-Ganainy and M. Hefeeda, “Streaming Virtual Reality Content,” CoRR , vol. abs/1612.08350, 2016. [Online].",
      "type": "sliding_window",
      "tokens": 44
    },
    {
      "text": "[Online]. Available: http://arxiv.org/ abs/1612.08350 [7] Facebook, “Facebook 360,” ”https://facebook360.fb.com/”, 2019. [8] Facebook Inc., “Facebook Oculus,” ”https://www.oculus.com/”.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "[8] Facebook Inc., “Facebook Oculus,” ”https://www.oculus.com/”. [9] Google, “360° videos - Google Arts & Culture,” ”https://artsandculture. google.com/project/360-videos”.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "google.com/project/360-videos”. [10] Google, “More Ways to Watch and Play with AR and VR.” ”https:// blog.google/products/google-vr/more-ways-watch-and-play-ar-and-vr”. [11] Google, “Build Virtual Worlds.” ”https://developers.google.com/vr”, 2019.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "[11] Google, “Build Virtual Worlds.” ”https://developers.google.com/vr”, 2019. [12] Google, “GVR Android SDK Samples - Video360.” ”https://github.com/ googlevr/gvr-android-sdk/blob/master/samples/sdk-video360/src/main/ java/com/google/vr/sdk/samples/video360/VrVideoActivity.java#L257”, 2019. [13] M. Ham, I. Dae, and C. Choi, “LPD: Low Power Display Mechanism for Mobile and Wearable Devices,” in  Proceedings of the USENIX Conference on Usenix Annual Technical Conference (ATC) , 2015, pp.",
      "type": "sliding_window",
      "tokens": 194
    },
    {
      "text": "[13] M. Ham, I. Dae, and C. Choi, “LPD: Low Power Display Mechanism for Mobile and Wearable Devices,” in  Proceedings of the USENIX Conference on Usenix Annual Technical Conference (ATC) , 2015, pp. 587–598. [14] K. Han, Z. Fang, P. Diefenbaugh, R. Forand, R. R. Iyer, and D. Newell, “Using Checksum to Reduce Power Consumption of Display Systems for Low-motion Content,” in  2009 IEEE International Conference on Computer Design , 2009, pp.",
      "type": "sliding_window",
      "tokens": 140
    },
    {
      "text": "[14] K. Han, Z. Fang, P. Diefenbaugh, R. Forand, R. R. Iyer, and D. Newell, “Using Checksum to Reduce Power Consumption of Display Systems for Low-motion Content,” in  2009 IEEE International Conference on Computer Design , 2009, pp. 47–53. [15] K. Han, A. W. Min, N. S. Jeganathan, and P. S. Diefenbaugh, “A Hybrid Display Frame Buffer Architecture for Energy Efﬁcient Display Subsystems,” in  International Symposium on Low Power Electronics and Design (ISLPED) , 2013, pp.",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "[15] K. Han, A. W. Min, N. S. Jeganathan, and P. S. Diefenbaugh, “A Hybrid Display Frame Buffer Architecture for Energy Efﬁcient Display Subsystems,” in  International Symposium on Low Power Electronics and Design (ISLPED) , 2013, pp. 347–353. [16] T. HARDWARE, “Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.” ”https://support.oculus.com/248749509016567/”, 2019.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "[16] T. HARDWARE, “Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.” ”https://support.oculus.com/248749509016567/”, 2019. [17] B. Haynes, A. Mazumdar, A. Alaghi, M. Balazinska, L. Ceze, and A. Cheung, “LightDB: A DBMS for Virtual Reality Video,”  Proc. VLDB Endow.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "1192–1205, 2018. [18] J. He, M. A. Qureshi, L. Qiu, J. Li, F. Li, and L. Han, “Rubiks: Practical 360-Degree Streaming for Smartphones,” in  Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services , 2018, pp.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "He, M. A. Qureshi, L. Qiu, J. Li, F. Li, and L. Han, “Rubiks: Practical 360-Degree Streaming for Smartphones,” in  Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services , 2018, pp. 482–494. [19] HEADJACK, “The Best Encoding Settings For Your 4k 360 3D VR Videos + FREE Encoding Tool,” ”https://headjack.io/blog/best- encoding-settings-resolution-for-4k-360-3d-vr-videos/”.",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "[19] HEADJACK, “The Best Encoding Settings For Your 4k 360 3D VR Videos + FREE Encoding Tool,” ”https://headjack.io/blog/best- encoding-settings-resolution-for-4k-360-3d-vr-videos/”. [20] C. Heather Bellini, W. Chen, M. Sugiyama, M. Shin, S. Alam, and D. Takayama, “Virtual and Augmented Reality.” ”https://www.goldmansachs.com/insights/pages/technology-driving- innovation-folder/virtual-and-augmented-reality/report.pdf”, 2016. [21] L. F. Hodges, “Tutorial: Time-multiplexed Stereoscopic Computer Graphics,”  IEEE Computer Graphics and Applications , pp.",
      "type": "sliding_window",
      "tokens": 205
    },
    {
      "text": "[21] L. F. Hodges, “Tutorial: Time-multiplexed Stereoscopic Computer Graphics,”  IEEE Computer Graphics and Applications , pp. 20–30, 1992. 252 \n[22] A.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "252 \n[22] A. Holdings, “White Paper: 360-Degree Video Rendering.” ”https://community.arm.com/developer/tools-software/graphics/b/ blog/posts/white-paper-360-degree-video-rendering”, 2019. [23] J. Huang, Z. Chen, D. Ceylan, and H. Jin, “6-DOF VR Videos with a Single 360-camera,”  2017 IEEE Virtual Reality (VR) , pp.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "[23] J. Huang, Z. Chen, D. Ceylan, and H. Jin, “6-DOF VR Videos with a Single 360-camera,”  2017 IEEE Virtual Reality (VR) , pp. 37–44, 2017. [24] A. Inc., “Rendering Omni-directional Stereo Content.” ”https:// developers.google.com/vr/jump/rendering-ods-content.pdf”, 2019.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "[24] A. Inc., “Rendering Omni-directional Stereo Content.” ”https:// developers.google.com/vr/jump/rendering-ods-content.pdf”, 2019. [25] N. INSTRUMENTS, “Peak Signal-to-Noise Ratio as an Image Quality Metric.” ”https://www.ni.com/en-us/innovations/white-papers/11/peak- signal-to-noise-ratio-as-an-image-quality-metric.html”, 2019. [26] B. C. Kim and C. E. Rhee, “Compression Efﬁciency Evaluation for Virtual Reality Videos by Projection Scheme,”  IEIE Transactions on Smart Processing & Computing , pp.",
      "type": "sliding_window",
      "tokens": 182
    },
    {
      "text": "[26] B. C. Kim and C. E. Rhee, “Compression Efﬁciency Evaluation for Virtual Reality Videos by Projection Scheme,”  IEIE Transactions on Smart Processing & Computing , pp. 102–108, 2017. [27] S. M. LaValle, “The Geometry of Virtual Worlds.” ”http://msl.cs.uiuc.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "[27] S. M. LaValle, “The Geometry of Virtual Worlds.” ”http://msl.cs.uiuc. edu/vr/vrch3.pdf”, 2019. [28] Y. Leng, C.-C. Chen, Q.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "[28] Y. Leng, C.-C. Chen, Q. Sun, J. Huang, and Y. Zhu, “Energy-efﬁcient Video Processing for Virtual Reality,” in  Proceedings of the International Symposium on Computer Architecture (ISCA) , 2019, pp. 91–103.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "91–103. [29] Y. Li and W. Gao, “DeltaVR: Achieving High-Performance Mobile VR Dynamics through Pixel Reuse,” in  2019 18th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN) , 2019, pp. 13–24.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "13–24. [30] L. Liu, R. Zhong, W. Zhang, Y. Liu, J. Zhang, L. Zhang, and M. Gruteser, “Cutting the Cord: Designing a High-quality Untethered VR System with Low Latency Remote Rendering,” in  Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services , ser. MobiSys ’18, 2018, pp.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "MobiSys ’18, 2018, pp. 68–80. [31] X. Liu, Q. Xiao, V. Gopalakrishnan, B. Han, F. Qian, and M. Varvello, “360° Innovations for Panoramic Video Streaming,” in  Proceedings of the 16th ACM Workshop on Hot Topics in Networks , ser.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "[31] X. Liu, Q. Xiao, V. Gopalakrishnan, B. Han, F. Qian, and M. Varvello, “360° Innovations for Panoramic Video Streaming,” in  Proceedings of the 16th ACM Workshop on Hot Topics in Networks , ser. HotNets-XVI, 2017, pp. 50–56.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "50–56. [32] B. Luo, F. Xu, C. Richardt, and J. Yong, “Parallax360: Stereoscopic 360° Scene Representation for Head-Motion Parallax,”  IEEE Transactions on Visualization and Computer Graphics , pp. 1545–1553, 2018.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "1545–1553, 2018. [33] A. Mazumdar, T. Moreau, S. Kim, M. Cowan, A. Alaghi, L. Ceze, M. Oskin, and V. Sathe, “Exploring Computation-communication Trade- offs in Camera Systems,” in  2017 IEEE International Symposium on Workload Characterization (IISWC) , 2017, pp. 177–186.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "177–186. [34] H. Miao and F. X. Lin, “Tell Your Graphics Stack That the Display Is Circular,” in  Proceedings of the 17th International Workshop on Mobile Computing Systems and Applications , ser. HotMobile ’16, 2016, pp.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "HotMobile ’16, 2016, pp. 57– 62. [35] mooovr, “RollerCoaster at Seoul Grand Park.” ”https://www.youtube.",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "[35] mooovr, “RollerCoaster at Seoul Grand Park.” ”https://www.youtube. com/watch?v=8lsB-P8nGSM”, 2019. [36] Nvidia, “JETSON AGX XAVIER AND THE NEW ERA OF AU- TONOMOUS MACHINES.” ”http://info.nvidia.com/rs/156-OFN-742/ images/Jetson AGX Xavier New Era Autonomous Machines.pdf”, 2019.",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "[36] Nvidia, “JETSON AGX XAVIER AND THE NEW ERA OF AU- TONOMOUS MACHINES.” ”http://info.nvidia.com/rs/156-OFN-742/ images/Jetson AGX Xavier New Era Autonomous Machines.pdf”, 2019. [37] Oculus, “Rendering to the Oculus Rift,” ”https://developer.oculus.com/ documentation/pcsdk/latest/concepts/dg-render/”. [38] Oculus, “Asynchronous TimeWarp (ATW).” ”https://developer.",
      "type": "sliding_window",
      "tokens": 162
    },
    {
      "text": "[38] Oculus, “Asynchronous TimeWarp (ATW).” ”https://developer. oculus.com/documentation/mobilesdk/latest/concepts/mobile-timewarp- overview/?locale=en US”, 2019. [39] Oculus, “Oculus Rift and Rift S Minimum Requirements and Sys- tem Speciﬁcations.” ”https://www.tomshardware.com/news/magic-leap- tegra-specs-release,37443.html”, 2019.",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "[39] Oculus, “Oculus Rift and Rift S Minimum Requirements and Sys- tem Speciﬁcations.” ”https://www.tomshardware.com/news/magic-leap- tegra-specs-release,37443.html”, 2019. [40] OpenCV, “Similarity check (PNSR and SSIM) on the GPU.” ”https://docs.opencv.org/2.4/doc/tutorials/gpu/gpu-basics- similarity/gpu-basics-similarity.html”, 2019. [41] OpenGL, “Cubemaps - Learn OpenGL.” ”https://learnopengl.com/ Advanced-OpenGL/Cubemaps”, 2019.",
      "type": "sliding_window",
      "tokens": 191
    },
    {
      "text": "[41] OpenGL, “Cubemaps - Learn OpenGL.” ”https://learnopengl.com/ Advanced-OpenGL/Cubemaps”, 2019. [42] OpenGL, “The Industry’s Foundation for High Performance Graphics.” ”https://www.opengl.org/”, 2019. [43] O.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "[43] O. Rift, “Oculus Rift - How Does Time Warping Work?” ”https://www. youtube.com/watch?v=WvtEXMlQQtI”, 2019.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "youtube.com/watch?v=WvtEXMlQQtI”, 2019. [44] Samsung, “Samsung Gear VR,” ”https://www.samsung.com/global/ galaxy/gear-vr/”. [45] Samsung, “Explore New Dimensions.” ”https://www.samsung.com/ global/galaxy/gear-vr/#display”, 2019.",
      "type": "sliding_window",
      "tokens": 107
    },
    {
      "text": "[45] Samsung, “Explore New Dimensions.” ”https://www.samsung.com/ global/galaxy/gear-vr/#display”, 2019. [46] SkySports, “Sky VR Virtual Reality,” ”https://www.skysports.com/ mobile/apps/10606146/sky-vr-virtual-reality”, 2019. [47] Tom’s HARDWARE, “Nvidia’s Jetson TX2 Powers GameFace Labs’ Standalone VR Headset.” ”https://www.tomshardware.com/news/ gameface-labs-standalone-steamvr-headset,37112.html”, 2019.",
      "type": "sliding_window",
      "tokens": 173
    },
    {
      "text": "[47] Tom’s HARDWARE, “Nvidia’s Jetson TX2 Powers GameFace Labs’ Standalone VR Headset.” ”https://www.tomshardware.com/news/ gameface-labs-standalone-steamvr-headset,37112.html”, 2019. [48] R. Toth, J. Nilsson, and T. Akenine-M¨oller, “Comparison of Projection Methods for Rendering Virtual Reality,” in  Proceedings of High Perfor- mance Graphics , ser. HPG ’16, 2016, pp.",
      "type": "sliding_window",
      "tokens": 150
    },
    {
      "text": "HPG ’16, 2016, pp. 163–171. [49] C.-H. Tsai, H.-T. Wang, C.-L. Liu, Y. Li, and C.-Y.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "[49] C.-H. Tsai, H.-T. Wang, C.-L. Liu, Y. Li, and C.-Y. Lee, “A 446.6 K-gates 0.55–1.2 V H. 265/HEVC decoder for next generation video applications,” in  2013 IEEE Asian Solid-State Circuits Conference (A- SSCC) , 2013, pp. 305–308.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "305–308. [50] A. Vlachos, “Advanced VR Rendering in Valve.” ”http: //media.steampowered.com/apps/valve/2015/Alex Vlachos Advanced VR Rendering GDC2015.pdf”, 2019. [51] F. G. VR360, “Virtual guided tour of Paris.” ”https://www.youtube.com/ watch?v=sJxiPiAaB4k”, 2019.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "[51] F. G. VR360, “Virtual guided tour of Paris.” ”https://www.youtube.com/ watch?v=sJxiPiAaB4k”, 2019. [52] Wikepedia, “Equirectangular Projection.” ”https://en.wikipedia.org/wiki/ Equirectangular projection”, 2019. [53] Wikipedia, “Pixel 2,” ”https://en.wikipedia.org/wiki/Pixel 2”.",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "[53] Wikipedia, “Pixel 2,” ”https://en.wikipedia.org/wiki/Pixel 2”. [54] Wikipedia, “Active-Matrix Organic Light-Emitting Diode,” ”https://en. wikipedia.org/wiki/AMOLED”, 2019.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "wikipedia.org/wiki/AMOLED”, 2019. [55] Wikipedia, “Virtual Reality.” ”https://en.wikipedia.org/wiki/Peak signal-to-noise ratio#: ∼ :targetText=Typical \\ %20values%20for% 20the%20PSNR,20 \\ %20dB%20to%2025%20dB.”, 2019. [56] B.",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "[56] B. Worldwide, “NYC 360 Timelapse.” ”https://www.youtube.com/ watch?v=CIw8R8thnm8”, 2019. [57] C. Xie, X. Zhang, A. Li, X. Fu, and S. Song, “PIM-VR: Erasing Motion Anomalies In Highly-Interactive Virtual Reality World with Customized Memory Cube,” in  Proceedings of the International Symposium on High- Performance Computer Architecture (HPCA) , 2019, pp.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "[57] C. Xie, X. Zhang, A. Li, X. Fu, and S. Song, “PIM-VR: Erasing Motion Anomalies In Highly-Interactive Virtual Reality World with Customized Memory Cube,” in  Proceedings of the International Symposium on High- Performance Computer Architecture (HPCA) , 2019, pp. 609–622. [58] Xilinx, “Vivado Design Hub - Installation and Licensing,” ”https://www.xilinx.com/support/documentation-navigation/design- hubs/dh0013-vivado-installation-and-licensing-hub.html”.",
      "type": "sliding_window",
      "tokens": 165
    },
    {
      "text": "[58] Xilinx, “Vivado Design Hub - Installation and Licensing,” ”https://www.xilinx.com/support/documentation-navigation/design- hubs/dh0013-vivado-installation-and-licensing-hub.html”. [59] Xilinx, “Vivado Design Suite - HLx Editions.” ”https://www.xilinx.com/ products/design-tools/vivado.html”, 2019. [60] xinreality, “Asynchronous Spacewarp.” ”https://xinreality.com/wiki/ Asynchronous Spacewarp”, 2019.",
      "type": "sliding_window",
      "tokens": 169
    },
    {
      "text": "[60] xinreality, “Asynchronous Spacewarp.” ”https://xinreality.com/wiki/ Asynchronous Spacewarp”, 2019. [61] YouTube, “Get Started with YouTube VR.” ”https://support.google.com/ youtube/answer/7205134?hl=en”, 2019. [62] V. Zakharchenko, K. P. Choi, and J. H. Park, “Quality metric for spherical panoramic video,” in  Optics and Photonics for Information Processing X , K. M. Iftekharuddin, A.",
      "type": "sliding_window",
      "tokens": 143
    },
    {
      "text": "[62] V. Zakharchenko, K. P. Choi, and J. H. Park, “Quality metric for spherical panoramic video,” in  Optics and Photonics for Information Processing X , K. M. Iftekharuddin, A. A. S. Awwal, M. G. V´azquez, A. M´arquez, and M. A. Matin, Eds., International Society for Optics and Photonics. SPIE, 2016, pp.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "SPIE, 2016, pp. 57 – 65. [63] H. Zhang, P. V. Rengasamy, S. Zhao, N. C. Nachiappan, A. Sivasub- ramaniam, M. T. Kandemir, R. Iyer, and C. R. Das, “Race-to-sleep + Content Caching + Display Caching: A Recipe for Energy-efﬁcient Video Streaming on Handhelds,” in  Proceedings of the International Symposium on Microarchitecture (MICRO) , 2017, pp.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "[63] H. Zhang, P. V. Rengasamy, S. Zhao, N. C. Nachiappan, A. Sivasub- ramaniam, M. T. Kandemir, R. Iyer, and C. R. Das, “Race-to-sleep + Content Caching + Display Caching: A Recipe for Energy-efﬁcient Video Streaming on Handhelds,” in  Proceedings of the International Symposium on Microarchitecture (MICRO) , 2017, pp. 517–531. [64] H. Zhang, S. Zhao, A. Pattnaik, M. T. Kandemir, A. Sivasubramaniam, and C. R. Das, “Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices,” in  Proceedings of the International Symposium on Microarchitecture (MICRO) , 2019, pp.",
      "type": "sliding_window",
      "tokens": 205
    },
    {
      "text": "[64] H. Zhang, S. Zhao, A. Pattnaik, M. T. Kandemir, A. Sivasubramaniam, and C. R. Das, “Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices,” in  Proceedings of the International Symposium on Microarchitecture (MICRO) , 2019, pp. 657–669. [65] D. Zhou, S. Wang, H. Sun, J. Zhou, J. Zhu, Y. Zhao, J. Zhou, S. Zhang, S. Kimura, T. Yoshimura, and S. Goto, “14.7 a 4gpixel/s 8/10b h.265/hevc video decoder chip for 8k ultra hd applications,” in 2016 IEEE International Solid-State Circuits Conference (ISSCC) , 2016, pp.",
      "type": "sliding_window",
      "tokens": 208
    },
    {
      "text": "[65] D. Zhou, S. Wang, H. Sun, J. Zhou, J. Zhu, Y. Zhao, J. Zhou, S. Zhang, S. Kimura, T. Yoshimura, and S. Goto, “14.7 a 4gpixel/s 8/10b h.265/hevc video decoder chip for 8k ultra hd applications,” in 2016 IEEE International Solid-State Circuits Conference (ISSCC) , 2016, pp. 266–268. [66] Y. Zhu, A. Samajdar, M. Mattina, and P. Whatmough, “Euphrates: Algorithm-SoC Co-design for Low-power Mobile Continuous Vision,” in  Proceedings of the International Symposium on Computer Architec- ture (ISCA) , 2018, pp.",
      "type": "sliding_window",
      "tokens": 200
    },
    {
      "text": "[66] Y. Zhu, A. Samajdar, M. Mattina, and P. Whatmough, “Euphrates: Algorithm-SoC Co-design for Low-power Mobile Continuous Vision,” in  Proceedings of the International Symposium on Computer Architec- ture (ISCA) , 2018, pp. 547–560. 253",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Thus, improving the computational efﬁciency of the video processing pipeline in a VR is critical. Abstract —The emergence of virtual reality (VR) and aug- mented reality (AR) has revolutionized our lives by enabling a  360 °  artiﬁcial sensory stimulation across diverse domains, including, but not limited to, sports, media, healthcare, and gaming. Unlike the conventional planar video processing, where memory access is the main bottleneck, in  360 °  VR videos the compute is the primary bottleneck and contributes to more than 50%  energy consumption in battery-operated VR headsets.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "While prior efforts have attempted to address this problem through acceleration using a GPU or FPGA, none of them has analyzed the  360 °  VR pipeline to examine if there is any scope to optimize the computation with known techniques such as memoization. Thus, in this paper, we analyze the VR computation pipeline and observe that there is signiﬁcant scope to skip computations by leveraging the temporal and spatial locality in head orienta- tion and eye correlations, respectively, resulting in computation reduction and energy efﬁciency. Thus, improving the computational efﬁciency of the video processing pipeline in a VR is critical.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "Thus, in this paper, we analyze the VR computation pipeline and observe that there is signiﬁcant scope to skip computations by leveraging the temporal and spatial locality in head orienta- tion and eye correlations, respectively, resulting in computation reduction and energy efﬁciency. We propose both software modiﬁcations for existing compute pipeline and microarchitectural additions for further enhancement. The proposed  D´ej`a View  design takes advantage of temporal reuse by memoizing head orientation and spatial reuse by establishing a relationship between left and right eye projection, and can be implemented either on a GPU or an FPGA.",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "We propose both software modiﬁcations for existing compute pipeline and microarchitectural additions for further enhancement. We evaluate our design by implementing the software enhancements on an NVIDIA Jetson TX2 GPU board and our microarchitectural additions on a Xilinx Zynq-7000 FPGA model using ﬁve video workloads. Experimental results show that  D´ej`a View  can provide  34%  computation reduction and  17%  energy saving, compared to the state-of-the-art design.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "Index Terms —Virtual Reality, Edge Computing, IoT,  360 ° Video Processing \nI. I NTRODUCTION \nRecent developments in technology, computing and com- munication have brought signiﬁcant changes to the lifestyle of common people by providing them access to increasingly sophisticated devices. Experimental results show that  D´ej`a View  can provide  34%  computation reduction and  17%  energy saving, compared to the state-of-the-art design. Especially, VR and AR are now gaining traction because of their versatile nature of providing an immersive sensory experience, which is not possible with the conventional systems – especially in the domain of video streaming.",
      "type": "sliding_window_shuffled",
      "tokens": 141,
      "augmented": true
    },
    {
      "text": "Even today, more than 10 million users enjoy  360 ° videos \nusing Google Cardboard [10], Samsung Gear VR [44], and Oculus VR [8], to experience  360 ° video [7], art museum [9], live stadium [46], etc. Especially, VR and AR are now gaining traction because of their versatile nature of providing an immersive sensory experience, which is not possible with the conventional systems – especially in the domain of video streaming. They are emerging as one of the most important entertainment markets and Goldman Sachs predicts that, by 2025, around 79 million users will use online video streaming from the VR/AR ecosystem, resulting in a multi-billion dollar market [20], penetrating the ﬁelds of media streaming, VR gaming, education, medicine, communication and many more.",
      "type": "sliding_window_shuffled",
      "tokens": 175,
      "augmented": true
    },
    {
      "text": "Even today, more than 10 million users enjoy  360 ° videos \nusing Google Cardboard [10], Samsung Gear VR [44], and Oculus VR [8], to experience  360 ° video [7], art museum [9], live stadium [46], etc. The  360 ° videos are created by capturing scenes in all directions typically using omnidirectional cameras or a set of cameras. They are further encoded by the conventional video encoders, as if they are planar videos, for transmission efﬁciency.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "The video frames are transmitted to the users, who wear a portable VR headset (like Facebook Oculus or Google Cardboard), via  Youtube  or  Facebook 360  services [7], [61]. 360 ° video streaming creates an interactive and immersive environment by connecting the user and the video content; the users are allowed to move their heads’ orientation to enjoy the surroundings in all perspectives along with a 3D view, i.e., a different view for each of the eyes, and hence creating an illusion that the user is present at the scene rather than viewing it on a projected surface. They are further encoded by the conventional video encoders, as if they are planar videos, for transmission efﬁciency.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "This immersive experience comes at the cost of additional computations - not only is the video being streamed,  the streaming itself changes with the head orientation. Moreover, streaming requires two projections for both the eyes. 360 ° video streaming creates an interactive and immersive environment by connecting the user and the video content; the users are allowed to move their heads’ orientation to enjoy the surroundings in all perspectives along with a 3D view, i.e., a different view for each of the eyes, and hence creating an illusion that the user is present at the scene rather than viewing it on a projected surface.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "Moreover, streaming requires two projections for both the eyes. Thus, unlike planar videos, in  360 ° videos, speciﬁcally the projection computations for capturing the head movements and eye correlations, are sig- niﬁcantly computation-heavy, amounting to  59%  of the overall VR (headset) power budget. As the 360 ° video is not in a planar format, the VR ecosystem converts it to a conformal 2D format by passing it through multiple stages of transformations.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "Current head mounted VR devices use a GPU for this heavy computation. Thus, unlike planar videos, in  360 ° videos, speciﬁcally the projection computations for capturing the head movements and eye correlations, are sig- niﬁcantly computation-heavy, amounting to  59%  of the overall VR (headset) power budget. Since the head mounted VR devices are battery-backed, the computations that draw high power from the battery greatly hinder the experience of watching long  360 ° videos [39].",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "However, prior works do not consider other avenues for optimizing the computation. This heavy computation has become an acceleration candi- date/target in previous works, by ofﬂoading the entire compu- tation, as is, to an accelerator (GPU [39], or FPGA [28]). Since the head mounted VR devices are battery-backed, the computations that draw high power from the battery greatly hinder the experience of watching long  360 ° videos [39].",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "However, prior works do not consider other avenues for optimizing the computation. In this context, this paper dives deep to understand the projection computation pipeline for ex- ploring available opportunities and optimizations for speedup as well as power savings. Since  head movement  and  cor- relations between the left and right eye projections  are the two critical components of the projection computation, we analyze and study them to explore possible opportunities to exploit these relations.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Speciﬁcally, we analyze four scenarios, \n241 \n2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA) \n978-1-7281-4661-4/20/$31.00 ©2020 IEEE DOI 10.1109/ISCA45697.2020.00030 \namely,  InterFrame-IntraEye (EA) ,  IntraFrame-InterEye (AE) , IntraFrame-IntraEye (AA) , and  InterFrame-InterEye (EE) , that are critical in capturing the head movement and eye correlation for projection computation. Out of these four scenarios, we observe that  EA  computation for head orientation can be exploited for  temporal reuse/memoization  since there is little difference between two previous head orientations, and  AE computation for exploiting the correlation between both the eyes by  spatial reuse – correlating the coordinate relationship between both eyes . Since  head movement  and  cor- relations between the left and right eye projections  are the two critical components of the projection computation, we analyze and study them to explore possible opportunities to exploit these relations.",
      "type": "sliding_window_shuffled",
      "tokens": 250,
      "augmented": true
    },
    {
      "text": "The proposed ar- chitecture is named  D´ej`a View , a play on the word D´ej`a vu, as it uses previous or  already seen  views. Based on this observation, we develop computation optimization mechanisms for facilitating temporal reuse/memoization and spatial reuse that can be integrated with a VR projection computation pipeline to signiﬁcantly reduce energy consumption of the device. Out of these four scenarios, we observe that  EA  computation for head orientation can be exploited for  temporal reuse/memoization  since there is little difference between two previous head orientations, and  AE computation for exploiting the correlation between both the eyes by  spatial reuse – correlating the coordinate relationship between both eyes .",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "The major  contributions of the paper can be summarized as follows: \n•  From an open-source 360° VR video dataset [3],  we identify both temporal reuse and spatial locality that exists in user behavior . To the best of our knowledge, this is the ﬁrst work that leverages head orientation and correlation between eyes to do efﬁcient memoization and in turn result in compute reduction in the VR video streaming domain. The proposed ar- chitecture is named  D´ej`a View , a play on the word D´ej`a vu, as it uses previous or  already seen  views.",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "Such invariances are leveraged as reuse opportunities to reduce the compute-heavy projection computation. We formally analyze the potential “input invari- ability” in the  projection computation  during  360 ° video streaming, which manifests in the head movement locality (temporal reuse) and the stationarity relationship between two eyes (spatial reuse). The major  contributions of the paper can be summarized as follows: \n•  From an open-source 360° VR video dataset [3],  we identify both temporal reuse and spatial locality that exists in user behavior .",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Such invariances are leveraged as reuse opportunities to reduce the compute-heavy projection computation. •  We design two complementary schemes to capture both temporal and spatial reuse opportunities. We propose a memoization scheme, called  EA , to capture recent head orientation data for temporal reuse, and for the spatial reuse, we design the  AE  scheme, which leverages the stationary relationship between two eyes to efﬁciently reduce the amount of projection computation.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "•  We implement both our schemes as a  software  enhancement to the existing compute pipeline in NVIDIA GPUs. We propose a memoization scheme, called  EA , to capture recent head orientation data for temporal reuse, and for the spatial reuse, we design the  AE  scheme, which leverages the stationary relationship between two eyes to efﬁciently reduce the amount of projection computation. To further exploit the energy efﬁciency, we also implement our hardware  prototype using an FPGA to evaluate the energy beneﬁts brought by the microarchitectural augmentations.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Both the proposed software and hardware solutions are modular , and hence can be integrated to the existing pipeline with little change. •  We evaluate our integrated design, including both  EA  and AE , using an open-source 360° VR video dataset [3] with the traces of 20 users watching 5 different VR videos. To further exploit the energy efﬁciency, we also implement our hardware  prototype using an FPGA to evaluate the energy beneﬁts brought by the microarchitectural augmentations.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "Over- all, our experimental results show that, on an average,  D´ej`a View  can provide  54%  compute reduction, which translates to  28%  total energy savings compared to the baseline setup. •  We evaluate our integrated design, including both  EA  and AE , using an open-source 360° VR video dataset [3] with the traces of 20 users watching 5 different VR videos. Compared to a state-of-the-art scheme [28], our design provides  34%  reduction in projection computations, which translates to  17%  additional energy savings.",
      "type": "sliding_window_shuffled",
      "tokens": 125,
      "augmented": true
    },
    {
      "text": "Fig. Compared to a state-of-the-art scheme [28], our design provides  34%  reduction in projection computations, which translates to  17%  additional energy savings. 1: A  360 °  video processing pipeline on a battery-backed stereoscopic HMD with an Inertial Measurement Unit (IMU) and an SoC equipped with a GPU [28], [39].",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "II. B ACKGROUND AND  M OTIVATION \nBefore getting into the details of the existing issues and possible solutions, we ﬁrst outline the computation pipeline of the state-of-the-art  360 ° VR streaming (Fig. 1: A  360 °  video processing pipeline on a battery-backed stereoscopic HMD with an Inertial Measurement Unit (IMU) and an SoC equipped with a GPU [28], [39].",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "B ACKGROUND AND  M OTIVATION \nBefore getting into the details of the existing issues and possible solutions, we ﬁrst outline the computation pipeline of the state-of-the-art  360 ° VR streaming (Fig. 1). Further, we describe the existing energy inefﬁciencies in processing  360 ° VR systems, to motivate our design for mitigating the com- putational inefﬁciencies by avoiding redundant computations.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "360 °  Video Streaming Pipeline \nThe key  difference  between a  360 ° VR video compared to a conventional 2D video is that the former provides content-rich immersive user experience. Further, we describe the existing energy inefﬁciencies in processing  360 ° VR systems, to motivate our design for mitigating the com- putational inefﬁciencies by avoiding redundant computations. A.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Wearing a head mounted display (HMD), a user navigates in a virtual world by  looking around , or  moving around  [55], to interact with the virtual world. 360 °  Video Streaming Pipeline \nThe key  difference  between a  360 ° VR video compared to a conventional 2D video is that the former provides content-rich immersive user experience. As shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "As shown in Fig. More speciﬁcally, the  360 ° video processing pipeline can be summarized as follows: Video Decoder:  The HMD receives encoded  360 ° video bitstream from the network (YouTube [61], Facebook-360 [7], etc). 1, a typical VR HMD [39] has two major components: (i) an SoC with a video decoder, a GPU for processing projection computation, and a display controller, and (ii) a video buffer in DRAM for storing the decoded  360 ° frames and projected frames for both the left and right eyes.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "The next step is to decode the original frame from the bitstream, and today, this is mostly done using a hardware- based h264/MPEG decoder for more energy efﬁciency. Similar to 2D videos, the  360 ° video bitstreams are encoded in H.264/MPEG formats [19] for network efﬁciency. More speciﬁcally, the  360 ° video processing pipeline can be summarized as follows: Video Decoder:  The HMD receives encoded  360 ° video bitstream from the network (YouTube [61], Facebook-360 [7], etc).",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "The next step is to decode the original frame from the bitstream, and today, this is mostly done using a hardware- based h264/MPEG decoder for more energy efﬁciency. After decoding, the  360 ° output frames are then buffered in the video buffer, waiting to be rendered. Projection:  Note that, the output frames from the decoder are still in the  spherical coordinate system .",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "This is because, for encoding purpose, the original  360 ° videos are projected into the 2D plane (usually represented in 2D format such as Equirectangular [52], Cubemap [41], etc.). Projection:  Note that, the output frames from the decoder are still in the  spherical coordinate system . Therefore, unlike the 2D video processing where the display can directly read \n242 \n59% \n29% \n6%  6% \nCompute \nMemory Decode Display \n(a) Power breakdown.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "(c) Head movement and pupillary distance as inputs. Therefore, unlike the 2D video processing where the display can directly read \n242 \n59% \n29% \n6%  6% \nCompute \nMemory Decode Display \n(a) Power breakdown. (b) Overview of  360 °  video projection.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "2: Overview of  360 °  video projection. Fig. (c) Head movement and pupillary distance as inputs.",
      "type": "sliding_window_shuffled",
      "tokens": 28,
      "augmented": true
    },
    {
      "text": "(a) Power breakdown consuming 3.4 Watts; (b) Projection pipeline taking head orientation and pupillary distance to compute projection matrices for both the eyes, which map  360 °  coordinates to 2D coordinates for generating stereoscopic frames. (c) Reusing projection matrices by exploiting relation between both eyes and fusing it with head orientation. 2: Overview of  360 °  video projection.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "(c) Reusing projection matrices by exploiting relation between both eyes and fusing it with head orientation. the video buffer to present the  decoded  frames, the  360 ° video frames require “additional rendering effort” to get displayed. More speciﬁcally, the rendering process is a projection from the  360 ° frame pixels’ 3D coordinates to the 2D frame pixels’ 2D coordinates on HMDs.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "More speciﬁcally, the rendering process is a projection from the  360 ° frame pixels’ 3D coordinates to the 2D frame pixels’ 2D coordinates on HMDs. The head orientation is sensed by an inertial measurement unit (IMU) on the HMD as a triple [ Y aw ,  Pitch ,  Roll ] for projection computation 2 . The projection process considers two user-side aspects –  head orientation and pupillary dis- tance 1   – to render stereoscopic views or Field of View (FoV) frames for both eyes, towards the head direction.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "For each frame, this computation is processed twice – one for left eye and the other for right eye – to reinforce users’ sense of depth. Display:  After the projection, the two generated FoV frames are stored in 2D format in the video buffer. The head orientation is sensed by an inertial measurement unit (IMU) on the HMD as a triple [ Y aw ,  Pitch ,  Roll ] for projection computation 2 .",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "The display controller just needs to read them from DRAM to the screen. To summarize, compared to 2D video processing,  360 ° video processing incurs additional projection computation. Display:  After the projection, the two generated FoV frames are stored in 2D format in the video buffer.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "To summarize, compared to 2D video processing,  360 ° video processing incurs additional projection computation. However, since the whole computation/rendering process takes place on a battery-backed device [39], one needs to consider the “energy efﬁciency” of this computation, i.e., even though we can meet the performance requirements of such video, energy efﬁciency needs to be improved. From our measurements collected from a smartphone [53] running a  360 ° VR application [11], even with extra computa- tion, the overall processing for rendering one  360 ° frame can be completed within 22  ms  on average (translating to 45 fps).",
      "type": "sliding_window_shuffled",
      "tokens": 148,
      "augmented": true
    },
    {
      "text": "B. However, since the whole computation/rendering process takes place on a battery-backed device [39], one needs to consider the “energy efﬁciency” of this computation, i.e., even though we can meet the performance requirements of such video, energy efﬁciency needs to be improved. Motivation \nTo understand the energy proﬁle in the current VR devices, we characterize the energy consumption of  360 ° video pro- cessing on a prototype [36] (conﬁgured similar to a com- mercial VR device [39], discussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Motivation \nTo understand the energy proﬁle in the current VR devices, we characterize the energy consumption of  360 ° video pro- cessing on a prototype [36] (conﬁgured similar to a com- mercial VR device [39], discussed in Sec. V) in Fig. 2a.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "2a. Overall,  360 ° VR video processing consumes 3.4  Watts , which is  2 . 27 ×  the power compared to its planar counter- parts (1.5  Watts ).",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "2 Yaw: vertical; Pitch: side-to-side; Roll: front-to-back. 27 ×  the power compared to its planar counter- parts (1.5  Watts ). We also observe that, unlike conventional planar video processing where  memory  is the main bottleneck ( 43% ), in  360 ° VR video processing,  compute  dominates the \n1 Pupillary distance is the distance, typically measured in millimeters, between the centers of the pupils of the eyes.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "2 Yaw: vertical; Pitch: side-to-side; Roll: front-to-back. Previous studies have observed that the computation in  360 ° video processing is, mainly, the  projection transformation  [28]. power consumption, constituting  59% .",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "Previous studies have observed that the computation in  360 ° video processing is, mainly, the  projection transformation  [28]. These observations motivate us to explore the potential opportunities for reducing the power/energy consumption in the  projection  stage. We illustrate the  360 ° video projection/projection transfor- mation 3   computation in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "2b. We illustrate the  360 ° video projection/projection transfor- mation 3   computation in Fig. At a high level, we need two major inputs to generate the ﬁnal projected frames on the display.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "Since there is a small offset between the two eyes, the projection computation needs to cap- ture the pupillary distance to generate a separate view for each eye. The ﬁrst input is from the user side, including the head orientation and pupillary distance. At a high level, we need two major inputs to generate the ﬁnal projected frames on the display.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Note that, this projection process is quite compute-intensive. Therefore, the output of the projection computation is  two projection matrices, each indicating the mapping between each coordinate in  360 ° video frame and a 2D coordinate on screen for the left and right eye. Since there is a small offset between the two eyes, the projection computation needs to cap- ture the pupillary distance to generate a separate view for each eye.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "Our characterization indicates that, on average, around 2.3 GFLOPS is required for this projection transformation (details are discussed in Sec. Furthermore, in a typical VR headset, the above computation needs to repeat millions of times, for processing just one  360 ° frame. Note that, this projection process is quite compute-intensive.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "The second input is the decoded  360 ° frame that contains the pixel values. III). Our characterization indicates that, on average, around 2.3 GFLOPS is required for this projection transformation (details are discussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "The second input is the decoded  360 ° frame that contains the pixel values. It is to be noted that, when a user’s head orientation is changed, the Projection Computation stage needs to  recompute the transformations to reﬂect the user’s head movement. The decoded  360 ° frame is fed to the Projection Mapping stage, which uses the projection matrix, locates the coordinates in the decoded  360 ° frame, and moves their pixel values to the transformed 2D coordinates in the FoV frames.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Hence, the projection transformation computation has to be executed at every frame to reﬂect user movements in real-time, and the whole process is very compute intensive (36 times per second [3]) and power hungry. The VR headset allows users to freely move their heads and eyes at any time to any degree. It is to be noted that, when a user’s head orientation is changed, the Projection Computation stage needs to  recompute the transformations to reﬂect the user’s head movement.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "However, it is too conservative to execute the projection transformation in a short period of time even for the same set of inputs. Intuitively, if the inputs of the transformation computation do not change, the output of the transformation will also be same. Hence, the projection transformation computation has to be executed at every frame to reﬂect user movements in real-time, and the whole process is very compute intensive (36 times per second [3]) and power hungry.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "In fact, we observe the \n3 We use “projection transformation” and “ 360 ° video projection” inter- changeably. 243 \nFig. Intuitively, if the inputs of the transformation computation do not change, the output of the transformation will also be same.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "243 \nFig. The projection transformation ﬁrst calculates the transformation matrix ( T  ), and then uses the transformation matrix to map each of the pixel coordinates to generate the projection matrices ( P ) for the FoV frames. 3: Detailed illustration of projection transformation.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "The projection transformation ﬁrst calculates the transformation matrix ( T  ), and then uses the transformation matrix to map each of the pixel coordinates to generate the projection matrices ( P ) for the FoV frames. following two properties from a published  360 ° VR dataset [3], as shown in Fig. Then projection mapping stage uses this coordinate mapping ( P ) to move pixels from the original 360 frame  F 360  to the 2D frame  F .",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "2c: Head Orientation Proximity:  In a short period of time, the user’s head orientation is usually stable in a small space range (3D) or even still. following two properties from a published  360 ° VR dataset [3], as shown in Fig. In fact, from this dataset we have found that, the head orientation for users does not often change within around  150 ms  period of time.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "Since two identical head orientations lead to the same projection matrices, one opportunity to reduce computation is to  memoize  a set of head orientations as well as their corresponding compute results. Furthermore, even in cases where head orientation changes, the change is usually within a small range – in a few consecutive frames. In fact, from this dataset we have found that, the head orientation for users does not often change within around  150 ms  period of time.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Vision Proximity:  It is to be emphasized that, even when the head orientation input for two computations are the same, the two eye coordinates can be different. Because of this, in current designs, the projection transformation is invoked  twice as it needs to generate two different transformation matrices for the left eye and the right eye. Since two identical head orientations lead to the same projection matrices, one opportunity to reduce computation is to  memoize  a set of head orientations as well as their corresponding compute results.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "On the other hand, the distance between the two eyes is small and is constant for a particular user 4 . Because of this, in current designs, the projection transformation is invoked  twice as it needs to generate two different transformation matrices for the left eye and the right eye. The two transformation matrices are very “similar” as they inherit a relationship between them as a function of the small pupillary distance.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Motivated by these observations, in the following sections, we explore and address two critical questions:  Can we identify the proximity in the projection computation? The two transformation matrices are very “similar” as they inherit a relationship between them as a function of the small pupillary distance. , and  Can we leverage this proximity to safely skip some computations to save energy?",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": ", and  Can we leverage this proximity to safely skip some computations to save energy? 360 ° V IDEO  P ROJECTION \nTo leverage the opportunities in the  360 ° video projection, we need to understand the execution of the entire projection processing in a  360 ° VR system. III.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "360 ° V IDEO  P ROJECTION \nTo leverage the opportunities in the  360 ° video projection, we need to understand the execution of the entire projection processing in a  360 ° VR system. We illustrate the details of  360 ° video projection in Fig. 3 as three stages (detailed background of this projection transformation can be found in [21], [27]).",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "The ﬁrst stage,  Transformation  (denoted  a in Fig. 3, is to determine a transformation matrix by com- bining ﬁve different transforms. 3 as three stages (detailed background of this projection transformation can be found in [21], [27]).",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "The second stage,  Projection Computation  (denoted  b  in Fig. 3), uses the transformation matrix and the 2D FoV coordinates for both eyes to obtain \n4 We used an averaged pupillary distance in our evaluations [27], [37]. 3, is to determine a transformation matrix by com- bining ﬁve different transforms.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "TABLE I: Projection Computation description. 3), uses the transformation matrix and the 2D FoV coordinates for both eyes to obtain \n4 We used an averaged pupillary distance in our evaluations [27], [37]. Label Description HO dependent?",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Known-time Eye-dependent? T 1 Rigid body No Compile-time Left = Right T 2 An eye’s view Yes Runtime Left = Right T 3 Eye adjusting No Compile-time Left  ̸ =  Right T 4 Perspective No Design-time Left = Right T 5 Viewport No Design-time Left = Right \ntheir corresponding  360 ° video frame coordinates. Label Description HO dependent?",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "3) ,which can be projected to both eyes on the HMD. T 1 Rigid body No Compile-time Left = Right T 2 An eye’s view Yes Runtime Left = Right T 3 Eye adjusting No Compile-time Left  ̸ =  Right T 4 Perspective No Design-time Left = Right T 5 Viewport No Design-time Left = Right \ntheir corresponding  360 ° video frame coordinates. Finally, the third stage, i.e.,  Projection Mapping , uses the mapping results from the second stage and the  360 ° video frame to deduce the pixel values for 2D FoV frames (shown in  c  in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 147,
      "augmented": true
    },
    {
      "text": "3) ,which can be projected to both eyes on the HMD. 3  a  for compu- tation of the  Transformation Matrix , is used for projecting the 360 ° frame pixels onto the  2 D  FoV plane in the subsequent stages. The  Transformation  stage, shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "•  T 1  serves as a  rigid body transformation  matrix which ap- plies 3D rotation ( Y aw, Pitch, Roll ) and translation so that, the objects do not get distorted. 3  a  for compu- tation of the  Transformation Matrix , is used for projecting the 360 ° frame pixels onto the  2 D  FoV plane in the subsequent stages. This matrix is calculated by applying ﬁve different transforms –  T 1 ,  T 2 ,  T 3 ,  T 4 , and  T 5  – in a serial fashion.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "•  T 1  serves as a  rigid body transformation  matrix which ap- plies 3D rotation ( Y aw, Pitch, Roll ) and translation so that, the objects do not get distorted. •  T 2  gives us  eyes’ view ; i.e., this changes the virtual world’s coordinate frame to match the frame of the eye. Since this transformation does not depend on any of the sensor inputs, it can be pre- calculated at compile-time.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "This requires knowledge of the head orientation or the direction of gaze, which can be read at runtime from the IMU sensors embedded in the VR headset. •  T 2  gives us  eyes’ view ; i.e., this changes the virtual world’s coordinate frame to match the frame of the eye. •  T 3  transforms the  360 ° coordinates from a  monocular view to a  stereoscopic view .",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "•  T 3  transforms the  360 ° coordinates from a  monocular view to a  stereoscopic view . •  T 4 , also known as the  perspective transformation  ma- trix, maps all  360 ° coordinates onto 2D coordinates. Since each eye sees the same object differently , this transformation matrix is different for each eye to give the user a more realistic experience.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "This transformation depends only on the HMD characteristics, including, but not limited to, the display size and resolution, and hence, is known apriori (at design-time). •  T 5 , the last transformation to be applied, performs a view- port transformation 5 , bringing the projected points to the coordinates used to index the pixels on the HMD. •  T 4 , also known as the  perspective transformation  ma- trix, maps all  360 ° coordinates onto 2D coordinates.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "As in the case of  T 4 , this transformation is also HMD design- dependent and is known at design-time. •  T 5 , the last transformation to be applied, performs a view- port transformation 5 , bringing the projected points to the coordinates used to index the pixels on the HMD. Note that, the  product  of these ﬁve transforms gives us the ﬁnal transformation matrices ( T L  and  T R ), which together convert the  3 D  coordinates of the  360 ° frame to the  2 D  coordinates suitable for HMD.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Note that, the  product  of these ﬁve transforms gives us the ﬁnal transformation matrices ( T L  and  T R ), which together convert the  3 D  coordinates of the  360 ° frame to the  2 D  coordinates suitable for HMD. T L = T 5  ×  T 4  ×  T   L 3   ×  T 2   ×  T 1 T R = T 5  ×  T 4  ×  T   R 3   ×  T 2   ×  T 1 (1) \nThese ﬁve transforms are of dimension of  4 × 4  ( 3  dimensions for rotation;  1  for translation), thus producing  4  ×  4  T L  and T R  matrices [27]. Mathematically, the transformation matrix for each eye is shown in Equation 1.",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "Note that, given an arbitrary FoV frame, these transformation matrices remain the same for all the pixel \n5 A Viewport Transformation is the process of transforming a 2D coordinate objects to device coordinates [27]. T L = T 5  ×  T 4  ×  T   L 3   ×  T 2   ×  T 1 T R = T 5  ×  T 4  ×  T   R 3   ×  T 2   ×  T 1 (1) \nThese ﬁve transforms are of dimension of  4 × 4  ( 3  dimensions for rotation;  1  for translation), thus producing  4  ×  4  T L  and T R  matrices [27]. 244 \ncoordinates in that frame, thus are evaluated only  once  for that frame, and account for only  4 .",
      "type": "sliding_window_shuffled",
      "tokens": 159,
      "augmented": true
    },
    {
      "text": "244 \ncoordinates in that frame, thus are evaluated only  once  for that frame, and account for only  4 . 8 MFLOPS  without any optimization. In the  Projection Computation  stage (refer to  b  in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "At any instance, a user is only concerned about the FoV pixels in the entire  360 ° frame. 3), we use the transformation matrix ( T  ) to generate the mapping ( P ) between the  360 ° frame coordinates and the  2 D  FoV frame coordinates. In the  Projection Computation  stage (refer to  b  in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "At any instance, a user is only concerned about the FoV pixels in the entire  360 ° frame. So, instead of evaluating the mapping for all coordinates in the  360 ° frame, we only generate the mapping for those pixels which are within the user’s view. As the target  2 D  FoV coordinates are already known (VR screen dimensions), these mappings can be performed by multiplying the inverse of the transformation matrix ( T   − 1 ) with the  2 D  FoV coordinates ( V 2 D ), thus generating the corresponding  360 ° pixel coordinates ( P ), as shown in Equation 2.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "As the target  2 D  FoV coordinates are already known (VR screen dimensions), these mappings can be performed by multiplying the inverse of the transformation matrix ( T   − 1 ) with the  2 D  FoV coordinates ( V 2 D ), thus generating the corresponding  360 ° pixel coordinates ( P ), as shown in Equation 2. P i L   =  T  − 1 L × V i 2 D ; ∀ i  ≤ num pixels \nP i R   =  T  − 1 R × V i 2 D ; ∀ i  ≤ num pixels (2) \nHere,  V 2 D  = [ q 0 , q 1 , q 2 , q 3 ] ⊤ represents the quaternion equiva- lent of the  2 D  FoV coordinates used for matrix multiplication with the inverse transformation matrix ( T   − 1 ). Note that, this operation, which is a matrix multiplication on each FoV pixel coordinate, can be quite compute intensive.",
      "type": "sliding_window_shuffled",
      "tokens": 232,
      "augmented": true
    },
    {
      "text": "Note that, this operation, which is a matrix multiplication on each FoV pixel coordinate, can be quite compute intensive. This amounts to about  2.3 GFLOPS , which represents a substantial amount of computation, given the limited compute capabilities and power in such edge devices. In fact, the number of pixels in the FoV is usually around 1 million, and the videos stream at a rate of 30 fps for an immersive experience.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "This amounts to about  2.3 GFLOPS , which represents a substantial amount of computation, given the limited compute capabilities and power in such edge devices. Note that, even though theoretically one represents the  360 ° frame coordinates as quaternions, in practice, they are typically represented using speciﬁc projection formats, e.g., equirectangular, cube map, equi-angular cubemap, pyramid format, etc. The details of these formats are in the purview of cartography and computer graphics domain, and hence we do not evaluate all of the aforementioned formats.",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "In our evaluations and experiments, we used the equirectangular format [52], which is one of the most popular projection formats. The  Projection Mapping  stage (  c  in Fig. The details of these formats are in the purview of cartography and computer graphics domain, and hence we do not evaluate all of the aforementioned formats.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "This stage mostly comprises of memory operations, and thus is not a compute bottleneck. The  Projection Mapping  stage (  c  in Fig. 3) takes the projection matrices for both the eyes ( P L ,  P R ) of Equation 2 as well as the pixel values of the  360 ° video frame ( F 360 ), to obtain the  2 D  FoV frames ( F L  and  F R ), which can be further displayed on the HMD.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Our discussion in this section is summarized in Tab. This stage mostly comprises of memory operations, and thus is not a compute bottleneck. I.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "I. This also determines the “order of computation”, which is ﬁrst  T  , then  P , and ﬁnally  F . We have two important takeaways: •  Computation Dependence Chain: We note that there exists a data dependence from the  360 ° frame to generate the ﬁnal FoV frame, where  F  depends on  P , which in turn depends on  T  .",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "However,  T 2  can change at runtime, and if any element in  T 2 is changed, the transformation matrix needs re-computation \nFig. This also determines the “order of computation”, which is ﬁrst  T  , then  P , and ﬁnally  F . •  Input (in-)Variability:  It should be clear from the discus- sion above that  T 1 ,  T 3  T 4 , and  T 5  can be determined apriori.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "This example illustrates 3 consecutive frames processing, each of which consists of two projection matrices ( P L and  P R ) for both eyes. However,  T 2  can change at runtime, and if any element in  T 2 is changed, the transformation matrix needs re-computation \nFig. 4: InterFrame-IntraEye ( EA ) and IntraFrame-InterEye ( AE ) reuse opportunities.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "This example illustrates 3 consecutive frames processing, each of which consists of two projection matrices ( P L and  P R ) for both eyes. Moreover, the reuse between both eyes is further optimized by  AE . The  3 rd frame shares the same head orientation with the  1 st, thus can be optimized by  EA .",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "Moreover, the reuse between both eyes is further optimized by  AE . along with  P  and  F , due to their dependencies. However, if  T 2  does not change across frames,  P  is identical to the previous frame.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "R EDUCING  P ROJECTION  C OMPUTATION \nAs discussed in Sec. However, if  T 2  does not change across frames,  P  is identical to the previous frame. IV.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "II, computations dominate the energy consumption in  360 ° VR video processing. R EDUCING  P ROJECTION  C OMPUTATION \nAs discussed in Sec. Further, we also observed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "Unlike prior works targeting at optimizing the efﬁciency of  each computation  [28], [57], we primarily focus on reducing  the amount of computation to be performed, by exploring the intrinsic “compute reuse opportunities” in  360 ° VR video processing. Further, we also observed in Sec. III that, the main reason behind this is that the compute is executed repeatedly both within a single frame (due to offset between the eyes) and across frames (due to changes in head orientation at runtime).",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "Unlike prior works targeting at optimizing the efﬁciency of  each computation  [28], [57], we primarily focus on reducing  the amount of computation to be performed, by exploring the intrinsic “compute reuse opportunities” in  360 ° VR video processing. Opportunities \nExploring and exploiting computation output reuse oppor- tunities is non-trivial in this context. A.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "As discussed in Sec. Opportunities \nExploring and exploiting computation output reuse oppor- tunities is non-trivial in this context. First of all, the projec- tion transformation is multi-staged and is a composition of multiple mathematical operations, e.g., transformation matrix, projection computation, mapping, etc.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "Moreover, in many cases, computations are also sensor input-dependent such as the IMU data for determining the head orientation, which is updated across frames , at runtime. As discussed in Sec. III, the projection computation varies across  eyes  even for the same head orientation.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "Thus, to explore computation reuse op- portunities, we start by distinguishing between 4 complemen- tary opportunities –  InterFrame-IntraEye (EA) ,  IntraFrame- InterEye (AE) ,  IntraFrame-IntraEye (AA) , and  InterFrame- InterEye (EE) , using a represent example shown in Fig. Moreover, in many cases, computations are also sensor input-dependent such as the IMU data for determining the head orientation, which is updated across frames , at runtime. 4.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "•  In  EA , as discussed in Sec. 4. III, the transformation matrix ( T  ) is determined by the head orientation, which is sampled from the built-in IMU sensors.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "III, the transformation matrix ( T  ) is determined by the head orientation, which is sampled from the built-in IMU sensors. 4. We observe that, if the head orientation does not change across two frames, the ﬁve transforms and the  360 ° coordinate inputs remain the same, thereby providing ample opportunities for directly  reusing \n245 \nthe compute results from the previous frame ( P 1 ), as shown in  a  in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "For such scenarios, due to the prevailing relationship between the left and right eye transformation matrices ( T L  and  T R ), we can further avail the spatial compute reuse opportunity shown in b in Fig. •  AE  comes to play when there is a change in head orientation in consecutive frames, and we cannot enjoy the oppor- tunities in  EA . 4.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "•  In  AA , the input and output mapping are unique, that is, no two input coordinates in  360 ° frame map to the same coordinates in the 2D FoV frame, thereby eliminating any compute reuse scope. 4, by reconstructing the computation needed for one eye ( P R ) from the other ( P L ). For such scenarios, due to the prevailing relationship between the left and right eye transformation matrices ( T L  and  T R ), we can further avail the spatial compute reuse opportunity shown in b in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "•  In  AA , the input and output mapping are unique, that is, no two input coordinates in  360 ° frame map to the same coordinates in the 2D FoV frame, thereby eliminating any compute reuse scope. Although, in principle, for computing the transformation for consecutive pixels, one can leverage data value similarity to reduce the computation, in this work we are not focusing on leveraging any such opportunity. •  EE  offers little chance of reuse, and can only be leveraged in rare occasions, where we have oracular knowledge of head movements.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "•  EE  offers little chance of reuse, and can only be leveraged in rare occasions, where we have oracular knowledge of head movements. Furthermore, in such cases of head movement, there is likely to be some reuse from inter-eye reusability within a frame, rather than inter-frame reusability. B.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "B. Why have  EA / AE  Opportunities been previously Ignored? Based on the above discussion, in this work, we focus on EA  and  AE  opportunities.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "Based on the above discussion, in this work, we focus on EA  and  AE  opportunities. We are unaware of any existing implementation or research work that focus on compute reuse by leveraging across-frames and across-eyes memoization. In fact, the existing state-of-the-art software stack, such as GoogleVR-SDK [11], simply uses the IMU sensor inputs to calculate the updated transformation matrices, then passes them to the OpenGL [42] engine to process the projection computation, as shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "2b. We would also like to point that capturing these opportunities is not trivial and cannot be efﬁciently done by just optimizing the existing application layer and software stack. In fact, the existing state-of-the-art software stack, such as GoogleVR-SDK [11], simply uses the IMU sensor inputs to calculate the updated transformation matrices, then passes them to the OpenGL [42] engine to process the projection computation, as shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "We would also like to point that capturing these opportunities is not trivial and cannot be efﬁciently done by just optimizing the existing application layer and software stack. We describe the underlying issues to address and emphasize the non-trivialities: •  To ease development efforts, state-of-the-art VR applica- tions reuse APIs provided by OpenGL [24], [42], and whenever a new frame is decoded, they always invoke the glDrawFrame  twice for both eyes (see line number  257  in googlevr-video360 application [12]). They do not seem to leverage the fact that the transformation matrices are unique for each head orientation and memoizing them will save re-calculating the transformation matrix ( T  ) as well as the projection matrix ( P ).",
      "type": "sliding_window_shuffled",
      "tokens": 177,
      "augmented": true
    },
    {
      "text": "They do not seem to leverage the fact that the transformation matrices are unique for each head orientation and memoizing them will save re-calculating the transformation matrix ( T  ) as well as the projection matrix ( P ). •  Even if they do realize such opportunities, the projection matrix ( P ) is very big ( ≈ 8 MB , details in Sec. IV-C), and one edge VR headset cannot afford to memoize them for all  possible head orientations.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "To address this, we need to study the impact of head orientation on the computation and whether we can establish a relationship between the computation executed for both the eyes to get rid of any existing redundancies. All these are possible avenues for optimization and demand a detailed study of the computa- \ntion pipeline, the workloads, user behavior, etc., to ﬁnd a way to further improve the state-of-the-art. IV-C), and one edge VR headset cannot afford to memoize them for all  possible head orientations.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "•  Furthermore, a software-only approach may not give us the desired solution as some of these additional execution cycles, control and data path manipulations may need ar- chitectural support, especially to reduce memory and power overheads on edge VRs. All these are possible avenues for optimization and demand a detailed study of the computa- \ntion pipeline, the workloads, user behavior, etc., to ﬁnd a way to further improve the state-of-the-art. Therefore, we believe that, achiev- ing beneﬁts by exploiting the  EA  and  AE  opportunities needs an extensive study and a careful design, especially from an architectural perspective, to maximize the beneﬁts.",
      "type": "sliding_window_shuffled",
      "tokens": 152,
      "augmented": true
    },
    {
      "text": "Therefore, we believe that, achiev- ing beneﬁts by exploiting the  EA  and  AE  opportunities needs an extensive study and a careful design, especially from an architectural perspective, to maximize the beneﬁts. As shown in Fig. Driven by the above discussion and the potential optimiza- tion opportunities presented by  EA  and  AE , we propose  D´ej`a View , an energy-efﬁcient design for  360 ° video streaming on VRs.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "4,  D´ej`a View  leverages compute lo- cality to bypass computations and provides signiﬁcant energy savings, with the following two-step optimization strategy: \na  For each frame, if the head orientation remains the same, we take advantage of the  EA  opportunity. b  If exploiting the  EA  opportunity is not possible, we take advantage of the  AE  opportunity, by performing computation for only one eye (and construct the result for the other eye). As shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "b  If exploiting the  EA  opportunity is not possible, we take advantage of the  AE  opportunity, by performing computation for only one eye (and construct the result for the other eye). Intuitively, as mentioned earlier in Sec. C. InterFrame-IntraEye (EA) Computation Optimization \nWe plan to leverage the  EA  opportunity when the user’s head orientation does not change.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "Intuitively, as mentioned earlier in Sec. III (Fig. 3),  a  Transformation and  b  Projec- tion Computation remain unchanged.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "This can help us identify and isolate proper memoization candidates for carefully tweaking our design decisions to maximize the reuse beneﬁts. 3),  a  Transformation and  b  Projec- tion Computation remain unchanged. To understand all the contributing factors which affect computations, we further investigate the important inputs of the VR headset.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "Further, we also study the overheads introduced by our design modiﬁcations to perform a fair comparison with the state-of-the-art. This can help us identify and isolate proper memoization candidates for carefully tweaking our design decisions to maximize the reuse beneﬁts. What (features) to Memoize?",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "To better under- stand which of these are the best candidates ( features , using machine learning parlance) for memoization and whether they are sufﬁcient or not, we next discuss input parameters and their impact on the computation: •  Head orientation:  Any changes in this affect the matrix T 2  as discussed in Tab. What (features) to Memoize? As discussed earlier, at any moment during VR video processing, the execution pipeline is not only impacted by the head orientation, but also by other features such as video frame rate, video types/semantics information, pixel values, user interactions.",
      "type": "sliding_window_shuffled",
      "tokens": 126,
      "augmented": true
    },
    {
      "text": "I, thus changing the transformation matrix  T  and eventually leading to  re-computation  of the most compute-intensive projection matrix  P . To better under- stand which of these are the best candidates ( features , using machine learning parlance) for memoization and whether they are sufﬁcient or not, we next discuss input parameters and their impact on the computation: •  Head orientation:  Any changes in this affect the matrix T 2  as discussed in Tab. Thus, it is a critical feature in projection computation executions.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "3) matter only during data transfer (from the input 360 ° frame to the framebuffer) in the projection mapping stage, after the coordinate mappings ( P  in Fig. •  Pixel values:  The pixel contents/values (denoted as  F  in Fig. Thus, it is a critical feature in projection computation executions.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "3) matter only during data transfer (from the input 360 ° frame to the framebuffer) in the projection mapping stage, after the coordinate mappings ( P  in Fig. 3) are gener- ated. Potentially, content-based optimizations (e.g., content cache [63]) can beneﬁt the data transfer; however, they are not attractive candidates to leverage compute reuse, which is the major power-hungry stage (as shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "In \n246 \nTABLE II: Video workloads. 2a). Potentially, content-based optimizations (e.g., content cache [63]) can beneﬁt the data transfer; however, they are not attractive candidates to leverage compute reuse, which is the major power-hungry stage (as shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Video Type (Cam movement/focus of attention direction) \nFrame Rate (fps) \n#Frames Bit Rate (kbps) \nV1 Rhinos [4] Stationary cam, no focus direction 30 3280 13462 \nV2 Timelapse [56] \nStationary cam, fast-moving objects, no focus direction \n30 2730 15581 \nV3 Rollercoaster [35] \nFast-moving cam hooked in front of a rollercoaster, uni-direction focus \n29.97 6194 16075 \nV4 Paris [51] \nStationary cam, smooth scene cuts, no focus direction \n59.94 14629 14268 \nV5 Elephants [5] Stationary cam, uni-direction focus 30 5510 16522 \nthis work, we are focusing on reusing computation results rather than reducing the content maintenance/transfer, and hence do not consider that optimization. In \n246 \nTABLE II: Video workloads. No.",
      "type": "sliding_window_shuffled",
      "tokens": 210,
      "augmented": true
    },
    {
      "text": "•  Video meta-information:  This contains the additional infor- mation, such as frame rates, video semantics/types, etc., about the video inputs. Video Type (Cam movement/focus of attention direction) \nFrame Rate (fps) \n#Frames Bit Rate (kbps) \nV1 Rhinos [4] Stationary cam, no focus direction 30 3280 13462 \nV2 Timelapse [56] \nStationary cam, fast-moving objects, no focus direction \n30 2730 15581 \nV3 Rollercoaster [35] \nFast-moving cam hooked in front of a rollercoaster, uni-direction focus \n29.97 6194 16075 \nV4 Paris [51] \nStationary cam, smooth scene cuts, no focus direction \n59.94 14629 14268 \nV5 Elephants [5] Stationary cam, uni-direction focus 30 5510 16522 \nthis work, we are focusing on reusing computation results rather than reducing the content maintenance/transfer, and hence do not consider that optimization. This feature can only be used as an add-on, along with other inputs to further improve compute reuse scope.",
      "type": "sliding_window_shuffled",
      "tokens": 259,
      "augmented": true
    },
    {
      "text": "This feature can only be used as an add-on, along with other inputs to further improve compute reuse scope. For example, if the  360 ° video frame rate increases from the typical 30 fps to 60 fps, then one can potentially leverage this enhanced compute frequency in conjunction with the head orientation, to further expand the compute reuse window. Note however that, this meta- information is not on the data-dependence chain, and we do not consider it for memoization.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "To summarize, among the above discussed features, we identify  head orientation  as the only suitable memoization candidate for boosting the compute reuse scope. Thus, we memoize both head orientation and its corresponding projec- tion matrix (i.e., projection computation results) in a memory buffer, namely,  P buff , and use the head orientation to index the address/pointer of that  P buff  stored in DRAM. Note however that, this meta- information is not on the data-dependence chain, and we do not consider it for memoization.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "How Much to Memoize? Thus, we memoize both head orientation and its corresponding projec- tion matrix (i.e., projection computation results) in a memory buffer, namely,  P buff , and use the head orientation to index the address/pointer of that  P buff  stored in DRAM. The occupied DRAM size is mainly determined by  P buff .",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "The occupied DRAM size is mainly determined by  P buff . Since this puts a high demand on memory, one edge VR headset cannot afford to memoize for all possible head orientations. In fact, with a VR screen size of  1 ,  000 × 1 ,  000 , one  P buff  occupies  ≈ 8 MB  in DRAM.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Since this puts a high demand on memory, one edge VR headset cannot afford to memoize for all possible head orientations. Thus, we want to limit the number of  P buff  that we need to store. To address this, we need to carefully decide how much his- tory is to be memoized for leveraging computation reuse.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "We performed a study on the VR video dataset [3] to investigate the head orientation traces of 20 users watching 5 widely- variant  360 ° VR videos (summarized in Tab. To address this, we need to carefully decide how much his- tory is to be memoized for leveraging computation reuse. II).",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "II). Typically, the resolution of the IMU traces can be as high as 20 bits per ﬁeld [3], [28]. From the dataset, we report the average reuse distance, i.e., the average number of preceding frames with same head orientation to be memoized, and show it in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "It can be concluded from these results that, memoizing the last  two  frames is sufﬁcient for most of the cases. 5a. From the dataset, we report the average reuse distance, i.e., the average number of preceding frames with same head orientation to be memoized, and show it in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "It can be concluded from these results that, memoizing the last  two  frames is sufﬁcient for most of the cases. Memoizing more frames may not bring much additional beneﬁts because of the high sensitivity of the IMU sensors. Storing only  two \nhead orientations (in registers) and their associated  P buff  in the DRAM occupies only  ≈ 16 MB  memory space.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Storing only  two \nhead orientations (in registers) and their associated  P buff  in the DRAM occupies only  ≈ 16 MB  memory space. 5b), limiting the memoization opportunities to those instances. Further, we also observe that, the duration for which the head orientation does not change for three consecutive frames sums up to only  ≈ 28%  of the video runtime on average (refer to Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "5b), limiting the memoization opportunities to those instances. Such low reuse ratio is expected because of the high sensitivity of the IMU sensors. However, a higher reuse ratio can be achieved by relaxing the precision of the IMU output.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "However, a higher reuse ratio can be achieved by relaxing the precision of the IMU output. Furthermore, we study the  V3  (i.e.,  Rollercoaster ) video and examine the trade-offs between (i) quantizing/approximat- ing the head orientation (thus compromising video quality) with more reuse, vs. (ii) maintaining the lossless video quality but with a lower reuse ratio, in Fig. 5c, to provide an intuitive comparison in different scenarios.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Here, we quantify the video quality with the popular Peak Signal-to-Noise Ratio (PSNR, normalized to the ground-truth; the higher, the better) [25], [40]. 5c, to provide an intuitive comparison in different scenarios. From Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "From Fig. 0001 ) to  1  (resolution is  0 . 5c, we can observe that, as the precision decreases from  4  (resolution is  0 .",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "0001 ) to  1  (resolution is  0 . This is because low precision leads to a mis-projection, which fails to reﬂect the current head orientation. 1 ), the reuse ratio increases from  18%  to  92% ; however, the PSNR drops from  85%  to only  19% .",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "In this work, we do not want to distort video quality and thus explore the ground-truth only. This is because low precision leads to a mis-projection, which fails to reﬂect the current head orientation. The Effect of EA:  With this  EA  memoization, once a new head orientation is received, we ﬁrst search it in the two head orientation registers.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "If there is a match, the associated  P buff will return the memory address of the saved  P  so that we can reuse  P  and skip the  entire  coordinate projection computation (refer to  a  in Fig. The Effect of EA:  With this  EA  memoization, once a new head orientation is received, we ﬁrst search it in the two head orientation registers. 4), with only  1%  overhead w.r.t.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "4), with only  1%  overhead w.r.t. baseline. If not, we have to execute the entire computation as in the baseline case.",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": "As a result, by exploiting the  EA  scheme on the second frame, its compute energy consumption can be reduced to only  1%  of that consumed by  Baseline . If not, we have to execute the entire computation as in the baseline case. D. IntraFrame-InterEye (AE) Computation Optimization \nIn  EA , the compute can be bypassed by reusing the pre- computed results, if the head orientation matches with any of the two previously memoized head orientations (stored in reg- isters).",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "De- spite this variation, there may still be matches/recomputations within a frame between two eyes, i.e., IntraFrame-InterEye as shown in  b  in Fig. D. IntraFrame-InterEye (AE) Computation Optimization \nIn  EA , the compute can be bypassed by reusing the pre- computed results, if the head orientation matches with any of the two previously memoized head orientations (stored in reg- isters). However, we also note that these opportunities might be limited owing to the “non-repetitive” user behavior.",
      "type": "sliding_window_shuffled",
      "tokens": 143,
      "augmented": true
    },
    {
      "text": "De- spite this variation, there may still be matches/recomputations within a frame between two eyes, i.e., IntraFrame-InterEye as shown in  b  in Fig. To leverage this opportunity, we next study the coordinate projection results relationship between left-eye and right-eye. 4.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "If there exists a simple mechanism to describe the difference between the two projection matrices of the two eyes ( P L  and  P R ), one can simplify the computation from matrix  multiplications  to matrix  additions . To leverage this opportunity, we next study the coordinate projection results relationship between left-eye and right-eye. Distance Vector Study:  Let us further look into the detailed mapping of a  360 ° frame (in equirectangular format) onto a 2 D  FoV frame in the Projection Mapping stage (refer  c  in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "Distance Vector Study:  Let us further look into the detailed mapping of a  360 ° frame (in equirectangular format) onto a 2 D  FoV frame in the Projection Mapping stage (refer  c  in Fig. The pixel rendered at  [ x 0 l   , y 0 l   ]  on the left VR screen is mapped from position  [( x 360 ) 0 l   ,  ( y 360 ) 0 l   ] on the equirectangular  360 ° frame, as shown in Fig. 3), at a pixel granularity.",
      "type": "sliding_window_shuffled",
      "tokens": 142,
      "augmented": true
    },
    {
      "text": "Simi- larly, the pixel value rendered at  [ x 0 r , y 0 r ]  on the right VR screen \n247 \n0 \n2 \n4 \n6 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \nAvg. The pixel rendered at  [ x 0 l   , y 0 l   ]  on the left VR screen is mapped from position  [( x 360 ) 0 l   ,  ( y 360 ) 0 l   ] on the equirectangular  360 ° frame, as shown in Fig. 6a.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "Simi- larly, the pixel value rendered at  [ x 0 r , y 0 r ]  on the right VR screen \n247 \n0 \n2 \n4 \n6 \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \nAvg. Reuse distance (# frame) \nUsers \nRhinos Timelapse Rollercoaster Paris Elephants \n(a) Reuse distance is just  2  for most of cases \n28% \n0% \n20% \n40% \n60% \nReuse Ratio \nVideo (b) Reuse ratio \n0% 20% 40% 60% 80% 100% \nG 4 3 2 1 \nNorm. to Ground Truth(G) \nPrecision \nReuseRatio PSNR \n(c) Precision vs. reuse ratio tradeoffs \nFig.",
      "type": "sliding_window_shuffled",
      "tokens": 165,
      "augmented": true
    },
    {
      "text": "This indicates two memoization buffers are sufﬁcient. 5: In  EA , (a) shows that, on average, how many frame(s) from the current frame to a previous one with the same head orientation, as denoted as reuse distance. to Ground Truth(G) \nPrecision \nReuseRatio PSNR \n(c) Precision vs. reuse ratio tradeoffs \nFig.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "(b) plots among all the head orientations, how many can be memoized by these two buffers. (c) illustrates the trade-off between the precision level and reuse ratio. This indicates two memoization buffers are sufﬁcient.",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "(c) illustrates the trade-off between the precision level and reuse ratio. -5 5 15 25 \n1 225 449 673 897 1121 1345 1569 1793 2017 2241 \nDistance \nPixel ID \ndistanceY distanceX \n-10 \n-5 \n0 \n5 \n10 \n0 10 20 \nDistance-x \nDistance-y \n(b) (0.00, 1.57, -0.73) \n-20 0 20 40 60 \n1 202 403 604 805 1006 1207 1408 1609 1810 2011 2212 \nDistance \nPixel ID \ndistanceY distanceX \n-10 -5 0 5 10 \n0 20 40 60 \nDistance-x \nDistance-y \n(c) (0.52, 1.05, -0.73) \nFig. (a) Distance vector.",
      "type": "sliding_window_shuffled",
      "tokens": 158,
      "augmented": true
    },
    {
      "text": "-5 5 15 25 \n1 225 449 673 897 1121 1345 1569 1793 2017 2241 \nDistance \nPixel ID \ndistanceY distanceX \n-10 \n-5 \n0 \n5 \n10 \n0 10 20 \nDistance-x \nDistance-y \n(b) (0.00, 1.57, -0.73) \n-20 0 20 40 60 \n1 202 403 604 805 1006 1207 1408 1609 1810 2011 2212 \nDistance \nPixel ID \ndistanceY distanceX \n-10 -5 0 5 10 \n0 20 40 60 \nDistance-x \nDistance-y \n(c) (0.52, 1.05, -0.73) \nFig. 6: In  AE , distance vector (a) patterns with two different head orientations  ( Y aw, Pitch, Roll )  in (b) and (c). is mapped from position  [( x 360 ) 0 r ,  ( y 360 ) 0 r ]  on the  360 ° frame.",
      "type": "sliding_window_shuffled",
      "tokens": 212,
      "augmented": true
    },
    {
      "text": "is mapped from position  [( x 360 ) 0 r ,  ( y 360 ) 0 r ]  on the  360 ° frame. This encourages us to further study whether this distance vector changes with head orientation or not, and also whether it is invariant for any particular frame – if yes, then how? To study the relationship between the coordinate projection re- sults between both eyes, for the  same  two coordinates  [ x i , y i ] on both  2 D  FoV frames, we determine the distance vector ( ⃗ d ) i   between their equirectangular counterparts, represented in Equation 3: \n( ⃗ d ) i   = [( x 360 ) i r   − ( x 360 ) i l ,  ( y 360 ) i r   − ( y 360 ) i l ] ⊤ (3) \nThe knowledge of distance vector   ⃗ d  is critical to explore the AE  opportunity, because if it was known apriori, then we only need to process the entire projection transformation to generate the mapping results ( P L ) for one eye, and then deduce the coordinate projection computation results for the other ( P R ) by simply adding   ⃗ d  with  P L .",
      "type": "sliding_window_shuffled",
      "tokens": 299,
      "augmented": true
    },
    {
      "text": "6b and Fig. To investigate these questions, we examine how the distance vector varies within a same frame, with two different head orientations, shown in Fig. This encourages us to further study whether this distance vector changes with head orientation or not, and also whether it is invariant for any particular frame – if yes, then how?",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "6c. 6b and Fig. On plotting the distance vectors for each row of the FoV frames, we observe a recurring  ellipse  pattern.",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "Second, for different head orientations, their distance vectors plotted in Fig. The intuitive reason behind this  ellipse  pattern is related to the built-in features of the equirectangular format. On plotting the distance vectors for each row of the FoV frames, we observe a recurring  ellipse  pattern.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "6b and Fig. 6c retain the same ellipse behavior but different shapes. Second, for different head orientations, their distance vectors plotted in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "6c retain the same ellipse behavior but different shapes. Furthermore, by exacting the x (or y) coordinate in the distance vector, the above  ellipse  pattern can be represented as  Δ x  =  a  ·  cos ( θ )  and  Δ y  =  b  ·  sin ( θ ) +  c , where  θ  ∈ [0 , π ] , and  a, b, c  vary with head orientation change but remain same \nfor each row in the same frame. Additionally, there are few pixel positions at the frame edges which can only be viewed by one eye (denoted as  exclusive ), which cannot be captured by the above pattern.",
      "type": "sliding_window_shuffled",
      "tokens": 162,
      "augmented": true
    },
    {
      "text": "These pixels amount to only  2 . 7%  of the entire FoV frame. Additionally, there are few pixel positions at the frame edges which can only be viewed by one eye (denoted as  exclusive ), which cannot be captured by the above pattern.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "Due to this inherent nature of compute, the pattern between left eye and right eye can be easily  captured  by proﬁling the distance vector for only the ﬁrst row on the screens:  Δ = \u0002 ⃗x 0 r   − ⃗x 0 l   , ⃗y 0 r   − ⃗y 0 l \u0003 , as shown in line number  2  in Algorithm 1. How to capture the pattern and utilize the pattern? 7%  of the entire FoV frame.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "Note that, as discussed above, 2 . Due to this inherent nature of compute, the pattern between left eye and right eye can be easily  captured  by proﬁling the distance vector for only the ﬁrst row on the screens:  Δ = \u0002 ⃗x 0 r   − ⃗x 0 l   , ⃗y 0 r   − ⃗y 0 l \u0003 , as shown in line number  2  in Algorithm 1. With the learned pattern, the remaining  i th rows for the right- eye ( i  ∈ [1 , n  − 1] , where  n  is the  height  of the VR screen) can be  reconstructed  by using the projection computation results of the left-eye ( [ ⃗x i l , ⃗y i l ] ) and the pattern  Δ , as shown in line  6  in Algorithm 1.",
      "type": "sliding_window_shuffled",
      "tokens": 198,
      "augmented": true
    },
    {
      "text": "7%  exclusive  pixel coordinates for the right-eye cannot be reconstructed by this algorithm. Note that, as discussed above, 2 . Therefore, only for this small number of pixel coordinates, the entire coordinate projection computations need to be processed.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "As a result, by exploiting the  AE  scheme on the ﬁrst frame in Fig. Therefore, only for this small number of pixel coordinates, the entire coordinate projection computations need to be processed. Algorithm 1  Algorithm to capture and utilize the pattern  Δ \nInput:  [ ⃗x 0: n − 1 l , ⃗y 0: n − 1 l ] : left-eye projection (all rows) Input:  [ ⃗x 0 r , ⃗y 0 r ] : right-eye ﬁrst-row’s projection Output:  [ ⃗x 1: n − 1 r , ⃗y 1: n − 1 r ] : right-eye projection \n1:  procedure  C APTURE P ATTERN ( ⃗x 0 r ,  ⃗y 0 r ,  ⃗x 0 l   ,  ⃗y 0 l   ) \n2: [Δ x ,  Δ y ]  :=  [ ⃗x 0 r   − ⃗x 0 l   ,  ⃗y 0 r   − ⃗y 0 l   ] 3: return  Δ = [Δ x ,  Δ y ] \n4:  end procedure \n5:  procedure  U TILIZE P ATTERN ( ⃗x 1: n − 1 l ,  ⃗y 1: n − 1 l ,  Δ ) 6: [ ⃗x 1: n − 1 r ,  ⃗y 1: n − 1 r ]  :=  [ ⃗x 1: n − 1 l +  Δ x  ,  ⃗y 1: n − 1 l +  Δ y ] \n7: return  [ ⃗x 1: n − 1 r ,  ⃗y 1: n − 1 r ] 8:  end procedure \nThe Effect of AE:  with this  AE  optimization, for the right eye, the intensive projective transformation computations can now be short-circuited by light-weight  Add  operations.",
      "type": "sliding_window_shuffled",
      "tokens": 494,
      "augmented": true
    },
    {
      "text": "As a result, by exploiting the  AE  scheme on the ﬁrst frame in Fig. 35%  of that consumed by baseline. 4  b  , its compute energy can be reduced to only  62 .",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "248 \nFig. 7: The proposed  EA  and  AE  design blocks implementation. 35%  of that consumed by baseline.",
      "type": "sliding_window_shuffled",
      "tokens": 27,
      "augmented": true
    },
    {
      "text": "7: The proposed  EA  and  AE  design blocks implementation. E. Design Considerations and Implementation \nWe designed both our schemes as modular and scalable additions to the existing pipeline (refer the  EA  and  AE  blocks shown in Fig. 7).",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "The  EA  block is placed before the original compute engine (OCE, e.g., GPU) to opportunistically bypass the projection computation. However, when  AE  cannot take advantage of memoization due to a head orientation change, then the compute is distributed across the OCE ( 51% ) and  AE block ( 49% ); to be precise, only the entire coordinates on the left screen and the ﬁrst row on the right-screen are processed by the OCE – the remaining rows on the right screen are reconstructed by the less power-hungry  AE  block. 7).",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "Implementation Details:  We abstract the  EA  and  AE  design blocks irrespective of the underlying hardware SoCs, and plot them in Fig. 7. However, when  AE  cannot take advantage of memoization due to a head orientation change, then the compute is distributed across the OCE ( 51% ) and  AE block ( 49% ); to be precise, only the entire coordinates on the left screen and the ﬁrst row on the right-screen are processed by the OCE – the remaining rows on the right screen are reconstructed by the less power-hungry  AE  block.",
      "type": "sliding_window_shuffled",
      "tokens": 128,
      "augmented": true
    },
    {
      "text": "7. In our  EA  design, the last two head orientations are  cached  in local SRAM ( Comp 1  and  Comp 2  in the  EA block) and their corresponding projection computation results are stored in DRAM ( P i − 2 buff   and  P i − 1 buff ). In the  Baseline , the OCE takes the head orientation as its input, processes the entire projection transformation for each pixel coordinate in the FoV region, and then stores the compute results for both eyes in DRAM for the subsequent mapping stage from the  360 ° frame to framebuffer.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "Once the current head orientation is received, the  EA  block ﬁrst compares it with the memoized  Comp 1  and  Comp 2 . In our  EA  design, the last two head orientations are  cached  in local SRAM ( Comp 1  and  Comp 2  in the  EA block) and their corresponding projection computation results are stored in DRAM ( P i − 2 buff   and  P i − 1 buff ). If a match is detected, then the corresponding  P i − 2 buff   or  P i − 2 buff   buffer address pointer is directly returned.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "If no match is found, the OCE is invoked for the entire left eye and only the ﬁrst row for the right eye, and then terminates by an external signal sent from our  AE  block, and bypasses the computation for rest rows. In the proposed AE  design, the  Δ  pattern buffer is ﬁrst initialized by subtract- ing  Result [1] .R  from  Result [1] .L , as shown in the  AE  block in Fig. If a match is detected, then the corresponding  P i − 2 buff   or  P i − 2 buff   buffer address pointer is directly returned.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "7. After the pattern between left eye and right eye is captured, an external signal is propagated to the OCE to bypass the further original projection computations. In the proposed AE  design, the  Δ  pattern buffer is ﬁrst initialized by subtract- ing  Result [1] .R  from  Result [1] .L , as shown in the  AE  block in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "We prototyped our proposed  EA  and  AE  design blocks using System Verilog in Xilinx Vivado 2019.2 [58], targeting the Xil- \ninx Zynq-7000 SoC ZC706 board running at 100MHz (same as state-of-the-art EVR [28]). After the pattern between left eye and right eye is captured, an external signal is propagated to the OCE to bypass the further original projection computations. Consequently, the projection computation results ( Result [2 :  n ] .R ) for the remaining rows of the right eye can be easily reconstructed by adding  Result [2 :  n ] .L  and the  Δ .",
      "type": "sliding_window_shuffled",
      "tokens": 166,
      "augmented": true
    },
    {
      "text": "The evaluation shows that our  EA and  AE  designs consume only  2 mW  and  65 mW , respectively, and are able to deliver around  100  fps, which is more than sufﬁcient for the current VR application requirements. We prototyped our proposed  EA  and  AE  design blocks using System Verilog in Xilinx Vivado 2019.2 [58], targeting the Xil- \ninx Zynq-7000 SoC ZC706 board running at 100MHz (same as state-of-the-art EVR [28]). V. E VALUATION \nWe compare our proposed  EA  and  AE  designs with six different VR streaming setups, by evaluating the computation and the total energy consumption.",
      "type": "sliding_window_shuffled",
      "tokens": 163,
      "augmented": true
    },
    {
      "text": "In this section, we ﬁrst describe the experimental platforms, datasets and measurement tools used in this study. V. E VALUATION \nWe compare our proposed  EA  and  AE  designs with six different VR streaming setups, by evaluating the computation and the total energy consumption. We then analyze the results measured using these platforms.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "This GPU is commonly used in contemporary VR devices (Oculus [39], Magic Leap [16], and GameFace [47], etc. We then analyze the results measured using these platforms. A. VR Design Conﬁgurations \nWe evaluate the following six conﬁgurations of VR stream- ing to demonstrate the effectiveness of D´ej`a View: •  Baseline  (SW):  We use a mobile GPU [36] to evaluate the baseline VR video streaming. ).",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "We then analyze the results measured using these platforms. A. VR Design Conﬁgurations \nWe evaluate the following six conﬁgurations of VR stream- ing to demonstrate the effectiveness of D´ej`a View: •  Baseline  (SW):  We use a mobile GPU [36] to evaluate the baseline VR video streaming.",
      "type": "sliding_window_partial",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "•  PTU  (HW):  A recent optimized solution [28] utilizes a more energy-efﬁcient hardware accelerator, i.e., Projec- tive Transformation Unit ( PTU ), to process the compute- intensive projection operations. This GPU is commonly used in contemporary VR devices (Oculus [39], Magic Leap [16], and GameFace [47], etc.). Note that, with this setup, the projection computation is triggered for each frame, and also per projection computation invocation includes computation of two projection matrices for the two eyes.",
      "type": "sliding_window_shuffled",
      "tokens": 126,
      "augmented": true
    },
    {
      "text": "We consider this design as the  state-of-the-art . This is the most recent VR design that uses an FPGA for accelerating the computation. •  PTU  (HW):  A recent optimized solution [28] utilizes a more energy-efﬁcient hardware accelerator, i.e., Projec- tive Transformation Unit ( PTU ), to process the compute- intensive projection operations.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "We consider this design as the  state-of-the-art . In contrast, as explained earlier in Sec. However, PTU  only optimizes the energy per compute through accel- eration, with exactly the “same amount of computations” as in the baseline design.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "•  EA  (SW) : We evaluate the  InterFrame, IntraEye ( EA ) design on a GPU, as shown in the  EA  block in Fig. In contrast, as explained earlier in Sec. IV, our design skips a huge amount of computations by exploiting the  EA  and  AE .",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "7. Note that, this implementation is purely done in software, without any hardware modiﬁcation. •  EA  (SW) : We evaluate the  InterFrame, IntraEye ( EA ) design on a GPU, as shown in the  EA  block in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "Note that, this implementation is purely done in software, without any hardware modiﬁcation. •  AE  (SW) : We evaluate the  IntraFrame, InterEye ( AE ) design on a GPU, and bypasses the projection computation for the right-eye by  reconstructing  the results with a learned pattern, as shown in the  AE  block in Fig. 7.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "We denote this design combination as  EA+AE . •  EA+AE  (SW) : The above two designs can be seamlessly integrated into the original SoC, with the  EA  block placed before the GPU and the  AE  block after the GPU. 7.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "The  PTU+EA+AE  implementation combines the  PTU  and our  EA+AE  optimizations together. We denote this design combination as  EA+AE . •  PTU+EA+AE  (HW) : In addition to the GPU-based design, our proposed designs can also be integrated into any other hardware platforms, including the FPGA-based PTU [28].",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "The  PTU+EA+AE  implementation combines the  PTU  and our  EA+AE  optimizations together. Experimental Platforms and Datasets \nEvaluation Platforms:  The  Baseline  GPU platform described in Fig. B.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "8 consists of a 512-core Volta GPU, a 4Kp60 HEVC \n249 \ncodec, 16GB LPDDR4x memory, 32GB eMMC storage, and a power management unit (PMU) that exposes the real-time power traces to users. Experimental Platforms and Datasets \nEvaluation Platforms:  The  Baseline  GPU platform described in Fig. To evaluate our design implementation in hardware, we use an FPGA platform, which is the same as the state-of-the-art PTU [28], with a 100MHz system clock, onboard conﬁguration circuitry, 2x16MB Quad SPI Flash, 1GB DDR2 Component Memory, and also a hardware PMU.",
      "type": "sliding_window_shuffled",
      "tokens": 164,
      "augmented": true
    },
    {
      "text": "A full seat Vivado design suite [58], [59] is utilized to synthesize the design and report the power and timing numbers. To evaluate our design implementation in hardware, we use an FPGA platform, which is the same as the state-of-the-art PTU [28], with a 100MHz system clock, onboard conﬁguration circuitry, 2x16MB Quad SPI Flash, 1GB DDR2 Component Memory, and also a hardware PMU. We collect the display traces from a 5-inch (130 mm) 16:9 1080p (1920 × 1080) AMOLED display [54], which is similar to the Samsung Gear VR display [45].",
      "type": "sliding_window_shuffled",
      "tokens": 154,
      "augmented": true
    },
    {
      "text": "We collect the display traces from a 5-inch (130 mm) 16:9 1080p (1920 × 1080) AMOLED display [54], which is similar to the Samsung Gear VR display [45]. Fig. 8: Evaluation proto- type – Nvidia Jetson TX2 GPU board [36] (PMU: Power Management Unit).",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "8: Evaluation proto- type – Nvidia Jetson TX2 GPU board [36] (PMU: Power Management Unit). 6 \nThe meta information of these VR videos are listed in Tab. 360 °  VR Video Dataset:  We use the published 360° Head Movements Dataset [3], which includes head movement traces from 59 users viewing seven widely-variant 360° VR videos.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "6 \nThe meta information of these VR videos are listed in Tab. II. C. Experimental Results \nWe present and compare the energy consumption of the pro- jection computation and the cor- responding video quality impact, when running the ﬁve VR videos described in Tab.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "II, with the six design conﬁgurations discussed in Sec. C. Experimental Results \nWe present and compare the energy consumption of the pro- jection computation and the cor- responding video quality impact, when running the ﬁve VR videos described in Tab. V-A.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "the  Baseline  method. V-A. These energy results are normalized w.r.t.",
      "type": "sliding_window_shuffled",
      "tokens": 23,
      "augmented": true
    },
    {
      "text": "Later, we show quality results compared to the baseline design. In addition, we also discuss the  our design’s versatility  on other 360 ° video representation formats. the  Baseline  method.",
      "type": "sliding_window_shuffled",
      "tokens": 40,
      "augmented": true
    },
    {
      "text": "Energy Savings : Overall, our software implementation EA+AE on GPU can save 54% computation, which translates to 28% total energy savings, compared to the baseline. Com- pared to the state-of-the-art hardware-modiﬁed PTU, our soft- ware implementation can still provide 16% computation and 8% total energy savings. In addition, we also discuss the  our design’s versatility  on other 360 ° video representation formats.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Our FPGA results can further provide 18% more computation and 9% more total energy savings, compared to the state-of-the-art design. Com- pared to the state-of-the-art hardware-modiﬁed PTU, our soft- ware implementation can still provide 16% computation and 8% total energy savings. More speciﬁcally, for each of the ﬁve video inputs (shown in the x-axis in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "More speciﬁcally, for each of the ﬁve video inputs (shown in the x-axis in Fig. 9, which is further translated to the total end-to-end energy savings shown in the right y-axis in Fig. 9), we compare the compute energy consumption incurred by six schemes with left-eye and right-eye breakdown, and present the respective compute energy in the left y-axis Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "9, which is further translated to the total end-to-end energy savings shown in the right y-axis in Fig. 9. From this ﬁgure, we observe that: •  Baseline:  In  Baseline , since there are no optimizations, the projection operations for both eyes consume equal energy (on GPU), i.e., each eye’s compute consumes  50%  energy.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "overhead, as discussed in Sec. From this ﬁgure, we observe that: •  Baseline:  In  Baseline , since there are no optimizations, the projection operations for both eyes consume equal energy (on GPU), i.e., each eye’s compute consumes  50%  energy. •  EA:  With our proposed  EA  scheme, we fully exploit the temporal compute reuse  across frames with head orien- tations unchanged, with a negligible overhead ( 1%  extra \n6 Due to space limitation, here we only present 5 videos and 20 users.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "In this scheme, one can observe from Fig. IV-C). overhead, as discussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 23,
      "augmented": true
    },
    {
      "text": "9 that, the compute consumes less energy than the  Baseline , i.e., only  72%  on average. This occurs as a result of reusing the memoized results which have been computed and stored previously, ranging from 21 . In this scheme, one can observe from Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "63%  ( Rollercoaster  video) to  50 . This occurs as a result of reusing the memoized results which have been computed and stored previously, ranging from 21 . 28%  ( Paris video).",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "28%  ( Paris video). 9. By applying the  EA  optimization itself, on an average, the energy beneﬁt is translated to  14%  end-to-end energy savings, as shown on the right y-axis in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "•  AE:  For those head orientations not memoized, we further exploited the  spatial compute reuse  across eyes within a frame. 9. In the proposed  AE  scheme, one can observe that for the left-eye computation, the energy consumption is the same as in the  Baseline .",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "In the proposed  AE  scheme, one can observe that for the left-eye computation, the energy consumption is the same as in the  Baseline . Recall from the  AE  design logic in Fig. 7 that, the results of the left-eye are generated by the Original Compute Engine (GPU in this case) and fed into the  AE  block with the ﬁrst row for the right-eye, to store the pattern into the Delta Buffer.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Therefore, as shown in Fig. 7 that, the results of the left-eye are generated by the Original Compute Engine (GPU in this case) and fed into the  AE  block with the ﬁrst row for the right-eye, to store the pattern into the Delta Buffer. After that, the computation for the right-eye can be easily reconstructed by the left- eye’s compute results and the pattern, which only consumes 13%  energy compared to the  Baseline .",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "•  EA+AE:  With both  EA  and  AE  optimizations deployed, as shown in Fig. Therefore, as shown in Fig. 9, our proposed  AE  optimization alone saves  37% compute energy compared to the  Baseline , translating to 19%  total energy saving.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "the  Baseline , with only  10%  for the right-eye, translating to  28%  total energy saving. •  EA+AE:  With both  EA  and  AE  optimizations deployed, as shown in Fig. 9, on average, the left-eye compute consumes only  36%  energy w.r.t.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "•  PTU:  In the current state-of-the-art scheme, which is the hardware-based PTU [28], they explored the energy- efﬁcient hardware accelerator (namely, PTU) to replace power-hungry GPU. the  Baseline , with only  10%  for the right-eye, translating to  28%  total energy saving. Due to this, one can observe from Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "the  Baseline , which contributes to  20%  total energy saving. 9 that, to execute the same amount of the projection computation, the  PTU  scheme consumes only  62%  of energy w.r.t. Due to this, one can observe from Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "the  Baseline , which contributes to  20%  total energy saving. As a result, they can also be deployed on top of the PTU-based SoC. •  PTU+EA+AE:  Note that, our proposed  EA  and  AE  designs are “independent” of the underlying hardware used.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "This can be further asserted from Fig. 9, that only 28%  of the compute energy is consumed w.r.t. As a result, they can also be deployed on top of the PTU-based SoC.",
      "type": "sliding_window_shuffled",
      "tokens": 51,
      "augmented": true
    },
    {
      "text": "Impact on Quality : The proposed  AE  scheme captures the pattern between both the eyes with only the  1 st  row of the frame, and then uses the same pattern to  bypass  the projection computation for the remaining rows of the right eye. 9, that only 28%  of the compute energy is consumed w.r.t. the  Baseline ( 22%  for the left-eye,  6%  for the right-eye), translating to a  37%  total energy saving.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Impact on Quality : The proposed  AE  scheme captures the pattern between both the eyes with only the  1 st  row of the frame, and then uses the same pattern to  bypass  the projection computation for the remaining rows of the right eye. 6b and 6c, the  i th-row’s pattern may not be exactly the same as the  j th-row. Note that, as shown in Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "This is due to the ﬂoating- point hardware rounding (to ﬁnd the nearest-neighbor integer coordinates) and the transformation matrix’s various weights are dependent on the row numbers. 6b and 6c, the  i th-row’s pattern may not be exactly the same as the  j th-row. To simplify our  AE  design, we simply reuse the pattern captured in the  1 st row, and do not consider the deeper information related to the row numbers.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "To study how this decision affects the video quality, we report \n250 \n0% 20% 40% 60% 80% 100% \n0% 20% 40% 60% 80% 100% \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nBaseline \nEA \nAE \nEA+AE \nPTU \nPTU+EA+AE \nRhinos Timelapse Rollercoaster Paris Elephants Avg. % Total Energy Saving \nCompute Energy  Consumption \nL \nR %TotalEnergySaving \nFig. To simplify our  AE  design, we simply reuse the pattern captured in the  1 st row, and do not consider the deeper information related to the row numbers.",
      "type": "sliding_window_shuffled",
      "tokens": 221,
      "augmented": true
    },
    {
      "text": "9: Normalized energy consumption and savings with different conﬁgurations and video inputs. % Total Energy Saving \nCompute Energy  Consumption \nL \nR %TotalEnergySaving \nFig. The left y-axis shows the compute energy consumption normalized to the compute energy consumption in Baseline (the lower, the better).",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "The left y-axis shows the compute energy consumption normalized to the compute energy consumption in Baseline (the lower, the better). 0 \n20 \n40 \n60 \n80 \nV1 V2 V3 V4 V5 Avg. The right y-axis shows the amount of energy savings compared to the end-to-end total energy consumption in Baseline (the higher, the better).",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "0 \n20 \n40 \n60 \n80 \nV1 V2 V3 V4 V5 Avg. 10: Sensitivity study. PSNR (dB) \n-10 \n-5 \n0 \n5 \n10 \n1 224 447 670 893 1116 1339 1562 1785 2008 2231 2454 2677 2900 3123 3346 3569 3792 \nDistance \nPixel ID \nDelta-y Delta-x \n(a): PSNR (b): Pattern in CubeMap format \nFig.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "10: Sensitivity study. (b): The pattern between left-eye and right-eye in the  front  face in Cube Mapping [41]. (a): Video quality metric (PSNR [25], [40]) across video inputs.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "(b): The pattern between left-eye and right-eye in the  front  face in Cube Mapping [41]. the averaged PSNR [25], [40] of the ﬁve videos represented in Equirectangular format [52] in Fig. 10  a  .",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "71  on average) for VR video applications [26], [62]. 10  a  . These results indicate that, although we ignore the row-number related information, the resulting PSNR is still sufﬁcient ( 47 .",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "General Applicability of D´ej`a View : The above discussion assumes that the Equirectangular format [52] is used to represent the  360 ° videos. We want to emphasize that our underlying ideas behind the proposed  EA  and  AE  (designed for the Equirectangular format) can work irrespective of the representation formats used [48]. 71  on average) for VR video applications [26], [62].",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "6b and Fig. For example, similar to the distance vector study in Fig. We want to emphasize that our underlying ideas behind the proposed  EA  and  AE  (designed for the Equirectangular format) can work irrespective of the representation formats used [48].",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "6c, we plot the distance pattern between both eyes of a  360 ° frame using the CubeMap format [41] in Fig. 10  b  . 6b and Fig.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "Note that the pattern behavior also depends on the row numbers. 10  b  . Clearly, a pattern (different from the  ellipse  observed with the Equirectangular format) exists in  Δ x  and  Δ y .",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "This again validates the quality impact discussed earlier. Note that the pattern behavior also depends on the row numbers. Putting together, these above observa- tions indicate that, with very little change (to capture the row- dependent information) in our original  AE  design, our idea is able to work with any representation format.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "VI. Putting together, these above observa- tions indicate that, with very little change (to capture the row- dependent information) in our original  AE  design, our idea is able to work with any representation format. This motivates us to target on further improving quality across video formats by capturing the information related to row numbers in future.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "VI. For example, ATW [38] is a post-render technique, which sits between rendering (our focus) and display. R ELATED  W ORK \nOptimizations in Planar Video Streaming:  Pixel-similarity based optimizations [38], [57] have been exploited to improve \nperformance in 2D rendering.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "To reduce the impact by the long-latency from rendering, ATW either guesses the next head-orientation or only considers the rotation (no translation), then skews two already-rendered planar FoV frames to remove judders [43]. Note that, this computation still happens in planar-format, and remains the same between two-eyes for one frame. For example, ATW [38] is a post-render technique, which sits between rendering (our focus) and display.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "Targeting on ATW, PIMVR [57] proposed a 3D-stacked HMC to re- duce Motion-to-Photon latency and off-chip memory accesses. Motivated by the observation that the ATW transform matrix generated by rotation on a 2D image is shared by both eyes, PIMVR [57] calculated the transform matrix only once, and scheduled two tiles (one for left-eye one for right-eye) with the same coordinate to the same vault in HMC. Note that, this computation still happens in planar-format, and remains the same between two-eyes for one frame.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "Motivated by the observation that the ATW transform matrix generated by rotation on a 2D image is shared by both eyes, PIMVR [57] calculated the transform matrix only once, and scheduled two tiles (one for left-eye one for right-eye) with the same coordinate to the same vault in HMC. However, in contrast to the  360 ° VR video streaming, ATW is in 2D planar format, and share the same compute results across eyes. These two characterizations indicate that such optimizations in the planar world are infeasible to be applied in 3D PT-rendering.",
      "type": "sliding_window_shuffled",
      "tokens": 135,
      "augmented": true
    },
    {
      "text": "For example, PTU [28] uses a hardware-accelerated rendering unit (HAR) to mitigate energy- overheads due to on-device rendering. Hardware assist on VRs:  Various energy-efﬁcient hardware modiﬁcations [28] have been proposed to reduce energy con- sumption in the VR domain. These two characterizations indicate that such optimizations in the planar world are infeasible to be applied in 3D PT-rendering.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "V). For example, PTU [28] uses a hardware-accelerated rendering unit (HAR) to mitigate energy- overheads due to on-device rendering. In this work, we pro- pose two optimizations, i.e.,  EA  and  AE , which can be coupled with the existing  360 ° video compute engine (without any hardware modiﬁcations), and are even more energy efﬁcient than the existing state-of-the-art  PTU  (discussed in Sec.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "V). Pixel Content Reuse on VRs:  Pixel value reuse has been well-studied in VRs [17], [22], [23], [29], [33], [50], [60], [66] to improve throughput and performance. Moreover,  EA  and  AE  can further be integrated into  PTU  to save even more energy.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "For example, DeltaVR [29] adaptively reuses the redundant VR pixels across multiple VR frames to improve performance. These works focus on the pixel content reuse, which is the last stage ( Projection Mapping ) in the  360 ° video projection pipeline (discussed in Sec III). Pixel Content Reuse on VRs:  Pixel value reuse has been well-studied in VRs [17], [22], [23], [29], [33], [50], [60], [66] to improve throughput and performance.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "These works focus on the pixel content reuse, which is the last stage ( Projection Mapping ) in the  360 ° video projection pipeline (discussed in Sec III). However, none of these existing schemes leverage reducing the large amounts of “redundant” computations in the preceding stage (projection computation). 251 \nOur proposed  EA  and  AE  designs focus on these intensive projection computations, and as such are orthogonal to these prior efforts.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "In our future work, we would like to further explore the beneﬁts by incorporating them into our design. 251 \nOur proposed  EA  and  AE  designs focus on these intensive projection computations, and as such are orthogonal to these prior efforts. Head Orientation Prediction for  360 °  Video Streaming: To optimize both performance and energy, researchers have leveraged the powerful remote rendering engines on cloud to predict the next head orientation for the VR clients [2], [6], [18], [23], [30]–[32].",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Head Orientation Prediction for  360 °  Video Streaming: To optimize both performance and energy, researchers have leveraged the powerful remote rendering engines on cloud to predict the next head orientation for the VR clients [2], [6], [18], [23], [30]–[32]. FlashBack maintains a storage cache of multiple versions of pre-rendered frames, which can be quickly indexed by head orientations [2]. In comparison, Semantic- Aware-Streaming (SAS) exploits the semantic information inherent in a VR video content to precisely predict users’ next head orientations [28].",
      "type": "sliding_window_shuffled",
      "tokens": 135,
      "augmented": true
    },
    {
      "text": "However, our work focuses on edge-side optimization, which can also be implemented as a comple- mentary add-on in such cloud-assisted systems. These optimizations rely on the powerful cloud with a high bandwidth access, which may not be always available. In comparison, Semantic- Aware-Streaming (SAS) exploits the semantic information inherent in a VR video content to precisely predict users’ next head orientations [28].",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "For example, AFBC [1] is proposed to efﬁciently compress video streams between the processing pipeline blocks. However, our work focuses on edge-side optimization, which can also be implemented as a comple- mentary add-on in such cloud-assisted systems. Energy Optimizations in Conventional Video Processing: In the existing planar video processing pipeline on mobile de- vices, prior works have looked at memory [1], [63], [64], dis- play [13]–[15], [34] and codec [49], [65], and identiﬁed ”mem- ory” as the major energy bottleneck.",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "MACH [63] integrates a display cache to reduce the amount of memory bandwidth. Although, these techniques can potentially save memory usage/energy for  360 ° VR videos, as discussed in earlier sections, due to inherent nature of  360 ° video processing, which introduces additional overheads for projection computation, we identify compute  to be the major energy bottleneck. For example, AFBC [1] is proposed to efﬁciently compress video streams between the processing pipeline blocks.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "VII. Although, these techniques can potentially save memory usage/energy for  360 ° VR videos, as discussed in earlier sections, due to inherent nature of  360 ° video processing, which introduces additional overheads for projection computation, we identify compute  to be the major energy bottleneck. Hence, these memory optimizations are not applicable to reduce compute energy on  360 ° VR videos.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "C ONCLUDING  R EMARKS \n360° VR videos have become the next trend in entertain- ment media and soon will become an integral part of the technology inﬂuencing many application domains. However, unlike planar videos, the 360° VR video streaming demands signiﬁcantly more compute power from a battery-operated headset. VII.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "In contrast, this paper attempts to exploit available “re- dundancies” in computation by analyzing the VR projection computation pipeline. However, unlike planar videos, the 360° VR video streaming demands signiﬁcantly more compute power from a battery-operated headset. Thus, prior research has proposed using accelerators for optimizing the computations.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "Speciﬁcally, we propose and evaluate in detail two pluggable schemes (for taking advantage of intrinsic temporal-spatial reuse), and prototype them as microarchitec- tural augmentations using FPGA. In contrast, this paper attempts to exploit available “re- dundancies” in computation by analyzing the VR projection computation pipeline. Our experimental results show 34% computation reduction and 17% energy savings, compared to the state-of-the-art [28].",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Our experimental results show 34% computation reduction and 17% energy savings, compared to the state-of-the-art [28]. In the future, we would also like to explore other opportunities to improve energy efﬁciency and tune the computation pipelines to cater more towards VR applications. We believe, given that the current VR devices are battery-backed, these kinds of energy savings and performance improvements will not only enable the users to experience longer videos, but also encourage both industry \nand academia to work further on improving the pipeline to make VR more pervasive and versatile.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "A CKNOWLEDGMENT \nThis research is supported in part by NSF grants #1763681, #1629915, #1629129, #1317560, #1526750, #1714389, #1912495, and a DARPA/SRC JUMP grant. We believe, given that the current VR devices are battery-backed, these kinds of energy savings and performance improvements will not only enable the users to experience longer videos, but also encourage both industry \nand academia to work further on improving the pipeline to make VR more pervasive and versatile. We would also like to thank Dr. Jack Sampson, Dr. Aasheesh Kolli and Dr. Timothy Zhu for their feedback on this paper.",
      "type": "sliding_window_shuffled",
      "tokens": 161,
      "augmented": true
    },
    {
      "text": "We would also like to thank Dr. Jack Sampson, Dr. Aasheesh Kolli and Dr. Timothy Zhu for their feedback on this paper. [2] K. Boos, D. Chu, and E. Cuervo, “FlashBack: Immersive Virtual Reality on Mobile Devices via Rendering Memoization,” in  Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services , ser. R EFERENCES \n[1] Arm Holdings, “Arm Frame Buffer Compression (AFBC).” ”https: //developer.arm.com/architectures/media-architectures/afbc”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 153,
      "augmented": true
    },
    {
      "text": "[2] K. Boos, D. Chu, and E. Cuervo, “FlashBack: Immersive Virtual Reality on Mobile Devices via Rendering Memoization,” in  Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services , ser. MobiSys ’16, 2016, pp. 291–304.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "291–304. [3] X. Corbillon, F. De Simone, and G. Simon, “360-Degreee Video Head Movement Dataset,” in  Proceedings of the 8th ACM on Multimedia Systems Conference , 2017, pp. 199–204.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "[4] Discovery, “Caring for Rhinos: Discovery VR (360 Video).” ”https:// www.youtube.com/watch?v=7IWp875pCxQ”, 2019. [5] Discovery, “Elephants on the Brink.” ”https://www.youtube.com/watch? 199–204.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "[5] Discovery, “Elephants on the Brink.” ”https://www.youtube.com/watch? [6] T. El-Ganainy and M. Hefeeda, “Streaming Virtual Reality Content,” CoRR , vol. v=2bpICIClAIg”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 76,
      "augmented": true
    },
    {
      "text": "abs/1612.08350, 2016. [6] T. El-Ganainy and M. Hefeeda, “Streaming Virtual Reality Content,” CoRR , vol. [Online].",
      "type": "sliding_window_shuffled",
      "tokens": 44,
      "augmented": true
    },
    {
      "text": "Available: http://arxiv.org/ abs/1612.08350 [7] Facebook, “Facebook 360,” ”https://facebook360.fb.com/”, 2019. [8] Facebook Inc., “Facebook Oculus,” ”https://www.oculus.com/”. [Online].",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "google.com/project/360-videos”. [8] Facebook Inc., “Facebook Oculus,” ”https://www.oculus.com/”. [9] Google, “360° videos - Google Arts & Culture,” ”https://artsandculture.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "[10] Google, “More Ways to Watch and Play with AR and VR.” ”https:// blog.google/products/google-vr/more-ways-watch-and-play-ar-and-vr”. [11] Google, “Build Virtual Worlds.” ”https://developers.google.com/vr”, 2019. google.com/project/360-videos”.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "[12] Google, “GVR Android SDK Samples - Video360.” ”https://github.com/ googlevr/gvr-android-sdk/blob/master/samples/sdk-video360/src/main/ java/com/google/vr/sdk/samples/video360/VrVideoActivity.java#L257”, 2019. [13] M. Ham, I. Dae, and C. Choi, “LPD: Low Power Display Mechanism for Mobile and Wearable Devices,” in  Proceedings of the USENIX Conference on Usenix Annual Technical Conference (ATC) , 2015, pp. [11] Google, “Build Virtual Worlds.” ”https://developers.google.com/vr”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 194,
      "augmented": true
    },
    {
      "text": "[14] K. Han, Z. Fang, P. Diefenbaugh, R. Forand, R. R. Iyer, and D. Newell, “Using Checksum to Reduce Power Consumption of Display Systems for Low-motion Content,” in  2009 IEEE International Conference on Computer Design , 2009, pp. 587–598. [13] M. Ham, I. Dae, and C. Choi, “LPD: Low Power Display Mechanism for Mobile and Wearable Devices,” in  Proceedings of the USENIX Conference on Usenix Annual Technical Conference (ATC) , 2015, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 140,
      "augmented": true
    },
    {
      "text": "[14] K. Han, Z. Fang, P. Diefenbaugh, R. Forand, R. R. Iyer, and D. Newell, “Using Checksum to Reduce Power Consumption of Display Systems for Low-motion Content,” in  2009 IEEE International Conference on Computer Design , 2009, pp. 47–53. [15] K. Han, A. W. Min, N. S. Jeganathan, and P. S. Diefenbaugh, “A Hybrid Display Frame Buffer Architecture for Energy Efﬁcient Display Subsystems,” in  International Symposium on Low Power Electronics and Design (ISLPED) , 2013, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "347–353. [15] K. Han, A. W. Min, N. S. Jeganathan, and P. S. Diefenbaugh, “A Hybrid Display Frame Buffer Architecture for Energy Efﬁcient Display Subsystems,” in  International Symposium on Low Power Electronics and Design (ISLPED) , 2013, pp. [16] T. HARDWARE, “Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.” ”https://support.oculus.com/248749509016567/”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "[17] B. Haynes, A. Mazumdar, A. Alaghi, M. Balazinska, L. Ceze, and A. Cheung, “LightDB: A DBMS for Virtual Reality Video,”  Proc. [16] T. HARDWARE, “Magic Leap One Powered by Nvidia Tegra TX2, Available Summer.” ”https://support.oculus.com/248749509016567/”, 2019. VLDB Endow.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "He, M. A. Qureshi, L. Qiu, J. Li, F. Li, and L. Han, “Rubiks: Practical 360-Degree Streaming for Smartphones,” in  Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services , 2018, pp. [18] J. 1192–1205, 2018.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "482–494. [19] HEADJACK, “The Best Encoding Settings For Your 4k 360 3D VR Videos + FREE Encoding Tool,” ”https://headjack.io/blog/best- encoding-settings-resolution-for-4k-360-3d-vr-videos/”. He, M. A. Qureshi, L. Qiu, J. Li, F. Li, and L. Han, “Rubiks: Practical 360-Degree Streaming for Smartphones,” in  Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services , 2018, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "[19] HEADJACK, “The Best Encoding Settings For Your 4k 360 3D VR Videos + FREE Encoding Tool,” ”https://headjack.io/blog/best- encoding-settings-resolution-for-4k-360-3d-vr-videos/”. [21] L. F. Hodges, “Tutorial: Time-multiplexed Stereoscopic Computer Graphics,”  IEEE Computer Graphics and Applications , pp. [20] C. Heather Bellini, W. Chen, M. Sugiyama, M. Shin, S. Alam, and D. Takayama, “Virtual and Augmented Reality.” ”https://www.goldmansachs.com/insights/pages/technology-driving- innovation-folder/virtual-and-augmented-reality/report.pdf”, 2016.",
      "type": "sliding_window_shuffled",
      "tokens": 205,
      "augmented": true
    },
    {
      "text": "252 \n[22] A. [21] L. F. Hodges, “Tutorial: Time-multiplexed Stereoscopic Computer Graphics,”  IEEE Computer Graphics and Applications , pp. 20–30, 1992.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "Holdings, “White Paper: 360-Degree Video Rendering.” ”https://community.arm.com/developer/tools-software/graphics/b/ blog/posts/white-paper-360-degree-video-rendering”, 2019. 252 \n[22] A. [23] J. Huang, Z. Chen, D. Ceylan, and H. Jin, “6-DOF VR Videos with a Single 360-camera,”  2017 IEEE Virtual Reality (VR) , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "[24] A. Inc., “Rendering Omni-directional Stereo Content.” ”https:// developers.google.com/vr/jump/rendering-ods-content.pdf”, 2019. 37–44, 2017. [23] J. Huang, Z. Chen, D. Ceylan, and H. Jin, “6-DOF VR Videos with a Single 360-camera,”  2017 IEEE Virtual Reality (VR) , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "[25] N. INSTRUMENTS, “Peak Signal-to-Noise Ratio as an Image Quality Metric.” ”https://www.ni.com/en-us/innovations/white-papers/11/peak- signal-to-noise-ratio-as-an-image-quality-metric.html”, 2019. [26] B. C. Kim and C. E. Rhee, “Compression Efﬁciency Evaluation for Virtual Reality Videos by Projection Scheme,”  IEIE Transactions on Smart Processing & Computing , pp. [24] A. Inc., “Rendering Omni-directional Stereo Content.” ”https:// developers.google.com/vr/jump/rendering-ods-content.pdf”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 182,
      "augmented": true
    },
    {
      "text": "[27] S. M. LaValle, “The Geometry of Virtual Worlds.” ”http://msl.cs.uiuc. [26] B. C. Kim and C. E. Rhee, “Compression Efﬁciency Evaluation for Virtual Reality Videos by Projection Scheme,”  IEIE Transactions on Smart Processing & Computing , pp. 102–108, 2017.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "[28] Y. Leng, C.-C. Chen, Q. edu/vr/vrch3.pdf”, 2019. [27] S. M. LaValle, “The Geometry of Virtual Worlds.” ”http://msl.cs.uiuc.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "[28] Y. Leng, C.-C. Chen, Q. 91–103. Sun, J. Huang, and Y. Zhu, “Energy-efﬁcient Video Processing for Virtual Reality,” in  Proceedings of the International Symposium on Computer Architecture (ISCA) , 2019, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "13–24. [29] Y. Li and W. Gao, “DeltaVR: Achieving High-Performance Mobile VR Dynamics through Pixel Reuse,” in  2019 18th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN) , 2019, pp. 91–103.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "13–24. [30] L. Liu, R. Zhong, W. Zhang, Y. Liu, J. Zhang, L. Zhang, and M. Gruteser, “Cutting the Cord: Designing a High-quality Untethered VR System with Low Latency Remote Rendering,” in  Proceedings of the 16th Annual International Conference on Mobile Systems, Applications, and Services , ser. MobiSys ’18, 2018, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "[31] X. Liu, Q. Xiao, V. Gopalakrishnan, B. Han, F. Qian, and M. Varvello, “360° Innovations for Panoramic Video Streaming,” in  Proceedings of the 16th ACM Workshop on Hot Topics in Networks , ser. MobiSys ’18, 2018, pp. 68–80.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "[31] X. Liu, Q. Xiao, V. Gopalakrishnan, B. Han, F. Qian, and M. Varvello, “360° Innovations for Panoramic Video Streaming,” in  Proceedings of the 16th ACM Workshop on Hot Topics in Networks , ser. HotNets-XVI, 2017, pp. 50–56.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "1545–1553, 2018. 50–56. [32] B. Luo, F. Xu, C. Richardt, and J. Yong, “Parallax360: Stereoscopic 360° Scene Representation for Head-Motion Parallax,”  IEEE Transactions on Visualization and Computer Graphics , pp.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "[33] A. Mazumdar, T. Moreau, S. Kim, M. Cowan, A. Alaghi, L. Ceze, M. Oskin, and V. Sathe, “Exploring Computation-communication Trade- offs in Camera Systems,” in  2017 IEEE International Symposium on Workload Characterization (IISWC) , 2017, pp. 1545–1553, 2018. 177–186.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "177–186. HotMobile ’16, 2016, pp. [34] H. Miao and F. X. Lin, “Tell Your Graphics Stack That the Display Is Circular,” in  Proceedings of the 17th International Workshop on Mobile Computing Systems and Applications , ser.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "HotMobile ’16, 2016, pp. 57– 62. [35] mooovr, “RollerCoaster at Seoul Grand Park.” ”https://www.youtube.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "[36] Nvidia, “JETSON AGX XAVIER AND THE NEW ERA OF AU- TONOMOUS MACHINES.” ”http://info.nvidia.com/rs/156-OFN-742/ images/Jetson AGX Xavier New Era Autonomous Machines.pdf”, 2019. com/watch?v=8lsB-P8nGSM”, 2019. [35] mooovr, “RollerCoaster at Seoul Grand Park.” ”https://www.youtube.",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "[38] Oculus, “Asynchronous TimeWarp (ATW).” ”https://developer. [36] Nvidia, “JETSON AGX XAVIER AND THE NEW ERA OF AU- TONOMOUS MACHINES.” ”http://info.nvidia.com/rs/156-OFN-742/ images/Jetson AGX Xavier New Era Autonomous Machines.pdf”, 2019. [37] Oculus, “Rendering to the Oculus Rift,” ”https://developer.oculus.com/ documentation/pcsdk/latest/concepts/dg-render/”.",
      "type": "sliding_window_shuffled",
      "tokens": 162,
      "augmented": true
    },
    {
      "text": "oculus.com/documentation/mobilesdk/latest/concepts/mobile-timewarp- overview/?locale=en US”, 2019. [38] Oculus, “Asynchronous TimeWarp (ATW).” ”https://developer. [39] Oculus, “Oculus Rift and Rift S Minimum Requirements and Sys- tem Speciﬁcations.” ”https://www.tomshardware.com/news/magic-leap- tegra-specs-release,37443.html”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "[39] Oculus, “Oculus Rift and Rift S Minimum Requirements and Sys- tem Speciﬁcations.” ”https://www.tomshardware.com/news/magic-leap- tegra-specs-release,37443.html”, 2019. [40] OpenCV, “Similarity check (PNSR and SSIM) on the GPU.” ”https://docs.opencv.org/2.4/doc/tutorials/gpu/gpu-basics- similarity/gpu-basics-similarity.html”, 2019. [41] OpenGL, “Cubemaps - Learn OpenGL.” ”https://learnopengl.com/ Advanced-OpenGL/Cubemaps”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 191,
      "augmented": true
    },
    {
      "text": "[43] O. [41] OpenGL, “Cubemaps - Learn OpenGL.” ”https://learnopengl.com/ Advanced-OpenGL/Cubemaps”, 2019. [42] OpenGL, “The Industry’s Foundation for High Performance Graphics.” ”https://www.opengl.org/”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "Rift, “Oculus Rift - How Does Time Warping Work?” ”https://www. [43] O. youtube.com/watch?v=WvtEXMlQQtI”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "youtube.com/watch?v=WvtEXMlQQtI”, 2019. [45] Samsung, “Explore New Dimensions.” ”https://www.samsung.com/ global/galaxy/gear-vr/#display”, 2019. [44] Samsung, “Samsung Gear VR,” ”https://www.samsung.com/global/ galaxy/gear-vr/”.",
      "type": "sliding_window_shuffled",
      "tokens": 107,
      "augmented": true
    },
    {
      "text": "[45] Samsung, “Explore New Dimensions.” ”https://www.samsung.com/ global/galaxy/gear-vr/#display”, 2019. [47] Tom’s HARDWARE, “Nvidia’s Jetson TX2 Powers GameFace Labs’ Standalone VR Headset.” ”https://www.tomshardware.com/news/ gameface-labs-standalone-steamvr-headset,37112.html”, 2019. [46] SkySports, “Sky VR Virtual Reality,” ”https://www.skysports.com/ mobile/apps/10606146/sky-vr-virtual-reality”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 173,
      "augmented": true
    },
    {
      "text": "HPG ’16, 2016, pp. [47] Tom’s HARDWARE, “Nvidia’s Jetson TX2 Powers GameFace Labs’ Standalone VR Headset.” ”https://www.tomshardware.com/news/ gameface-labs-standalone-steamvr-headset,37112.html”, 2019. [48] R. Toth, J. Nilsson, and T. Akenine-M¨oller, “Comparison of Projection Methods for Rendering Virtual Reality,” in  Proceedings of High Perfor- mance Graphics , ser.",
      "type": "sliding_window_shuffled",
      "tokens": 150,
      "augmented": true
    },
    {
      "text": "163–171. HPG ’16, 2016, pp. [49] C.-H. Tsai, H.-T. Wang, C.-L. Liu, Y. Li, and C.-Y.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "[49] C.-H. Tsai, H.-T. Wang, C.-L. Liu, Y. Li, and C.-Y. 305–308. Lee, “A 446.6 K-gates 0.55–1.2 V H. 265/HEVC decoder for next generation video applications,” in  2013 IEEE Asian Solid-State Circuits Conference (A- SSCC) , 2013, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "[51] F. G. VR360, “Virtual guided tour of Paris.” ”https://www.youtube.com/ watch?v=sJxiPiAaB4k”, 2019. [50] A. Vlachos, “Advanced VR Rendering in Valve.” ”http: //media.steampowered.com/apps/valve/2015/Alex Vlachos Advanced VR Rendering GDC2015.pdf”, 2019. 305–308.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "[51] F. G. VR360, “Virtual guided tour of Paris.” ”https://www.youtube.com/ watch?v=sJxiPiAaB4k”, 2019. [52] Wikepedia, “Equirectangular Projection.” ”https://en.wikipedia.org/wiki/ Equirectangular projection”, 2019. [53] Wikipedia, “Pixel 2,” ”https://en.wikipedia.org/wiki/Pixel 2”.",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "[53] Wikipedia, “Pixel 2,” ”https://en.wikipedia.org/wiki/Pixel 2”. [54] Wikipedia, “Active-Matrix Organic Light-Emitting Diode,” ”https://en. wikipedia.org/wiki/AMOLED”, 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "[55] Wikipedia, “Virtual Reality.” ”https://en.wikipedia.org/wiki/Peak signal-to-noise ratio#: ∼ :targetText=Typical \\ %20values%20for% 20the%20PSNR,20 \\ %20dB%20to%2025%20dB.”, 2019. wikipedia.org/wiki/AMOLED”, 2019. [56] B.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "[56] B. Worldwide, “NYC 360 Timelapse.” ”https://www.youtube.com/ watch?v=CIw8R8thnm8”, 2019. [57] C. Xie, X. Zhang, A. Li, X. Fu, and S. Song, “PIM-VR: Erasing Motion Anomalies In Highly-Interactive Virtual Reality World with Customized Memory Cube,” in  Proceedings of the International Symposium on High- Performance Computer Architecture (HPCA) , 2019, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "609–622. [58] Xilinx, “Vivado Design Hub - Installation and Licensing,” ”https://www.xilinx.com/support/documentation-navigation/design- hubs/dh0013-vivado-installation-and-licensing-hub.html”. [57] C. Xie, X. Zhang, A. Li, X. Fu, and S. Song, “PIM-VR: Erasing Motion Anomalies In Highly-Interactive Virtual Reality World with Customized Memory Cube,” in  Proceedings of the International Symposium on High- Performance Computer Architecture (HPCA) , 2019, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 165,
      "augmented": true
    },
    {
      "text": "[59] Xilinx, “Vivado Design Suite - HLx Editions.” ”https://www.xilinx.com/ products/design-tools/vivado.html”, 2019. [60] xinreality, “Asynchronous Spacewarp.” ”https://xinreality.com/wiki/ Asynchronous Spacewarp”, 2019. [58] Xilinx, “Vivado Design Hub - Installation and Licensing,” ”https://www.xilinx.com/support/documentation-navigation/design- hubs/dh0013-vivado-installation-and-licensing-hub.html”.",
      "type": "sliding_window_shuffled",
      "tokens": 169,
      "augmented": true
    },
    {
      "text": "[60] xinreality, “Asynchronous Spacewarp.” ”https://xinreality.com/wiki/ Asynchronous Spacewarp”, 2019. [61] YouTube, “Get Started with YouTube VR.” ”https://support.google.com/ youtube/answer/7205134?hl=en”, 2019. [62] V. Zakharchenko, K. P. Choi, and J. H. Park, “Quality metric for spherical panoramic video,” in  Optics and Photonics for Information Processing X , K. M. Iftekharuddin, A.",
      "type": "sliding_window_shuffled",
      "tokens": 143,
      "augmented": true
    },
    {
      "text": "SPIE, 2016, pp. A. S. Awwal, M. G. V´azquez, A. M´arquez, and M. A. Matin, Eds., International Society for Optics and Photonics. [62] V. Zakharchenko, K. P. Choi, and J. H. Park, “Quality metric for spherical panoramic video,” in  Optics and Photonics for Information Processing X , K. M. Iftekharuddin, A.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "[63] H. Zhang, P. V. Rengasamy, S. Zhao, N. C. Nachiappan, A. Sivasub- ramaniam, M. T. Kandemir, R. Iyer, and C. R. Das, “Race-to-sleep + Content Caching + Display Caching: A Recipe for Energy-efﬁcient Video Streaming on Handhelds,” in  Proceedings of the International Symposium on Microarchitecture (MICRO) , 2017, pp. 57 – 65. SPIE, 2016, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "[63] H. Zhang, P. V. Rengasamy, S. Zhao, N. C. Nachiappan, A. Sivasub- ramaniam, M. T. Kandemir, R. Iyer, and C. R. Das, “Race-to-sleep + Content Caching + Display Caching: A Recipe for Energy-efﬁcient Video Streaming on Handhelds,” in  Proceedings of the International Symposium on Microarchitecture (MICRO) , 2017, pp. 517–531. [64] H. Zhang, S. Zhao, A. Pattnaik, M. T. Kandemir, A. Sivasubramaniam, and C. R. Das, “Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices,” in  Proceedings of the International Symposium on Microarchitecture (MICRO) , 2019, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 205,
      "augmented": true
    },
    {
      "text": "657–669. [65] D. Zhou, S. Wang, H. Sun, J. Zhou, J. Zhu, Y. Zhao, J. Zhou, S. Zhang, S. Kimura, T. Yoshimura, and S. Goto, “14.7 a 4gpixel/s 8/10b h.265/hevc video decoder chip for 8k ultra hd applications,” in 2016 IEEE International Solid-State Circuits Conference (ISSCC) , 2016, pp. [64] H. Zhang, S. Zhao, A. Pattnaik, M. T. Kandemir, A. Sivasubramaniam, and C. R. Das, “Distilling the Essence of Raw Video to Reduce Memory Usage and Energy at Edge Devices,” in  Proceedings of the International Symposium on Microarchitecture (MICRO) , 2019, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 208,
      "augmented": true
    },
    {
      "text": "266–268. [66] Y. Zhu, A. Samajdar, M. Mattina, and P. Whatmough, “Euphrates: Algorithm-SoC Co-design for Low-power Mobile Continuous Vision,” in  Proceedings of the International Symposium on Computer Architec- ture (ISCA) , 2018, pp. [65] D. Zhou, S. Wang, H. Sun, J. Zhou, J. Zhu, Y. Zhao, J. Zhou, S. Zhang, S. Kimura, T. Yoshimura, and S. Goto, “14.7 a 4gpixel/s 8/10b h.265/hevc video decoder chip for 8k ultra hd applications,” in 2016 IEEE International Solid-State Circuits Conference (ISSCC) , 2016, pp.",
      "type": "sliding_window_shuffled",
      "tokens": 200,
      "augmented": true
    },
    {
      "text": "[66] Y. Zhu, A. Samajdar, M. Mattina, and P. Whatmough, “Euphrates: Algorithm-SoC Co-design for Low-power Mobile Continuous Vision,” in  Proceedings of the International Symposium on Computer Architec- ture (ISCA) , 2018, pp. 547–560. 253",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    }
  ]
}