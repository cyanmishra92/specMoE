=== ORIGINAL PDF: 2506.06505v1_InstantFT_An_FPGA-Based_Runtime_Subsecond_Fine-tun.pdf ===\n\nRaw text length: 24600 characters\nCleaned text length: 24357 characters\nNumber of segments: 17\n\n=== CLEANED TEXT ===\n\narXiv:2506.06505v1 [cs.LG] 6 Jun 2025 INSTANTFT: AN FPGA-BASED RUNTIME SUBSECOND FINE-TUNING OF CNN MODELS A PREPRINT Keisuke Sugiura University of Tsukuba 1-1-1 Tennôdai, Tsukuba, Ibaraki, Japan Hiroki Matsutani Keio University 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan June 13, 2025 ABSTRACT Training deep neural networks (DNNs) requires significantly more computation and memory than inference, making runtime adaptation of DNNs challenging on resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for ultra-fast CNN fine-tuning on IoT devices, by op- timizing the forward and backward computations in parameter-efficient fine-tuning (PEFT). Ex- periments on datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches, while achieving compa- rable accuracy. Our FPGA-based InstantFT reduces the fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x, enabling on-the-fly adaptation of CNNs to non-stationary data distribu- tions. 1 Introduction On-device learning of DNNs is an effective solution to bridge the gap between training data and deployed environ- ment [1]. Since full retraining requires prohibitive costs, lightweight PEFT methods that only update a small set of parameters have been explored for resource-limited edge devices [2, 3]. LoRA (Low-Rank Adaptation) [4, 5] intro- duces trainable low-rank matrices while keeping the original pre-trained network fixed, allowing efficient adaptation to new tasks with minimal overhead. Instead of backpropagation and gradient descent, other approaches, e.g., Extreme Learning Machines (ELMs) [6] and zeroth-order optimization (ZOO) [7], have been applied to on-device learning. However, ELM is only applicable to tiny-scale models consisting of one hidden layer [8], while ZOO suffers from a slow convergence and long training time [9]. This work presents InstantFT, an ultra-fast fine-tuning approach based on LoRA. As shown in Fig. 1, InstantFT runs significantly faster than the baseline LoRA approaches, without com- promising accuracy and robustness against data distribution shifts. 2 Baselines This section describes the baseline fine-tuning strategies. Table 1 presents a quantitative comparison between our InstantFT and baselines when applied to a LeNet-5-like model (Fig. 4) for two image classification datasets. We consider a simple L-layer network composed of fully-connected (FC) and convolution (Conv) layers, each having a weight and bias Wi, bi i 1,...,L. We denote the forward activation of the i-th layer by xi. The FC layer computes xi Wixi 1 bi, where xi 1, xi, bi Rd, Wi Rd d. The simplest approach is to fine-tune the entire network (FT-All) or only biases b1, . . . , bL (FT-Bias [10]). As illustrated in Fig. 2 (top right), another possible approach is to fine-tune parameters of the last layer (WL, bL) while keeping the rest frozen (FT-Last). While FT-Last requires significantly (854 1133x) less computational cost for backpropagation than FT-All, it only updates 1.4 of the pa- rameters and has a limited fine-tuning capability (Fig. 1). FT-Bias updates even fewer parameters than FT-Last, but it only reduces the backward FLOPs by 2.4 3.1x compared to FT-All, as it still requires gradients of forward activations dx. InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT 0 50 100 150 Fine-tuning Time (s) 50 55 60 65 70 75 80 Accuracy ( ) InstantFT (FPGA) (0.36s) InstantFT (6.2s) FT-Last (26.3s) LoRA-Last (26.2s) LoRA-All (140.5s) Figure 1: Fine-tuning time vs. accuracy of InstantFT and baselines (Rotated Fashion-MNIST, 75deg). Table 1: Comparison between InstantFT and baselines (LeNet-5) MNIST SVHN Method Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) FT-All 61706 0.851M 1.444M 567.8KB 62006 1.321M 1.914M 579.4KB FT-Last 850 0.851M 0.002M 285.8KB 850 1.321M 0.002M 296.1KB FT-Bias 236 0.851M 0.611M 322.0KB 236 1.321M 0.611M 332.3KB LoRA-All 36328 0.923M 0.743M 611.8KB 45480 1.412M 0.762M 695.4KB LoRA-Last 376 0.852M 0.001M 285.4KB 376 1.322M 0.001M 295.8KB InstantFT 10456 0.872M 0.036M 366.2KB 19608 1.360M 0.054M 449.8KB As in Fig. 2 (bottom), we consider two LoRA-based approaches: inserting LoRA adapters into all layers (LoRA-All) or applying LoRA to only the last layer (LoRA-Last). For the i-th layer, LoRA introduces a new matrix Wi 1,i Rd d. It is further decomposed into two trainable matrices A Rr d, B Rd r of rank r d, which are initialized with random Gaussian values and zeros. The modified layer now produces: xi Wi Wi 1,i xi 1 bi Wi Bi 1,iAi 1,i xi 1 bi. The trainable parameters in LoRA-All and LoRA-Last are denoted as Ai 1,i, Bi 1,i i 1,...,L and AL 1,L, BL 1,L , respectively. LoRA-All performs backpropagation across the entire network, and therefore needs to store additional LoRA parameters, all forward activations x1, . . . , xL, and their gradients dx1, . . . , dxL, resulting in 7.7 20.0 higher memory consumption than FT-All. Similar to FT-Bias, LoRA-All still retains 39.8 51.5 of the Figure 2: Baseline fine-tuning methods and InstantFT. 2 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT Figure 3: Flow of fine-tuning with InstantFT. backward FLOPs of FT-All due to the computation of activation gradients. In contrast, LoRA-Last updates only the last layer using the activation xL, resulting in significantly reduced FLOPs and memory costs for backpropagation, at the cost of lower fine-tuning capability (Fig. 1). 3 InstantFT InstantFT achieves high fine-tuning accuracy as {FT, LoRA}-All, while maintaining low backward FLOPs and mem- ory usage similar to {FT, LoRA}-Last, thereby greatly improving the resource-accuracy trade-off. The algorithm flow is illustrated in Fig. 3. 3.1 Forward Pass Unlike the baselines, InstantFT introduces LoRA modules that connect each intermediate layer directly to the final layer, where we denote learnable parameters as Ai,L, Bi,L i 0,...,L 1 (Wi,L Ai,LBi,L). InstantFT produces an output as follows: xL WLxL 1 bL L 1 X i 0 Bi,LAi,Lxi ˆxL L 1 X i 0 xi. (1) The first term ˆxL is an output of the pre-trained base network, while the rest xi Bi,LAi,Lxi are obtained using LoRA adapters. Since the pre-trained network is fixed during fine-tuning, x1, . . . , xL 1 and ˆxL remain unchanged for the same input x0. InstantFT thus introduces a Forward Cache to store and reuse these intermediate results. As depicted in Fig. 3, InstantFT first checks that the entry for the input x0 (with an index j) is present in the cache. If so, InstantFT reuses the precomputed results and only performs the adapter part to compute P i xi; otherwise, InstantFT also runs a forward pass of the pre-trained network to obtain xi , ˆxL and stores them in the cache for reuse. As such, InstantFT performs a forward pass of the pre-trained network only once per input, and repeatedly uses LoRA adapters in later epochs. For a fixed dataset, the intermediate results for all samples are computed and cached in the first epoch. This significantly reduces the overall computational cost and training time, as the pre-trained network involves more computation than LoRA modules (e.g., 33.7 40.7x higher forward FLOPs in our case, Table 1). Additionally, InstantFT has 3.8 5.9 less forward FLOPs than LoRA-All, due to a lowered output dimension of LoRA modules. 3.2 Cache Quantization Since the Forward Cache stores forward activations for every input, its size grows with the dataset size, model depth, and output dimensions, resulting in a prohibitive memory overhead. To mitigate this, InstantFT adopts the 4-bit NormalFloat (NF4) quantization [5], which is well-suited for normally distributed parameters and activations. This reduces the cache size by approximately 8x compared to FP32. Note that the cached data is dequantized before used in the forward pass. 3 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT Figure 4: InstantFT for LeNet-5-like model. Conv-MP Conv-MP FC FC FC Params or Quant Dequant LConv LConv LFC LFC LFC Params BPLConv BPLConv BPLFC BPLFC BPLFC or Softmax CELoss AXI Interconnect S S S M 4GB 64-bit DDR4 Quad-core ARM Cortex-A53 (1.33GHz) HP0 HP1 HP2 HPM0 M M M S AXI AXI AXI AXI-Lite AXI Interrupt Controller M S irq gmem0 gmem1 gmem2 control InstantFT M S InstantFT S Signals M M AXI-Lite AXI-Lite Zynq PL (Programmable Logic) Zynq PS (Processing System) (3) (2) (1) Xilinx Kria KV260 Figure 5: InstantFT implemented on Xilinx Kria KV260. 3.3 Backward Pass In backpropagation, given an activation gradient dxi, the i-th layer computes gradients w.r.t. the parameters and input: dWi dxi(xi 1) dbi dxi dxi 1 (Wi) dxi. (2) In LoRA (with the main network frozen), gradients are propagated as: dBj,k dxk(hj,k) dhj,k (Bj,k) dxk dAj,k dhj,k(xj) dxj dxj (Aj,k) dhj,k, (3) where hj,k Aj,kxj. Given an output gradient dxL, InstantFT computes LoRA gradients n dAi,L, dBi,Lo i 0,...,L 1 following Eq. 3. Unlike dx, dW, which require quadratic computational com- plexity O(d2), LoRA gradients dA, dB have lower complexity of O(dr) (r d). InstantFT uses the same number of adapters as LoRA-All but involves 14.1 40.6x less backward FLOPs than {FT, LoRA}-All, since it neither computes dx, dW nor propagates dx through the entire network. Notably, InstantFT fine-tunes 12.3 52.1x more parameters with only 1.28 1.52x higher memory footprint compared to {FT, LoRA}-Last. InstantFT is likely to achieve better fine-tuning performance since trainable parameters are distributed across the network. 4 Implementations We use a LeNet-5-like network [11] (Fig. 4) as a backbone, where five LoRA adapters (r 4) are inserted into the Conv and FC layers for fine-tuning. Each adapter takes an activation xi Rcin from the previous layer and computes an update xi Bi,LAi,Lxi Rcout (Eq. 1) to the last layer output ˆxL. For Conv adapters, the input xi Rcin h w is first flattened into a vector vec(xi) Rcinhw before computation (Fig. 4 (left)). 4 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT LConv LConv LFC LFC LFC Add Conv-MP Conv-MP FC FC FC Dequant mux mux mux mux mux Quant !cached Params Params CELoss BPLConv Duplicate BPLConv BPLFC BPLFC BPLFC Params Softmax idx (1)(2) (3) Conv x LineBuf Window MaxPool 2x2 Compute outputs in parallel Compute outputs in parallel Conv-MP Mul Update Update BPLFC Mul Mul LFC Conv-MP BPLFC LFC Figure 6: Block diagram of InstantFT core. 4.1 InstantFT on FPGA We design a dedicated InstantFT core for embedded FPGAs. The core (Fig. 5) consists of three module groups and a set of on-chip buffers (for storing inputs, parameters, forward activations, and gradients). The first two groups (Fig. 6, top left) perform the forward pass of the base network and adapters, where Conv-MP FC correspond to Conv- MaxPool FC layers, and LConv LFC are their respective LoRA adapters. Conv-MP (Fig. 6, top right) consists of a pipeline of two modules: Conv performs 2D convolution and stores the output pixels into a line buffer (implemented as a shift register), while MaxPool fetches 2x2 pixels from this buffer and performs 2D max-pooling. During fine-tuning, a mini-batch of samples and their corresponding indices are transferred to the core. InstantFT uses a Forward Cache placed on the external DRAM to avoid repeated forward passes of the base network for the same input. The cached forward activations x1, . . . , x4, ˆx5 are transferred to the on-chip buffers if available; otherwise, Conv-MP FC compute them and send to the cache. Then, LConv LFC compute deltas x1, . . . , x5 via matrix multiplications (Fig. 6, bottom center), which are summed together and added into ˆx5 to produce a final output x5. Dequant Quant handle the cache read write operations. Dequant reads 4-bit quantized cache entries for the input samples and writes them to respective buffers after dequantization, where indices are used to calculate memory ad- dresses and locate the entries. Quant performs the opposite. The probabilities p for each label are obtained from the logits x5 via softmax; we adopt precomputed lookup tables [12] instead of computing exponentials inverses to simplify the logic. Based on the predictions p and true labels t, the last group (Fig. 6, bottom left) performs backpropagation and gradient descent to update adapters. CELoss (cross-entropy loss) computes an output gradient dx5 and copies it to buffers that originally stored deltas xi. Then, given dx5, saved activations xj, hj,k Aj,kxj, and a learning rate η R , BPLConv BPLFC (Fig. 6, bottom right) perform matrix operations to obtain parameter gradients dA, dB and update LoRA parameters via SGD (A A ηdA). To improve latency, these modules are parallelized through loop unrolling and buffer partitioning (e.g., Conv-MP computes output pixels for multiple samples and channels per clock cycle). Besides, the LoRA modules (LConv LFC and BPLConv BPLFC) are executed in parallel to simultaneously compute deltas gradients and update parameters at once. Importantly, InstantFT allows such an optimization because all LoRA adapters directly connect to the last layer. In contrast, LoRA-All requires LoRA adapters to be executed sequentially, as the gradient dx should be backpropa- gated through the entire network. We use Xilinx Kria KV260 running PetaLinux 2022.1 as a target platform (Fig. 5, right), which features a Zynq UltraScale MPSoC FPGA, a quad-core ARM Cortex-A53 CPU (1333MHz), and 4GB DDR4 DRAM. We use Xilinx 5 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT 15 30 45 60 75 90 Rotation Angle ( ) 70 80 90 Accuracy ( ) MNIST to RotMNIST 15 30 45 60 75 90 Rotation Angle ( ) 50 60 70 80 FMNIST to RotFMNIST Full 18 12 6 3 Samples (x104) 75 80 85 SVHN-Extra to SVHN w o Fine-tuning FT-Last LoRA-Last LoRA-All InstantFT w o Cache InstantFT w Cache (NF4) InstantFT (FPGA) (a) Accuracy. 130 140 140.5s 0 20 Inference Only FT-Last LoRA-Last LoRA-All InstantFT w o InstantFT w (FP32) InstantFT w (NF4) 4.9s 26.3s 26.1s 29.1s 5.7s 6.2s RotMNIST Seconds 22000 21495.5s 0 2000 183.1s 2929.1s 2925.1s 3280.0s 669.1s 714.9s SVHN Seconds (b) Execution time. 0 50 100 Utilization ( ) LUT FF BRAM URAM DSP 48.71 25.38 62.50 71.88 37.18 (c) FPGA resource utilization. Figure 7: Evaluation of InstantFT and baselines. Vitis 2024.1 to implement the core in C C and generate an FPGA bitstream. The core along with two AXI IPs (an interrupt controller and interconnect) are clocked at 200MHz and implemented on PL (Programmable Logic). The core has 128-bit AXI manager interfaces connected to the HP (high-performance) ports for data transfer from to the DRAM on PS (Processing System), and a 32-bit AXI-Lite subordinate interface for control register accesses. The frozen network parameters and initial LoRA parameters are pre-loaded into on-chip buffers to minimize the DRAM accesses. We use Q8.16 fixed-point format to represent forward activations, and Q4.12 for parameters and gradients. The host code is developed in C C using XRT (Xilinx Runtime) library to communicate with the core. It transfers input data (samples labels indices) and the updated LoRA parameters via memory-mapped AXI interfaces. 5 Experimental Results 5.1 Experimental Setup We compare InstantFT with the baseline fine-tuning approaches (Fig. 2) under the following three cases: (1) The LeNet-5-like model (Fig. 4) is pre-trained on MNIST and then fine-tuned on its rotated version (RotMNIST). (2) Pre-trained on Fashion-MNIST (FMNIST) and fine-tuned on its rotated counterpart (RotFMNIST). (3) Pre-trained on SVHN-Extra and fine-tuned on the test split of SVHN. We randomly pick 1024 samples each from the test sets of {MNIST, FMNIST} and rotate them by θ: 15 , 30 , . . ., 90 to generate six variants of Rot{MNIST, FMNIST} datasets. In (3), we pre-train the network with different numbers of samples D : 30000, 60000, 120000, 180000, 521131. We simulate challenging fine-tuning tasks with greater data distribution shifts using a larger θ or smaller D . The model is pre-trained and fine-tuned for 10 epochs with a batch size of 20. The learning rate η is set to 0.1 and 0.025 in (1) (2) and (3). The code for pre-training and fine-tuning is developed in C and compiled using GCC 11.2.0 with -O3 flag. We use OpenMP to utilize all four CPU cores. 6 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT 5.2 Accuracy Fig. 7a shows the classification accuracy of InstantFT and baselines on three cases. We report the average of ten runs with different random seeds. Without fine-tuning, the accuracy drops sharply under challenging settings (larger rotation angles (Rot{MNIST, FMNIST}) or fewer pre-training samples (SVHN)). While fine-tuning the last layer recovers the accuracy to some extent, FT-Last LoRA-Last is still 0.04 12.2 0.5 20.3 less accurate on RotMNIST (6.5 13.6 9.8 22.6 , 0.2 4.5 0.2 4.9 less accurate on RotFMNIST, SVHN) compared to fine-tuning the entire network with LoRA adapters (LoRA-All). In contrast, InstantFT performs comparably or even better than LoRA-All; it maintains an accuracy of 91.8 77.9 79.0 on three cases under the most difficult settings (θ 90 , D 30000), while LoRA-All is 91.2 75.1 81.7 accurate. InstantFT shows a similar robustness to data distribution shifts as LoRA- All, with an accuracy drop of 5.2 2.8 9.4 (LoRA-All: 5.7 6.2 6.7 ). Besides, NF4 quantization of the forward cache only leads to a marginal accuracy loss of 0.09 0.4 . The FPGA implementation (red) attains accuracy within 3.2 of its software counterpart and remains robust under data distribution shifts. 5.3 Execution Time Fig. 7b shows the time required for 10-epoch fine-tuning on RotMNIST and SVHN, measured on an ARM Cortex- A53 CPU. Due to higher forward backward FLOPs, LoRA-All takes 5.3x more time to fine-tune all Conv FC layers on RotMNIST compared to only updating the last layer (FT-Last LoRA-Last). While InstantFT employs the same number of LoRA adapters, it runs 4.8x faster than LoRA-All on RotMNIST, as it obviates the need to compute output gradients dxi (except dx5) and saves the computational cost for backpropagation by 20.9x. Forward Cache leads to further 5.1x speedup on RotMNIST, as forward activations of the base network are computed only once per input at the first epoch and then reused in the subsequent epochs. Notably, InstantFT performs fine-tuning with only a 17 increase in execution time compared to the inference-only case, and successfully addresses the discrepancy between training and testing data. While NF4 (de)quantization of cached activations introduces an additional 8.8 overhead, InstantFT still runs significantly faster than baselines. The results on SVHN show a similar trend except for the 38 153x longer execution time, which is mainly due to 72x larger dataset size (73257 1024 images). The FPGA implementation of InstantFT (with quantized cache) only takes 0.36s on Kria KV260 to fine-tune the model for 10 epochs, which is 17.4x faster than the CPU counterpart, thereby enabling on-the-fly adaptation of CNNs. For a fair comparison, the execution time includes various overheads such as PS PL data transfers and FPGA kernel invocations. 5.4 Power Consumption During fine-tuning, we measure the power consumption of KV260 every 50ms using an onboard INA260 sensor and compute its average value. While the FPGA-implemented InstantFT consumes 0.24W more power (3.79W 4.03W) compared to only using ARM Cortex-A53, it achieves 16.32x higher energy-efficiency thanks to the attained speedup. 5.5 Forward Cache Size The size of Forward Cache depends on both the number of samples in a given dataset and model architecture. On Rot{MNIST, FMNIST} and SVHN, the full-precision cache requires 7.33 524.52MB of memory, which is brought down to 1.02 72.89MB by NF4 quantization (7.20x reduction) without compromising accuracy (Fig. 7a). 5.6 FPGA Resource Utilization Fig. 7c shows the FPGA resource utilization of InstantFT on Kria KV260. The design effectively uses on-chip memory resources to store (frozen LoRA) parameters, activations, and gradients, thereby minimizing external memory accesses and improving overall performance. The on-chip memory can be further saved by applying low-bit quantization to parameters and gradients as well. In addition to LeNet-5, larger-scale networks can be fine-tuned on KV260, thanks to the low utilization of logic resources (LUTs DSPs). 6 Summary This paper presents InstantFT, a new ultra-fast fine-tuning method that enables edge applications to quickly adapt to data distribution shifts within subseconds. InstantFT leverages trainable adapters directly connected to the output layer, significantly saving the computational cost of backpropagation. In addition, it introduces 4-bit quantized Forward Cache to eliminate redundant forward passes of the frozen base network. The proposed FPGA-based design fully 7 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT exploits the high parallelism and simplified computation flow of the adapters. Experimental results demonstrate that InstantFT achieves accuracy comparable to the original LoRA in just 0.36s on Xilinx Kria KV260, while being 17.4x faster and 16.3x more energy-efficient than ARM Cortex-A53. While InstantFT is currently evaluated on small-scale networks, we aim to extend its application to large-scale models including LLMs. References [1] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), pages 11285 11297, December 2020. [2] Kazuki Sunaga, Masaaki Kondo, and Hiroki Matsutani. Addressing Gap between Training Data and Deployed Environment by On-Device Learning. IEEE Micro, 43(6):66 73, Nov Dec 2023. [3] Hiroki Matsutani, Masaaki Kondo, Kazuki Sunaga, and Radu Marculescu. Skip2-LoRA: A Lightweight On- device DNN Fine-tuning Method for Low-cost Edge Devices. In Proceedings of the Asia and South Pacific Design Automation Conference (ASP-DAC), pages 51 57, January 2025. [4] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations (ICLR), January 2022. [5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), pages 10088 10115, December 2023. [6] Nan ying Liang, Guang bin Huang, P. Saratchandran, and N. Sundararajan. A Fast and Accurate Online Sequen- tial Learning Algorithm for Feedforward Networks. IEEE Transactions on Neural Networks, 17(6):1411 1423, November 2006. [7] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O. Hero III, and Pramod K. Varshney. A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning: Principals, Recent Advances, and Applications. IEEE Signal Processing Magazine, 37(5):43 54, September 2020. [8] Mineto Tsukada, Masaaki Kondo, and Hiroki Matsutani. A Neural Network-Based On-device Learning Anomaly Detector for Edge Devices. IEEE Transactions on Computers, 69(7):1027 1044, July 2020. [9] Yequan Zhao, Hai Li, Ian Young, and Zheng Zhang. Poor Man s Training on MCUs: A Memory-Efficient Quantized Back-Propagation-Free Approach. arXiv preprint arXiv:2411.05873, November 2024. [10] Haoyu Ren, Darko Anicic, and Thomas A. Runkler. TinyOL: TinyML with Online-Learning on Microcontrollers. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pages 1 8, July 2021. [11] Yann Lecun, L eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11):2278 2324, November 1998. [12] Javier Duarte, Song Han, Philip Harris, Sergo Jindariani, Edward Kreinar, Benjamin Kreis, Jennifer Ngadiuba, Maurizio Pierini, Ryan Rivera, Nhan Tran, and Zhenbin Wu. Fast Inference of Deep Neural Networks in FPGAs for Particle Physics. Journal of Instrumentation, 13(7):P07027, July 2018. 8\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.06505v1 [cs.LG] 6 Jun 2025 INSTANTFT: AN FPGA-BASED RUNTIME SUBSECOND FINE-TUNING OF CNN MODELS A PREPRINT Keisuke Sugiura University of Tsukuba 1-1-1 Tennôdai, Tsukuba, Ibaraki, Japan Hiroki Matsutani Keio University 3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan June 13, 2025 ABSTRACT Training deep neural networks (DNNs) requires significantly more computation and memory than inference, making runtime adaptation of DNNs challenging on resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for ultra-fast CNN fine-tuning on IoT devices, by op- timizing the forward and backward computations in parameter-efficient fine-tuning (PEFT). Ex- periments on datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches, while achieving compa- rable accuracy. Our FPGA-based InstantFT reduces the fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x, enabling on-the-fly adaptation of CNNs to non-stationary data distribu- tions. 1 Introduction On-device learning of DNNs is an effective solution to bridge the gap between training data and deployed environ- ment [1]. Since full retraining requires prohibitive costs, lightweight PEFT methods that only update a small set of parameters have been explored for resource-limited edge devices [2, 3]. LoRA (Low-Rank Adaptation) [4, 5] intro- duces trainable low-rank matrices while keeping the original pre-trained network fixed, allowing efficient adaptation to new tasks with minimal overhead. Instead of backpropagation and gradient descent, other approaches, e.g., Extreme Learning Machines (ELMs) [6] and zeroth-order optimization (ZOO) [7], have been applied to on-device learning. However, ELM is only applicable to tiny-scale models consisting of one hidden layer [8], while ZOO suffers from a slow convergence and long training time [9].\n\n--- Segment 2 ---\nInstead of backpropagation and gradient descent, other approaches, e.g., Extreme Learning Machines (ELMs) [6] and zeroth-order optimization (ZOO) [7], have been applied to on-device learning. However, ELM is only applicable to tiny-scale models consisting of one hidden layer [8], while ZOO suffers from a slow convergence and long training time [9]. This work presents InstantFT, an ultra-fast fine-tuning approach based on LoRA. As shown in Fig. 1, InstantFT runs significantly faster than the baseline LoRA approaches, without com- promising accuracy and robustness against data distribution shifts. 2 Baselines This section describes the baseline fine-tuning strategies. Table 1 presents a quantitative comparison between our InstantFT and baselines when applied to a LeNet-5-like model (Fig. 4) for two image classification datasets. We consider a simple L-layer network composed of fully-connected (FC) and convolution (Conv) layers, each having a weight and bias Wi, bi i 1,...,L. We denote the forward activation of the i-th layer by xi. The FC layer computes xi Wixi 1 bi, where xi 1, xi, bi Rd, Wi Rd d. The simplest approach is to fine-tune the entire network (FT-All) or only biases b1, . . . , bL (FT-Bias [10]). As illustrated in Fig. 2 (top right), another possible approach is to fine-tune parameters of the last layer (WL, bL) while keeping the rest frozen (FT-Last). While FT-Last requires significantly (854 1133x) less computational cost for backpropagation than FT-All, it only updates 1.4 of the pa- rameters and has a limited fine-tuning capability (Fig. 1). FT-Bias updates even fewer parameters than FT-Last, but it only reduces the backward FLOPs by 2.4 3.1x compared to FT-All, as it still requires gradients of forward activations dx.\n\n--- Segment 3 ---\n1). FT-Bias updates even fewer parameters than FT-Last, but it only reduces the backward FLOPs by 2.4 3.1x compared to FT-All, as it still requires gradients of forward activations dx. InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT 0 50 100 150 Fine-tuning Time (s) 50 55 60 65 70 75 80 Accuracy ( ) InstantFT (FPGA) (0.36s) InstantFT (6.2s) FT-Last (26.3s) LoRA-Last (26.2s) LoRA-All (140.5s) Figure 1: Fine-tuning time vs. accuracy of InstantFT and baselines (Rotated Fashion-MNIST, 75deg). Table 1: Comparison between InstantFT and baselines (LeNet-5) MNIST SVHN Method Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) FT-All 61706 0.851M 1.444M 567.8KB 62006 1.321M 1.914M 579.4KB FT-Last 850 0.851M 0.002M 285.8KB 850 1.321M 0.002M 296.1KB FT-Bias 236 0.851M 0.611M 322.0KB 236 1.321M 0.611M 332.3KB LoRA-All 36328 0.923M 0.743M 611.8KB 45480 1.412M 0.762M 695.4KB LoRA-Last 376 0.852M 0.001M 285.4KB 376 1.322M 0.001M 295.8KB InstantFT 10456 0.872M 0.036M 366.2KB 19608 1.360M 0.054M 449.8KB As in Fig. 2 (bottom), we consider two LoRA-based approaches: inserting LoRA adapters into all layers (LoRA-All) or applying LoRA to only the last layer (LoRA-Last).\n\n--- Segment 4 ---\nTable 1: Comparison between InstantFT and baselines (LeNet-5) MNIST SVHN Method Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) Trainable params FLOPs (forward) FLOPs (backward) Memory usage (KB) FT-All 61706 0.851M 1.444M 567.8KB 62006 1.321M 1.914M 579.4KB FT-Last 850 0.851M 0.002M 285.8KB 850 1.321M 0.002M 296.1KB FT-Bias 236 0.851M 0.611M 322.0KB 236 1.321M 0.611M 332.3KB LoRA-All 36328 0.923M 0.743M 611.8KB 45480 1.412M 0.762M 695.4KB LoRA-Last 376 0.852M 0.001M 285.4KB 376 1.322M 0.001M 295.8KB InstantFT 10456 0.872M 0.036M 366.2KB 19608 1.360M 0.054M 449.8KB As in Fig. 2 (bottom), we consider two LoRA-based approaches: inserting LoRA adapters into all layers (LoRA-All) or applying LoRA to only the last layer (LoRA-Last). For the i-th layer, LoRA introduces a new matrix Wi 1,i Rd d. It is further decomposed into two trainable matrices A Rr d, B Rd r of rank r d, which are initialized with random Gaussian values and zeros. The modified layer now produces: xi Wi Wi 1,i xi 1 bi Wi Bi 1,iAi 1,i xi 1 bi. The trainable parameters in LoRA-All and LoRA-Last are denoted as Ai 1,i, Bi 1,i i 1,...,L and AL 1,L, BL 1,L , respectively. LoRA-All performs backpropagation across the entire network, and therefore needs to store additional LoRA parameters, all forward activations x1, . . . , xL, and their gradients dx1, . . .\n\n--- Segment 5 ---\n. . , dxL, resulting in 7.7 20.0 higher memory consumption than FT-All. Similar to FT-Bias, LoRA-All still retains 39.8 51.5 of the Figure 2: Baseline fine-tuning methods and InstantFT. 2 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT Figure 3: Flow of fine-tuning with InstantFT. backward FLOPs of FT-All due to the computation of activation gradients. In contrast, LoRA-Last updates only the last layer using the activation xL, resulting in significantly reduced FLOPs and memory costs for backpropagation, at the cost of lower fine-tuning capability (Fig. 1). 3 InstantFT InstantFT achieves high fine-tuning accuracy as {FT, LoRA}-All, while maintaining low backward FLOPs and mem- ory usage similar to {FT, LoRA}-Last, thereby greatly improving the resource-accuracy trade-off. The algorithm flow is illustrated in Fig. 3. 3.1 Forward Pass Unlike the baselines, InstantFT introduces LoRA modules that connect each intermediate layer directly to the final layer, where we denote learnable parameters as Ai,L, Bi,L i 0,...,L 1 (Wi,L Ai,LBi,L). InstantFT produces an output as follows: xL WLxL 1 bL L 1 X i 0 Bi,LAi,Lxi ˆxL L 1 X i 0 xi. (1) The first term ˆxL is an output of the pre-trained base network, while the rest xi Bi,LAi,Lxi are obtained using LoRA adapters. Since the pre-trained network is fixed during fine-tuning, x1, . . . , xL 1 and ˆxL remain unchanged for the same input x0. InstantFT thus introduces a Forward Cache to store and reuse these intermediate results. As depicted in Fig. 3, InstantFT first checks that the entry for the input x0 (with an index j) is present in the cache.\n\n--- Segment 6 ---\nAs depicted in Fig. 3, InstantFT first checks that the entry for the input x0 (with an index j) is present in the cache. If so, InstantFT reuses the precomputed results and only performs the adapter part to compute P i xi; otherwise, InstantFT also runs a forward pass of the pre-trained network to obtain xi , ˆxL and stores them in the cache for reuse. As such, InstantFT performs a forward pass of the pre-trained network only once per input, and repeatedly uses LoRA adapters in later epochs. For a fixed dataset, the intermediate results for all samples are computed and cached in the first epoch. This significantly reduces the overall computational cost and training time, as the pre-trained network involves more computation than LoRA modules (e.g., 33.7 40.7x higher forward FLOPs in our case, Table 1). Additionally, InstantFT has 3.8 5.9 less forward FLOPs than LoRA-All, due to a lowered output dimension of LoRA modules. 3.2 Cache Quantization Since the Forward Cache stores forward activations for every input, its size grows with the dataset size, model depth, and output dimensions, resulting in a prohibitive memory overhead. To mitigate this, InstantFT adopts the 4-bit NormalFloat (NF4) quantization [5], which is well-suited for normally distributed parameters and activations. This reduces the cache size by approximately 8x compared to FP32. Note that the cached data is dequantized before used in the forward pass. 3 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT Figure 4: InstantFT for LeNet-5-like model.\n\n--- Segment 7 ---\nNote that the cached data is dequantized before used in the forward pass. 3 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT Figure 4: InstantFT for LeNet-5-like model. Conv-MP Conv-MP FC FC FC Params or Quant Dequant LConv LConv LFC LFC LFC Params BPLConv BPLConv BPLFC BPLFC BPLFC or Softmax CELoss AXI Interconnect S S S M 4GB 64-bit DDR4 Quad-core ARM Cortex-A53 (1.33GHz) HP0 HP1 HP2 HPM0 M M M S AXI AXI AXI AXI-Lite AXI Interrupt Controller M S irq gmem0 gmem1 gmem2 control InstantFT M S InstantFT S Signals M M AXI-Lite AXI-Lite Zynq PL (Programmable Logic) Zynq PS (Processing System) (3) (2) (1) Xilinx Kria KV260 Figure 5: InstantFT implemented on Xilinx Kria KV260. 3.3 Backward Pass In backpropagation, given an activation gradient dxi, the i-th layer computes gradients w.r.t. the parameters and input: dWi dxi(xi 1) dbi dxi dxi 1 (Wi) dxi. (2) In LoRA (with the main network frozen), gradients are propagated as: dBj,k dxk(hj,k) dhj,k (Bj,k) dxk dAj,k dhj,k(xj) dxj dxj (Aj,k) dhj,k, (3) where hj,k Aj,kxj. Given an output gradient dxL, InstantFT computes LoRA gradients n dAi,L, dBi,Lo i 0,...,L 1 following Eq. 3. Unlike dx, dW, which require quadratic computational com- plexity O(d2), LoRA gradients dA, dB have lower complexity of O(dr) (r d).\n\n--- Segment 8 ---\n3. Unlike dx, dW, which require quadratic computational com- plexity O(d2), LoRA gradients dA, dB have lower complexity of O(dr) (r d). InstantFT uses the same number of adapters as LoRA-All but involves 14.1 40.6x less backward FLOPs than {FT, LoRA}-All, since it neither computes dx, dW nor propagates dx through the entire network. Notably, InstantFT fine-tunes 12.3 52.1x more parameters with only 1.28 1.52x higher memory footprint compared to {FT, LoRA}-Last. InstantFT is likely to achieve better fine-tuning performance since trainable parameters are distributed across the network. 4 Implementations We use a LeNet-5-like network [11] (Fig. 4) as a backbone, where five LoRA adapters (r 4) are inserted into the Conv and FC layers for fine-tuning. Each adapter takes an activation xi Rcin from the previous layer and computes an update xi Bi,LAi,Lxi Rcout (Eq. 1) to the last layer output ˆxL. For Conv adapters, the input xi Rcin h w is first flattened into a vector vec(xi) Rcinhw before computation (Fig. 4 (left)). 4 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT LConv LConv LFC LFC LFC Add Conv-MP Conv-MP FC FC FC Dequant mux mux mux mux mux Quant !cached Params Params CELoss BPLConv Duplicate BPLConv BPLFC BPLFC BPLFC Params Softmax idx (1)(2) (3) Conv x LineBuf Window MaxPool 2x2 Compute outputs in parallel Compute outputs in parallel Conv-MP Mul Update Update BPLFC Mul Mul LFC Conv-MP BPLFC LFC Figure 6: Block diagram of InstantFT core. 4.1 InstantFT on FPGA We design a dedicated InstantFT core for embedded FPGAs. The core (Fig. 5) consists of three module groups and a set of on-chip buffers (for storing inputs, parameters, forward activations, and gradients).\n\n--- Segment 9 ---\nThe core (Fig. 5) consists of three module groups and a set of on-chip buffers (for storing inputs, parameters, forward activations, and gradients). The first two groups (Fig. 6, top left) perform the forward pass of the base network and adapters, where Conv-MP FC correspond to Conv- MaxPool FC layers, and LConv LFC are their respective LoRA adapters. Conv-MP (Fig. 6, top right) consists of a pipeline of two modules: Conv performs 2D convolution and stores the output pixels into a line buffer (implemented as a shift register), while MaxPool fetches 2x2 pixels from this buffer and performs 2D max-pooling. During fine-tuning, a mini-batch of samples and their corresponding indices are transferred to the core. InstantFT uses a Forward Cache placed on the external DRAM to avoid repeated forward passes of the base network for the same input. The cached forward activations x1, . . . , x4, ˆx5 are transferred to the on-chip buffers if available; otherwise, Conv-MP FC compute them and send to the cache. Then, LConv LFC compute deltas x1, . . . , x5 via matrix multiplications (Fig. 6, bottom center), which are summed together and added into ˆx5 to produce a final output x5. Dequant Quant handle the cache read write operations. Dequant reads 4-bit quantized cache entries for the input samples and writes them to respective buffers after dequantization, where indices are used to calculate memory ad- dresses and locate the entries. Quant performs the opposite. The probabilities p for each label are obtained from the logits x5 via softmax; we adopt precomputed lookup tables [12] instead of computing exponentials inverses to simplify the logic. Based on the predictions p and true labels t, the last group (Fig. 6, bottom left) performs backpropagation and gradient descent to update adapters. CELoss (cross-entropy loss) computes an output gradient dx5 and copies it to buffers that originally stored deltas xi. Then, given dx5, saved activations xj, hj,k Aj,kxj, and a learning rate η R , BPLConv BPLFC (Fig.\n\n--- Segment 10 ---\nCELoss (cross-entropy loss) computes an output gradient dx5 and copies it to buffers that originally stored deltas xi. Then, given dx5, saved activations xj, hj,k Aj,kxj, and a learning rate η R , BPLConv BPLFC (Fig. 6, bottom right) perform matrix operations to obtain parameter gradients dA, dB and update LoRA parameters via SGD (A A ηdA). To improve latency, these modules are parallelized through loop unrolling and buffer partitioning (e.g., Conv-MP computes output pixels for multiple samples and channels per clock cycle). Besides, the LoRA modules (LConv LFC and BPLConv BPLFC) are executed in parallel to simultaneously compute deltas gradients and update parameters at once. Importantly, InstantFT allows such an optimization because all LoRA adapters directly connect to the last layer. In contrast, LoRA-All requires LoRA adapters to be executed sequentially, as the gradient dx should be backpropa- gated through the entire network. We use Xilinx Kria KV260 running PetaLinux 2022.1 as a target platform (Fig. 5, right), which features a Zynq UltraScale MPSoC FPGA, a quad-core ARM Cortex-A53 CPU (1333MHz), and 4GB DDR4 DRAM. We use Xilinx 5 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT 15 30 45 60 75 90 Rotation Angle ( ) 70 80 90 Accuracy ( ) MNIST to RotMNIST 15 30 45 60 75 90 Rotation Angle ( ) 50 60 70 80 FMNIST to RotFMNIST Full 18 12 6 3 Samples (x104) 75 80 85 SVHN-Extra to SVHN w o Fine-tuning FT-Last LoRA-Last LoRA-All InstantFT w o Cache InstantFT w Cache (NF4) InstantFT (FPGA) (a) Accuracy.\n\n--- Segment 11 ---\n5, right), which features a Zynq UltraScale MPSoC FPGA, a quad-core ARM Cortex-A53 CPU (1333MHz), and 4GB DDR4 DRAM. We use Xilinx 5 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT 15 30 45 60 75 90 Rotation Angle ( ) 70 80 90 Accuracy ( ) MNIST to RotMNIST 15 30 45 60 75 90 Rotation Angle ( ) 50 60 70 80 FMNIST to RotFMNIST Full 18 12 6 3 Samples (x104) 75 80 85 SVHN-Extra to SVHN w o Fine-tuning FT-Last LoRA-Last LoRA-All InstantFT w o Cache InstantFT w Cache (NF4) InstantFT (FPGA) (a) Accuracy. 130 140 140.5s 0 20 Inference Only FT-Last LoRA-Last LoRA-All InstantFT w o InstantFT w (FP32) InstantFT w (NF4) 4.9s 26.3s 26.1s 29.1s 5.7s 6.2s RotMNIST Seconds 22000 21495.5s 0 2000 183.1s 2929.1s 2925.1s 3280.0s 669.1s 714.9s SVHN Seconds (b) Execution time. 0 50 100 Utilization ( ) LUT FF BRAM URAM DSP 48.71 25.38 62.50 71.88 37.18 (c) FPGA resource utilization. Figure 7: Evaluation of InstantFT and baselines. Vitis 2024.1 to implement the core in C C and generate an FPGA bitstream. The core along with two AXI IPs (an interrupt controller and interconnect) are clocked at 200MHz and implemented on PL (Programmable Logic). The core has 128-bit AXI manager interfaces connected to the HP (high-performance) ports for data transfer from to the DRAM on PS (Processing System), and a 32-bit AXI-Lite subordinate interface for control register accesses. The frozen network parameters and initial LoRA parameters are pre-loaded into on-chip buffers to minimize the DRAM accesses.\n\n--- Segment 12 ---\nThe core has 128-bit AXI manager interfaces connected to the HP (high-performance) ports for data transfer from to the DRAM on PS (Processing System), and a 32-bit AXI-Lite subordinate interface for control register accesses. The frozen network parameters and initial LoRA parameters are pre-loaded into on-chip buffers to minimize the DRAM accesses. We use Q8.16 fixed-point format to represent forward activations, and Q4.12 for parameters and gradients. The host code is developed in C C using XRT (Xilinx Runtime) library to communicate with the core. It transfers input data (samples labels indices) and the updated LoRA parameters via memory-mapped AXI interfaces. 5 Experimental Results 5.1 Experimental Setup We compare InstantFT with the baseline fine-tuning approaches (Fig. 2) under the following three cases: (1) The LeNet-5-like model (Fig. 4) is pre-trained on MNIST and then fine-tuned on its rotated version (RotMNIST). (2) Pre-trained on Fashion-MNIST (FMNIST) and fine-tuned on its rotated counterpart (RotFMNIST). (3) Pre-trained on SVHN-Extra and fine-tuned on the test split of SVHN. We randomly pick 1024 samples each from the test sets of {MNIST, FMNIST} and rotate them by θ: 15 , 30 , . . ., 90 to generate six variants of Rot{MNIST, FMNIST} datasets. In (3), we pre-train the network with different numbers of samples D : 30000, 60000, 120000, 180000, 521131. We simulate challenging fine-tuning tasks with greater data distribution shifts using a larger θ or smaller D . The model is pre-trained and fine-tuned for 10 epochs with a batch size of 20. The learning rate η is set to 0.1 and 0.025 in (1) (2) and (3). The code for pre-training and fine-tuning is developed in C and compiled using GCC 11.2.0 with -O3 flag. We use OpenMP to utilize all four CPU cores.\n\n--- Segment 13 ---\nThe code for pre-training and fine-tuning is developed in C and compiled using GCC 11.2.0 with -O3 flag. We use OpenMP to utilize all four CPU cores. 6 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT 5.2 Accuracy Fig. 7a shows the classification accuracy of InstantFT and baselines on three cases. We report the average of ten runs with different random seeds. Without fine-tuning, the accuracy drops sharply under challenging settings (larger rotation angles (Rot{MNIST, FMNIST}) or fewer pre-training samples (SVHN)). While fine-tuning the last layer recovers the accuracy to some extent, FT-Last LoRA-Last is still 0.04 12.2 0.5 20.3 less accurate on RotMNIST (6.5 13.6 9.8 22.6 , 0.2 4.5 0.2 4.9 less accurate on RotFMNIST, SVHN) compared to fine-tuning the entire network with LoRA adapters (LoRA-All). In contrast, InstantFT performs comparably or even better than LoRA-All; it maintains an accuracy of 91.8 77.9 79.0 on three cases under the most difficult settings (θ 90 , D 30000), while LoRA-All is 91.2 75.1 81.7 accurate. InstantFT shows a similar robustness to data distribution shifts as LoRA- All, with an accuracy drop of 5.2 2.8 9.4 (LoRA-All: 5.7 6.2 6.7 ). Besides, NF4 quantization of the forward cache only leads to a marginal accuracy loss of 0.09 0.4 . The FPGA implementation (red) attains accuracy within 3.2 of its software counterpart and remains robust under data distribution shifts. 5.3 Execution Time Fig. 7b shows the time required for 10-epoch fine-tuning on RotMNIST and SVHN, measured on an ARM Cortex- A53 CPU. Due to higher forward backward FLOPs, LoRA-All takes 5.3x more time to fine-tune all Conv FC layers on RotMNIST compared to only updating the last layer (FT-Last LoRA-Last).\n\n--- Segment 14 ---\n7b shows the time required for 10-epoch fine-tuning on RotMNIST and SVHN, measured on an ARM Cortex- A53 CPU. Due to higher forward backward FLOPs, LoRA-All takes 5.3x more time to fine-tune all Conv FC layers on RotMNIST compared to only updating the last layer (FT-Last LoRA-Last). While InstantFT employs the same number of LoRA adapters, it runs 4.8x faster than LoRA-All on RotMNIST, as it obviates the need to compute output gradients dxi (except dx5) and saves the computational cost for backpropagation by 20.9x. Forward Cache leads to further 5.1x speedup on RotMNIST, as forward activations of the base network are computed only once per input at the first epoch and then reused in the subsequent epochs. Notably, InstantFT performs fine-tuning with only a 17 increase in execution time compared to the inference-only case, and successfully addresses the discrepancy between training and testing data. While NF4 (de)quantization of cached activations introduces an additional 8.8 overhead, InstantFT still runs significantly faster than baselines. The results on SVHN show a similar trend except for the 38 153x longer execution time, which is mainly due to 72x larger dataset size (73257 1024 images). The FPGA implementation of InstantFT (with quantized cache) only takes 0.36s on Kria KV260 to fine-tune the model for 10 epochs, which is 17.4x faster than the CPU counterpart, thereby enabling on-the-fly adaptation of CNNs. For a fair comparison, the execution time includes various overheads such as PS PL data transfers and FPGA kernel invocations. 5.4 Power Consumption During fine-tuning, we measure the power consumption of KV260 every 50ms using an onboard INA260 sensor and compute its average value. While the FPGA-implemented InstantFT consumes 0.24W more power (3.79W 4.03W) compared to only using ARM Cortex-A53, it achieves 16.32x higher energy-efficiency thanks to the attained speedup. 5.5 Forward Cache Size The size of Forward Cache depends on both the number of samples in a given dataset and model architecture.\n\n--- Segment 15 ---\nWhile the FPGA-implemented InstantFT consumes 0.24W more power (3.79W 4.03W) compared to only using ARM Cortex-A53, it achieves 16.32x higher energy-efficiency thanks to the attained speedup. 5.5 Forward Cache Size The size of Forward Cache depends on both the number of samples in a given dataset and model architecture. On Rot{MNIST, FMNIST} and SVHN, the full-precision cache requires 7.33 524.52MB of memory, which is brought down to 1.02 72.89MB by NF4 quantization (7.20x reduction) without compromising accuracy (Fig. 7a). 5.6 FPGA Resource Utilization Fig. 7c shows the FPGA resource utilization of InstantFT on Kria KV260. The design effectively uses on-chip memory resources to store (frozen LoRA) parameters, activations, and gradients, thereby minimizing external memory accesses and improving overall performance. The on-chip memory can be further saved by applying low-bit quantization to parameters and gradients as well. In addition to LeNet-5, larger-scale networks can be fine-tuned on KV260, thanks to the low utilization of logic resources (LUTs DSPs). 6 Summary This paper presents InstantFT, a new ultra-fast fine-tuning method that enables edge applications to quickly adapt to data distribution shifts within subseconds. InstantFT leverages trainable adapters directly connected to the output layer, significantly saving the computational cost of backpropagation. In addition, it introduces 4-bit quantized Forward Cache to eliminate redundant forward passes of the frozen base network. The proposed FPGA-based design fully 7 InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models A PREPRINT exploits the high parallelism and simplified computation flow of the adapters. Experimental results demonstrate that InstantFT achieves accuracy comparable to the original LoRA in just 0.36s on Xilinx Kria KV260, while being 17.4x faster and 16.3x more energy-efficient than ARM Cortex-A53. While InstantFT is currently evaluated on small-scale networks, we aim to extend its application to large-scale models including LLMs.\n\n--- Segment 16 ---\nExperimental results demonstrate that InstantFT achieves accuracy comparable to the original LoRA in just 0.36s on Xilinx Kria KV260, while being 17.4x faster and 16.3x more energy-efficient than ARM Cortex-A53. While InstantFT is currently evaluated on small-scale networks, we aim to extend its application to large-scale models including LLMs. References [1] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), pages 11285 11297, December 2020. [2] Kazuki Sunaga, Masaaki Kondo, and Hiroki Matsutani. Addressing Gap between Training Data and Deployed Environment by On-Device Learning. IEEE Micro, 43(6):66 73, Nov Dec 2023. [3] Hiroki Matsutani, Masaaki Kondo, Kazuki Sunaga, and Radu Marculescu. Skip2-LoRA: A Lightweight On- device DNN Fine-tuning Method for Low-cost Edge Devices. In Proceedings of the Asia and South Pacific Design Automation Conference (ASP-DAC), pages 51 57, January 2025. [4] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations (ICLR), January 2022. [5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), pages 10088 10115, December 2023. [6] Nan ying Liang, Guang bin Huang, P. Saratchandran, and N. Sundararajan. A Fast and Accurate Online Sequen- tial Learning Algorithm for Feedforward Networks. IEEE Transactions on Neural Networks, 17(6):1411 1423, November 2006.\n\n--- Segment 17 ---\nA Fast and Accurate Online Sequen- tial Learning Algorithm for Feedforward Networks. IEEE Transactions on Neural Networks, 17(6):1411 1423, November 2006. [7] Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O. Hero III, and Pramod K. Varshney. A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning: Principals, Recent Advances, and Applications. IEEE Signal Processing Magazine, 37(5):43 54, September 2020. [8] Mineto Tsukada, Masaaki Kondo, and Hiroki Matsutani. A Neural Network-Based On-device Learning Anomaly Detector for Edge Devices. IEEE Transactions on Computers, 69(7):1027 1044, July 2020. [9] Yequan Zhao, Hai Li, Ian Young, and Zheng Zhang. Poor Man s Training on MCUs: A Memory-Efficient Quantized Back-Propagation-Free Approach. arXiv preprint arXiv:2411.05873, November 2024. [10] Haoyu Ren, Darko Anicic, and Thomas A. Runkler. TinyOL: TinyML with Online-Learning on Microcontrollers. In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pages 1 8, July 2021. [11] Yann Lecun, L eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11):2278 2324, November 1998. [12] Javier Duarte, Song Han, Philip Harris, Sergo Jindariani, Edward Kreinar, Benjamin Kreis, Jennifer Ngadiuba, Maurizio Pierini, Ryan Rivera, Nhan Tran, and Zhenbin Wu. Fast Inference of Deep Neural Networks in FPGAs for Particle Physics. Journal of Instrumentation, 13(7):P07027, July 2018. 8\n\n