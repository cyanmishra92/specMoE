=== ORIGINAL PDF: 2502.12444v1_SparAMX_Accelerating_Compressed_LLMs_Token_Generat.pdf ===\n\nRaw text length: 52774 characters\nCleaned text length: 52329 characters\nNumber of segments: 31\n\n=== CLEANED TEXT ===\n\nSPARAMX: ACCELERATING COMPRESSED LLMS TOKEN GENERATION ON AMX-POWERED CPUS Ahmed F. AbouElhamayed 1 Jordan Dotzel 1 Yash Akhauri 1 Chi-Chih Chang 1 Sameh Gobriel 2 J. Pablo Mu noz 2 Vui Seng Chua 2 Nilesh Jain 2 Mohamed S. Abdelfattah 1 ABSTRACT Large language models have high compute, latency, and memory requirements. While specialized accelerators such as GPUs and TPUs typically run these workloads, CPUs are more widely available and consume less energy. Accelerating LLMs with CPUs enables broader AI access at a lower cost and power consumption. This acceleration potential for CPUs is especially relevant during the memory-bound decoding stage of LLM inference, which processes one token at a time and is becoming increasingly utilized with reasoning models. We utilize Advanced Matrix Extensions (AMX) support on the latest Intel CPUs together with unstructured sparsity to achieve a 1.42 reduction in end-to-end latency compared to the current PyTorch implementation by applying our technique in linear layers. We provide a set of open-source customized sparse kernels that can speed up any PyTorch model by automatically replacing all linear layers with our custom sparse implementation. Furthermore, we demonstrate for the first time the use of unstructured sparsity in the attention computation achieving a 1.14 speedup over the current systems without compromising accuracy. Code: Aware-Automated-Machine-Learning tree main SparAMX 1 INTRODUCTION The usage of large language models (LLMs) has grown exponentially over the past few years and is expected to con- tinue its unprecedented growth. This has enabled many AI- driven applications, yet significant hardware and power re- sources are required, which have motivated recent attempts at model compression and acceleration. One such compres- sion method is unstructured pruning, which removes some model parameters without structural constraints like contigu- ous nonzero values. Although such a method can achieve high sparsity while maintaining accuracy, achieving actual speedup on current hardware, such as GPUs and TPUs, is challenging. This high-cost specialized hardware makes LLMs inacces- sible to many people and limits their use. CPUs, on the other hand, are more ubiquitous and therefore can be used to accelerate LLM-driven applications for a wider audience. Newer CPUs, such as the Intel Sapphire Rapids, contain units that natively accelerate matrix multiplication at low cost and power. For example, AMX units present in the Sapphire Rapids chip enable direct acceleration of LLM workloads on the CPU. We explore combining the capa- 1Cornell University, New York, USA 2Intel Labs, Oregon, USA. Correspondence to: Ahmed F. AbouEl- hamayed J. Pablo Mu noz Mohamed S. Abdelfattah mo- bilities of this unit with unstructured sparsity to accelerate LLMs on the CPU. As shown in Figure 1, our sparse AMX kernel, SparAMX, leads to faster decoding times across common LLMs compared to the stock PyTorch. In this work, we start with background material on sparsity and the hardware features used in our kernel, explore the cur- rent literature, and then introduce our detailed optimizations and design of kernels. We then demonstrate better efficiency over stock PyTorch that uses Intel s optimized libraries (In- tel, 2024) and other proprietary commercial solutions like DeepSparse (Neuralmagic, 2024) by optimizing the dom- inant matrix multiplication operations within the model s linear layers. Finally, we apply unstructured pruning to the KV cache for the first time and propose a kernel that makes use of the sparsity to optimize the attention operation as well to increase model performance. Our summarized contributions are as follows: An end-to-end system that uses unstructured sparsity to improve latency by up to 1.42 over the stock PyTorch on CPU. An INT8 CPU kernel that uses unstructured sparsity and AMX to achieve up to 1.46 better performance than current proprietary kernels (Neuralmagic, 2024) for quantized models. A novel study of unstructured sparsity in the KV cache achieving 1.14 speedup on 16K context with minimal accuracy loss. arXiv:2502.12444v1 [cs.LG] 18 Feb 2025 SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Phi-3-mini-4K Llama2-7B Mistral-7B Llama3-8B 1.0 1.1 1.2 1.3 1.4 1.5 Speedup (x) 1.08x 1.27x 1.30x 1.42x Figure 1. SparAMX Performance: SparAMX consistently leads to a speedup in decode latency over PyTorch. The improvement tends to be greater as the model size increases. Context length is set to 512. hidden_dim IN W OUT hidden_dim batch_size num_neurons batch_size hidden_dim num_neurons num_neurons batch_size Figure 2. Linear GEMM Mapping: A linear layer can be com- puted with matrix multiplication. In this example hidden dim 4, num neurons 3, and batch size 2. 2 BACKGROUND 2.1 LLM Inference LLM inference consists of two distinct stages with varying compute and memory requirements. During the first stage, known as prefill, all input tokens are processed at the same time, allowing significant memory reuse and making the operation compute-bound. However, this stage occurs only once per prompt. The needed results (keys and values of each token in each attention head in each layer) are stored in the KV cache for subsequent use during the second stage. The second stage, known as decode, performs auto- regressive sampling one token at a time. This token-by- token processing reduces memory reuse and compute inten- sity, making the decode stage memory-bound and bottle- necked by weight loading from memory in short contexts. This opens an opportunity to leverage unstructured spar- sity, which enables higher sparsity levels and more efficient compression to minimize memory transfer. Weights can be stored in a compressed format, retrieved with reduced memory overhead, and decompressed on the compute unit before performing the dense matrix multiplication. While this approach introduces some additional computa- tion, the process is memory-bound; thus, reducing memory transfer, even at the cost of extra computation, results in an overall speedup. We propose a generic system that applies this optimization and integrates seamlessly with PyTorch. Our approach demonstrates a significant speedup compared to the current PyTorch implementation. 2.2 DNN Sparsity In Deep Neural Networks (DNNs), some weights con- tribute more significantly to the model s performance than others. Prior research has extensively explored methods for identifying weight importance (Liu et al., 2023b; Akhauri et al., 2024). Introducing sparsity into the weights via prun- ing leverages this insight to reduce the size of the DNN while improving efficiency. Sparsity reduces memory trans- fer costs by avoiding the loading of insignificant weights and lowers compute requirements by skipping operations involving those weights. There are two types of sparsity: structured and unstruc- tured. Structured sparsity involves pruning weights in full blocks, such as a full row (mapping to a full neuron in Figure 2) or a full tile. This pattern allows for simpler acceleration since entire sets of weights can be skipped during loading and computation. Unstructured sparsity imposes no constraints on the patterns of pruned weights. This flexibility enables higher sparsity levels, but achieving actual speedup is more challenging because the irregular distribution of zeros requires full computation and weight loading unless some special handling is used. Structured sparsity typically results in lower sparsity levels because important weights often co-exist within the same structure as prunable weights, preventing their independent removal. In contrast, unstructured sparsity achieves greater sparsity due to its flexibility, making it a better approach for model compression in scenarios where lower data transfer is desired and where hardware can handle its computational challenges. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Linear 90.4 Others 9.6 8 Cores Linear 85.8 Others 14.2 16 Cores Linear 81.0 Others 19.0 32 Cores Figure 3. Inference Breakdown: Linear layers dominate the la- tency during LLM inference. Results profiled from Llama 3 8B running on Intel(R) Xeon(R) Gold 6430L CPU with 512 context length. 2.3 GEMM Mapping During both inference stages, the most compute-intensive operations in LLMs are mapped to matrix multiplication, also known as General Matrix Multiply (GEMM). These operations dominate the computational workload in both the core linear layers and attention mechanisms. Figure 3 shows that linear layers dominate the latency, especially at small contexts. Linear layers in LLMs can be represented as matrix mul- tiplications, as illustrated in Figure 2. In the example, the weight matrix (W) contains the weights, where each column corresponds to the weights of a single neuron. Each neuron uses its unique set of weights to process inputs. The input matrix (IN) contains the input values, with each row repre- senting an individual input. Each input undergoes identical processing, being multiplied by the same set of weights. The result of this computation is the output matrix (OUT). Each column in OUT corresponds to a neuron, with its rows containing the computed outputs for each input. The num- ber of rows is the same as the number of inputs as an output row is computed for each of the input rows. 2.4 Advanced ISA Extensions Our kernel is designed to utilize two specialized instruc- tion sets: AVX (Advanced Vector Extensions) and AMX (Advanced Matrix Extensions). AVX is a set of SIMD (Single Instruction, Multiple Data) instructions extending the x86 architecture, enabling parallel operations on data vectors. It uses special AVX registers to store arguments and outputs for these operations. This work focuses on AVX-512, which operates on 512-bit registers. For example, the instruction mm512 loadu si512 can load 512 bits of data from main memory into an AVX register. To perform a dot product, you can load elements from two vectors into AVX registers and use the mm512 dpbf16 ps instruction. This operation multiplies 32 pairs of 16-bit elements of the two registers, adds the results of each two consecutive elements together to form 16 32-bit elements which are accumulated in a third register. OUT IN W 0 1 2 3 ... 31 30 0 1 2 3 30 31 0 1 2 3 4 5 6 7 60616263 ... 0 1 2 3 4 5 6 7 60616263 0 0 ... 63 63 ... INT8 Mode BF16 Mode ... 0 0 ... 31 31 Figure 4. AMX Matrix Multiplication: AMX tiles support BF16 (blue) and INT8 (red) values. First two tiles contain the input matrix tiles, which are then multiplied and accumulated to the third tile. Building on the success of AVX, Intel introduced AMX in their Sapphire Rapids processor series. Unlike AVX, which focuses on vectorized operations, AMX provides specialized hardware acceleration for matrix multiplication. AMX introduces a set of tiled registers, which are two- dimensional and significantly larger than AVX registers (as shown in Figure 4). Each tile consists of up to 16 rows, with each row containing up to 512 bits, and there are eight tile registers in each AMX unit. These tiles support operations to zero all elements, load data from memory, store data into memory, and perform matrix multiplication. Matrix multiplication in AMX involves multiplying two tiles and accumulating the result in a third tile. These operations support two data formats: BF16 (Bfloat16): Each row can store up to 512 16 32 elements. Results are stored in FP32. INT8 (8-bit integers): Each row can store up to 512 8 64 elements. Results are stored in INT32. During matrix multiplication, each word from the first operand tile is multiplied by the corresponding word from the second operand tile, requiring special arrangement of items in the matrices to match the ordering shown in Fig- ure 4. 3 RELATED WORK Quantization Weight quantization in LLMs has been ex- tensively studied, achieving high model quality with as few as three or four bits (Lin et al., 2024; Frantar et al., 2022; Dettmers et al., 2022; Dotzel et al., 2024). Quantization of both weights and activations has also advanced signif- icantly, enabling four-bit computation while maintaining accuracy (Xiao et al., 2023a; Liu et al., 2023a). Given that trend, support for low-bit datatypes and opera- tions has been added in recent CPUs and GPUs. For exam- ple, Intel Sapphire Rapids CPUs support INT8 operations, and NVIDIA s H100 Tensor Cores support INT8 and INT4 SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs operations. To harness these features, specialized kernels have been developed, demonstrating up to a 3 speedup over full-precision models (Lin et al., 2024), with further optimizations achieving an additional 1.9 speedup (Kim et al., 2024b). Some kernels, such as (Shen et al., 2023) efficiently run 4-bit models on multiple Intel platforms, including AMX, but do not incorporate sparsity. Sparsity Like quantization, sparsity can be used to accel- erate LLM workloads, as introduced in Section 2.2. Many studies focus on static structured sparsity (Ma et al., 2023; Dery et al., 2024), which removes entire components, of- ten yielding strong speedups at the cost of accuracy. For instance, LLM-Pruner (Ma et al., 2023) performs one-shot structured pruning using gradient information and reaches only 20 sparsity with an extra perplexity of 3.6 points on WikiText-2 (Merity et al., 2016). Other works leverage dy- namic structured sparsity to optimize the runtime of LLMs on the GPU (Liu et al., 2023b; Akhauri et al., 2024). For unstructured sparsity, techniques such as SparseGPT (Frantar Alistarh, 2023) and Wanda (Sun et al., 2024) achieve up to 50 pruning. However, these methods often incur accuracy losses for example, 3.47 for SparseGPT on Llama 2 7B. Wanda s semi-structured variant suffers even greater perplexity degradation ( 2.69) compared to its unstructured counterpart ( 1.01) (Zhu et al., 2023). Some techniques like Shears (Mu noz et al., 2024) can recover some of the lost accuracy with additional fine-tuning. As wall-time acceleration of unstructured sparsity can be difficult, Flash-LLM (Xia et al., 2023) proposes a GPU kernel that makes use of unstructured sparsity to minimize memory transfer requirements by cleverly using the general purpose GPU streaming multi-processors and tensor cores, while Marlin (Frantar et al., 2024) develops a GPU kernel that makes use of both quantization and semi-structured sparsity to improve performance. While most prior works focus on GPUs, recent re- search has explored unstructured sparsity on CPUs. SparseDNN (Wang, 2021) uses static code generation to process only non-zero elements efficiently. DeepSparse En- gine (Neuralmagic, 2024) applies unstructured sparsity with additional optimizations to accelerate LLMs, though it does not leverage AMX units and is closed-source. Our work inte- grates unstructured sparsity with AMX on Sapphire Rapids CPUs to improve LLM performance. Furthermore, we ex- tend the unstructured sparsity to compress the KV cache and show 1.14 speedup using our kernel in the attention mechanism. Some recent work showed that utilizing a CPU with AMX can help optimize the performance of a CPU- GPU system (Kim et al., 2024a). Our work aims to make running operations utilizing AMX even faster. 4 KERNEL DESIGN We design our kernels as PyTorch C extensions, accom- panied by Python classes, enabling seamless replacement of layers in arbitrary PyTorch models. These kernels are general-purpose and do not assume or optimize for a spe- cific sparsity pattern. Consequently, the achieved speedup depends on the sparsity percentage of the model. We begin by introducing a dense kernel that performs stan- dard GEMM operations in BF16 using AMX. We then ex- tend this kernel to incorporate unstructured sparsity, adapt- ing it to improve performance under sparse conditions. To evaluate performance gains of AMX, we also implement the sparse kernel using AVX and use it for comparisons. Finally, we present the quantized INT8 kernels, designed to leverage low-bit computation for further optimization. 4.1 Dense Kernel We begin by developing a kernel for linear layer compu- tation using AMX, without incorporating any custom op- timizations. This kernel, referred to as the dense kernel, assumes a fully dense model with no sparsity-related modi- fications, as illustrated in Figure 5. The AMX unit in each core can hold up to eight distinct tiles simultaneously. In our design: Tiles 0-3 are utilized to store intermediate results, which remain in the AMX unit during iteration over the inner dimension. During each iteration, we load two input tiles (Tiles 4 and 5) and two weight tiles (Tiles 6 and 7), compute the matrix multiplication, and accumulate the results in the four result tiles. Upon completing the inner dimension loop, the four result tiles are saved to memory, and the next set of four result tiles is initialized. By leveraging all eight tiles, instead of the naive approach of using one tile for result and two for operands, we achieve a compute-to-load ratio of 1:1, improving significantly over the 1:2 ratio that results from computing one tile at a time after loading two. Next, we parallelize the operations in the kernel. Since each input row (representing a different input token) and output column (representing a neuron) is independent, par- allelization can be applied over out rows and out cols. In the decoding stage, a single batch contains only one out- put row. On the other hand, out cols is input-independent and layer-dependent, making it the preferred dimension for parallelization. For smaller models, where out cols is less than 32 number of available threads (since two tiles are SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs out_rows inner_dim out_cols out_cols W IN OUT 4 5 6 7 inner_dim out_rows out_rows inner_dim out_cols out_cols W IN OUT inner_dim out_rows 4 5 6 7 0 1 2 3 0 out_rows inner_dim out_cols out_cols W IN OUT 6 7 inner_dim out_rows 1 2 3 0 out_rows inner_dim out_cols out_cols W IN OUT 4 6 7 inner_dim out_rows 1 1 2 3 4 1 0 2 3 4 5 6 7 1 0 2 3 4 5 6 7 0 1 2 3 1 0 2 3 4 6 7 0 1 0 11 0 Figure 5. AMX Dense Kernel: (1) Tiles 0, 1, 2, and 3 act as accumulators for the results computed by multiplying (4x6), (4x7), (5x6), and (5x7) respectively. (2) Tiles 4 and 5 are utilized to load all columns of the input rows (denoted as out rows) while tiles 6 and 7 are utilized to load all rows of the weight columns. (3) After the loop over the inner dimension ends, results are stored in memory and a new set of result tiles is initialized and computed in the same way. (4) Some boundary conditions might occur near the end of the matrix. Table 1. Compute Analysis: Percentage of pipeline slots that were memory bound (waiting on any memory access, including caches) and DRAM Bound (missed the cache and waiting for main memory access). Evaluated on 32 consecutive linear layers with 4192 inputs and 14336 outputs to mimic Llama 3 8B up proj layer. Kernel Memory Bound ( ) DRAM Bound ( ) Dense 100 87.5 Sparse 21.1 5.7 processed at a time, each with 16 columns), parallelizing over out rows as well can be beneficial. Parallelization over the inner dimension is another option, but it requires combining partial results from multiple result tiles, adding computational and memory overheads. This approach is only effective when both out cols and out rows are small and do not fully utilize all available threads. 4.2 Sparse Format While the dense kernel leverages AMX efficiently, the anal- ysis in Table 1 highlights further opportunities for perfor- mance optimization. Using the VTune Profiler to profile a Llama 3 model layer repeated 32 times, we observe that most pipeline slots are memory-bound, with nearly 50 of the time spent waiting on slow DRAM access. To ad- dress this bottleneck, we reduce memory transfer by storing weights in a compressed format and decompressing each tile only when needed for computation. Typically, weights, whether zero or non-zero, use the same number of bits. To save memory and bandwidth, we employ a compressed format where zero weights are represented with a single bit, while non-zero weights require an addi- tional bit. Before computation, these values are converted back to their original format. Although this approach incurs 1 0 0 1 1 0 weight_metadata weight_values 0 0 Dense Representation Sparse Representation Figure 6. Sparse Format: Weights are stored with a sparse format that stores only the non-zero weights and an associated bitmap to indicate corresponding weight indices. some computational overhead during format conversion, it significantly reduces memory transfer, making it highly effective for the memory-bound decode phase. This compressed weight format is illustrated in Figure 6. The representation is divided into two components: weight metadata: A bitmap where 1 indicates non- zero weights and 0 represents zero weights. weight values: A list containing all non-zero weights in the order required for processing. We select this method for its simplicity and because modern hardware supports efficient extraction from this format. 4.3 AMX Sparse Kernel Parallelizing over weights compressed in this format is challenging due to its unstructured nature. In a multi- threaded program, the access points for each thread within weight values are not predetermined. To address this, we introduce a precomputed index list, weight value index, generated during model initialization. This list specifies the starting position for each thread within weight values, as illustrated in Figure 9. Consequently, the number of threads must remain fixed during initialization, introducing a one- SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs CPU weight_buffer AVX Register Stack AVX Register weight_values AMX Weight Tile Register Memory weight_metadata Load metadata into AVX register Store local variable in stack Load weight row into a buffer Load weight tile into AMX 2 1 3 4 5 Decompress weights into AVX register Figure 7. Decompression Process: To construct a single weight tile in case of BF16 computation, (1) 16 32-bit metadata elements are fetched into an AVX register. (2) The values in the local AVX register are stored in the stack and (3) accessed individually to guide the decompression into another AVX register. (4) The data in the register is stored in the weight buffer. (5) After all 16 weight rows are stored in the buffer, the data in the buffer is loaded into the AMX tile. See Appendix A for algorithm version. time compute overhead during model loading. However, this overhead occurs only once, and the runtime memory overhead is minimal, as it requires storing just one index per thread so it is practical for real-world application. The kernel computation follows the dense kernel in Sec- tion 4.1, with the primary difference being that weights are not directly loaded into AMX registers because they are stored in a compressed format. Instead, the weightmetadata items corresponding to the current weight tile are first loaded into an AVX register using the vmovdqu32 operation as shown in (Figure 7), which loads 512 bits of data as 16 groups of 32-bit elements. Each 32-bit element, along with a pointer to the index of the first non- consumed weight values item (weight value index), serves as input to the vpexpandw operation. This oper- ation expands the bitmask into individual weights, placing zeros in positions marked by 0s in weightmetadata and loading the corresponding weights for positions marked by 1s. With a tile size of 16 rows by 32 columns, each 32-bit ele- ment expands to fill a row, and vpexpandw is applied 16 times to populate the entire weight tile. The result of each vpexpandw is stored temporarily in memory because direct transfers from AVX to AMX registers are currently unsup- ported. Instead, the complete tile is stored in memory and then loaded into AMX registers via an AMX load operation. Although this process involves memory access, frequent reuse of this memory region likely ensures it remains in the cache, mitigating expensive memory operations. To update the weight value index pointer, we utilize the popcount operation. For efficient instruction-level paral- lelism, each of the 16 loop iterations is kept independent. The offset for each weight row is computed using vpopcntd, Algorithm 1 Parallel Prefix Sum with AVX-512 Intrinsics Input: AVX Register v [v0, v1, . . . , v15] of 16 32-bit integers Output: AVX Register s [s0, s1, . . . , s15] where si Pi j 0 vj s v Shift(v, 1) Shift right 1 element and add s s Shift(s, 2) Shift right 2 elements and add s s Shift(s, 4) Shift right 4 elements and add s s Shift(s, 8) Shift right 8 elements and add return s which calculates a popcount on each of the 16 32-bit ele- ments and stores the results in a new AVX register. Finally, a parallel prefix sum operation, as described in Algorithm 1, computes the final offset needed for each tile row. 4.4 AVX Sparse Kernel During the decode phase with batch size 1, only one row of the 16-row AMX tile used for input is utilized, leading to significant inefficiency. To address this, we implement the compression technique using only AVX instructions, en- abling a performance comparison between AVX and AMX while ensuring compatibility with older CPUs lacking AMX support. The flow is illustrated in Figure 8. In this setup: An AVX register holds the weights for multiple neurons in the inner dimension. Another AVX register holds the corresponding input value repeated across the register. A third AVX register accumulates results for multiple neurons. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs 1 1 2 IN OUT W 1 inner_dim out_cols out_cols 1 inner_dim Single value fills all items of AVX register Different AVX Registers 2 1 3 Loop through inner_dim Repeat for next neurons 0 0 W thr0 thr1 1 It is possible to use more AVX Registers to compute multiple neurons outputs using a single input load 0 2 2 3 2 Figure 8. AVX Matrix Multiplication: Weights for different neurons are loaded into AVX register 1 and a single element of the input is fetched and used to fill all elements of AVX register 2. AVX register 0 is initialized to 0 to store the result, AVX registers 1 and 2 are multiplied and the results are added to the elements in AVX register 0. The process is repeated along the hidden dimension till all weights of the considered neurons are multiplied by the corresponding parts of the input. Another set of neurons are then processed. W thr0 thr1 thr2 thr3 weight_values weight_value_index thr0 thr1 thr2 thr3 Figure 9. Sparse Kernel Parallelization: Parallelization over threads requires each thread to know where to begin processing the weights. The index list weight value index stores this infor- mation and is computed offline allowing efficient parallelization at run time. Further implementation details are provided in Appendix B. 4.5 INT8 Kernels AMX supports both BF16 and INT8 operations. We de- velop a kernel for INT8 matrix multiplication and adapt our framework to quantize weights and activations for com- patibility. The flow of the INT8 kernel mirrors that of the AMX dense and sparse kernels, with adjustments for 8-bit elements instead of 16-bit. Each weight tile now contains 16 64 1024 weights, and their metadata is fetched into two AVX registers, each covering eight rows of the tile. Additionally, the weight ordering during preprocessing is modified to divide each weight column into four segments rather than two. 30 40 50 60 70 Accuracy ( ) 1.0 1.1 1.2 1.3 1.4 1.5 Speedup (x) Model Llama-3 8B Mistral-7B phi-3 Figure 10. Speedup vs. Accuracy. Different points are from different sparsity percentages. 5 RESULTS We use stock PyTorch running in dense format1 as the base- line to ensure a fair comparison and consistency across all other operations, especially since it utilizes AMX when available. Table 2 presents the latencies observed for layer 5 of Llama 3 8B when run using stock PyTorch versus our custom sparse kernel. Our kernel outperforms PyTorch across all projections, with performance improvements rang- ing from 1.22 in the most time-consuming projection to 2.03 in the least time-consuming projection. Figure 10 illustrates the tradeoff between end-to-end speedup and ac- curacy on GSM8K(Cobbe et al., 2021) for multiple models adopted from SQFT(Munoz et al., 2024). For end-to-end speedup, we evaluate the full Llama3 8B 1PyTorch has a beta API for sparse tensors, but we could not run our model in BF16 or FP16 using it because some operations are not implemented for these formats yet. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Table 2. Speedup in latency of the individual linear modules of layer 5 in Llama 3 8B Name Dimensions Speedup ( ) q proj 4096 4096 1.44 k proj 4096 1024 2.03 v proj 4096 1024 1.41 o proj 4096 4096 1.3 gate proj 4096 14336 1.26 up proj 4096 14336 1.22 down proj 14336 4096 1.36 model using our AVX and AMX sparse kernels against stock PyTorch. Figure 11 demonstrates how performance scales with sparsity across different numbers of CPU cores. Both AMX and AVX sparse kernels achieve a speedup compared to the stock PyTorch as sparsity increases. The gap between AMX and AVX decreases as the number of cores increases, likely due to reduced cache contention, which decreases memory access bottlenecks for AMX weight construction. The advantage of AMX becomes apparent at higher batch sizes. Figure 12 shows performance across various batch sizes comparing decoding throughput of the stock PyTorch and our AMX kernels to the throughput of our AVX kernel. The Llama3 8B checkpoint was taken from Shears (Mu noz et al., 2024), which maintains accuracy with 50 unstruc- tured sparsity. The results indicate that the two AMX kernels achieve higher throughput as batch size increases than the AVX kernel that is designed for vector multiplication rather than matrix multiplication for which AMX is designed. We also compare our kernel to the proprietary DeepSparse engine (Neuralmagic, 2024) and llama.cpp (Gerganov, 2024) in Figure 13. While DeepSparse employs optimiza- tions beyond unstructured sparsity, it is limited to specific models. Our kernel is general-purpose and compatible with any PyTorch model, but the latest Llama model that DeepSparse supports at the time of writing is INT8 Llama 2 7B so we use that model for this comparison. For a fair comparison, we use our INT8 kernel, and to minimize the effect of attention, which is irrelevant, we set the context length to 22. The results indicate that our kernel outperforms DeepSparse and llama.cpp at higher batch sizes. This ad- vantage is primarily due to AMX and its ability to perform matrix-matrix multiply rather than vector-vector multiply. Other runtimes like OpenVINO, which also leverage AMX, achieve higher performance but employ additional optimiza- tions, such as operator fusion, which are outside the scope of our framework. 2DeepSparse benchmarking tool does not work out of the box with context length 1. 6 ATTENTION KERNEL In the previous sections, we focused on linear layers. Here, we explore the potential of applying unstructured sparsity to attention computations. During attention, the incoming query value is multiplied by the cached K values, followed by a softmax operation, and the resulting values are mul- tiplied by the cached V values. The cached K and V ma- trices can be treated as weight matrices and sparsified in an unstructured manner. We first evaluate the impact of unstructured sparsity on accuracy and then adapt the kernel to accelerate the matrix multiplication workload of attention, demonstrating the resulting speedup. 6.1 Sparsity in the KV Cache Various methods have been proposed to optimize the KV cache, such as dropping certain tokens (Xiao et al., 2023b; Zhang et al., 2023), clustering tokens (Zandieh et al., 2024), or applying channel sparsity (Xu et al., 2024). In our ap- proach, we apply unstructured sparsity to the KV values using magnitude-based pruning, where values with the low- est magnitudes are dropped within each layer. Figure 14 shows the accuracy drop across downstream tasks (calculated as the geometric mean of accuracies for PIQA (Bisk et al., 2020), ARC (Easy Challenge)(Clark et al., 2018), BoolQ(Clark et al., 2019), HellaSwag (Zellers et al., 2019), and WinoGrande (Sakaguchi et al., 2021)). The drop in accuracy is less than 1 with 30 K pruning and 50 V pruning. 6.2 Acceleration The sparse kernel introduced in Section 4.3 is adapted here for attention computations. Within attention, matrix multi- plication is required for computing QK and RV (where R is the result of the scaled softmax of QK). This operation is a batched matrix multiplication with an additional head dimension. The sparse kernel is modified to handle this op- eration, leveraging the independence of heads to parallelize across them. To manage the KV cache efficiently, we modify the atten- tion code to initialize an empty cache after prefill, storing all previously cached values in the model state similar to how weights are stored. PyTorch s native functions for up- dating the cache and its repeat kv function (used in Llama 3 8B s Grouped Query Attention (Ainslie et al., 2023)) incur significant overhead with large KV caches due to memory reallocation for each new token. By replacing the cached tokens with our sparse format, which maintains a constant size within the model state, and saving new tokens in a sep- arate dynamic set, decoding becomes over 6 faster. This enables efficient decoding at high context lengths (e.g., 16K) on CPUs, allowing queries on long contexts with reasonable response times. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs 40 50 60 70 Sparsity ( ) 1.0 1.2 1.4 1.6 Speedup (x) Number of Cores: 8 Mode AMX AVX 40 50 60 70 Sparsity ( ) Number of Cores: 16 40 50 60 70 Sparsity ( ) Number of Cores: 32 Figure 11. Speedup over Stock PyTorch vs. Sparsity: Llama3 8B decoding with context length 512. The sparse kernels with AVX and AMX have better performance compared to stock PyTorch across the number of cores. As sparsity increases, speedup increases. 1 2 4 8 16 32 Batch Size 2 4 6 8 10 12 Relative Throughput (vs. AVX) Mode AMX AVX Stock PyTorch Figure 12. Batched Decoding: AMX is superior to AVX, and our kernel achieves 20.8 better performance than stock PyTorch at batch size 32. For a fair comparison, we benchmark our sparse kernel against the dense kernel, as shown in Figure 15. Since the dense kernel performs similarly to the stock PyTorch during the decoding stage, this comparison isolates the effect of sparsity. The results show that decode latency improves as the sparsity percentage of K and V increases. At a set- ting with less than 1 accuracy loss, we observe a 1.14 improvement in latency over the dense kernel. 7 DISCUSSION The presented kernels demonstrate that unstructured sparsity can achieve considerable speedup in linear layers. Notably, computations are still performed using dense weights; the speedup is realized by reducing memory transfer in a load- as-sparse, compute-as-dense approach (Xia et al., 2023). While this requires additional computation to reconstruct dense weights from the compressed format, the approach is particularly effective in memory-bound scenarios. Con- versely, in compute-bound scenarios, applying unstructured sparsity may reduce performance, as confirmed by our re- sults. 16 32 64 128 256 Batch Size 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 Relative Throughput (vs. llama.cpp) Mode Dense(INT8) Sparse(INT8) DeepSparse llama.cpp Figure 13. INT8 Kernels: Decoding throughput comparison be- tween our AMX INT8 dense kernel, sparse kernel, DeepSparse and llama.cpp with context length 2 running on 32 CPU cores for Llama 3 7B model with 50 sparsity. AMX allows us to outper- form DeepSparse and llama.cpp at high batch sizes. We also show that unstructured sparsity can complement other compression techniques, such as quantization. For instance, SQFT (Munoz et al., 2024) achieves comparable sparsity levels with INT4 quantization while maintaining accuracy. As shown in Figure 13, our sparse INT8 kernel outperforms its dense counterpart in memory-bound sce- narios. However, at higher batch sizes, the dense kernel performs better as the system shifts to a compute-bound regime. AMX enables efficient inference at high batch sizes, sur- passing AVX implementations, including highly optimized proprietary solutions like DeepSparse. However, at lower batch sizes, AMX and AVX performance are similar, with AVX sometimes outperforming AMX. This is because our SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs 0 10 20 30 40 50 Sparsity Percentage of K 0 10 20 30 40 50 Sparsity Percentage of V 71.0 71.5 72.0 72.5 73.0 73.5 74.0 Accuracy ( ) Figure 14. KV Sparsity Accuracy Downstream Accuracy on mul- tiple one-shot tasks for different percentages of KV pruning. Accu- racy change is less than 1 at 30 K Sparsity and 50 V Sparsity. 0 10 20 30 40 50 Sparsity Percentage of K 0 10 20 30 40 50 Sparsity Percentage of V 1.000 1.025 1.050 1.075 1.100 1.125 1.150 Speedup ( ) Figure 15. KV Sparsity Performance: End-to-end decode latency speedup when using the sparse kernel for different percentages of KV pruning. Baseline is the dense kernel latency. We are able to achieve 1.14 speedup with minimal accuracy loss. decompression approach relies on expanding weight meta- data into an AVX register, which must then be stored to memory before loading into AMX tile registers due to cur- rent architectural limitations. This extra step incurs addi- tional memory operations. Caching mitigates this issue by assuming the weight tile remains in the cache. While our results validate this assumption in most cases, ineffi- ciencies arise potentially because data movement between memory and cache reduces the intended bandwidth savings from compression. Addressing this limitation would require direct data transfer from AVX registers to AMX tiles or instructions to expand bit-masked data directly into AMX tiles. Given the efficiency of AMX for high-batch inference, Sap- phire Rapids CPUs can now feasibly serve certain LLMs without requiring GPUs, potentially reducing cost and power consumption. However, this method requires offline preprocessing to organize weights into the compressed for- mat, including weight metadata, weight values, and thread start indices. Changing the number of threads necessitates recomputation of this representation. Additionally, the at- tention kernel is not suitable for dynamic KV values but re- mains effective for cached prompts, such as system prompts or static knowledge bases. Decoding at long context lengths is highly inefficient in PyTorch, becoming impractical at 16K context length. As discussed in Section 6.2, decoding can be efficiently per- formed at this length for precomputed cached tokens. For example, companies using CPU-only devices can preload their knowledge base as a cached context in an LLM and direct customer queries to it for faster responses, saving both cost and power. 8 LIMITATIONS Our system has several limitations. First, it requires prepro- cessing time, which, although only a few minutes for 8B models, makes it unsuitable for accelerating dynamically generated sparsity, such as activation sparsity. Additionally, our system currently supports only INT8 and BF16 formats, as AMX units do not natively support more compressed formats like INT4. Extending support to INT4 is feasible by dequantizing INT4 values into INT8 before computation. While our system supports any PyTorch model and serves as a useful tool for measuring the speedup of unstructured sparsity techniques, other systems, such as OpenVINO, achieve better results by incorporating additional optimiza- tions, such as operation fusion, which are not included in our system. These optimizations make OpenVINO more suitable for production use. 9 CONCLUSION In this paper, we introduced a system that leverages un- structured sparsity to achieve significant speedup in the decode stage compared to current implementations. Our system is general-purpose, compatible with any PyTorch model, and runs out of the box. It delivers a 1.42 perfor- mance improvement over the current PyTorch version for Llama 3 8B and, at high batch sizes, achieves over 1.4 higher throughput compared to proprietary systems like DeepSparse. Additionally, we demonstrate the potential of using unstructured sparsity in the KV cache to reduce latency per token, achieving a 1.14 speedup with simple magnitude pruning. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs REFERENCES Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head check- points. In Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing, pp. 4895 4901, 2023. Akhauri, Y., AbouElhamayed, A. F., Dotzel, J., Zhang, Z., Rush, A. M., Huda, S., and Abdelfattah, M. S. Shad- owllm: Predictor-based contextual sparsity for large lan- guage models. EMNLP, 2024. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Pro- ceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432 7439, 2020. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pp. 2924 2936, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dery, L., Kolawole, S., Kagey, J.-F., Smith, V., Neubig, G., and Talwalkar, A. Everybody prune now: Structured pruning of llms with only forward passes. arXiv preprint arXiv:2402.05406, 2024. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318 30332, 2022. Dotzel, J., Chen, Y., Kotb, B., Prasad, S., Wu, G., Li, S., Abdelfattah, M. S., and Zhang, Z. Learning from students: Applying t-distributions to explore accurate and efficient formats for llms. ICML, 2024. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in one-shot. In Interna- tional Conference on Machine Learning. PMLR, 2023. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre- trained transformers. arXiv preprint arXiv:2210.17323, 2022. Frantar, E., Castro, R. L., Chen, J., Hoefler, T., and Alis- tarh, D. Marlin: Mixed-precision auto-regressive paral- lel inference on large language models. arXiv preprint arXiv:2408.11743, 2024. Gerganov, G. llama.cpp, 2024. URL com ggerganov llama.cpp. Intel. Intel oneapi deep neural network library, 2024. URL en developer tools oneapi onednn.html. Kim, H., Ye, G., Wang, N., Yazdanbakhsh, A., and Kim, N. S. Exploiting intel advanced matrix extensions (amx) for large language model inference. IEEE Computer Architecture Letters, 23(1):117 120, 2024a. doi: 10. 1109 LCA.2024.3397747. Kim, T., Lee, J., Ahn, D., Kim, S., Choi, J., Kim, M., and Kim, H. Quick: Quantization-aware interleaving and conflict-free kernel for efficient llm inference, 2024b. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87 100, 2024. Liu, S.-y., Liu, Z., Huang, X., Dong, P., and Cheng, K.-T. LLM-FP4: 4-bit floating-point quantized transformers. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceed- ings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 592 605, Singapore, December 2023a. Association for Computational Linguis- tics. URL emnlp-main.39. Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pp. 22137 22176. PMLR, 2023b. Ma, X., Fang, G., and Wang, X. Llm-pruner: On the struc- tural pruning of large language models. Advances in Neu- ral Information Processing Systems, 36:21702 21720, 2023. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Mu noz, J. P., Yuan, J., and Jain, N. Shears: Unstructured sparsity with neural low-rank adapter search. In Pro- ceedings of the 2024 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pp. 395 405, Mexico City, Mex- ico, June 2024. Association for Computational Linguis- tics. URL naacl-industry.34. Munoz, J. P., Yuan, J., and Jain, N. SQFT: Low-cost model adaptation in low-precision sparse foundation models. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Findings of the Association for Computational Linguis- tics: EMNLP 2024, pp. 12817 12832, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653 v1 2024.findings-emnlp. 749. URL findings-emnlp.749 . Neuralmagic. Neuralmagic deepsparse: Sparsity-aware deep learning inference runtime for cpus, 2024. URL deepsparse. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99 106, 2021. Shen, H., Chang, H., Dong, B., Luo, Y., and Meng, H. Efficient llm inference on cpus. arXiv preprint arXiv:2311.00502, 2023. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. Wang, Z. Sparsednn: Fast sparse deep learning inference on cpus. arXiv preprint arXiv:2101.07948, 2021. Xia, H., Zheng, Z., Li, Y., Zhuang, D., Zhou, Z., Qiu, X., Li, Y., Lin, W., and Song, S. L. Flash-llm: En- abling cost-effective and highly-efficient large genera- tive model inference with unstructured sparsity. arXiv preprint arXiv:2309.10285, 2023. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 38087 38099. PMLR, 2023a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef- ficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023b. Xu, Y., Jie, Z., Dong, H., Wang, L., Lu, X., Zhou, A., Saha, A., Xiong, C., and Sahoo, D. Think: Thinner key cache by query-driven pruning. arXiv preprint arXiv:2407.21018, 2024. Zandieh, A., Han, I., Mirrokni, V., and Karbasi, A. Subgen: Token generation in sublinear time and memory. arXiv preprint arXiv:2402.06082, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pp. 4791 4800, 2019. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., R e, C., Barrett, C., et al. H2o: heavy-hitter oracle for efficient generative inference of large language models. In Proceedings of the 37th Inter- national Conference on Neural Information Processing Systems, 2023. Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey on model compression for large language models. Transac- tions from the Association of Computational Linguistics, 2023. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs A SPARSE WEIGHTS DECOMPRESSION PROCESS We show the steps used to perform weight decompression in Algorithm 2. Algorithm 2 Processing Sparse Format in the Sparse Kernel Input: weight metadata: Bitmap indicating non-zero weights weight values: Array of non-zero weight values weight values i: Pointer to the current index in weight values for the next non-zero weight weight tile mem: Memory region for storing weight values Output: weight tile: AMX tile filled with expanded weights Initialization: Fetch 512 bits (16 rows 32 bits per row) of weight metadata into AVX register weight metadata local. Compute prefix sum to get pop count of each element in weight metadata local into AVX register weight metadata local popcounts. Loop through all tile rows: for row index 0 to 15 do Step 1: Expand the current row Use vpexpandw with row index from weight metadata local and weight values[weight values i] to get 32 weights in dense format in weight tile mem. Step 2: Update weight values i weight values i weight values i weight metadata local popcounts[row index] end for Post-Processing: Load data from weight tile mem into weight tile. Return: weight tile B AVX KERNEL OPTIMIZATION To enhance utilization, we use multiple AVX registers for both weights and outputs, enabling the input register to be multiplied across all of them. We refer to the number of registers used as num neuron groups. As shown in Figure 16, increasing num neuron groups improves per- formance, matching or even slightly surpassing the AMX kernel in some cases. This is likely because of the extra memory load and write needed for loading the data into AMX while this step is not needed if the computation hap- pens using AVX registers. 0 5 10 15 20 25 30 Number of Column Groups 1.0 1.5 2.0 2.5 3.0 3.5 Speedup Cores 8 Cores 16 Cores 32 Mode AVX AMX Figure 16. Speedup vs. Column Groups: Speedup for decoding a single token at different number of CPU cores available for AVX implementation having different number of column groups processed together with a single input load. Generally, using more groups leads to better performance, even surpassing that of the AMX implementation. Baseline is 1 column group for AVX kernel on 8 cores. C KV SPARSITY Figure 17 shows how the WikiText2(Merity et al., 2016) perplexity changes as we apply our suggested unstructured sparsity. We see that at 30 sparsity in the K and 50 sparsity in the V, perplexity increases from 6.136 to 6.745. Figure 18 shows how the perplexity changes for quantized KV. At 30 K sparsity and 50 V sparsity, the perplexity difference is still less than 1. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs 0 10 20 30 40 50 60 Sparsity Percentage of K 0 10 20 30 40 50 60 Sparsity Percentage of V 6.136 6.145 6.195 6.300 6.520 6.896 7.730 6.182 6.192 6.236 6.338 6.546 6.927 7.781 6.214 6.226 6.267 6.366 6.575 6.964 7.833 6.289 6.298 6.337 6.432 6.629 7.026 8.010 6.408 6.417 6.467 6.564 6.772 7.168 8.273 6.599 6.599 6.648 6.745 6.940 7.347 8.661 7.059 7.070 7.137 7.244 7.442 7.87310.171 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 Figure 17. Perplexity on WikiText2 with unstructured sparsity in the KV values. 0 10 20 30 40 50 60 Sparsity Percentage of K 0 10 20 30 40 50 60 Sparsity Percentage of V 6.392 6.416 6.484 6.621 6.962 7.388 9.279 6.443 6.472 6.537 6.671 6.997 7.428 9.395 6.492 6.521 6.580 6.715 7.053 7.478 9.505 6.589 6.617 6.673 6.799 7.125 7.542 9.864 6.832 6.854 6.932 7.070 7.403 7.77810.326 7.053 7.064 7.147 7.289 7.606 8.01811.232 7.751 7.790 7.922 8.105 8.437 8.92314.153 7 8 9 10 11 12 13 14 Figure 18. Perplexity on WikiText2 with unstructured sparsity in the KV values after they re quantized to 8 bits.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nSPARAMX: ACCELERATING COMPRESSED LLMS TOKEN GENERATION ON AMX-POWERED CPUS Ahmed F. AbouElhamayed 1 Jordan Dotzel 1 Yash Akhauri 1 Chi-Chih Chang 1 Sameh Gobriel 2 J. Pablo Mu noz 2 Vui Seng Chua 2 Nilesh Jain 2 Mohamed S. Abdelfattah 1 ABSTRACT Large language models have high compute, latency, and memory requirements. While specialized accelerators such as GPUs and TPUs typically run these workloads, CPUs are more widely available and consume less energy. Accelerating LLMs with CPUs enables broader AI access at a lower cost and power consumption. This acceleration potential for CPUs is especially relevant during the memory-bound decoding stage of LLM inference, which processes one token at a time and is becoming increasingly utilized with reasoning models. We utilize Advanced Matrix Extensions (AMX) support on the latest Intel CPUs together with unstructured sparsity to achieve a 1.42 reduction in end-to-end latency compared to the current PyTorch implementation by applying our technique in linear layers. We provide a set of open-source customized sparse kernels that can speed up any PyTorch model by automatically replacing all linear layers with our custom sparse implementation. Furthermore, we demonstrate for the first time the use of unstructured sparsity in the attention computation achieving a 1.14 speedup over the current systems without compromising accuracy. Code: Aware-Automated-Machine-Learning tree main SparAMX 1 INTRODUCTION The usage of large language models (LLMs) has grown exponentially over the past few years and is expected to con- tinue its unprecedented growth. This has enabled many AI- driven applications, yet significant hardware and power re- sources are required, which have motivated recent attempts at model compression and acceleration. One such compres- sion method is unstructured pruning, which removes some model parameters without structural constraints like contigu- ous nonzero values. Although such a method can achieve high sparsity while maintaining accuracy, achieving actual speedup on current hardware, such as GPUs and TPUs, is challenging. This high-cost specialized hardware makes LLMs inacces- sible to many people and limits their use. CPUs, on the other hand, are more ubiquitous and therefore can be used to accelerate LLM-driven applications for a wider audience.\n\n--- Segment 2 ---\nThis high-cost specialized hardware makes LLMs inacces- sible to many people and limits their use. CPUs, on the other hand, are more ubiquitous and therefore can be used to accelerate LLM-driven applications for a wider audience. Newer CPUs, such as the Intel Sapphire Rapids, contain units that natively accelerate matrix multiplication at low cost and power. For example, AMX units present in the Sapphire Rapids chip enable direct acceleration of LLM workloads on the CPU. We explore combining the capa- 1Cornell University, New York, USA 2Intel Labs, Oregon, USA. Correspondence to: Ahmed F. AbouEl- hamayed J. Pablo Mu noz Mohamed S. Abdelfattah mo- bilities of this unit with unstructured sparsity to accelerate LLMs on the CPU. As shown in Figure 1, our sparse AMX kernel, SparAMX, leads to faster decoding times across common LLMs compared to the stock PyTorch. In this work, we start with background material on sparsity and the hardware features used in our kernel, explore the cur- rent literature, and then introduce our detailed optimizations and design of kernels. We then demonstrate better efficiency over stock PyTorch that uses Intel s optimized libraries (In- tel, 2024) and other proprietary commercial solutions like DeepSparse (Neuralmagic, 2024) by optimizing the dom- inant matrix multiplication operations within the model s linear layers. Finally, we apply unstructured pruning to the KV cache for the first time and propose a kernel that makes use of the sparsity to optimize the attention operation as well to increase model performance. Our summarized contributions are as follows: An end-to-end system that uses unstructured sparsity to improve latency by up to 1.42 over the stock PyTorch on CPU. An INT8 CPU kernel that uses unstructured sparsity and AMX to achieve up to 1.46 better performance than current proprietary kernels (Neuralmagic, 2024) for quantized models. A novel study of unstructured sparsity in the KV cache achieving 1.14 speedup on 16K context with minimal accuracy loss.\n\n--- Segment 3 ---\nAn INT8 CPU kernel that uses unstructured sparsity and AMX to achieve up to 1.46 better performance than current proprietary kernels (Neuralmagic, 2024) for quantized models. A novel study of unstructured sparsity in the KV cache achieving 1.14 speedup on 16K context with minimal accuracy loss. arXiv:2502.12444v1 [cs.LG] 18 Feb 2025 SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Phi-3-mini-4K Llama2-7B Mistral-7B Llama3-8B 1.0 1.1 1.2 1.3 1.4 1.5 Speedup (x) 1.08x 1.27x 1.30x 1.42x Figure 1. SparAMX Performance: SparAMX consistently leads to a speedup in decode latency over PyTorch. The improvement tends to be greater as the model size increases. Context length is set to 512. hidden_dim IN W OUT hidden_dim batch_size num_neurons batch_size hidden_dim num_neurons num_neurons batch_size Figure 2. Linear GEMM Mapping: A linear layer can be com- puted with matrix multiplication. In this example hidden dim 4, num neurons 3, and batch size 2. 2 BACKGROUND 2.1 LLM Inference LLM inference consists of two distinct stages with varying compute and memory requirements. During the first stage, known as prefill, all input tokens are processed at the same time, allowing significant memory reuse and making the operation compute-bound. However, this stage occurs only once per prompt. The needed results (keys and values of each token in each attention head in each layer) are stored in the KV cache for subsequent use during the second stage. The second stage, known as decode, performs auto- regressive sampling one token at a time. This token-by- token processing reduces memory reuse and compute inten- sity, making the decode stage memory-bound and bottle- necked by weight loading from memory in short contexts. This opens an opportunity to leverage unstructured spar- sity, which enables higher sparsity levels and more efficient compression to minimize memory transfer.\n\n--- Segment 4 ---\nThis token-by- token processing reduces memory reuse and compute inten- sity, making the decode stage memory-bound and bottle- necked by weight loading from memory in short contexts. This opens an opportunity to leverage unstructured spar- sity, which enables higher sparsity levels and more efficient compression to minimize memory transfer. Weights can be stored in a compressed format, retrieved with reduced memory overhead, and decompressed on the compute unit before performing the dense matrix multiplication. While this approach introduces some additional computa- tion, the process is memory-bound; thus, reducing memory transfer, even at the cost of extra computation, results in an overall speedup. We propose a generic system that applies this optimization and integrates seamlessly with PyTorch. Our approach demonstrates a significant speedup compared to the current PyTorch implementation. 2.2 DNN Sparsity In Deep Neural Networks (DNNs), some weights con- tribute more significantly to the model s performance than others. Prior research has extensively explored methods for identifying weight importance (Liu et al., 2023b; Akhauri et al., 2024). Introducing sparsity into the weights via prun- ing leverages this insight to reduce the size of the DNN while improving efficiency. Sparsity reduces memory trans- fer costs by avoiding the loading of insignificant weights and lowers compute requirements by skipping operations involving those weights. There are two types of sparsity: structured and unstruc- tured. Structured sparsity involves pruning weights in full blocks, such as a full row (mapping to a full neuron in Figure 2) or a full tile. This pattern allows for simpler acceleration since entire sets of weights can be skipped during loading and computation. Unstructured sparsity imposes no constraints on the patterns of pruned weights. This flexibility enables higher sparsity levels, but achieving actual speedup is more challenging because the irregular distribution of zeros requires full computation and weight loading unless some special handling is used. Structured sparsity typically results in lower sparsity levels because important weights often co-exist within the same structure as prunable weights, preventing their independent removal. In contrast, unstructured sparsity achieves greater sparsity due to its flexibility, making it a better approach for model compression in scenarios where lower data transfer is desired and where hardware can handle its computational challenges.\n\n--- Segment 5 ---\nStructured sparsity typically results in lower sparsity levels because important weights often co-exist within the same structure as prunable weights, preventing their independent removal. In contrast, unstructured sparsity achieves greater sparsity due to its flexibility, making it a better approach for model compression in scenarios where lower data transfer is desired and where hardware can handle its computational challenges. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Linear 90.4 Others 9.6 8 Cores Linear 85.8 Others 14.2 16 Cores Linear 81.0 Others 19.0 32 Cores Figure 3. Inference Breakdown: Linear layers dominate the la- tency during LLM inference. Results profiled from Llama 3 8B running on Intel(R) Xeon(R) Gold 6430L CPU with 512 context length. 2.3 GEMM Mapping During both inference stages, the most compute-intensive operations in LLMs are mapped to matrix multiplication, also known as General Matrix Multiply (GEMM). These operations dominate the computational workload in both the core linear layers and attention mechanisms. Figure 3 shows that linear layers dominate the latency, especially at small contexts. Linear layers in LLMs can be represented as matrix mul- tiplications, as illustrated in Figure 2. In the example, the weight matrix (W) contains the weights, where each column corresponds to the weights of a single neuron. Each neuron uses its unique set of weights to process inputs. The input matrix (IN) contains the input values, with each row repre- senting an individual input. Each input undergoes identical processing, being multiplied by the same set of weights. The result of this computation is the output matrix (OUT). Each column in OUT corresponds to a neuron, with its rows containing the computed outputs for each input. The num- ber of rows is the same as the number of inputs as an output row is computed for each of the input rows. 2.4 Advanced ISA Extensions Our kernel is designed to utilize two specialized instruc- tion sets: AVX (Advanced Vector Extensions) and AMX (Advanced Matrix Extensions). AVX is a set of SIMD (Single Instruction, Multiple Data) instructions extending the x86 architecture, enabling parallel operations on data vectors. It uses special AVX registers to store arguments and outputs for these operations.\n\n--- Segment 6 ---\nAVX is a set of SIMD (Single Instruction, Multiple Data) instructions extending the x86 architecture, enabling parallel operations on data vectors. It uses special AVX registers to store arguments and outputs for these operations. This work focuses on AVX-512, which operates on 512-bit registers. For example, the instruction mm512 loadu si512 can load 512 bits of data from main memory into an AVX register. To perform a dot product, you can load elements from two vectors into AVX registers and use the mm512 dpbf16 ps instruction. This operation multiplies 32 pairs of 16-bit elements of the two registers, adds the results of each two consecutive elements together to form 16 32-bit elements which are accumulated in a third register. OUT IN W 0 1 2 3 ... 31 30 0 1 2 3 30 31 0 1 2 3 4 5 6 7 60616263 ... 0 1 2 3 4 5 6 7 60616263 0 0 ... 63 63 ... INT8 Mode BF16 Mode ... 0 0 ... 31 31 Figure 4. AMX Matrix Multiplication: AMX tiles support BF16 (blue) and INT8 (red) values. First two tiles contain the input matrix tiles, which are then multiplied and accumulated to the third tile. Building on the success of AVX, Intel introduced AMX in their Sapphire Rapids processor series. Unlike AVX, which focuses on vectorized operations, AMX provides specialized hardware acceleration for matrix multiplication. AMX introduces a set of tiled registers, which are two- dimensional and significantly larger than AVX registers (as shown in Figure 4). Each tile consists of up to 16 rows, with each row containing up to 512 bits, and there are eight tile registers in each AMX unit. These tiles support operations to zero all elements, load data from memory, store data into memory, and perform matrix multiplication. Matrix multiplication in AMX involves multiplying two tiles and accumulating the result in a third tile. These operations support two data formats: BF16 (Bfloat16): Each row can store up to 512 16 32 elements. Results are stored in FP32. INT8 (8-bit integers): Each row can store up to 512 8 64 elements. Results are stored in INT32.\n\n--- Segment 7 ---\nINT8 (8-bit integers): Each row can store up to 512 8 64 elements. Results are stored in INT32. During matrix multiplication, each word from the first operand tile is multiplied by the corresponding word from the second operand tile, requiring special arrangement of items in the matrices to match the ordering shown in Fig- ure 4. 3 RELATED WORK Quantization Weight quantization in LLMs has been ex- tensively studied, achieving high model quality with as few as three or four bits (Lin et al., 2024; Frantar et al., 2022; Dettmers et al., 2022; Dotzel et al., 2024). Quantization of both weights and activations has also advanced signif- icantly, enabling four-bit computation while maintaining accuracy (Xiao et al., 2023a; Liu et al., 2023a). Given that trend, support for low-bit datatypes and opera- tions has been added in recent CPUs and GPUs. For exam- ple, Intel Sapphire Rapids CPUs support INT8 operations, and NVIDIA s H100 Tensor Cores support INT8 and INT4 SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs operations. To harness these features, specialized kernels have been developed, demonstrating up to a 3 speedup over full-precision models (Lin et al., 2024), with further optimizations achieving an additional 1.9 speedup (Kim et al., 2024b). Some kernels, such as (Shen et al., 2023) efficiently run 4-bit models on multiple Intel platforms, including AMX, but do not incorporate sparsity. Sparsity Like quantization, sparsity can be used to accel- erate LLM workloads, as introduced in Section 2.2. Many studies focus on static structured sparsity (Ma et al., 2023; Dery et al., 2024), which removes entire components, of- ten yielding strong speedups at the cost of accuracy. For instance, LLM-Pruner (Ma et al., 2023) performs one-shot structured pruning using gradient information and reaches only 20 sparsity with an extra perplexity of 3.6 points on WikiText-2 (Merity et al., 2016).\n\n--- Segment 8 ---\nMany studies focus on static structured sparsity (Ma et al., 2023; Dery et al., 2024), which removes entire components, of- ten yielding strong speedups at the cost of accuracy. For instance, LLM-Pruner (Ma et al., 2023) performs one-shot structured pruning using gradient information and reaches only 20 sparsity with an extra perplexity of 3.6 points on WikiText-2 (Merity et al., 2016). Other works leverage dy- namic structured sparsity to optimize the runtime of LLMs on the GPU (Liu et al., 2023b; Akhauri et al., 2024). For unstructured sparsity, techniques such as SparseGPT (Frantar Alistarh, 2023) and Wanda (Sun et al., 2024) achieve up to 50 pruning. However, these methods often incur accuracy losses for example, 3.47 for SparseGPT on Llama 2 7B. Wanda s semi-structured variant suffers even greater perplexity degradation ( 2.69) compared to its unstructured counterpart ( 1.01) (Zhu et al., 2023). Some techniques like Shears (Mu noz et al., 2024) can recover some of the lost accuracy with additional fine-tuning. As wall-time acceleration of unstructured sparsity can be difficult, Flash-LLM (Xia et al., 2023) proposes a GPU kernel that makes use of unstructured sparsity to minimize memory transfer requirements by cleverly using the general purpose GPU streaming multi-processors and tensor cores, while Marlin (Frantar et al., 2024) develops a GPU kernel that makes use of both quantization and semi-structured sparsity to improve performance. While most prior works focus on GPUs, recent re- search has explored unstructured sparsity on CPUs. SparseDNN (Wang, 2021) uses static code generation to process only non-zero elements efficiently. DeepSparse En- gine (Neuralmagic, 2024) applies unstructured sparsity with additional optimizations to accelerate LLMs, though it does not leverage AMX units and is closed-source. Our work inte- grates unstructured sparsity with AMX on Sapphire Rapids CPUs to improve LLM performance.\n\n--- Segment 9 ---\nDeepSparse En- gine (Neuralmagic, 2024) applies unstructured sparsity with additional optimizations to accelerate LLMs, though it does not leverage AMX units and is closed-source. Our work inte- grates unstructured sparsity with AMX on Sapphire Rapids CPUs to improve LLM performance. Furthermore, we ex- tend the unstructured sparsity to compress the KV cache and show 1.14 speedup using our kernel in the attention mechanism. Some recent work showed that utilizing a CPU with AMX can help optimize the performance of a CPU- GPU system (Kim et al., 2024a). Our work aims to make running operations utilizing AMX even faster. 4 KERNEL DESIGN We design our kernels as PyTorch C extensions, accom- panied by Python classes, enabling seamless replacement of layers in arbitrary PyTorch models. These kernels are general-purpose and do not assume or optimize for a spe- cific sparsity pattern. Consequently, the achieved speedup depends on the sparsity percentage of the model. We begin by introducing a dense kernel that performs stan- dard GEMM operations in BF16 using AMX. We then ex- tend this kernel to incorporate unstructured sparsity, adapt- ing it to improve performance under sparse conditions. To evaluate performance gains of AMX, we also implement the sparse kernel using AVX and use it for comparisons. Finally, we present the quantized INT8 kernels, designed to leverage low-bit computation for further optimization. 4.1 Dense Kernel We begin by developing a kernel for linear layer compu- tation using AMX, without incorporating any custom op- timizations. This kernel, referred to as the dense kernel, assumes a fully dense model with no sparsity-related modi- fications, as illustrated in Figure 5. The AMX unit in each core can hold up to eight distinct tiles simultaneously. In our design: Tiles 0-3 are utilized to store intermediate results, which remain in the AMX unit during iteration over the inner dimension. During each iteration, we load two input tiles (Tiles 4 and 5) and two weight tiles (Tiles 6 and 7), compute the matrix multiplication, and accumulate the results in the four result tiles. Upon completing the inner dimension loop, the four result tiles are saved to memory, and the next set of four result tiles is initialized.\n\n--- Segment 10 ---\nDuring each iteration, we load two input tiles (Tiles 4 and 5) and two weight tiles (Tiles 6 and 7), compute the matrix multiplication, and accumulate the results in the four result tiles. Upon completing the inner dimension loop, the four result tiles are saved to memory, and the next set of four result tiles is initialized. By leveraging all eight tiles, instead of the naive approach of using one tile for result and two for operands, we achieve a compute-to-load ratio of 1:1, improving significantly over the 1:2 ratio that results from computing one tile at a time after loading two. Next, we parallelize the operations in the kernel. Since each input row (representing a different input token) and output column (representing a neuron) is independent, par- allelization can be applied over out rows and out cols. In the decoding stage, a single batch contains only one out- put row. On the other hand, out cols is input-independent and layer-dependent, making it the preferred dimension for parallelization. For smaller models, where out cols is less than 32 number of available threads (since two tiles are SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs out_rows inner_dim out_cols out_cols W IN OUT 4 5 6 7 inner_dim out_rows out_rows inner_dim out_cols out_cols W IN OUT inner_dim out_rows 4 5 6 7 0 1 2 3 0 out_rows inner_dim out_cols out_cols W IN OUT 6 7 inner_dim out_rows 1 2 3 0 out_rows inner_dim out_cols out_cols W IN OUT 4 6 7 inner_dim out_rows 1 1 2 3 4 1 0 2 3 4 5 6 7 1 0 2 3 4 5 6 7 0 1 2 3 1 0 2 3 4 6 7 0 1 0 11 0 Figure 5. AMX Dense Kernel: (1) Tiles 0, 1, 2, and 3 act as accumulators for the results computed by multiplying (4x6), (4x7), (5x6), and (5x7) respectively.\n\n--- Segment 11 ---\nFor smaller models, where out cols is less than 32 number of available threads (since two tiles are SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs out_rows inner_dim out_cols out_cols W IN OUT 4 5 6 7 inner_dim out_rows out_rows inner_dim out_cols out_cols W IN OUT inner_dim out_rows 4 5 6 7 0 1 2 3 0 out_rows inner_dim out_cols out_cols W IN OUT 6 7 inner_dim out_rows 1 2 3 0 out_rows inner_dim out_cols out_cols W IN OUT 4 6 7 inner_dim out_rows 1 1 2 3 4 1 0 2 3 4 5 6 7 1 0 2 3 4 5 6 7 0 1 2 3 1 0 2 3 4 6 7 0 1 0 11 0 Figure 5. AMX Dense Kernel: (1) Tiles 0, 1, 2, and 3 act as accumulators for the results computed by multiplying (4x6), (4x7), (5x6), and (5x7) respectively. (2) Tiles 4 and 5 are utilized to load all columns of the input rows (denoted as out rows) while tiles 6 and 7 are utilized to load all rows of the weight columns. (3) After the loop over the inner dimension ends, results are stored in memory and a new set of result tiles is initialized and computed in the same way. (4) Some boundary conditions might occur near the end of the matrix. Table 1. Compute Analysis: Percentage of pipeline slots that were memory bound (waiting on any memory access, including caches) and DRAM Bound (missed the cache and waiting for main memory access). Evaluated on 32 consecutive linear layers with 4192 inputs and 14336 outputs to mimic Llama 3 8B up proj layer. Kernel Memory Bound ( ) DRAM Bound ( ) Dense 100 87.5 Sparse 21.1 5.7 processed at a time, each with 16 columns), parallelizing over out rows as well can be beneficial. Parallelization over the inner dimension is another option, but it requires combining partial results from multiple result tiles, adding computational and memory overheads.\n\n--- Segment 12 ---\nKernel Memory Bound ( ) DRAM Bound ( ) Dense 100 87.5 Sparse 21.1 5.7 processed at a time, each with 16 columns), parallelizing over out rows as well can be beneficial. Parallelization over the inner dimension is another option, but it requires combining partial results from multiple result tiles, adding computational and memory overheads. This approach is only effective when both out cols and out rows are small and do not fully utilize all available threads. 4.2 Sparse Format While the dense kernel leverages AMX efficiently, the anal- ysis in Table 1 highlights further opportunities for perfor- mance optimization. Using the VTune Profiler to profile a Llama 3 model layer repeated 32 times, we observe that most pipeline slots are memory-bound, with nearly 50 of the time spent waiting on slow DRAM access. To ad- dress this bottleneck, we reduce memory transfer by storing weights in a compressed format and decompressing each tile only when needed for computation. Typically, weights, whether zero or non-zero, use the same number of bits. To save memory and bandwidth, we employ a compressed format where zero weights are represented with a single bit, while non-zero weights require an addi- tional bit. Before computation, these values are converted back to their original format. Although this approach incurs 1 0 0 1 1 0 weight_metadata weight_values 0 0 Dense Representation Sparse Representation Figure 6. Sparse Format: Weights are stored with a sparse format that stores only the non-zero weights and an associated bitmap to indicate corresponding weight indices. some computational overhead during format conversion, it significantly reduces memory transfer, making it highly effective for the memory-bound decode phase. This compressed weight format is illustrated in Figure 6. The representation is divided into two components: weight metadata: A bitmap where 1 indicates non- zero weights and 0 represents zero weights. weight values: A list containing all non-zero weights in the order required for processing. We select this method for its simplicity and because modern hardware supports efficient extraction from this format. 4.3 AMX Sparse Kernel Parallelizing over weights compressed in this format is challenging due to its unstructured nature. In a multi- threaded program, the access points for each thread within weight values are not predetermined. To address this, we introduce a precomputed index list, weight value index, generated during model initialization.\n\n--- Segment 13 ---\nIn a multi- threaded program, the access points for each thread within weight values are not predetermined. To address this, we introduce a precomputed index list, weight value index, generated during model initialization. This list specifies the starting position for each thread within weight values, as illustrated in Figure 9. Consequently, the number of threads must remain fixed during initialization, introducing a one- SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs CPU weight_buffer AVX Register Stack AVX Register weight_values AMX Weight Tile Register Memory weight_metadata Load metadata into AVX register Store local variable in stack Load weight row into a buffer Load weight tile into AMX 2 1 3 4 5 Decompress weights into AVX register Figure 7. Decompression Process: To construct a single weight tile in case of BF16 computation, (1) 16 32-bit metadata elements are fetched into an AVX register. (2) The values in the local AVX register are stored in the stack and (3) accessed individually to guide the decompression into another AVX register. (4) The data in the register is stored in the weight buffer. (5) After all 16 weight rows are stored in the buffer, the data in the buffer is loaded into the AMX tile. See Appendix A for algorithm version. time compute overhead during model loading. However, this overhead occurs only once, and the runtime memory overhead is minimal, as it requires storing just one index per thread so it is practical for real-world application. The kernel computation follows the dense kernel in Sec- tion 4.1, with the primary difference being that weights are not directly loaded into AMX registers because they are stored in a compressed format. Instead, the weightmetadata items corresponding to the current weight tile are first loaded into an AVX register using the vmovdqu32 operation as shown in (Figure 7), which loads 512 bits of data as 16 groups of 32-bit elements. Each 32-bit element, along with a pointer to the index of the first non- consumed weight values item (weight value index), serves as input to the vpexpandw operation. This oper- ation expands the bitmask into individual weights, placing zeros in positions marked by 0s in weightmetadata and loading the corresponding weights for positions marked by 1s.\n\n--- Segment 14 ---\nEach 32-bit element, along with a pointer to the index of the first non- consumed weight values item (weight value index), serves as input to the vpexpandw operation. This oper- ation expands the bitmask into individual weights, placing zeros in positions marked by 0s in weightmetadata and loading the corresponding weights for positions marked by 1s. With a tile size of 16 rows by 32 columns, each 32-bit ele- ment expands to fill a row, and vpexpandw is applied 16 times to populate the entire weight tile. The result of each vpexpandw is stored temporarily in memory because direct transfers from AVX to AMX registers are currently unsup- ported. Instead, the complete tile is stored in memory and then loaded into AMX registers via an AMX load operation. Although this process involves memory access, frequent reuse of this memory region likely ensures it remains in the cache, mitigating expensive memory operations. To update the weight value index pointer, we utilize the popcount operation. For efficient instruction-level paral- lelism, each of the 16 loop iterations is kept independent. The offset for each weight row is computed using vpopcntd, Algorithm 1 Parallel Prefix Sum with AVX-512 Intrinsics Input: AVX Register v [v0, v1, . . . , v15] of 16 32-bit integers Output: AVX Register s [s0, s1, . . . , s15] where si Pi j 0 vj s v Shift(v, 1) Shift right 1 element and add s s Shift(s, 2) Shift right 2 elements and add s s Shift(s, 4) Shift right 4 elements and add s s Shift(s, 8) Shift right 8 elements and add return s which calculates a popcount on each of the 16 32-bit ele- ments and stores the results in a new AVX register. Finally, a parallel prefix sum operation, as described in Algorithm 1, computes the final offset needed for each tile row. 4.4 AVX Sparse Kernel During the decode phase with batch size 1, only one row of the 16-row AMX tile used for input is utilized, leading to significant inefficiency.\n\n--- Segment 15 ---\nFinally, a parallel prefix sum operation, as described in Algorithm 1, computes the final offset needed for each tile row. 4.4 AVX Sparse Kernel During the decode phase with batch size 1, only one row of the 16-row AMX tile used for input is utilized, leading to significant inefficiency. To address this, we implement the compression technique using only AVX instructions, en- abling a performance comparison between AVX and AMX while ensuring compatibility with older CPUs lacking AMX support. The flow is illustrated in Figure 8. In this setup: An AVX register holds the weights for multiple neurons in the inner dimension. Another AVX register holds the corresponding input value repeated across the register. A third AVX register accumulates results for multiple neurons. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs 1 1 2 IN OUT W 1 inner_dim out_cols out_cols 1 inner_dim Single value fills all items of AVX register Different AVX Registers 2 1 3 Loop through inner_dim Repeat for next neurons 0 0 W thr0 thr1 1 It is possible to use more AVX Registers to compute multiple neurons outputs using a single input load 0 2 2 3 2 Figure 8. AVX Matrix Multiplication: Weights for different neurons are loaded into AVX register 1 and a single element of the input is fetched and used to fill all elements of AVX register 2. AVX register 0 is initialized to 0 to store the result, AVX registers 1 and 2 are multiplied and the results are added to the elements in AVX register 0. The process is repeated along the hidden dimension till all weights of the considered neurons are multiplied by the corresponding parts of the input. Another set of neurons are then processed. W thr0 thr1 thr2 thr3 weight_values weight_value_index thr0 thr1 thr2 thr3 Figure 9. Sparse Kernel Parallelization: Parallelization over threads requires each thread to know where to begin processing the weights. The index list weight value index stores this infor- mation and is computed offline allowing efficient parallelization at run time. Further implementation details are provided in Appendix B. 4.5 INT8 Kernels AMX supports both BF16 and INT8 operations. We de- velop a kernel for INT8 matrix multiplication and adapt our framework to quantize weights and activations for com- patibility.\n\n--- Segment 16 ---\n4.5 INT8 Kernels AMX supports both BF16 and INT8 operations. We de- velop a kernel for INT8 matrix multiplication and adapt our framework to quantize weights and activations for com- patibility. The flow of the INT8 kernel mirrors that of the AMX dense and sparse kernels, with adjustments for 8-bit elements instead of 16-bit. Each weight tile now contains 16 64 1024 weights, and their metadata is fetched into two AVX registers, each covering eight rows of the tile. Additionally, the weight ordering during preprocessing is modified to divide each weight column into four segments rather than two. 30 40 50 60 70 Accuracy ( ) 1.0 1.1 1.2 1.3 1.4 1.5 Speedup (x) Model Llama-3 8B Mistral-7B phi-3 Figure 10. Speedup vs. Accuracy. Different points are from different sparsity percentages. 5 RESULTS We use stock PyTorch running in dense format1 as the base- line to ensure a fair comparison and consistency across all other operations, especially since it utilizes AMX when available. Table 2 presents the latencies observed for layer 5 of Llama 3 8B when run using stock PyTorch versus our custom sparse kernel. Our kernel outperforms PyTorch across all projections, with performance improvements rang- ing from 1.22 in the most time-consuming projection to 2.03 in the least time-consuming projection. Figure 10 illustrates the tradeoff between end-to-end speedup and ac- curacy on GSM8K(Cobbe et al., 2021) for multiple models adopted from SQFT(Munoz et al., 2024). For end-to-end speedup, we evaluate the full Llama3 8B 1PyTorch has a beta API for sparse tensors, but we could not run our model in BF16 or FP16 using it because some operations are not implemented for these formats yet. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Table 2.\n\n--- Segment 17 ---\nFor end-to-end speedup, we evaluate the full Llama3 8B 1PyTorch has a beta API for sparse tensors, but we could not run our model in BF16 or FP16 using it because some operations are not implemented for these formats yet. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Table 2. Speedup in latency of the individual linear modules of layer 5 in Llama 3 8B Name Dimensions Speedup ( ) q proj 4096 4096 1.44 k proj 4096 1024 2.03 v proj 4096 1024 1.41 o proj 4096 4096 1.3 gate proj 4096 14336 1.26 up proj 4096 14336 1.22 down proj 14336 4096 1.36 model using our AVX and AMX sparse kernels against stock PyTorch. Figure 11 demonstrates how performance scales with sparsity across different numbers of CPU cores. Both AMX and AVX sparse kernels achieve a speedup compared to the stock PyTorch as sparsity increases. The gap between AMX and AVX decreases as the number of cores increases, likely due to reduced cache contention, which decreases memory access bottlenecks for AMX weight construction. The advantage of AMX becomes apparent at higher batch sizes. Figure 12 shows performance across various batch sizes comparing decoding throughput of the stock PyTorch and our AMX kernels to the throughput of our AVX kernel. The Llama3 8B checkpoint was taken from Shears (Mu noz et al., 2024), which maintains accuracy with 50 unstruc- tured sparsity. The results indicate that the two AMX kernels achieve higher throughput as batch size increases than the AVX kernel that is designed for vector multiplication rather than matrix multiplication for which AMX is designed. We also compare our kernel to the proprietary DeepSparse engine (Neuralmagic, 2024) and llama.cpp (Gerganov, 2024) in Figure 13. While DeepSparse employs optimiza- tions beyond unstructured sparsity, it is limited to specific models. Our kernel is general-purpose and compatible with any PyTorch model, but the latest Llama model that DeepSparse supports at the time of writing is INT8 Llama 2 7B so we use that model for this comparison.\n\n--- Segment 18 ---\nWhile DeepSparse employs optimiza- tions beyond unstructured sparsity, it is limited to specific models. Our kernel is general-purpose and compatible with any PyTorch model, but the latest Llama model that DeepSparse supports at the time of writing is INT8 Llama 2 7B so we use that model for this comparison. For a fair comparison, we use our INT8 kernel, and to minimize the effect of attention, which is irrelevant, we set the context length to 22. The results indicate that our kernel outperforms DeepSparse and llama.cpp at higher batch sizes. This ad- vantage is primarily due to AMX and its ability to perform matrix-matrix multiply rather than vector-vector multiply. Other runtimes like OpenVINO, which also leverage AMX, achieve higher performance but employ additional optimiza- tions, such as operator fusion, which are outside the scope of our framework. 2DeepSparse benchmarking tool does not work out of the box with context length 1. 6 ATTENTION KERNEL In the previous sections, we focused on linear layers. Here, we explore the potential of applying unstructured sparsity to attention computations. During attention, the incoming query value is multiplied by the cached K values, followed by a softmax operation, and the resulting values are mul- tiplied by the cached V values. The cached K and V ma- trices can be treated as weight matrices and sparsified in an unstructured manner. We first evaluate the impact of unstructured sparsity on accuracy and then adapt the kernel to accelerate the matrix multiplication workload of attention, demonstrating the resulting speedup. 6.1 Sparsity in the KV Cache Various methods have been proposed to optimize the KV cache, such as dropping certain tokens (Xiao et al., 2023b; Zhang et al., 2023), clustering tokens (Zandieh et al., 2024), or applying channel sparsity (Xu et al., 2024). In our ap- proach, we apply unstructured sparsity to the KV values using magnitude-based pruning, where values with the low- est magnitudes are dropped within each layer.\n\n--- Segment 19 ---\n6.1 Sparsity in the KV Cache Various methods have been proposed to optimize the KV cache, such as dropping certain tokens (Xiao et al., 2023b; Zhang et al., 2023), clustering tokens (Zandieh et al., 2024), or applying channel sparsity (Xu et al., 2024). In our ap- proach, we apply unstructured sparsity to the KV values using magnitude-based pruning, where values with the low- est magnitudes are dropped within each layer. Figure 14 shows the accuracy drop across downstream tasks (calculated as the geometric mean of accuracies for PIQA (Bisk et al., 2020), ARC (Easy Challenge)(Clark et al., 2018), BoolQ(Clark et al., 2019), HellaSwag (Zellers et al., 2019), and WinoGrande (Sakaguchi et al., 2021)). The drop in accuracy is less than 1 with 30 K pruning and 50 V pruning. 6.2 Acceleration The sparse kernel introduced in Section 4.3 is adapted here for attention computations. Within attention, matrix multi- plication is required for computing QK and RV (where R is the result of the scaled softmax of QK). This operation is a batched matrix multiplication with an additional head dimension. The sparse kernel is modified to handle this op- eration, leveraging the independence of heads to parallelize across them. To manage the KV cache efficiently, we modify the atten- tion code to initialize an empty cache after prefill, storing all previously cached values in the model state similar to how weights are stored. PyTorch s native functions for up- dating the cache and its repeat kv function (used in Llama 3 8B s Grouped Query Attention (Ainslie et al., 2023)) incur significant overhead with large KV caches due to memory reallocation for each new token. By replacing the cached tokens with our sparse format, which maintains a constant size within the model state, and saving new tokens in a sep- arate dynamic set, decoding becomes over 6 faster. This enables efficient decoding at high context lengths (e.g., 16K) on CPUs, allowing queries on long contexts with reasonable response times.\n\n--- Segment 20 ---\nBy replacing the cached tokens with our sparse format, which maintains a constant size within the model state, and saving new tokens in a sep- arate dynamic set, decoding becomes over 6 faster. This enables efficient decoding at high context lengths (e.g., 16K) on CPUs, allowing queries on long contexts with reasonable response times. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs 40 50 60 70 Sparsity ( ) 1.0 1.2 1.4 1.6 Speedup (x) Number of Cores: 8 Mode AMX AVX 40 50 60 70 Sparsity ( ) Number of Cores: 16 40 50 60 70 Sparsity ( ) Number of Cores: 32 Figure 11. Speedup over Stock PyTorch vs. Sparsity: Llama3 8B decoding with context length 512. The sparse kernels with AVX and AMX have better performance compared to stock PyTorch across the number of cores. As sparsity increases, speedup increases. 1 2 4 8 16 32 Batch Size 2 4 6 8 10 12 Relative Throughput (vs. AVX) Mode AMX AVX Stock PyTorch Figure 12. Batched Decoding: AMX is superior to AVX, and our kernel achieves 20.8 better performance than stock PyTorch at batch size 32. For a fair comparison, we benchmark our sparse kernel against the dense kernel, as shown in Figure 15. Since the dense kernel performs similarly to the stock PyTorch during the decoding stage, this comparison isolates the effect of sparsity. The results show that decode latency improves as the sparsity percentage of K and V increases. At a set- ting with less than 1 accuracy loss, we observe a 1.14 improvement in latency over the dense kernel. 7 DISCUSSION The presented kernels demonstrate that unstructured sparsity can achieve considerable speedup in linear layers. Notably, computations are still performed using dense weights; the speedup is realized by reducing memory transfer in a load- as-sparse, compute-as-dense approach (Xia et al., 2023). While this requires additional computation to reconstruct dense weights from the compressed format, the approach is particularly effective in memory-bound scenarios. Con- versely, in compute-bound scenarios, applying unstructured sparsity may reduce performance, as confirmed by our re- sults.\n\n--- Segment 21 ---\nWhile this requires additional computation to reconstruct dense weights from the compressed format, the approach is particularly effective in memory-bound scenarios. Con- versely, in compute-bound scenarios, applying unstructured sparsity may reduce performance, as confirmed by our re- sults. 16 32 64 128 256 Batch Size 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 Relative Throughput (vs. llama.cpp) Mode Dense(INT8) Sparse(INT8) DeepSparse llama.cpp Figure 13. INT8 Kernels: Decoding throughput comparison be- tween our AMX INT8 dense kernel, sparse kernel, DeepSparse and llama.cpp with context length 2 running on 32 CPU cores for Llama 3 7B model with 50 sparsity. AMX allows us to outper- form DeepSparse and llama.cpp at high batch sizes. We also show that unstructured sparsity can complement other compression techniques, such as quantization. For instance, SQFT (Munoz et al., 2024) achieves comparable sparsity levels with INT4 quantization while maintaining accuracy. As shown in Figure 13, our sparse INT8 kernel outperforms its dense counterpart in memory-bound sce- narios. However, at higher batch sizes, the dense kernel performs better as the system shifts to a compute-bound regime. AMX enables efficient inference at high batch sizes, sur- passing AVX implementations, including highly optimized proprietary solutions like DeepSparse. However, at lower batch sizes, AMX and AVX performance are similar, with AVX sometimes outperforming AMX. This is because our SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs 0 10 20 30 40 50 Sparsity Percentage of K 0 10 20 30 40 50 Sparsity Percentage of V 71.0 71.5 72.0 72.5 73.0 73.5 74.0 Accuracy ( ) Figure 14. KV Sparsity Accuracy Downstream Accuracy on mul- tiple one-shot tasks for different percentages of KV pruning. Accu- racy change is less than 1 at 30 K Sparsity and 50 V Sparsity.\n\n--- Segment 22 ---\nKV Sparsity Accuracy Downstream Accuracy on mul- tiple one-shot tasks for different percentages of KV pruning. Accu- racy change is less than 1 at 30 K Sparsity and 50 V Sparsity. 0 10 20 30 40 50 Sparsity Percentage of K 0 10 20 30 40 50 Sparsity Percentage of V 1.000 1.025 1.050 1.075 1.100 1.125 1.150 Speedup ( ) Figure 15. KV Sparsity Performance: End-to-end decode latency speedup when using the sparse kernel for different percentages of KV pruning. Baseline is the dense kernel latency. We are able to achieve 1.14 speedup with minimal accuracy loss. decompression approach relies on expanding weight meta- data into an AVX register, which must then be stored to memory before loading into AMX tile registers due to cur- rent architectural limitations. This extra step incurs addi- tional memory operations. Caching mitigates this issue by assuming the weight tile remains in the cache. While our results validate this assumption in most cases, ineffi- ciencies arise potentially because data movement between memory and cache reduces the intended bandwidth savings from compression. Addressing this limitation would require direct data transfer from AVX registers to AMX tiles or instructions to expand bit-masked data directly into AMX tiles. Given the efficiency of AMX for high-batch inference, Sap- phire Rapids CPUs can now feasibly serve certain LLMs without requiring GPUs, potentially reducing cost and power consumption. However, this method requires offline preprocessing to organize weights into the compressed for- mat, including weight metadata, weight values, and thread start indices. Changing the number of threads necessitates recomputation of this representation. Additionally, the at- tention kernel is not suitable for dynamic KV values but re- mains effective for cached prompts, such as system prompts or static knowledge bases. Decoding at long context lengths is highly inefficient in PyTorch, becoming impractical at 16K context length. As discussed in Section 6.2, decoding can be efficiently per- formed at this length for precomputed cached tokens. For example, companies using CPU-only devices can preload their knowledge base as a cached context in an LLM and direct customer queries to it for faster responses, saving both cost and power. 8 LIMITATIONS Our system has several limitations.\n\n--- Segment 23 ---\nFor example, companies using CPU-only devices can preload their knowledge base as a cached context in an LLM and direct customer queries to it for faster responses, saving both cost and power. 8 LIMITATIONS Our system has several limitations. First, it requires prepro- cessing time, which, although only a few minutes for 8B models, makes it unsuitable for accelerating dynamically generated sparsity, such as activation sparsity. Additionally, our system currently supports only INT8 and BF16 formats, as AMX units do not natively support more compressed formats like INT4. Extending support to INT4 is feasible by dequantizing INT4 values into INT8 before computation. While our system supports any PyTorch model and serves as a useful tool for measuring the speedup of unstructured sparsity techniques, other systems, such as OpenVINO, achieve better results by incorporating additional optimiza- tions, such as operation fusion, which are not included in our system. These optimizations make OpenVINO more suitable for production use. 9 CONCLUSION In this paper, we introduced a system that leverages un- structured sparsity to achieve significant speedup in the decode stage compared to current implementations. Our system is general-purpose, compatible with any PyTorch model, and runs out of the box. It delivers a 1.42 perfor- mance improvement over the current PyTorch version for Llama 3 8B and, at high batch sizes, achieves over 1.4 higher throughput compared to proprietary systems like DeepSparse. Additionally, we demonstrate the potential of using unstructured sparsity in the KV cache to reduce latency per token, achieving a 1.14 speedup with simple magnitude pruning. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs REFERENCES Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., Lebron, F., and Sanghai, S. Gqa: Training generalized multi-query transformer models from multi-head check- points. In Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing, pp. 4895 4901, 2023.\n\n--- Segment 24 ---\nIn Proceedings of the 2023 Conference on Em- pirical Methods in Natural Language Processing, pp. 4895 4901, 2023. Akhauri, Y., AbouElhamayed, A. F., Dotzel, J., Zhang, Z., Rush, A. M., Huda, S., and Abdelfattah, M. S. Shad- owllm: Predictor-based contextual sparsity for large lan- guage models. EMNLP, 2024. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Pro- ceedings of the AAAI conference on artificial intelligence, volume 34, pp. 7432 7439, 2020. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long and Short Papers), pp. 2924 2936, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dery, L., Kolawole, S., Kagey, J.-F., Smith, V., Neubig, G., and Talwalkar, A. Everybody prune now: Structured pruning of llms with only forward passes. arXiv preprint arXiv:2402.05406, 2024.\n\n--- Segment 25 ---\nEverybody prune now: Structured pruning of llms with only forward passes. arXiv preprint arXiv:2402.05406, 2024. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318 30332, 2022. Dotzel, J., Chen, Y., Kotb, B., Prasad, S., Wu, G., Li, S., Abdelfattah, M. S., and Zhang, Z. Learning from students: Applying t-distributions to explore accurate and efficient formats for llms. ICML, 2024. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in one-shot. In Interna- tional Conference on Machine Learning. PMLR, 2023. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pre- trained transformers. arXiv preprint arXiv:2210.17323, 2022. Frantar, E., Castro, R. L., Chen, J., Hoefler, T., and Alis- tarh, D. Marlin: Mixed-precision auto-regressive paral- lel inference on large language models. arXiv preprint arXiv:2408.11743, 2024. Gerganov, G. llama.cpp, 2024. URL com ggerganov llama.cpp. Intel. Intel oneapi deep neural network library, 2024. URL en developer tools oneapi onednn.html. Kim, H., Ye, G., Wang, N., Yazdanbakhsh, A., and Kim, N. S. Exploiting intel advanced matrix extensions (amx) for large language model inference. IEEE Computer Architecture Letters, 23(1):117 120, 2024a. doi: 10. 1109 LCA.2024.3397747.\n\n--- Segment 26 ---\ndoi: 10. 1109 LCA.2024.3397747. Kim, T., Lee, J., Ahn, D., Kim, S., Choi, J., Kim, M., and Kim, H. Quick: Quantization-aware interleaving and conflict-free kernel for efficient llm inference, 2024b. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87 100, 2024. Liu, S.-y., Liu, Z., Huang, X., Dong, P., and Cheng, K.-T. LLM-FP4: 4-bit floating-point quantized transformers. In Bouamor, H., Pino, J., and Bali, K. (eds. ), Proceed- ings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 592 605, Singapore, December 2023a. Association for Computational Linguis- tics. URL emnlp-main.39. Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., Shrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pp. 22137 22176. PMLR, 2023b. Ma, X., Fang, G., and Wang, X. Llm-pruner: On the struc- tural pruning of large language models. Advances in Neu- ral Information Processing Systems, 36:21702 21720, 2023. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.\n\n--- Segment 27 ---\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs Mu noz, J. P., Yuan, J., and Jain, N. Shears: Unstructured sparsity with neural low-rank adapter search. In Pro- ceedings of the 2024 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pp. 395 405, Mexico City, Mex- ico, June 2024. Association for Computational Linguis- tics. URL naacl-industry.34. Munoz, J. P., Yuan, J., and Jain, N. SQFT: Low-cost model adaptation in low-precision sparse foundation models. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds. ), Findings of the Association for Computational Linguis- tics: EMNLP 2024, pp. 12817 12832, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653 v1 2024.findings-emnlp. 749. URL findings-emnlp.749 . Neuralmagic. Neuralmagic deepsparse: Sparsity-aware deep learning inference runtime for cpus, 2024. URL deepsparse. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99 106, 2021. Shen, H., Chang, H., Dong, B., Luo, Y., and Meng, H. Efficient llm inference on cpus. arXiv preprint arXiv:2311.00502, 2023. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024.\n\n--- Segment 28 ---\nA simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. Wang, Z. Sparsednn: Fast sparse deep learning inference on cpus. arXiv preprint arXiv:2101.07948, 2021. Xia, H., Zheng, Z., Li, Y., Zhuang, D., Zhou, Z., Qiu, X., Li, Y., Lin, W., and Song, S. L. Flash-llm: En- abling cost-effective and highly-efficient large genera- tive model inference with unstructured sparsity. arXiv preprint arXiv:2309.10285, 2023. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 38087 38099. PMLR, 2023a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef- ficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023b. Xu, Y., Jie, Z., Dong, H., Wang, L., Lu, X., Zhou, A., Saha, A., Xiong, C., and Sahoo, D. Think: Thinner key cache by query-driven pruning. arXiv preprint arXiv:2407.21018, 2024. Zandieh, A., Han, I., Mirrokni, V., and Karbasi, A. Subgen: Token generation in sublinear time and memory. arXiv preprint arXiv:2402.06082, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pp. 4791 4800, 2019.\n\n--- Segment 29 ---\nIn Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pp. 4791 4800, 2019. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., R e, C., Barrett, C., et al. H2o: heavy-hitter oracle for efficient generative inference of large language models. In Proceedings of the 37th Inter- national Conference on Neural Information Processing Systems, 2023. Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey on model compression for large language models. Transac- tions from the Association of Computational Linguistics, 2023. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs A SPARSE WEIGHTS DECOMPRESSION PROCESS We show the steps used to perform weight decompression in Algorithm 2. Algorithm 2 Processing Sparse Format in the Sparse Kernel Input: weight metadata: Bitmap indicating non-zero weights weight values: Array of non-zero weight values weight values i: Pointer to the current index in weight values for the next non-zero weight weight tile mem: Memory region for storing weight values Output: weight tile: AMX tile filled with expanded weights Initialization: Fetch 512 bits (16 rows 32 bits per row) of weight metadata into AVX register weight metadata local. Compute prefix sum to get pop count of each element in weight metadata local into AVX register weight metadata local popcounts. Loop through all tile rows: for row index 0 to 15 do Step 1: Expand the current row Use vpexpandw with row index from weight metadata local and weight values[weight values i] to get 32 weights in dense format in weight tile mem. Step 2: Update weight values i weight values i weight values i weight metadata local popcounts[row index] end for Post-Processing: Load data from weight tile mem into weight tile. Return: weight tile B AVX KERNEL OPTIMIZATION To enhance utilization, we use multiple AVX registers for both weights and outputs, enabling the input register to be multiplied across all of them. We refer to the number of registers used as num neuron groups.\n\n--- Segment 30 ---\nReturn: weight tile B AVX KERNEL OPTIMIZATION To enhance utilization, we use multiple AVX registers for both weights and outputs, enabling the input register to be multiplied across all of them. We refer to the number of registers used as num neuron groups. As shown in Figure 16, increasing num neuron groups improves per- formance, matching or even slightly surpassing the AMX kernel in some cases. This is likely because of the extra memory load and write needed for loading the data into AMX while this step is not needed if the computation hap- pens using AVX registers. 0 5 10 15 20 25 30 Number of Column Groups 1.0 1.5 2.0 2.5 3.0 3.5 Speedup Cores 8 Cores 16 Cores 32 Mode AVX AMX Figure 16. Speedup vs. Column Groups: Speedup for decoding a single token at different number of CPU cores available for AVX implementation having different number of column groups processed together with a single input load. Generally, using more groups leads to better performance, even surpassing that of the AMX implementation. Baseline is 1 column group for AVX kernel on 8 cores. C KV SPARSITY Figure 17 shows how the WikiText2(Merity et al., 2016) perplexity changes as we apply our suggested unstructured sparsity. We see that at 30 sparsity in the K and 50 sparsity in the V, perplexity increases from 6.136 to 6.745. Figure 18 shows how the perplexity changes for quantized KV. At 30 K sparsity and 50 V sparsity, the perplexity difference is still less than 1.\n\n--- Segment 31 ---\nFigure 18 shows how the perplexity changes for quantized KV. At 30 K sparsity and 50 V sparsity, the perplexity difference is still less than 1. SparAMX: Accelerating Compressed LLMs Token Generation on AMX-Powered CPUs 0 10 20 30 40 50 60 Sparsity Percentage of K 0 10 20 30 40 50 60 Sparsity Percentage of V 6.136 6.145 6.195 6.300 6.520 6.896 7.730 6.182 6.192 6.236 6.338 6.546 6.927 7.781 6.214 6.226 6.267 6.366 6.575 6.964 7.833 6.289 6.298 6.337 6.432 6.629 7.026 8.010 6.408 6.417 6.467 6.564 6.772 7.168 8.273 6.599 6.599 6.648 6.745 6.940 7.347 8.661 7.059 7.070 7.137 7.244 7.442 7.87310.171 6.5 7.0 7.5 8.0 8.5 9.0 9.5 10.0 Figure 17. Perplexity on WikiText2 with unstructured sparsity in the KV values. 0 10 20 30 40 50 60 Sparsity Percentage of K 0 10 20 30 40 50 60 Sparsity Percentage of V 6.392 6.416 6.484 6.621 6.962 7.388 9.279 6.443 6.472 6.537 6.671 6.997 7.428 9.395 6.492 6.521 6.580 6.715 7.053 7.478 9.505 6.589 6.617 6.673 6.799 7.125 7.542 9.864 6.832 6.854 6.932 7.070 7.403 7.77810.326 7.053 7.064 7.147 7.289 7.606 8.01811.232 7.751 7.790 7.922 8.105 8.437 8.92314.153 7 8 9 10 11 12 13 14 Figure 18. Perplexity on WikiText2 with unstructured sparsity in the KV values after they re quantized to 8 bits.\n\n