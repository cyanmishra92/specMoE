{
  "source": "PubCloudHet.pdf",
  "raw_length": 33220,
  "cleaned_length": 32609,
  "base_segments": 131,
  "augmented_segments": 262,
  "segments": [
    {
      "text": "Abstract \nWe are witnessing an increasing trend towards using Ma- chine Learning (ML) based prediction systems, spanning across different application domains, including product rec- ommendation systems, personal assistant devices, facial recognition, etc. These applications typically have diverse requirements in terms of accuracy and response latency, that can be satisfied by a myriad of ML models. However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by them- selves are heterogeneous in terms of provisioning latencies and billing complexity.",
      "type": "sliding_window",
      "tokens": 125
    },
    {
      "text": "However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by them- selves are heterogeneous in terms of provisioning latencies and billing complexity. Thus, it is strenuous for an infer- ence serving system to choose from this confounding array of resource types and model types to provide low-latency and cost-effective inferences. In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource of- ferings.",
      "type": "sliding_window",
      "tokens": 124
    },
    {
      "text": "In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource of- ferings. Our evaluation shows that, prior work does not solve the problem from both dimensions of model and resource heterogeneity. Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints. Towards this, we discuss the design implications of a self-managed inference serving system, which can optimize for application requirements based on public cloud resource characteristics. CCS Concepts:  ·  Computer systems organization  → Real-time system architecture .",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "CCS Concepts:  ·  Computer systems organization  → Real-time system architecture . Keywords:  serverless, resource-management, inference \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WoSC’20, December 7ś11, 2020, Delft, Netherlands © 2020 Association for Computing Machinery.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "WoSC’20, December 7ś11, 2020, Delft, Netherlands © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8204-5/20/12...$15.00 https://doi.org/10.1145/3429880.3430093 \nACM Reference Format: Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thi- nakaran, Mahmut Taylan Kandemir, and Chita R. Das. 2020.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "2020. Im- plications of Public Cloud Resource Heterogeneity for Inference Serving. In  Workshop on Serverless Computing (WoSC’20), December 7ś11, 2020, Delft, Netherlands.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "In  Workshop on Serverless Computing (WoSC’20), December 7ś11, 2020, Delft, Netherlands. ACM, New York, NY, USA,  6  pages. https://doi.org/10.1145/3429880.3430093 \n1 Introduction \nSustained advances in ML has fueled the proliferation of emerging applications such as product recommendation sys- tems, facial recognition systems, and intelligent personal assistants [ 7 ].",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "https://doi.org/10.1145/3429880.3430093 \n1 Introduction \nSustained advances in ML has fueled the proliferation of emerging applications such as product recommendation sys- tems, facial recognition systems, and intelligent personal assistants [ 7 ]. Among many ML paradigms, Deep Neural Net- works (DNNs), owing to their generalization and massively- parallel nature, has been predominant in making all these applications pervasive and accessible to developers. A typical DNN model has two different phases, namely,  training  and inference .",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "A typical DNN model has two different phases, namely,  training  and inference . Training a DNN, which is the process of extracting and learning the patterns and the features from millions of sample-data, typically takes a few hours to days. The trained models can then be used to perform inferences, i.e., the clas- sification task.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "The trained models can then be used to perform inferences, i.e., the clas- sification task. Since typical large scale DNNs have millions of parameters and perform billions of multiplications and accumulations for executing a single inference, they are typ- ically hosted as web-services, which are often queried for predictions. Conventionally, training is much more compute intensive (compared to an inference), takes many iterations and hence has been given considerable attention for better accuracy and convergence time.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "Conventionally, training is much more compute intensive (compared to an inference), takes many iterations and hence has been given considerable attention for better accuracy and convergence time. However, given the preva- lence and demand of inferences, serving them on public cloud with a tight bound of latency, throughput and cost is becoming increasingly more challenging [ 7 ]. These inference queries are typically administered with strict response laten- cies of under one second [ 4 ].",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "These inference queries are typically administered with strict response laten- cies of under one second [ 4 ]. Based on the application needs, prediction queries require different compute resources, and have different accuracy, latency, and cost requirements. To ensure a required accuracy with given latency, applications have to choose from a confounding array of different types of models (shown in Figure  1 ).",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "To ensure a required accuracy with given latency, applications have to choose from a confounding array of different types of models (shown in Figure  1 ). Therefore, it is  non-trivial for an application to choose the right model that can collectively optimize for all requirements together . WoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "WoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. Unlike accuracy and latency, which depends on the right model, cost is dictated by the type of deployment used to host them in a public cloud. The deployment costs differ based on the provisioning times and longevity of the resource pro- cured.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "The deployment costs differ based on the provisioning times and longevity of the resource pro- cured. Typically, these inference serving systems are hosted using Virtual Machines (VMs), which take a few minutes to start-up. Due to high start-up latencies, using VMs for hosting ML services can lead to over-provisioning, espe- cially during periods of poor workload predictability (flash crowds) [ 10 ].",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Due to high start-up latencies, using VMs for hosting ML services can lead to over-provisioning, espe- cially during periods of poor workload predictability (flash crowds) [ 10 ]. In stark contrast to VMs, serverless functions have been made available by cloud providers, which can spin-up within a few seconds [ 2 ]. As we will discuss in this paper, the cost of using VMs vs. serverless functions highly depends on the dynamically varying needs of the user query submission rates.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "As we will discuss in this paper, the cost of using VMs vs. serverless functions highly depends on the dynamically varying needs of the user query submission rates. Besides workload arrival rates, there is fur- ther variability in terms of configuring serverless functions to meet the end-user demands of latency and cost require- ments. This is because, serverless functions are billed based on of number of invocations, compute time and memory requirement of the function.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "This is because, serverless functions are billed based on of number of invocations, compute time and memory requirement of the function. However, increased memory allocation leads to faster execution time owing to powerful compute-core allocation, but exacerbates the billing cost. Therefore, these apparent deficiencies of choosing the appropriate resource type and model type for a given user re- quirement motivates the central question of this work:  Does there exist an optimal resource procurement system which can balance the goals of diverse user requirements for accuracy, latency and cost, by efficiently mapping model parameters to heterogeneous resource specifications?",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "Therefore, these apparent deficiencies of choosing the appropriate resource type and model type for a given user re- quirement motivates the central question of this work:  Does there exist an optimal resource procurement system which can balance the goals of diverse user requirements for accuracy, latency and cost, by efficiently mapping model parameters to heterogeneous resource specifications? Our preliminary results suggest that using a combination of VMs and serverless func- tions could potentially provide a solution to this problem. As opposed to prior works [ 5 ,  10 ], which try to combine serverless functions with VMs to hide the start-up latencies of VMs, our primary interest lies in exploring the different key aspects  to address when hosting DNN-based ML pre- diction serving systems in public cloud, as given below: •  Diverse Models:  How to make the users oblivious of model selection from the extensive pool of models, for satis- fying the accuracy, and latency requirements?",
      "type": "sliding_window",
      "tokens": 217
    },
    {
      "text": "As opposed to prior works [ 5 ,  10 ], which try to combine serverless functions with VMs to hide the start-up latencies of VMs, our primary interest lies in exploring the different key aspects  to address when hosting DNN-based ML pre- diction serving systems in public cloud, as given below: •  Diverse Models:  How to make the users oblivious of model selection from the extensive pool of models, for satis- fying the accuracy, and latency requirements? •  Heterogeneous Public Cloud Resources:  What are the different options available in terms of combining different VM-based cloud services and serverless functions for a given user requirement? •  Configuring Resources:  From the diverse options, how to right-size VMs and appropriately configure the serverless functions to efficiently cater to user specified cost, accuracy and latency constraint?",
      "type": "sliding_window",
      "tokens": 193
    },
    {
      "text": "•  Configuring Resources:  From the diverse options, how to right-size VMs and appropriately configure the serverless functions to efficiently cater to user specified cost, accuracy and latency constraint? •  Bring in Tune : Based on the dynamically changing query arrivals over time, what is the right way to combine model diversity along with resource heterogeneity without com- promising the user-specified requirements? By exploring these key aspects, we envision developing a self-managed inference-serving system, which can provide for different diverse needs of applications by leveraging the \n0 50 100 150 200 250 300 350 \n20.00% \n40.00% \n60.00% \n80.00% \n100.00% \nMobileNet V1 \nMobileNEt V2 \nInception V3 \nResnet50 \nResNet50-V2 \nDenseNet-201 \nDenseNet-121 \nXxception \nNasNetMobile \nInceptionResnetV2 \nvgg16 NasNetLarge \nLatency (ms) \nAccuracy % \nTop1-Accuracy Latency \nFigure 1.",
      "type": "sliding_window",
      "tokens": 225
    },
    {
      "text": "By exploring these key aspects, we envision developing a self-managed inference-serving system, which can provide for different diverse needs of applications by leveraging the \n0 50 100 150 200 250 300 350 \n20.00% \n40.00% \n60.00% \n80.00% \n100.00% \nMobileNet V1 \nMobileNEt V2 \nInception V3 \nResnet50 \nResNet50-V2 \nDenseNet-201 \nDenseNet-121 \nXxception \nNasNetMobile \nInceptionResnetV2 \nvgg16 NasNetLarge \nLatency (ms) \nAccuracy % \nTop1-Accuracy Latency \nFigure 1. Accuracy and Latency of Different Pretrained Models. 0 \n20 \n40 \n60 \n80 \nPercentage \nModel Type \n(a)  Different accuracy for ISO- latency.",
      "type": "sliding_window",
      "tokens": 174
    },
    {
      "text": "0 \n20 \n40 \n60 \n80 \nPercentage \nModel Type \n(a)  Different accuracy for ISO- latency. 0 \n200 \n400 \n600 \n800 \n1000 \nTime(ms) \nModel Type \n(b)  Different response latencies for ISO-accuracy. Figure 2.",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "Figure 2. Comparison of different models under ISO-latency and ISO-accuracy setup. heterogeneous resource availability from the public cloud.",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": "heterogeneous resource availability from the public cloud. Towards this, we make the following  key contributions . 1.",
      "type": "sliding_window",
      "tokens": 26
    },
    {
      "text": "1. We comprehensively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource offerings and unravel the suitable model/resource configurations to meet the cost, latency and accuracy demands. 2.",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "2. We quantitatively evaluate prior works [ 5 ,  9 ,  10 ] which are geared towards achieving this vision and show that they still suffer from several issues when trying to solve the complex problem of combining model and resource heterogeneity. 3.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "3. We propose detailed design choices that can adopted to- wards designing a self-managed inference-serving system. In addition, we design a scheme named Paragon on top of AWS platform, which incorporates some of the proposed design choices.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "In addition, we design a scheme named Paragon on top of AWS platform, which incorporates some of the proposed design choices. Our initial results show that Paragon can reduce cost of hosting ML prediction serving by up to 20% when compared to the state-of-the-art prior works, for diverse accuracy and latency constraints. 2 Characterization and Motivation \n2.1 Variability across model types \nDepending on the accuracy and latency requirements of an end-user application, multiple models (shown in Figure  1 ) might satisfy a given constraint.",
      "type": "sliding_window",
      "tokens": 118
    },
    {
      "text": "2 Characterization and Motivation \n2.1 Variability across model types \nDepending on the accuracy and latency requirements of an end-user application, multiple models (shown in Figure  1 ) might satisfy a given constraint. For example, consider a face- recognition application that demands a response latency of under 500ms (ISO-latency). As shown in Figure  2a , four differ- ent models can satisfy the response latency, but each model comes with a different prediction accuracy.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "As shown in Figure  2a , four differ- ent models can satisfy the response latency, but each model comes with a different prediction accuracy. Similarly, if the \nImplications of Public Cloud Resource Heterogeneity for Inference Serving WoSC’20, December 7ś11, 2020, Delft, Netherlands \n0 \n10 \n20 \nCost($) \nVM cost \nLambda Cost \ninception                   nasnet densenet mobilenet \n(a)  Cost for ISO-latency models. 0 \n10 \n20 \nCost($) \nVM cost \nLambda Cost \ninception          resnet-200 resnext-50 nasnet \n(b)  Cost for ISO-accuracy models.",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "0 \n10 \n20 \nCost($) \nVM cost \nLambda Cost \ninception          resnet-200 resnext-50 nasnet \n(b)  Cost for ISO-accuracy models. Figure 3. Variation of cost of using VMs vs.  serverless functions  under constant request load.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Variation of cost of using VMs vs.  serverless functions  under constant request load. Each of the four bars under any model type corresponds to request arrival rates of 10, 50, 100, and 200 requests/second. same application requires accuracy to be at-least 80% (ISO- accuracy), as shown in Figure  2b , four different models with different response latencies can satisfy the accuracy.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "same application requires accuracy to be at-least 80% (ISO- accuracy), as shown in Figure  2b , four different models with different response latencies can satisfy the accuracy. There- fore, depending on the cost budget of the application, one can choose among the different model types by  trading-off accuracy or response latency . Hence, it is evident that there is a large optimization space where different models can be selected based upon the needs of the applications.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Hence, it is evident that there is a large optimization space where different models can be selected based upon the needs of the applications. Prior work  [ 9 ] tries to solve model selection only from a through- put perspective where different sized batching of multiple inference queries together results in varied throughput. Observation 1:  Model selection should be focused on meeting the cost requirement of an application without compromising on the accuracy and/or latency constraint.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Observation 1:  Model selection should be focused on meeting the cost requirement of an application without compromising on the accuracy and/or latency constraint. 2.2 Performance under given constraint \nModel selection is not an independent problem because the user-applications also have a cost constraint incurred as a result of procuring resources from the public cloud. We compare the cost of deploying the inference service on a group of virtual machines and  serverless functions .",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "We compare the cost of deploying the inference service on a group of virtual machines and  serverless functions . We use m4- large instances for VMs and we fix the number of inference queries each VM can handle in parallel, without violating response latencies based on our characterization on AWS EC2. The  serverless functions  are configured according to the memory requirements of each model.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "The  serverless functions  are configured according to the memory requirements of each model. Figure  3a  plots the cost of hosting the iso-latency model types (shown in Figure  2a ) for a constant request arrival rates of 10, 50, 100, 200 req/sec over 1 hour duration. It can be seen that virtual machines are always cheaper compared to using  serverless functions for all constant request rates.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "It can be seen that virtual machines are always cheaper compared to using  serverless functions for all constant request rates. A similar trend is observed for the iso-accuracy model types, which is shown in Figure  3b . It is also possible to use bigger VMs, which can handle more concurrent requests compared to m4-large, thus mini- mizing the total number of VMs used.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "It is also possible to use bigger VMs, which can handle more concurrent requests compared to m4-large, thus mini- mizing the total number of VMs used. However, we observe that the pricing of EC2 VMs is a linear function of the VM size in terms of compute capacity and memory. Hence, nor- malized by number of requests, bigger VMs would still incur similar costs as smaller VMs.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "Hence, nor- malized by number of requests, bigger VMs would still incur similar costs as smaller VMs. Observation 2:  VMs should be used to handle requests during constant arrival rates. Also, the number of concurrent requests which can be executed in VMs should be accurately determined to meet response latency.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "Also, the number of concurrent requests which can be executed in VMs should be accurately determined to meet response latency. 0 \n0.4 \n0.8 \n1.2 \n1.6 \nBerkley WITS Twitter Wiki \nNormalized VMs \nreactive util_aware exascale \nFigure 4. Over-provisioning of  util_aware  and  exascale , nor- malized to a baseline  reactive  scheme for four traces.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Over-provisioning of  util_aware  and  exascale , nor- malized to a baseline  reactive  scheme for four traces. 2.3 Over-provisioning VMs \nReal-world request arrivals rates are usually not constant as they significantly vary over time (e.g. diurnal, flash-crowds etc.)",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "diurnal, flash-crowds etc.) Therefore, resource procurement and management deci- sions need to be adjusted depending on the resource utiliza- tion/load and arrival rates. Public cloud providers leave these major decisions to be łmanually handled\" by users, which is very time consuming and strenuous.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "Public cloud providers leave these major decisions to be łmanually handled\" by users, which is very time consuming and strenuous. As a result, the major- ity of application providers use  static resource provisioning , which results in poor resource utilization and higher costs. Prior works  [ 3 ,  9 ] have tried to solve the resource scal- ing problem with respect to hosting the applications in VMs.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Prior works  [ 3 ,  9 ] have tried to solve the resource scal- ing problem with respect to hosting the applications in VMs. They employ autoscaling mechanisms to cope up with dy- namic load. These autoscaling mechanisms can be of two types: (i) spawn VMs if the resource utilization of existing VMs reaches a certain threshold (80% in most cases) [ 9 ], and (ii) spawn additional VMs than predicted request demand [ 6 ].",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "These autoscaling mechanisms can be of two types: (i) spawn VMs if the resource utilization of existing VMs reaches a certain threshold (80% in most cases) [ 9 ], and (ii) spawn additional VMs than predicted request demand [ 6 ]. We name the former autoscaling scheme as  util_aware  and the later as  exascale . Both these schemes suffer from over- provisioning VMs because (i) we cannot always accurately predict the future load, and (ii) resource utilization is not always the right indicator for increased load.",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "Both these schemes suffer from over- provisioning VMs because (i) we cannot always accurately predict the future load, and (ii) resource utilization is not always the right indicator for increased load. We conduct simulation experiments to compare the schemes, using the profiled values (explained in Section  2.2 ) for four different well-known request arrival traces. Each request in the trace is associated with an ML inference query, which is randomly picked from our model pool.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Each request in the trace is associated with an ML inference query, which is randomly picked from our model pool. Figure  4 shows the ratio of over-provisioned VMs compared to a baseline  reactive  autoscaling mechanism. It can be seen that although both  util_aware  and  exascale  can reduce SLO vi- olations (shown in Figure  5 ), they still suffer from 20% to 30% over-provisioned VMs across all four traces.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "It can be seen that although both  util_aware  and  exascale  can reduce SLO vi- olations (shown in Figure  5 ), they still suffer from 20% to 30% over-provisioned VMs across all four traces. This, in turn, increases the cost of deployment (shown in Figure  5 ), compared to baseline  reactive  scheme. WoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "WoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. 0 \n5 \n10 \n0 \n1 \n2 \nBerkley WITS Twitter Wiki \nSLO Violations \nNormalized Cost \nreactive util_aware exascale mixed \nFigure 5. Cost of using  mixed  compared to  util_aware  and  exascale , normalized to a baseline  reactive  scheme for four traces.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Cost of using  mixed  compared to  util_aware  and  exascale , normalized to a baseline  reactive  scheme for four traces. Percentage of SLO violations for each scheme are shown in the line graph (the corresponding color in the bar-graph is used for all the schemes.) 0 \n200 \n400 \n600 \n0 \n0.6 \n1.2 \n1.8 \n2.4 \n3 \nCost ($) \nModel Type \nexec_time(ms) \nMemory(GB) Cost($) \nFigure 7.",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "0 \n200 \n400 \n600 \n0 \n0.6 \n1.2 \n1.8 \n2.4 \n3 \nCost ($) \nModel Type \nexec_time(ms) \nMemory(GB) Cost($) \nFigure 7. Cost variation for different allocations in  serverless func- tions . Compute time (seconds) and Memory allocated (GB) is shown on left Y-axis.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "Compute time (seconds) and Memory allocated (GB) is shown on left Y-axis. Cost ($) is shown in right Y-axis. Observation 3:  Only-VM based resource procurement should not be used during dynamic load as it leads to over-provisioned resources and increased cost.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "Observation 3:  Only-VM based resource procurement should not be used during dynamic load as it leads to over-provisioned resources and increased cost. 2.4 Using  serverless functions  with VMs \nThe provisioning latency is a major contributor for VM over-provisioning during request surges be- cause the increased time to provision new VMs results in the increase of response latencies which in-turn leads to provisioning more VMs in advance. 0 \n500 \n1000 \n1500 \nwiki WITS berkley Twitter \nRequest Rate \nAvg Req \nMax Req \nFigure 6.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "0 \n500 \n1000 \n1500 \nwiki WITS berkley Twitter \nRequest Rate \nAvg Req \nMax Req \nFigure 6. Peak-to-median ratio. Prior works  [ 5 ,  10 ] try to hide the pro- visioning latency of VMs by using  server- less functions  as a handover mechanism when starting new VMs.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Prior works  [ 5 ,  10 ] try to hide the pro- visioning latency of VMs by using  server- less functions  as a handover mechanism when starting new VMs. We name this scheme as  mixed  pro- curement. However, these schemes do not address the holis- tic problem by taking into account model selection, resource selection, and resource scaling to cope up with user-specified constraints.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "However, these schemes do not address the holis- tic problem by taking into account model selection, resource selection, and resource scaling to cope up with user-specified constraints. We conduct similar experiments to mimic the mixed  procurement scheme. As shown in Figure  5 ,  mixed procurement reduces the over-provisioning cost of VMs.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "As shown in Figure  5 ,  mixed procurement reduces the over-provisioning cost of VMs. At the same time it also minimizes latency violations equivalent to  exascale  scheme. However, we argue that there is scope to further optimize resource procurement based on the fre- quency of peak load and constant load in a given request \narrival scenario.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "However, we argue that there is scope to further optimize resource procurement based on the fre- quency of peak load and constant load in a given request \narrival scenario. Figure  6  plots the peak-to-median ratio for three different traces. From our simulation experiments we observe that  mixed  procurement did not reduce cost of Wiki trace.",
      "type": "sliding_window",
      "tokens": 73
    },
    {
      "text": "From our simulation experiments we observe that  mixed  procurement did not reduce cost of Wiki trace. This is because the difference between peak-to-median in the traces are not large and therefore more functions get offloaded to  serverless functions . Thus, using  serverless func- tions  for such scenarios will not drastically reduce cost.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Thus, using  serverless func- tions  for such scenarios will not drastically reduce cost. For the other traces like Berkeley, WITS and Twitter, the peak- to-median difference is more than 50% and therefore they can benefit from offloading requests to  serverless functions . Observation 4:  It is important to note that, the request arrival pattern plays a key role in determining if mixed procurement can be cost effective.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Observation 4:  It is important to note that, the request arrival pattern plays a key role in determining if mixed procurement can be cost effective. 2.5 Challenges with  serverless functions \nApart from arrival rates, memory allocation to  serverless functions  play a non-trivial role in terms of cost. In our exper- iments we configure the memory allocation to the lambda function such that individual query latency is within the user-specified latency constraint.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "In our exper- iments we configure the memory allocation to the lambda function such that individual query latency is within the user-specified latency constraint. We conducted character- ization experiments on AWS Lambda, currently the most predominant serverless function provider, to study the mem- ory allocated vs computation time trade-off. Figure  7  shows the computation time and cost for executing 1 million infer- ence queries for three different model types with different memory allocations.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Figure  7  shows the computation time and cost for executing 1 million infer- ence queries for three different model types with different memory allocations. We vary the memory allocation, starting from least required memory for the model to the maximum available limit in AWS (3GB) 1 . It can be clearly seen that the computation time reduces with increased memory alloca- tion but also results in higher cost of deployment for every model type.",
      "type": "sliding_window",
      "tokens": 92
    },
    {
      "text": "It can be clearly seen that the computation time reduces with increased memory alloca- tion but also results in higher cost of deployment for every model type. This is because, inherently, serverless providers allocate a powerful compute core for functions with higher memory allocation. Therefore, depending on the latency re- quirements of the user applications,  serverless functions  need to be allocated the appropriate memory.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Therefore, depending on the latency re- quirements of the user applications,  serverless functions  need to be allocated the appropriate memory. However, this might result in increased cost when using  serverless functions  along with VMs for varying latency requirements. Hence, the over- all cost incurred by  mixed  procurement can be higher or lower than VM-only autoscaling policies.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "Hence, the over- all cost incurred by  mixed  procurement can be higher or lower than VM-only autoscaling policies. Prior work  like Cherrypick [ 1 ] solves the resource se- lection and configuration problems from VM perspective but does not consider Serverless Functions. We argue that compared to VMs there is more variability in configurations for  serverless functions  because the resources are billed at a more fine-grained 2   allocation of CPU and memory.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "We argue that compared to VMs there is more variability in configurations for  serverless functions  because the resources are billed at a more fine-grained 2   allocation of CPU and memory. Observation 5:  Serverless functions can be used with VMs to avoid over-provisioning resources, but the right configuration needs to be accurately determined for the functions such that it satisfies the application cost and latency constraints. 1 For squeezenet model, allocating beyond 2GB did not reduce computation time, but resulted in increased cost 2 The smallest standard performance VM (C4 family) comes with 2 vcpus and 3.75GB memory.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "1 For squeezenet model, allocating beyond 2GB did not reduce computation time, but resulted in increased cost 2 The smallest standard performance VM (C4 family) comes with 2 vcpus and 3.75GB memory. But serverless functions can be configured starting from 1 vcpu and 0.128GB memory. Implications of Public Cloud Resource Heterogeneity for Inference Serving WoSC’20, December 7ś11, 2020, Delft, Netherlands \n3 How to Design Self-Managed ML Prediction Serving System?",
      "type": "sliding_window",
      "tokens": 120
    },
    {
      "text": "Implications of Public Cloud Resource Heterogeneity for Inference Serving WoSC’20, December 7ś11, 2020, Delft, Netherlands \n3 How to Design Self-Managed ML Prediction Serving System? The objectives from Section  2  strongly motivate the need for a self-managed ML-prediction system that avoids the over-provisioning problem in VMs by efficiently blending serverless functions  with VMs. At the same time, right-sizing the number of requests in VMs and correctly configuring serverless functions  is quintessential to satisfy the three pri- mary application constraints: cost, latency, and accuracy.",
      "type": "sliding_window",
      "tokens": 147
    },
    {
      "text": "At the same time, right-sizing the number of requests in VMs and correctly configuring serverless functions  is quintessential to satisfy the three pri- mary application constraints: cost, latency, and accuracy. 3.1 Model Selection \nIn accordance with  Observation 1 , model selection should be a function of any two parameters which optimize the remain- ing (third) parameter. Prior work [ 9 ] solves an optimization problem such that the input parameters are model_type, hardware_type (CPU or GPU), and the output parameter is response latency.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "Prior work [ 9 ] solves an optimization problem such that the input parameters are model_type, hardware_type (CPU or GPU), and the output parameter is response latency. To do so, they suggest using offline profil- ing or results from previous executions. Unlike prior works, we suggest that the input and output parameters can be any linear combination of the three primary parameters men- tioned above, depending on the application constraints.",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "Unlike prior works, we suggest that the input and output parameters can be any linear combination of the three primary parameters men- tioned above, depending on the application constraints. Note that, in contrast to cost and response-latency, accuracy can- not be determined just from the previous runs. We need some feedback from the end-user to make a correct estimate of accuracy.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "We need some feedback from the end-user to make a correct estimate of accuracy. Therefore, it would be best to build a learning- based system, which takes into account feedback (user-given data) to build a novel model selection system. 3.2 Resource selection \n3.2.1 Static Load  From  Observation 2 , it is clear that, be- sides model selection, it is crucial to select and configure the right resource to satisfy the application constraints.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "3.2 Resource selection \n3.2.1 Static Load  From  Observation 2 , it is clear that, be- sides model selection, it is crucial to select and configure the right resource to satisfy the application constraints. For applications where the request load is fairly constant over time, only VM-based resources can be procured to serve the requests. To determine the number of requests each VM can handle in parallel, we can conduct offline profiling for different model types.",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "To determine the number of requests each VM can handle in parallel, we can conduct offline profiling for different model types. 3.2.2 Dynamic Load  For applications with dynamic load ( Observation 3 ),  serverless functions  can be used to mitigate the over-provisioning cost of VMs. However, a single ap- plication can contain a mix of queries with varying latency demands.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "However, a single ap- plication can contain a mix of queries with varying latency demands. Therefore, queries with strict latency requirements can be scheduled on  serverless functions , if a VM with free resources is unavailable. To handle dynamic load variations, a load-monitor can be designed such that it constantly moni- tors different periods of static load and peak load.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "To handle dynamic load variations, a load-monitor can be designed such that it constantly moni- tors different periods of static load and peak load. We propose to plug-in intelligent peak-to-median prediction policies (in accordance to  Observation 4 ) , which can aid the load-monitor to estimate the duration of static load. Furthermore, it can measure the peak-to-median ratio in sampling windows, which can be used to decide if  serverless functions  are re- quired to balance the load.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "Furthermore, it can measure the peak-to-median ratio in sampling windows, which can be used to decide if  serverless functions  are re- quired to balance the load. However, during flash-crowds, where load-prediction fails to accurately estimate the load, \nserverless functions  can inherently be used to handle requests to meet the response latency, but by incurring higher costs. 3.2.3 Provisioning Time vs Execution Time  We know that new VMs take a few hundred seconds to start-up.",
      "type": "sliding_window",
      "tokens": 117
    },
    {
      "text": "3.2.3 Provisioning Time vs Execution Time  We know that new VMs take a few hundred seconds to start-up. Server- less functions  can start-up much faster (1s-10s), but they also incur additional latency to load a pre-trained model from external data-store. Prior literature [ 5 ,  10 ] tries to hide the model load latency by pre-warming serverless function in- stances through periodically issuing dummy requests.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Prior literature [ 5 ,  10 ] tries to hide the model load latency by pre-warming serverless function in- stances through periodically issuing dummy requests. How- ever, such hacks can fail if the cloud service provider decides to change the idle timeout of function instances or change the overall mechanism to recycle idle function instances. Rather than capitalizing on such design hacks, we need to develop prediction policies to estimate load correctly.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Rather than capitalizing on such design hacks, we need to develop prediction policies to estimate load correctly. Also, we suggest service providers should handle the pre-warming decision by knowing model-wise usage statistics to enable instance sharing, which uses the same models. This would lead to a reduction in cold-start latencies incurred for users with the same type of requests.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "This would lead to a reduction in cold-start latencies incurred for users with the same type of requests. 3.2.4 Configuring Serverless Functions  In keeping with  Observation 5 , it is quintessential to configure the mem- ory allocation of  serverless functions  to meet the application SLOs. Through offline profiling or initial runs, we can de- termine the right memory allocation for a given response latency.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "Through offline profiling or initial runs, we can de- termine the right memory allocation for a given response latency. From our observations in  AWS Lambda , three types of cores are allocated in the increasing order of the memory allocation ( 0.5GB, 1.5GB, and  > 2GB ). Also, these policies can be changed over time by Amazon, and they can also be dif- ferent for other cloud providers [ 8 ].",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "Also, these policies can be changed over time by Amazon, and they can also be dif- ferent for other cloud providers [ 8 ]. Therefore, the resource manager should be able to leverage this information to make optimal serverless function configuration decisions. 4 Evaluation and Initial Results \nThis section introduces how an ML-serving framework can capitalize on the design choices discussed in Section  3 .",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "4 Evaluation and Initial Results \nThis section introduces how an ML-serving framework can capitalize on the design choices discussed in Section  3 . We design three different experiments to study the effects on cost of ML servings due to (i) varying SLOs and (ii) varying application constraints. Implementation Methodology:  We developed a proto- type on top of Amazon EC2 and Lambda services to evaluate the some of the benefits of our proposed design choices.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Implementation Methodology:  We developed a proto- type on top of Amazon EC2 and Lambda services to evaluate the some of the benefits of our proposed design choices. We use AWS as the testbed for conducting extensive experiments. The types of instance used in our evaluation include all the c5 and m5 instances for EC2.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "The types of instance used in our evaluation include all the c5 and m5 instances for EC2. By offline profiling, we estimate the number of model instances each VM can execute in par- allel without violating the model latency. Also, we estimate the right configuration of lambda functions by conduction offline experiments.",
      "type": "sliding_window",
      "tokens": 72
    },
    {
      "text": "Also, we estimate the right configuration of lambda functions by conduction offline experiments. For the model selection problem, we maintain an offline model cache which consists of the de- tails of individual model latency and accuracy profiled by executing on c4 ˙ large VM. The scheduler will pick the right model combinations from the cache based on the application requirements.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "The scheduler will pick the right model combinations from the cache based on the application requirements. We implement a load generator, which uses a \nWoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. 0 \n1 \n2 \n3 \n4 \n0 \n0.5 \n1 \n1.5 \nutil_aware exascale mixed paragon \nSLO Violations \nNormalized Cost \nCost SLO violations \n(a)  Workload-1: Berkeley Trace.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "0 \n1 \n2 \n3 \n4 \n0 \n0.5 \n1 \n1.5 \nutil_aware exascale mixed paragon \nSLO Violations \nNormalized Cost \nCost SLO violations \n(a)  Workload-1: Berkeley Trace. 0 \n5 \n10 \n0 \n0.5 \n1 \n1.5 \nutil_aware exascale mixed paragon \nSLO Violations \nNormalized Cost \nCost SLO violations \n(b)  Workload-1: WITS Trace. Figure 8.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "Figure 8. Comparison of the resource procurement cost for two different traces using five different schemes. The cost is normalized to reactive scaling scheme.",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "The cost is normalized to reactive scaling scheme. 1 hour sample of the real-world trace for request arrival time generation. Each request is derived from a pool of pre-trained ML inference models for image classification (as explained in Section  2 ).",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "Each request is derived from a pool of pre-trained ML inference models for image classification (as explained in Section  2 ). We use  Apache MXNet  and  TensorFlow  frame- work to deploy and run inference on the models. Evaluation:  We evaluate our results by comparing the cost, latency and accuracy for two different workloads.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Evaluation:  We evaluate our results by comparing the cost, latency and accuracy for two different workloads. Workload- 1 consists of a mix of queries which have both strict and relaxed latency requirements. We compare the execution of this workload against the following resource procure- ment schemes: (i)  util_aware , (ii)  exascale , (iii)  mixed  and (iv) Paragon .",
      "type": "sliding_window",
      "tokens": 96
    },
    {
      "text": "We compare the execution of this workload against the following resource procure- ment schemes: (i)  util_aware , (ii)  exascale , (iii)  mixed  and (iv) Paragon . These schemes are modeled after state-of-the-art prior works as explained earlier in Section  2.3 . The  Paragon scheme does not offload to lambdas for relaxed latency queries.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "The  Paragon scheme does not offload to lambdas for relaxed latency queries. Workload-2 consists of different cost, accuracy and latency requirements for all queries. We compare the  Paragon model selection scheme against a naive constraints-unaware model selection scheme.",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "We compare the  Paragon model selection scheme against a naive constraints-unaware model selection scheme. Results:  Figure  8  plots the SLO and cost for workload-1 across Berkley and WITS trace. It can be seen that mixed scheme has similar cost to reactive but it reduces SLO vi- olations by up to 60%.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "It can be seen that mixed scheme has similar cost to reactive but it reduces SLO vi- olations by up to 60%. This is because, the mixed scheme offloads request in the peak to serverless functions. However, the  Paragon  scheme is 10% more cost-effective than mixed and at the same time ensures similar SLOs.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "However, the  Paragon  scheme is 10% more cost-effective than mixed and at the same time ensures similar SLOs. This is because the  Paragon  scheme is aware of the latency requirements of individual queries and does not blindly offload queries to lambdas when there is increase in load. Therefore, this results in reduced cost and at the same time does not violate SLOs.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "Therefore, this results in reduced cost and at the same time does not violate SLOs. This re-instantiates our claim that the resource procurement scheme needs to be aware of request constraints. Figure  1  shows the candidate models which can be used for a given latency and accuracy.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "Figure  1  shows the candidate models which can be used for a given latency and accuracy. Our  Paragon  scheme optimizes the model selection for workload-2 such that, it chooses the least cost-effective model for the given accuracy and latency constraint. The naive model selection policy would not choose the models as its oblivious to user requirements and model characteristics.",
      "type": "sliding_window",
      "tokens": 80
    },
    {
      "text": "The naive model selection policy would not choose the models as its oblivious to user requirements and model characteristics. In our experiments, compared to naive selection scheme which does not optimize model selection for cost, the  Paragon  schemes reduces the cost of resource procurement by up to 20% (results are not plotted). This is because the  Paragon  scheme jointly considers all three parameters and chooses the least costing model.",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "This is because the  Paragon  scheme jointly considers all three parameters and chooses the least costing model. 5 Conclusion \nThere is wide-spread prominence in the adoption of ML- based prediction systems spanning across a wide range of application domains. The critical challenge of deploying ML prediction serving applications in public cloud is to combine both model and resource heterogeneity towards optimizing for application constraints.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "The critical challenge of deploying ML prediction serving applications in public cloud is to combine both model and resource heterogeneity towards optimizing for application constraints. In this paper, we propose to build a self-managed ML prediction system, which can optimize the diverse application requirements based on characteristics of heterogeneous public cloud resource offerings. Towards this, we discuss the trade-offs of intermixing resources like serverless functions along with VMs and identify the key challenges associated with configuring these resources.",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "Towards this, we discuss the trade-offs of intermixing resources like serverless functions along with VMs and identify the key challenges associated with configuring these resources. We propose multiple key-policies to make resource manage- ment; (i) latency aware, (ii) multi-dimensional SLO aware, and (iii) request load variation aware. These policies can be collectively used for cost-effective prediction serving with- out compromising on latency and accuracy.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "These policies can be collectively used for cost-effective prediction serving with- out compromising on latency and accuracy. Acknowledgement \nThis research was partially supported by NSF grants #1931531, #1955815, #1629129, #1763681, #1629915, #1908793, #1526750 and we thank NSF Chameleon Cloud project CH- 819640 for their generous compute grant. References \n[1]  Omid Alipourfard, Hongqiang Harry Liu, Jianshu Chen, Shivaram Venkataraman, Minlan Yu, and Ming Zhang.",
      "type": "sliding_window",
      "tokens": 133
    },
    {
      "text": "References \n[1]  Omid Alipourfard, Hongqiang Harry Liu, Jianshu Chen, Shivaram Venkataraman, Minlan Yu, and Ming Zhang. 2017. CherryPick: Adap- tively Unearthing the Best Cloud Configurations for Big Data Analytics.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "CherryPick: Adap- tively Unearthing the Best Cloud Configurations for Big Data Analytics. In  (NSDI) . [2]  Marc Brooker, Andreea Florescu, Diana-Maria Popa, Rolf Neugebauer, Alexandru Agache, Alexandra Iordache, Anthony Liguori, and Phil Piwonka.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "[2]  Marc Brooker, Andreea Florescu, Diana-Maria Popa, Rolf Neugebauer, Alexandru Agache, Alexandra Iordache, Anthony Liguori, and Phil Piwonka. 2020. Firecracker: Lightweight Virtualization for Serverless Applications.",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Firecracker: Lightweight Virtualization for Serverless Applications. In  NSDI . [3]  Andrew Chung, Jun Woo Park, and Gregory R. Ganger.",
      "type": "sliding_window",
      "tokens": 39
    },
    {
      "text": "[3]  Andrew Chung, Jun Woo Park, and Gregory R. Ganger. 2018. Stratus: Cost-aware Container Scheduling in the Public Cloud.",
      "type": "sliding_window",
      "tokens": 38
    },
    {
      "text": "Stratus: Cost-aware Container Scheduling in the Public Cloud. In  SoCC . [4]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "[4]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. 2017. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. In  USENIX Middleware Conference . [5]  J. R. Gunasekaran, P. Thinakaran, et al .",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "[5]  J. R. Gunasekaran, P. Thinakaran, et al . 2019. Spock: Exploiting Server- less Functions for SLO and Cost Aware Resource Procurement in Public Cloud.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Spock: Exploiting Server- less Functions for SLO and Cost Aware Resource Procurement in Public Cloud. In  CLOUD . [6] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "[6] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons. 2018. Tributary: spot-dancing for elastic ser- vices with latency SLOs.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "Tributary: spot-dancing for elastic ser- vices with latency SLOs. In  ATC . [7]  Akshitha Sriraman, Abhishek Dhanotia, and Thomas Wenisch.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "[7]  Akshitha Sriraman, Abhishek Dhanotia, and Thomas Wenisch. 2019. Softsku: Optimizing server architectures for microservice diversity@ scale.",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "Softsku: Optimizing server architectures for microservice diversity@ scale. In  ISCA . [8]  Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "[8]  Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift. 2018. Peeking Behind the Curtains of Serverless Plat- forms.",
      "type": "sliding_window",
      "tokens": 48
    },
    {
      "text": "Peeking Behind the Curtains of Serverless Plat- forms. In  ATC . [9]  Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "[9]  Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. 2019. A Case for Managed and Model-Less Inference Serv- ing.",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "A Case for Managed and Model-Less Inference Serv- ing. In  HotOS . ACM.",
      "type": "sliding_window",
      "tokens": 28
    },
    {
      "text": "ACM. https://doi.org/10.1145/3317550.3321443 [10]  Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. 2019.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "2019. MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving. In  ATC .",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "These applications typically have diverse requirements in terms of accuracy and response latency, that can be satisfied by a myriad of ML models. Abstract \nWe are witnessing an increasing trend towards using Ma- chine Learning (ML) based prediction systems, spanning across different application domains, including product rec- ommendation systems, personal assistant devices, facial recognition, etc. However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by them- selves are heterogeneous in terms of provisioning latencies and billing complexity.",
      "type": "sliding_window_shuffled",
      "tokens": 125,
      "augmented": true
    },
    {
      "text": "In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource of- ferings. Thus, it is strenuous for an infer- ence serving system to choose from this confounding array of resource types and model types to provide low-latency and cost-effective inferences. However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by them- selves are heterogeneous in terms of provisioning latencies and billing complexity.",
      "type": "sliding_window_shuffled",
      "tokens": 124,
      "augmented": true
    },
    {
      "text": "In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource of- ferings. Our evaluation shows that, prior work does not solve the problem from both dimensions of model and resource heterogeneity. Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "CCS Concepts:  ·  Computer systems organization  → Real-time system architecture . Towards this, we discuss the design implications of a self-managed inference serving system, which can optimize for application requirements based on public cloud resource characteristics. Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "CCS Concepts:  ·  Computer systems organization  → Real-time system architecture . Copyrights for components of this work owned by others than ACM must be honored. Keywords:  serverless, resource-management, inference \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyrights for components of this work owned by others than ACM must be honored.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. WoSC’20, December 7ś11, 2020, Delft, Netherlands © 2020 Association for Computing Machinery.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "WoSC’20, December 7ś11, 2020, Delft, Netherlands © 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8204-5/20/12...$15.00 https://doi.org/10.1145/3429880.3430093 \nACM Reference Format: Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thi- nakaran, Mahmut Taylan Kandemir, and Chita R. Das. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "Im- plications of Public Cloud Resource Heterogeneity for Inference Serving. In  Workshop on Serverless Computing (WoSC’20), December 7ś11, 2020, Delft, Netherlands. 2020.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "https://doi.org/10.1145/3429880.3430093 \n1 Introduction \nSustained advances in ML has fueled the proliferation of emerging applications such as product recommendation sys- tems, facial recognition systems, and intelligent personal assistants [ 7 ]. ACM, New York, NY, USA,  6  pages. In  Workshop on Serverless Computing (WoSC’20), December 7ś11, 2020, Delft, Netherlands.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "A typical DNN model has two different phases, namely,  training  and inference . Among many ML paradigms, Deep Neural Net- works (DNNs), owing to their generalization and massively- parallel nature, has been predominant in making all these applications pervasive and accessible to developers. https://doi.org/10.1145/3429880.3430093 \n1 Introduction \nSustained advances in ML has fueled the proliferation of emerging applications such as product recommendation sys- tems, facial recognition systems, and intelligent personal assistants [ 7 ].",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "The trained models can then be used to perform inferences, i.e., the clas- sification task. A typical DNN model has two different phases, namely,  training  and inference . Training a DNN, which is the process of extracting and learning the patterns and the features from millions of sample-data, typically takes a few hours to days.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "Conventionally, training is much more compute intensive (compared to an inference), takes many iterations and hence has been given considerable attention for better accuracy and convergence time. The trained models can then be used to perform inferences, i.e., the clas- sification task. Since typical large scale DNNs have millions of parameters and perform billions of multiplications and accumulations for executing a single inference, they are typ- ically hosted as web-services, which are often queried for predictions.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "Conventionally, training is much more compute intensive (compared to an inference), takes many iterations and hence has been given considerable attention for better accuracy and convergence time. These inference queries are typically administered with strict response laten- cies of under one second [ 4 ]. However, given the preva- lence and demand of inferences, serving them on public cloud with a tight bound of latency, throughput and cost is becoming increasingly more challenging [ 7 ].",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "To ensure a required accuracy with given latency, applications have to choose from a confounding array of different types of models (shown in Figure  1 ). These inference queries are typically administered with strict response laten- cies of under one second [ 4 ]. Based on the application needs, prediction queries require different compute resources, and have different accuracy, latency, and cost requirements.",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "WoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. Therefore, it is  non-trivial for an application to choose the right model that can collectively optimize for all requirements together . To ensure a required accuracy with given latency, applications have to choose from a confounding array of different types of models (shown in Figure  1 ).",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "The deployment costs differ based on the provisioning times and longevity of the resource pro- cured. WoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. Unlike accuracy and latency, which depends on the right model, cost is dictated by the type of deployment used to host them in a public cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "Typically, these inference serving systems are hosted using Virtual Machines (VMs), which take a few minutes to start-up. The deployment costs differ based on the provisioning times and longevity of the resource pro- cured. Due to high start-up latencies, using VMs for hosting ML services can lead to over-provisioning, espe- cially during periods of poor workload predictability (flash crowds) [ 10 ].",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "In stark contrast to VMs, serverless functions have been made available by cloud providers, which can spin-up within a few seconds [ 2 ]. Due to high start-up latencies, using VMs for hosting ML services can lead to over-provisioning, espe- cially during periods of poor workload predictability (flash crowds) [ 10 ]. As we will discuss in this paper, the cost of using VMs vs. serverless functions highly depends on the dynamically varying needs of the user query submission rates.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "As we will discuss in this paper, the cost of using VMs vs. serverless functions highly depends on the dynamically varying needs of the user query submission rates. Besides workload arrival rates, there is fur- ther variability in terms of configuring serverless functions to meet the end-user demands of latency and cost require- ments. This is because, serverless functions are billed based on of number of invocations, compute time and memory requirement of the function.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "However, increased memory allocation leads to faster execution time owing to powerful compute-core allocation, but exacerbates the billing cost. Therefore, these apparent deficiencies of choosing the appropriate resource type and model type for a given user re- quirement motivates the central question of this work:  Does there exist an optimal resource procurement system which can balance the goals of diverse user requirements for accuracy, latency and cost, by efficiently mapping model parameters to heterogeneous resource specifications? This is because, serverless functions are billed based on of number of invocations, compute time and memory requirement of the function.",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "Our preliminary results suggest that using a combination of VMs and serverless func- tions could potentially provide a solution to this problem. Therefore, these apparent deficiencies of choosing the appropriate resource type and model type for a given user re- quirement motivates the central question of this work:  Does there exist an optimal resource procurement system which can balance the goals of diverse user requirements for accuracy, latency and cost, by efficiently mapping model parameters to heterogeneous resource specifications? As opposed to prior works [ 5 ,  10 ], which try to combine serverless functions with VMs to hide the start-up latencies of VMs, our primary interest lies in exploring the different key aspects  to address when hosting DNN-based ML pre- diction serving systems in public cloud, as given below: •  Diverse Models:  How to make the users oblivious of model selection from the extensive pool of models, for satis- fying the accuracy, and latency requirements?",
      "type": "sliding_window_shuffled",
      "tokens": 217,
      "augmented": true
    },
    {
      "text": "As opposed to prior works [ 5 ,  10 ], which try to combine serverless functions with VMs to hide the start-up latencies of VMs, our primary interest lies in exploring the different key aspects  to address when hosting DNN-based ML pre- diction serving systems in public cloud, as given below: •  Diverse Models:  How to make the users oblivious of model selection from the extensive pool of models, for satis- fying the accuracy, and latency requirements? •  Heterogeneous Public Cloud Resources:  What are the different options available in terms of combining different VM-based cloud services and serverless functions for a given user requirement? •  Configuring Resources:  From the diverse options, how to right-size VMs and appropriately configure the serverless functions to efficiently cater to user specified cost, accuracy and latency constraint?",
      "type": "sliding_window_shuffled",
      "tokens": 193,
      "augmented": true
    },
    {
      "text": "By exploring these key aspects, we envision developing a self-managed inference-serving system, which can provide for different diverse needs of applications by leveraging the \n0 50 100 150 200 250 300 350 \n20.00% \n40.00% \n60.00% \n80.00% \n100.00% \nMobileNet V1 \nMobileNEt V2 \nInception V3 \nResnet50 \nResNet50-V2 \nDenseNet-201 \nDenseNet-121 \nXxception \nNasNetMobile \nInceptionResnetV2 \nvgg16 NasNetLarge \nLatency (ms) \nAccuracy % \nTop1-Accuracy Latency \nFigure 1. •  Bring in Tune : Based on the dynamically changing query arrivals over time, what is the right way to combine model diversity along with resource heterogeneity without com- promising the user-specified requirements? •  Configuring Resources:  From the diverse options, how to right-size VMs and appropriately configure the serverless functions to efficiently cater to user specified cost, accuracy and latency constraint?",
      "type": "sliding_window_shuffled",
      "tokens": 225,
      "augmented": true
    },
    {
      "text": "0 \n20 \n40 \n60 \n80 \nPercentage \nModel Type \n(a)  Different accuracy for ISO- latency. Accuracy and Latency of Different Pretrained Models. By exploring these key aspects, we envision developing a self-managed inference-serving system, which can provide for different diverse needs of applications by leveraging the \n0 50 100 150 200 250 300 350 \n20.00% \n40.00% \n60.00% \n80.00% \n100.00% \nMobileNet V1 \nMobileNEt V2 \nInception V3 \nResnet50 \nResNet50-V2 \nDenseNet-201 \nDenseNet-121 \nXxception \nNasNetMobile \nInceptionResnetV2 \nvgg16 NasNetLarge \nLatency (ms) \nAccuracy % \nTop1-Accuracy Latency \nFigure 1.",
      "type": "sliding_window_shuffled",
      "tokens": 174,
      "augmented": true
    },
    {
      "text": "Figure 2. 0 \n200 \n400 \n600 \n800 \n1000 \nTime(ms) \nModel Type \n(b)  Different response latencies for ISO-accuracy. 0 \n20 \n40 \n60 \n80 \nPercentage \nModel Type \n(a)  Different accuracy for ISO- latency.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "heterogeneous resource availability from the public cloud. Comparison of different models under ISO-latency and ISO-accuracy setup. Figure 2.",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": "heterogeneous resource availability from the public cloud. Towards this, we make the following  key contributions . 1.",
      "type": "sliding_window_shuffled",
      "tokens": 26,
      "augmented": true
    },
    {
      "text": "1. 2. We comprehensively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource offerings and unravel the suitable model/resource configurations to meet the cost, latency and accuracy demands.",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "2. 3. We quantitatively evaluate prior works [ 5 ,  9 ,  10 ] which are geared towards achieving this vision and show that they still suffer from several issues when trying to solve the complex problem of combining model and resource heterogeneity.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "In addition, we design a scheme named Paragon on top of AWS platform, which incorporates some of the proposed design choices. 3. We propose detailed design choices that can adopted to- wards designing a self-managed inference-serving system.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "In addition, we design a scheme named Paragon on top of AWS platform, which incorporates some of the proposed design choices. 2 Characterization and Motivation \n2.1 Variability across model types \nDepending on the accuracy and latency requirements of an end-user application, multiple models (shown in Figure  1 ) might satisfy a given constraint. Our initial results show that Paragon can reduce cost of hosting ML prediction serving by up to 20% when compared to the state-of-the-art prior works, for diverse accuracy and latency constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 118,
      "augmented": true
    },
    {
      "text": "2 Characterization and Motivation \n2.1 Variability across model types \nDepending on the accuracy and latency requirements of an end-user application, multiple models (shown in Figure  1 ) might satisfy a given constraint. As shown in Figure  2a , four differ- ent models can satisfy the response latency, but each model comes with a different prediction accuracy. For example, consider a face- recognition application that demands a response latency of under 500ms (ISO-latency).",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Similarly, if the \nImplications of Public Cloud Resource Heterogeneity for Inference Serving WoSC’20, December 7ś11, 2020, Delft, Netherlands \n0 \n10 \n20 \nCost($) \nVM cost \nLambda Cost \ninception                   nasnet densenet mobilenet \n(a)  Cost for ISO-latency models. As shown in Figure  2a , four differ- ent models can satisfy the response latency, but each model comes with a different prediction accuracy. 0 \n10 \n20 \nCost($) \nVM cost \nLambda Cost \ninception          resnet-200 resnext-50 nasnet \n(b)  Cost for ISO-accuracy models.",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "0 \n10 \n20 \nCost($) \nVM cost \nLambda Cost \ninception          resnet-200 resnext-50 nasnet \n(b)  Cost for ISO-accuracy models. Variation of cost of using VMs vs.  serverless functions  under constant request load. Figure 3.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Variation of cost of using VMs vs.  serverless functions  under constant request load. same application requires accuracy to be at-least 80% (ISO- accuracy), as shown in Figure  2b , four different models with different response latencies can satisfy the accuracy. Each of the four bars under any model type corresponds to request arrival rates of 10, 50, 100, and 200 requests/second.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "Hence, it is evident that there is a large optimization space where different models can be selected based upon the needs of the applications. same application requires accuracy to be at-least 80% (ISO- accuracy), as shown in Figure  2b , four different models with different response latencies can satisfy the accuracy. There- fore, depending on the cost budget of the application, one can choose among the different model types by  trading-off accuracy or response latency .",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Observation 1:  Model selection should be focused on meeting the cost requirement of an application without compromising on the accuracy and/or latency constraint. Prior work  [ 9 ] tries to solve model selection only from a through- put perspective where different sized batching of multiple inference queries together results in varied throughput. Hence, it is evident that there is a large optimization space where different models can be selected based upon the needs of the applications.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "2.2 Performance under given constraint \nModel selection is not an independent problem because the user-applications also have a cost constraint incurred as a result of procuring resources from the public cloud. We compare the cost of deploying the inference service on a group of virtual machines and  serverless functions . Observation 1:  Model selection should be focused on meeting the cost requirement of an application without compromising on the accuracy and/or latency constraint.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "We compare the cost of deploying the inference service on a group of virtual machines and  serverless functions . The  serverless functions  are configured according to the memory requirements of each model. We use m4- large instances for VMs and we fix the number of inference queries each VM can handle in parallel, without violating response latencies based on our characterization on AWS EC2.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "Figure  3a  plots the cost of hosting the iso-latency model types (shown in Figure  2a ) for a constant request arrival rates of 10, 50, 100, 200 req/sec over 1 hour duration. It can be seen that virtual machines are always cheaper compared to using  serverless functions for all constant request rates. The  serverless functions  are configured according to the memory requirements of each model.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "It is also possible to use bigger VMs, which can handle more concurrent requests compared to m4-large, thus mini- mizing the total number of VMs used. It can be seen that virtual machines are always cheaper compared to using  serverless functions for all constant request rates. A similar trend is observed for the iso-accuracy model types, which is shown in Figure  3b .",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "However, we observe that the pricing of EC2 VMs is a linear function of the VM size in terms of compute capacity and memory. Hence, nor- malized by number of requests, bigger VMs would still incur similar costs as smaller VMs. It is also possible to use bigger VMs, which can handle more concurrent requests compared to m4-large, thus mini- mizing the total number of VMs used.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "Also, the number of concurrent requests which can be executed in VMs should be accurately determined to meet response latency. Observation 2:  VMs should be used to handle requests during constant arrival rates. Hence, nor- malized by number of requests, bigger VMs would still incur similar costs as smaller VMs.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "0 \n0.4 \n0.8 \n1.2 \n1.6 \nBerkley WITS Twitter Wiki \nNormalized VMs \nreactive util_aware exascale \nFigure 4. Also, the number of concurrent requests which can be executed in VMs should be accurately determined to meet response latency. Over-provisioning of  util_aware  and  exascale , nor- malized to a baseline  reactive  scheme for four traces.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "diurnal, flash-crowds etc.) 2.3 Over-provisioning VMs \nReal-world request arrivals rates are usually not constant as they significantly vary over time (e.g. Over-provisioning of  util_aware  and  exascale , nor- malized to a baseline  reactive  scheme for four traces.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Therefore, resource procurement and management deci- sions need to be adjusted depending on the resource utiliza- tion/load and arrival rates. diurnal, flash-crowds etc.) Public cloud providers leave these major decisions to be łmanually handled\" by users, which is very time consuming and strenuous.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "As a result, the major- ity of application providers use  static resource provisioning , which results in poor resource utilization and higher costs. Public cloud providers leave these major decisions to be łmanually handled\" by users, which is very time consuming and strenuous. Prior works  [ 3 ,  9 ] have tried to solve the resource scal- ing problem with respect to hosting the applications in VMs.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "They employ autoscaling mechanisms to cope up with dy- namic load. These autoscaling mechanisms can be of two types: (i) spawn VMs if the resource utilization of existing VMs reaches a certain threshold (80% in most cases) [ 9 ], and (ii) spawn additional VMs than predicted request demand [ 6 ]. Prior works  [ 3 ,  9 ] have tried to solve the resource scal- ing problem with respect to hosting the applications in VMs.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Both these schemes suffer from over- provisioning VMs because (i) we cannot always accurately predict the future load, and (ii) resource utilization is not always the right indicator for increased load. We name the former autoscaling scheme as  util_aware  and the later as  exascale . These autoscaling mechanisms can be of two types: (i) spawn VMs if the resource utilization of existing VMs reaches a certain threshold (80% in most cases) [ 9 ], and (ii) spawn additional VMs than predicted request demand [ 6 ].",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "Both these schemes suffer from over- provisioning VMs because (i) we cannot always accurately predict the future load, and (ii) resource utilization is not always the right indicator for increased load. Each request in the trace is associated with an ML inference query, which is randomly picked from our model pool. We conduct simulation experiments to compare the schemes, using the profiled values (explained in Section  2.2 ) for four different well-known request arrival traces.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "It can be seen that although both  util_aware  and  exascale  can reduce SLO vi- olations (shown in Figure  5 ), they still suffer from 20% to 30% over-provisioned VMs across all four traces. Each request in the trace is associated with an ML inference query, which is randomly picked from our model pool. Figure  4 shows the ratio of over-provisioned VMs compared to a baseline  reactive  autoscaling mechanism.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "This, in turn, increases the cost of deployment (shown in Figure  5 ), compared to baseline  reactive  scheme. WoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. It can be seen that although both  util_aware  and  exascale  can reduce SLO vi- olations (shown in Figure  5 ), they still suffer from 20% to 30% over-provisioned VMs across all four traces.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "0 \n5 \n10 \n0 \n1 \n2 \nBerkley WITS Twitter Wiki \nSLO Violations \nNormalized Cost \nreactive util_aware exascale mixed \nFigure 5. Cost of using  mixed  compared to  util_aware  and  exascale , normalized to a baseline  reactive  scheme for four traces. WoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Percentage of SLO violations for each scheme are shown in the line graph (the corresponding color in the bar-graph is used for all the schemes.) 0 \n200 \n400 \n600 \n0 \n0.6 \n1.2 \n1.8 \n2.4 \n3 \nCost ($) \nModel Type \nexec_time(ms) \nMemory(GB) Cost($) \nFigure 7. Cost of using  mixed  compared to  util_aware  and  exascale , normalized to a baseline  reactive  scheme for four traces.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "0 \n200 \n400 \n600 \n0 \n0.6 \n1.2 \n1.8 \n2.4 \n3 \nCost ($) \nModel Type \nexec_time(ms) \nMemory(GB) Cost($) \nFigure 7. Cost variation for different allocations in  serverless func- tions . Compute time (seconds) and Memory allocated (GB) is shown on left Y-axis.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "Observation 3:  Only-VM based resource procurement should not be used during dynamic load as it leads to over-provisioned resources and increased cost. Cost ($) is shown in right Y-axis. Compute time (seconds) and Memory allocated (GB) is shown on left Y-axis.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "Observation 3:  Only-VM based resource procurement should not be used during dynamic load as it leads to over-provisioned resources and increased cost. 0 \n500 \n1000 \n1500 \nwiki WITS berkley Twitter \nRequest Rate \nAvg Req \nMax Req \nFigure 6. 2.4 Using  serverless functions  with VMs \nThe provisioning latency is a major contributor for VM over-provisioning during request surges be- cause the increased time to provision new VMs results in the increase of response latencies which in-turn leads to provisioning more VMs in advance.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "Peak-to-median ratio. Prior works  [ 5 ,  10 ] try to hide the pro- visioning latency of VMs by using  server- less functions  as a handover mechanism when starting new VMs. 0 \n500 \n1000 \n1500 \nwiki WITS berkley Twitter \nRequest Rate \nAvg Req \nMax Req \nFigure 6.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "However, these schemes do not address the holis- tic problem by taking into account model selection, resource selection, and resource scaling to cope up with user-specified constraints. We name this scheme as  mixed  pro- curement. Prior works  [ 5 ,  10 ] try to hide the pro- visioning latency of VMs by using  server- less functions  as a handover mechanism when starting new VMs.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "As shown in Figure  5 ,  mixed procurement reduces the over-provisioning cost of VMs. However, these schemes do not address the holis- tic problem by taking into account model selection, resource selection, and resource scaling to cope up with user-specified constraints. We conduct similar experiments to mimic the mixed  procurement scheme.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "As shown in Figure  5 ,  mixed procurement reduces the over-provisioning cost of VMs. However, we argue that there is scope to further optimize resource procurement based on the fre- quency of peak load and constant load in a given request \narrival scenario. At the same time it also minimizes latency violations equivalent to  exascale  scheme.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "However, we argue that there is scope to further optimize resource procurement based on the fre- quency of peak load and constant load in a given request \narrival scenario. From our simulation experiments we observe that  mixed  procurement did not reduce cost of Wiki trace. Figure  6  plots the peak-to-median ratio for three different traces.",
      "type": "sliding_window_shuffled",
      "tokens": 73,
      "augmented": true
    },
    {
      "text": "Thus, using  serverless func- tions  for such scenarios will not drastically reduce cost. From our simulation experiments we observe that  mixed  procurement did not reduce cost of Wiki trace. This is because the difference between peak-to-median in the traces are not large and therefore more functions get offloaded to  serverless functions .",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Thus, using  serverless func- tions  for such scenarios will not drastically reduce cost. Observation 4:  It is important to note that, the request arrival pattern plays a key role in determining if mixed procurement can be cost effective. For the other traces like Berkeley, WITS and Twitter, the peak- to-median difference is more than 50% and therefore they can benefit from offloading requests to  serverless functions .",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "2.5 Challenges with  serverless functions \nApart from arrival rates, memory allocation to  serverless functions  play a non-trivial role in terms of cost. In our exper- iments we configure the memory allocation to the lambda function such that individual query latency is within the user-specified latency constraint. Observation 4:  It is important to note that, the request arrival pattern plays a key role in determining if mixed procurement can be cost effective.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "We conducted character- ization experiments on AWS Lambda, currently the most predominant serverless function provider, to study the mem- ory allocated vs computation time trade-off. In our exper- iments we configure the memory allocation to the lambda function such that individual query latency is within the user-specified latency constraint. Figure  7  shows the computation time and cost for executing 1 million infer- ence queries for three different model types with different memory allocations.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "Figure  7  shows the computation time and cost for executing 1 million infer- ence queries for three different model types with different memory allocations. We vary the memory allocation, starting from least required memory for the model to the maximum available limit in AWS (3GB) 1 . It can be clearly seen that the computation time reduces with increased memory alloca- tion but also results in higher cost of deployment for every model type.",
      "type": "sliding_window_shuffled",
      "tokens": 92,
      "augmented": true
    },
    {
      "text": "It can be clearly seen that the computation time reduces with increased memory alloca- tion but also results in higher cost of deployment for every model type. Therefore, depending on the latency re- quirements of the user applications,  serverless functions  need to be allocated the appropriate memory. This is because, inherently, serverless providers allocate a powerful compute core for functions with higher memory allocation.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Hence, the over- all cost incurred by  mixed  procurement can be higher or lower than VM-only autoscaling policies. However, this might result in increased cost when using  serverless functions  along with VMs for varying latency requirements. Therefore, depending on the latency re- quirements of the user applications,  serverless functions  need to be allocated the appropriate memory.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "Hence, the over- all cost incurred by  mixed  procurement can be higher or lower than VM-only autoscaling policies. We argue that compared to VMs there is more variability in configurations for  serverless functions  because the resources are billed at a more fine-grained 2   allocation of CPU and memory. Prior work  like Cherrypick [ 1 ] solves the resource se- lection and configuration problems from VM perspective but does not consider Serverless Functions.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "Observation 5:  Serverless functions can be used with VMs to avoid over-provisioning resources, but the right configuration needs to be accurately determined for the functions such that it satisfies the application cost and latency constraints. We argue that compared to VMs there is more variability in configurations for  serverless functions  because the resources are billed at a more fine-grained 2   allocation of CPU and memory. 1 For squeezenet model, allocating beyond 2GB did not reduce computation time, but resulted in increased cost 2 The smallest standard performance VM (C4 family) comes with 2 vcpus and 3.75GB memory.",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "1 For squeezenet model, allocating beyond 2GB did not reduce computation time, but resulted in increased cost 2 The smallest standard performance VM (C4 family) comes with 2 vcpus and 3.75GB memory. Implications of Public Cloud Resource Heterogeneity for Inference Serving WoSC’20, December 7ś11, 2020, Delft, Netherlands \n3 How to Design Self-Managed ML Prediction Serving System? But serverless functions can be configured starting from 1 vcpu and 0.128GB memory.",
      "type": "sliding_window_shuffled",
      "tokens": 120,
      "augmented": true
    },
    {
      "text": "Implications of Public Cloud Resource Heterogeneity for Inference Serving WoSC’20, December 7ś11, 2020, Delft, Netherlands \n3 How to Design Self-Managed ML Prediction Serving System? The objectives from Section  2  strongly motivate the need for a self-managed ML-prediction system that avoids the over-provisioning problem in VMs by efficiently blending serverless functions  with VMs. At the same time, right-sizing the number of requests in VMs and correctly configuring serverless functions  is quintessential to satisfy the three pri- mary application constraints: cost, latency, and accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 147,
      "augmented": true
    },
    {
      "text": "Prior work [ 9 ] solves an optimization problem such that the input parameters are model_type, hardware_type (CPU or GPU), and the output parameter is response latency. 3.1 Model Selection \nIn accordance with  Observation 1 , model selection should be a function of any two parameters which optimize the remain- ing (third) parameter. At the same time, right-sizing the number of requests in VMs and correctly configuring serverless functions  is quintessential to satisfy the three pri- mary application constraints: cost, latency, and accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "Prior work [ 9 ] solves an optimization problem such that the input parameters are model_type, hardware_type (CPU or GPU), and the output parameter is response latency. To do so, they suggest using offline profil- ing or results from previous executions. Unlike prior works, we suggest that the input and output parameters can be any linear combination of the three primary parameters men- tioned above, depending on the application constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "Unlike prior works, we suggest that the input and output parameters can be any linear combination of the three primary parameters men- tioned above, depending on the application constraints. Note that, in contrast to cost and response-latency, accuracy can- not be determined just from the previous runs. We need some feedback from the end-user to make a correct estimate of accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "We need some feedback from the end-user to make a correct estimate of accuracy. 3.2 Resource selection \n3.2.1 Static Load  From  Observation 2 , it is clear that, be- sides model selection, it is crucial to select and configure the right resource to satisfy the application constraints. Therefore, it would be best to build a learning- based system, which takes into account feedback (user-given data) to build a novel model selection system.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "To determine the number of requests each VM can handle in parallel, we can conduct offline profiling for different model types. For applications where the request load is fairly constant over time, only VM-based resources can be procured to serve the requests. 3.2 Resource selection \n3.2.1 Static Load  From  Observation 2 , it is clear that, be- sides model selection, it is crucial to select and configure the right resource to satisfy the application constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "3.2.2 Dynamic Load  For applications with dynamic load ( Observation 3 ),  serverless functions  can be used to mitigate the over-provisioning cost of VMs. However, a single ap- plication can contain a mix of queries with varying latency demands. To determine the number of requests each VM can handle in parallel, we can conduct offline profiling for different model types.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "To handle dynamic load variations, a load-monitor can be designed such that it constantly moni- tors different periods of static load and peak load. Therefore, queries with strict latency requirements can be scheduled on  serverless functions , if a VM with free resources is unavailable. However, a single ap- plication can contain a mix of queries with varying latency demands.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "We propose to plug-in intelligent peak-to-median prediction policies (in accordance to  Observation 4 ) , which can aid the load-monitor to estimate the duration of static load. To handle dynamic load variations, a load-monitor can be designed such that it constantly moni- tors different periods of static load and peak load. Furthermore, it can measure the peak-to-median ratio in sampling windows, which can be used to decide if  serverless functions  are re- quired to balance the load.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "Furthermore, it can measure the peak-to-median ratio in sampling windows, which can be used to decide if  serverless functions  are re- quired to balance the load. 3.2.3 Provisioning Time vs Execution Time  We know that new VMs take a few hundred seconds to start-up. However, during flash-crowds, where load-prediction fails to accurately estimate the load, \nserverless functions  can inherently be used to handle requests to meet the response latency, but by incurring higher costs.",
      "type": "sliding_window_shuffled",
      "tokens": 117,
      "augmented": true
    },
    {
      "text": "3.2.3 Provisioning Time vs Execution Time  We know that new VMs take a few hundred seconds to start-up. Prior literature [ 5 ,  10 ] tries to hide the model load latency by pre-warming serverless function in- stances through periodically issuing dummy requests. Server- less functions  can start-up much faster (1s-10s), but they also incur additional latency to load a pre-trained model from external data-store.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "How- ever, such hacks can fail if the cloud service provider decides to change the idle timeout of function instances or change the overall mechanism to recycle idle function instances. Prior literature [ 5 ,  10 ] tries to hide the model load latency by pre-warming serverless function in- stances through periodically issuing dummy requests. Rather than capitalizing on such design hacks, we need to develop prediction policies to estimate load correctly.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Also, we suggest service providers should handle the pre-warming decision by knowing model-wise usage statistics to enable instance sharing, which uses the same models. Rather than capitalizing on such design hacks, we need to develop prediction policies to estimate load correctly. This would lead to a reduction in cold-start latencies incurred for users with the same type of requests.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "Through offline profiling or initial runs, we can de- termine the right memory allocation for a given response latency. This would lead to a reduction in cold-start latencies incurred for users with the same type of requests. 3.2.4 Configuring Serverless Functions  In keeping with  Observation 5 , it is quintessential to configure the mem- ory allocation of  serverless functions  to meet the application SLOs.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "From our observations in  AWS Lambda , three types of cores are allocated in the increasing order of the memory allocation ( 0.5GB, 1.5GB, and  > 2GB ). Through offline profiling or initial runs, we can de- termine the right memory allocation for a given response latency. Also, these policies can be changed over time by Amazon, and they can also be dif- ferent for other cloud providers [ 8 ].",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "Therefore, the resource manager should be able to leverage this information to make optimal serverless function configuration decisions. 4 Evaluation and Initial Results \nThis section introduces how an ML-serving framework can capitalize on the design choices discussed in Section  3 . Also, these policies can be changed over time by Amazon, and they can also be dif- ferent for other cloud providers [ 8 ].",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "Implementation Methodology:  We developed a proto- type on top of Amazon EC2 and Lambda services to evaluate the some of the benefits of our proposed design choices. We design three different experiments to study the effects on cost of ML servings due to (i) varying SLOs and (ii) varying application constraints. 4 Evaluation and Initial Results \nThis section introduces how an ML-serving framework can capitalize on the design choices discussed in Section  3 .",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "We use AWS as the testbed for conducting extensive experiments. Implementation Methodology:  We developed a proto- type on top of Amazon EC2 and Lambda services to evaluate the some of the benefits of our proposed design choices. The types of instance used in our evaluation include all the c5 and m5 instances for EC2.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "Also, we estimate the right configuration of lambda functions by conduction offline experiments. By offline profiling, we estimate the number of model instances each VM can execute in par- allel without violating the model latency. The types of instance used in our evaluation include all the c5 and m5 instances for EC2.",
      "type": "sliding_window_shuffled",
      "tokens": 72,
      "augmented": true
    },
    {
      "text": "Also, we estimate the right configuration of lambda functions by conduction offline experiments. For the model selection problem, we maintain an offline model cache which consists of the de- tails of individual model latency and accuracy profiled by executing on c4 ˙ large VM. The scheduler will pick the right model combinations from the cache based on the application requirements.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "0 \n1 \n2 \n3 \n4 \n0 \n0.5 \n1 \n1.5 \nutil_aware exascale mixed paragon \nSLO Violations \nNormalized Cost \nCost SLO violations \n(a)  Workload-1: Berkeley Trace. We implement a load generator, which uses a \nWoSC’20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. The scheduler will pick the right model combinations from the cache based on the application requirements.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "0 \n5 \n10 \n0 \n0.5 \n1 \n1.5 \nutil_aware exascale mixed paragon \nSLO Violations \nNormalized Cost \nCost SLO violations \n(b)  Workload-1: WITS Trace. Figure 8. 0 \n1 \n2 \n3 \n4 \n0 \n0.5 \n1 \n1.5 \nutil_aware exascale mixed paragon \nSLO Violations \nNormalized Cost \nCost SLO violations \n(a)  Workload-1: Berkeley Trace.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "The cost is normalized to reactive scaling scheme. Figure 8. Comparison of the resource procurement cost for two different traces using five different schemes.",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "The cost is normalized to reactive scaling scheme. 1 hour sample of the real-world trace for request arrival time generation. Each request is derived from a pool of pre-trained ML inference models for image classification (as explained in Section  2 ).",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "Each request is derived from a pool of pre-trained ML inference models for image classification (as explained in Section  2 ). Evaluation:  We evaluate our results by comparing the cost, latency and accuracy for two different workloads. We use  Apache MXNet  and  TensorFlow  frame- work to deploy and run inference on the models.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Workload- 1 consists of a mix of queries which have both strict and relaxed latency requirements. We compare the execution of this workload against the following resource procure- ment schemes: (i)  util_aware , (ii)  exascale , (iii)  mixed  and (iv) Paragon . Evaluation:  We evaluate our results by comparing the cost, latency and accuracy for two different workloads.",
      "type": "sliding_window_shuffled",
      "tokens": 96,
      "augmented": true
    },
    {
      "text": "These schemes are modeled after state-of-the-art prior works as explained earlier in Section  2.3 . The  Paragon scheme does not offload to lambdas for relaxed latency queries. We compare the execution of this workload against the following resource procure- ment schemes: (i)  util_aware , (ii)  exascale , (iii)  mixed  and (iv) Paragon .",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Workload-2 consists of different cost, accuracy and latency requirements for all queries. The  Paragon scheme does not offload to lambdas for relaxed latency queries. We compare the  Paragon model selection scheme against a naive constraints-unaware model selection scheme.",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": "We compare the  Paragon model selection scheme against a naive constraints-unaware model selection scheme. It can be seen that mixed scheme has similar cost to reactive but it reduces SLO vi- olations by up to 60%. Results:  Figure  8  plots the SLO and cost for workload-1 across Berkley and WITS trace.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "This is because, the mixed scheme offloads request in the peak to serverless functions. However, the  Paragon  scheme is 10% more cost-effective than mixed and at the same time ensures similar SLOs. It can be seen that mixed scheme has similar cost to reactive but it reduces SLO vi- olations by up to 60%.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "However, the  Paragon  scheme is 10% more cost-effective than mixed and at the same time ensures similar SLOs. Therefore, this results in reduced cost and at the same time does not violate SLOs. This is because the  Paragon  scheme is aware of the latency requirements of individual queries and does not blindly offload queries to lambdas when there is increase in load.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "Figure  1  shows the candidate models which can be used for a given latency and accuracy. This re-instantiates our claim that the resource procurement scheme needs to be aware of request constraints. Therefore, this results in reduced cost and at the same time does not violate SLOs.",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "Our  Paragon  scheme optimizes the model selection for workload-2 such that, it chooses the least cost-effective model for the given accuracy and latency constraint. The naive model selection policy would not choose the models as its oblivious to user requirements and model characteristics. Figure  1  shows the candidate models which can be used for a given latency and accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 80,
      "augmented": true
    },
    {
      "text": "The naive model selection policy would not choose the models as its oblivious to user requirements and model characteristics. In our experiments, compared to naive selection scheme which does not optimize model selection for cost, the  Paragon  schemes reduces the cost of resource procurement by up to 20% (results are not plotted). This is because the  Paragon  scheme jointly considers all three parameters and chooses the least costing model.",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "The critical challenge of deploying ML prediction serving applications in public cloud is to combine both model and resource heterogeneity towards optimizing for application constraints. 5 Conclusion \nThere is wide-spread prominence in the adoption of ML- based prediction systems spanning across a wide range of application domains. This is because the  Paragon  scheme jointly considers all three parameters and chooses the least costing model.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "In this paper, we propose to build a self-managed ML prediction system, which can optimize the diverse application requirements based on characteristics of heterogeneous public cloud resource offerings. Towards this, we discuss the trade-offs of intermixing resources like serverless functions along with VMs and identify the key challenges associated with configuring these resources. The critical challenge of deploying ML prediction serving applications in public cloud is to combine both model and resource heterogeneity towards optimizing for application constraints.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "These policies can be collectively used for cost-effective prediction serving with- out compromising on latency and accuracy. Towards this, we discuss the trade-offs of intermixing resources like serverless functions along with VMs and identify the key challenges associated with configuring these resources. We propose multiple key-policies to make resource manage- ment; (i) latency aware, (ii) multi-dimensional SLO aware, and (iii) request load variation aware.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Acknowledgement \nThis research was partially supported by NSF grants #1931531, #1955815, #1629129, #1763681, #1629915, #1908793, #1526750 and we thank NSF Chameleon Cloud project CH- 819640 for their generous compute grant. These policies can be collectively used for cost-effective prediction serving with- out compromising on latency and accuracy. References \n[1]  Omid Alipourfard, Hongqiang Harry Liu, Jianshu Chen, Shivaram Venkataraman, Minlan Yu, and Ming Zhang.",
      "type": "sliding_window_shuffled",
      "tokens": 133,
      "augmented": true
    },
    {
      "text": "References \n[1]  Omid Alipourfard, Hongqiang Harry Liu, Jianshu Chen, Shivaram Venkataraman, Minlan Yu, and Ming Zhang. 2017. CherryPick: Adap- tively Unearthing the Best Cloud Configurations for Big Data Analytics.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "CherryPick: Adap- tively Unearthing the Best Cloud Configurations for Big Data Analytics. [2]  Marc Brooker, Andreea Florescu, Diana-Maria Popa, Rolf Neugebauer, Alexandru Agache, Alexandra Iordache, Anthony Liguori, and Phil Piwonka. In  (NSDI) .",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "2020. [2]  Marc Brooker, Andreea Florescu, Diana-Maria Popa, Rolf Neugebauer, Alexandru Agache, Alexandra Iordache, Anthony Liguori, and Phil Piwonka. Firecracker: Lightweight Virtualization for Serverless Applications.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "[3]  Andrew Chung, Jun Woo Park, and Gregory R. Ganger. Firecracker: Lightweight Virtualization for Serverless Applications. In  NSDI .",
      "type": "sliding_window_shuffled",
      "tokens": 39,
      "augmented": true
    },
    {
      "text": "2018. Stratus: Cost-aware Container Scheduling in the Public Cloud. [3]  Andrew Chung, Jun Woo Park, and Gregory R. Ganger.",
      "type": "sliding_window_shuffled",
      "tokens": 38,
      "augmented": true
    },
    {
      "text": "In  SoCC . [4]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. Stratus: Cost-aware Container Scheduling in the Public Cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "2017. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. [4]  Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "[5]  J. R. Gunasekaran, P. Thinakaran, et al . In  USENIX Middleware Conference . Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Spock: Exploiting Server- less Functions for SLO and Cost Aware Resource Procurement in Public Cloud. 2019. [5]  J. R. Gunasekaran, P. Thinakaran, et al .",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "In  CLOUD . [6] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons. Spock: Exploiting Server- less Functions for SLO and Cost Aware Resource Procurement in Public Cloud.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "2018. Tributary: spot-dancing for elastic ser- vices with latency SLOs. [6] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "[7]  Akshitha Sriraman, Abhishek Dhanotia, and Thomas Wenisch. In  ATC . Tributary: spot-dancing for elastic ser- vices with latency SLOs.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "[7]  Akshitha Sriraman, Abhishek Dhanotia, and Thomas Wenisch. Softsku: Optimizing server architectures for microservice diversity@ scale. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "Softsku: Optimizing server architectures for microservice diversity@ scale. In  ISCA . [8]  Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "2018. [8]  Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift. Peeking Behind the Curtains of Serverless Plat- forms.",
      "type": "sliding_window_shuffled",
      "tokens": 48,
      "augmented": true
    },
    {
      "text": "Peeking Behind the Curtains of Serverless Plat- forms. In  ATC . [9]  Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "[9]  Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. A Case for Managed and Model-Less Inference Serv- ing. 2019.",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "ACM. In  HotOS . A Case for Managed and Model-Less Inference Serv- ing.",
      "type": "sliding_window_shuffled",
      "tokens": 28,
      "augmented": true
    },
    {
      "text": "2019. ACM. https://doi.org/10.1145/3317550.3321443 [10]  Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "2019. MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving. In  ATC .",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    }
  ]
}