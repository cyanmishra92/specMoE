=== ORIGINAL PDF: PubCloudHet.pdf ===\n\nRaw text length: 32549 characters\nCleaned text length: 32268 characters\nNumber of segments: 16\n\n=== CLEANED TEXT ===\n\nImplications of Public Cloud Resource Heterogeneity for Inference Serving Jashwant Raj Gunasekaran The Pennsylvania State University Cyan Subhra Mishra The Pennsylvania State University Prashanth Thinakaran The Pennsylvania State University Mahmut Taylan Kandemir The Pennsylvania State University Chita R. Das The Pennsylvania State University Abstract We are witnessing an increasing trend towards using Ma- chine Learning (ML) based prediction systems, spanning across different application domains, including product rec- ommendation systems, personal assistant devices, facial recognition, etc. These applications typically have diverse requirements in terms of accuracy and response latency, that can be satisfied by a myriad of ML models. However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by them- selves are heterogeneous in terms of provisioning latencies and billing complexity. Thus, it is strenuous for an infer- ence serving system to choose from this confounding array of resource types and model types to provide low-latency and cost-effective inferences. In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource of- ferings. Our evaluation shows that, prior work does not solve the problem from both dimensions of model and resource heterogeneity. Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints. Towards this, we discuss the design implications of a self-managed inference serving system, which can optimize for application requirements based on public cloud resource characteristics. CCS Concepts: Computer systems organization Real-time system architecture. Keywords: serverless, resource-management, inference Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from WoSC 20, December 7ś11, 2020, Delft, Netherlands 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8204-5 20 12... 15.00 ACM Reference Format: Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thi- nakaran, Mahmut Taylan Kandemir, and Chita R. Das. 2020. Im- plications of Public Cloud Resource Heterogeneity for Inference Serving. In Workshop on Serverless Computing (WoSC 20), December 7ś11, 2020, Delft, Netherlands. ACM, New York, NY, USA, 6 pages. 1 Introduction Sustained advances in ML has fueled the proliferation of emerging applications such as product recommendation sys- tems, facial recognition systems, and intelligent personal assistants [7]. Among many ML paradigms, Deep Neural Net- works (DNNs), owing to their generalization and massively- parallel nature, has been predominant in making all these applications pervasive and accessible to developers. A typical DNN model has two different phases, namely, training and inference. Training a DNN, which is the process of extracting and learning the patterns and the features from millions of sample-data, typically takes a few hours to days. The trained models can then be used to perform inferences, i.e., the clas- sification task. Since typical large scale DNNs have millions of parameters and perform billions of multiplications and accumulations for executing a single inference, they are typ- ically hosted as web-services, which are often queried for predictions. Conventionally, training is much more compute intensive (compared to an inference), takes many iterations and hence has been given considerable attention for better accuracy and convergence time. However, given the preva- lence and demand of inferences, serving them on public cloud with a tight bound of latency, throughput and cost is becoming increasingly more challenging [7]. These inference queries are typically administered with strict response laten- cies of under one second [4]. Based on the application needs, prediction queries require different compute resources, and have different accuracy, latency, and cost requirements. To ensure a required accuracy with given latency, applications have to choose from a confounding array of different types of models (shown in Figure 1). Therefore, it is non-trivial for an application to choose the right model that can collectively optimize for all requirements together. WoSC 20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. Unlike accuracy and latency, which depends on the right model, cost is dictated by the type of deployment used to host them in a public cloud. The deployment costs differ based on the provisioning times and longevity of the resource pro- cured. Typically, these inference serving systems are hosted using Virtual Machines (VMs), which take a few minutes to start-up. Due to high start-up latencies, using VMs for hosting ML services can lead to over-provisioning, espe- cially during periods of poor workload predictability (flash crowds) [10]. In stark contrast to VMs, serverless functions have been made available by cloud providers, which can spin-up within a few seconds [2]. As we will discuss in this paper, the cost of using VMs vs. serverless functions highly depends on the dynamically varying needs of the user query submission rates. Besides workload arrival rates, there is fur- ther variability in terms of configuring serverless functions to meet the end-user demands of latency and cost require- ments. This is because, serverless functions are billed based on of number of invocations, compute time and memory requirement of the function. However, increased memory allocation leads to faster execution time owing to powerful compute-core allocation, but exacerbates the billing cost. Therefore, these apparent deficiencies of choosing the appropriate resource type and model type for a given user re- quirement motivates the central question of this work: Does there exist an optimal resource procurement system which can balance the goals of diverse user requirements for accuracy, latency and cost, by efficiently mapping model parameters to heterogeneous resource specifications? Our preliminary results suggest that using a combination of VMs and serverless func- tions could potentially provide a solution to this problem. As opposed to prior works [5, 10], which try to combine serverless functions with VMs to hide the start-up latencies of VMs, our primary interest lies in exploring the different key aspects to address when hosting DNN-based ML pre- diction serving systems in public cloud, as given below: Diverse Models: How to make the users oblivious of model selection from the extensive pool of models, for satis- fying the accuracy, and latency requirements? Heterogeneous Public Cloud Resources: What are the different options available in terms of combining different VM-based cloud services and serverless functions for a given user requirement? Configuring Resources: From the diverse options, how to right-size VMs and appropriately configure the serverless functions to efficiently cater to user specified cost, accuracy and latency constraint? Bring in Tune: Based on the dynamically changing query arrivals over time, what is the right way to combine model diversity along with resource heterogeneity without com- promising the user-specified requirements? By exploring these key aspects, we envision developing a self-managed inference-serving system, which can provide for different diverse needs of applications by leveraging the 0 50 100 150 200 250 300 350 20.00 40.00 60.00 80.00 100.00 MobileNet V1 MobileNEt V2 Inception V3 Resnet50 ResNet50-V2 DenseNet-201 DenseNet-121 Xxception NasNetMobile InceptionResnetV2 vgg16 NasNetLarge Latency (ms) Accuracy Top1-Accuracy Latency Figure 1. Accuracy and Latency of Different Pretrained Models. 0 20 40 60 80 Percentage Model Type (a) Different accuracy for ISO- latency. 0 200 400 600 800 1000 Time(ms) Model Type (b) Different response latencies for ISO-accuracy. Figure 2. Comparison of different models under ISO-latency and ISO-accuracy setup. heterogeneous resource availability from the public cloud. Towards this, we make the following key contributions. 1. We comprehensively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource offerings and unravel the suitable model resource configurations to meet the cost, latency and accuracy demands. 2. We quantitatively evaluate prior works [5, 9, 10] which are geared towards achieving this vision and show that they still suffer from several issues when trying to solve the complex problem of combining model and resource heterogeneity. 3. We propose detailed design choices that can adopted to- wards designing a self-managed inference-serving system. In addition, we design a scheme named Paragon on top of AWS platform, which incorporates some of the proposed design choices. Our initial results show that Paragon can reduce cost of hosting ML prediction serving by up to 20 when compared to the state-of-the-art prior works, for diverse accuracy and latency constraints. 2 Characterization and Motivation 2.1 Variability across model types Depending on the accuracy and latency requirements of an end-user application, multiple models (shown in Figure 1) might satisfy a given constraint. For example, consider a face- recognition application that demands a response latency of under 500ms (ISO-latency). As shown in Figure 2a, four differ- ent models can satisfy the response latency, but each model comes with a different prediction accuracy. Similarly, if the Implications of Public Cloud Resource Heterogeneity for Inference Serving WoSC 20, December 7ś11, 2020, Delft, Netherlands 0 10 20 Cost( ) VM cost Lambda Cost inception nasnet densenet mobilenet (a) Cost for ISO-latency models. 0 10 20 Cost( ) VM cost Lambda Cost inception resnet-200 resnext-50 nasnet (b) Cost for ISO-accuracy models. Figure 3. Variation of cost of using VMs vs. serverless functions under constant request load. Each of the four bars under any model type corresponds to request arrival rates of 10, 50, 100, and 200 requests second. same application requires accuracy to be at-least 80 (ISO- accuracy), as shown in Figure 2b, four different models with different response latencies can satisfy the accuracy. There- fore, depending on the cost budget of the application, one can choose among the different model types by trading-off accuracy or response latency. Hence, it is evident that there is a large optimization space where different models can be selected based upon the needs of the applications. Prior work [9] tries to solve model selection only from a through- put perspective where different sized batching of multiple inference queries together results in varied throughput. Observation 1: Model selection should be focused on meeting the cost requirement of an application without compromising on the accuracy and or latency constraint. 2.2 Performance under given constraint Model selection is not an independent problem because the user-applications also have a cost constraint incurred as a result of procuring resources from the public cloud. We compare the cost of deploying the inference service on a group of virtual machines and serverless functions. We use m4- large instances for VMs and we fix the number of inference queries each VM can handle in parallel, without violating response latencies based on our characterization on AWS EC2. The serverless functions are configured according to the memory requirements of each model. Figure 3a plots the cost of hosting the iso-latency model types (shown in Figure 2a) for a constant request arrival rates of 10, 50, 100, 200 req sec over 1 hour duration. It can be seen that virtual machines are always cheaper compared to using serverless functions for all constant request rates. A similar trend is observed for the iso-accuracy model types, which is shown in Figure 3b. It is also possible to use bigger VMs, which can handle more concurrent requests compared to m4-large, thus mini- mizing the total number of VMs used. However, we observe that the pricing of EC2 VMs is a linear function of the VM size in terms of compute capacity and memory. Hence, nor- malized by number of requests, bigger VMs would still incur similar costs as smaller VMs. Observation 2: VMs should be used to handle requests during constant arrival rates. Also, the number of concurrent requests which can be executed in VMs should be accurately determined to meet response latency. 0 0.4 0.8 1.2 1.6 Berkley WITS Twitter Wiki Normalized VMs reactive util_aware exascale Figure 4. Over-provisioning of util_aware and exascale, nor- malized to a baseline reactive scheme for four traces. 2.3 Over-provisioning VMs Real-world request arrivals rates are usually not constant as they significantly vary over time (e.g. diurnal, flash-crowds etc.) Therefore, resource procurement and management deci- sions need to be adjusted depending on the resource utiliza- tion load and arrival rates. Public cloud providers leave these major decisions to be łmanually handled" by users, which is very time consuming and strenuous. As a result, the major- ity of application providers use static resource provisioning, which results in poor resource utilization and higher costs. Prior works [3, 9] have tried to solve the resource scal- ing problem with respect to hosting the applications in VMs. They employ autoscaling mechanisms to cope up with dy- namic load. These autoscaling mechanisms can be of two types: (i) spawn VMs if the resource utilization of existing VMs reaches a certain threshold (80 in most cases) [9], and (ii) spawn additional VMs than predicted request demand [6]. We name the former autoscaling scheme as util_aware and the later as exascale. Both these schemes suffer from over- provisioning VMs because (i) we cannot always accurately predict the future load, and (ii) resource utilization is not always the right indicator for increased load. We conduct simulation experiments to compare the schemes, using the profiled values (explained in Section 2.2) for four different well-known request arrival traces. Each request in the trace is associated with an ML inference query, which is randomly picked from our model pool. Figure 4 shows the ratio of over-provisioned VMs compared to a baseline reactive autoscaling mechanism. It can be seen that although both util_aware and exascale can reduce SLO vi- olations (shown in Figure 5), they still suffer from 20 to 30 over-provisioned VMs across all four traces. This, in turn, increases the cost of deployment (shown in Figure 5), compared to baseline reactive scheme. WoSC 20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. 0 5 10 0 1 2 Berkley WITS Twitter Wiki SLO Violations Normalized Cost reactive util_aware exascale mixed Figure 5. Cost of using mixed compared to util_aware and exascale, normalized to a baseline reactive scheme for four traces. Percentage of SLO violations for each scheme are shown in the line graph (the corresponding color in the bar-graph is used for all the schemes.) 0 200 400 600 0 0.6 1.2 1.8 2.4 3 Cost ( ) Model Type exec_time(ms) Memory(GB) Cost( ) Figure 7. Cost variation for different allocations in serverless func- tions. Compute time (seconds) and Memory allocated (GB) is shown on left Y-axis. Cost ( ) is shown in right Y-axis. Observation 3: Only-VM based resource procurement should not be used during dynamic load as it leads to over-provisioned resources and increased cost. 2.4 Using serverless functions with VMs The provisioning latency is a major contributor for VM over-provisioning during request surges be- cause the increased time to provision new VMs results in the increase of response latencies which in-turn leads to provisioning more VMs in advance. 0 500 1000 1500 wiki WITS berkley Twitter Request Rate Avg Req Max Req Figure 6. Peak-to-median ratio. Prior works [5, 10] try to hide the pro- visioning latency of VMs by using server- less functions as a handover mechanism when starting new VMs. We name this scheme as mixed pro- curement. However, these schemes do not address the holis- tic problem by taking into account model selection, resource selection, and resource scaling to cope up with user-specified constraints. We conduct similar experiments to mimic the mixed procurement scheme. As shown in Figure 5, mixed procurement reduces the over-provisioning cost of VMs. At the same time it also minimizes latency violations equivalent to exascale scheme. However, we argue that there is scope to further optimize resource procurement based on the fre- quency of peak load and constant load in a given request arrival scenario. Figure 6 plots the peak-to-median ratio for three different traces. From our simulation experiments we observe that mixed procurement did not reduce cost of Wiki trace. This is because the difference between peak-to-median in the traces are not large and therefore more functions get offloaded to serverless functions. Thus, using serverless func- tions for such scenarios will not drastically reduce cost. For the other traces like Berkeley, WITS and Twitter, the peak- to-median difference is more than 50 and therefore they can benefit from offloading requests to serverless functions. Observation 4: It is important to note that, the request arrival pattern plays a key role in determining if mixed procurement can be cost effective. 2.5 Challenges with serverless functions Apart from arrival rates, memory allocation to serverless functions play a non-trivial role in terms of cost. In our exper- iments we configure the memory allocation to the lambda function such that individual query latency is within the user-specified latency constraint. We conducted character- ization experiments on AWS Lambda, currently the most predominant serverless function provider, to study the mem- ory allocated vs computation time trade-off. Figure 7 shows the computation time and cost for executing 1 million infer- ence queries for three different model types with different memory allocations. We vary the memory allocation, starting from least required memory for the model to the maximum available limit in AWS (3GB)1. It can be clearly seen that the computation time reduces with increased memory alloca- tion but also results in higher cost of deployment for every model type. This is because, inherently, serverless providers allocate a powerful compute core for functions with higher memory allocation. Therefore, depending on the latency re- quirements of the user applications, serverless functions need to be allocated the appropriate memory. However, this might result in increased cost when using serverless functions along with VMs for varying latency requirements. Hence, the over- all cost incurred by mixed procurement can be higher or lower than VM-only autoscaling policies. Prior work like Cherrypick [1] solves the resource se- lection and configuration problems from VM perspective but does not consider Serverless Functions. We argue that compared to VMs there is more variability in configurations for serverless functions because the resources are billed at a more fine-grained2 allocation of CPU and memory. Observation 5: Serverless functions can be used with VMs to avoid over-provisioning resources, but the right configuration needs to be accurately determined for the functions such that it satisfies the application cost and latency constraints. 1For squeezenet model, allocating beyond 2GB did not reduce computation time, but resulted in increased cost 2The smallest standard performance VM (C4 family) comes with 2 vcpus and 3.75GB memory. But serverless functions can be configured starting from 1 vcpu and 0.128GB memory. Implications of Public Cloud Resource Heterogeneity for Inference Serving WoSC 20, December 7ś11, 2020, Delft, Netherlands 3 How to Design Self-Managed ML Prediction Serving System? The objectives from Section 2 strongly motivate the need for a self-managed ML-prediction system that avoids the over-provisioning problem in VMs by efficiently blending serverless functions with VMs. At the same time, right-sizing the number of requests in VMs and correctly configuring serverless functions is quintessential to satisfy the three pri- mary application constraints: cost, latency, and accuracy. 3.1 Model Selection In accordance with Observation 1, model selection should be a function of any two parameters which optimize the remain- ing (third) parameter. Prior work [9] solves an optimization problem such that the input parameters are model_type, hardware_type (CPU or GPU), and the output parameter is response latency. To do so, they suggest using offline profil- ing or results from previous executions. Unlike prior works, we suggest that the input and output parameters can be any linear combination of the three primary parameters men- tioned above, depending on the application constraints. Note that, in contrast to cost and response-latency, accuracy can- not be determined just from the previous runs. We need some feedback from the end-user to make a correct estimate of accuracy. Therefore, it would be best to build a learning- based system, which takes into account feedback (user-given data) to build a novel model selection system. 3.2 Resource selection 3.2.1 Static Load From Observation 2, it is clear that, be- sides model selection, it is crucial to select and configure the right resource to satisfy the application constraints. For applications where the request load is fairly constant over time, only VM-based resources can be procured to serve the requests. To determine the number of requests each VM can handle in parallel, we can conduct offline profiling for different model types. 3.2.2 Dynamic Load For applications with dynamic load (Observation 3), serverless functions can be used to mitigate the over-provisioning cost of VMs. However, a single ap- plication can contain a mix of queries with varying latency demands. Therefore, queries with strict latency requirements can be scheduled on serverless functions, if a VM with free resources is unavailable. To handle dynamic load variations, a load-monitor can be designed such that it constantly moni- tors different periods of static load and peak load. We propose to plug-in intelligent peak-to-median prediction policies (in accordance to Observation 4) , which can aid the load-monitor to estimate the duration of static load. Furthermore, it can measure the peak-to-median ratio in sampling windows, which can be used to decide if serverless functions are re- quired to balance the load. However, during flash-crowds, where load-prediction fails to accurately estimate the load, serverless functions can inherently be used to handle requests to meet the response latency, but by incurring higher costs. 3.2.3 Provisioning Time vs Execution Time We know that new VMs take a few hundred seconds to start-up. Server- less functions can start-up much faster (1s-10s), but they also incur additional latency to load a pre-trained model from external data-store. Prior literature [5, 10] tries to hide the model load latency by pre-warming serverless function in- stances through periodically issuing dummy requests. How- ever, such hacks can fail if the cloud service provider decides to change the idle timeout of function instances or change the overall mechanism to recycle idle function instances. Rather than capitalizing on such design hacks, we need to develop prediction policies to estimate load correctly. Also, we suggest service providers should handle the pre-warming decision by knowing model-wise usage statistics to enable instance sharing, which uses the same models. This would lead to a reduction in cold-start latencies incurred for users with the same type of requests. 3.2.4 Configuring Serverless Functions In keeping with Observation 5, it is quintessential to configure the mem- ory allocation of serverless functions to meet the application SLOs. Through offline profiling or initial runs, we can de- termine the right memory allocation for a given response latency. From our observations in AWS Lambda, three types of cores are allocated in the increasing order of the memory allocation (0.5GB, 1.5GB, and 2GB). Also, these policies can be changed over time by Amazon, and they can also be dif- ferent for other cloud providers [8]. Therefore, the resource manager should be able to leverage this information to make optimal serverless function configuration decisions. 4 Evaluation and Initial Results This section introduces how an ML-serving framework can capitalize on the design choices discussed in Section 3. We design three different experiments to study the effects on cost of ML servings due to (i) varying SLOs and (ii) varying application constraints. Implementation Methodology: We developed a proto- type on top of Amazon EC2 and Lambda services to evaluate the some of the benefits of our proposed design choices. We use AWS as the testbed for conducting extensive experiments. The types of instance used in our evaluation include all the c5 and m5 instances for EC2. By offline profiling, we estimate the number of model instances each VM can execute in par- allel without violating the model latency. Also, we estimate the right configuration of lambda functions by conduction offline experiments. For the model selection problem, we maintain an offline model cache which consists of the de- tails of individual model latency and accuracy profiled by executing on c4 large VM. The scheduler will pick the right model combinations from the cache based on the application requirements. We implement a load generator, which uses a WoSC 20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. 0 1 2 3 4 0 0.5 1 1.5 util_aware exascale mixed paragon SLO Violations Normalized Cost Cost SLO violations (a) Workload-1: Berkeley Trace. 0 5 10 0 0.5 1 1.5 util_aware exascale mixed paragon SLO Violations Normalized Cost Cost SLO violations (b) Workload-1: WITS Trace. Figure 8. Comparison of the resource procurement cost for two different traces using five different schemes. The cost is normalized to reactive scaling scheme. 1 hour sample of the real-world trace for request arrival time generation. Each request is derived from a pool of pre-trained ML inference models for image classification (as explained in Section 2). We use Apache MXNet and TensorFlow frame- work to deploy and run inference on the models. Evaluation: We evaluate our results by comparing the cost, latency and accuracy for two different workloads. Workload- 1 consists of a mix of queries which have both strict and relaxed latency requirements. We compare the execution of this workload against the following resource procure- ment schemes: (i) util_aware, (ii) exascale, (iii) mixed and (iv) Paragon. These schemes are modeled after state-of-the-art prior works as explained earlier in Section 2.3. The Paragon scheme does not offload to lambdas for relaxed latency queries. Workload-2 consists of different cost, accuracy and latency requirements for all queries. We compare the Paragon model selection scheme against a naive constraints-unaware model selection scheme. Results: Figure 8 plots the SLO and cost for workload-1 across Berkley and WITS trace. It can be seen that mixed scheme has similar cost to reactive but it reduces SLO vi- olations by up to 60 . This is because, the mixed scheme offloads request in the peak to serverless functions. However, the Paragon scheme is 10 more cost-effective than mixed and at the same time ensures similar SLOs. This is because the Paragon scheme is aware of the latency requirements of individual queries and does not blindly offload queries to lambdas when there is increase in load. Therefore, this results in reduced cost and at the same time does not violate SLOs. This re-instantiates our claim that the resource procurement scheme needs to be aware of request constraints. Figure 1 shows the candidate models which can be used for a given latency and accuracy. Our Paragon scheme optimizes the model selection for workload-2 such that, it chooses the least cost-effective model for the given accuracy and latency constraint. The naive model selection policy would not choose the models as its oblivious to user requirements and model characteristics. In our experiments, compared to naive selection scheme which does not optimize model selection for cost, the Paragon schemes reduces the cost of resource procurement by up to 20 (results are not plotted). This is because the Paragon scheme jointly considers all three parameters and chooses the least costing model. 5 Conclusion There is wide-spread prominence in the adoption of ML- based prediction systems spanning across a wide range of application domains. The critical challenge of deploying ML prediction serving applications in public cloud is to combine both model and resource heterogeneity towards optimizing for application constraints. In this paper, we propose to build a self-managed ML prediction system, which can optimize the diverse application requirements based on characteristics of heterogeneous public cloud resource offerings. Towards this, we discuss the trade-offs of intermixing resources like serverless functions along with VMs and identify the key challenges associated with configuring these resources. We propose multiple key-policies to make resource manage- ment; (i) latency aware, (ii) multi-dimensional SLO aware, and (iii) request load variation aware. These policies can be collectively used for cost-effective prediction serving with- out compromising on latency and accuracy. Acknowledgement This research was partially supported by NSF grants 1931531, 1955815, 1629129, 1763681, 1629915, 1908793, 1526750 and we thank NSF Chameleon Cloud project CH- 819640 for their generous compute grant. References [1] Omid Alipourfard, Hongqiang Harry Liu, Jianshu Chen, Shivaram Venkataraman, Minlan Yu, and Ming Zhang. 2017. CherryPick: Adap- tively Unearthing the Best Cloud Configurations for Big Data Analytics. In (NSDI). [2] Marc Brooker, Andreea Florescu, Diana-Maria Popa, Rolf Neugebauer, Alexandru Agache, Alexandra Iordache, Anthony Liguori, and Phil Piwonka. 2020. Firecracker: Lightweight Virtualization for Serverless Applications. In NSDI. [3] Andrew Chung, Jun Woo Park, and Gregory R. Ganger. 2018. Stratus: Cost-aware Container Scheduling in the Public Cloud. In SoCC. [4] Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. 2017. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. In USENIX Middleware Conference. [5] J. R. Gunasekaran, P. Thinakaran, et al. 2019. Spock: Exploiting Server- less Functions for SLO and Cost Aware Resource Procurement in Public Cloud. In CLOUD. [6] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons. 2018. Tributary: spot-dancing for elastic ser- vices with latency SLOs. In ATC. [7] Akshitha Sriraman, Abhishek Dhanotia, and Thomas Wenisch. 2019. Softsku: Optimizing server architectures for microservice diversity scale. In ISCA. [8] Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift. 2018. Peeking Behind the Curtains of Serverless Plat- forms. In ATC. [9] Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. 2019. A Case for Managed and Model-Less Inference Serv- ing. In HotOS. ACM. [10] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. 2019. MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving. In ATC.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nImplications of Public Cloud Resource Heterogeneity for Inference Serving Jashwant Raj Gunasekaran The Pennsylvania State University Cyan Subhra Mishra The Pennsylvania State University Prashanth Thinakaran The Pennsylvania State University Mahmut Taylan Kandemir The Pennsylvania State University Chita R. Das The Pennsylvania State University Abstract We are witnessing an increasing trend towards using Ma- chine Learning (ML) based prediction systems, spanning across different application domains, including product rec- ommendation systems, personal assistant devices, facial recognition, etc. These applications typically have diverse requirements in terms of accuracy and response latency, that can be satisfied by a myriad of ML models. However, the deployment cost of prediction serving primarily depends on the type of resources being procured, which by them- selves are heterogeneous in terms of provisioning latencies and billing complexity. Thus, it is strenuous for an infer- ence serving system to choose from this confounding array of resource types and model types to provide low-latency and cost-effective inferences. In this work we quantitatively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource of- ferings. Our evaluation shows that, prior work does not solve the problem from both dimensions of model and resource heterogeneity. Hence, to holistically address this problem, we need to solve the issues that arise from combining both model and resource heterogeneity towards optimizing for application constraints. Towards this, we discuss the design implications of a self-managed inference serving system, which can optimize for application requirements based on public cloud resource characteristics. CCS Concepts: Computer systems organization Real-time system architecture. Keywords: serverless, resource-management, inference Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from WoSC 20, December 7ś11, 2020, Delft, Netherlands 2020 Association for Computing Machinery.\n\n--- Segment 2 ---\nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and or a fee. Request permissions from WoSC 20, December 7ś11, 2020, Delft, Netherlands 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-8204-5 20 12... 15.00 ACM Reference Format: Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thi- nakaran, Mahmut Taylan Kandemir, and Chita R. Das. 2020. Im- plications of Public Cloud Resource Heterogeneity for Inference Serving. In Workshop on Serverless Computing (WoSC 20), December 7ś11, 2020, Delft, Netherlands. ACM, New York, NY, USA, 6 pages. 1 Introduction Sustained advances in ML has fueled the proliferation of emerging applications such as product recommendation sys- tems, facial recognition systems, and intelligent personal assistants [7]. Among many ML paradigms, Deep Neural Net- works (DNNs), owing to their generalization and massively- parallel nature, has been predominant in making all these applications pervasive and accessible to developers. A typical DNN model has two different phases, namely, training and inference. Training a DNN, which is the process of extracting and learning the patterns and the features from millions of sample-data, typically takes a few hours to days. The trained models can then be used to perform inferences, i.e., the clas- sification task. Since typical large scale DNNs have millions of parameters and perform billions of multiplications and accumulations for executing a single inference, they are typ- ically hosted as web-services, which are often queried for predictions. Conventionally, training is much more compute intensive (compared to an inference), takes many iterations and hence has been given considerable attention for better accuracy and convergence time. However, given the preva- lence and demand of inferences, serving them on public cloud with a tight bound of latency, throughput and cost is becoming increasingly more challenging [7]. These inference queries are typically administered with strict response laten- cies of under one second [4]. Based on the application needs, prediction queries require different compute resources, and have different accuracy, latency, and cost requirements.\n\n--- Segment 3 ---\nThese inference queries are typically administered with strict response laten- cies of under one second [4]. Based on the application needs, prediction queries require different compute resources, and have different accuracy, latency, and cost requirements. To ensure a required accuracy with given latency, applications have to choose from a confounding array of different types of models (shown in Figure 1). Therefore, it is non-trivial for an application to choose the right model that can collectively optimize for all requirements together. WoSC 20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. Unlike accuracy and latency, which depends on the right model, cost is dictated by the type of deployment used to host them in a public cloud. The deployment costs differ based on the provisioning times and longevity of the resource pro- cured. Typically, these inference serving systems are hosted using Virtual Machines (VMs), which take a few minutes to start-up. Due to high start-up latencies, using VMs for hosting ML services can lead to over-provisioning, espe- cially during periods of poor workload predictability (flash crowds) [10]. In stark contrast to VMs, serverless functions have been made available by cloud providers, which can spin-up within a few seconds [2]. As we will discuss in this paper, the cost of using VMs vs. serverless functions highly depends on the dynamically varying needs of the user query submission rates. Besides workload arrival rates, there is fur- ther variability in terms of configuring serverless functions to meet the end-user demands of latency and cost require- ments. This is because, serverless functions are billed based on of number of invocations, compute time and memory requirement of the function. However, increased memory allocation leads to faster execution time owing to powerful compute-core allocation, but exacerbates the billing cost. Therefore, these apparent deficiencies of choosing the appropriate resource type and model type for a given user re- quirement motivates the central question of this work: Does there exist an optimal resource procurement system which can balance the goals of diverse user requirements for accuracy, latency and cost, by efficiently mapping model parameters to heterogeneous resource specifications? Our preliminary results suggest that using a combination of VMs and serverless func- tions could potentially provide a solution to this problem.\n\n--- Segment 4 ---\nTherefore, these apparent deficiencies of choosing the appropriate resource type and model type for a given user re- quirement motivates the central question of this work: Does there exist an optimal resource procurement system which can balance the goals of diverse user requirements for accuracy, latency and cost, by efficiently mapping model parameters to heterogeneous resource specifications? Our preliminary results suggest that using a combination of VMs and serverless func- tions could potentially provide a solution to this problem. As opposed to prior works [5, 10], which try to combine serverless functions with VMs to hide the start-up latencies of VMs, our primary interest lies in exploring the different key aspects to address when hosting DNN-based ML pre- diction serving systems in public cloud, as given below: Diverse Models: How to make the users oblivious of model selection from the extensive pool of models, for satis- fying the accuracy, and latency requirements? Heterogeneous Public Cloud Resources: What are the different options available in terms of combining different VM-based cloud services and serverless functions for a given user requirement? Configuring Resources: From the diverse options, how to right-size VMs and appropriately configure the serverless functions to efficiently cater to user specified cost, accuracy and latency constraint? Bring in Tune: Based on the dynamically changing query arrivals over time, what is the right way to combine model diversity along with resource heterogeneity without com- promising the user-specified requirements? By exploring these key aspects, we envision developing a self-managed inference-serving system, which can provide for different diverse needs of applications by leveraging the 0 50 100 150 200 250 300 350 20.00 40.00 60.00 80.00 100.00 MobileNet V1 MobileNEt V2 Inception V3 Resnet50 ResNet50-V2 DenseNet-201 DenseNet-121 Xxception NasNetMobile InceptionResnetV2 vgg16 NasNetLarge Latency (ms) Accuracy Top1-Accuracy Latency Figure 1. Accuracy and Latency of Different Pretrained Models. 0 20 40 60 80 Percentage Model Type (a) Different accuracy for ISO- latency. 0 200 400 600 800 1000 Time(ms) Model Type (b) Different response latencies for ISO-accuracy. Figure 2. Comparison of different models under ISO-latency and ISO-accuracy setup. heterogeneous resource availability from the public cloud.\n\n--- Segment 5 ---\nComparison of different models under ISO-latency and ISO-accuracy setup. heterogeneous resource availability from the public cloud. Towards this, we make the following key contributions. 1. We comprehensively characterize the cost, accuracy and latency implications of hosting ML inferences on different public cloud resource offerings and unravel the suitable model resource configurations to meet the cost, latency and accuracy demands. 2. We quantitatively evaluate prior works [5, 9, 10] which are geared towards achieving this vision and show that they still suffer from several issues when trying to solve the complex problem of combining model and resource heterogeneity. 3. We propose detailed design choices that can adopted to- wards designing a self-managed inference-serving system. In addition, we design a scheme named Paragon on top of AWS platform, which incorporates some of the proposed design choices. Our initial results show that Paragon can reduce cost of hosting ML prediction serving by up to 20 when compared to the state-of-the-art prior works, for diverse accuracy and latency constraints. 2 Characterization and Motivation 2.1 Variability across model types Depending on the accuracy and latency requirements of an end-user application, multiple models (shown in Figure 1) might satisfy a given constraint. For example, consider a face- recognition application that demands a response latency of under 500ms (ISO-latency). As shown in Figure 2a, four differ- ent models can satisfy the response latency, but each model comes with a different prediction accuracy. Similarly, if the Implications of Public Cloud Resource Heterogeneity for Inference Serving WoSC 20, December 7ś11, 2020, Delft, Netherlands 0 10 20 Cost( ) VM cost Lambda Cost inception nasnet densenet mobilenet (a) Cost for ISO-latency models. 0 10 20 Cost( ) VM cost Lambda Cost inception resnet-200 resnext-50 nasnet (b) Cost for ISO-accuracy models. Figure 3. Variation of cost of using VMs vs. serverless functions under constant request load. Each of the four bars under any model type corresponds to request arrival rates of 10, 50, 100, and 200 requests second. same application requires accuracy to be at-least 80 (ISO- accuracy), as shown in Figure 2b, four different models with different response latencies can satisfy the accuracy.\n\n--- Segment 6 ---\nEach of the four bars under any model type corresponds to request arrival rates of 10, 50, 100, and 200 requests second. same application requires accuracy to be at-least 80 (ISO- accuracy), as shown in Figure 2b, four different models with different response latencies can satisfy the accuracy. There- fore, depending on the cost budget of the application, one can choose among the different model types by trading-off accuracy or response latency. Hence, it is evident that there is a large optimization space where different models can be selected based upon the needs of the applications. Prior work [9] tries to solve model selection only from a through- put perspective where different sized batching of multiple inference queries together results in varied throughput. Observation 1: Model selection should be focused on meeting the cost requirement of an application without compromising on the accuracy and or latency constraint. 2.2 Performance under given constraint Model selection is not an independent problem because the user-applications also have a cost constraint incurred as a result of procuring resources from the public cloud. We compare the cost of deploying the inference service on a group of virtual machines and serverless functions. We use m4- large instances for VMs and we fix the number of inference queries each VM can handle in parallel, without violating response latencies based on our characterization on AWS EC2. The serverless functions are configured according to the memory requirements of each model. Figure 3a plots the cost of hosting the iso-latency model types (shown in Figure 2a) for a constant request arrival rates of 10, 50, 100, 200 req sec over 1 hour duration. It can be seen that virtual machines are always cheaper compared to using serverless functions for all constant request rates. A similar trend is observed for the iso-accuracy model types, which is shown in Figure 3b. It is also possible to use bigger VMs, which can handle more concurrent requests compared to m4-large, thus mini- mizing the total number of VMs used. However, we observe that the pricing of EC2 VMs is a linear function of the VM size in terms of compute capacity and memory. Hence, nor- malized by number of requests, bigger VMs would still incur similar costs as smaller VMs. Observation 2: VMs should be used to handle requests during constant arrival rates.\n\n--- Segment 7 ---\nHence, nor- malized by number of requests, bigger VMs would still incur similar costs as smaller VMs. Observation 2: VMs should be used to handle requests during constant arrival rates. Also, the number of concurrent requests which can be executed in VMs should be accurately determined to meet response latency. 0 0.4 0.8 1.2 1.6 Berkley WITS Twitter Wiki Normalized VMs reactive util_aware exascale Figure 4. Over-provisioning of util_aware and exascale, nor- malized to a baseline reactive scheme for four traces. 2.3 Over-provisioning VMs Real-world request arrivals rates are usually not constant as they significantly vary over time (e.g. diurnal, flash-crowds etc.) Therefore, resource procurement and management deci- sions need to be adjusted depending on the resource utiliza- tion load and arrival rates. Public cloud providers leave these major decisions to be łmanually handled" by users, which is very time consuming and strenuous. As a result, the major- ity of application providers use static resource provisioning, which results in poor resource utilization and higher costs. Prior works [3, 9] have tried to solve the resource scal- ing problem with respect to hosting the applications in VMs. They employ autoscaling mechanisms to cope up with dy- namic load. These autoscaling mechanisms can be of two types: (i) spawn VMs if the resource utilization of existing VMs reaches a certain threshold (80 in most cases) [9], and (ii) spawn additional VMs than predicted request demand [6]. We name the former autoscaling scheme as util_aware and the later as exascale. Both these schemes suffer from over- provisioning VMs because (i) we cannot always accurately predict the future load, and (ii) resource utilization is not always the right indicator for increased load. We conduct simulation experiments to compare the schemes, using the profiled values (explained in Section 2.2) for four different well-known request arrival traces. Each request in the trace is associated with an ML inference query, which is randomly picked from our model pool. Figure 4 shows the ratio of over-provisioned VMs compared to a baseline reactive autoscaling mechanism.\n\n--- Segment 8 ---\nEach request in the trace is associated with an ML inference query, which is randomly picked from our model pool. Figure 4 shows the ratio of over-provisioned VMs compared to a baseline reactive autoscaling mechanism. It can be seen that although both util_aware and exascale can reduce SLO vi- olations (shown in Figure 5), they still suffer from 20 to 30 over-provisioned VMs across all four traces. This, in turn, increases the cost of deployment (shown in Figure 5), compared to baseline reactive scheme. WoSC 20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. 0 5 10 0 1 2 Berkley WITS Twitter Wiki SLO Violations Normalized Cost reactive util_aware exascale mixed Figure 5. Cost of using mixed compared to util_aware and exascale, normalized to a baseline reactive scheme for four traces. Percentage of SLO violations for each scheme are shown in the line graph (the corresponding color in the bar-graph is used for all the schemes.) 0 200 400 600 0 0.6 1.2 1.8 2.4 3 Cost ( ) Model Type exec_time(ms) Memory(GB) Cost( ) Figure 7. Cost variation for different allocations in serverless func- tions. Compute time (seconds) and Memory allocated (GB) is shown on left Y-axis. Cost ( ) is shown in right Y-axis. Observation 3: Only-VM based resource procurement should not be used during dynamic load as it leads to over-provisioned resources and increased cost. 2.4 Using serverless functions with VMs The provisioning latency is a major contributor for VM over-provisioning during request surges be- cause the increased time to provision new VMs results in the increase of response latencies which in-turn leads to provisioning more VMs in advance. 0 500 1000 1500 wiki WITS berkley Twitter Request Rate Avg Req Max Req Figure 6. Peak-to-median ratio. Prior works [5, 10] try to hide the pro- visioning latency of VMs by using server- less functions as a handover mechanism when starting new VMs. We name this scheme as mixed pro- curement.\n\n--- Segment 9 ---\nPrior works [5, 10] try to hide the pro- visioning latency of VMs by using server- less functions as a handover mechanism when starting new VMs. We name this scheme as mixed pro- curement. However, these schemes do not address the holis- tic problem by taking into account model selection, resource selection, and resource scaling to cope up with user-specified constraints. We conduct similar experiments to mimic the mixed procurement scheme. As shown in Figure 5, mixed procurement reduces the over-provisioning cost of VMs. At the same time it also minimizes latency violations equivalent to exascale scheme. However, we argue that there is scope to further optimize resource procurement based on the fre- quency of peak load and constant load in a given request arrival scenario. Figure 6 plots the peak-to-median ratio for three different traces. From our simulation experiments we observe that mixed procurement did not reduce cost of Wiki trace. This is because the difference between peak-to-median in the traces are not large and therefore more functions get offloaded to serverless functions. Thus, using serverless func- tions for such scenarios will not drastically reduce cost. For the other traces like Berkeley, WITS and Twitter, the peak- to-median difference is more than 50 and therefore they can benefit from offloading requests to serverless functions. Observation 4: It is important to note that, the request arrival pattern plays a key role in determining if mixed procurement can be cost effective. 2.5 Challenges with serverless functions Apart from arrival rates, memory allocation to serverless functions play a non-trivial role in terms of cost. In our exper- iments we configure the memory allocation to the lambda function such that individual query latency is within the user-specified latency constraint. We conducted character- ization experiments on AWS Lambda, currently the most predominant serverless function provider, to study the mem- ory allocated vs computation time trade-off. Figure 7 shows the computation time and cost for executing 1 million infer- ence queries for three different model types with different memory allocations. We vary the memory allocation, starting from least required memory for the model to the maximum available limit in AWS (3GB)1. It can be clearly seen that the computation time reduces with increased memory alloca- tion but also results in higher cost of deployment for every model type.\n\n--- Segment 10 ---\nWe vary the memory allocation, starting from least required memory for the model to the maximum available limit in AWS (3GB)1. It can be clearly seen that the computation time reduces with increased memory alloca- tion but also results in higher cost of deployment for every model type. This is because, inherently, serverless providers allocate a powerful compute core for functions with higher memory allocation. Therefore, depending on the latency re- quirements of the user applications, serverless functions need to be allocated the appropriate memory. However, this might result in increased cost when using serverless functions along with VMs for varying latency requirements. Hence, the over- all cost incurred by mixed procurement can be higher or lower than VM-only autoscaling policies. Prior work like Cherrypick [1] solves the resource se- lection and configuration problems from VM perspective but does not consider Serverless Functions. We argue that compared to VMs there is more variability in configurations for serverless functions because the resources are billed at a more fine-grained2 allocation of CPU and memory. Observation 5: Serverless functions can be used with VMs to avoid over-provisioning resources, but the right configuration needs to be accurately determined for the functions such that it satisfies the application cost and latency constraints. 1For squeezenet model, allocating beyond 2GB did not reduce computation time, but resulted in increased cost 2The smallest standard performance VM (C4 family) comes with 2 vcpus and 3.75GB memory. But serverless functions can be configured starting from 1 vcpu and 0.128GB memory. Implications of Public Cloud Resource Heterogeneity for Inference Serving WoSC 20, December 7ś11, 2020, Delft, Netherlands 3 How to Design Self-Managed ML Prediction Serving System? The objectives from Section 2 strongly motivate the need for a self-managed ML-prediction system that avoids the over-provisioning problem in VMs by efficiently blending serverless functions with VMs. At the same time, right-sizing the number of requests in VMs and correctly configuring serverless functions is quintessential to satisfy the three pri- mary application constraints: cost, latency, and accuracy. 3.1 Model Selection In accordance with Observation 1, model selection should be a function of any two parameters which optimize the remain- ing (third) parameter.\n\n--- Segment 11 ---\nAt the same time, right-sizing the number of requests in VMs and correctly configuring serverless functions is quintessential to satisfy the three pri- mary application constraints: cost, latency, and accuracy. 3.1 Model Selection In accordance with Observation 1, model selection should be a function of any two parameters which optimize the remain- ing (third) parameter. Prior work [9] solves an optimization problem such that the input parameters are model_type, hardware_type (CPU or GPU), and the output parameter is response latency. To do so, they suggest using offline profil- ing or results from previous executions. Unlike prior works, we suggest that the input and output parameters can be any linear combination of the three primary parameters men- tioned above, depending on the application constraints. Note that, in contrast to cost and response-latency, accuracy can- not be determined just from the previous runs. We need some feedback from the end-user to make a correct estimate of accuracy. Therefore, it would be best to build a learning- based system, which takes into account feedback (user-given data) to build a novel model selection system. 3.2 Resource selection 3.2.1 Static Load From Observation 2, it is clear that, be- sides model selection, it is crucial to select and configure the right resource to satisfy the application constraints. For applications where the request load is fairly constant over time, only VM-based resources can be procured to serve the requests. To determine the number of requests each VM can handle in parallel, we can conduct offline profiling for different model types. 3.2.2 Dynamic Load For applications with dynamic load (Observation 3), serverless functions can be used to mitigate the over-provisioning cost of VMs. However, a single ap- plication can contain a mix of queries with varying latency demands. Therefore, queries with strict latency requirements can be scheduled on serverless functions, if a VM with free resources is unavailable. To handle dynamic load variations, a load-monitor can be designed such that it constantly moni- tors different periods of static load and peak load. We propose to plug-in intelligent peak-to-median prediction policies (in accordance to Observation 4) , which can aid the load-monitor to estimate the duration of static load.\n\n--- Segment 12 ---\nTo handle dynamic load variations, a load-monitor can be designed such that it constantly moni- tors different periods of static load and peak load. We propose to plug-in intelligent peak-to-median prediction policies (in accordance to Observation 4) , which can aid the load-monitor to estimate the duration of static load. Furthermore, it can measure the peak-to-median ratio in sampling windows, which can be used to decide if serverless functions are re- quired to balance the load. However, during flash-crowds, where load-prediction fails to accurately estimate the load, serverless functions can inherently be used to handle requests to meet the response latency, but by incurring higher costs. 3.2.3 Provisioning Time vs Execution Time We know that new VMs take a few hundred seconds to start-up. Server- less functions can start-up much faster (1s-10s), but they also incur additional latency to load a pre-trained model from external data-store. Prior literature [5, 10] tries to hide the model load latency by pre-warming serverless function in- stances through periodically issuing dummy requests. How- ever, such hacks can fail if the cloud service provider decides to change the idle timeout of function instances or change the overall mechanism to recycle idle function instances. Rather than capitalizing on such design hacks, we need to develop prediction policies to estimate load correctly. Also, we suggest service providers should handle the pre-warming decision by knowing model-wise usage statistics to enable instance sharing, which uses the same models. This would lead to a reduction in cold-start latencies incurred for users with the same type of requests. 3.2.4 Configuring Serverless Functions In keeping with Observation 5, it is quintessential to configure the mem- ory allocation of serverless functions to meet the application SLOs. Through offline profiling or initial runs, we can de- termine the right memory allocation for a given response latency. From our observations in AWS Lambda, three types of cores are allocated in the increasing order of the memory allocation (0.5GB, 1.5GB, and 2GB). Also, these policies can be changed over time by Amazon, and they can also be dif- ferent for other cloud providers [8]. Therefore, the resource manager should be able to leverage this information to make optimal serverless function configuration decisions.\n\n--- Segment 13 ---\nAlso, these policies can be changed over time by Amazon, and they can also be dif- ferent for other cloud providers [8]. Therefore, the resource manager should be able to leverage this information to make optimal serverless function configuration decisions. 4 Evaluation and Initial Results This section introduces how an ML-serving framework can capitalize on the design choices discussed in Section 3. We design three different experiments to study the effects on cost of ML servings due to (i) varying SLOs and (ii) varying application constraints. Implementation Methodology: We developed a proto- type on top of Amazon EC2 and Lambda services to evaluate the some of the benefits of our proposed design choices. We use AWS as the testbed for conducting extensive experiments. The types of instance used in our evaluation include all the c5 and m5 instances for EC2. By offline profiling, we estimate the number of model instances each VM can execute in par- allel without violating the model latency. Also, we estimate the right configuration of lambda functions by conduction offline experiments. For the model selection problem, we maintain an offline model cache which consists of the de- tails of individual model latency and accuracy profiled by executing on c4 large VM. The scheduler will pick the right model combinations from the cache based on the application requirements. We implement a load generator, which uses a WoSC 20, December 7ś11, 2020, Delft, Netherlands J.R. Gunasekaran, et al. 0 1 2 3 4 0 0.5 1 1.5 util_aware exascale mixed paragon SLO Violations Normalized Cost Cost SLO violations (a) Workload-1: Berkeley Trace. 0 5 10 0 0.5 1 1.5 util_aware exascale mixed paragon SLO Violations Normalized Cost Cost SLO violations (b) Workload-1: WITS Trace. Figure 8. Comparison of the resource procurement cost for two different traces using five different schemes. The cost is normalized to reactive scaling scheme. 1 hour sample of the real-world trace for request arrival time generation. Each request is derived from a pool of pre-trained ML inference models for image classification (as explained in Section 2). We use Apache MXNet and TensorFlow frame- work to deploy and run inference on the models. Evaluation: We evaluate our results by comparing the cost, latency and accuracy for two different workloads.\n\n--- Segment 14 ---\nWe use Apache MXNet and TensorFlow frame- work to deploy and run inference on the models. Evaluation: We evaluate our results by comparing the cost, latency and accuracy for two different workloads. Workload- 1 consists of a mix of queries which have both strict and relaxed latency requirements. We compare the execution of this workload against the following resource procure- ment schemes: (i) util_aware, (ii) exascale, (iii) mixed and (iv) Paragon. These schemes are modeled after state-of-the-art prior works as explained earlier in Section 2.3. The Paragon scheme does not offload to lambdas for relaxed latency queries. Workload-2 consists of different cost, accuracy and latency requirements for all queries. We compare the Paragon model selection scheme against a naive constraints-unaware model selection scheme. Results: Figure 8 plots the SLO and cost for workload-1 across Berkley and WITS trace. It can be seen that mixed scheme has similar cost to reactive but it reduces SLO vi- olations by up to 60 . This is because, the mixed scheme offloads request in the peak to serverless functions. However, the Paragon scheme is 10 more cost-effective than mixed and at the same time ensures similar SLOs. This is because the Paragon scheme is aware of the latency requirements of individual queries and does not blindly offload queries to lambdas when there is increase in load. Therefore, this results in reduced cost and at the same time does not violate SLOs. This re-instantiates our claim that the resource procurement scheme needs to be aware of request constraints. Figure 1 shows the candidate models which can be used for a given latency and accuracy. Our Paragon scheme optimizes the model selection for workload-2 such that, it chooses the least cost-effective model for the given accuracy and latency constraint. The naive model selection policy would not choose the models as its oblivious to user requirements and model characteristics. In our experiments, compared to naive selection scheme which does not optimize model selection for cost, the Paragon schemes reduces the cost of resource procurement by up to 20 (results are not plotted). This is because the Paragon scheme jointly considers all three parameters and chooses the least costing model. 5 Conclusion There is wide-spread prominence in the adoption of ML- based prediction systems spanning across a wide range of application domains.\n\n--- Segment 15 ---\nThis is because the Paragon scheme jointly considers all three parameters and chooses the least costing model. 5 Conclusion There is wide-spread prominence in the adoption of ML- based prediction systems spanning across a wide range of application domains. The critical challenge of deploying ML prediction serving applications in public cloud is to combine both model and resource heterogeneity towards optimizing for application constraints. In this paper, we propose to build a self-managed ML prediction system, which can optimize the diverse application requirements based on characteristics of heterogeneous public cloud resource offerings. Towards this, we discuss the trade-offs of intermixing resources like serverless functions along with VMs and identify the key challenges associated with configuring these resources. We propose multiple key-policies to make resource manage- ment; (i) latency aware, (ii) multi-dimensional SLO aware, and (iii) request load variation aware. These policies can be collectively used for cost-effective prediction serving with- out compromising on latency and accuracy. Acknowledgement This research was partially supported by NSF grants 1931531, 1955815, 1629129, 1763681, 1629915, 1908793, 1526750 and we thank NSF Chameleon Cloud project CH- 819640 for their generous compute grant. References [1] Omid Alipourfard, Hongqiang Harry Liu, Jianshu Chen, Shivaram Venkataraman, Minlan Yu, and Ming Zhang. 2017. CherryPick: Adap- tively Unearthing the Best Cloud Configurations for Big Data Analytics. In (NSDI). [2] Marc Brooker, Andreea Florescu, Diana-Maria Popa, Rolf Neugebauer, Alexandru Agache, Alexandra Iordache, Anthony Liguori, and Phil Piwonka. 2020. Firecracker: Lightweight Virtualization for Serverless Applications. In NSDI. [3] Andrew Chung, Jun Woo Park, and Gregory R. Ganger. 2018. Stratus: Cost-aware Container Scheduling in the Public Cloud. In SoCC. [4] Arpan Gujarati, Sameh Elnikety, Yuxiong He, Kathryn S. McKinley, and Björn B. Brandenburg. 2017. Swayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. In USENIX Middleware Conference.\n\n--- Segment 16 ---\nSwayam: Distributed Autoscaling to Meet SLAs of Machine Learning Inference Services with Resource Efficiency. In USENIX Middleware Conference. [5] J. R. Gunasekaran, P. Thinakaran, et al. 2019. Spock: Exploiting Server- less Functions for SLO and Cost Aware Resource Procurement in Public Cloud. In CLOUD. [6] Aaron Harlap, Andrew Chung, Alexey Tumanov, Gregory R. Ganger, and Phillip B. Gibbons. 2018. Tributary: spot-dancing for elastic ser- vices with latency SLOs. In ATC. [7] Akshitha Sriraman, Abhishek Dhanotia, and Thomas Wenisch. 2019. Softsku: Optimizing server architectures for microservice diversity scale. In ISCA. [8] Liang Wang, Mengyuan Li, Yinqian Zhang, Thomas Ristenpart, and Michael Swift. 2018. Peeking Behind the Curtains of Serverless Plat- forms. In ATC. [9] Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis. 2019. A Case for Managed and Model-Less Inference Serv- ing. In HotOS. ACM. [10] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. 2019. MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving. In ATC.\n\n