=== ORIGINAL PDF: 2506.08911v1_Implementing_Keyword_Spotting_on_the_MCUX947_Micro.pdf ===\n\nRaw text length: 16173 characters\nCleaned text length: 16050 characters\nNumber of segments: 12\n\n=== CLEANED TEXT ===\n\nImplementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU 1st Petar Jakuš Faculty of Electrical Engineering and Computing Zagreb, Croatia 2nd Hrvoje Džapo Faculty of Electrical Engineering and Computing Zagreb, Croatia Abstract This paper presents a keyword spotting (KWS) system implemented on the NXP MCXN947 microcontroller with an integrated Neural Processing Unit (NPU), enabling real-time voice interaction on resource-constrained devices. The system combines MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training to reduce model size with minimal accuracy drop. Experimen- tal results demonstrate a 59 speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06 accuracy with a model size of 30.58 KB, demonstrating the feasibility of efficient, low-power voice interfaces on embedded platforms. Index Terms keyword spotting, microcontroller, MCU, Neural Processing Unit, NPU, edge AI, QAT I. INTRODUCTION With the growing adoption of IoT devices, keyword spotting (KWS) has become essential for voice-controlled interaction in smart assistants and embedded systems. However, implementing KWS on resource-constrained devices remains challenging due to limited processing power, memory constraints, and strict energy require- ments, demanding optimized solutions for microcontroller deployment. A typical keyword spotting system consists of signal acquisition, feature extraction and a neural network [7]. Signal acquisition in keyword spotting systems is typically performed using a microphone, most often a MEMS microphone, which converts pressure variations into electrical signals. The raw audio signal often con- tains noise and amplitude variations, requiring signal preprocessing to improve robustness and accuracy. Com- mon noise reduction techniques include adaptive noise cancellation (ANC) [9] and beamforming [10]. Feature extraction transforms the input signal into a compact and informative representation suitable for classification. Widely used methods include Mel-frequency cepstral coefficients (MFCC) [1], [11] and RASTA-PLP [8], [18]. To reduce the computational complexity of the preprocessing stage, simplified approaches such as direct application of the Fast Fourier Transform (FFT) have been proposed [2]. For ultra-low-power applications, analog front-end approaches and featureless techniques have been explored [3], [14], [17]. Alternatively, feature extraction can be integrated into the neural network itself, as shown in [4], [15]. The classification stage typically employs neural network architectures. Convolutional Neural Networks (CNNs) have shown strong performance in KWS tasks, especially when combined with model compression techniques such as pruning and quantization [5], [12], [19]. Hardware acceleration for neural network infer- ence, such as using dedicated co-processors, has been shown to significantly reduce power consumption [14]. Recurrent architectures like Long Short-Term Memory (LSTM) networks [6] and Gated Recurrent Units (GRUs), including energy-efficient variants such as eGRU [2], are also employed to capture temporal dependencies in speech signals. Advanced compression techniques such as network binarization have further reduced memory footprints for deployment on edge devices [3]. Hybrid architectures combining CNNs and RNNs have been proposed to improve classification performance while maintaining efficiency [19]. This work presents an optimized KWS system target- ing the NXP MCXN947 microcontroller, leveraging its integrated NPU to accelerate a quantized CNN model with MFCC-based features. By employing quantization- aware training (QAT), the method minimizes accuracy loss while ensuring efficient quantization, significantly reducing model size and inference latency. Experimental results demonstrate the feasibility of deploying responsive, low-power voice interfaces on resource-constrained edge devices. II. METHODOLOGY A. Data The Google Speech Commands dataset [16] con- tains 105,829 one-second voice recordings of 35 words, recorded at 16 kHz sample rate on mobile devices. The dataset is divided into training (84,843), test (11,005), and validation (9,981) sets as originally proposed. arXiv:2506.08911v1 [cs.HC] 10 Jun 2025 Figure 1. MFCC original signal and frames. Figure 2. Frames after applying Hamming function. 1) Preprocessing: Mel-Frequency Cepstral Coeffi- cients (MFCC) were used for feature extraction through the following pipeline: Framing and Windowing: Audio signals were segmented into 25 ms frames with 10 ms hop size (400 samples at 16 kHz), as illustrated in Figure 1. A Hamming window was applied to each frame to reduce spectral leakage (Figure 2): w(n) 0.54 0.46 cos 2πn N Spectral Analysis: Fast Fourier Transform (FFT) converted each windowed frame to the frequency domain, followed by power spectrum calculation (Figure 3). For a given FFT output X[k], the power spectrum P[k] is defined as: P[k] 1 N X[k] 2 where N is the FFT length. Mel-scale Filtering: A bank of 40 filters spanning 40 Hz to 7.6 kHz was applied to align with human auditory perception and 4). The Mel scale provides higher frequency resolution at lower frequencies, matching human hearing characteristics. MFCC Computation: Discrete Cosine Transform (DCT) was applied to the log mel-spectrum to decor- Figure 3. Power spectrum. Figure 4. Frames Mel spectrums. Figure 5. MFCC spectogram. relate coefficients, producing 20 MFCC features per frame following HTK convention (Figure 5). This preprocessing pipeline transforms raw audio into perceptually-motivated features suitable for speech recognition tasks. B. Model Architecture and Quantization The proposed architecture employs a compact CNN op- timized for edge deployment, featuring two convolutional layers (each with 2D convolution, batch normalization, and max pooling) followed by two dense layers. The model is trained specifically for the "Marvin" keyword using Adam optimization (learning rate 0.001) over 10 epochs, with class weights of 24.81 (Marvin) and 0.51 (non-Marvin) to address imbalance. The model architecture incorporates quantization-aware training (QAT) to enable efficient conversion from 32- bit floating-point to 8-bit fixed-point weight representa- tion. The complete QAT approach applies quantization constraints to all layers during training, simulating 8- bit operations in forward passes while maintaining full precision for backward propagation. While this increases training time by approximately 20 , it ensures minimal accuracy loss when after quantization. Alternatively, a fine-tuning approach can be applied where a pre-trained full-precision model is adapted with QAT for fewer epochs, offering reduced training time with comparable accuracy. The regular model architecture, the QAT prepared version and final quantized implementation are detailed in Tables I, II, and III, respectively, including layer-specific quantization parameters. C. Model deployment The quantized TensorFlow Lite model was deployed on the MCXN947 microcontroller both using its eIQ Neutron NPU accelerator and the ARM Cortex-M33 CPU. The quantized model was converted to an NPU-compatible format through NXP s eIQ Toolkit. As shown in Figure 6, 2 Table I REGULAR NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 conv1 (Conv2D) (None, 96, 18, 32) 320 bn1 (BatchNormalization) (None, 96, 18, 32) 128 pool1 (MaxPooling2D) (None, 48, 9, 32) 0 conv2 (Conv2D) (None, 46, 7, 64) 18496 bn2 (BatchNormalization) (None, 46, 7, 64) 256 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 dropout (Dropout) (None, 64) 0 fc1 (Dense) (None, 128) 8320 output (Dense) (None, 1) 129 Total params: 27649 (108.00 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 192 (768.00 B) Table II QAT PREPARED NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 quantize_layer (QuantizeLayer) (None, 98, 20, 1) 3 quant_conv1 (QuantizeWrapperV2) (None, 96, 18, 32) 387 quant_bn1 (QuantizeWrapperV2) (None, 96, 18, 32) 129 quant_pool1 (QuantizeWrapperV2) (None, 48, 9, 32) 1 quant_conv2 (QuantizeWrapperV2) (None, 46, 7, 64) 18627 quant_bn2 (QuantizeWrapperV2) (None, 46, 7, 64) 257 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 quant_dropout (QuantizeWrapperV2) (None, 64) 1 quant_fc1 (QuantizeWrapperV2) (None, 128) 8325 quant_output (QuantizeWrapperV2) (None, 1) 134 Total params: 27864 (108.84 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 407 (1.59 KB) the conversion process restructured certain layers to take advantage of the dedicated hardware acceleration capabilities of the NPU. Both models are converted to static arrays and stored in flash memory of the MCU. Figure 6. Model architecture after NPU conversion [13]. III. RESULTS The MFCC feature extraction demonstrated efficient processing with a average computation time of 431 µs Table III QUANTIZED MODEL ARCHITECTURE Layer (type) Output Shape Param InputLayer [-1, 98, 20, 1] 0 Conv2D [-1, 96, 18, 32] 320 weights 32 biases MUL, ADD (BatchNorm) [-1, 96, 18, 32] 64 scale 32 offset MaxPool2D [-1, 48, 9, 32] 0 Conv2D [-1, 46, 7, 64] 18,432 weights 64 biases MUL, ADD (BatchNorm) [-1, 46, 7, 64] 64 scale 64 offset MaxPool2D [-1, 23, 3, 64] 0 Mean [-1, 64] 0 FullyConnected [-1, 128] 8,192 weights 128 biases FullyConnected [-1, 1] 128 weights 1 bias Logistic [-1, 1] 0 Total params: 27521 (28.20 KB) Trainable params: 27297 (27.97 KB) Non-trainable params: 224 (224 B) per frame on Cortex-M33. Evaluation of three variants of the model revealed significant performance differences. The regular floating-point model achieved 99.14 accuracy, with the confusion matrix shown in Table IV. On the Intel Core i5-10210U CPU, this model required 58.67 ms per inference with a memory footprint of 383,674 bytes. Quantization reduced the size of the model to 35,744 bytes (reduction of 90.68 ) while achieving accuracy of 97.06 , with the confusion matrix shown in Table V. The quantized version showed faster inference at 0.42 ms on the same Intel CPU. When deployed on the MCXN947 s Cortex-M33, the inference time increased to 228.2 ms. Table IV REGULAR MODEL CONFUSION MATRIX Predicted Non-Marvin Predicted Marvin Real Non-Marvin 10746 64 Real Marvin 31 164 Table V QUANTIZED MODEL CONFUSION MATRIX Predicted Non-Marvin Predicted Marvin Real Non-Marvin 10501 309 Real Marvin 15 180 3 The NPU-optimized version preserved the quantized model s 97.06 accuracy while further reducing both model size (30,576 bytes) and inference time (3,847 µs on MCXN947). This represents a 98.3 reduction in model size and 59 speedup compared to the implementation using only the ARM Cortex-M33 core. The complete performance metrics are summarized in Table VI. Table VI MODEL STATISTICS Size [B] Acc. [ ] i5 [ms] MCXN947 [µs] Regular 383674 99.14 50.67 - Quantized 35744 97.06 0.42 228210 NPU 30576 97.06 - 3847 IV. CONCLUSION This paper demonstrates the successful implemen- tation of keyword spotting on the NXP MCXN947 microcontroller using quantization-aware training and NPU acceleration. The approach achieves a 90.68 reduction in model size with 97.06 accuracy, while NPU acceleration provides a 59 speedup compared to CPU execution. The complete processing pipeline requires less than 5 ms, enabling deployment on embedded platforms. The results validate the feasibility of implementing a speech recognition system in resource-constrained devices through the combination of model compression and specialized hardware acceleration. This work con- tributes quantifiable performance metrics for edge AI deployment in privacy-preserving and energy-constrained applications. Future work should investigate power consumption characteristics, multi-keyword architectures, and robust- ness evaluation under varying acoustic conditions. REFERENCES [1] Shalbbya Ali, Safdar Tanweer, Syed Sibtain Khalid, and Naseem Rao. Mel frequency cepstral coefficient: a review. ICIDSSD, 2020. [2] Justice Amoh and Kofi M Odame. An optimized recurrent unit for ultra-low-power keyword spotting. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 3(2):1 17, 2019. [3] Gianmarco Cerutti, Lukas Cavigelli, Renzo Andri, Michele Magno, Elisabetta Farella, and Luca Benini. Sub-mw keyword spotting on an mcu: Analog binary feature extraction and binary neural networks. IEEE Transactions on Circuits and Systems I: Regular Papers, 69(5):2002 2012, 2022. [4] Guoguo Chen, Carolina Parada, and Tara N Sainath. Query- by-example keyword spotting using long short-term memory networks. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5236 5240. IEEE, 2015. [5] Cristian Cioflan, Lukas Cavigelli, Manuele Rusci, Miguel De Prado, and Luca Benini. On-device domain learning for keyword spotting on low-power extreme edge embedded systems. In 2024 IEEE 6th International Conference on AI Circuits and Systems (AICAS), pages 6 10. IEEE, 2024. [6] Juan Sebastian Piedrahita Giraldo and Marian Verhelst. Laika: A 5uw programmable lstm accelerator for always-on keyword spotting in 65nm cmos. In ESSCIRC 2018-IEEE 44th European Solid State Circuits Conference (ESSCIRC), pages 166 169. IEEE, 2018. [7] Alexander Gruenstein, Raziel Alvarez, Chris Thornton, and Mohammadali Ghodrat. A cascade architecture for keyword spotting on mobile devices. arXiv preprint arXiv:1712.03603, 2017. [8] Hynek Hermansky, Nelson Morgan, Aruna Bayya, and Phil Kohn. Rasta-plp speech analysis. In Proc. IEEE Int l Conf. Acoustics, speech and signal processing, volume 1, pages 121 124, 1991. [9] Yiteng Huang, Thad Hughes, Turaj Z. Shabestary, and Taylor Applebaum. Supervised noise reduction for multichannel keyword spotting. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5474 5478, 2018. [10] Xuan Ji, Meng Yu, Jie Chen, Jimeng Zheng, Dan Su, and Dong Yu. Integration of multi-look beamformers for multi-channel keyword spotting. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7464 7468, 2020. [11] Arnav Kundu, Mohammad Samragh, Minsik Cho, Priyanka Padmanabhan, and Devang Naik. Heimdal: Highly efficient method for detection and localization of wake-words. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1 5. IEEE, 2023. [12] David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph Dureau. Federated learning for keyword spotting. In ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 6341 6345. IEEE, 2019. [13] Roeder Lutz. Netron. 2025. Model visualiza- tion, Accessed: 2025-04-16. [14] Mehmet Gorkem Ulkar and Osman Erman Okman. Ultra- low power keyword spotting at the edge. arXiv preprint arXiv:2111.04988, 2021. [15] Paola Vitolo, Rosalba Liguori, Luigi Di Benedetto, Alfredo Rubino, and Gian Domenico Licciardo. Automatic audio feature extraction for keyword spotting. IEEE Signal Processing Letters, 31:161 165, 2023. [16] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition, 2018. [17] Minhao Yang, Chung-Heng Yeh, Yiyin Zhou, Joao P. Cerqueira, Aurel A. Lazar, and Mingoo Seok. A 1uw voice activity detector using analog feature extraction and digital deep neural network. In 2018 IEEE International Solid-State Circuits Conference - (ISSCC), pages 346 348, 2018. [18] Yu-min Zeng, Zhen-yang Wu, Tiago Falk, and Wai-yip Chan. Robust gmm based gender classification using pitch and rasta- plp parameters of speech. In 2006 International Conference on Machine Learning and Cybernetics, pages 3376 3379, 2006. [19] Yundong Zhang, Naveen Suda, Liangzhen Lai, and Vikas Chandra. Hello edge: Keyword spotting on microcontrollers. arXiv preprint arXiv:1711.07128, 2017. 4\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nImplementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU 1st Petar Jakuš Faculty of Electrical Engineering and Computing Zagreb, Croatia 2nd Hrvoje Džapo Faculty of Electrical Engineering and Computing Zagreb, Croatia Abstract This paper presents a keyword spotting (KWS) system implemented on the NXP MCXN947 microcontroller with an integrated Neural Processing Unit (NPU), enabling real-time voice interaction on resource-constrained devices. The system combines MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training to reduce model size with minimal accuracy drop. Experimen- tal results demonstrate a 59 speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06 accuracy with a model size of 30.58 KB, demonstrating the feasibility of efficient, low-power voice interfaces on embedded platforms. Index Terms keyword spotting, microcontroller, MCU, Neural Processing Unit, NPU, edge AI, QAT I. INTRODUCTION With the growing adoption of IoT devices, keyword spotting (KWS) has become essential for voice-controlled interaction in smart assistants and embedded systems. However, implementing KWS on resource-constrained devices remains challenging due to limited processing power, memory constraints, and strict energy require- ments, demanding optimized solutions for microcontroller deployment. A typical keyword spotting system consists of signal acquisition, feature extraction and a neural network [7]. Signal acquisition in keyword spotting systems is typically performed using a microphone, most often a MEMS microphone, which converts pressure variations into electrical signals. The raw audio signal often con- tains noise and amplitude variations, requiring signal preprocessing to improve robustness and accuracy. Com- mon noise reduction techniques include adaptive noise cancellation (ANC) [9] and beamforming [10]. Feature extraction transforms the input signal into a compact and informative representation suitable for classification. Widely used methods include Mel-frequency cepstral coefficients (MFCC) [1], [11] and RASTA-PLP [8], [18]. To reduce the computational complexity of the preprocessing stage, simplified approaches such as direct application of the Fast Fourier Transform (FFT) have been proposed [2]. For ultra-low-power applications, analog front-end approaches and featureless techniques have been explored [3], [14], [17].\n\n--- Segment 2 ---\nTo reduce the computational complexity of the preprocessing stage, simplified approaches such as direct application of the Fast Fourier Transform (FFT) have been proposed [2]. For ultra-low-power applications, analog front-end approaches and featureless techniques have been explored [3], [14], [17]. Alternatively, feature extraction can be integrated into the neural network itself, as shown in [4], [15]. The classification stage typically employs neural network architectures. Convolutional Neural Networks (CNNs) have shown strong performance in KWS tasks, especially when combined with model compression techniques such as pruning and quantization [5], [12], [19]. Hardware acceleration for neural network infer- ence, such as using dedicated co-processors, has been shown to significantly reduce power consumption [14]. Recurrent architectures like Long Short-Term Memory (LSTM) networks [6] and Gated Recurrent Units (GRUs), including energy-efficient variants such as eGRU [2], are also employed to capture temporal dependencies in speech signals. Advanced compression techniques such as network binarization have further reduced memory footprints for deployment on edge devices [3]. Hybrid architectures combining CNNs and RNNs have been proposed to improve classification performance while maintaining efficiency [19]. This work presents an optimized KWS system target- ing the NXP MCXN947 microcontroller, leveraging its integrated NPU to accelerate a quantized CNN model with MFCC-based features. By employing quantization- aware training (QAT), the method minimizes accuracy loss while ensuring efficient quantization, significantly reducing model size and inference latency. Experimental results demonstrate the feasibility of deploying responsive, low-power voice interfaces on resource-constrained edge devices. II. METHODOLOGY A. Data The Google Speech Commands dataset [16] con- tains 105,829 one-second voice recordings of 35 words, recorded at 16 kHz sample rate on mobile devices. The dataset is divided into training (84,843), test (11,005), and validation (9,981) sets as originally proposed. arXiv:2506.08911v1 [cs.HC] 10 Jun 2025 Figure 1. MFCC original signal and frames. Figure 2. Frames after applying Hamming function.\n\n--- Segment 3 ---\nFigure 2. Frames after applying Hamming function. 1) Preprocessing: Mel-Frequency Cepstral Coeffi- cients (MFCC) were used for feature extraction through the following pipeline: Framing and Windowing: Audio signals were segmented into 25 ms frames with 10 ms hop size (400 samples at 16 kHz), as illustrated in Figure 1. A Hamming window was applied to each frame to reduce spectral leakage (Figure 2): w(n) 0.54 0.46 cos 2πn N Spectral Analysis: Fast Fourier Transform (FFT) converted each windowed frame to the frequency domain, followed by power spectrum calculation (Figure 3). For a given FFT output X[k], the power spectrum P[k] is defined as: P[k] 1 N X[k] 2 where N is the FFT length. Mel-scale Filtering: A bank of 40 filters spanning 40 Hz to 7.6 kHz was applied to align with human auditory perception and 4). The Mel scale provides higher frequency resolution at lower frequencies, matching human hearing characteristics. MFCC Computation: Discrete Cosine Transform (DCT) was applied to the log mel-spectrum to decor- Figure 3. Power spectrum. Figure 4. Frames Mel spectrums. Figure 5. MFCC spectogram. relate coefficients, producing 20 MFCC features per frame following HTK convention (Figure 5). This preprocessing pipeline transforms raw audio into perceptually-motivated features suitable for speech recognition tasks. B. Model Architecture and Quantization The proposed architecture employs a compact CNN op- timized for edge deployment, featuring two convolutional layers (each with 2D convolution, batch normalization, and max pooling) followed by two dense layers. The model is trained specifically for the "Marvin" keyword using Adam optimization (learning rate 0.001) over 10 epochs, with class weights of 24.81 (Marvin) and 0.51 (non-Marvin) to address imbalance. The model architecture incorporates quantization-aware training (QAT) to enable efficient conversion from 32- bit floating-point to 8-bit fixed-point weight representa- tion. The complete QAT approach applies quantization constraints to all layers during training, simulating 8- bit operations in forward passes while maintaining full precision for backward propagation.\n\n--- Segment 4 ---\nThe model architecture incorporates quantization-aware training (QAT) to enable efficient conversion from 32- bit floating-point to 8-bit fixed-point weight representa- tion. The complete QAT approach applies quantization constraints to all layers during training, simulating 8- bit operations in forward passes while maintaining full precision for backward propagation. While this increases training time by approximately 20 , it ensures minimal accuracy loss when after quantization. Alternatively, a fine-tuning approach can be applied where a pre-trained full-precision model is adapted with QAT for fewer epochs, offering reduced training time with comparable accuracy. The regular model architecture, the QAT prepared version and final quantized implementation are detailed in Tables I, II, and III, respectively, including layer-specific quantization parameters. C. Model deployment The quantized TensorFlow Lite model was deployed on the MCXN947 microcontroller both using its eIQ Neutron NPU accelerator and the ARM Cortex-M33 CPU. The quantized model was converted to an NPU-compatible format through NXP s eIQ Toolkit.\n\n--- Segment 5 ---\nC. Model deployment The quantized TensorFlow Lite model was deployed on the MCXN947 microcontroller both using its eIQ Neutron NPU accelerator and the ARM Cortex-M33 CPU. The quantized model was converted to an NPU-compatible format through NXP s eIQ Toolkit. As shown in Figure 6, 2 Table I REGULAR NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 conv1 (Conv2D) (None, 96, 18, 32) 320 bn1 (BatchNormalization) (None, 96, 18, 32) 128 pool1 (MaxPooling2D) (None, 48, 9, 32) 0 conv2 (Conv2D) (None, 46, 7, 64) 18496 bn2 (BatchNormalization) (None, 46, 7, 64) 256 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 dropout (Dropout) (None, 64) 0 fc1 (Dense) (None, 128) 8320 output (Dense) (None, 1) 129 Total params: 27649 (108.00 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 192 (768.00 B) Table II QAT PREPARED NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 quantize_layer (QuantizeLayer) (None, 98, 20, 1) 3 quant_conv1 (QuantizeWrapperV2) (None, 96, 18, 32) 387 quant_bn1 (QuantizeWrapperV2) (None, 96, 18, 32) 129 quant_pool1 (QuantizeWrapperV2) (None, 48, 9, 32) 1 quant_conv2 (QuantizeWrapperV2) (None, 46, 7, 64) 18627 quant_bn2 (QuantizeWrapperV2) (None, 46, 7, 64) 257 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 quant_dropout (QuantizeWrapperV2) (None, 64) 1 quant_fc1 (QuantizeWrapperV2) (None, 128) 8325 quant_output (QuantizeWrapperV2) (None, 1) 134 Total params: 27864 (108.84 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 407 (1.59 KB) the conversion process restructured certain layers to take advantage of the dedicated hardware acceleration capabilities of the NPU.\n\n--- Segment 6 ---\nThe quantized model was converted to an NPU-compatible format through NXP s eIQ Toolkit. As shown in Figure 6, 2 Table I REGULAR NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 conv1 (Conv2D) (None, 96, 18, 32) 320 bn1 (BatchNormalization) (None, 96, 18, 32) 128 pool1 (MaxPooling2D) (None, 48, 9, 32) 0 conv2 (Conv2D) (None, 46, 7, 64) 18496 bn2 (BatchNormalization) (None, 46, 7, 64) 256 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 dropout (Dropout) (None, 64) 0 fc1 (Dense) (None, 128) 8320 output (Dense) (None, 1) 129 Total params: 27649 (108.00 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 192 (768.00 B) Table II QAT PREPARED NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 quantize_layer (QuantizeLayer) (None, 98, 20, 1) 3 quant_conv1 (QuantizeWrapperV2) (None, 96, 18, 32) 387 quant_bn1 (QuantizeWrapperV2) (None, 96, 18, 32) 129 quant_pool1 (QuantizeWrapperV2) (None, 48, 9, 32) 1 quant_conv2 (QuantizeWrapperV2) (None, 46, 7, 64) 18627 quant_bn2 (QuantizeWrapperV2) (None, 46, 7, 64) 257 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 quant_dropout (QuantizeWrapperV2) (None, 64) 1 quant_fc1 (QuantizeWrapperV2) (None, 128) 8325 quant_output (QuantizeWrapperV2) (None, 1) 134 Total params: 27864 (108.84 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 407 (1.59 KB) the conversion process restructured certain layers to take advantage of the dedicated hardware acceleration capabilities of the NPU. Both models are converted to static arrays and stored in flash memory of the MCU.\n\n--- Segment 7 ---\nAs shown in Figure 6, 2 Table I REGULAR NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 conv1 (Conv2D) (None, 96, 18, 32) 320 bn1 (BatchNormalization) (None, 96, 18, 32) 128 pool1 (MaxPooling2D) (None, 48, 9, 32) 0 conv2 (Conv2D) (None, 46, 7, 64) 18496 bn2 (BatchNormalization) (None, 46, 7, 64) 256 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 dropout (Dropout) (None, 64) 0 fc1 (Dense) (None, 128) 8320 output (Dense) (None, 1) 129 Total params: 27649 (108.00 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 192 (768.00 B) Table II QAT PREPARED NEURAL NETWORK ARCHITECTURE Layer (type) Output Shape Param input (InputLayer) (None, 98, 20, 1) 0 quantize_layer (QuantizeLayer) (None, 98, 20, 1) 3 quant_conv1 (QuantizeWrapperV2) (None, 96, 18, 32) 387 quant_bn1 (QuantizeWrapperV2) (None, 96, 18, 32) 129 quant_pool1 (QuantizeWrapperV2) (None, 48, 9, 32) 1 quant_conv2 (QuantizeWrapperV2) (None, 46, 7, 64) 18627 quant_bn2 (QuantizeWrapperV2) (None, 46, 7, 64) 257 pool2 (MaxPooling2D) (None, 23, 3, 64) 0 gap (GlobalAveragePooling2D) (None, 64) 0 quant_dropout (QuantizeWrapperV2) (None, 64) 1 quant_fc1 (QuantizeWrapperV2) (None, 128) 8325 quant_output (QuantizeWrapperV2) (None, 1) 134 Total params: 27864 (108.84 KB) Trainable params: 27457 (107.25 KB) Non-trainable params: 407 (1.59 KB) the conversion process restructured certain layers to take advantage of the dedicated hardware acceleration capabilities of the NPU. Both models are converted to static arrays and stored in flash memory of the MCU. Figure 6.\n\n--- Segment 8 ---\nBoth models are converted to static arrays and stored in flash memory of the MCU. Figure 6. Model architecture after NPU conversion [13]. III. RESULTS The MFCC feature extraction demonstrated efficient processing with a average computation time of 431 µs Table III QUANTIZED MODEL ARCHITECTURE Layer (type) Output Shape Param InputLayer [-1, 98, 20, 1] 0 Conv2D [-1, 96, 18, 32] 320 weights 32 biases MUL, ADD (BatchNorm) [-1, 96, 18, 32] 64 scale 32 offset MaxPool2D [-1, 48, 9, 32] 0 Conv2D [-1, 46, 7, 64] 18,432 weights 64 biases MUL, ADD (BatchNorm) [-1, 46, 7, 64] 64 scale 64 offset MaxPool2D [-1, 23, 3, 64] 0 Mean [-1, 64] 0 FullyConnected [-1, 128] 8,192 weights 128 biases FullyConnected [-1, 1] 128 weights 1 bias Logistic [-1, 1] 0 Total params: 27521 (28.20 KB) Trainable params: 27297 (27.97 KB) Non-trainable params: 224 (224 B) per frame on Cortex-M33. Evaluation of three variants of the model revealed significant performance differences. The regular floating-point model achieved 99.14 accuracy, with the confusion matrix shown in Table IV. On the Intel Core i5-10210U CPU, this model required 58.67 ms per inference with a memory footprint of 383,674 bytes. Quantization reduced the size of the model to 35,744 bytes (reduction of 90.68 ) while achieving accuracy of 97.06 , with the confusion matrix shown in Table V. The quantized version showed faster inference at 0.42 ms on the same Intel CPU.\n\n--- Segment 9 ---\nOn the Intel Core i5-10210U CPU, this model required 58.67 ms per inference with a memory footprint of 383,674 bytes. Quantization reduced the size of the model to 35,744 bytes (reduction of 90.68 ) while achieving accuracy of 97.06 , with the confusion matrix shown in Table V. The quantized version showed faster inference at 0.42 ms on the same Intel CPU. When deployed on the MCXN947 s Cortex-M33, the inference time increased to 228.2 ms. Table IV REGULAR MODEL CONFUSION MATRIX Predicted Non-Marvin Predicted Marvin Real Non-Marvin 10746 64 Real Marvin 31 164 Table V QUANTIZED MODEL CONFUSION MATRIX Predicted Non-Marvin Predicted Marvin Real Non-Marvin 10501 309 Real Marvin 15 180 3 The NPU-optimized version preserved the quantized model s 97.06 accuracy while further reducing both model size (30,576 bytes) and inference time (3,847 µs on MCXN947). This represents a 98.3 reduction in model size and 59 speedup compared to the implementation using only the ARM Cortex-M33 core. The complete performance metrics are summarized in Table VI. Table VI MODEL STATISTICS Size [B] Acc. [ ] i5 [ms] MCXN947 [µs] Regular 383674 99.14 50.67 - Quantized 35744 97.06 0.42 228210 NPU 30576 97.06 - 3847 IV. CONCLUSION This paper demonstrates the successful implemen- tation of keyword spotting on the NXP MCXN947 microcontroller using quantization-aware training and NPU acceleration. The approach achieves a 90.68 reduction in model size with 97.06 accuracy, while NPU acceleration provides a 59 speedup compared to CPU execution. The complete processing pipeline requires less than 5 ms, enabling deployment on embedded platforms. The results validate the feasibility of implementing a speech recognition system in resource-constrained devices through the combination of model compression and specialized hardware acceleration. This work con- tributes quantifiable performance metrics for edge AI deployment in privacy-preserving and energy-constrained applications. Future work should investigate power consumption characteristics, multi-keyword architectures, and robust- ness evaluation under varying acoustic conditions.\n\n--- Segment 10 ---\nThis work con- tributes quantifiable performance metrics for edge AI deployment in privacy-preserving and energy-constrained applications. Future work should investigate power consumption characteristics, multi-keyword architectures, and robust- ness evaluation under varying acoustic conditions. REFERENCES [1] Shalbbya Ali, Safdar Tanweer, Syed Sibtain Khalid, and Naseem Rao. Mel frequency cepstral coefficient: a review. ICIDSSD, 2020. [2] Justice Amoh and Kofi M Odame. An optimized recurrent unit for ultra-low-power keyword spotting. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 3(2):1 17, 2019. [3] Gianmarco Cerutti, Lukas Cavigelli, Renzo Andri, Michele Magno, Elisabetta Farella, and Luca Benini. Sub-mw keyword spotting on an mcu: Analog binary feature extraction and binary neural networks. IEEE Transactions on Circuits and Systems I: Regular Papers, 69(5):2002 2012, 2022. [4] Guoguo Chen, Carolina Parada, and Tara N Sainath. Query- by-example keyword spotting using long short-term memory networks. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5236 5240. IEEE, 2015. [5] Cristian Cioflan, Lukas Cavigelli, Manuele Rusci, Miguel De Prado, and Luca Benini. On-device domain learning for keyword spotting on low-power extreme edge embedded systems. In 2024 IEEE 6th International Conference on AI Circuits and Systems (AICAS), pages 6 10. IEEE, 2024. [6] Juan Sebastian Piedrahita Giraldo and Marian Verhelst. Laika: A 5uw programmable lstm accelerator for always-on keyword spotting in 65nm cmos. In ESSCIRC 2018-IEEE 44th European Solid State Circuits Conference (ESSCIRC), pages 166 169. IEEE, 2018. [7] Alexander Gruenstein, Raziel Alvarez, Chris Thornton, and Mohammadali Ghodrat. A cascade architecture for keyword spotting on mobile devices. arXiv preprint arXiv:1712.03603, 2017.\n\n--- Segment 11 ---\nA cascade architecture for keyword spotting on mobile devices. arXiv preprint arXiv:1712.03603, 2017. [8] Hynek Hermansky, Nelson Morgan, Aruna Bayya, and Phil Kohn. Rasta-plp speech analysis. In Proc. IEEE Int l Conf. Acoustics, speech and signal processing, volume 1, pages 121 124, 1991. [9] Yiteng Huang, Thad Hughes, Turaj Z. Shabestary, and Taylor Applebaum. Supervised noise reduction for multichannel keyword spotting. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5474 5478, 2018. [10] Xuan Ji, Meng Yu, Jie Chen, Jimeng Zheng, Dan Su, and Dong Yu. Integration of multi-look beamformers for multi-channel keyword spotting. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7464 7468, 2020. [11] Arnav Kundu, Mohammad Samragh, Minsik Cho, Priyanka Padmanabhan, and Devang Naik. Heimdal: Highly efficient method for detection and localization of wake-words. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1 5. IEEE, 2023. [12] David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph Dureau. Federated learning for keyword spotting. In ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 6341 6345. IEEE, 2019. [13] Roeder Lutz. Netron. 2025. Model visualiza- tion, Accessed: 2025-04-16. [14] Mehmet Gorkem Ulkar and Osman Erman Okman. Ultra- low power keyword spotting at the edge. arXiv preprint arXiv:2111.04988, 2021. [15] Paola Vitolo, Rosalba Liguori, Luigi Di Benedetto, Alfredo Rubino, and Gian Domenico Licciardo. Automatic audio feature extraction for keyword spotting.\n\n--- Segment 12 ---\n[15] Paola Vitolo, Rosalba Liguori, Luigi Di Benedetto, Alfredo Rubino, and Gian Domenico Licciardo. Automatic audio feature extraction for keyword spotting. IEEE Signal Processing Letters, 31:161 165, 2023. [16] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition, 2018. [17] Minhao Yang, Chung-Heng Yeh, Yiyin Zhou, Joao P. Cerqueira, Aurel A. Lazar, and Mingoo Seok. A 1uw voice activity detector using analog feature extraction and digital deep neural network. In 2018 IEEE International Solid-State Circuits Conference - (ISSCC), pages 346 348, 2018. [18] Yu-min Zeng, Zhen-yang Wu, Tiago Falk, and Wai-yip Chan. Robust gmm based gender classification using pitch and rasta- plp parameters of speech. In 2006 International Conference on Machine Learning and Cybernetics, pages 3376 3379, 2006. [19] Yundong Zhang, Naveen Suda, Liangzhen Lai, and Vikas Chandra. Hello edge: Keyword spotting on microcontrollers. arXiv preprint arXiv:1711.07128, 2017. 4\n\n