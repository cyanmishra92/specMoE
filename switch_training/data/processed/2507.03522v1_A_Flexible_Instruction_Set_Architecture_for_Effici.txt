=== ORIGINAL PDF: 2507.03522v1_A_Flexible_Instruction_Set_Architecture_for_Effici.pdf ===\n\nRaw text length: 78591 characters\nCleaned text length: 77264 characters\nNumber of segments: 49\n\n=== CLEANED TEXT ===\n\narXiv:2507.03522v1 [cs.AR] 4 Jul 2025 A Flexible Instruction Set Architecture for Efficient GEMMs Alexandre de Limas Santana Barcelona Supercomputing Center Universitat Polit ecnica de Catalunya Email: Adri a Armejach Sanosa Barcelona Supercomputing Center Universitat Polit ecnica de Catalunya Email: Francesc Martinez Barcelona Supercomputing Center Universitat Polit ecnica de Catalunya Email: Erich Focht OpenChip Email: Marc Casas Barcelona Supercomputing Center Universitat Polit ecnica de Catalunya Email: Abstract GEneral Matrix Multiplications (GEMMs) are re- current in high-performance computing and deep learning work- loads. Typically, high-end CPUs accelerate GEMM workloads with Single-Instruction Multiple Data (SIMD) or vector Instruc- tion Set Architectures (ISAs). Since these ISAs face significant issues when running GEMM workloads, particularly when deal- ing with small, tall, or skinny matrices, matrix ISAs have been proposed and implemented by major hardware vendors in the last years. Although these matrix ISAs deliver larger throughput when running GEMMs than their SIMD vector counterparts, they are rigid solutions unable to dynamically adapt themselves to application-specific aspects like the data format. This paper demonstrates that the state-of-the-art matrix ISAs deliver sub- optimal performance when running the most commonly used convolution and transformer models. This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA that completely decouples the instruction set architecture from the microarchitecture and seamlessly interacts with existing vector ISAs. MTE incurs minimal implementation overhead since it only requires a few additional instructions and a 64-bit Control Status Register (CSR) to keep its state. Specifically, MTE can i) vectorize GEMMs across the three dimensions M, N, and K; ii) leverage the capacity of the existing vector register file; and iii) decouple the tile shape from the underlying microarchitecture. MTE achieves speed-ups of 1.35 over the best state-of-the-art matrix ISA. I. INTRODUCTION GEneral Matrix Multiplications (GEMMs) are ubiquitous in high-performance computing and deep learning workloads [1]. Typically, high-end CPUs accelerate GEMM workloads with Single-Instruction Multiple Data (SIMD) or vector Instruction Set Architectures (ISAs) [2], [3], [4] to leverage their high- throughput floating-point functional units. In a push for even higher throughput, major hardware vendors are now incor- porating matrix ISAs into CPU architectures with the first implementations released in the last years [5], [6], [7], [8]. These approaches achieve better compute throughput than SIMD vector ISAs by exploiting specialized Matrix-Multiply- Accumulate (MMA) units. Ultimately, these matrix extensions complement the existing SIMD vector ISAs instead of replac- ing them [9], [5], [10], [7]. The advantages of matrix ISAs do not come for free since the architecture must efficiently handle matrix operands in hardware and expose a concise software hardware interface. Solutions to expose matrix operands to hardware include vector register grouping [9], tile registers [5], or registers with a capacity of n2 elements, with n being the vector SIMD register width [10]. These approaches are tightly coupled with microarchitectures implementing short-vector ISAs [11], [12], [6]. Although they are effective, these solutions consist in rigid matrix ISAs supporting static matrix tile shapes unable to dynamically adapt themselves to application-specific aspects like the data format. This paper demonstrates the state-of-the- art matrix ISAs deliver suboptimal performance when running the commonly used convolution and transformer models. This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA that completely decouples the instruction set architecture from the microarchitecture. Additionally, the MTE ISA seamlessly interacts with the existing vector ISAs, which makes MTE able to leverage vector architecture registers for matrix computations and vector instructions for element-wise operations. MTE incurs minimal implementation overhead since it only requires six additional operations and a 64-bit Control Status Register (CSR) to keep its state. Specifically, MTE can i) vectorize GEMMs across the three GEMM loops over M, N, and K; ii) fully exploit the capacity of the vector register file for storing matrices in uniform and mixed- precision scenarios; and iii) decouple the matrix tile geometry from the underlying microarchitecture. By leveraging these three properties, MTE achieves significant performance gains with respect to state-of-the-art vector [11], [13], [14] and matrix [5], [7] ISAs for a heterogeneous set of GEMM workloads belonging to relevant convolution and transformer models. Specifically, this paper makes the following contributions: It identifies the main performance bottlenecks of state- of-the-art matrix ISAs. These bottlenecks come from poor utilization of the matrix storage space, and also the restricted number of registers these ISAs expose to the compiler. It proposes MTE, the first geometry-agnostic ISA decoupled TABLE I THE BLAS GEMM ROUTINE ARGUMENTS Parameter Description A, B, C Pointer to the operand matrices M, N, K Matrix multiplication size LD{A, B, C} Matrices leading dimension offset TRANS{A, B, C} Matrices memory layout α,β Scaling factors from the microarchitecture, enabling code portability across implementations without programmer intervention. The paper also describes two microarchitectures supporting MTE, the first one is based on a lean extension of a long vector processor and the second one leverages a systolic array. It evaluates the performance of MTE considering 75 convo- lution workloads belonging to deep neural networks [15], [16], [17], [18], [19] and 18 transformer workloads obtained from natural language processing models [20], [21] and recommen- dation systems [22], [23], [24]. Our evaluation compares MTE against two approaches based on a vector ISA featuring 8192- and 16,384-bit vector registers, and two recently proposed matrix ISAs [5], [7]. Our evaluation indicates that MTE delivers better performance than these four approaches on top of a high-performance computing architecture. In particular, MTE beats the best state-of-the-art approach, AMX [5], by 1.35 on average due to software leveraging a larger number of architecturally-visible registers for matrix operands. II. BACKGROUND AND MOTIVATION This section describes the standard software interface of GEMM operations (Section II-A) and presents modern vector architectures (Section II-B). Also, this section describes state- of-the-art CPU matrix ISAs (Section II-C), and indicates their shortcomings (Section II-D). A. The Basic Linear Algebra Subprograms (BLAS) Interface The BLAS [25] specification is an ubiquitous standard soft- ware interface describing, among others, the GEneral Matrix Multiplication (GEMM). The GEMM operation is defined as C αAB βC where A, B, and C are matrices with shapes (M,K), (K,N), and (M,N), respectively, while α and β are scalar scaling factors. Table I describes the BLAS GEMM call interface. The M, N, and K parameters define the operand s shape and the geometry of the GEMM computation, which is characterized by the M N K triplet. M describes the number of rows in A and C, N specifies the number of columns of B and C, and K indicates the number of columns of A and rows of B. The leading dimension offset states the distance between elements in consecutive rows or columns, and decouples the matrix memory organization from the operation shape. The TRANS{A, B, C} flags express the memory layout of operands as row-major or col-major. B. SIMD Vector Support in Modern Processors Modern processors expose some form of vector or SIMD programming model as part of an Instruction Set Architecture (ISA) extension to accelerate linear algebra and AI work- loads [11], [14], [13]. To implement such models, processors employ parallel compute units operating on vector registers, delivering several floating-point operations per cycle on data- parallel workloads [3], [2], [26]. 1) Vector ISAs: Emerging vector ISAs such as the RISC- V Vector Extension (RISC-V V) [13] and the ARM Scalar Vector Extension (SVE) [14] support Vector Length Agnostic (VLA) programming models [14], allowing applications to run in vector processors implementing any vector register size without code changes. The flexibility of VLA programming models has renewed interest in long vector architectures, as evidenced by production systems [27] and research on archi- tectures employing vector registers as wide as 16384 bits [28], [29]. Despite their good performance on HPC workloads [30], [31], [27], long vector architectures are not able to fully use their floating-point computing capacity on GEMMs coming from computer vision AI tasks [32], [4]. 2) Microarchitecture: The defining characteristic of vector arithmetic instructions is that they implement operations that combine the i-th element stored in a vector register with the corresponding i-th element of another register. This charac- teristic drives the design of vector units, which are often implemented with an array of deeply pipelined functional units, or lanes, operating on subvectors [33]. Figure 1 rep- resents the i-th lane of a vector architecture with N lanes that completes a vector instruction over V L elements in V L N steps. The vector register file is interleaved across lanes and a lane interconnect enables inter-lane communication to support vector reductions, slides, and similar operations [27]. The vector operand buffers are present on each lane to receive vector element operands from the local Vector Register File (VRF) slice in preparation toward the functional units. Vector architectures support configurable vector length which disables computation on vector tail elements by scheduling fewer elements into the operand buffers. In addition, vector architec- tures support predication, which discards the functional unit outputs for specific elements and thus preserve their values. C. Matrix ISAs for CPU Architectures General-purpose CPU architectures are gradually incor- porating matrix multiplication ISA extensions to accelerate Artificial Intelligence (AI) and High-Performance Computing (HPC) workloads [5], [7], [9], [10]. These systems expose matrix multiplication instructions implemented by a Matrix Multiply-Accumulate (MMA) unit, which leverages the data reuse potential of MMA operations and outperform approaches based purely on SIMD Vector units. 1) The Intel Advanced Matrix Extensions (AMX): AMX consists of a dedicated set of eight tile registers with 1KB of storage each, and a tile matrix multiply unit (TMUL) to operate on them [34]. To use the TMUL, software must first configure the shape of each individual tile register, in terms 2 Lane i 1 VRF slice Lane i Write-back buffer VL-N i jN i N i i VL-N i jN i N i i VL-N i jN i N i i FPU ALU Lane interconnect Lane N-1 vd vs1 vs2 Last step VL N Pipeline step 0 Pipeline step 1 Pipeline step j Operand Buffers Functional Units Fig. 1. Structure of the i-th lane of a vector unit with N lanes. The vector register file storage is interleaved across the N lanes. Functional units contain one execution pipeline per lane operating on local elements across V L N steps. The lane interconnect enables communication between lanes. of the number of active rows and the row size in bytes, by populating a 64-byte CSR using a dedicated instruction. The AMX TMUL unit is a grid of fused multiply-add units able to read and write to tile registers. AMX TMUL instructions sup- port only mixed-precision integer (int8 to int32) and floating- point (bf16 to fp32) operations with a maximum M N K geometry of 16 16 64 and 16 16 32, respectively. To enforce maximum utilization of the tile storage, TMUL instructions require the A and C matrices to be laid out in row-major layout while the B matrix must undergo a relayout, packing adjacent rows into 32-bit words (two bf16 or four int8 elements). Element-wise operations like α and β scaling required by the BLAS GEMM we describe in Section II-A are not supported by AMX and must be implemented using either scalar instructions or SIMD approaches like AVX512 [11]. However, since there is no interface between AVX512 and AMX register files, communication is done through memory before post-processing. 2) SiFive Intelligence Custom Extensions (SiFiveInt): The most recent custom matrix extension for RISC-V V is SiFiveInt [7], which introduces an MMA instruction involving several 4 4 matrix tiles. This instruction uses three vector register operands, two bf16 input vectors (vs1 and vs2), and a single fp32 input output vector (vd), and conceives vector registers as a sequence of independent 4 4 matrix tiles. The MMA instruction multiplies the first tile in vs1, the A operand, to all tiles in vs2, the B operand, and accumulates the results into the vd tiles, the C operand. Despite being a vector-length agnostic extension, the constant 4 4 operand shape for vs1 incurs poor hardware utilization on long vector implementations, as the MMA uses just the first 128 bits of the vs1 register (i.e., one 4 4 Bfloat16 tile). Finally, the SiFiveInt extension does not define matrix memory move operations and relies on tiled memory layouts for efficient data movement with unit-stride vector load stores, otherwise requiring expensive vector gather scatter. D. Shortcomings of Matrix ISAs To illustrate the shortcomings of state-of-the-art matrix ISAs, we evaluate the single-core efficiency of the x86 Advanced Matrix Extension (AMX) [5] and the Advanced Vector Extension (AVX512) [11] on an Intel Xeon Platinum 8480 [35] processor locked to a frequency of 2.1 GHz. Our evaluation considers 75 unique convolution layers from computer vision networks [15], [17], [19], [18], [16] and 31 GEMM workloads commonly featured on transformer networks [20], GPT [21] and BERT-based models for recom- mendation systems [22], [23], [24]. Figure 2 shows our results. The y-axis represents the mea- sured performance in terms of Giga Floating-Point operations per second (GFLOP s) in logarithmic scale. The x-axis on the left subplot displays all the convolution workloads we consider sorted by the ascending number of output feature maps. The x-axis on the right subplot showcases the GEMM workloads extracted from transformers and recommendation systems sorted by the ascending number of output matrix rows. We measure average efficiencies of 35.4 for AMX and 85.6 for AVX512 in terms of percentage of peak performance. While the arithmetic throughput of AMX is 16 greater than AVX512 (2150 bf16 GOps s versus 134 fp32 GOps s), the limited single-core efficiency constrains AMX speedup over AVX512 to a range of 5.7 10 [36]. The poor efficiency of AMX in terms of floating-point performance with respect to its peak performance is explained by two main factors: i) the fact that AMX requires dedicated 1KB registers to store matrix operands reduces the number of architectural registers to only 8, otherwise the matrix register file would be prohibitively large, which limits the potential performance enhancements of compiler-based approaches like loop unrolling; and ii) the rigid M N K geometry of AMX, which is restricted to just 16 16 64 and 16 16 32 matrix tile computations and does not match tall and skinny matrix tiles, which usually appear in AI workloads like the ones we represent in Figure 2. In addition, the shapes of GEMM kernels coming from transformer models [20], [21] depend on input parameters, which makes it impractical to store input matrices on pre-defined high-performance layouts as it happens on modern direct convolution kernels. Therefore, we observe comparatively lower AMX performance for GEMMs coming from transformer models as data must be transposed and or re-ordered via SIMD instructions, stored to memory, and reloaded to AMX tile registers. This paper proposes a novel matrix ISA extension targeting GEMMs to fix the limitations of current approaches, since i) it uses vector registers to store matrix tiles and therefore does not require dedicated matrix registers to store matrix tiles; ii) it is able to dynamically adapt its tile geometry to workload requirements. III. THE MATRIX TILE EXTENSION This section describes the Matrix Tile Extension (MTE) ISA, which extends the concept of vector length agnosti- cism [13], [14] to support a flexible geometry-agnostic matrix 3 32 64 128 256 512 1024 of output feature maps 64 128 256 512 1024 2048 Performance (GFLOP s) AVX512 AMX AVX512 peak AMX peak 16 32 64 of output matrix rows Fig. 2. Single core Intel Xeon Platinum 8480 performance of vector (AVX512) and matrix (AMX) ISAs during computer vision convolutions and language models GEMMs. instruction set architecture. MTE reuses the vector register file for tile storage, avoiding the need for dedicated registers and matrix-to-vector register moves. A. Determining Tile Dimensions This section describes how MTE determines the sizes of A, B, and C matrix tiles to support a geometry-agnostic matrix instruction set that maximizes the use of vector registers storage capacity. Section III-A1 describes the scenario where the A, B, and C matrix operands involved in the GEMM com- putation are composed of scalar coefficients of the same data type. We call this scenario Uniform Precision. Section III-A2 describes the scenario where matrix C is composed of scalar coefficients represented with a larger data type than matrices A and B. We call this scenario Mixed Precision, similarly as previous work [37]. 1) Uniform Precision: In the uniform precision scenario, all matrices contain scalar coefficients represented with the same data type. In this context, the RISC-V V ISA [13] uses vector registers to store a set of V LEN SEW elements, where V LEN is the size of the vector registers in bits and SEW indicates the single element bit-width. The SVE ISA [14] applies a similar approach and expresses the state contained by the V LEN and SEW parameters in terms of predicates. For simplicity, and without loss of generality, we express the state required by MTE using the RISC-V V nomenclature, although it could be written in terms of any vector-length agnostic ISA. MTE considers 32 vector registers, like RISC-V V and SVE. MTE organizes matrix elements by segmenting the vector registers into V LEN RLEN matrix rows storing RLEN SEW elements each, as illustrated in Figure 3. The RLEN parameter is a MTE design-time constant informing the tile row size in bits. Formula 1 expresses the largest hardware matrix geometry considering the V LEN, SEW, and RLEN parameters. The product ROWS COLS is equal to the storage of a vector register in terms of element count, V LEN SEW. ROWS V LEN RLEN , COLS RLEN SEW (1) In the uniform precision scenario, MTE stores matrix tiles in vector registers in row-major order. The three equations of Formula 2 determine the maximum M, N, and K dimen- Nvregs VLEN SEW RLEN SEW VLEN RLEN Nvregs Fig. 3. Rank-1 (left) and rank-2 (right) interpretations of the vector register file. The vector view interprets registers as vectors of VLEN SEW elements. The rank-2 view interprets registers as VLEN RLEN RLEN SEW matrices. sion sizes supported by a processor with V LEN-bits vector registers, and RLEN-bits rows for a given value of SEW. The K dimension formula constrains the matrix size to the storage space of a single vector register. Formula 2 guarantees the full use of vector registers storing C tiles regardless of the V LEN, RLEN, and SEW. Fully utilizing the vector registers storing A and B tiles requires M N since the K dimension represents the number of rows in B and columns in A. M V LEN RLEN , N RLEN SEW , K min(M, N) (2) 2) Mixed Precision: In the mixed precision scenario, matrix C is composed of scalar coefficients represented with a larger data type than A and B. This difference implies that two different SEW sizes must be considered in the mixed precision scenario: the SEW of matrices A and B, which we call SEWi, and the one of C, which we call SEWo. Having distinct SEW values brings an additional restriction when determining N, i.e., N min( RLEN SEWi , RLEN SEWo ). Since SEWi SEWo, the data size of C determines the value of N, and therefore the B tile size is smaller than the vector register capacity. To overcome this issue, MTE uses a transposed layout to store B tiles in vector registers in the mixed precision scenario, i.e., MTE organizes and operates on B tiles in a col-major ordering within vector registers. Using a transposed layout exclusively for B operands alters the semantics of the K dimension which now determines the number of columns of A and B matrices, i.e. K RLEN SEWi . In addition, N now determines both the number of rows of B and columns of C, therefore N min( V LEN RLEN , RLEN SEWo ). Taking into account that M V LEN RLEN , we obtain the three equations of Formula 3. They determine the maximum M, N, and K dimension sizes involving a transposed B operand and considering the different input and output data sizes, SEWi and SEWo. M V LEN RLEN , N min(M, RLEN SEWo ) , K RLEN SEWi (3) Therefore, transposing B makes it possible to determine N by considering SEWo instead of both SEWo and SEWi. Because there is no equation in Formula 3 involving both SEWi and SEWo, the inequality SEWi SEWo does not restrict the full use of the vector register length. For 4 TABLE II 64-BIT CSR FOR THE MATRIX TILE EXTENSION Name Description Bits t[m,n,k] Tile dimension shapes 36 ttype[i,o] Input output matrix tile types 8 rlenb RLEN in Bytes 12 Reserved additional data 8 TABLE III MATRIX TILE EXTENSION INSTRUCTIONS Instruction Arguments Description tss[m,n,k] rd, rs1, ttypeio tile set shape [M,N,K] t{t}l[a,b,c,bt] vd, rs1, rs2 [A,B,C,BT ] tile {transposed} load t{t}sc vd, rs1, rs2 C tile {transposed} store t{f}{w}mul vd, vs1, vs2 tile {FP} {widening} dot product tvmask[a,b,c,bt] vd, rs1 set vector mask instance, an architecture targeting 32-bit matrix operations with V LEN 8192 and RLEN 512 describes a maximum matrix multiplication geometry of 16x16x16 in a uniform precision scenario with 32-bit data types. This scenario fully utilizes the vector register capacity (256 elements) on all operands, according to Formula 2. The same architecture executing mixed-precision operations with SEWo 32 and SEWi 16 describes a maximum geometry of 16x16x32, also using full vector registers capacity (256 output elements, 512 input elements), according to Formula 3. B. Control Status Registers MTE stores all the state defining the tile s geometry in a 64-bit Control Status Register (CSR) described in Table II. The tm, tn, and tk fields store the current hardware geometry settings for the M, N, and K dimensions with a maximum dimension size of 212 4096 elements. The ttypei and ttypeo are two 4-bit fields specifying traits of matrix input and output operands. The ttype fields use 2 bits to specify the SEW of each operand tile, which can be either 8, 16, 32, or 64 bits. The remaining 2 bits on the ttype fields codify how to handle elements on inactive columns and rows. One option is to leave the inactive bits untouched (undisturbed policy in the RISC-V nomenclature [13]). Alternatively, the software is responsible for not accessing such elements as they can be dirty (agnostic policy in RISC-V). The rlenb CSR holds the RLEN size in bytes. The remaining bits are reserved for MTE extensions. C. MTE Instructions This section describes the 19 MTE instructions organized in five instruction groups, as seen in Table III. 1) Configuring the tile geometry: The tssm, tssn, and tssk instructions configure the M, N, and K GEMM matrix dimensions by updating the respective tm, tn, or tk CSR fields. The instructions take the requested dimension size from RLEN VLEN Mask Tail Fig. 4. Vector processing of matrices in vector registers. White and dark strips represent active and inactive elements respectively. The vector length deactivates inactive matrix rows at the vector tail, and the vector mask deactivates inactive columns at each row spanning RLEN bits. the application in rs1 and return the updated CSR value, the granted dimension size, in rd. The return value is the minimum between the application request and the maximum dimension size allowed by the underlying microarchitecture, discussed in Sections III-A1 and III-A2, and the data-width of input and output matrices, SEWi, and SEWo. The ttypeio is a 3-bit immediate encoding of the input and output matrix element widths, SEWi and SEWo, for configuring the ttype CSR fields. 2) Matrix data movement: The tile load store, tl ts, and their transposed variants ttl tts, move GEMM matrix operands between memory and the vector register file. In the gen- eral case, these instructions access blocks of up to RLEN consecutive bits found at constant strides in memory. The load instructions are encoded with the GEMM operand type they produce, A, B, C, or BT to specify the relevant CSR dimension shape fields to the microarchitecture for instruction decoding. For instance, C tile operations require the CSR tm, and tn fields to compute the C tile shape. The special t{t}lbt instruction indicates that the B operand is organized in a col- major layout within the vector register state. Finally, MTE defines store instructions exclusively for the C matrix operand. MTE memory instructions require two scalar parameters: The base memory address from rs1, and the leading dimension offset from rs2, expressed in bytes. The MTE operands on memory instructions are directly associated to the BLAS software interface parameters pairs (A, lda), (B, ldb), and (C, ldc). The special case with stride zero corresponds to row or column broadcast operations where a single row column value in memory is replicated across all vector register rows - columns. MTE implementations may optimize row column broadcasts when rs2 describes the 0-stride scenario. 3) Matrix Multiplication: The tfmul and tmul matrix tile multiply instructions compute the product of the A and B matrix tiles and accumulate the result to a C tile matrix, i.e., they are MMA instructions. The former instruction operates on floating-point data types and the latter on integers. These instructions require three vector register operands, the C tile in vd, the A tile in vs1, and the B tile in vs2. The tm, tn, and tk MTE CSR fields determine the operation shape. The tfwmul and twmul are the mixed-precision variants of tfmul and tmul, respectively, and interpret the B operand in terms of a col-major layout, as Section III-A2 indicates. 4) Vector processing mode: MTE employs vector instruc- tions to implement element-wise operations on matrix data stored in vector registers. This interaction removes the need for 5 Algorithm 1 MTE SGEMM C αAB βC Input: A, B, C, M, N, K, LDA, LDB, LDC, α, β Output: C 1: sm, sn, sk 0 2: erow read csr(rlenb) sizeof(float) 3: for m 0, M, m sm do 4: sm tssm(M m) 5: vl sm erow 6: for n 0, N, n sn do 7: sn tssn(N n) 8: gvl vsetvl(vl, e32) 9: vm tvmaskc(gvl) 10: c vbroadcast(0.0f, gvl) 11: for k 0, K, k sk do 12: sk tssk(K k) 13: a tla( A[m LDA k], LDA, tm, tk) 14: b tlb( B[k LDB n], LDB, tk, tn) 15: c tfmul(c, a, b, tm, tn) 16: t tlc( C[m LDC n], LDC, tm, tn) 17: c vfmul vf mask(c, α, vm, gvl) 18: c vfmacc vf mask(t, β, vm, gvl) 19: tsc(c, C[m LDC n], LDC, tm, tn) data movement between memory or register files. For instance, the vector ISA can implement matrix additions and scalar- matrix multiplications, like the ones of BLAS GEMM, with vector additions (vfadd.vv) and vector-scalar multiplications (vfadd.vf), respectively. Figure 4 illustrates how to disable computations on specific matrix elements stored inside the vector register state using the programmable vector length and vector mask features of vector architectures. Setting up the vector instructions to operate on matrix tiles requires: i) setting the vector length to guarantee that all active rows are covered; and ii) creating a vector mask to disable computations on inactive columns at each row. In the context of RISC-V V, the vsetvl instruction [13] performs the first step, gathering the application vector length and type from source scalar registers. The application can read the architecture row size in the rlenb CSR field to compute the vector length for vsetvl in a geometry-agnostic way. Finally, for the vector mask, MTE introduces the tvmask instructions to create a vector mask for either A, B, C, or BT tiles based on the active tile geometry settings. D. BLAS GEMM Algorithm using MTE Algorithm 1 shows a BLAS SGEMM C αAB βC routine implemented with MTE assuming the RISC-V V ISA extension is supported. Line 2 computes the number of elements per matrix row in hardware. Lines 4, 7, and 12 configure the GEMM geometry at each iteration. The granted dimension sizes are stored in the sm, sn, and sk variables for incrementing the M, N, and K loop iterators in Lines 3, 6, and 11. The Lines 4-5, and 8-9 configure the hardware to operate on the C matrix. Line 10 initializes the C output matrix accumulator with zeros using the RISC-V V scalar-vector broadcast instruction. Lines 13-15 depict the computational kernel of the algorithm, consisting of two tile load instructions, from A and B, and a tile multiply operation. Lines 16-18 use standard masked vector arithmetic operations to implement the α and β scalar-matrix multiplications on the C matrix loaded from memory. Typically, this algorithm is optimized by unrolling the M and or N loops to reuse the B and or A matrix tiles loaded into registers in operations across multiple independent C output tiles within the K loop. This optimization serves the purpose of hiding the latencies of the tile load operations by increasing the compute density of the innermost loop and exposing more independent work to the hardware. The K loop may also be unrolled to avoid frequent CSR writes. E. Summary of the MTE ISA MTE proposes to reuse the vector register file to store matrix tile operands. This strategy makes it possible to eliminate the need for extra architectural state to support MMA instructions. Similarly to AMX, MTE also uses a CSR for controlling the tile shapes but simplifies the configuration process by enabling only three simultaneous matrix shapes for A, B, and C, which are programmed by software using a few scalar instructions. MTE also provides a set of instructions to configure the vector programming model based on the MTE state, allowing a seamless transition from matrix to vector processing mode without the need for memory or register moves. In addition, MTE defines a geometry-agnostic programming model independent of the matrix multiplication shapes supported by the microarchitecture, while defining a standard way for the software to expose the required geometry to the hardware. IV. ARCHITECTURE SUPPORT FOR MTE INSTRUCTIONS This section describes the architecture support required by MTE. Section IV-A describes two possible microarchitecture implementations of the MTE MMA instructions. Section IV-B describes the support required by the MTE memory instruc- tions, while Section IV-C discuses additional aspects to fully support MTE. A. Supporting the tfmul and tmul Instructions When computing tfmul and tmul MMA operations, de- scribed in Section III-C3, inputs from the A and B tiles contribute to multiple C output elements. In this section, we describe the architecture support required by MTE in the context of i) a dedicated systolic array; and ii) a standard vector processor. 1) Systolic array:: A common approach to implement MMA operations is through dedicated systolic arrays consist- ing of a grid of functional units, such as those found in Intel AMX or Google TPUs [5], [38], [39], [29]. These architectures directly read from tile registers, delivering high throughput per operation. However, a notable drawback is their specialization for MMA operations, which can result in unused resources 6 jN N 0 i i i jN i N i i C[0,i] A[0,0] B[0,i] C[1,i] A[1,0] B[0,i] C[j,i] A[j,0] B[0,i] cvfma 0, step 0 cvfma 0, step 1 cvfma 0, step j vs2 (B) vd (C) vs1 (A) jN 1 N 1 1 N i N i N i jN i N i i cvfma 1, step 0 cvfma 1, step 1 cvfma 1, step j C[0,i] A[0,1] B[1,i] C[1,i] A[1,1] B[1,i] C[j,i] A[j,1] B[1,i] jN (K-1) N (K-1) K-1 (K-1)N i (K-1)N i (K-1)N i jN i N i i cvfma K-1, step 0 cvfma K-1, step 1 cvfma K-1, step j C[0,i] A[0,K-1] B[K-1,i] C[1,i] A[1,K-1] B[K-1,i] C[j,i] A[j,K-1] B[K-1,i] Operations Decomposition Fig. 5. Operand buffers contents within one vector lane during a matrix multiplication decomposed into K cvfma operations. The left and right sections depict the correlation between cvfma steps and matrix semantics. The middle section shows the operand buffer contents. and limited datatype support. For example, AMX and TPU hardware only support up to 16-bit input operands. Extending support to 32-bit or 64-bit inputs would incur significant area overheads, as a multiplier area scales quadratically with mantissa bit width [40], [41]. 2) Vector processor:: To exploit data reuse at the reg- ister level in a vector architecture, we propose breaking down MMA instructions into a sequence of new vector micro-instructions called Component Vector Fused-Multiply- Accumulate (cvfma). The cvfma instructions behave like standard vector FMAs except for the lane control logic, which modifies the operand buffer filling to implement the data reuse patterns required by matrix multiplication. Without loss of generality, this section focuses on supporting tmul; the same method applies to tfmul. The tmul instruction is decoded into K cvfma operations, based on the MTE tk CSR field. Each cvfma operation processes a vector length of M RLEN bits, determined by the MTE tm CSR field, which disables computation on tail elements corresponding to inactive tile rows. To avoid computations on inactive columns across the N dimension, the architecture employs an implicit vector predication mask on cvfma instructions, derived from the MTE tn CSR field. This technique is compatible with software predication masks, combining them with a logical and operation to disable computations on arbitrary elements. Figure 5 illustrates the tmul decomposition and shows the operand buffer contents for the i-th vector lane across all K cvfma instructions. The vd operand buffer, containing the C tile, is filled similarly to regular vector architectures, as indicated in Figure 1. The vs2 operand buffer, mapped to the B tile, holds a single lane-local element for all cvfma steps. This pattern, called implicit broadcast, is already used in scalar- vector arithmetic instructions like the RISC-V V vfmacc.vf instruction [13]. No inter-lane communication is required for the vs2 operand. Finally, the A tile operand requires an access pattern unsupported by standard vector instructions. Specifically, each cvfma operation in the sequence requires accessing the operand buffer of a specific lane: the first cvfma accesses lane zero, the second lane one, and so forth. To support this access pattern, MTE can leverage the interconnect between the different vector lanes to move vs1 operands between lanes. Our approach supports the tfmul and tmul MMA instruc- tions using a state-of-the-art vector architecture as a basis. Each lane manages all computations within a matrix column, the pipeline steps make it possible to iterate over the matrix rows, the operand buffers support the vertical flow of B values, and the lane interconnect implements the flow of A values. Reusing the existing vector engine hardware for computation improves hardware utilization while supporting wide datatypes (32 and 64 bit) seamlessly, necessary in classic HPC applications [42], [43]. B. Memory Instructions The general use case of MTE matrix memory instructions can be broken down into a series of unit-stride vector load - stores of up to RLEN bits in a constant stride. If the stride value matches the architecture RLEN, all the matrix tile data is contiguous in memory and therefore the hardware can map the matrix memory operations to a simple unit-stride vector load store. A tl ttl instruction with zero stride, possibly signaled with the zero register (x0 in the case RISC-V V), triggers a row column broadcast, replicating a sequence of RLEN bits to all V LEN bits if the vector register. C. Other Support The tss instructions for configuring the hardware GEMM shape are similar to vsetvl instructions in RISC-V V that, in turn, configure the vector instruction traits using CSRs. Similar to some existing vector HPC architectures, we support register renaming on the MTE CSRs to enable speculation and allowing matrix instructions to be executed out-of-order. V. EVALUATION METHODOLOGY This section describes our evaluation methodology in terms of system architecture, workloads, and considered vector ma- trix ISAs. A. System Architecture Tables IV, V, and VI depict the system, vector process- ing unit, and the systolic array accelerator architectures we consider in our experimentation campaign. The system and systolic array parameters match an Intel Xeon Platinum 8480 processor [35] core, the latest x86 server-class processors implementing the AMX extension. The Vector Processing Unit (VPU) parameters resemble a single NEC SX-Aurora 20B vector accelerator core [44] augmented with one vector unit for a total of four. Each unit has 2048-bit vector lanes, i.e. delivers 64 32-bit FMA instructions per cycle, resulting in a peak performance of 512 Single-Precision (SP) FLOP cycle, i.e., 1024 GFLOP s at 2.0GHz. In this evaluation, we consider systolic and vector systems delivering the same peak throughput, that is, 512 FLOP cycle 7 TABLE IV SYSTEM CONFIGURATION Scalar core Out of order, 2 GHz, 512 ROB entries Issue Commit width 6 L1D 48KB, 8-way, 4cc, 10-entry MSHR, LRU L1I 32KB, 8-way, 4cc, 10-entry MSHR, LRU L2 2MB, 8-way, 26cc, 256-entry MSHR, LRU, 128-byte lines Main memory bandwidth 191.25 GB s per core Main memory latency 110ns TABLE V VECTOR PROCESSING UNIT Vector registers 32 (40 physical) Vector length 8192 bits for all approaches but Vector 2KB (16384 bits). Vector lane width 2048 bits Vector units 4 Throughput 512 FLOP (SP) per cycle and 1024 GFLOP s. This methodology makes it possible to compare ISAs with different hardware GEMM implementa- tions while highlighting the impacts of the ISA when express- ing GEMMs. B. Workloads This section highlights the workloads used in our evaluation as well as our code generation method. 1) Code generation: We extend oneDNN [45], an architecture-agnostic kernel library providing AI operators under a standard software interface, with convolution and GEMM algorithms for all the evaluated architectures de- scribed in Section V-C. Popular interfaces like Pytorch [46] and Tensorflow [47] employ oneDNN as a CPU backend for compute-intensive operations such as convolutions and GEMMs. oneDNN CPU primitives for x86 and Arm architec- tures use the Xbyak Just-in-Time (JIT) assembler [48], [49] to produce micro-kernels specialized for the operation parameters and target architecture on the fly. We develop rvjit, a RISC-V JIT assembler library similar to Xbyak to dynamically generate micro-kernels for RISC-V V and our proposed extensions. We implement direct convolution kernels for vector and ma- trix architectures following state-of-the-art recipes for SIMD architectures [4], [2], [50]. The direct algorithm employs a tiled matrix memory layout for both activation and weight tensors, and reduces the convolution to a series of matrix tile multiplications. The tiled memory layouts enable unit-stride vector memory operations. Since input and output activation tensors have a consistent organization, no data reshapes are required as the layout propagates across network layers. TABLE VI SYSTOLIC ARRAY ACCELERATOR Tile registers 8 (24 physical) on MTE8s, 32 (40 physical) on MTE32s Tile length 8192 bits Row length 512 bits Throughput 512 FLOP (SP) per cycle Our JIT-generated micro-kernels for vector and MTE ISAs use system balance equations [2], [4] to tune performance aspects like loop unrolling and tiling to the architecture VLEN, RLEN, and cache size parameters. The micro-kernels vectorize the N loop, unroll the M and N loop, and apply cache- blocking on the K loops of the GEMM formulation. When applied to convolutions, we map the minibatch, output feature map, and input feature map dimensions to the M, N, and K GEMM matrix dimensions. 2) Convolution Workloads: We evaluate convolution work- loads from the popular computer vision networks ResNet [15], Inception [16], VGG [17], yolo [18], and SqueezeNet [19]. We obtain the models from torchvision [51], an open-source machine vision package, execute them using Pytorch [46], and collect the convolution parameters by inspecting the library calls to oneDNN. We reproduce the Pytorch convolution calls for each layer using benchdnn [52]. Our evaluation covers 75 unique convolution operations including pointwise, spatial, strided, and padded convolutions with square and non-square kernel geometry, and utilizes 32-bit floating-point datatypes. Finally, we define a minibatch size of 16 for our experiments, creating sufficiently large problems to saturate all the evaluated architectures. 3) Transformer Workloads: We evaluate MTE considering 18 GEMM workloads commonly featured on transformer networks [20], GPT [21] and BERT-based models for rec- ommendation systems [22], [23], [24]. These GEMMs cover the linear projections necessary for multi-head attention, the scalar-dot-product function used by the attention block, and the feed-forward network [20]. We evaluate these operations on an inference scenario with small query sizes (16, and 32), two different encoding dimensions, dmodel, of 512 and 768 elements with 8 and 12 heads, respectively, and feed-forward operations with 2048 hidden connections. C. Evaluated architectures This section describes the ISAs we consider in our eval- uation. All ISAs run on top of the systems specified in Section V-A except AMX, which runs on an Intel Xeon Platinum 8480 [35] processor. Vector 1KB: uses the RISC-V V ISA to support a direct convolution approach [2], [4], [50] vectorizing the N loop, and unrolling the M loop of the GEMM kernel. Our JIT assembler generates RISC-V V code and uses all 32 architecture registers for unrolling. The maximum vector length is 8192 bits or 256 32-bit elements. Vector 2KB: it employs the same approach as Vector 1KB except for the maximum vector length, which is set to 16384 bits. In this scenario, vector instructions operate on up to 512 32-bit elements. SiFiveInt: it uses a matrix kernel with a custom MMA instruction similar to SiFiveIntelligence [7], adapted to fp32 operands, which multiplies independent 4 4 matrix tiles spanning 512 consecutive bits within a vector register. We consider a long vector architecture with V LEN 8192 and use MTE instructions to emulate SiFiveInt semantics by defining 8 TABLE VII EVALUATED ARCHITECTURES Architecture Constants Register file size tfmul (matrix) or vfma (vector) instruction support Acronym VLEN RLEN Architecture Physical Static latency Dynamic latency Geometry VPUs Systolic array Vector 1KB 8192 - 32 40 20 4 1 256 1 4 - Vector 2KB 16384 - 32 40 20 8 1 512 1 4 - SiFiveInt 8192 2048 32 40 28 16 16 16 4 4 No MTE8s 8192 512 8 24 36 16 16 16 16 2 Yes MTE32s 8192 512 32 40 36 16 16 16 16 2 Yes MTE32v 8192 512 32 40 36 64 16 16 16 4 No TABLE VIII PHYSICAL REGISTER FILE AREA OF EACH ARCHITECTURE Vector 1KB Vector 2KB SiFiveInt MTE8s MTE32s MTE32v mm2 1.66 4.15 1.66 1.65 1.66 1.66 RLEN as 2048, resulting in a capacity for 16 4 4 matrix tiles and a maximum GEMM geometry of 4 64 4. We organize the tensor data in 4 4 tiles to enable stride-1 memory requests which corresponds to the best use case for this ISA. MTE32v: uses the proposal described in Section III and implements the GEMM instructions as Section III-C4 indicates on top of the VPU described in Table V. We consider an MTE implementetion with a V LEN of 8192 and RLEN of 512, resulting in a GEMM geometry of 16 16 16, similar to AMX. The JIT generator uses all 32 RISC-V V registers for loop unrolling. MTE32s: It employs the same approach as MTE32v but implements GEMM instructions using the systolic array de- scribed in Table VI. Both MTE32s and MTE32v have the same peak performance since MTE32v can use all vector units to compute four simultaneous GEMMs whereas MTE32s computes one tile at a time. MTE8s: It reproduces the semantics of the x86 AMX ISA [5] using MTE. This strategy uses at most eight architec- tural registers for code generation. AMX: We consider the x86 Advanced Matrix Extension (AMX) ISA [5] implemented on the Intel Xeon Platinum 8480 [35] processor locked to a frequency of 2.1 GHz. Our evaluation leverages Intel s optimized kernels in oneDNN v3.5 [45], integrated as the CPU backend within PyTorch v2.2 [46]. Table VII describes the evaluated architectures architectural parameters, register file sizes, and support for matrix or vector instructions. We do not include AMX since its architecture parameters are very similar as the ones of MTE8s. The latency fields, expressed in cycles, corresponds to the static and dynamic instruction cost, described in Section V-E. All considered strategies display the same peak performance of 1024 GFLOPs s considering 32-bit datatypes. D. Area Cost This section describes the area cost of the architectures we discuss in Section V-C. Section VI-B describes the energy consumption of these architectures. We estimate the energy and area consumption for different hardware components using McPAT 1.3 [53] and PCACTI [54], [55], [56], respectively. We incorporate the enhancements proposed by Xi et al. [57]. These enhancements improve accuracy by modeling additional core structures and correcting erroneous modeling assumptions. Our analysis assumes a 5nm FinFET technology node to estimate the area cost [56]. Table VIII details the physical register file area for each architecture we describe in Section V-C. The Vector 2KB architecture has the largest register file area, as it includes 40 2KB registers, whereas the other approaches use 1KB reg- isters, as shown in Table VII. Architectures with 40 1KB reg- isters require less than half the area of Vector 2KB. Among all the considered approaches, MTE8s has the smallest register file area, as it contains only 24 1KB registers. Using the same methodology, we determine that the primary contributor to the area cost is the register file for all considered architectures. E. Simulation methodology and validation We evaluate the architectures from Section V-A using a trace-driven micro-architecture simulator that models physical register allocation, memory movement across cache levels, and dynamic instruction latency based on active vector length. Vector and matrix instructions are simulated with two cost components: i) a static, non-blocking, front-end latency paid after decode and before reserving compute resources which can be overlapped with the execution of other instructions, and ii) a dynamic latency tied to vector length and compute throughput that blocks the compute resource. We validate our simulation infrastructure by comparing its results for the MTE8s scenario, which replicates the x86 AMX ISA (Section V-C), against real AMX executions on an Intel Xeon Platinum 8480 processor [35]. Figure 6 presents data for 52 unique convolutions with OC 256 (Section V-B2). The left subplot shows simulated vs. measured efficiency, and the right shows absolute error. The simulator matches AMX peak performance and showcases a median error of 5.0 and first third quartiles at 3.0 and 8.7 . 9 32 64 128 256 of output feature maps 0 20 40 60 80 100 Efficiency ( ) AMX MTE8s Absolute Error Distribution 0 2 4 6 8 10 12 14 16 Fig. 6. Measured and simulated performance of convolution dataset on an Intel Xeon Platinum 8480 (left) and the absolute error distribution (right). We observe that MTE8s delivers slightly better performance than AMX on problems with larger OC values. We attribute this behavior to cache conflict misses taking place on AMX oneDNN algorithm on the 8480 processor s cascading cache architecture [58]. VI. EVALUATION We evaluate the seven approaches described in Section V-C when running the workloads of Sections V-B2 and V-B3. All approaches run on top of the system described in Table IV, except AMX, which runs on an Intel Xeon Platinum [35] processor. Section VI-A evaluates the performance of these approaches in terms of the peak performance percentage they achieve. Section VI-B evaluates our proposals in terms of energy consumption. Section VI-C presents an analysis on the retired instruction count. A. Performance Figure 7 shows the peak performance percentage achieved by each approach across 75 convolutions and 18 transformer workloads, organized by increasing output feature maps (OC) or output matrix columns (N), which are most suited for vectorization. We classify Workloads into six categories based on OC N sizes: i) 1 32, ii) 33 64, iii) 65 128, iv) 129 256, v) 257 512, and vi) 513 2048. Vector 1KB and Vector 2KB show increasing average efficiencies across categories I-IV (6.3 to 53.1 ) because their performance is limited by the data-parallelism available in the vectorized dimension, i.e., OC, or N. For convolution workloads in categories V and VI, Vector 2KB reaches 78.7 , and 70.7 efficiency, respectively, by exploiting its wider vector length. In contrast, some GEMM shapes do not equally divide the Vector 2KB maximum vector length on transformer workloads (e.g. N 768, VL 512), causing hardware under- utilization on the last N loop iterations and lower efficiencies of 32.7 and 19.5 for transformer GEMMs of categories V-VI. The SiFiveInt kernel obtains average efficiencies of 11.3 , 26.8 , 36.6 , 42.9 , 41.8 , and 43.2 on categories I- VI considering both convolution and transformer workloads. Despite exploiting the parallelism of the entire GEMM loop nest, SiFiveInt fails to deliver floating-point throughput close to the peak. The SiFiveInt ISA defines a hardware GEMM shape of 4 64 4 when implemented on processors with a V LEN of 8192 bits. This geometry results in a relatively small 4 4 A matrix operand, causing a register state under- utilization and comparatively lower arithmetic density. MTE8s, which reproduces the semantics of the x86 AMX ISA, successfully vectorizes all GEMM loops and, therefore, it exposes larger data-level parallelism than Vector 1KB, Vector 2KB, and SiFiveInt for categories I-IV. For categories V and VI, the Vector 2KB approach delivers larger performance than MTE8s for convolution workloads since the larger OC size enables Vector 2KB to fully use the wider vector length, as well as maximizing loop unrolling across the 32 available vector registers, while AMX is limited to eight. MTE32v and MTE32s successfully use the full vector length in all workloads, leading to better performance than Vector 1KB and Vector 2KB, especially when dimensions OC and N are less than 256. MTE32v obtains 29.1 average efficiency on category I and between 51.8 and 86.8 in categories II-VI. MTE32s obtains 40.3 average efficiency on category I and between 67.3 and 93.2 in categories II- VI. MTE32v and MTE32s are 1.16 and 1.35 faster, on average, than MTE8s since the former approaches are able to leverage a larger number of architectural registers than the latter. The Vector 2KB approach reaches MTE performance when the vectorized dimension contains: i) 512 elements or more elements; and ii) a number of elements that is multiple of 512, as it is the case for category VI convolutions. MTE32v obtains geometric mean speedups of 2.3 , 2.11 , 1.98 and 1.16 over Vector 1KB, Vector 2KB, SiFiveInt, and MTE8s while MTE32s achieves 2.67 , 2.45 , 2.3 and 1.35 . 1) End-to-end evaluation: This section evaluates MTE in the context of end-to-end inference on complete AI models. We conduct our analysis using PyTorch 2.5.1 [46] and con- sider computer vision models from the torchvision package v0.20.1 [59], including: SqueezeNet [19], Inception [16], and ResNet50 [15]. Additionally, we evaluate the language models BERT [20] (configured for the fill-mask task) and GPT-2 [21] (configured for text generation) obtained via the transformers package v4.48.3 [60]. Figure 8 shows the performance speedup of MTE32s and MTE32v over MTE8s, which models the AMX ISA [5] semantics. Vector 1KB, Vector 2KB, and SiFiveInt are omitted due to inferior performance. MTE32s achieves speedups of 1.05 , 1.09 , and 1.13 on SqueezeNet, Inception, and ResNet50, respectively, while MTE32v sees 1.02 , 1.04 , and 1.10 gains. For BERT and GPT-2, MTE32s reaches 1.20 and 1.22 , and MTE32v gets 1.15 and 1.16 . These gains stem from MTE s acceleration of convolution workloads in computer vision models and GEMMs in language models. BERT and GPT-2 show the highest gains, as they spend 76.16 and 67.04 of inference time on GEMMs, compared to 37.22 , 51.36 , and 48.92 for SqueezeNet, Inception, and ResNet50 on convolutions. 2) Comparison between MTE and AMX: Figure 9 shows the AMX performance for the considered convolution work- loads on the 8084 processor. The figure also shows the 10 32 64 128 256 512 2048 of output feature maps Convolutions 0 20 40 60 80 100 Efficiency ( ) Vector 1KB Vector 2KB SiFiveInt MTE8s MTE32v MTE32s 32 64 512 768 of C matrix columns Transformers 0.0 204.8 409.6 614.4 819.2 1024.0 GFLOP s SP Fig. 7. Percentage of the peak performance (efficiency) obtained by convolution and GEMM kernels. Convolution and GEMM workloads are displayed in ascending order concerning the number of output feature maps and C matrix columns respectively. Squeezenet inception-v3 ResNet50 BERT GPT-2 1.00 1.05 1.10 1.15 1.20 1.25 Speedup over MTE8s MTE8s MTE32v MTE32s Fig. 8. Application-level speedup of MTE32v and MTE32s, considering computer vision and language models. 32 64 128 256 512 of output feature maps 0 20 40 60 80 100 Efficiency ( ) AMX MTE32v Fig. 9. Convolution efficiency obtained by AMX compared to MTE32v simulated performance data. performance of the MTE32v approach on the architecture described by Tables IV and V. AMX and MTE32v obtain average efficiencies of 52.8 and 68.1 , respectively, defining a MTE32v speedup of 1.29 over AMX. MTE32v obtains better performance than AMX primarily when OC 64, since MTE32v leverages its 32 architecture registers for unrolling in this scenario. Specifically, MTE enables greater degrees of unrolling for the loop over the M dimension in Algorithm 1, which exposes more independent tfmul instructions (line 15) and improves reuse of the b tile (line 14). To perform such software opti- mizations, the algorithm needs: i) a large M dimension size, or OC for convolutions; and ii) a larger number of architectural registers to hold multiple C and A operands. The latter is not fulfilled by AMX, which results in a lower performance than MTE on operations with sufficiently large M and OC dimension sizes. B. Energy Consumption This section evaluates the energy consumption of the MTE32v, MTE32s, and MTE8s architectures presented in Section V-C following the methodology we describe in Sec- tion V-D and considering the workload categories we define in Section VI-A. We do not present results for Vector 1KB, I II III IV V VI 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 Energy-to-solution normalized to MTE8s MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v Core VU Systolic LLC Other Fig. 10. Energy-to-solution per category for MTE32v and MTE32s, normalized to MTE8s. Components: core VU Systolic includes the scalar core, all functional units, and register files; LLC is the last-level cache; and other primarily consists of network-on-chip and memory controllers. TABLE IX RETIRED VECTOR MATRIX INSTRUCTIONS REDUCTION FOR EACH WORKLOAD CONSIDERING THE VECTOR 1KB BASELINE. Category (OC or N) Vector 2KB SiFiveInt MTE8s MTE32v MTE32s I) 1-32 1.00 5.97 36.40 37.22 37.22 II) 33-64 1.00 5.87 17.48 18.55 18.55 III) 65-128 1.00 3.69 8.95 11.37 11.37 IV) 129-256 1.00 2.78 5.57 7.89 7.89 V) 257-512 2.00 2.76 4.95 7.88 7.88 VI) 513-2048 1.81 2.44 4.67 6.92 6.92 Arithmetic average 1.24 4.05 12.38 14.31 14.31 Vector 2KB, and SiFiveInt since their performance is worse than MTE32v, MTE32s, and MTE8s. Figure 10 shows the geometric mean of energy-to-solution for different hard- ware components, normalized to MTE8s. In all evaluated configurations, energy consumption is primarily driven by the vector register file, accounting for 77.01 , 76.59 , and 77.06 of the total in MTE8s, MTE32s, and MTE32v, respectively. MTE32s and MTE32v outperform MTE8s in terms of energy-to-solution by 30.0 and 25.4 , respectively, for workloads belonging to Category VI. C. Vector matrix dynamic instruction count Table IX shows the reduction in the retired vector matrix instruction count considering the Vector 1KB baseline on the evaluated convolution and transformer workload groups. Vector 1KB and Vector 2KB trigger the same number of vector instructions when the vectorized dimensions, OC and N, have fewer than 256 element. Otherwise, the Vector 2KB approach requires up to 2 less instructions than Vector 1KB. 11 SiFiveInt, MTE8s, MTE32v, and MTE32s require 4.05 , 12.38 , 14.31 , and 14.31 fewer instructions than Vector 1KB on average, respectively. The largest differences consist of workloads with OC (or N) 256, where Vector 1KB and Vector 2KB are unable to fully use the vector length. Even for scenarios featuring the largest OC values, MTE32v and MTE32s require 1.34 less instructions than MTE8s as they leverage a larger architecture register file for loop unrolling and run the workloads with fewer micro-kernel calls. VII. RELATED WORK The Open Power ISA v3.1 (OpenPower) [9] introduces outer product instructions for mixed-, single-, double-precision, and integer matrices, using 512-bit accumulators and register-to- register transfers between accumulators and 128-bit vector registers. The new outer product instructions sources A and B operands from vector registers and C operands from the larger accumulator registers. To minimize overheads, the IBM Power10 processor [6] repurposes half of the register file for accumulators, avoiding added architectural state and context switch costs. In contrast, MTE eliminates the need for register transfers and matrix accumulators, while offering a geometry- agnostic programming model. The XuanTie Matrix Multiply Extension (MME) proposes a detached matrix programming model for RISC-V with eight new dedicated matrix registers [8]. MME implemen- tations selecte the RLEN value from a set of valid row bit-lengths settings to determine both the maximum number of rows, RLEN 32, and columns, RLEN SEW. MME defines matrix shape configuration instructions, 2D memory moves, MMA instructions, but also vector-matrix register- register move, as well as many element-wise instructions on matrix register (e.g., data type conversions). Compared to MME, MTE reuses the vector registers, and leverages existing RISC-V V instructions for element-wise operations to reduce the implementation overhead. The Arm Scalable Matrix Extension (SME) [10] defines a new matrix storage state of size V LEN V LEN, where V LEN is the vector register length of the Scalable Vec- tor Extension (SVE) ISA. SME defines an outer product instruction that regards the matrix storage as a number of independent tile registers (e.g. 4 tiles for fp32) employed as accumulators during the outer product of two SVE vectors. Besides the outer product, SME includes 2d memory moves, mixed vector to multi-vector operations, and others. MTE differs from SME by i) using existing architectural state for accumulators; and ii) implementing MMAs as matrix dot products, which matches with long vector architectures and other contexts where the V LEN 2 architectural matrix register is prohibitively expensive. The Hopper architecture [38] is the fourth generation of NVIDIA GPUs augmented with tensor cores to accelerate deep learning workloads via 4 4 8 mixed-precision, and 4 4 4 single-precision, matrix multiplication instructions. Recent work showcases the benefits of explicit tensor core program- ming via the Warp Matrix Multiply Accumulate (WMMA) TABLE X MATRIX EXTENSIONS SUMMARY ISA Tile Size FP32 Tile 2D Memory Dedicated Name ( Bits) Shape ( Elements) Instructions Registers OpenPower [9] 512 4 4 No Yes MME [8] 512-16384 Agnostic Yes Yes SME [10] V LEN 2 Agnostic (Square) Yes Yes Tensor Cores [38] 512 4 4 Yes - AMX [5] 8192 16 16 Yes Yes SiFiveInt [7] 512-16384 4 (V LEN 128) No No MTE (Section III) Agnostic Agnostic Yes No API, which exposes tile shape configuration, matrix load s- tores, and multiplication, to accelerate cross-correlation [61] and convolutions [62]. Developing efficient kernels for tensor cores relies on adapting the application to the predetermined geometry in the micro-architecture and managing the GPU memory subsystem, which incentives the use of kernel li- braries such as cuDNN [63] maintained by the vendors. Table X summarizes the main aspects of the matrix ISAs outlined in this section and Section III-E. The MTE approach is the first matrix ISA that is agnostic to both the tile shape and tile size to decouple the ISA from the underlying microarchi- tecture. In addition, MTE does not require dedicated registers to store matrix tiles as it relies on the vector register file, and efficiently vectorizes GEMMs across the three dimensions M, N, and K. MTE enables the seamless interplay of vector and matrix instructions, which supports software kernel fusion between GEMM-based workloads and common post-operation like batchnorm and activations. VIII. CONCLUSION This paper demonstrates the limitations of existing vector and matrix ISAs when dealing with GEMM-based work- loads. The paper links the shortcomings of state-of-the-art approaches when dealing with tall, skinny, or small matrices to the under-utilization of the vector register file, and proposes a lean matrix tile extension, MTE, to tackle this issue. MTE reinterprets vector registers as matrix tile operands that are manipulated by a novel geometry-agnostic matrix ISA. MTE decouples the matrix instruction set architecture from the microarchitecture and leverages existing vector instructions for element-wise operations. We evaluate MTE as well as state-of-the-art matrix and vector ISAs considering 75 convolution and 18 transformer workloads extracted from modern deep learning workloads. MTE obtains average speedups of 2.67 , 2.45 , 2.3 and 1.35 over vector ISA with 8192- and 16384-bit vector registers, and the state-of-the-art SiFiveInt and AMX ISAs, respectively. ACKNOWLEDGMENT This work has received funding from Future of Computing, a Barcelona Supercomputing Center and IBM initiative (2023) and has been partially supported by the Spanish Ministry of Science and Innovation 12 MCINAEI 10.13039 501100011033 (contract PID2019- 107255GB-C21) and ESF Investing in your future, the Generalitat of Catalunya (contract 2021-SGR00763). Adri a Armejach is a Serra Hunter Fellow and has been partially supported by the Grant IJCI-2017-33945 funded by MCIN AEI 10.13039 501100011033. The authors thank the Departament de Recerca i Universitats de la Generalitat de Catalunya for supporting the Research Group Performance understanding, analysis, and simulation emulation of novel architectures (Code: 2021 SGR 00865). REFERENCES [1] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, Efficient processing of deep neural networks: A tutorial and survey, Proceedings of the IEEE, vol. 105, no. 12, pp. 2295 2329, 2017. [2] E. Georganas, S. Avancha, K. Banerjee, D. Kalamkar, G. Henry, H. Pabst, and A. Heinecke, Anatomy of high-performance deep learning convolutions on simd architectures, in SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2018, pp. 830 841. [3] R. Lim, Y. Lee, R. Kim, and J. Choi, An implementation of matrix matrix multiplication on the intel knl processor with avx-512, Cluster Computing, vol. 21, no. 4, p. 1785 1795, dec 2018. [Online]. Available: [4] A. d. L. Santana, A. Armejach, and M. Casas, Efficient direct convo- lution using long simd instructions, in Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, 2023, pp. 342 353. [5] Intel, Intel Architecture Optimization Reference Manual, 2023, intel-64-and-ia-32-architectures-optimization-reference-manual-volume-1. html. [6] J. E. Moreira, K. Barton, S. Battle, P. Bergner, R. Bertran, P. Bhat, P. Caldeira, D. Edelsohn, G. Fossum, B. Frey et al., A matrix math facility for power isa (tm) processors, arXiv preprint arXiv:2104.03142, 2021. [7] SiFive, Sifive intelligence extensions documentation, 2024, https: www.sifive.com documentation. [8] T.-H. Semiconductor, T-head risc-v matrix extension specification, 2024, [9] O. P. Foundation, The power instruction set architecture v3.1, 2024, [10] ARM, The scalable matrix extension (sme), for armv9-a, 2024, https: developer.arm.com documentation ddi0616. [11] A. Sodani, R. Gramunt, J. Corbal, H. Kim, K. Vinod, S. Chinthamani, S. Hutsell, R. Agarwal, and Y. Liu, Knights landing: Second-generation intel xeon phi product, IEEE Micro, vol. 36, no. 02, pp. 34 46, mar 2016. [12] M. Sato, Y. Ishikawa, H. Tomita, Y. Kodama, T. Odajima, M. Tsuji, H. Yashiro, M. Aoki, N. Shida, I. Miyoshi, K. Hirai, A. Furuya, A. Asato, K. Morita, and T. Shimizu, Co-Design for A64FX Manycore Processor and Fugaku , ser. SC 20. IEEE Press, 2020. [13] T. R.-V. Foundation, The risc-v vector extension, 2024, com riscv riscv-v-spec releases download v1.0 riscv-vspec-1.0.pdf. [14] N. Stephens, S. Biles, M. Boettcher, J. Eapen, M. Eyole, G. Gabrielli, M. Horsnell, G. Magklis, A. Martinez, N. Premillieu, A. Reid, A. Rico, and P. Walker, The arm scalable vector extension, IEEE Micro, vol. 37, no. 02, pp. 26 39, mar 2017. [15] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770 778. [16] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818 2826. [17] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556, 2014. [18] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, You only look once: Unified, real-time object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779 788. [19] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size, arXiv preprint arXiv:1602.07360, 2016. [20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [21] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod- els are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020. [22] F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang, Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer, in Proceedings of the 28th ACM international confer- ence on information and knowledge management, 2019, pp. 1441 1450. [23] L. Wu, S. Li, C.-J. Hsieh, and J. Sharpnack, Sse-pt: Sequential recommendation via personalized transformer, in Proceedings of the 14th ACM conference on recommender systems, 2020, pp. 328 337. [24] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazelwood, M. Hempstead, B. Jia et al., The architectural implications of facebook s dnn-based personalized recommendation, in 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2020, pp. 488 501. [25] L. S. Blackford, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, and G. Henry, An updated set of basic linear algebra subprograms (blas), ACM Transactions on Mathematical Software, vol. 28, no. 2, pp. 135 151, 2002. [26] J. Zhang, F. Franchetti, and T. M. Low, High performance zero-memory overhead direct convolutions, in International Conference on Machine Learning. PMLR, 2018, pp. 5776 5785. [27] K. Takahashi, S. Fujimoto, S. Nagase, Y. Isobe, Y. Shimomura, R. Egawa, and H. Takizawa, Performance evaluation of a next-generation sx-aurora tsubasa vector supercomputer, in High Performance Computing: 38th International Conference, ISC High Performance 2023, Hamburg, Germany, May 21 25, 2023, Proceedings. Berlin, Heidelberg: Springer-Verlag, 2023, p. 359 378. [Online]. Available: 19 [28] F. Minervini, O. Palomar, O. Unsal, E. Reggiani, J. Quiroga, J. Marimon, C. Rojas, R. Figueras, A. Ruiz, A. Gonzalez et al., Vitruvius : an area-efficient risc-v decoupled vector coprocessor for high performance computing applications, ACM Transactions on Architecture and Code Optimization, vol. 20, no. 2, pp. 1 25, 2023. [29] M. V. Maceiras, M. W. Azhar, and P. Trancoso, Vsa: A hybrid vector- systolic architecture, in 2022 IEEE 40th International Conference on Computer Design (ICCD). IEEE, 2022, pp. 368 376. [30] A. Armejach, H. Caminal, J. M. Cebrian, R. Gonz alez-Alberquilla, C. Adeniyi-Jones, M. Valero, M. Casas, and M. Moret o, Stencil codes on a vector length agnostic architecture, in Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, 2018, pp. 1 12. [31] C. G omez, F. Mantovani, E. Focht, and M. Casas, Efficiently running spmv on long vector architectures, in Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Program- ming, 2021, pp. 292 303. [32] S. R. Gupta, N. Papadopoulou, and M. Peric as, Challenges and oppor- tunities in the co-design of convolutions and risc-v vector processors, in Proceedings of the SC 23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, 2023, pp. 1550 1556. [33] D. A. Patterson and J. L. Hennessy, Computer organization and design RISC-V edition: the hardware software interface. Morgan kaufmann, 2020. [34] Intel, Intel Software Development Manual, 2023, content www us en developer articles technical intel-sdm.html. [35] , Intel Xeon Platinum 8480 Processor, 2024, intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz specifications.html. [36] , Accelerate AI workloads with Intel AMX, 2022, documents 2022-12 accelerate-ai-with-amx-sb.pdf. [37] A. Abdelfattah, H. Anzt, E. G. Boman, E. C. Carson, T. Cojean, J. J. Dongarra, M. Gates, T. Gr utzmacher, N. J. Higham, X. S. 13 Li, N. Lindquist, Y. Liu, J. A. Loe, P. Luszczek, P. Nayak, S. Pranesh, S. Rajamanickam, T. Ribizel, B. Smith, K. Swirydowicz, S. J. Thomas, S. Tomov, Y. M. Tsai, I. Yamazaki, and U. M. Yang, A survey of numerical methods utilizing mixed precision arithmetic, CoRR, vol. abs 2007.06674, 2020. [Online]. Available: [38] J. Choquette, Nvidia hopper h100 gpu: Scaling performance, IEEE Micro, 2023. [39] N. P. Jouppi, D. H. Yoon, M. Ashcraft, M. Gottscho, T. B. Jablin, G. Kurian, J. Laudon, S. Li, P. Ma, X. Ma et al., Ten lessons from three generations shaped google s tpuv4i: Industrial product, in 2021 ACM IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2021, pp. 1 14. [40] J. Tong, D. Nagle, and R. Rutenbar, Reducing power by optimizing the necessary precision range of floating-point arithmetic, IEEE Trans- actions on Very Large Scale Integration (VLSI) Systems, vol. 8, no. 3, pp. 273 286, 2000. [41] N. Wang, J. Choi, D. Brand, C.-Y. Chen, and K. Gopalakrishnan, Training deep neural networks with 8-bit floating point numbers, in Proceedings of the 32nd International Conference on Neural Information Processing Systems, ser. NIPS 18. Red Hook, NY, USA: Curran Associates Inc., 2018, p. 7686 7695. [42] J. J. Dongarra, C. B. Moler, J. R. Bunch, and G. W. Stewart, LINPACK users guide. SIAM, 1979. [43] M. A. Heroux, J. Dongarra, and P. Luszczek, Hpcg benchmark technical specification, Sandia National Lab.(SNL-NM), Albuquerque, NM (United States), Tech. Rep., 10 2013. [Online]. Available: [44] Y. Yamada and S. Momose, Vector engine processor of NEC s brand- new supercomputer SX-Aurora TSUBASA, in Proceedings of A Sympo- sium on High Performance Chips, Hot Chips, vol. 30, 2018, pp. 19 21. [45] Intel, Oneapi deep neural network library, 2024, github.io oneDNN . [46] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, Advances in neural information processing systems, vol. 32, pp. 8026 8037, 2019. [47] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al., Tensorflow: A system for large-scale machine learning, in 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 2016, pp. 265 283. [48] M. Shigeo, Xbyak, a c jit assembler for x86 (ia32), x64 (amd64, x86-64), 2024, [49] K. Kawakami, K. Kurihara, M. Yamazaki, T. Honda, and N. Fukumoto, A binary translator to accelerate development of deep learning pro- cessing library for aarch64 cpu, IEICE Transactions on Electronics, vol. 105, no. 6, pp. 222 231, 2022. [50] V. Ferrari, R. Sousa, M. Pereira, J. P. L. De Carvalho, J. N. Amaral, J. Moreira, and G. Araujo, Advancing direct convolution using convo- lution slicing optimization and isa extensions, ACM Transactions on Architecture and Code Optimization, vol. 20, no. 4, pp. 1 26, 2023. [51] S. Marcel and Y. Rodriguez, Torchvision the machine-vision package of torch, in Proceedings of the 18th ACM international conference on Multimedia, 2010, pp. 1485 1488. [52] Intel, Benchdnn github repository, 2024, oneDNN blob master tests benchdnn README.md. [53] S. Li, J. H. Ahn, R. D. Strong, J. B. Brockman, D. M. Tullsen, and N. P. Jouppi, McPAT: An Integrated Power, Area, and Timing Modeling Framework for Multicore and Manycore Architectures, in International Symposium on Microarchitecture (MICRO), 2009, pp. 469 480. [54] Pcacti, 2025. [Online]. Available: packages [55] A. Shafaei, Y. Wang, X. Lin, and M. Pedram, Fincacti: Architectural analysis and modeling of caches with deeply-scaled finfet devices, in 2014 IEEE Computer Society Annual Symposium on VLSI, 2014, pp. 290 295. [56] Q. Xie, X. Lin, Y. Wang, M. J. Dousti, A. Shafaei, M. Ghasemi-Gol, and M. Pedram, 5nm finfet standard cell library optimization and circuit synthesis in near-and super-threshold voltage regimes, in IEEE Computer Society Annual Symposium on VLSI, ISVLSI 2014, Tampa, FL, USA, July 9-11, 2014. IEEE Computer Society, 2014, pp. 424 429. [Online]. Available: [57] S. Xi, H. Jacobson, P. Bose, G.-Y. Wei, and D. Brooks, Quantifying sources of error in McPAT and potential impacts on architectural studies, in International Symposium on High Performance Computer Architecture (HPCA), 2015, pp. 577 589. [58] Intel, Tips to Measure the Performance of Matrix Multiplication Using Intel MKL, 2024, intel.com content www us en developer articles technical a-simple-example-to-measure-the-performance-of-an-intel-mkl-function. html. [59] T. maintainers and contributors, Torchvision: Pytorch s computer vision library, 2016. [60] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, Transformers: State-of- the-art natural language processing, in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 38 45. [Online]. Available: https: www.aclweb.org anthology 2020.emnlp-demos.6 [61] K. Fujita, T. Yamaguchi, Y. Kikuchi, T. Ichimura, M. Hori, and L. Maddegedara, Calculation of cross-correlation function accelerated by tensorfloat-32 tensor core operations on nvidia s ampere and hopper gpus, Journal of Computational Science, vol. 68, p. 101986, 2023. [62] J. Liu, D. Yang, and J. Lai, Optimizing winograd-based convolution with tensor cores, in Proceedings of the 50th International Conference on Parallel Processing, 2021, pp. 1 10. [63] M. Jorda, P. Valero-Lara, and A. J. Pena, Performance evaluation of cudnn convolution algorithms on nvidia volta gpus, IEEE Access, vol. 7, pp. 70 461 70 473, 2019. 14\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2507.03522v1 [cs.AR] 4 Jul 2025 A Flexible Instruction Set Architecture for Efficient GEMMs Alexandre de Limas Santana Barcelona Supercomputing Center Universitat Polit ecnica de Catalunya Email: Adri a Armejach Sanosa Barcelona Supercomputing Center Universitat Polit ecnica de Catalunya Email: Francesc Martinez Barcelona Supercomputing Center Universitat Polit ecnica de Catalunya Email: Erich Focht OpenChip Email: Marc Casas Barcelona Supercomputing Center Universitat Polit ecnica de Catalunya Email: Abstract GEneral Matrix Multiplications (GEMMs) are re- current in high-performance computing and deep learning work- loads. Typically, high-end CPUs accelerate GEMM workloads with Single-Instruction Multiple Data (SIMD) or vector Instruc- tion Set Architectures (ISAs). Since these ISAs face significant issues when running GEMM workloads, particularly when deal- ing with small, tall, or skinny matrices, matrix ISAs have been proposed and implemented by major hardware vendors in the last years. Although these matrix ISAs deliver larger throughput when running GEMMs than their SIMD vector counterparts, they are rigid solutions unable to dynamically adapt themselves to application-specific aspects like the data format. This paper demonstrates that the state-of-the-art matrix ISAs deliver sub- optimal performance when running the most commonly used convolution and transformer models. This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA that completely decouples the instruction set architecture from the microarchitecture and seamlessly interacts with existing vector ISAs. MTE incurs minimal implementation overhead since it only requires a few additional instructions and a 64-bit Control Status Register (CSR) to keep its state. Specifically, MTE can i) vectorize GEMMs across the three dimensions M, N, and K; ii) leverage the capacity of the existing vector register file; and iii) decouple the tile shape from the underlying microarchitecture. MTE achieves speed-ups of 1.35 over the best state-of-the-art matrix ISA. I. INTRODUCTION GEneral Matrix Multiplications (GEMMs) are ubiquitous in high-performance computing and deep learning workloads [1].\n\n--- Segment 2 ---\nI. INTRODUCTION GEneral Matrix Multiplications (GEMMs) are ubiquitous in high-performance computing and deep learning workloads [1]. Typically, high-end CPUs accelerate GEMM workloads with Single-Instruction Multiple Data (SIMD) or vector Instruction Set Architectures (ISAs) [2], [3], [4] to leverage their high- throughput floating-point functional units. In a push for even higher throughput, major hardware vendors are now incor- porating matrix ISAs into CPU architectures with the first implementations released in the last years [5], [6], [7], [8]. These approaches achieve better compute throughput than SIMD vector ISAs by exploiting specialized Matrix-Multiply- Accumulate (MMA) units. Ultimately, these matrix extensions complement the existing SIMD vector ISAs instead of replac- ing them [9], [5], [10], [7]. The advantages of matrix ISAs do not come for free since the architecture must efficiently handle matrix operands in hardware and expose a concise software hardware interface. Solutions to expose matrix operands to hardware include vector register grouping [9], tile registers [5], or registers with a capacity of n2 elements, with n being the vector SIMD register width [10]. These approaches are tightly coupled with microarchitectures implementing short-vector ISAs [11], [12], [6]. Although they are effective, these solutions consist in rigid matrix ISAs supporting static matrix tile shapes unable to dynamically adapt themselves to application-specific aspects like the data format. This paper demonstrates the state-of-the- art matrix ISAs deliver suboptimal performance when running the commonly used convolution and transformer models. This paper proposes the Matrix Tile Extension (MTE), the first matrix ISA that completely decouples the instruction set architecture from the microarchitecture. Additionally, the MTE ISA seamlessly interacts with the existing vector ISAs, which makes MTE able to leverage vector architecture registers for matrix computations and vector instructions for element-wise operations. MTE incurs minimal implementation overhead since it only requires six additional operations and a 64-bit Control Status Register (CSR) to keep its state.\n\n--- Segment 3 ---\nAdditionally, the MTE ISA seamlessly interacts with the existing vector ISAs, which makes MTE able to leverage vector architecture registers for matrix computations and vector instructions for element-wise operations. MTE incurs minimal implementation overhead since it only requires six additional operations and a 64-bit Control Status Register (CSR) to keep its state. Specifically, MTE can i) vectorize GEMMs across the three GEMM loops over M, N, and K; ii) fully exploit the capacity of the vector register file for storing matrices in uniform and mixed- precision scenarios; and iii) decouple the matrix tile geometry from the underlying microarchitecture. By leveraging these three properties, MTE achieves significant performance gains with respect to state-of-the-art vector [11], [13], [14] and matrix [5], [7] ISAs for a heterogeneous set of GEMM workloads belonging to relevant convolution and transformer models. Specifically, this paper makes the following contributions: It identifies the main performance bottlenecks of state- of-the-art matrix ISAs. These bottlenecks come from poor utilization of the matrix storage space, and also the restricted number of registers these ISAs expose to the compiler. It proposes MTE, the first geometry-agnostic ISA decoupled TABLE I THE BLAS GEMM ROUTINE ARGUMENTS Parameter Description A, B, C Pointer to the operand matrices M, N, K Matrix multiplication size LD{A, B, C} Matrices leading dimension offset TRANS{A, B, C} Matrices memory layout α,β Scaling factors from the microarchitecture, enabling code portability across implementations without programmer intervention. The paper also describes two microarchitectures supporting MTE, the first one is based on a lean extension of a long vector processor and the second one leverages a systolic array. It evaluates the performance of MTE considering 75 convo- lution workloads belonging to deep neural networks [15], [16], [17], [18], [19] and 18 transformer workloads obtained from natural language processing models [20], [21] and recommen- dation systems [22], [23], [24].\n\n--- Segment 4 ---\nThe paper also describes two microarchitectures supporting MTE, the first one is based on a lean extension of a long vector processor and the second one leverages a systolic array. It evaluates the performance of MTE considering 75 convo- lution workloads belonging to deep neural networks [15], [16], [17], [18], [19] and 18 transformer workloads obtained from natural language processing models [20], [21] and recommen- dation systems [22], [23], [24]. Our evaluation compares MTE against two approaches based on a vector ISA featuring 8192- and 16,384-bit vector registers, and two recently proposed matrix ISAs [5], [7]. Our evaluation indicates that MTE delivers better performance than these four approaches on top of a high-performance computing architecture. In particular, MTE beats the best state-of-the-art approach, AMX [5], by 1.35 on average due to software leveraging a larger number of architecturally-visible registers for matrix operands. II. BACKGROUND AND MOTIVATION This section describes the standard software interface of GEMM operations (Section II-A) and presents modern vector architectures (Section II-B). Also, this section describes state- of-the-art CPU matrix ISAs (Section II-C), and indicates their shortcomings (Section II-D). A. The Basic Linear Algebra Subprograms (BLAS) Interface The BLAS [25] specification is an ubiquitous standard soft- ware interface describing, among others, the GEneral Matrix Multiplication (GEMM). The GEMM operation is defined as C αAB βC where A, B, and C are matrices with shapes (M,K), (K,N), and (M,N), respectively, while α and β are scalar scaling factors. Table I describes the BLAS GEMM call interface. The M, N, and K parameters define the operand s shape and the geometry of the GEMM computation, which is characterized by the M N K triplet. M describes the number of rows in A and C, N specifies the number of columns of B and C, and K indicates the number of columns of A and rows of B. The leading dimension offset states the distance between elements in consecutive rows or columns, and decouples the matrix memory organization from the operation shape.\n\n--- Segment 5 ---\nM describes the number of rows in A and C, N specifies the number of columns of B and C, and K indicates the number of columns of A and rows of B. The leading dimension offset states the distance between elements in consecutive rows or columns, and decouples the matrix memory organization from the operation shape. The TRANS{A, B, C} flags express the memory layout of operands as row-major or col-major. B. SIMD Vector Support in Modern Processors Modern processors expose some form of vector or SIMD programming model as part of an Instruction Set Architecture (ISA) extension to accelerate linear algebra and AI work- loads [11], [14], [13]. To implement such models, processors employ parallel compute units operating on vector registers, delivering several floating-point operations per cycle on data- parallel workloads [3], [2], [26]. 1) Vector ISAs: Emerging vector ISAs such as the RISC- V Vector Extension (RISC-V V) [13] and the ARM Scalar Vector Extension (SVE) [14] support Vector Length Agnostic (VLA) programming models [14], allowing applications to run in vector processors implementing any vector register size without code changes. The flexibility of VLA programming models has renewed interest in long vector architectures, as evidenced by production systems [27] and research on archi- tectures employing vector registers as wide as 16384 bits [28], [29]. Despite their good performance on HPC workloads [30], [31], [27], long vector architectures are not able to fully use their floating-point computing capacity on GEMMs coming from computer vision AI tasks [32], [4]. 2) Microarchitecture: The defining characteristic of vector arithmetic instructions is that they implement operations that combine the i-th element stored in a vector register with the corresponding i-th element of another register. This charac- teristic drives the design of vector units, which are often implemented with an array of deeply pipelined functional units, or lanes, operating on subvectors [33]. Figure 1 rep- resents the i-th lane of a vector architecture with N lanes that completes a vector instruction over V L elements in V L N steps. The vector register file is interleaved across lanes and a lane interconnect enables inter-lane communication to support vector reductions, slides, and similar operations [27].\n\n--- Segment 6 ---\nFigure 1 rep- resents the i-th lane of a vector architecture with N lanes that completes a vector instruction over V L elements in V L N steps. The vector register file is interleaved across lanes and a lane interconnect enables inter-lane communication to support vector reductions, slides, and similar operations [27]. The vector operand buffers are present on each lane to receive vector element operands from the local Vector Register File (VRF) slice in preparation toward the functional units. Vector architectures support configurable vector length which disables computation on vector tail elements by scheduling fewer elements into the operand buffers. In addition, vector architec- tures support predication, which discards the functional unit outputs for specific elements and thus preserve their values. C. Matrix ISAs for CPU Architectures General-purpose CPU architectures are gradually incor- porating matrix multiplication ISA extensions to accelerate Artificial Intelligence (AI) and High-Performance Computing (HPC) workloads [5], [7], [9], [10]. These systems expose matrix multiplication instructions implemented by a Matrix Multiply-Accumulate (MMA) unit, which leverages the data reuse potential of MMA operations and outperform approaches based purely on SIMD Vector units. 1) The Intel Advanced Matrix Extensions (AMX): AMX consists of a dedicated set of eight tile registers with 1KB of storage each, and a tile matrix multiply unit (TMUL) to operate on them [34]. To use the TMUL, software must first configure the shape of each individual tile register, in terms 2 Lane i 1 VRF slice Lane i Write-back buffer VL-N i jN i N i i VL-N i jN i N i i VL-N i jN i N i i FPU ALU Lane interconnect Lane N-1 vd vs1 vs2 Last step VL N Pipeline step 0 Pipeline step 1 Pipeline step j Operand Buffers Functional Units Fig. 1. Structure of the i-th lane of a vector unit with N lanes. The vector register file storage is interleaved across the N lanes. Functional units contain one execution pipeline per lane operating on local elements across V L N steps. The lane interconnect enables communication between lanes. of the number of active rows and the row size in bytes, by populating a 64-byte CSR using a dedicated instruction.\n\n--- Segment 7 ---\nThe lane interconnect enables communication between lanes. of the number of active rows and the row size in bytes, by populating a 64-byte CSR using a dedicated instruction. The AMX TMUL unit is a grid of fused multiply-add units able to read and write to tile registers. AMX TMUL instructions sup- port only mixed-precision integer (int8 to int32) and floating- point (bf16 to fp32) operations with a maximum M N K geometry of 16 16 64 and 16 16 32, respectively. To enforce maximum utilization of the tile storage, TMUL instructions require the A and C matrices to be laid out in row-major layout while the B matrix must undergo a relayout, packing adjacent rows into 32-bit words (two bf16 or four int8 elements). Element-wise operations like α and β scaling required by the BLAS GEMM we describe in Section II-A are not supported by AMX and must be implemented using either scalar instructions or SIMD approaches like AVX512 [11]. However, since there is no interface between AVX512 and AMX register files, communication is done through memory before post-processing. 2) SiFive Intelligence Custom Extensions (SiFiveInt): The most recent custom matrix extension for RISC-V V is SiFiveInt [7], which introduces an MMA instruction involving several 4 4 matrix tiles. This instruction uses three vector register operands, two bf16 input vectors (vs1 and vs2), and a single fp32 input output vector (vd), and conceives vector registers as a sequence of independent 4 4 matrix tiles. The MMA instruction multiplies the first tile in vs1, the A operand, to all tiles in vs2, the B operand, and accumulates the results into the vd tiles, the C operand. Despite being a vector-length agnostic extension, the constant 4 4 operand shape for vs1 incurs poor hardware utilization on long vector implementations, as the MMA uses just the first 128 bits of the vs1 register (i.e., one 4 4 Bfloat16 tile). Finally, the SiFiveInt extension does not define matrix memory move operations and relies on tiled memory layouts for efficient data movement with unit-stride vector load stores, otherwise requiring expensive vector gather scatter.\n\n--- Segment 8 ---\nDespite being a vector-length agnostic extension, the constant 4 4 operand shape for vs1 incurs poor hardware utilization on long vector implementations, as the MMA uses just the first 128 bits of the vs1 register (i.e., one 4 4 Bfloat16 tile). Finally, the SiFiveInt extension does not define matrix memory move operations and relies on tiled memory layouts for efficient data movement with unit-stride vector load stores, otherwise requiring expensive vector gather scatter. D. Shortcomings of Matrix ISAs To illustrate the shortcomings of state-of-the-art matrix ISAs, we evaluate the single-core efficiency of the x86 Advanced Matrix Extension (AMX) [5] and the Advanced Vector Extension (AVX512) [11] on an Intel Xeon Platinum 8480 [35] processor locked to a frequency of 2.1 GHz. Our evaluation considers 75 unique convolution layers from computer vision networks [15], [17], [19], [18], [16] and 31 GEMM workloads commonly featured on transformer networks [20], GPT [21] and BERT-based models for recom- mendation systems [22], [23], [24]. Figure 2 shows our results. The y-axis represents the mea- sured performance in terms of Giga Floating-Point operations per second (GFLOP s) in logarithmic scale. The x-axis on the left subplot displays all the convolution workloads we consider sorted by the ascending number of output feature maps. The x-axis on the right subplot showcases the GEMM workloads extracted from transformers and recommendation systems sorted by the ascending number of output matrix rows. We measure average efficiencies of 35.4 for AMX and 85.6 for AVX512 in terms of percentage of peak performance. While the arithmetic throughput of AMX is 16 greater than AVX512 (2150 bf16 GOps s versus 134 fp32 GOps s), the limited single-core efficiency constrains AMX speedup over AVX512 to a range of 5.7 10 [36].\n\n--- Segment 9 ---\nWe measure average efficiencies of 35.4 for AMX and 85.6 for AVX512 in terms of percentage of peak performance. While the arithmetic throughput of AMX is 16 greater than AVX512 (2150 bf16 GOps s versus 134 fp32 GOps s), the limited single-core efficiency constrains AMX speedup over AVX512 to a range of 5.7 10 [36]. The poor efficiency of AMX in terms of floating-point performance with respect to its peak performance is explained by two main factors: i) the fact that AMX requires dedicated 1KB registers to store matrix operands reduces the number of architectural registers to only 8, otherwise the matrix register file would be prohibitively large, which limits the potential performance enhancements of compiler-based approaches like loop unrolling; and ii) the rigid M N K geometry of AMX, which is restricted to just 16 16 64 and 16 16 32 matrix tile computations and does not match tall and skinny matrix tiles, which usually appear in AI workloads like the ones we represent in Figure 2. In addition, the shapes of GEMM kernels coming from transformer models [20], [21] depend on input parameters, which makes it impractical to store input matrices on pre-defined high-performance layouts as it happens on modern direct convolution kernels. Therefore, we observe comparatively lower AMX performance for GEMMs coming from transformer models as data must be transposed and or re-ordered via SIMD instructions, stored to memory, and reloaded to AMX tile registers. This paper proposes a novel matrix ISA extension targeting GEMMs to fix the limitations of current approaches, since i) it uses vector registers to store matrix tiles and therefore does not require dedicated matrix registers to store matrix tiles; ii) it is able to dynamically adapt its tile geometry to workload requirements. III. THE MATRIX TILE EXTENSION This section describes the Matrix Tile Extension (MTE) ISA, which extends the concept of vector length agnosti- cism [13], [14] to support a flexible geometry-agnostic matrix 3 32 64 128 256 512 1024 of output feature maps 64 128 256 512 1024 2048 Performance (GFLOP s) AVX512 AMX AVX512 peak AMX peak 16 32 64 of output matrix rows Fig. 2.\n\n--- Segment 10 ---\nTHE MATRIX TILE EXTENSION This section describes the Matrix Tile Extension (MTE) ISA, which extends the concept of vector length agnosti- cism [13], [14] to support a flexible geometry-agnostic matrix 3 32 64 128 256 512 1024 of output feature maps 64 128 256 512 1024 2048 Performance (GFLOP s) AVX512 AMX AVX512 peak AMX peak 16 32 64 of output matrix rows Fig. 2. Single core Intel Xeon Platinum 8480 performance of vector (AVX512) and matrix (AMX) ISAs during computer vision convolutions and language models GEMMs. instruction set architecture. MTE reuses the vector register file for tile storage, avoiding the need for dedicated registers and matrix-to-vector register moves. A. Determining Tile Dimensions This section describes how MTE determines the sizes of A, B, and C matrix tiles to support a geometry-agnostic matrix instruction set that maximizes the use of vector registers storage capacity. Section III-A1 describes the scenario where the A, B, and C matrix operands involved in the GEMM com- putation are composed of scalar coefficients of the same data type. We call this scenario Uniform Precision. Section III-A2 describes the scenario where matrix C is composed of scalar coefficients represented with a larger data type than matrices A and B. We call this scenario Mixed Precision, similarly as previous work [37]. 1) Uniform Precision: In the uniform precision scenario, all matrices contain scalar coefficients represented with the same data type. In this context, the RISC-V V ISA [13] uses vector registers to store a set of V LEN SEW elements, where V LEN is the size of the vector registers in bits and SEW indicates the single element bit-width. The SVE ISA [14] applies a similar approach and expresses the state contained by the V LEN and SEW parameters in terms of predicates. For simplicity, and without loss of generality, we express the state required by MTE using the RISC-V V nomenclature, although it could be written in terms of any vector-length agnostic ISA. MTE considers 32 vector registers, like RISC-V V and SVE.\n\n--- Segment 11 ---\nFor simplicity, and without loss of generality, we express the state required by MTE using the RISC-V V nomenclature, although it could be written in terms of any vector-length agnostic ISA. MTE considers 32 vector registers, like RISC-V V and SVE. MTE organizes matrix elements by segmenting the vector registers into V LEN RLEN matrix rows storing RLEN SEW elements each, as illustrated in Figure 3. The RLEN parameter is a MTE design-time constant informing the tile row size in bits. Formula 1 expresses the largest hardware matrix geometry considering the V LEN, SEW, and RLEN parameters. The product ROWS COLS is equal to the storage of a vector register in terms of element count, V LEN SEW. ROWS V LEN RLEN , COLS RLEN SEW (1) In the uniform precision scenario, MTE stores matrix tiles in vector registers in row-major order. The three equations of Formula 2 determine the maximum M, N, and K dimen- Nvregs VLEN SEW RLEN SEW VLEN RLEN Nvregs Fig. 3. Rank-1 (left) and rank-2 (right) interpretations of the vector register file. The vector view interprets registers as vectors of VLEN SEW elements. The rank-2 view interprets registers as VLEN RLEN RLEN SEW matrices. sion sizes supported by a processor with V LEN-bits vector registers, and RLEN-bits rows for a given value of SEW. The K dimension formula constrains the matrix size to the storage space of a single vector register. Formula 2 guarantees the full use of vector registers storing C tiles regardless of the V LEN, RLEN, and SEW. Fully utilizing the vector registers storing A and B tiles requires M N since the K dimension represents the number of rows in B and columns in A. M V LEN RLEN , N RLEN SEW , K min(M, N) (2) 2) Mixed Precision: In the mixed precision scenario, matrix C is composed of scalar coefficients represented with a larger data type than A and B.\n\n--- Segment 12 ---\nFormula 2 guarantees the full use of vector registers storing C tiles regardless of the V LEN, RLEN, and SEW. Fully utilizing the vector registers storing A and B tiles requires M N since the K dimension represents the number of rows in B and columns in A. M V LEN RLEN , N RLEN SEW , K min(M, N) (2) 2) Mixed Precision: In the mixed precision scenario, matrix C is composed of scalar coefficients represented with a larger data type than A and B. This difference implies that two different SEW sizes must be considered in the mixed precision scenario: the SEW of matrices A and B, which we call SEWi, and the one of C, which we call SEWo. Having distinct SEW values brings an additional restriction when determining N, i.e., N min( RLEN SEWi , RLEN SEWo ). Since SEWi SEWo, the data size of C determines the value of N, and therefore the B tile size is smaller than the vector register capacity. To overcome this issue, MTE uses a transposed layout to store B tiles in vector registers in the mixed precision scenario, i.e., MTE organizes and operates on B tiles in a col-major ordering within vector registers. Using a transposed layout exclusively for B operands alters the semantics of the K dimension which now determines the number of columns of A and B matrices, i.e. K RLEN SEWi . In addition, N now determines both the number of rows of B and columns of C, therefore N min( V LEN RLEN , RLEN SEWo ). Taking into account that M V LEN RLEN , we obtain the three equations of Formula 3. They determine the maximum M, N, and K dimension sizes involving a transposed B operand and considering the different input and output data sizes, SEWi and SEWo. M V LEN RLEN , N min(M, RLEN SEWo ) , K RLEN SEWi (3) Therefore, transposing B makes it possible to determine N by considering SEWo instead of both SEWo and SEWi. Because there is no equation in Formula 3 involving both SEWi and SEWo, the inequality SEWi SEWo does not restrict the full use of the vector register length.\n\n--- Segment 13 ---\nM V LEN RLEN , N min(M, RLEN SEWo ) , K RLEN SEWi (3) Therefore, transposing B makes it possible to determine N by considering SEWo instead of both SEWo and SEWi. Because there is no equation in Formula 3 involving both SEWi and SEWo, the inequality SEWi SEWo does not restrict the full use of the vector register length. For 4 TABLE II 64-BIT CSR FOR THE MATRIX TILE EXTENSION Name Description Bits t[m,n,k] Tile dimension shapes 36 ttype[i,o] Input output matrix tile types 8 rlenb RLEN in Bytes 12 Reserved additional data 8 TABLE III MATRIX TILE EXTENSION INSTRUCTIONS Instruction Arguments Description tss[m,n,k] rd, rs1, ttypeio tile set shape [M,N,K] t{t}l[a,b,c,bt] vd, rs1, rs2 [A,B,C,BT ] tile {transposed} load t{t}sc vd, rs1, rs2 C tile {transposed} store t{f}{w}mul vd, vs1, vs2 tile {FP} {widening} dot product tvmask[a,b,c,bt] vd, rs1 set vector mask instance, an architecture targeting 32-bit matrix operations with V LEN 8192 and RLEN 512 describes a maximum matrix multiplication geometry of 16x16x16 in a uniform precision scenario with 32-bit data types. This scenario fully utilizes the vector register capacity (256 elements) on all operands, according to Formula 2. The same architecture executing mixed-precision operations with SEWo 32 and SEWi 16 describes a maximum geometry of 16x16x32, also using full vector registers capacity (256 output elements, 512 input elements), according to Formula 3. B. Control Status Registers MTE stores all the state defining the tile s geometry in a 64-bit Control Status Register (CSR) described in Table II. The tm, tn, and tk fields store the current hardware geometry settings for the M, N, and K dimensions with a maximum dimension size of 212 4096 elements.\n\n--- Segment 14 ---\nControl Status Registers MTE stores all the state defining the tile s geometry in a 64-bit Control Status Register (CSR) described in Table II. The tm, tn, and tk fields store the current hardware geometry settings for the M, N, and K dimensions with a maximum dimension size of 212 4096 elements. The ttypei and ttypeo are two 4-bit fields specifying traits of matrix input and output operands. The ttype fields use 2 bits to specify the SEW of each operand tile, which can be either 8, 16, 32, or 64 bits. The remaining 2 bits on the ttype fields codify how to handle elements on inactive columns and rows. One option is to leave the inactive bits untouched (undisturbed policy in the RISC-V nomenclature [13]). Alternatively, the software is responsible for not accessing such elements as they can be dirty (agnostic policy in RISC-V). The rlenb CSR holds the RLEN size in bytes. The remaining bits are reserved for MTE extensions. C. MTE Instructions This section describes the 19 MTE instructions organized in five instruction groups, as seen in Table III. 1) Configuring the tile geometry: The tssm, tssn, and tssk instructions configure the M, N, and K GEMM matrix dimensions by updating the respective tm, tn, or tk CSR fields. The instructions take the requested dimension size from RLEN VLEN Mask Tail Fig. 4. Vector processing of matrices in vector registers. White and dark strips represent active and inactive elements respectively. The vector length deactivates inactive matrix rows at the vector tail, and the vector mask deactivates inactive columns at each row spanning RLEN bits. the application in rs1 and return the updated CSR value, the granted dimension size, in rd. The return value is the minimum between the application request and the maximum dimension size allowed by the underlying microarchitecture, discussed in Sections III-A1 and III-A2, and the data-width of input and output matrices, SEWi, and SEWo. The ttypeio is a 3-bit immediate encoding of the input and output matrix element widths, SEWi and SEWo, for configuring the ttype CSR fields.\n\n--- Segment 15 ---\nThe return value is the minimum between the application request and the maximum dimension size allowed by the underlying microarchitecture, discussed in Sections III-A1 and III-A2, and the data-width of input and output matrices, SEWi, and SEWo. The ttypeio is a 3-bit immediate encoding of the input and output matrix element widths, SEWi and SEWo, for configuring the ttype CSR fields. 2) Matrix data movement: The tile load store, tl ts, and their transposed variants ttl tts, move GEMM matrix operands between memory and the vector register file. In the gen- eral case, these instructions access blocks of up to RLEN consecutive bits found at constant strides in memory. The load instructions are encoded with the GEMM operand type they produce, A, B, C, or BT to specify the relevant CSR dimension shape fields to the microarchitecture for instruction decoding. For instance, C tile operations require the CSR tm, and tn fields to compute the C tile shape. The special t{t}lbt instruction indicates that the B operand is organized in a col- major layout within the vector register state. Finally, MTE defines store instructions exclusively for the C matrix operand. MTE memory instructions require two scalar parameters: The base memory address from rs1, and the leading dimension offset from rs2, expressed in bytes. The MTE operands on memory instructions are directly associated to the BLAS software interface parameters pairs (A, lda), (B, ldb), and (C, ldc). The special case with stride zero corresponds to row or column broadcast operations where a single row column value in memory is replicated across all vector register rows - columns. MTE implementations may optimize row column broadcasts when rs2 describes the 0-stride scenario. 3) Matrix Multiplication: The tfmul and tmul matrix tile multiply instructions compute the product of the A and B matrix tiles and accumulate the result to a C tile matrix, i.e., they are MMA instructions. The former instruction operates on floating-point data types and the latter on integers. These instructions require three vector register operands, the C tile in vd, the A tile in vs1, and the B tile in vs2.\n\n--- Segment 16 ---\nThe former instruction operates on floating-point data types and the latter on integers. These instructions require three vector register operands, the C tile in vd, the A tile in vs1, and the B tile in vs2. The tm, tn, and tk MTE CSR fields determine the operation shape. The tfwmul and twmul are the mixed-precision variants of tfmul and tmul, respectively, and interpret the B operand in terms of a col-major layout, as Section III-A2 indicates. 4) Vector processing mode: MTE employs vector instruc- tions to implement element-wise operations on matrix data stored in vector registers. This interaction removes the need for 5 Algorithm 1 MTE SGEMM C αAB βC Input: A, B, C, M, N, K, LDA, LDB, LDC, α, β Output: C 1: sm, sn, sk 0 2: erow read csr(rlenb) sizeof(float) 3: for m 0, M, m sm do 4: sm tssm(M m) 5: vl sm erow 6: for n 0, N, n sn do 7: sn tssn(N n) 8: gvl vsetvl(vl, e32) 9: vm tvmaskc(gvl) 10: c vbroadcast(0.0f, gvl) 11: for k 0, K, k sk do 12: sk tssk(K k) 13: a tla( A[m LDA k], LDA, tm, tk) 14: b tlb( B[k LDB n], LDB, tk, tn) 15: c tfmul(c, a, b, tm, tn) 16: t tlc( C[m LDC n], LDC, tm, tn) 17: c vfmul vf mask(c, α, vm, gvl) 18: c vfmacc vf mask(t, β, vm, gvl) 19: tsc(c, C[m LDC n], LDC, tm, tn) data movement between memory or register files.\n\n--- Segment 17 ---\n4) Vector processing mode: MTE employs vector instruc- tions to implement element-wise operations on matrix data stored in vector registers. This interaction removes the need for 5 Algorithm 1 MTE SGEMM C αAB βC Input: A, B, C, M, N, K, LDA, LDB, LDC, α, β Output: C 1: sm, sn, sk 0 2: erow read csr(rlenb) sizeof(float) 3: for m 0, M, m sm do 4: sm tssm(M m) 5: vl sm erow 6: for n 0, N, n sn do 7: sn tssn(N n) 8: gvl vsetvl(vl, e32) 9: vm tvmaskc(gvl) 10: c vbroadcast(0.0f, gvl) 11: for k 0, K, k sk do 12: sk tssk(K k) 13: a tla( A[m LDA k], LDA, tm, tk) 14: b tlb( B[k LDB n], LDB, tk, tn) 15: c tfmul(c, a, b, tm, tn) 16: t tlc( C[m LDC n], LDC, tm, tn) 17: c vfmul vf mask(c, α, vm, gvl) 18: c vfmacc vf mask(t, β, vm, gvl) 19: tsc(c, C[m LDC n], LDC, tm, tn) data movement between memory or register files. For instance, the vector ISA can implement matrix additions and scalar- matrix multiplications, like the ones of BLAS GEMM, with vector additions (vfadd.vv) and vector-scalar multiplications (vfadd.vf), respectively. Figure 4 illustrates how to disable computations on specific matrix elements stored inside the vector register state using the programmable vector length and vector mask features of vector architectures. Setting up the vector instructions to operate on matrix tiles requires: i) setting the vector length to guarantee that all active rows are covered; and ii) creating a vector mask to disable computations on inactive columns at each row.\n\n--- Segment 18 ---\nFigure 4 illustrates how to disable computations on specific matrix elements stored inside the vector register state using the programmable vector length and vector mask features of vector architectures. Setting up the vector instructions to operate on matrix tiles requires: i) setting the vector length to guarantee that all active rows are covered; and ii) creating a vector mask to disable computations on inactive columns at each row. In the context of RISC-V V, the vsetvl instruction [13] performs the first step, gathering the application vector length and type from source scalar registers. The application can read the architecture row size in the rlenb CSR field to compute the vector length for vsetvl in a geometry-agnostic way. Finally, for the vector mask, MTE introduces the tvmask instructions to create a vector mask for either A, B, C, or BT tiles based on the active tile geometry settings. D. BLAS GEMM Algorithm using MTE Algorithm 1 shows a BLAS SGEMM C αAB βC routine implemented with MTE assuming the RISC-V V ISA extension is supported. Line 2 computes the number of elements per matrix row in hardware. Lines 4, 7, and 12 configure the GEMM geometry at each iteration. The granted dimension sizes are stored in the sm, sn, and sk variables for incrementing the M, N, and K loop iterators in Lines 3, 6, and 11. The Lines 4-5, and 8-9 configure the hardware to operate on the C matrix. Line 10 initializes the C output matrix accumulator with zeros using the RISC-V V scalar-vector broadcast instruction. Lines 13-15 depict the computational kernel of the algorithm, consisting of two tile load instructions, from A and B, and a tile multiply operation. Lines 16-18 use standard masked vector arithmetic operations to implement the α and β scalar-matrix multiplications on the C matrix loaded from memory. Typically, this algorithm is optimized by unrolling the M and or N loops to reuse the B and or A matrix tiles loaded into registers in operations across multiple independent C output tiles within the K loop. This optimization serves the purpose of hiding the latencies of the tile load operations by increasing the compute density of the innermost loop and exposing more independent work to the hardware. The K loop may also be unrolled to avoid frequent CSR writes.\n\n--- Segment 19 ---\nThis optimization serves the purpose of hiding the latencies of the tile load operations by increasing the compute density of the innermost loop and exposing more independent work to the hardware. The K loop may also be unrolled to avoid frequent CSR writes. E. Summary of the MTE ISA MTE proposes to reuse the vector register file to store matrix tile operands. This strategy makes it possible to eliminate the need for extra architectural state to support MMA instructions. Similarly to AMX, MTE also uses a CSR for controlling the tile shapes but simplifies the configuration process by enabling only three simultaneous matrix shapes for A, B, and C, which are programmed by software using a few scalar instructions. MTE also provides a set of instructions to configure the vector programming model based on the MTE state, allowing a seamless transition from matrix to vector processing mode without the need for memory or register moves. In addition, MTE defines a geometry-agnostic programming model independent of the matrix multiplication shapes supported by the microarchitecture, while defining a standard way for the software to expose the required geometry to the hardware. IV. ARCHITECTURE SUPPORT FOR MTE INSTRUCTIONS This section describes the architecture support required by MTE. Section IV-A describes two possible microarchitecture implementations of the MTE MMA instructions. Section IV-B describes the support required by the MTE memory instruc- tions, while Section IV-C discuses additional aspects to fully support MTE. A. Supporting the tfmul and tmul Instructions When computing tfmul and tmul MMA operations, de- scribed in Section III-C3, inputs from the A and B tiles contribute to multiple C output elements. In this section, we describe the architecture support required by MTE in the context of i) a dedicated systolic array; and ii) a standard vector processor. 1) Systolic array:: A common approach to implement MMA operations is through dedicated systolic arrays consist- ing of a grid of functional units, such as those found in Intel AMX or Google TPUs [5], [38], [39], [29]. These architectures directly read from tile registers, delivering high throughput per operation.\n\n--- Segment 20 ---\n1) Systolic array:: A common approach to implement MMA operations is through dedicated systolic arrays consist- ing of a grid of functional units, such as those found in Intel AMX or Google TPUs [5], [38], [39], [29]. These architectures directly read from tile registers, delivering high throughput per operation. However, a notable drawback is their specialization for MMA operations, which can result in unused resources 6 jN N 0 i i i jN i N i i C[0,i] A[0,0] B[0,i] C[1,i] A[1,0] B[0,i] C[j,i] A[j,0] B[0,i] cvfma 0, step 0 cvfma 0, step 1 cvfma 0, step j vs2 (B) vd (C) vs1 (A) jN 1 N 1 1 N i N i N i jN i N i i cvfma 1, step 0 cvfma 1, step 1 cvfma 1, step j C[0,i] A[0,1] B[1,i] C[1,i] A[1,1] B[1,i] C[j,i] A[j,1] B[1,i] jN (K-1) N (K-1) K-1 (K-1)N i (K-1)N i (K-1)N i jN i N i i cvfma K-1, step 0 cvfma K-1, step 1 cvfma K-1, step j C[0,i] A[0,K-1] B[K-1,i] C[1,i] A[1,K-1] B[K-1,i] C[j,i] A[j,K-1] B[K-1,i] Operations Decomposition Fig. 5. Operand buffers contents within one vector lane during a matrix multiplication decomposed into K cvfma operations. The left and right sections depict the correlation between cvfma steps and matrix semantics. The middle section shows the operand buffer contents. and limited datatype support.\n\n--- Segment 21 ---\nThe middle section shows the operand buffer contents. and limited datatype support. For example, AMX and TPU hardware only support up to 16-bit input operands. Extending support to 32-bit or 64-bit inputs would incur significant area overheads, as a multiplier area scales quadratically with mantissa bit width [40], [41]. 2) Vector processor:: To exploit data reuse at the reg- ister level in a vector architecture, we propose breaking down MMA instructions into a sequence of new vector micro-instructions called Component Vector Fused-Multiply- Accumulate (cvfma). The cvfma instructions behave like standard vector FMAs except for the lane control logic, which modifies the operand buffer filling to implement the data reuse patterns required by matrix multiplication. Without loss of generality, this section focuses on supporting tmul; the same method applies to tfmul. The tmul instruction is decoded into K cvfma operations, based on the MTE tk CSR field. Each cvfma operation processes a vector length of M RLEN bits, determined by the MTE tm CSR field, which disables computation on tail elements corresponding to inactive tile rows. To avoid computations on inactive columns across the N dimension, the architecture employs an implicit vector predication mask on cvfma instructions, derived from the MTE tn CSR field. This technique is compatible with software predication masks, combining them with a logical and operation to disable computations on arbitrary elements. Figure 5 illustrates the tmul decomposition and shows the operand buffer contents for the i-th vector lane across all K cvfma instructions. The vd operand buffer, containing the C tile, is filled similarly to regular vector architectures, as indicated in Figure 1. The vs2 operand buffer, mapped to the B tile, holds a single lane-local element for all cvfma steps. This pattern, called implicit broadcast, is already used in scalar- vector arithmetic instructions like the RISC-V V vfmacc.vf instruction [13]. No inter-lane communication is required for the vs2 operand. Finally, the A tile operand requires an access pattern unsupported by standard vector instructions.\n\n--- Segment 22 ---\nNo inter-lane communication is required for the vs2 operand. Finally, the A tile operand requires an access pattern unsupported by standard vector instructions. Specifically, each cvfma operation in the sequence requires accessing the operand buffer of a specific lane: the first cvfma accesses lane zero, the second lane one, and so forth. To support this access pattern, MTE can leverage the interconnect between the different vector lanes to move vs1 operands between lanes. Our approach supports the tfmul and tmul MMA instruc- tions using a state-of-the-art vector architecture as a basis. Each lane manages all computations within a matrix column, the pipeline steps make it possible to iterate over the matrix rows, the operand buffers support the vertical flow of B values, and the lane interconnect implements the flow of A values. Reusing the existing vector engine hardware for computation improves hardware utilization while supporting wide datatypes (32 and 64 bit) seamlessly, necessary in classic HPC applications [42], [43]. B. Memory Instructions The general use case of MTE matrix memory instructions can be broken down into a series of unit-stride vector load - stores of up to RLEN bits in a constant stride. If the stride value matches the architecture RLEN, all the matrix tile data is contiguous in memory and therefore the hardware can map the matrix memory operations to a simple unit-stride vector load store. A tl ttl instruction with zero stride, possibly signaled with the zero register (x0 in the case RISC-V V), triggers a row column broadcast, replicating a sequence of RLEN bits to all V LEN bits if the vector register. C. Other Support The tss instructions for configuring the hardware GEMM shape are similar to vsetvl instructions in RISC-V V that, in turn, configure the vector instruction traits using CSRs. Similar to some existing vector HPC architectures, we support register renaming on the MTE CSRs to enable speculation and allowing matrix instructions to be executed out-of-order. V. EVALUATION METHODOLOGY This section describes our evaluation methodology in terms of system architecture, workloads, and considered vector ma- trix ISAs. A.\n\n--- Segment 23 ---\nV. EVALUATION METHODOLOGY This section describes our evaluation methodology in terms of system architecture, workloads, and considered vector ma- trix ISAs. A. System Architecture Tables IV, V, and VI depict the system, vector process- ing unit, and the systolic array accelerator architectures we consider in our experimentation campaign. The system and systolic array parameters match an Intel Xeon Platinum 8480 processor [35] core, the latest x86 server-class processors implementing the AMX extension. The Vector Processing Unit (VPU) parameters resemble a single NEC SX-Aurora 20B vector accelerator core [44] augmented with one vector unit for a total of four. Each unit has 2048-bit vector lanes, i.e. delivers 64 32-bit FMA instructions per cycle, resulting in a peak performance of 512 Single-Precision (SP) FLOP cycle, i.e., 1024 GFLOP s at 2.0GHz. In this evaluation, we consider systolic and vector systems delivering the same peak throughput, that is, 512 FLOP cycle 7 TABLE IV SYSTEM CONFIGURATION Scalar core Out of order, 2 GHz, 512 ROB entries Issue Commit width 6 L1D 48KB, 8-way, 4cc, 10-entry MSHR, LRU L1I 32KB, 8-way, 4cc, 10-entry MSHR, LRU L2 2MB, 8-way, 26cc, 256-entry MSHR, LRU, 128-byte lines Main memory bandwidth 191.25 GB s per core Main memory latency 110ns TABLE V VECTOR PROCESSING UNIT Vector registers 32 (40 physical) Vector length 8192 bits for all approaches but Vector 2KB (16384 bits). Vector lane width 2048 bits Vector units 4 Throughput 512 FLOP (SP) per cycle and 1024 GFLOP s. This methodology makes it possible to compare ISAs with different hardware GEMM implementa- tions while highlighting the impacts of the ISA when express- ing GEMMs. B. Workloads This section highlights the workloads used in our evaluation as well as our code generation method.\n\n--- Segment 24 ---\nVector lane width 2048 bits Vector units 4 Throughput 512 FLOP (SP) per cycle and 1024 GFLOP s. This methodology makes it possible to compare ISAs with different hardware GEMM implementa- tions while highlighting the impacts of the ISA when express- ing GEMMs. B. Workloads This section highlights the workloads used in our evaluation as well as our code generation method. 1) Code generation: We extend oneDNN [45], an architecture-agnostic kernel library providing AI operators under a standard software interface, with convolution and GEMM algorithms for all the evaluated architectures de- scribed in Section V-C. Popular interfaces like Pytorch [46] and Tensorflow [47] employ oneDNN as a CPU backend for compute-intensive operations such as convolutions and GEMMs. oneDNN CPU primitives for x86 and Arm architec- tures use the Xbyak Just-in-Time (JIT) assembler [48], [49] to produce micro-kernels specialized for the operation parameters and target architecture on the fly. We develop rvjit, a RISC-V JIT assembler library similar to Xbyak to dynamically generate micro-kernels for RISC-V V and our proposed extensions. We implement direct convolution kernels for vector and ma- trix architectures following state-of-the-art recipes for SIMD architectures [4], [2], [50]. The direct algorithm employs a tiled matrix memory layout for both activation and weight tensors, and reduces the convolution to a series of matrix tile multiplications. The tiled memory layouts enable unit-stride vector memory operations. Since input and output activation tensors have a consistent organization, no data reshapes are required as the layout propagates across network layers. TABLE VI SYSTOLIC ARRAY ACCELERATOR Tile registers 8 (24 physical) on MTE8s, 32 (40 physical) on MTE32s Tile length 8192 bits Row length 512 bits Throughput 512 FLOP (SP) per cycle Our JIT-generated micro-kernels for vector and MTE ISAs use system balance equations [2], [4] to tune performance aspects like loop unrolling and tiling to the architecture VLEN, RLEN, and cache size parameters.\n\n--- Segment 25 ---\nSince input and output activation tensors have a consistent organization, no data reshapes are required as the layout propagates across network layers. TABLE VI SYSTOLIC ARRAY ACCELERATOR Tile registers 8 (24 physical) on MTE8s, 32 (40 physical) on MTE32s Tile length 8192 bits Row length 512 bits Throughput 512 FLOP (SP) per cycle Our JIT-generated micro-kernels for vector and MTE ISAs use system balance equations [2], [4] to tune performance aspects like loop unrolling and tiling to the architecture VLEN, RLEN, and cache size parameters. The micro-kernels vectorize the N loop, unroll the M and N loop, and apply cache- blocking on the K loops of the GEMM formulation. When applied to convolutions, we map the minibatch, output feature map, and input feature map dimensions to the M, N, and K GEMM matrix dimensions. 2) Convolution Workloads: We evaluate convolution work- loads from the popular computer vision networks ResNet [15], Inception [16], VGG [17], yolo [18], and SqueezeNet [19]. We obtain the models from torchvision [51], an open-source machine vision package, execute them using Pytorch [46], and collect the convolution parameters by inspecting the library calls to oneDNN. We reproduce the Pytorch convolution calls for each layer using benchdnn [52]. Our evaluation covers 75 unique convolution operations including pointwise, spatial, strided, and padded convolutions with square and non-square kernel geometry, and utilizes 32-bit floating-point datatypes. Finally, we define a minibatch size of 16 for our experiments, creating sufficiently large problems to saturate all the evaluated architectures. 3) Transformer Workloads: We evaluate MTE considering 18 GEMM workloads commonly featured on transformer networks [20], GPT [21] and BERT-based models for rec- ommendation systems [22], [23], [24]. These GEMMs cover the linear projections necessary for multi-head attention, the scalar-dot-product function used by the attention block, and the feed-forward network [20].\n\n--- Segment 26 ---\n3) Transformer Workloads: We evaluate MTE considering 18 GEMM workloads commonly featured on transformer networks [20], GPT [21] and BERT-based models for rec- ommendation systems [22], [23], [24]. These GEMMs cover the linear projections necessary for multi-head attention, the scalar-dot-product function used by the attention block, and the feed-forward network [20]. We evaluate these operations on an inference scenario with small query sizes (16, and 32), two different encoding dimensions, dmodel, of 512 and 768 elements with 8 and 12 heads, respectively, and feed-forward operations with 2048 hidden connections. C. Evaluated architectures This section describes the ISAs we consider in our eval- uation. All ISAs run on top of the systems specified in Section V-A except AMX, which runs on an Intel Xeon Platinum 8480 [35] processor. Vector 1KB: uses the RISC-V V ISA to support a direct convolution approach [2], [4], [50] vectorizing the N loop, and unrolling the M loop of the GEMM kernel. Our JIT assembler generates RISC-V V code and uses all 32 architecture registers for unrolling. The maximum vector length is 8192 bits or 256 32-bit elements. Vector 2KB: it employs the same approach as Vector 1KB except for the maximum vector length, which is set to 16384 bits. In this scenario, vector instructions operate on up to 512 32-bit elements. SiFiveInt: it uses a matrix kernel with a custom MMA instruction similar to SiFiveIntelligence [7], adapted to fp32 operands, which multiplies independent 4 4 matrix tiles spanning 512 consecutive bits within a vector register.\n\n--- Segment 27 ---\nIn this scenario, vector instructions operate on up to 512 32-bit elements. SiFiveInt: it uses a matrix kernel with a custom MMA instruction similar to SiFiveIntelligence [7], adapted to fp32 operands, which multiplies independent 4 4 matrix tiles spanning 512 consecutive bits within a vector register. We consider a long vector architecture with V LEN 8192 and use MTE instructions to emulate SiFiveInt semantics by defining 8 TABLE VII EVALUATED ARCHITECTURES Architecture Constants Register file size tfmul (matrix) or vfma (vector) instruction support Acronym VLEN RLEN Architecture Physical Static latency Dynamic latency Geometry VPUs Systolic array Vector 1KB 8192 - 32 40 20 4 1 256 1 4 - Vector 2KB 16384 - 32 40 20 8 1 512 1 4 - SiFiveInt 8192 2048 32 40 28 16 16 16 4 4 No MTE8s 8192 512 8 24 36 16 16 16 16 2 Yes MTE32s 8192 512 32 40 36 16 16 16 16 2 Yes MTE32v 8192 512 32 40 36 64 16 16 16 4 No TABLE VIII PHYSICAL REGISTER FILE AREA OF EACH ARCHITECTURE Vector 1KB Vector 2KB SiFiveInt MTE8s MTE32s MTE32v mm2 1.66 4.15 1.66 1.65 1.66 1.66 RLEN as 2048, resulting in a capacity for 16 4 4 matrix tiles and a maximum GEMM geometry of 4 64 4. We organize the tensor data in 4 4 tiles to enable stride-1 memory requests which corresponds to the best use case for this ISA. MTE32v: uses the proposal described in Section III and implements the GEMM instructions as Section III-C4 indicates on top of the VPU described in Table V. We consider an MTE implementetion with a V LEN of 8192 and RLEN of 512, resulting in a GEMM geometry of 16 16 16, similar to AMX. The JIT generator uses all 32 RISC-V V registers for loop unrolling. MTE32s: It employs the same approach as MTE32v but implements GEMM instructions using the systolic array de- scribed in Table VI.\n\n--- Segment 28 ---\nThe JIT generator uses all 32 RISC-V V registers for loop unrolling. MTE32s: It employs the same approach as MTE32v but implements GEMM instructions using the systolic array de- scribed in Table VI. Both MTE32s and MTE32v have the same peak performance since MTE32v can use all vector units to compute four simultaneous GEMMs whereas MTE32s computes one tile at a time. MTE8s: It reproduces the semantics of the x86 AMX ISA [5] using MTE. This strategy uses at most eight architec- tural registers for code generation. AMX: We consider the x86 Advanced Matrix Extension (AMX) ISA [5] implemented on the Intel Xeon Platinum 8480 [35] processor locked to a frequency of 2.1 GHz. Our evaluation leverages Intel s optimized kernels in oneDNN v3.5 [45], integrated as the CPU backend within PyTorch v2.2 [46]. Table VII describes the evaluated architectures architectural parameters, register file sizes, and support for matrix or vector instructions. We do not include AMX since its architecture parameters are very similar as the ones of MTE8s. The latency fields, expressed in cycles, corresponds to the static and dynamic instruction cost, described in Section V-E. All considered strategies display the same peak performance of 1024 GFLOPs s considering 32-bit datatypes. D. Area Cost This section describes the area cost of the architectures we discuss in Section V-C. Section VI-B describes the energy consumption of these architectures. We estimate the energy and area consumption for different hardware components using McPAT 1.3 [53] and PCACTI [54], [55], [56], respectively. We incorporate the enhancements proposed by Xi et al. [57]. These enhancements improve accuracy by modeling additional core structures and correcting erroneous modeling assumptions. Our analysis assumes a 5nm FinFET technology node to estimate the area cost [56]. Table VIII details the physical register file area for each architecture we describe in Section V-C. The Vector 2KB architecture has the largest register file area, as it includes 40 2KB registers, whereas the other approaches use 1KB reg- isters, as shown in Table VII. Architectures with 40 1KB reg- isters require less than half the area of Vector 2KB.\n\n--- Segment 29 ---\nThe Vector 2KB architecture has the largest register file area, as it includes 40 2KB registers, whereas the other approaches use 1KB reg- isters, as shown in Table VII. Architectures with 40 1KB reg- isters require less than half the area of Vector 2KB. Among all the considered approaches, MTE8s has the smallest register file area, as it contains only 24 1KB registers. Using the same methodology, we determine that the primary contributor to the area cost is the register file for all considered architectures. E. Simulation methodology and validation We evaluate the architectures from Section V-A using a trace-driven micro-architecture simulator that models physical register allocation, memory movement across cache levels, and dynamic instruction latency based on active vector length. Vector and matrix instructions are simulated with two cost components: i) a static, non-blocking, front-end latency paid after decode and before reserving compute resources which can be overlapped with the execution of other instructions, and ii) a dynamic latency tied to vector length and compute throughput that blocks the compute resource. We validate our simulation infrastructure by comparing its results for the MTE8s scenario, which replicates the x86 AMX ISA (Section V-C), against real AMX executions on an Intel Xeon Platinum 8480 processor [35]. Figure 6 presents data for 52 unique convolutions with OC 256 (Section V-B2). The left subplot shows simulated vs. measured efficiency, and the right shows absolute error. The simulator matches AMX peak performance and showcases a median error of 5.0 and first third quartiles at 3.0 and 8.7 . 9 32 64 128 256 of output feature maps 0 20 40 60 80 100 Efficiency ( ) AMX MTE8s Absolute Error Distribution 0 2 4 6 8 10 12 14 16 Fig. 6. Measured and simulated performance of convolution dataset on an Intel Xeon Platinum 8480 (left) and the absolute error distribution (right). We observe that MTE8s delivers slightly better performance than AMX on problems with larger OC values. We attribute this behavior to cache conflict misses taking place on AMX oneDNN algorithm on the 8480 processor s cascading cache architecture [58]. VI. EVALUATION We evaluate the seven approaches described in Section V-C when running the workloads of Sections V-B2 and V-B3.\n\n--- Segment 30 ---\nVI. EVALUATION We evaluate the seven approaches described in Section V-C when running the workloads of Sections V-B2 and V-B3. All approaches run on top of the system described in Table IV, except AMX, which runs on an Intel Xeon Platinum [35] processor. Section VI-A evaluates the performance of these approaches in terms of the peak performance percentage they achieve. Section VI-B evaluates our proposals in terms of energy consumption. Section VI-C presents an analysis on the retired instruction count. A. Performance Figure 7 shows the peak performance percentage achieved by each approach across 75 convolutions and 18 transformer workloads, organized by increasing output feature maps (OC) or output matrix columns (N), which are most suited for vectorization. We classify Workloads into six categories based on OC N sizes: i) 1 32, ii) 33 64, iii) 65 128, iv) 129 256, v) 257 512, and vi) 513 2048. Vector 1KB and Vector 2KB show increasing average efficiencies across categories I-IV (6.3 to 53.1 ) because their performance is limited by the data-parallelism available in the vectorized dimension, i.e., OC, or N. For convolution workloads in categories V and VI, Vector 2KB reaches 78.7 , and 70.7 efficiency, respectively, by exploiting its wider vector length. In contrast, some GEMM shapes do not equally divide the Vector 2KB maximum vector length on transformer workloads (e.g. N 768, VL 512), causing hardware under- utilization on the last N loop iterations and lower efficiencies of 32.7 and 19.5 for transformer GEMMs of categories V-VI. The SiFiveInt kernel obtains average efficiencies of 11.3 , 26.8 , 36.6 , 42.9 , 41.8 , and 43.2 on categories I- VI considering both convolution and transformer workloads. Despite exploiting the parallelism of the entire GEMM loop nest, SiFiveInt fails to deliver floating-point throughput close to the peak. The SiFiveInt ISA defines a hardware GEMM shape of 4 64 4 when implemented on processors with a V LEN of 8192 bits. This geometry results in a relatively small 4 4 A matrix operand, causing a register state under- utilization and comparatively lower arithmetic density.\n\n--- Segment 31 ---\nThe SiFiveInt ISA defines a hardware GEMM shape of 4 64 4 when implemented on processors with a V LEN of 8192 bits. This geometry results in a relatively small 4 4 A matrix operand, causing a register state under- utilization and comparatively lower arithmetic density. MTE8s, which reproduces the semantics of the x86 AMX ISA, successfully vectorizes all GEMM loops and, therefore, it exposes larger data-level parallelism than Vector 1KB, Vector 2KB, and SiFiveInt for categories I-IV. For categories V and VI, the Vector 2KB approach delivers larger performance than MTE8s for convolution workloads since the larger OC size enables Vector 2KB to fully use the wider vector length, as well as maximizing loop unrolling across the 32 available vector registers, while AMX is limited to eight. MTE32v and MTE32s successfully use the full vector length in all workloads, leading to better performance than Vector 1KB and Vector 2KB, especially when dimensions OC and N are less than 256. MTE32v obtains 29.1 average efficiency on category I and between 51.8 and 86.8 in categories II-VI. MTE32s obtains 40.3 average efficiency on category I and between 67.3 and 93.2 in categories II- VI. MTE32v and MTE32s are 1.16 and 1.35 faster, on average, than MTE8s since the former approaches are able to leverage a larger number of architectural registers than the latter. The Vector 2KB approach reaches MTE performance when the vectorized dimension contains: i) 512 elements or more elements; and ii) a number of elements that is multiple of 512, as it is the case for category VI convolutions. MTE32v obtains geometric mean speedups of 2.3 , 2.11 , 1.98 and 1.16 over Vector 1KB, Vector 2KB, SiFiveInt, and MTE8s while MTE32s achieves 2.67 , 2.45 , 2.3 and 1.35 . 1) End-to-end evaluation: This section evaluates MTE in the context of end-to-end inference on complete AI models.\n\n--- Segment 32 ---\nMTE32v obtains geometric mean speedups of 2.3 , 2.11 , 1.98 and 1.16 over Vector 1KB, Vector 2KB, SiFiveInt, and MTE8s while MTE32s achieves 2.67 , 2.45 , 2.3 and 1.35 . 1) End-to-end evaluation: This section evaluates MTE in the context of end-to-end inference on complete AI models. We conduct our analysis using PyTorch 2.5.1 [46] and con- sider computer vision models from the torchvision package v0.20.1 [59], including: SqueezeNet [19], Inception [16], and ResNet50 [15]. Additionally, we evaluate the language models BERT [20] (configured for the fill-mask task) and GPT-2 [21] (configured for text generation) obtained via the transformers package v4.48.3 [60]. Figure 8 shows the performance speedup of MTE32s and MTE32v over MTE8s, which models the AMX ISA [5] semantics. Vector 1KB, Vector 2KB, and SiFiveInt are omitted due to inferior performance. MTE32s achieves speedups of 1.05 , 1.09 , and 1.13 on SqueezeNet, Inception, and ResNet50, respectively, while MTE32v sees 1.02 , 1.04 , and 1.10 gains. For BERT and GPT-2, MTE32s reaches 1.20 and 1.22 , and MTE32v gets 1.15 and 1.16 . These gains stem from MTE s acceleration of convolution workloads in computer vision models and GEMMs in language models. BERT and GPT-2 show the highest gains, as they spend 76.16 and 67.04 of inference time on GEMMs, compared to 37.22 , 51.36 , and 48.92 for SqueezeNet, Inception, and ResNet50 on convolutions. 2) Comparison between MTE and AMX: Figure 9 shows the AMX performance for the considered convolution work- loads on the 8084 processor.\n\n--- Segment 33 ---\nBERT and GPT-2 show the highest gains, as they spend 76.16 and 67.04 of inference time on GEMMs, compared to 37.22 , 51.36 , and 48.92 for SqueezeNet, Inception, and ResNet50 on convolutions. 2) Comparison between MTE and AMX: Figure 9 shows the AMX performance for the considered convolution work- loads on the 8084 processor. The figure also shows the 10 32 64 128 256 512 2048 of output feature maps Convolutions 0 20 40 60 80 100 Efficiency ( ) Vector 1KB Vector 2KB SiFiveInt MTE8s MTE32v MTE32s 32 64 512 768 of C matrix columns Transformers 0.0 204.8 409.6 614.4 819.2 1024.0 GFLOP s SP Fig. 7. Percentage of the peak performance (efficiency) obtained by convolution and GEMM kernels. Convolution and GEMM workloads are displayed in ascending order concerning the number of output feature maps and C matrix columns respectively. Squeezenet inception-v3 ResNet50 BERT GPT-2 1.00 1.05 1.10 1.15 1.20 1.25 Speedup over MTE8s MTE8s MTE32v MTE32s Fig. 8. Application-level speedup of MTE32v and MTE32s, considering computer vision and language models. 32 64 128 256 512 of output feature maps 0 20 40 60 80 100 Efficiency ( ) AMX MTE32v Fig. 9. Convolution efficiency obtained by AMX compared to MTE32v simulated performance data. performance of the MTE32v approach on the architecture described by Tables IV and V. AMX and MTE32v obtain average efficiencies of 52.8 and 68.1 , respectively, defining a MTE32v speedup of 1.29 over AMX. MTE32v obtains better performance than AMX primarily when OC 64, since MTE32v leverages its 32 architecture registers for unrolling in this scenario. Specifically, MTE enables greater degrees of unrolling for the loop over the M dimension in Algorithm 1, which exposes more independent tfmul instructions (line 15) and improves reuse of the b tile (line 14).\n\n--- Segment 34 ---\nMTE32v obtains better performance than AMX primarily when OC 64, since MTE32v leverages its 32 architecture registers for unrolling in this scenario. Specifically, MTE enables greater degrees of unrolling for the loop over the M dimension in Algorithm 1, which exposes more independent tfmul instructions (line 15) and improves reuse of the b tile (line 14). To perform such software opti- mizations, the algorithm needs: i) a large M dimension size, or OC for convolutions; and ii) a larger number of architectural registers to hold multiple C and A operands. The latter is not fulfilled by AMX, which results in a lower performance than MTE on operations with sufficiently large M and OC dimension sizes. B. Energy Consumption This section evaluates the energy consumption of the MTE32v, MTE32s, and MTE8s architectures presented in Section V-C following the methodology we describe in Sec- tion V-D and considering the workload categories we define in Section VI-A. We do not present results for Vector 1KB, I II III IV V VI 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 Energy-to-solution normalized to MTE8s MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v MTE8s MTE32s MTE32v Core VU Systolic LLC Other Fig. 10. Energy-to-solution per category for MTE32v and MTE32s, normalized to MTE8s. Components: core VU Systolic includes the scalar core, all functional units, and register files; LLC is the last-level cache; and other primarily consists of network-on-chip and memory controllers. TABLE IX RETIRED VECTOR MATRIX INSTRUCTIONS REDUCTION FOR EACH WORKLOAD CONSIDERING THE VECTOR 1KB BASELINE.\n\n--- Segment 35 ---\nComponents: core VU Systolic includes the scalar core, all functional units, and register files; LLC is the last-level cache; and other primarily consists of network-on-chip and memory controllers. TABLE IX RETIRED VECTOR MATRIX INSTRUCTIONS REDUCTION FOR EACH WORKLOAD CONSIDERING THE VECTOR 1KB BASELINE. Category (OC or N) Vector 2KB SiFiveInt MTE8s MTE32v MTE32s I) 1-32 1.00 5.97 36.40 37.22 37.22 II) 33-64 1.00 5.87 17.48 18.55 18.55 III) 65-128 1.00 3.69 8.95 11.37 11.37 IV) 129-256 1.00 2.78 5.57 7.89 7.89 V) 257-512 2.00 2.76 4.95 7.88 7.88 VI) 513-2048 1.81 2.44 4.67 6.92 6.92 Arithmetic average 1.24 4.05 12.38 14.31 14.31 Vector 2KB, and SiFiveInt since their performance is worse than MTE32v, MTE32s, and MTE8s. Figure 10 shows the geometric mean of energy-to-solution for different hard- ware components, normalized to MTE8s. In all evaluated configurations, energy consumption is primarily driven by the vector register file, accounting for 77.01 , 76.59 , and 77.06 of the total in MTE8s, MTE32s, and MTE32v, respectively. MTE32s and MTE32v outperform MTE8s in terms of energy-to-solution by 30.0 and 25.4 , respectively, for workloads belonging to Category VI. C. Vector matrix dynamic instruction count Table IX shows the reduction in the retired vector matrix instruction count considering the Vector 1KB baseline on the evaluated convolution and transformer workload groups. Vector 1KB and Vector 2KB trigger the same number of vector instructions when the vectorized dimensions, OC and N, have fewer than 256 element. Otherwise, the Vector 2KB approach requires up to 2 less instructions than Vector 1KB.\n\n--- Segment 36 ---\nVector 1KB and Vector 2KB trigger the same number of vector instructions when the vectorized dimensions, OC and N, have fewer than 256 element. Otherwise, the Vector 2KB approach requires up to 2 less instructions than Vector 1KB. 11 SiFiveInt, MTE8s, MTE32v, and MTE32s require 4.05 , 12.38 , 14.31 , and 14.31 fewer instructions than Vector 1KB on average, respectively. The largest differences consist of workloads with OC (or N) 256, where Vector 1KB and Vector 2KB are unable to fully use the vector length. Even for scenarios featuring the largest OC values, MTE32v and MTE32s require 1.34 less instructions than MTE8s as they leverage a larger architecture register file for loop unrolling and run the workloads with fewer micro-kernel calls. VII. RELATED WORK The Open Power ISA v3.1 (OpenPower) [9] introduces outer product instructions for mixed-, single-, double-precision, and integer matrices, using 512-bit accumulators and register-to- register transfers between accumulators and 128-bit vector registers. The new outer product instructions sources A and B operands from vector registers and C operands from the larger accumulator registers. To minimize overheads, the IBM Power10 processor [6] repurposes half of the register file for accumulators, avoiding added architectural state and context switch costs. In contrast, MTE eliminates the need for register transfers and matrix accumulators, while offering a geometry- agnostic programming model. The XuanTie Matrix Multiply Extension (MME) proposes a detached matrix programming model for RISC-V with eight new dedicated matrix registers [8]. MME implemen- tations selecte the RLEN value from a set of valid row bit-lengths settings to determine both the maximum number of rows, RLEN 32, and columns, RLEN SEW. MME defines matrix shape configuration instructions, 2D memory moves, MMA instructions, but also vector-matrix register- register move, as well as many element-wise instructions on matrix register (e.g., data type conversions). Compared to MME, MTE reuses the vector registers, and leverages existing RISC-V V instructions for element-wise operations to reduce the implementation overhead.\n\n--- Segment 37 ---\nMME defines matrix shape configuration instructions, 2D memory moves, MMA instructions, but also vector-matrix register- register move, as well as many element-wise instructions on matrix register (e.g., data type conversions). Compared to MME, MTE reuses the vector registers, and leverages existing RISC-V V instructions for element-wise operations to reduce the implementation overhead. The Arm Scalable Matrix Extension (SME) [10] defines a new matrix storage state of size V LEN V LEN, where V LEN is the vector register length of the Scalable Vec- tor Extension (SVE) ISA. SME defines an outer product instruction that regards the matrix storage as a number of independent tile registers (e.g. 4 tiles for fp32) employed as accumulators during the outer product of two SVE vectors. Besides the outer product, SME includes 2d memory moves, mixed vector to multi-vector operations, and others. MTE differs from SME by i) using existing architectural state for accumulators; and ii) implementing MMAs as matrix dot products, which matches with long vector architectures and other contexts where the V LEN 2 architectural matrix register is prohibitively expensive. The Hopper architecture [38] is the fourth generation of NVIDIA GPUs augmented with tensor cores to accelerate deep learning workloads via 4 4 8 mixed-precision, and 4 4 4 single-precision, matrix multiplication instructions. Recent work showcases the benefits of explicit tensor core program- ming via the Warp Matrix Multiply Accumulate (WMMA) TABLE X MATRIX EXTENSIONS SUMMARY ISA Tile Size FP32 Tile 2D Memory Dedicated Name ( Bits) Shape ( Elements) Instructions Registers OpenPower [9] 512 4 4 No Yes MME [8] 512-16384 Agnostic Yes Yes SME [10] V LEN 2 Agnostic (Square) Yes Yes Tensor Cores [38] 512 4 4 Yes - AMX [5] 8192 16 16 Yes Yes SiFiveInt [7] 512-16384 4 (V LEN 128) No No MTE (Section III) Agnostic Agnostic Yes No API, which exposes tile shape configuration, matrix load s- tores, and multiplication, to accelerate cross-correlation [61] and convolutions [62].\n\n--- Segment 38 ---\nThe Hopper architecture [38] is the fourth generation of NVIDIA GPUs augmented with tensor cores to accelerate deep learning workloads via 4 4 8 mixed-precision, and 4 4 4 single-precision, matrix multiplication instructions. Recent work showcases the benefits of explicit tensor core program- ming via the Warp Matrix Multiply Accumulate (WMMA) TABLE X MATRIX EXTENSIONS SUMMARY ISA Tile Size FP32 Tile 2D Memory Dedicated Name ( Bits) Shape ( Elements) Instructions Registers OpenPower [9] 512 4 4 No Yes MME [8] 512-16384 Agnostic Yes Yes SME [10] V LEN 2 Agnostic (Square) Yes Yes Tensor Cores [38] 512 4 4 Yes - AMX [5] 8192 16 16 Yes Yes SiFiveInt [7] 512-16384 4 (V LEN 128) No No MTE (Section III) Agnostic Agnostic Yes No API, which exposes tile shape configuration, matrix load s- tores, and multiplication, to accelerate cross-correlation [61] and convolutions [62]. Developing efficient kernels for tensor cores relies on adapting the application to the predetermined geometry in the micro-architecture and managing the GPU memory subsystem, which incentives the use of kernel li- braries such as cuDNN [63] maintained by the vendors. Table X summarizes the main aspects of the matrix ISAs outlined in this section and Section III-E. The MTE approach is the first matrix ISA that is agnostic to both the tile shape and tile size to decouple the ISA from the underlying microarchi- tecture. In addition, MTE does not require dedicated registers to store matrix tiles as it relies on the vector register file, and efficiently vectorizes GEMMs across the three dimensions M, N, and K. MTE enables the seamless interplay of vector and matrix instructions, which supports software kernel fusion between GEMM-based workloads and common post-operation like batchnorm and activations. VIII. CONCLUSION This paper demonstrates the limitations of existing vector and matrix ISAs when dealing with GEMM-based work- loads.\n\n--- Segment 39 ---\nVIII. CONCLUSION This paper demonstrates the limitations of existing vector and matrix ISAs when dealing with GEMM-based work- loads. The paper links the shortcomings of state-of-the-art approaches when dealing with tall, skinny, or small matrices to the under-utilization of the vector register file, and proposes a lean matrix tile extension, MTE, to tackle this issue. MTE reinterprets vector registers as matrix tile operands that are manipulated by a novel geometry-agnostic matrix ISA. MTE decouples the matrix instruction set architecture from the microarchitecture and leverages existing vector instructions for element-wise operations. We evaluate MTE as well as state-of-the-art matrix and vector ISAs considering 75 convolution and 18 transformer workloads extracted from modern deep learning workloads. MTE obtains average speedups of 2.67 , 2.45 , 2.3 and 1.35 over vector ISA with 8192- and 16384-bit vector registers, and the state-of-the-art SiFiveInt and AMX ISAs, respectively. ACKNOWLEDGMENT This work has received funding from Future of Computing, a Barcelona Supercomputing Center and IBM initiative (2023) and has been partially supported by the Spanish Ministry of Science and Innovation 12 MCINAEI 10.13039 501100011033 (contract PID2019- 107255GB-C21) and ESF Investing in your future, the Generalitat of Catalunya (contract 2021-SGR00763). Adri a Armejach is a Serra Hunter Fellow and has been partially supported by the Grant IJCI-2017-33945 funded by MCIN AEI 10.13039 501100011033. The authors thank the Departament de Recerca i Universitats de la Generalitat de Catalunya for supporting the Research Group Performance understanding, analysis, and simulation emulation of novel architectures (Code: 2021 SGR 00865). REFERENCES [1] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer, Efficient processing of deep neural networks: A tutorial and survey, Proceedings of the IEEE, vol. 105, no. 12, pp. 2295 2329, 2017.\n\n--- Segment 40 ---\n12, pp. 2295 2329, 2017. [2] E. Georganas, S. Avancha, K. Banerjee, D. Kalamkar, G. Henry, H. Pabst, and A. Heinecke, Anatomy of high-performance deep learning convolutions on simd architectures, in SC18: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 2018, pp. 830 841. [3] R. Lim, Y. Lee, R. Kim, and J. Choi, An implementation of matrix matrix multiplication on the intel knl processor with avx-512, Cluster Computing, vol. 21, no. 4, p. 1785 1795, dec 2018. [Online]. Available: [4] A. d. L. Santana, A. Armejach, and M. Casas, Efficient direct convo- lution using long simd instructions, in Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, 2023, pp. 342 353. [5] Intel, Intel Architecture Optimization Reference Manual, 2023, intel-64-and-ia-32-architectures-optimization-reference-manual-volume-1. html. [6] J. E. Moreira, K. Barton, S. Battle, P. Bergner, R. Bertran, P. Bhat, P. Caldeira, D. Edelsohn, G. Fossum, B. Frey et al., A matrix math facility for power isa (tm) processors, arXiv preprint arXiv:2104.03142, 2021. [7] SiFive, Sifive intelligence extensions documentation, 2024, https: www.sifive.com documentation. [8] T.-H. Semiconductor, T-head risc-v matrix extension specification, 2024, [9] O. P. Foundation, The power instruction set architecture v3.1, 2024, [10] ARM, The scalable matrix extension (sme), for armv9-a, 2024, https: developer.arm.com documentation ddi0616.\n\n--- Segment 41 ---\n[7] SiFive, Sifive intelligence extensions documentation, 2024, https: www.sifive.com documentation. [8] T.-H. Semiconductor, T-head risc-v matrix extension specification, 2024, [9] O. P. Foundation, The power instruction set architecture v3.1, 2024, [10] ARM, The scalable matrix extension (sme), for armv9-a, 2024, https: developer.arm.com documentation ddi0616. [11] A. Sodani, R. Gramunt, J. Corbal, H. Kim, K. Vinod, S. Chinthamani, S. Hutsell, R. Agarwal, and Y. Liu, Knights landing: Second-generation intel xeon phi product, IEEE Micro, vol. 36, no. 02, pp. 34 46, mar 2016. [12] M. Sato, Y. Ishikawa, H. Tomita, Y. Kodama, T. Odajima, M. Tsuji, H. Yashiro, M. Aoki, N. Shida, I. Miyoshi, K. Hirai, A. Furuya, A. Asato, K. Morita, and T. Shimizu, Co-Design for A64FX Manycore Processor and Fugaku , ser. SC 20. IEEE Press, 2020. [13] T. R.-V. Foundation, The risc-v vector extension, 2024, com riscv riscv-v-spec releases download v1.0 riscv-vspec-1.0.pdf. [14] N. Stephens, S. Biles, M. Boettcher, J. Eapen, M. Eyole, G. Gabrielli, M. Horsnell, G. Magklis, A. Martinez, N. Premillieu, A. Reid, A. Rico, and P. Walker, The arm scalable vector extension, IEEE Micro, vol. 37, no. 02, pp. 26 39, mar 2017. [15] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770 778.\n\n--- Segment 42 ---\nSun, Deep residual learning for image recognition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770 778. [16] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, Rethinking the inception architecture for computer vision, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818 2826. [17] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, arXiv preprint arXiv:1409.1556, 2014. [18] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, You only look once: Unified, real-time object detection, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 779 788. [19] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer, Squeezenet: Alexnet-level accuracy with 50x fewer parameters and 0.5 mb model size, arXiv preprint arXiv:1602.07360, 2016. [20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. [21] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod- els are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020.\n\n--- Segment 43 ---\n33, pp. 1877 1901, 2020. [22] F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang, Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer, in Proceedings of the 28th ACM international confer- ence on information and knowledge management, 2019, pp. 1441 1450. [23] L. Wu, S. Li, C.-J. Hsieh, and J. Sharpnack, Sse-pt: Sequential recommendation via personalized transformer, in Proceedings of the 14th ACM conference on recommender systems, 2020, pp. 328 337. [24] U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazelwood, M. Hempstead, B. Jia et al., The architectural implications of facebook s dnn-based personalized recommendation, in 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE, 2020, pp. 488 501. [25] L. S. Blackford, A. Petitet, R. Pozo, K. Remington, R. C. Whaley, J. Demmel, J. Dongarra, I. Duff, S. Hammarling, and G. Henry, An updated set of basic linear algebra subprograms (blas), ACM Transactions on Mathematical Software, vol. 28, no. 2, pp. 135 151, 2002. [26] J. Zhang, F. Franchetti, and T. M. Low, High performance zero-memory overhead direct convolutions, in International Conference on Machine Learning. PMLR, 2018, pp. 5776 5785. [27] K. Takahashi, S. Fujimoto, S. Nagase, Y. Isobe, Y. Shimomura, R. Egawa, and H. Takizawa, Performance evaluation of a next-generation sx-aurora tsubasa vector supercomputer, in High Performance Computing: 38th International Conference, ISC High Performance 2023, Hamburg, Germany, May 21 25, 2023, Proceedings. Berlin, Heidelberg: Springer-Verlag, 2023, p. 359 378. [Online].\n\n--- Segment 44 ---\nBerlin, Heidelberg: Springer-Verlag, 2023, p. 359 378. [Online]. Available: 19 [28] F. Minervini, O. Palomar, O. Unsal, E. Reggiani, J. Quiroga, J. Marimon, C. Rojas, R. Figueras, A. Ruiz, A. Gonzalez et al., Vitruvius : an area-efficient risc-v decoupled vector coprocessor for high performance computing applications, ACM Transactions on Architecture and Code Optimization, vol. 20, no. 2, pp. 1 25, 2023. [29] M. V. Maceiras, M. W. Azhar, and P. Trancoso, Vsa: A hybrid vector- systolic architecture, in 2022 IEEE 40th International Conference on Computer Design (ICCD). IEEE, 2022, pp. 368 376. [30] A. Armejach, H. Caminal, J. M. Cebrian, R. Gonz alez-Alberquilla, C. Adeniyi-Jones, M. Valero, M. Casas, and M. Moret o, Stencil codes on a vector length agnostic architecture, in Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, 2018, pp. 1 12. [31] C. G omez, F. Mantovani, E. Focht, and M. Casas, Efficiently running spmv on long vector architectures, in Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Program- ming, 2021, pp. 292 303. [32] S. R. Gupta, N. Papadopoulou, and M. Peric as, Challenges and oppor- tunities in the co-design of convolutions and risc-v vector processors, in Proceedings of the SC 23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis, 2023, pp. 1550 1556. [33] D. A. Patterson and J. L. Hennessy, Computer organization and design RISC-V edition: the hardware software interface. Morgan kaufmann, 2020.\n\n--- Segment 45 ---\n[33] D. A. Patterson and J. L. Hennessy, Computer organization and design RISC-V edition: the hardware software interface. Morgan kaufmann, 2020. [34] Intel, Intel Software Development Manual, 2023, content www us en developer articles technical intel-sdm.html. [35] , Intel Xeon Platinum 8480 Processor, 2024, intel-xeon-platinum-8480-processor-105m-cache-2-00-ghz specifications.html. [36] , Accelerate AI workloads with Intel AMX, 2022, documents 2022-12 accelerate-ai-with-amx-sb.pdf. [37] A. Abdelfattah, H. Anzt, E. G. Boman, E. C. Carson, T. Cojean, J. J. Dongarra, M. Gates, T. Gr utzmacher, N. J. Higham, X. S. 13 Li, N. Lindquist, Y. Liu, J. A. Loe, P. Luszczek, P. Nayak, S. Pranesh, S. Rajamanickam, T. Ribizel, B. Smith, K. Swirydowicz, S. J. Thomas, S. Tomov, Y. M. Tsai, I. Yamazaki, and U. M. Yang, A survey of numerical methods utilizing mixed precision arithmetic, CoRR, vol. abs 2007.06674, 2020. [Online]. Available: [38] J. Choquette, Nvidia hopper h100 gpu: Scaling performance, IEEE Micro, 2023. [39] N. P. Jouppi, D. H. Yoon, M. Ashcraft, M. Gottscho, T. B. Jablin, G. Kurian, J. Laudon, S. Li, P. Ma, X. Ma et al., Ten lessons from three generations shaped google s tpuv4i: Industrial product, in 2021 ACM IEEE 48th Annual International Symposium on Computer Architecture (ISCA). IEEE, 2021, pp. 1 14.\n\n--- Segment 46 ---\nIEEE, 2021, pp. 1 14. [40] J. Tong, D. Nagle, and R. Rutenbar, Reducing power by optimizing the necessary precision range of floating-point arithmetic, IEEE Trans- actions on Very Large Scale Integration (VLSI) Systems, vol. 8, no. 3, pp. 273 286, 2000. [41] N. Wang, J. Choi, D. Brand, C.-Y. Chen, and K. Gopalakrishnan, Training deep neural networks with 8-bit floating point numbers, in Proceedings of the 32nd International Conference on Neural Information Processing Systems, ser. NIPS 18. Red Hook, NY, USA: Curran Associates Inc., 2018, p. 7686 7695. [42] J. J. Dongarra, C. B. Moler, J. R. Bunch, and G. W. Stewart, LINPACK users guide. SIAM, 1979. [43] M. A. Heroux, J. Dongarra, and P. Luszczek, Hpcg benchmark technical specification, Sandia National Lab. (SNL-NM), Albuquerque, NM (United States), Tech. Rep., 10 2013. [Online]. Available: [44] Y. Yamada and S. Momose, Vector engine processor of NEC s brand- new supercomputer SX-Aurora TSUBASA, in Proceedings of A Sympo- sium on High Performance Chips, Hot Chips, vol. 30, 2018, pp. 19 21. [45] Intel, Oneapi deep neural network library, 2024, github.io oneDNN . [46] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., Pytorch: An imperative style, high-performance deep learning library, Advances in neural information processing systems, vol. 32, pp. 8026 8037, 2019. [47] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J.\n\n--- Segment 47 ---\n8026 8037, 2019. [47] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al., Tensorflow: A system for large-scale machine learning, in 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16), 2016, pp. 265 283. [48] M. Shigeo, Xbyak, a c jit assembler for x86 (ia32), x64 (amd64, x86-64), 2024, [49] K. Kawakami, K. Kurihara, M. Yamazaki, T. Honda, and N. Fukumoto, A binary translator to accelerate development of deep learning pro- cessing library for aarch64 cpu, IEICE Transactions on Electronics, vol. 105, no. 6, pp. 222 231, 2022. [50] V. Ferrari, R. Sousa, M. Pereira, J. P. L. De Carvalho, J. N. Amaral, J. Moreira, and G. Araujo, Advancing direct convolution using convo- lution slicing optimization and isa extensions, ACM Transactions on Architecture and Code Optimization, vol. 20, no. 4, pp. 1 26, 2023. [51] S. Marcel and Y. Rodriguez, Torchvision the machine-vision package of torch, in Proceedings of the 18th ACM international conference on Multimedia, 2010, pp. 1485 1488. [52] Intel, Benchdnn github repository, 2024, oneDNN blob master tests benchdnn README.md. [53] S. Li, J. H. Ahn, R. D. Strong, J. B. Brockman, D. M. Tullsen, and N. P. Jouppi, McPAT: An Integrated Power, Area, and Timing Modeling Framework for Multicore and Manycore Architectures, in International Symposium on Microarchitecture (MICRO), 2009, pp. 469 480. [54] Pcacti, 2025. [Online].\n\n--- Segment 48 ---\n[54] Pcacti, 2025. [Online]. Available: packages [55] A. Shafaei, Y. Wang, X. Lin, and M. Pedram, Fincacti: Architectural analysis and modeling of caches with deeply-scaled finfet devices, in 2014 IEEE Computer Society Annual Symposium on VLSI, 2014, pp. 290 295. [56] Q. Xie, X. Lin, Y. Wang, M. J. Dousti, A. Shafaei, M. Ghasemi-Gol, and M. Pedram, 5nm finfet standard cell library optimization and circuit synthesis in near-and super-threshold voltage regimes, in IEEE Computer Society Annual Symposium on VLSI, ISVLSI 2014, Tampa, FL, USA, July 9-11, 2014. IEEE Computer Society, 2014, pp. 424 429. [Online]. Available: [57] S. Xi, H. Jacobson, P. Bose, G.-Y. Wei, and D. Brooks, Quantifying sources of error in McPAT and potential impacts on architectural studies, in International Symposium on High Performance Computer Architecture (HPCA), 2015, pp. 577 589. [58] Intel, Tips to Measure the Performance of Matrix Multiplication Using Intel MKL, 2024, intel.com content www us en developer articles technical a-simple-example-to-measure-the-performance-of-an-intel-mkl-function. html. [59] T. maintainers and contributors, Torchvision: Pytorch s computer vision library, 2016.\n\n--- Segment 49 ---\nhtml. [59] T. maintainers and contributors, Torchvision: Pytorch s computer vision library, 2016. [60] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, Transformers: State-of- the-art natural language processing, in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 38 45. [Online]. Available: https: www.aclweb.org anthology 2020.emnlp-demos.6 [61] K. Fujita, T. Yamaguchi, Y. Kikuchi, T. Ichimura, M. Hori, and L. Maddegedara, Calculation of cross-correlation function accelerated by tensorfloat-32 tensor core operations on nvidia s ampere and hopper gpus, Journal of Computational Science, vol. 68, p. 101986, 2023. [62] J. Liu, D. Yang, and J. Lai, Optimizing winograd-based convolution with tensor cores, in Proceedings of the 50th International Conference on Parallel Processing, 2021, pp. 1 10. [63] M. Jorda, P. Valero-Lara, and A. J. Pena, Performance evaluation of cudnn convolution algorithms on nvidia volta gpus, IEEE Access, vol. 7, pp. 70 461 70 473, 2019. 14\n\n