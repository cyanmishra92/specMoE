{
  "source": "ParticipationGamesICML25.pdf",
  "raw_length": 54872,
  "cleaned_length": 54254,
  "base_segments": 209,
  "augmented_segments": 418,
  "segments": [
    {
      "text": "Abstract \nEnergy-harvesting wireless sensor networks (EH- WSNs) offer sustainable solutions for large-scale IoT deployments but face challenges due to the unreliability and intermittent availability of in- dividual sensors. We propose a comprehensive framework that integrates a game-theoretic partici- pation strategy with a federated learning approach tailored for EH-WSNs. Our game-theoretic model enables sensors to make optimal participation de- cisions based on energy levels, data quality, and collective inference impact, fostering cooperative behavior while managing individual energy con- straints.",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "Our game-theoretic model enables sensors to make optimal participation de- cisions based on energy levels, data quality, and collective inference impact, fostering cooperative behavior while managing individual energy con- straints. The federated learning framework ac- commodates intermittent participation and vari- able data quality, ensuring robust model training despite sensor unreliability. Simulation results demonstrate that our integrated approach signif- icantly enhances inference accuracy and energy efficiency compared to traditional participation strategies.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "Simulation results demonstrate that our integrated approach signif- icantly enhances inference accuracy and energy efficiency compared to traditional participation strategies. 1. Introduction \nThe rapid proliferation of the Internet of Things (IoT) has sparked a tremendous growth in the scale and diversity of sensor deployments, from smart homes to expansive in- dustrial and environmental monitoring systems.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "Introduction \nThe rapid proliferation of the Internet of Things (IoT) has sparked a tremendous growth in the scale and diversity of sensor deployments, from smart homes to expansive in- dustrial and environmental monitoring systems. As these networks continue to expand, sustaining continuous opera- tion in the face of finite power sources becomes a paramount concern. To address this, energy harvesting (EH) technolo- gies have emerged as a viable solution, enabling sensors to convert ambient energy (e.g., solar, thermal, or vibration) into electrical power.",
      "type": "sliding_window",
      "tokens": 125
    },
    {
      "text": "To address this, energy harvesting (EH) technolo- gies have emerged as a viable solution, enabling sensors to convert ambient energy (e.g., solar, thermal, or vibration) into electrical power. This approach promises perpetual, maintenance-free operation, significantly reducing environ- mental impact and long-term operational costs. 1 Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "1 Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Correspondence to: Anonymous Author < anon.email@domain.com > . Preliminary work.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "Preliminary work. Under review by the International Conference on Machine Learning (ICML). Do not distribute.",
      "type": "sliding_window",
      "tokens": 24
    },
    {
      "text": "Do not distribute. Despite these advantages, large-scale EH Wireless Sensor Networks (EH-WSNs) remain inherently uncertain. Ambi- ent energy availability varies over time and space, leading to fluctuating sensor activity levels and intermittent participa- tion in both training and inference tasks.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "Ambi- ent energy availability varies over time and space, leading to fluctuating sensor activity levels and intermittent participa- tion in both training and inference tasks. Some sensors may frequently become inactive or produce low-quality data due to energy scarcity or environmental noise. Consequently, the mere presence of numerous EH sensors does not guaran- tee robust and reliable performance for complex tasks such as image recognition, acoustic surveillance, or precision agriculture monitoring.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "Consequently, the mere presence of numerous EH sensors does not guaran- tee robust and reliable performance for complex tasks such as image recognition, acoustic surveillance, or precision agriculture monitoring. Achieving accurate inference in these complex scenarios de- pends on effective coordination. Multiple sensors observing the same phenomenon from different angles can collectively provide more comprehensive and reliable insights than any single sensor could.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Multiple sensors observing the same phenomenon from different angles can collectively provide more comprehensive and reliable insights than any single sensor could. However, requiring all sensors to partic- ipate at all times is impractical, as it drains energy reserves too quickly. Conversely, simplistic policies—such as se- lecting only the highest-energy sensors—ignore factors like data relevance, sensor quality, and the strategic implications of current participation on future network states.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Conversely, simplistic policies—such as se- lecting only the highest-energy sensors—ignore factors like data relevance, sensor quality, and the strategic implications of current participation on future network states. This challenge motivates the need for intelligent, context- aware participation strategies that dynamically determine which sensors should engage during both the  training phase—where global model parameters are periodically fine-tuned or updated—and the  inference  phase—where newly observed data are aggregated to produce predictions. Sensors must carefully balance immediate accuracy gains against conserving energy for future tasks, while also antici- pating the behavior of other sensors that may be collaborat- ing or competing.",
      "type": "sliding_window",
      "tokens": 143
    },
    {
      "text": "Sensors must carefully balance immediate accuracy gains against conserving energy for future tasks, while also antici- pating the behavior of other sensors that may be collaborat- ing or competing. To address these interdependent decisions, we employ a game-theoretic framework. Unlike simple heuristic meth- ods that ignore future resource allocation or complex ap- proaches like reinforcement learning that may be too costly to implement, game theory provides equilibrium guaran- tees.",
      "type": "sliding_window",
      "tokens": 112
    },
    {
      "text": "Unlike simple heuristic meth- ods that ignore future resource allocation or complex ap- proaches like reinforcement learning that may be too costly to implement, game theory provides equilibrium guaran- tees. By modeling each sensor as a rational player aiming to optimize its own long-term utility, we achieve stable, coop- erative equilibria where no sensor can improve its outcome through unilateral deviation. This strategic equilibrium un- derpins both training and inference participation decisions, ensuring that the sensors most likely to improve the global \n1 \n055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 \nmodel—given their energy, data quality, and network condi- tions—are the ones that engage.",
      "type": "sliding_window",
      "tokens": 269
    },
    {
      "text": "This strategic equilibrium un- derpins both training and inference participation decisions, ensuring that the sensors most likely to improve the global \n1 \n055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 \nmodel—given their energy, data quality, and network condi- tions—are the ones that engage. To refine the global model parameters without incurring continuous on-edge training costs, we adopt a federated learning paradigm adapted to EH-WSNs. Rather than rely- ing on persistent, centralized updates or continuous feder- ated aggregation, we perform  periodic  or  equilibrium-driven fine-tuning rounds.",
      "type": "sliding_window",
      "tokens": 243
    },
    {
      "text": "Rather than rely- ing on persistent, centralized updates or continuous feder- ated aggregation, we perform  periodic  or  equilibrium-driven fine-tuning rounds. These updates occur only when sensors have sufficient energy to participate meaningfully, guided by the game-theoretic equilibrium strategy. By integrating the game-theoretic approach with federated learning principles, we reduce communication overhead and ensure that contri- butions to model updates come from sensors best positioned to improve accuracy under energy constraints and uncertain availability.",
      "type": "sliding_window",
      "tokens": 114
    },
    {
      "text": "By integrating the game-theoretic approach with federated learning principles, we reduce communication overhead and ensure that contri- butions to model updates come from sensors best positioned to improve accuracy under energy constraints and uncertain availability. Our key contributions are as follows: \n•  Game-Theoretic Participation Strategy:  We develop a novel game-theoretic model for EH-WSNs that applies to both training and inference phases. This model balances anticipated energy availability, local data quality, and global benefit to establish stable and cooperative equilibria, opti- mizing the energy-accuracy trade-offs.",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "This model balances anticipated energy availability, local data quality, and global benefit to establish stable and cooperative equilibria, opti- mizing the energy-accuracy trade-offs. •  Federated Learning Integration:  We introduce a fed- erated learning-based framework tailored for intermittent participation and heterogeneous data quality. Unlike con- tinuous on-edge training, we employ periodic or triggered fine-tuning sessions aligned with equilibrium strategies, en- suring robust and progressively improving global models.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "Unlike con- tinuous on-edge training, we employ periodic or triggered fine-tuning sessions aligned with equilibrium strategies, en- suring robust and progressively improving global models. •  Joint Optimization of Training and Inference:  Our unified solution aligns training participation decisions with inference needs. Sensors strategically decide when to ex- pend energy on local model updates and when to engage in inference tasks, ultimately maximizing their long-term contribution to the network’s performance.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Sensors strategically decide when to ex- pend energy on local model updates and when to engage in inference tasks, ultimately maximizing their long-term contribution to the network’s performance. •  Demonstrated Performance Gains:  Through simula- tions  (add simulation details later) , we show that our inte- grated framework outperforms baseline approaches—such as always-on participation or simplistic energy-based selec- tion—by achieving higher inference accuracy, lower energy consumption, and more sustainable long-term operation in EH-WSNs. By tackling the dual challenges of sensor unreliability and energy scarcity through a rigorous game-theoretic and fed- erated learning lens, our work addresses a critical gap in the design of sustainable, intelligent EH-WSNs.",
      "type": "sliding_window",
      "tokens": 182
    },
    {
      "text": "By tackling the dual challenges of sensor unreliability and energy scarcity through a rigorous game-theoretic and fed- erated learning lens, our work addresses a critical gap in the design of sustainable, intelligent EH-WSNs. This inte- grated framework is theoretically grounded, yet practical for a wide range of IoT applications, from remote wildlife mon- itoring to large-scale industrial status tracking and precision agriculture. The remainder of this paper is organized as follows.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "The remainder of this paper is organized as follows. In Sec- tion  3 , we present the system model, detailing the EH-WSN \nsetup and data capture process. In Section  4 , we introduce the game-theoretic model of sensor participation, motivat- ing our approach against simpler heuristics and discussing why equilibrium solutions are desirable.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "In Section  4 , we introduce the game-theoretic model of sensor participation, motivat- ing our approach against simpler heuristics and discussing why equilibrium solutions are desirable. Section  5  outlines the training and fine-tuning framework that integrates the equilibrium strategies into a federated learning paradigm. Fi- nally, Section  ?",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Fi- nally, Section  ? ? presents simulation results and Section  ?",
      "type": "sliding_window",
      "tokens": 19
    },
    {
      "text": "presents simulation results and Section  ? ? concludes with a discussion of limitations and future work.",
      "type": "sliding_window",
      "tokens": 22
    },
    {
      "text": "concludes with a discussion of limitations and future work. 2. Background and Related Work \nVery basic, just the outline, compact and make robust \n2.1.",
      "type": "sliding_window",
      "tokens": 32
    },
    {
      "text": "Background and Related Work \nVery basic, just the outline, compact and make robust \n2.1. Energy Harvesting Wireless Sensor Networks \nEnergy harvesting wireless sensor networks (EH-WSNs) have emerged as a sustainable solution for long-term envi- ronmental monitoring, infrastructure surveillance, and IoT applications ( ? ).",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "). By harnessing ambient energy sources such as solar, thermal, or kinetic energy, EH sensors can operate indefinitely without the need for battery replace- ment or external power supplies. However, the intermittent and unpredictable nature of harvested energy introduces significant challenges in maintaining reliable and consistent network performance ( ?",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "However, the intermittent and unpredictable nature of harvested energy introduces significant challenges in maintaining reliable and consistent network performance ( ? ). The unreliability of individual EH sensors, due to fluctua- tions in energy availability, necessitates the deployment of a large number of inexpensive and potentially unreliable de- vices to ensure network robustness.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "The unreliability of individual EH sensors, due to fluctua- tions in energy availability, necessitates the deployment of a large number of inexpensive and potentially unreliable de- vices to ensure network robustness. This redundancy allows for continuous operation despite individual sensor failures or downtime. However, it also introduces complexities in coordinating sensor activities, managing energy resources, and ensuring efficient data collection and processing ( ?",
      "type": "sliding_window",
      "tokens": 99
    },
    {
      "text": "However, it also introduces complexities in coordinating sensor activities, managing energy resources, and ensuring efficient data collection and processing ( ? ). 2.2.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "2.2. Participation Strategies in EH-WSNs \nEfficient participation strategies are critical in EH-WSNs to optimize network performance while conserving limited energy resources. Traditional approaches often assume con- tinuous participation of all sensors, which is impractical in energy-constrained environments ( ?",
      "type": "sliding_window",
      "tokens": 68
    },
    {
      "text": "Traditional approaches often assume con- tinuous participation of all sensors, which is impractical in energy-constrained environments ( ? ). Some methods pro- pose selecting a subset of sensors based on energy levels or predefined schedules ( ?",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "Some methods pro- pose selecting a subset of sensors based on energy levels or predefined schedules ( ? ), but these can lead to suboptimal performance by not considering the sensors’ data quality or potential future contributions. Several works have explored adaptive participation strate- gies that consider energy harvesting rates, energy consump- tion patterns, and application-specific requirements ( ?",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "Several works have explored adaptive participation strate- gies that consider energy harvesting rates, energy consump- tion patterns, and application-specific requirements ( ? ). These strategies aim to balance energy expenditure with the need for timely and accurate data, often using heuristic or optimization-based approaches.",
      "type": "sliding_window",
      "tokens": 67
    },
    {
      "text": "These strategies aim to balance energy expenditure with the need for timely and accurate data, often using heuristic or optimization-based approaches. However, they may not fully exploit the potential for collaboration among sensors or account for the strategic interactions inherent in decen- tralized networks. 2 \n110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 \n2.3.",
      "type": "sliding_window",
      "tokens": 167
    },
    {
      "text": "2 \n110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 \n2.3. Game-Theoretic Models in Sensor Networks \nGame theory provides a powerful framework for modeling and analyzing strategic interactions in distributed systems, including sensor networks ( ? ).",
      "type": "sliding_window",
      "tokens": 146
    },
    {
      "text": "). In the context of EH-WSNs, game-theoretic models have been employed to design dis- tributed algorithms for resource allocation, power control, and cooperative communication ( ? ).",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "). Cooperative game theory has been used to encourage collab- oration among sensors to enhance network performance ( ? ).",
      "type": "sliding_window",
      "tokens": 29
    },
    {
      "text": "). Non-cooperative game models allow sensors to make au- tonomous decisions while considering the potential ac- tions of others, leading to equilibria that balance individ- ual utility with collective goals ( ? ).",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "). However, integrating game-theoretic participation strategies with machine learn- ing tasks, particularly in energy-harvesting environments, remains an area with limited exploration. 2.4.",
      "type": "sliding_window",
      "tokens": 41
    },
    {
      "text": "2.4. Federated Learning in Resource-Constrained Environments \nFederated learning enables multiple devices to collabora- tively train a global model without sharing raw data, pre- serving privacy and reducing communication overhead ( ? ).",
      "type": "sliding_window",
      "tokens": 52
    },
    {
      "text": "). While federated learning has gained significant attention in mobile and IoT devices, applying it to EH-WSNs presents unique challenges due to intermittent participation, limited computational capabilities, and variable data quality ( ? ).",
      "type": "sliding_window",
      "tokens": 50
    },
    {
      "text": "). Recent studies have begun to address federated learning in resource-constrained and unreliable networks. Strategies in- clude adaptive aggregation methods, energy-aware training schedules, and robustness to device dropouts ( ?",
      "type": "sliding_window",
      "tokens": 55
    },
    {
      "text": "Strategies in- clude adaptive aggregation methods, energy-aware training schedules, and robustness to device dropouts ( ? ). However, these approaches often assume some level of reliability or do not fully integrate energy harvesting dynamics into the learning process.",
      "type": "sliding_window",
      "tokens": 58
    },
    {
      "text": "However, these approaches often assume some level of reliability or do not fully integrate energy harvesting dynamics into the learning process. 2.5. Multi-View Learning and Collaborative Inference \nMulti-view learning leverages multiple sources or perspec- tives to improve learning performance ( ?",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "Multi-View Learning and Collaborative Inference \nMulti-view learning leverages multiple sources or perspec- tives to improve learning performance ( ? ). In EH-WSNs, sensors providing different views of the same scene can en- hance inference accuracy through collaborative processing.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "In EH-WSNs, sensors providing different views of the same scene can en- hance inference accuracy through collaborative processing. Techniques such as co-training, consensus learning, and en- semble methods have been explored to combine information from multiple sensors ( ? ).",
      "type": "sliding_window",
      "tokens": 62
    },
    {
      "text": "). Collaborative inference in sensor networks involves com- bining local inferences to achieve a global understanding of the environment ( ? ).",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "). Challenges include aligning hetero- geneous data, managing communication costs, and dealing with unreliable or missing inputs. Existing methods may not account for the energy constraints and participation vari- ability inherent in EH-WSNs.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "Existing methods may not account for the energy constraints and participation vari- ability inherent in EH-WSNs. 3. System Model \nWe consider a network of  N EH sensors  S = { s 1 , s 2 , .",
      "type": "sliding_window",
      "tokens": 54
    },
    {
      "text": "System Model \nWe consider a network of  N EH sensors  S = { s 1 , s 2 , . . .",
      "type": "sliding_window",
      "tokens": 33
    },
    {
      "text": ". , s N }  deployed to monitor a common scene. Each sensor observes the environment from a distinct van- tage point.",
      "type": "sliding_window",
      "tokens": 34
    },
    {
      "text": "Each sensor observes the environment from a distinct van- tage point. Time is slotted and indexed by  t  ∈ N . In each time slot, the network may perform an inference event, dur- ing which sensors have the opportunity to contribute data that enhances the accuracy of a global inference task, such as object detection or environmental classification.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "In each time slot, the network may perform an inference event, dur- ing which sensors have the opportunity to contribute data that enhances the accuracy of a global inference task, such as object detection or environmental classification. Each sensor  s i  harvests energy from ambient sources, such as solar or vibrational energy, resulting in a stochastically varying energy supply. We denote by  E i ( t )  the energy harvested by sensor  s i  during slot  t .",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "We denote by  E i ( t )  the energy harvested by sensor  s i  during slot  t . The sensor maintains an energy buffer whose state evolves as \nB i ( t  + 1) =  B i ( t ) +  E i ( t )  − e i ( t ) , \nwhere  B i ( t )  is the energy available at the beginning of slot  t , and  e i ( t )  is the energy expended during that slot. Predicting future energy intake is challenging, so each sensor employs an estimator   ˆ E i ( t  + 1)  to anticipate its upcoming energy resources.",
      "type": "sliding_window",
      "tokens": 156
    },
    {
      "text": "Predicting future energy intake is challenging, so each sensor employs an estimator   ˆ E i ( t  + 1)  to anticipate its upcoming energy resources. Incorporating uncertainty-aware models or robust estimation techniques is beyond the scope of this paper. Prior to deployment, a global inference model  f θ  is trained offline on representative data and distributed to each sensor.",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "Prior to deployment, a global inference model  f θ  is trained offline on representative data and distributed to each sensor. This model maps sensor observations to inference outputs. Although parameters  θ  can theoretically be updated through on-edge training, we assume that frequent retraining in situ is prohibitively expensive given energy constraints.",
      "type": "sliding_window",
      "tokens": 70
    },
    {
      "text": "Although parameters  θ  can theoretically be updated through on-edge training, we assume that frequent retraining in situ is prohibitively expensive given energy constraints. Thus, θ  remains largely static post-deployment. Sensors focus on inference using their local copies of  f θ .",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "Sensors focus on inference using their local copies of  f θ . However, the sub- sequent training framework, discussed in Section  5 , allows for occasional fine-tuning of  θ  based on equilibrium-driven participation, thereby refining the model to better suit the operational dynamics of the network. In each inference event, sensors decide whether to partic- ipate.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "In each inference event, sensors decide whether to partic- ipate. If sensor  s i  participates at time  t , it must capture data at a chosen Signal-to-Noise Ratio (SNR), process the data using  f θ , and transmit the result to a designated  lead sensor . High-SNR data capture improves the sensor’s con- tribution to global accuracy but consumes more energy.",
      "type": "sliding_window",
      "tokens": 103
    },
    {
      "text": "High-SNR data capture improves the sensor’s con- tribution to global accuracy but consumes more energy. Let e cap ( SNR )  denote the energy required for capture at a given SNR level. We assume a monotonic relationship: higher SNR increases both the capture cost and the expected ac- curacy contribution.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "We assume a monotonic relationship: higher SNR increases both the capture cost and the expected ac- curacy contribution. This assumption simplifies the model by ensuring that better data quality unequivocally enhances inference performance, while also making the energy ex- penditure predictable. In addition to capture costs, partici- pation incurs inference computation cost  e inf  and communi- cation cost  e comm .",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "In addition to capture costs, partici- pation incurs inference computation cost  e inf  and communi- cation cost  e comm . Thus, if sensor  s i  participates with SNR SNR i ( t ) , its total energy expenditure is \ne i ( t ) =  e cap ( SNR i ( t )) +  e inf  +  e comm \n3 \n165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 \nThe improvement in global inference accuracy due to sen- sor  s i  is denoted by  ∆ A i ( t ) . This quantity depends on SNR i ( t )  and on the data contributed by other participating sensors, as their combined perspectives shape the overall result.",
      "type": "sliding_window",
      "tokens": 279
    },
    {
      "text": "This quantity depends on SNR i ( t )  and on the data contributed by other participating sensors, as their combined perspectives shape the overall result. While  ∆ A i ( t )  may not be known precisely, we as- sume that each sensor can estimate its expected contribution based on historical observations and current conditions. Ev- ery inference event presents a binary decision for sensor  s i : a i ( t )  ∈{ Participate (P) ,  Not Participate (NP) } .",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Ev- ery inference event presents a binary decision for sensor  s i : a i ( t )  ∈{ Participate (P) ,  Not Participate (NP) } . Choosing P  involves selecting an SNR level, incurring energy costs, and aiming to improve global accuracy. Choosing  NP  con- serves energy but forfeits any contribution or associated reward.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Choosing  NP  con- serves energy but forfeits any contribution or associated reward. Because sensors have limited energy and the net- work may operate for extended periods, each sensor must consider the future implications of its current actions. The interplay of multiple sensors making similar decisions un- der uncertainty and energy constraints naturally suggests a game-theoretic framework for modeling their interactions.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "The interplay of multiple sensors making similar decisions un- der uncertainty and energy constraints naturally suggests a game-theoretic framework for modeling their interactions. 4. Game-Theoretic Modeling \n4.1.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "Game-Theoretic Modeling \n4.1. Motivation for Game Theory over Simpler Methods \nWhile heuristic methods—such as always selecting the top- k  sensors based on current energy levels—or greedy algo- rithms that maximize immediate utility might offer straight- forward solutions, they fall short in addressing the strate- gic and long-term dynamics inherent in EH-WSNs. These simplistic approaches ignore the interdependencies among sensor decisions and fail to account for future resource allo- cation, potentially leading to suboptimal performance over time.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "These simplistic approaches ignore the interdependencies among sensor decisions and fail to account for future resource allo- cation, potentially leading to suboptimal performance over time. For instance, always selecting the highest-energy sen- sors can rapidly deplete their energy reserves, reducing the network’s resilience during critical future events. Alternatively, reinforcement learning or Markov Decision Process-based approaches could adaptively learn partici- pation policies that consider both immediate rewards and future states.",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "Alternatively, reinforcement learning or Markov Decision Process-based approaches could adaptively learn partici- pation policies that consider both immediate rewards and future states. However, these methods often require exten- sive training data, significant computational resources, and complex communication protocols, which may be impracti- cal for resource-constrained sensor nodes. In contrast, a game-theoretic framework provides equilib- rium guarantees, ensuring stable and cooperative partici- pation strategies.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "In contrast, a game-theoretic framework provides equilib- rium guarantees, ensuring stable and cooperative partici- pation strategies. By modeling each sensor as a rational player optimizing its own utility, we can derive participation patterns that are robust against unilateral deviations. This stability is crucial for maintaining long-term network per- formance without necessitating continuous recalibration or extensive communication overhead.",
      "type": "sliding_window",
      "tokens": 94
    },
    {
      "text": "This stability is crucial for maintaining long-term network per- formance without necessitating continuous recalibration or extensive communication overhead. 4.2. Utility Function Definition \nWe define a utility function  U i ( t )  for each sensor  s i  that en- capsulates the trade-off between accuracy gains and energy \nexpenditures, as well as future opportunities.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "Utility Function Definition \nWe define a utility function  U i ( t )  for each sensor  s i  that en- capsulates the trade-off between accuracy gains and energy \nexpenditures, as well as future opportunities. The utility function is designed to reflect both immediate rewards and long-term sustainability. Immediate Rewards and Penalties: Let  γ >  0  be a scal- ing factor that translates accuracy gains into utility rewards.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Immediate Rewards and Penalties: Let  γ >  0  be a scal- ing factor that translates accuracy gains into utility rewards. When sensor  s i  participates ( a i ( t ) =  P ) and contributes cor- rectly to the inference task, it receives a reward proportional to the improvement in global accuracy, denoted by  ∆ A i ( t ) : \nR i ( t ) = \n   \n  \nγ  ·  ∆ A i ( t ) , if  a i ( t ) =  P and correct inference , − δ, if  a i ( t ) =  P and incorrect inference , − η, if  a i ( t ) =  NP . Here,  δ >  0  penalizes incorrect participation, discourag- ing sensors from submitting low-quality data, while  η >  0 penalizes non-participation to prevent perpetual abstention.",
      "type": "sliding_window",
      "tokens": 251
    },
    {
      "text": "Here,  δ >  0  penalizes incorrect participation, discourag- ing sensors from submitting low-quality data, while  η >  0 penalizes non-participation to prevent perpetual abstention. Importantly, we set  η > δ , ensuring that consistently opting out is more detrimental than occasionally providing inaccu- rate data. Energy Costs and Future Utility: Participation incurs energy costs, reducing the sensor’s capacity for future tasks.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Energy Costs and Future Utility: Participation incurs energy costs, reducing the sensor’s capacity for future tasks. Additionally, sensors must consider the discounted value of future utility. Let  C i ( t )  represent the cost component: \nC i ( t ) =  e i ( t ) +  βV i ( t  + 1) , \nwhere  e i ( t )  is the total energy expenditure for participa- tion, encompassing data capture, inference computation, and communication: \ne i ( t ) =  e cap ( SNR i ( t )) +  e inf  +  e comm .",
      "type": "sliding_window",
      "tokens": 153
    },
    {
      "text": "Let  C i ( t )  represent the cost component: \nC i ( t ) =  e i ( t ) +  βV i ( t  + 1) , \nwhere  e i ( t )  is the total energy expenditure for participa- tion, encompassing data capture, inference computation, and communication: \ne i ( t ) =  e cap ( SNR i ( t )) +  e inf  +  e comm . The discount factor  β  ∈ [0 ,  1)  captures how sensors value future utility, with  V i ( t  + 1)  representing the expected fu- ture utility given current decisions and predicted energy availability   ˆ E i ( t  + 1) . Overall Utility Function: Combining immediate rewards and costs, the overall utility function for sensor  s i  at time  t is: U i ( t ) =  R i ( t )  − C i ( t ) .",
      "type": "sliding_window",
      "tokens": 228
    },
    {
      "text": "Overall Utility Function: Combining immediate rewards and costs, the overall utility function for sensor  s i  at time  t is: U i ( t ) =  R i ( t )  − C i ( t ) . This utility function effectively balances the benefits of participation against the associated costs and future oppor- tunities, guiding sensors to make strategic decisions that optimize their long-term contributions to the network. Nash Equilibrium and Stability: A Nash equilibrium (NE) represents a stable action profile  a ∗ ( t )  where no sensor can unilaterally improve its utility by deviating from its current strategy: \nU i ( a ∗ i   ( t ) ,  a ∗ − i ( t ))  ≥ U i ( a i ( t ) ,  a ∗ − i ( t )) ∀ a i ( t ) ,  ∀ i.",
      "type": "sliding_window",
      "tokens": 227
    },
    {
      "text": "Nash Equilibrium and Stability: A Nash equilibrium (NE) represents a stable action profile  a ∗ ( t )  where no sensor can unilaterally improve its utility by deviating from its current strategy: \nU i ( a ∗ i   ( t ) ,  a ∗ − i ( t ))  ≥ U i ( a i ( t ) ,  a ∗ − i ( t )) ∀ a i ( t ) ,  ∀ i. 4 \n220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 \nAchieving an NE ensures that sensor participation patterns are stable; once equilibrium is reached, no single sensor ben- efits from changing its participation decision independently. This stability is critical for maintaining consistent network performance and energy sustainability over time.",
      "type": "sliding_window",
      "tokens": 292
    },
    {
      "text": "This stability is critical for maintaining consistent network performance and energy sustainability over time. Distributed Best-Response Algorithm: To realize the NE, we propose a distributed best-response algorithm where each sensor iteratively adjusts its action based on the current state and the expected actions of others. The algorithm operates as follows: \nAlgorithm 1  Distributed Best-Response Participation Algo- rithm \n1:  Input: Current energies  B i ( t ) , predicted harvest ˆ E i ( t  + 1) , parameters  γ, δ, η, β , and energy costs e cap ( · ) , e inf , e comm .",
      "type": "sliding_window",
      "tokens": 173
    },
    {
      "text": "The algorithm operates as follows: \nAlgorithm 1  Distributed Best-Response Participation Algo- rithm \n1:  Input: Current energies  B i ( t ) , predicted harvest ˆ E i ( t  + 1) , parameters  γ, δ, η, β , and energy costs e cap ( · ) , e inf , e comm . 2:  At each inference event: 3:  Each sensor  s i  receives a solicitation from the lead sensor and forms an estimate of  ∆ A i ( t )  given potential SNR choices and expected actions of others. 4:  For each action candidate  a i ( t )  ∈{ P ,  NP } , the sensor computes the expected utility: \nU   a i ( t ) i =  E [ R i ( t )]  − E [ e i ( t )]  − β E [ V i ( t  + 1)] , \nwhere the expectations are taken over uncertainties in correctness, SNR impact, and future energy.",
      "type": "sliding_window",
      "tokens": 261
    },
    {
      "text": "4:  For each action candidate  a i ( t )  ∈{ P ,  NP } , the sensor computes the expected utility: \nU   a i ( t ) i =  E [ R i ( t )]  − E [ e i ( t )]  − β E [ V i ( t  + 1)] , \nwhere the expectations are taken over uncertainties in correctness, SNR impact, and future energy. 5:  If  U   P i   ≥ U  NP i and  B i ( t )  ≥ e cap ( SNR i ( t ))+ e inf + e comm , then  s i  chooses P. Otherwise, it chooses NP. 6:  After all sensors decide, the action profile  a ( t )  is real- ized, and energies are updated: \nB i ( t  + 1) =  B i ( t ) +  E i ( t )  − e i ( t ) .",
      "type": "sliding_window",
      "tokens": 246
    },
    {
      "text": "6:  After all sensors decide, the action profile  a ( t )  is real- ized, and energies are updated: \nB i ( t  + 1) =  B i ( t ) +  E i ( t )  − e i ( t ) . 7:  Sensors iterate this process at each inference event, re- fining their estimates and converging to stable action patterns. Sensors employ this best-response mechanism, continuously updating their participation decisions based on the evolving network state and the actions of other sensors.",
      "type": "sliding_window",
      "tokens": 132
    },
    {
      "text": "Sensors employ this best-response mechanism, continuously updating their participation decisions based on the evolving network state and the actions of other sensors. Over repeated iterations, under suitable conditions, this process converges to a Nash equilibrium where participation strategies are mutually optimal. Existence and Convergence of Equilibrium: \nTheorem 4.1.",
      "type": "sliding_window",
      "tokens": 81
    },
    {
      "text": "Existence and Convergence of Equilibrium: \nTheorem 4.1. Suppose that each utility function  U i ( t )  is non-decreasing in  ∆ A i ( t ) , that energy constraints and dis- counting ensure diminishing marginal returns for repeated deviations, and that sensors have consistent estimation of ∆ A i ( t )  and   ˆ E i ( t  + 1) . Then, the iterative best-response updates described in Algorithm  1  converge to a Nash equi- librium action profile  a ∗ ( t ) .",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "Then, the iterative best-response updates described in Algorithm  1  converge to a Nash equi- librium action profile  a ∗ ( t ) . Proof of Theorem  4.1 : The proof constructs a potential function  Φ( a ( t )) =   P N i =1   U i ( a i ( t ) ,  a − i ( t ))  that strictly in- creases whenever a sensor makes a profitable unilateral deviation. Since utilities are bounded (due to finite en- ergy and limited accuracy gains) and returns diminish over time, no infinite sequence of profitable deviations is possi- ble.",
      "type": "sliding_window",
      "tokens": 177
    },
    {
      "text": "Since utilities are bounded (due to finite en- ergy and limited accuracy gains) and returns diminish over time, no infinite sequence of profitable deviations is possi- ble. Hence, the best-response dynamics must terminate at a profile where no sensor can improve its utility alone, i.e., a Nash equilibrium. A complete formal proof, including all technical conditions, is provided in Appendix  B .",
      "type": "sliding_window",
      "tokens": 105
    },
    {
      "text": "A complete formal proof, including all technical conditions, is provided in Appendix  B . Guidelines for Hyperparameter Selection: The param- eters  γ ,  δ , and  η  critically influence sensor behavior by dictating the trade-offs between participation rewards, penal- ties for incorrect submissions, and deterrents against non- participation. Detailed guidelines for selecting these hy- perparameters are provided in Appendix  A .",
      "type": "sliding_window",
      "tokens": 104
    },
    {
      "text": "Detailed guidelines for selecting these hy- perparameters are provided in Appendix  A . Briefly, these parameters should be chosen to ensure that:  (1)  γ  suffi- ciently incentivizes correct participation without leading to excessive energy expenditure. (2)  δ  appropriately penalizes incorrect inferences, discouraging low-quality data contri- butions.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "(2)  δ  appropriately penalizes incorrect inferences, discouraging low-quality data contri- butions. (3)  η > δ  to prevent sensors from consistently abstaining, thereby promoting overall network engagement. These guidelines help in balancing immediate utility gains with long-term energy sustainability, ensuring that the game- theoretic model drives desirable participation behaviors.",
      "type": "sliding_window",
      "tokens": 82
    },
    {
      "text": "These guidelines help in balancing immediate utility gains with long-term energy sustainability, ensuring that the game- theoretic model drives desirable participation behaviors. 5. Training and Aggregation Framework \nHaving established the equilibrium participation strategies and the underlying reward-based utility functions, we now consider the training process that fine-tunes the global in- ference model  θ  ∈ R d   within this EH, multi-sensor envi- ronment.",
      "type": "sliding_window",
      "tokens": 100
    },
    {
      "text": "Training and Aggregation Framework \nHaving established the equilibrium participation strategies and the underlying reward-based utility functions, we now consider the training process that fine-tunes the global in- ference model  θ  ∈ R d   within this EH, multi-sensor envi- ronment. Initially,  θ  is pre-trained offline and deployed to all sensors, enabling them to perform basic inference tasks. However, this initial model may not be optimally adapted to the complex operational reality of the network, where sensors strategically choose SNR levels, participate inter- mittently according to equilibrium strategies, and generate data distributions that deviate from the original training set.",
      "type": "sliding_window",
      "tokens": 149
    },
    {
      "text": "However, this initial model may not be optimally adapted to the complex operational reality of the network, where sensors strategically choose SNR levels, participate inter- mittently according to equilibrium strategies, and generate data distributions that deviate from the original training set. The goal of the training process is to adjust  θ  to these condi- tions, effectively  fine-tuning  the model to the nonstationary data distribution  D  induced by the sensors’ equilibrium be- haviors. At equilibrium, sensors strike a balance between accurate data contribution and energy conservation, result- ing in a stable pattern of participation and SNR choices.",
      "type": "sliding_window",
      "tokens": 139
    },
    {
      "text": "At equilibrium, sensors strike a balance between accurate data contribution and energy conservation, result- ing in a stable pattern of participation and SNR choices. Over time, this induces a stationary, albeit non-trivial, effec- tive data distribution  D . Learning Approach: Our approach diverges from classi- cal Learning paradigms.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "Learning Approach: Our approach diverges from classi- cal Learning paradigms. We adopt a hybrid strategy where periodic or event-triggered updates refine the model parame- ters based on equilibrium-driven data collection. This hybrid \n5 \n275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 \napproach mitigates the high communication overhead and energy consumption, making it more suitable for resource- constrained EH-WSNs.",
      "type": "sliding_window",
      "tokens": 187
    },
    {
      "text": "This hybrid \n5 \n275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 \napproach mitigates the high communication overhead and energy consumption, making it more suitable for resource- constrained EH-WSNs. While traditional methods require persistent communication between sensors and the aggre- gator, our framework leverages the established equilibrium strategies to determine optimal times for model updates. By aligning update events with periods when sensors are most likely to participate meaningfully, we ensure that the global model is refined efficiently without imposing excessive en- ergy demands on the sensors.",
      "type": "sliding_window",
      "tokens": 214
    },
    {
      "text": "By aligning update events with periods when sensors are most likely to participate meaningfully, we ensure that the global model is refined efficiently without imposing excessive en- ergy demands on the sensors. Training Objective and Regularization: To enhance ro- bustness and efficiency, we incorporate regularizers that pe- nalize undesirable model properties. Specifically, we intro- duce two regularizers:  (1)  Ω SNR ( θ ) : Encourages the model to maintain performance across varying SNR levels, pre- venting over-reliance on high-SNR data.",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "Specifically, we intro- duce two regularizers:  (1)  Ω SNR ( θ ) : Encourages the model to maintain performance across varying SNR levels, pre- venting over-reliance on high-SNR data. (2)  Ω complexity ( θ ) : Controls model complexity, reducing computational and communication overheads by discouraging overly intricate models. The full training objective is formulated as: \nJ ( θ ) =  L ( θ ) +  λ 1 Ω SNR ( θ ) +  λ 2 Ω complexity ( θ ) , \nwhere L ( θ ) =  E ( x,y ) ∼D [ ℓ ( f θ ( x ) , y )] , \nand  λ 1 , λ 2  ≥ 0  are hyperparameters that balance accuracy, robustness, and efficiency.",
      "type": "sliding_window",
      "tokens": 203
    },
    {
      "text": "The full training objective is formulated as: \nJ ( θ ) =  L ( θ ) +  λ 1 Ω SNR ( θ ) +  λ 2 Ω complexity ( θ ) , \nwhere L ( θ ) =  E ( x,y ) ∼D [ ℓ ( f θ ( x ) , y )] , \nand  λ 1 , λ 2  ≥ 0  are hyperparameters that balance accuracy, robustness, and efficiency. Gradient Computation and Backpropagation: Integrat- ing regularizers into the training process is straightforward due to their known closed-form gradients. During back- propagation, each sensor computes the gradient of the loss function  ∇ ℓ ( f θ ( x ) , y )  with respect to  θ  based on locally available samples from  D .",
      "type": "sliding_window",
      "tokens": 205
    },
    {
      "text": "During back- propagation, each sensor computes the gradient of the loss function  ∇ ℓ ( f θ ( x ) , y )  with respect to  θ  based on locally available samples from  D . Additionally, the gradients of the regularizers,  ∇ Ω SNR ( θ )  and  ∇ Ω complexity ( θ ) , are analyt- ically derived and added to the local gradient estimates. Since both regularizers are convex and smooth, their in- clusion ensures that the overall objective  J ( θ )  maintains desirable convexity and smoothness properties, facilitating the convergence of stochastic gradient descent (SGD).",
      "type": "sliding_window",
      "tokens": 161
    },
    {
      "text": "Since both regularizers are convex and smooth, their in- clusion ensures that the overall objective  J ( θ )  maintains desirable convexity and smoothness properties, facilitating the convergence of stochastic gradient descent (SGD). Periodic Equilibrium-Aware Training: Model updates are performed periodically at an aggregator node that col- lects gradient estimates from participating sensors. Partici- pation during training follows the same equilibrium model: sensors decide whether to compute and send gradients based on their current energy states, predicted future utilities, and the established reward structure.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "Partici- pation during training follows the same equilibrium model: sensors decide whether to compute and send gradients based on their current energy states, predicted future utilities, and the established reward structure. By aggregating these gradi- ent updates over multiple training rounds, the aggregator ap- proximates the gradient  ∇ J ( θ )  and performs an SGD step. Our training framework is encapsulated in Algorithm  2 , which outlines the periodic equilibrium-aware training pro- cess.",
      "type": "sliding_window",
      "tokens": 119
    },
    {
      "text": "Our training framework is encapsulated in Algorithm  2 , which outlines the periodic equilibrium-aware training pro- cess. This training framework is intrinsically linked to the game-theoretic participation strategies. Sensors participate \nin training rounds based on their equilibrium-driven de- cisions, ensuring that gradient updates are contributed by those sensors most capable and willing to improve the global model.",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "Sensors participate \nin training rounds based on their equilibrium-driven de- cisions, ensuring that gradient updates are contributed by those sensors most capable and willing to improve the global model. This alignment minimizes unnecessary energy ex- penditure and maximizes the efficacy of each training round. Algorithm 2  Periodic Equilibrium-Aware Training Algo- rithm \n1:  Initialization:  Initialize  θ 0 .",
      "type": "sliding_window",
      "tokens": 97
    },
    {
      "text": "Algorithm 2  Periodic Equilibrium-Aware Training Algo- rithm \n1:  Initialization:  Initialize  θ 0 . Broadcast  θ 0  to all sensors. Set a diminishing step-size schedule  { α k } k ≥ 0 .",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "Set a diminishing step-size schedule  { α k } k ≥ 0 . 2:  for  each training round  k  = 0 ,  1 ,  2 , . .",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": ". . do 3: The aggregator signals that a training update round is imminent.",
      "type": "sliding_window",
      "tokens": 22
    },
    {
      "text": "do 3: The aggregator signals that a training update round is imminent. 4: Sensors decide on participation. Participation in- volves: \n1.",
      "type": "sliding_window",
      "tokens": 35
    },
    {
      "text": "Participation in- volves: \n1. Determining if they have enough energy and incentive (based on the established equilibrium strategy and reward parameters  γ, δ, η ). 2.",
      "type": "sliding_window",
      "tokens": 42
    },
    {
      "text": "2. If participating: capturing data at their cho- sen SNR, performing inference, and computing local gradients  ∇ ℓ ( f θ k ( x ) , y )  on their locally available samples drawn from  D . 3.",
      "type": "sliding_window",
      "tokens": 61
    },
    {
      "text": "3. Adding regularizer gradients  λ 1 ∇ Ω SNR ( θ k )  and λ 2 ∇ Ω complexity ( θ k ) . 5: A subset of sensors, determined by the equilibrium, send their gradient estimates to the aggregator.",
      "type": "sliding_window",
      "tokens": 65
    },
    {
      "text": "5: A subset of sensors, determined by the equilibrium, send their gradient estimates to the aggregator. 6: The aggregator forms an unbiased estimate of the full gradient: \nb ∇ J ( θ k ) =  b ∇ L ( θ k ) +  λ 1 ∇ Ω SNR ( θ k ) \n+ λ 2 ∇ Ω complexity ( θ k ) . 7: Update model parameters: \nθ k +1  =  θ k  − α k   b ∇ J ( θ k ) .",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "7: Update model parameters: \nθ k +1  =  θ k  − α k   b ∇ J ( θ k ) . 8: Broadcast  θ k +1  to all sensors. 9:  end for \nRegularizers and SGD Convergence: The chosen reg- ularizers  Ω SNR ( θ )  and  Ω complexity ( θ )  are both convex and smooth, with known closed-form gradients.",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "9:  end for \nRegularizers and SGD Convergence: The chosen reg- ularizers  Ω SNR ( θ )  and  Ω complexity ( θ )  are both convex and smooth, with known closed-form gradients. This property guarantees that the inclusion of regularizers does not com- promise the convexity or smoothness of the overall objective J ( θ ) . Consequently, the stochastic gradient descent (SGD) updates retain their convergence properties, ensuring that the training process reliably optimizes  J ( θ ) .",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "Consequently, the stochastic gradient descent (SGD) updates retain their convergence properties, ensuring that the training process reliably optimizes  J ( θ ) . 6 \n330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 \n6. Implementation and Evaluation \n6.1.",
      "type": "sliding_window",
      "tokens": 155
    },
    {
      "text": "Implementation and Evaluation \n6.1. Discussions and Limitations \n7. Conclusion \nThis should finish at 8 pages.",
      "type": "sliding_window",
      "tokens": 22
    },
    {
      "text": "Conclusion \nThis should finish at 8 pages. Impact Statement \nAuthors are  required  to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences. This statement should be in an unnumbered section at the end of the paper (co- located with Acknowledgments – the two may appear in either order, but both must be before References), and does not count toward the paper page limit.",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "This statement should be in an unnumbered section at the end of the paper (co- located with Acknowledgments – the two may appear in either order, but both must be before References), and does not count toward the paper page limit. In many cases, where the ethical impacts and expected societal implications are those that are well established when advancing the field of Machine Learning, substantial discussion is not required, and a simple statement such as the following will suffice: \n“This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.” \nThe above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review.",
      "type": "sliding_window",
      "tokens": 187
    },
    {
      "text": "There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.” \nThe above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review. References \nA. Guidelines for Hyperparameter Selection and Bounds on Reward Parameters \nThe parameters  γ ,  δ,  and  η  govern the reward structure of the proposed framework, influencing whether sensors participate consistently, over-participate and waste energy, or abstain altogether.",
      "type": "sliding_window",
      "tokens": 134
    },
    {
      "text": "Guidelines for Hyperparameter Selection and Bounds on Reward Parameters \nThe parameters  γ ,  δ,  and  η  govern the reward structure of the proposed framework, influencing whether sensors participate consistently, over-participate and waste energy, or abstain altogether. This appendix provides a systematic approach to selecting these parameters, including formal bounds, practical heuristics, and an algorithmic procedure to explore suitable values. Conceptual Role of Parameters \nThe scalar  γ >  0  represents the reward scaling for correct participation.",
      "type": "sliding_window",
      "tokens": 123
    },
    {
      "text": "Conceptual Role of Parameters \nThe scalar  γ >  0  represents the reward scaling for correct participation. If  γ  is too low, sensors will not have suffi- cient incentive to expend energy on high-SNR captures. If γ  is too high, sensors may waste energy attempting difficult inferences.",
      "type": "sliding_window",
      "tokens": 74
    },
    {
      "text": "If γ  is too high, sensors may waste energy attempting difficult inferences. The parameter  δ >  0  penalizes incorrect in- ferences, discouraging reckless submissions of low-quality data. The parameter  η >  0  penalizes non-participation, ensuring that sensors do not remain idle indefinitely.",
      "type": "sliding_window",
      "tokens": 77
    },
    {
      "text": "The parameter  η >  0  penalizes non-participation, ensuring that sensors do not remain idle indefinitely. As discussed, maintaining  η > δ  encourages sensors to at least attempt participation rather than always remain offline. Formal Bounds and Conditions \nTo ensure balanced behavior, it is helpful to relate  γ, δ,  and η  to typical values of accuracy improvement and energy costs.",
      "type": "sliding_window",
      "tokens": 89
    },
    {
      "text": "Formal Bounds and Conditions \nTo ensure balanced behavior, it is helpful to relate  γ, δ,  and η  to typical values of accuracy improvement and energy costs. Accuracy Gains and Costs: Let  ∆ A min  and  ∆ A max  de- note the minimum and maximum expected accuracy im- provements from any sensor’s participation. Let  e max total   = e max cap   + e inf   + e comm   represent the maximum energy cost (for a chosen SNR mode).",
      "type": "sliding_window",
      "tokens": 111
    },
    {
      "text": "Let  e max total   = e max cap   + e inf   + e comm   represent the maximum energy cost (for a chosen SNR mode). A baseline condition that ensures correct participation can overcome occasional penalties is: \nγ  ·  ∆ A min  > δ  +  e max total   . This inequality implies that even in a worst-case scenario for accuracy gain, the net expected benefit of correct par- ticipation surpasses the sum of potential incorrect penalties and energy costs.",
      "type": "sliding_window",
      "tokens": 110
    },
    {
      "text": "This inequality implies that even in a worst-case scenario for accuracy gain, the net expected benefit of correct par- ticipation surpasses the sum of potential incorrect penalties and energy costs. Without this condition, sensors might find participation systematically unprofitable. Non-Participation and Equilibrium: Since  η > δ , we ensure that sensors prefer risking occasional incorrect infer- ences over consistently abstaining.",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "Non-Participation and Equilibrium: Since  η > δ , we ensure that sensors prefer risking occasional incorrect infer- ences over consistently abstaining. A suitable gap might be chosen so that: δ < η  ≤ δ  +  c, \nfor some small  c >  0 . Choosing  c  relative to typical gains, say  c  ≈ 0 .",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Choosing  c  relative to typical gains, say  c  ≈ 0 . 1 · γ · ∆ A max , helps maintain a moderate deterrent against non-participation without forcing sensors to always participate. 7 \n385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 \nEnergy Preservation: If  γ  is too large, sensors might not value future energy at all.",
      "type": "sliding_window",
      "tokens": 183
    },
    {
      "text": "7 \n385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 \nEnergy Preservation: If  γ  is too large, sensors might not value future energy at all. To prevent myopic strategies, one can limit  γ  such that continuously investing in high-SNR captures does not dominate long-term considerations. For example: \nγ  ·  ∆ A max  < η  +  margin , \nwhere  margin  accounts for future opportunities and energy savings.",
      "type": "sliding_window",
      "tokens": 191
    },
    {
      "text": "For example: \nγ  ·  ∆ A max  < η  +  margin , \nwhere  margin  accounts for future opportunities and energy savings. A small  margin  ensures sensors do not always expend maximal energy for short-term gains. Practical Hyperparameter Tuning Strategies \n1.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "Practical Hyperparameter Tuning Strategies \n1. Baseline Ratios:  Start with ratios that link  γ  to typical accuracy gains and set  δ, η  based on fractions or multiples of  γ  ·  ∆ A min  or  γ  ·  ∆ A max . 2.",
      "type": "sliding_window",
      "tokens": 64
    },
    {
      "text": "2. Iterative Refinement:  Use simulation or small-scale ex- perimental runs to refine parameters. If sensors rarely par- ticipate, increase  γ  or decrease  η .",
      "type": "sliding_window",
      "tokens": 45
    },
    {
      "text": "If sensors rarely par- ticipate, increase  γ  or decrease  η . If sensors over-exert themselves, reduce  γ  or increase  δ, η . 3.",
      "type": "sliding_window",
      "tokens": 43
    },
    {
      "text": "3. Adaptive Tuning:  If conditions change over time, adjust γ, δ,  and  η  dynamically based on observed participation rates, accuracy levels, and energy depletion patterns. Exploration Algorithm \nAlgorithm  3  outlines a systematic approach to exploring suit- able hyperparameter values.",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "Exploration Algorithm \nAlgorithm  3  outlines a systematic approach to exploring suit- able hyperparameter values. It combines theoretical bounds with empirical evaluation, guiding the search toward stable and efficient equilibria. The above guidelines and the explo- ration algorithm provide a structured approach to selecting and refining  γ, δ,  and  η .",
      "type": "sliding_window",
      "tokens": 88
    },
    {
      "text": "The above guidelines and the explo- ration algorithm provide a structured approach to selecting and refining  γ, δ,  and  η . By starting from theoretically in- formed baseline conditions and iteratively refining through simulation-based feedback, it is possible to reach a stable set of parameters that promotes balanced participation, discour- ages perpetual abstention, and prevents excessive energy expenditure. Regular re-tuning may be warranted as oper- ating conditions, energy harvesting patterns, or accuracy requirements evolve over the network’s lifetime.",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "Regular re-tuning may be warranted as oper- ating conditions, energy harvesting patterns, or accuracy requirements evolve over the network’s lifetime. B. Equilibrium Existence and Convergence with Reward-Based Utility \nIn this appendix, we provide a detailed and formal proof that the best-response dynamics, incorporating the newly defined reward-based utility functions, converge to a Nash equilibrium (NE). We first restate the key assumptions and the utility model.",
      "type": "sliding_window",
      "tokens": 113
    },
    {
      "text": "We first restate the key assumptions and the utility model. We then show that the iterative best- response updates cannot lead to infinite improvement cycles, implying the existence of an NE. Finally, we prove that the equilibrium is reached under the given assumptions.",
      "type": "sliding_window",
      "tokens": 56
    },
    {
      "text": "Finally, we prove that the equilibrium is reached under the given assumptions. Algorithm 3  Hyperparameter Exploration for Reward Pa- rameters \n1:  Inputs: Estimates  ∆ A min ,  ∆ A max , energy costs e max cap   , e inf , e comm , initial guesses  γ 0 , δ 0 , η 0 , and tuning increments  ∆ γ ,  ∆ δ ,  ∆ η . 2:  Compute  e max total   =  e max cap   +  e inf   +  e comm .",
      "type": "sliding_window",
      "tokens": 147
    },
    {
      "text": "2:  Compute  e max total   =  e max cap   +  e inf   +  e comm . 3:  Ensure baseline feasibility: If  γ 0  ·  ∆ A min  ≤ δ 0  +  e max total   , increase  γ 0  until this condition is met. 4:  Set  η 0  > δ 0 .",
      "type": "sliding_window",
      "tokens": 83
    },
    {
      "text": "4:  Set  η 0  > δ 0 . Start with  η 0  =  δ 0  +  c , where  c  is a small positive number. If preliminary tests show insufficient participation, slightly increase  η 0 .",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "If preliminary tests show insufficient participation, slightly increase  η 0 . If participation is overly aggressive, reduce  γ 0  or increase  δ 0 . 5:  Simulation-Refinement Loop: 6:  for  k  = 1 ,  2 , .",
      "type": "sliding_window",
      "tokens": 60
    },
    {
      "text": "5:  Simulation-Refinement Loop: 6:  for  k  = 1 ,  2 , . . .",
      "type": "sliding_window",
      "tokens": 28
    },
    {
      "text": ". , K  (number of refinement iterations) do 7: Run a simulation or small-scale test deployment us- ing the current  γ k , δ k , η k . 8: Measure key indicators: participation rate, average energy depletion rate, frequency of incorrect infer- ences, and overall inference accuracy.",
      "type": "sliding_window",
      "tokens": 85
    },
    {
      "text": "8: Measure key indicators: participation rate, average energy depletion rate, frequency of incorrect infer- ences, and overall inference accuracy. 9: if  participation is too low (e.g.,  < p min ) or sensors remain idle too often  then 10: Increase  γ k  ← γ k  + ∆ γ  or decrease  η k  ← η k  − ∆ η . 11: else if  participation is too high, leading to frequent energy depletion  then 12: Decrease  γ k  ← γ k − ∆ γ  or increase  δ k  ← δ k +∆ δ to discourage high-risk attempts.",
      "type": "sliding_window",
      "tokens": 160
    },
    {
      "text": "11: else if  participation is too high, leading to frequent energy depletion  then 12: Decrease  γ k  ← γ k − ∆ γ  or increase  δ k  ← δ k +∆ δ to discourage high-risk attempts. 13: else if  incorrect inferences are prevalent  then 14: Increase  δ k  ← δ k  + ∆ δ  to penalize low-quality submissions more strongly. 15: end if 16: Check feasibility conditions again to ensure no viola- tion of baseline inequalities.",
      "type": "sliding_window",
      "tokens": 130
    },
    {
      "text": "15: end if 16: Check feasibility conditions again to ensure no viola- tion of baseline inequalities. 17: If performance metrics (accuracy, sustainability) are satisfactory, terminate. Otherwise, continue refine- ment.",
      "type": "sliding_window",
      "tokens": 53
    },
    {
      "text": "Otherwise, continue refine- ment. 18:  end for \nRestatement of the Utility Function and Assumptions \nRecall that at each inference event  t , each sensor  s i  chooses an action  a i ( t )  ∈{ P ,  NP } . The chosen action profile is a ( t ) = ( a 1 ( t ) , .",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "The chosen action profile is a ( t ) = ( a 1 ( t ) , . . .",
      "type": "sliding_window",
      "tokens": 31
    },
    {
      "text": ". , a N ( t )) . The immediate reward for sensor  s i  is defined as: \nR i ( t ) = \n     \n    \nγ  ·  ∆ A i ( t ) , if  a i ( t ) =  P and inference is correct , \n− δ, if  a i ( t ) =  P and inference is incorrect , \n− η, if  a i ( t ) =  NP .",
      "type": "sliding_window",
      "tokens": 138
    },
    {
      "text": "The immediate reward for sensor  s i  is defined as: \nR i ( t ) = \n     \n    \nγ  ·  ∆ A i ( t ) , if  a i ( t ) =  P and inference is correct , \n− δ, if  a i ( t ) =  P and inference is incorrect , \n− η, if  a i ( t ) =  NP . Here,  γ >  0  scales the reward for correct participation, δ >  0  penalizes incorrect inference, and  η >  0  penalizes non-participation, with  η > δ  ensuring that remaining idle \n8 \n440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 \nis more penalizing than at least attempting participation. The cost incorporates energy consumption and future op- portunities.",
      "type": "sliding_window",
      "tokens": 315
    },
    {
      "text": "The cost incorporates energy consumption and future op- portunities. Let  e i ( t )  be the energy expenditure for sensor s i  if it participates at time  t , accounting for capture, infer- ence, and communication costs. Introduce a discount factor β  ∈ [0 ,  1) , and let  V i ( t  + 1)  represent the expected future utility of sensor  s i  given its current decisions and predicted energy availability.",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Introduce a discount factor β  ∈ [0 ,  1) , and let  V i ( t  + 1)  represent the expected future utility of sensor  s i  given its current decisions and predicted energy availability. The cost is: \nC i ( t ) =  e i ( t ) +  βV i ( t  + 1) . The overall utility is: \nU i ( t ) =  R i ( t )  − C i ( t ) .",
      "type": "sliding_window",
      "tokens": 116
    },
    {
      "text": "The overall utility is: \nU i ( t ) =  R i ( t )  − C i ( t ) . We assume that  ∆ A i ( t )  is non-decreasing in the quality of sensor  s i ’s data (e.g., higher SNR yields higher  ∆ A i ( t ) ). We also assume that energy resources, accuracy gains, and reward/penalty parameters are finite and bounded, and that sensors have consistent estimation mechanisms for  ∆ A i ( t ) and   ˆ E i ( t  + 1) .",
      "type": "sliding_window",
      "tokens": 149
    },
    {
      "text": "We also assume that energy resources, accuracy gains, and reward/penalty parameters are finite and bounded, and that sensors have consistent estimation mechanisms for  ∆ A i ( t ) and   ˆ E i ( t  + 1) . Potential Function Construction \nTo prove convergence, we define a potential function that reflects the collective utility of the sensor network: \nΦ( a ( t )) = \nN X \ni =1 U i ( a i ( t ) ,  a − i ( t )) . Since  U i ( t ) =  R i ( t )  − C i ( t ) , we have: \nΦ( a ( t )) = \nN X \ni =1 [ R i ( t )  − C i ( t )] .",
      "type": "sliding_window",
      "tokens": 203
    },
    {
      "text": "Since  U i ( t ) =  R i ( t )  − C i ( t ) , we have: \nΦ( a ( t )) = \nN X \ni =1 [ R i ( t )  − C i ( t )] . The terms  R i ( t )  depend on the chosen actions and correct- ness of inferences. Due to bounded  γ, δ,  and  η , and the fact that  ∆ A i ( t )  and energy costs are bounded, each  U i ( t ) is finite.",
      "type": "sliding_window",
      "tokens": 151
    },
    {
      "text": "Due to bounded  γ, δ,  and  η , and the fact that  ∆ A i ( t )  and energy costs are bounded, each  U i ( t ) is finite. Thus,  Φ( a ( t ))  is also finite for all feasible action profiles. Monotonicity of the Potential Function \nConsider a unilateral deviation by a single sensor  s j  from an action  a j ( t )  to a different action  a ′ j ( t ) .",
      "type": "sliding_window",
      "tokens": 127
    },
    {
      "text": "Monotonicity of the Potential Function \nConsider a unilateral deviation by a single sensor  s j  from an action  a j ( t )  to a different action  a ′ j ( t ) . Such a deviation affects only  U j ( t ) , not the utilities of other sensors directly in a one-step change. If this deviation is profitable for sensor s j , we have: \nU j ( a ′ j ( t ) ,  a − j ( t ))  > U j ( a j ( t ) ,  a − j ( t )) .",
      "type": "sliding_window",
      "tokens": 160
    },
    {
      "text": "If this deviation is profitable for sensor s j , we have: \nU j ( a ′ j ( t ) ,  a − j ( t ))  > U j ( a j ( t ) ,  a − j ( t )) . Because the other sensors’ utilities do not change instan- taneously by  s j ’s unilateral action, the increment in  U j ( t ) results in: \nΦ( a − j ( t ) , a ′ j ( t ))  − Φ( a ( t )) = \nU j ( a ′ j ( t ) ,  a − j ( t ))  − U j ( a j ( t ) ,  a − j ( t ))  >  0 . Thus, any unilateral profitable deviation increases  Φ( a ( t )) .",
      "type": "sliding_window",
      "tokens": 243
    },
    {
      "text": "Thus, any unilateral profitable deviation increases  Φ( a ( t )) . Boundedness and Impossibility of Infinite Improvement Sequences \nSince all utilities are bounded (due to finite  γ, δ, η,  bounded ∆ A i ( t ) , and bounded energy resources), there exists a finite upper bound  Φ max  such that: \nΦ( a ( t ))  ≤ Φ max ∀ a ( t ) . Suppose, for contradiction, that there exists an infinite se- quence of unilateral profitable deviations.",
      "type": "sliding_window",
      "tokens": 144
    },
    {
      "text": "Suppose, for contradiction, that there exists an infinite se- quence of unilateral profitable deviations. Each such de- viation strictly increases  Φ( a ( t )) . Because  Φ  is bounded above by  Φ max , only a finite number of increments can occur before no further improvements are possible.",
      "type": "sliding_window",
      "tokens": 75
    },
    {
      "text": "Because  Φ  is bounded above by  Φ max , only a finite number of increments can occur before no further improvements are possible. This contradiction shows that no infinite improvement sequence can occur. Existence of a Nash Equilibrium \nSince no infinite sequence of profitable unilateral deviations can occur, the best-response dynamics must terminate in a state where no sensor can unilaterally improve its utility.",
      "type": "sliding_window",
      "tokens": 91
    },
    {
      "text": "Existence of a Nash Equilibrium \nSince no infinite sequence of profitable unilateral deviations can occur, the best-response dynamics must terminate in a state where no sensor can unilaterally improve its utility. By definition, this state is a Nash equilibrium  a ∗ ( t ) : \nU i ( a ∗ i   ( t ) ,  a ∗ − i ( t ))  ≥ U i ( a i ( t ) ,  a ∗ − i ( t )) ∀ a i ( t ) ,  ∀ i. Thus, the existence of a Nash equilibrium follows directly from the finiteness of utilities, the monotonicity of  Φ , and the impossibility of infinite improvement sequences.",
      "type": "sliding_window",
      "tokens": 191
    },
    {
      "text": "Thus, the existence of a Nash equilibrium follows directly from the finiteness of utilities, the monotonicity of  Φ , and the impossibility of infinite improvement sequences. Convergence to the Nash Equilibrium \nThe final step is to show that the iterative best-response dy- namics indeed converge to the NE identified above. Since each sensor’s best-response update seeks to maximize its own utility, sensors will continue to deviate as long as prof- itable deviations exist.",
      "type": "sliding_window",
      "tokens": 129
    },
    {
      "text": "Since each sensor’s best-response update seeks to maximize its own utility, sensors will continue to deviate as long as prof- itable deviations exist. Our argument shows that profitable deviations must terminate. Under the assumptions that ∆ A i ( t )  is non-decreasing and that sensors have consistent energy and accuracy estimates, no cyclical behavior can persist.",
      "type": "sliding_window",
      "tokens": 90
    },
    {
      "text": "Under the assumptions that ∆ A i ( t )  is non-decreasing and that sensors have consistent energy and accuracy estimates, no cyclical behavior can persist. A cycle would imply an infinite sequence of im- provements or a return to a previously visited state without improvement, which cannot occur since profitable devia- tions strictly increase  Φ( a ( t )) . The presence of the discount factor  β  further stabilizes the process.",
      "type": "sliding_window",
      "tokens": 106
    },
    {
      "text": "The presence of the discount factor  β  further stabilizes the process. With  β  ∈ [0 ,  1) , sensors value future utility less than immediate utility. This discounting ensures diminish- ing returns for postponing beneficial participation or indefi- nitely waiting for ideal conditions.",
      "type": "sliding_window",
      "tokens": 66
    },
    {
      "text": "This discounting ensures diminish- ing returns for postponing beneficial participation or indefi- nitely waiting for ideal conditions. As a result, sensors do not continually defer improvements, preventing complex long-term cycles. 9 \n495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 \nBecause the best-response process eliminates profitable de- viations step by step and cannot cycle indefinitely, the action profile sequence generated by iterative best responses con- verges to the NE.",
      "type": "sliding_window",
      "tokens": 209
    },
    {
      "text": "9 \n495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 \nBecause the best-response process eliminates profitable de- viations step by step and cannot cycle indefinitely, the action profile sequence generated by iterative best responses con- verges to the NE. We have shown that, with the reward-based utility function that includes correct participation rewards ( γ  ·  ∆ A i ( t ) ), penalties for incorrect inferences ( δ ), and penalties for non- participation ( η ), the best-response dynamics lead to a Nash equilibrium. The proof relies on constructing a potential function  Φ  that is strictly increased by unilateral profitable deviations and bounded above.",
      "type": "sliding_window",
      "tokens": 259
    },
    {
      "text": "The proof relies on constructing a potential function  Φ  that is strictly increased by unilateral profitable deviations and bounded above. The impossibility of infinite improvement sequences guarantees the existence of an NE, and the assumptions on monotonicity, boundedness, and discounting ensure that the iterative best-response process converges to this equilibrium. □ \nC. Proof of Convergence for the Equilibrium-Aware Training Process \nIn this appendix, we provide a comprehensive and detailed proof of the convergence theorem stated in the main text.",
      "type": "sliding_window",
      "tokens": 131
    },
    {
      "text": "□ \nC. Proof of Convergence for the Equilibrium-Aware Training Process \nIn this appendix, we provide a comprehensive and detailed proof of the convergence theorem stated in the main text. We also elaborate on how the new loss function, the introduced regularizers, and their gradients integrate into the backprop- agation and stochastic gradient descent (SGD) steps. Addi- tionally, we discuss bounds on the newly introduced hyper- parameters and provide guidelines for selecting them.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "Addi- tionally, we discuss bounds on the newly introduced hyper- parameters and provide guidelines for selecting them. Problem Setting and Notation \nWe consider a global inference model  f θ  :  X →Y  parame- terized by  θ  ∈ R d . The model’s performance is measured by a loss function  ℓ :  Y × Y → R ≥ 0  that is convex in  θ  for any fixed input-label pair  ( x, y ) .",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "The model’s performance is measured by a loss function  ℓ :  Y × Y → R ≥ 0  that is convex in  θ  for any fixed input-label pair  ( x, y ) . The model operates in an energy-harvesting wireless sensor network (EH-WSN) envi- ronment where sensors participate strategically in inference tasks based on a game-theoretic equilibrium. Let  D  denote the effective data distribution induced by the equilibrium strategies of the sensors.",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "Let  D  denote the effective data distribution induced by the equilibrium strategies of the sensors. Under equilibrium conditions, the distribution  D  is stationary or at least sta- tionary over sufficiently large timescales. The expected loss is L ( θ ) =  E ( x,y ) ∼D [ ℓ ( f θ ( x ) , y )] .",
      "type": "sliding_window",
      "tokens": 87
    },
    {
      "text": "The expected loss is L ( θ ) =  E ( x,y ) ∼D [ ℓ ( f θ ( x ) , y )] . To enhance robustness and efficiency, we introduce two regularizers: \nΩ SNR ( θ ) and Ω complexity ( θ ) . Ω SNR ( θ )  encourages the model to perform reasonably well across varying SNR levels, while  Ω complexity ( θ )  penalizes overly complex models that might demand excessive energy or communication costs.",
      "type": "sliding_window",
      "tokens": 128
    },
    {
      "text": "Ω SNR ( θ )  encourages the model to perform reasonably well across varying SNR levels, while  Ω complexity ( θ )  penalizes overly complex models that might demand excessive energy or communication costs. Both are assumed convex and have bounded gradients. The final training objective is: \nJ ( θ ) =  L ( θ ) +  λ 1 Ω SNR ( θ ) +  λ 2 Ω complexity ( θ ) , \nwhere  λ 1 , λ 2  ≥ 0  are hyperparameters controlling the influ- ence of the regularizers.",
      "type": "sliding_window",
      "tokens": 141
    },
    {
      "text": "The final training objective is: \nJ ( θ ) =  L ( θ ) +  λ 1 Ω SNR ( θ ) +  λ 2 Ω complexity ( θ ) , \nwhere  λ 1 , λ 2  ≥ 0  are hyperparameters controlling the influ- ence of the regularizers. Our goal is to show that by running a diminishing step-size SGD on  J ( θ ) , using unbiased gradient estimates from the equilibrium distribution  D , the parameters  { θ k }  converge in expectation to a stationary point  θ ∗ of  J ( θ ) . Key Assumptions and Conditions \n1.",
      "type": "sliding_window",
      "tokens": 156
    },
    {
      "text": "Key Assumptions and Conditions \n1. Convexity of  ℓ . The loss  ℓ ( f θ ( x ) , y )  is convex in  θ .",
      "type": "sliding_window",
      "tokens": 47
    },
    {
      "text": "The loss  ℓ ( f θ ( x ) , y )  is convex in  θ . Consequently, the expected loss  L ( θ )  is also convex. 2.",
      "type": "sliding_window",
      "tokens": 49
    },
    {
      "text": "2. L -smoothness of  ℓ . There exists a constant  L >  0 such that for all  θ, θ ′ , \n∥∇ L ( θ )  −∇ L ( θ ′ ) ∥≤ L ∥ θ  − θ ′ ∥ .",
      "type": "sliding_window",
      "tokens": 71
    },
    {
      "text": "There exists a constant  L >  0 such that for all  θ, θ ′ , \n∥∇ L ( θ )  −∇ L ( θ ′ ) ∥≤ L ∥ θ  − θ ′ ∥ . This ensures that  L ( θ )  is Lipschitz-smooth. 3.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "3. Convexity and boundedness of regularizers. The regularizers  Ω SNR  and  Ω complexity  are convex in  θ , and their gradients are bounded.",
      "type": "sliding_window",
      "tokens": 46
    },
    {
      "text": "The regularizers  Ω SNR  and  Ω complexity  are convex in  θ , and their gradients are bounded. Let \n∥∇ Ω SNR ( θ ) ∥≤ G 1 , ∥∇ Ω complexity ( θ ) ∥≤ G 2 ∀ θ. 4.",
      "type": "sliding_window",
      "tokens": 69
    },
    {
      "text": "4. Stationary distribution  D . The equilibrium participa- tion strategies induce a stationary effective distribution D .",
      "type": "sliding_window",
      "tokens": 24
    },
    {
      "text": "The equilibrium participa- tion strategies induce a stationary effective distribution D . Over sufficiently large timescales, the system does not drift away from this equilibrium, and samples  ( x, y ) can be considered drawn i.i.d. from  D .",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "Unbiased gradient estimates. When the aggregator requests a training update, a subset of sensors, deter- mined by equilibrium conditions, provide local gradi- ents. Although not all sensors participate every time, the equilibrium ensures a stable pattern of participation.",
      "type": "sliding_window",
      "tokens": 63
    },
    {
      "text": "Although not all sensors participate every time, the equilibrium ensures a stable pattern of participation. Averaged over multiple rounds, the collected gradients form an unbiased estimator   b ∇ L ( θ )  of  ∇ L ( θ ) : \nE [ b ∇ L ( θ )] =  ∇ L ( θ ) . Since the regularizers are deterministic, their gradients ∇ Ω SNR ( θ )  and  ∇ Ω complexity ( θ )  do not introduce bias.",
      "type": "sliding_window",
      "tokens": 121
    },
    {
      "text": "Since the regularizers are deterministic, their gradients ∇ Ω SNR ( θ )  and  ∇ Ω complexity ( θ )  do not introduce bias. Integration of Regularizers in Backpropagation and SGD \nDuring the training iteration  k : \n1. Forward pass:  Each participating sensor collects data ( x, y )  and evaluates  ℓ ( f θ k ( x ) , y ) .",
      "type": "sliding_window",
      "tokens": 108
    },
    {
      "text": "Forward pass:  Each participating sensor collects data ( x, y )  and evaluates  ℓ ( f θ k ( x ) , y ) . 10 \n550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 \n2. Backward pass:  The sensor computes  ∇ θ ℓ ( f θ k ( x ) , y ) via standard backpropagation.",
      "type": "sliding_window",
      "tokens": 193
    },
    {
      "text": "Backward pass:  The sensor computes  ∇ θ ℓ ( f θ k ( x ) , y ) via standard backpropagation. To incorporate regu- larizers, the sensor (or the aggregator after collecting updates) adds  λ 1 ∇ Ω SNR ( θ k )  and  λ 2 ∇ Ω complexity ( θ k ) . These gradients are computed analytically since the regularizers are explicit, differentiable functions of  θ .",
      "type": "sliding_window",
      "tokens": 122
    },
    {
      "text": "These gradients are computed analytically since the regularizers are explicit, differentiable functions of  θ . 3. Aggregation:  The aggregator averages the received gradients: b ∇ J ( θ k ) =  b ∇ L ( θ k )+ λ 1 ∇ Ω SNR ( θ k )+ λ 2 ∇ Ω complexity ( θ k ) .",
      "type": "sliding_window",
      "tokens": 102
    },
    {
      "text": "Aggregation:  The aggregator averages the received gradients: b ∇ J ( θ k ) =  b ∇ L ( θ k )+ λ 1 ∇ Ω SNR ( θ k )+ λ 2 ∇ Ω complexity ( θ k ) . Since  E [ b ∇ L ( θ k )] = ∇ L ( θ k ) , we also have E [ b ∇ J ( θ k )] =  ∇ J ( θ k ) . 4.",
      "type": "sliding_window",
      "tokens": 137
    },
    {
      "text": "4. Update step:  With a chosen step size  α k , \nθ k +1  =  θ k  − α k   b ∇ J ( θ k ) . Diminishing Step-Size and Convergence Results \nClassical convex optimization theory (see Bottou et al.",
      "type": "sliding_window",
      "tokens": 78
    },
    {
      "text": "Diminishing Step-Size and Convergence Results \nClassical convex optimization theory (see Bottou et al. (2018) or Nemirovski et al. (2009)) states that for con- vex, Lipschitz-smooth objectives and unbiased gradient oracles, SGD converges to a stationary point if the step sizes  { α k }  decrease at an appropriate rate.",
      "type": "sliding_window",
      "tokens": 101
    },
    {
      "text": "(2009)) states that for con- vex, Lipschitz-smooth objectives and unbiased gradient oracles, SGD converges to a stationary point if the step sizes  { α k }  decrease at an appropriate rate. A common choice is  α k  = 1 / √ \nk , but any diminishing sequence with P \nk   α k  =  ∞ and  P \nk   α 2 k   <  ∞ works. Under these conditions, we have: \nlim k →∞ E [ J ( θ k )] =  J ( θ ∗ ) and lim k →∞ E [ ∥∇ J ( θ k ) ∥ ] = 0 .",
      "type": "sliding_window",
      "tokens": 169
    },
    {
      "text": "Under these conditions, we have: \nlim k →∞ E [ J ( θ k )] =  J ( θ ∗ ) and lim k →∞ E [ ∥∇ J ( θ k ) ∥ ] = 0 . This implies  θ k  converges in expectation to a stationary point  θ ∗ of  J ( θ ) . Equilibrium Stability and Impact on Stationarity \nThe key subtlety is that  D  depends on equilibrium strategies.",
      "type": "sliding_window",
      "tokens": 115
    },
    {
      "text": "Equilibrium Stability and Impact on Stationarity \nThe key subtlety is that  D  depends on equilibrium strategies. However, the equilibrium ensures a stable operating regime where sensor behaviors—and thus  D —do not change dras- tically over time. This stability allows us to treat  D  as effectively fixed for the purpose of the asymptotic analy- sis.",
      "type": "sliding_window",
      "tokens": 86
    },
    {
      "text": "This stability allows us to treat  D  as effectively fixed for the purpose of the asymptotic analy- sis. If  D  were to drift significantly, standard SGD results would not directly apply. The equilibrium prevents such non-stationary behavior in the long run.",
      "type": "sliding_window",
      "tokens": 59
    },
    {
      "text": "The equilibrium prevents such non-stationary behavior in the long run. Furthermore, since the regularizers are deterministic and have bounded gradients, they do not add pathological con- ditions to the optimization landscape. They may alter the shape of  J ( θ ) , encouraging certain regions of parameter space, but they do not prevent convergence.",
      "type": "sliding_window",
      "tokens": 79
    },
    {
      "text": "They may alter the shape of  J ( θ ) , encouraging certain regions of parameter space, but they do not prevent convergence. On the con- trary, they may help by smoothing out undesirable minima or limiting model complexity. Bounding and Selecting Hyperparameters  λ 1 , λ 2 \nThe choice of  λ 1  and  λ 2  affects the curvature of  J ( θ )  and can influence convergence speed and the location of  θ ∗ .",
      "type": "sliding_window",
      "tokens": 109
    },
    {
      "text": "Bounding and Selecting Hyperparameters  λ 1 , λ 2 \nThe choice of  λ 1  and  λ 2  affects the curvature of  J ( θ )  and can influence convergence speed and the location of  θ ∗ . Some guidelines include: \n1. Start with small values of  λ 1  and  λ 2  to avoid over- whelming the primary loss  L ( θ ) .",
      "type": "sliding_window",
      "tokens": 93
    },
    {
      "text": "Start with small values of  λ 1  and  λ 2  to avoid over- whelming the primary loss  L ( θ ) . Gradually increase them if the model relies too heavily on high-SNR data or becomes too complex. 2.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "2. Ensure  λ 1  ≤ c 1 G 1   and  λ 2  ≤ c 2 G 2   for some constants c 1 , c 2  >  0 , to prevent excessively large gradients due to the regularizers. 3.",
      "type": "sliding_window",
      "tokens": 57
    },
    {
      "text": "3. Tune  λ 1 , λ 2  based on validation performance. If the model overfits high-SNR data, increase  λ 1 .",
      "type": "sliding_window",
      "tokens": 36
    },
    {
      "text": "If the model overfits high-SNR data, increase  λ 1 . If it be- comes too large and slow to run, increase  λ 2 . By keeping  λ 1 , λ 2  within reasonable bounds, we ensure that the modified gradient   b ∇ J ( θ )  remains well-behaved, preserving the conditions for SGD convergence.",
      "type": "sliding_window",
      "tokens": 84
    },
    {
      "text": "By keeping  λ 1 , λ 2  within reasonable bounds, we ensure that the modified gradient   b ∇ J ( θ )  remains well-behaved, preserving the conditions for SGD convergence. We have shown that under the stated assump- tions—convexity and smoothness of ℓ , convexity and bounded gradients of  Ω SNR  and  Ω complexity , stationarity of  D  induced by equilibrium strategies, and unbiased gradient estimates—the diminishing step-size SGD applied to  J ( θ )  converges in expectation to a stationary point  θ ∗ . The equilibrium ensures  D  remains stable, allowing classi- cal stochastic optimization theory to hold.",
      "type": "sliding_window",
      "tokens": 167
    },
    {
      "text": "The equilibrium ensures  D  remains stable, allowing classi- cal stochastic optimization theory to hold. The regularizers, being convex and with bounded gradients, integrate seam- lessly into the backpropagation and SGD updates, shaping the optimization landscape but not invalidating convergence properties. Proper selection and tuning of  λ 1 , λ 2  help main- tain stable and robust training dynamics.",
      "type": "sliding_window",
      "tokens": 95
    },
    {
      "text": "Proper selection and tuning of  λ 1 , λ 2  help main- tain stable and robust training dynamics. Thus, the proposed training process achieves a harmonious balance: it respects the strategic, energy-constrained envi- ronment (through equilibrium and game-theoretic consider- ations), while leveraging well-established convex optimiza- tion guarantees to ensure convergence of the global model parameters. 11",
      "type": "sliding_window",
      "tokens": 98
    },
    {
      "text": "We propose a comprehensive framework that integrates a game-theoretic partici- pation strategy with a federated learning approach tailored for EH-WSNs. Abstract \nEnergy-harvesting wireless sensor networks (EH- WSNs) offer sustainable solutions for large-scale IoT deployments but face challenges due to the unreliability and intermittent availability of in- dividual sensors. Our game-theoretic model enables sensors to make optimal participation de- cisions based on energy levels, data quality, and collective inference impact, fostering cooperative behavior while managing individual energy con- straints.",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "The federated learning framework ac- commodates intermittent participation and vari- able data quality, ensuring robust model training despite sensor unreliability. Our game-theoretic model enables sensors to make optimal participation de- cisions based on energy levels, data quality, and collective inference impact, fostering cooperative behavior while managing individual energy con- straints. Simulation results demonstrate that our integrated approach signif- icantly enhances inference accuracy and energy efficiency compared to traditional participation strategies.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Introduction \nThe rapid proliferation of the Internet of Things (IoT) has sparked a tremendous growth in the scale and diversity of sensor deployments, from smart homes to expansive in- dustrial and environmental monitoring systems. 1. Simulation results demonstrate that our integrated approach signif- icantly enhances inference accuracy and energy efficiency compared to traditional participation strategies.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "To address this, energy harvesting (EH) technolo- gies have emerged as a viable solution, enabling sensors to convert ambient energy (e.g., solar, thermal, or vibration) into electrical power. Introduction \nThe rapid proliferation of the Internet of Things (IoT) has sparked a tremendous growth in the scale and diversity of sensor deployments, from smart homes to expansive in- dustrial and environmental monitoring systems. As these networks continue to expand, sustaining continuous opera- tion in the face of finite power sources becomes a paramount concern.",
      "type": "sliding_window_shuffled",
      "tokens": 125,
      "augmented": true
    },
    {
      "text": "To address this, energy harvesting (EH) technolo- gies have emerged as a viable solution, enabling sensors to convert ambient energy (e.g., solar, thermal, or vibration) into electrical power. 1 Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. This approach promises perpetual, maintenance-free operation, significantly reducing environ- mental impact and long-term operational costs.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "1 Anonymous Institution, Anonymous City, Anonymous Region, Anonymous Country. Preliminary work. Correspondence to: Anonymous Author < anon.email@domain.com > .",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "Do not distribute. Under review by the International Conference on Machine Learning (ICML). Preliminary work.",
      "type": "sliding_window_shuffled",
      "tokens": 24,
      "augmented": true
    },
    {
      "text": "Do not distribute. Ambi- ent energy availability varies over time and space, leading to fluctuating sensor activity levels and intermittent participa- tion in both training and inference tasks. Despite these advantages, large-scale EH Wireless Sensor Networks (EH-WSNs) remain inherently uncertain.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "Some sensors may frequently become inactive or produce low-quality data due to energy scarcity or environmental noise. Ambi- ent energy availability varies over time and space, leading to fluctuating sensor activity levels and intermittent participa- tion in both training and inference tasks. Consequently, the mere presence of numerous EH sensors does not guaran- tee robust and reliable performance for complex tasks such as image recognition, acoustic surveillance, or precision agriculture monitoring.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "Consequently, the mere presence of numerous EH sensors does not guaran- tee robust and reliable performance for complex tasks such as image recognition, acoustic surveillance, or precision agriculture monitoring. Achieving accurate inference in these complex scenarios de- pends on effective coordination. Multiple sensors observing the same phenomenon from different angles can collectively provide more comprehensive and reliable insights than any single sensor could.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "However, requiring all sensors to partic- ipate at all times is impractical, as it drains energy reserves too quickly. Conversely, simplistic policies—such as se- lecting only the highest-energy sensors—ignore factors like data relevance, sensor quality, and the strategic implications of current participation on future network states. Multiple sensors observing the same phenomenon from different angles can collectively provide more comprehensive and reliable insights than any single sensor could.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Sensors must carefully balance immediate accuracy gains against conserving energy for future tasks, while also antici- pating the behavior of other sensors that may be collaborat- ing or competing. Conversely, simplistic policies—such as se- lecting only the highest-energy sensors—ignore factors like data relevance, sensor quality, and the strategic implications of current participation on future network states. This challenge motivates the need for intelligent, context- aware participation strategies that dynamically determine which sensors should engage during both the  training phase—where global model parameters are periodically fine-tuned or updated—and the  inference  phase—where newly observed data are aggregated to produce predictions.",
      "type": "sliding_window_shuffled",
      "tokens": 143,
      "augmented": true
    },
    {
      "text": "Unlike simple heuristic meth- ods that ignore future resource allocation or complex ap- proaches like reinforcement learning that may be too costly to implement, game theory provides equilibrium guaran- tees. Sensors must carefully balance immediate accuracy gains against conserving energy for future tasks, while also antici- pating the behavior of other sensors that may be collaborat- ing or competing. To address these interdependent decisions, we employ a game-theoretic framework.",
      "type": "sliding_window_shuffled",
      "tokens": 112,
      "augmented": true
    },
    {
      "text": "Unlike simple heuristic meth- ods that ignore future resource allocation or complex ap- proaches like reinforcement learning that may be too costly to implement, game theory provides equilibrium guaran- tees. By modeling each sensor as a rational player aiming to optimize its own long-term utility, we achieve stable, coop- erative equilibria where no sensor can improve its outcome through unilateral deviation. This strategic equilibrium un- derpins both training and inference participation decisions, ensuring that the sensors most likely to improve the global \n1 \n055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 \nmodel—given their energy, data quality, and network condi- tions—are the ones that engage.",
      "type": "sliding_window_shuffled",
      "tokens": 269,
      "augmented": true
    },
    {
      "text": "This strategic equilibrium un- derpins both training and inference participation decisions, ensuring that the sensors most likely to improve the global \n1 \n055 056 057 058 059 060 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 100 101 102 103 104 105 106 107 108 109 \nmodel—given their energy, data quality, and network condi- tions—are the ones that engage. Rather than rely- ing on persistent, centralized updates or continuous feder- ated aggregation, we perform  periodic  or  equilibrium-driven fine-tuning rounds. To refine the global model parameters without incurring continuous on-edge training costs, we adopt a federated learning paradigm adapted to EH-WSNs.",
      "type": "sliding_window_shuffled",
      "tokens": 243,
      "augmented": true
    },
    {
      "text": "By integrating the game-theoretic approach with federated learning principles, we reduce communication overhead and ensure that contri- butions to model updates come from sensors best positioned to improve accuracy under energy constraints and uncertain availability. These updates occur only when sensors have sufficient energy to participate meaningfully, guided by the game-theoretic equilibrium strategy. Rather than rely- ing on persistent, centralized updates or continuous feder- ated aggregation, we perform  periodic  or  equilibrium-driven fine-tuning rounds.",
      "type": "sliding_window_shuffled",
      "tokens": 114,
      "augmented": true
    },
    {
      "text": "Our key contributions are as follows: \n•  Game-Theoretic Participation Strategy:  We develop a novel game-theoretic model for EH-WSNs that applies to both training and inference phases. By integrating the game-theoretic approach with federated learning principles, we reduce communication overhead and ensure that contri- butions to model updates come from sensors best positioned to improve accuracy under energy constraints and uncertain availability. This model balances anticipated energy availability, local data quality, and global benefit to establish stable and cooperative equilibria, opti- mizing the energy-accuracy trade-offs.",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "This model balances anticipated energy availability, local data quality, and global benefit to establish stable and cooperative equilibria, opti- mizing the energy-accuracy trade-offs. Unlike con- tinuous on-edge training, we employ periodic or triggered fine-tuning sessions aligned with equilibrium strategies, en- suring robust and progressively improving global models. •  Federated Learning Integration:  We introduce a fed- erated learning-based framework tailored for intermittent participation and heterogeneous data quality.",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "Sensors strategically decide when to ex- pend energy on local model updates and when to engage in inference tasks, ultimately maximizing their long-term contribution to the network’s performance. •  Joint Optimization of Training and Inference:  Our unified solution aligns training participation decisions with inference needs. Unlike con- tinuous on-edge training, we employ periodic or triggered fine-tuning sessions aligned with equilibrium strategies, en- suring robust and progressively improving global models.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "By tackling the dual challenges of sensor unreliability and energy scarcity through a rigorous game-theoretic and fed- erated learning lens, our work addresses a critical gap in the design of sustainable, intelligent EH-WSNs. •  Demonstrated Performance Gains:  Through simula- tions  (add simulation details later) , we show that our inte- grated framework outperforms baseline approaches—such as always-on participation or simplistic energy-based selec- tion—by achieving higher inference accuracy, lower energy consumption, and more sustainable long-term operation in EH-WSNs. Sensors strategically decide when to ex- pend energy on local model updates and when to engage in inference tasks, ultimately maximizing their long-term contribution to the network’s performance.",
      "type": "sliding_window_shuffled",
      "tokens": 182,
      "augmented": true
    },
    {
      "text": "This inte- grated framework is theoretically grounded, yet practical for a wide range of IoT applications, from remote wildlife mon- itoring to large-scale industrial status tracking and precision agriculture. The remainder of this paper is organized as follows. By tackling the dual challenges of sensor unreliability and energy scarcity through a rigorous game-theoretic and fed- erated learning lens, our work addresses a critical gap in the design of sustainable, intelligent EH-WSNs.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "In Section  4 , we introduce the game-theoretic model of sensor participation, motivat- ing our approach against simpler heuristics and discussing why equilibrium solutions are desirable. In Sec- tion  3 , we present the system model, detailing the EH-WSN \nsetup and data capture process. The remainder of this paper is organized as follows.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Fi- nally, Section  ? Section  5  outlines the training and fine-tuning framework that integrates the equilibrium strategies into a federated learning paradigm. In Section  4 , we introduce the game-theoretic model of sensor participation, motivat- ing our approach against simpler heuristics and discussing why equilibrium solutions are desirable.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "Fi- nally, Section  ? ? presents simulation results and Section  ?",
      "type": "sliding_window_shuffled",
      "tokens": 19,
      "augmented": true
    },
    {
      "text": "? presents simulation results and Section  ? concludes with a discussion of limitations and future work.",
      "type": "sliding_window_shuffled",
      "tokens": 22,
      "augmented": true
    },
    {
      "text": "Background and Related Work \nVery basic, just the outline, compact and make robust \n2.1. concludes with a discussion of limitations and future work. 2.",
      "type": "sliding_window_shuffled",
      "tokens": 32,
      "augmented": true
    },
    {
      "text": "Background and Related Work \nVery basic, just the outline, compact and make robust \n2.1. ). Energy Harvesting Wireless Sensor Networks \nEnergy harvesting wireless sensor networks (EH-WSNs) have emerged as a sustainable solution for long-term envi- ronmental monitoring, infrastructure surveillance, and IoT applications ( ?",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "However, the intermittent and unpredictable nature of harvested energy introduces significant challenges in maintaining reliable and consistent network performance ( ? By harnessing ambient energy sources such as solar, thermal, or kinetic energy, EH sensors can operate indefinitely without the need for battery replace- ment or external power supplies. ).",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "). The unreliability of individual EH sensors, due to fluctua- tions in energy availability, necessitates the deployment of a large number of inexpensive and potentially unreliable de- vices to ensure network robustness. However, the intermittent and unpredictable nature of harvested energy introduces significant challenges in maintaining reliable and consistent network performance ( ?",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "The unreliability of individual EH sensors, due to fluctua- tions in energy availability, necessitates the deployment of a large number of inexpensive and potentially unreliable de- vices to ensure network robustness. However, it also introduces complexities in coordinating sensor activities, managing energy resources, and ensuring efficient data collection and processing ( ? This redundancy allows for continuous operation despite individual sensor failures or downtime.",
      "type": "sliding_window_shuffled",
      "tokens": 99,
      "augmented": true
    },
    {
      "text": "). However, it also introduces complexities in coordinating sensor activities, managing energy resources, and ensuring efficient data collection and processing ( ? 2.2.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "2.2. Traditional approaches often assume con- tinuous participation of all sensors, which is impractical in energy-constrained environments ( ? Participation Strategies in EH-WSNs \nEfficient participation strategies are critical in EH-WSNs to optimize network performance while conserving limited energy resources.",
      "type": "sliding_window_shuffled",
      "tokens": 68,
      "augmented": true
    },
    {
      "text": "Some methods pro- pose selecting a subset of sensors based on energy levels or predefined schedules ( ? Traditional approaches often assume con- tinuous participation of all sensors, which is impractical in energy-constrained environments ( ? ).",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "Several works have explored adaptive participation strate- gies that consider energy harvesting rates, energy consump- tion patterns, and application-specific requirements ( ? Some methods pro- pose selecting a subset of sensors based on energy levels or predefined schedules ( ? ), but these can lead to suboptimal performance by not considering the sensors’ data quality or potential future contributions.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "Several works have explored adaptive participation strate- gies that consider energy harvesting rates, energy consump- tion patterns, and application-specific requirements ( ? ). These strategies aim to balance energy expenditure with the need for timely and accurate data, often using heuristic or optimization-based approaches.",
      "type": "sliding_window_shuffled",
      "tokens": 67,
      "augmented": true
    },
    {
      "text": "However, they may not fully exploit the potential for collaboration among sensors or account for the strategic interactions inherent in decen- tralized networks. These strategies aim to balance energy expenditure with the need for timely and accurate data, often using heuristic or optimization-based approaches. 2 \n110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 \n2.3.",
      "type": "sliding_window_shuffled",
      "tokens": 167,
      "augmented": true
    },
    {
      "text": "2 \n110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 \n2.3. Game-Theoretic Models in Sensor Networks \nGame theory provides a powerful framework for modeling and analyzing strategic interactions in distributed systems, including sensor networks ( ? ).",
      "type": "sliding_window_shuffled",
      "tokens": 146,
      "augmented": true
    },
    {
      "text": "). ). In the context of EH-WSNs, game-theoretic models have been employed to design dis- tributed algorithms for resource allocation, power control, and cooperative communication ( ?",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "). ). Cooperative game theory has been used to encourage collab- oration among sensors to enhance network performance ( ?",
      "type": "sliding_window_shuffled",
      "tokens": 29,
      "augmented": true
    },
    {
      "text": "). Non-cooperative game models allow sensors to make au- tonomous decisions while considering the potential ac- tions of others, leading to equilibria that balance individ- ual utility with collective goals ( ? ).",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "2.4. ). However, integrating game-theoretic participation strategies with machine learn- ing tasks, particularly in energy-harvesting environments, remains an area with limited exploration.",
      "type": "sliding_window_shuffled",
      "tokens": 41,
      "augmented": true
    },
    {
      "text": "2.4. Federated Learning in Resource-Constrained Environments \nFederated learning enables multiple devices to collabora- tively train a global model without sharing raw data, pre- serving privacy and reducing communication overhead ( ? ).",
      "type": "sliding_window_shuffled",
      "tokens": 52,
      "augmented": true
    },
    {
      "text": "). ). While federated learning has gained significant attention in mobile and IoT devices, applying it to EH-WSNs presents unique challenges due to intermittent participation, limited computational capabilities, and variable data quality ( ?",
      "type": "sliding_window_shuffled",
      "tokens": 50,
      "augmented": true
    },
    {
      "text": "Recent studies have begun to address federated learning in resource-constrained and unreliable networks. Strategies in- clude adaptive aggregation methods, energy-aware training schedules, and robustness to device dropouts ( ? ).",
      "type": "sliding_window_shuffled",
      "tokens": 55,
      "augmented": true
    },
    {
      "text": "). Strategies in- clude adaptive aggregation methods, energy-aware training schedules, and robustness to device dropouts ( ? However, these approaches often assume some level of reliability or do not fully integrate energy harvesting dynamics into the learning process.",
      "type": "sliding_window_shuffled",
      "tokens": 58,
      "augmented": true
    },
    {
      "text": "However, these approaches often assume some level of reliability or do not fully integrate energy harvesting dynamics into the learning process. 2.5. Multi-View Learning and Collaborative Inference \nMulti-view learning leverages multiple sources or perspec- tives to improve learning performance ( ?",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "Multi-View Learning and Collaborative Inference \nMulti-view learning leverages multiple sources or perspec- tives to improve learning performance ( ? In EH-WSNs, sensors providing different views of the same scene can en- hance inference accuracy through collaborative processing. ).",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "In EH-WSNs, sensors providing different views of the same scene can en- hance inference accuracy through collaborative processing. ). Techniques such as co-training, consensus learning, and en- semble methods have been explored to combine information from multiple sensors ( ?",
      "type": "sliding_window_shuffled",
      "tokens": 62,
      "augmented": true
    },
    {
      "text": "). ). Collaborative inference in sensor networks involves com- bining local inferences to achieve a global understanding of the environment ( ?",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "Challenges include aligning hetero- geneous data, managing communication costs, and dealing with unreliable or missing inputs. Existing methods may not account for the energy constraints and participation vari- ability inherent in EH-WSNs. ).",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "System Model \nWe consider a network of  N EH sensors  S = { s 1 , s 2 , . Existing methods may not account for the energy constraints and participation vari- ability inherent in EH-WSNs. 3.",
      "type": "sliding_window_shuffled",
      "tokens": 54,
      "augmented": true
    },
    {
      "text": "System Model \nWe consider a network of  N EH sensors  S = { s 1 , s 2 , . . .",
      "type": "sliding_window_shuffled",
      "tokens": 33,
      "augmented": true
    },
    {
      "text": ", s N }  deployed to monitor a common scene. Each sensor observes the environment from a distinct van- tage point. .",
      "type": "sliding_window_shuffled",
      "tokens": 34,
      "augmented": true
    },
    {
      "text": "Time is slotted and indexed by  t  ∈ N . In each time slot, the network may perform an inference event, dur- ing which sensors have the opportunity to contribute data that enhances the accuracy of a global inference task, such as object detection or environmental classification. Each sensor observes the environment from a distinct van- tage point.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "In each time slot, the network may perform an inference event, dur- ing which sensors have the opportunity to contribute data that enhances the accuracy of a global inference task, such as object detection or environmental classification. Each sensor  s i  harvests energy from ambient sources, such as solar or vibrational energy, resulting in a stochastically varying energy supply. We denote by  E i ( t )  the energy harvested by sensor  s i  during slot  t .",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "We denote by  E i ( t )  the energy harvested by sensor  s i  during slot  t . Predicting future energy intake is challenging, so each sensor employs an estimator   ˆ E i ( t  + 1)  to anticipate its upcoming energy resources. The sensor maintains an energy buffer whose state evolves as \nB i ( t  + 1) =  B i ( t ) +  E i ( t )  − e i ( t ) , \nwhere  B i ( t )  is the energy available at the beginning of slot  t , and  e i ( t )  is the energy expended during that slot.",
      "type": "sliding_window_shuffled",
      "tokens": 156,
      "augmented": true
    },
    {
      "text": "Predicting future energy intake is challenging, so each sensor employs an estimator   ˆ E i ( t  + 1)  to anticipate its upcoming energy resources. Prior to deployment, a global inference model  f θ  is trained offline on representative data and distributed to each sensor. Incorporating uncertainty-aware models or robust estimation techniques is beyond the scope of this paper.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "This model maps sensor observations to inference outputs. Although parameters  θ  can theoretically be updated through on-edge training, we assume that frequent retraining in situ is prohibitively expensive given energy constraints. Prior to deployment, a global inference model  f θ  is trained offline on representative data and distributed to each sensor.",
      "type": "sliding_window_shuffled",
      "tokens": 70,
      "augmented": true
    },
    {
      "text": "Although parameters  θ  can theoretically be updated through on-edge training, we assume that frequent retraining in situ is prohibitively expensive given energy constraints. Thus, θ  remains largely static post-deployment. Sensors focus on inference using their local copies of  f θ .",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "Sensors focus on inference using their local copies of  f θ . However, the sub- sequent training framework, discussed in Section  5 , allows for occasional fine-tuning of  θ  based on equilibrium-driven participation, thereby refining the model to better suit the operational dynamics of the network. In each inference event, sensors decide whether to partic- ipate.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "In each inference event, sensors decide whether to partic- ipate. If sensor  s i  participates at time  t , it must capture data at a chosen Signal-to-Noise Ratio (SNR), process the data using  f θ , and transmit the result to a designated  lead sensor . High-SNR data capture improves the sensor’s con- tribution to global accuracy but consumes more energy.",
      "type": "sliding_window_shuffled",
      "tokens": 103,
      "augmented": true
    },
    {
      "text": "We assume a monotonic relationship: higher SNR increases both the capture cost and the expected ac- curacy contribution. High-SNR data capture improves the sensor’s con- tribution to global accuracy but consumes more energy. Let e cap ( SNR )  denote the energy required for capture at a given SNR level.",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "This assumption simplifies the model by ensuring that better data quality unequivocally enhances inference performance, while also making the energy ex- penditure predictable. In addition to capture costs, partici- pation incurs inference computation cost  e inf  and communi- cation cost  e comm . We assume a monotonic relationship: higher SNR increases both the capture cost and the expected ac- curacy contribution.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Thus, if sensor  s i  participates with SNR SNR i ( t ) , its total energy expenditure is \ne i ( t ) =  e cap ( SNR i ( t )) +  e inf  +  e comm \n3 \n165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 \nThe improvement in global inference accuracy due to sen- sor  s i  is denoted by  ∆ A i ( t ) . In addition to capture costs, partici- pation incurs inference computation cost  e inf  and communi- cation cost  e comm . This quantity depends on SNR i ( t )  and on the data contributed by other participating sensors, as their combined perspectives shape the overall result.",
      "type": "sliding_window_shuffled",
      "tokens": 279,
      "augmented": true
    },
    {
      "text": "While  ∆ A i ( t )  may not be known precisely, we as- sume that each sensor can estimate its expected contribution based on historical observations and current conditions. This quantity depends on SNR i ( t )  and on the data contributed by other participating sensors, as their combined perspectives shape the overall result. Ev- ery inference event presents a binary decision for sensor  s i : a i ( t )  ∈{ Participate (P) ,  Not Participate (NP) } .",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Choosing  NP  con- serves energy but forfeits any contribution or associated reward. Ev- ery inference event presents a binary decision for sensor  s i : a i ( t )  ∈{ Participate (P) ,  Not Participate (NP) } . Choosing P  involves selecting an SNR level, incurring energy costs, and aiming to improve global accuracy.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "The interplay of multiple sensors making similar decisions un- der uncertainty and energy constraints naturally suggests a game-theoretic framework for modeling their interactions. Because sensors have limited energy and the net- work may operate for extended periods, each sensor must consider the future implications of its current actions. Choosing  NP  con- serves energy but forfeits any contribution or associated reward.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "4. The interplay of multiple sensors making similar decisions un- der uncertainty and energy constraints naturally suggests a game-theoretic framework for modeling their interactions. Game-Theoretic Modeling \n4.1.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "These simplistic approaches ignore the interdependencies among sensor decisions and fail to account for future resource allo- cation, potentially leading to suboptimal performance over time. Game-Theoretic Modeling \n4.1. Motivation for Game Theory over Simpler Methods \nWhile heuristic methods—such as always selecting the top- k  sensors based on current energy levels—or greedy algo- rithms that maximize immediate utility might offer straight- forward solutions, they fall short in addressing the strate- gic and long-term dynamics inherent in EH-WSNs.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "These simplistic approaches ignore the interdependencies among sensor decisions and fail to account for future resource allo- cation, potentially leading to suboptimal performance over time. Alternatively, reinforcement learning or Markov Decision Process-based approaches could adaptively learn partici- pation policies that consider both immediate rewards and future states. For instance, always selecting the highest-energy sen- sors can rapidly deplete their energy reserves, reducing the network’s resilience during critical future events.",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "In contrast, a game-theoretic framework provides equilib- rium guarantees, ensuring stable and cooperative partici- pation strategies. Alternatively, reinforcement learning or Markov Decision Process-based approaches could adaptively learn partici- pation policies that consider both immediate rewards and future states. However, these methods often require exten- sive training data, significant computational resources, and complex communication protocols, which may be impracti- cal for resource-constrained sensor nodes.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "By modeling each sensor as a rational player optimizing its own utility, we can derive participation patterns that are robust against unilateral deviations. In contrast, a game-theoretic framework provides equilib- rium guarantees, ensuring stable and cooperative partici- pation strategies. This stability is crucial for maintaining long-term network per- formance without necessitating continuous recalibration or extensive communication overhead.",
      "type": "sliding_window_shuffled",
      "tokens": 94,
      "augmented": true
    },
    {
      "text": "4.2. Utility Function Definition \nWe define a utility function  U i ( t )  for each sensor  s i  that en- capsulates the trade-off between accuracy gains and energy \nexpenditures, as well as future opportunities. This stability is crucial for maintaining long-term network per- formance without necessitating continuous recalibration or extensive communication overhead.",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "Utility Function Definition \nWe define a utility function  U i ( t )  for each sensor  s i  that en- capsulates the trade-off between accuracy gains and energy \nexpenditures, as well as future opportunities. The utility function is designed to reflect both immediate rewards and long-term sustainability. Immediate Rewards and Penalties: Let  γ >  0  be a scal- ing factor that translates accuracy gains into utility rewards.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Immediate Rewards and Penalties: Let  γ >  0  be a scal- ing factor that translates accuracy gains into utility rewards. Here,  δ >  0  penalizes incorrect participation, discourag- ing sensors from submitting low-quality data, while  η >  0 penalizes non-participation to prevent perpetual abstention. When sensor  s i  participates ( a i ( t ) =  P ) and contributes cor- rectly to the inference task, it receives a reward proportional to the improvement in global accuracy, denoted by  ∆ A i ( t ) : \nR i ( t ) = \n   \n  \nγ  ·  ∆ A i ( t ) , if  a i ( t ) =  P and correct inference , − δ, if  a i ( t ) =  P and incorrect inference , − η, if  a i ( t ) =  NP .",
      "type": "sliding_window_shuffled",
      "tokens": 251,
      "augmented": true
    },
    {
      "text": "Energy Costs and Future Utility: Participation incurs energy costs, reducing the sensor’s capacity for future tasks. Here,  δ >  0  penalizes incorrect participation, discourag- ing sensors from submitting low-quality data, while  η >  0 penalizes non-participation to prevent perpetual abstention. Importantly, we set  η > δ , ensuring that consistently opting out is more detrimental than occasionally providing inaccu- rate data.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Energy Costs and Future Utility: Participation incurs energy costs, reducing the sensor’s capacity for future tasks. Additionally, sensors must consider the discounted value of future utility. Let  C i ( t )  represent the cost component: \nC i ( t ) =  e i ( t ) +  βV i ( t  + 1) , \nwhere  e i ( t )  is the total energy expenditure for participa- tion, encompassing data capture, inference computation, and communication: \ne i ( t ) =  e cap ( SNR i ( t )) +  e inf  +  e comm .",
      "type": "sliding_window_shuffled",
      "tokens": 153,
      "augmented": true
    },
    {
      "text": "The discount factor  β  ∈ [0 ,  1)  captures how sensors value future utility, with  V i ( t  + 1)  representing the expected fu- ture utility given current decisions and predicted energy availability   ˆ E i ( t  + 1) . Let  C i ( t )  represent the cost component: \nC i ( t ) =  e i ( t ) +  βV i ( t  + 1) , \nwhere  e i ( t )  is the total energy expenditure for participa- tion, encompassing data capture, inference computation, and communication: \ne i ( t ) =  e cap ( SNR i ( t )) +  e inf  +  e comm . Overall Utility Function: Combining immediate rewards and costs, the overall utility function for sensor  s i  at time  t is: U i ( t ) =  R i ( t )  − C i ( t ) .",
      "type": "sliding_window_shuffled",
      "tokens": 228,
      "augmented": true
    },
    {
      "text": "Overall Utility Function: Combining immediate rewards and costs, the overall utility function for sensor  s i  at time  t is: U i ( t ) =  R i ( t )  − C i ( t ) . This utility function effectively balances the benefits of participation against the associated costs and future oppor- tunities, guiding sensors to make strategic decisions that optimize their long-term contributions to the network. Nash Equilibrium and Stability: A Nash equilibrium (NE) represents a stable action profile  a ∗ ( t )  where no sensor can unilaterally improve its utility by deviating from its current strategy: \nU i ( a ∗ i   ( t ) ,  a ∗ − i ( t ))  ≥ U i ( a i ( t ) ,  a ∗ − i ( t )) ∀ a i ( t ) ,  ∀ i.",
      "type": "sliding_window_shuffled",
      "tokens": 227,
      "augmented": true
    },
    {
      "text": "This stability is critical for maintaining consistent network performance and energy sustainability over time. 4 \n220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 \nAchieving an NE ensures that sensor participation patterns are stable; once equilibrium is reached, no single sensor ben- efits from changing its participation decision independently. Nash Equilibrium and Stability: A Nash equilibrium (NE) represents a stable action profile  a ∗ ( t )  where no sensor can unilaterally improve its utility by deviating from its current strategy: \nU i ( a ∗ i   ( t ) ,  a ∗ − i ( t ))  ≥ U i ( a i ( t ) ,  a ∗ − i ( t )) ∀ a i ( t ) ,  ∀ i.",
      "type": "sliding_window_shuffled",
      "tokens": 292,
      "augmented": true
    },
    {
      "text": "This stability is critical for maintaining consistent network performance and energy sustainability over time. The algorithm operates as follows: \nAlgorithm 1  Distributed Best-Response Participation Algo- rithm \n1:  Input: Current energies  B i ( t ) , predicted harvest ˆ E i ( t  + 1) , parameters  γ, δ, η, β , and energy costs e cap ( · ) , e inf , e comm . Distributed Best-Response Algorithm: To realize the NE, we propose a distributed best-response algorithm where each sensor iteratively adjusts its action based on the current state and the expected actions of others.",
      "type": "sliding_window_shuffled",
      "tokens": 173,
      "augmented": true
    },
    {
      "text": "The algorithm operates as follows: \nAlgorithm 1  Distributed Best-Response Participation Algo- rithm \n1:  Input: Current energies  B i ( t ) , predicted harvest ˆ E i ( t  + 1) , parameters  γ, δ, η, β , and energy costs e cap ( · ) , e inf , e comm . 4:  For each action candidate  a i ( t )  ∈{ P ,  NP } , the sensor computes the expected utility: \nU   a i ( t ) i =  E [ R i ( t )]  − E [ e i ( t )]  − β E [ V i ( t  + 1)] , \nwhere the expectations are taken over uncertainties in correctness, SNR impact, and future energy. 2:  At each inference event: 3:  Each sensor  s i  receives a solicitation from the lead sensor and forms an estimate of  ∆ A i ( t )  given potential SNR choices and expected actions of others.",
      "type": "sliding_window_shuffled",
      "tokens": 261,
      "augmented": true
    },
    {
      "text": "4:  For each action candidate  a i ( t )  ∈{ P ,  NP } , the sensor computes the expected utility: \nU   a i ( t ) i =  E [ R i ( t )]  − E [ e i ( t )]  − β E [ V i ( t  + 1)] , \nwhere the expectations are taken over uncertainties in correctness, SNR impact, and future energy. 6:  After all sensors decide, the action profile  a ( t )  is real- ized, and energies are updated: \nB i ( t  + 1) =  B i ( t ) +  E i ( t )  − e i ( t ) . 5:  If  U   P i   ≥ U  NP i and  B i ( t )  ≥ e cap ( SNR i ( t ))+ e inf + e comm , then  s i  chooses P. Otherwise, it chooses NP.",
      "type": "sliding_window_shuffled",
      "tokens": 246,
      "augmented": true
    },
    {
      "text": "Sensors employ this best-response mechanism, continuously updating their participation decisions based on the evolving network state and the actions of other sensors. 6:  After all sensors decide, the action profile  a ( t )  is real- ized, and energies are updated: \nB i ( t  + 1) =  B i ( t ) +  E i ( t )  − e i ( t ) . 7:  Sensors iterate this process at each inference event, re- fining their estimates and converging to stable action patterns.",
      "type": "sliding_window_shuffled",
      "tokens": 132,
      "augmented": true
    },
    {
      "text": "Over repeated iterations, under suitable conditions, this process converges to a Nash equilibrium where participation strategies are mutually optimal. Sensors employ this best-response mechanism, continuously updating their participation decisions based on the evolving network state and the actions of other sensors. Existence and Convergence of Equilibrium: \nTheorem 4.1.",
      "type": "sliding_window_shuffled",
      "tokens": 81,
      "augmented": true
    },
    {
      "text": "Suppose that each utility function  U i ( t )  is non-decreasing in  ∆ A i ( t ) , that energy constraints and dis- counting ensure diminishing marginal returns for repeated deviations, and that sensors have consistent estimation of ∆ A i ( t )  and   ˆ E i ( t  + 1) . Then, the iterative best-response updates described in Algorithm  1  converge to a Nash equi- librium action profile  a ∗ ( t ) . Existence and Convergence of Equilibrium: \nTheorem 4.1.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "Proof of Theorem  4.1 : The proof constructs a potential function  Φ( a ( t )) =   P N i =1   U i ( a i ( t ) ,  a − i ( t ))  that strictly in- creases whenever a sensor makes a profitable unilateral deviation. Then, the iterative best-response updates described in Algorithm  1  converge to a Nash equi- librium action profile  a ∗ ( t ) . Since utilities are bounded (due to finite en- ergy and limited accuracy gains) and returns diminish over time, no infinite sequence of profitable deviations is possi- ble.",
      "type": "sliding_window_shuffled",
      "tokens": 177,
      "augmented": true
    },
    {
      "text": "Since utilities are bounded (due to finite en- ergy and limited accuracy gains) and returns diminish over time, no infinite sequence of profitable deviations is possi- ble. A complete formal proof, including all technical conditions, is provided in Appendix  B . Hence, the best-response dynamics must terminate at a profile where no sensor can improve its utility alone, i.e., a Nash equilibrium.",
      "type": "sliding_window_shuffled",
      "tokens": 105,
      "augmented": true
    },
    {
      "text": "Detailed guidelines for selecting these hy- perparameters are provided in Appendix  A . Guidelines for Hyperparameter Selection: The param- eters  γ ,  δ , and  η  critically influence sensor behavior by dictating the trade-offs between participation rewards, penal- ties for incorrect submissions, and deterrents against non- participation. A complete formal proof, including all technical conditions, is provided in Appendix  B .",
      "type": "sliding_window_shuffled",
      "tokens": 104,
      "augmented": true
    },
    {
      "text": "(2)  δ  appropriately penalizes incorrect inferences, discouraging low-quality data contri- butions. Briefly, these parameters should be chosen to ensure that:  (1)  γ  suffi- ciently incentivizes correct participation without leading to excessive energy expenditure. Detailed guidelines for selecting these hy- perparameters are provided in Appendix  A .",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "(2)  δ  appropriately penalizes incorrect inferences, discouraging low-quality data contri- butions. (3)  η > δ  to prevent sensors from consistently abstaining, thereby promoting overall network engagement. These guidelines help in balancing immediate utility gains with long-term energy sustainability, ensuring that the game- theoretic model drives desirable participation behaviors.",
      "type": "sliding_window_shuffled",
      "tokens": 82,
      "augmented": true
    },
    {
      "text": "Training and Aggregation Framework \nHaving established the equilibrium participation strategies and the underlying reward-based utility functions, we now consider the training process that fine-tunes the global in- ference model  θ  ∈ R d   within this EH, multi-sensor envi- ronment. These guidelines help in balancing immediate utility gains with long-term energy sustainability, ensuring that the game- theoretic model drives desirable participation behaviors. 5.",
      "type": "sliding_window_shuffled",
      "tokens": 100,
      "augmented": true
    },
    {
      "text": "Initially,  θ  is pre-trained offline and deployed to all sensors, enabling them to perform basic inference tasks. However, this initial model may not be optimally adapted to the complex operational reality of the network, where sensors strategically choose SNR levels, participate inter- mittently according to equilibrium strategies, and generate data distributions that deviate from the original training set. Training and Aggregation Framework \nHaving established the equilibrium participation strategies and the underlying reward-based utility functions, we now consider the training process that fine-tunes the global in- ference model  θ  ∈ R d   within this EH, multi-sensor envi- ronment.",
      "type": "sliding_window_shuffled",
      "tokens": 149,
      "augmented": true
    },
    {
      "text": "The goal of the training process is to adjust  θ  to these condi- tions, effectively  fine-tuning  the model to the nonstationary data distribution  D  induced by the sensors’ equilibrium be- haviors. However, this initial model may not be optimally adapted to the complex operational reality of the network, where sensors strategically choose SNR levels, participate inter- mittently according to equilibrium strategies, and generate data distributions that deviate from the original training set. At equilibrium, sensors strike a balance between accurate data contribution and energy conservation, result- ing in a stable pattern of participation and SNR choices.",
      "type": "sliding_window_shuffled",
      "tokens": 139,
      "augmented": true
    },
    {
      "text": "Learning Approach: Our approach diverges from classi- cal Learning paradigms. Over time, this induces a stationary, albeit non-trivial, effec- tive data distribution  D . At equilibrium, sensors strike a balance between accurate data contribution and energy conservation, result- ing in a stable pattern of participation and SNR choices.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "Learning Approach: Our approach diverges from classi- cal Learning paradigms. We adopt a hybrid strategy where periodic or event-triggered updates refine the model parame- ters based on equilibrium-driven data collection. This hybrid \n5 \n275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 \napproach mitigates the high communication overhead and energy consumption, making it more suitable for resource- constrained EH-WSNs.",
      "type": "sliding_window_shuffled",
      "tokens": 187,
      "augmented": true
    },
    {
      "text": "This hybrid \n5 \n275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 \napproach mitigates the high communication overhead and energy consumption, making it more suitable for resource- constrained EH-WSNs. By aligning update events with periods when sensors are most likely to participate meaningfully, we ensure that the global model is refined efficiently without imposing excessive en- ergy demands on the sensors. While traditional methods require persistent communication between sensors and the aggre- gator, our framework leverages the established equilibrium strategies to determine optimal times for model updates.",
      "type": "sliding_window_shuffled",
      "tokens": 214,
      "augmented": true
    },
    {
      "text": "Training Objective and Regularization: To enhance ro- bustness and efficiency, we incorporate regularizers that pe- nalize undesirable model properties. By aligning update events with periods when sensors are most likely to participate meaningfully, we ensure that the global model is refined efficiently without imposing excessive en- ergy demands on the sensors. Specifically, we intro- duce two regularizers:  (1)  Ω SNR ( θ ) : Encourages the model to maintain performance across varying SNR levels, pre- venting over-reliance on high-SNR data.",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "The full training objective is formulated as: \nJ ( θ ) =  L ( θ ) +  λ 1 Ω SNR ( θ ) +  λ 2 Ω complexity ( θ ) , \nwhere L ( θ ) =  E ( x,y ) ∼D [ ℓ ( f θ ( x ) , y )] , \nand  λ 1 , λ 2  ≥ 0  are hyperparameters that balance accuracy, robustness, and efficiency. Specifically, we intro- duce two regularizers:  (1)  Ω SNR ( θ ) : Encourages the model to maintain performance across varying SNR levels, pre- venting over-reliance on high-SNR data. (2)  Ω complexity ( θ ) : Controls model complexity, reducing computational and communication overheads by discouraging overly intricate models.",
      "type": "sliding_window_shuffled",
      "tokens": 203,
      "augmented": true
    },
    {
      "text": "During back- propagation, each sensor computes the gradient of the loss function  ∇ ℓ ( f θ ( x ) , y )  with respect to  θ  based on locally available samples from  D . Gradient Computation and Backpropagation: Integrat- ing regularizers into the training process is straightforward due to their known closed-form gradients. The full training objective is formulated as: \nJ ( θ ) =  L ( θ ) +  λ 1 Ω SNR ( θ ) +  λ 2 Ω complexity ( θ ) , \nwhere L ( θ ) =  E ( x,y ) ∼D [ ℓ ( f θ ( x ) , y )] , \nand  λ 1 , λ 2  ≥ 0  are hyperparameters that balance accuracy, robustness, and efficiency.",
      "type": "sliding_window_shuffled",
      "tokens": 205,
      "augmented": true
    },
    {
      "text": "Since both regularizers are convex and smooth, their in- clusion ensures that the overall objective  J ( θ )  maintains desirable convexity and smoothness properties, facilitating the convergence of stochastic gradient descent (SGD). Additionally, the gradients of the regularizers,  ∇ Ω SNR ( θ )  and  ∇ Ω complexity ( θ ) , are analyt- ically derived and added to the local gradient estimates. During back- propagation, each sensor computes the gradient of the loss function  ∇ ℓ ( f θ ( x ) , y )  with respect to  θ  based on locally available samples from  D .",
      "type": "sliding_window_shuffled",
      "tokens": 161,
      "augmented": true
    },
    {
      "text": "Since both regularizers are convex and smooth, their in- clusion ensures that the overall objective  J ( θ )  maintains desirable convexity and smoothness properties, facilitating the convergence of stochastic gradient descent (SGD). Partici- pation during training follows the same equilibrium model: sensors decide whether to compute and send gradients based on their current energy states, predicted future utilities, and the established reward structure. Periodic Equilibrium-Aware Training: Model updates are performed periodically at an aggregator node that col- lects gradient estimates from participating sensors.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "Partici- pation during training follows the same equilibrium model: sensors decide whether to compute and send gradients based on their current energy states, predicted future utilities, and the established reward structure. By aggregating these gradi- ent updates over multiple training rounds, the aggregator ap- proximates the gradient  ∇ J ( θ )  and performs an SGD step. Our training framework is encapsulated in Algorithm  2 , which outlines the periodic equilibrium-aware training pro- cess.",
      "type": "sliding_window_shuffled",
      "tokens": 119,
      "augmented": true
    },
    {
      "text": "This training framework is intrinsically linked to the game-theoretic participation strategies. Sensors participate \nin training rounds based on their equilibrium-driven de- cisions, ensuring that gradient updates are contributed by those sensors most capable and willing to improve the global model. Our training framework is encapsulated in Algorithm  2 , which outlines the periodic equilibrium-aware training pro- cess.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "This alignment minimizes unnecessary energy ex- penditure and maximizes the efficacy of each training round. Sensors participate \nin training rounds based on their equilibrium-driven de- cisions, ensuring that gradient updates are contributed by those sensors most capable and willing to improve the global model. Algorithm 2  Periodic Equilibrium-Aware Training Algo- rithm \n1:  Initialization:  Initialize  θ 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 97,
      "augmented": true
    },
    {
      "text": "Algorithm 2  Periodic Equilibrium-Aware Training Algo- rithm \n1:  Initialization:  Initialize  θ 0 . Set a diminishing step-size schedule  { α k } k ≥ 0 . Broadcast  θ 0  to all sensors.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "Set a diminishing step-size schedule  { α k } k ≥ 0 . 2:  for  each training round  k  = 0 ,  1 ,  2 , . .",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": ". . do 3: The aggregator signals that a training update round is imminent.",
      "type": "sliding_window_shuffled",
      "tokens": 22,
      "augmented": true
    },
    {
      "text": "do 3: The aggregator signals that a training update round is imminent. Participation in- volves: \n1. 4: Sensors decide on participation.",
      "type": "sliding_window_shuffled",
      "tokens": 35,
      "augmented": true
    },
    {
      "text": "Participation in- volves: \n1. Determining if they have enough energy and incentive (based on the established equilibrium strategy and reward parameters  γ, δ, η ). 2.",
      "type": "sliding_window_shuffled",
      "tokens": 42,
      "augmented": true
    },
    {
      "text": "3. 2. If participating: capturing data at their cho- sen SNR, performing inference, and computing local gradients  ∇ ℓ ( f θ k ( x ) , y )  on their locally available samples drawn from  D .",
      "type": "sliding_window_shuffled",
      "tokens": 61,
      "augmented": true
    },
    {
      "text": "3. 5: A subset of sensors, determined by the equilibrium, send their gradient estimates to the aggregator. Adding regularizer gradients  λ 1 ∇ Ω SNR ( θ k )  and λ 2 ∇ Ω complexity ( θ k ) .",
      "type": "sliding_window_shuffled",
      "tokens": 65,
      "augmented": true
    },
    {
      "text": "7: Update model parameters: \nθ k +1  =  θ k  − α k   b ∇ J ( θ k ) . 5: A subset of sensors, determined by the equilibrium, send their gradient estimates to the aggregator. 6: The aggregator forms an unbiased estimate of the full gradient: \nb ∇ J ( θ k ) =  b ∇ L ( θ k ) +  λ 1 ∇ Ω SNR ( θ k ) \n+ λ 2 ∇ Ω complexity ( θ k ) .",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "7: Update model parameters: \nθ k +1  =  θ k  − α k   b ∇ J ( θ k ) . 9:  end for \nRegularizers and SGD Convergence: The chosen reg- ularizers  Ω SNR ( θ )  and  Ω complexity ( θ )  are both convex and smooth, with known closed-form gradients. 8: Broadcast  θ k +1  to all sensors.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "Consequently, the stochastic gradient descent (SGD) updates retain their convergence properties, ensuring that the training process reliably optimizes  J ( θ ) . 9:  end for \nRegularizers and SGD Convergence: The chosen reg- ularizers  Ω SNR ( θ )  and  Ω complexity ( θ )  are both convex and smooth, with known closed-form gradients. This property guarantees that the inclusion of regularizers does not com- promise the convexity or smoothness of the overall objective J ( θ ) .",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "Consequently, the stochastic gradient descent (SGD) updates retain their convergence properties, ensuring that the training process reliably optimizes  J ( θ ) . 6 \n330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 \n6. Implementation and Evaluation \n6.1.",
      "type": "sliding_window_shuffled",
      "tokens": 155,
      "augmented": true
    },
    {
      "text": "Conclusion \nThis should finish at 8 pages. Implementation and Evaluation \n6.1. Discussions and Limitations \n7.",
      "type": "sliding_window_shuffled",
      "tokens": 22,
      "augmented": true
    },
    {
      "text": "This statement should be in an unnumbered section at the end of the paper (co- located with Acknowledgments – the two may appear in either order, but both must be before References), and does not count toward the paper page limit. Conclusion \nThis should finish at 8 pages. Impact Statement \nAuthors are  required  to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "In many cases, where the ethical impacts and expected societal implications are those that are well established when advancing the field of Machine Learning, substantial discussion is not required, and a simple statement such as the following will suffice: \n“This paper presents work whose goal is to advance the field of Machine Learning. This statement should be in an unnumbered section at the end of the paper (co- located with Acknowledgments – the two may appear in either order, but both must be before References), and does not count toward the paper page limit. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.” \nThe above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review.",
      "type": "sliding_window_shuffled",
      "tokens": 187,
      "augmented": true
    },
    {
      "text": "There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.” \nThe above statement can be used verbatim in such cases, but we encourage authors to think about whether there is content which does warrant further discussion, as this statement will be apparent if the paper is later flagged for ethics review. Guidelines for Hyperparameter Selection and Bounds on Reward Parameters \nThe parameters  γ ,  δ,  and  η  govern the reward structure of the proposed framework, influencing whether sensors participate consistently, over-participate and waste energy, or abstain altogether. References \nA.",
      "type": "sliding_window_shuffled",
      "tokens": 134,
      "augmented": true
    },
    {
      "text": "This appendix provides a systematic approach to selecting these parameters, including formal bounds, practical heuristics, and an algorithmic procedure to explore suitable values. Guidelines for Hyperparameter Selection and Bounds on Reward Parameters \nThe parameters  γ ,  δ,  and  η  govern the reward structure of the proposed framework, influencing whether sensors participate consistently, over-participate and waste energy, or abstain altogether. Conceptual Role of Parameters \nThe scalar  γ >  0  represents the reward scaling for correct participation.",
      "type": "sliding_window_shuffled",
      "tokens": 123,
      "augmented": true
    },
    {
      "text": "If γ  is too high, sensors may waste energy attempting difficult inferences. If  γ  is too low, sensors will not have suffi- cient incentive to expend energy on high-SNR captures. Conceptual Role of Parameters \nThe scalar  γ >  0  represents the reward scaling for correct participation.",
      "type": "sliding_window_shuffled",
      "tokens": 74,
      "augmented": true
    },
    {
      "text": "The parameter  η >  0  penalizes non-participation, ensuring that sensors do not remain idle indefinitely. If γ  is too high, sensors may waste energy attempting difficult inferences. The parameter  δ >  0  penalizes incorrect in- ferences, discouraging reckless submissions of low-quality data.",
      "type": "sliding_window_shuffled",
      "tokens": 77,
      "augmented": true
    },
    {
      "text": "As discussed, maintaining  η > δ  encourages sensors to at least attempt participation rather than always remain offline. The parameter  η >  0  penalizes non-participation, ensuring that sensors do not remain idle indefinitely. Formal Bounds and Conditions \nTo ensure balanced behavior, it is helpful to relate  γ, δ,  and η  to typical values of accuracy improvement and energy costs.",
      "type": "sliding_window_shuffled",
      "tokens": 89,
      "augmented": true
    },
    {
      "text": "Let  e max total   = e max cap   + e inf   + e comm   represent the maximum energy cost (for a chosen SNR mode). Accuracy Gains and Costs: Let  ∆ A min  and  ∆ A max  de- note the minimum and maximum expected accuracy im- provements from any sensor’s participation. Formal Bounds and Conditions \nTo ensure balanced behavior, it is helpful to relate  γ, δ,  and η  to typical values of accuracy improvement and energy costs.",
      "type": "sliding_window_shuffled",
      "tokens": 111,
      "augmented": true
    },
    {
      "text": "Let  e max total   = e max cap   + e inf   + e comm   represent the maximum energy cost (for a chosen SNR mode). A baseline condition that ensures correct participation can overcome occasional penalties is: \nγ  ·  ∆ A min  > δ  +  e max total   . This inequality implies that even in a worst-case scenario for accuracy gain, the net expected benefit of correct par- ticipation surpasses the sum of potential incorrect penalties and energy costs.",
      "type": "sliding_window_shuffled",
      "tokens": 110,
      "augmented": true
    },
    {
      "text": "This inequality implies that even in a worst-case scenario for accuracy gain, the net expected benefit of correct par- ticipation surpasses the sum of potential incorrect penalties and energy costs. Without this condition, sensors might find participation systematically unprofitable. Non-Participation and Equilibrium: Since  η > δ , we ensure that sensors prefer risking occasional incorrect infer- ences over consistently abstaining.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    },
    {
      "text": "A suitable gap might be chosen so that: δ < η  ≤ δ  +  c, \nfor some small  c >  0 . Choosing  c  relative to typical gains, say  c  ≈ 0 . Non-Participation and Equilibrium: Since  η > δ , we ensure that sensors prefer risking occasional incorrect infer- ences over consistently abstaining.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "7 \n385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 \nEnergy Preservation: If  γ  is too large, sensors might not value future energy at all. 1 · γ · ∆ A max , helps maintain a moderate deterrent against non-participation without forcing sensors to always participate. Choosing  c  relative to typical gains, say  c  ≈ 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 183,
      "augmented": true
    },
    {
      "text": "To prevent myopic strategies, one can limit  γ  such that continuously investing in high-SNR captures does not dominate long-term considerations. For example: \nγ  ·  ∆ A max  < η  +  margin , \nwhere  margin  accounts for future opportunities and energy savings. 7 \n385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 \nEnergy Preservation: If  γ  is too large, sensors might not value future energy at all.",
      "type": "sliding_window_shuffled",
      "tokens": 191,
      "augmented": true
    },
    {
      "text": "For example: \nγ  ·  ∆ A max  < η  +  margin , \nwhere  margin  accounts for future opportunities and energy savings. Practical Hyperparameter Tuning Strategies \n1. A small  margin  ensures sensors do not always expend maximal energy for short-term gains.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "2. Baseline Ratios:  Start with ratios that link  γ  to typical accuracy gains and set  δ, η  based on fractions or multiples of  γ  ·  ∆ A min  or  γ  ·  ∆ A max . Practical Hyperparameter Tuning Strategies \n1.",
      "type": "sliding_window_shuffled",
      "tokens": 64,
      "augmented": true
    },
    {
      "text": "2. If sensors rarely par- ticipate, increase  γ  or decrease  η . Iterative Refinement:  Use simulation or small-scale ex- perimental runs to refine parameters.",
      "type": "sliding_window_shuffled",
      "tokens": 45,
      "augmented": true
    },
    {
      "text": "If sensors over-exert themselves, reduce  γ  or increase  δ, η . If sensors rarely par- ticipate, increase  γ  or decrease  η . 3.",
      "type": "sliding_window_shuffled",
      "tokens": 43,
      "augmented": true
    },
    {
      "text": "Adaptive Tuning:  If conditions change over time, adjust γ, δ,  and  η  dynamically based on observed participation rates, accuracy levels, and energy depletion patterns. Exploration Algorithm \nAlgorithm  3  outlines a systematic approach to exploring suit- able hyperparameter values. 3.",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "It combines theoretical bounds with empirical evaluation, guiding the search toward stable and efficient equilibria. The above guidelines and the explo- ration algorithm provide a structured approach to selecting and refining  γ, δ,  and  η . Exploration Algorithm \nAlgorithm  3  outlines a systematic approach to exploring suit- able hyperparameter values.",
      "type": "sliding_window_shuffled",
      "tokens": 88,
      "augmented": true
    },
    {
      "text": "Regular re-tuning may be warranted as oper- ating conditions, energy harvesting patterns, or accuracy requirements evolve over the network’s lifetime. The above guidelines and the explo- ration algorithm provide a structured approach to selecting and refining  γ, δ,  and  η . By starting from theoretically in- formed baseline conditions and iteratively refining through simulation-based feedback, it is possible to reach a stable set of parameters that promotes balanced participation, discour- ages perpetual abstention, and prevents excessive energy expenditure.",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "We first restate the key assumptions and the utility model. B. Equilibrium Existence and Convergence with Reward-Based Utility \nIn this appendix, we provide a detailed and formal proof that the best-response dynamics, incorporating the newly defined reward-based utility functions, converge to a Nash equilibrium (NE). Regular re-tuning may be warranted as oper- ating conditions, energy harvesting patterns, or accuracy requirements evolve over the network’s lifetime.",
      "type": "sliding_window_shuffled",
      "tokens": 113,
      "augmented": true
    },
    {
      "text": "Finally, we prove that the equilibrium is reached under the given assumptions. We first restate the key assumptions and the utility model. We then show that the iterative best- response updates cannot lead to infinite improvement cycles, implying the existence of an NE.",
      "type": "sliding_window_shuffled",
      "tokens": 56,
      "augmented": true
    },
    {
      "text": "2:  Compute  e max total   =  e max cap   +  e inf   +  e comm . Finally, we prove that the equilibrium is reached under the given assumptions. Algorithm 3  Hyperparameter Exploration for Reward Pa- rameters \n1:  Inputs: Estimates  ∆ A min ,  ∆ A max , energy costs e max cap   , e inf , e comm , initial guesses  γ 0 , δ 0 , η 0 , and tuning increments  ∆ γ ,  ∆ δ ,  ∆ η .",
      "type": "sliding_window_shuffled",
      "tokens": 147,
      "augmented": true
    },
    {
      "text": "4:  Set  η 0  > δ 0 . 2:  Compute  e max total   =  e max cap   +  e inf   +  e comm . 3:  Ensure baseline feasibility: If  γ 0  ·  ∆ A min  ≤ δ 0  +  e max total   , increase  γ 0  until this condition is met.",
      "type": "sliding_window_shuffled",
      "tokens": 83,
      "augmented": true
    },
    {
      "text": "4:  Set  η 0  > δ 0 . If preliminary tests show insufficient participation, slightly increase  η 0 . Start with  η 0  =  δ 0  +  c , where  c  is a small positive number.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "5:  Simulation-Refinement Loop: 6:  for  k  = 1 ,  2 , . If participation is overly aggressive, reduce  γ 0  or increase  δ 0 . If preliminary tests show insufficient participation, slightly increase  η 0 .",
      "type": "sliding_window_shuffled",
      "tokens": 60,
      "augmented": true
    },
    {
      "text": ". . 5:  Simulation-Refinement Loop: 6:  for  k  = 1 ,  2 , .",
      "type": "sliding_window_shuffled",
      "tokens": 28,
      "augmented": true
    },
    {
      "text": "8: Measure key indicators: participation rate, average energy depletion rate, frequency of incorrect infer- ences, and overall inference accuracy. , K  (number of refinement iterations) do 7: Run a simulation or small-scale test deployment us- ing the current  γ k , δ k , η k . .",
      "type": "sliding_window_shuffled",
      "tokens": 85,
      "augmented": true
    },
    {
      "text": "8: Measure key indicators: participation rate, average energy depletion rate, frequency of incorrect infer- ences, and overall inference accuracy. 11: else if  participation is too high, leading to frequent energy depletion  then 12: Decrease  γ k  ← γ k − ∆ γ  or increase  δ k  ← δ k +∆ δ to discourage high-risk attempts. 9: if  participation is too low (e.g.,  < p min ) or sensors remain idle too often  then 10: Increase  γ k  ← γ k  + ∆ γ  or decrease  η k  ← η k  − ∆ η .",
      "type": "sliding_window_shuffled",
      "tokens": 160,
      "augmented": true
    },
    {
      "text": "15: end if 16: Check feasibility conditions again to ensure no viola- tion of baseline inequalities. 13: else if  incorrect inferences are prevalent  then 14: Increase  δ k  ← δ k  + ∆ δ  to penalize low-quality submissions more strongly. 11: else if  participation is too high, leading to frequent energy depletion  then 12: Decrease  γ k  ← γ k − ∆ γ  or increase  δ k  ← δ k +∆ δ to discourage high-risk attempts.",
      "type": "sliding_window_shuffled",
      "tokens": 130,
      "augmented": true
    },
    {
      "text": "Otherwise, continue refine- ment. 15: end if 16: Check feasibility conditions again to ensure no viola- tion of baseline inequalities. 17: If performance metrics (accuracy, sustainability) are satisfactory, terminate.",
      "type": "sliding_window_shuffled",
      "tokens": 53,
      "augmented": true
    },
    {
      "text": "The chosen action profile is a ( t ) = ( a 1 ( t ) , . 18:  end for \nRestatement of the Utility Function and Assumptions \nRecall that at each inference event  t , each sensor  s i  chooses an action  a i ( t )  ∈{ P ,  NP } . Otherwise, continue refine- ment.",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": ". . The chosen action profile is a ( t ) = ( a 1 ( t ) , .",
      "type": "sliding_window_shuffled",
      "tokens": 31,
      "augmented": true
    },
    {
      "text": ". The immediate reward for sensor  s i  is defined as: \nR i ( t ) = \n     \n    \nγ  ·  ∆ A i ( t ) , if  a i ( t ) =  P and inference is correct , \n− δ, if  a i ( t ) =  P and inference is incorrect , \n− η, if  a i ( t ) =  NP . , a N ( t )) .",
      "type": "sliding_window_shuffled",
      "tokens": 138,
      "augmented": true
    },
    {
      "text": "The cost incorporates energy consumption and future op- portunities. Here,  γ >  0  scales the reward for correct participation, δ >  0  penalizes incorrect inference, and  η >  0  penalizes non-participation, with  η > δ  ensuring that remaining idle \n8 \n440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 \nis more penalizing than at least attempting participation. The immediate reward for sensor  s i  is defined as: \nR i ( t ) = \n     \n    \nγ  ·  ∆ A i ( t ) , if  a i ( t ) =  P and inference is correct , \n− δ, if  a i ( t ) =  P and inference is incorrect , \n− η, if  a i ( t ) =  NP .",
      "type": "sliding_window_shuffled",
      "tokens": 315,
      "augmented": true
    },
    {
      "text": "The cost incorporates energy consumption and future op- portunities. Let  e i ( t )  be the energy expenditure for sensor s i  if it participates at time  t , accounting for capture, infer- ence, and communication costs. Introduce a discount factor β  ∈ [0 ,  1) , and let  V i ( t  + 1)  represent the expected future utility of sensor  s i  given its current decisions and predicted energy availability.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Introduce a discount factor β  ∈ [0 ,  1) , and let  V i ( t  + 1)  represent the expected future utility of sensor  s i  given its current decisions and predicted energy availability. The cost is: \nC i ( t ) =  e i ( t ) +  βV i ( t  + 1) . The overall utility is: \nU i ( t ) =  R i ( t )  − C i ( t ) .",
      "type": "sliding_window_shuffled",
      "tokens": 116,
      "augmented": true
    },
    {
      "text": "The overall utility is: \nU i ( t ) =  R i ( t )  − C i ( t ) . We assume that  ∆ A i ( t )  is non-decreasing in the quality of sensor  s i ’s data (e.g., higher SNR yields higher  ∆ A i ( t ) ). We also assume that energy resources, accuracy gains, and reward/penalty parameters are finite and bounded, and that sensors have consistent estimation mechanisms for  ∆ A i ( t ) and   ˆ E i ( t  + 1) .",
      "type": "sliding_window_shuffled",
      "tokens": 149,
      "augmented": true
    },
    {
      "text": "We also assume that energy resources, accuracy gains, and reward/penalty parameters are finite and bounded, and that sensors have consistent estimation mechanisms for  ∆ A i ( t ) and   ˆ E i ( t  + 1) . Potential Function Construction \nTo prove convergence, we define a potential function that reflects the collective utility of the sensor network: \nΦ( a ( t )) = \nN X \ni =1 U i ( a i ( t ) ,  a − i ( t )) . Since  U i ( t ) =  R i ( t )  − C i ( t ) , we have: \nΦ( a ( t )) = \nN X \ni =1 [ R i ( t )  − C i ( t )] .",
      "type": "sliding_window_shuffled",
      "tokens": 203,
      "augmented": true
    },
    {
      "text": "The terms  R i ( t )  depend on the chosen actions and correct- ness of inferences. Since  U i ( t ) =  R i ( t )  − C i ( t ) , we have: \nΦ( a ( t )) = \nN X \ni =1 [ R i ( t )  − C i ( t )] . Due to bounded  γ, δ,  and  η , and the fact that  ∆ A i ( t )  and energy costs are bounded, each  U i ( t ) is finite.",
      "type": "sliding_window_shuffled",
      "tokens": 151,
      "augmented": true
    },
    {
      "text": "Due to bounded  γ, δ,  and  η , and the fact that  ∆ A i ( t )  and energy costs are bounded, each  U i ( t ) is finite. Thus,  Φ( a ( t ))  is also finite for all feasible action profiles. Monotonicity of the Potential Function \nConsider a unilateral deviation by a single sensor  s j  from an action  a j ( t )  to a different action  a ′ j ( t ) .",
      "type": "sliding_window_shuffled",
      "tokens": 127,
      "augmented": true
    },
    {
      "text": "If this deviation is profitable for sensor s j , we have: \nU j ( a ′ j ( t ) ,  a − j ( t ))  > U j ( a j ( t ) ,  a − j ( t )) . Such a deviation affects only  U j ( t ) , not the utilities of other sensors directly in a one-step change. Monotonicity of the Potential Function \nConsider a unilateral deviation by a single sensor  s j  from an action  a j ( t )  to a different action  a ′ j ( t ) .",
      "type": "sliding_window_shuffled",
      "tokens": 160,
      "augmented": true
    },
    {
      "text": "Thus, any unilateral profitable deviation increases  Φ( a ( t )) . If this deviation is profitable for sensor s j , we have: \nU j ( a ′ j ( t ) ,  a − j ( t ))  > U j ( a j ( t ) ,  a − j ( t )) . Because the other sensors’ utilities do not change instan- taneously by  s j ’s unilateral action, the increment in  U j ( t ) results in: \nΦ( a − j ( t ) , a ′ j ( t ))  − Φ( a ( t )) = \nU j ( a ′ j ( t ) ,  a − j ( t ))  − U j ( a j ( t ) ,  a − j ( t ))  >  0 .",
      "type": "sliding_window_shuffled",
      "tokens": 243,
      "augmented": true
    },
    {
      "text": "Suppose, for contradiction, that there exists an infinite se- quence of unilateral profitable deviations. Thus, any unilateral profitable deviation increases  Φ( a ( t )) . Boundedness and Impossibility of Infinite Improvement Sequences \nSince all utilities are bounded (due to finite  γ, δ, η,  bounded ∆ A i ( t ) , and bounded energy resources), there exists a finite upper bound  Φ max  such that: \nΦ( a ( t ))  ≤ Φ max ∀ a ( t ) .",
      "type": "sliding_window_shuffled",
      "tokens": 144,
      "augmented": true
    },
    {
      "text": "Each such de- viation strictly increases  Φ( a ( t )) . Suppose, for contradiction, that there exists an infinite se- quence of unilateral profitable deviations. Because  Φ  is bounded above by  Φ max , only a finite number of increments can occur before no further improvements are possible.",
      "type": "sliding_window_shuffled",
      "tokens": 75,
      "augmented": true
    },
    {
      "text": "Existence of a Nash Equilibrium \nSince no infinite sequence of profitable unilateral deviations can occur, the best-response dynamics must terminate in a state where no sensor can unilaterally improve its utility. This contradiction shows that no infinite improvement sequence can occur. Because  Φ  is bounded above by  Φ max , only a finite number of increments can occur before no further improvements are possible.",
      "type": "sliding_window_shuffled",
      "tokens": 91,
      "augmented": true
    },
    {
      "text": "Thus, the existence of a Nash equilibrium follows directly from the finiteness of utilities, the monotonicity of  Φ , and the impossibility of infinite improvement sequences. Existence of a Nash Equilibrium \nSince no infinite sequence of profitable unilateral deviations can occur, the best-response dynamics must terminate in a state where no sensor can unilaterally improve its utility. By definition, this state is a Nash equilibrium  a ∗ ( t ) : \nU i ( a ∗ i   ( t ) ,  a ∗ − i ( t ))  ≥ U i ( a i ( t ) ,  a ∗ − i ( t )) ∀ a i ( t ) ,  ∀ i.",
      "type": "sliding_window_shuffled",
      "tokens": 191,
      "augmented": true
    },
    {
      "text": "Thus, the existence of a Nash equilibrium follows directly from the finiteness of utilities, the monotonicity of  Φ , and the impossibility of infinite improvement sequences. Since each sensor’s best-response update seeks to maximize its own utility, sensors will continue to deviate as long as prof- itable deviations exist. Convergence to the Nash Equilibrium \nThe final step is to show that the iterative best-response dy- namics indeed converge to the NE identified above.",
      "type": "sliding_window_shuffled",
      "tokens": 129,
      "augmented": true
    },
    {
      "text": "Under the assumptions that ∆ A i ( t )  is non-decreasing and that sensors have consistent energy and accuracy estimates, no cyclical behavior can persist. Our argument shows that profitable deviations must terminate. Since each sensor’s best-response update seeks to maximize its own utility, sensors will continue to deviate as long as prof- itable deviations exist.",
      "type": "sliding_window_shuffled",
      "tokens": 90,
      "augmented": true
    },
    {
      "text": "A cycle would imply an infinite sequence of im- provements or a return to a previously visited state without improvement, which cannot occur since profitable devia- tions strictly increase  Φ( a ( t )) . Under the assumptions that ∆ A i ( t )  is non-decreasing and that sensors have consistent energy and accuracy estimates, no cyclical behavior can persist. The presence of the discount factor  β  further stabilizes the process.",
      "type": "sliding_window_shuffled",
      "tokens": 106,
      "augmented": true
    },
    {
      "text": "This discounting ensures diminish- ing returns for postponing beneficial participation or indefi- nitely waiting for ideal conditions. The presence of the discount factor  β  further stabilizes the process. With  β  ∈ [0 ,  1) , sensors value future utility less than immediate utility.",
      "type": "sliding_window_shuffled",
      "tokens": 66,
      "augmented": true
    },
    {
      "text": "9 \n495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 \nBecause the best-response process eliminates profitable de- viations step by step and cannot cycle indefinitely, the action profile sequence generated by iterative best responses con- verges to the NE. As a result, sensors do not continually defer improvements, preventing complex long-term cycles. This discounting ensures diminish- ing returns for postponing beneficial participation or indefi- nitely waiting for ideal conditions.",
      "type": "sliding_window_shuffled",
      "tokens": 209,
      "augmented": true
    },
    {
      "text": "9 \n495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 \nBecause the best-response process eliminates profitable de- viations step by step and cannot cycle indefinitely, the action profile sequence generated by iterative best responses con- verges to the NE. We have shown that, with the reward-based utility function that includes correct participation rewards ( γ  ·  ∆ A i ( t ) ), penalties for incorrect inferences ( δ ), and penalties for non- participation ( η ), the best-response dynamics lead to a Nash equilibrium. The proof relies on constructing a potential function  Φ  that is strictly increased by unilateral profitable deviations and bounded above.",
      "type": "sliding_window_shuffled",
      "tokens": 259,
      "augmented": true
    },
    {
      "text": "The impossibility of infinite improvement sequences guarantees the existence of an NE, and the assumptions on monotonicity, boundedness, and discounting ensure that the iterative best-response process converges to this equilibrium. The proof relies on constructing a potential function  Φ  that is strictly increased by unilateral profitable deviations and bounded above. □ \nC. Proof of Convergence for the Equilibrium-Aware Training Process \nIn this appendix, we provide a comprehensive and detailed proof of the convergence theorem stated in the main text.",
      "type": "sliding_window_shuffled",
      "tokens": 131,
      "augmented": true
    },
    {
      "text": "We also elaborate on how the new loss function, the introduced regularizers, and their gradients integrate into the backprop- agation and stochastic gradient descent (SGD) steps. □ \nC. Proof of Convergence for the Equilibrium-Aware Training Process \nIn this appendix, we provide a comprehensive and detailed proof of the convergence theorem stated in the main text. Addi- tionally, we discuss bounds on the newly introduced hyper- parameters and provide guidelines for selecting them.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "The model’s performance is measured by a loss function  ℓ :  Y × Y → R ≥ 0  that is convex in  θ  for any fixed input-label pair  ( x, y ) . Problem Setting and Notation \nWe consider a global inference model  f θ  :  X →Y  parame- terized by  θ  ∈ R d . Addi- tionally, we discuss bounds on the newly introduced hyper- parameters and provide guidelines for selecting them.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Let  D  denote the effective data distribution induced by the equilibrium strategies of the sensors. The model operates in an energy-harvesting wireless sensor network (EH-WSN) envi- ronment where sensors participate strategically in inference tasks based on a game-theoretic equilibrium. The model’s performance is measured by a loss function  ℓ :  Y × Y → R ≥ 0  that is convex in  θ  for any fixed input-label pair  ( x, y ) .",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "Under equilibrium conditions, the distribution  D  is stationary or at least sta- tionary over sufficiently large timescales. The expected loss is L ( θ ) =  E ( x,y ) ∼D [ ℓ ( f θ ( x ) , y )] . Let  D  denote the effective data distribution induced by the equilibrium strategies of the sensors.",
      "type": "sliding_window_shuffled",
      "tokens": 87,
      "augmented": true
    },
    {
      "text": "Ω SNR ( θ )  encourages the model to perform reasonably well across varying SNR levels, while  Ω complexity ( θ )  penalizes overly complex models that might demand excessive energy or communication costs. The expected loss is L ( θ ) =  E ( x,y ) ∼D [ ℓ ( f θ ( x ) , y )] . To enhance robustness and efficiency, we introduce two regularizers: \nΩ SNR ( θ ) and Ω complexity ( θ ) .",
      "type": "sliding_window_shuffled",
      "tokens": 128,
      "augmented": true
    },
    {
      "text": "Both are assumed convex and have bounded gradients. The final training objective is: \nJ ( θ ) =  L ( θ ) +  λ 1 Ω SNR ( θ ) +  λ 2 Ω complexity ( θ ) , \nwhere  λ 1 , λ 2  ≥ 0  are hyperparameters controlling the influ- ence of the regularizers. Ω SNR ( θ )  encourages the model to perform reasonably well across varying SNR levels, while  Ω complexity ( θ )  penalizes overly complex models that might demand excessive energy or communication costs.",
      "type": "sliding_window_shuffled",
      "tokens": 141,
      "augmented": true
    },
    {
      "text": "Key Assumptions and Conditions \n1. The final training objective is: \nJ ( θ ) =  L ( θ ) +  λ 1 Ω SNR ( θ ) +  λ 2 Ω complexity ( θ ) , \nwhere  λ 1 , λ 2  ≥ 0  are hyperparameters controlling the influ- ence of the regularizers. Our goal is to show that by running a diminishing step-size SGD on  J ( θ ) , using unbiased gradient estimates from the equilibrium distribution  D , the parameters  { θ k }  converge in expectation to a stationary point  θ ∗ of  J ( θ ) .",
      "type": "sliding_window_shuffled",
      "tokens": 156,
      "augmented": true
    },
    {
      "text": "Convexity of  ℓ . The loss  ℓ ( f θ ( x ) , y )  is convex in  θ . Key Assumptions and Conditions \n1.",
      "type": "sliding_window_shuffled",
      "tokens": 47,
      "augmented": true
    },
    {
      "text": "The loss  ℓ ( f θ ( x ) , y )  is convex in  θ . 2. Consequently, the expected loss  L ( θ )  is also convex.",
      "type": "sliding_window_shuffled",
      "tokens": 49,
      "augmented": true
    },
    {
      "text": "There exists a constant  L >  0 such that for all  θ, θ ′ , \n∥∇ L ( θ )  −∇ L ( θ ′ ) ∥≤ L ∥ θ  − θ ′ ∥ . 2. L -smoothness of  ℓ .",
      "type": "sliding_window_shuffled",
      "tokens": 71,
      "augmented": true
    },
    {
      "text": "3. This ensures that  L ( θ )  is Lipschitz-smooth. There exists a constant  L >  0 such that for all  θ, θ ′ , \n∥∇ L ( θ )  −∇ L ( θ ′ ) ∥≤ L ∥ θ  − θ ′ ∥ .",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "The regularizers  Ω SNR  and  Ω complexity  are convex in  θ , and their gradients are bounded. Convexity and boundedness of regularizers. 3.",
      "type": "sliding_window_shuffled",
      "tokens": 46,
      "augmented": true
    },
    {
      "text": "Let \n∥∇ Ω SNR ( θ ) ∥≤ G 1 , ∥∇ Ω complexity ( θ ) ∥≤ G 2 ∀ θ. The regularizers  Ω SNR  and  Ω complexity  are convex in  θ , and their gradients are bounded. 4.",
      "type": "sliding_window_shuffled",
      "tokens": 69,
      "augmented": true
    },
    {
      "text": "4. Stationary distribution  D . The equilibrium participa- tion strategies induce a stationary effective distribution D .",
      "type": "sliding_window_shuffled",
      "tokens": 24,
      "augmented": true
    },
    {
      "text": "Over sufficiently large timescales, the system does not drift away from this equilibrium, and samples  ( x, y ) can be considered drawn i.i.d. The equilibrium participa- tion strategies induce a stationary effective distribution D . from  D .",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "Unbiased gradient estimates. When the aggregator requests a training update, a subset of sensors, deter- mined by equilibrium conditions, provide local gradi- ents. Although not all sensors participate every time, the equilibrium ensures a stable pattern of participation.",
      "type": "sliding_window_shuffled",
      "tokens": 63,
      "augmented": true
    },
    {
      "text": "Since the regularizers are deterministic, their gradients ∇ Ω SNR ( θ )  and  ∇ Ω complexity ( θ )  do not introduce bias. Averaged over multiple rounds, the collected gradients form an unbiased estimator   b ∇ L ( θ )  of  ∇ L ( θ ) : \nE [ b ∇ L ( θ )] =  ∇ L ( θ ) . Although not all sensors participate every time, the equilibrium ensures a stable pattern of participation.",
      "type": "sliding_window_shuffled",
      "tokens": 121,
      "augmented": true
    },
    {
      "text": "Integration of Regularizers in Backpropagation and SGD \nDuring the training iteration  k : \n1. Forward pass:  Each participating sensor collects data ( x, y )  and evaluates  ℓ ( f θ k ( x ) , y ) . Since the regularizers are deterministic, their gradients ∇ Ω SNR ( θ )  and  ∇ Ω complexity ( θ )  do not introduce bias.",
      "type": "sliding_window_shuffled",
      "tokens": 108,
      "augmented": true
    },
    {
      "text": "Forward pass:  Each participating sensor collects data ( x, y )  and evaluates  ℓ ( f θ k ( x ) , y ) . 10 \n550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 \n2. Backward pass:  The sensor computes  ∇ θ ℓ ( f θ k ( x ) , y ) via standard backpropagation.",
      "type": "sliding_window_shuffled",
      "tokens": 193,
      "augmented": true
    },
    {
      "text": "Backward pass:  The sensor computes  ∇ θ ℓ ( f θ k ( x ) , y ) via standard backpropagation. To incorporate regu- larizers, the sensor (or the aggregator after collecting updates) adds  λ 1 ∇ Ω SNR ( θ k )  and  λ 2 ∇ Ω complexity ( θ k ) . These gradients are computed analytically since the regularizers are explicit, differentiable functions of  θ .",
      "type": "sliding_window_shuffled",
      "tokens": 122,
      "augmented": true
    },
    {
      "text": "Aggregation:  The aggregator averages the received gradients: b ∇ J ( θ k ) =  b ∇ L ( θ k )+ λ 1 ∇ Ω SNR ( θ k )+ λ 2 ∇ Ω complexity ( θ k ) . These gradients are computed analytically since the regularizers are explicit, differentiable functions of  θ . 3.",
      "type": "sliding_window_shuffled",
      "tokens": 102,
      "augmented": true
    },
    {
      "text": "4. Since  E [ b ∇ L ( θ k )] = ∇ L ( θ k ) , we also have E [ b ∇ J ( θ k )] =  ∇ J ( θ k ) . Aggregation:  The aggregator averages the received gradients: b ∇ J ( θ k ) =  b ∇ L ( θ k )+ λ 1 ∇ Ω SNR ( θ k )+ λ 2 ∇ Ω complexity ( θ k ) .",
      "type": "sliding_window_shuffled",
      "tokens": 137,
      "augmented": true
    },
    {
      "text": "4. Diminishing Step-Size and Convergence Results \nClassical convex optimization theory (see Bottou et al. Update step:  With a chosen step size  α k , \nθ k +1  =  θ k  − α k   b ∇ J ( θ k ) .",
      "type": "sliding_window_shuffled",
      "tokens": 78,
      "augmented": true
    },
    {
      "text": "(2009)) states that for con- vex, Lipschitz-smooth objectives and unbiased gradient oracles, SGD converges to a stationary point if the step sizes  { α k }  decrease at an appropriate rate. Diminishing Step-Size and Convergence Results \nClassical convex optimization theory (see Bottou et al. (2018) or Nemirovski et al.",
      "type": "sliding_window_shuffled",
      "tokens": 101,
      "augmented": true
    },
    {
      "text": "Under these conditions, we have: \nlim k →∞ E [ J ( θ k )] =  J ( θ ∗ ) and lim k →∞ E [ ∥∇ J ( θ k ) ∥ ] = 0 . A common choice is  α k  = 1 / √ \nk , but any diminishing sequence with P \nk   α k  =  ∞ and  P \nk   α 2 k   <  ∞ works. (2009)) states that for con- vex, Lipschitz-smooth objectives and unbiased gradient oracles, SGD converges to a stationary point if the step sizes  { α k }  decrease at an appropriate rate.",
      "type": "sliding_window_shuffled",
      "tokens": 169,
      "augmented": true
    },
    {
      "text": "Under these conditions, we have: \nlim k →∞ E [ J ( θ k )] =  J ( θ ∗ ) and lim k →∞ E [ ∥∇ J ( θ k ) ∥ ] = 0 . This implies  θ k  converges in expectation to a stationary point  θ ∗ of  J ( θ ) . Equilibrium Stability and Impact on Stationarity \nThe key subtlety is that  D  depends on equilibrium strategies.",
      "type": "sliding_window_shuffled",
      "tokens": 115,
      "augmented": true
    },
    {
      "text": "Equilibrium Stability and Impact on Stationarity \nThe key subtlety is that  D  depends on equilibrium strategies. This stability allows us to treat  D  as effectively fixed for the purpose of the asymptotic analy- sis. However, the equilibrium ensures a stable operating regime where sensor behaviors—and thus  D —do not change dras- tically over time.",
      "type": "sliding_window_shuffled",
      "tokens": 86,
      "augmented": true
    },
    {
      "text": "This stability allows us to treat  D  as effectively fixed for the purpose of the asymptotic analy- sis. If  D  were to drift significantly, standard SGD results would not directly apply. The equilibrium prevents such non-stationary behavior in the long run.",
      "type": "sliding_window_shuffled",
      "tokens": 59,
      "augmented": true
    },
    {
      "text": "They may alter the shape of  J ( θ ) , encouraging certain regions of parameter space, but they do not prevent convergence. The equilibrium prevents such non-stationary behavior in the long run. Furthermore, since the regularizers are deterministic and have bounded gradients, they do not add pathological con- ditions to the optimization landscape.",
      "type": "sliding_window_shuffled",
      "tokens": 79,
      "augmented": true
    },
    {
      "text": "Bounding and Selecting Hyperparameters  λ 1 , λ 2 \nThe choice of  λ 1  and  λ 2  affects the curvature of  J ( θ )  and can influence convergence speed and the location of  θ ∗ . They may alter the shape of  J ( θ ) , encouraging certain regions of parameter space, but they do not prevent convergence. On the con- trary, they may help by smoothing out undesirable minima or limiting model complexity.",
      "type": "sliding_window_shuffled",
      "tokens": 109,
      "augmented": true
    },
    {
      "text": "Bounding and Selecting Hyperparameters  λ 1 , λ 2 \nThe choice of  λ 1  and  λ 2  affects the curvature of  J ( θ )  and can influence convergence speed and the location of  θ ∗ . Some guidelines include: \n1. Start with small values of  λ 1  and  λ 2  to avoid over- whelming the primary loss  L ( θ ) .",
      "type": "sliding_window_shuffled",
      "tokens": 93,
      "augmented": true
    },
    {
      "text": "Gradually increase them if the model relies too heavily on high-SNR data or becomes too complex. Start with small values of  λ 1  and  λ 2  to avoid over- whelming the primary loss  L ( θ ) . 2.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "2. Ensure  λ 1  ≤ c 1 G 1   and  λ 2  ≤ c 2 G 2   for some constants c 1 , c 2  >  0 , to prevent excessively large gradients due to the regularizers. 3.",
      "type": "sliding_window_shuffled",
      "tokens": 57,
      "augmented": true
    },
    {
      "text": "If the model overfits high-SNR data, increase  λ 1 . 3. Tune  λ 1 , λ 2  based on validation performance.",
      "type": "sliding_window_shuffled",
      "tokens": 36,
      "augmented": true
    },
    {
      "text": "If it be- comes too large and slow to run, increase  λ 2 . By keeping  λ 1 , λ 2  within reasonable bounds, we ensure that the modified gradient   b ∇ J ( θ )  remains well-behaved, preserving the conditions for SGD convergence. If the model overfits high-SNR data, increase  λ 1 .",
      "type": "sliding_window_shuffled",
      "tokens": 84,
      "augmented": true
    },
    {
      "text": "The equilibrium ensures  D  remains stable, allowing classi- cal stochastic optimization theory to hold. By keeping  λ 1 , λ 2  within reasonable bounds, we ensure that the modified gradient   b ∇ J ( θ )  remains well-behaved, preserving the conditions for SGD convergence. We have shown that under the stated assump- tions—convexity and smoothness of ℓ , convexity and bounded gradients of  Ω SNR  and  Ω complexity , stationarity of  D  induced by equilibrium strategies, and unbiased gradient estimates—the diminishing step-size SGD applied to  J ( θ )  converges in expectation to a stationary point  θ ∗ .",
      "type": "sliding_window_shuffled",
      "tokens": 167,
      "augmented": true
    },
    {
      "text": "Proper selection and tuning of  λ 1 , λ 2  help main- tain stable and robust training dynamics. The regularizers, being convex and with bounded gradients, integrate seam- lessly into the backpropagation and SGD updates, shaping the optimization landscape but not invalidating convergence properties. The equilibrium ensures  D  remains stable, allowing classi- cal stochastic optimization theory to hold.",
      "type": "sliding_window_shuffled",
      "tokens": 95,
      "augmented": true
    },
    {
      "text": "11 Proper selection and tuning of  λ 1 , λ 2  help main- tain stable and robust training dynamics. Thus, the proposed training process achieves a harmonious balance: it respects the strategic, energy-constrained envi- ronment (through equilibrium and game-theoretic consider- ations), while leveraging well-established convex optimiza- tion guarantees to ensure convergence of the global model parameters.",
      "type": "sliding_window_shuffled",
      "tokens": 98,
      "augmented": true
    }
  ]
}