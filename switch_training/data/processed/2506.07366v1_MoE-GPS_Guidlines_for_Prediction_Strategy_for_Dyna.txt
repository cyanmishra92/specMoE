=== ORIGINAL PDF: 2506.07366v1_MoE-GPS_Guidlines_for_Prediction_Strategy_for_Dyna.pdf ===\n\nRaw text length: 53786 characters\nCleaned text length: 53337 characters\nNumber of segments: 30\n\n=== CLEANED TEXT ===\n\narXiv:2506.07366v1 [cs.LG] 9 Jun 2025 MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing Haiyue Ma Princeton University Zhixu Du Duke University Yiran Chen Duke University Abstract In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across different GPUs, which creates load imbalance as each expert processes different number of tokens. Recent works improve MoE inference load balance by dy- namically duplicating popular experts to more GPUs to process excessive tokens, which requires predicting the distribution before routing. In this paper, we discuss the tradeoff of prediction strategies, accuracies, overhead, and end-to-end system performance. We propose MoE-GPS, a framework that guides the selection of the optimal predictor design under various system configurations, by quantifying the performance impact to system-level model runtime. Specifically, we advocate for Distribution-Only Prediction, a prediction strategy that only predicts overall token distribution which significantly reduces overhead compared to the traditional Token-to-Expert Prediction. On Mixtral 8 7B MMLU dataset, MoE-GPS suggests Distribution-Only Prediction which improves end-to-end inference performance by more than 23 compared with Token-to-Expert Prediction. 1 Introduction Distribution-Only Prediction Token-to-Expert Prediction Bottlenecked by Communication? High Load Imbalance? MoE-GPS Model Architecture System Configuration Best Prediction Strategy Y N Y N Figure 1: MoE-GPS guidelines for selecting op- timal expert prediction strategies that minimizes end-to-end inference latency based on model and hardware characteristics. Mixture-of-Experts (MoE) [13, 15, 7, 28] mod- els reduce the computation of Large Language Models (LLMs) by activating only a subset of experts per token. In large multi-GPU datacen- ters, Expert Parallelism (EP) [17] is typically used for Feed Forward Network (FFN) layers, where each GPU only hosts one or few experts. However, due to skewed token-to-expert distri- bution [26, 3], this approach often results in load imbalance, underutilized resources, and in- creased end-to-end latency. The growing num- ber of experts in modern LLM models exagger- ates the imbalance [18]. While both training and inference can have im- balance, existing works proactively balance to- ken distribution in training, such as using aux- iliary loss [33] which can achieve near-perfect balance. For inference, expert-to-token mappings are fixed and imbalance is unavoidable. A popular approach to mitigate load imbalance is expert Preprint. Under review. FFN Layers All-to-All Shuffle Tensor Parallelism Expert Parallelism E1 E2 E3 E4 Tokens E1 Distribution GPU Expert Routing Attention Layers Ring Reduce- Scatter Figure 2: Overview of a typical Mixture-of-Expert inference based on a Transformer network, with four-way Tensor Parallelism and Expert Parallelism. Four Experts (denoted E1-E4) each resides in one GPU. Routing assigns each token to its expert, which creates load imbalance for both compute and communication, if the token-to-expert distribution is skewed. duplication [3, 34, 9, 20], where heavily used experts are replicated across GPUs to distribute tokens more evenly. Since the token distribution changes over time, dynamic duplication is often needed, which requires a predictor for the distribution ahead of the routing stage. Higher prediction accuracy improves load balancing but also incurs greater overhead, creating a complex tradeoff between pre- dictor complexity, accuracy, and system performance. Moreover, the effectiveness of each prediction strategy depends on multiple factors, such as workload characteristics, hardware communication schemes, and the degree of token imbalance. Despite its importance, there is currently no systematic method to model the runtime implications of MoE load imbalance, and to help choose the best predictor for different workload and system setups. We present MoE-GPS, a framework that simulates end-to-end MoE inference performance with imbalance, and guides the selection of expert prediction strategies that yield the shortest runtime. Built on top of an architectural simulator, LLMCompass [36], MoE-GPS models the runtime tradeoffs among prediction strategies, accuracy, and overhead. Given an arbitrary model architecture and hardware setup, MoE-GPS identifies the strategy that delivers the best system performance. With insights from MoE-GPS, we advocate for Distribution-Only Prediction strategy, which only predicts the coarse-grained token distribution across experts instead of exact token-to-expert mappings (Token-to-Expert Prediction). This lightweight approach is particularly effective when communication is not a bottleneck, because it reduces prediction complexity and still improves compute load balancing. Exact token-level prediction optimizes both computation and communication at the cost of higher overhead, which becomes more favorable when communication cost dominates. In addition, we observe that Distribution-Only Prediction performs better with more balanced workloads. These findings are summarized as design guidelines in Figure 1 and further elaborated in Section 4 and 5. Our contribution can be summarized as follows: We propose MoE-GPS, a system performance simulation framework that selects the optimal expert prediction strategy to minimize end-to-end inference latency, given a model and hardware setup. We identify and validate the effectiveness of Distribution-Only Prediction as a lightweight alternative to Token-to-Expert Prediction, offering better scalability and efficiency under varying system bottlenecks. For example, Distribution-Only Prediction introduces more than 23 system-level performance improvements on Mixtral 8 7B MMLU dataset compared to Token-to-Expert Prediction. Our work provides a practical tool and actionable insights for designing MoE predictors that improve inference speed across various workloads and hardware configurations. 2 Background Load Imbalance in Inference Prefill Is Critical. Imbalance can occur in both training and inference in a MoE network. Current approaches mitigates training imbalance by proactively routing tokens to less popular experts [33]. In inference, token-to-expert mapping is fixed. Discarding or re-routing excessive tokens on popular experts may improve balance at the cost of model accuracy. 2 In this paper, we focus on inference prefill, and do not make changes to the original token-to- expert mapping. The decode stage has much less tokens being processed at the same time and is latency-critical, and load balancing is a secondary issue. Figure 2 shows the overall flow of a Transformer Block in a Mixture-of-Expert inference workload. For illustration purposes, we assume a four-GPU system, four experts (denoted E1-E4) each residing on one GPU, and each token is routed to its top-1 experts [7]. The general concepts apply to arbitrary number of devices, experts, and tokens with top-K experts [14, 37]. Expert Parallelism Leads to Imbalance. The parallelism scheme we use is Tensor Parallelism (TP) for the Attention layers and Expert Parallelism (EP) for the FFN layers [30]. Using TP for the Attention layers excludes the need to duplicate KV cache for adjacent tokens when the sequence is split to different GPUs. We used Ring All Reduce [23] to optimize the communication runtime after TP. Using EP for the FFN layers has two benefits over TP: first, it reduces communication latency by the factor of number of devices; second, it avoids splitting the weights in each expert to narrower matrices, which can lower Tensor Core utilization and increase compute time. The introduction of EP assigns experts to certain GPUs and creates load imbalance. Industry practices suggest more advanced hybrid parallelism (TP EP) [21] when necessary. For simplicity, this work assumes TP-only Attention and EP-only FFN, and the insights are generalizable. Quantifying Imbalance. We assume that Expert 1 is the most popular expert which takes 75 of all tokens as shown in Figure 2. FFN computation and communication runtime will be dominated by GPU 1 which has Expert 1. To quantify the load imbalance of each GPU, we introduced a parameter called "skewness", the number of tokens in the most popular expert divided by the average number of tokens per expert if the workload is evenly distributed: skewness of tokens in the most popular expert of average tokens per experts Figure 2 s workload skewness is 3 since Expert 1 has 75 of the tokens but the average token per expert is 25 . Performance Impacts of Load Imbalance. Skewness only impacts the runtime of FFN compute and communication since tokens are routed to the experts for FFN. FFN computation typically fully utilizes GPU resources, and the bottleneck FFN runtime is increased by a factor of the skewness. Routing incurs inter-device communication. With a fully connected multi-GPU topology, for a perfectly balanced distribution (skewness 1) each GPU needs to move (N 1) N of its existing tokens to other GPUs (N number of GPUs), assuming the token placement is completely random after scatter. This yields (N 1) N 2 tokens to move per GPU. However, for a workload with skewed distribution, the GPU with the most tokens will be receiving more tokens than other GPUs, and thus has longer communication latency. The overall communication time, which is bottlenecked by the GPU with the most popular expert, is scaled by skewness: (N 1) skewness N 2. The same amount of communication happens for All-to-All shuffle after the FFN layers. Current Solutions. In a multi-GPU datacenter setting, many existing solutions have proposed expert duplication - mirroring popular experts in other GPUs - to offload excessive token processing. MoE-Prediction [3] observes highly skewed token-to-expert distribution and proposes predicting such distribution to guide expert placement. Prophet [34], FlexMoE [20], SE-MoE [29], and FasterMoE [9, 8] propose specific strategies for dynamic expert duplication. Other techniques focus on reducing the distribution imbalance in training by imposing constraints, such as auxiliary load balancing [33] and expert biasing [18]. In a different setting with fewer GPUs and the GPU memory capacity is limited for storing experts, token-to-expert mappings are also predicted to facilitate expert offloading to CPU [5]. Others proposes different solutions to serve as the token-to-expert mapping, such as expert buffer that stores the hottest experts [12] and expert activation correlation model [35]. These solutions are orthogonal to the discussion of this paper. 3 E3 E1 E2 E1 E1 E1 E4 Token-to-Expert Mapping Prediction (skipping Scatter) Distribution-Only Prediction Prediction Strategy Ring Reduce- Scatter Attention Layers Predictor Inference Expert Duplication FFN Layers Figure 3: Integration of expert duplication and different prediction strategies into the MoE model architecture for a single layer. Token-to-Expert Prediction eliminates communication by directly routing tokens to their predicted GPUs, while Distribution-Only Prediction assumes random token distribution during scattering. 3 Methodology This section discusses our approach to modeling and analyzing expert prediction strategies in MoE inference. We describe the integration of expert duplication into the MoE model architecture, and the two different prediction strategies (Distribution-Only and Token-to-Expert). We also show normalized system performance obtained from a performance simulator, LLMCompass [36], to illustrate high-level trends of the runtime implications of expert duplication. We defer detailed analyses of LLMCompass s simluated performance to Section 4. 3.1 Approach to Expert Duplication We integrate dynamic expert duplication into the MoE model by inserting a pre-trained predictor before Attention in each layer. For a chosen frequency, the predictor does inference with current batched inputs to predict distribution and guide the expert placement. Since dynamic duplication incurs extra inter-GPU communication for moving experts, existing works propose different intervals for prediction and moving, from every single batch [8, 34] to every 10 minutes [18], to balance overhead and effectiveness. In this study, we assume single-batch prediction and placement frequency. We show calculation in Section 5 that the placement latency can be hidden within the Attention layers which is compute- bound with moderate batch sizes. Our simulator can be configured to model different frequencies of prediction and placement, by averaging out the overhead to multiple batches. Figure 3 illustrates how expert duplication helps balance token workloads. In our example, Expert 1 has the most tokens, so it is replicated across multiple GPUs to evenly distribute the load. In general, given arbitrary token-to-expert mappings, experts can be duplicated to achieve per-GPU balance by iteratively shifting experts from overloaded to underloaded devices: keep duplicating the experts on GPUs with 1 N tokens to GPUs with 1 N tokens until all GPUs process the same amount of tokens. A detailed algorithm can be found in Algorithm 1. 3.2 Prediction Strategies and Tradeoffs We explore two prediction strategies with distinct tradeoffs: Distribution-Only Prediction, which estimates static, aggregate expert usage; and Token-to-Expert Prediction, which targets exact token- level routing. The former has lower overhead and complexity and targets at compute imbalance, while the latter can reduce both compute and communication costs at the cost of high overhead. 3.2.1 Distribution-Only Prediction Distribution-only prediction estimates the proportion of tokens routed to each expert (e.g., Expert 1 gets 75 of tokens) without specifying which tokens. This enables balanced compute across GPUs but does not reduce communication costs, as tokens are still randomly scattered post-ring all-reduce. 4 Algorithm 1: Expert Duplication in MoE Load Balancing Input: Token expert map f : {1, . . . , T} {1, . . . , E}; Per-GPU memory capacities M NG; Initial placement P {1, . . . , E} {1, . . . , G}; Maximum copies per expert Cmax. Output: Balanced placement P and dispatch d : {1, . . . , T} {1, . . . , G}. 1 Assign each token t to any GPU g that hosts its expert: d(t) min{g (f(t), g) P}; 2 Lg {t d(t) g} for all g {1, . . . , G}; 3 while maxg Lg ming Lg 1 do 4 gh arg maxg Lg; gc arg ming Lg; 5 l Lgh Lgc 2 m ; 6 e arg maxe E(gh) {t d(t) gh f(t) e} ; 7 if (e , gc) P and copies(e ) Cmax and params(e ) Mgc then 8 copy weights of e on gc; P P {(e , gc)}; 9 Reassign the first tokens {t d(t) gh f(t) e } to gc by setting d(t) gc; 10 Update Lgh and Lgc; 11 return P, d; We model the expert activation distribution in each layer of the MoE model using a multinomial distribution and estimate its parameters via Maximum Likelihood Estimation (MLE). The multinomial distribution is commonly used to model counts of outcomes across discrete classes, in our case the selection of experts within a layer. MLE selects the distribution that the observed data has the maximum likelihood to fit into. Table 1: Impact of skewness on expert distribution estimation and system performance. Higher skew- ness leads to higher error rate, indicating reduced estimation accuracy and degraded performance. Dataset Skewness Error rate ( ) MMLU 1.39 1.80 Alpaca Eval 1.40 0.98 SST2 1.99 16.00 Formally, let pl i denote the probability of se- lecting expert i in layer l. Assuming that each token s expert selection is an independent and identically distributed (i.i.d.) sample from this multinomial distribution, the MLE of pl i is given by: ˆpl i nl i N , (1) where N is the total number of tokens observed at layer l, and nl i is the number of tokens that activated expert i. When the training data come as batches, the estimation becomes a moving average. We note that expert selection is primarily governed by local token-level features and routing mechanisms. Hence, modeling activations as independent draws to mimick per-batch distribution provides a reasonable approximation while significantly simplifying analysis. We provide detailed derivation in Appendix A. We experiment the Distribution-Only Prediction on three datasets: MMLU [10], Alpaca Eval [6], and SST2 [31], on the Mixtral 8 7B [14] MoE model. For each batch, we set sequence length to 512 and measure average skewness across batches: 1.388, 1.402, and 1.990 respectively. We measure the averaged error rate over layers between our estimation of the probability on the trainset and empirical probability on the testset. We define error rate as ˆp p 1 of experts. Higher error rate indicates less accurate estimation. For datasets that do not have a dedicated test split, we use train test split to randomly partition the trainset with 80 training samples and 20 test samples. We evaluate the normalized end-to-end system performance across all datasets given the model size (Mixtral 8 7B), network parameters (batch size 1, sequence length 512), and hardware configurations (four A100 GPUs, fully connected with NVLINK). Performance is simulated with an augmented version of LLMCompass [36]. The error rate is used to scale the runtime of the GPU that processes the most tokens, which will be detailed in Section 3.3. 5 0.0 0.2 0.4 0.6 0.8 1.0 Prediction Accuracy 0.0 0.1 0.2 0.3 Overhead ( Runtime) 0.0 0.5 1.0 Normalized Performance (a) Mixtral 8 7B (skew 1.4). 0.0 0.2 0.4 0.6 0.8 1.0 Prediction Accuracy 0.0 0.1 0.2 0.3 Overhead ( Runtime) 0.0 0.5 1.0 Normalized Performance (b) Mixtral 8 7B (skew 2.0). Figure 4: Trade-off between prediction accuracy and end-to-end system performance for Token-to- Expert Prediction. Increasing prediction accuracy improves expert placement but also raises runtime overhead. Although higher accuracy results in better initial expert placement, excessive overhead can reduce overall gains. In higher-skewness settings (right), accurate predictions are easier to achieve, shifting the optimal performance point toward higher accuracy. Table 1 shows that lower error rate in expert distribution (more accurate estimation) leads to improved system performance. Notably, higher skewness results in larger error rates and degraded normal- ized performance. This is primarily because greater skewness leads to underutilized experts that receive fewer tokens. The reduced token count increases estimation error for these experts, which disproportionately contributes to the overall error rate. 3.2.2 Token-to-Expert Prediction Token-to-Expert Prediction targets exact routing of each token to its expert s GPU. With predicted mapping, we can send each token to the GPU that has its experts directly. This strategy skips the scatter phase in communication after the ring all-reduce, and saves both FFN compute and communication runtime, as shown in Figure 3. We formulate the expert selection task as a classification problem, where the goal is to predict the activated expert for each token in the batch that will be processed by the MoE model. We explore three types of models: a simple probability-based model, a conditional probability model, and neural network-based predictors. Probability Model. We always assign the expert with the highest global frequency observed in the training data, treating all tokens identically regardless of their identity or position. Conditional Probability Model. This model improves by conditioning on either the token index or its position index. For each token (or position), we select the expert that appears the most frequently in the training data for that specific token index or position index. Neural Networks. We train neural models using pairs of token embeddings and their corresponding expert activations. The models are optimized using cross-entropy loss and the Adam optimizer [16] and trained to converge. All samples are padded to sequence length 512. We experiment with both simple Feed-Forward Networks (FFNs) and Long Short-Term Memory networks (LSTMs) [11]. The detailed architectures of these networks are provided in Appendix B. We investigate Token-to-Expert Prediction on the same three datasets, i.e., MMLU, Alpaca Eval, and SST2. The results are shown in Figure 4a with the MMLU and Alpaca Eval data set (shown together because they have similar skewness) and Figure 4b with the SST2 dataset, for Mixtral 8X7B. The x-axis refers to the prediction accuracy for the predictor, each point representing a different predictor being used. The overhead y-axis shows the percentage of the prediction overhead of the total runtime for the MoE model, measured on A100 GPUs. The normalized performance y-axis is the simulated end-to-end system performance by LLMCompass, using four A100 GPUs fully connected with NVLink, and batch size 1 and sequence length 512. The runtime includes the overhead of each prediction strategy. We use exponential functions to fit the accuracy to overhead curves and polynomial functions to fit the accuracy to performance curves. Higher prediction accuracy can result in better initial expert placement leading to better system performance. However, higher accuracy also incurs larger overhead, which degrades the end-to-end 6 performance. The figure demonstrates a trade-off that it is not always the case that predictors with higher accuracy are better. Figure 4a and Figure 4b also demonstrates results of different skewness. Note that, for scenarios with higher skewness, it costs less for the predictor to acquire higher accuracy. Consequently, the sweet point of the system performance moves towards higher accuracy end. 3.3 Modeling the Effect of Prediction Error Optimistic: T Typical: T (1 ε) Errors averaged across all devices ε Error Rate T Avg. Time per Device N Num. of Devices ε ε εε Pessimistic: T (1 ε (N-1)) All errors on one device Figure 5: Modeling the impact of prediction errors on end-to-end system runtime. Three scenarios for the same prediction error rate ϵ: (1) Optimistic errors do not affect load balancing; (2) Typical errors are evenly dis- tributed across devices, leading to moderate slowdown; (3) Pessimistic errors concen- trate on one device, causing worst-case load imbalance. We model the system-level effects of imperfect pre- diction by analyzing how different error distributions affect FFN computation load balance and communi- cation cost. First, we model FFN compute s runtime with predic- tion error rate ϵ, which is 1 accuracy for Token- to-Expert Prediction, and averaged L1 distance over number of experts for Distribution-Only Prediction. Figure 5 shows three potential outcomes for the same prediction accuracy for FFN compute s runtime. As- sume that ϵ 0.1: Optimistic: Errors still result in perfect load balancing; e.g. predicting 85 of tokens instead of 75 for Expert 1 in Figure 2. Typical: Errors are uniformly distributed across GPUs, leading to moderate imbal- ance. The most loaded GPU processes up to (1 ϵ) avg_tokens. This is the default model used in our runtime simulations. Pessimistic: All errors occur on a single GPU, leading to worst-case imbalance where the bottleneck GPU handles up to N (1 ϵ) avg_tokens. While unlikely to happen, this scenario represents an upper bound on performance degradation. We apply the same typical-case assumption when modeling communication overhead under Expert Parallelism (EP) for Token-to-Expert Prediction. Unlike compute, however, communication costs always increase with prediction errors, as misrouted tokens inevitably trigger additional inter-GPU data transfers. Optimistic cases do not exist in this context. 3.4 Network Performance Simulation We conduct performance evaluations using an extended version of LLMCompass [36], a block- level simulator for large language model inference, validated with silicon measurements. This simulation-based approach allows us to easily explore design tradeoffs across hardware and software configurations without requiring access to real clusters, which are often costly and impractical for exhaustive testing. Our simulations focus on a single Transformer layer, modeling all relevant operations including GEMM, communication, and element-wise computations. Since LLMCompass does not yet support FlashAttention [4], our Attention layer runtimes are conservatively overestimated. We augmented LLMCompass with: MoE and Expert Parallelism (EP): We introduced custom modules to model EP-specific communication and FFN workloads. Mixtral Support: We added support for Mixtral-style model architectures, integrating existing Grouped Query Attention (GQA) [1] and SwiGLU [27] activation implementations. We also implemented Sliding Window attention [2]. Prediction Strategy Modeling: We added support for evaluating both Distribution-Only and Token-to-Expert prediction strategies with tunable accuracy and overhead. 7 1.0 1.4 1.8 2.2 Skewness 0.0 0.2 0.4 0.6 0.8 1.0 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (a) Baseline latency with no predic- tion (interconnect NVLink). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To-Expert Prediction(Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink). 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 1.0 1.5 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (d) Latency of different prediction strategies and accuracies (intercon- nect PCIe). Figure 6: Simulated prefill latency for a single layer of Mixtral 8 7B under different prediction strate- gies and interconnect types. Latency is broken down by component (attention, FFN, communication, overhead) and evaluated across skewness levels on 4 A100 GPUs using NVLink (top) and PCIe (bottom). (a, c) show baseline latencies without prediction; (b, d) show improvements from prediction strategies at varying accuracies. Distribution-Only Prediction reduces FFN compute without overhead, while Token-to-Expert Prediction introduces overhead that trades off with improved load balancing. For each skewness, the best preditor has the minimum total latency across strategies and accuracies. 4 Results In this section, we discuss how to choose the best predictor for overall system performance under different system configurations, workload sizes and token distribution skewnesses. Our goal is to identify when to use Distribution-Only Prediction, which reduces FFN computation load imbalance without communication savings, and when to use Token-to-Expert Prediction, which additionally reduces communication latency at the cost of higher predictor complexity. Additionally, for Token-to- Expert Prediction, we want to find the optimal prediction accuracy to minimize overall runtime. Figure 6 shows the time-to-first-token latency for a single layer of Mixtral 8 7B prefill workload, simulated by LLMCompass. To match the experimental setup described in Section 3.2, we set the batch size to 1 and the sequence length to 512. The simulation includes all architectural components of Mixtral, such as a 4K sliding window, Grouped Query Attention, and SwiGLU activation. For system configurations, we modeled four A100s, either connected via NVLink 3.0 [22] (high-end interconnect with 2 TB s bandwidth) or PCIe 4.0 [24] (low-end interconnect with 32GB s bandwidth). Figures 6a and 6c show baseline latencies without any prediction strategy with NVLink and PCIe interconnect respectively. While FFN and Attention latencies remain constant across configurations, PCIe s limited bandwidth makes communication latency a dominant bottleneck. 8 Figure 6b and 6d show the impact of applying prediction strategies under varying skewness and accuracy. For Token-to-Expert Prediction, we use the fitted curve in Figure 4 to model the overhead for each prediction accuracy. For skewness numbers without a matching dataset, we interpolated the overhead from the skewnesses that we measured in Section 3.2. Within each skewness group, the leftmost bar shows latency without prediction, same as those in Figure 6a and 6c. The second bar shows latency under Distribution-Only Prediction. This strategy reduces FFN compute time, though communication time remains unchanged. Since distribution is estimated offline, there is no prediction overhead. As skewness increases, distribution estimation becomes less accurate, slightly diminishing its benefits. The remaining stacked bars (shown as curves) show latencies for Token-to-Expert Prediction across multiple accuracy levels. Each curve exhibits a U-shape: higher accuracy improves load balancing (reducing FFN and communication time), but also increases overhead. The optimal configuration is the one with the lowest total latency, typically at an intermediate accuracy level. We see that Distribution-Only Prediction performs better in most cases. For example, with skewness 1.4 (a typical case, close to MMLU dataset), Distribution-Only Prediction achieves 23 speedup compared to the best configuration of Token-to-Expert Prediction (the bottom of the U-shape). Compared to Distribution-Only, Token-to-Expert Prediction incurs significantly higher overhead, especially at low skewness where it requires more complex models. However, it becomes beneficial when 1) skewness is high: accurate predictions are easier to make, reducing overhead, and 2) communication cost is high (e.g., PCIe): saved communication time outweighs added prediction cost. Across all configurations, FFN latency reductions from prediction are largely skewness-independent. This is because mispredictions are measured relative to a perfectly balanced reference scenario. Only overhead varies with skew as higher skewness makes prediction easier. Figure 7: Simulated effectiveness of two predic- tion strategies best savings for Mixtral 8X7B on 4 fully-connected A100 with different system inter- connect settings. To clearly draw our insights on which predic- tion strategy is better under different scenar- ios, we plotted the runtime savings on base- line (no prediction) of the best configuration of Token-to-Expert Prediction (the point of lowest latency for each skewness), and com- pared that with the savings of Distribution- Only Prediction. Figure 7 shows the differ- ence in savings for the two strategies, calculated by Distribution-Only Prediction saving Token-to-Expert Prediction saving. Every bar above zero indicates Distribution-Only Pre- diction outperforms the best configuration of Token-to-Expert Prediction, and every bar be- low zero indicates otherwise. The 600GB s and 64GB s interconnect bandwidth correspond to settings in Figure 6b and 6d respectively. The other two are arbitrary interconnect settings to model cases where a mixture of both interconnects are used (such as across NVLink domains). Key takeaways: Distribution-Only Prediction is preferred when skewness is low or communication is not a bottleneck. Its low complexity and zero overhead make it a strong strategy. Token-to-Expert Prediction becomes more effective as skewness increases, since predictors achieve high accuracy at lower cost. It is particularly beneficial under low-bandwidth interconnects like PCIe, where communication savings dominate. 5 Discussion and Limitation Generality across model architectures. While our results focus on Mixtral 8 7B, the core insights generalize across other MoE-based architectures such as Switch Transformer [7], LLaMA variants [37, 32], and DeepSeek [18]. Although these models differ in their expert construction and token routing 9 decisions, we observe consistent system-level behaviors for both Distribution-Only and Token-to- Expert predictions. Specifically, for token-to-expert mapping, more complex predictors lead to higher accuracy but also higher overhead, which brings negative impact to FFN layers performance. Supporting experiments for LLaMA and Switch Transformer are provided in Appendix C. At the system level, Attention and FFN dominate the cost of MoE inference across models. LLaMA shares Mixtral s SwiGLU-based FFN but lacks sliding-window Attention; Switch Transformer uses ReLU in FFN and does not use Grouped Query Attention (GQA)[1]; DeepSeek introduces Multi-head Latent Attention (MLA)[19] to further optimize Attention. Despite these differences, FFN layers remain a substantial portion of runtime, and expert duplication yields consistent benefits across all. Additionally, scaling model size (e.g., Mixtral 8 7B vs. 8 22B) changes absolute latency but not the qualitative trends or effectiveness of each prediction strategy. Generality across hardware systems. In our study, we assumed fully connected GPUs with the same interconnect bandwidth between each pair. For a larger-scale system with more GPU nodes, different topologies including Mesh, Torus, and Tree topologies can be used. These topology choices will impact specific runtime but are orthogonal to our core insights, and can be modeled by changing the topology implementation. Further, while we assume full TP for Attention layers and full EP for FFN layers in this study, hybrid parallelism (such as using TP EP for FFN) have been proven to be useful in certain settings. Support can easily be added by incorporating current frameworks for hybrid parallelism [25]. Long sequence lengths. The experiments shown in the paper use a sequence length of 512. Longer sequences introduce new tradeoffs, particularly for Token-to-Expert Prediction. For FFN-based predictors, although computation remains parallelizable and overhead manageable, we observe a lower bound on achievable accuracy as sequence length increases. LSTM-based predictors are theoretically sequence-length agnostic in accuracy but suffer from poor parallelism and can hardly scale across different devices. Therefore, under long-sequence workloads, Distribution-Only Prediction may become more favorable due to its scalability and low complexity. Kernel underutilization at small scale. To enable fast evaluation, we use small batch sizes and short sequences, which can expose low-level inefficiencies in kernel execution, such as insufficient overlap between prologue epilogue and the main MMA loop, which leads to underutilization of Tensor Core FLOPs. Our simulator is throughput-oriented and assumes at least one hardware unit (in this case, compute) is saturated. To validate our findings, we compared simulated results against actual GPU measurements for both model and predictor inference. Although the absolute runtimes differ, the relative overhead between prediction and inference is consistent. Thus, we report and analyze prediction overhead as a ratio to the simulated inference runtime for an accurate estimation. Expert duplication s communication overhead. We also quantify the communication overhead of expert duplication to show that we can hide its latency with the Attention layers. For Mixtral 8 7B, each FP16 expert contains approximately 4,096 14,336 2 2 bytes of weight data. Assuming one expert is sent and received per GPU per layer, the transfer takes 0.1 ms over NVLink 3.0 (2 TB s bandwidth), smaller than the Attention layer runtime for batch size 1 and sequence length 512. In practice, expert weights are static across inputs, and this duplication can be hidden with Attention computation. Even with PCIe 4.0 (32 GB s), the duplication latency can be hidden with modest increases in batch size or sequence length (e.g., batch size 16, sequence length 2K), which are still on the low-end of practical inference workload sizes. In more advanced systems, expert prefetching or pipelined movement between layers can further reduce this cost. 6 Conclusion In this work, we presented MoE-GPS, a framework for selecting expert prediction strategies that minimize end-to-end inference latency in Mixture-of-Experts models. By modeling the trade-offs between prediction accuracy, overhead, and system runtime across different hardware and workload configurations, MoE-GPS provides actionable guidance for system designers. We also demonstrated the effectiveness of Distribution-Only Prediction as a lightweight alternative to token-level prediction, particularly in scenarios where load imbalance is low and communication is not a bottleneck. 10 References [1] Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [3] Peizhuang Cong, Aomufei Yuan, Shimao Chen, Yuxuan Tian, Bowen Ye, and Tong Yang. Prediction is all moe needs: Expert load distribution goes from fluctuating to stabilizing. arXiv preprint arXiv:2404.16914, 2024. [4] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:16344 16359, 2022. [5] Zhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai Li, and Yiran Chen. Sida: Sparsity-inspired data-aware serving for efficient and scalable large mixture-of-experts models. Proceedings of Machine Learning and Systems, 6:224 238, 2024. [6] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [7] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1 39, 2022. [8] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: A fast mixture-of-expert training system. arXiv preprint arXiv:2103.13262, 2021. [9] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 120 134, 2022. [10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [11] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735 1780, 1997. [12] Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, and Benjamin Lee. Towards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference. arXiv preprint arXiv:2303.06182, 2023. [13] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79 87, 1991. [14] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [15] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181 214, 1994. [16] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [17] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi- tional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. 11 [18] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [19] Fanxu Meng, Zengwei Yao, and Muhan Zhang. Transmla: Multi-head latent attention is all you need. arXiv preprint arXiv:2502.07864, 2025. [20] Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Gang Cao, and Bin Cui. Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement. Proceedings of the ACM on Management of Data, 1(1):1 19, 2023. [21] NVIDIA. Mixture of experts package. developer-guide latest api-guide moe.html, 2025. [22] NVIDIA. Nvlink and nvlink switch. nvlink , 2025. [23] Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of worksta- tions. Journal of Parallel and Distributed Computing, 69(2):117 124, 2009. [24] PCI-SIG. Pci express base specification revision 4.0, version 1.0. Technical report, PCI Special Interest Group, 2017. Available at [25] Le Qin, Junwei Cui, Weilin Cai, and Jiayi Huang. Chimera: Communication fusion for hybrid parallelism in large language models. In Proceedings of the 52nd Annual International Symposium on Computer Architecture, ISCA 25. ACM, 2025. [26] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of- experts inference and training to power next-generation ai scale. In International conference on machine learning, pages 18332 18346. PMLR, 2022. [27] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of- experts layer. In International Conference on Learning Representations, 2017. [29] Liang Shen, Zhihua Wu, WeiBao Gong, Hongxiang Hao, Yangfan Bai, HuaChao Wu, Xinxuan Wu, Jiang Bian, Haoyi Xiong, Dianhai Yu, et al. Se-moe: A scalable and efficient mixture-of- experts distributed training and inference system. arXiv preprint arXiv:2205.10034, 2022. [30] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [31] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631 1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. [32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [33] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024. [34] Wei Wang, Zhiquan Lai, Shengwei Li, Weijie Liu, Keshi Ge, Yujie Liu, Ao Shen, and Dongsheng Li. Prophet: Fine-grained load balancing for parallel training of large-scale moe models. In 2023 IEEE International Conference on Cluster Computing (CLUSTER), pages 82 94. IEEE, 2023. 12 [35] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. Edgemoe: Fast on-device inference of moe-based large language models. arXiv preprint arXiv:2308.14352, 2023. [36] Hengrui Zhang, August Ning, Rohan Baskar Prabhakar, and David Wentzlaff. Llmcompass: Enabling efficient hardware design for large language model inference. In 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 1080 1096. IEEE, 2024. [37] Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. Llama-moe: Building mixture-of-experts from llama with continual pre-training. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15913 15923, 2024. 13 A MLE for Multinomial Distribution We seek to model the expert activation pattern in a Mixture-of-Experts (MoE) model using a proba- bilistic approach. Specifically, we are interested in estimating the distribution over expert selections for each MoE layer, based on observed activation frequencies in the training data. To this end, we adopt a multinomial modeling framework and employ Maximum Likelihood Estimation (MLE) to infer the activation probabilities. Assumptions. We assume that each token independently selects an expert from a fixed pool of K experts in a given layer. Let E {e1, . . . , eK} denote the set of experts in a particular MoE layer. For each token routed through the layer, the expert selection is modeled as an i.i.d. draw from a multinomial distribution with parameters p (p1, . . . , pK), where pi is the probability that expert ei is selected. Naturally, PK i 1 pi 1 and pi 0 for all i. Maximum Likelihood Estimation. Given N tokens routed through the layer, let ni denote the number of tokens that selected expert ei, so that PK i 1 ni N. The likelihood of the observed expert assignments under the multinomial distribution is: L(p) Pr(n1, . . . , nK p) N! n1! nK! K Y i 1 pni i . (2) To estimate p via MLE, we maximize the log-likelihood: log L(p) log N! n1! nK! K X i 1 ni log pi. (3) Ignoring the constant term that does not depend on p, the optimization problem reduces to: max p K X i 1 ni log pi (4) s.t. K X i 1 pi 1, pi 0. (5) This is a standard constrained optimization problem, and the solution is obtained via the method of Lagrange multipliers. The resulting MLE estimator for each expert s activation probability is: ˆpi ni N , i {1, . . . , K}. (6) B Predictor Architectures e formulate the expert selection problem in Mixture-of-Experts (MoE) as a multi-class classification task, where the objective is to predict the activated expert for each token in a sequence. Let T denote the set of input tokens, and let E {e1, . . . , eK} denote the set of experts available in a given MoE layer. For each token t T , the goal is to predict an expert label yt {1, . . . , K} that will be used by the MoE routing mechanism. We explore three modeling paradigms for this task: a global frequency-based model, a conditional frequency model, and neural network-based predictors. Probability-Based Model. This baseline treats all tokens identically and assigns each token to the expert that is most frequently activated in the training data. Let ni be the number of times expert ei was selected across all tokens in the training corpus. The model estimates the global activation probabilities using maximum likelihood as: 14 ˆpi ni PK j 1 nj , i {1, . . . , K}. (7) The predicted expert for any token is then: ˆyt arg max i ˆpi. (8) This approach ignores token-specific context, providing a static prediction rule that reflects global expert utilization frequencies. Conditional Probability Model. To improve over the static assignment, we consider a token- or position-conditioned frequency model. Let It be the token index or its absolute position in the sequence. For each token index i, we count how many times each expert ek was selected and compute: ˆpk i nk,i PK j 1 nj,i , (9) where nk,i denotes the number of times token index i selected expert ek. The model then predicts: ˆyt arg max k ˆpk It. (10) This conditional model captures per-token or per-position biases in expert activation. Neural Networks. To learn token-aware expert selection strategies, we train neural models that take token embeddings as input and predict the corresponding expert activation for each MoE layer. Each model is trained with cross-entropy loss and optimized using the Adam optimizer. Input sequences are padded to a fixed length of 512 tokens during training, and separate classifiers are maintained for each layer of the MoE model. We experiment with the following two architectures: Feed-Forward Network (FFN). The FFN model is a lightweight two-layer MLP. Each input token embedding (of dimension 4096 for Mixtral) is first passed through a linear projection to a 128-dimensional hidden space, followed by a ReLU activation. This is then followed by another linear layer of the same hidden size. Finally, for each target MoE layer, a separate classifier head is implemented as a linear layer mapping from the 64-dimensional hidden state to 8 expert logits. The FFN model is shared across tokens and layers, with layer-specific output heads. LSTM with Sparse Attention. To capture temporal dependencies, we also design a re- current model based on an LSTM encoder augmented with sparse attention. The input token embeddings are first projected from dimension 4096 (Mixtral) to 128 using a linear compression layer, followed by a ReLU activation. These projected embeddings are passed through a 2-layer LSTM with hidden size 64, applied in a batch-first manner. To enhance contextual modeling, we incorporate a sparse attention mechanism over the LSTM outputs (i.e., attention is applied using the LSTM outputs as query, key, and value). A residual connection is then added between the attention output and a separate feedforward transfor- mation of the compressed input. Finally, for each MoE layer, a dedicated linear classifier maps the resulting vector to expert logits (8 classes for Mixtral). C Results on LlaMA-MoE and Switch Transformer To generalize our claim, we evaluated the performance implications and ran our simulations on other model architectures beyond Mixtral. We show results obtained from the Llama-MoE model [37] in Figure 8 and the Switch Transformer model [7] in Figure C. We used the same datasets and the same hardware configurations as for the Mixtral experiments (MMLU, Alpaca Eval, and SST2; 4 A100 GPUs connected by NVLink or PCIe). 15 1.0 1.4 1.8 2.2 Skewness 0 10 20 30 40 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (a) Baseline latency with no predic- tion (interconnect NVLink). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink). 1.0 1.4 1.8 2.2 Skewness 0 10 20 30 40 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (d) Latency of different prediction strategies and accuracies (intercon- nect PCIe). Figure 8: Simulated prefill latency for a single layer of Llama-MoE model [37] under different prediction strategies and interconnect types. Workload sizes and hardware configurations are the same as Figure 6. For illustration purposes, overhead 0.5 of original latency is omitted. Overall, the trends and the insights are similar to those we derived from the Mixtral model. We observed that the datasets generally have higher skewness in both models compared to Mixtral due to different routing decisions. We also noticed that it is more difficult to obtain very high prediction accuracy, and the prediction complexity required when approaching perfect prediction grows exponentially. For illustration purposes, we have omitted results where the overhead latency is greater than half of the original latency (layer-wise latency without overhead). 16 1.0 1.4 1.8 2.2 Skewness 0 5 10 15 20 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (a) Baseline latency with no predic- tion (interconnect NVLink). 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink). 1.0 1.4 1.8 2.2 Skewness 0 5 10 15 20 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe). 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (d) Latency of different prediction strategies and accuracies (intercon- nect PCIe). Figure 9: Simulated prefill latency for a single layer of Switch Transformer model [7] under different prediction strategies and interconnect types. Workload sizes and hardware configurations are the same as Figure 6. For illustration purposes, overhead 0.5 of original latency is omitted. 17\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.07366v1 [cs.LG] 9 Jun 2025 MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing Haiyue Ma Princeton University Zhixu Du Duke University Yiran Chen Duke University Abstract In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across different GPUs, which creates load imbalance as each expert processes different number of tokens. Recent works improve MoE inference load balance by dy- namically duplicating popular experts to more GPUs to process excessive tokens, which requires predicting the distribution before routing. In this paper, we discuss the tradeoff of prediction strategies, accuracies, overhead, and end-to-end system performance. We propose MoE-GPS, a framework that guides the selection of the optimal predictor design under various system configurations, by quantifying the performance impact to system-level model runtime. Specifically, we advocate for Distribution-Only Prediction, a prediction strategy that only predicts overall token distribution which significantly reduces overhead compared to the traditional Token-to-Expert Prediction. On Mixtral 8 7B MMLU dataset, MoE-GPS suggests Distribution-Only Prediction which improves end-to-end inference performance by more than 23 compared with Token-to-Expert Prediction. 1 Introduction Distribution-Only Prediction Token-to-Expert Prediction Bottlenecked by Communication? High Load Imbalance? MoE-GPS Model Architecture System Configuration Best Prediction Strategy Y N Y N Figure 1: MoE-GPS guidelines for selecting op- timal expert prediction strategies that minimizes end-to-end inference latency based on model and hardware characteristics. Mixture-of-Experts (MoE) [13, 15, 7, 28] mod- els reduce the computation of Large Language Models (LLMs) by activating only a subset of experts per token. In large multi-GPU datacen- ters, Expert Parallelism (EP) [17] is typically used for Feed Forward Network (FFN) layers, where each GPU only hosts one or few experts. However, due to skewed token-to-expert distri- bution [26, 3], this approach often results in load imbalance, underutilized resources, and in- creased end-to-end latency.\n\n--- Segment 2 ---\nIn large multi-GPU datacen- ters, Expert Parallelism (EP) [17] is typically used for Feed Forward Network (FFN) layers, where each GPU only hosts one or few experts. However, due to skewed token-to-expert distri- bution [26, 3], this approach often results in load imbalance, underutilized resources, and in- creased end-to-end latency. The growing num- ber of experts in modern LLM models exagger- ates the imbalance [18]. While both training and inference can have im- balance, existing works proactively balance to- ken distribution in training, such as using aux- iliary loss [33] which can achieve near-perfect balance. For inference, expert-to-token mappings are fixed and imbalance is unavoidable. A popular approach to mitigate load imbalance is expert Preprint. Under review. FFN Layers All-to-All Shuffle Tensor Parallelism Expert Parallelism E1 E2 E3 E4 Tokens E1 Distribution GPU Expert Routing Attention Layers Ring Reduce- Scatter Figure 2: Overview of a typical Mixture-of-Expert inference based on a Transformer network, with four-way Tensor Parallelism and Expert Parallelism. Four Experts (denoted E1-E4) each resides in one GPU. Routing assigns each token to its expert, which creates load imbalance for both compute and communication, if the token-to-expert distribution is skewed. duplication [3, 34, 9, 20], where heavily used experts are replicated across GPUs to distribute tokens more evenly. Since the token distribution changes over time, dynamic duplication is often needed, which requires a predictor for the distribution ahead of the routing stage. Higher prediction accuracy improves load balancing but also incurs greater overhead, creating a complex tradeoff between pre- dictor complexity, accuracy, and system performance. Moreover, the effectiveness of each prediction strategy depends on multiple factors, such as workload characteristics, hardware communication schemes, and the degree of token imbalance. Despite its importance, there is currently no systematic method to model the runtime implications of MoE load imbalance, and to help choose the best predictor for different workload and system setups. We present MoE-GPS, a framework that simulates end-to-end MoE inference performance with imbalance, and guides the selection of expert prediction strategies that yield the shortest runtime.\n\n--- Segment 3 ---\nDespite its importance, there is currently no systematic method to model the runtime implications of MoE load imbalance, and to help choose the best predictor for different workload and system setups. We present MoE-GPS, a framework that simulates end-to-end MoE inference performance with imbalance, and guides the selection of expert prediction strategies that yield the shortest runtime. Built on top of an architectural simulator, LLMCompass [36], MoE-GPS models the runtime tradeoffs among prediction strategies, accuracy, and overhead. Given an arbitrary model architecture and hardware setup, MoE-GPS identifies the strategy that delivers the best system performance. With insights from MoE-GPS, we advocate for Distribution-Only Prediction strategy, which only predicts the coarse-grained token distribution across experts instead of exact token-to-expert mappings (Token-to-Expert Prediction). This lightweight approach is particularly effective when communication is not a bottleneck, because it reduces prediction complexity and still improves compute load balancing. Exact token-level prediction optimizes both computation and communication at the cost of higher overhead, which becomes more favorable when communication cost dominates. In addition, we observe that Distribution-Only Prediction performs better with more balanced workloads. These findings are summarized as design guidelines in Figure 1 and further elaborated in Section 4 and 5. Our contribution can be summarized as follows: We propose MoE-GPS, a system performance simulation framework that selects the optimal expert prediction strategy to minimize end-to-end inference latency, given a model and hardware setup. We identify and validate the effectiveness of Distribution-Only Prediction as a lightweight alternative to Token-to-Expert Prediction, offering better scalability and efficiency under varying system bottlenecks. For example, Distribution-Only Prediction introduces more than 23 system-level performance improvements on Mixtral 8 7B MMLU dataset compared to Token-to-Expert Prediction. Our work provides a practical tool and actionable insights for designing MoE predictors that improve inference speed across various workloads and hardware configurations. 2 Background Load Imbalance in Inference Prefill Is Critical. Imbalance can occur in both training and inference in a MoE network. Current approaches mitigates training imbalance by proactively routing tokens to less popular experts [33]. In inference, token-to-expert mapping is fixed.\n\n--- Segment 4 ---\nCurrent approaches mitigates training imbalance by proactively routing tokens to less popular experts [33]. In inference, token-to-expert mapping is fixed. Discarding or re-routing excessive tokens on popular experts may improve balance at the cost of model accuracy. 2 In this paper, we focus on inference prefill, and do not make changes to the original token-to- expert mapping. The decode stage has much less tokens being processed at the same time and is latency-critical, and load balancing is a secondary issue. Figure 2 shows the overall flow of a Transformer Block in a Mixture-of-Expert inference workload. For illustration purposes, we assume a four-GPU system, four experts (denoted E1-E4) each residing on one GPU, and each token is routed to its top-1 experts [7]. The general concepts apply to arbitrary number of devices, experts, and tokens with top-K experts [14, 37]. Expert Parallelism Leads to Imbalance. The parallelism scheme we use is Tensor Parallelism (TP) for the Attention layers and Expert Parallelism (EP) for the FFN layers [30]. Using TP for the Attention layers excludes the need to duplicate KV cache for adjacent tokens when the sequence is split to different GPUs. We used Ring All Reduce [23] to optimize the communication runtime after TP. Using EP for the FFN layers has two benefits over TP: first, it reduces communication latency by the factor of number of devices; second, it avoids splitting the weights in each expert to narrower matrices, which can lower Tensor Core utilization and increase compute time. The introduction of EP assigns experts to certain GPUs and creates load imbalance. Industry practices suggest more advanced hybrid parallelism (TP EP) [21] when necessary. For simplicity, this work assumes TP-only Attention and EP-only FFN, and the insights are generalizable. Quantifying Imbalance. We assume that Expert 1 is the most popular expert which takes 75 of all tokens as shown in Figure 2. FFN computation and communication runtime will be dominated by GPU 1 which has Expert 1.\n\n--- Segment 5 ---\nWe assume that Expert 1 is the most popular expert which takes 75 of all tokens as shown in Figure 2. FFN computation and communication runtime will be dominated by GPU 1 which has Expert 1. To quantify the load imbalance of each GPU, we introduced a parameter called "skewness", the number of tokens in the most popular expert divided by the average number of tokens per expert if the workload is evenly distributed: skewness of tokens in the most popular expert of average tokens per experts Figure 2 s workload skewness is 3 since Expert 1 has 75 of the tokens but the average token per expert is 25 . Performance Impacts of Load Imbalance. Skewness only impacts the runtime of FFN compute and communication since tokens are routed to the experts for FFN. FFN computation typically fully utilizes GPU resources, and the bottleneck FFN runtime is increased by a factor of the skewness. Routing incurs inter-device communication. With a fully connected multi-GPU topology, for a perfectly balanced distribution (skewness 1) each GPU needs to move (N 1) N of its existing tokens to other GPUs (N number of GPUs), assuming the token placement is completely random after scatter. This yields (N 1) N 2 tokens to move per GPU. However, for a workload with skewed distribution, the GPU with the most tokens will be receiving more tokens than other GPUs, and thus has longer communication latency. The overall communication time, which is bottlenecked by the GPU with the most popular expert, is scaled by skewness: (N 1) skewness N 2. The same amount of communication happens for All-to-All shuffle after the FFN layers. Current Solutions. In a multi-GPU datacenter setting, many existing solutions have proposed expert duplication - mirroring popular experts in other GPUs - to offload excessive token processing. MoE-Prediction [3] observes highly skewed token-to-expert distribution and proposes predicting such distribution to guide expert placement. Prophet [34], FlexMoE [20], SE-MoE [29], and FasterMoE [9, 8] propose specific strategies for dynamic expert duplication. Other techniques focus on reducing the distribution imbalance in training by imposing constraints, such as auxiliary load balancing [33] and expert biasing [18].\n\n--- Segment 6 ---\nProphet [34], FlexMoE [20], SE-MoE [29], and FasterMoE [9, 8] propose specific strategies for dynamic expert duplication. Other techniques focus on reducing the distribution imbalance in training by imposing constraints, such as auxiliary load balancing [33] and expert biasing [18]. In a different setting with fewer GPUs and the GPU memory capacity is limited for storing experts, token-to-expert mappings are also predicted to facilitate expert offloading to CPU [5]. Others proposes different solutions to serve as the token-to-expert mapping, such as expert buffer that stores the hottest experts [12] and expert activation correlation model [35]. These solutions are orthogonal to the discussion of this paper. 3 E3 E1 E2 E1 E1 E1 E4 Token-to-Expert Mapping Prediction (skipping Scatter) Distribution-Only Prediction Prediction Strategy Ring Reduce- Scatter Attention Layers Predictor Inference Expert Duplication FFN Layers Figure 3: Integration of expert duplication and different prediction strategies into the MoE model architecture for a single layer. Token-to-Expert Prediction eliminates communication by directly routing tokens to their predicted GPUs, while Distribution-Only Prediction assumes random token distribution during scattering. 3 Methodology This section discusses our approach to modeling and analyzing expert prediction strategies in MoE inference. We describe the integration of expert duplication into the MoE model architecture, and the two different prediction strategies (Distribution-Only and Token-to-Expert). We also show normalized system performance obtained from a performance simulator, LLMCompass [36], to illustrate high-level trends of the runtime implications of expert duplication. We defer detailed analyses of LLMCompass s simluated performance to Section 4. 3.1 Approach to Expert Duplication We integrate dynamic expert duplication into the MoE model by inserting a pre-trained predictor before Attention in each layer. For a chosen frequency, the predictor does inference with current batched inputs to predict distribution and guide the expert placement. Since dynamic duplication incurs extra inter-GPU communication for moving experts, existing works propose different intervals for prediction and moving, from every single batch [8, 34] to every 10 minutes [18], to balance overhead and effectiveness. In this study, we assume single-batch prediction and placement frequency.\n\n--- Segment 7 ---\nSince dynamic duplication incurs extra inter-GPU communication for moving experts, existing works propose different intervals for prediction and moving, from every single batch [8, 34] to every 10 minutes [18], to balance overhead and effectiveness. In this study, we assume single-batch prediction and placement frequency. We show calculation in Section 5 that the placement latency can be hidden within the Attention layers which is compute- bound with moderate batch sizes. Our simulator can be configured to model different frequencies of prediction and placement, by averaging out the overhead to multiple batches. Figure 3 illustrates how expert duplication helps balance token workloads. In our example, Expert 1 has the most tokens, so it is replicated across multiple GPUs to evenly distribute the load. In general, given arbitrary token-to-expert mappings, experts can be duplicated to achieve per-GPU balance by iteratively shifting experts from overloaded to underloaded devices: keep duplicating the experts on GPUs with 1 N tokens to GPUs with 1 N tokens until all GPUs process the same amount of tokens. A detailed algorithm can be found in Algorithm 1. 3.2 Prediction Strategies and Tradeoffs We explore two prediction strategies with distinct tradeoffs: Distribution-Only Prediction, which estimates static, aggregate expert usage; and Token-to-Expert Prediction, which targets exact token- level routing. The former has lower overhead and complexity and targets at compute imbalance, while the latter can reduce both compute and communication costs at the cost of high overhead. 3.2.1 Distribution-Only Prediction Distribution-only prediction estimates the proportion of tokens routed to each expert (e.g., Expert 1 gets 75 of tokens) without specifying which tokens. This enables balanced compute across GPUs but does not reduce communication costs, as tokens are still randomly scattered post-ring all-reduce. 4 Algorithm 1: Expert Duplication in MoE Load Balancing Input: Token expert map f : {1, . . . , T} {1, . . . , E}; Per-GPU memory capacities M NG; Initial placement P {1, . . . , E} {1, . . . , G}; Maximum copies per expert Cmax. Output: Balanced placement P and dispatch d : {1, . . . , T} {1, . . . , G}.\n\n--- Segment 8 ---\n. , G}. 1 Assign each token t to any GPU g that hosts its expert: d(t) min{g (f(t), g) P}; 2 Lg {t d(t) g} for all g {1, . . . , G}; 3 while maxg Lg ming Lg 1 do 4 gh arg maxg Lg; gc arg ming Lg; 5 l Lgh Lgc 2 m ; 6 e arg maxe E(gh) {t d(t) gh f(t) e} ; 7 if (e , gc) P and copies(e ) Cmax and params(e ) Mgc then 8 copy weights of e on gc; P P {(e , gc)}; 9 Reassign the first tokens {t d(t) gh f(t) e } to gc by setting d(t) gc; 10 Update Lgh and Lgc; 11 return P, d; We model the expert activation distribution in each layer of the MoE model using a multinomial distribution and estimate its parameters via Maximum Likelihood Estimation (MLE). The multinomial distribution is commonly used to model counts of outcomes across discrete classes, in our case the selection of experts within a layer. MLE selects the distribution that the observed data has the maximum likelihood to fit into. Table 1: Impact of skewness on expert distribution estimation and system performance. Higher skew- ness leads to higher error rate, indicating reduced estimation accuracy and degraded performance. Dataset Skewness Error rate ( ) MMLU 1.39 1.80 Alpaca Eval 1.40 0.98 SST2 1.99 16.00 Formally, let pl i denote the probability of se- lecting expert i in layer l. Assuming that each token s expert selection is an independent and identically distributed (i.i.d.) sample from this multinomial distribution, the MLE of pl i is given by: ˆpl i nl i N , (1) where N is the total number of tokens observed at layer l, and nl i is the number of tokens that activated expert i. When the training data come as batches, the estimation becomes a moving average. We note that expert selection is primarily governed by local token-level features and routing mechanisms.\n\n--- Segment 9 ---\nWhen the training data come as batches, the estimation becomes a moving average. We note that expert selection is primarily governed by local token-level features and routing mechanisms. Hence, modeling activations as independent draws to mimick per-batch distribution provides a reasonable approximation while significantly simplifying analysis. We provide detailed derivation in Appendix A. We experiment the Distribution-Only Prediction on three datasets: MMLU [10], Alpaca Eval [6], and SST2 [31], on the Mixtral 8 7B [14] MoE model. For each batch, we set sequence length to 512 and measure average skewness across batches: 1.388, 1.402, and 1.990 respectively. We measure the averaged error rate over layers between our estimation of the probability on the trainset and empirical probability on the testset. We define error rate as ˆp p 1 of experts. Higher error rate indicates less accurate estimation. For datasets that do not have a dedicated test split, we use train test split to randomly partition the trainset with 80 training samples and 20 test samples. We evaluate the normalized end-to-end system performance across all datasets given the model size (Mixtral 8 7B), network parameters (batch size 1, sequence length 512), and hardware configurations (four A100 GPUs, fully connected with NVLINK). Performance is simulated with an augmented version of LLMCompass [36]. The error rate is used to scale the runtime of the GPU that processes the most tokens, which will be detailed in Section 3.3. 5 0.0 0.2 0.4 0.6 0.8 1.0 Prediction Accuracy 0.0 0.1 0.2 0.3 Overhead ( Runtime) 0.0 0.5 1.0 Normalized Performance (a) Mixtral 8 7B (skew 1.4). 0.0 0.2 0.4 0.6 0.8 1.0 Prediction Accuracy 0.0 0.1 0.2 0.3 Overhead ( Runtime) 0.0 0.5 1.0 Normalized Performance (b) Mixtral 8 7B (skew 2.0). Figure 4: Trade-off between prediction accuracy and end-to-end system performance for Token-to- Expert Prediction. Increasing prediction accuracy improves expert placement but also raises runtime overhead.\n\n--- Segment 10 ---\nFigure 4: Trade-off between prediction accuracy and end-to-end system performance for Token-to- Expert Prediction. Increasing prediction accuracy improves expert placement but also raises runtime overhead. Although higher accuracy results in better initial expert placement, excessive overhead can reduce overall gains. In higher-skewness settings (right), accurate predictions are easier to achieve, shifting the optimal performance point toward higher accuracy. Table 1 shows that lower error rate in expert distribution (more accurate estimation) leads to improved system performance. Notably, higher skewness results in larger error rates and degraded normal- ized performance. This is primarily because greater skewness leads to underutilized experts that receive fewer tokens. The reduced token count increases estimation error for these experts, which disproportionately contributes to the overall error rate. 3.2.2 Token-to-Expert Prediction Token-to-Expert Prediction targets exact routing of each token to its expert s GPU. With predicted mapping, we can send each token to the GPU that has its experts directly. This strategy skips the scatter phase in communication after the ring all-reduce, and saves both FFN compute and communication runtime, as shown in Figure 3. We formulate the expert selection task as a classification problem, where the goal is to predict the activated expert for each token in the batch that will be processed by the MoE model. We explore three types of models: a simple probability-based model, a conditional probability model, and neural network-based predictors. Probability Model. We always assign the expert with the highest global frequency observed in the training data, treating all tokens identically regardless of their identity or position. Conditional Probability Model. This model improves by conditioning on either the token index or its position index. For each token (or position), we select the expert that appears the most frequently in the training data for that specific token index or position index. Neural Networks. We train neural models using pairs of token embeddings and their corresponding expert activations. The models are optimized using cross-entropy loss and the Adam optimizer [16] and trained to converge. All samples are padded to sequence length 512. We experiment with both simple Feed-Forward Networks (FFNs) and Long Short-Term Memory networks (LSTMs) [11]. The detailed architectures of these networks are provided in Appendix B.\n\n--- Segment 11 ---\nWe experiment with both simple Feed-Forward Networks (FFNs) and Long Short-Term Memory networks (LSTMs) [11]. The detailed architectures of these networks are provided in Appendix B. We investigate Token-to-Expert Prediction on the same three datasets, i.e., MMLU, Alpaca Eval, and SST2. The results are shown in Figure 4a with the MMLU and Alpaca Eval data set (shown together because they have similar skewness) and Figure 4b with the SST2 dataset, for Mixtral 8X7B. The x-axis refers to the prediction accuracy for the predictor, each point representing a different predictor being used. The overhead y-axis shows the percentage of the prediction overhead of the total runtime for the MoE model, measured on A100 GPUs. The normalized performance y-axis is the simulated end-to-end system performance by LLMCompass, using four A100 GPUs fully connected with NVLink, and batch size 1 and sequence length 512. The runtime includes the overhead of each prediction strategy. We use exponential functions to fit the accuracy to overhead curves and polynomial functions to fit the accuracy to performance curves. Higher prediction accuracy can result in better initial expert placement leading to better system performance. However, higher accuracy also incurs larger overhead, which degrades the end-to-end 6 performance. The figure demonstrates a trade-off that it is not always the case that predictors with higher accuracy are better. Figure 4a and Figure 4b also demonstrates results of different skewness. Note that, for scenarios with higher skewness, it costs less for the predictor to acquire higher accuracy. Consequently, the sweet point of the system performance moves towards higher accuracy end. 3.3 Modeling the Effect of Prediction Error Optimistic: T Typical: T (1 ε) Errors averaged across all devices ε Error Rate T Avg. Time per Device N Num. of Devices ε ε εε Pessimistic: T (1 ε (N-1)) All errors on one device Figure 5: Modeling the impact of prediction errors on end-to-end system runtime.\n\n--- Segment 12 ---\nTime per Device N Num. of Devices ε ε εε Pessimistic: T (1 ε (N-1)) All errors on one device Figure 5: Modeling the impact of prediction errors on end-to-end system runtime. Three scenarios for the same prediction error rate ϵ: (1) Optimistic errors do not affect load balancing; (2) Typical errors are evenly dis- tributed across devices, leading to moderate slowdown; (3) Pessimistic errors concen- trate on one device, causing worst-case load imbalance. We model the system-level effects of imperfect pre- diction by analyzing how different error distributions affect FFN computation load balance and communi- cation cost. First, we model FFN compute s runtime with predic- tion error rate ϵ, which is 1 accuracy for Token- to-Expert Prediction, and averaged L1 distance over number of experts for Distribution-Only Prediction. Figure 5 shows three potential outcomes for the same prediction accuracy for FFN compute s runtime. As- sume that ϵ 0.1: Optimistic: Errors still result in perfect load balancing; e.g. predicting 85 of tokens instead of 75 for Expert 1 in Figure 2. Typical: Errors are uniformly distributed across GPUs, leading to moderate imbal- ance. The most loaded GPU processes up to (1 ϵ) avg_tokens. This is the default model used in our runtime simulations. Pessimistic: All errors occur on a single GPU, leading to worst-case imbalance where the bottleneck GPU handles up to N (1 ϵ) avg_tokens. While unlikely to happen, this scenario represents an upper bound on performance degradation. We apply the same typical-case assumption when modeling communication overhead under Expert Parallelism (EP) for Token-to-Expert Prediction. Unlike compute, however, communication costs always increase with prediction errors, as misrouted tokens inevitably trigger additional inter-GPU data transfers. Optimistic cases do not exist in this context. 3.4 Network Performance Simulation We conduct performance evaluations using an extended version of LLMCompass [36], a block- level simulator for large language model inference, validated with silicon measurements. This simulation-based approach allows us to easily explore design tradeoffs across hardware and software configurations without requiring access to real clusters, which are often costly and impractical for exhaustive testing.\n\n--- Segment 13 ---\n3.4 Network Performance Simulation We conduct performance evaluations using an extended version of LLMCompass [36], a block- level simulator for large language model inference, validated with silicon measurements. This simulation-based approach allows us to easily explore design tradeoffs across hardware and software configurations without requiring access to real clusters, which are often costly and impractical for exhaustive testing. Our simulations focus on a single Transformer layer, modeling all relevant operations including GEMM, communication, and element-wise computations. Since LLMCompass does not yet support FlashAttention [4], our Attention layer runtimes are conservatively overestimated. We augmented LLMCompass with: MoE and Expert Parallelism (EP): We introduced custom modules to model EP-specific communication and FFN workloads. Mixtral Support: We added support for Mixtral-style model architectures, integrating existing Grouped Query Attention (GQA) [1] and SwiGLU [27] activation implementations. We also implemented Sliding Window attention [2]. Prediction Strategy Modeling: We added support for evaluating both Distribution-Only and Token-to-Expert prediction strategies with tunable accuracy and overhead. 7 1.0 1.4 1.8 2.2 Skewness 0.0 0.2 0.4 0.6 0.8 1.0 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (a) Baseline latency with no predic- tion (interconnect NVLink). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To-Expert Prediction(Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink). 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 1.0 1.5 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe).\n\n--- Segment 14 ---\n0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To-Expert Prediction(Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink). 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 1.0 1.5 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (d) Latency of different prediction strategies and accuracies (intercon- nect PCIe). Figure 6: Simulated prefill latency for a single layer of Mixtral 8 7B under different prediction strate- gies and interconnect types. Latency is broken down by component (attention, FFN, communication, overhead) and evaluated across skewness levels on 4 A100 GPUs using NVLink (top) and PCIe (bottom). (a, c) show baseline latencies without prediction; (b, d) show improvements from prediction strategies at varying accuracies. Distribution-Only Prediction reduces FFN compute without overhead, while Token-to-Expert Prediction introduces overhead that trades off with improved load balancing. For each skewness, the best preditor has the minimum total latency across strategies and accuracies. 4 Results In this section, we discuss how to choose the best predictor for overall system performance under different system configurations, workload sizes and token distribution skewnesses.\n\n--- Segment 15 ---\nFor each skewness, the best preditor has the minimum total latency across strategies and accuracies. 4 Results In this section, we discuss how to choose the best predictor for overall system performance under different system configurations, workload sizes and token distribution skewnesses. Our goal is to identify when to use Distribution-Only Prediction, which reduces FFN computation load imbalance without communication savings, and when to use Token-to-Expert Prediction, which additionally reduces communication latency at the cost of higher predictor complexity. Additionally, for Token-to- Expert Prediction, we want to find the optimal prediction accuracy to minimize overall runtime. Figure 6 shows the time-to-first-token latency for a single layer of Mixtral 8 7B prefill workload, simulated by LLMCompass. To match the experimental setup described in Section 3.2, we set the batch size to 1 and the sequence length to 512. The simulation includes all architectural components of Mixtral, such as a 4K sliding window, Grouped Query Attention, and SwiGLU activation. For system configurations, we modeled four A100s, either connected via NVLink 3.0 [22] (high-end interconnect with 2 TB s bandwidth) or PCIe 4.0 [24] (low-end interconnect with 32GB s bandwidth). Figures 6a and 6c show baseline latencies without any prediction strategy with NVLink and PCIe interconnect respectively. While FFN and Attention latencies remain constant across configurations, PCIe s limited bandwidth makes communication latency a dominant bottleneck. 8 Figure 6b and 6d show the impact of applying prediction strategies under varying skewness and accuracy. For Token-to-Expert Prediction, we use the fitted curve in Figure 4 to model the overhead for each prediction accuracy. For skewness numbers without a matching dataset, we interpolated the overhead from the skewnesses that we measured in Section 3.2. Within each skewness group, the leftmost bar shows latency without prediction, same as those in Figure 6a and 6c. The second bar shows latency under Distribution-Only Prediction. This strategy reduces FFN compute time, though communication time remains unchanged. Since distribution is estimated offline, there is no prediction overhead. As skewness increases, distribution estimation becomes less accurate, slightly diminishing its benefits. The remaining stacked bars (shown as curves) show latencies for Token-to-Expert Prediction across multiple accuracy levels.\n\n--- Segment 16 ---\nAs skewness increases, distribution estimation becomes less accurate, slightly diminishing its benefits. The remaining stacked bars (shown as curves) show latencies for Token-to-Expert Prediction across multiple accuracy levels. Each curve exhibits a U-shape: higher accuracy improves load balancing (reducing FFN and communication time), but also increases overhead. The optimal configuration is the one with the lowest total latency, typically at an intermediate accuracy level. We see that Distribution-Only Prediction performs better in most cases. For example, with skewness 1.4 (a typical case, close to MMLU dataset), Distribution-Only Prediction achieves 23 speedup compared to the best configuration of Token-to-Expert Prediction (the bottom of the U-shape). Compared to Distribution-Only, Token-to-Expert Prediction incurs significantly higher overhead, especially at low skewness where it requires more complex models. However, it becomes beneficial when 1) skewness is high: accurate predictions are easier to make, reducing overhead, and 2) communication cost is high (e.g., PCIe): saved communication time outweighs added prediction cost. Across all configurations, FFN latency reductions from prediction are largely skewness-independent. This is because mispredictions are measured relative to a perfectly balanced reference scenario. Only overhead varies with skew as higher skewness makes prediction easier. Figure 7: Simulated effectiveness of two predic- tion strategies best savings for Mixtral 8X7B on 4 fully-connected A100 with different system inter- connect settings. To clearly draw our insights on which predic- tion strategy is better under different scenar- ios, we plotted the runtime savings on base- line (no prediction) of the best configuration of Token-to-Expert Prediction (the point of lowest latency for each skewness), and com- pared that with the savings of Distribution- Only Prediction. Figure 7 shows the differ- ence in savings for the two strategies, calculated by Distribution-Only Prediction saving Token-to-Expert Prediction saving. Every bar above zero indicates Distribution-Only Pre- diction outperforms the best configuration of Token-to-Expert Prediction, and every bar be- low zero indicates otherwise. The 600GB s and 64GB s interconnect bandwidth correspond to settings in Figure 6b and 6d respectively.\n\n--- Segment 17 ---\nEvery bar above zero indicates Distribution-Only Pre- diction outperforms the best configuration of Token-to-Expert Prediction, and every bar be- low zero indicates otherwise. The 600GB s and 64GB s interconnect bandwidth correspond to settings in Figure 6b and 6d respectively. The other two are arbitrary interconnect settings to model cases where a mixture of both interconnects are used (such as across NVLink domains). Key takeaways: Distribution-Only Prediction is preferred when skewness is low or communication is not a bottleneck. Its low complexity and zero overhead make it a strong strategy. Token-to-Expert Prediction becomes more effective as skewness increases, since predictors achieve high accuracy at lower cost. It is particularly beneficial under low-bandwidth interconnects like PCIe, where communication savings dominate. 5 Discussion and Limitation Generality across model architectures. While our results focus on Mixtral 8 7B, the core insights generalize across other MoE-based architectures such as Switch Transformer [7], LLaMA variants [37, 32], and DeepSeek [18]. Although these models differ in their expert construction and token routing 9 decisions, we observe consistent system-level behaviors for both Distribution-Only and Token-to- Expert predictions. Specifically, for token-to-expert mapping, more complex predictors lead to higher accuracy but also higher overhead, which brings negative impact to FFN layers performance. Supporting experiments for LLaMA and Switch Transformer are provided in Appendix C. At the system level, Attention and FFN dominate the cost of MoE inference across models. LLaMA shares Mixtral s SwiGLU-based FFN but lacks sliding-window Attention; Switch Transformer uses ReLU in FFN and does not use Grouped Query Attention (GQA)[1]; DeepSeek introduces Multi-head Latent Attention (MLA)[19] to further optimize Attention. Despite these differences, FFN layers remain a substantial portion of runtime, and expert duplication yields consistent benefits across all. Additionally, scaling model size (e.g., Mixtral 8 7B vs. 8 22B) changes absolute latency but not the qualitative trends or effectiveness of each prediction strategy. Generality across hardware systems. In our study, we assumed fully connected GPUs with the same interconnect bandwidth between each pair.\n\n--- Segment 18 ---\nGenerality across hardware systems. In our study, we assumed fully connected GPUs with the same interconnect bandwidth between each pair. For a larger-scale system with more GPU nodes, different topologies including Mesh, Torus, and Tree topologies can be used. These topology choices will impact specific runtime but are orthogonal to our core insights, and can be modeled by changing the topology implementation. Further, while we assume full TP for Attention layers and full EP for FFN layers in this study, hybrid parallelism (such as using TP EP for FFN) have been proven to be useful in certain settings. Support can easily be added by incorporating current frameworks for hybrid parallelism [25]. Long sequence lengths. The experiments shown in the paper use a sequence length of 512. Longer sequences introduce new tradeoffs, particularly for Token-to-Expert Prediction. For FFN-based predictors, although computation remains parallelizable and overhead manageable, we observe a lower bound on achievable accuracy as sequence length increases. LSTM-based predictors are theoretically sequence-length agnostic in accuracy but suffer from poor parallelism and can hardly scale across different devices. Therefore, under long-sequence workloads, Distribution-Only Prediction may become more favorable due to its scalability and low complexity. Kernel underutilization at small scale. To enable fast evaluation, we use small batch sizes and short sequences, which can expose low-level inefficiencies in kernel execution, such as insufficient overlap between prologue epilogue and the main MMA loop, which leads to underutilization of Tensor Core FLOPs. Our simulator is throughput-oriented and assumes at least one hardware unit (in this case, compute) is saturated. To validate our findings, we compared simulated results against actual GPU measurements for both model and predictor inference. Although the absolute runtimes differ, the relative overhead between prediction and inference is consistent. Thus, we report and analyze prediction overhead as a ratio to the simulated inference runtime for an accurate estimation. Expert duplication s communication overhead. We also quantify the communication overhead of expert duplication to show that we can hide its latency with the Attention layers. For Mixtral 8 7B, each FP16 expert contains approximately 4,096 14,336 2 2 bytes of weight data.\n\n--- Segment 19 ---\nWe also quantify the communication overhead of expert duplication to show that we can hide its latency with the Attention layers. For Mixtral 8 7B, each FP16 expert contains approximately 4,096 14,336 2 2 bytes of weight data. Assuming one expert is sent and received per GPU per layer, the transfer takes 0.1 ms over NVLink 3.0 (2 TB s bandwidth), smaller than the Attention layer runtime for batch size 1 and sequence length 512. In practice, expert weights are static across inputs, and this duplication can be hidden with Attention computation. Even with PCIe 4.0 (32 GB s), the duplication latency can be hidden with modest increases in batch size or sequence length (e.g., batch size 16, sequence length 2K), which are still on the low-end of practical inference workload sizes. In more advanced systems, expert prefetching or pipelined movement between layers can further reduce this cost. 6 Conclusion In this work, we presented MoE-GPS, a framework for selecting expert prediction strategies that minimize end-to-end inference latency in Mixture-of-Experts models. By modeling the trade-offs between prediction accuracy, overhead, and system runtime across different hardware and workload configurations, MoE-GPS provides actionable guidance for system designers. We also demonstrated the effectiveness of Distribution-Only Prediction as a lightweight alternative to token-level prediction, particularly in scenarios where load imbalance is low and communication is not a bottleneck. 10 References [1] Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [3] Peizhuang Cong, Aomufei Yuan, Shimao Chen, Yuxuan Tian, Bowen Ye, and Tong Yang. Prediction is all moe needs: Expert load distribution goes from fluctuating to stabilizing. arXiv preprint arXiv:2404.16914, 2024.\n\n--- Segment 20 ---\nPrediction is all moe needs: Expert load distribution goes from fluctuating to stabilizing. arXiv preprint arXiv:2404.16914, 2024. [4] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:16344 16359, 2022. [5] Zhixu Du, Shiyu Li, Yuhao Wu, Xiangyu Jiang, Jingwei Sun, Qilin Zheng, Yongkai Wu, Ang Li, Hai Li, and Yiran Chen. Sida: Sparsity-inspired data-aware serving for efficient and scalable large mixture-of-experts models. Proceedings of Machine Learning and Systems, 6:224 238, 2024. [6] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. [7] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):1 39, 2022. [8] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, and Jie Tang. Fastmoe: A fast mixture-of-expert training system. arXiv preprint arXiv:2103.13262, 2021. [9] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 120 134, 2022. [10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding.\n\n--- Segment 21 ---\n[10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [11] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735 1780, 1997. [12] Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, and Benjamin Lee. Towards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference. arXiv preprint arXiv:2303.06182, 2023. [13] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79 87, 1991. [14] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [15] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181 214, 1994. [16] Diederik P Kingma. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [17] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi- tional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n\n--- Segment 22 ---\nGshard: Scaling giant models with condi- tional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. 11 [18] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [19] Fanxu Meng, Zengwei Yao, and Muhan Zhang. Transmla: Multi-head latent attention is all you need. arXiv preprint arXiv:2502.07864, 2025. [20] Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Gang Cao, and Bin Cui. Flexmoe: Scaling large-scale sparse pre-trained model training via dynamic device placement. Proceedings of the ACM on Management of Data, 1(1):1 19, 2023. [21] NVIDIA. Mixture of experts package. developer-guide latest api-guide moe.html, 2025. [22] NVIDIA. Nvlink and nvlink switch. nvlink , 2025. [23] Pitch Patarasuk and Xin Yuan. Bandwidth optimal all-reduce algorithms for clusters of worksta- tions. Journal of Parallel and Distributed Computing, 69(2):117 124, 2009. [24] PCI-SIG. Pci express base specification revision 4.0, version 1.0. Technical report, PCI Special Interest Group, 2017. Available at [25] Le Qin, Junwei Cui, Weilin Cai, and Jiayi Huang. Chimera: Communication fusion for hybrid parallelism in large language models. In Proceedings of the 52nd Annual International Symposium on Computer Architecture, ISCA 25. ACM, 2025. [26] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He.\n\n--- Segment 23 ---\nACM, 2025. [26] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of- experts inference and training to power next-generation ai scale. In International conference on machine learning, pages 18332 18346. PMLR, 2022. [27] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of- experts layer. In International Conference on Learning Representations, 2017. [29] Liang Shen, Zhihua Wu, WeiBao Gong, Hongxiang Hao, Yangfan Bai, HuaChao Wu, Xinxuan Wu, Jiang Bian, Haoyi Xiong, Dianhai Yu, et al. Se-moe: A scalable and efficient mixture-of- experts distributed training and inference system. arXiv preprint arXiv:2205.10034, 2022. [30] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [31] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631 1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.\n\n--- Segment 24 ---\nIn David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard, editors, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631 1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. [32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [33] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024. [34] Wei Wang, Zhiquan Lai, Shengwei Li, Weijie Liu, Keshi Ge, Yujie Liu, Ao Shen, and Dongsheng Li. Prophet: Fine-grained load balancing for parallel training of large-scale moe models. In 2023 IEEE International Conference on Cluster Computing (CLUSTER), pages 82 94. IEEE, 2023. 12 [35] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. Edgemoe: Fast on-device inference of moe-based large language models. arXiv preprint arXiv:2308.14352, 2023. [36] Hengrui Zhang, August Ning, Rohan Baskar Prabhakar, and David Wentzlaff. Llmcompass: Enabling efficient hardware design for large language model inference. In 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA), pages 1080 1096. IEEE, 2024. [37] Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. Llama-moe: Building mixture-of-experts from llama with continual pre-training.\n\n--- Segment 25 ---\n[37] Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. Llama-moe: Building mixture-of-experts from llama with continual pre-training. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15913 15923, 2024. 13 A MLE for Multinomial Distribution We seek to model the expert activation pattern in a Mixture-of-Experts (MoE) model using a proba- bilistic approach. Specifically, we are interested in estimating the distribution over expert selections for each MoE layer, based on observed activation frequencies in the training data. To this end, we adopt a multinomial modeling framework and employ Maximum Likelihood Estimation (MLE) to infer the activation probabilities. Assumptions. We assume that each token independently selects an expert from a fixed pool of K experts in a given layer. Let E {e1, . . . , eK} denote the set of experts in a particular MoE layer. For each token routed through the layer, the expert selection is modeled as an i.i.d. draw from a multinomial distribution with parameters p (p1, . . . , pK), where pi is the probability that expert ei is selected. Naturally, PK i 1 pi 1 and pi 0 for all i. Maximum Likelihood Estimation. Given N tokens routed through the layer, let ni denote the number of tokens that selected expert ei, so that PK i 1 ni N. The likelihood of the observed expert assignments under the multinomial distribution is: L(p) Pr(n1, . . . , nK p) N! n1! nK! K Y i 1 pni i . (2) To estimate p via MLE, we maximize the log-likelihood: log L(p) log N! n1! nK! K X i 1 ni log pi. (3) Ignoring the constant term that does not depend on p, the optimization problem reduces to: max p K X i 1 ni log pi (4) s.t. K X i 1 pi 1, pi 0. (5) This is a standard constrained optimization problem, and the solution is obtained via the method of Lagrange multipliers.\n\n--- Segment 26 ---\nK X i 1 pi 1, pi 0. (5) This is a standard constrained optimization problem, and the solution is obtained via the method of Lagrange multipliers. The resulting MLE estimator for each expert s activation probability is: ˆpi ni N , i {1, . . . , K}. (6) B Predictor Architectures e formulate the expert selection problem in Mixture-of-Experts (MoE) as a multi-class classification task, where the objective is to predict the activated expert for each token in a sequence. Let T denote the set of input tokens, and let E {e1, . . . , eK} denote the set of experts available in a given MoE layer. For each token t T , the goal is to predict an expert label yt {1, . . . , K} that will be used by the MoE routing mechanism. We explore three modeling paradigms for this task: a global frequency-based model, a conditional frequency model, and neural network-based predictors. Probability-Based Model. This baseline treats all tokens identically and assigns each token to the expert that is most frequently activated in the training data. Let ni be the number of times expert ei was selected across all tokens in the training corpus. The model estimates the global activation probabilities using maximum likelihood as: 14 ˆpi ni PK j 1 nj , i {1, . . . , K}. (7) The predicted expert for any token is then: ˆyt arg max i ˆpi. (8) This approach ignores token-specific context, providing a static prediction rule that reflects global expert utilization frequencies. Conditional Probability Model. To improve over the static assignment, we consider a token- or position-conditioned frequency model. Let It be the token index or its absolute position in the sequence. For each token index i, we count how many times each expert ek was selected and compute: ˆpk i nk,i PK j 1 nj,i , (9) where nk,i denotes the number of times token index i selected expert ek. The model then predicts: ˆyt arg max k ˆpk It. (10) This conditional model captures per-token or per-position biases in expert activation. Neural Networks.\n\n--- Segment 27 ---\n(10) This conditional model captures per-token or per-position biases in expert activation. Neural Networks. To learn token-aware expert selection strategies, we train neural models that take token embeddings as input and predict the corresponding expert activation for each MoE layer. Each model is trained with cross-entropy loss and optimized using the Adam optimizer. Input sequences are padded to a fixed length of 512 tokens during training, and separate classifiers are maintained for each layer of the MoE model. We experiment with the following two architectures: Feed-Forward Network (FFN). The FFN model is a lightweight two-layer MLP. Each input token embedding (of dimension 4096 for Mixtral) is first passed through a linear projection to a 128-dimensional hidden space, followed by a ReLU activation. This is then followed by another linear layer of the same hidden size. Finally, for each target MoE layer, a separate classifier head is implemented as a linear layer mapping from the 64-dimensional hidden state to 8 expert logits. The FFN model is shared across tokens and layers, with layer-specific output heads. LSTM with Sparse Attention. To capture temporal dependencies, we also design a re- current model based on an LSTM encoder augmented with sparse attention. The input token embeddings are first projected from dimension 4096 (Mixtral) to 128 using a linear compression layer, followed by a ReLU activation. These projected embeddings are passed through a 2-layer LSTM with hidden size 64, applied in a batch-first manner. To enhance contextual modeling, we incorporate a sparse attention mechanism over the LSTM outputs (i.e., attention is applied using the LSTM outputs as query, key, and value). A residual connection is then added between the attention output and a separate feedforward transfor- mation of the compressed input. Finally, for each MoE layer, a dedicated linear classifier maps the resulting vector to expert logits (8 classes for Mixtral). C Results on LlaMA-MoE and Switch Transformer To generalize our claim, we evaluated the performance implications and ran our simulations on other model architectures beyond Mixtral.\n\n--- Segment 28 ---\nFinally, for each MoE layer, a dedicated linear classifier maps the resulting vector to expert logits (8 classes for Mixtral). C Results on LlaMA-MoE and Switch Transformer To generalize our claim, we evaluated the performance implications and ran our simulations on other model architectures beyond Mixtral. We show results obtained from the Llama-MoE model [37] in Figure 8 and the Switch Transformer model [7] in Figure C. We used the same datasets and the same hardware configurations as for the Mixtral experiments (MMLU, Alpaca Eval, and SST2; 4 A100 GPUs connected by NVLink or PCIe). 15 1.0 1.4 1.8 2.2 Skewness 0 10 20 30 40 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (a) Baseline latency with no predic- tion (interconnect NVLink). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink). 1.0 1.4 1.8 2.2 Skewness 0 10 20 30 40 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (d) Latency of different prediction strategies and accuracies (intercon- nect PCIe).\n\n--- Segment 29 ---\n1.0 1.4 1.8 2.2 Skewness 0 10 20 30 40 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe). 0.5 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (d) Latency of different prediction strategies and accuracies (intercon- nect PCIe). Figure 8: Simulated prefill latency for a single layer of Llama-MoE model [37] under different prediction strategies and interconnect types. Workload sizes and hardware configurations are the same as Figure 6. For illustration purposes, overhead 0.5 of original latency is omitted. Overall, the trends and the insights are similar to those we derived from the Mixtral model. We observed that the datasets generally have higher skewness in both models compared to Mixtral due to different routing decisions. We also noticed that it is more difficult to obtain very high prediction accuracy, and the prediction complexity required when approaching perfect prediction grows exponentially. For illustration purposes, we have omitted results where the overhead latency is greater than half of the original latency (layer-wise latency without overhead). 16 1.0 1.4 1.8 2.2 Skewness 0 5 10 15 20 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (a) Baseline latency with no predic- tion (interconnect NVLink). 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink).\n\n--- Segment 30 ---\n16 1.0 1.4 1.8 2.2 Skewness 0 5 10 15 20 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (a) Baseline latency with no predic- tion (interconnect NVLink). 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (b) Latency of different prediction strategies and accuracies (intercon- nect NVLink). 1.0 1.4 1.8 2.2 Skewness 0 5 10 15 20 Latency (ms) Attn Layers Attn Comm FFN FFN Comm (c) Baseline latency with no predic- tion (interconnect PCIe). 0.6 0.7 0.8 0.9 1.0 Normalized Latency No Prediction Dist-Only Prediction Token-To- Expert Prediction (Accuracy) 0.5 0.95 0.5 0.95 0.5 0.95 0.5 0.95 1.0 1.4 1.8 2.2 Skewness 0.0 0.5 Attn Layers Attn Comm FFN FFN Comm Overhead (d) Latency of different prediction strategies and accuracies (intercon- nect PCIe). Figure 9: Simulated prefill latency for a single layer of Switch Transformer model [7] under different prediction strategies and interconnect types. Workload sizes and hardware configurations are the same as Figure 6. For illustration purposes, overhead 0.5 of original latency is omitted. 17\n\n