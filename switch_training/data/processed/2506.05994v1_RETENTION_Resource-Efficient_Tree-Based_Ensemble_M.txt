=== ORIGINAL PDF: 2506.05994v1_RETENTION_Resource-Efficient_Tree-Based_Ensemble_M.pdf ===\n\nRaw text length: 77092 characters\nCleaned text length: 76416 characters\nNumber of segments: 40\n\n=== CLEANED TEXT ===\n\narXiv:2506.05994v1 [cs.LG] 6 Jun 2025 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS 1 RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory Yi-Chun Liao, Chieh-Lin Tsai, Yuan-Hao Chang, Fellow, IEEE, Cam elia Slimani, Jalil Boukhobza, Senior Member, IEEE, and Tei-Wei Kuo, Fellow, IEEE Abstract Although deep learning has demonstrated remark- able capabilities in learning from unstructured data, modern tree- based ensemble models remain superior in extracting relevant information and learning from structured datasets. While several efforts have been made to accelerate tree-based models, the inherent characteristics of the models pose significant challenges for conventional accelerators. Recent research leveraging content- addressable memory (CAM) offers a promising solution for accelerating tree-based models, yet existing designs suffer from excessive memory consumption and low utilization. This work addresses these challenges by introducing RETENTION, an end-to-end framework that significantly reduces CAM capacity requirement for tree-based model inference. We propose an iterative pruning algorithm with a novel pruning criterion tailored for bagging-based models (e.g., Random Forest), which minimizes model complexity while ensuring controlled accuracy degradation. Additionally, we present a tree mapping scheme that incorporates two innovative data placement strategies to alleviate the memory redundancy caused by the widespread use of don t care states in CAM. Experimental results show that implementing the tree mapping scheme alone achieves 1.46 to 21.30 better space efficiency, while the full RETENTION framework yields 4.35 to 207.12 improvement with less than 3 accuracy loss. These results demonstrate that RETENTION is highly effective in reducing CAM capacity requirement, providing a resource- efficient direction for tree-based model acceleration. Index Terms Tree-based machine learning, bagging-based model pruning, content-addressable memory, data placement optimization, in-memory computing. I. INTRODUCTION Structured (i.e., tabular) data is one of the most prevalent formats in data science. It is typically represented as a matrix, This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Yi-Chun Liao is with the Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan. E-mail: Chieh-Lin Tsai is with the Department of Computer Science and Informa- tion Engineering, National Taiwan University, Taipei 10617, Taiwan. E-mail: Yuan-Hao Chang is with the Institute of Information Science, Academia Sinica, Taipei 11529, Taiwan. E-mail: Cam elia Slimani is with IRIT, Universit e de Toulouse, Toulouse INP UT3, CNRS, 31062 Toulouse, France. E-mail: Jalil Boukhobza is with Lab-STICC, CNRS UMR 6285 , EN- STA, Institut Polytechnique de Paris, 29806 Brest, France. E-mail: Tei-Wei Kuo is with the Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan. He is also with Delta Electronics, Taoyuan 33378, Taiwan. E-mail: where each row corresponds to an instance and all instances share the same set of features across columns. This format is widely used in fields such as finance, medicine, and scientific research, as its structured nature enables efficient processing. For example, sensors generate data in tabular format to support real-time analysis for AI-driven closed-loop control. Despite significant advancements in deep learning for unstructured data (e.g., text, images, and speech), studies [1], [2] have shown that modern tree-based ensemble models consistently outperform deep learning in several tasks. In fact, tree-based models re- main the state-of-the-art for classification and regression tasks involving medium-sized structured datasets [2], and are often favored over deep learning due to their high interpretability [3], particularly in sensitive applications where understanding model decisions is critical [4]. Tree-based models are widely applied in domains such as scientific research [5], credit card fraud detection [6], and machinery fault diagnosis [7]. A recent survey [8] found that over 74 of data scientists prefer tree-based models, whereas fewer than 40 opt for neural networks, underscoring their continued importance. However, despite their widespread adoption and effective- ness, tree-based models have received less attention in recent years, and the inefficiency of tree-based model inference remains a critical yet unresolved challenge. Since inference requires multiple tree traversals, and the number of possible paths grows exponentially with tree depth, predicting and prefetching data becomes challenging. This may lead to rela- tively high inference latency, which is particularly unfriendly for real-time applications. The issue is further exacerbated as modern tree-based models (e.g., XGBoost [9]) can con- tain thousands of trees, and are often deployed in resource- constrained environments [10]. Although various accelerators have been proposed in recent years [11], [12], researchers [13] have found that conventional accelerators, such as multi-core CPUs, GP-GPUs, and FPGAs, offer limited effectiveness for tree-based model acceleration due to the non-deterministic memory access patterns and irregular tree structures. One promising approach to improving tree-based model inference is leveraging in-memory com- puting (IMC), which integrates data storage and processing within the same location, thereby eliminating latency and energy costs associated with data access and transfer. While conventional SRAM-based IMC accelerators can effectively accelerate model inference, they come at the cost of high energy consumption and extensive area overhead [14], making 2 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS it impractical for resource-constrained environments. Recently, there has been significant interest in developing emerging non-volatile-memory-based (NVM-based) IMC ac- celerators [15], [16], [17], as the characteristics of emerg- ing NVM (e.g., high density, low power consumption, and low cost) make it well-suited for resource-constrained envi- ronments. Among potential candidates, non-volatile ternary content-addressable memory (nvTCAM) [18], [19] exhibits the best fit for accelerating tree-based models due to its capability to perform sequence matching with extremely high parallelism and its high reliability. Since every root-to-leaf path within tree-based models can be encoded into a binary sequence, nvTCAM can traverse all the paths in one shot, enabling unprecedented acceleration for model inference. Nevertheless, to support in-memory search, data must be organized in a spe- cialized format (refer to Section II-B for further explanation). Although nvTCAM effectively accelerates tree-based model inference in a cost- and energy-efficient manner, a considerable portion of memory cells is allocated for format alignment rather than storing actual model data. This leads to excessive memory consumption with substantial redundancy, which is highly inefficient and requires further optimization. To achieve resource-efficient acceleration, the enormous CAM capacity requirement with considerable redundancy is the major obstacle that must be addressed before practical implementation. Since model complexity is highly correlated with memory consumption, pruning models can significantly mitigate this issue. Modern tree-based models often employ pruning algorithms, such as limiting maximum depth or min- imum impurity decrease [20], in order to reduce overfitting and simplify model structure. While these techniques are well- suited for boosting-based ensemble models (e.g., XGBoost), where each tree in the ensemble is trained to correct the errors of the previous ones, applying them to bagging-based tree ensemble models (e.g., Random Forest [21]) can lead to severe accuracy degradation due to the independent training of each tree. On the other hand, memory redundancy arises from structuring data for in-memory search, suggesting that a tailored data placement strategy could further reduce CAM capacity requirement. While several studies [22], [23], [24], [25], [26] have explored accelerating tree-based models using non-volatile CAM, little attention has been paid to the issue of memory redundancy, and existing data placement strategies fail to address this challenge effectively and efficiently. Miti- gating redundancy requires a solution that not only reduces model complexity but also optimizes memory utilization, paving the way for a more resource-efficient acceleration. Based on the above observations, we propose RETENTION, an end-to-end framework that minimizes memory consumption for accelerating tree-based models with CAM. RETENTION offers a pruning algorithm with a novel pruning criterion. The algorithm is applied to bagging-based models iteratively during out-of-bag (OOB) estimation [27], which effectively reduces model complexity while ensuring controlled accuracy degradation. In addition, after analyzing the tradeoffs between different data placement strategies, RETENTION incorporates a tree mapping scheme with two innovative data placement strategies. The strategies are tailored for different optimization criteria, aiming to alleviate memory redundancy and further reduce CAM capacity requirement. To the best of our knowledge, this is the first work that systematically optimizes data placement strategies for tree- based model acceleration with CAM, explicitly considering the tradeoffs between memory redundancy and processing overhead. RETENTION is evaluated on Random Forest and XGBoost using five datasets. Although validated with these two models, the framework can be generalized to other en- semble models with similar structures such as LightGBM [28] and CatBoost [29]. Experimental results show that our tree mapping scheme alone achieves 1.46 to 21.30 better space efficiency, while the complete framework with pruning and tree mapping optimization improves space efficiency by 4.35 to 207.12 with less than 3 accuracy degradation. These results demonstrate that RETENTION effectively re- duces CAM capacity requirement, making tree-based model acceleration with nvTCAM more resource-efficient and feasi- ble for resource-constrained environments. The rest of this paper is organized as follows: Section II presents the background, observation, and motivation of this work. Section III introduces the philosophy and detailed design of RETENTION. Section IV evaluates RETENTION s effectiveness and compares it with existing works. Finally, Section V provides concluding remarks. II. BACKGROUND, OBSERVATION, AND MOTIVATION A. Decision Tree, Bagging, and Boosting Decision tree [30] is a commonly used supervised machine learning algorithm for classification and regression tasks, val- ued for its simplicity in training and high interpretability. Typ- ically structured as a binary tree, each internal node represents a decision condition based on a feature and a corresponding threshold, while each leaf node stores a predicted result. The training process involves a series of node-splitting operations, where all instances initially start at the root node. During each split, instances within the current node are evaluated, and the optimal feature-threshold pair that best separates them (i.e., generates maximum impurity decrease) is selected as the node s condition. The instances are then distributed to the appropriate child nodes based on whether they satisfy the condition. This process continues iteratively until all instances are separated or predefined constraints, such as maximum depth or minimum impurity decrease, are reached. The left part of Fig. 1 presents an example of decision tree inference. As shown in Fig. 1, inference follows a similar tree traversal process, where an instance starts at the root node and follows a path determined by the conditions at each encountered node. The prediction result is the value stored in the reached leaf node. Since each root-to-leaf path is determined by a set of conditions that an instance must satisfy during traversal, it can be viewed as a sequence of condition checks leading to the final prediction. Additionally, as there is no contradiction between the conditions within a path, the order of condition checks is irrelevant to the prediction result, providing an opportunity to accelerate inference with CAM. However, the simplicity in training comes with a major drawback: overfitting, where the model captures noise in the RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 3 F F X X F T X X T X F F T X F T T X T T Min 0.5 Min 0.3 Min Max Min 0.5 0.3 Max Min Max 0.5 0.8 Min Max Min 0.7 0.8 Max Min Max Min 0.7 0.5 Max Min Max 0.7 Max f1 0.5 f2 0.3 Class1 Class2 f3 0.7 Class3 f1 0.8 Class2 Class3 T T T T F F F F Path1 Path2 Path3 Path4 Path5 Input: {f1 0.4, f2 0.5, f3 0.8} Path1 Path2 Path3 Path4 Path5 f1 0.5 f2 0.3 f3 0.7 f1 0.8 F T T F In-Memory Search with Ternary CAM In-Memory Search with Analog CAM 0.4 0.5 0.8 f1 f2 f3 White: Unvisited Nodes Unmatched Conditions. Blue: Visited Nodes Matched Conditions. Diagonal Hatch: Don t Care. Fig. 1. Visualization of decision tree inference acceleration with CAM. training data, reducing its ability to generalize to unseen instances. To mitigate overfitting and enhance model perfor- mance, decision-tree-based ensemble models were introduced. Bagging (i.e., bootstrap aggregation) [31] is a widely used algorithm for tree-based ensemble model training. It trains multiple fully-grown decision trees independently, each on a unique subset of the dataset generated through bootstrapping (i.e., sampling with replacement). When making a prediction, each tree produces an individual result, and the ensemble de- termines the final output via majority voting (if classification) or averaging (if regression). By training trees on different subsets of data, bagging enhances robustness against overfit- ting, reducing the need for pruning. When additional model compression is required, early-stopping (i.e., pre-pruning) is preferred to pruning constructed trees (i.e., post-pruning), as conventional post-pruning algorithms ignore the collective behavior of the model and can erode the diversity that bagging- based model relies on. Additionally, bagging enables OOB estimation, an inherent validation method that eliminates the need for a separate validation set. Since each tree is trained on a subset of data, the unused instances can serve as the validation set, which is later passed through the correspond- ing tree for evaluation. By aggregating predictions from all trees that did not train on a given instance, OOB estimation provides an unbiased measure of model performance, making it particularly useful when data is limited. Bagging alleviates overfitting, while the ensemble compensates for the missing information in individual trees, resulting in improved accuracy compared to a single decision tree. Additionally, since each tree in a bagging-based model is trained independently and contributes equally to the final prediction, models such as Random Forest perform exceptionally well on noisy and small datasets. However, they struggle to capture complex patterns, as they do not fully exploit interactions among trees within the ensemble. Moreover, the averaging mechanism limits the model performance in regression tasks, highlighting the need for an alternative. Another class of tree ensemble model primitives is the boosting-based model, which is commonly utilized for both classification and regression tasks. In contrast to bagging, which trains multiple fully-grown decision trees indepen- dently, boosting [32] trains shallow decision trees sequentially, with each tree attempting to correct the errors of its predeces- sors. During training, boosting fits the first tree on the entire dataset, and each subsequent tree is trained to correct the errors of its predecessors, with errors being reevaluated at each step as new trees are added to the ensemble. When making a prediction, the final result is computed as a weighted sum of individual tree predictions rather than relying on majority voting or averaging, as trees in boosting-based models con- tribute with different importance weights. While the error cor- rection mechanism and the sequential training process adapt the models to task complexity, they are more susceptible to overfitting and therefore rely heavily on model pruning. How- ever, since the training phase leverages interactions among trees, post-pruning can disrupt these inter-tree dependencies, leading to catastrophic accuracy degradation. Consequently, boosting-based models often opt for pre-pruning algorithms such as limiting maximum depth and minimum impurity decrease. However, modern boosting implementations, such as XGBoost, can easily contain thousands of trees, resulting in substantial memory consumption. Additionally, the irregular and non-deterministic memory access patterns in tree traver- sal make efficient execution on conventional hardware chal- lenging. To mitigate these inefficiencies, content-addressable memory (CAM) enables massively parallel in-memory search, significantly improving inference speed and efficiency. B. Ternary CAM and Analog CAM Content-addressable memory (CAM) [14], [33], [34] is a specialized memory architecture widely used in various applications. Unlike conventional memory, which retrieves data based on a given address, CAM allows simultaneous comparison of an input query sequence against all stored binary sequences, and returns the addresses or associated data of matching entries. Data in CAM is stored row-wise, with 4 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS TABLE I DATASET INFORMATION AND THE EXPERIMENT RESULTS FOR SIGNIFICANT REDUNDANCY. Dataset Samples Features Classes Attribute Paths Avg. Length Unique Cond. Size (MB) Redundancy ( ) Adult [36] 48842 14 2 medium 206152 16.91 29436 723.40 99.94 CreditApproval [37] 690 15 2 small 6246 7.39 1934 1.44 99.61 DryBean [38] 13611 16 7 multi-class 52663 12.11 41752 262.12 99.97 Letter [39] 20000 16 26 multi-class 190377 15.59 421 9.55 96.30 Wine [40] 4898 11 11 multi-class 107772 13.53 5467 70.24 99.75 comparisons performed in parallel on a column basis. This high degree of parallelism makes CAM particularly well-suited for applications requiring rapid lookups, such as network routing, database indexing, and cache systems. Ternary content-addressable memory (TCAM) [35] extends the functionality of binary CAM by introducing a third state, don t care, denoted as X. Bits set to the X state are treated as matches during comparison, enabling greater flexibility in search operations. Since each root-to-leaf path in a tree- based model represents a sequence of conditions an input must satisfy to reach the corresponding prediction, TCAM can efficiently traverse these paths, making it a compelling solution for tree-based model acceleration. Prior work [22] proposed mapping each path to a TCAM row, with each column representing a unique condition. Unencountered conditions within a path are assigned the X state as they do not affect the traversal process. When an input instance is received, it is first encoded into a binary sequence that aligns with the condition order stored in TCAM. This encoded sequence is then fed into TCAM for in-memory search. The middle part of Fig. 1 illustrates how TCAM serves as a decision tree accelerator. The decision tree at the left part of Fig. 1 is mapped to the TCAM, where four unique conditions are assigned to separate columns, and each path is represented as a row. Conditions are encoded as 0, 1, and X, corresponding to False, True, and don t care, respectively. Once an input is received, the features are encoded into a binary sequence representing the condition check results, and each bit is then delivered to the corresponding column to perform in-memory search. Since all bits in the second row are matched, Path2 is the traversal path for the input instance. Analog content-addressable memory (ACAM) [41] follows a similar structural design to TCAM, but differs in its data representation. Instead of discrete states (0, 1, X), each ACAM cell can store a range of analog values. A cell is considered matched if the given input value falls within the stored range, meaning that a min-to-max range in ACAM functions analogously to the X state in TCAM. Previous research [23] has demonstrated the potential of ACAM for accelerating Random Forest. Similar to [22], each root-to-leaf path is mapped to an ACAM row. However, unlike TCAM, where each column represents a feature-threshold condition, ACAM columns directly correspond to features. When an input instance is received, a digital-to-analog converter first transforms the features into analog voltages. ACAM then performs in-memory search operations to retrieve the final result. The right part of Fig. 1 provides an overview of ACAM serving as the accelerator for the decision tree in the left part of Fig. 1. In contrast to TCAM, ACAM only requires three columns to represent the features, and each of the cells in ACAM stores an analog range instead. However, despite its space efficiency, ACAM requires addi- tional peripheral circuits, and its larger cell size does not nec- essarily offer better area efficiency than TCAM. Furthermore, analog computing is susceptible to various non-idealities, such as stuck-at faults, IR drop, thermal noise, shot noise, and random telegraph noise [42]. While mitigation techniques exist [43], they often introduce additional memory overhead and offer limited effectiveness. Due to these reliability chal- lenges, ACAM is not yet viable for real-world implementation. Therefore, this work opts for TCAM as the accelerator. Still, RETENTION remains compatible with ACAM after adapting the condition-based operations to operate on features only. C. Observation To accelerate tree-based model inference using TCAM, each root-to-leaf path is mapped to a TCAM row, with each column representing a unique condition within the ensemble. Since a single path encounters only a minimal fraction of the condi- tions within the ensemble, the majority of TCAM cells store the X state for format alignment, leading to significant redun- dancy. In fact, mapping a tree-based model to TCAM without optimizations requires at least paths unique conditions bits, far exceeding the memory capacity available in resource- constrained environments. As shown in Fig. 1, mapping a decision tree to a CAM with perfect capacity leads to over one-third of cells storing the X state. Moreover, due to the extremely high write latency of nvTCAM, real-time writing severely degrades performance, making it critical to fit the entire model into nvTCAM. Table I presents an experiment that highlights the substantial memory requirement and the overwhelming redundancy caused by the widespread use of the X states in TCAM. In this experiment, Random Forest models with 100 decision trees require up to 700MB of memory when mapped with naive unified mapping (refer to Section III-D1 for further explanation), and all exhibited redundancy of more than 96 . These results highlight that directly deploying nvTCAM as a tree-based model accelerator is both infeasible and inefficient in resource-constrained environments. Previous work DT2CAM [22] introduced a framework for mapping decision trees to TCAM. However, instead of addressing memory redundancy, it only proposed an energy- saving precharge mechanism that disables unmatched rows to conserve power. Pedretti et al. [23] proposed mapping Random RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 5 Model Construction Tree Mapping Scheme with Tailored Data Placement Strategy Energy-Efficient Mapping (ODR) Space-Efficient Mapping (SPC) Tolerance Dataset OR Model Model Training and Purity Threshold Pruning Bagging-Based Models Boosting-Based Models Input Fig. 2. Overview of RETENTION. Input Dataset Ensemble Training with Purity Saved Purity Threshold Pruning Final Model Tolerance Minimum Threshold Search Assume current purity threshold is 75 51 68 Class1 Class2 70 Class3 78 Class2 Class3 Fig. 3. Workflow of model construction with purity threshold pruning. Forests to ACAM, incorporating techniques such as limiting tree depth and reordering features while eliminating empty rows to mitigate memory redundancy. However, pre-pruning algorithms can result in severe accuracy degradation when implementing on bagging-based models (refer to Section III-B for further explanation), while alternative pruning strategies could achieve better accuracy within the same memory budget. Furthermore, although feature reordering groups redundant cells and empty-row elimination reduces capacity requirement, the associated computational overhead for result retrieval is relatively high. In fact, comparable computational overhead could yield even greater memory savings. A follow-up study by Pedretti et al. [24] proposed assigning individual trees to separate ACAMs, improving space efficiency. While this approach effectively reduces redundancy across different trees, redundancy within each separate tree remains, leaving room for further optimization. Addressing this residual redundancy could lead to more reductions in memory consumption while maintaining computational efficiency. D. Motivation This work is driven by the challenges identified in the observations and previous work;. While nvTCAM holds great promise for accelerating tree-based model inference in resource-constrained environments, the excessive CAM capacity requirement remains a critical challenge that must be addressed before practical implementation. Therefore, this paper proposes RETENTION, an end-to-end framework that significantly reduces CAM capacity requirement, enabling resource-efficient tree-based ensemble model acceleration. III. RETENTION A. Overview nvTCAM enables high-speed, parallel inference for tree- based models, making it a promising solution for acceleration. However, substantial memory consumption and redundancy re- main key barriers to real-world deployment, limiting its feasi- bility in resource-constrained environments. In this section, we introduce RETENTION, an end-to-end framework serving as a crucial component to enable practical and resource-efficient acceleration of tree-based model inference with nvTCAM. It addresses the challenges by (1) minimizing model complexity through purity threshold pruning and (2) enhancing memory utilization by introducing an optimized tree mapping scheme with two data placement strategies, namely occurrence-based double reordering (ODR) and similarity-based path clustering (SPC). Fig. 2 presents an overview of RETENTION, detailing its key components and their interactions. The following sections elaborate on the philosophy and implementation. B. Purity Threshold Pruning As the required memory capacity is correlated to both paths and unique conditions, pruning models can effec- tively reduce CAM capacity requirement. Existing pruning techniques for reducing tree-based ensemble model complexity mainly focus on pre-pruning (since post-pruning algorithms are rarely implemented on ensemble models, as explained in Section II-A), and the algorithms fall into two main categories: (1) structural restriction, which constrains the tree s shape using parameters such as maximum depth, and (2) split-based pruning, which prevents further splits when subsequent splits are deemed worthless (e.g., minimum impurity decrease). While both methods reduce model complexity, neither takes into account the class distribution of instances within nodes, potentially leading to significant accuracy degradation. For example, in complex datasets, instances from different classes may remain entangled even at the maximum depth, resulting in serious accuracy loss when forced truncation occurs. Sim- ilarly, if an early split does not effectively separate instances, later splits might still provide meaningful differentiation, yet split-based pruning prematurely terminates such opportunities. Although these pruning techniques have minimal impact on boosting-based models, where trees iteratively correct previ- ous errors, bagging-based models can suffer severe accuracy degradation due to their independently trained trees, which lack an error correction mechanism. To avoid potential catastrophic accuracy degradation, this work introduces purity threshold pruning, a novel pruning algorithm designed for bagging-based models. The workflow is depicted in the left part of Fig. 3. Unlike conventional training, which only takes a dataset as input, our approach incorporates a user-specified tolerance for OOB accuracy loss. During training, each node records its majority class and the corresponding purity (i.e., proportion). Once the ensemble is constructed, rather than concluding with OOB estimation, we 6 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS TABLE II COMPARISON OF NAIVE UNIFIED MAPPING AND NAIVE INDEPENDENT MAPPING. Attribute Naive Unified Mapping Naive Independent Mapping Mapping Unit entire ensemble single tree Width per Unit l unique conditions S m TCAMs l unique conditions S m TCAMs Height per Unit l paths S m TCAMs l paths S m TCAMs Total TCAM Required l unique conditions S m l paths S m P l unique conditions S m l paths S m Key Strength lower input pre-processing cost higher space efficiency Category energy-efficient mapping space-efficient mapping iteratively determine the minimum purity threshold that main- tains accuracy within the specified tolerance. Nodes exceeding this threshold are converted into leaf nodes and assigned their respective majority class, as illustrated in the right part of Fig. 3. The core principle of purity threshold pruning is to reduce model complexity while preserving accuracy at the ensemble level. Since individual trees are trained on different subsets with distinct splits, a minority class overlooked in one tree can still be classified correctly in others. For highly imbalanced datasets, applying class weighting helps prevent the model from favoring majority classes. Additionally, by explicitly considering OOB accuracy during pruning, the method ensures that performance degradation remains controlled. Unlike stan- dard pruning techniques that may create leaf nodes with evenly distributed classes, potentially leading to misclassifications, purity threshold pruning ensures that each leaf node main- tains sufficient class purity to guarantee the desired level of accuracy. Moreover, when early splits already satisfy the target accuracy, further splits are omitted, significantly reducing both paths and unique conditions compared to conventional pruning methods. Different from most post-pruning algorithms that operate on individual decision trees in isolation, purity threshold pruning is specifically designed to account for the collective behavior of ensemble models during inference. This global perspective enables purity threshold pruning to pre- serve diversity and effectively reduce model complexity while explicitly constraining accuracy degradation within a user- defined tolerance, a capability not addressed by conventional single-tree pruning methods. Furthermore, recent pruning techniques focus on removing entire trees from models if they exhibit substantial redundancy or structural resemblance [44], [45]. Since purity threshold pruning operates at the node level and assumes trees are diversely grown, it is fully compatible with such whole-tree pruning methods. This compatibility allows further optimiza- tion in space efficiency without sacrificing accuracy, making purity threshold pruning a highly adaptable approach for resource-efficient tree-based model. However, the algorithm is not compatible with boosting-based models, as performing pruning after model construction destroys the error correc- tion mechanism, leading to catastrophic accuracy degrada- tion. Nonetheless, the pre-pruning algorithms can already sufficiently reduce boosting-based model complexity without compromising accuracy. C. Input pre-processing during Inference To accelerate tree-based model inference with CAM, raw input data must be transformed into the specific query format for individual search operations. This transformation involves two key steps: (1) feature encoding and (2) query packing. For feature encoding, the thresholds of each feature must be sorted before inference. When raw input data for a feature is received, binary search can be employed to accelerate condition checks. This step can also be performed on the sensor side, as modern smart sensors are capable of handling such lightweight tasks [46]. Additionally, since each feature is typically associated with a dedicated sensor, pre-processing at the sensor level is feasible and efficient. Once features are encoded into binary sequences, the query packing step organizes condition check results into the query format, ensuring alignment with the order of TCAM columns. These queries are then transmitted to the corresponding TCAM via a network-on-chip (NoC) [47], following the same implementation as in [24]. The input pre- processing for ACAM is almost the same, as quantization can be interpreted as a sequence of condition checks. However, since ACAM performs search operations using analog values, additional digital-to-analog conversions are required after re- ceiving the query. While tree traversal is essentially a sequence of condition checks, encoding input features into sequences and performing in-memory search may initially seem redundant. However, due to the shared conditions across the ensemble, the actual num- ber of unique conditions is significantly lower than expected. Moreover, binary search can further reduce the number of condition checks. For instance, in a Random Forest model with 100 trees and an average path length of 17.38, traversing the forest requires about 1738 condition checks. However, experimental results reveal that as there are 5446 unique conditions within the model, applying binary search can re- duce the number of condition checks to approximately 120 (14 features log2 5446 14 ), which is a significant improvement. Additionally, feature encoding is highly efficient compared to conventional CPU-based tree traversal, because all data accesses are deterministic and easily cached. Given that the computational overhead for feature encoding is constant when using TCAM as an accelerator, and can also be done on the sensor side, this work focuses on the computational overhead of query packing, which is primarily determined by the selected data placement strategy. RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 7 Naive Unified Mapping Condition Reordering Path Reordering Occurrence-Based Double Reordering (Energy-Efficient Mapping) Similarity-Based Path Clustering (Space-Efficient Mapping) Requires 3 TCAMs Requires 6 TCAMs Requires 9 TCAMs Bold: TCAMs. Gray White: Encountered Unencountered Conditions. Colorful: Similar Encountered Conditions. Fig. 4. Visualization of the proposed data placement strategies. D. Tree Mapping Scheme 1) Naive Tree Mapping Approaches: To utilize TCAM for acceleration, each root-to-leaf path within the model needs to be mapped to a TCAM row, with every column of the TCAM corresponding to a unique condition within the paths being searched. There are two naive approaches for mapping: (1) naive unified mapping and (2) naive independent mapping, with their differences summarized in Table II, assuming a TCAM size of S S. In naive unified mapping, paths from the entire ensemble are mapped together, so that each row entry corresponds to the same set of conditions. This structure allows for efficient input pre-processing since TCAMs in the same column receive identical input sequences. However, most nodes in the ensemble are not encountered by any single path, resulting in a vast number of cells storing the X state. In fact, mapping 100 trees together can lead to over 96 re- dundancy, as shown in Table I. Conversely, naive independent mapping maps each tree separately, ensuring that conditions unique to a tree do not introduce unnecessary columns. This approach significantly reduces storage redundancy, as only the conditions relevant to a specific tree are stored, making it far more space-efficient than naive unified mapping. However, this independence disrupts input pre-processing because each TCAM now requires a distinct input sequence, increasing pre- processing overhead for query packing. Although naive unified mapping generally demands more TCAMs than naive independent mapping, the energy-saving precharge mechanism proposed in DT2CAM [22] can help mitigate energy waste by deactivating unmatched rows early. Moreover, input pre-processing dominates energy consumption in inference acceleration, as CPU computations are signif- icantly more power-intensive than in-memory search oper- ations. Given that many nodes are shared among multiple trees, naive unified mapping reduces input pre-processing over- head, making it more energy-efficient than naive independent mapping. Therefore, based on the above observations, we categorize the optimization criteria of data placement strategies into two types: energy-efficient mapping and space-efficient mapping. The key difference between these two types is whether to minimize memory usage at the cost of increased computational overhead. 2) Existing Data Placement Strategy Analysis: DT2CAM primarily focused on mapping a single decision tree to TCAM without addressing redundancy or ensemble models. When directly applied to ensembles, DT2CAM acts as either naive unified mapping or naive independent mapping, depending on the chosen mapping unit. Pedretti et al. [23] introduced a data placement strategy, referred to as the feature reordering with row elimination (FR) algorithm, for ACAM. This method selects the whole ensemble as a mapping unit, and proposes (1) reordering features based on frequency to cluster redundant cells, and (2) eliminating entire path segments when they become redundant. However, due to row elimination, paths were no longer stored in fixed rows, making the result retrieval process substantially more complex and CPU-dependent. As a result, the additional computational overhead is proportional to ACAMs, similar to that of naive independent mapping. Thus, we classify FR as a space-efficient mapping approach, though its space efficiency may still fall short of naive independent mapping. A follow-up study by Pedretti et al. [24] proposed fitting each tree into a separate ACAM without mitigating redundancy, resembling naive independent mapping. Conse- quently, we also categorize this approach under space-efficient mapping. After a thorough analysis of the tradeoffs among existing data placement strategies, we find that prior approaches fail to effectively alleviate memory redundancy, offering efficiency comparable to naive methods. To bridge this gap, we introduce two novel data placement strategies: one optimized for energy- efficient mapping and the other for space-efficient mapping. The following sections provide a detailed explanation of these strategies, with a visualization presented in Fig. 4. 3) Occurrence-Based Double Reordering (ODR): To al- leviate memory redundancy in an energy-efficient manner, each path should be stored in a fixed row to avoid additional computational overhead during result retrieval. Moreover, the input sequences for TCAMs in the same column must remain consistent to minimize input pre-processing overhead for query packing. Given these constraints, we propose ODR to enhance memory utilization for resource-constrained environments that prioritize energy efficiency. As outlined in Algorithm 1, ODR first sorts conditions by occurrence frequency in descending 8 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS Algorithm 1 Occurrence-Based Double Reordering (ODR) Require: pool root to leaf paths Require: conditions pool.union() Ensure: condition order, path order 1: paths {} 2: sorted conditions conditions.sort() descending freq. 3: for c in sorted conditions.reverse() do 4: for path in pool do 5: if path.contains(c) then 6: paths.append(path) 7: pool.remove(path) 8: return sorted conditions, paths order, similar to FR. Instead of eliminating partial sequences when an entire TCAM row is irrelevant to a given path, which introduces sequential computational overhead as seen in FR, ODR optimizes path ordering based on condition usage. Paths that contain rare conditions are placed at the top, concentrating most of the X-state cells in the bottom-right TCAMs. TCAMs that are entirely filled with X-state cells can then be removed, reducing redundancy without distributing a single path across multiple rows, thereby avoiding additional computational overhead. Despite its effectiveness, ODR still leaves considerable redundancy, as shown in Fig. 4. For example, in the top- right TCAM after applying ODR, only a single meaningful cell remains, yet the entire TCAM is required for correct functionality. To further improve memory efficiency, the next section introduces a more aggressive data placement strategy that reduces CAM capacity requirement at the cost of higher energy consumption. However, its computational overhead is comparable to that of naive unified mapping, and is, in fact, even lower in practice. 4) Similarity-Based Path Clustering (SPC): Since memory redundancy arises from unencountered conditions, clustering paths with high similarity (i.e., those that share many con- ditions) can substantially improve space efficiency. Although naive independent mapping can be viewed as a basic form of clustering, paths within a tree exhibit limited similarity, and some TCAMs can still remain nearly redundant especially in cases where paths S 1 and the TCAM size is S S, leading to inefficient capacity utilization. Finding the optimal clusters to minimize TCAM consumption is an NP-hard prob- lem. In this section, we propose SPC, a heuristic algorithm that greedily maximizes similarity among paths within TCAM, and thus minimizes CAM capacity requirement. As presented in Algorithm 2, SPC starts with a pool containing all root- to-leaf paths in the model. While pool is not empty, SPC evaluates the similarity between current cluster and each of the paths within pool, and designates the path that can fit in current cluster based on (1) the highest number of shared con- ditions, and (2) the smallest resulting set of unique conditions after being integrated into current cluster, as candidate. Since the objective is to accommodate all paths using the smallest number of clusters, and the capacity of each cluster depends on both paths and unique conditions because we limit a Algorithm 2 Similarity-Based Path Clustering (SPC) Require: S TCAM size Require: pool root to leaf paths Ensure: clusters 1: clusters {}, current cluster {} 2: current num 0 3: while pool is not empty do 4: similarity calc similarity(pool, current cluster) 5: candidate find best candidate(similarity, pool) 6: if candidate is NULL or current num S then 7: clusters.append(current cluster) 8: current cluster {} 9: current num 0 10: else 11: current cluster.append(candidate) 12: current num current num 1 13: pool.remove(candidate) 14: if current cluster is not empty then 15: clusters.append(current cluster) 16: return clusters cluster to a single TCAM, the key intuition behind SPC is to minimize unique conditions at every step. This approach allows each TCAM to hold as many paths as possible, resulting in an effective heuristic. Once current cluster reaches capacity or none of the paths is suitable, current cluster is added to clusters representing the completed ones, and a new empty cluster is initiated. Otherwise, SPC greedily adds the candidate to current cluster and removes it from pool, and this process is repeated until all paths have been assigned to clusters. SPC efficiently exploits similarities across paths, diminish- ing unnecessary X states. By filling TCAMs as fully as pos- sible, it maximizes utilization and minimizes wasted capacity. Since each cluster is constrained to contain at most S unique conditions and S paths, there is no dependency between the match results of different TCAMs, eliminating any compu- tational overhead associated with intersecting partial match results. Therefore, the only computational overhead lies in the query packing for every TCAM, which is the same as naive independent mapping. However, since SPC requires far fewer TCAMs than naive independent mapping, it emerges as a significantly more resource-efficient approach in terms of both energy and memory consumption. IV. EVALUATION A. Experiment Setup 1) Objective and Metric: Since previous works already demonstrate that CAM outperform conventional accelerators, in this work we focus on the CAM-oriented optimization, omitting performance comparison with CPU, GPU, FPGA. To evaluate the effectiveness of RETENTION, we compare the number of required TCAMs against those in existing works. Therefore, we select the reduced number of TCAMs as our evaluation metric, and the higher value represents better performance. RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 9 50 55 60 65 70 75 80 85 90 95 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Tolerance 1 Tolerance 3 Tolerance 5 Space-Efficient Mapping (SPC) 50 55 60 65 70 75 80 85 90 95 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Tolerance 1 Tolerance 3 Tolerance 5 Energy-Efficient Mapping (ODR) Fig. 5. Overall performance of RETENTION. 2) Model Settings: Two ensemble models are selected for case study: Random Forest, representing bagging-based meth- ods, and XGBoost, representing boosting-based methods. Both models are trained on five datasets from the widely used UCI Machine Learning Repository [48], as listed in Table I. Each dataset is divided into training and testing sets in a 7:3 ratio to examine the impact of purity threshold pruning on accuracy. For Random Forest, we utilized the open-source library Ranger [49] for forest construction, setting the number of trees to 100. Purity threshold pruning is applied on Random Forest to reduce model complexity, with tolerance set to {1 , 2 , 3 , 4 , 5 }. On the other hand, we used the official XGBoost library [9] to construct XGBoost with default parameters, including max depth 6 and num trees 100. 3) Data Placement Strategies and Baselines: To achieve resource-efficient acceleration, we implemented ODR and SPC to further enhance efficiency. As analyzed in Section III-D2, the data placement strategies presented in prior works are mostly equivalent to naive unified mapping and naive inde- pendent mapping, which fall under the categories of energy- efficient mapping and space-efficient mapping, respectively. While FR incorporates additional optimizations, the associated computational overhead is comparable to naive independent mapping, and is therefore categorized into space-efficient map- ping. Since the algorithm is originally designed for ACAM, we make a minor modification, reordering conditions in- stead of features, to adapt it for TCAM deployment. In this work, energy-efficient mapping and space-efficient mapping are evaluated separately, with naive unified mapping and naive independent mapping as baselines. 4) Simulation: The mapping simulation is conducted by storing root-to-leaf paths into tabular format (i.e., each row is a root-to-leaf path, and every column corresponds to a unique condition), and directly calculate the required number of TCAMs, as no open-source simulator currently supports such functionality. All models are mapped to TCAMs with size 64 64 if not specified. The subsequent sections present detailed experimental re- sults: Section IV-B analyzes RETENTION s overall perfor- mance. Section IV-C investigates the effects of purity thresh- old pruning on accuracy and TCAM capacity requirement. Section IV-D evaluates the contributions of ODR and SPC individually. Section IV-E explores the impact of the number of trees. Section IV-F examines the influence of TCAM size. and Section IV-G discusses computational overhead, energy consumption, and throughput. B. Experiment 1: Overall Performance of RETENTION To evaluate the overall performance of RETENTION, we compare Random Forest models with tolerance set to {1 , 3 , 5 } against baseline methods. As shown in Fig. 5, RE- TENTION exhibits substantial reductions, achieving 66.25 to 99.96 fewer TCAMs required (equivalent to 2.96 to 2420.81 better space efficiency) in energy-efficient map- ping, and 70.83 to 99.70 reduction (3.43 to 334.48 improvement) in space-efficient mapping. Specifically, setting tolerance to a moderate value of 3 yields 4.35 to 207.12 better space efficiency. These results demonstrate that RE- TENTION effectively minimizes CAM capacity requirement, offering a promising solution for resource-efficient tree-based model acceleration with CAM. C. Experiment 2: Purity Threshold Pruning Noticing the extraordinary performance of RETENTION, we break down the framework and evaluate each component separately. This section analyzes the effectiveness of purity threshold pruning by mapping Random Forest models with naive unified mapping. By setting tolerance to {1 , 3 , 5 }, purity threshold pruning alone achieves considerable im- provements, reducing CAM capacity requirement by 21.04 to 99.93 (1.27 to 1357.12 improvement) compared to unpruned models. The relatively lower improvement on the Letter and Wine datasets stems from the increased number of classes, which makes it more challenging for nodes to achieve high purity. Pruning such nodes may result in sig- nificant accuracy degradation, which the tolerance mecha- nism prevents. Consequently, only a few higher-purity nodes can be pruned, thereby limiting the impact of the purity threshold pruning. We further scale tolerance up to {10 , 15 , 20 } to investigate the correlation between tolerance and different datasets. As shown in the left part of Fig. 6, the performance of purity threshold pruning converges at different levels of tolerance, depending on task complexity. Though all can achieve over 90 reduction, simple tasks easily converge with low tolerance ({1 , 3 , 5 }, refer to 10 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Number of TCAMs 1 3 5 10 15 20 Tolerance vs Accuracy Loss 0 1 2 3 4 5 6 1 2 3 4 5 Testing Accuracy Loss Tolerance Adult CreditApproval DryBean Letter Wine IDEAL Purity Threshold Pruning Fig. 6. Effectiveness of purity threshold pruning. 0 10 20 30 40 50 60 70 80 90 100 XGB RF XGB RF XGB RF XGB RF XGB RF Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Energy-Efficient Mapping (ODR) -100 -80 -60 -40 -20 0 20 40 60 80 100 XGB RF XGB RF XGB RF XGB RF XGB RF Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs FR SPC Space-Efficient Mapping (SPC) Fig. 7. Comparison of different data placement strategies (RF: Random Forest, XGB: XGBoost). the red columns), while the convergence for complex tasks occurs with relatively higher tolerance ({10 , 15 , 20 }, refer to the blue columns). However, for imbalanced datasets such as Adult, higher tolerance may lead to a model always favoring the majority class. This should be carefully dealt with by leveraging techniques such as class weights, as discussed in Section III-B. The right part of Fig. 6 exhibits that the user-specified tolerance is highly correlated to testing accuracy loss. Since tolerance is the expected accuracy degradation that is sacrificed to achieve lower model complexity, ideally the testing accuracy loss should be lower than or similar to toler- ance. Results show that most of the testing accuracy losses are lower than the predefined tolerance, with only one case slightly higher, suggesting that purity threshold pruning effectively reduces model complexity while ensuring controlled accuracy degradation. Models pruned with higher tolerance also follow this trend, though we omitted the statistics here for clarity. D. Experiment 3: Tree Mapping Scheme After validating purity threshold pruning, we estimate the effectiveness of the proposed data placement strategies. Fig. 7 presents the results of implementing ODR and SPC on XGBoost and unpruned Random Forest. Since unpruned Random Forest is usually more complex than XGBoost, the improvement of applying ODR and SPC on Random Forest is often more obvious. For energy-efficient mapping, ODR presents more than 50 reduction in all cases. Since unique conditions of the models trained on CreditAp- proval and Letter datasets is an order of magnitude fewer than in other datasets, the improvement observed in the Random Forest trained on these two datasets is relatively limited. For space-efficient mapping, SPC exhibits notable improvement, with over 30 reduction in every case, while FR performs worse than the baseline in half of the cases. The underlying reason is that FR only eliminates completely redundant path segments within a TCAM, whereas significant redundancy is still left behind. Thus, FR beats naive independent mapping only when the redundancy within a single tree is overwhelm- ing. On the other hand, by greedily clustering similar paths together, SPC minimizes redundancy while incurring less runtime computational overhead, consistently outperforming FR and naive independent mapping in all cases. The per- formance of applying SPC to the Random Forest trained on the CreditApproval dataset is nearly optimal (SPC requires 99 TCAMs, while the theoretical minimum is 98 for 6246 paths). However, the exceptionally strong performance of naive inde- pendent mapping on this model limits the potential for further improvement. Experimental results show that implementing the tree mapping scheme alone yields 1.46 to 21.30 better space efficiency, highlighting the effectiveness of both ODR and SPC. E. Experiment 4: Number of Trees Prior experiments demonstrate that RETENTION is ex- tremely effective in minimizing CAM capacity requirement. RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 11 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Trees 100 Trees 300 Trees 500 Space-Efficient Mapping (SPC) 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Trees 100 Trees 300 Trees 500 Energy-Efficient Mapping (ODR) Fig. 8. Impact of number of trees on the performance of data placement strategies. 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs 64 64 128 128 256 256 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs 64 64 128 128 256 256 Space-Efficient Mapping (SPC) Energy-Efficient Mapping (ODR) Fig. 9. Impact of TCAM sizes on the performance of data placement strategies. In this section, we study the impact of number of trees on RE- TENTION s performance. Since XGBoost can achieve higher accuracy with an increased number of trees and appropriate hyperparameter tuning, XGBoost with num trees set to {100, 300, 500} are evaluated in this experiment. As shown in Fig. 8, the performance of RETENTION tends to improve as num trees increases because the inefficient issue is amplified by num trees. However, since the training phase incorporates randomness, and the selected conditions for splitting nodes can strongly affect RETENTION s performance, the result may not always follow the trend. F. Experiment 5: Different TCAM Sizes In addition to the number of trees, the size of the TCAM is another factor that may affect RETENTION s performance. Fig. 9 illustrates the impact of mapping XGBoost to TCAMs with three different sizes. Results suggest that as TCAM size increases, RETENTION yields gradually diminishing improvements in energy-efficient mapping, while offering no- table gains in space-efficient mapping. Although the designs of both naive unified mapping and ODR are not directly correlated with TCAM size, it becomes increasingly difficult for ODR to eliminate TCAMs as the size increases, since only those entirely filled with X states can be removed. In some edge cases, such as mapping the models trained on the CreditApproval or Letter datasets to 256 256 TCAMs, ODR may offer no improvement because none of the TCAMs are fully redundant even after reordering. Conversely, increasing the TCAM size can sometimes create partially redundant TCAMs with only a few non-X-state cells. In such cases, ODR can effectively mitigate this issue, allowing these TCAMs to be removed and improving space efficiency. For example, mapping the Letter model to 128 128 TCAMs may benefit from this effect. On the other hand, although improvement increases with TCAM size when RETENTION is applied for space-efficient mapping, CAM utilization actually decreases because larger TCAMs require more X states per row, which leads to greater inefficiency. However, since naive indepen- dent mapping assigns each tree to a separate TCAM, larger capacities often fail to be fully exploited, leading to greater performance degradation compared to SPC. Consequently, a smaller TCAM is recommended for RETENTION regardless of the optimization objective, though it must still be large enough to accommodate the longest path in the model. G. Computational Overhead, Energy Consumption, and Throughput Because prior studies omit the cost of input pre-processing, we analyze the computational overhead, energy consumption, and throughput analytically rather than with hardware software simulation. For energy-efficient mapping, both naive unified mapping and ODR share the same input sequence among TCAMs in the same column, and a path is stored in a single row to prevent additional computational overhead for result retrieval. For space-efficient mapping, naive independent map- ping and SPC require a unique input sequence for each TCAM, 12 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS leading to higher computational overhead. However, as naive independent mapping stores paths to independent rows, and SPC stores paths in a single TCAM, none of them require further processing for result retrieval. Although FR prevents additional input pre-processing by sharing input sequences among TCAM columns, the paths are no longer stored in the same row after the row elimination process, and the associated computational overhead for result retrieval is proportional to TCAMs. Therefore, considering computational overhead, the complexity of both ODR and SPC is comparable to previous works. In terms of energy consumption, as RETENTION does not require any additional on-the-fly computation, the reduction in the number of TCAMs is equivalent to energy savings. Furthermore, the energy-saving precharge mechanism proposed in DT2CAM is compatible with ODR, enabling additional reductions in energy consumption. Considering throughput, since RETENTION does not require any hardware modification, and the computational overhead is comparable to or even slightly lower than previous works as the required number of TCAM is dramatically reduced, RETENTION is supposed to offer a similar or even higher throughput than others. V. CONCLUSION nvTCAM shows great promise to accelerate tree-based model inference effectively and efficiently, yet the CAM capacity requirement is unacceptably high, and most of the cells are storing the X state for format alignment, which is re- dundant. In this work, we introduce RETENTION, an end-to- end framework designed to reduce the CAM capacity require- ment for tree-based model inference. RETENTION introduces purity threshold pruning to minimize model complexity while ensuring controlled accuracy degradation for bagging-based models. A tree mapping scheme with two data placement strategies, occurrence-based double reordering and similarity- based path clustering, is proposed to further alleviate memory redundancy for energy-efficient mapping and space-efficient mapping. Experimental results show that implementing the tree mapping scheme alone achieves 1.46 to 21.30 better space efficiency, while the full RETENTION framework yields 4.35 to 207.12 improvement with less than 3 accuracy loss. These results demonstrate that RETENTION is extremely effective in reducing CAM capacity requirement, offering a resource-efficient solution for tree-based model acceleration. By implementing RETENTION, resource retention becomes feasible, bringing CAM-based acceleration closer to practical- ity in resource-constrained environments. REFERENCES [1] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Inf. Fusion, 81(C):84 90, May 2022. [2] L eo Grinsztajn, Edouard Oyallon, and Ga el Varoquaux. Why do tree- based models still outperform deep learning on typical tabular data? In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. [3] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jor- dan M. Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global understanding with explainable ai for trees. Nature Machine Intelligence, 2(1):56 67, 2020. [4] DARPA. Explainable artificial intelligence (xai). research programs explainable-artificial-intelligence, 2017. [5] Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May, Geoffrey I. Webb, and Shirui Pan. Large language models for scientific discovery in molecular property prediction. Nature Machine Intelligence, 2025. [6] Jonathan Kwaku Afriyie, Kassim Tawiah, Wilhemina Adoma Pels, Sandra Addai-Henne, Harriet Achiaa Dwamena, Emmanuel Odame Owiredu, Samuel Amening Ayeh, and John Eshun. A supervised machine learning algorithm for detecting and predicting fraud in credit card transactions. Decision Analytics Journal, 6:100163, 2023. [7] Rongtao Zhang, Binbin Li, and Bin Jiao. Application of xgboost algorithm in bearing fault diagnosis. IOP Conference Series: Materials Science and Engineering, 490(7):072062, apr 2019. [8] Kaggle. Kaggle machine learning data science survey 2021, 2021. [9] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16, page 785 794, New York, NY, USA, 2016. Association for Computing Machinery. [10] Francesco Daghero, Alessio Burrello, Enrico Macii, Paolo Montuschi, Massimo Poncino, and Daniele Jahier Pagliari. Dynamic decision tree ensembles for energy-efficient inference on iot edge nodes. IEEE Internet of Things Journal, 11(1):742 757, 2024. [11] Zhen Xie, Wenqian Dong, Jiawen Liu, Hang Liu, and Dong Li. Tahoe: tree structure-aware high performance inference engine for decision tree ensemble on gpu. In Proceedings of the Sixteenth European Conference on Computer Systems, EuroSys 21, page 426 440, New York, NY, USA, 2021. Association for Computing Machinery. [12] Adri an Alcolea and Javier Resano. Fpga accelerator for gradient boosting decision trees. Electronics, 10:314, 01 2021. [13] Brian Van Essen, Chris Macaraeg, Maya Gokhale, and Ryan Prenger. Accelerating a random forest classifier: Multi-core, gp-gpu, or fpga? In 2012 IEEE 20th International Symposium on Field-Programmable Custom Computing Machines, pages 232 239, 2012. [14] Kostas Pagiamtzis and Ali Sheikholeslami. Content-addressable memory (cam) circuits and architectures: a tutorial and survey. IEEE Journal of Solid-State Circuits, 41(3):712 727, 2006. [15] Ali Shafiee, Anirban Nag, Naveen Muralimanohar, Rajeev Balasubra- monian, John Paul Strachan, Miao Hu, R. Stanley Williams, and Vivek Srikumar. Isaac: a convolutional neural network accelerator with in-situ analog arithmetic in crossbars. In Proceedings of the 43rd International Symposium on Computer Architecture, ISCA 16, page 14 26. IEEE Press, 2016. [16] Ping Chi, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, and Yuan Xie. Prime: A novel processing-in-memory ar- chitecture for neural network computation in reram-based main memory. In 2016 ACM IEEE 43rd Annual International Symposium on Computer Architecture (ISCA), pages 27 39, 2016. [17] An Chen. A review of emerging non-volatile memory (nvm) technolo- gies and applications. Solid-State Electronics, 125, 07 2016. [18] Le Zheng, Sangho Shin, Scott Lloyd, Maya Gokhale, Kyungmin Kim, and Sung-Mo Kang. Rram-based tcams for pattern search. In 2016 IEEE International Symposium on Circuits and Systems (ISCAS), pages 1382 1385, 2016. [19] Ke-Ji Zhou, Chen Mu, Bo Wen, Xu-Meng Zhang, Guang-Jian Wu, Can Li, Hao Jiang, Xiao-Yong Xue, Shang Tang, Chi-Xiao Chen, and Qi Liu. The trend of emerging non-volatile tcam for parallel search and ai applications. Chip, 1(2):100012, 2022. [20] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Ga el Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pages 108 122, 2013. [21] Leo Breiman. Random forests. Machine Learning, 45(1):5 32, 2001. [22] Mariam Rakka, Mohammed E. Fouda, Rouwaida Kanj, and Fadi Kurdahi. Dt2cam: A decision tree to content addressable memory framework. IEEE Transactions on Emerging Topics in Computing, 11(3):805 810, 2023. [23] Giacomo Pedretti, Catherine E. Graves, Sergey Serebryakov, Ruibin Mao, Xia Sheng, Martin Foltin, Can Li, and John Paul Strachan. Tree- based machine learning performed in-memory with memristive analog cam. Nature Communications, 12(1):5806, 2021. [24] Giacomo Pedretti, John Moon, Pedro Bruel, Sergey Serebryakov, Ron Roth, Luca Buonanno, Archit Gajjar, Lei Zhao, Tobias Ziegler, Cong Xu, RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 13 Martin Foltin, Paolo Faraboschi, Jim Ignowski, and Catherine Graves. X-time: Accelerating large tree ensembles inference for tabular data with analog cams. IEEE Journal on Exploratory Solid-State Computational Devices and Circuits, PP:1 1, 01 2024. [25] Chieh-Lin Tsai, Chun-Feng Wu, Yuan-Hao Chang, Han-Wen Hu, Yung- Chun Lee, Hsiang-Pang Li, and Tei-Wei Kuo. A digital 3d tcam accelerator for the inference phase of random forest. In 2023 60th ACM IEEE Design Automation Conference (DAC), pages 1 6, 2023. [26] Xunzhao Yin, Franz M uller, Ann Franchesca Laguna, Chao Li, Qingrong Huang, Zhiguo Shi, Maximilian Lederer, Nellie Laleni, Shan Deng, Zijian Zhao, Mohsen Imani, Yiyu Shi, Michael Niemier, Xiaobo Sharon Hu, Cheng Zhuo, Thomas K ampfe, and Kai Ni. Deep random forest with ferroelectric analog content addressable memory. Science Advances, 10(23):eadk8471, 2024. [27] Leo Breiman. Out-of-bag estimation. 1996. [28] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017. [29] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018. [30] Leo Breiman, Jerome Friedman, R. A. Olshen, and Charles J. Stone. Classification and Regression Trees. Chapman and Hall CRC, 1st edition, 1984. [31] Leo Breiman. Bagging predictors. Mach. Learn., 24(2):123 140, August 1996. [32] Robert E. Schapire. A brief introduction to boosting. In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI 99, page 1401 1406, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. [33] Robert Karam, Ruchir Puri, Swaroop Ghosh, and Swarup Bhunia. Emerging trends in design and applications of memory-based com- puting and content-addressable memories. Proceedings of the IEEE, 103(8):1311 1330, 2015. [34] Xunzhao Yin, Chao Li, Qingrong Huang, Li Zhang, Michael Niemier, Xiaobo Sharon Hu, Cheng Zhuo, and Kai Ni. Fecam: A universal com- pact digital and analog content addressable memory using ferroelectric. IEEE Transactions on Electron Devices, 67(7):2785 2792, 2020. [35] Igor Arsovski, Trevis Chandler, and Ali Sheikholeslami. A ternary content-addressable memory (tcam) based on 4t static storage and including a current-race sensing scheme. IEEE Journal of Solid-State Circuits, 38(1):155 158, 2003. [36] Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. [37] John Ross Quinlan. Credit Approval. UCI Machine Learning Repository, 1987. [38] Dry Bean. UCI Machine Learning Repository, 2020. 24432 C50S4B. [39] David Slate. Letter Recognition. UCI Machine Learning Repository, 1991. [40] Cortez, Paulo, Cerdeira, A., Almeida, F., Matos, T., and Reis, J. Wine quality. UCI Machine Learning Repository, 2009. 24432 C56S3T. [41] Can Li, Catherine E. Graves, Xia Sheng, Darrin Miller, Martin Foltin, Giacomo Pedretti, and John Paul Strachan. Analog content-addressable memories with memristors. Nature Communications, 11(1):1638, 2020. [42] Zhezhi He, Jie Lin, Rickard Ewetz, Jiann-Shiun Yuan, and Deliang Fan. Noise injection adaption: End-to-end reram crossbar non-ideal effect adaption for neural network mapping. In Proceedings of the 56th Annual Design Automation Conference 2019, DAC 19, New York, NY, USA, 2019. Association for Computing Machinery. [43] Yao-Wen Kang, Chun-Feng Wu, Yuan-Hao Chang, Tei-Wei Kuo, and Shu-Yin Ho. On minimizing analog variation errors to resolve the scalability issue of reram-based crossbar accelerators. IEEE Transac- tions on Computer-Aided Design of Integrated Circuits and Systems, 39(11):3856 3867, 2020. [44] Fan Yang, Wei hang Lu, Lin kai Luo, and Tao Li. Margin optimization based pruning for random forest. Neurocomputing, 94:54 63, 2012. [45] Heping Zhang and Minghui Wang. Search for the smallest random forest. Statistics and its interface, 2:381, 01 2009. [46] Deepti Sehrawat and Nasib Singh Gill. Smart sensors: Analysis of different types of iot sensors. In 2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI), pages 523 528, 2019. [47] Tobias Bjerregaard and Shankar Mahadevan. A survey of research and practices of network-on-chip. ACM Comput. Surv., 38(1):1 es, June 2006. [48] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. The uci machine learning repository. 2024. [49] Marvin Wright and Andreas Ziegler. ranger: A fast implementation of random forests for high dimensional data in c and r. Journal of Statistical Software, 77, 08 2015. Yi-Chun Liao is currently pursuing the bachelor degree with the Department of Computer Science and Information Engineering, National Taiwan Uni- versity, Taipei, Taiwan. His primary research interests include in-memory computing, emerging non-volatile memory, and hardware software co-design. Chieh-Lin Tsai received his M.S. degree from the Department of Computer Science, National Tsing- Hua University, Hsinchu, Taiwan, in 2017. He is currently working toward the PhD degree in the Department of Computer Science and Information Engineering, National Taiwan University. His pri- mary research interests include non-volatile memory systems and system-level design with in-memory processing. Yuan-Hao Chang (SM 14 F 23) received his Ph.D. in Computer Science from the Department of Com- puter Science and Information Engineering at Na- tional Taiwan University, Taipei, Taiwan. He is cur- rently a Research Fellow at the Institute of Infor- mation Science, Academia Sinica, Taipei, Taiwan, where he served as an Associate Research Fellow between March 2015 and June 2018, and as an As- sistant Research Fellow from August 2011 to March 2015. His research interests include memory storage systems, operating systems, embedded systems, and real-time systems. He is an IEEE Fellow. Cam elia Slimani is an associate professor at the National Polytechnic Institute of Toulouse, France, and a member of the IRIT laboratory. She is part of a research group working on operating systems, distributed systems, and middleware. She received a Ph.D. in computer science from the University of Western Brittany, France, in 2022. She was then a postdoctoral fellow at ENSTA Bretagne, France. Her research interests include optimizing machine learning algorithms for memory-constrained envi- ronments. 14 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS Jalil Boukhobza received the electrical engineer- ing (with Hons.) degree from the Institut Na- tionale d Electricite et d electronique (I.N.E.L.E.C) Boumerdes, Algeria, in 1999, and the MSc and PhD degrees in computer science from the University of Versailles, France, in 2000 and 2004, respectively. He is a Professor with the ENSTA, a French State Graduate, PostGraduate and Research Institute part of the Institut Polytechnique de Paris. He was a research fellow with the PRiSM Laboratory (Uni- versity of Versailles) from 2004 to 2006. He was an associate professor with the University Bretagne Occidentale, Brest, France, from 2006 to 2020 and is a member of Lab-STICC. He has also been working with the Technology Research Institute (IRT) bcom since 2013. His main research interests include storage system design, performance evaluation and energy optimization, and operating system design. He works on different application domains such as embedded systems,cloud computing, and database systems. He is an IEEE Senior member. Tei-Wei Kuo Prof. Kuo received his B.S.E. and Ph.D. degrees in Computer Science from National Taiwan University and University of Texas at Austin in 1986 and 1994, respectively. He is CTO of Delta Electronics (2024.02-now), a global leader in power and thermal solutions with annual revenue USD14 billions in 2024. He was Distinguished Professor of Department of Computer Science and Infor- mation Engineering of National Taiwan University (2009.08-2025.07), where he was Interim President (2017.10-2019.01) and Executive Vice President for Academics and Research (2016.08-2019.01). Prof. Kuo was Lee Shau Kee Chair Professor of Information Engineering, Advisor to President (Information Technology), and Founding Dean of College of Engineering, City University of Hong Kong (2019.08-2022.07). His research interest includes embedded systems, non-volatile-memory software designs, neuromorphic computing, and real-time systems. Dr. Kuo is Fellow of ACM, IEEE, and US National Academy of Inventors. He is Chair of ACM SIGAPP (since 2023). Prof. Kuo received numerous awards and recognition, including Academic Award of Taiwan Ministry of Education (2023), Humboldt Research Award (Germany; 2021), Outstanding Technical Achievement and Leadership Award (2017) from IEEE TC on Real-Time Systems, and Distinguished Research Award from Taiwan National Science and Technology Council for three times. Prof. Kuo is the founding Editor-in-Chief of ACM Transactions on Cyber-Physical Systems (2015-2021) and a program committee member of many top conferences. He has over 350 technical papers published in international journals and conferences and received many best paper awards, including Best Paper Award from ACM IEEE CODES ISSS 2019, 2022, and 2024, and ACM HotStorage 2021.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.05994v1 [cs.LG] 6 Jun 2025 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS 1 RETENTION: Resource-Efficient Tree-Based Ensemble Model Acceleration with Content-Addressable Memory Yi-Chun Liao, Chieh-Lin Tsai, Yuan-Hao Chang, Fellow, IEEE, Cam elia Slimani, Jalil Boukhobza, Senior Member, IEEE, and Tei-Wei Kuo, Fellow, IEEE Abstract Although deep learning has demonstrated remark- able capabilities in learning from unstructured data, modern tree- based ensemble models remain superior in extracting relevant information and learning from structured datasets. While several efforts have been made to accelerate tree-based models, the inherent characteristics of the models pose significant challenges for conventional accelerators. Recent research leveraging content- addressable memory (CAM) offers a promising solution for accelerating tree-based models, yet existing designs suffer from excessive memory consumption and low utilization. This work addresses these challenges by introducing RETENTION, an end-to-end framework that significantly reduces CAM capacity requirement for tree-based model inference. We propose an iterative pruning algorithm with a novel pruning criterion tailored for bagging-based models (e.g., Random Forest), which minimizes model complexity while ensuring controlled accuracy degradation. Additionally, we present a tree mapping scheme that incorporates two innovative data placement strategies to alleviate the memory redundancy caused by the widespread use of don t care states in CAM. Experimental results show that implementing the tree mapping scheme alone achieves 1.46 to 21.30 better space efficiency, while the full RETENTION framework yields 4.35 to 207.12 improvement with less than 3 accuracy loss. These results demonstrate that RETENTION is highly effective in reducing CAM capacity requirement, providing a resource- efficient direction for tree-based model acceleration. Index Terms Tree-based machine learning, bagging-based model pruning, content-addressable memory, data placement optimization, in-memory computing. I. INTRODUCTION Structured (i.e., tabular) data is one of the most prevalent formats in data science. It is typically represented as a matrix, This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.\n\n--- Segment 2 ---\nIt is typically represented as a matrix, This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Yi-Chun Liao is with the Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan. E-mail: Chieh-Lin Tsai is with the Department of Computer Science and Informa- tion Engineering, National Taiwan University, Taipei 10617, Taiwan. E-mail: Yuan-Hao Chang is with the Institute of Information Science, Academia Sinica, Taipei 11529, Taiwan. E-mail: Cam elia Slimani is with IRIT, Universit e de Toulouse, Toulouse INP UT3, CNRS, 31062 Toulouse, France. E-mail: Jalil Boukhobza is with Lab-STICC, CNRS UMR 6285 , EN- STA, Institut Polytechnique de Paris, 29806 Brest, France. E-mail: Tei-Wei Kuo is with the Department of Computer Science and Information Engineering, National Taiwan University, Taipei 10617, Taiwan. He is also with Delta Electronics, Taoyuan 33378, Taiwan. E-mail: where each row corresponds to an instance and all instances share the same set of features across columns. This format is widely used in fields such as finance, medicine, and scientific research, as its structured nature enables efficient processing. For example, sensors generate data in tabular format to support real-time analysis for AI-driven closed-loop control. Despite significant advancements in deep learning for unstructured data (e.g., text, images, and speech), studies [1], [2] have shown that modern tree-based ensemble models consistently outperform deep learning in several tasks. In fact, tree-based models re- main the state-of-the-art for classification and regression tasks involving medium-sized structured datasets [2], and are often favored over deep learning due to their high interpretability [3], particularly in sensitive applications where understanding model decisions is critical [4]. Tree-based models are widely applied in domains such as scientific research [5], credit card fraud detection [6], and machinery fault diagnosis [7].\n\n--- Segment 3 ---\nIn fact, tree-based models re- main the state-of-the-art for classification and regression tasks involving medium-sized structured datasets [2], and are often favored over deep learning due to their high interpretability [3], particularly in sensitive applications where understanding model decisions is critical [4]. Tree-based models are widely applied in domains such as scientific research [5], credit card fraud detection [6], and machinery fault diagnosis [7]. A recent survey [8] found that over 74 of data scientists prefer tree-based models, whereas fewer than 40 opt for neural networks, underscoring their continued importance. However, despite their widespread adoption and effective- ness, tree-based models have received less attention in recent years, and the inefficiency of tree-based model inference remains a critical yet unresolved challenge. Since inference requires multiple tree traversals, and the number of possible paths grows exponentially with tree depth, predicting and prefetching data becomes challenging. This may lead to rela- tively high inference latency, which is particularly unfriendly for real-time applications. The issue is further exacerbated as modern tree-based models (e.g., XGBoost [9]) can con- tain thousands of trees, and are often deployed in resource- constrained environments [10]. Although various accelerators have been proposed in recent years [11], [12], researchers [13] have found that conventional accelerators, such as multi-core CPUs, GP-GPUs, and FPGAs, offer limited effectiveness for tree-based model acceleration due to the non-deterministic memory access patterns and irregular tree structures. One promising approach to improving tree-based model inference is leveraging in-memory com- puting (IMC), which integrates data storage and processing within the same location, thereby eliminating latency and energy costs associated with data access and transfer. While conventional SRAM-based IMC accelerators can effectively accelerate model inference, they come at the cost of high energy consumption and extensive area overhead [14], making 2 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS it impractical for resource-constrained environments.\n\n--- Segment 4 ---\nOne promising approach to improving tree-based model inference is leveraging in-memory com- puting (IMC), which integrates data storage and processing within the same location, thereby eliminating latency and energy costs associated with data access and transfer. While conventional SRAM-based IMC accelerators can effectively accelerate model inference, they come at the cost of high energy consumption and extensive area overhead [14], making 2 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS it impractical for resource-constrained environments. Recently, there has been significant interest in developing emerging non-volatile-memory-based (NVM-based) IMC ac- celerators [15], [16], [17], as the characteristics of emerg- ing NVM (e.g., high density, low power consumption, and low cost) make it well-suited for resource-constrained envi- ronments. Among potential candidates, non-volatile ternary content-addressable memory (nvTCAM) [18], [19] exhibits the best fit for accelerating tree-based models due to its capability to perform sequence matching with extremely high parallelism and its high reliability. Since every root-to-leaf path within tree-based models can be encoded into a binary sequence, nvTCAM can traverse all the paths in one shot, enabling unprecedented acceleration for model inference. Nevertheless, to support in-memory search, data must be organized in a spe- cialized format (refer to Section II-B for further explanation). Although nvTCAM effectively accelerates tree-based model inference in a cost- and energy-efficient manner, a considerable portion of memory cells is allocated for format alignment rather than storing actual model data. This leads to excessive memory consumption with substantial redundancy, which is highly inefficient and requires further optimization. To achieve resource-efficient acceleration, the enormous CAM capacity requirement with considerable redundancy is the major obstacle that must be addressed before practical implementation. Since model complexity is highly correlated with memory consumption, pruning models can significantly mitigate this issue. Modern tree-based models often employ pruning algorithms, such as limiting maximum depth or min- imum impurity decrease [20], in order to reduce overfitting and simplify model structure.\n\n--- Segment 5 ---\nSince model complexity is highly correlated with memory consumption, pruning models can significantly mitigate this issue. Modern tree-based models often employ pruning algorithms, such as limiting maximum depth or min- imum impurity decrease [20], in order to reduce overfitting and simplify model structure. While these techniques are well- suited for boosting-based ensemble models (e.g., XGBoost), where each tree in the ensemble is trained to correct the errors of the previous ones, applying them to bagging-based tree ensemble models (e.g., Random Forest [21]) can lead to severe accuracy degradation due to the independent training of each tree. On the other hand, memory redundancy arises from structuring data for in-memory search, suggesting that a tailored data placement strategy could further reduce CAM capacity requirement. While several studies [22], [23], [24], [25], [26] have explored accelerating tree-based models using non-volatile CAM, little attention has been paid to the issue of memory redundancy, and existing data placement strategies fail to address this challenge effectively and efficiently. Miti- gating redundancy requires a solution that not only reduces model complexity but also optimizes memory utilization, paving the way for a more resource-efficient acceleration. Based on the above observations, we propose RETENTION, an end-to-end framework that minimizes memory consumption for accelerating tree-based models with CAM. RETENTION offers a pruning algorithm with a novel pruning criterion. The algorithm is applied to bagging-based models iteratively during out-of-bag (OOB) estimation [27], which effectively reduces model complexity while ensuring controlled accuracy degradation. In addition, after analyzing the tradeoffs between different data placement strategies, RETENTION incorporates a tree mapping scheme with two innovative data placement strategies. The strategies are tailored for different optimization criteria, aiming to alleviate memory redundancy and further reduce CAM capacity requirement. To the best of our knowledge, this is the first work that systematically optimizes data placement strategies for tree- based model acceleration with CAM, explicitly considering the tradeoffs between memory redundancy and processing overhead. RETENTION is evaluated on Random Forest and XGBoost using five datasets. Although validated with these two models, the framework can be generalized to other en- semble models with similar structures such as LightGBM [28] and CatBoost [29].\n\n--- Segment 6 ---\nRETENTION is evaluated on Random Forest and XGBoost using five datasets. Although validated with these two models, the framework can be generalized to other en- semble models with similar structures such as LightGBM [28] and CatBoost [29]. Experimental results show that our tree mapping scheme alone achieves 1.46 to 21.30 better space efficiency, while the complete framework with pruning and tree mapping optimization improves space efficiency by 4.35 to 207.12 with less than 3 accuracy degradation. These results demonstrate that RETENTION effectively re- duces CAM capacity requirement, making tree-based model acceleration with nvTCAM more resource-efficient and feasi- ble for resource-constrained environments. The rest of this paper is organized as follows: Section II presents the background, observation, and motivation of this work. Section III introduces the philosophy and detailed design of RETENTION. Section IV evaluates RETENTION s effectiveness and compares it with existing works. Finally, Section V provides concluding remarks. II. BACKGROUND, OBSERVATION, AND MOTIVATION A. Decision Tree, Bagging, and Boosting Decision tree [30] is a commonly used supervised machine learning algorithm for classification and regression tasks, val- ued for its simplicity in training and high interpretability. Typ- ically structured as a binary tree, each internal node represents a decision condition based on a feature and a corresponding threshold, while each leaf node stores a predicted result. The training process involves a series of node-splitting operations, where all instances initially start at the root node. During each split, instances within the current node are evaluated, and the optimal feature-threshold pair that best separates them (i.e., generates maximum impurity decrease) is selected as the node s condition. The instances are then distributed to the appropriate child nodes based on whether they satisfy the condition. This process continues iteratively until all instances are separated or predefined constraints, such as maximum depth or minimum impurity decrease, are reached. The left part of Fig. 1 presents an example of decision tree inference. As shown in Fig. 1, inference follows a similar tree traversal process, where an instance starts at the root node and follows a path determined by the conditions at each encountered node. The prediction result is the value stored in the reached leaf node.\n\n--- Segment 7 ---\n1, inference follows a similar tree traversal process, where an instance starts at the root node and follows a path determined by the conditions at each encountered node. The prediction result is the value stored in the reached leaf node. Since each root-to-leaf path is determined by a set of conditions that an instance must satisfy during traversal, it can be viewed as a sequence of condition checks leading to the final prediction. Additionally, as there is no contradiction between the conditions within a path, the order of condition checks is irrelevant to the prediction result, providing an opportunity to accelerate inference with CAM. However, the simplicity in training comes with a major drawback: overfitting, where the model captures noise in the RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 3 F F X X F T X X T X F F T X F T T X T T Min 0.5 Min 0.3 Min Max Min 0.5 0.3 Max Min Max 0.5 0.8 Min Max Min 0.7 0.8 Max Min Max Min 0.7 0.5 Max Min Max 0.7 Max f1 0.5 f2 0.3 Class1 Class2 f3 0.7 Class3 f1 0.8 Class2 Class3 T T T T F F F F Path1 Path2 Path3 Path4 Path5 Input: {f1 0.4, f2 0.5, f3 0.8} Path1 Path2 Path3 Path4 Path5 f1 0.5 f2 0.3 f3 0.7 f1 0.8 F T T F In-Memory Search with Ternary CAM In-Memory Search with Analog CAM 0.4 0.5 0.8 f1 f2 f3 White: Unvisited Nodes Unmatched Conditions. Blue: Visited Nodes Matched Conditions. Diagonal Hatch: Don t Care. Fig. 1. Visualization of decision tree inference acceleration with CAM. training data, reducing its ability to generalize to unseen instances. To mitigate overfitting and enhance model perfor- mance, decision-tree-based ensemble models were introduced. Bagging (i.e., bootstrap aggregation) [31] is a widely used algorithm for tree-based ensemble model training.\n\n--- Segment 8 ---\nTo mitigate overfitting and enhance model perfor- mance, decision-tree-based ensemble models were introduced. Bagging (i.e., bootstrap aggregation) [31] is a widely used algorithm for tree-based ensemble model training. It trains multiple fully-grown decision trees independently, each on a unique subset of the dataset generated through bootstrapping (i.e., sampling with replacement). When making a prediction, each tree produces an individual result, and the ensemble de- termines the final output via majority voting (if classification) or averaging (if regression). By training trees on different subsets of data, bagging enhances robustness against overfit- ting, reducing the need for pruning. When additional model compression is required, early-stopping (i.e., pre-pruning) is preferred to pruning constructed trees (i.e., post-pruning), as conventional post-pruning algorithms ignore the collective behavior of the model and can erode the diversity that bagging- based model relies on. Additionally, bagging enables OOB estimation, an inherent validation method that eliminates the need for a separate validation set. Since each tree is trained on a subset of data, the unused instances can serve as the validation set, which is later passed through the correspond- ing tree for evaluation. By aggregating predictions from all trees that did not train on a given instance, OOB estimation provides an unbiased measure of model performance, making it particularly useful when data is limited. Bagging alleviates overfitting, while the ensemble compensates for the missing information in individual trees, resulting in improved accuracy compared to a single decision tree. Additionally, since each tree in a bagging-based model is trained independently and contributes equally to the final prediction, models such as Random Forest perform exceptionally well on noisy and small datasets. However, they struggle to capture complex patterns, as they do not fully exploit interactions among trees within the ensemble. Moreover, the averaging mechanism limits the model performance in regression tasks, highlighting the need for an alternative. Another class of tree ensemble model primitives is the boosting-based model, which is commonly utilized for both classification and regression tasks. In contrast to bagging, which trains multiple fully-grown decision trees indepen- dently, boosting [32] trains shallow decision trees sequentially, with each tree attempting to correct the errors of its predeces- sors.\n\n--- Segment 9 ---\nAnother class of tree ensemble model primitives is the boosting-based model, which is commonly utilized for both classification and regression tasks. In contrast to bagging, which trains multiple fully-grown decision trees indepen- dently, boosting [32] trains shallow decision trees sequentially, with each tree attempting to correct the errors of its predeces- sors. During training, boosting fits the first tree on the entire dataset, and each subsequent tree is trained to correct the errors of its predecessors, with errors being reevaluated at each step as new trees are added to the ensemble. When making a prediction, the final result is computed as a weighted sum of individual tree predictions rather than relying on majority voting or averaging, as trees in boosting-based models con- tribute with different importance weights. While the error cor- rection mechanism and the sequential training process adapt the models to task complexity, they are more susceptible to overfitting and therefore rely heavily on model pruning. How- ever, since the training phase leverages interactions among trees, post-pruning can disrupt these inter-tree dependencies, leading to catastrophic accuracy degradation. Consequently, boosting-based models often opt for pre-pruning algorithms such as limiting maximum depth and minimum impurity decrease. However, modern boosting implementations, such as XGBoost, can easily contain thousands of trees, resulting in substantial memory consumption. Additionally, the irregular and non-deterministic memory access patterns in tree traver- sal make efficient execution on conventional hardware chal- lenging. To mitigate these inefficiencies, content-addressable memory (CAM) enables massively parallel in-memory search, significantly improving inference speed and efficiency. B. Ternary CAM and Analog CAM Content-addressable memory (CAM) [14], [33], [34] is a specialized memory architecture widely used in various applications. Unlike conventional memory, which retrieves data based on a given address, CAM allows simultaneous comparison of an input query sequence against all stored binary sequences, and returns the addresses or associated data of matching entries. Data in CAM is stored row-wise, with 4 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS TABLE I DATASET INFORMATION AND THE EXPERIMENT RESULTS FOR SIGNIFICANT REDUNDANCY. Dataset Samples Features Classes Attribute Paths Avg. Length Unique Cond.\n\n--- Segment 10 ---\nDataset Samples Features Classes Attribute Paths Avg. Length Unique Cond. Size (MB) Redundancy ( ) Adult [36] 48842 14 2 medium 206152 16.91 29436 723.40 99.94 CreditApproval [37] 690 15 2 small 6246 7.39 1934 1.44 99.61 DryBean [38] 13611 16 7 multi-class 52663 12.11 41752 262.12 99.97 Letter [39] 20000 16 26 multi-class 190377 15.59 421 9.55 96.30 Wine [40] 4898 11 11 multi-class 107772 13.53 5467 70.24 99.75 comparisons performed in parallel on a column basis. This high degree of parallelism makes CAM particularly well-suited for applications requiring rapid lookups, such as network routing, database indexing, and cache systems. Ternary content-addressable memory (TCAM) [35] extends the functionality of binary CAM by introducing a third state, don t care, denoted as X. Bits set to the X state are treated as matches during comparison, enabling greater flexibility in search operations. Since each root-to-leaf path in a tree- based model represents a sequence of conditions an input must satisfy to reach the corresponding prediction, TCAM can efficiently traverse these paths, making it a compelling solution for tree-based model acceleration. Prior work [22] proposed mapping each path to a TCAM row, with each column representing a unique condition. Unencountered conditions within a path are assigned the X state as they do not affect the traversal process. When an input instance is received, it is first encoded into a binary sequence that aligns with the condition order stored in TCAM. This encoded sequence is then fed into TCAM for in-memory search. The middle part of Fig. 1 illustrates how TCAM serves as a decision tree accelerator. The decision tree at the left part of Fig. 1 is mapped to the TCAM, where four unique conditions are assigned to separate columns, and each path is represented as a row. Conditions are encoded as 0, 1, and X, corresponding to False, True, and don t care, respectively. Once an input is received, the features are encoded into a binary sequence representing the condition check results, and each bit is then delivered to the corresponding column to perform in-memory search.\n\n--- Segment 11 ---\nConditions are encoded as 0, 1, and X, corresponding to False, True, and don t care, respectively. Once an input is received, the features are encoded into a binary sequence representing the condition check results, and each bit is then delivered to the corresponding column to perform in-memory search. Since all bits in the second row are matched, Path2 is the traversal path for the input instance. Analog content-addressable memory (ACAM) [41] follows a similar structural design to TCAM, but differs in its data representation. Instead of discrete states (0, 1, X), each ACAM cell can store a range of analog values. A cell is considered matched if the given input value falls within the stored range, meaning that a min-to-max range in ACAM functions analogously to the X state in TCAM. Previous research [23] has demonstrated the potential of ACAM for accelerating Random Forest. Similar to [22], each root-to-leaf path is mapped to an ACAM row. However, unlike TCAM, where each column represents a feature-threshold condition, ACAM columns directly correspond to features. When an input instance is received, a digital-to-analog converter first transforms the features into analog voltages. ACAM then performs in-memory search operations to retrieve the final result. The right part of Fig. 1 provides an overview of ACAM serving as the accelerator for the decision tree in the left part of Fig. 1. In contrast to TCAM, ACAM only requires three columns to represent the features, and each of the cells in ACAM stores an analog range instead. However, despite its space efficiency, ACAM requires addi- tional peripheral circuits, and its larger cell size does not nec- essarily offer better area efficiency than TCAM. Furthermore, analog computing is susceptible to various non-idealities, such as stuck-at faults, IR drop, thermal noise, shot noise, and random telegraph noise [42]. While mitigation techniques exist [43], they often introduce additional memory overhead and offer limited effectiveness. Due to these reliability chal- lenges, ACAM is not yet viable for real-world implementation. Therefore, this work opts for TCAM as the accelerator. Still, RETENTION remains compatible with ACAM after adapting the condition-based operations to operate on features only.\n\n--- Segment 12 ---\nTherefore, this work opts for TCAM as the accelerator. Still, RETENTION remains compatible with ACAM after adapting the condition-based operations to operate on features only. C. Observation To accelerate tree-based model inference using TCAM, each root-to-leaf path is mapped to a TCAM row, with each column representing a unique condition within the ensemble. Since a single path encounters only a minimal fraction of the condi- tions within the ensemble, the majority of TCAM cells store the X state for format alignment, leading to significant redun- dancy. In fact, mapping a tree-based model to TCAM without optimizations requires at least paths unique conditions bits, far exceeding the memory capacity available in resource- constrained environments. As shown in Fig. 1, mapping a decision tree to a CAM with perfect capacity leads to over one-third of cells storing the X state. Moreover, due to the extremely high write latency of nvTCAM, real-time writing severely degrades performance, making it critical to fit the entire model into nvTCAM. Table I presents an experiment that highlights the substantial memory requirement and the overwhelming redundancy caused by the widespread use of the X states in TCAM. In this experiment, Random Forest models with 100 decision trees require up to 700MB of memory when mapped with naive unified mapping (refer to Section III-D1 for further explanation), and all exhibited redundancy of more than 96 . These results highlight that directly deploying nvTCAM as a tree-based model accelerator is both infeasible and inefficient in resource-constrained environments. Previous work DT2CAM [22] introduced a framework for mapping decision trees to TCAM. However, instead of addressing memory redundancy, it only proposed an energy- saving precharge mechanism that disables unmatched rows to conserve power. Pedretti et al. [23] proposed mapping Random RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 5 Model Construction Tree Mapping Scheme with Tailored Data Placement Strategy Energy-Efficient Mapping (ODR) Space-Efficient Mapping (SPC) Tolerance Dataset OR Model Model Training and Purity Threshold Pruning Bagging-Based Models Boosting-Based Models Input Fig. 2. Overview of RETENTION.\n\n--- Segment 13 ---\n2. Overview of RETENTION. Input Dataset Ensemble Training with Purity Saved Purity Threshold Pruning Final Model Tolerance Minimum Threshold Search Assume current purity threshold is 75 51 68 Class1 Class2 70 Class3 78 Class2 Class3 Fig. 3. Workflow of model construction with purity threshold pruning. Forests to ACAM, incorporating techniques such as limiting tree depth and reordering features while eliminating empty rows to mitigate memory redundancy. However, pre-pruning algorithms can result in severe accuracy degradation when implementing on bagging-based models (refer to Section III-B for further explanation), while alternative pruning strategies could achieve better accuracy within the same memory budget. Furthermore, although feature reordering groups redundant cells and empty-row elimination reduces capacity requirement, the associated computational overhead for result retrieval is relatively high. In fact, comparable computational overhead could yield even greater memory savings. A follow-up study by Pedretti et al. [24] proposed assigning individual trees to separate ACAMs, improving space efficiency. While this approach effectively reduces redundancy across different trees, redundancy within each separate tree remains, leaving room for further optimization. Addressing this residual redundancy could lead to more reductions in memory consumption while maintaining computational efficiency. D. Motivation This work is driven by the challenges identified in the observations and previous work;. While nvTCAM holds great promise for accelerating tree-based model inference in resource-constrained environments, the excessive CAM capacity requirement remains a critical challenge that must be addressed before practical implementation. Therefore, this paper proposes RETENTION, an end-to-end framework that significantly reduces CAM capacity requirement, enabling resource-efficient tree-based ensemble model acceleration. III. RETENTION A. Overview nvTCAM enables high-speed, parallel inference for tree- based models, making it a promising solution for acceleration. However, substantial memory consumption and redundancy re- main key barriers to real-world deployment, limiting its feasi- bility in resource-constrained environments. In this section, we introduce RETENTION, an end-to-end framework serving as a crucial component to enable practical and resource-efficient acceleration of tree-based model inference with nvTCAM.\n\n--- Segment 14 ---\nHowever, substantial memory consumption and redundancy re- main key barriers to real-world deployment, limiting its feasi- bility in resource-constrained environments. In this section, we introduce RETENTION, an end-to-end framework serving as a crucial component to enable practical and resource-efficient acceleration of tree-based model inference with nvTCAM. It addresses the challenges by (1) minimizing model complexity through purity threshold pruning and (2) enhancing memory utilization by introducing an optimized tree mapping scheme with two data placement strategies, namely occurrence-based double reordering (ODR) and similarity-based path clustering (SPC). Fig. 2 presents an overview of RETENTION, detailing its key components and their interactions. The following sections elaborate on the philosophy and implementation. B. Purity Threshold Pruning As the required memory capacity is correlated to both paths and unique conditions, pruning models can effec- tively reduce CAM capacity requirement. Existing pruning techniques for reducing tree-based ensemble model complexity mainly focus on pre-pruning (since post-pruning algorithms are rarely implemented on ensemble models, as explained in Section II-A), and the algorithms fall into two main categories: (1) structural restriction, which constrains the tree s shape using parameters such as maximum depth, and (2) split-based pruning, which prevents further splits when subsequent splits are deemed worthless (e.g., minimum impurity decrease). While both methods reduce model complexity, neither takes into account the class distribution of instances within nodes, potentially leading to significant accuracy degradation. For example, in complex datasets, instances from different classes may remain entangled even at the maximum depth, resulting in serious accuracy loss when forced truncation occurs. Sim- ilarly, if an early split does not effectively separate instances, later splits might still provide meaningful differentiation, yet split-based pruning prematurely terminates such opportunities. Although these pruning techniques have minimal impact on boosting-based models, where trees iteratively correct previ- ous errors, bagging-based models can suffer severe accuracy degradation due to their independently trained trees, which lack an error correction mechanism. To avoid potential catastrophic accuracy degradation, this work introduces purity threshold pruning, a novel pruning algorithm designed for bagging-based models. The workflow is depicted in the left part of Fig. 3.\n\n--- Segment 15 ---\nThe workflow is depicted in the left part of Fig. 3. Unlike conventional training, which only takes a dataset as input, our approach incorporates a user-specified tolerance for OOB accuracy loss. During training, each node records its majority class and the corresponding purity (i.e., proportion). Once the ensemble is constructed, rather than concluding with OOB estimation, we 6 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS TABLE II COMPARISON OF NAIVE UNIFIED MAPPING AND NAIVE INDEPENDENT MAPPING. Attribute Naive Unified Mapping Naive Independent Mapping Mapping Unit entire ensemble single tree Width per Unit l unique conditions S m TCAMs l unique conditions S m TCAMs Height per Unit l paths S m TCAMs l paths S m TCAMs Total TCAM Required l unique conditions S m l paths S m P l unique conditions S m l paths S m Key Strength lower input pre-processing cost higher space efficiency Category energy-efficient mapping space-efficient mapping iteratively determine the minimum purity threshold that main- tains accuracy within the specified tolerance. Nodes exceeding this threshold are converted into leaf nodes and assigned their respective majority class, as illustrated in the right part of Fig. 3. The core principle of purity threshold pruning is to reduce model complexity while preserving accuracy at the ensemble level. Since individual trees are trained on different subsets with distinct splits, a minority class overlooked in one tree can still be classified correctly in others. For highly imbalanced datasets, applying class weighting helps prevent the model from favoring majority classes. Additionally, by explicitly considering OOB accuracy during pruning, the method ensures that performance degradation remains controlled. Unlike stan- dard pruning techniques that may create leaf nodes with evenly distributed classes, potentially leading to misclassifications, purity threshold pruning ensures that each leaf node main- tains sufficient class purity to guarantee the desired level of accuracy. Moreover, when early splits already satisfy the target accuracy, further splits are omitted, significantly reducing both paths and unique conditions compared to conventional pruning methods. Different from most post-pruning algorithms that operate on individual decision trees in isolation, purity threshold pruning is specifically designed to account for the collective behavior of ensemble models during inference.\n\n--- Segment 16 ---\nMoreover, when early splits already satisfy the target accuracy, further splits are omitted, significantly reducing both paths and unique conditions compared to conventional pruning methods. Different from most post-pruning algorithms that operate on individual decision trees in isolation, purity threshold pruning is specifically designed to account for the collective behavior of ensemble models during inference. This global perspective enables purity threshold pruning to pre- serve diversity and effectively reduce model complexity while explicitly constraining accuracy degradation within a user- defined tolerance, a capability not addressed by conventional single-tree pruning methods. Furthermore, recent pruning techniques focus on removing entire trees from models if they exhibit substantial redundancy or structural resemblance [44], [45]. Since purity threshold pruning operates at the node level and assumes trees are diversely grown, it is fully compatible with such whole-tree pruning methods. This compatibility allows further optimiza- tion in space efficiency without sacrificing accuracy, making purity threshold pruning a highly adaptable approach for resource-efficient tree-based model. However, the algorithm is not compatible with boosting-based models, as performing pruning after model construction destroys the error correc- tion mechanism, leading to catastrophic accuracy degrada- tion. Nonetheless, the pre-pruning algorithms can already sufficiently reduce boosting-based model complexity without compromising accuracy. C. Input pre-processing during Inference To accelerate tree-based model inference with CAM, raw input data must be transformed into the specific query format for individual search operations. This transformation involves two key steps: (1) feature encoding and (2) query packing. For feature encoding, the thresholds of each feature must be sorted before inference. When raw input data for a feature is received, binary search can be employed to accelerate condition checks. This step can also be performed on the sensor side, as modern smart sensors are capable of handling such lightweight tasks [46]. Additionally, since each feature is typically associated with a dedicated sensor, pre-processing at the sensor level is feasible and efficient. Once features are encoded into binary sequences, the query packing step organizes condition check results into the query format, ensuring alignment with the order of TCAM columns. These queries are then transmitted to the corresponding TCAM via a network-on-chip (NoC) [47], following the same implementation as in [24]. The input pre- processing for ACAM is almost the same, as quantization can be interpreted as a sequence of condition checks.\n\n--- Segment 17 ---\nThese queries are then transmitted to the corresponding TCAM via a network-on-chip (NoC) [47], following the same implementation as in [24]. The input pre- processing for ACAM is almost the same, as quantization can be interpreted as a sequence of condition checks. However, since ACAM performs search operations using analog values, additional digital-to-analog conversions are required after re- ceiving the query. While tree traversal is essentially a sequence of condition checks, encoding input features into sequences and performing in-memory search may initially seem redundant. However, due to the shared conditions across the ensemble, the actual num- ber of unique conditions is significantly lower than expected. Moreover, binary search can further reduce the number of condition checks. For instance, in a Random Forest model with 100 trees and an average path length of 17.38, traversing the forest requires about 1738 condition checks. However, experimental results reveal that as there are 5446 unique conditions within the model, applying binary search can re- duce the number of condition checks to approximately 120 (14 features log2 5446 14 ), which is a significant improvement. Additionally, feature encoding is highly efficient compared to conventional CPU-based tree traversal, because all data accesses are deterministic and easily cached. Given that the computational overhead for feature encoding is constant when using TCAM as an accelerator, and can also be done on the sensor side, this work focuses on the computational overhead of query packing, which is primarily determined by the selected data placement strategy. RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 7 Naive Unified Mapping Condition Reordering Path Reordering Occurrence-Based Double Reordering (Energy-Efficient Mapping) Similarity-Based Path Clustering (Space-Efficient Mapping) Requires 3 TCAMs Requires 6 TCAMs Requires 9 TCAMs Bold: TCAMs. Gray White: Encountered Unencountered Conditions. Colorful: Similar Encountered Conditions. Fig. 4. Visualization of the proposed data placement strategies.\n\n--- Segment 18 ---\n4. Visualization of the proposed data placement strategies. D. Tree Mapping Scheme 1) Naive Tree Mapping Approaches: To utilize TCAM for acceleration, each root-to-leaf path within the model needs to be mapped to a TCAM row, with every column of the TCAM corresponding to a unique condition within the paths being searched. There are two naive approaches for mapping: (1) naive unified mapping and (2) naive independent mapping, with their differences summarized in Table II, assuming a TCAM size of S S. In naive unified mapping, paths from the entire ensemble are mapped together, so that each row entry corresponds to the same set of conditions. This structure allows for efficient input pre-processing since TCAMs in the same column receive identical input sequences. However, most nodes in the ensemble are not encountered by any single path, resulting in a vast number of cells storing the X state. In fact, mapping 100 trees together can lead to over 96 re- dundancy, as shown in Table I. Conversely, naive independent mapping maps each tree separately, ensuring that conditions unique to a tree do not introduce unnecessary columns. This approach significantly reduces storage redundancy, as only the conditions relevant to a specific tree are stored, making it far more space-efficient than naive unified mapping. However, this independence disrupts input pre-processing because each TCAM now requires a distinct input sequence, increasing pre- processing overhead for query packing. Although naive unified mapping generally demands more TCAMs than naive independent mapping, the energy-saving precharge mechanism proposed in DT2CAM [22] can help mitigate energy waste by deactivating unmatched rows early. Moreover, input pre-processing dominates energy consumption in inference acceleration, as CPU computations are signif- icantly more power-intensive than in-memory search oper- ations. Given that many nodes are shared among multiple trees, naive unified mapping reduces input pre-processing over- head, making it more energy-efficient than naive independent mapping. Therefore, based on the above observations, we categorize the optimization criteria of data placement strategies into two types: energy-efficient mapping and space-efficient mapping. The key difference between these two types is whether to minimize memory usage at the cost of increased computational overhead. 2) Existing Data Placement Strategy Analysis: DT2CAM primarily focused on mapping a single decision tree to TCAM without addressing redundancy or ensemble models.\n\n--- Segment 19 ---\nThe key difference between these two types is whether to minimize memory usage at the cost of increased computational overhead. 2) Existing Data Placement Strategy Analysis: DT2CAM primarily focused on mapping a single decision tree to TCAM without addressing redundancy or ensemble models. When directly applied to ensembles, DT2CAM acts as either naive unified mapping or naive independent mapping, depending on the chosen mapping unit. Pedretti et al. [23] introduced a data placement strategy, referred to as the feature reordering with row elimination (FR) algorithm, for ACAM. This method selects the whole ensemble as a mapping unit, and proposes (1) reordering features based on frequency to cluster redundant cells, and (2) eliminating entire path segments when they become redundant. However, due to row elimination, paths were no longer stored in fixed rows, making the result retrieval process substantially more complex and CPU-dependent. As a result, the additional computational overhead is proportional to ACAMs, similar to that of naive independent mapping. Thus, we classify FR as a space-efficient mapping approach, though its space efficiency may still fall short of naive independent mapping. A follow-up study by Pedretti et al. [24] proposed fitting each tree into a separate ACAM without mitigating redundancy, resembling naive independent mapping. Conse- quently, we also categorize this approach under space-efficient mapping. After a thorough analysis of the tradeoffs among existing data placement strategies, we find that prior approaches fail to effectively alleviate memory redundancy, offering efficiency comparable to naive methods. To bridge this gap, we introduce two novel data placement strategies: one optimized for energy- efficient mapping and the other for space-efficient mapping. The following sections provide a detailed explanation of these strategies, with a visualization presented in Fig. 4. 3) Occurrence-Based Double Reordering (ODR): To al- leviate memory redundancy in an energy-efficient manner, each path should be stored in a fixed row to avoid additional computational overhead during result retrieval. Moreover, the input sequences for TCAMs in the same column must remain consistent to minimize input pre-processing overhead for query packing. Given these constraints, we propose ODR to enhance memory utilization for resource-constrained environments that prioritize energy efficiency.\n\n--- Segment 20 ---\nMoreover, the input sequences for TCAMs in the same column must remain consistent to minimize input pre-processing overhead for query packing. Given these constraints, we propose ODR to enhance memory utilization for resource-constrained environments that prioritize energy efficiency. As outlined in Algorithm 1, ODR first sorts conditions by occurrence frequency in descending 8 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS Algorithm 1 Occurrence-Based Double Reordering (ODR) Require: pool root to leaf paths Require: conditions pool.union() Ensure: condition order, path order 1: paths {} 2: sorted conditions conditions.sort() descending freq. 3: for c in sorted conditions.reverse() do 4: for path in pool do 5: if path.contains(c) then 6: paths.append(path) 7: pool.remove(path) 8: return sorted conditions, paths order, similar to FR. Instead of eliminating partial sequences when an entire TCAM row is irrelevant to a given path, which introduces sequential computational overhead as seen in FR, ODR optimizes path ordering based on condition usage. Paths that contain rare conditions are placed at the top, concentrating most of the X-state cells in the bottom-right TCAMs. TCAMs that are entirely filled with X-state cells can then be removed, reducing redundancy without distributing a single path across multiple rows, thereby avoiding additional computational overhead. Despite its effectiveness, ODR still leaves considerable redundancy, as shown in Fig. 4. For example, in the top- right TCAM after applying ODR, only a single meaningful cell remains, yet the entire TCAM is required for correct functionality. To further improve memory efficiency, the next section introduces a more aggressive data placement strategy that reduces CAM capacity requirement at the cost of higher energy consumption. However, its computational overhead is comparable to that of naive unified mapping, and is, in fact, even lower in practice. 4) Similarity-Based Path Clustering (SPC): Since memory redundancy arises from unencountered conditions, clustering paths with high similarity (i.e., those that share many con- ditions) can substantially improve space efficiency.\n\n--- Segment 21 ---\nHowever, its computational overhead is comparable to that of naive unified mapping, and is, in fact, even lower in practice. 4) Similarity-Based Path Clustering (SPC): Since memory redundancy arises from unencountered conditions, clustering paths with high similarity (i.e., those that share many con- ditions) can substantially improve space efficiency. Although naive independent mapping can be viewed as a basic form of clustering, paths within a tree exhibit limited similarity, and some TCAMs can still remain nearly redundant especially in cases where paths S 1 and the TCAM size is S S, leading to inefficient capacity utilization. Finding the optimal clusters to minimize TCAM consumption is an NP-hard prob- lem. In this section, we propose SPC, a heuristic algorithm that greedily maximizes similarity among paths within TCAM, and thus minimizes CAM capacity requirement. As presented in Algorithm 2, SPC starts with a pool containing all root- to-leaf paths in the model. While pool is not empty, SPC evaluates the similarity between current cluster and each of the paths within pool, and designates the path that can fit in current cluster based on (1) the highest number of shared con- ditions, and (2) the smallest resulting set of unique conditions after being integrated into current cluster, as candidate. Since the objective is to accommodate all paths using the smallest number of clusters, and the capacity of each cluster depends on both paths and unique conditions because we limit a Algorithm 2 Similarity-Based Path Clustering (SPC) Require: S TCAM size Require: pool root to leaf paths Ensure: clusters 1: clusters {}, current cluster {} 2: current num 0 3: while pool is not empty do 4: similarity calc similarity(pool, current cluster) 5: candidate find best candidate(similarity, pool) 6: if candidate is NULL or current num S then 7: clusters.append(current cluster) 8: current cluster {} 9: current num 0 10: else 11: current cluster.append(candidate) 12: current num current num 1 13: pool.remove(candidate) 14: if current cluster is not empty then 15: clusters.append(current cluster) 16: return clusters cluster to a single TCAM, the key intuition behind SPC is to minimize unique conditions at every step.\n\n--- Segment 22 ---\nWhile pool is not empty, SPC evaluates the similarity between current cluster and each of the paths within pool, and designates the path that can fit in current cluster based on (1) the highest number of shared con- ditions, and (2) the smallest resulting set of unique conditions after being integrated into current cluster, as candidate. Since the objective is to accommodate all paths using the smallest number of clusters, and the capacity of each cluster depends on both paths and unique conditions because we limit a Algorithm 2 Similarity-Based Path Clustering (SPC) Require: S TCAM size Require: pool root to leaf paths Ensure: clusters 1: clusters {}, current cluster {} 2: current num 0 3: while pool is not empty do 4: similarity calc similarity(pool, current cluster) 5: candidate find best candidate(similarity, pool) 6: if candidate is NULL or current num S then 7: clusters.append(current cluster) 8: current cluster {} 9: current num 0 10: else 11: current cluster.append(candidate) 12: current num current num 1 13: pool.remove(candidate) 14: if current cluster is not empty then 15: clusters.append(current cluster) 16: return clusters cluster to a single TCAM, the key intuition behind SPC is to minimize unique conditions at every step. This approach allows each TCAM to hold as many paths as possible, resulting in an effective heuristic. Once current cluster reaches capacity or none of the paths is suitable, current cluster is added to clusters representing the completed ones, and a new empty cluster is initiated. Otherwise, SPC greedily adds the candidate to current cluster and removes it from pool, and this process is repeated until all paths have been assigned to clusters. SPC efficiently exploits similarities across paths, diminish- ing unnecessary X states. By filling TCAMs as fully as pos- sible, it maximizes utilization and minimizes wasted capacity. Since each cluster is constrained to contain at most S unique conditions and S paths, there is no dependency between the match results of different TCAMs, eliminating any compu- tational overhead associated with intersecting partial match results. Therefore, the only computational overhead lies in the query packing for every TCAM, which is the same as naive independent mapping.\n\n--- Segment 23 ---\nSince each cluster is constrained to contain at most S unique conditions and S paths, there is no dependency between the match results of different TCAMs, eliminating any compu- tational overhead associated with intersecting partial match results. Therefore, the only computational overhead lies in the query packing for every TCAM, which is the same as naive independent mapping. However, since SPC requires far fewer TCAMs than naive independent mapping, it emerges as a significantly more resource-efficient approach in terms of both energy and memory consumption. IV. EVALUATION A. Experiment Setup 1) Objective and Metric: Since previous works already demonstrate that CAM outperform conventional accelerators, in this work we focus on the CAM-oriented optimization, omitting performance comparison with CPU, GPU, FPGA. To evaluate the effectiveness of RETENTION, we compare the number of required TCAMs against those in existing works. Therefore, we select the reduced number of TCAMs as our evaluation metric, and the higher value represents better performance. RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 9 50 55 60 65 70 75 80 85 90 95 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Tolerance 1 Tolerance 3 Tolerance 5 Space-Efficient Mapping (SPC) 50 55 60 65 70 75 80 85 90 95 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Tolerance 1 Tolerance 3 Tolerance 5 Energy-Efficient Mapping (ODR) Fig. 5. Overall performance of RETENTION. 2) Model Settings: Two ensemble models are selected for case study: Random Forest, representing bagging-based meth- ods, and XGBoost, representing boosting-based methods. Both models are trained on five datasets from the widely used UCI Machine Learning Repository [48], as listed in Table I. Each dataset is divided into training and testing sets in a 7:3 ratio to examine the impact of purity threshold pruning on accuracy. For Random Forest, we utilized the open-source library Ranger [49] for forest construction, setting the number of trees to 100. Purity threshold pruning is applied on Random Forest to reduce model complexity, with tolerance set to {1 , 2 , 3 , 4 , 5 }.\n\n--- Segment 24 ---\nFor Random Forest, we utilized the open-source library Ranger [49] for forest construction, setting the number of trees to 100. Purity threshold pruning is applied on Random Forest to reduce model complexity, with tolerance set to {1 , 2 , 3 , 4 , 5 }. On the other hand, we used the official XGBoost library [9] to construct XGBoost with default parameters, including max depth 6 and num trees 100. 3) Data Placement Strategies and Baselines: To achieve resource-efficient acceleration, we implemented ODR and SPC to further enhance efficiency. As analyzed in Section III-D2, the data placement strategies presented in prior works are mostly equivalent to naive unified mapping and naive inde- pendent mapping, which fall under the categories of energy- efficient mapping and space-efficient mapping, respectively. While FR incorporates additional optimizations, the associated computational overhead is comparable to naive independent mapping, and is therefore categorized into space-efficient map- ping. Since the algorithm is originally designed for ACAM, we make a minor modification, reordering conditions in- stead of features, to adapt it for TCAM deployment. In this work, energy-efficient mapping and space-efficient mapping are evaluated separately, with naive unified mapping and naive independent mapping as baselines. 4) Simulation: The mapping simulation is conducted by storing root-to-leaf paths into tabular format (i.e., each row is a root-to-leaf path, and every column corresponds to a unique condition), and directly calculate the required number of TCAMs, as no open-source simulator currently supports such functionality. All models are mapped to TCAMs with size 64 64 if not specified. The subsequent sections present detailed experimental re- sults: Section IV-B analyzes RETENTION s overall perfor- mance. Section IV-C investigates the effects of purity thresh- old pruning on accuracy and TCAM capacity requirement. Section IV-D evaluates the contributions of ODR and SPC individually. Section IV-E explores the impact of the number of trees. Section IV-F examines the influence of TCAM size. and Section IV-G discusses computational overhead, energy consumption, and throughput. B. Experiment 1: Overall Performance of RETENTION To evaluate the overall performance of RETENTION, we compare Random Forest models with tolerance set to {1 , 3 , 5 } against baseline methods. As shown in Fig.\n\n--- Segment 25 ---\nExperiment 1: Overall Performance of RETENTION To evaluate the overall performance of RETENTION, we compare Random Forest models with tolerance set to {1 , 3 , 5 } against baseline methods. As shown in Fig. 5, RE- TENTION exhibits substantial reductions, achieving 66.25 to 99.96 fewer TCAMs required (equivalent to 2.96 to 2420.81 better space efficiency) in energy-efficient map- ping, and 70.83 to 99.70 reduction (3.43 to 334.48 improvement) in space-efficient mapping. Specifically, setting tolerance to a moderate value of 3 yields 4.35 to 207.12 better space efficiency. These results demonstrate that RE- TENTION effectively minimizes CAM capacity requirement, offering a promising solution for resource-efficient tree-based model acceleration with CAM. C. Experiment 2: Purity Threshold Pruning Noticing the extraordinary performance of RETENTION, we break down the framework and evaluate each component separately. This section analyzes the effectiveness of purity threshold pruning by mapping Random Forest models with naive unified mapping. By setting tolerance to {1 , 3 , 5 }, purity threshold pruning alone achieves considerable im- provements, reducing CAM capacity requirement by 21.04 to 99.93 (1.27 to 1357.12 improvement) compared to unpruned models. The relatively lower improvement on the Letter and Wine datasets stems from the increased number of classes, which makes it more challenging for nodes to achieve high purity. Pruning such nodes may result in sig- nificant accuracy degradation, which the tolerance mecha- nism prevents. Consequently, only a few higher-purity nodes can be pruned, thereby limiting the impact of the purity threshold pruning. We further scale tolerance up to {10 , 15 , 20 } to investigate the correlation between tolerance and different datasets. As shown in the left part of Fig. 6, the performance of purity threshold pruning converges at different levels of tolerance, depending on task complexity.\n\n--- Segment 26 ---\nAs shown in the left part of Fig. 6, the performance of purity threshold pruning converges at different levels of tolerance, depending on task complexity. Though all can achieve over 90 reduction, simple tasks easily converge with low tolerance ({1 , 3 , 5 }, refer to 10 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Number of TCAMs 1 3 5 10 15 20 Tolerance vs Accuracy Loss 0 1 2 3 4 5 6 1 2 3 4 5 Testing Accuracy Loss Tolerance Adult CreditApproval DryBean Letter Wine IDEAL Purity Threshold Pruning Fig. 6. Effectiveness of purity threshold pruning. 0 10 20 30 40 50 60 70 80 90 100 XGB RF XGB RF XGB RF XGB RF XGB RF Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Energy-Efficient Mapping (ODR) -100 -80 -60 -40 -20 0 20 40 60 80 100 XGB RF XGB RF XGB RF XGB RF XGB RF Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs FR SPC Space-Efficient Mapping (SPC) Fig. 7. Comparison of different data placement strategies (RF: Random Forest, XGB: XGBoost). the red columns), while the convergence for complex tasks occurs with relatively higher tolerance ({10 , 15 , 20 }, refer to the blue columns). However, for imbalanced datasets such as Adult, higher tolerance may lead to a model always favoring the majority class. This should be carefully dealt with by leveraging techniques such as class weights, as discussed in Section III-B. The right part of Fig. 6 exhibits that the user-specified tolerance is highly correlated to testing accuracy loss. Since tolerance is the expected accuracy degradation that is sacrificed to achieve lower model complexity, ideally the testing accuracy loss should be lower than or similar to toler- ance. Results show that most of the testing accuracy losses are lower than the predefined tolerance, with only one case slightly higher, suggesting that purity threshold pruning effectively reduces model complexity while ensuring controlled accuracy degradation. Models pruned with higher tolerance also follow this trend, though we omitted the statistics here for clarity.\n\n--- Segment 27 ---\nResults show that most of the testing accuracy losses are lower than the predefined tolerance, with only one case slightly higher, suggesting that purity threshold pruning effectively reduces model complexity while ensuring controlled accuracy degradation. Models pruned with higher tolerance also follow this trend, though we omitted the statistics here for clarity. D. Experiment 3: Tree Mapping Scheme After validating purity threshold pruning, we estimate the effectiveness of the proposed data placement strategies. Fig. 7 presents the results of implementing ODR and SPC on XGBoost and unpruned Random Forest. Since unpruned Random Forest is usually more complex than XGBoost, the improvement of applying ODR and SPC on Random Forest is often more obvious. For energy-efficient mapping, ODR presents more than 50 reduction in all cases. Since unique conditions of the models trained on CreditAp- proval and Letter datasets is an order of magnitude fewer than in other datasets, the improvement observed in the Random Forest trained on these two datasets is relatively limited. For space-efficient mapping, SPC exhibits notable improvement, with over 30 reduction in every case, while FR performs worse than the baseline in half of the cases. The underlying reason is that FR only eliminates completely redundant path segments within a TCAM, whereas significant redundancy is still left behind. Thus, FR beats naive independent mapping only when the redundancy within a single tree is overwhelm- ing. On the other hand, by greedily clustering similar paths together, SPC minimizes redundancy while incurring less runtime computational overhead, consistently outperforming FR and naive independent mapping in all cases. The per- formance of applying SPC to the Random Forest trained on the CreditApproval dataset is nearly optimal (SPC requires 99 TCAMs, while the theoretical minimum is 98 for 6246 paths). However, the exceptionally strong performance of naive inde- pendent mapping on this model limits the potential for further improvement. Experimental results show that implementing the tree mapping scheme alone yields 1.46 to 21.30 better space efficiency, highlighting the effectiveness of both ODR and SPC. E. Experiment 4: Number of Trees Prior experiments demonstrate that RETENTION is ex- tremely effective in minimizing CAM capacity requirement.\n\n--- Segment 28 ---\nExperimental results show that implementing the tree mapping scheme alone yields 1.46 to 21.30 better space efficiency, highlighting the effectiveness of both ODR and SPC. E. Experiment 4: Number of Trees Prior experiments demonstrate that RETENTION is ex- tremely effective in minimizing CAM capacity requirement. RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 11 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Trees 100 Trees 300 Trees 500 Space-Efficient Mapping (SPC) 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs Trees 100 Trees 300 Trees 500 Energy-Efficient Mapping (ODR) Fig. 8. Impact of number of trees on the performance of data placement strategies. 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs 64 64 128 128 256 256 0 10 20 30 40 50 60 70 80 90 100 Adult CreditApproval DryBean Letter Wine Reduced Percentage of TCAMs 64 64 128 128 256 256 Space-Efficient Mapping (SPC) Energy-Efficient Mapping (ODR) Fig. 9. Impact of TCAM sizes on the performance of data placement strategies. In this section, we study the impact of number of trees on RE- TENTION s performance. Since XGBoost can achieve higher accuracy with an increased number of trees and appropriate hyperparameter tuning, XGBoost with num trees set to {100, 300, 500} are evaluated in this experiment. As shown in Fig. 8, the performance of RETENTION tends to improve as num trees increases because the inefficient issue is amplified by num trees. However, since the training phase incorporates randomness, and the selected conditions for splitting nodes can strongly affect RETENTION s performance, the result may not always follow the trend. F. Experiment 5: Different TCAM Sizes In addition to the number of trees, the size of the TCAM is another factor that may affect RETENTION s performance. Fig. 9 illustrates the impact of mapping XGBoost to TCAMs with three different sizes.\n\n--- Segment 29 ---\nFig. 9 illustrates the impact of mapping XGBoost to TCAMs with three different sizes. Results suggest that as TCAM size increases, RETENTION yields gradually diminishing improvements in energy-efficient mapping, while offering no- table gains in space-efficient mapping. Although the designs of both naive unified mapping and ODR are not directly correlated with TCAM size, it becomes increasingly difficult for ODR to eliminate TCAMs as the size increases, since only those entirely filled with X states can be removed. In some edge cases, such as mapping the models trained on the CreditApproval or Letter datasets to 256 256 TCAMs, ODR may offer no improvement because none of the TCAMs are fully redundant even after reordering. Conversely, increasing the TCAM size can sometimes create partially redundant TCAMs with only a few non-X-state cells. In such cases, ODR can effectively mitigate this issue, allowing these TCAMs to be removed and improving space efficiency. For example, mapping the Letter model to 128 128 TCAMs may benefit from this effect. On the other hand, although improvement increases with TCAM size when RETENTION is applied for space-efficient mapping, CAM utilization actually decreases because larger TCAMs require more X states per row, which leads to greater inefficiency. However, since naive indepen- dent mapping assigns each tree to a separate TCAM, larger capacities often fail to be fully exploited, leading to greater performance degradation compared to SPC. Consequently, a smaller TCAM is recommended for RETENTION regardless of the optimization objective, though it must still be large enough to accommodate the longest path in the model. G. Computational Overhead, Energy Consumption, and Throughput Because prior studies omit the cost of input pre-processing, we analyze the computational overhead, energy consumption, and throughput analytically rather than with hardware software simulation. For energy-efficient mapping, both naive unified mapping and ODR share the same input sequence among TCAMs in the same column, and a path is stored in a single row to prevent additional computational overhead for result retrieval. For space-efficient mapping, naive independent map- ping and SPC require a unique input sequence for each TCAM, 12 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS leading to higher computational overhead.\n\n--- Segment 30 ---\nFor energy-efficient mapping, both naive unified mapping and ODR share the same input sequence among TCAMs in the same column, and a path is stored in a single row to prevent additional computational overhead for result retrieval. For space-efficient mapping, naive independent map- ping and SPC require a unique input sequence for each TCAM, 12 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS leading to higher computational overhead. However, as naive independent mapping stores paths to independent rows, and SPC stores paths in a single TCAM, none of them require further processing for result retrieval. Although FR prevents additional input pre-processing by sharing input sequences among TCAM columns, the paths are no longer stored in the same row after the row elimination process, and the associated computational overhead for result retrieval is proportional to TCAMs. Therefore, considering computational overhead, the complexity of both ODR and SPC is comparable to previous works. In terms of energy consumption, as RETENTION does not require any additional on-the-fly computation, the reduction in the number of TCAMs is equivalent to energy savings. Furthermore, the energy-saving precharge mechanism proposed in DT2CAM is compatible with ODR, enabling additional reductions in energy consumption. Considering throughput, since RETENTION does not require any hardware modification, and the computational overhead is comparable to or even slightly lower than previous works as the required number of TCAM is dramatically reduced, RETENTION is supposed to offer a similar or even higher throughput than others. V. CONCLUSION nvTCAM shows great promise to accelerate tree-based model inference effectively and efficiently, yet the CAM capacity requirement is unacceptably high, and most of the cells are storing the X state for format alignment, which is re- dundant. In this work, we introduce RETENTION, an end-to- end framework designed to reduce the CAM capacity require- ment for tree-based model inference. RETENTION introduces purity threshold pruning to minimize model complexity while ensuring controlled accuracy degradation for bagging-based models. A tree mapping scheme with two data placement strategies, occurrence-based double reordering and similarity- based path clustering, is proposed to further alleviate memory redundancy for energy-efficient mapping and space-efficient mapping.\n\n--- Segment 31 ---\nRETENTION introduces purity threshold pruning to minimize model complexity while ensuring controlled accuracy degradation for bagging-based models. A tree mapping scheme with two data placement strategies, occurrence-based double reordering and similarity- based path clustering, is proposed to further alleviate memory redundancy for energy-efficient mapping and space-efficient mapping. Experimental results show that implementing the tree mapping scheme alone achieves 1.46 to 21.30 better space efficiency, while the full RETENTION framework yields 4.35 to 207.12 improvement with less than 3 accuracy loss. These results demonstrate that RETENTION is extremely effective in reducing CAM capacity requirement, offering a resource-efficient solution for tree-based model acceleration. By implementing RETENTION, resource retention becomes feasible, bringing CAM-based acceleration closer to practical- ity in resource-constrained environments. REFERENCES [1] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Inf. Fusion, 81(C):84 90, May 2022. [2] L eo Grinsztajn, Edouard Oyallon, and Ga el Varoquaux. Why do tree- based models still outperform deep learning on typical tabular data? In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. [3] Scott M. Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jor- dan M. Prutkin, Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee. From local explanations to global understanding with explainable ai for trees. Nature Machine Intelligence, 2(1):56 67, 2020. [4] DARPA. Explainable artificial intelligence (xai). research programs explainable-artificial-intelligence, 2017. [5] Yizhen Zheng, Huan Yee Koh, Jiaxin Ju, Anh T. N. Nguyen, Lauren T. May, Geoffrey I. Webb, and Shirui Pan. Large language models for scientific discovery in molecular property prediction. Nature Machine Intelligence, 2025.\n\n--- Segment 32 ---\nLarge language models for scientific discovery in molecular property prediction. Nature Machine Intelligence, 2025. [6] Jonathan Kwaku Afriyie, Kassim Tawiah, Wilhemina Adoma Pels, Sandra Addai-Henne, Harriet Achiaa Dwamena, Emmanuel Odame Owiredu, Samuel Amening Ayeh, and John Eshun. A supervised machine learning algorithm for detecting and predicting fraud in credit card transactions. Decision Analytics Journal, 6:100163, 2023. [7] Rongtao Zhang, Binbin Li, and Bin Jiao. Application of xgboost algorithm in bearing fault diagnosis. IOP Conference Series: Materials Science and Engineering, 490(7):072062, apr 2019. [8] Kaggle. Kaggle machine learning data science survey 2021, 2021. [9] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16, page 785 794, New York, NY, USA, 2016. Association for Computing Machinery. [10] Francesco Daghero, Alessio Burrello, Enrico Macii, Paolo Montuschi, Massimo Poncino, and Daniele Jahier Pagliari. Dynamic decision tree ensembles for energy-efficient inference on iot edge nodes. IEEE Internet of Things Journal, 11(1):742 757, 2024. [11] Zhen Xie, Wenqian Dong, Jiawen Liu, Hang Liu, and Dong Li. Tahoe: tree structure-aware high performance inference engine for decision tree ensemble on gpu. In Proceedings of the Sixteenth European Conference on Computer Systems, EuroSys 21, page 426 440, New York, NY, USA, 2021. Association for Computing Machinery. [12] Adri an Alcolea and Javier Resano. Fpga accelerator for gradient boosting decision trees. Electronics, 10:314, 01 2021. [13] Brian Van Essen, Chris Macaraeg, Maya Gokhale, and Ryan Prenger. Accelerating a random forest classifier: Multi-core, gp-gpu, or fpga?\n\n--- Segment 33 ---\n[13] Brian Van Essen, Chris Macaraeg, Maya Gokhale, and Ryan Prenger. Accelerating a random forest classifier: Multi-core, gp-gpu, or fpga? In 2012 IEEE 20th International Symposium on Field-Programmable Custom Computing Machines, pages 232 239, 2012. [14] Kostas Pagiamtzis and Ali Sheikholeslami. Content-addressable memory (cam) circuits and architectures: a tutorial and survey. IEEE Journal of Solid-State Circuits, 41(3):712 727, 2006. [15] Ali Shafiee, Anirban Nag, Naveen Muralimanohar, Rajeev Balasubra- monian, John Paul Strachan, Miao Hu, R. Stanley Williams, and Vivek Srikumar. Isaac: a convolutional neural network accelerator with in-situ analog arithmetic in crossbars. In Proceedings of the 43rd International Symposium on Computer Architecture, ISCA 16, page 14 26. IEEE Press, 2016. [16] Ping Chi, Shuangchen Li, Cong Xu, Tao Zhang, Jishen Zhao, Yongpan Liu, Yu Wang, and Yuan Xie. Prime: A novel processing-in-memory ar- chitecture for neural network computation in reram-based main memory. In 2016 ACM IEEE 43rd Annual International Symposium on Computer Architecture (ISCA), pages 27 39, 2016. [17] An Chen. A review of emerging non-volatile memory (nvm) technolo- gies and applications. Solid-State Electronics, 125, 07 2016. [18] Le Zheng, Sangho Shin, Scott Lloyd, Maya Gokhale, Kyungmin Kim, and Sung-Mo Kang. Rram-based tcams for pattern search. In 2016 IEEE International Symposium on Circuits and Systems (ISCAS), pages 1382 1385, 2016. [19] Ke-Ji Zhou, Chen Mu, Bo Wen, Xu-Meng Zhang, Guang-Jian Wu, Can Li, Hao Jiang, Xiao-Yong Xue, Shang Tang, Chi-Xiao Chen, and Qi Liu. The trend of emerging non-volatile tcam for parallel search and ai applications. Chip, 1(2):100012, 2022.\n\n--- Segment 34 ---\nThe trend of emerging non-volatile tcam for parallel search and ai applications. Chip, 1(2):100012, 2022. [20] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Ga el Varoquaux. API design for machine learning software: experiences from the scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pages 108 122, 2013. [21] Leo Breiman. Random forests. Machine Learning, 45(1):5 32, 2001. [22] Mariam Rakka, Mohammed E. Fouda, Rouwaida Kanj, and Fadi Kurdahi. Dt2cam: A decision tree to content addressable memory framework. IEEE Transactions on Emerging Topics in Computing, 11(3):805 810, 2023. [23] Giacomo Pedretti, Catherine E. Graves, Sergey Serebryakov, Ruibin Mao, Xia Sheng, Martin Foltin, Can Li, and John Paul Strachan. Tree- based machine learning performed in-memory with memristive analog cam. Nature Communications, 12(1):5806, 2021. [24] Giacomo Pedretti, John Moon, Pedro Bruel, Sergey Serebryakov, Ron Roth, Luca Buonanno, Archit Gajjar, Lei Zhao, Tobias Ziegler, Cong Xu, RETENTION: RESOURCE-EFFICIENT TREE-BASED ENSEMBLE MODEL ACCELERATION WITH CONTENT-ADDRESSABLE MEMORY 13 Martin Foltin, Paolo Faraboschi, Jim Ignowski, and Catherine Graves. X-time: Accelerating large tree ensembles inference for tabular data with analog cams. IEEE Journal on Exploratory Solid-State Computational Devices and Circuits, PP:1 1, 01 2024.\n\n--- Segment 35 ---\nX-time: Accelerating large tree ensembles inference for tabular data with analog cams. IEEE Journal on Exploratory Solid-State Computational Devices and Circuits, PP:1 1, 01 2024. [25] Chieh-Lin Tsai, Chun-Feng Wu, Yuan-Hao Chang, Han-Wen Hu, Yung- Chun Lee, Hsiang-Pang Li, and Tei-Wei Kuo. A digital 3d tcam accelerator for the inference phase of random forest. In 2023 60th ACM IEEE Design Automation Conference (DAC), pages 1 6, 2023. [26] Xunzhao Yin, Franz M uller, Ann Franchesca Laguna, Chao Li, Qingrong Huang, Zhiguo Shi, Maximilian Lederer, Nellie Laleni, Shan Deng, Zijian Zhao, Mohsen Imani, Yiyu Shi, Michael Niemier, Xiaobo Sharon Hu, Cheng Zhuo, Thomas K ampfe, and Kai Ni. Deep random forest with ferroelectric analog content addressable memory. Science Advances, 10(23):eadk8471, 2024. [27] Leo Breiman. Out-of-bag estimation. 1996. [28] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017. [29] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018. [30] Leo Breiman, Jerome Friedman, R. A. Olshen, and Charles J. Stone. Classification and Regression Trees. Chapman and Hall CRC, 1st edition, 1984. [31] Leo Breiman. Bagging predictors. Mach. Learn., 24(2):123 140, August 1996. [32] Robert E. Schapire. A brief introduction to boosting.\n\n--- Segment 36 ---\n[32] Robert E. Schapire. A brief introduction to boosting. In Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI 99, page 1401 1406, San Francisco, CA, USA, 1999. Morgan Kaufmann Publishers Inc. [33] Robert Karam, Ruchir Puri, Swaroop Ghosh, and Swarup Bhunia. Emerging trends in design and applications of memory-based com- puting and content-addressable memories. Proceedings of the IEEE, 103(8):1311 1330, 2015. [34] Xunzhao Yin, Chao Li, Qingrong Huang, Li Zhang, Michael Niemier, Xiaobo Sharon Hu, Cheng Zhuo, and Kai Ni. Fecam: A universal com- pact digital and analog content addressable memory using ferroelectric. IEEE Transactions on Electron Devices, 67(7):2785 2792, 2020. [35] Igor Arsovski, Trevis Chandler, and Ali Sheikholeslami. A ternary content-addressable memory (tcam) based on 4t static storage and including a current-race sensing scheme. IEEE Journal of Solid-State Circuits, 38(1):155 158, 2003. [36] Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. [37] John Ross Quinlan. Credit Approval. UCI Machine Learning Repository, 1987. [38] Dry Bean. UCI Machine Learning Repository, 2020. 24432 C50S4B. [39] David Slate. Letter Recognition. UCI Machine Learning Repository, 1991. [40] Cortez, Paulo, Cerdeira, A., Almeida, F., Matos, T., and Reis, J. Wine quality. UCI Machine Learning Repository, 2009. 24432 C56S3T. [41] Can Li, Catherine E. Graves, Xia Sheng, Darrin Miller, Martin Foltin, Giacomo Pedretti, and John Paul Strachan. Analog content-addressable memories with memristors. Nature Communications, 11(1):1638, 2020.\n\n--- Segment 37 ---\nAnalog content-addressable memories with memristors. Nature Communications, 11(1):1638, 2020. [42] Zhezhi He, Jie Lin, Rickard Ewetz, Jiann-Shiun Yuan, and Deliang Fan. Noise injection adaption: End-to-end reram crossbar non-ideal effect adaption for neural network mapping. In Proceedings of the 56th Annual Design Automation Conference 2019, DAC 19, New York, NY, USA, 2019. Association for Computing Machinery. [43] Yao-Wen Kang, Chun-Feng Wu, Yuan-Hao Chang, Tei-Wei Kuo, and Shu-Yin Ho. On minimizing analog variation errors to resolve the scalability issue of reram-based crossbar accelerators. IEEE Transac- tions on Computer-Aided Design of Integrated Circuits and Systems, 39(11):3856 3867, 2020. [44] Fan Yang, Wei hang Lu, Lin kai Luo, and Tao Li. Margin optimization based pruning for random forest. Neurocomputing, 94:54 63, 2012. [45] Heping Zhang and Minghui Wang. Search for the smallest random forest. Statistics and its interface, 2:381, 01 2009. [46] Deepti Sehrawat and Nasib Singh Gill. Smart sensors: Analysis of different types of iot sensors. In 2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI), pages 523 528, 2019. [47] Tobias Bjerregaard and Shankar Mahadevan. A survey of research and practices of network-on-chip. ACM Comput. Surv., 38(1):1 es, June 2006. [48] Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. The uci machine learning repository. 2024. [49] Marvin Wright and Andreas Ziegler. ranger: A fast implementation of random forests for high dimensional data in c and r. Journal of Statistical Software, 77, 08 2015. Yi-Chun Liao is currently pursuing the bachelor degree with the Department of Computer Science and Information Engineering, National Taiwan Uni- versity, Taipei, Taiwan. His primary research interests include in-memory computing, emerging non-volatile memory, and hardware software co-design.\n\n--- Segment 38 ---\nYi-Chun Liao is currently pursuing the bachelor degree with the Department of Computer Science and Information Engineering, National Taiwan Uni- versity, Taipei, Taiwan. His primary research interests include in-memory computing, emerging non-volatile memory, and hardware software co-design. Chieh-Lin Tsai received his M.S. degree from the Department of Computer Science, National Tsing- Hua University, Hsinchu, Taiwan, in 2017. He is currently working toward the PhD degree in the Department of Computer Science and Information Engineering, National Taiwan University. His pri- mary research interests include non-volatile memory systems and system-level design with in-memory processing. Yuan-Hao Chang (SM 14 F 23) received his Ph.D. in Computer Science from the Department of Com- puter Science and Information Engineering at Na- tional Taiwan University, Taipei, Taiwan. He is cur- rently a Research Fellow at the Institute of Infor- mation Science, Academia Sinica, Taipei, Taiwan, where he served as an Associate Research Fellow between March 2015 and June 2018, and as an As- sistant Research Fellow from August 2011 to March 2015. His research interests include memory storage systems, operating systems, embedded systems, and real-time systems. He is an IEEE Fellow. Cam elia Slimani is an associate professor at the National Polytechnic Institute of Toulouse, France, and a member of the IRIT laboratory. She is part of a research group working on operating systems, distributed systems, and middleware. She received a Ph.D. in computer science from the University of Western Brittany, France, in 2022. She was then a postdoctoral fellow at ENSTA Bretagne, France. Her research interests include optimizing machine learning algorithms for memory-constrained envi- ronments. 14 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS Jalil Boukhobza received the electrical engineer- ing (with Hons.) degree from the Institut Na- tionale d Electricite et d electronique (I.N.E.L.E.C) Boumerdes, Algeria, in 1999, and the MSc and PhD degrees in computer science from the University of Versailles, France, in 2000 and 2004, respectively.\n\n--- Segment 39 ---\n14 IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS Jalil Boukhobza received the electrical engineer- ing (with Hons.) degree from the Institut Na- tionale d Electricite et d electronique (I.N.E.L.E.C) Boumerdes, Algeria, in 1999, and the MSc and PhD degrees in computer science from the University of Versailles, France, in 2000 and 2004, respectively. He is a Professor with the ENSTA, a French State Graduate, PostGraduate and Research Institute part of the Institut Polytechnique de Paris. He was a research fellow with the PRiSM Laboratory (Uni- versity of Versailles) from 2004 to 2006. He was an associate professor with the University Bretagne Occidentale, Brest, France, from 2006 to 2020 and is a member of Lab-STICC. He has also been working with the Technology Research Institute (IRT) bcom since 2013. His main research interests include storage system design, performance evaluation and energy optimization, and operating system design. He works on different application domains such as embedded systems,cloud computing, and database systems. He is an IEEE Senior member. Tei-Wei Kuo Prof. Kuo received his B.S.E. and Ph.D. degrees in Computer Science from National Taiwan University and University of Texas at Austin in 1986 and 1994, respectively. He is CTO of Delta Electronics (2024.02-now), a global leader in power and thermal solutions with annual revenue USD14 billions in 2024. He was Distinguished Professor of Department of Computer Science and Infor- mation Engineering of National Taiwan University (2009.08-2025.07), where he was Interim President (2017.10-2019.01) and Executive Vice President for Academics and Research (2016.08-2019.01). Prof. Kuo was Lee Shau Kee Chair Professor of Information Engineering, Advisor to President (Information Technology), and Founding Dean of College of Engineering, City University of Hong Kong (2019.08-2022.07). His research interest includes embedded systems, non-volatile-memory software designs, neuromorphic computing, and real-time systems. Dr. Kuo is Fellow of ACM, IEEE, and US National Academy of Inventors.\n\n--- Segment 40 ---\nHis research interest includes embedded systems, non-volatile-memory software designs, neuromorphic computing, and real-time systems. Dr. Kuo is Fellow of ACM, IEEE, and US National Academy of Inventors. He is Chair of ACM SIGAPP (since 2023). Prof. Kuo received numerous awards and recognition, including Academic Award of Taiwan Ministry of Education (2023), Humboldt Research Award (Germany; 2021), Outstanding Technical Achievement and Leadership Award (2017) from IEEE TC on Real-Time Systems, and Distinguished Research Award from Taiwan National Science and Technology Council for three times. Prof. Kuo is the founding Editor-in-Chief of ACM Transactions on Cyber-Physical Systems (2015-2021) and a program committee member of many top conferences. He has over 350 technical papers published in international journals and conferences and received many best paper awards, including Best Paper Award from ACM IEEE CODES ISSS 2019, 2022, and 2024, and ACM HotStorage 2021.\n\n