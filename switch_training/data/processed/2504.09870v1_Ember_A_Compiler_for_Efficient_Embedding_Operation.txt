=== ORIGINAL PDF: 2504.09870v1_Ember_A_Compiler_for_Efficient_Embedding_Operation.pdf ===\n\nRaw text length: 89657 characters\nCleaned text length: 88128 characters\nNumber of segments: 68\n\n=== CLEANED TEXT ===\n\nEmber: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures Marco Siracusa Barcelona Supercomputing Center Barcelona, Spain Olivia Hsu Stanford University Stanford, USA Victor Soria-Pardos Barcelona Supercomputing Center Barcelona, Spain Joshua Randall Arm Austin, USA Arnaud Grasset Arm Biot, France Eric Biscondi Arm Biot, France Doug Joseph Arm Austin, USA Randy Allen Barcelona Supercomputing Center Barcelona, Spain Fredrik Kjolstad Stanford University Stanford, USA Miquel MoretÃ³ Planas Barcelona Supercomputing Center Universitat PolitÃ¨cnica de Catalunya Barcelona, Spain AdriÃ  Armejach Barcelona Supercomputing Center Universitat PolitÃ¨cnica de Catalunya Barcelona, Spain Abstract Irregular embedding lookups are a critical bottleneck in recom- mender models, sparse large language models, and graph learn- ing models. In this paper, we first demonstrate that, by offloading these lookups to specialized access units, Decoupled Access-Execute (DAE) processors achieve 2.6 higher performance and 6.4 higher performance watt than GPUs on end-to-end models. Then, we pro- pose the Ember compiler for automatically generating optimized DAE code from PyTorch and TensorFlow. Conversely from other DAE compilers, Ember features multiple intermediate representa- tions specifically designed for different optimization levels. In this way, Ember can implement all optimizations to match the perfor- mance of hand-written code, unlocking the full potential of DAE architectures at scale. 1 Introduction Recommender models, sparse large language models, and graph- learning models represent categorical features such as users, prod- ucts, and words using dense embedding vectors [51]. Due to the vast number of categories, interactions between these features are inherently sparse [26]. For instance, recommender models must lookup the embedding vectors of the products a user has inter- acted with, often loading hundreds of vectors among millions [18]. This leads to irregular memory accesses with low cache reuse [23], posing significant challenges for traditional architectures [15, 69]. In this paper, we first demonstrate that, by offloading embed- ding lookup to specialized access units, Decoupled Access-Execute (DAE) processors achieve 2.6 higher performance and 6.4 higher perf watt than GPUs on end-to-end models. Then, we design the Ember compiler to automatically lower PyTorch [50] and Tensor- Flow [1] embedding operations to high-performance DAE code. The goal of a DAE compiler is to decouple memory accesses from computation and generate optimized code for the access unit and execute unit. However, optimizing already decoupled code is challenging since memory access is separated from computation and (de)serialized through queues. This breaks the control flow and data flow of the input program, hindering global optimizations such as vectorization or code motion across access and execute unit. For this reason, Ember features multiple Intermediate Repre- sentations (IRs), each one designed for a different optimization level. Ember s low-level Decoupled Lookup-Compute (DLC) IR is designed to abstract implementation details of the underlying DAE architecture and facilitate local, target-agnostic optimizations of lookup code and compute code separately. Ember s high-level Struc- tured Lookup-Compute (SLC) IR is designed to also abstract the queue (de)serialization mechanism and facilitate global optimiza- tions within lookup and compute code. By lowering PyTorch and TensorFlow embedding operations through the SLC and DLC IRs, Ember can implement all necessary optimizations to match the per- formance of hand-optimized code, enabling the full DAE potential at no programmability cost. Overall, our contributions are: (1) A characterization of a large class of embedding operations in machine learning models to study the fundamental scal- ing limitations of traditional architectures (Section 2). (2) An evaluation of the potential of DAE architectures for embedding operations (Section 3). (3) The DLC IR, a low-level IR designed to abstract and optimize decoupled lookup and compute code (Section 4). arXiv:2504.09870v1 [cs.AR] 14 Apr 2025 Siracusa et al. dlrm_uni dlrm_gau llm_bigbird kg_biokg kg_wikikg2 gnn_arxiv gnn_mag gnn_products gnn_proteins 0 20 40 60 80 100 Time breakdown Other Scalar elem Vector elem GEMM Embedding 10 100 Operational Intensity (FLOPs Byte) 10 100 Performance (TFLOPs s) 1 BW util 3 BW util 6 BW util 12 BW util 25 BW util 50 BW util HBM bandwidth dlrm_uni dlrm_gau kg_biokg kg_wikikg2 gnn_arxiv gnn_mag gnn_products gnn_proteins Figure 1: Deep-learning recommendation models (dlrm) (Sec- tion 2.2.1), large language models (llm) with sparse attention (Section 2.2.2), knowledge graphs (kg) (Section 2.2.3), and graph neural networks (gnn) (Section 2.2.3) heavily rely on embedding operations that do not perform efficiently even on modern Nvidia H100 GPUs [45]. All experiments use highly- optimized models from the literature (Section 2.2). ptrs vec: 0 2 3 idxs vec: 2 Embedding table: 4 0 0.4 -0.3 0.7 0.1 0.8 -0.1 0.2 0.0 -0.7 0.6 0.3 -0.5 0.0 -0.2 -0.6 0.4 -0.1 0.3 0.4 -0.3 Hi there! Hello! Sentence 1 [0 2] Word Pos: 0 1 2 Emb. idx: 2 4 0 Sentence 2 [2 3] Figure 2: Feature embedding requires scattered memory lookups to fetch embedding vectors from embedding tables. (4) An end-to-end implementation of the Ember compiler to lower PyTorch TensorFlow embedding operations to such abstraction, and then to DAE architectures (Section 5). (5) The SLC IR, a high-level IR designed for global optimiza- tions of DAE embedding operations, and algorithms to lower to from it (Section 6). (6) Optimizations for DAE embedding operations (Section 7). After introducing these contributions, this paper evaluates Ember (Section 8) and discusses its impact in the field (Section 9). 2 Challenges of Embedding Operations Figure 1 shows that several machine learning models heavily rely on embedding operations that achieve low system utilization even on the latest datacenter GPUs. This section examines the charac- teristics of embedding operations, their architectural implications, and the challenges they present for performance scaling. 2.1 Background Modern machine learning models process an increasing amount of categorical features [23] representation variables that can take on a distinct set of values and are not comparable. Example categorical features include product categories, words, and actors in a movie. As these categories have no numerical meaning, they cannot be directly processed with deep neural networks [18]. Hence, machine learning models embed the numerical meaning of these categories into dense embedding vectors [51], which are all stored into large embedding tables. Figure 2 illustrates how categorical features are processed during inference. Incoming categories are tokenized into IDs, which are used to lookup into the embedding table. Embedding lookup is generally fused with downstream computation (e.g., reduction) in an optimized embedding operation. 2.2 Characterization and Implications Table 1 characterizes embedding operations across various machine learning models. For each model, we first analyze its loop hierarchy and compute-per-lookup ratio, which proxy compute requirements. Then, we compute the memory footprint of their embedding tables to understand their storage demand. Finally, we characterize their spatio-temporal locality to gauge the effectiveness of caches and prefetchers for embedding lookups. We characterize temporal lo- cality through the concept of reuse distance the number of other vectors accessed before a vector is accessed again [56]. If a cache can hold ğ‘¥vectors, an access with reuse distance greater than ğ‘¥ will likely result in eviction. For a given input, we first construct a histogram of all reuse distances, which we then integrate and normalize to obtain the Cumulative Distribution Function (ğ¶ğ·ğ¹). Hence, ğ¶ğ·ğ¹(ğ‘¥) proxies the hit probability of that cache. The spa- tial locality, instead, is characterized by the size of the embedding vector. 2.2.1 Deep Learning Recommendation Models (DLRMs). DLRMs recommend content to users by processing a large set of categorical features under strict latency constraints [18, 23]. DLRMs currently account for a large fraction of inference cycles in production data centers, with embedding operations consuming a significant run- time portion [18], as shown in Figure 1. Categorical features in DLRMs are generally multi-hot, mean- ing they can take on multiple values (e.g. movie genres or cast). At inference time, the embedding vectors corresponding to these values need to be looked up and aggregated with, for instance, an element sum. High-performance implementations (1) fuse these two operations to avoid materializing intermediate results and (2) batch together the categories of multiple queries to improve system throughput (column 2 in Table 1), which yields a compute- per-lookup ratio of 1 (column 3 in Table 1). The nn.EmbeddingBag (EB) in PyTorch [50] and Sparse-Lenghts Sum (SLS) function in Caffe2 [24] implement these optimizations. DLRMs invoke EB SLS functions 10s 1000s of times to aggregate all of the categorical features in a batch [23] (clusters of crosses and circles in Figure 1). Each invocation aggregates 10s 100s embed- ding vectors from a single table with millions of entries [23], each vector storing 32-256 elements (column 6 in Table 1). While this necessitates a large memory footprint (column 4 in Table 1), embed- ding vectors are accessed with some degree of reuse [2, 16, 25, 56], and modern caches can filter a significant fraction of embedding accesses [23]. If we assume embedding vectors with 256 FP32 ele- ments, a 1MB cache can fit 1K vectors. Given the CDF of a real-world input such as the Criteo 1TB dataset [31] (column 5 in Table 1), CDF_ftr0(1K) 63 whereas CDF_ftr2(1K) 99 , which is the cache hit probability. Similarly, a 2MB cache would filter 65 99 of the total accesses. Hence, while caches are essential for DLRMs, they bring diminishing returns. Moreover, given the relatively small embedding size, prefetchers would yield limited benefits [26]. Figure 1 tests two high-performance DLRMs [43]. While dlrm_rnd has a random access distribution, dlrm_uni has a CDF similar to criteo_ftr1 in Table 1. In both cases, many accesses cannot be Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures Embedding Operation and Model Class Embedding operation reference implementation Ncompute Nlookups Memory footprint Temporal locality (reuse distance CDF) Spatial locality (vector dim.) EmbeddingBag (EB) or Sparse-Lenghts Sum (SLS) for Deep-Learning Recommendation Models (DLRMs) (Section 2.2.1) segment in batch category in segment lookup vector accumulate vector 1 1 All embedding tables require GBs [18] to TBs [23] 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF criteo ftr2 criteo ftr1 criteo ftr0 32-256 Sparse attention (SpAttn) for Large Language Models (LLMs) (Section 2.2.2) block in Q input block in K input token in K block lookup K vector token in Q block copy K vector 0 K-V tensors are MBs Out tensor is MB-GB 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF 8 emb blk 4 emb blk 2 emb blk 1 emb blk 8x8x512 Sparse Matrix-Matrix Multiplication (SpMM) for Graph Neural Networks (GNNs) (Section 2.2.3) vertex in graph neighbor of vertex lookup vector rescale and accumulate vector 2 1 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 CDF proteins products arxiv mag 8-349 Message Passing (MP) models (Section 2.2.3) vertex in graph lookup vertex vector neighbor of vertex lookup neighbor vector multiply vertex-neighbor vectors scale resulting vector reduce into tmp multiply tmp by vertex vector accumulate resulting vector 5 2 Two embedding tables, each GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 0.4 0.5 0.6 CDF com-Youtube roadNet-CA web-Google wiki-Talk 128 Knowledge Graphs (KGs) (Section 2.2.3) sample in batch lookup vector compute head-rel-tail norm 4 3 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 CDF biokg wikikg2 512 Table 1: Characterization of the embedding operations in Section 2.2. The compute-per-lookup ratio captures code complexity. The memory footprint defines storage requirements for all embedding vectors. The temporal locality proxies cache efficiency through the Cumulative Distribution Function (ğ¶ğ·ğ¹) of the vector reuse distance. The spatial locality proxies prefetcher efficiency through the number of embedding elements in each vector. filtered by the GPU cache, and require hundreds of cycles to reach memory. As discussed in Section 2.3, GPUs cannot issue enough requests to hide such latency, only achieving 28 utilization. 2.2.2 Sparse Attention Mechanisms. Large Language Models (LLMs) embed text into vectors [11]. Transformers such as Google s Big- Bird [69] sparsify the attention mechanism to only compare a se- lected subset of input tokens and improve memory footprint and performance on long input sequences. Although this technique drastically reduces inference latency, it requires an extra step to gather embedding blocks. This gathering step is generally not fused with downstream deep neural networks [69], requiring only embed- ding lookups with no compute (SpAttn in Table 1 column 3). The execution time of this gather operation depends on many factors, and can take up to 20 if gathering, for instance, 8 random blocks per query element, as shown in Figure 1. This operation differs from a normal tf.gather [1] as it repli- cates blocks of embeddings from the key tensor into the query ten- sor. As shown in Table 1, the larger the block, the larger the spatio- temporal locality over keys (higher horizontal CDF part). However, the smaller the block, the lower the compute required. Hence, larger blocks enable better system utilization on flop-oriented machines, Model Input Nodes Edges Layers sizes GNN arxiv [22] 0.2M 1.2M 128 256 256 40 GNN mag [22] 1.9M 21.1M 128 256 349 GNN products [22] 2.4M 61.9M 100 256 256 47 GNN proteins [22] 0.1M 39.6M 8 256 256 112 MP com-Youtube [35] 1.1M 6.0M 128 MP roadNet-CA [35] 2.0M 5.5M 128 MP web-Google [35] 0.9M 5.1M 128 MP wiki-Talk [35] 2.4M 5.0M 128 KG biokg [22] 0.1M 5.1M 512 KG wikikg2 [22] 2.5M 17.1M 512 Table 2: Typical inputs for graph-learning models. and smaller blocks enable lower inference latency on machines with more efficient memory subsystems. 2.2.3 Graph Machine Learning Models. Graph machine learning models such as Graph Neural Networks (GNNs) [22], Message Pass- ing (MP) models [53], and Knowledge Graphs (KGs) [7] embed node or edge features of a graph into vectors. GNNs, for instance, perform inference by chaining layers of deep neural networks and graph convolutions, which gather embeddings from neighbors with the SpMM-like operation shown in Table 1. These SpMM-like em- bedding operations generally have a higher compute-per-lookup Siracusa et al. 1x 20x 40x 60x 80x 100x Request latency L1D latency 0 20 40 60 80 100 of longer requests arxiv mag products proteins arxiv mag products proteins Core in-flight requests 10 12 19 10 arxiv mag products proteins Core request throughput 6 5 5 11 arxiv mag products proteins HBM2 bandwidth [ ] 1.8 2.0 2.4 1.4 (a) (b) (c) (d) Figure 3: Architectural implications of embedding lookups on a traditional CPU core (details in Figure 5b). A large frac- tion of embedding lookups in GNNs models (Section 2.2.3) take orders of magnitude longer than L1D accesses (a). For instance, more than 74 of product s lookups are 10 longer than an L1D access, and 40 more than 100 longer. How- ever, traditional CPU cores have limited memory-level par- allelism, and can only track a few in-flight lookups (b), stalling the CPU pipeline. This results in low memory re- quest throughput (c), and low HBM per core utilization (d). 0.95x 1.00x 1.05x 1.10x 1.15x Performance 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 0.80x 0.85x 0.90x 0.95x 1.00x 1.05x Perf watt arxiv mag products proteins Figure 4: Implications of scaling up the memory-level par- allelism of a traditional CPU core (details in Figure 5b) for GNNs embedding operations (Section 2.2.3). Doubling reorder buffer, load-store queue, and L1D miss-status handling regis- ters (2R.2L.2M) provides limited performance improvements and worse perf watt than off-the-shelf cores (1R.1L.1M). ratio than DLRMs, with MP having the highest. Table 2 shows typ- ical inputs for these models, with their CDFs reported in Table 1. Overall, graph-learning models often have a lower temporal locality (flatter CDF) than DLRMs and LLMs, resulting in more memory accesses. Hence, even gnn_proteins, which has the highest reuse among GNNs, only achieves 32 peak GPU performance. 2.3 Challenges on Performance Scaling Compute cores in traditional architectures such as CPUs and GPUs are optimized to access data in local caches, which are pop- ulated by CPU prefetchers or GPU DMAs [45]. However, these mechanisms are ineffective for irregular workloads such as embed- ding operations [61]. As shown in Figure 3a, this results in a high volume of long memory accesses, where up to 86 of requests are more than 10 longer than L1D accesses, and up to 36 are more than 100 longer. However, compute units such as GPU SMs and LLC HBM CPU0 L1 Core TMU CPU1 L2 OutQ 8 cores, 8 LLC slices, 1 HBM stack N1-like OoO core 2.4 GHz, 224-entries ROB, 2x96-entries LSQ, 2x64B vector units One 64Bx64B matrix unit per core 64 KiB core, 4-way, 2-cycle access, 32 MSHRs, stride IMP prefetcher 512 KiB core, 8-way, 8-cycle access, 64 MSHRs, best offset prefetcher Mostly exclusive, 1 MiB slice, 16 ways, 12-cycle access, 128 MSHRs 2D mesh, 1 cycle routers, 1 cycle links, MOESI-like AMBA 5 CHI 300 GB s HBM2 8KB storage, 256 MSHRs, 1 GHz SoC: Core: Acc: L1D: L2: LLC: NoC: HBM: TMU: (a) (b) Figure 5: A DAE processor. Each traditional core offloads embedding lookup to an access unit like the TMU [61]. CPU cores are not dimensioned for such long memory access laten- cies [44]. For instance, as shown in Figure 3b and 3c, off-the-shelf CPU cores can only track a few of these long memory requests, achieving low throughput (i.e. loads cycle) and performance. Increasing the memory-level parallelism of a CPU core to track more in-flight requests requires scaling up the reorder buffer, load- store queue, and miss-status handling registers [58]. However, these resources scale inefficiently. As shown in Figure 4, doubling them only improves performance by up to 12 , with a 21 power over- head. Besides being inefficient, scaling up these resources intro- duces challenges in timing closure [59], which would require lower- ing the core s frequency and, consequently, compute performance. Scaling out the number of cores is also ineffective. As shown in Figure 3d, we would need 43 72 traditional CPU cores to saturate a single HBM2 stack. Hence, we would need an unfeasible amount of cores to saturate the large memory bandwidth of current archi- tectures [13]. Similarly, as shown in Figure 1, current GPUs utilize 0.08 52 of the HBM bandwidth. To achieve full HBM utilization, GPUs should use 2 12 more warps, which is challenging. Hence, alternative solutions are needed. 3 The Potential of DAE Architectures In this section, we demonstrate that Decoupled Access-Execute (DAE) multicore processors that offload embedding lookup to spe- cialized units outperform GPUs in performance and perf watt. 3.1 Target DAE Architecture Figure 5 shows the target DAE processor for this study, a tradi- tional multicore processor that features one Tensor Marshaling Unit (TMU) [61] per traditional core to offload embedding lookup, and one matrix unit per core (like Arm SME [65]) to offload deep neural network computation. The TMU is an access unit specifically designed to accelerate memory access of all sparse dense tensor algebra expressions. As discussed later in Section 4, by interpreting embedding operations as tensor algebra expressions, we can pro- gram the TMU to lookup embedding vectors for the core, a SIMD CPU in this case. In all experiments, we measured performance with full-system gem5 [40] and power with McPAT [37]. Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures 1.5 1.8 2.0 Power overhead 2x 3x 4x 5x 6x 7x 8x 9x Requests s improvement arxiv mag products proteins arxiv mag products proteins Requests s watt improvement 5.9x 6.7x 8.3x 3.1x arxiv mag products proteins HBM utilization improvement 7.0x 7.6x 6.8x 3.7x (a) (b) (c) Figure 6: Performance and efficiency of a DAE core over a traditional core (details in Section 3.1) on GNN embedding operations (Section 2.2.3). Access units like the TMU can issue and track more memory requests at low power consumption (a), achieving high efficiency (b) and HBM utilization (c). Property Description RM1 RM2 RM3 Segments per batch per core 64 32 16 Embedding entries per table 16K 16K 16K Elements per embedding vector 32 64 128 Tables per core 2 2 2 Lookups per segment 64 128 256 Table 3: Tested DLRM models. 3.2 Architectural Advantage of DAE Designs The main advantage of DAE designs is that, by decoupling embed- ding lookup and computation into two distinct units, these units can be specialized and optimized independently. For instance, as the TMU implements all the logic to traverse and load tensor operands using specialized dataflow hardware, it can issue memory requests with higher throughput and efficiency than the core [61]. Moreover, as the TMU can run at a lower frequency, it can track 8 more outstanding requests, with no timing issues, and low power consumption (less than 2 overhead) [61]. Hence, as shown in Figure 6a and 6b, the TMU achieves 5.7 and 5.6 higher requests s and requests s watt than traditional cores, 5.2 and 6.3 better than doubling core s outstanding requests. In this way, as shown in Figure 6c, the TMU utilizes 4 8 more memory bandwidth than a traditional core, requiring smaller and fewer cores to saturate the processor s HBM bandwidth. From the core s perspective, offloading embedding lookup to the TMU allows to fully utilize the core s resources for compute opera- tions like embedding reductions. In the end, as shown in Figure 7, decoupling embedding lookup from computation on multicore pro- cessors achieves an average 5.8 speedup on the embedding opera- tions in Section 2.2. These speedups are mostly proportional to the temporal locality and compute-per-lookup ratio of the model, all the way to 17 improvements for SpAttn which has no compute and can be fully offloaded to the TMU. 3.3 Impact of DAE in Datacenters In the remainder of this section, we demonstrate that DAE multi- core processors outperform GPUs in end-to-end GNN models. We compare performance against an Nvidia T4 GPU [47], as it offers similar peak memory bandwidth and computational performance com-Youtube roadNet-CA web-Google wiki-Talk arxiv mag products proteins bioKg wikiKg2 RM1 L0 RM1 L1 RM1 L2 RM2 L0 RM2 L1 RM2 L2 RM3 L0 RM3 L1 RM3 L2 1 emb blk 2 emb blk 4 emb blk 8 emb blk 0x 2x 4x 6x 8x 10x 12x Speedup of DAE optimizations 17.0 MP SpMM(GNN) KG SLS(DLRM) SpAttn(LLM) Figure 7: Performance benefit of offloading embedding lookup to a near-core access unit like the TMU (details in Section 3.1). All embedding operations are high-performance multicore implementations from the literature (Section 2). For graph-learning models, the inputs and feature sizes are reported in Table 2. For DLRMs, we tested the three config- urations in Table 3, each one running inputs with low (L0), medium (L1), and high (L2) locality [18]. For LLMs, we tested the original BigBird setting [69] while varying the blocks sizes. Overall, offloading lookup to a specialized access unit improves performance of embedding operations by 5.8 . of our DAE multicore processor, and evaluate performance per watt against an Nvidia H100 [45], a widely used solution in datacenters. We focus on GNNs models as (1) their GPU implementations use high-performance libraries [22], (2) their input datasets have var- ious localities, and (3) they pose unique architectural challenges. As GNNs alternate layers of embedding operations and Deep Neu- ral Networks (DNNs), these two operations need to be computed on the same device, as offloading DNNs to a compute accelerator would incur prohibitive host-to-device transfers. Figure 8a shows the inference latency breakdown of GNN mod- els on the DAE processor and the Nvidia T4 GPU [47]. Although the two systems have the same peak memory bandwidth, the DAE system utilizes 4.6 higher bandwidth, executing embedding oper- ations 1.6 6.3 faster than the T4 GPU. As the two systems have similar peak compute, the DNN layers have similar execution time. Hence, the DAE processor achieves 2.6 higher end-to-end perfor- mance than the T4 GPU. As the DAE processor saturates memory bandwidth with only 8 cores, it consumes less power than the T4 GPU, and achieves 6.4 higher perf watt, as shown in Figure 8b. For the same reasons, Figure 8c demonstrates that the DAE processor also achieves 4 higher perf watt than Nvidia s H100 datacenter GPU [45], which employs a smaller process node than the T4. 4 DAE Abstraction for Embedding Operations So far, we have demonstrated the benefits of decoupling embedding lookup from computation. However, such optimization requires a different programming model. In this section, we formalize the DAE programming model with the Decoupled Lookup-Compute (DLC) IR, a low-level programming abstraction for embedding operations on general DAE designs. Similarly to the LLVM IR[32], the DLC IR provides an abstraction to separate target-agnostic optimizations Siracusa et al. dae gpu dae gpu dae gpu Inference Latency w.r.t. T4 GPU 62 100 41 100 23 100 Better mag proteins arxiv Other Elemental DNN Embedding mag proteins arxiv Perf Watt w.r.t. T4 GPU 4.3x 6.4x 9.4x Better mag proteins arxiv products Perf Watt w.r.t. H100 GPU 2.8x 3.8x 5.2x 4.4x Better (a) (b) (c) Figure 8: Performance and efficiency comparison of DAE pro- cessors w.r.t. GPUs on OGB GNNs models [22] (products did not fit on T4 GPU). For the DAE multicore processor, we mea- sured inference latency with gem5. For the T4 GPU, we used Nvidia Nsys [46], and only included the kernel execution time and no host-to-device transfer. GPU power consump- tion is measured with the torch.cuda.power_draw function whereas the power consumption of the DAE processor is (over)estimated by utilizing the TDP of similar traditional processors [42]. DAE processors outperform GPUs on both inference latency and performance watt. Control queue Data queue Access unit Dataflow lookup code Execute unit Memory Imperative compute code Figure 9: DAE architectural abstraction. from target-specific code generation for DAE designs. As discussed in Section 5, the DLC IR greatly simplifies the Ember design. Figure 9 shows our abstract DAE architecture, which is com- posed of an access unit to load embedding vectors, an execute unit for computation, and a data queue and control queue to stream data and commands from the access unit to the execute unit. To map embedding operations to such abstraction, we observe that general embedding operations like the ones in Table 1 are all variants of sparse-dense tensor operations. For instance, the SLS function in Figure 10a can be interpreted as a sparse-dense matrix multiplica- tion (SpMM) ğ‘ğ‘–,ğ‘— ğ´ğ‘–,ğ‘˜ğµğ‘˜,ğ‘—, with an ğ‘–ğ‘˜ğ‘—loop schedule, and sparse matrix stored in a Compressed Sparse-Row (CSR) format [6]. For the example in Figure 2, dimension ğ‘–of the sparse matrix ğ´contains the sentences in a batch whereas dimension ğ‘˜contains the possible words in a sentence. ğ´ğ‘ğ‘ 1 if the sentence ğ‘contains the word ğ‘, and 0 otherwise. The CSR format stores all the indices of the non-zero elements and the pointers where each sentence begins. Multiplying the sparse matrix ğ´for the dense matrix ğµaccumulates all embedding vectors of the words in each sentence, which is the SLS code in Figure 10b. Non-zero values different than one rescale the embedding vector, which is needed in GNNs (Section 2.2.3). MP models are a Sampled-Dense-Dense Matrix Multiplication (SD- DMM) fused with an SpMM kernel [53]. KGs are SLS functions that use semirings general algebraic structures with addition and ptrs vec: AI BJ CK DL 0 2 3 idxs vec: 0 b_tr vals mtx: out mtx: s_tr e_tr 2 5 I J K L M N O P Q R S T U V W X E F G H A B C D U V W X Access unit Execute unit Queues (a) Example of SLS function. 1 void sls(idxs, ptrs, vals, out){ 2 for(int b 0; b num_batches; b ){ Batch traversal (b_tr) 3 for(int p ptrs[b]; p ptrs[b 1]; p ){ Segment traversal (s_tr) 4 idx idxs[p]; Load embedding index 5 for(int e 0; e emb_len; e ){ Embedding vector traversal (e_tr) 6 val vals[p,e]; Load embedding element 7 out[b,e] val; }}}} Reduce embedding elements (b) Imperative code. 1 tu b_tr loop_tr(0, num_batches, 1) iterate thru batch 2 str beg_ptr b_tr.mem_str(ptrs, b_tr.ite) load segment beg ptr 3 str end_pos b_tr.alu_str(' ', b_tr.ite, 1) compute next ptr pos 4 str end_ptr b_tr.mem_str(ptrs, end_pos) load segment end ptr 5 6 tu s_tr loop_tr(beg_ptr, end_ptr, 1) iterate thru segment 7 str emb_idx s_tr.mem_str(s_tr, idxs, s_tr.ite) load embedding index 8 str emb_beg s_tr.alu_str(s_tr, ' ', emb_idx, emb_len) compute emb addr 9 10 tu e_tr loop_tr(0, emb_len, 1) iterate thru emb vec 11 str emb_pos alu_str(' ', emb_beg, e_tr.ite) compute element addr 12 str emb_val mem_str(vals, emb_pos) load emb element 13 14 push_op(b_tr.ite, e_tr, ite) push batch position 15 push_op(e_tr.ite, e_tr, ite) push emb el position 16 push_op(emb_val, e_tr, ite) push emb el value 17 callback(e_tr, ite) trigger call on every emb iter (c) DLC dataflow lookup code. 0,0,A 0,1,B 0,2,C 0,3,D 0,0,I 0,1,J 0,2,K 0,3,L 1,0,U 1,1,V 1,2,W 1,3,X ei ei ei ei ei ei ei ei ei ei ei ei done ctrlQ: dataQ: queue pop (d) DAE queue content. 1 while(ctrlQ.pop() ! done){ until some element 2 b dataQ.pop 1 x i32 (); read embedding position 3 e dataQ.pop 1 x i32 (); read element position 4 v dataQ.pop 1 x f32 (); read element value 5 fma( out[b,e],v); compute and store (e) DLC imperative compute code. Figure 10: DAE abstraction for the SLS operation. multiplication and just have one non-zero per row, not requiring segment pointers or lengths. SpAttn is similar to KG but with a blocked format and no compute. The Sparse Abstract Machine (SAM) [21] proves that tensor alge- bra operations, and hence embedding operations, can be expressed as a dataflow chain of access blocks and compute blocks. Hence, we can map lookup blocks to the access unit and compute blocks to the execute unit, and stream data and commands through the queues [61]. The DLC IR represents lookup blocks with streaming dataflow code, as it maps well to spatial architectures specialized for irregular memory-intensive workloads [12, 21, 44, 49, 57, 59, 61, 67]. Compute blocks, instead, are represented with imperative code as it can implement all the compute variants introduced above, and others (e.g., quantization and approximate computing). Figure 10 shows an example of The DLC IR for the SLS function, which is discussed in the reminder of this section. The DLC lookup code loads embedding elements through mem- ory streams. The iteration space for these memory streams is con- tained in index streams, which are generated by traversal operators and can be transformed with integer ALU streams. Formally: Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures PyTorch Structured Imperative (MLIR scf, memref, arith) Structured Lookup-Compute (SLC) IR Imperative Compute IR Access Unit (e.g. TMU) Code Execute Unit (e.g. CPU) Code Optimization (Sec. 7) Lowering (Sec. 6.2) torch-mlir, MPACT Lowering (Sec. 6.3) Codegen (Sec. 6.3) Ember (Sec. 5) Sec. 6.1 Decoupled Lookup-Compute (DLC) IR (Sec 4) Dataflow Lookup IR Figure 11: Overview of Ember. Yellow represents compiler inputs outputs and blue represents new contributions. loop_tr(lb,ub,stride): traverses the iteration space i lb; i ub;i stride, where lb and ub are streams or immedi- ate values and stride is an immediate value. The stream loop_tr.0 contains the induction variable. mem_str(base,idx): loads into a stream the values of the base memory locations indexed by the idx stream. alu_str(op,op1,op2): computes an integer binary opera- tion op { , , , } on operands op1 and op2, which can either be streams or immediate values. Lines 1-12 in Figure 10c represent lines 2-6 in Figure 10b. Compute code consists of callbacks that the access unit triggers while traversing and loading tensor operands. In Figure 10b, we can wrap the FMA operation in line 7 into a callback that the access unit can trigger at each iteration of its parent loop. To trigger such callback, the access unit marshals its corresponding token (the inner-loop iteration token (ğ‘’ğ‘–)) in the control queue and its operands (result coordinatesğ‘and ğ‘’and value ğ‘£) in the data queue. Line 14-17 in Figure 10c and line 2-4 in Figure 10e show how these tokens and data are pushed and popped from the queues, whose content is shown in Figure 10d. Formally, we can program the access unit to marshal control tokens and operands at each iteration, begin, and end of a fiber traversal through the following operations: push_op(s_id,tu_id,event): pushes into the data queue (dataQ) the content of the s_id stream every time a traver- sal event {beg, ite, end} occurs in tu_id. callback(tu_id,event): pushes into the control queue (ctrlQ) the corresponding control token every time a tra- versal event {beg, ite, end} occurs in tu_id. whereas the execute unit reads tokens and operands with: pop(): pops a control token from the data queue. pop ty (): pops a value with type ty from the data queue. Figure 14d shows a more complex example with multiple callbacks. In the remainder of this paper, we demonstrate how Ember gener- ates optimized DLC code from PyTorch and TensorFlow embedding operations, enabling full DAE potential at no programmability cost. 5 Ember Overview Figure 11 shows an MLIR [33] implementation of Ember. Overall, Ember compiles PyTorch and TensorFlow embedding operations into optimized DLC code (Section 4), which is then used to generate Any CPU Statement ğ‘†ğ‘‡ğ‘€ğ‘‡ Memory Reference ğ‘šğ‘Ÿğ‘’ğ‘“ CPU variable ğ‘£ğ‘ğ‘Ÿ Stream Variable ğ‘  Unsigned Integer uint For-Loop ğ¹ğ‘‚ğ‘… :: for (ğ»ğ¸ğ´ğ·) { ğ‘†ğ·ğ¸ğ¶ ğµğ‘‚ğ·ğ‘Œ} For Header ğ»ğ¸ğ´ğ· :: ğ‘  ğ‘Ÿto ğ‘Ÿstep uint Stream Declaration ğ‘†ğ·ğ¸ğ¶ :: ğ‘  ğ‘ ğ‘’ Loop Body ğµğ‘‚ğ·ğ‘Œ :: ğ¶ğ´ğ¿ğ¿ ğ¶ğ´ğ¿ğ¿? ğ¹ğ‘‚ğ‘…ğ¶ğ´ğ¿ğ¿? Callback ğ¶ğ´ğ¿ğ¿ :: callback() { ğ‘‡ğ‘‰ğ´ğ¿ ğ‘†ğ‘‡ğ‘€ğ‘‡ } Value Conversion ğ‘‡ğ‘‰ğ´ğ¿ :: ğ‘£ğ‘ğ‘Ÿ to_val(ğ‘ ) Loop Range ğ‘Ÿ :: ğ‘  ğ‘£ğ‘ğ‘Ÿ uint Stream Expression ğ‘ ğ‘’ :: ğ‘™ğ‘  ğ‘ğ‘  ğ‘ğ‘  ğ‘ğ‘  Load Stream ğ‘™ğ‘  :: load_str(ğ‘šğ‘Ÿğ‘’ğ‘“[ğ‘–ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  ],ğ‘¢ğ‘–ğ‘›ğ‘¡) Indices ğ‘–ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  :: ğ‘  ğ‘£ğ‘ğ‘Ÿ uint ALU Stream ğ‘ğ‘  :: alu_str(ğ‘ ğ‘œğ‘,ğ‘ 1,ğ‘ 2) Stream Ops ğ‘ ğ‘œğ‘ :: . . . Figure 12: The Structured Lookup-Compute (SLC) IR. target-specific access and execute code. However, the DLC IR al- ready separates access code and execute code, which communicate through queue (de)serialization. This breaks the control flow and data flow of the input application, hindering global optimizations such as vectorization or code motion across access and execute code. Ember overcomes such issue with the Structured Lookup-Compute (SLC) IR (Section 6.1). Structured IRs represent for-loops as structured operations, which facilitates analysis and optimizations of complex code like embed- ding operations. The Structured Control Flow (SCF) IR is perhaps the most popular structured IR in MLIR, and resembles the impera- tive code in Figure 10b. Ember s SLC IR is the natural extension of the SCF IR for DAE code, and is specifically designed to facilitate global optimizations across access and execute code. Hence, Ember first generates the SCF IR from PyTorch embed- ding operations or general tensor algebra expressions with tools like torch-mlir [39] or MPACT [17], respectively. Then, Ember lowers SCF code to the SLC IR and performs global optimizations (Sec- tion 6.2 and 7). After lowering to the DLC IR (Section 6.3), access and execute code are further optimized independently, and lowered to target-specific IRs such as the tmu and llvm dialect, which are then used for code generation. All these steps are presented in the following sections. 6 SCF decoupling and lowering to from SLC IR This section introduces the SLC IR, and how Ember lowers SCF and DLC code to and from it, respectively. 6.1 The SLC IR Figure 12 shows the SLC grammar whereas Figure 13b shows the SLC IR of the SLS function. The SLC IR has a similar structure to SCF code, besides (1) SCF loops, index arithmetic, and load oper- ations to offload to the access unit are represented as SLC loops and streams and (2) compute code for the execute unit is wrapped into SLC callbacks within the SLC loops. In this way, callbacks can access data from streams with stream-to-value conversion opera- tions, preserving the data flow of the embedding operation which is no longer (de)serialized through queues. This enables Ember to simultaneously optimize lookup and compute code through global analyses and transformations, including dominance analysis, vec- torization, and code motion across different compute regions as well as across access and execute code, as discussed in Section 7. Siracusa et al. 1 void sls(idxs: mref ? x idx , 2 ptrs: mref ? x idx , 3 vals: mref ? x f32 , 4 out: mref ? x ? x f32 ){ 5 for(idx b 0; b n_batches; b ){ 6 idx beg ptrs[b]; 7 idx end ptrs[b 1]; 8 for(idx p beg; p end; p ){ 9 idx i idxs[ptr]; 10 for(idx e 0; e emb_len; e ){ 11 f32 val vals[i,e]; 12 f32 acc out[b,e]; 13 out[b,e] acc val; }}}} (a) SCF IR 1 void sls(idxs: mref ? x idx , 2 ptrs: mref ? x idx , 3 vals: mref ? x f32 , 4 out: mref ? x ? x f32 ){ 5 slc.for(str s_b from 0 to n_batches){ 6 str s_beg slc.mem_str(ptrs[s_b]); 7 str s_end slc.mem_str(ptrs[s_b 1]); 8 slc.for(str s_p from s_beg to s_end){ 9 str s_i slc.mem_str(idxs[s_p]); 10 slc.for(str s_e from 0 to emb_len){ 11 str s_val slc.mem_str(vals[s_i,s_e]); 12 slc.callback{ 13 idx b slc.to_val(s_b); 14 idx e slc.to_val(s_e); 15 f32 val slc.to_val(s_val); 16 f32 acc out[b,e]; 17 out[b,e] acc val; }}}}} (b) SLC IR Figure 13: Lowering step from SCF IR to SLC IR. Ember con- verts SCF for-loops into SLC for-loops and SCF memory ac- cesses (loads and index computation) into SLC streams. Em- ber wraps computation into SLC callbacks that can access SLC streams with stream-to-value operations. 6.2 Lowering the SCF IR to the SLC IR Figure 13 shows a lowering example from SCF to SLC. To generate the SLC IR, Ember recursively traverses the loop hierarchy of the SCF code looking for loops to offload to the access unit. We define an offloading candidate as a loop that (1) has iteration bounds which are either static or computed by another offloading candidate and (2) loads from at least one read-only memory location which has not been read from a parent loop. Condition (1) is necessary as access units cannot generally read data from the execute units. Condition (2) excludes workspace loops [28, 70] (i.e. loops that just work on partial results). As partial results are likely cached, workspace loops do not benefit from memory acceleration. All loops in Figure 13a are offloading candidates. Instead, the last two MP lines in Table 1 are workspaces loops, as they just multiply and accumulate the temporary vector with the vertex vector, which has been read already. As embedding operations are variants of sparse-dense tensor multiplications (Section 4), their loop hierarchy can only have at most one offloading candidate per level and at most one workspace loop per level [28, 29]. Hence, the decoupling algorithm recursively traverses and selects one offloading candidate per level, leaving the other loops for software execution. The SCF offloading candidates are lowered to SLC for-loops. Then, Ember places callbacks in the SLC loops. Operations to be offloaded (i.e., read-only load operations and index arithmetic) are transformed into streams and moved before their correspond- ing callback. Operations not to be offloaded (compute code and workspace loops) are moved inside their corresponding callback, which is executed in software. For instance, the read-only load operation in line 11 Figure 13a is transformed and moved before the callback whereas the accumulation operations in line 12 and 13 are moved inside the callback. Finally, stream-to-value operations are added for each operation in a callback reading from a stream. 1 while(ctrlQ.pop() ! done){ 2 b dataQ.pop 1 x index (); 3 e dataQ.pop 1 x index (); 4 v dataQ.pop 1 x f32 (); 5 fma( out[b,e],v); } 0,0,A 0,1,B 0,2,C 0,3,D 0,0,I 0,1,J 0,2,K 0,3,L ei ei ei ei ei ei ei ei ei ei ei ei done ctrlQ: dataQ: (a) Unoptimized code. 1 while(ctrlQ.pop() ! done){ 2 b dataQ.pop 1 x index (); 3 e dataQ.pop 1 x index (); 4 v dataQ.pop vlen x f32 (); 5 v_fma( out[b,e], v); 0,0,AB 0,2,CD 0,0,IJ 0,2,KL 1,0,UV 1,2,WX ei ei ei ei ei ei done ctrlQ: dataQ: (b) Vectorized code. 1 while(ctrlQ.pop() ! done){ 2 b dataQ.pop 1 x index () 3 for(e 0; e emb_len; e vlen){ 4 v dataQ.pop vlen x f32 () 5 v_fma( out[b,e],v) } ee ee ee done 0,ABCD 0,IJKL 1,UVWX ctrlQ: dataQ: (c) Bufferized code. 1 out_ptr out 2 while(tkn ctrlQ.pop() ! done){ 3 if(tkn ğ‘’ğ‘’){ 4 for(e 0; e emb_len; e vlen){ 5 v dataQ.pop vlen x f32 () 6 v_fma(out_ptr e, v) }} 7 else if(tkn ğ‘ ğ‘’){ 8 out_ptr emb_len } ee ee se ee se done ABCD IJKL UVWX ctrlQ: dataQ: (d) Queue aligned code. Figure 14: Impact of SLC optimizations on the SLS compute code and output queues. 6.3 Lowering SLC IR to DLC IR After applying optimizations (Section 7), the SLC IR is lowered to the low-level DLC IR (Section 4). The SLC IR is generated by traversing the SLC IR from the outer to the inner SLC for-loop. SLC for-loops and streams are lowered to DLC traversal operators and streams. Callbacks, instead, are moved into the while-loop in the compute code (Figure 9). Multiple callbacks are chained into an if-then-else construct that cases the token IDs popped from the control queue (e.g. Figure 14d). Token s push instructions are generated according to the location of the callback in the SLC IR. Operands push and pop instructions are generated according to the SLC stream-to-value operations. 7 Optimizing Embedding Operations This section describes three key DAE optimizations enabled by the SLC IR. Figure 15 and 14 show how these optimizations transform the SLC IR, compute code, and queues. 7.1 Vectorization Vectorization is one of the most impactful DAE optimizations [61]. As shown in Figure 14b, for each token, vectorization pops a vector of vector length (vlen) elements, improving marshaling and compute efficiency. While auto-vectorizing general code is challenging [27], the SLC IR provides an effective representation to vectorize loading, marshaling, and computing of embedding operations. As shown in Figure 15b, Ember vectorizes code by converting SLC operations into their vectorized SLCV duals. Conversely from the SLC for-loop, the SLCV for-loop (1) adds a vector length attribute, (2) instantiates vectorized induction variables, and (3) introduces the concept of mask to handle loop boundaries not multiple of Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 Access: Iterate over embedding vector elements 11 slc.for(stream s_e from 0 to emb_len){ 12 stream s_val slc.mem_str(vals[s_idx,s_e]); 13 Execute: Reduce embedding vectors 14 slc.callback{ 15 index b slc.to_val(s_b); 16 index e slc.to_val(s_e); 17 f32 val slc.to_val(s_val); 18 f32 acc out[b,e]; 19 out[b,e] acc val; }}}}} (a) Unoptimized code. 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 Access: Iterate over embedding vector elements 11 slcv.for vlen ((stream s_e, stream msk) from 0 to emb_len){ 12 stream s_val slcv.mem_str vlen (vals[s_idx,s_e], msk); 13 Execute: Reduce embedding vectors 14 slcv.callback{ 15 index b slc.to_val(s_b); 16 index e slcv.to_val(s_e)[0]; 17 vec vlen x f32 val slcv.to_val vlen (s_val); 18 vec vlen x f32 acc vload vlen (out[b,e]); 19 vstore vlen (acc val, out[b,e], acc); }}}}} (b) Vectorized code. 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 stream vec vlen x f32 buf slcv.buf_str(); Buffer stream 11 Access: Iterate over embedding vector elements 12 slcv.for vlen ((stream s_e, stream msk) from 0 to emb_len){ 13 stream s_val slcv.mem_str vlen (vals[s_idx,s_e], msk); 14 slc.push(buf, s_val); } Push into the buffer 15 Execute: Reduce embedding vectors 16 slcv.callback{ Callback moved at the end of inner loop 17 index b slc.to_val(s_b); 18 vec vlen x f32 buf_vec slc.to_val(buf); Get buffer 19 for(index e 0; e emb_len; e ){ Iterate buffer 20 vec vlen x f32 val buf_vec[e]; Get buffered element 21 vec vlen x f32 acc vload vlen (out[b,e]); 22 vstore vlen (acc val, out[b,e], acc); }}}}}} (c) Bufferized code. 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 stream vec vlen x f32 buf slcv.buf_str(); 11 Access: Iterate over embedding vector elements 12 slcv.for vlen ((stream s_e, stream msk) from 0 to emb_len)(index i 0) { 13 stream s_val slcv.mem_str vlen (vals[s_idx,s_e], msk); 14 slc.push(buf, s_val); } 15 Execute: Reduce embedding vectors 16 slcv.callback{ 17 vec vlen x f32 buf_vec slc.to_val(buf); 18 for(index e 0; e emb_len; e ){ 19 vec vlen x f32 val buf_vec[e]; 20 vec vlen x f32 acc vload vlen (out[i,e]); 21 vstore vlen (acc val, out[i,e], acc); }}}} 22 slc.callback{ i ; }}} end of indices in segment loop (d) Queue aligned code. Figure 15: Progressive optimization of the SLS SLC IR. the vector length. Masks are used by SLCV streams to perform vectorized index computation and data loading. We define a vectorization scheme as the set of for-loops within a parent loop ğ‘and the inner loop ğ‘–, with ğ‘ ğ‘–, that we intend to vectorize. A for-loop can be vectorized if and only if all of its callbacks can be vectorized. A vectorization scheme is legal if and only if all of its for-loops can be vectorized. Vector extensions such as Arm SVE [62] provide instructions to vectorize most callbacks in embedding operations, making the space of legal vectorization schemes large. However, prior work has demonstrated that the most efficient vectorization scheme for sparse-dense tensor multiplication is inner-loop vectorization, as- suming the dense tensor is in row-major order and has rows which are larger than the vector length [61]. Embedding operations gen- erally satisfy these assumptions (Section 2). Hence, similarly to the MLIR sparsifier [8], Ember only attempts inner-loop vectorization. If the inner for-loop is legal, Ember vectorizes its access and execute code in two steps. Ember starts by vectorizing the inner for-loop and its streams. As the stream-to-value operations in the callbacks expect stream types, the algorithm adds a temporary cast operation for source materialization. Then, during the callback vec- torization step, Ember recursively vectorizes the uses of these cast operations, producing full SLCV code. To keep the conversion step simple, load and store operations into callbacks are firstly converted to vector gather and scatter operations, with vector indices and masks coming from its parent vectorized for-loop operation. A fur- ther transformation pass simplifies these operations into contiguous vector loads and stores. 7.2 Bufferization Bufferization allows to marshal and compute embedding vectors as compound types. As shown in Figure 14c, the access unit pushes, in the control queue, one ğ‘’ğ‘’(embedding-vector end) token for each embedding vector and, in the data queue, the position of the output embedding vector and all of its values. As the length of the embedding vector (emb_len) is constant, once the core reads an ğ‘’ğ‘’token, it pops emb_len elements with a vectorized for-loop. Bufferization greatly improves marshaling and compute efficiency, especially for long embedding vectors. As shown in Figure 15c, Ember bufferizes code by firstly initial- izing a buffer stream of vector type (line 10) before the inner SLCV for-loop, where such loop can push the loaded embedding elements (line 14). Then, Ember moves the inner-loop body callback (line 14-19 in Figure 15b) right after the loop (line 16-22 in Figure 15c), adds a stream-to-value operation for the buffer (line 18), and adds a loop to iterate through it (line 19-22). 7.3 Queue alignment Vector loads are more efficient when aligned to cache lines. How- ever, as shown in Figure 14c, scalar operands like segment IDs hinder alignment of embedding vectors. Ember tries to align em- bedding accesses with different optimizations depending on the code characteristics. For simpler functions like the SLS in Figure 15c where all segment IDs are just loop induction variables, Ember stores a reference of Siracusa et al. Name MLIR dialects Description emb-opt0 slc, scf, memref, arith unoptimized Ember DAE code emb-opt1 slcv, scf, memref, arith, vector emb-opt0 vectorization emb-opt2 slcv, scf, memref, arith, vector emb-opt1 bufferization emb-opt3 slcv, scf, memref, arith, vector emb-opt2 queue alignment ref-dae tmu, scf, memref, arith, vector hand-optimized TMU-CPU code Table 4: Evaluated code and reference. Ember s input is torch- mlir [39], output is TMU-CPU machine code. these indices in the core and increments them after the loop. This increment is triggered by a segment-end token, ğ‘ ğ‘’in Figure 14d. As shown in Figure 15d, Ember implement such optimization by looking into iteration callbacks for stream-to-value operations that just read the induction variable of their own loop (e.g. line 17 in Figure 15c). Then Ember adds a new variable in the SLC loop (i variable in line 12 in Figure 15d), replaces all uses of the stream- to-value operations with that variable (line 20-21), and increments it in the end callback of its child loop (line 22). However, for more complex models like MP, certain scalars can- not be simplified (e.g. rescaling values). In this case, Ember preserves alignment by padding scalars to vectors while generating the DLC IR. As a further optimization, instead of sending segment IDs, Em- ber offloads to the access unit full index calculation of partial and output results and directly sends addresses to the core, reducing pressure on core s ALUs. 7.4 Model-Specific Optimizations While the optimization we presented so far are for general embed- ding operations, Ember can also implement model-specific opti- mizations. For instance, block-sparse attention mechanisms (Sec- tion 2.2.2) exhibit (1) large structured reuse within each block, (2) low reuse throughout blocks, and (3) no computation. Hence, we can add store streams to write directly into memory without passing through the core. Moreover, we can extend load streams to (1) select what cache level to read from and (2) whether to issue temporal or non-temporal requests. As demonstrated in Section 8.2, this greatly improves core and cache utilization. Other optimizations include, for instance, introducing toggle callbacks to trigger computation on sparse coordinate formats [52] or accumulation streams to track boundaries of SLS segments by accumulating lengths, instead of using offsets [41]. 8 Evaluation In this section, we demonstrate how Ember enables full DAE poten- tial at scale. Our evaluation firstly demonstrates the benefit of the SLC IR by performing an ablation study on the general embedding optimizations (vectorization, bufferization, and queue alignment) introduced in Section 7. Then, it demonstrates the generality of the SLC IR by showing the impact of the model-specific optimizations for LLMs introduced in Section 7.4. Finally, it demonstrates that Ember optimizations match the performance of handwritten code specifically optimized for our TMU-CPU target system. The experimental setting in this section follows the DAE study in Section 3.2. Table 4 summarizes the code utilized in the experiments. RM1 L0 RM1 L1 RM1 L3 RM2 L0 RM2 L1 RM2 L3 RM3 L0 RM3 L1 RM3 L3 wiki-Talk roadNet-CA com-Youtube web-Google 0x 5x 10x 15x 20x 25x Opt speedup emb-opt3 emb-opt2 emb-opt1 Figure 16: Performance speedup of Ember optimizations on MP models and SLS function for various DLRM mod- els (Table 3) and input locality. All optimizations combined (emb-opt3) improve performance by 6.6 21 . 0 10 20 30 40 50 60 70 80 Compute throughput 0 20 40 60 Access throughput emb-opt0 emb-opt1 emb-opt2 emb-opt3 RM1_L0 RM1_L1 RM1_L2 RM2_L0 RM2_L1 RM2_L2 RM3_L0 RM3_L1 RM3_L2 Figure 17: Impact of Ember optimizations on access through- put (TMU) and compute throughput (CPU). By optimizing both, Ember achieves highest performance (top-right). 8.1 Impact of General Optimizations Figure 16 shows the performance impact of general embedding opti- mizations such as vectorization (emb-opt1), bufferization (emb-opt2), and queue alignment (emb-opt3) over unoptimized Ember-generated code (emb-opt0) for the SLS function and more compute-intensive MP models. For the SLS function, we evaluated the three DLRMs in Table 3, each one running representative synthetic inputs [18] with low (L0), medium (L1), and high (L2) locality. Overall, all optimizations com- bined (emb-opt3) achieve 6.6 , 12.1 , and 21 better performance over unoptimized code (emb-opt0) for RM1, RM2, and RM3, respec- tively. Vectorization is consistently the most impactful optimization with a 5.13 speedup and only 17 deviation, whereas other opti- mizations deliver widely-different performance improvements on different configurations. To better understand these results, Figure 17 shows how these optimizations impact the throughput at which the compute unit reads and the access unit writes into the L2 queue. Compute op- timizations move upward whereas memory optimizations move rightward. The blue line indicates where the compute-unit through- put equals the access-unit throughput. Only optimizing compute code cannot move above the blue line as the access unit would not be able to marshal enough data to process. This would only improve throughput up to 8 (RM3, emb-opt0) before the access unit starts to be the bottleneck (blue line). However, because of the SLC IR, Ember can perform global optimizations on both access and compute code and move both rightward and upward in the plot, improving performance by up to 21 (RM3, emb-opt3). For RM1, the most control-intensive model (shorter loops), vectoriza- tion already saturates throughput, leaving other optimizations little room for improvement. For RM2 and RM3, by reducing coordinate overhead, bufferization helps to move closer to the blue line. For Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures L3 1 L2 1 L3 2 L2 2 L3 4 L2 4 L3 8 L2 8 0 20 40 60 80 100 L3 APKE Non-temporal accesses Temporal accesses Figure 18: L3 Accesses Per Kilo Element (APKE) of the Big- Bird gather function with different block sizes (1, 2, 4, and 8) and TMU configurations. Temporal accesses load indexes and non-temporal accesses load embedding vectors. Reading from L2 substantially helps filtering LLC accesses. wiki-Talk roadNet-CA com-Youtube web-Google arxiv mag products proteins bioKg wikiKg2 RM1 L0 RM1 L1 RM1 L2 RM2 L0 RM2 L1 RM2 L2 RM3 L0 RM3 L1 RM3 L2 1 emb blk 2 emb blk 4 emb blk 8 emb blk 60 70 80 90 100 Ember vs hand-optimized code MP SpMM(GNN) KG SLS(DLRM) SpAttn(LLM) Figure 19: DAE code automatically generated and optimized by Ember (emb-opt3) achieves a geomean 99 performance compared to hand-optimized code (ref-dae) across the dif- ferent model classes in Table 1. RM3, the model with largest loops, queue alignment removes index overhead, pushing performance closer to the limit. As shown in Figure 16, MP models have similar trends, and the optimization impact is mostly proportional to their compute-per- lookup ratio (Table 1). 8.2 Impact of Model-Specific Optimizations As discussed in Section 7.4, Ember lends itself to model-specific optimizations. Figure 18 shows that, in block-sparse attention mech- anisms (Section 2.2.2), loading highly-reused embedding blocks from L2 rather than LLC filters 67 74 of the embedding reads and 50 65 of the overall accesses, which include non-temporal loads for indexes, with larger reductions on larger block sizes with more intrinsic reuse. By directly writing data with the store streams instead of going through the core, Ember enables efficient gather operations with low resource utilization. 8.3 Comparison with Hand-optimized Code Figure 19 shows the performance of DAE code automatically gen- erated and optimized by Ember (emb-opt3) compared to hand- optimized code (ref-dae) for all model classes in Table 1. Besides all optimizations discussed in Section 7, hand-optimized code also includes low-level, CPU-specific optimizations to improve callback invocations. These CPU-specific optimizations include, for instance, (1) reordering the if-cases of multi-callback code (like the code in Figure 14d) according to their taken frequency or (2) set the val- ues of control tokens to be directly used in compute code (e.g. to increment variables). Overall, these CPU-specific optimizations primarily affect multi- callback code such as MP, SLS, and SpMM, yielding performance improvements of up to 5 , with an average geometric mean im- provement of 1 . The limited impact of these low-level optimiza- tions arises because, as discussed in Section 8.1, the optimizations introduced in Section 7 already push the architecture close to its limits, leaving little room for further gains. Nevertheless, because these low-level optimizations are highly CPU-specific, we chose not to integrate them into Ember, which is designed to provide a more general solution for a larger class of architectures. Ultimately, we believe that the optimizations presented in Sec- tion 7 are sufficient to fully unlock the potential of general DAE architectures at no programmability cost. 9 Related Work Embedding operations are becoming increasingly critical in several machine learning models, and DAE architectures a more widely adopted solution for similar irregular workloads. However, no study demonstrates the potential of DAE architectures on such a large class of models, and no compiler can generate DAE code that matches the performance of hand-optimized DAE embedding operations, as explained in this section. Embedding operations in Machine Learning Compilers: For traditional architectures such as CPUs and GPUs, end-users call em- bedding operations through multiple deep learning frameworks [1, 9, 14, 24, 50, 54], which can either be hand-written or automatically generated from progressive lowering. However, to the best of our knowledge, no machine learning compiler integrates automated DAE compilation, especially for embedding operations. While ac- celerators such as Meta s MTIA [15] feature cores to accelerate embedding lookups, embedding operations for such an accelerator typically come as libraries written by experts in low-level code [15]. However, this solution does not scale to general embedding oper- ations where the space of models [22], algorithmic variants [53], algorithmic optimizations [20], and input formats [29] is large. Fur- thermore, libraries prevent fusion across operations [36]. Sparse Tensor Algebra Compilers: When interpreted as sparse tensor expressions, embedding operations can be compiled to CPUs [3, 4, 8, 29, 30], GPUs [38, 55, 68], FPGAs [38], and distributed sys- tems [66], but not DAE architectures. The Sparse Abstract Machine (SAM) [21] compiles sparse tensor algebra to a stream dataflow representation, which might generate code for the access unit but not for the execute unit nor the marshaling code. DAE Programming Models in Other Domains: Several au- thors already proposed different DAE architectures for various domains, each one with a different programming interface. Pla- nar [5] offloads strided and irregular memory accesses from CPUs to tiny near-memory cores. As Planar primarily targets scientific workloads, it can be used through software libraries (e.g. BLAS [34]) optimized by experts. SpZip [67], instead, integrates near-core spe- cialized units in multicore processors to accelerate traversal and (de)compression operations in graph analytics. As these algorithms are generally hand-written by end users, SpZip comes with its own domain-specific language. Finally, MAPLE [48] offloads indirect accesses to specialized units placed in the network-on-chip of a Siracusa et al. multicore processor. As MAPLE targets general irregular workloads, it comes with its own compiler, DeSC [19], which decouples access from execute operations in LLVM [32]. A similar LLVM technique has been used to offload affine, indirect, and pointer-chasing mem- ory accesses to stream engines on CPUs [63]. However, the LLVM IR is an unstructured IR that represents loops with conditional branches. This substantially limits code analyses, decoupling, and optimizations of programs with complex loops like embedding op- erations [60]. While other frameworks, including HLS tools [10, 60], proposed to overcome LLVM limitations by using pragmas [59, 64], this solution is not applicable to machine learning compilers. More- over, the LLVM IR is not designed for DAE code, making advanced optimizations impractical. 10 Conclusions In conclusion, we demonstrated that DAE architectures outperform GPUs by 2.6 in performance and 6.4 in performance watt on embedding-intensive models. Then, we designed the Ember com- piler to integrate DAE architectures in common machine-learning frameworks. Compared to other approaches in the literature, Ember progressively lowers embedding operations through custom inter- mediate representations to optimize code at different abstraction levels. In this way, Ember implements all the necessary optimiza- tions, both local and global optimizations, to match the performance of hand-optimized code, enabling the potential of DAE architectures at no programmability cost. References [1] MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, San- jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion ManÃ©, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda ViÃ©gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org. [2] Bilge Acun, Matthew Murphy, Xiaodong Wang, Jade Nie, Carole-Jean Wu, and Kim Hazelwood. 2021. Understanding Training Efficiency of Deep Learning Recommendation Models at Scale. In 2021 IEEE International Symposium on High- Performance Computer Architecture (HPCA). 802 814. doi:10.1109 HPCA51647. 2021.00072 [3] Willow Ahrens, Daniel Donenfeld, Fredrik Kjolstad, and Saman Amarasinghe. 2023. Looplets: A Language for Structured Coiteration. In Proceedings of the 21st ACM IEEE International Symposium on Code Generation and Optimization (MontrÃ©al, QC, Canada) (CGO 2023). Association for Computing Machinery, New York, NY, USA, 41 54. doi:10.1145 3579990.3580020 [4] Manya Bansal, Olivia Hsu, Kunle Olukotun, and Fredrik Kjolstad. 2023. Mosaic: An Interoperable Compiler for Tensor Algebra. Proc. ACM Program. Lang. 7, PLDI, Article 122 (jun 2023), 26 pages. doi:10.1145 3591236 [5] AdriÃ¡n Barredo, AdriÃ  Armejach, Jonathan Beard, and Miquel Moreto. 2021. PLANAR: a programmable accelerator for near-memory data rearrangement. In Proceedings of the 35th ACM International Conference on Supercomputing (Virtual Event, USA) (ICS 21). Association for Computing Machinery, New York, NY, USA, 164 176. doi:10.1145 3447818.3460368 [6] Richard Barrett, Michael Berry, Tony F. Chan, James Demmel, June Donato, Jack Dongarra, Victor Eijkhout, Roldan Pozo, Charles Romine, and Henk van der Vorst. 1994. Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. Society for Industrial and Applied Mathematics. doi:10.1137 1. 9781611971538 arXiv: [7] Tim Berners-Lee, James Hendler, and Ora Lassila. 2001. The Semantic Web: A New Form of Web Content that is Meaningful to Computers will Unleash a Revolution of New Possibilities. Scientific American (May 2001). [8] Aart Bik, Penporn Koanantakool, Tatiana Shpeisman, Nicolas Vasilache, Bixia Zheng, and Fredrik Kjolstad. 2022. Compiler Support for Sparse Tensor Com- putations in MLIR. ACM Trans. Archit. Code Optim. 19, 4, Article 50 (sep 2022), 25 pages. doi:10.1145 3544559 [9] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python NumPy programs. [10] Jason Cong, Bin Liu, Stephen Neuendorffer, Juanjo Noguera, Kees Vissers, and Zhiru Zhang. 2011. High-Level Synthesis for FPGAs: From Prototyping to Deployment. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 30, 4 (2011), 473 491. doi:10.1109 TCAD.2011.2110592 [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 4171 4186. doi:10.18653 v1 N19-1423 [12] Douglas Doerfler, Farzad Fatollahi-Fard, Colin MacLean, Tan Nguyen, Samuel Williams, Nicholas Wright, and Marco Siracusa. 2021. Experiences Porting the SU3_Bench Microbenchmark to the Intel Arria 10 and Xilinx Alveo U280 FPGAs. In Proceedings of the 9th International Workshop on OpenCL (Munich, Germany) (IWOCL 21). Association for Computing Machinery, New York, NY, USA, Article 1, 9 pages. doi:10.1145 3456669.3456671 [13] Pouya Esmaili-Dokht, Francesco Sgherzi, ValÃ©ria Soldera Girelli, Isaac Boixaderas, Mariana Carmin, Alireza Monemi, AdriÃ  Armejach, Estanislao Mercadal, GermÃ¡n Llort, Petar RadojkoviÄ‡, Miquel Moreto, Judit GimÃ©nez, Xavier Martorell, Eduard AyguadÃ©, Jesus Labarta, Emanuele Confalonieri, Rishabh Dubey, and Jason Adlard. 2024. A Mess of Memory System Benchmarking, Simulation and Application Profiling. In 2024 57th IEEE ACM International Symposium on Microarchitecture (MICRO). 136 152. doi:10.1109 MICRO61859.2024.00020 [14] Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds. [15] Amin Firoozshahian, Joel Coburn, Roman Levenstein, Rakesh Nattoji, Ashwin Ka- math, Olivia Wu, Gurdeepak Grewal, Harish Aepala, Bhasker Jakka, Bob Dreyer, Adam Hutchin, Utku Diril, Krishnakumar Nair, Ehsan K. Aredestani, Martin Schatz, Yuchen Hao, Rakesh Komuravelli, Kunming Ho, Sameer Abu Asal, Joe Shajrawi, Kevin Quinn, Nagesh Sreedhara, Pankaj Kansal, Willie Wei, Dheepak Jayaraman, Linda Cheng, Pritam Chopda, Eric Wang, Ajay Bikumandla, Arun Karthik Sengottuvel, Krishna Thottempudi, Ashwin Narasimha, Brian Dodds, Cao Gao, Jiyuan Zhang, Mohammed Al-Sanabani, Ana Zehtabioskuie, Jordan Fix, Hangchen Yu, Richard Li, Kaustubh Gondkar, Jack Montgomery, Mike Tsai, Saritha Dwarakapuram, Sanjay Desai, Nili Avidan, Poorvaja Ramani, Karthik Narayanan, Ajit Mathews, Sethu Gopal, Maxim Naumov, Vijay Rao, Krishna Noru, Harikrishna Reddy, Prahlad Venkatapuram, and Alexis Bjorlin. 2023. MTIA: First Generation Silicon Targeting Meta s Recommendation Systems. In Proceedings of the 50th Annual International Symposium on Computer Architecture (Orlando, FL, USA) (ISCA 23). Association for Computing Machinery, New York, NY, USA, Article 80, 13 pages. doi:10.1145 3579371.3589348 [16] A.A. Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021. Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. In 2021 IEEE International Symposium on Informa- tion Theory (ISIT) (Melbourne, Australia). IEEE Press, 2786 2791. doi:10.1109 ISIT45174.2021.9517710 [17] Google. 2021. MLIR Sparsifier. [18] U. Gupta, C. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazel- wood, M. Hempstead, B. Jia, H. S. Lee, A. Malevich, D. Mudigere, M. Smelyanskiy, L. Xiong, and X. Zhang. 2020. The Architectural Implications of Facebook s DNN- Based Personalized Recommendation. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos, CA, USA, 488 501. doi:10.1109 HPCA47549.2020.00047 [19] Tae Jun Ham, Juan L. AragÃ³n, and Margaret Martonosi. 2015. DeSC: decoupled supply-compute communication management for heterogeneous architectures. In Proceedings of the 48th International Symposium on Microarchitecture (Waikiki, Hawaii) (MICRO-48). Association for Computing Machinery, New York, NY, USA, 191 203. doi:10.1145 2830772.2830800 [20] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS 17). Curran Associates Inc., Red Hook, NY, USA, 1025 1035. [21] Olivia Hsu, Maxwell Strange, Ritvik Sharma, Jaeyeon Won, Kunle Olukotun, Joel S. Emer, Mark A. Horowitz, and Fredrik KjÃ¸lstad. 2023. The Sparse Abstract Machine. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 ( conf- loc , city Vancouver city , state BC state , country Canada country , conf-loc ) (ASPLOS 2023). Association for Computing Machinery, New York, NY, USA, 710 726. doi:10.1145 3582016.3582051 [22] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: datasets for Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures machine learning on graphs. In Proceedings of the 34th International Conference on Neural Information Processing Systems ( conf-loc , city Vancouver city , state BC state , country Canada country , conf-loc ) (NIPS 20). Curran Associates Inc., Red Hook, NY, USA, Article 1855, 16 pages. [23] Mohamed Assem Ibrahim, Onur Kayiran, and Shaizeen Aga. 2022. Efficient Cache Utilization via Model-aware Data Placement for Recommendation Models. In Proceedings of the International Symposium on Memory Systems (Washington DC, DC, USA) (MEMSYS 21). Association for Computing Machinery, New York, NY, USA, Article 2, 11 pages. doi:10.1145 3488423.3519317 [24] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093 (2014). [25] Manas R. Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav Khaitan, Jiahui Liu, and Quoc V. Le. 2020. Neural Input Search for Large Scale Recommendation Models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining (Virtual Event, CA, USA) (KDD 20). Association for Computing Machinery, New York, NY, USA, 2387 2397. doi:10.1145 3394486.3403288 [26] Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David Brooks, Vikas Chandra, Utku Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng Li, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail Smelyanskiy, Xiaodong Wang, Brandon Reagen, Carole-Jean Wu, Mark Hempstead, and Xuan Zhang. 2020. RecNMP: accelerating personalized recom- mendation with near-memory processing. In Proceedings of the ACM IEEE 47th Annual International Symposium on Computer Architecture (Virtual Event) (ISCA 20). IEEE Press, 790 803. doi:10.1109 ISCA45697.2020.00070 [27] Ken Kennedy and John R. Allen. 2001. Optimizing compilers for modern archi- tectures: a dependence-based approach. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. [28] Fredrik Kjolstad, Willow Ahrens, Shoaib Kamil, and Saman Amarasinghe. 2019. Tensor algebra compilation with workspaces. In Proceedings of the 2019 IEEE ACM International Symposium on Code Generation and Optimization (Washington, DC, USA) (CGO 2019). IEEE Press, 180 192. [29] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amaras- inghe. 2017. The tensor algebra compiler. Proc. ACM Program. Lang. 1, OOPSLA, Article 77 (oct 2017), 29 pages. doi:10.1145 3133901 [30] Scott Kovach, Praneeth Kolichala, Tiancheng Gu, and Fredrik Kjolstad. 2023. Indexed Streams: A Formal Intermediate Representation for Fused Contraction Programs. Proc. ACM Program. Lang. 7, PLDI, Article 154 (jun 2023), 25 pages. doi:10.1145 3591268 [31] Criteo AI Lab. 2024. Criteo 1TB Click Logs Dataset. download-criteo-1tb-click-logs-dataset . [32] Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for Lifelong Program Analysis Transformation. In Proceedings of the International Symposium on Code Generation and Optimization: Feedback-Directed and Runtime Optimization (Palo Alto, California) (CGO 04). IEEE Computer Society, USA, 75. [33] Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Olek- sandr Zinenko. 2021. MLIR: scaling compiler infrastructure for domain specific computation. In Proceedings of the 2021 IEEE ACM International Symposium on Code Generation and Optimization (Virtual Event, Republic of Korea) (CGO 21). IEEE Press, 2 14. doi:10.1109 CGO51591.2021.9370308 [34] C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. 1979. Basic Linear Algebra Subprograms for Fortran Usage. ACM Trans. Math. Softw. 5, 3 (Sept. 1979), 308 323. doi:10.1145 355841.355847 [35] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network Dataset Collection. [36] Jiajun Li, Ahmed Louri, Avinash Karanth, and Razvan Bunescu. 2021. GCNAX: A Flexible and Energy-efficient Accelerator for Graph Convolutional Neural Networks. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 775 788. doi:10.1109 HPCA51647.2021.00070 [37] Sheng Li, Jung Ho Ahn, Richard D. Strong, Jay B. Brockman, Dean M. Tullsen, and Norman P. Jouppi. 2009. McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures. In 2009 42nd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 469 480. [38] Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang. 2024. UniSparse: An Intermediate Language for General Sparse Format Customization. Proc. ACM Program. Lang. 8, OOPSLA1, Article 99 (April 2024), 29 pages. doi:10.1145 3649816 [39] LLVM. [n. d.]. Torch-MLIR. [40] J. Lowe-Power, A. Mutaal Ahmad, A. Akram, M. Alian, R. Amslinger, M. An- dreozzi, A. Armejach, N. Asmussen, B. Beckmann, S. Bharadwaj, G. Black, G. Bloom, B. R. Bruce, D. Rodrigues Carvalho, J. Castrillon, L. Chen, N. Derumigny, S. Diestelhorst, W. Elsasser, C. Escuin, M. Fariborz, A. Farmahini-Farahani, P. Fotouhi, R. Gambord, J. Gandhi, D. Gope, T. Grass, A. Gutierrez, B. Hanindhito, A. Hansson, S. Haria, A. Harris, T. Hayes, A. Herrera, M. Horsnell, S. A. R. Jafri, R. Jagtap, H. Jang, R. Jeyapaul, T. M. Jones, M. Jung, S. Kannoth, H. Khaleghzadeh, Y. Kodama, T. Krishna, T. Marinelli, C. Menard, A. Mondelli, M. Moreto, T. MÃ¼ck, O. Naji, K. Nathella, H. Nguyen, N. Nikoleris, L. E. Olson, M. Orr, B. Pham, P. Prieto, T. Reddy, A. Roelke, M. Samani, A. Sandberg, J. Setoain, B. Shingarov, M. D. Sinclair, T. Ta, R. Thakur, G. Travaglini, M. Upton, N. Vaish, I. Vougioukas, W. Wang, Z. Wang, N. Wehn, C. Weis, D. A. Wood, H. Yoon, and Ã‰. F. Zulian. 2020. The gem5 Simulator: Version 20.0 . arXiv:2007.03152 [cs.AR] [41] Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie (Amy) Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnaku- mar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, and Vijay Rao. 2022. Software-hardware co-design for fast and scalable training of deep learning recommendation models. In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 993 1011. doi:10.1145 3470496.3533727 [42] Nevine Nassif, Ashley O. Munch, Carleton L. Molnar, Gerald Pasdast, Sitara- man V. Lyer, Zibing Yang, Oscar Mendoza, Mark Huddart, Srikrishnan Venkatara- man, Sireesha Kandula, Rafi Marom, Alexandra M. Kern, Bill Bowhill, David R. Mulvihill, Srikanth Nimmagadda, Varma Kalidindi, Jonathan Krause, Moham- mad M. Haq, Roopali Sharma, and Kevin Duda. 2022. Sapphire Rapids: The Next- Generation Intel Xeon Scalable Processor. In 2022 IEEE International Solid-State Circuits Conference (ISSCC), Vol. 65. 44 46. doi:10.1109 ISSCC42614.2022.9731107 [43] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherni- avskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kon- dratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. 2019. Deep Learning Recommendation Model for Personalization and Recommendation Systems. CoRR abs 1906.00091 (2019). [44] Tan Nguyen, Colin MacLean, Marco Siracusa, Douglas Doerfler, Nicholas J. Wright, and Samuel Williams. 2022. FPGA-based HPC accelerators: An evaluation on performance and energy efficiency. Concurrency and Com- putation: Practice and Experience 34, 20 (2022), e6570. doi:10.1002 cpe.6570 arXiv: [45] Nvidia. 2024. Nvidia H100 specs. h100 [46] Nvidia. 2024. Nvidia Nsight Systems. systems [47] Nvidia. 2024. Nvidia T4 specs. t4 [48] Marcelo Orenes-Vera, Aninda Manocha, Jonathan Balkind, Fei Gao, Juan L. AragÃ³n, David Wentzlaff, and Margaret Martonosi. 2022. Tiny but mighty: designing and realizing scalable latency tolerance for manycore SoCs. In Pro- ceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 817 830. doi:10.1145 3470496.3527400 [49] Alberto Parravicini, Luca Giuseppe Cellamare, Marco Siracusa, and Marco D Santambrogio. 2021. Scaling up HBM efficiency of Top-K SpMV for approximate embedding similarity on FPGAs. In 2021 58th ACM IEEE Design Automation Conference (DAC). IEEE, 799 804. [50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas KÃ¶pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: an imperative style, high-performance deep learning library. Curran Associates Inc., Red Hook, NY, USA. [51] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Alessandro Mos- chitti, Bo Pang, and Walter Daelemans (Eds.). Association for Computational Linguistics, Doha, Qatar, 1532 1543. doi:10.3115 v1 D14-1162 [52] Eric T. Phipps and Tamara G. Kolda. 2019. Software for Sparse Ten- sor Decomposition on Emerging Computing Architectures. SIAM Journal on Scientific Computing 41, 3 (2019), C269 C290. doi:10.1137 18M1210691 arXiv: [53] Md. Khaledur Rahman, Majedul Haque Sujon, and Ariful Azad. 2021. FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Net- works. In 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS). 256 266. doi:10.1109 IPDPS49936.2021.00034 Siracusa et al. [54] Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Summer Deng, Roman Dzhabarov, James Hegeman, Roman Levenstein, Bert Maher, Nadathur Satish, Jakob Olesen, Jongsoo Park, Artem Rakhov, and Misha Smelyanskiy. 2018. Glow: Graph Low- ering Compiler Techniques for Neural Networks. CoRR abs 1805.00907 (2018). arXiv:1805.00907 [55] Ryan Senanayake, Changwan Hong, Ziheng Wang, Amalee Wilson, Stephen Chou, Shoaib Kamil, Saman Amarasinghe, and Fredrik Kjolstad. 2020. A sparse iteration space transformation framework for sparse tensor algebra. Proc. ACM Program. Lang. 4, OOPSLA, Article 158 (nov 2020), 30 pages. doi:10.1145 3428226 [56] Geet Sethi, Bilge Acun, Niket Agarwal, Christos Kozyrakis, Caroline Trippel, and Carole-Jean Wu. 2022. RecShard: statistical feature-based memory optimization for industry-scale neural recommendation. In Proceedings of the 27th ACM In- ternational Conference on Architectural Support for Programming Languages and Operating Systems (Lausanne, Switzerland) (ASPLOS 22). Association for Com- puting Machinery, New York, NY, USA, 344 358. doi:10.1145 3503222.3507777 [57] Francesco Sgherzi, Alberto Parravicini, Marco Siracusa, and Marco D. Santam- brogio. 2021. Solving Large Top-K Graph Eigenproblems with a Memory and Compute-optimized FPGA Design. In 2021 IEEE 29th Annual International Sym- posium on Field-Programmable Custom Computing Machines (FCCM). 78 87. doi:10.1109 FCCM51124.2021.00017 [58] Francesco Sgherzi, Marco Siracusa, Ivan Fernandez, AdriÃ  Armejach, and Miquel MoretÃ³. 2024. SpChar: Characterizing the sparse puzzle via decision trees. J. Parallel and Distrib. Comput. 192 (2024), 104941. doi:10.1016 j.jpdc.2024.104941 [59] Marco Siracusa, Emanuele Del Sozzo, Marco Rabozzi, Lorenzo Di Tucci, Samuel Williams, Donatella Sciuto, and Marco Domenico Santambrogio. 2022. A Com- prehensive Methodology to Optimize FPGA Designs via the Roofline Model. IEEE Trans. Comput. 71, 8 (2022), 1903 1915. doi:10.1109 TC.2021.3111761 [60] Marco Siracusa and Fabrizio Ferrandi. 2020. Tensor Optimization for High- Level Synthesis Design Flows. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 39, 11 (2020), 4217 4228. doi:10.1109 TCAD.2020. 3012318 [61] Marco Siracusa, VÃ­ctor Soria-Pardos, Francesco Sgherzi, Joshua Randall, Dou- glas J. Joseph, Miquel MoretÃ³ Planas, and AdriÃ  Armejach. 2023. A Tensor Marshaling Unit for Sparse Tensor Algebra on General-Purpose Processors. In Proceedings of the 56th Annual IEEE ACM International Symposium on Mi- croarchitecture ( conf-loc , city Toronto city , state ON state , coun- try Canada country , conf-loc ) (MICRO 23). Association for Computing Machinery, New York, NY, USA, 1332 1346. doi:10.1145 3613424.3614284 [62] N. Stephens, S. Biles, M. Boettcher, J. Eapen, M. Eyole, G. Gabrielli, M. Horsnell, G. Magklis, A. Martinez, N. Premillieu, A. Reid, A. Rico, and P. Walker. 2017. The ARM Scalable Vector Extension. IEEE Micro 37, 02 (mar 2017), 26 39. doi:10. 1109 MM.2017.35 [63] Zhengrong Wang and Tony Nowatzki. 2019. Stream-based Memory Access Specialization for General Purpose Processors. In 2019 ACM IEEE 46th Annual International Symposium on Computer Architecture (ISCA). 736 749. [64] Jian Weng, Sihao Liu, Vidushi Dadu, Zhengrong Wang, Preyas Shah, and Tony Nowatzki. 2020. DSAGEN: Synthesizing Programmable Spatial Accelerators. In 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). 268 281. doi:10.1109 ISCA45697.2020.00032 [65] Finn Wilkinson and Simon McIntosh-Smith. 2022. An Initial Evaluation of Arm s Scalable Matrix Extension. In 2022 IEEE ACM International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS). 135 140. doi:10.1109 PMBS56514.2022.00018 [66] Rohan Yadav, Alex Aiken, and Fredrik Kjolstad. 2022. SpDISTAL: compiling distributed sparse tensor computations. In Proceedings of the International Confer- ence on High Performance Computing, Networking, Storage and Analysis (Dallas, Texas) (SC 22). IEEE Press, Article 59, 15 pages. [67] Yifan Yang, Joel S. Emer, and Daniel Sanchez. 2021. SpZip: Architectural Support for Effective Data Compression In Irregular Applications. In 2021 ACM IEEE 48th Annual International Symposium on Computer Architecture (ISCA). 1069 1082. doi:10.1109 ISCA52012.2021.00087 [68] Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze. 2023. Sparse- TIR: Composable Abstractions for Sparse Compilation in Deep Learning. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (Vancouver, BC, Canada) (ASPLOS 2023). Association for Computing Machinery, New York, NY, USA, 660 678. doi:10.1145 3582016.3582047 [69] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran- zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 17283 17297. c8512d142a2d849725f31a9a7a361ab9-Paper.pdf [70] Genghan Zhang, Olivia Hsu, and Fredrik Kjolstad. 2024. Compilation of Modular and General Sparse Workspaces. Proc. ACM Program. Lang. 8, PLDI, Article 196 (jun 2024), 26 pages. doi:10.1145 3656426\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nEmber: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures Marco Siracusa Barcelona Supercomputing Center Barcelona, Spain Olivia Hsu Stanford University Stanford, USA Victor Soria-Pardos Barcelona Supercomputing Center Barcelona, Spain Joshua Randall Arm Austin, USA Arnaud Grasset Arm Biot, France Eric Biscondi Arm Biot, France Doug Joseph Arm Austin, USA Randy Allen Barcelona Supercomputing Center Barcelona, Spain Fredrik Kjolstad Stanford University Stanford, USA Miquel MoretÃ³ Planas Barcelona Supercomputing Center Universitat PolitÃ¨cnica de Catalunya Barcelona, Spain AdriÃ  Armejach Barcelona Supercomputing Center Universitat PolitÃ¨cnica de Catalunya Barcelona, Spain Abstract Irregular embedding lookups are a critical bottleneck in recom- mender models, sparse large language models, and graph learn- ing models. In this paper, we first demonstrate that, by offloading these lookups to specialized access units, Decoupled Access-Execute (DAE) processors achieve 2.6 higher performance and 6.4 higher performance watt than GPUs on end-to-end models. Then, we pro- pose the Ember compiler for automatically generating optimized DAE code from PyTorch and TensorFlow. Conversely from other DAE compilers, Ember features multiple intermediate representa- tions specifically designed for different optimization levels. In this way, Ember can implement all optimizations to match the perfor- mance of hand-written code, unlocking the full potential of DAE architectures at scale. 1 Introduction Recommender models, sparse large language models, and graph- learning models represent categorical features such as users, prod- ucts, and words using dense embedding vectors [51]. Due to the vast number of categories, interactions between these features are inherently sparse [26]. For instance, recommender models must lookup the embedding vectors of the products a user has inter- acted with, often loading hundreds of vectors among millions [18]. This leads to irregular memory accesses with low cache reuse [23], posing significant challenges for traditional architectures [15, 69].\n\n--- Segment 2 ---\nFor instance, recommender models must lookup the embedding vectors of the products a user has inter- acted with, often loading hundreds of vectors among millions [18]. This leads to irregular memory accesses with low cache reuse [23], posing significant challenges for traditional architectures [15, 69]. In this paper, we first demonstrate that, by offloading embed- ding lookup to specialized access units, Decoupled Access-Execute (DAE) processors achieve 2.6 higher performance and 6.4 higher perf watt than GPUs on end-to-end models. Then, we design the Ember compiler to automatically lower PyTorch [50] and Tensor- Flow [1] embedding operations to high-performance DAE code. The goal of a DAE compiler is to decouple memory accesses from computation and generate optimized code for the access unit and execute unit. However, optimizing already decoupled code is challenging since memory access is separated from computation and (de)serialized through queues. This breaks the control flow and data flow of the input program, hindering global optimizations such as vectorization or code motion across access and execute unit. For this reason, Ember features multiple Intermediate Repre- sentations (IRs), each one designed for a different optimization level. Ember s low-level Decoupled Lookup-Compute (DLC) IR is designed to abstract implementation details of the underlying DAE architecture and facilitate local, target-agnostic optimizations of lookup code and compute code separately. Ember s high-level Struc- tured Lookup-Compute (SLC) IR is designed to also abstract the queue (de)serialization mechanism and facilitate global optimiza- tions within lookup and compute code. By lowering PyTorch and TensorFlow embedding operations through the SLC and DLC IRs, Ember can implement all necessary optimizations to match the per- formance of hand-optimized code, enabling the full DAE potential at no programmability cost. Overall, our contributions are: (1) A characterization of a large class of embedding operations in machine learning models to study the fundamental scal- ing limitations of traditional architectures (Section 2). (2) An evaluation of the potential of DAE architectures for embedding operations (Section 3). (3) The DLC IR, a low-level IR designed to abstract and optimize decoupled lookup and compute code (Section 4).\n\n--- Segment 3 ---\n(2) An evaluation of the potential of DAE architectures for embedding operations (Section 3). (3) The DLC IR, a low-level IR designed to abstract and optimize decoupled lookup and compute code (Section 4). arXiv:2504.09870v1 [cs.AR] 14 Apr 2025 Siracusa et al. dlrm_uni dlrm_gau llm_bigbird kg_biokg kg_wikikg2 gnn_arxiv gnn_mag gnn_products gnn_proteins 0 20 40 60 80 100 Time breakdown Other Scalar elem Vector elem GEMM Embedding 10 100 Operational Intensity (FLOPs Byte) 10 100 Performance (TFLOPs s) 1 BW util 3 BW util 6 BW util 12 BW util 25 BW util 50 BW util HBM bandwidth dlrm_uni dlrm_gau kg_biokg kg_wikikg2 gnn_arxiv gnn_mag gnn_products gnn_proteins Figure 1: Deep-learning recommendation models (dlrm) (Sec- tion 2.2.1), large language models (llm) with sparse attention (Section 2.2.2), knowledge graphs (kg) (Section 2.2.3), and graph neural networks (gnn) (Section 2.2.3) heavily rely on embedding operations that do not perform efficiently even on modern Nvidia H100 GPUs [45]. All experiments use highly- optimized models from the literature (Section 2.2). ptrs vec: 0 2 3 idxs vec: 2 Embedding table: 4 0 0.4 -0.3 0.7 0.1 0.8 -0.1 0.2 0.0 -0.7 0.6 0.3 -0.5 0.0 -0.2 -0.6 0.4 -0.1 0.3 0.4 -0.3 Hi there! Hello! Sentence 1 [0 2] Word Pos: 0 1 2 Emb. idx: 2 4 0 Sentence 2 [2 3] Figure 2: Feature embedding requires scattered memory lookups to fetch embedding vectors from embedding tables.\n\n--- Segment 4 ---\nSentence 1 [0 2] Word Pos: 0 1 2 Emb. idx: 2 4 0 Sentence 2 [2 3] Figure 2: Feature embedding requires scattered memory lookups to fetch embedding vectors from embedding tables. (4) An end-to-end implementation of the Ember compiler to lower PyTorch TensorFlow embedding operations to such abstraction, and then to DAE architectures (Section 5). (5) The SLC IR, a high-level IR designed for global optimiza- tions of DAE embedding operations, and algorithms to lower to from it (Section 6). (6) Optimizations for DAE embedding operations (Section 7). After introducing these contributions, this paper evaluates Ember (Section 8) and discusses its impact in the field (Section 9). 2 Challenges of Embedding Operations Figure 1 shows that several machine learning models heavily rely on embedding operations that achieve low system utilization even on the latest datacenter GPUs. This section examines the charac- teristics of embedding operations, their architectural implications, and the challenges they present for performance scaling. 2.1 Background Modern machine learning models process an increasing amount of categorical features [23] representation variables that can take on a distinct set of values and are not comparable. Example categorical features include product categories, words, and actors in a movie. As these categories have no numerical meaning, they cannot be directly processed with deep neural networks [18]. Hence, machine learning models embed the numerical meaning of these categories into dense embedding vectors [51], which are all stored into large embedding tables. Figure 2 illustrates how categorical features are processed during inference. Incoming categories are tokenized into IDs, which are used to lookup into the embedding table. Embedding lookup is generally fused with downstream computation (e.g., reduction) in an optimized embedding operation. 2.2 Characterization and Implications Table 1 characterizes embedding operations across various machine learning models. For each model, we first analyze its loop hierarchy and compute-per-lookup ratio, which proxy compute requirements. Then, we compute the memory footprint of their embedding tables to understand their storage demand. Finally, we characterize their spatio-temporal locality to gauge the effectiveness of caches and prefetchers for embedding lookups.\n\n--- Segment 5 ---\nThen, we compute the memory footprint of their embedding tables to understand their storage demand. Finally, we characterize their spatio-temporal locality to gauge the effectiveness of caches and prefetchers for embedding lookups. We characterize temporal lo- cality through the concept of reuse distance the number of other vectors accessed before a vector is accessed again [56]. If a cache can hold ğ‘¥vectors, an access with reuse distance greater than ğ‘¥ will likely result in eviction. For a given input, we first construct a histogram of all reuse distances, which we then integrate and normalize to obtain the Cumulative Distribution Function (ğ¶ğ·ğ¹). Hence, ğ¶ğ·ğ¹(ğ‘¥) proxies the hit probability of that cache. The spa- tial locality, instead, is characterized by the size of the embedding vector. 2.2.1 Deep Learning Recommendation Models (DLRMs). DLRMs recommend content to users by processing a large set of categorical features under strict latency constraints [18, 23]. DLRMs currently account for a large fraction of inference cycles in production data centers, with embedding operations consuming a significant run- time portion [18], as shown in Figure 1. Categorical features in DLRMs are generally multi-hot, mean- ing they can take on multiple values (e.g. movie genres or cast). At inference time, the embedding vectors corresponding to these values need to be looked up and aggregated with, for instance, an element sum. High-performance implementations (1) fuse these two operations to avoid materializing intermediate results and (2) batch together the categories of multiple queries to improve system throughput (column 2 in Table 1), which yields a compute- per-lookup ratio of 1 (column 3 in Table 1). The nn.EmbeddingBag (EB) in PyTorch [50] and Sparse-Lenghts Sum (SLS) function in Caffe2 [24] implement these optimizations. DLRMs invoke EB SLS functions 10s 1000s of times to aggregate all of the categorical features in a batch [23] (clusters of crosses and circles in Figure 1).\n\n--- Segment 6 ---\nThe nn.EmbeddingBag (EB) in PyTorch [50] and Sparse-Lenghts Sum (SLS) function in Caffe2 [24] implement these optimizations. DLRMs invoke EB SLS functions 10s 1000s of times to aggregate all of the categorical features in a batch [23] (clusters of crosses and circles in Figure 1). Each invocation aggregates 10s 100s embed- ding vectors from a single table with millions of entries [23], each vector storing 32-256 elements (column 6 in Table 1). While this necessitates a large memory footprint (column 4 in Table 1), embed- ding vectors are accessed with some degree of reuse [2, 16, 25, 56], and modern caches can filter a significant fraction of embedding accesses [23]. If we assume embedding vectors with 256 FP32 ele- ments, a 1MB cache can fit 1K vectors. Given the CDF of a real-world input such as the Criteo 1TB dataset [31] (column 5 in Table 1), CDF_ftr0(1K) 63 whereas CDF_ftr2(1K) 99 , which is the cache hit probability. Similarly, a 2MB cache would filter 65 99 of the total accesses. Hence, while caches are essential for DLRMs, they bring diminishing returns. Moreover, given the relatively small embedding size, prefetchers would yield limited benefits [26]. Figure 1 tests two high-performance DLRMs [43]. While dlrm_rnd has a random access distribution, dlrm_uni has a CDF similar to criteo_ftr1 in Table 1. In both cases, many accesses cannot be Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures Embedding Operation and Model Class Embedding operation reference implementation Ncompute Nlookups Memory footprint Temporal locality (reuse distance CDF) Spatial locality (vector dim.)\n\n--- Segment 7 ---\nWhile dlrm_rnd has a random access distribution, dlrm_uni has a CDF similar to criteo_ftr1 in Table 1. In both cases, many accesses cannot be Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures Embedding Operation and Model Class Embedding operation reference implementation Ncompute Nlookups Memory footprint Temporal locality (reuse distance CDF) Spatial locality (vector dim.) EmbeddingBag (EB) or Sparse-Lenghts Sum (SLS) for Deep-Learning Recommendation Models (DLRMs) (Section 2.2.1) segment in batch category in segment lookup vector accumulate vector 1 1 All embedding tables require GBs [18] to TBs [23] 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF criteo ftr2 criteo ftr1 criteo ftr0 32-256 Sparse attention (SpAttn) for Large Language Models (LLMs) (Section 2.2.2) block in Q input block in K input token in K block lookup K vector token in Q block copy K vector 0 K-V tensors are MBs Out tensor is MB-GB 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF 8 emb blk 4 emb blk 2 emb blk 1 emb blk 8x8x512 Sparse Matrix-Matrix Multiplication (SpMM) for Graph Neural Networks (GNNs) (Section 2.2.3) vertex in graph neighbor of vertex lookup vector rescale and accumulate vector 2 1 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 CDF proteins products arxiv mag 8-349 Message Passing (MP) models (Section 2.2.3) vertex in graph lookup vertex vector neighbor of vertex lookup neighbor vector multiply vertex-neighbor vectors scale resulting vector reduce into tmp multiply tmp by vertex vector accumulate resulting vector 5 2 Two embedding tables, each GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 0.4 0.5 0.6 CDF com-Youtube roadNet-CA web-Google wiki-Talk 128 Knowledge Graphs (KGs) (Section 2.2.3) sample in batch lookup vector compute head-rel-tail norm 4 3 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 CDF biokg wikikg2 512 Table 1: Characterization of the embedding operations in Section 2.2.\n\n--- Segment 8 ---\nIn both cases, many accesses cannot be Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures Embedding Operation and Model Class Embedding operation reference implementation Ncompute Nlookups Memory footprint Temporal locality (reuse distance CDF) Spatial locality (vector dim.) EmbeddingBag (EB) or Sparse-Lenghts Sum (SLS) for Deep-Learning Recommendation Models (DLRMs) (Section 2.2.1) segment in batch category in segment lookup vector accumulate vector 1 1 All embedding tables require GBs [18] to TBs [23] 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF criteo ftr2 criteo ftr1 criteo ftr0 32-256 Sparse attention (SpAttn) for Large Language Models (LLMs) (Section 2.2.2) block in Q input block in K input token in K block lookup K vector token in Q block copy K vector 0 K-V tensors are MBs Out tensor is MB-GB 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF 8 emb blk 4 emb blk 2 emb blk 1 emb blk 8x8x512 Sparse Matrix-Matrix Multiplication (SpMM) for Graph Neural Networks (GNNs) (Section 2.2.3) vertex in graph neighbor of vertex lookup vector rescale and accumulate vector 2 1 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 CDF proteins products arxiv mag 8-349 Message Passing (MP) models (Section 2.2.3) vertex in graph lookup vertex vector neighbor of vertex lookup neighbor vector multiply vertex-neighbor vectors scale resulting vector reduce into tmp multiply tmp by vertex vector accumulate resulting vector 5 2 Two embedding tables, each GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 0.4 0.5 0.6 CDF com-Youtube roadNet-CA web-Google wiki-Talk 128 Knowledge Graphs (KGs) (Section 2.2.3) sample in batch lookup vector compute head-rel-tail norm 4 3 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 CDF biokg wikikg2 512 Table 1: Characterization of the embedding operations in Section 2.2. The compute-per-lookup ratio captures code complexity.\n\n--- Segment 9 ---\nEmbeddingBag (EB) or Sparse-Lenghts Sum (SLS) for Deep-Learning Recommendation Models (DLRMs) (Section 2.2.1) segment in batch category in segment lookup vector accumulate vector 1 1 All embedding tables require GBs [18] to TBs [23] 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF criteo ftr2 criteo ftr1 criteo ftr0 32-256 Sparse attention (SpAttn) for Large Language Models (LLMs) (Section 2.2.2) block in Q input block in K input token in K block lookup K vector token in Q block copy K vector 0 K-V tensors are MBs Out tensor is MB-GB 0 200 400 600 800 1000 Reuse Distance 0.0 0.2 0.4 0.6 0.8 1.0 CDF 8 emb blk 4 emb blk 2 emb blk 1 emb blk 8x8x512 Sparse Matrix-Matrix Multiplication (SpMM) for Graph Neural Networks (GNNs) (Section 2.2.3) vertex in graph neighbor of vertex lookup vector rescale and accumulate vector 2 1 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 CDF proteins products arxiv mag 8-349 Message Passing (MP) models (Section 2.2.3) vertex in graph lookup vertex vector neighbor of vertex lookup neighbor vector multiply vertex-neighbor vectors scale resulting vector reduce into tmp multiply tmp by vertex vector accumulate resulting vector 5 2 Two embedding tables, each GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 0.2 0.3 0.4 0.5 0.6 CDF com-Youtube roadNet-CA web-Google wiki-Talk 128 Knowledge Graphs (KGs) (Section 2.2.3) sample in batch lookup vector compute head-rel-tail norm 4 3 Single embedding table GBs large 0 200 400 600 800 1000 Reuse Distance 0.0 0.1 CDF biokg wikikg2 512 Table 1: Characterization of the embedding operations in Section 2.2. The compute-per-lookup ratio captures code complexity. The memory footprint defines storage requirements for all embedding vectors.\n\n--- Segment 10 ---\nThe compute-per-lookup ratio captures code complexity. The memory footprint defines storage requirements for all embedding vectors. The temporal locality proxies cache efficiency through the Cumulative Distribution Function (ğ¶ğ·ğ¹) of the vector reuse distance. The spatial locality proxies prefetcher efficiency through the number of embedding elements in each vector. filtered by the GPU cache, and require hundreds of cycles to reach memory. As discussed in Section 2.3, GPUs cannot issue enough requests to hide such latency, only achieving 28 utilization. 2.2.2 Sparse Attention Mechanisms. Large Language Models (LLMs) embed text into vectors [11]. Transformers such as Google s Big- Bird [69] sparsify the attention mechanism to only compare a se- lected subset of input tokens and improve memory footprint and performance on long input sequences. Although this technique drastically reduces inference latency, it requires an extra step to gather embedding blocks. This gathering step is generally not fused with downstream deep neural networks [69], requiring only embed- ding lookups with no compute (SpAttn in Table 1 column 3). The execution time of this gather operation depends on many factors, and can take up to 20 if gathering, for instance, 8 random blocks per query element, as shown in Figure 1. This operation differs from a normal tf.gather [1] as it repli- cates blocks of embeddings from the key tensor into the query ten- sor. As shown in Table 1, the larger the block, the larger the spatio- temporal locality over keys (higher horizontal CDF part). However, the smaller the block, the lower the compute required.\n\n--- Segment 11 ---\nAs shown in Table 1, the larger the block, the larger the spatio- temporal locality over keys (higher horizontal CDF part). However, the smaller the block, the lower the compute required. Hence, larger blocks enable better system utilization on flop-oriented machines, Model Input Nodes Edges Layers sizes GNN arxiv [22] 0.2M 1.2M 128 256 256 40 GNN mag [22] 1.9M 21.1M 128 256 349 GNN products [22] 2.4M 61.9M 100 256 256 47 GNN proteins [22] 0.1M 39.6M 8 256 256 112 MP com-Youtube [35] 1.1M 6.0M 128 MP roadNet-CA [35] 2.0M 5.5M 128 MP web-Google [35] 0.9M 5.1M 128 MP wiki-Talk [35] 2.4M 5.0M 128 KG biokg [22] 0.1M 5.1M 512 KG wikikg2 [22] 2.5M 17.1M 512 Table 2: Typical inputs for graph-learning models. and smaller blocks enable lower inference latency on machines with more efficient memory subsystems. 2.2.3 Graph Machine Learning Models. Graph machine learning models such as Graph Neural Networks (GNNs) [22], Message Pass- ing (MP) models [53], and Knowledge Graphs (KGs) [7] embed node or edge features of a graph into vectors. GNNs, for instance, perform inference by chaining layers of deep neural networks and graph convolutions, which gather embeddings from neighbors with the SpMM-like operation shown in Table 1. These SpMM-like em- bedding operations generally have a higher compute-per-lookup Siracusa et al.\n\n--- Segment 12 ---\nGNNs, for instance, perform inference by chaining layers of deep neural networks and graph convolutions, which gather embeddings from neighbors with the SpMM-like operation shown in Table 1. These SpMM-like em- bedding operations generally have a higher compute-per-lookup Siracusa et al. 1x 20x 40x 60x 80x 100x Request latency L1D latency 0 20 40 60 80 100 of longer requests arxiv mag products proteins arxiv mag products proteins Core in-flight requests 10 12 19 10 arxiv mag products proteins Core request throughput 6 5 5 11 arxiv mag products proteins HBM2 bandwidth [ ] 1.8 2.0 2.4 1.4 (a) (b) (c) (d) Figure 3: Architectural implications of embedding lookups on a traditional CPU core (details in Figure 5b). A large frac- tion of embedding lookups in GNNs models (Section 2.2.3) take orders of magnitude longer than L1D accesses (a). For instance, more than 74 of product s lookups are 10 longer than an L1D access, and 40 more than 100 longer. How- ever, traditional CPU cores have limited memory-level par- allelism, and can only track a few in-flight lookups (b), stalling the CPU pipeline. This results in low memory re- quest throughput (c), and low HBM per core utilization (d).\n\n--- Segment 13 ---\nHow- ever, traditional CPU cores have limited memory-level par- allelism, and can only track a few in-flight lookups (b), stalling the CPU pipeline. This results in low memory re- quest throughput (c), and low HBM per core utilization (d). 0.95x 1.00x 1.05x 1.10x 1.15x Performance 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 1R.1L.1M 1R.1L.2M 1R.2L.1M 1R.2L.2M 2R.1L.1M 2R.1L.2M 2R.2L.1M 2R.2L.2M 0.80x 0.85x 0.90x 0.95x 1.00x 1.05x Perf watt arxiv mag products proteins Figure 4: Implications of scaling up the memory-level par- allelism of a traditional CPU core (details in Figure 5b) for GNNs embedding operations (Section 2.2.3). Doubling reorder buffer, load-store queue, and L1D miss-status handling regis- ters (2R.2L.2M) provides limited performance improvements and worse perf watt than off-the-shelf cores (1R.1L.1M). ratio than DLRMs, with MP having the highest.\n\n--- Segment 14 ---\nDoubling reorder buffer, load-store queue, and L1D miss-status handling regis- ters (2R.2L.2M) provides limited performance improvements and worse perf watt than off-the-shelf cores (1R.1L.1M). ratio than DLRMs, with MP having the highest. Table 2 shows typ- ical inputs for these models, with their CDFs reported in Table 1. Overall, graph-learning models often have a lower temporal locality (flatter CDF) than DLRMs and LLMs, resulting in more memory accesses. Hence, even gnn_proteins, which has the highest reuse among GNNs, only achieves 32 peak GPU performance. 2.3 Challenges on Performance Scaling Compute cores in traditional architectures such as CPUs and GPUs are optimized to access data in local caches, which are pop- ulated by CPU prefetchers or GPU DMAs [45]. However, these mechanisms are ineffective for irregular workloads such as embed- ding operations [61]. As shown in Figure 3a, this results in a high volume of long memory accesses, where up to 86 of requests are more than 10 longer than L1D accesses, and up to 36 are more than 100 longer. However, compute units such as GPU SMs and LLC HBM CPU0 L1 Core TMU CPU1 L2 OutQ 8 cores, 8 LLC slices, 1 HBM stack N1-like OoO core 2.4 GHz, 224-entries ROB, 2x96-entries LSQ, 2x64B vector units One 64Bx64B matrix unit per core 64 KiB core, 4-way, 2-cycle access, 32 MSHRs, stride IMP prefetcher 512 KiB core, 8-way, 8-cycle access, 64 MSHRs, best offset prefetcher Mostly exclusive, 1 MiB slice, 16 ways, 12-cycle access, 128 MSHRs 2D mesh, 1 cycle routers, 1 cycle links, MOESI-like AMBA 5 CHI 300 GB s HBM2 8KB storage, 256 MSHRs, 1 GHz SoC: Core: Acc: L1D: L2: LLC: NoC: HBM: TMU: (a) (b) Figure 5: A DAE processor.\n\n--- Segment 15 ---\nAs shown in Figure 3a, this results in a high volume of long memory accesses, where up to 86 of requests are more than 10 longer than L1D accesses, and up to 36 are more than 100 longer. However, compute units such as GPU SMs and LLC HBM CPU0 L1 Core TMU CPU1 L2 OutQ 8 cores, 8 LLC slices, 1 HBM stack N1-like OoO core 2.4 GHz, 224-entries ROB, 2x96-entries LSQ, 2x64B vector units One 64Bx64B matrix unit per core 64 KiB core, 4-way, 2-cycle access, 32 MSHRs, stride IMP prefetcher 512 KiB core, 8-way, 8-cycle access, 64 MSHRs, best offset prefetcher Mostly exclusive, 1 MiB slice, 16 ways, 12-cycle access, 128 MSHRs 2D mesh, 1 cycle routers, 1 cycle links, MOESI-like AMBA 5 CHI 300 GB s HBM2 8KB storage, 256 MSHRs, 1 GHz SoC: Core: Acc: L1D: L2: LLC: NoC: HBM: TMU: (a) (b) Figure 5: A DAE processor. Each traditional core offloads embedding lookup to an access unit like the TMU [61]. CPU cores are not dimensioned for such long memory access laten- cies [44]. For instance, as shown in Figure 3b and 3c, off-the-shelf CPU cores can only track a few of these long memory requests, achieving low throughput (i.e. loads cycle) and performance. Increasing the memory-level parallelism of a CPU core to track more in-flight requests requires scaling up the reorder buffer, load- store queue, and miss-status handling registers [58]. However, these resources scale inefficiently. As shown in Figure 4, doubling them only improves performance by up to 12 , with a 21 power over- head. Besides being inefficient, scaling up these resources intro- duces challenges in timing closure [59], which would require lower- ing the core s frequency and, consequently, compute performance. Scaling out the number of cores is also ineffective. As shown in Figure 3d, we would need 43 72 traditional CPU cores to saturate a single HBM2 stack.\n\n--- Segment 16 ---\nScaling out the number of cores is also ineffective. As shown in Figure 3d, we would need 43 72 traditional CPU cores to saturate a single HBM2 stack. Hence, we would need an unfeasible amount of cores to saturate the large memory bandwidth of current archi- tectures [13]. Similarly, as shown in Figure 1, current GPUs utilize 0.08 52 of the HBM bandwidth. To achieve full HBM utilization, GPUs should use 2 12 more warps, which is challenging. Hence, alternative solutions are needed. 3 The Potential of DAE Architectures In this section, we demonstrate that Decoupled Access-Execute (DAE) multicore processors that offload embedding lookup to spe- cialized units outperform GPUs in performance and perf watt. 3.1 Target DAE Architecture Figure 5 shows the target DAE processor for this study, a tradi- tional multicore processor that features one Tensor Marshaling Unit (TMU) [61] per traditional core to offload embedding lookup, and one matrix unit per core (like Arm SME [65]) to offload deep neural network computation. The TMU is an access unit specifically designed to accelerate memory access of all sparse dense tensor algebra expressions. As discussed later in Section 4, by interpreting embedding operations as tensor algebra expressions, we can pro- gram the TMU to lookup embedding vectors for the core, a SIMD CPU in this case. In all experiments, we measured performance with full-system gem5 [40] and power with McPAT [37]. Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures 1.5 1.8 2.0 Power overhead 2x 3x 4x 5x 6x 7x 8x 9x Requests s improvement arxiv mag products proteins arxiv mag products proteins Requests s watt improvement 5.9x 6.7x 8.3x 3.1x arxiv mag products proteins HBM utilization improvement 7.0x 7.6x 6.8x 3.7x (a) (b) (c) Figure 6: Performance and efficiency of a DAE core over a traditional core (details in Section 3.1) on GNN embedding operations (Section 2.2.3).\n\n--- Segment 17 ---\nIn all experiments, we measured performance with full-system gem5 [40] and power with McPAT [37]. Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures 1.5 1.8 2.0 Power overhead 2x 3x 4x 5x 6x 7x 8x 9x Requests s improvement arxiv mag products proteins arxiv mag products proteins Requests s watt improvement 5.9x 6.7x 8.3x 3.1x arxiv mag products proteins HBM utilization improvement 7.0x 7.6x 6.8x 3.7x (a) (b) (c) Figure 6: Performance and efficiency of a DAE core over a traditional core (details in Section 3.1) on GNN embedding operations (Section 2.2.3). Access units like the TMU can issue and track more memory requests at low power consumption (a), achieving high efficiency (b) and HBM utilization (c). Property Description RM1 RM2 RM3 Segments per batch per core 64 32 16 Embedding entries per table 16K 16K 16K Elements per embedding vector 32 64 128 Tables per core 2 2 2 Lookups per segment 64 128 256 Table 3: Tested DLRM models. 3.2 Architectural Advantage of DAE Designs The main advantage of DAE designs is that, by decoupling embed- ding lookup and computation into two distinct units, these units can be specialized and optimized independently. For instance, as the TMU implements all the logic to traverse and load tensor operands using specialized dataflow hardware, it can issue memory requests with higher throughput and efficiency than the core [61]. Moreover, as the TMU can run at a lower frequency, it can track 8 more outstanding requests, with no timing issues, and low power consumption (less than 2 overhead) [61]. Hence, as shown in Figure 6a and 6b, the TMU achieves 5.7 and 5.6 higher requests s and requests s watt than traditional cores, 5.2 and 6.3 better than doubling core s outstanding requests. In this way, as shown in Figure 6c, the TMU utilizes 4 8 more memory bandwidth than a traditional core, requiring smaller and fewer cores to saturate the processor s HBM bandwidth.\n\n--- Segment 18 ---\nHence, as shown in Figure 6a and 6b, the TMU achieves 5.7 and 5.6 higher requests s and requests s watt than traditional cores, 5.2 and 6.3 better than doubling core s outstanding requests. In this way, as shown in Figure 6c, the TMU utilizes 4 8 more memory bandwidth than a traditional core, requiring smaller and fewer cores to saturate the processor s HBM bandwidth. From the core s perspective, offloading embedding lookup to the TMU allows to fully utilize the core s resources for compute opera- tions like embedding reductions. In the end, as shown in Figure 7, decoupling embedding lookup from computation on multicore pro- cessors achieves an average 5.8 speedup on the embedding opera- tions in Section 2.2. These speedups are mostly proportional to the temporal locality and compute-per-lookup ratio of the model, all the way to 17 improvements for SpAttn which has no compute and can be fully offloaded to the TMU. 3.3 Impact of DAE in Datacenters In the remainder of this section, we demonstrate that DAE multi- core processors outperform GPUs in end-to-end GNN models. We compare performance against an Nvidia T4 GPU [47], as it offers similar peak memory bandwidth and computational performance com-Youtube roadNet-CA web-Google wiki-Talk arxiv mag products proteins bioKg wikiKg2 RM1 L0 RM1 L1 RM1 L2 RM2 L0 RM2 L1 RM2 L2 RM3 L0 RM3 L1 RM3 L2 1 emb blk 2 emb blk 4 emb blk 8 emb blk 0x 2x 4x 6x 8x 10x 12x Speedup of DAE optimizations 17.0 MP SpMM(GNN) KG SLS(DLRM) SpAttn(LLM) Figure 7: Performance benefit of offloading embedding lookup to a near-core access unit like the TMU (details in Section 3.1). All embedding operations are high-performance multicore implementations from the literature (Section 2). For graph-learning models, the inputs and feature sizes are reported in Table 2.\n\n--- Segment 19 ---\nAll embedding operations are high-performance multicore implementations from the literature (Section 2). For graph-learning models, the inputs and feature sizes are reported in Table 2. For DLRMs, we tested the three config- urations in Table 3, each one running inputs with low (L0), medium (L1), and high (L2) locality [18]. For LLMs, we tested the original BigBird setting [69] while varying the blocks sizes. Overall, offloading lookup to a specialized access unit improves performance of embedding operations by 5.8 . of our DAE multicore processor, and evaluate performance per watt against an Nvidia H100 [45], a widely used solution in datacenters. We focus on GNNs models as (1) their GPU implementations use high-performance libraries [22], (2) their input datasets have var- ious localities, and (3) they pose unique architectural challenges. As GNNs alternate layers of embedding operations and Deep Neu- ral Networks (DNNs), these two operations need to be computed on the same device, as offloading DNNs to a compute accelerator would incur prohibitive host-to-device transfers. Figure 8a shows the inference latency breakdown of GNN mod- els on the DAE processor and the Nvidia T4 GPU [47]. Although the two systems have the same peak memory bandwidth, the DAE system utilizes 4.6 higher bandwidth, executing embedding oper- ations 1.6 6.3 faster than the T4 GPU. As the two systems have similar peak compute, the DNN layers have similar execution time. Hence, the DAE processor achieves 2.6 higher end-to-end perfor- mance than the T4 GPU. As the DAE processor saturates memory bandwidth with only 8 cores, it consumes less power than the T4 GPU, and achieves 6.4 higher perf watt, as shown in Figure 8b. For the same reasons, Figure 8c demonstrates that the DAE processor also achieves 4 higher perf watt than Nvidia s H100 datacenter GPU [45], which employs a smaller process node than the T4. 4 DAE Abstraction for Embedding Operations So far, we have demonstrated the benefits of decoupling embedding lookup from computation. However, such optimization requires a different programming model.\n\n--- Segment 20 ---\n4 DAE Abstraction for Embedding Operations So far, we have demonstrated the benefits of decoupling embedding lookup from computation. However, such optimization requires a different programming model. In this section, we formalize the DAE programming model with the Decoupled Lookup-Compute (DLC) IR, a low-level programming abstraction for embedding operations on general DAE designs. Similarly to the LLVM IR[32], the DLC IR provides an abstraction to separate target-agnostic optimizations Siracusa et al. dae gpu dae gpu dae gpu Inference Latency w.r.t. T4 GPU 62 100 41 100 23 100 Better mag proteins arxiv Other Elemental DNN Embedding mag proteins arxiv Perf Watt w.r.t. T4 GPU 4.3x 6.4x 9.4x Better mag proteins arxiv products Perf Watt w.r.t. H100 GPU 2.8x 3.8x 5.2x 4.4x Better (a) (b) (c) Figure 8: Performance and efficiency comparison of DAE pro- cessors w.r.t. GPUs on OGB GNNs models [22] (products did not fit on T4 GPU). For the DAE multicore processor, we mea- sured inference latency with gem5. For the T4 GPU, we used Nvidia Nsys [46], and only included the kernel execution time and no host-to-device transfer. GPU power consump- tion is measured with the torch.cuda.power_draw function whereas the power consumption of the DAE processor is (over)estimated by utilizing the TDP of similar traditional processors [42]. DAE processors outperform GPUs on both inference latency and performance watt. Control queue Data queue Access unit Dataflow lookup code Execute unit Memory Imperative compute code Figure 9: DAE architectural abstraction. from target-specific code generation for DAE designs. As discussed in Section 5, the DLC IR greatly simplifies the Ember design. Figure 9 shows our abstract DAE architecture, which is com- posed of an access unit to load embedding vectors, an execute unit for computation, and a data queue and control queue to stream data and commands from the access unit to the execute unit.\n\n--- Segment 21 ---\nAs discussed in Section 5, the DLC IR greatly simplifies the Ember design. Figure 9 shows our abstract DAE architecture, which is com- posed of an access unit to load embedding vectors, an execute unit for computation, and a data queue and control queue to stream data and commands from the access unit to the execute unit. To map embedding operations to such abstraction, we observe that general embedding operations like the ones in Table 1 are all variants of sparse-dense tensor operations. For instance, the SLS function in Figure 10a can be interpreted as a sparse-dense matrix multiplica- tion (SpMM) ğ‘ğ‘–,ğ‘— ğ´ğ‘–,ğ‘˜ğµğ‘˜,ğ‘—, with an ğ‘–ğ‘˜ğ‘—loop schedule, and sparse matrix stored in a Compressed Sparse-Row (CSR) format [6]. For the example in Figure 2, dimension ğ‘–of the sparse matrix ğ´contains the sentences in a batch whereas dimension ğ‘˜contains the possible words in a sentence. ğ´ğ‘ğ‘ 1 if the sentence ğ‘contains the word ğ‘, and 0 otherwise. The CSR format stores all the indices of the non-zero elements and the pointers where each sentence begins. Multiplying the sparse matrix ğ´for the dense matrix ğµaccumulates all embedding vectors of the words in each sentence, which is the SLS code in Figure 10b. Non-zero values different than one rescale the embedding vector, which is needed in GNNs (Section 2.2.3). MP models are a Sampled-Dense-Dense Matrix Multiplication (SD- DMM) fused with an SpMM kernel [53]. KGs are SLS functions that use semirings general algebraic structures with addition and ptrs vec: AI BJ CK DL 0 2 3 idxs vec: 0 b_tr vals mtx: out mtx: s_tr e_tr 2 5 I J K L M N O P Q R S T U V W X E F G H A B C D U V W X Access unit Execute unit Queues (a) Example of SLS function.\n\n--- Segment 22 ---\nMP models are a Sampled-Dense-Dense Matrix Multiplication (SD- DMM) fused with an SpMM kernel [53]. KGs are SLS functions that use semirings general algebraic structures with addition and ptrs vec: AI BJ CK DL 0 2 3 idxs vec: 0 b_tr vals mtx: out mtx: s_tr e_tr 2 5 I J K L M N O P Q R S T U V W X E F G H A B C D U V W X Access unit Execute unit Queues (a) Example of SLS function. 1 void sls(idxs, ptrs, vals, out){ 2 for(int b 0; b num_batches; b ){ Batch traversal (b_tr) 3 for(int p ptrs[b]; p ptrs[b 1]; p ){ Segment traversal (s_tr) 4 idx idxs[p]; Load embedding index 5 for(int e 0; e emb_len; e ){ Embedding vector traversal (e_tr) 6 val vals[p,e]; Load embedding element 7 out[b,e] val; }}}} Reduce embedding elements (b) Imperative code.\n\n--- Segment 23 ---\nKGs are SLS functions that use semirings general algebraic structures with addition and ptrs vec: AI BJ CK DL 0 2 3 idxs vec: 0 b_tr vals mtx: out mtx: s_tr e_tr 2 5 I J K L M N O P Q R S T U V W X E F G H A B C D U V W X Access unit Execute unit Queues (a) Example of SLS function. 1 void sls(idxs, ptrs, vals, out){ 2 for(int b 0; b num_batches; b ){ Batch traversal (b_tr) 3 for(int p ptrs[b]; p ptrs[b 1]; p ){ Segment traversal (s_tr) 4 idx idxs[p]; Load embedding index 5 for(int e 0; e emb_len; e ){ Embedding vector traversal (e_tr) 6 val vals[p,e]; Load embedding element 7 out[b,e] val; }}}} Reduce embedding elements (b) Imperative code. 1 tu b_tr loop_tr(0, num_batches, 1) iterate thru batch 2 str beg_ptr b_tr.mem_str(ptrs, b_tr.ite) load segment beg ptr 3 str end_pos b_tr.alu_str(' ', b_tr.ite, 1) compute next ptr pos 4 str end_ptr b_tr.mem_str(ptrs, end_pos) load segment end ptr 5 6 tu s_tr loop_tr(beg_ptr, end_ptr, 1) iterate thru segment 7 str emb_idx s_tr.mem_str(s_tr, idxs, s_tr.ite) load embedding index 8 str emb_beg s_tr.alu_str(s_tr, ' ', emb_idx, emb_len) compute emb addr 9 10 tu e_tr loop_tr(0, emb_len, 1) iterate thru emb vec 11 str emb_pos alu_str(' ', emb_beg, e_tr.ite) compute element addr 12 str emb_val mem_str(vals, emb_pos) load emb element 13 14 push_op(b_tr.ite, e_tr, ite) push batch position 15 push_op(e_tr.ite, e_tr, ite) push emb el position 16 push_op(emb_val, e_tr, ite) push emb el value 17 callback(e_tr, ite) trigger call on every emb iter (c) DLC dataflow lookup code.\n\n--- Segment 24 ---\n1 void sls(idxs, ptrs, vals, out){ 2 for(int b 0; b num_batches; b ){ Batch traversal (b_tr) 3 for(int p ptrs[b]; p ptrs[b 1]; p ){ Segment traversal (s_tr) 4 idx idxs[p]; Load embedding index 5 for(int e 0; e emb_len; e ){ Embedding vector traversal (e_tr) 6 val vals[p,e]; Load embedding element 7 out[b,e] val; }}}} Reduce embedding elements (b) Imperative code. 1 tu b_tr loop_tr(0, num_batches, 1) iterate thru batch 2 str beg_ptr b_tr.mem_str(ptrs, b_tr.ite) load segment beg ptr 3 str end_pos b_tr.alu_str(' ', b_tr.ite, 1) compute next ptr pos 4 str end_ptr b_tr.mem_str(ptrs, end_pos) load segment end ptr 5 6 tu s_tr loop_tr(beg_ptr, end_ptr, 1) iterate thru segment 7 str emb_idx s_tr.mem_str(s_tr, idxs, s_tr.ite) load embedding index 8 str emb_beg s_tr.alu_str(s_tr, ' ', emb_idx, emb_len) compute emb addr 9 10 tu e_tr loop_tr(0, emb_len, 1) iterate thru emb vec 11 str emb_pos alu_str(' ', emb_beg, e_tr.ite) compute element addr 12 str emb_val mem_str(vals, emb_pos) load emb element 13 14 push_op(b_tr.ite, e_tr, ite) push batch position 15 push_op(e_tr.ite, e_tr, ite) push emb el position 16 push_op(emb_val, e_tr, ite) push emb el value 17 callback(e_tr, ite) trigger call on every emb iter (c) DLC dataflow lookup code. 0,0,A 0,1,B 0,2,C 0,3,D 0,0,I 0,1,J 0,2,K 0,3,L 1,0,U 1,1,V 1,2,W 1,3,X ei ei ei ei ei ei ei ei ei ei ei ei done ctrlQ: dataQ: queue pop (d) DAE queue content.\n\n--- Segment 25 ---\n1 tu b_tr loop_tr(0, num_batches, 1) iterate thru batch 2 str beg_ptr b_tr.mem_str(ptrs, b_tr.ite) load segment beg ptr 3 str end_pos b_tr.alu_str(' ', b_tr.ite, 1) compute next ptr pos 4 str end_ptr b_tr.mem_str(ptrs, end_pos) load segment end ptr 5 6 tu s_tr loop_tr(beg_ptr, end_ptr, 1) iterate thru segment 7 str emb_idx s_tr.mem_str(s_tr, idxs, s_tr.ite) load embedding index 8 str emb_beg s_tr.alu_str(s_tr, ' ', emb_idx, emb_len) compute emb addr 9 10 tu e_tr loop_tr(0, emb_len, 1) iterate thru emb vec 11 str emb_pos alu_str(' ', emb_beg, e_tr.ite) compute element addr 12 str emb_val mem_str(vals, emb_pos) load emb element 13 14 push_op(b_tr.ite, e_tr, ite) push batch position 15 push_op(e_tr.ite, e_tr, ite) push emb el position 16 push_op(emb_val, e_tr, ite) push emb el value 17 callback(e_tr, ite) trigger call on every emb iter (c) DLC dataflow lookup code. 0,0,A 0,1,B 0,2,C 0,3,D 0,0,I 0,1,J 0,2,K 0,3,L 1,0,U 1,1,V 1,2,W 1,3,X ei ei ei ei ei ei ei ei ei ei ei ei done ctrlQ: dataQ: queue pop (d) DAE queue content. 1 while(ctrlQ.pop() !\n\n--- Segment 26 ---\n0,0,A 0,1,B 0,2,C 0,3,D 0,0,I 0,1,J 0,2,K 0,3,L 1,0,U 1,1,V 1,2,W 1,3,X ei ei ei ei ei ei ei ei ei ei ei ei done ctrlQ: dataQ: queue pop (d) DAE queue content. 1 while(ctrlQ.pop() ! done){ until some element 2 b dataQ.pop 1 x i32 (); read embedding position 3 e dataQ.pop 1 x i32 (); read element position 4 v dataQ.pop 1 x f32 (); read element value 5 fma( out[b,e],v); compute and store (e) DLC imperative compute code. Figure 10: DAE abstraction for the SLS operation. multiplication and just have one non-zero per row, not requiring segment pointers or lengths. SpAttn is similar to KG but with a blocked format and no compute. The Sparse Abstract Machine (SAM) [21] proves that tensor alge- bra operations, and hence embedding operations, can be expressed as a dataflow chain of access blocks and compute blocks. Hence, we can map lookup blocks to the access unit and compute blocks to the execute unit, and stream data and commands through the queues [61]. The DLC IR represents lookup blocks with streaming dataflow code, as it maps well to spatial architectures specialized for irregular memory-intensive workloads [12, 21, 44, 49, 57, 59, 61, 67]. Compute blocks, instead, are represented with imperative code as it can implement all the compute variants introduced above, and others (e.g., quantization and approximate computing). Figure 10 shows an example of The DLC IR for the SLS function, which is discussed in the reminder of this section. The DLC lookup code loads embedding elements through mem- ory streams. The iteration space for these memory streams is con- tained in index streams, which are generated by traversal operators and can be transformed with integer ALU streams.\n\n--- Segment 27 ---\nThe DLC lookup code loads embedding elements through mem- ory streams. The iteration space for these memory streams is con- tained in index streams, which are generated by traversal operators and can be transformed with integer ALU streams. Formally: Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures PyTorch Structured Imperative (MLIR scf, memref, arith) Structured Lookup-Compute (SLC) IR Imperative Compute IR Access Unit (e.g. TMU) Code Execute Unit (e.g. CPU) Code Optimization (Sec. 7) Lowering (Sec. 6.2) torch-mlir, MPACT Lowering (Sec. 6.3) Codegen (Sec. 6.3) Ember (Sec. 5) Sec. 6.1 Decoupled Lookup-Compute (DLC) IR (Sec 4) Dataflow Lookup IR Figure 11: Overview of Ember. Yellow represents compiler inputs outputs and blue represents new contributions. loop_tr(lb,ub,stride): traverses the iteration space i lb; i ub;i stride, where lb and ub are streams or immedi- ate values and stride is an immediate value. The stream loop_tr.0 contains the induction variable. mem_str(base,idx): loads into a stream the values of the base memory locations indexed by the idx stream. alu_str(op,op1,op2): computes an integer binary opera- tion op { , , , } on operands op1 and op2, which can either be streams or immediate values. Lines 1-12 in Figure 10c represent lines 2-6 in Figure 10b. Compute code consists of callbacks that the access unit triggers while traversing and loading tensor operands. In Figure 10b, we can wrap the FMA operation in line 7 into a callback that the access unit can trigger at each iteration of its parent loop. To trigger such callback, the access unit marshals its corresponding token (the inner-loop iteration token (ğ‘’ğ‘–)) in the control queue and its operands (result coordinatesğ‘and ğ‘’and value ğ‘£) in the data queue.\n\n--- Segment 28 ---\nIn Figure 10b, we can wrap the FMA operation in line 7 into a callback that the access unit can trigger at each iteration of its parent loop. To trigger such callback, the access unit marshals its corresponding token (the inner-loop iteration token (ğ‘’ğ‘–)) in the control queue and its operands (result coordinatesğ‘and ğ‘’and value ğ‘£) in the data queue. Line 14-17 in Figure 10c and line 2-4 in Figure 10e show how these tokens and data are pushed and popped from the queues, whose content is shown in Figure 10d. Formally, we can program the access unit to marshal control tokens and operands at each iteration, begin, and end of a fiber traversal through the following operations: push_op(s_id,tu_id,event): pushes into the data queue (dataQ) the content of the s_id stream every time a traver- sal event {beg, ite, end} occurs in tu_id. callback(tu_id,event): pushes into the control queue (ctrlQ) the corresponding control token every time a tra- versal event {beg, ite, end} occurs in tu_id. whereas the execute unit reads tokens and operands with: pop(): pops a control token from the data queue. pop ty (): pops a value with type ty from the data queue. Figure 14d shows a more complex example with multiple callbacks. In the remainder of this paper, we demonstrate how Ember gener- ates optimized DLC code from PyTorch and TensorFlow embedding operations, enabling full DAE potential at no programmability cost. 5 Ember Overview Figure 11 shows an MLIR [33] implementation of Ember.\n\n--- Segment 29 ---\nIn the remainder of this paper, we demonstrate how Ember gener- ates optimized DLC code from PyTorch and TensorFlow embedding operations, enabling full DAE potential at no programmability cost. 5 Ember Overview Figure 11 shows an MLIR [33] implementation of Ember. Overall, Ember compiles PyTorch and TensorFlow embedding operations into optimized DLC code (Section 4), which is then used to generate Any CPU Statement ğ‘†ğ‘‡ğ‘€ğ‘‡ Memory Reference ğ‘šğ‘Ÿğ‘’ğ‘“ CPU variable ğ‘£ğ‘ğ‘Ÿ Stream Variable ğ‘  Unsigned Integer uint For-Loop ğ¹ğ‘‚ğ‘… :: for (ğ»ğ¸ğ´ğ·) { ğ‘†ğ·ğ¸ğ¶ ğµğ‘‚ğ·ğ‘Œ} For Header ğ»ğ¸ğ´ğ· :: ğ‘  ğ‘Ÿto ğ‘Ÿstep uint Stream Declaration ğ‘†ğ·ğ¸ğ¶ :: ğ‘  ğ‘ ğ‘’ Loop Body ğµğ‘‚ğ·ğ‘Œ :: ğ¶ğ´ğ¿ğ¿ ğ¶ğ´ğ¿ğ¿? ğ¹ğ‘‚ğ‘…ğ¶ğ´ğ¿ğ¿?\n\n--- Segment 30 ---\nOverall, Ember compiles PyTorch and TensorFlow embedding operations into optimized DLC code (Section 4), which is then used to generate Any CPU Statement ğ‘†ğ‘‡ğ‘€ğ‘‡ Memory Reference ğ‘šğ‘Ÿğ‘’ğ‘“ CPU variable ğ‘£ğ‘ğ‘Ÿ Stream Variable ğ‘  Unsigned Integer uint For-Loop ğ¹ğ‘‚ğ‘… :: for (ğ»ğ¸ğ´ğ·) { ğ‘†ğ·ğ¸ğ¶ ğµğ‘‚ğ·ğ‘Œ} For Header ğ»ğ¸ğ´ğ· :: ğ‘  ğ‘Ÿto ğ‘Ÿstep uint Stream Declaration ğ‘†ğ·ğ¸ğ¶ :: ğ‘  ğ‘ ğ‘’ Loop Body ğµğ‘‚ğ·ğ‘Œ :: ğ¶ğ´ğ¿ğ¿ ğ¶ğ´ğ¿ğ¿? ğ¹ğ‘‚ğ‘…ğ¶ğ´ğ¿ğ¿? Callback ğ¶ğ´ğ¿ğ¿ :: callback() { ğ‘‡ğ‘‰ğ´ğ¿ ğ‘†ğ‘‡ğ‘€ğ‘‡ } Value Conversion ğ‘‡ğ‘‰ğ´ğ¿ :: ğ‘£ğ‘ğ‘Ÿ to_val(ğ‘ ) Loop Range ğ‘Ÿ :: ğ‘  ğ‘£ğ‘ğ‘Ÿ uint Stream Expression ğ‘ ğ‘’ :: ğ‘™ğ‘  ğ‘ğ‘  ğ‘ğ‘  ğ‘ğ‘  Load Stream ğ‘™ğ‘  :: load_str(ğ‘šğ‘Ÿğ‘’ğ‘“[ğ‘–ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  ],ğ‘¢ğ‘–ğ‘›ğ‘¡) Indices ğ‘–ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  :: ğ‘  ğ‘£ğ‘ğ‘Ÿ uint ALU Stream ğ‘ğ‘  :: alu_str(ğ‘ ğ‘œğ‘,ğ‘ 1,ğ‘ 2) Stream Ops ğ‘ ğ‘œğ‘ :: .\n\n--- Segment 31 ---\nğ¹ğ‘‚ğ‘…ğ¶ğ´ğ¿ğ¿? Callback ğ¶ğ´ğ¿ğ¿ :: callback() { ğ‘‡ğ‘‰ğ´ğ¿ ğ‘†ğ‘‡ğ‘€ğ‘‡ } Value Conversion ğ‘‡ğ‘‰ğ´ğ¿ :: ğ‘£ğ‘ğ‘Ÿ to_val(ğ‘ ) Loop Range ğ‘Ÿ :: ğ‘  ğ‘£ğ‘ğ‘Ÿ uint Stream Expression ğ‘ ğ‘’ :: ğ‘™ğ‘  ğ‘ğ‘  ğ‘ğ‘  ğ‘ğ‘  Load Stream ğ‘™ğ‘  :: load_str(ğ‘šğ‘Ÿğ‘’ğ‘“[ğ‘–ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  ],ğ‘¢ğ‘–ğ‘›ğ‘¡) Indices ğ‘–ğ‘›ğ‘‘ğ‘–ğ‘ğ‘’ğ‘  :: ğ‘  ğ‘£ğ‘ğ‘Ÿ uint ALU Stream ğ‘ğ‘  :: alu_str(ğ‘ ğ‘œğ‘,ğ‘ 1,ğ‘ 2) Stream Ops ğ‘ ğ‘œğ‘ :: . . . Figure 12: The Structured Lookup-Compute (SLC) IR. target-specific access and execute code. However, the DLC IR al- ready separates access code and execute code, which communicate through queue (de)serialization. This breaks the control flow and data flow of the input application, hindering global optimizations such as vectorization or code motion across access and execute code. Ember overcomes such issue with the Structured Lookup-Compute (SLC) IR (Section 6.1). Structured IRs represent for-loops as structured operations, which facilitates analysis and optimizations of complex code like embed- ding operations. The Structured Control Flow (SCF) IR is perhaps the most popular structured IR in MLIR, and resembles the impera- tive code in Figure 10b.\n\n--- Segment 32 ---\nStructured IRs represent for-loops as structured operations, which facilitates analysis and optimizations of complex code like embed- ding operations. The Structured Control Flow (SCF) IR is perhaps the most popular structured IR in MLIR, and resembles the impera- tive code in Figure 10b. Ember s SLC IR is the natural extension of the SCF IR for DAE code, and is specifically designed to facilitate global optimizations across access and execute code. Hence, Ember first generates the SCF IR from PyTorch embed- ding operations or general tensor algebra expressions with tools like torch-mlir [39] or MPACT [17], respectively. Then, Ember lowers SCF code to the SLC IR and performs global optimizations (Sec- tion 6.2 and 7). After lowering to the DLC IR (Section 6.3), access and execute code are further optimized independently, and lowered to target-specific IRs such as the tmu and llvm dialect, which are then used for code generation. All these steps are presented in the following sections. 6 SCF decoupling and lowering to from SLC IR This section introduces the SLC IR, and how Ember lowers SCF and DLC code to and from it, respectively. 6.1 The SLC IR Figure 12 shows the SLC grammar whereas Figure 13b shows the SLC IR of the SLS function. The SLC IR has a similar structure to SCF code, besides (1) SCF loops, index arithmetic, and load oper- ations to offload to the access unit are represented as SLC loops and streams and (2) compute code for the execute unit is wrapped into SLC callbacks within the SLC loops. In this way, callbacks can access data from streams with stream-to-value conversion opera- tions, preserving the data flow of the embedding operation which is no longer (de)serialized through queues. This enables Ember to simultaneously optimize lookup and compute code through global analyses and transformations, including dominance analysis, vec- torization, and code motion across different compute regions as well as across access and execute code, as discussed in Section 7. Siracusa et al. 1 void sls(idxs: mref ? x idx , 2 ptrs: mref ? x idx , 3 vals: mref ? x f32 , 4 out: mref ? x ?\n\n--- Segment 33 ---\nx f32 , 4 out: mref ? x ? x f32 ){ 5 for(idx b 0; b n_batches; b ){ 6 idx beg ptrs[b]; 7 idx end ptrs[b 1]; 8 for(idx p beg; p end; p ){ 9 idx i idxs[ptr]; 10 for(idx e 0; e emb_len; e ){ 11 f32 val vals[i,e]; 12 f32 acc out[b,e]; 13 out[b,e] acc val; }}}} (a) SCF IR 1 void sls(idxs: mref ? x idx , 2 ptrs: mref ? x idx , 3 vals: mref ? x f32 , 4 out: mref ? x ? x f32 ){ 5 slc.for(str s_b from 0 to n_batches){ 6 str s_beg slc.mem_str(ptrs[s_b]); 7 str s_end slc.mem_str(ptrs[s_b 1]); 8 slc.for(str s_p from s_beg to s_end){ 9 str s_i slc.mem_str(idxs[s_p]); 10 slc.for(str s_e from 0 to emb_len){ 11 str s_val slc.mem_str(vals[s_i,s_e]); 12 slc.callback{ 13 idx b slc.to_val(s_b); 14 idx e slc.to_val(s_e); 15 f32 val slc.to_val(s_val); 16 f32 acc out[b,e]; 17 out[b,e] acc val; }}}}} (b) SLC IR Figure 13: Lowering step from SCF IR to SLC IR. Ember con- verts SCF for-loops into SLC for-loops and SCF memory ac- cesses (loads and index computation) into SLC streams. Em- ber wraps computation into SLC callbacks that can access SLC streams with stream-to-value operations. 6.2 Lowering the SCF IR to the SLC IR Figure 13 shows a lowering example from SCF to SLC.\n\n--- Segment 34 ---\nEm- ber wraps computation into SLC callbacks that can access SLC streams with stream-to-value operations. 6.2 Lowering the SCF IR to the SLC IR Figure 13 shows a lowering example from SCF to SLC. To generate the SLC IR, Ember recursively traverses the loop hierarchy of the SCF code looking for loops to offload to the access unit. We define an offloading candidate as a loop that (1) has iteration bounds which are either static or computed by another offloading candidate and (2) loads from at least one read-only memory location which has not been read from a parent loop. Condition (1) is necessary as access units cannot generally read data from the execute units. Condition (2) excludes workspace loops [28, 70] (i.e. loops that just work on partial results). As partial results are likely cached, workspace loops do not benefit from memory acceleration. All loops in Figure 13a are offloading candidates. Instead, the last two MP lines in Table 1 are workspaces loops, as they just multiply and accumulate the temporary vector with the vertex vector, which has been read already. As embedding operations are variants of sparse-dense tensor multiplications (Section 4), their loop hierarchy can only have at most one offloading candidate per level and at most one workspace loop per level [28, 29]. Hence, the decoupling algorithm recursively traverses and selects one offloading candidate per level, leaving the other loops for software execution. The SCF offloading candidates are lowered to SLC for-loops. Then, Ember places callbacks in the SLC loops. Operations to be offloaded (i.e., read-only load operations and index arithmetic) are transformed into streams and moved before their correspond- ing callback. Operations not to be offloaded (compute code and workspace loops) are moved inside their corresponding callback, which is executed in software. For instance, the read-only load operation in line 11 Figure 13a is transformed and moved before the callback whereas the accumulation operations in line 12 and 13 are moved inside the callback. Finally, stream-to-value operations are added for each operation in a callback reading from a stream. 1 while(ctrlQ.pop() !\n\n--- Segment 35 ---\nFinally, stream-to-value operations are added for each operation in a callback reading from a stream. 1 while(ctrlQ.pop() ! done){ 2 b dataQ.pop 1 x index (); 3 e dataQ.pop 1 x index (); 4 v dataQ.pop 1 x f32 (); 5 fma( out[b,e],v); } 0,0,A 0,1,B 0,2,C 0,3,D 0,0,I 0,1,J 0,2,K 0,3,L ei ei ei ei ei ei ei ei ei ei ei ei done ctrlQ: dataQ: (a) Unoptimized code. 1 while(ctrlQ.pop() ! done){ 2 b dataQ.pop 1 x index (); 3 e dataQ.pop 1 x index (); 4 v dataQ.pop vlen x f32 (); 5 v_fma( out[b,e], v); 0,0,AB 0,2,CD 0,0,IJ 0,2,KL 1,0,UV 1,2,WX ei ei ei ei ei ei done ctrlQ: dataQ: (b) Vectorized code. 1 while(ctrlQ.pop() ! done){ 2 b dataQ.pop 1 x index () 3 for(e 0; e emb_len; e vlen){ 4 v dataQ.pop vlen x f32 () 5 v_fma( out[b,e],v) } ee ee ee done 0,ABCD 0,IJKL 1,UVWX ctrlQ: dataQ: (c) Bufferized code. 1 out_ptr out 2 while(tkn ctrlQ.pop() ! done){ 3 if(tkn ğ‘’ğ‘’){ 4 for(e 0; e emb_len; e vlen){ 5 v dataQ.pop vlen x f32 () 6 v_fma(out_ptr e, v) }} 7 else if(tkn ğ‘ ğ‘’){ 8 out_ptr emb_len } ee ee se ee se done ABCD IJKL UVWX ctrlQ: dataQ: (d) Queue aligned code.\n\n--- Segment 36 ---\n1 out_ptr out 2 while(tkn ctrlQ.pop() ! done){ 3 if(tkn ğ‘’ğ‘’){ 4 for(e 0; e emb_len; e vlen){ 5 v dataQ.pop vlen x f32 () 6 v_fma(out_ptr e, v) }} 7 else if(tkn ğ‘ ğ‘’){ 8 out_ptr emb_len } ee ee se ee se done ABCD IJKL UVWX ctrlQ: dataQ: (d) Queue aligned code. Figure 14: Impact of SLC optimizations on the SLS compute code and output queues. 6.3 Lowering SLC IR to DLC IR After applying optimizations (Section 7), the SLC IR is lowered to the low-level DLC IR (Section 4). The SLC IR is generated by traversing the SLC IR from the outer to the inner SLC for-loop. SLC for-loops and streams are lowered to DLC traversal operators and streams. Callbacks, instead, are moved into the while-loop in the compute code (Figure 9). Multiple callbacks are chained into an if-then-else construct that cases the token IDs popped from the control queue (e.g. Figure 14d). Token s push instructions are generated according to the location of the callback in the SLC IR. Operands push and pop instructions are generated according to the SLC stream-to-value operations. 7 Optimizing Embedding Operations This section describes three key DAE optimizations enabled by the SLC IR. Figure 15 and 14 show how these optimizations transform the SLC IR, compute code, and queues. 7.1 Vectorization Vectorization is one of the most impactful DAE optimizations [61]. As shown in Figure 14b, for each token, vectorization pops a vector of vector length (vlen) elements, improving marshaling and compute efficiency. While auto-vectorizing general code is challenging [27], the SLC IR provides an effective representation to vectorize loading, marshaling, and computing of embedding operations. As shown in Figure 15b, Ember vectorizes code by converting SLC operations into their vectorized SLCV duals.\n\n--- Segment 37 ---\nWhile auto-vectorizing general code is challenging [27], the SLC IR provides an effective representation to vectorize loading, marshaling, and computing of embedding operations. As shown in Figure 15b, Ember vectorizes code by converting SLC operations into their vectorized SLCV duals. Conversely from the SLC for-loop, the SLCV for-loop (1) adds a vector length attribute, (2) instantiates vectorized induction variables, and (3) introduces the concept of mask to handle loop boundaries not multiple of Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 Access: Iterate over embedding vector elements 11 slc.for(stream s_e from 0 to emb_len){ 12 stream s_val slc.mem_str(vals[s_idx,s_e]); 13 Execute: Reduce embedding vectors 14 slc.callback{ 15 index b slc.to_val(s_b); 16 index e slc.to_val(s_e); 17 f32 val slc.to_val(s_val); 18 f32 acc out[b,e]; 19 out[b,e] acc val; }}}}} (a) Unoptimized code. 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ?\n\n--- Segment 38 ---\nx f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 Access: Iterate over embedding vector elements 11 slcv.for vlen ((stream s_e, stream msk) from 0 to emb_len){ 12 stream s_val slcv.mem_str vlen (vals[s_idx,s_e], msk); 13 Execute: Reduce embedding vectors 14 slcv.callback{ 15 index b slc.to_val(s_b); 16 index e slcv.to_val(s_e)[0]; 17 vec vlen x f32 val slcv.to_val vlen (s_val); 18 vec vlen x f32 acc vload vlen (out[b,e]); 19 vstore vlen (acc val, out[b,e], acc); }}}}} (b) Vectorized code. 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ?\n\n--- Segment 39 ---\nx f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 stream vec vlen x f32 buf slcv.buf_str(); Buffer stream 11 Access: Iterate over embedding vector elements 12 slcv.for vlen ((stream s_e, stream msk) from 0 to emb_len){ 13 stream s_val slcv.mem_str vlen (vals[s_idx,s_e], msk); 14 slc.push(buf, s_val); } Push into the buffer 15 Execute: Reduce embedding vectors 16 slcv.callback{ Callback moved at the end of inner loop 17 index b slc.to_val(s_b); 18 vec vlen x f32 buf_vec slc.to_val(buf); Get buffer 19 for(index e 0; e emb_len; e ){ Iterate buffer 20 vec vlen x f32 val buf_vec[e]; Get buffered element 21 vec vlen x f32 acc vload vlen (out[b,e]); 22 vstore vlen (acc val, out[b,e], acc); }}}}}} (c) Bufferized code. 1 void sls(idxs: mref ? x index , offs: mref ? x index , 2 vals: mref ? x f32 , out: mref ? x ?\n\n--- Segment 40 ---\nx f32 , out: mref ? x ? x f32 ){ 3 Access: Iterate over segments in a batch 4 slc.for(stream s_b from 0 to num_batches){ 5 stream s_beg slc.mem_str(offs[s_b]); 6 stream s_end slc.mem_str(offs[s_b 1]); 7 Access: Iterate over embeddings in a segment 8 slc.for(stream s_ptr from s_beg to s_end){ 9 stream s_idx slc.mem_str(idxs[s_ptr]); 10 stream vec vlen x f32 buf slcv.buf_str(); 11 Access: Iterate over embedding vector elements 12 slcv.for vlen ((stream s_e, stream msk) from 0 to emb_len)(index i 0) { 13 stream s_val slcv.mem_str vlen (vals[s_idx,s_e], msk); 14 slc.push(buf, s_val); } 15 Execute: Reduce embedding vectors 16 slcv.callback{ 17 vec vlen x f32 buf_vec slc.to_val(buf); 18 for(index e 0; e emb_len; e ){ 19 vec vlen x f32 val buf_vec[e]; 20 vec vlen x f32 acc vload vlen (out[i,e]); 21 vstore vlen (acc val, out[i,e], acc); }}}} 22 slc.callback{ i ; }}} end of indices in segment loop (d) Queue aligned code. Figure 15: Progressive optimization of the SLS SLC IR. the vector length. Masks are used by SLCV streams to perform vectorized index computation and data loading. We define a vectorization scheme as the set of for-loops within a parent loop ğ‘and the inner loop ğ‘–, with ğ‘ ğ‘–, that we intend to vectorize. A for-loop can be vectorized if and only if all of its callbacks can be vectorized. A vectorization scheme is legal if and only if all of its for-loops can be vectorized.\n\n--- Segment 41 ---\nA for-loop can be vectorized if and only if all of its callbacks can be vectorized. A vectorization scheme is legal if and only if all of its for-loops can be vectorized. Vector extensions such as Arm SVE [62] provide instructions to vectorize most callbacks in embedding operations, making the space of legal vectorization schemes large. However, prior work has demonstrated that the most efficient vectorization scheme for sparse-dense tensor multiplication is inner-loop vectorization, as- suming the dense tensor is in row-major order and has rows which are larger than the vector length [61]. Embedding operations gen- erally satisfy these assumptions (Section 2). Hence, similarly to the MLIR sparsifier [8], Ember only attempts inner-loop vectorization. If the inner for-loop is legal, Ember vectorizes its access and execute code in two steps. Ember starts by vectorizing the inner for-loop and its streams. As the stream-to-value operations in the callbacks expect stream types, the algorithm adds a temporary cast operation for source materialization. Then, during the callback vec- torization step, Ember recursively vectorizes the uses of these cast operations, producing full SLCV code. To keep the conversion step simple, load and store operations into callbacks are firstly converted to vector gather and scatter operations, with vector indices and masks coming from its parent vectorized for-loop operation. A fur- ther transformation pass simplifies these operations into contiguous vector loads and stores. 7.2 Bufferization Bufferization allows to marshal and compute embedding vectors as compound types. As shown in Figure 14c, the access unit pushes, in the control queue, one ğ‘’ğ‘’(embedding-vector end) token for each embedding vector and, in the data queue, the position of the output embedding vector and all of its values. As the length of the embedding vector (emb_len) is constant, once the core reads an ğ‘’ğ‘’token, it pops emb_len elements with a vectorized for-loop. Bufferization greatly improves marshaling and compute efficiency, especially for long embedding vectors.\n\n--- Segment 42 ---\nAs the length of the embedding vector (emb_len) is constant, once the core reads an ğ‘’ğ‘’token, it pops emb_len elements with a vectorized for-loop. Bufferization greatly improves marshaling and compute efficiency, especially for long embedding vectors. As shown in Figure 15c, Ember bufferizes code by firstly initial- izing a buffer stream of vector type (line 10) before the inner SLCV for-loop, where such loop can push the loaded embedding elements (line 14). Then, Ember moves the inner-loop body callback (line 14-19 in Figure 15b) right after the loop (line 16-22 in Figure 15c), adds a stream-to-value operation for the buffer (line 18), and adds a loop to iterate through it (line 19-22). 7.3 Queue alignment Vector loads are more efficient when aligned to cache lines. How- ever, as shown in Figure 14c, scalar operands like segment IDs hinder alignment of embedding vectors. Ember tries to align em- bedding accesses with different optimizations depending on the code characteristics. For simpler functions like the SLS in Figure 15c where all segment IDs are just loop induction variables, Ember stores a reference of Siracusa et al. Name MLIR dialects Description emb-opt0 slc, scf, memref, arith unoptimized Ember DAE code emb-opt1 slcv, scf, memref, arith, vector emb-opt0 vectorization emb-opt2 slcv, scf, memref, arith, vector emb-opt1 bufferization emb-opt3 slcv, scf, memref, arith, vector emb-opt2 queue alignment ref-dae tmu, scf, memref, arith, vector hand-optimized TMU-CPU code Table 4: Evaluated code and reference. Ember s input is torch- mlir [39], output is TMU-CPU machine code. these indices in the core and increments them after the loop. This increment is triggered by a segment-end token, ğ‘ ğ‘’in Figure 14d. As shown in Figure 15d, Ember implement such optimization by looking into iteration callbacks for stream-to-value operations that just read the induction variable of their own loop (e.g.\n\n--- Segment 43 ---\nThis increment is triggered by a segment-end token, ğ‘ ğ‘’in Figure 14d. As shown in Figure 15d, Ember implement such optimization by looking into iteration callbacks for stream-to-value operations that just read the induction variable of their own loop (e.g. line 17 in Figure 15c). Then Ember adds a new variable in the SLC loop (i variable in line 12 in Figure 15d), replaces all uses of the stream- to-value operations with that variable (line 20-21), and increments it in the end callback of its child loop (line 22). However, for more complex models like MP, certain scalars can- not be simplified (e.g. rescaling values). In this case, Ember preserves alignment by padding scalars to vectors while generating the DLC IR. As a further optimization, instead of sending segment IDs, Em- ber offloads to the access unit full index calculation of partial and output results and directly sends addresses to the core, reducing pressure on core s ALUs. 7.4 Model-Specific Optimizations While the optimization we presented so far are for general embed- ding operations, Ember can also implement model-specific opti- mizations. For instance, block-sparse attention mechanisms (Sec- tion 2.2.2) exhibit (1) large structured reuse within each block, (2) low reuse throughout blocks, and (3) no computation. Hence, we can add store streams to write directly into memory without passing through the core. Moreover, we can extend load streams to (1) select what cache level to read from and (2) whether to issue temporal or non-temporal requests. As demonstrated in Section 8.2, this greatly improves core and cache utilization. Other optimizations include, for instance, introducing toggle callbacks to trigger computation on sparse coordinate formats [52] or accumulation streams to track boundaries of SLS segments by accumulating lengths, instead of using offsets [41]. 8 Evaluation In this section, we demonstrate how Ember enables full DAE poten- tial at scale. Our evaluation firstly demonstrates the benefit of the SLC IR by performing an ablation study on the general embedding optimizations (vectorization, bufferization, and queue alignment) introduced in Section 7. Then, it demonstrates the generality of the SLC IR by showing the impact of the model-specific optimizations for LLMs introduced in Section 7.4.\n\n--- Segment 44 ---\nOur evaluation firstly demonstrates the benefit of the SLC IR by performing an ablation study on the general embedding optimizations (vectorization, bufferization, and queue alignment) introduced in Section 7. Then, it demonstrates the generality of the SLC IR by showing the impact of the model-specific optimizations for LLMs introduced in Section 7.4. Finally, it demonstrates that Ember optimizations match the performance of handwritten code specifically optimized for our TMU-CPU target system. The experimental setting in this section follows the DAE study in Section 3.2. Table 4 summarizes the code utilized in the experiments. RM1 L0 RM1 L1 RM1 L3 RM2 L0 RM2 L1 RM2 L3 RM3 L0 RM3 L1 RM3 L3 wiki-Talk roadNet-CA com-Youtube web-Google 0x 5x 10x 15x 20x 25x Opt speedup emb-opt3 emb-opt2 emb-opt1 Figure 16: Performance speedup of Ember optimizations on MP models and SLS function for various DLRM mod- els (Table 3) and input locality. All optimizations combined (emb-opt3) improve performance by 6.6 21 . 0 10 20 30 40 50 60 70 80 Compute throughput 0 20 40 60 Access throughput emb-opt0 emb-opt1 emb-opt2 emb-opt3 RM1_L0 RM1_L1 RM1_L2 RM2_L0 RM2_L1 RM2_L2 RM3_L0 RM3_L1 RM3_L2 Figure 17: Impact of Ember optimizations on access through- put (TMU) and compute throughput (CPU). By optimizing both, Ember achieves highest performance (top-right). 8.1 Impact of General Optimizations Figure 16 shows the performance impact of general embedding opti- mizations such as vectorization (emb-opt1), bufferization (emb-opt2), and queue alignment (emb-opt3) over unoptimized Ember-generated code (emb-opt0) for the SLS function and more compute-intensive MP models. For the SLS function, we evaluated the three DLRMs in Table 3, each one running representative synthetic inputs [18] with low (L0), medium (L1), and high (L2) locality.\n\n--- Segment 45 ---\n8.1 Impact of General Optimizations Figure 16 shows the performance impact of general embedding opti- mizations such as vectorization (emb-opt1), bufferization (emb-opt2), and queue alignment (emb-opt3) over unoptimized Ember-generated code (emb-opt0) for the SLS function and more compute-intensive MP models. For the SLS function, we evaluated the three DLRMs in Table 3, each one running representative synthetic inputs [18] with low (L0), medium (L1), and high (L2) locality. Overall, all optimizations com- bined (emb-opt3) achieve 6.6 , 12.1 , and 21 better performance over unoptimized code (emb-opt0) for RM1, RM2, and RM3, respec- tively. Vectorization is consistently the most impactful optimization with a 5.13 speedup and only 17 deviation, whereas other opti- mizations deliver widely-different performance improvements on different configurations. To better understand these results, Figure 17 shows how these optimizations impact the throughput at which the compute unit reads and the access unit writes into the L2 queue. Compute op- timizations move upward whereas memory optimizations move rightward. The blue line indicates where the compute-unit through- put equals the access-unit throughput. Only optimizing compute code cannot move above the blue line as the access unit would not be able to marshal enough data to process. This would only improve throughput up to 8 (RM3, emb-opt0) before the access unit starts to be the bottleneck (blue line). However, because of the SLC IR, Ember can perform global optimizations on both access and compute code and move both rightward and upward in the plot, improving performance by up to 21 (RM3, emb-opt3). For RM1, the most control-intensive model (shorter loops), vectoriza- tion already saturates throughput, leaving other optimizations little room for improvement. For RM2 and RM3, by reducing coordinate overhead, bufferization helps to move closer to the blue line.\n\n--- Segment 46 ---\nFor RM1, the most control-intensive model (shorter loops), vectoriza- tion already saturates throughput, leaving other optimizations little room for improvement. For RM2 and RM3, by reducing coordinate overhead, bufferization helps to move closer to the blue line. For Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures L3 1 L2 1 L3 2 L2 2 L3 4 L2 4 L3 8 L2 8 0 20 40 60 80 100 L3 APKE Non-temporal accesses Temporal accesses Figure 18: L3 Accesses Per Kilo Element (APKE) of the Big- Bird gather function with different block sizes (1, 2, 4, and 8) and TMU configurations. Temporal accesses load indexes and non-temporal accesses load embedding vectors. Reading from L2 substantially helps filtering LLC accesses. wiki-Talk roadNet-CA com-Youtube web-Google arxiv mag products proteins bioKg wikiKg2 RM1 L0 RM1 L1 RM1 L2 RM2 L0 RM2 L1 RM2 L2 RM3 L0 RM3 L1 RM3 L2 1 emb blk 2 emb blk 4 emb blk 8 emb blk 60 70 80 90 100 Ember vs hand-optimized code MP SpMM(GNN) KG SLS(DLRM) SpAttn(LLM) Figure 19: DAE code automatically generated and optimized by Ember (emb-opt3) achieves a geomean 99 performance compared to hand-optimized code (ref-dae) across the dif- ferent model classes in Table 1. RM3, the model with largest loops, queue alignment removes index overhead, pushing performance closer to the limit. As shown in Figure 16, MP models have similar trends, and the optimization impact is mostly proportional to their compute-per- lookup ratio (Table 1). 8.2 Impact of Model-Specific Optimizations As discussed in Section 7.4, Ember lends itself to model-specific optimizations.\n\n--- Segment 47 ---\nAs shown in Figure 16, MP models have similar trends, and the optimization impact is mostly proportional to their compute-per- lookup ratio (Table 1). 8.2 Impact of Model-Specific Optimizations As discussed in Section 7.4, Ember lends itself to model-specific optimizations. Figure 18 shows that, in block-sparse attention mech- anisms (Section 2.2.2), loading highly-reused embedding blocks from L2 rather than LLC filters 67 74 of the embedding reads and 50 65 of the overall accesses, which include non-temporal loads for indexes, with larger reductions on larger block sizes with more intrinsic reuse. By directly writing data with the store streams instead of going through the core, Ember enables efficient gather operations with low resource utilization. 8.3 Comparison with Hand-optimized Code Figure 19 shows the performance of DAE code automatically gen- erated and optimized by Ember (emb-opt3) compared to hand- optimized code (ref-dae) for all model classes in Table 1. Besides all optimizations discussed in Section 7, hand-optimized code also includes low-level, CPU-specific optimizations to improve callback invocations. These CPU-specific optimizations include, for instance, (1) reordering the if-cases of multi-callback code (like the code in Figure 14d) according to their taken frequency or (2) set the val- ues of control tokens to be directly used in compute code (e.g. to increment variables). Overall, these CPU-specific optimizations primarily affect multi- callback code such as MP, SLS, and SpMM, yielding performance improvements of up to 5 , with an average geometric mean im- provement of 1 . The limited impact of these low-level optimiza- tions arises because, as discussed in Section 8.1, the optimizations introduced in Section 7 already push the architecture close to its limits, leaving little room for further gains. Nevertheless, because these low-level optimizations are highly CPU-specific, we chose not to integrate them into Ember, which is designed to provide a more general solution for a larger class of architectures. Ultimately, we believe that the optimizations presented in Sec- tion 7 are sufficient to fully unlock the potential of general DAE architectures at no programmability cost. 9 Related Work Embedding operations are becoming increasingly critical in several machine learning models, and DAE architectures a more widely adopted solution for similar irregular workloads.\n\n--- Segment 48 ---\nUltimately, we believe that the optimizations presented in Sec- tion 7 are sufficient to fully unlock the potential of general DAE architectures at no programmability cost. 9 Related Work Embedding operations are becoming increasingly critical in several machine learning models, and DAE architectures a more widely adopted solution for similar irregular workloads. However, no study demonstrates the potential of DAE architectures on such a large class of models, and no compiler can generate DAE code that matches the performance of hand-optimized DAE embedding operations, as explained in this section. Embedding operations in Machine Learning Compilers: For traditional architectures such as CPUs and GPUs, end-users call em- bedding operations through multiple deep learning frameworks [1, 9, 14, 24, 50, 54], which can either be hand-written or automatically generated from progressive lowering. However, to the best of our knowledge, no machine learning compiler integrates automated DAE compilation, especially for embedding operations. While ac- celerators such as Meta s MTIA [15] feature cores to accelerate embedding lookups, embedding operations for such an accelerator typically come as libraries written by experts in low-level code [15]. However, this solution does not scale to general embedding oper- ations where the space of models [22], algorithmic variants [53], algorithmic optimizations [20], and input formats [29] is large. Fur- thermore, libraries prevent fusion across operations [36]. Sparse Tensor Algebra Compilers: When interpreted as sparse tensor expressions, embedding operations can be compiled to CPUs [3, 4, 8, 29, 30], GPUs [38, 55, 68], FPGAs [38], and distributed sys- tems [66], but not DAE architectures. The Sparse Abstract Machine (SAM) [21] compiles sparse tensor algebra to a stream dataflow representation, which might generate code for the access unit but not for the execute unit nor the marshaling code. DAE Programming Models in Other Domains: Several au- thors already proposed different DAE architectures for various domains, each one with a different programming interface. Pla- nar [5] offloads strided and irregular memory accesses from CPUs to tiny near-memory cores. As Planar primarily targets scientific workloads, it can be used through software libraries (e.g. BLAS [34]) optimized by experts.\n\n--- Segment 49 ---\nAs Planar primarily targets scientific workloads, it can be used through software libraries (e.g. BLAS [34]) optimized by experts. SpZip [67], instead, integrates near-core spe- cialized units in multicore processors to accelerate traversal and (de)compression operations in graph analytics. As these algorithms are generally hand-written by end users, SpZip comes with its own domain-specific language. Finally, MAPLE [48] offloads indirect accesses to specialized units placed in the network-on-chip of a Siracusa et al. multicore processor. As MAPLE targets general irregular workloads, it comes with its own compiler, DeSC [19], which decouples access from execute operations in LLVM [32]. A similar LLVM technique has been used to offload affine, indirect, and pointer-chasing mem- ory accesses to stream engines on CPUs [63]. However, the LLVM IR is an unstructured IR that represents loops with conditional branches. This substantially limits code analyses, decoupling, and optimizations of programs with complex loops like embedding op- erations [60]. While other frameworks, including HLS tools [10, 60], proposed to overcome LLVM limitations by using pragmas [59, 64], this solution is not applicable to machine learning compilers. More- over, the LLVM IR is not designed for DAE code, making advanced optimizations impractical. 10 Conclusions In conclusion, we demonstrated that DAE architectures outperform GPUs by 2.6 in performance and 6.4 in performance watt on embedding-intensive models. Then, we designed the Ember com- piler to integrate DAE architectures in common machine-learning frameworks. Compared to other approaches in the literature, Ember progressively lowers embedding operations through custom inter- mediate representations to optimize code at different abstraction levels. In this way, Ember implements all the necessary optimiza- tions, both local and global optimizations, to match the performance of hand-optimized code, enabling the potential of DAE architectures at no programmability cost.\n\n--- Segment 50 ---\nCompared to other approaches in the literature, Ember progressively lowers embedding operations through custom inter- mediate representations to optimize code at different abstraction levels. In this way, Ember implements all the necessary optimiza- tions, both local and global optimizations, to match the performance of hand-optimized code, enabling the potential of DAE architectures at no programmability cost. References [1] MartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, San- jay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion ManÃ©, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda ViÃ©gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org. [2] Bilge Acun, Matthew Murphy, Xiaodong Wang, Jade Nie, Carole-Jean Wu, and Kim Hazelwood. 2021. Understanding Training Efficiency of Deep Learning Recommendation Models at Scale. In 2021 IEEE International Symposium on High- Performance Computer Architecture (HPCA). 802 814. doi:10.1109 HPCA51647. 2021.00072 [3] Willow Ahrens, Daniel Donenfeld, Fredrik Kjolstad, and Saman Amarasinghe. 2023. Looplets: A Language for Structured Coiteration. In Proceedings of the 21st ACM IEEE International Symposium on Code Generation and Optimization (MontrÃ©al, QC, Canada) (CGO 2023).\n\n--- Segment 51 ---\nLooplets: A Language for Structured Coiteration. In Proceedings of the 21st ACM IEEE International Symposium on Code Generation and Optimization (MontrÃ©al, QC, Canada) (CGO 2023). Association for Computing Machinery, New York, NY, USA, 41 54. doi:10.1145 3579990.3580020 [4] Manya Bansal, Olivia Hsu, Kunle Olukotun, and Fredrik Kjolstad. 2023. Mosaic: An Interoperable Compiler for Tensor Algebra. Proc. ACM Program. Lang. 7, PLDI, Article 122 (jun 2023), 26 pages. doi:10.1145 3591236 [5] AdriÃ¡n Barredo, AdriÃ  Armejach, Jonathan Beard, and Miquel Moreto. 2021. PLANAR: a programmable accelerator for near-memory data rearrangement. In Proceedings of the 35th ACM International Conference on Supercomputing (Virtual Event, USA) (ICS 21). Association for Computing Machinery, New York, NY, USA, 164 176. doi:10.1145 3447818.3460368 [6] Richard Barrett, Michael Berry, Tony F. Chan, James Demmel, June Donato, Jack Dongarra, Victor Eijkhout, Roldan Pozo, Charles Romine, and Henk van der Vorst. 1994. Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. Society for Industrial and Applied Mathematics. doi:10.1137 1. 9781611971538 arXiv: [7] Tim Berners-Lee, James Hendler, and Ora Lassila. 2001. The Semantic Web: A New Form of Web Content that is Meaningful to Computers will Unleash a Revolution of New Possibilities. Scientific American (May 2001). [8] Aart Bik, Penporn Koanantakool, Tatiana Shpeisman, Nicolas Vasilache, Bixia Zheng, and Fredrik Kjolstad. 2022. Compiler Support for Sparse Tensor Com- putations in MLIR. ACM Trans. Archit. Code Optim. 19, 4, Article 50 (sep 2022), 25 pages.\n\n--- Segment 52 ---\nCode Optim. 19, 4, Article 50 (sep 2022), 25 pages. doi:10.1145 3544559 [9] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. 2018. JAX: composable transformations of Python NumPy programs. [10] Jason Cong, Bin Liu, Stephen Neuendorffer, Juanjo Noguera, Kees Vissers, and Zhiru Zhang. 2011. High-Level Synthesis for FPGAs: From Prototyping to Deployment. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 30, 4 (2011), 473 491. doi:10.1109 TCAD.2011.2110592 [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 4171 4186. doi:10.18653 v1 N19-1423 [12] Douglas Doerfler, Farzad Fatollahi-Fard, Colin MacLean, Tan Nguyen, Samuel Williams, Nicholas Wright, and Marco Siracusa. 2021. Experiences Porting the SU3_Bench Microbenchmark to the Intel Arria 10 and Xilinx Alveo U280 FPGAs. In Proceedings of the 9th International Workshop on OpenCL (Munich, Germany) (IWOCL 21). Association for Computing Machinery, New York, NY, USA, Article 1, 9 pages.\n\n--- Segment 53 ---\nIn Proceedings of the 9th International Workshop on OpenCL (Munich, Germany) (IWOCL 21). Association for Computing Machinery, New York, NY, USA, Article 1, 9 pages. doi:10.1145 3456669.3456671 [13] Pouya Esmaili-Dokht, Francesco Sgherzi, ValÃ©ria Soldera Girelli, Isaac Boixaderas, Mariana Carmin, Alireza Monemi, AdriÃ  Armejach, Estanislao Mercadal, GermÃ¡n Llort, Petar RadojkoviÄ‡, Miquel Moreto, Judit GimÃ©nez, Xavier Martorell, Eduard AyguadÃ©, Jesus Labarta, Emanuele Confalonieri, Rishabh Dubey, and Jason Adlard. 2024. A Mess of Memory System Benchmarking, Simulation and Application Profiling. In 2024 57th IEEE ACM International Symposium on Microarchitecture (MICRO). 136 152. doi:10.1109 MICRO61859.2024.00020 [14] Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds.\n\n--- Segment 54 ---\nFast Graph Representation Learning with PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and Manifolds. [15] Amin Firoozshahian, Joel Coburn, Roman Levenstein, Rakesh Nattoji, Ashwin Ka- math, Olivia Wu, Gurdeepak Grewal, Harish Aepala, Bhasker Jakka, Bob Dreyer, Adam Hutchin, Utku Diril, Krishnakumar Nair, Ehsan K. Aredestani, Martin Schatz, Yuchen Hao, Rakesh Komuravelli, Kunming Ho, Sameer Abu Asal, Joe Shajrawi, Kevin Quinn, Nagesh Sreedhara, Pankaj Kansal, Willie Wei, Dheepak Jayaraman, Linda Cheng, Pritam Chopda, Eric Wang, Ajay Bikumandla, Arun Karthik Sengottuvel, Krishna Thottempudi, Ashwin Narasimha, Brian Dodds, Cao Gao, Jiyuan Zhang, Mohammed Al-Sanabani, Ana Zehtabioskuie, Jordan Fix, Hangchen Yu, Richard Li, Kaustubh Gondkar, Jack Montgomery, Mike Tsai, Saritha Dwarakapuram, Sanjay Desai, Nili Avidan, Poorvaja Ramani, Karthik Narayanan, Ajit Mathews, Sethu Gopal, Maxim Naumov, Vijay Rao, Krishna Noru, Harikrishna Reddy, Prahlad Venkatapuram, and Alexis Bjorlin. 2023. MTIA: First Generation Silicon Targeting Meta s Recommendation Systems. In Proceedings of the 50th Annual International Symposium on Computer Architecture (Orlando, FL, USA) (ISCA 23). Association for Computing Machinery, New York, NY, USA, Article 80, 13 pages. doi:10.1145 3579371.3589348 [16] A.A. Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James Zou. 2021. Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems.\n\n--- Segment 55 ---\n2021. Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems. In 2021 IEEE International Symposium on Informa- tion Theory (ISIT) (Melbourne, Australia). IEEE Press, 2786 2791. doi:10.1109 ISIT45174.2021.9517710 [17] Google. 2021. MLIR Sparsifier. [18] U. Gupta, C. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel, K. Hazel- wood, M. Hempstead, B. Jia, H. S. Lee, A. Malevich, D. Mudigere, M. Smelyanskiy, L. Xiong, and X. Zhang. 2020. The Architectural Implications of Facebook s DNN- Based Personalized Recommendation. In 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA). IEEE Computer Society, Los Alamitos, CA, USA, 488 501. doi:10.1109 HPCA47549.2020.00047 [19] Tae Jun Ham, Juan L. AragÃ³n, and Margaret Martonosi. 2015. DeSC: decoupled supply-compute communication management for heterogeneous architectures. In Proceedings of the 48th International Symposium on Microarchitecture (Waikiki, Hawaii) (MICRO-48). Association for Computing Machinery, New York, NY, USA, 191 203. doi:10.1145 2830772.2830800 [20] William L. Hamilton, Rex Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS 17). Curran Associates Inc., Red Hook, NY, USA, 1025 1035. [21] Olivia Hsu, Maxwell Strange, Ritvik Sharma, Jaeyeon Won, Kunle Olukotun, Joel S. Emer, Mark A. Horowitz, and Fredrik KjÃ¸lstad. 2023. The Sparse Abstract Machine.\n\n--- Segment 56 ---\n2023. The Sparse Abstract Machine. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 ( conf- loc , city Vancouver city , state BC state , country Canada country , conf-loc ) (ASPLOS 2023). Association for Computing Machinery, New York, NY, USA, 710 726. doi:10.1145 3582016.3582051 [22] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: datasets for Ember: A Compiler for Efficient Embedding Operations on Decoupled Access-Execute Architectures machine learning on graphs. In Proceedings of the 34th International Conference on Neural Information Processing Systems ( conf-loc , city Vancouver city , state BC state , country Canada country , conf-loc ) (NIPS 20). Curran Associates Inc., Red Hook, NY, USA, Article 1855, 16 pages. [23] Mohamed Assem Ibrahim, Onur Kayiran, and Shaizeen Aga. 2022. Efficient Cache Utilization via Model-aware Data Placement for Recommendation Models. In Proceedings of the International Symposium on Memory Systems (Washington DC, DC, USA) (MEMSYS 21). Association for Computing Machinery, New York, NY, USA, Article 2, 11 pages. doi:10.1145 3488423.3519317 [24] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093 (2014). [25] Manas R. Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K. Adams, Pranav Khaitan, Jiahui Liu, and Quoc V. Le. 2020. Neural Input Search for Large Scale Recommendation Models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining (Virtual Event, CA, USA) (KDD 20).\n\n--- Segment 57 ---\nNeural Input Search for Large Scale Recommendation Models. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining (Virtual Event, CA, USA) (KDD 20). Association for Computing Machinery, New York, NY, USA, 2387 2397. doi:10.1145 3394486.3403288 [26] Liu Ke, Udit Gupta, Benjamin Youngjae Cho, David Brooks, Vikas Chandra, Utku Diril, Amin Firoozshahian, Kim Hazelwood, Bill Jia, Hsien-Hsin S. Lee, Meng Li, Bert Maher, Dheevatsa Mudigere, Maxim Naumov, Martin Schatz, Mikhail Smelyanskiy, Xiaodong Wang, Brandon Reagen, Carole-Jean Wu, Mark Hempstead, and Xuan Zhang. 2020. RecNMP: accelerating personalized recom- mendation with near-memory processing. In Proceedings of the ACM IEEE 47th Annual International Symposium on Computer Architecture (Virtual Event) (ISCA 20). IEEE Press, 790 803. doi:10.1109 ISCA45697.2020.00070 [27] Ken Kennedy and John R. Allen. 2001. Optimizing compilers for modern archi- tectures: a dependence-based approach. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. [28] Fredrik Kjolstad, Willow Ahrens, Shoaib Kamil, and Saman Amarasinghe. 2019. Tensor algebra compilation with workspaces. In Proceedings of the 2019 IEEE ACM International Symposium on Code Generation and Optimization (Washington, DC, USA) (CGO 2019). IEEE Press, 180 192. [29] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amaras- inghe. 2017. The tensor algebra compiler. Proc. ACM Program. Lang. 1, OOPSLA, Article 77 (oct 2017), 29 pages. doi:10.1145 3133901 [30] Scott Kovach, Praneeth Kolichala, Tiancheng Gu, and Fredrik Kjolstad. 2023. Indexed Streams: A Formal Intermediate Representation for Fused Contraction Programs. Proc. ACM Program.\n\n--- Segment 58 ---\nProc. ACM Program. Lang. 7, PLDI, Article 154 (jun 2023), 25 pages. doi:10.1145 3591268 [31] Criteo AI Lab. 2024. Criteo 1TB Click Logs Dataset. download-criteo-1tb-click-logs-dataset . [32] Chris Lattner and Vikram Adve. 2004. LLVM: A Compilation Framework for Lifelong Program Analysis Transformation. In Proceedings of the International Symposium on Code Generation and Optimization: Feedback-Directed and Runtime Optimization (Palo Alto, California) (CGO 04). IEEE Computer Society, USA, 75. [33] Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Olek- sandr Zinenko. 2021. MLIR: scaling compiler infrastructure for domain specific computation. In Proceedings of the 2021 IEEE ACM International Symposium on Code Generation and Optimization (Virtual Event, Republic of Korea) (CGO 21). IEEE Press, 2 14. doi:10.1109 CGO51591.2021.9370308 [34] C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. 1979. Basic Linear Algebra Subprograms for Fortran Usage. ACM Trans. Math. Softw. 5, 3 (Sept. 1979), 308 323. doi:10.1145 355841.355847 [35] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network Dataset Collection. [36] Jiajun Li, Ahmed Louri, Avinash Karanth, and Razvan Bunescu. 2021. GCNAX: A Flexible and Energy-efficient Accelerator for Graph Convolutional Neural Networks. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA).\n\n--- Segment 59 ---\nGCNAX: A Flexible and Energy-efficient Accelerator for Graph Convolutional Neural Networks. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA). 775 788. doi:10.1109 HPCA51647.2021.00070 [37] Sheng Li, Jung Ho Ahn, Richard D. Strong, Jay B. Brockman, Dean M. Tullsen, and Norman P. Jouppi. 2009. McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures. In 2009 42nd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). 469 480. [38] Jie Liu, Zhongyuan Zhao, Zijian Ding, Benjamin Brock, Hongbo Rong, and Zhiru Zhang. 2024. UniSparse: An Intermediate Language for General Sparse Format Customization. Proc. ACM Program. Lang. 8, OOPSLA1, Article 99 (April 2024), 29 pages. doi:10.1145 3649816 [39] LLVM. [n. d.]. Torch-MLIR.\n\n--- Segment 60 ---\n[n. d.]. Torch-MLIR. [40] J. Lowe-Power, A. Mutaal Ahmad, A. Akram, M. Alian, R. Amslinger, M. An- dreozzi, A. Armejach, N. Asmussen, B. Beckmann, S. Bharadwaj, G. Black, G. Bloom, B. R. Bruce, D. Rodrigues Carvalho, J. Castrillon, L. Chen, N. Derumigny, S. Diestelhorst, W. Elsasser, C. Escuin, M. Fariborz, A. Farmahini-Farahani, P. Fotouhi, R. Gambord, J. Gandhi, D. Gope, T. Grass, A. Gutierrez, B. Hanindhito, A. Hansson, S. Haria, A. Harris, T. Hayes, A. Herrera, M. Horsnell, S. A. R. Jafri, R. Jagtap, H. Jang, R. Jeyapaul, T. M. Jones, M. Jung, S. Kannoth, H. Khaleghzadeh, Y. Kodama, T. Krishna, T. Marinelli, C. Menard, A. Mondelli, M. Moreto, T. MÃ¼ck, O. Naji, K. Nathella, H. Nguyen, N. Nikoleris, L. E. Olson, M. Orr, B. Pham, P. Prieto, T. Reddy, A. Roelke, M. Samani, A. Sandberg, J. Setoain, B. Shingarov, M. D. Sinclair, T. Ta, R. Thakur, G. Travaglini, M. Upton, N. Vaish, I. Vougioukas, W. Wang, Z. Wang, N. Wehn, C. Weis, D. A. Wood, H. Yoon, and Ã‰. F. Zulian. 2020. The gem5 Simulator: Version 20.0 .\n\n--- Segment 61 ---\n2020. The gem5 Simulator: Version 20.0 . arXiv:2007.03152 [cs.AR] [41] Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie (Amy) Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnaku- mar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, and Vijay Rao. 2022. Software-hardware co-design for fast and scalable training of deep learning recommendation models. In Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 993 1011. doi:10.1145 3470496.3533727 [42] Nevine Nassif, Ashley O.\n\n--- Segment 62 ---\nIn Proceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 993 1011. doi:10.1145 3470496.3533727 [42] Nevine Nassif, Ashley O. Munch, Carleton L. Molnar, Gerald Pasdast, Sitara- man V. Lyer, Zibing Yang, Oscar Mendoza, Mark Huddart, Srikrishnan Venkatara- man, Sireesha Kandula, Rafi Marom, Alexandra M. Kern, Bill Bowhill, David R. Mulvihill, Srikanth Nimmagadda, Varma Kalidindi, Jonathan Krause, Moham- mad M. Haq, Roopali Sharma, and Kevin Duda. 2022. Sapphire Rapids: The Next- Generation Intel Xeon Scalable Processor. In 2022 IEEE International Solid-State Circuits Conference (ISSCC), Vol. 65. 44 46. doi:10.1109 ISSCC42614.2022.9731107 [43] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia Cherni- avskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr Kon- dratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong, and Misha Smelyanskiy. 2019. Deep Learning Recommendation Model for Personalization and Recommendation Systems. CoRR abs 1906.00091 (2019). [44] Tan Nguyen, Colin MacLean, Marco Siracusa, Douglas Doerfler, Nicholas J. Wright, and Samuel Williams. 2022. FPGA-based HPC accelerators: An evaluation on performance and energy efficiency. Concurrency and Com- putation: Practice and Experience 34, 20 (2022), e6570.\n\n--- Segment 63 ---\nFPGA-based HPC accelerators: An evaluation on performance and energy efficiency. Concurrency and Com- putation: Practice and Experience 34, 20 (2022), e6570. doi:10.1002 cpe.6570 arXiv: [45] Nvidia. 2024. Nvidia H100 specs. h100 [46] Nvidia. 2024. Nvidia Nsight Systems. systems [47] Nvidia. 2024. Nvidia T4 specs. t4 [48] Marcelo Orenes-Vera, Aninda Manocha, Jonathan Balkind, Fei Gao, Juan L. AragÃ³n, David Wentzlaff, and Margaret Martonosi. 2022. Tiny but mighty: designing and realizing scalable latency tolerance for manycore SoCs. In Pro- ceedings of the 49th Annual International Symposium on Computer Architecture (New York, New York) (ISCA 22). Association for Computing Machinery, New York, NY, USA, 817 830. doi:10.1145 3470496.3527400 [49] Alberto Parravicini, Luca Giuseppe Cellamare, Marco Siracusa, and Marco D Santambrogio. 2021. Scaling up HBM efficiency of Top-K SpMV for approximate embedding similarity on FPGAs. In 2021 58th ACM IEEE Design Automation Conference (DAC). IEEE, 799 804. [50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas KÃ¶pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: an imperative style, high-performance deep learning library. Curran Associates Inc., Red Hook, NY, USA. [51] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation.\n\n--- Segment 64 ---\n2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Alessandro Mos- chitti, Bo Pang, and Walter Daelemans (Eds.). Association for Computational Linguistics, Doha, Qatar, 1532 1543. doi:10.3115 v1 D14-1162 [52] Eric T. Phipps and Tamara G. Kolda. 2019. Software for Sparse Ten- sor Decomposition on Emerging Computing Architectures. SIAM Journal on Scientific Computing 41, 3 (2019), C269 C290. doi:10.1137 18M1210691 arXiv: [53] Md. Khaledur Rahman, Majedul Haque Sujon, and Ariful Azad. 2021. FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph Neural Net- works. In 2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS). 256 266. doi:10.1109 IPDPS49936.2021.00034 Siracusa et al. [54] Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Summer Deng, Roman Dzhabarov, James Hegeman, Roman Levenstein, Bert Maher, Nadathur Satish, Jakob Olesen, Jongsoo Park, Artem Rakhov, and Misha Smelyanskiy. 2018. Glow: Graph Low- ering Compiler Techniques for Neural Networks. CoRR abs 1805.00907 (2018). arXiv:1805.00907 [55] Ryan Senanayake, Changwan Hong, Ziheng Wang, Amalee Wilson, Stephen Chou, Shoaib Kamil, Saman Amarasinghe, and Fredrik Kjolstad. 2020. A sparse iteration space transformation framework for sparse tensor algebra. Proc. ACM Program. Lang. 4, OOPSLA, Article 158 (nov 2020), 30 pages. doi:10.1145 3428226 [56] Geet Sethi, Bilge Acun, Niket Agarwal, Christos Kozyrakis, Caroline Trippel, and Carole-Jean Wu. 2022.\n\n--- Segment 65 ---\ndoi:10.1145 3428226 [56] Geet Sethi, Bilge Acun, Niket Agarwal, Christos Kozyrakis, Caroline Trippel, and Carole-Jean Wu. 2022. RecShard: statistical feature-based memory optimization for industry-scale neural recommendation. In Proceedings of the 27th ACM In- ternational Conference on Architectural Support for Programming Languages and Operating Systems (Lausanne, Switzerland) (ASPLOS 22). Association for Com- puting Machinery, New York, NY, USA, 344 358. doi:10.1145 3503222.3507777 [57] Francesco Sgherzi, Alberto Parravicini, Marco Siracusa, and Marco D. Santam- brogio. 2021. Solving Large Top-K Graph Eigenproblems with a Memory and Compute-optimized FPGA Design. In 2021 IEEE 29th Annual International Sym- posium on Field-Programmable Custom Computing Machines (FCCM). 78 87. doi:10.1109 FCCM51124.2021.00017 [58] Francesco Sgherzi, Marco Siracusa, Ivan Fernandez, AdriÃ  Armejach, and Miquel MoretÃ³. 2024. SpChar: Characterizing the sparse puzzle via decision trees. J. Parallel and Distrib. Comput. 192 (2024), 104941. doi:10.1016 j.jpdc.2024.104941 [59] Marco Siracusa, Emanuele Del Sozzo, Marco Rabozzi, Lorenzo Di Tucci, Samuel Williams, Donatella Sciuto, and Marco Domenico Santambrogio. 2022. A Com- prehensive Methodology to Optimize FPGA Designs via the Roofline Model. IEEE Trans. Comput. 71, 8 (2022), 1903 1915. doi:10.1109 TC.2021.3111761 [60] Marco Siracusa and Fabrizio Ferrandi. 2020. Tensor Optimization for High- Level Synthesis Design Flows. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 39, 11 (2020), 4217 4228. doi:10.1109 TCAD.2020.\n\n--- Segment 66 ---\nTensor Optimization for High- Level Synthesis Design Flows. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 39, 11 (2020), 4217 4228. doi:10.1109 TCAD.2020. 3012318 [61] Marco Siracusa, VÃ­ctor Soria-Pardos, Francesco Sgherzi, Joshua Randall, Dou- glas J. Joseph, Miquel MoretÃ³ Planas, and AdriÃ  Armejach. 2023. A Tensor Marshaling Unit for Sparse Tensor Algebra on General-Purpose Processors. In Proceedings of the 56th Annual IEEE ACM International Symposium on Mi- croarchitecture ( conf-loc , city Toronto city , state ON state , coun- try Canada country , conf-loc ) (MICRO 23). Association for Computing Machinery, New York, NY, USA, 1332 1346. doi:10.1145 3613424.3614284 [62] N. Stephens, S. Biles, M. Boettcher, J. Eapen, M. Eyole, G. Gabrielli, M. Horsnell, G. Magklis, A. Martinez, N. Premillieu, A. Reid, A. Rico, and P. Walker. 2017. The ARM Scalable Vector Extension. IEEE Micro 37, 02 (mar 2017), 26 39. doi:10. 1109 MM.2017.35 [63] Zhengrong Wang and Tony Nowatzki. 2019. Stream-based Memory Access Specialization for General Purpose Processors. In 2019 ACM IEEE 46th Annual International Symposium on Computer Architecture (ISCA). 736 749. [64] Jian Weng, Sihao Liu, Vidushi Dadu, Zhengrong Wang, Preyas Shah, and Tony Nowatzki. 2020. DSAGEN: Synthesizing Programmable Spatial Accelerators. In 2020 ACM IEEE 47th Annual International Symposium on Computer Architecture (ISCA). 268 281. doi:10.1109 ISCA45697.2020.00032 [65] Finn Wilkinson and Simon McIntosh-Smith. 2022. An Initial Evaluation of Arm s Scalable Matrix Extension. In 2022 IEEE ACM International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS).\n\n--- Segment 67 ---\nAn Initial Evaluation of Arm s Scalable Matrix Extension. In 2022 IEEE ACM International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS). 135 140. doi:10.1109 PMBS56514.2022.00018 [66] Rohan Yadav, Alex Aiken, and Fredrik Kjolstad. 2022. SpDISTAL: compiling distributed sparse tensor computations. In Proceedings of the International Confer- ence on High Performance Computing, Networking, Storage and Analysis (Dallas, Texas) (SC 22). IEEE Press, Article 59, 15 pages. [67] Yifan Yang, Joel S. Emer, and Daniel Sanchez. 2021. SpZip: Architectural Support for Effective Data Compression In Irregular Applications. In 2021 ACM IEEE 48th Annual International Symposium on Computer Architecture (ISCA). 1069 1082. doi:10.1109 ISCA52012.2021.00087 [68] Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze. 2023. Sparse- TIR: Composable Abstractions for Sparse Compilation in Deep Learning. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3 (Vancouver, BC, Canada) (ASPLOS 2023). Association for Computing Machinery, New York, NY, USA, 660 678. doi:10.1145 3582016.3582047 [69] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran- zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds. ), Vol. 33. Curran Associates, Inc., 17283 17297. c8512d142a2d849725f31a9a7a361ab9-Paper.pdf [70] Genghan Zhang, Olivia Hsu, and Fredrik Kjolstad. 2024.\n\n--- Segment 68 ---\nCurran Associates, Inc., 17283 17297. c8512d142a2d849725f31a9a7a361ab9-Paper.pdf [70] Genghan Zhang, Olivia Hsu, and Fredrik Kjolstad. 2024. Compilation of Modular and General Sparse Workspaces. Proc. ACM Program. Lang. 8, PLDI, Article 196 (jun 2024), 26 pages. doi:10.1145 3656426\n\n