=== ORIGINAL PDF: 2503.03088v2_AHCPTQ_Accurate_and_Hardware-Compatible_Post-Train.pdf ===\n\nRaw text length: 66590 characters\nCleaned text length: 65642 characters\nNumber of segments: 44\n\n=== CLEANED TEXT ===\n\nAHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model Wenlun Zhang1 Yunshan Zhong2 Shimpei Ando1 Kentaro Yoshioka1 1Keio University 2Hainan University {wenlun zhang, shimpeiando, Abstract The Segment Anything Model (SAM) has demonstrated strong versatility across various visual tasks. However, its large storage requirements and high computational cost pose challenges for practical deployment. Post-training quantization (PTQ) has emerged as an effective strategy for efficient deployment, but we identify two key challenges in SAM that hinder the effectiveness of existing PTQ meth- ods: the heavy-tailed and skewed distribution of post-GELU activations, and significant inter-channel variation in lin- ear projection activations. To address these challenges, we propose AHCPTQ, an accurate and hardware-efficient PTQ method for SAM. AHCPTQ introduces hardware- compatible Hybrid Log-Uniform Quantization (HLUQ) to manage post-GELU activations, employing log2 quantiza- tion for dense small values and uniform quantization for sparse large values to enhance quantization resolution. Ad- ditionally, AHCPTQ incorporates Channel-Aware Group- ing (CAG) to mitigate inter-channel variation by progres- sively clustering activation channels with similar distri- butions, enabling them to share quantization parameters and improving hardware efficiency. The combination of HLUQ and CAG not only enhances quantization effective- ness but also ensures compatibility with efficient hardware execution. For instance, under the W4A4 configuration on the SAM-L model, AHCPTQ achieves 36.6 mAP on in- stance segmentation with the DINO detector, while achiev- ing a 7.89 speedup and 8.64 energy efficiency over its floating-point counterpart in FPGA implementation. Code is available at 1. Introduction The Segment Anything Model (SAM) [13] is a power- ful tool for promptable segmentation, demonstrating strong zero-shot performance across diverse visual domains and Corresponding author. broad applicability in real-world scenarios [2, 30, 33, 38, 44]. However, its large-scale parameters, substantial stor- age demands, and high computational costs pose significant challenges for deployment on edge devices [11, 19, 28, 45]. 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.1 0.2 0.3 Relative Frequency Distribution Post-GELU Activations (a) Challenge 1: Heavy-tailed and skewed post-GELU activations (from the 10th block of the image encoder). 56 63 70 77 84 91 98 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 Channel Index Range Token-to-Image: Linear Projection Activations (b) Challenge 2: Inter-channel variation for channel indices 50 to 100 (from the first block of the mask decoder). Figure 1. Challenges in SAM quantization (Data obtained from the SAM-B model with the YOLOX detector). To tackle this challenge, model quantization has been widely adopted to replace floating-point weights and acti- vations with low-bit representations, reducing storage over- head and enabling efficient integer-based computations, making it well-suited for edge devices with constrained resources [14]. One prominent approach is Quantization- Aware Training (QAT), which incorporates quantization ef- fects during training, allowing the model to adapt to quan- tized weights and activations [5, 8, 16, 18, 25]. However, applying QAT to SAM is impractical due to the compu- tational expense of SAM training, which relies on a co- developed data engine and consequently utilizes the SA- 1 arXiv:2503.03088v2 [cs.CV] 9 Jul 2025 1B dataset comprising 1.1B masks and 11M images [13]. As a more practical alternative, Post-Training Quantization (PTQ) has gained increasing attention. PTQ requires only a small calibration dataset, significantly reducing data and computational demands while maintaining competitive ac- curacy [4, 6, 20, 23, 27, 34, 47 49]. PTQ4SAM [29] has recently demonstrated the feasibil- ity of applying PTQ to SAM [26, 35], utilizing equivalent sign transformations and adaptive resolution quantization to accommodate its unique activation distributions. Despite these advancements, our study reveals that existing PTQ methods suffer from quantization collapse at 4-bit and se- vere accuracy degradation at 5-bit, limiting their practical utility for ultra-low-bit deployment. As presented in Ta- ble 1, PTQ4SAM achieves only 3.8 mAP at 4-bit and 18.4 mAP at 5-bit quantization on the YOLOX detector with the SAM-B model, compared to 37.2 and 40.4 mAP in floating-point, respectively. Such substantial accu- racy loss significantly hampers the feasibility of deploying SAM on edge devices, motivating the need for more effec- tive ultra-low-bit quantization techniques. In this paper, we identify two additional challenges that limit PTQ performance on the SAM model. First, as il- lustrated in Fig. 1a, post-GELU activations exhibit an ex- tremely imbalanced distribution, where numerous small values are densely clustered within a narrow range, while sparse large values extend across a wide range. This imbal- ance poses a significant challenge for traditional hardware- friendly uniform and log2 quantizers. Although non- uniform quantizers [3, 9, 36] can better adapt to this dis- tribution, they introduce substantial challenges in hardware implementation. Second, activations in Query Key Value linear projections and Linear-1 of MLP exhibit high inter- channel variance, as exemplified in Fig. 1b. Such vari- ations render per-tensor quantization ineffective a small scale factor results in large quantization errors for channels with wide dynamic ranges, while a large scale factor ze- ros out activations in channels with small dynamic ranges. Although per-channel quantization improves accuracy, it in- troduces deployment inefficiencies due to the high overhead of storing and transferring quantization parameters. Ad- dressing these challenges requires a quantization strategy that both closely aligns with activation distributions and re- mains compatible with efficient hardware deployment. To overcome the challenges in PTQ for SAM, we present AHCPTQ, a novel approach that enhances quan- tization effectiveness while ensuring hardware compati- bility. AHCPTQ addresses the two identified challenges through Hybrid Log-Uniform Quantization (HLUQ) and Channel-Aware Grouping (CAG), respectively. HLUQ in- novatively employs log2 quantization for densely clus- tered small values and uniform quantization for sparse but widely distributed large values, effectively adapting to the heavy-tailed and skewed distribution of post-GELU activa- tions while maintaining hardware-efficient implementation. Meanwhile, CAG selectively clusters channels with simi- lar statistical properties, assigning a shared quantization pa- rameter to each group. This method achieves accuracy com- parable to per-channel quantization while reducing on-chip register overhead by 99.7 and enabling efficient storage of quantization parameters within a compact set of on-chip registers. As a result, AHCPTQ significantly reduces accu- racy loss at 5-bit PTQ and, to the best of our knowledge, is the first to enable 4-bit PTQ on SAM. Our main contribu- tions are summarized as follows: We identify two key challenges limiting PTQ perfor- mance on SAM and propose AHCPTQ, a framework that integrates CAG and HLUQ to achieve a balance between quantization effectiveness and hardware feasibility. Extensive experiments show that AHCPTQ consistently surpasses previous PTQ methods, setting a new state-of- the-art in SAM quantization. Under the W4A4 configura- tion, AHCPTQ achieves 36.6 mAP on instance segmen- tation with the DINO detector on SAM-L, representing a substantial improvement over PTQ4SAM, which fails to function under the same conditions. We further develop an FPGA-based accelerator to eval- uate AHCPTQ s hardware efficiency. Our results indi- cate that AHCPTQ delivers a 7.89 speedup and 8.64 energy efficiency improvement over floating-point imple- mentations, demonstrating superior resource utilization. 2. Related Works PTQ utilizes a small calibration dataset to determine quan- tization parameters, enabling rapid deployment on edge devices without requiring extensive retraining. This pa- per focuses on learning-based PTQ methods, which lever- age optimization techniques to derive optimal quantiza- tion parameters, often outperforming static PTQ methods. AdaRound [31] identifies the sensitivity of weight rounding and introduces an optimization technique to reduce overall model loss. BRECQ [17] employs block reconstruction to strike a balance between cross-layer dependency and gen- eralization error. QDrop [39] integrates dropout into the reconstruction process to improve the flatness of the opti- mized models. Despite their success, these methods are pri- marily designed for CNN-based models and face challenges when applied to Transformer architectures. In the do- main of Transformer-based models, FQ-ViT [23] improves granularity using powers-of-two scale and Log-Int-Softmax while maintaining hardware efficiency. RepQ-ViT [21] eliminates parameter overhead in per-channel quantiza- tion by conducting reparameterization techniques to post- LayerNorm activations. ERQ [48, 49] minimizes quan- tization errors via ridge regression, while PTQ4ViT [41] employs a twin uniform quantizer to efficiently handle 2 post-Softmax and post-GELU activations. While these ap- proaches offer insights for mitigating challenges in SAM, they are not easily adaptable to learning-based PTQ, and the mask decoder architecture of SAM restricts reparameteriza- tion techniques. PTQ4SAM [29], the first PTQ method spe- cialized for SAM, introduces bimodal integration and adap- tive log quantization to address unique distribution chal- lenges while maintaining hardware efficiency. However, it fails to achieve 4-bit quantization functionality, motivat- ing us to explore additional constraints and propose novel strategies to further improve performance. 3. Methodology 3.1. Preliminaries 3.1.1. Quantizer Quantization discretizes continuous values into low-bit rep- resentations, enabling efficient computation and reducing memory usage. Two widely adopted quantization schemes are uniform quantization and log2 quantization. The uni- form quantizer divides the input range into equally spaced intervals, mapping each value to the closest quantized level: x _{q} \ t e x t {c la m p } \left ( \left \lfloor \frac {x}{s} \right \rceil z, 0, 2 k - 1 \right ). \label {Eq_Uniform_Quant} (1) x \ a p pro x \hat {x} s \cdot (x_{q} - z). \label {Eq_Uniform_Dequant} (2) Here, x is the original floating-point input, xq is the quan- tized integer representation, k is the bit-width, s is the scale factor, and z is the zero point. The rounding function en- sures proper discretization. Uniform quantization is widely adopted due to its straightforward hardware implementa- tion, allowing integer arithmetic to replace floating-point operations, leading to higher efficiency and lower compu- tational cost. For highly skewed data distributions, the log2 quantizer provides a more effective alternative, as it assigns quantization levels based on powers of two, offering higher precision for small values: x _{q} \ text { c l am p} \ left ( \left \lfloor -\log _{2}\frac {x}{s} \right \rceil , 0, 2 k - 1 \right ) \label {Eq_Log_Quant} (3) x \ a p prox \hat {x} s \cdot 2 {-x_{q}} \label {Eq_Log_Dequant} (4) Log2 quantization is particularly beneficial for hardware, as it enables multiplications to be replaced by bit shifts, im- proving computational speed and energy efficiency. 3.1.2. Quantization Granularity Quantization operates at varying levels of granularity, in- troducing a trade-off between computational efficiency and quantization effectiveness. The two most common ap- proaches are per-tensor and per-channel quantization. Per- Tensor quantization employs a single scale and zero-point across an entire weight or activation tensor, reducing com- putational complexity and memory overhead. However, it struggles with large inter-channel variations, leading to suboptimal quantization performance. Per-Channel quan- tization assigns individual quantization parameters to each output channel, effectively mitigating distribution variance across channels. However, it requires storing more quanti- zation parameters, increasing memory usage and data trans- fer costs. 3.1.3. Block-Wise Reconstruction We employ block-wise reconstruction [39], as adopted in PTQ4SAM [29], to mitigate the quantization-induced er- ror in weight and activation quantization by minimizing the mean squared error: \ma t hcal {L} \ \mathbf {O}_\mathcal {B} - \hat {\mathbf {O}}_\mathcal {B} \ _2 2, \label {Eq_Block_Recon} (5) where OB and ˆOB represent the floating-point and quan- tized outputs of the B-th block, respectively. 3.2. Challenges of SAM Quantization Recent PTQ approaches [29] have primarily focused on ad- dressing the bimodal distribution of post-projection Key activations and the diverse distribution patterns of post- Softmax attention scores in SAM. Despite these efforts, ex- isting methods fail to account for additional key factors cru- cial to quantization, resulting in severe performance degra- dation in ultra-low-bit scenarios. As shown in Table 1, existing methods exhibit significant precision loss at 5-bit quantization and collapse at 4-bit quantization. In this pa- per, we identify two additional challenges that profoundly affect SAM s quantization performance, as illustrated in Fig. 1. The first challenge arises from the heavy-tailed and skewed distribution of post-GELU activations. As depicted in Fig. 1a, over 90 of activations are densely concentrated within -0.2 to 0, whereas a sparser but evenly distributed subset of large values, critical for inference accuracy, ex- tends from 0 to 0.8. As illustrated in Fig. 2 (left panel), existing log2 and uniform quantizers fail to effectively han- dle this distribution. The log2 quantizer efficiently cap- tures small, densely clustered values by allocating most of its quantization grids to this range. However, its exponen- tially spaced grid structure results in insufficient represen- tation density for larger values, leading to high quantiza- tion errors in the upper range. On the other hand, the uni- form quantizer provides consistent resolution across the full range, making it better suited for large, evenly spaced val- ues, but it lacks sufficient resolution for small activations, introducing substantial quantization errors. This problem is further exacerbated as bit-width decreases, particularly in ultra-low-bit settings, where the limited number of grids im- poses severe constraints. Although non-uniform quantizers 3 may provide a better alternative, their hardware complexity poses deployment challenges [3, 9, 36] and often necessi- tates substantial QAT-based retraining [15, 40]. To effec- tively address this issue, an efficient, hardware-compatible quantizer that captures both small and large activations tai- lored for PTQ is required. The second challenge arises from high inter-channel variation, particularly in the activations of Query Key Value linear projections in the attention module and Linear projec- tions in the MLP module. These variations originate from LayerNorm operations and the unique activation distribu- tions of the SAM mask decoder. As shown in Fig. 1b, activation ranges in the Token-to-Image Value linear pro- jection exhibit significant inter-channel disparities, making per-tensor quantization ineffective due to the challenge of finding a single optimal scale factor and zero point [1]. A large scale factor, selected to accommodate high-range channels, leads to reduced quantization granularity for low- range channels, often causing them to be rounded to zero. Conversely, a small scale factor, optimized for low-range channels, results in severe clipping in high-range chan- nels, leading to significant information loss [46]. Moreover, while a zero point could compensate for activation skew- ness, the varying inter-channel median values prevent a sin- gle zero point from being optimal for all channels. While per-channel quantization can effectively mitigate quantiza- tion errors, it introduces considerable hardware inefficien- cies. As depicted in Fig. 4, transferring per-channel quanti- zation parameters between DRAM and compute units in- curs a memory access overhead of up to several kB per layer [10, 37]. Storing these parameters on-chip can elim- inate the transfer cost, but at the expense of a significantly larger memory footprint, requiring tens of thousands of on- chip registers, thereby increasing chip area utilization. To address this trade-off, a granularity-aware quantization ap- proach with hardware co-optimization is essential for bal- ancing quantization accuracy and deployment efficiency. 3.3. AHCPTQ To advance SAM quantization, we propose AHCPTQ, a framework specifically designed to overcome the two key challenges discussed earlier. As shown in Fig. 2, AHCPTQ leverages CAG and HLUQ, each targeting a specific quan- tization issue. 3.3.1. Hybrid Log-Uniform Quantization The heavy-tailed and skewed post-GELU activations hin- der the quantization effectiveness of conventional uniform and log2 quantizers, while non-uniform quantizers, though potentially more adaptable, lack hardware efficiency. To en- hance SAM quantization performance while ensuring hard- ware compatibility, we propose HLUQ, a method that re- duces quantization errors in post-GELU activations by com- bining log2 and uniform quantization. Specifically, HLUQ applies log2 quantization to densely packed small values, where fine-grained precision is crucial, and uniform quan- tization to sparse, evenly distributed large values, ensuring minimal quantization error: x _ q \be gi n {ca s es } \tex t {c l a mp} \lef t ( \l ef t \lf lo o r - \l o g _{2}\frac {x}{s_{1}} \right \rceil , 0, \hat {b} \right ), \text {if } x \leq s_{1}, \\[0.5em] \text {clamp} \left ( \left \lfloor \frac {x-s_{1}}{s_{2}} \right \rceil , \hat {b}, 2 k - 1 \right ), \text {if } x s_{1}. \end {cases} \label {Eq_Hybrid_Quant} (6) x \ a pp r ox \h at { x } \ b eg i n { ca se s } s_{1} \cdot 2 {-x_{q}}, \text {if } x_q \leq \hat {b}, \\ s_{2} \cdot x_{q} s_{1}, \text {if } x_q \hat {b}. \end {cases} \label {Eq_Hybrid_Dequant} (7) Here, s1 defines the log2 quantization scale, and s2 deter- mines the uniform quantization scale. The threshold ˆb par- titions the quantization grid, assigning log2 quantization to [0,ˆb] and uniform quantization to [ˆb, 2k 1]. By adjusting s1 and ˆb, HLUQ offers a high degree of adaptability, allowing it to accommodate various activation distributions. If s1 spans the full range and ˆb is close to 2k 1, HLUQ behaves like a log2 quantizer. In contrast, if s1 and ˆb are near zero, it essentially acts as a uniform quantizer, ensuring uniform step sizes. For heavy-tailed and skewed post-GELU activations, HLUQ can be configured with intermediate values of s1 and ˆb, allowing it to capture small, densely distributed values using log2 quantization while retaining precision for large, sparse values through uniform quantization, as illustrated in Fig. 2 (left panel). Furthermore, HLUQ maintains the hardware efficiency of log2 and uniform quantization, introducing only minimal overhead to partition the input at s1. To initialize ˆb, s1, and s2, we introduce two auxiliary parameters, α and β, and search their optimal values dur- ing initial calibration by minimizing the following objective function: \ mat hop { \ar g } \m a thop {\min }_{\alpha ,\beta } \mathbb {E} \left [ \ \mathbf {X}\mathbf {W} - \hat {\mathbf {X}}\mathbf {W} \ _F 2 \right ] \label {Eq_HLUQ_Cali} (8) Here, α partitions the original activation range r, defin- ing the quantization scales s1 and s2 as s1 α r and s2 (1 α) r. Meanwhile, β determines the allocation of quantization grids between log2 and uniform quantiza- tion segments, setting the threshold ˆb as ˆb β (2k 1). Once α and β are obtained, the scales s1, s2, and the grid threshold ˆb are configured to initialize HLUQ quantizer for subsequent reconstruction training. 3.3.2. Channel-Aware Grouping The substantial inter-channel variation in activations presents a major obstacle for low-granularity per-tensor quantization, while fine-granularity per-channel quantiza- tion remains inefficient for hardware implementation. Inter- estingly, although linear projection activations exhibit sub- stantial inter-channel variation, their statistical properties 4 Channel-Aware Grouping (CAG) Linear Projection Activations: Channel Variation Challenge Distribution per Channel Per-Channel Quantization: Increasing Parameter Count Merging Similar Parameters during Reconstruction Reducing Parameter Count while Preserve Granularity Hybrid Log-Uniform Quantization (HLUQ) For Post-GELU Activations: Poor Resolution for Small Activations Poor Resolution for Large Activations Good Resolution for Entire Quantization Range Combining Two Types of Quantizer to Improve Resolution LayerNorm Projection Attention LayerNorm MLP.Linear-1 GELU MLP.Linear-2 Q K V O Figure 2. AHCPTQ framework: HLUQ refines quantization resolution for post-GELU activations, while CAG effectively groups parame- ters to manage channel-wise variations in linear projection activations to reduce quantization error. remain remarkably consistent across different samples. As shown in Fig. 3, the cosine similarity of normalized quan- tization parameters, searched over 100 samples per chan- nel, is consistently close to 1.0, indicating that the optimal quantization parameters for each channel are largely invari- ant across samples. This stability presents an opportunity to leverage shared quantization parameters within grouped channels, improving quantization efficiency while main- taining accuracy. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.847 Figure 3. Cosine similarity of normalized quantization parameter across 100 samples for each channel. To balance quantization granularity with hardware effi- ciency, we propose CAG, which achieves accuracy com- parable to per-channel quantization while significantly re- ducing parameter overhead, thereby enabling efficient on- chip deployment. Unlike static per-group quantization [42], the fundamental concept of CAG is to progressively group channels with similar activation distributions and assign them shared quantization parameters. As outlined in Al- gorithm 1, the process begins with quantization parame- ter initialization (scales and zero points) for each channel via model calibration, treating each channel as an indepen- dent group initially. Next, block-wise reconstruction is ap- plied to refine both quantization parameters and weights, Algorithm 1 Channel-Aware Grouping Input: Total iterations T, milestones t {T1, T2, . . . , TJ} Output: K groups of optimized parameters {si, zi}K i 1 1: Initialize quantization parameters {si, zi}N i 1 for N channels, G N 2: for t 1, 2, ..., T do 3: Update {si, zi}G i 1 according to Eq. 5 4: if t {T1, T2, . . . , TJ} then 5: Apply K-Means to {si, zi}G i 1 to form centroids {si, zi} GTj i 1 6: Assign each channel to the nearest centroid to form new groups 7: Update number of groups G GTj 8: end if 9: end for 10: return {si, zi}K i 1 minimizing quantization error as formulated in Eq. 5, en- suring alignment with each group s distribution characteris- tics. At designated milestones, channels with similar quan- tization parameters are progressively clustered, and the re- sulting centroids are adopted as shared quantization param- eters for all channels within a group. This iterative pro- cess continues until the target group count is reached. The right panel of Fig. 2 illustrates an example of this grouping process for linear projection activations in Token-to-Image Value cross-attention, demonstrating how CAG effectively reduces the number of groups while preserving quantization performance. With a group number of 4, the edge acceler- ator can minimize quantization parameter overhead, reduc- ing either data transmission or on-chip storage by 99.7 , as shown in Fig. 4. In AHCPTQ, we adopt on-chip stor- 5 age for quantization parameters, requiring only 144 regis- ters to store the scales and zero points of these 4 groups under 4-bit quantization. As depicted in Fig. 6, CAG main- tains accuracy comparable to per-channel quantization, of- fering high hardware efficiency while significantly enhanc- ing model performance. 3.4. Hardware Co-optimization In this subsection, we briefly describe the strategy of hard- ware co-optimization for the proposed HLUQ and CAG. To implement HLUQ, a dual processing-element (PE) archi- tecture is employed to facilitate matrix multiplication within the HLUQ quantizer. The uniform quantization branch uti- lizes a standard integer multiplier and an adder tree, while the log2 quantization branch employs a bit-shifter and a decimal adder tree. The grid ratio between the two quantiz- ers follows the 2 n rule, ensuring that the MSBs of quan- tized values function as label bits. These label bits enable the accelerator to efficiently route quantized values to their respective PEs. Finally, the outputs from both branches are merged during dequantization, producing the final result. To efficiently implement CAG, a small set of on-chip reg- isters is used to store quantization parameters, and a cus- tomized quantizer logic is designed. Specifically, weights and activations are reordered by channel indices, ensuring that grouped channels are sequentially aligned. Weights are pre-processed offline, while activations are reordered on- chip. This reordering is integrated into the quantization and dequantization processes within linear projection compu- tations. By employing counter-based logic for parameter switching, the design effectively minimizes hardware com- plexity while maintaining computational efficiency. Per-Tensor Quantization Per-Channel Quantization Param. Storage in DRAM 99.7 Reduction on Data Transfer Per-Channel Quantization Param. Storage On-chip 99.7 Reduction on Register Count AHCPTQ Configuration CAG Group 4 Figure 4. Hardware cost analysis of the linear projection layer in the SAM-H decoder under different quantization granularities. 4. Experiments 4.1. Experiment and Implementation Details We evaluate the effectiveness of AHCPTQ on instance seg- mentation using the mean Average Precision (mAP) met- ric on the COCO dataset[22]. The selected detectors in- clude CNN-based Faster RCNN[32] and YOLOX[7], as well as Transformer-based H-Deformable-DETR[12] and DINO [43]. The bounding boxes generated by these detec- tors serve as prompts for SAM. For CNN-based detectors, the box threshold is set to 0.05, while Transformer-based detectors utilize a set of 100 adaptive anchors. Following the PTQ4SAM framework [29], we randomly sample 32 training images and use the MinMax approach to initialize quantization parameters. Block reconstruction is performed over 20,000 iterations on both attention and MLP blocks. To ensure fair comparisons [24, 29, 39, 41], we exclude the first and last layers or blocks from quantization while keep- ing all others quantized. HLUQ is applied to all Linear- 2 activations in MLP blocks, with initial scales and grid thresholds determined by searching α {0.1, 0.3, 0.5} and β { 1 2, 1 4, 1 8}. CAG is applied to all linear projection acti- vations in Query Key Value projections of attention blocks and Linear-1 activations in MLP blocks, using a group num- ber of 4. For other activations, we adopt per-tensor asym- metric quantization. For weights, we apply per-channel asymmetric quantization to maintain alignment with base- line settings. 4.2. Experimental Results We evaluate AHCPTQ against baselines such as BRECQ [17], QDrop [39], and PTQ4SAM [29], with results summarized in Table 1 across four types of de- tectors. For SAM-L and SAM-H models, other baselines exhibit significant accuracy drops in the W5A5 configu- ration, while AHCPTQ reduces the mAP loss to within 2 . In the W4A4 configuration, where other baselines struggle to achieve basic functionality, AHCPTQ restores mAP to approximately 80 of the FP32 performance, particularly for SAM-H. For instance, AHCPTQ improves DINO s mAP from 43.8 to 47.6 in the W5A5 config- uration of SAM-H and from 2.3 to 36.6 in the W4A4 configuration of SAM-L, surpassing all baselines by a large margin. In the challenging SAM-B model, AHCPTQ doubles the mAP compared to PTQ4SAM in the W5A5 configuration, reduces the mAP loss to below 3 in W6A6, and restores mAP to a usable level in W4A4. For example, AHCPTQ improves H-DETR s mAP by PTQ4SAM from 2.8 to 14.1 , 16.9 to 32.1 , and 30.7 to 36.6 in W4A4, W5A5, and W6A6 configurations, respectively. These results indicate that AHCPTQ consistently surpasses baselines, addressing the challenges identified in Sec. 3.2 and advancing SAM quantization to lower bit-widths with superior effectiveness. 4.3. Ablation Studies Ablation of Components. Table 2 details the ablation re- sults for AHCPTQ, which are tested on YOLOX and H- Deformable-DETR in the W4A4 configuration. The re- 6 Table 1. Quantization results of AHCPTQ for instance segmentation on the COCO dataset across four detector types. Floating-point (FP32) mAP values for SAM-B L H are shown below each detector name for reference. AHCPTQ achieves a new state-of-the-art performance, outperforming all existing baselines. Detector Method SAM-B SAM-L SAM-H FP32: (B L H) W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 Faster R-CNN BRECQ 0.2 16.7 28.0 5.0 31.8 35.2 17.5 31.3 35.8 QDrop 2.3 12.9 26.2 0.8 31.9 35.0 6.0 32.6 36.0 33.1 36.0 36.8 PTQ4SAM 2.7 14.4 26.8 2.4 33.0 35.5 6.7 33.3 36.2 AHCPTQ 11.7 27.5 31.6 27.4 34.8 35.6 31.4 35.7 36.3 YOLOX BRECQ 0.2 19.0 31.9 6.3 35.3 39.4 19.7 34.7 39.7 QDrop 2.6 15.6 30.3 1.0 36.2 39.4 6.8 36.0 40.1 37.2 40.4 41.0 PTQ4SAM 3.8 18.4 30.9 2.4 37.1 39.9 7.4 37.1 40.3 AHCPTQ 13.4 31.8 35.4 31.0 39.1 40.0 35.2 40.0 40.4 H-DETR BRECQ 0.3 11.2 32.0 5.2 36.1 40.4 19.1 35.3 40.6 QDrop 2.0 13.1 30.5 1.3 37.0 40.3 6.9 37.0 41.1 38.2 41.5 42.0 PTQ4SAM 2.8 16.9 30.7 2.6 38.1 40.9 7.1 38.0 41.4 AHCPTQ 14.1 32.1 36.6 32.3 40.3 41.0 35.6 40.9 41.5 DINO BRECQ 0.2 13.5 34.8 3.6 41.4 47.0 20.7 40.3 47.2 QDrop 1.9 13.4 34.5 1.0 42.7 47.0 7.0 42.4 47.9 44.5 48.6 49.1 PTQ4SAM 1.9 17.6 35.1 2.3 44.1 47.8 8.9 43.8 48.2 AHCPTQ 16.8 36.7 41.9 36.6 46.8 47.9 41.2 47.6 48.3 Note: This paper reports the corrected performance of PTQ4SAM and QDrop after fixing a bug (refer to code repo.) in the PTQ4SAM [29] framework. sults demonstrate that each technique plays a vital role in enhancing SAM s performance, with the highest gains achieved when both CAG and HLUQ are applied together. For SAM-B and SAM-L models, inter-channel variance emerges as the primary challenge. Thus, while applying HLUQ alone leads to moderate improvements, applying CAG yields significant performance gains. Once the major challenge is addressed by CAG, HLUQ becomes essential for further improvement. For instance, HLUQ achieves an additional 8.3 mAP improvement in the SAM-L model of YOLOX when CAG is applied. For SAM-H, heavy-tailed and skewed activations become the dominant limitation. As a result, HLUQ contributes more significantly than CAG, and their combined application results in a 28 mAP im- provement. Ablation of Quantizers. To investigate the effectiveness of HLUQ, we analyze SAM performance by substitut- ing HLUQ with uniform and log2 quantizers, respectively. As the standard log2 quantizer cannot handle the negative branch of post-GELU activations, a floating-point bias is added for reasonable comparison. The results for YOLOX and H-Deformable-DETR under the W4A4 configuration are presented in Fig. 5. HLUQ significantly outperforms the uniform quantizer and surpasses the biased log2 quan- tizer, demonstrating its ability to improve resolution under low-bit quantization and reduce quantization error, thereby boosting SAM performance. 10.8 22.7 20.2 12.5 30.4 34.3 13.4 31.0 35.2 SAM-B SAM-L SAM-H 0 5 10 15 20 25 30 35 40 mAP ( ) Uniform Log2 w Bias HLUQ Standard Log2 is Not Applicable (a) YOLOX 11.2 22.1 20.5 13.5 31.4 34.8 14.1 32.3 35.6 SAM-B SAM-L SAM-H 0 5 10 15 20 25 30 35 40 mAP ( ) Uniform Log2 w Bias HLUQ Standard Log2 is Not Applicable (b) H-DETR Figure 5. Ablation study comparing the effectiveness of HLUQ with other quantizers in the W4A4 configuration. Dependence on Group Number. To analyze the effect of group number on SAM s performance in CAG, we evaluate mAP as the group number varies from 2 to 32 in YOLOX and H-Deformable-DETR under the W4A4 configuration. The results, presented in Fig. 6, also include per-tensor and per-channel configurations for reference. For SAM-L and SAM-H models, mAP increases sharply and saturates at a group number of 2, with slight fluctuations and marginal improvements closer to the per-channel configuration. For the SAM-B model, the turning point starts at a group num- ber of 4, with a larger performance gap of approximately 3 compared to the per-channel configuration. Consider- ing the need for a consistent configuration across models while minimizing hardware overhead, we choose a group 7 number of 4 in the AHCPTQ framework. Table 2. Results of the ablation study analyzing the contribution of components in the W4A4 configuration. Detector CAG HLUQ S.-B S.-L S.-H YOLOX 3.7 2.5 7.1 10.8 22.7 20.2 5.7 5.7 24.8 13.4 31.0 35.2 H-DETR 3.1 2.7 7.3 11.2 22.1 20.5 4.1 6.5 24.6 14.1 32.3 35.6 1 2 4 8 16 32 64 0 5 10 15 20 25 30 35 40 mAP ( ) Group Number SAM-B SAM-L SAM-H 5.7 8.8 13.4 13.7 13.6 16.9 16.2 5.7 31.9 31.0 31.3 32.4 32.6 33.2 24.8 35.3 35.2 35.4 35.7 36.3 36.4 Per-Tensor Per-Channel AHCPTQ Configuration: Group Number 4 (a) YOLOX 1 2 4 8 16 32 64 0 5 10 15 20 25 30 35 40 mAP ( ) Group Number SAM-B SAM-L SAM-H 4.1 10.4 14.1 14.4 14.5 14.5 17.1 6.5 33.4 32.3 32.3 33.6 33.6 34.0 24.6 35.7 35.6 35.9 36.3 36.3 37.0 Per-Tensor Per-Channel AHCPTQ Configuration: Group Number 4 (b) H-DETR Figure 6. Dependence of SAM performance on group number in CAG for the W4A4 configuration. 4.4. Hardware Validation To evaluate the resource efficiency and practical perfor- mance of AHCPTQ in real-world applications, we devel- oped an FPGA accelerator for executing the MLP block in the SAM mask decoder. CAG is applied to Linear-1 ac- tivations to mitigate channel-wise variation, while HLUQ is applied to Linear-2 activations to accommodate heavy- tailed and skewed post-GELU distributions. The acceler- ator is designed in Verilog, synthesized using Vivado De- sign Suite, and implemented on a Xilinx Artix UltraScale FPGA running at 200 MHz. The overall system consists of an FPGA accelerator for computation, a DDR4 DRAM for data buffering, and a host PC for transferring activa- tions via Ethernet. In accordance with Sec. 3.4, the accel- erator uses MSBs indexing to assign activations to appro- priate PEs either multipliers or bit-shifters for HLUQ. We fuse the activation reordering logic into the quantiza- tion dequantization steps for CAG and pipeline the quanti- zation process with activation functions, effectively elim- inating extra quantization overhead in HLUQ. To estab- lish a comparative benchmark, we implemented two base- lines: a standard FP32 accelerator and a default 8-bit inte- ger (INT8) accelerator. Floating-point operations, includ- ing multiply-and-accumulate in the FP32 accelerator and quantization dequantization process in the integer acceler- ator, were implemented using the Floating-Point Operation IP generator with on-chip DSP resources. As shown in Table 3, the FP32 accelerator is limited by the number of available on-chip DSP resources, supporting only 8 parallel PE lanes. In contrast, both the default INT8 and AHCPTQ INT4 accelerators replace DSP-based PEs with LUT-based PEs, allowing for a significant increase in parallelism to 32 and 64 lanes, respectively. Meanwhile, DSP resources in in- teger accelerators are allocated to quantization and dequan- tization processing, optimizing overall resource utilization for improved system-level performance. As a result, 4- bit AHCPTQ reduces the computational complexity and overhead of data movement considerably, thereby achieving 8.64 gains in energy efficiency and 7.89 speedup over its floating-point counterpart. This demonstrates strong po- tential for efficient SAM deployment on edge devices. Table 3. Analysis of resource utilization and system-level perfor- mance of AHCPTQ on an FPGA platform. FP32 INT8 AHCPTQ INT4 LUT Usage 19,071 43,015 34,199 DSP Usage 447 153 213 Parallelism 8 32 64 GOPS W 5.04 21.97 43.57 Speedup 1.00 3.96 7.89 5. Limitation This study focuses on evaluating the feasibility of CAG and HLUQ on FPGA and ASIC platforms. We expect that simi- lar enhancements in energy efficiency and speedup could be realized on CPU and GPU platforms due to the reduction in FLOPs and data movement overhead. The deployment of AHCPTQ on these platforms is left for future research. 6. Conclusion This paper presents AHCPTQ, a novel PTQ framework for SAM, designed to efficiently handle complex activa- tion distributions while ensuring compatibility with hard- ware acceleration. AHCPTQ incorporates HLUQ to man- age heavy-tailed and skewed activations, and CAG to ad- dress high inter-channel variation, significantly improving quantization performance. Extensive experiments on mul- tiple SAM variants and quantization settings demonstrate its superior performance, establishing a new state-of-the- art in SAM quantization. Moreover, FPGA-based evalua- tions confirm its real-world deployability with substantial speedup and energy efficiency gain, providing valuable in- sights for practical applications. 8 Acknowledgments This research was supported in part by the JST CREST JP- MJCR21D2 including AIP challenge program, Japan, JSPS Kakenhi 23H00467, Futaba Foundation, Asahi Glass Foun- dation, Telecommunications Advancement Foundation, and JST Doctoral Program Student Support Project. We also thank the anonymous reviewers and area chairs for their constructive feedback. References [1] Ron Banner, Yury Nahshan, Daniel Soudry, et al. Post train- ing 4-bit quantization of convolutional networks for rapid- deployment. In Proceedings of the Advances in Neural In- formation Processing Systems, pages 7950 7958, 2019. 4 [2] Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Ji- long Chen, Lei Jiang, et al. Sam-med2d. arXiv preprint arXiv:2308.16184, 2023. 1 [3] Vladimir Chikin and Mikhail Antiukh. Data-free network compression via parametric non-uniform mixed precision quantization. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 450 459, 2022. 2, 4 [4] Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei, and Xianglong Liu. Towards accurate post- training quantization for vision transformer. In Proceedings of the 30th ACM international conference on multimedia, pages 5380 5388, 2022. 2, 4 [5] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. 1 [6] Natalia Frumkin, Dibakar Gope, and Diana Marculescu. Jumping through local minima: Quantization in the loss landscape of vision transformers. In Proceedings of the IEEE CVF International Conference on Computer Vision, pages 16978 16988, 2023. 2 [7] Z Ge. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021. 6 [8] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differ- entiable soft quantization: Bridging full-precision and low- bit neural networks. In Proceedings of the IEEE CVF inter- national conference on computer vision, pages 4852 4861, 2019. 1 [9] Cheeun Hong, Heewon Kim, Junghun Oh, and Ky- oung Mu Lee. Daq: distribution-aware quantization for deep image super-resolution networks. arXiv preprint arXiv:2012.11230, 2020. 2, 4 [10] Mark Horowitz. 1.1 computing s energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10 14, 2014. 4, 2 [11] Zejiang Hou and Sun-Yuan Kung. Multi-dimensional vi- sion transformer compression via dependency guided gaus- sian process search. In Proceedings of the IEEE CVF Con- ference on Computer Vision and Pattern Recognition, pages 3669 3678, 2022. 1 [12] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with hybrid matching. In Proceedings of the IEEE CVF con- ference on computer vision and pattern recognition, pages 19702 19712, 2023. 6 [13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chlo e Rolland, Laura Gustafson, Tete Xiao, Spencer White- head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ar, and Ross B. Girshick. Segment anything. 2023 IEEE CVF In- ternational Conference on Computer Vision (ICCV), pages 3992 4003, 2023. 1, 2 [14] Raghuraman Krishnamoorthi. Quantizing deep convolu- tional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. 1 [15] Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of- two quantization: An efficient non-uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. 4 [16] Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of- two quantization: An efficient non-uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. 1 [17] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruc- tion. arXiv preprint arXiv:2102.05426, 2021. 2, 6, 4, 5 [18] Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng Gao, and Guodong Guo. Q-vit: Accurate and fully quantized low-bit vision transformer. Advances in neural information processing systems, 35:34451 34463, 2022. 1 [19] Zhikai Li and Qingyi Gu. I-vit: Integer-only quantization for efficient vision transformer inference. In Proceedings of the IEEE CVF International Conference on Computer Vi- sion, pages 17065 17075, 2023. 1 [20] Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and Qingyi Gu. Patch similarity aware data-free quantization for vision transformers. In European conference on computer vision, pages 154 170. Springer, 2022. 2 [21] Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu. Repq- vit: Scale reparameterization for post-training quantization of vision transformers. In Proceedings of the IEEE CVF In- ternational Conference on Computer Vision, pages 17227 17236, 2023. 2, 4 [22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740 755. Springer, 2014. 6 [23] Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and Shuchang Zhou. Fq-vit: Post-training quantization for fully quantized vision transformer. arXiv preprint arXiv:2111.13824, 2021. 2, 4 [24] Jiawei Liu, Lin Niu, Zhihang Yuan, Dawei Yang, Xinggang Wang, and Wenyu Liu. Pd-quant: Post-training quantiza- tion based on prediction difference metric. In Proceedings of 9 the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 24427 24437, 2023. 6, 4 [25] Shih-Yang Liu, Zechun Liu, and Kwang-Ting Cheng. Oscillation-free quantization for low-bit vision transformers. In International Conference on Machine Learning, pages 21813 21824. PMLR, 2023. 1 [26] Xiaoyu Liu, Xin Ding, Lei Yu, Yuanyuan Xi, Wei Li, Zhi- jun Tu, Jie Hu, Hanting Chen, Baoqun Yin, and Zhiwei Xiong. Pq-sam: Post-training quantization for segment any- thing model. In European Conference on Computer Vision, pages 420 437. Springer, 2024. 2 [27] Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 20321 20330, 2023. 2 [28] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision trans- former. Advances in Neural Information Processing Systems, 34:28092 28103, 2021. 1 [29] Chengtao Lv, Hong Chen, Jinyang Guo, Yifu Ding, and Xi- anglong Liu. Ptq4sam: Post-training quantization for seg- ment anything. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 15941 15951, 2024. 2, 3, 6, 7, 5 [30] Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallab- hula, Makram Chahine, Daniel M Vogt, Robert J Wood, An- tonio Torralba, and Daniela Rus. Follow anything: Open- set detection, tracking, and following in real-time. IEEE Robotics and Automation Letters, 9(4):3283 3290, 2024. 1 [31] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Chris- tos Louizos, and Tijmen Blankevoort. Up or down? adap- tive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197 7206. PMLR, 2020. 2 [32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE transactions on pattern analysis and machine intelligence, 39(6):1137 1149, 2016. 6 [33] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything- 3d: Towards single-view anything reconstruction in the wild. arXiv preprint arXiv:2304.10261, 2023. 1 [34] Huihong Shi, Haikuo Shao, Wendong Mao, and Zhongfeng Wang. Trio-vit: Post-training quantization and acceleration for softmax-free efficient vision transformer. arXiv preprint arXiv:2405.03882, 2024. 2 [35] Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yi- hao Chen, Houqiang Li, Yunhe Wang, and Xinghao Chen. Tinysam: Pushing the envelope for efficient segment any- thing model. arXiv preprint arXiv:2312.13789, 2023. 2 [36] Longguang Wang, Xiaoyu Dong, Yingqian Wang, Li Liu, Wei An, and Yu Kuen Guo. Learnable lookup table for neural network quantization. 2022 IEEE CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 12413 12423, 2022. 2, 4 [37] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit- split and stitching. In Proceedings of the International Con- ference on Machine Learning, pages 9847 9856, 2020. 4 [38] Yonghui Wang, Wengang Zhou, Yunyao Mao, and Houqiang Li. Detect any shadow: Segment anything for video shadow detection. IEEE Transactions on Circuits and Systems for Video Technology, 34(5):3782 3794, 2024. 1 [39] Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization. arXiv preprint arXiv:2203.05740, 2022. 2, 3, 6, 4, 5 [40] Tian Xia, Boran Zhao, Jian Ma, Gelin Fu, Wenzhe Zhao, Nanning Zheng, and Pengju Ren. An energy-and-area- efficient cnn accelerator for universal powers-of-two quan- tization. IEEE Transactions on Circuits and Systems I: Reg- ular Papers, 70(3):1242 1255, 2022. 4 [41] Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In European conference on computer vision, pages 191 207. Springer, 2022. 2, 6, 4 [42] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models. arXiv preprint arXiv:2304.01089, 2023. 5 [43] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. 6 [44] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt- ing Pan, Xianzheng Ma, Hao Dong, Peng Gao, and Hong- sheng Li. Personalize segment anything model with one shot. arXiv preprint arXiv:2305.03048, 2023. 1 [45] Dehua Zheng, Wenhui Dong, Hailin Hu, Xinghao Chen, and Yunhe Wang. Less is more: Focus attention for efficient detr. In Proceedings of the IEEE CVF international conference on computer vision, pages 6674 6683, 2023. 1 [46] Yunshan Zhong, Mingbao Lin, Xunchao Li, Ke Li, Yun- hang Shen, Fei Chao, Yongjian Wu, and Rongrong Ji. Dy- namic dual trainable bounds for ultra-low precision super- resolution networks. In European Conference on Computer Vision, pages 1 18. Springer, 2022. 4 [47] Yunshan Zhong, Jiawei Hu, Mingbao Lin, Mengzhao Chen, and Rongrong Ji. I s-vit: An inclusive stable method for pushing the limit of post-training vits quantization. arXiv preprint arXiv:2311.10126, 2023. 2, 4 [48] Yunshan Zhong, Jiawei Hu, You Huang, Yuxin Zhang, and Rongrong Ji. Erq: Error reduction for post-training quanti- zation of vision transformers. In Proceedings of the Interna- tional Conference on Machine Learning (ICML), 2024. 2 [49] Yunshan Zhong, You Huang, Jiawei Hu, Yuxin Zhang, and Rongrong Ji. Towards accurate post-training quantization of vision transformers via error reduction. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1 18, 2025. 2 10 AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model Supplementary Material A. Analysis of Inter-Channel Variation and Inter-Sample Similarity in SAM Model In this section, we provide an in-depth analysis of inter- channel variation and inter-sample similarity using the SAM-B model with YOLOX as the prompt detector. We extract the quantization ranges for channel indices 50 to 100 to examine channel-wise variation across multiple lay- ers where CAG is applied. Additionally, we determine the optimal scale and zero point for each channel using different samples to assess parameter consistency across 100 sam- ples. In the image encoder, Figs. 7a and 8a show that QKV projection and Linear-1 activations in the MLP block ex- hibit channel-wise variation due to LayerNorm operations. 56 63 70 77 84 91 98 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Range Encoder: QKV Projection Channel Index (a) Range distribution for channel indices 50 to 100. 0 100 200 300 400 500 600 700 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.937 (b) Cosine similarity across 100 samples for each channel. Figure 7. Range distribution and cosine similarity of QKV projec- tion activations in the image encoder. In the mask decoder, channel-wise variation in certain layers poses a major challenge for low-bit quantization. In Token-to-Image cross-attention, linear projection activa- tions in the Query and Value projections serve as the pri- mary challenges, as shown in Fig. Figs. 1b and 9a. No- tably, activation outliers in the Query projection extend the quantization range to approximately 400, severely increas- ing quantization error under per-tensor quantization. In Image-to-Token cross-attention, linear projection acti- vations in the Key and Value projections exhibit distribution variations that degrade SAM s performance. As shown in 56 63 70 77 84 91 98 8 6 4 2 0 2 4 6 8 Range Encoder: MLP Linear-1 Channel Index (a) Range distribution for channel indices 50 to 100. 0 100 200 300 400 500 600 700 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.954 (b) Cosine similarity across 100 samples for each channel. Figure 8. Range distribution and cosine similarity of Linear-1 ac- tivations in the MLP block of the image encoder. 56 63 70 77 84 91 98 200 150 100 50 0 50 100 150 200 Range Decoder: Token-to-Image Q Projection Channel Index (a) Range distribution for channel indices 50 to 100. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.937 (b) Cosine similarity across 100 samples for each channel. Figure 9. Range distribution and cosine similarity of pre- projection activations for Token-to-Image Query in the mask de- coder. Figs. 10a and 11a, Key activations remain relatively stable, whereas Value activations demonstrate higher variance. Lastly, the range distribution of Linear-1 activations in the MLP block of the mask decoder is shown in Fig. 12a. 1 56 63 70 77 84 91 98 8 6 4 2 0 2 4 6 8 Range Decoder: Image-to-Token K Projection Channel Index (a) Range distribution for channel indices 50 to 100. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.955 (b) Cosine similarity across 100 samples for each channel. Figure 10. Range distribution and cosine similarity of pre- projection activations for Image-to-Token Key in the mask de- coder. 56 63 70 77 84 91 98 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 Range Decoder: Image-to-Token V Projection Channel Index (a) Range distribution for channel indices 50 to 100. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.854 (b) Cosine similarity across 100 samples for each channel. Figure 11. Range distribution and cosine similarity of pre- projection activations for Image-to-Token Value in the mask de- coder. Fortunately, we verified that in all layers exhibiting se- vere channel-wise variation, the optimal quantization pa- rameters remain stable across different samples. As shown in Figs. 3, 7b, 8b, 9b, 10b, 11b and 12b, the normalized cosine similarity scores remain consistently high across most channels. Therefore, the CAG-based grouping strat- egy proves to be a highly reliable approach for enhancing SAM s quantization performance. 56 63 70 77 84 91 98 0.10 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 0.10 Range Decoder: MLP Linear-1 Channel Index (a) Range distribution for channel indices 50 to 100. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.904 (b) Cosine similarity across 100 samples for each channel. Figure 12. Range distribution and cosine similarity of Linear-1 activations in the MLP block of the mask decoder. B. Hardware Cost Benchmark Across Differ- ent Quantization Granularity To benchmark the hardware cost associated with different quantization granularities, we analyze the implementation overhead of the SAM-H model under three configurations: per-tensor quantization, per-channel quantization, and CAG in AHCPTQ. Deep learning accelerators typically handle quantization parameters in two ways: (1) Storing scales and zero points in on-chip registers. This approach completely eliminates data transfer between the off-chip DRAM and the accelerator, ensuring zero trans- mission latency and cost. However, it increases area and resource overhead due to the large number of registers re- quired. (2) Storing scales and zero points in off-chip DRAM. This method reduces the on-chip resource overhead but in- curs substantial energy and latency costs due to frequent DRAM accesses [10]. As illustrated in Fig. 4, any combination between these two approaches follows a trade-off curve reducing one overhead inevitably increases the other. To mitigate this trade-off, we propose CAG, which clusters quantization pa- rameters to significantly reduce their count. By grouping scale and zero-point values into just four clusters, CAG reduces the required registers in (1) by 99.7 or the data transfer cost in (2) by 99.7 . This approach enables hard- ware efficiency close to per-tensor quantization while main- taining accuracy comparable to per-channel quantization. Since only 144 registers are needed to store four groups of scale and zero points in 4-bit quantization, we opt to store them on-chip, ensuring a more efficient and simplified hard- 2 ware design. C. Hardware Implementation Details Our AHCPTQ approach is implemented on an FPGA, with a custom accelerator designed to validate the quantization strategy. Fig. 13a depicts the validation system, which includes a host PC for data transfer via Ethernet, DDR4 DRAM for data buffering, and a Xilinx Artix UltraScale FPGA configured with the accelerator. The general ar- chitecture of the accelerator, shown in Fig. 13b, incorpo- rates four groups of buffers that leverage on-chip BRAM resources. These buffers temporarily store input activa- tions, model weights, and output activations before and af- ter quantization. The accelerator supports two configura- tions of PEs: (1) an 8-input integer multiplier-and-adder PE for uniform quantization, where both inputs and out- puts are integers and partial sums are updated using an ac- cumulator, and (2) an 8-input decimal bit-shift-and-adder PE for log2 quantization, with integer inputs, decimal out- puts, and a decimal accumulator for partial sums. In our 4- bit AHCPTQ implementation, the accelerator uses 64 lanes for multiplier and bit-shift PEs. To enable HLUQ, weights and activations must be dynamically allocated to their cor- responding PEs. This allocation logic is embedded in the controller, which indexes the MSBs of activations. When data is read from the IA buffer, weights and activations are automatically distributed to the corresponding PE types. After completing the integer and decimal inner product operations, the output values are transferred to the quantiza- tion processor in the FP32 domain. A small set of quantiza- tion parameter registers loads the scales required for the cur- rent and next layers, switching addresses to provide these scales to the quantization processor. The output activations are then fed into the dequantization unit, where, if HLUQ is applied, the uniform and log2 branches are merged. Subse- quently, the activations pass through the activation functions and quantization units, ensuring the resulting integer values can be directly used by the next layer. Finally, the activa- tions are sent to the output buffer. The dequantization, acti- vation function, and quantization processes are designed in a pipeline to minimize latency, which is critical for HLUQ deployment in practical applications. Weights are grouped offline and reordered based on group indices, while activations are reordered on-the-fly. At the start of layer computation, the accelerator accesses the DRAM to transfer weights and activations to the weight and IA buffers. Simultaneously, the quantization processor loads the quantization parameters required for quantization and dequantization. The controller then dispatches paired weights and activations to the appropriate PE lanes based on the MSB of the activations. For instance, with β 1 2 as described in Sec. 3.3.1, activations with an MSB of 1 are routed to multiplier PE lanes, while those with an MSB of PC Host Ethernet DDR4 FPGA Accelerator (a) Evaluation system. DRAM 8 8 Processing Core Multiplier PE Lanes Bit-Shift PE Lanes Weight Buffer IA Buffer Controller MSB Index MSB Index Quantization Reorder Buffer Output Buffer Pipeline Processing Quant Param Registers FP32 Processing DeQuantization Arithmetic Activation Functions Quantization Arithmetic (b) Accelerator architecture. Figure 13. Overview of the evaluation system and the accelerator architecture for the AHCPTQ configuration. 0 are sent to bit-shift PE lanes. Once computation is com- pleted, the results stored in the accumulators are clustered into two categories and passed to their respective uniform or log2 dequantization units. The dequantized FP32 activa- tions are then written back to the quantization buffer using a tailored addressing logic to restore their default sequence. Following this, the activations undergo the activation func- tion and quantization for the next layer before being sent to the output buffer and ultimately recycled back to DRAM. To streamline the process, the reorder logic is fused into the DRAM controller, which writes activations to grouped ad- dresses in DRAM when CAG is applied to the next layer. We implemented the RTL of the accelerator, Ethernet interface, and DRAM controller in Verilog, synthesized the design using Vivado Design Suite, and deployed it on the ALINX AXAU15 development board. As illustrated in Fig. 14, weights and activations are stored in on-board DDR4 DRAM and transferred via Ethernet from the host PC. The accelerator operates at 200 MHz to assess speedup and energy efficiency. For comparison, we implemented two baseline accelerators alongside our AHCPTQ config- 3 PC Host ALINX AXAU15 Dev. Board LAN Cable Figure 14. FPGA validation environment. uration: (1) a standard FP32 implementation, (2) a default INT8 implementation. In all designs, floating-point opera- tions such as quantization and dequantization are handled by an IP generator utilizing on-chip DSP resources. The detailed experimental results are presented in Sec. 4.4. D. Experiment on Vision Transformers To ensure that AHCPTQ generalizes to other vision models and tasks, we evaluate its effectiveness on ImageNet for im- age classification using DeiT. We integrate AHCPTQ into I S-ViT [47], the latest state-of-the-art PTQ framework, by replacing its original post-GELU quantizer with HLUQ. Table 4. Comparison of W4A4 PTQ methods on DeiT based on image classification accuracy on the ImageNet dataset. Method Opti. DeiT-T DeiT-S DeiT-B FQ-ViT [23] 0.10 0.10 0.10 PTQ4ViT [41] 36.96 34.08 64.39 APQ-ViT [4] 47.94 43.55 67.48 BRECQ [17] 55.63 63.73 72.31 QDrop [39] 61.93 68.27 72.60 PD-Quant [24] 62.46 71.21 73.76 RepQ-ViT [21] 57.43 69.03 75.61 I S-ViT [47] 65.21 75.81 79.97 AHCPTQ 66.11 76.12 80.07 In vision transformers (ViTs), inter-channel variation can be effectively addressed by reparameterizing Layer- Norm s weight and bias, as LayerNorm consistently pre- cedes the QKV linear projection. Consequently, we follow the settings of RepQ-ViT [21] and I S-ViT [47], applying reparameterization to reduce inter-channel variance, mak- ing CAG unnecessary in this case. However, in SAM s de- coder, the LayerNorm placement differs significantly from ViT s image encoder, making efficient reparameterization infeasible. This fundamental difference motivated the in- troduction of CAG as a dedicated PTQ solution for SAM with high hardware efficiency. We perform PTQ on DeiT-T, DeiT-S, and DeiT-B, comparing AHCPTQ against static PTQ methods (FQ- ViT [23], PTQ4ViT [41], APQ-ViT [4], RepQ-ViT [21]) and optimization-based PTQ methods (BRECQ [17], QDrop [39], PD-Quant [24], I S-ViT [47]). As shown in Table 4, AHCPTQ achieves the highest classification accu- racy in W4A4 quantization, surpassing all competing meth- ods. E. Parameter Sensitivity Analysis To initialize ˆb, s1, and s2 for subsequent optimization in HLUQ, we perform a grid search over two parameters, α and β. We extend the search to α {0.1, 0.2, . . . , 0.9} and β { 1 8, 1 4, . . . , 7 8} on SAM-B with YOLOX. The resulting W4A4 W5A5 W6A6 mAP scores of 13.1 32.0 35.3 align with the reported values as shown in Table 5, indicating that quantization performance is empirically insensitive to these parameters and a limited subset suffices. Table 5. Parameter sensitivity analysis on SAM-B with YOLOX. Method W4A4 W5A6 W6A6 Default 13.4 31.8 35.4 Extended Search 13.1 32.0 35.3 F. Generalizing CAG to Weight Grouping To ensure a fair comparison with existing baselines, we ap- ply per-channel quantization to model weights. However, CAG can also be extended to weight quantization, poten- tially broadening its applicability. To evaluate this, we ap- ply CAG with 32 groups to all model weights on SAM- B with YOLOX. The resulting quantization performance is comparable to the per-channel baseline as shown in Table 6, demonstrating CAG s generalizability to model weights. Table 6. Performance analysis of applying CAG with 32 groups to the model weights of SAM-B using YOLOX. Method W4A4 W5A6 W6A6 Default 13.4 31.8 35.4 Weight Grouping 12.3 30.8 34.3 4 Floating-Point BRECQ QDrop PTQ4SAM AHCPTQ Figure 15. Qualitative comparison of segmentation masks generated by different quantization methods on SAM-B with YOLOX. Our AHCPTQ closely matches the floating-point reference, significantly outperforming other baselines. G. Comparison of Visualization Results We further provide visualization results on W4A4 quan- tization of SAM-H using YOLOX, comparing AHCPTQ with existing quantization methods, including BRECQ [17], QDrop [39], and PTQ4SAM [29], as illustrated in Fig. 15. Qualitatively, our AHCPTQ consistently generates segmen- tation masks closely resembling those obtained from the original floating-point model, preserving finer structural details and accurate object boundaries. In contrast, the masks produced by other PTQ methods exhibit notable degradations. For instance, QDrop and PTQ4SAM often fail to capture intricate object contours, resulting in coarse segmentation masks and inaccurate boundary delineations. BRECQ, while generally performing better than QDrop and PTQ4SAM, still experiences noticeable detail loss and frag- mented segmentation in complex regions. Overall, the vi- sualization outcomes clearly demonstrate that AHCPTQ ef- fectively addresses the quantization challenges inherent in SAM, providing superior segmentation quality comparable to the floating-point baseline, thus confirming its effective- ness and robustness in practical low-bit deployment scenar- ios. 5\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nAHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model Wenlun Zhang1 Yunshan Zhong2 Shimpei Ando1 Kentaro Yoshioka1 1Keio University 2Hainan University {wenlun zhang, shimpeiando, Abstract The Segment Anything Model (SAM) has demonstrated strong versatility across various visual tasks. However, its large storage requirements and high computational cost pose challenges for practical deployment. Post-training quantization (PTQ) has emerged as an effective strategy for efficient deployment, but we identify two key challenges in SAM that hinder the effectiveness of existing PTQ meth- ods: the heavy-tailed and skewed distribution of post-GELU activations, and significant inter-channel variation in lin- ear projection activations. To address these challenges, we propose AHCPTQ, an accurate and hardware-efficient PTQ method for SAM. AHCPTQ introduces hardware- compatible Hybrid Log-Uniform Quantization (HLUQ) to manage post-GELU activations, employing log2 quantiza- tion for dense small values and uniform quantization for sparse large values to enhance quantization resolution. Ad- ditionally, AHCPTQ incorporates Channel-Aware Group- ing (CAG) to mitigate inter-channel variation by progres- sively clustering activation channels with similar distri- butions, enabling them to share quantization parameters and improving hardware efficiency. The combination of HLUQ and CAG not only enhances quantization effective- ness but also ensures compatibility with efficient hardware execution. For instance, under the W4A4 configuration on the SAM-L model, AHCPTQ achieves 36.6 mAP on in- stance segmentation with the DINO detector, while achiev- ing a 7.89 speedup and 8.64 energy efficiency over its floating-point counterpart in FPGA implementation. Code is available at 1. Introduction The Segment Anything Model (SAM) [13] is a power- ful tool for promptable segmentation, demonstrating strong zero-shot performance across diverse visual domains and Corresponding author. broad applicability in real-world scenarios [2, 30, 33, 38, 44]. However, its large-scale parameters, substantial stor- age demands, and high computational costs pose significant challenges for deployment on edge devices [11, 19, 28, 45].\n\n--- Segment 2 ---\nbroad applicability in real-world scenarios [2, 30, 33, 38, 44]. However, its large-scale parameters, substantial stor- age demands, and high computational costs pose significant challenges for deployment on edge devices [11, 19, 28, 45]. 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.1 0.2 0.3 Relative Frequency Distribution Post-GELU Activations (a) Challenge 1: Heavy-tailed and skewed post-GELU activations (from the 10th block of the image encoder). 56 63 70 77 84 91 98 1.0 0.8 0.6 0.4 0.2 0.0 0.2 0.4 0.6 0.8 1.0 Channel Index Range Token-to-Image: Linear Projection Activations (b) Challenge 2: Inter-channel variation for channel indices 50 to 100 (from the first block of the mask decoder). Figure 1. Challenges in SAM quantization (Data obtained from the SAM-B model with the YOLOX detector). To tackle this challenge, model quantization has been widely adopted to replace floating-point weights and acti- vations with low-bit representations, reducing storage over- head and enabling efficient integer-based computations, making it well-suited for edge devices with constrained resources [14]. One prominent approach is Quantization- Aware Training (QAT), which incorporates quantization ef- fects during training, allowing the model to adapt to quan- tized weights and activations [5, 8, 16, 18, 25]. However, applying QAT to SAM is impractical due to the compu- tational expense of SAM training, which relies on a co- developed data engine and consequently utilizes the SA- 1 arXiv:2503.03088v2 [cs.CV] 9 Jul 2025 1B dataset comprising 1.1B masks and 11M images [13]. As a more practical alternative, Post-Training Quantization (PTQ) has gained increasing attention. PTQ requires only a small calibration dataset, significantly reducing data and computational demands while maintaining competitive ac- curacy [4, 6, 20, 23, 27, 34, 47 49].\n\n--- Segment 3 ---\nAs a more practical alternative, Post-Training Quantization (PTQ) has gained increasing attention. PTQ requires only a small calibration dataset, significantly reducing data and computational demands while maintaining competitive ac- curacy [4, 6, 20, 23, 27, 34, 47 49]. PTQ4SAM [29] has recently demonstrated the feasibil- ity of applying PTQ to SAM [26, 35], utilizing equivalent sign transformations and adaptive resolution quantization to accommodate its unique activation distributions. Despite these advancements, our study reveals that existing PTQ methods suffer from quantization collapse at 4-bit and se- vere accuracy degradation at 5-bit, limiting their practical utility for ultra-low-bit deployment. As presented in Ta- ble 1, PTQ4SAM achieves only 3.8 mAP at 4-bit and 18.4 mAP at 5-bit quantization on the YOLOX detector with the SAM-B model, compared to 37.2 and 40.4 mAP in floating-point, respectively. Such substantial accu- racy loss significantly hampers the feasibility of deploying SAM on edge devices, motivating the need for more effec- tive ultra-low-bit quantization techniques. In this paper, we identify two additional challenges that limit PTQ performance on the SAM model. First, as il- lustrated in Fig. 1a, post-GELU activations exhibit an ex- tremely imbalanced distribution, where numerous small values are densely clustered within a narrow range, while sparse large values extend across a wide range. This imbal- ance poses a significant challenge for traditional hardware- friendly uniform and log2 quantizers. Although non- uniform quantizers [3, 9, 36] can better adapt to this dis- tribution, they introduce substantial challenges in hardware implementation. Second, activations in Query Key Value linear projections and Linear-1 of MLP exhibit high inter- channel variance, as exemplified in Fig. 1b. Such vari- ations render per-tensor quantization ineffective a small scale factor results in large quantization errors for channels with wide dynamic ranges, while a large scale factor ze- ros out activations in channels with small dynamic ranges. Although per-channel quantization improves accuracy, it in- troduces deployment inefficiencies due to the high overhead of storing and transferring quantization parameters.\n\n--- Segment 4 ---\nSuch vari- ations render per-tensor quantization ineffective a small scale factor results in large quantization errors for channels with wide dynamic ranges, while a large scale factor ze- ros out activations in channels with small dynamic ranges. Although per-channel quantization improves accuracy, it in- troduces deployment inefficiencies due to the high overhead of storing and transferring quantization parameters. Ad- dressing these challenges requires a quantization strategy that both closely aligns with activation distributions and re- mains compatible with efficient hardware deployment. To overcome the challenges in PTQ for SAM, we present AHCPTQ, a novel approach that enhances quan- tization effectiveness while ensuring hardware compati- bility. AHCPTQ addresses the two identified challenges through Hybrid Log-Uniform Quantization (HLUQ) and Channel-Aware Grouping (CAG), respectively. HLUQ in- novatively employs log2 quantization for densely clus- tered small values and uniform quantization for sparse but widely distributed large values, effectively adapting to the heavy-tailed and skewed distribution of post-GELU activa- tions while maintaining hardware-efficient implementation. Meanwhile, CAG selectively clusters channels with simi- lar statistical properties, assigning a shared quantization pa- rameter to each group. This method achieves accuracy com- parable to per-channel quantization while reducing on-chip register overhead by 99.7 and enabling efficient storage of quantization parameters within a compact set of on-chip registers. As a result, AHCPTQ significantly reduces accu- racy loss at 5-bit PTQ and, to the best of our knowledge, is the first to enable 4-bit PTQ on SAM. Our main contribu- tions are summarized as follows: We identify two key challenges limiting PTQ perfor- mance on SAM and propose AHCPTQ, a framework that integrates CAG and HLUQ to achieve a balance between quantization effectiveness and hardware feasibility. Extensive experiments show that AHCPTQ consistently surpasses previous PTQ methods, setting a new state-of- the-art in SAM quantization.\n\n--- Segment 5 ---\nOur main contribu- tions are summarized as follows: We identify two key challenges limiting PTQ perfor- mance on SAM and propose AHCPTQ, a framework that integrates CAG and HLUQ to achieve a balance between quantization effectiveness and hardware feasibility. Extensive experiments show that AHCPTQ consistently surpasses previous PTQ methods, setting a new state-of- the-art in SAM quantization. Under the W4A4 configura- tion, AHCPTQ achieves 36.6 mAP on instance segmen- tation with the DINO detector on SAM-L, representing a substantial improvement over PTQ4SAM, which fails to function under the same conditions. We further develop an FPGA-based accelerator to eval- uate AHCPTQ s hardware efficiency. Our results indi- cate that AHCPTQ delivers a 7.89 speedup and 8.64 energy efficiency improvement over floating-point imple- mentations, demonstrating superior resource utilization. 2. Related Works PTQ utilizes a small calibration dataset to determine quan- tization parameters, enabling rapid deployment on edge devices without requiring extensive retraining. This pa- per focuses on learning-based PTQ methods, which lever- age optimization techniques to derive optimal quantiza- tion parameters, often outperforming static PTQ methods. AdaRound [31] identifies the sensitivity of weight rounding and introduces an optimization technique to reduce overall model loss. BRECQ [17] employs block reconstruction to strike a balance between cross-layer dependency and gen- eralization error. QDrop [39] integrates dropout into the reconstruction process to improve the flatness of the opti- mized models. Despite their success, these methods are pri- marily designed for CNN-based models and face challenges when applied to Transformer architectures. In the do- main of Transformer-based models, FQ-ViT [23] improves granularity using powers-of-two scale and Log-Int-Softmax while maintaining hardware efficiency. RepQ-ViT [21] eliminates parameter overhead in per-channel quantiza- tion by conducting reparameterization techniques to post- LayerNorm activations.\n\n--- Segment 6 ---\nIn the do- main of Transformer-based models, FQ-ViT [23] improves granularity using powers-of-two scale and Log-Int-Softmax while maintaining hardware efficiency. RepQ-ViT [21] eliminates parameter overhead in per-channel quantiza- tion by conducting reparameterization techniques to post- LayerNorm activations. ERQ [48, 49] minimizes quan- tization errors via ridge regression, while PTQ4ViT [41] employs a twin uniform quantizer to efficiently handle 2 post-Softmax and post-GELU activations. While these ap- proaches offer insights for mitigating challenges in SAM, they are not easily adaptable to learning-based PTQ, and the mask decoder architecture of SAM restricts reparameteriza- tion techniques. PTQ4SAM [29], the first PTQ method spe- cialized for SAM, introduces bimodal integration and adap- tive log quantization to address unique distribution chal- lenges while maintaining hardware efficiency. However, it fails to achieve 4-bit quantization functionality, motivat- ing us to explore additional constraints and propose novel strategies to further improve performance. 3. Methodology 3.1. Preliminaries 3.1.1. Quantizer Quantization discretizes continuous values into low-bit rep- resentations, enabling efficient computation and reducing memory usage. Two widely adopted quantization schemes are uniform quantization and log2 quantization. The uni- form quantizer divides the input range into equally spaced intervals, mapping each value to the closest quantized level: x _{q} \ t e x t {c la m p } \left ( \left \lfloor \frac {x}{s} \right \rceil z, 0, 2 k - 1 \right ). \label {Eq_Uniform_Quant} (1) x \ a p pro x \hat {x} s \cdot (x_{q} - z). \label {Eq_Uniform_Dequant} (2) Here, x is the original floating-point input, xq is the quan- tized integer representation, k is the bit-width, s is the scale factor, and z is the zero point. The rounding function en- sures proper discretization.\n\n--- Segment 7 ---\n\label {Eq_Uniform_Dequant} (2) Here, x is the original floating-point input, xq is the quan- tized integer representation, k is the bit-width, s is the scale factor, and z is the zero point. The rounding function en- sures proper discretization. Uniform quantization is widely adopted due to its straightforward hardware implementa- tion, allowing integer arithmetic to replace floating-point operations, leading to higher efficiency and lower compu- tational cost. For highly skewed data distributions, the log2 quantizer provides a more effective alternative, as it assigns quantization levels based on powers of two, offering higher precision for small values: x _{q} \ text { c l am p} \ left ( \left \lfloor -\log _{2}\frac {x}{s} \right \rceil , 0, 2 k - 1 \right ) \label {Eq_Log_Quant} (3) x \ a p prox \hat {x} s \cdot 2 {-x_{q}} \label {Eq_Log_Dequant} (4) Log2 quantization is particularly beneficial for hardware, as it enables multiplications to be replaced by bit shifts, im- proving computational speed and energy efficiency. 3.1.2. Quantization Granularity Quantization operates at varying levels of granularity, in- troducing a trade-off between computational efficiency and quantization effectiveness. The two most common ap- proaches are per-tensor and per-channel quantization. Per- Tensor quantization employs a single scale and zero-point across an entire weight or activation tensor, reducing com- putational complexity and memory overhead. However, it struggles with large inter-channel variations, leading to suboptimal quantization performance. Per-Channel quan- tization assigns individual quantization parameters to each output channel, effectively mitigating distribution variance across channels. However, it requires storing more quanti- zation parameters, increasing memory usage and data trans- fer costs. 3.1.3.\n\n--- Segment 8 ---\nHowever, it requires storing more quanti- zation parameters, increasing memory usage and data trans- fer costs. 3.1.3. Block-Wise Reconstruction We employ block-wise reconstruction [39], as adopted in PTQ4SAM [29], to mitigate the quantization-induced er- ror in weight and activation quantization by minimizing the mean squared error: \ma t hcal {L} \ \mathbf {O}_\mathcal {B} - \hat {\mathbf {O}}_\mathcal {B} \ _2 2, \label {Eq_Block_Recon} (5) where OB and ˆOB represent the floating-point and quan- tized outputs of the B-th block, respectively. 3.2. Challenges of SAM Quantization Recent PTQ approaches [29] have primarily focused on ad- dressing the bimodal distribution of post-projection Key activations and the diverse distribution patterns of post- Softmax attention scores in SAM. Despite these efforts, ex- isting methods fail to account for additional key factors cru- cial to quantization, resulting in severe performance degra- dation in ultra-low-bit scenarios. As shown in Table 1, existing methods exhibit significant precision loss at 5-bit quantization and collapse at 4-bit quantization. In this pa- per, we identify two additional challenges that profoundly affect SAM s quantization performance, as illustrated in Fig. 1. The first challenge arises from the heavy-tailed and skewed distribution of post-GELU activations. As depicted in Fig. 1a, over 90 of activations are densely concentrated within -0.2 to 0, whereas a sparser but evenly distributed subset of large values, critical for inference accuracy, ex- tends from 0 to 0.8. As illustrated in Fig. 2 (left panel), existing log2 and uniform quantizers fail to effectively han- dle this distribution. The log2 quantizer efficiently cap- tures small, densely clustered values by allocating most of its quantization grids to this range. However, its exponen- tially spaced grid structure results in insufficient represen- tation density for larger values, leading to high quantiza- tion errors in the upper range.\n\n--- Segment 9 ---\nThe log2 quantizer efficiently cap- tures small, densely clustered values by allocating most of its quantization grids to this range. However, its exponen- tially spaced grid structure results in insufficient represen- tation density for larger values, leading to high quantiza- tion errors in the upper range. On the other hand, the uni- form quantizer provides consistent resolution across the full range, making it better suited for large, evenly spaced val- ues, but it lacks sufficient resolution for small activations, introducing substantial quantization errors. This problem is further exacerbated as bit-width decreases, particularly in ultra-low-bit settings, where the limited number of grids im- poses severe constraints. Although non-uniform quantizers 3 may provide a better alternative, their hardware complexity poses deployment challenges [3, 9, 36] and often necessi- tates substantial QAT-based retraining [15, 40]. To effec- tively address this issue, an efficient, hardware-compatible quantizer that captures both small and large activations tai- lored for PTQ is required. The second challenge arises from high inter-channel variation, particularly in the activations of Query Key Value linear projections in the attention module and Linear projec- tions in the MLP module. These variations originate from LayerNorm operations and the unique activation distribu- tions of the SAM mask decoder. As shown in Fig. 1b, activation ranges in the Token-to-Image Value linear pro- jection exhibit significant inter-channel disparities, making per-tensor quantization ineffective due to the challenge of finding a single optimal scale factor and zero point [1]. A large scale factor, selected to accommodate high-range channels, leads to reduced quantization granularity for low- range channels, often causing them to be rounded to zero. Conversely, a small scale factor, optimized for low-range channels, results in severe clipping in high-range chan- nels, leading to significant information loss [46]. Moreover, while a zero point could compensate for activation skew- ness, the varying inter-channel median values prevent a sin- gle zero point from being optimal for all channels. While per-channel quantization can effectively mitigate quantiza- tion errors, it introduces considerable hardware inefficien- cies. As depicted in Fig.\n\n--- Segment 10 ---\nWhile per-channel quantization can effectively mitigate quantiza- tion errors, it introduces considerable hardware inefficien- cies. As depicted in Fig. 4, transferring per-channel quanti- zation parameters between DRAM and compute units in- curs a memory access overhead of up to several kB per layer [10, 37]. Storing these parameters on-chip can elim- inate the transfer cost, but at the expense of a significantly larger memory footprint, requiring tens of thousands of on- chip registers, thereby increasing chip area utilization. To address this trade-off, a granularity-aware quantization ap- proach with hardware co-optimization is essential for bal- ancing quantization accuracy and deployment efficiency. 3.3. AHCPTQ To advance SAM quantization, we propose AHCPTQ, a framework specifically designed to overcome the two key challenges discussed earlier. As shown in Fig. 2, AHCPTQ leverages CAG and HLUQ, each targeting a specific quan- tization issue. 3.3.1. Hybrid Log-Uniform Quantization The heavy-tailed and skewed post-GELU activations hin- der the quantization effectiveness of conventional uniform and log2 quantizers, while non-uniform quantizers, though potentially more adaptable, lack hardware efficiency. To en- hance SAM quantization performance while ensuring hard- ware compatibility, we propose HLUQ, a method that re- duces quantization errors in post-GELU activations by com- bining log2 and uniform quantization.\n\n--- Segment 11 ---\nHybrid Log-Uniform Quantization The heavy-tailed and skewed post-GELU activations hin- der the quantization effectiveness of conventional uniform and log2 quantizers, while non-uniform quantizers, though potentially more adaptable, lack hardware efficiency. To en- hance SAM quantization performance while ensuring hard- ware compatibility, we propose HLUQ, a method that re- duces quantization errors in post-GELU activations by com- bining log2 and uniform quantization. Specifically, HLUQ applies log2 quantization to densely packed small values, where fine-grained precision is crucial, and uniform quan- tization to sparse, evenly distributed large values, ensuring minimal quantization error: x _ q \be gi n {ca s es } \tex t {c l a mp} \lef t ( \l ef t \lf lo o r - \l o g _{2}\frac {x}{s_{1}} \right \rceil , 0, \hat {b} \right ), \text {if } x \leq s_{1}, \\[0.5em] \text {clamp} \left ( \left \lfloor \frac {x-s_{1}}{s_{2}} \right \rceil , \hat {b}, 2 k - 1 \right ), \text {if } x s_{1}. \end {cases} \label {Eq_Hybrid_Quant} (6) x \ a pp r ox \h at { x } \ b eg i n { ca se s } s_{1} \cdot 2 {-x_{q}}, \text {if } x_q \leq \hat {b}, \\ s_{2} \cdot x_{q} s_{1}, \text {if } x_q \hat {b}. \end {cases} \label {Eq_Hybrid_Dequant} (7) Here, s1 defines the log2 quantization scale, and s2 deter- mines the uniform quantization scale. The threshold ˆb par- titions the quantization grid, assigning log2 quantization to [0,ˆb] and uniform quantization to [ˆb, 2k 1].\n\n--- Segment 12 ---\n\end {cases} \label {Eq_Hybrid_Dequant} (7) Here, s1 defines the log2 quantization scale, and s2 deter- mines the uniform quantization scale. The threshold ˆb par- titions the quantization grid, assigning log2 quantization to [0,ˆb] and uniform quantization to [ˆb, 2k 1]. By adjusting s1 and ˆb, HLUQ offers a high degree of adaptability, allowing it to accommodate various activation distributions. If s1 spans the full range and ˆb is close to 2k 1, HLUQ behaves like a log2 quantizer. In contrast, if s1 and ˆb are near zero, it essentially acts as a uniform quantizer, ensuring uniform step sizes. For heavy-tailed and skewed post-GELU activations, HLUQ can be configured with intermediate values of s1 and ˆb, allowing it to capture small, densely distributed values using log2 quantization while retaining precision for large, sparse values through uniform quantization, as illustrated in Fig. 2 (left panel). Furthermore, HLUQ maintains the hardware efficiency of log2 and uniform quantization, introducing only minimal overhead to partition the input at s1. To initialize ˆb, s1, and s2, we introduce two auxiliary parameters, α and β, and search their optimal values dur- ing initial calibration by minimizing the following objective function: \ mat hop { \ar g } \m a thop {\min }_{\alpha ,\beta } \mathbb {E} \left [ \ \mathbf {X}\mathbf {W} - \hat {\mathbf {X}}\mathbf {W} \ _F 2 \right ] \label {Eq_HLUQ_Cali} (8) Here, α partitions the original activation range r, defin- ing the quantization scales s1 and s2 as s1 α r and s2 (1 α) r. Meanwhile, β determines the allocation of quantization grids between log2 and uniform quantiza- tion segments, setting the threshold ˆb as ˆb β (2k 1).\n\n--- Segment 13 ---\nFurthermore, HLUQ maintains the hardware efficiency of log2 and uniform quantization, introducing only minimal overhead to partition the input at s1. To initialize ˆb, s1, and s2, we introduce two auxiliary parameters, α and β, and search their optimal values dur- ing initial calibration by minimizing the following objective function: \ mat hop { \ar g } \m a thop {\min }_{\alpha ,\beta } \mathbb {E} \left [ \ \mathbf {X}\mathbf {W} - \hat {\mathbf {X}}\mathbf {W} \ _F 2 \right ] \label {Eq_HLUQ_Cali} (8) Here, α partitions the original activation range r, defin- ing the quantization scales s1 and s2 as s1 α r and s2 (1 α) r. Meanwhile, β determines the allocation of quantization grids between log2 and uniform quantiza- tion segments, setting the threshold ˆb as ˆb β (2k 1). Once α and β are obtained, the scales s1, s2, and the grid threshold ˆb are configured to initialize HLUQ quantizer for subsequent reconstruction training. 3.3.2. Channel-Aware Grouping The substantial inter-channel variation in activations presents a major obstacle for low-granularity per-tensor quantization, while fine-granularity per-channel quantiza- tion remains inefficient for hardware implementation. Inter- estingly, although linear projection activations exhibit sub- stantial inter-channel variation, their statistical properties 4 Channel-Aware Grouping (CAG) Linear Projection Activations: Channel Variation Challenge Distribution per Channel Per-Channel Quantization: Increasing Parameter Count Merging Similar Parameters during Reconstruction Reducing Parameter Count while Preserve Granularity Hybrid Log-Uniform Quantization (HLUQ) For Post-GELU Activations: Poor Resolution for Small Activations Poor Resolution for Large Activations Good Resolution for Entire Quantization Range Combining Two Types of Quantizer to Improve Resolution LayerNorm Projection Attention LayerNorm MLP.Linear-1 GELU MLP.Linear-2 Q K V O Figure 2.\n\n--- Segment 14 ---\nChannel-Aware Grouping The substantial inter-channel variation in activations presents a major obstacle for low-granularity per-tensor quantization, while fine-granularity per-channel quantiza- tion remains inefficient for hardware implementation. Inter- estingly, although linear projection activations exhibit sub- stantial inter-channel variation, their statistical properties 4 Channel-Aware Grouping (CAG) Linear Projection Activations: Channel Variation Challenge Distribution per Channel Per-Channel Quantization: Increasing Parameter Count Merging Similar Parameters during Reconstruction Reducing Parameter Count while Preserve Granularity Hybrid Log-Uniform Quantization (HLUQ) For Post-GELU Activations: Poor Resolution for Small Activations Poor Resolution for Large Activations Good Resolution for Entire Quantization Range Combining Two Types of Quantizer to Improve Resolution LayerNorm Projection Attention LayerNorm MLP.Linear-1 GELU MLP.Linear-2 Q K V O Figure 2. AHCPTQ framework: HLUQ refines quantization resolution for post-GELU activations, while CAG effectively groups parame- ters to manage channel-wise variations in linear projection activations to reduce quantization error. remain remarkably consistent across different samples. As shown in Fig. 3, the cosine similarity of normalized quan- tization parameters, searched over 100 samples per chan- nel, is consistently close to 1.0, indicating that the optimal quantization parameters for each channel are largely invari- ant across samples. This stability presents an opportunity to leverage shared quantization parameters within grouped channels, improving quantization efficiency while main- taining accuracy. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.847 Figure 3. Cosine similarity of normalized quantization parameter across 100 samples for each channel. To balance quantization granularity with hardware effi- ciency, we propose CAG, which achieves accuracy com- parable to per-channel quantization while significantly re- ducing parameter overhead, thereby enabling efficient on- chip deployment. Unlike static per-group quantization [42], the fundamental concept of CAG is to progressively group channels with similar activation distributions and assign them shared quantization parameters.\n\n--- Segment 15 ---\nTo balance quantization granularity with hardware effi- ciency, we propose CAG, which achieves accuracy com- parable to per-channel quantization while significantly re- ducing parameter overhead, thereby enabling efficient on- chip deployment. Unlike static per-group quantization [42], the fundamental concept of CAG is to progressively group channels with similar activation distributions and assign them shared quantization parameters. As outlined in Al- gorithm 1, the process begins with quantization parame- ter initialization (scales and zero points) for each channel via model calibration, treating each channel as an indepen- dent group initially. Next, block-wise reconstruction is ap- plied to refine both quantization parameters and weights, Algorithm 1 Channel-Aware Grouping Input: Total iterations T, milestones t {T1, T2, . . . , TJ} Output: K groups of optimized parameters {si, zi}K i 1 1: Initialize quantization parameters {si, zi}N i 1 for N channels, G N 2: for t 1, 2, ..., T do 3: Update {si, zi}G i 1 according to Eq. 5 4: if t {T1, T2, . . . , TJ} then 5: Apply K-Means to {si, zi}G i 1 to form centroids {si, zi} GTj i 1 6: Assign each channel to the nearest centroid to form new groups 7: Update number of groups G GTj 8: end if 9: end for 10: return {si, zi}K i 1 minimizing quantization error as formulated in Eq. 5, en- suring alignment with each group s distribution characteris- tics. At designated milestones, channels with similar quan- tization parameters are progressively clustered, and the re- sulting centroids are adopted as shared quantization param- eters for all channels within a group. This iterative pro- cess continues until the target group count is reached. The right panel of Fig. 2 illustrates an example of this grouping process for linear projection activations in Token-to-Image Value cross-attention, demonstrating how CAG effectively reduces the number of groups while preserving quantization performance.\n\n--- Segment 16 ---\nThe right panel of Fig. 2 illustrates an example of this grouping process for linear projection activations in Token-to-Image Value cross-attention, demonstrating how CAG effectively reduces the number of groups while preserving quantization performance. With a group number of 4, the edge acceler- ator can minimize quantization parameter overhead, reduc- ing either data transmission or on-chip storage by 99.7 , as shown in Fig. 4. In AHCPTQ, we adopt on-chip stor- 5 age for quantization parameters, requiring only 144 regis- ters to store the scales and zero points of these 4 groups under 4-bit quantization. As depicted in Fig. 6, CAG main- tains accuracy comparable to per-channel quantization, of- fering high hardware efficiency while significantly enhanc- ing model performance. 3.4. Hardware Co-optimization In this subsection, we briefly describe the strategy of hard- ware co-optimization for the proposed HLUQ and CAG. To implement HLUQ, a dual processing-element (PE) archi- tecture is employed to facilitate matrix multiplication within the HLUQ quantizer. The uniform quantization branch uti- lizes a standard integer multiplier and an adder tree, while the log2 quantization branch employs a bit-shifter and a decimal adder tree. The grid ratio between the two quantiz- ers follows the 2 n rule, ensuring that the MSBs of quan- tized values function as label bits. These label bits enable the accelerator to efficiently route quantized values to their respective PEs. Finally, the outputs from both branches are merged during dequantization, producing the final result. To efficiently implement CAG, a small set of on-chip reg- isters is used to store quantization parameters, and a cus- tomized quantizer logic is designed. Specifically, weights and activations are reordered by channel indices, ensuring that grouped channels are sequentially aligned. Weights are pre-processed offline, while activations are reordered on- chip. This reordering is integrated into the quantization and dequantization processes within linear projection compu- tations. By employing counter-based logic for parameter switching, the design effectively minimizes hardware com- plexity while maintaining computational efficiency. Per-Tensor Quantization Per-Channel Quantization Param.\n\n--- Segment 17 ---\nBy employing counter-based logic for parameter switching, the design effectively minimizes hardware com- plexity while maintaining computational efficiency. Per-Tensor Quantization Per-Channel Quantization Param. Storage in DRAM 99.7 Reduction on Data Transfer Per-Channel Quantization Param. Storage On-chip 99.7 Reduction on Register Count AHCPTQ Configuration CAG Group 4 Figure 4. Hardware cost analysis of the linear projection layer in the SAM-H decoder under different quantization granularities. 4. Experiments 4.1. Experiment and Implementation Details We evaluate the effectiveness of AHCPTQ on instance seg- mentation using the mean Average Precision (mAP) met- ric on the COCO dataset[22]. The selected detectors in- clude CNN-based Faster RCNN[32] and YOLOX[7], as well as Transformer-based H-Deformable-DETR[12] and DINO [43]. The bounding boxes generated by these detec- tors serve as prompts for SAM. For CNN-based detectors, the box threshold is set to 0.05, while Transformer-based detectors utilize a set of 100 adaptive anchors. Following the PTQ4SAM framework [29], we randomly sample 32 training images and use the MinMax approach to initialize quantization parameters. Block reconstruction is performed over 20,000 iterations on both attention and MLP blocks. To ensure fair comparisons [24, 29, 39, 41], we exclude the first and last layers or blocks from quantization while keep- ing all others quantized. HLUQ is applied to all Linear- 2 activations in MLP blocks, with initial scales and grid thresholds determined by searching α {0.1, 0.3, 0.5} and β { 1 2, 1 4, 1 8}. CAG is applied to all linear projection acti- vations in Query Key Value projections of attention blocks and Linear-1 activations in MLP blocks, using a group num- ber of 4. For other activations, we adopt per-tensor asym- metric quantization. For weights, we apply per-channel asymmetric quantization to maintain alignment with base- line settings. 4.2.\n\n--- Segment 18 ---\nFor weights, we apply per-channel asymmetric quantization to maintain alignment with base- line settings. 4.2. Experimental Results We evaluate AHCPTQ against baselines such as BRECQ [17], QDrop [39], and PTQ4SAM [29], with results summarized in Table 1 across four types of de- tectors. For SAM-L and SAM-H models, other baselines exhibit significant accuracy drops in the W5A5 configu- ration, while AHCPTQ reduces the mAP loss to within 2 . In the W4A4 configuration, where other baselines struggle to achieve basic functionality, AHCPTQ restores mAP to approximately 80 of the FP32 performance, particularly for SAM-H. For instance, AHCPTQ improves DINO s mAP from 43.8 to 47.6 in the W5A5 config- uration of SAM-H and from 2.3 to 36.6 in the W4A4 configuration of SAM-L, surpassing all baselines by a large margin. In the challenging SAM-B model, AHCPTQ doubles the mAP compared to PTQ4SAM in the W5A5 configuration, reduces the mAP loss to below 3 in W6A6, and restores mAP to a usable level in W4A4. For example, AHCPTQ improves H-DETR s mAP by PTQ4SAM from 2.8 to 14.1 , 16.9 to 32.1 , and 30.7 to 36.6 in W4A4, W5A5, and W6A6 configurations, respectively. These results indicate that AHCPTQ consistently surpasses baselines, addressing the challenges identified in Sec. 3.2 and advancing SAM quantization to lower bit-widths with superior effectiveness. 4.3. Ablation Studies Ablation of Components. Table 2 details the ablation re- sults for AHCPTQ, which are tested on YOLOX and H- Deformable-DETR in the W4A4 configuration. The re- 6 Table 1. Quantization results of AHCPTQ for instance segmentation on the COCO dataset across four detector types. Floating-point (FP32) mAP values for SAM-B L H are shown below each detector name for reference.\n\n--- Segment 19 ---\nQuantization results of AHCPTQ for instance segmentation on the COCO dataset across four detector types. Floating-point (FP32) mAP values for SAM-B L H are shown below each detector name for reference. AHCPTQ achieves a new state-of-the-art performance, outperforming all existing baselines.\n\n--- Segment 20 ---\nFloating-point (FP32) mAP values for SAM-B L H are shown below each detector name for reference. AHCPTQ achieves a new state-of-the-art performance, outperforming all existing baselines. Detector Method SAM-B SAM-L SAM-H FP32: (B L H) W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 Faster R-CNN BRECQ 0.2 16.7 28.0 5.0 31.8 35.2 17.5 31.3 35.8 QDrop 2.3 12.9 26.2 0.8 31.9 35.0 6.0 32.6 36.0 33.1 36.0 36.8 PTQ4SAM 2.7 14.4 26.8 2.4 33.0 35.5 6.7 33.3 36.2 AHCPTQ 11.7 27.5 31.6 27.4 34.8 35.6 31.4 35.7 36.3 YOLOX BRECQ 0.2 19.0 31.9 6.3 35.3 39.4 19.7 34.7 39.7 QDrop 2.6 15.6 30.3 1.0 36.2 39.4 6.8 36.0 40.1 37.2 40.4 41.0 PTQ4SAM 3.8 18.4 30.9 2.4 37.1 39.9 7.4 37.1 40.3 AHCPTQ 13.4 31.8 35.4 31.0 39.1 40.0 35.2 40.0 40.4 H-DETR BRECQ 0.3 11.2 32.0 5.2 36.1 40.4 19.1 35.3 40.6 QDrop 2.0 13.1 30.5 1.3 37.0 40.3 6.9 37.0 41.1 38.2 41.5 42.0 PTQ4SAM 2.8 16.9 30.7 2.6 38.1 40.9 7.1 38.0 41.4 AHCPTQ 14.1 32.1 36.6 32.3 40.3 41.0 35.6 40.9 41.5 DINO BRECQ 0.2 13.5 34.8 3.6 41.4 47.0 20.7 40.3 47.2 QDrop 1.9 13.4 34.5 1.0 42.7 47.0 7.0 42.4 47.9 44.5 48.6 49.1 PTQ4SAM 1.9 17.6 35.1 2.3 44.1 47.8 8.9 43.8 48.2 AHCPTQ 16.8 36.7 41.9 36.6 46.8 47.9 41.2 47.6 48.3 Note: This paper reports the corrected performance of PTQ4SAM and QDrop after fixing a bug (refer to code repo.)\n\n--- Segment 21 ---\nAHCPTQ achieves a new state-of-the-art performance, outperforming all existing baselines. Detector Method SAM-B SAM-L SAM-H FP32: (B L H) W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 Faster R-CNN BRECQ 0.2 16.7 28.0 5.0 31.8 35.2 17.5 31.3 35.8 QDrop 2.3 12.9 26.2 0.8 31.9 35.0 6.0 32.6 36.0 33.1 36.0 36.8 PTQ4SAM 2.7 14.4 26.8 2.4 33.0 35.5 6.7 33.3 36.2 AHCPTQ 11.7 27.5 31.6 27.4 34.8 35.6 31.4 35.7 36.3 YOLOX BRECQ 0.2 19.0 31.9 6.3 35.3 39.4 19.7 34.7 39.7 QDrop 2.6 15.6 30.3 1.0 36.2 39.4 6.8 36.0 40.1 37.2 40.4 41.0 PTQ4SAM 3.8 18.4 30.9 2.4 37.1 39.9 7.4 37.1 40.3 AHCPTQ 13.4 31.8 35.4 31.0 39.1 40.0 35.2 40.0 40.4 H-DETR BRECQ 0.3 11.2 32.0 5.2 36.1 40.4 19.1 35.3 40.6 QDrop 2.0 13.1 30.5 1.3 37.0 40.3 6.9 37.0 41.1 38.2 41.5 42.0 PTQ4SAM 2.8 16.9 30.7 2.6 38.1 40.9 7.1 38.0 41.4 AHCPTQ 14.1 32.1 36.6 32.3 40.3 41.0 35.6 40.9 41.5 DINO BRECQ 0.2 13.5 34.8 3.6 41.4 47.0 20.7 40.3 47.2 QDrop 1.9 13.4 34.5 1.0 42.7 47.0 7.0 42.4 47.9 44.5 48.6 49.1 PTQ4SAM 1.9 17.6 35.1 2.3 44.1 47.8 8.9 43.8 48.2 AHCPTQ 16.8 36.7 41.9 36.6 46.8 47.9 41.2 47.6 48.3 Note: This paper reports the corrected performance of PTQ4SAM and QDrop after fixing a bug (refer to code repo.) in the PTQ4SAM [29] framework.\n\n--- Segment 22 ---\nDetector Method SAM-B SAM-L SAM-H FP32: (B L H) W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 W4A4 W5A5 W6A6 Faster R-CNN BRECQ 0.2 16.7 28.0 5.0 31.8 35.2 17.5 31.3 35.8 QDrop 2.3 12.9 26.2 0.8 31.9 35.0 6.0 32.6 36.0 33.1 36.0 36.8 PTQ4SAM 2.7 14.4 26.8 2.4 33.0 35.5 6.7 33.3 36.2 AHCPTQ 11.7 27.5 31.6 27.4 34.8 35.6 31.4 35.7 36.3 YOLOX BRECQ 0.2 19.0 31.9 6.3 35.3 39.4 19.7 34.7 39.7 QDrop 2.6 15.6 30.3 1.0 36.2 39.4 6.8 36.0 40.1 37.2 40.4 41.0 PTQ4SAM 3.8 18.4 30.9 2.4 37.1 39.9 7.4 37.1 40.3 AHCPTQ 13.4 31.8 35.4 31.0 39.1 40.0 35.2 40.0 40.4 H-DETR BRECQ 0.3 11.2 32.0 5.2 36.1 40.4 19.1 35.3 40.6 QDrop 2.0 13.1 30.5 1.3 37.0 40.3 6.9 37.0 41.1 38.2 41.5 42.0 PTQ4SAM 2.8 16.9 30.7 2.6 38.1 40.9 7.1 38.0 41.4 AHCPTQ 14.1 32.1 36.6 32.3 40.3 41.0 35.6 40.9 41.5 DINO BRECQ 0.2 13.5 34.8 3.6 41.4 47.0 20.7 40.3 47.2 QDrop 1.9 13.4 34.5 1.0 42.7 47.0 7.0 42.4 47.9 44.5 48.6 49.1 PTQ4SAM 1.9 17.6 35.1 2.3 44.1 47.8 8.9 43.8 48.2 AHCPTQ 16.8 36.7 41.9 36.6 46.8 47.9 41.2 47.6 48.3 Note: This paper reports the corrected performance of PTQ4SAM and QDrop after fixing a bug (refer to code repo.) in the PTQ4SAM [29] framework. sults demonstrate that each technique plays a vital role in enhancing SAM s performance, with the highest gains achieved when both CAG and HLUQ are applied together.\n\n--- Segment 23 ---\nin the PTQ4SAM [29] framework. sults demonstrate that each technique plays a vital role in enhancing SAM s performance, with the highest gains achieved when both CAG and HLUQ are applied together. For SAM-B and SAM-L models, inter-channel variance emerges as the primary challenge. Thus, while applying HLUQ alone leads to moderate improvements, applying CAG yields significant performance gains. Once the major challenge is addressed by CAG, HLUQ becomes essential for further improvement. For instance, HLUQ achieves an additional 8.3 mAP improvement in the SAM-L model of YOLOX when CAG is applied. For SAM-H, heavy-tailed and skewed activations become the dominant limitation. As a result, HLUQ contributes more significantly than CAG, and their combined application results in a 28 mAP im- provement. Ablation of Quantizers. To investigate the effectiveness of HLUQ, we analyze SAM performance by substitut- ing HLUQ with uniform and log2 quantizers, respectively. As the standard log2 quantizer cannot handle the negative branch of post-GELU activations, a floating-point bias is added for reasonable comparison. The results for YOLOX and H-Deformable-DETR under the W4A4 configuration are presented in Fig. 5. HLUQ significantly outperforms the uniform quantizer and surpasses the biased log2 quan- tizer, demonstrating its ability to improve resolution under low-bit quantization and reduce quantization error, thereby boosting SAM performance. 10.8 22.7 20.2 12.5 30.4 34.3 13.4 31.0 35.2 SAM-B SAM-L SAM-H 0 5 10 15 20 25 30 35 40 mAP ( ) Uniform Log2 w Bias HLUQ Standard Log2 is Not Applicable (a) YOLOX 11.2 22.1 20.5 13.5 31.4 34.8 14.1 32.3 35.6 SAM-B SAM-L SAM-H 0 5 10 15 20 25 30 35 40 mAP ( ) Uniform Log2 w Bias HLUQ Standard Log2 is Not Applicable (b) H-DETR Figure 5. Ablation study comparing the effectiveness of HLUQ with other quantizers in the W4A4 configuration.\n\n--- Segment 24 ---\n10.8 22.7 20.2 12.5 30.4 34.3 13.4 31.0 35.2 SAM-B SAM-L SAM-H 0 5 10 15 20 25 30 35 40 mAP ( ) Uniform Log2 w Bias HLUQ Standard Log2 is Not Applicable (a) YOLOX 11.2 22.1 20.5 13.5 31.4 34.8 14.1 32.3 35.6 SAM-B SAM-L SAM-H 0 5 10 15 20 25 30 35 40 mAP ( ) Uniform Log2 w Bias HLUQ Standard Log2 is Not Applicable (b) H-DETR Figure 5. Ablation study comparing the effectiveness of HLUQ with other quantizers in the W4A4 configuration. Dependence on Group Number. To analyze the effect of group number on SAM s performance in CAG, we evaluate mAP as the group number varies from 2 to 32 in YOLOX and H-Deformable-DETR under the W4A4 configuration. The results, presented in Fig. 6, also include per-tensor and per-channel configurations for reference. For SAM-L and SAM-H models, mAP increases sharply and saturates at a group number of 2, with slight fluctuations and marginal improvements closer to the per-channel configuration. For the SAM-B model, the turning point starts at a group num- ber of 4, with a larger performance gap of approximately 3 compared to the per-channel configuration. Consider- ing the need for a consistent configuration across models while minimizing hardware overhead, we choose a group 7 number of 4 in the AHCPTQ framework. Table 2. Results of the ablation study analyzing the contribution of components in the W4A4 configuration.\n\n--- Segment 25 ---\nTable 2. Results of the ablation study analyzing the contribution of components in the W4A4 configuration. Detector CAG HLUQ S.-B S.-L S.-H YOLOX 3.7 2.5 7.1 10.8 22.7 20.2 5.7 5.7 24.8 13.4 31.0 35.2 H-DETR 3.1 2.7 7.3 11.2 22.1 20.5 4.1 6.5 24.6 14.1 32.3 35.6 1 2 4 8 16 32 64 0 5 10 15 20 25 30 35 40 mAP ( ) Group Number SAM-B SAM-L SAM-H 5.7 8.8 13.4 13.7 13.6 16.9 16.2 5.7 31.9 31.0 31.3 32.4 32.6 33.2 24.8 35.3 35.2 35.4 35.7 36.3 36.4 Per-Tensor Per-Channel AHCPTQ Configuration: Group Number 4 (a) YOLOX 1 2 4 8 16 32 64 0 5 10 15 20 25 30 35 40 mAP ( ) Group Number SAM-B SAM-L SAM-H 4.1 10.4 14.1 14.4 14.5 14.5 17.1 6.5 33.4 32.3 32.3 33.6 33.6 34.0 24.6 35.7 35.6 35.9 36.3 36.3 37.0 Per-Tensor Per-Channel AHCPTQ Configuration: Group Number 4 (b) H-DETR Figure 6. Dependence of SAM performance on group number in CAG for the W4A4 configuration. 4.4. Hardware Validation To evaluate the resource efficiency and practical perfor- mance of AHCPTQ in real-world applications, we devel- oped an FPGA accelerator for executing the MLP block in the SAM mask decoder. CAG is applied to Linear-1 ac- tivations to mitigate channel-wise variation, while HLUQ is applied to Linear-2 activations to accommodate heavy- tailed and skewed post-GELU distributions.\n\n--- Segment 26 ---\nHardware Validation To evaluate the resource efficiency and practical perfor- mance of AHCPTQ in real-world applications, we devel- oped an FPGA accelerator for executing the MLP block in the SAM mask decoder. CAG is applied to Linear-1 ac- tivations to mitigate channel-wise variation, while HLUQ is applied to Linear-2 activations to accommodate heavy- tailed and skewed post-GELU distributions. The acceler- ator is designed in Verilog, synthesized using Vivado De- sign Suite, and implemented on a Xilinx Artix UltraScale FPGA running at 200 MHz. The overall system consists of an FPGA accelerator for computation, a DDR4 DRAM for data buffering, and a host PC for transferring activa- tions via Ethernet. In accordance with Sec. 3.4, the accel- erator uses MSBs indexing to assign activations to appro- priate PEs either multipliers or bit-shifters for HLUQ. We fuse the activation reordering logic into the quantiza- tion dequantization steps for CAG and pipeline the quanti- zation process with activation functions, effectively elim- inating extra quantization overhead in HLUQ. To estab- lish a comparative benchmark, we implemented two base- lines: a standard FP32 accelerator and a default 8-bit inte- ger (INT8) accelerator. Floating-point operations, includ- ing multiply-and-accumulate in the FP32 accelerator and quantization dequantization process in the integer acceler- ator, were implemented using the Floating-Point Operation IP generator with on-chip DSP resources. As shown in Table 3, the FP32 accelerator is limited by the number of available on-chip DSP resources, supporting only 8 parallel PE lanes. In contrast, both the default INT8 and AHCPTQ INT4 accelerators replace DSP-based PEs with LUT-based PEs, allowing for a significant increase in parallelism to 32 and 64 lanes, respectively. Meanwhile, DSP resources in in- teger accelerators are allocated to quantization and dequan- tization processing, optimizing overall resource utilization for improved system-level performance.\n\n--- Segment 27 ---\nIn contrast, both the default INT8 and AHCPTQ INT4 accelerators replace DSP-based PEs with LUT-based PEs, allowing for a significant increase in parallelism to 32 and 64 lanes, respectively. Meanwhile, DSP resources in in- teger accelerators are allocated to quantization and dequan- tization processing, optimizing overall resource utilization for improved system-level performance. As a result, 4- bit AHCPTQ reduces the computational complexity and overhead of data movement considerably, thereby achieving 8.64 gains in energy efficiency and 7.89 speedup over its floating-point counterpart. This demonstrates strong po- tential for efficient SAM deployment on edge devices. Table 3. Analysis of resource utilization and system-level perfor- mance of AHCPTQ on an FPGA platform. FP32 INT8 AHCPTQ INT4 LUT Usage 19,071 43,015 34,199 DSP Usage 447 153 213 Parallelism 8 32 64 GOPS W 5.04 21.97 43.57 Speedup 1.00 3.96 7.89 5. Limitation This study focuses on evaluating the feasibility of CAG and HLUQ on FPGA and ASIC platforms. We expect that simi- lar enhancements in energy efficiency and speedup could be realized on CPU and GPU platforms due to the reduction in FLOPs and data movement overhead. The deployment of AHCPTQ on these platforms is left for future research. 6. Conclusion This paper presents AHCPTQ, a novel PTQ framework for SAM, designed to efficiently handle complex activa- tion distributions while ensuring compatibility with hard- ware acceleration. AHCPTQ incorporates HLUQ to man- age heavy-tailed and skewed activations, and CAG to ad- dress high inter-channel variation, significantly improving quantization performance. Extensive experiments on mul- tiple SAM variants and quantization settings demonstrate its superior performance, establishing a new state-of-the- art in SAM quantization. Moreover, FPGA-based evalua- tions confirm its real-world deployability with substantial speedup and energy efficiency gain, providing valuable in- sights for practical applications.\n\n--- Segment 28 ---\nExtensive experiments on mul- tiple SAM variants and quantization settings demonstrate its superior performance, establishing a new state-of-the- art in SAM quantization. Moreover, FPGA-based evalua- tions confirm its real-world deployability with substantial speedup and energy efficiency gain, providing valuable in- sights for practical applications. 8 Acknowledgments This research was supported in part by the JST CREST JP- MJCR21D2 including AIP challenge program, Japan, JSPS Kakenhi 23H00467, Futaba Foundation, Asahi Glass Foun- dation, Telecommunications Advancement Foundation, and JST Doctoral Program Student Support Project. We also thank the anonymous reviewers and area chairs for their constructive feedback. References [1] Ron Banner, Yury Nahshan, Daniel Soudry, et al. Post train- ing 4-bit quantization of convolutional networks for rapid- deployment. In Proceedings of the Advances in Neural In- formation Processing Systems, pages 7950 7958, 2019. 4 [2] Junlong Cheng, Jin Ye, Zhongying Deng, Jianpin Chen, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Ji- long Chen, Lei Jiang, et al. Sam-med2d. arXiv preprint arXiv:2308.16184, 2023. 1 [3] Vladimir Chikin and Mikhail Antiukh. Data-free network compression via parametric non-uniform mixed precision quantization. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 450 459, 2022. 2, 4 [4] Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei, and Xianglong Liu. Towards accurate post- training quantization for vision transformer. In Proceedings of the 30th ACM international conference on multimedia, pages 5380 5388, 2022. 2, 4 [5] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\n\n--- Segment 29 ---\nLearned step size quantization. arXiv preprint arXiv:1902.08153, 2019. 1 [6] Natalia Frumkin, Dibakar Gope, and Diana Marculescu. Jumping through local minima: Quantization in the loss landscape of vision transformers. In Proceedings of the IEEE CVF International Conference on Computer Vision, pages 16978 16988, 2023. 2 [7] Z Ge. Yolox: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021. 6 [8] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differ- entiable soft quantization: Bridging full-precision and low- bit neural networks. In Proceedings of the IEEE CVF inter- national conference on computer vision, pages 4852 4861, 2019. 1 [9] Cheeun Hong, Heewon Kim, Junghun Oh, and Ky- oung Mu Lee. Daq: distribution-aware quantization for deep image super-resolution networks. arXiv preprint arXiv:2012.11230, 2020. 2, 4 [10] Mark Horowitz. 1.1 computing s energy problem (and what we can do about it). In 2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), pages 10 14, 2014. 4, 2 [11] Zejiang Hou and Sun-Yuan Kung. Multi-dimensional vi- sion transformer compression via dependency guided gaus- sian process search. In Proceedings of the IEEE CVF Con- ference on Computer Vision and Pattern Recognition, pages 3669 3678, 2022. 1 [12] Ding Jia, Yuhui Yuan, Haodi He, Xiaopei Wu, Haojun Yu, Weihong Lin, Lei Sun, Chao Zhang, and Han Hu. Detrs with hybrid matching. In Proceedings of the IEEE CVF con- ference on computer vision and pattern recognition, pages 19702 19712, 2023.\n\n--- Segment 30 ---\nDetrs with hybrid matching. In Proceedings of the IEEE CVF con- ference on computer vision and pattern recognition, pages 19702 19712, 2023. 6 [13] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chlo e Rolland, Laura Gustafson, Tete Xiao, Spencer White- head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ar, and Ross B. Girshick. Segment anything. 2023 IEEE CVF In- ternational Conference on Computer Vision (ICCV), pages 3992 4003, 2023. 1, 2 [14] Raghuraman Krishnamoorthi. Quantizing deep convolu- tional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018. 1 [15] Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of- two quantization: An efficient non-uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. 4 [16] Yuhang Li, Xin Dong, and Wei Wang. Additive powers-of- two quantization: An efficient non-uniform discretization for neural networks. arXiv preprint arXiv:1909.13144, 2019. 1 [17] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruc- tion. arXiv preprint arXiv:2102.05426, 2021. 2, 6, 4, 5 [18] Yanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao, Peng Gao, and Guodong Guo. Q-vit: Accurate and fully quantized low-bit vision transformer. Advances in neural information processing systems, 35:34451 34463, 2022. 1 [19] Zhikai Li and Qingyi Gu. I-vit: Integer-only quantization for efficient vision transformer inference. In Proceedings of the IEEE CVF International Conference on Computer Vi- sion, pages 17065 17075, 2023.\n\n--- Segment 31 ---\nI-vit: Integer-only quantization for efficient vision transformer inference. In Proceedings of the IEEE CVF International Conference on Computer Vi- sion, pages 17065 17075, 2023. 1 [20] Zhikai Li, Liping Ma, Mengjuan Chen, Junrui Xiao, and Qingyi Gu. Patch similarity aware data-free quantization for vision transformers. In European conference on computer vision, pages 154 170. Springer, 2022. 2 [21] Zhikai Li, Junrui Xiao, Lianwei Yang, and Qingyi Gu. Repq- vit: Scale reparameterization for post-training quantization of vision transformers. In Proceedings of the IEEE CVF In- ternational Conference on Computer Vision, pages 17227 17236, 2023. 2, 4 [22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740 755. Springer, 2014. 6 [23] Yang Lin, Tianyu Zhang, Peiqin Sun, Zheng Li, and Shuchang Zhou. Fq-vit: Post-training quantization for fully quantized vision transformer. arXiv preprint arXiv:2111.13824, 2021. 2, 4 [24] Jiawei Liu, Lin Niu, Zhihang Yuan, Dawei Yang, Xinggang Wang, and Wenyu Liu. Pd-quant: Post-training quantiza- tion based on prediction difference metric. In Proceedings of 9 the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 24427 24437, 2023. 6, 4 [25] Shih-Yang Liu, Zechun Liu, and Kwang-Ting Cheng. Oscillation-free quantization for low-bit vision transformers. In International Conference on Machine Learning, pages 21813 21824. PMLR, 2023.\n\n--- Segment 32 ---\nIn International Conference on Machine Learning, pages 21813 21824. PMLR, 2023. 1 [26] Xiaoyu Liu, Xin Ding, Lei Yu, Yuanyuan Xi, Wei Li, Zhi- jun Tu, Jie Hu, Hanting Chen, Baoqun Yin, and Zhiwei Xiong. Pq-sam: Post-training quantization for segment any- thing model. In European Conference on Computer Vision, pages 420 437. Springer, 2024. 2 [27] Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 20321 20330, 2023. 2 [28] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quantization for vision trans- former. Advances in Neural Information Processing Systems, 34:28092 28103, 2021. 1 [29] Chengtao Lv, Hong Chen, Jinyang Guo, Yifu Ding, and Xi- anglong Liu. Ptq4sam: Post-training quantization for seg- ment anything. In Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition, pages 15941 15951, 2024. 2, 3, 6, 7, 5 [30] Alaa Maalouf, Ninad Jadhav, Krishna Murthy Jatavallab- hula, Makram Chahine, Daniel M Vogt, Robert J Wood, An- tonio Torralba, and Daniela Rus. Follow anything: Open- set detection, tracking, and following in real-time. IEEE Robotics and Automation Letters, 9(4):3283 3290, 2024. 1 [31] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Chris- tos Louizos, and Tijmen Blankevoort. Up or down? adap- tive rounding for post-training quantization. In International Conference on Machine Learning, pages 7197 7206. PMLR, 2020.\n\n--- Segment 33 ---\nIn International Conference on Machine Learning, pages 7197 7206. PMLR, 2020. 2 [32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. IEEE transactions on pattern analysis and machine intelligence, 39(6):1137 1149, 2016. 6 [33] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything- 3d: Towards single-view anything reconstruction in the wild. arXiv preprint arXiv:2304.10261, 2023. 1 [34] Huihong Shi, Haikuo Shao, Wendong Mao, and Zhongfeng Wang. Trio-vit: Post-training quantization and acceleration for softmax-free efficient vision transformer. arXiv preprint arXiv:2405.03882, 2024. 2 [35] Han Shu, Wenshuo Li, Yehui Tang, Yiman Zhang, Yi- hao Chen, Houqiang Li, Yunhe Wang, and Xinghao Chen. Tinysam: Pushing the envelope for efficient segment any- thing model. arXiv preprint arXiv:2312.13789, 2023. 2 [36] Longguang Wang, Xiaoyu Dong, Yingqian Wang, Li Liu, Wei An, and Yu Kuen Guo. Learnable lookup table for neural network quantization. 2022 IEEE CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), pages 12413 12423, 2022. 2, 4 [37] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantization via bit- split and stitching. In Proceedings of the International Con- ference on Machine Learning, pages 9847 9856, 2020. 4 [38] Yonghui Wang, Wengang Zhou, Yunyao Mao, and Houqiang Li. Detect any shadow: Segment anything for video shadow detection. IEEE Transactions on Circuits and Systems for Video Technology, 34(5):3782 3794, 2024. 1 [39] Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu.\n\n--- Segment 34 ---\nIEEE Transactions on Circuits and Systems for Video Technology, 34(5):3782 3794, 2024. 1 [39] Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: Randomly dropping quantization for extremely low-bit post-training quantization. arXiv preprint arXiv:2203.05740, 2022. 2, 3, 6, 4, 5 [40] Tian Xia, Boran Zhao, Jian Ma, Gelin Fu, Wenzhe Zhao, Nanning Zheng, and Pengju Ren. An energy-and-area- efficient cnn accelerator for universal powers-of-two quan- tization. IEEE Transactions on Circuits and Systems I: Reg- ular Papers, 70(3):1242 1255, 2022. 4 [41] Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. Ptq4vit: Post-training quantization for vision transformers with twin uniform quantization. In European conference on computer vision, pages 191 207. Springer, 2022. 2, 6, 4 [42] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for large language models. arXiv preprint arXiv:2304.01089, 2023. 5 [43] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. 6 [44] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junt- ing Pan, Xianzheng Ma, Hao Dong, Peng Gao, and Hong- sheng Li. Personalize segment anything model with one shot. arXiv preprint arXiv:2305.03048, 2023.\n\n--- Segment 35 ---\nPersonalize segment anything model with one shot. arXiv preprint arXiv:2305.03048, 2023. 1 [45] Dehua Zheng, Wenhui Dong, Hailin Hu, Xinghao Chen, and Yunhe Wang. Less is more: Focus attention for efficient detr. In Proceedings of the IEEE CVF international conference on computer vision, pages 6674 6683, 2023. 1 [46] Yunshan Zhong, Mingbao Lin, Xunchao Li, Ke Li, Yun- hang Shen, Fei Chao, Yongjian Wu, and Rongrong Ji. Dy- namic dual trainable bounds for ultra-low precision super- resolution networks. In European Conference on Computer Vision, pages 1 18. Springer, 2022. 4 [47] Yunshan Zhong, Jiawei Hu, Mingbao Lin, Mengzhao Chen, and Rongrong Ji. I s-vit: An inclusive stable method for pushing the limit of post-training vits quantization. arXiv preprint arXiv:2311.10126, 2023. 2, 4 [48] Yunshan Zhong, Jiawei Hu, You Huang, Yuxin Zhang, and Rongrong Ji. Erq: Error reduction for post-training quanti- zation of vision transformers. In Proceedings of the Interna- tional Conference on Machine Learning (ICML), 2024. 2 [49] Yunshan Zhong, You Huang, Jiawei Hu, Yuxin Zhang, and Rongrong Ji. Towards accurate post-training quantization of vision transformers via error reduction. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1 18, 2025. 2 10 AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for Segment Anything Model Supplementary Material A. Analysis of Inter-Channel Variation and Inter-Sample Similarity in SAM Model In this section, we provide an in-depth analysis of inter- channel variation and inter-sample similarity using the SAM-B model with YOLOX as the prompt detector. We extract the quantization ranges for channel indices 50 to 100 to examine channel-wise variation across multiple lay- ers where CAG is applied.\n\n--- Segment 36 ---\nAnalysis of Inter-Channel Variation and Inter-Sample Similarity in SAM Model In this section, we provide an in-depth analysis of inter- channel variation and inter-sample similarity using the SAM-B model with YOLOX as the prompt detector. We extract the quantization ranges for channel indices 50 to 100 to examine channel-wise variation across multiple lay- ers where CAG is applied. Additionally, we determine the optimal scale and zero point for each channel using different samples to assess parameter consistency across 100 sam- ples. In the image encoder, Figs. 7a and 8a show that QKV projection and Linear-1 activations in the MLP block ex- hibit channel-wise variation due to LayerNorm operations. 56 63 70 77 84 91 98 2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 Range Encoder: QKV Projection Channel Index (a) Range distribution for channel indices 50 to 100. 0 100 200 300 400 500 600 700 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.937 (b) Cosine similarity across 100 samples for each channel. Figure 7. Range distribution and cosine similarity of QKV projec- tion activations in the image encoder. In the mask decoder, channel-wise variation in certain layers poses a major challenge for low-bit quantization. In Token-to-Image cross-attention, linear projection activa- tions in the Query and Value projections serve as the pri- mary challenges, as shown in Fig. Figs. 1b and 9a. No- tably, activation outliers in the Query projection extend the quantization range to approximately 400, severely increas- ing quantization error under per-tensor quantization. In Image-to-Token cross-attention, linear projection acti- vations in the Key and Value projections exhibit distribution variations that degrade SAM s performance. As shown in 56 63 70 77 84 91 98 8 6 4 2 0 2 4 6 8 Range Encoder: MLP Linear-1 Channel Index (a) Range distribution for channel indices 50 to 100.\n\n--- Segment 37 ---\nIn Image-to-Token cross-attention, linear projection acti- vations in the Key and Value projections exhibit distribution variations that degrade SAM s performance. As shown in 56 63 70 77 84 91 98 8 6 4 2 0 2 4 6 8 Range Encoder: MLP Linear-1 Channel Index (a) Range distribution for channel indices 50 to 100. 0 100 200 300 400 500 600 700 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.954 (b) Cosine similarity across 100 samples for each channel. Figure 8. Range distribution and cosine similarity of Linear-1 ac- tivations in the MLP block of the image encoder. 56 63 70 77 84 91 98 200 150 100 50 0 50 100 150 200 Range Decoder: Token-to-Image Q Projection Channel Index (a) Range distribution for channel indices 50 to 100. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.937 (b) Cosine similarity across 100 samples for each channel. Figure 9. Range distribution and cosine similarity of pre- projection activations for Token-to-Image Query in the mask de- coder. Figs. 10a and 11a, Key activations remain relatively stable, whereas Value activations demonstrate higher variance. Lastly, the range distribution of Linear-1 activations in the MLP block of the mask decoder is shown in Fig. 12a. 1 56 63 70 77 84 91 98 8 6 4 2 0 2 4 6 8 Range Decoder: Image-to-Token K Projection Channel Index (a) Range distribution for channel indices 50 to 100. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.955 (b) Cosine similarity across 100 samples for each channel. Figure 10. Range distribution and cosine similarity of pre- projection activations for Image-to-Token Key in the mask de- coder. 56 63 70 77 84 91 98 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 Range Decoder: Image-to-Token V Projection Channel Index (a) Range distribution for channel indices 50 to 100.\n\n--- Segment 38 ---\nRange distribution and cosine similarity of pre- projection activations for Image-to-Token Key in the mask de- coder. 56 63 70 77 84 91 98 7 6 5 4 3 2 1 0 1 2 3 4 5 6 7 Range Decoder: Image-to-Token V Projection Channel Index (a) Range distribution for channel indices 50 to 100. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.854 (b) Cosine similarity across 100 samples for each channel. Figure 11. Range distribution and cosine similarity of pre- projection activations for Image-to-Token Value in the mask de- coder. Fortunately, we verified that in all layers exhibiting se- vere channel-wise variation, the optimal quantization pa- rameters remain stable across different samples. As shown in Figs. 3, 7b, 8b, 9b, 10b, 11b and 12b, the normalized cosine similarity scores remain consistently high across most channels. Therefore, the CAG-based grouping strat- egy proves to be a highly reliable approach for enhancing SAM s quantization performance. 56 63 70 77 84 91 98 0.10 0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 0.10 Range Decoder: MLP Linear-1 Channel Index (a) Range distribution for channel indices 50 to 100. 0 50 100 150 200 250 0.8 0.9 1.0 Inter-Sample Similarity Cosine Similarity Channel Index Avg. Similarity Across Tensor 0.904 (b) Cosine similarity across 100 samples for each channel. Figure 12. Range distribution and cosine similarity of Linear-1 activations in the MLP block of the mask decoder. B. Hardware Cost Benchmark Across Differ- ent Quantization Granularity To benchmark the hardware cost associated with different quantization granularities, we analyze the implementation overhead of the SAM-H model under three configurations: per-tensor quantization, per-channel quantization, and CAG in AHCPTQ. Deep learning accelerators typically handle quantization parameters in two ways: (1) Storing scales and zero points in on-chip registers.\n\n--- Segment 39 ---\nHardware Cost Benchmark Across Differ- ent Quantization Granularity To benchmark the hardware cost associated with different quantization granularities, we analyze the implementation overhead of the SAM-H model under three configurations: per-tensor quantization, per-channel quantization, and CAG in AHCPTQ. Deep learning accelerators typically handle quantization parameters in two ways: (1) Storing scales and zero points in on-chip registers. This approach completely eliminates data transfer between the off-chip DRAM and the accelerator, ensuring zero trans- mission latency and cost. However, it increases area and resource overhead due to the large number of registers re- quired. (2) Storing scales and zero points in off-chip DRAM. This method reduces the on-chip resource overhead but in- curs substantial energy and latency costs due to frequent DRAM accesses [10]. As illustrated in Fig. 4, any combination between these two approaches follows a trade-off curve reducing one overhead inevitably increases the other. To mitigate this trade-off, we propose CAG, which clusters quantization pa- rameters to significantly reduce their count. By grouping scale and zero-point values into just four clusters, CAG reduces the required registers in (1) by 99.7 or the data transfer cost in (2) by 99.7 . This approach enables hard- ware efficiency close to per-tensor quantization while main- taining accuracy comparable to per-channel quantization. Since only 144 registers are needed to store four groups of scale and zero points in 4-bit quantization, we opt to store them on-chip, ensuring a more efficient and simplified hard- 2 ware design. C. Hardware Implementation Details Our AHCPTQ approach is implemented on an FPGA, with a custom accelerator designed to validate the quantization strategy. Fig. 13a depicts the validation system, which includes a host PC for data transfer via Ethernet, DDR4 DRAM for data buffering, and a Xilinx Artix UltraScale FPGA configured with the accelerator. The general ar- chitecture of the accelerator, shown in Fig. 13b, incorpo- rates four groups of buffers that leverage on-chip BRAM resources. These buffers temporarily store input activa- tions, model weights, and output activations before and af- ter quantization.\n\n--- Segment 40 ---\n13b, incorpo- rates four groups of buffers that leverage on-chip BRAM resources. These buffers temporarily store input activa- tions, model weights, and output activations before and af- ter quantization. The accelerator supports two configura- tions of PEs: (1) an 8-input integer multiplier-and-adder PE for uniform quantization, where both inputs and out- puts are integers and partial sums are updated using an ac- cumulator, and (2) an 8-input decimal bit-shift-and-adder PE for log2 quantization, with integer inputs, decimal out- puts, and a decimal accumulator for partial sums. In our 4- bit AHCPTQ implementation, the accelerator uses 64 lanes for multiplier and bit-shift PEs. To enable HLUQ, weights and activations must be dynamically allocated to their cor- responding PEs. This allocation logic is embedded in the controller, which indexes the MSBs of activations. When data is read from the IA buffer, weights and activations are automatically distributed to the corresponding PE types. After completing the integer and decimal inner product operations, the output values are transferred to the quantiza- tion processor in the FP32 domain. A small set of quantiza- tion parameter registers loads the scales required for the cur- rent and next layers, switching addresses to provide these scales to the quantization processor. The output activations are then fed into the dequantization unit, where, if HLUQ is applied, the uniform and log2 branches are merged. Subse- quently, the activations pass through the activation functions and quantization units, ensuring the resulting integer values can be directly used by the next layer. Finally, the activa- tions are sent to the output buffer. The dequantization, acti- vation function, and quantization processes are designed in a pipeline to minimize latency, which is critical for HLUQ deployment in practical applications. Weights are grouped offline and reordered based on group indices, while activations are reordered on-the-fly. At the start of layer computation, the accelerator accesses the DRAM to transfer weights and activations to the weight and IA buffers. Simultaneously, the quantization processor loads the quantization parameters required for quantization and dequantization.\n\n--- Segment 41 ---\nAt the start of layer computation, the accelerator accesses the DRAM to transfer weights and activations to the weight and IA buffers. Simultaneously, the quantization processor loads the quantization parameters required for quantization and dequantization. The controller then dispatches paired weights and activations to the appropriate PE lanes based on the MSB of the activations. For instance, with β 1 2 as described in Sec. 3.3.1, activations with an MSB of 1 are routed to multiplier PE lanes, while those with an MSB of PC Host Ethernet DDR4 FPGA Accelerator (a) Evaluation system. DRAM 8 8 Processing Core Multiplier PE Lanes Bit-Shift PE Lanes Weight Buffer IA Buffer Controller MSB Index MSB Index Quantization Reorder Buffer Output Buffer Pipeline Processing Quant Param Registers FP32 Processing DeQuantization Arithmetic Activation Functions Quantization Arithmetic (b) Accelerator architecture. Figure 13. Overview of the evaluation system and the accelerator architecture for the AHCPTQ configuration. 0 are sent to bit-shift PE lanes. Once computation is com- pleted, the results stored in the accumulators are clustered into two categories and passed to their respective uniform or log2 dequantization units. The dequantized FP32 activa- tions are then written back to the quantization buffer using a tailored addressing logic to restore their default sequence. Following this, the activations undergo the activation func- tion and quantization for the next layer before being sent to the output buffer and ultimately recycled back to DRAM. To streamline the process, the reorder logic is fused into the DRAM controller, which writes activations to grouped ad- dresses in DRAM when CAG is applied to the next layer. We implemented the RTL of the accelerator, Ethernet interface, and DRAM controller in Verilog, synthesized the design using Vivado Design Suite, and deployed it on the ALINX AXAU15 development board. As illustrated in Fig. 14, weights and activations are stored in on-board DDR4 DRAM and transferred via Ethernet from the host PC. The accelerator operates at 200 MHz to assess speedup and energy efficiency. For comparison, we implemented two baseline accelerators alongside our AHCPTQ config- 3 PC Host ALINX AXAU15 Dev. Board LAN Cable Figure 14. FPGA validation environment.\n\n--- Segment 42 ---\nBoard LAN Cable Figure 14. FPGA validation environment. uration: (1) a standard FP32 implementation, (2) a default INT8 implementation. In all designs, floating-point opera- tions such as quantization and dequantization are handled by an IP generator utilizing on-chip DSP resources. The detailed experimental results are presented in Sec. 4.4. D. Experiment on Vision Transformers To ensure that AHCPTQ generalizes to other vision models and tasks, we evaluate its effectiveness on ImageNet for im- age classification using DeiT. We integrate AHCPTQ into I S-ViT [47], the latest state-of-the-art PTQ framework, by replacing its original post-GELU quantizer with HLUQ. Table 4. Comparison of W4A4 PTQ methods on DeiT based on image classification accuracy on the ImageNet dataset. Method Opti. DeiT-T DeiT-S DeiT-B FQ-ViT [23] 0.10 0.10 0.10 PTQ4ViT [41] 36.96 34.08 64.39 APQ-ViT [4] 47.94 43.55 67.48 BRECQ [17] 55.63 63.73 72.31 QDrop [39] 61.93 68.27 72.60 PD-Quant [24] 62.46 71.21 73.76 RepQ-ViT [21] 57.43 69.03 75.61 I S-ViT [47] 65.21 75.81 79.97 AHCPTQ 66.11 76.12 80.07 In vision transformers (ViTs), inter-channel variation can be effectively addressed by reparameterizing Layer- Norm s weight and bias, as LayerNorm consistently pre- cedes the QKV linear projection. Consequently, we follow the settings of RepQ-ViT [21] and I S-ViT [47], applying reparameterization to reduce inter-channel variance, mak- ing CAG unnecessary in this case. However, in SAM s de- coder, the LayerNorm placement differs significantly from ViT s image encoder, making efficient reparameterization infeasible. This fundamental difference motivated the in- troduction of CAG as a dedicated PTQ solution for SAM with high hardware efficiency.\n\n--- Segment 43 ---\nHowever, in SAM s de- coder, the LayerNorm placement differs significantly from ViT s image encoder, making efficient reparameterization infeasible. This fundamental difference motivated the in- troduction of CAG as a dedicated PTQ solution for SAM with high hardware efficiency. We perform PTQ on DeiT-T, DeiT-S, and DeiT-B, comparing AHCPTQ against static PTQ methods (FQ- ViT [23], PTQ4ViT [41], APQ-ViT [4], RepQ-ViT [21]) and optimization-based PTQ methods (BRECQ [17], QDrop [39], PD-Quant [24], I S-ViT [47]). As shown in Table 4, AHCPTQ achieves the highest classification accu- racy in W4A4 quantization, surpassing all competing meth- ods. E. Parameter Sensitivity Analysis To initialize ˆb, s1, and s2 for subsequent optimization in HLUQ, we perform a grid search over two parameters, α and β. We extend the search to α {0.1, 0.2, . . . , 0.9} and β { 1 8, 1 4, . . . , 7 8} on SAM-B with YOLOX. The resulting W4A4 W5A5 W6A6 mAP scores of 13.1 32.0 35.3 align with the reported values as shown in Table 5, indicating that quantization performance is empirically insensitive to these parameters and a limited subset suffices. Table 5. Parameter sensitivity analysis on SAM-B with YOLOX. Method W4A4 W5A6 W6A6 Default 13.4 31.8 35.4 Extended Search 13.1 32.0 35.3 F. Generalizing CAG to Weight Grouping To ensure a fair comparison with existing baselines, we ap- ply per-channel quantization to model weights. However, CAG can also be extended to weight quantization, poten- tially broadening its applicability. To evaluate this, we ap- ply CAG with 32 groups to all model weights on SAM- B with YOLOX.\n\n--- Segment 44 ---\nHowever, CAG can also be extended to weight quantization, poten- tially broadening its applicability. To evaluate this, we ap- ply CAG with 32 groups to all model weights on SAM- B with YOLOX. The resulting quantization performance is comparable to the per-channel baseline as shown in Table 6, demonstrating CAG s generalizability to model weights. Table 6. Performance analysis of applying CAG with 32 groups to the model weights of SAM-B using YOLOX. Method W4A4 W5A6 W6A6 Default 13.4 31.8 35.4 Weight Grouping 12.3 30.8 34.3 4 Floating-Point BRECQ QDrop PTQ4SAM AHCPTQ Figure 15. Qualitative comparison of segmentation masks generated by different quantization methods on SAM-B with YOLOX. Our AHCPTQ closely matches the floating-point reference, significantly outperforming other baselines. G. Comparison of Visualization Results We further provide visualization results on W4A4 quan- tization of SAM-H using YOLOX, comparing AHCPTQ with existing quantization methods, including BRECQ [17], QDrop [39], and PTQ4SAM [29], as illustrated in Fig. 15. Qualitatively, our AHCPTQ consistently generates segmen- tation masks closely resembling those obtained from the original floating-point model, preserving finer structural details and accurate object boundaries. In contrast, the masks produced by other PTQ methods exhibit notable degradations. For instance, QDrop and PTQ4SAM often fail to capture intricate object contours, resulting in coarse segmentation masks and inaccurate boundary delineations. BRECQ, while generally performing better than QDrop and PTQ4SAM, still experiences noticeable detail loss and frag- mented segmentation in complex regions. Overall, the vi- sualization outcomes clearly demonstrate that AHCPTQ ef- fectively addresses the quantization challenges inherent in SAM, providing superior segmentation quality comparable to the floating-point baseline, thus confirming its effective- ness and robustness in practical low-bit deployment scenar- ios. 5\n\n