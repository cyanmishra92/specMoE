=== ORIGINAL PDF: 2504.14152v1_FGMP_Fine-Grained_Mixed-Precision_Weight_and_Activ.pdf ===\n\nRaw text length: 65749 characters\nCleaned text length: 65409 characters\nNumber of segments: 43\n\n=== CLEANED TEXT ===\n\narXiv:2504.14152v1 [cs.AR] 19 Apr 2025 FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference Coleman Hooper1,2 , Charbel Sakr1, Ben Keller1, Rangharajan Venkatesan1, Kurt Keutzer2, Sophia Shao2, Brucek Khailany1 1NVIDIA, 2University of California, Berkeley Abstract Quantization is a powerful tool to improve large language model (LLM) inference efficiency by utilizing more energy-efficient low- precision datapaths and reducing memory footprint. However, ac- curately quantizing LLM weights and activations to low preci- sion is challenging without degrading model accuracy. We pro- pose fine-grained mixed precision (FGMP) quantization, a post- training mixed-precision quantization methodology that maintains accuracy while quantizing the majority of weights and activations to reduced precision. We co-design hardware which exploits the mixed-precision data representation to achieve greater energy effi- ciency and which quantizes activations to mixed precision on the fly. Our work makes the following contributions: 1) We develop a policy that uses the perturbation in each value, weighted by the Fisher information, to select which weight and activation blocks to keep in higher precision. This approach preserves accuracy by identifying which weight and activation blocks need to be retained in higher precision to minimize the perturbation in the model loss. 2) We also propose a sensitivity-weighted clipping approach for fine-grained quantization which helps retain accuracy for blocks that are quantized to low precision. 3) We then propose hardware augmentations to leverage the efficiency benefits of FGMP quan- tization. Our hardware implementation encompasses i) datapath support for FGMP at block granularity, and ii) a mixed-precision activation quantization unit to assign activation blocks to high or low precision on the fly with minimal runtime and energy overhead. Our design, prototyped using NVFP4 (an FP4 format with microscal- ing) as the low-precision datatype and FP8 as the high-precision datatype, facilitates efficient FGMP quantization, attaining 1 perplexity degradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8 baseline design while consuming 14 less energy during inference and requiring 30 less weight memory. 1 Introduction Large language models (LLMs) are scaling rapidly in both model size and computational requirements, which makes it challenging to efficiently perform inference using these networks. One approach for reducing both computational and memory requirements for LLM inference is quantization of model weights and activations to low-precision number formats. However, aggressive quantization of all weights and activations is challenging due to the presence of data particularly sensitive to perturbations, such as large-magnitude outliers in weights and activations. Quantization-aware training (QAT) may be employed to mitigate these perturbations [13, 20]; however, the costs of re-training and unavailability of training Work done during internship at NVIDIA Figure 1: Perplexity degradation (less is better) versus com- pression rate (higher is better), evaluated using Llama-2-7B on Wikitext-103 with 4-bit weight-activation quantization. Compression rate is computed as 16 divided by the aver- age bit width for weights and activations (assuming 4K con- text length). We group prior work into algorithm-only ap- proaches that use integer quantization ( Algo. ), prior work on microscaling quantization ( ğœ‡scale ), and prior methods for mixed-precision quantization ( MP ). Our method, FGMP, (shown here with 70 , 80 , and 90 of blocks in FP4) at- tains reduced perplexity degradation relative to existing post- training quantization methods.1 data with recent LLMs can make QAT difficult and motivates post- training quantization (PTQ) approaches [10, 16]. In this paper, we explore techniques to improve PTQ inference accuracy. Using low precision (e.g. 4-bit) quantization for weights and activations for LLMs is desirable since it allows the use of energy- efficient low-precision compute datapaths (in addition to mem- ory savings). However, accurate post-training quantization with weights and activations in 4-bit precision is challenging; prior works still have substantial accuracy degradation when quantizing to low precision for both weights and activations [2, 19, 26, 32]. The rea- son that these methods struggle to retain accuracy in aggressive regimes like 4-bit quantization is due to challenges with numerical outliers which skew the quantization range [6, 7, 15], as well as disproportionately sensitive values [15]. To address this challenge, previous works have applied mixed-precision quantization, where a mixture of high-precision and low-precision quantization is used to preserve accuracy by retaining particularly sensitive or hard to 1Note that 6-bit configurations are reported for [26, 32] since the corresponding W4A4 configurations have greater than 1 PPL loss, and [26, 35] quantize the KV cache activations in addition to the linear layers. 1 Dot Product Dimension Activation Weight FP4 FP8 Block Size BS Large impact on model output Small impact on model output 1 FP8 FP4 BS 1 a) FGMP Visualization b) Sample block precision assignments with our policy Figure 2: a) Diagram outlining our approach for FGMP quantization at block granularity. We quantize both weights and activations to mixed precision, preserving sensitive blocks in higher precision (FP8) and quantizing the rest to low precision (NVFP4). b) Visualization of a slice of the activation and weight precision assignments for Layer 7 Fully Connected 1 ( FC1 ) in the Llama-2-7B model using our policy with 10 of blocks in FP8 and with our sensitivity-weighted clipping method applied. The NVFP4 and FP8 blocks are interleaved in an unstructured manner. quantize parts of the network in higher precision [1, 35]. However, existing methods typically perform coarse-grained mixed precision (e.g., at the granularity of rows of a matrix [1, 35]). As oulined in Figure 2, the distribution of sensitive and hard-to-quantize values is unstructured in both LLM weights and activations; existing coarse- grained mixed-precision quantization methods cannot fully adapt to these unstructured distributions. Our work addresses the challenges with existing mixed-precision quantization methods by leveraging fine-grained mixed-precision (FGMP) quantization, in which we identify small, contiguous re- gions of hard-to-quantize weights and activations to be retained in a higher-precision data format while the majority of weights and activations are quantized to a lower-precision format. We pursue mixed-precision quantization at much finer block granularity. We define a block as a small ( 100 elements) 1-dimensional vector along the dot product dimension of each tensor (as in [12]). We perform FGMP quantization by independently assigning each block to either a higher-precision or lower-precision data format (see Figure 2). This provides improved accuracy since it allows for more flexible precision assignment, adapting to different distributions of sensitive values (both inter-layer and intra-layer). However, although FGMP is a promising approach for maintaining model accuracy with the majority of values in reduced precision, it is challenging to leverage the potential efficiency benefits due to the difficulty of performing computation using different precisions for adjacent blocks. There are also distinct challenges with applying FGMP quantization to activations, as we need to be able to efficiently identify hard-to- quantize activation blocks online during inference to quantize the output activations of each layer. Even if we leveraged low-precision datapaths for the majority of computations, software implementa- tions for dynamically selecting precisions when quantizing blocks of activations may be impractical. Our work aims to address these challenges through hardware- software co-design in order to enable fine-grained mixed-precision quantization while supporting low-precision computation for the majority of blocks. We pursue FGMP quantization where the major- ity of LLM weights and activations are quantized to low precision (e.g. FP4), with only a small percentage of values kept in higher precision (e.g. FP8). As outlined in Figure 2, we quantize the weights and activations at block granularity. We use microscaling, which defines a fine-grained scaling factor as metadata for each block of data, to reduce quantization error for the blocks that are quantized to low precision. Microscaling has been shown to enable more ac- curate low-precision quantization [5, 24] with minimal hardware overhead. Specifically, we leverage NVFP4 [21] as our low-precision datatype in order to accurately quantize each block to reduced preci- sion. Additionally, we propose hardware augmentations to support FGMP at block granularity while performing the majority of compu- tation using low-precision datapaths. Figure 1 highlights how our methodology achieves reduced perplexity degradation for the same compression ratio relative to existing work on weight activation quantization. As highlighted in Section 5.4, utilizing low-precision datapaths also provides distinct energy savings, with NVFP4 dat- apaths with microscaling consuming 33 less energy than FP8. Our prototype demonstrates the efficiency benefits of our FGMP methodology. Our work makes the following contributions: (1) We develop a policy to determine which blocks of weights and activations need to be retained in higher precision. We leverage sensitivity information with respect to the final model output to determine which blocks can be quantized to reduced precision without degrading the accuracy of the network. Importantly, we ensure that this saliency policy is computationally efficient and can be computed on-the-fly for activations, thereby allowing for fast mixed-precision activation quantization. (2) We also design a fine-grained weight clipping strategy to improve the representation capacity for the blocks that are retained in lower precision. We tune the scale factor 2 for each low-precision block to minimize the impact of quantizing the block on the model s output. (3) We co-design the hardware to accelerate FGMP quantiza- tion: i) We adapt a Vector Multiply-Accumulate (VMAC) datapath to allow for performing mixed-precision dot prod- ucts at 1D block granularity. Our datapath design supports FGMP quantization for both weights and activations. ii) We also design a mixed-precision activation quantization unit to assign activation blocks to low or high precision on the fly based on runtime statistics, thereby allowing for dynam- ically identifying and preserving regions of the activations that are challenging to quantize in higher precision. By leveraging our FGMP quantization method with NVFP4 and FP8 as the low and high precision number formats, we are able to attain the efficiency benefits of low-precision quantization for LLM inference while maintaining accuracy. For the Llama-2-7B model, we attain 14 energy savings in dot product computations during inference with 1 accuracy degradation (measured as perplexity on Wikitext-103). Figure 1 also demonstrates how our method acheives less perplexity degradation for a target compres- sion ratio relative to prior methods. Our approach also requires 30 less parameter memory than serving the model in FP8, enabling the serving of a larger model with the same memory budget. 2 Related Work This section outlines prior work relevant to FGMP quantization. We first provide an overview of prior work on quantizing both weights and activations in LLMs. We then discuss previous efforts to leverage mixed-precision quantization to improve accuracy with low-precision quantization. Finally, we include a discussion of ex- isting hardware support for mixed-precision quantization. 2.1 LLM Quantization Quantization is a popular model compression method to improve inference efficiency. Most existing quantization strategies have em- ployed either integer or floating-point quantization [10, 17]. More complex non-uniform quantization schemes allow for more flexible quantizaton signpost placement compared to integer and floating- point quantization [15]; however, since quantization signposts are no longer uniformly placed, quantized values must first be dequan- tized and computation must be performed in higher precision. Prior work has also leveraged microscaling in order to enable accurate low-precision quantization [5, 24]. Microscaling formats leverage a fine-grained scaling factor for each block, which better preserves the values in the block by adjusting the represented range. Typical quantization methods for LLMs have performed either weight-only [10, 17] or weight-activation [1, 26] quantization. In the weight-only regime, prior methods have been able to maintain high accuracy while compressing the model to 4-bit quantization; how- ever, these approaches cannot utilize standard low-precision com- pute datapaths, as they still represented activations using higher precision. In the weight-activation regime, it is possible to leverage low-precision compute datapaths to attain higher energy efficiency. However, existing methods have struggled to retain accuracy in this regime due to the challenges of accurately quantizing both weights and activations to low precision without retraining [19, 26, 32]. In this work, we aim to perform post-training weight-activation quantization in order to exploit low-precision compute units, while maintaining accuracy through retaining a small portion of blocks in higher precision. 2.2 Mixed-Precision Quantization Several works have noted the presence of distinct outliers in both weights and activations, which significantly increase quantization difficulty [4, 6, 7, 15]. Previous works have split LLM weight matri- ces into a dense low-precision matrix and an unstructured sparse high-precision matrix, which is used to accurately store numerical outliers and highly sensitive values [4, 7, 15]. Although these ap- proaches achieved high accuracy with low memory consumption for weight quantization, they cannot be easily applied for activa- tion quantization and they lead to efficiency overheads in use cases that are not memory bandwidth-bound. [12] employed fine-grained mixed-precision quantization at the 1D block granularity for LLM weights by leveraging sensitivity information, demonstrating the advantages of exploiting sensitivity information when determining which blocks to retain in higher precision. However, this work was restricted to weight-only quantization, and was therefore unable to leverage standard low-precision compute datapaths. There have also been previous works on coarse-grained struc- tured mixed precision, in which a fixed block of the matrix is kept in higher precision, and the remainder of the matrix is quantized to low precision. Several prior works have assigned different preci- sions to different layers (aiming to preserve more sensitive layers in higher precision) [8, 9, 30, 34]. Other works have performed structured mixed-precision quantization by preserving a subset of input channels in higher precision. [6] preserved large-magnitude activation input channels (and the corresponding input channels in weights) in high precision, while quantizing the remaining values. [1, 35] reordered input channels in order to group large-magnitude activation channels together at the end of the matrix, and then left a portion of the input channels at the end of the matrix in higher precision for both weights and activations. These works were able to leverage low-precision datapaths for part of the computation since they quantize both weights and activations. However, these methods were unable to adapt to the unstructured nature of impor- tant values in weights and activations. In contrast, our fine-grained mixed-precision approach can adapt to unstructured sensitive val- ues in both weights and activations, while still performing the majority of computation using low precision datapaths. There have also been prior works which searched for the opti- mal mixed-precision configuration, either by using reinforcement learning [30] or by formulated the search as a Neural Architecture Search problem [31]. However, existing automatic mixed precision search methods are challenging to apply for LLMs, as these methods require repeated evaluation of the network with different precision configurations for each layer, which can be prohibitively expensive. Additionally, these approaches have previously been applied for layer wise mixed precision. With mixed precision at the 1D block granularity, the search space of potential precision assignments is much greater than the layer wise precision assignment problem. It is therefore infeasible to leverage existing automatic methods to search for the optimal fine-grained mixed precision assignment. 3 2.3 Hardware Support for Mixed-Precision Quantization Several prior works have investigated methods for adding archi- tecture support for representing numerical outliers in order to improve low-precision quantization. GOBO [33] combines outlier- aware quantization with non-uniform quantization for better low- precision representation. However, this approach requires sepa- rately loading outliers and dense values (as the sparse values are stored in a separate sparse matrix), as well as separate datapaths for outlier processing. OLIVE [11] employs an outlier-aware encod- ing, storing outlier values across multiple positions (sacrificing the neighboring dense values). They repurposed one of the 16 repre- sentative 4-bit values to indicate whether an outlier is present in a neighboring value, which amounts to an overhead of 1 bit per 4 elements. This has the benefit of allowing for a structured data stor- age format, but it negatively impacts the representation of nearby values. One concurrent work, MicroScopiQ [23], also retains outlier values in higher precision by pruning out a portion of non-outlier values and repurposing the saved bits to represent the outliers in higher precision. SPARK [18] employs a variable-precision encod- ing scheme where large and small magnitude values are encoded with different precisions, and a bit at the start of each element indicates which precision is stored in that location. This requires per-element metadata (eg. a bit for each low-precision element) to mark whether that element is in low or high precision. Both OLIVE and SPARK [11, 18] employ decoder modules at the boundary of a systolic array in order to decode their compact data format into a format that can be fed into an array of processing elements. While these works perform decoding at the per-element level, we instead implement FGMP at block granularity. This dramatically reduces metadata overhead compared to the per-element approach by amor- tizing the cost of the metadata across all elements in a block (so for a block size of 16, it requires only a single bit per 16 elements) and only requires performing a comparison with a single bit per block rather than performing per-element comparisons. FGMP greatly improves the efficiency of the hardware implementation by amor- tizing control overhead and enabling the use of efficient vector multiply-accumulate operations, while still providing sufficiently fine granularity to maintain high accuracy. 3 Fine-Grained Mixed-Precision Quantization In this work, we focus on joint weight activation quantization, specifically targeting the linear layers in LLM matrices, where a weight matrix W and activation matrix X are multiplied in order to compute the output Y, i.e. Y W X.2 The model is composed of ğ¿layers, and the weights and activations for layer ğ‘™will be denoted as W(ğ‘™) and X(ğ‘™), respectively. The model is trained by optimizing in order to reduce its loss L, which is a function of both the weights and the activations corresponding to a given input data, as well as the ground truth labels corresponding to this data: L({X(ğ‘™)}ğ¿ ğ‘™ 1, {W(ğ‘™)}ğ¿ ğ‘™ 1) (1) By minimizing the increase in the loss due to perturbation from quantizing the weights and activations, we can thereby minimize 2Throughout this work, we use capital boldface letters to refer to matrices and lower- case boldface letters to refer to 1D blocks. the perturbation in the model output, which is desirable for main- taining accuracy. When we quantize an activation or weight value ğ‘£to precision ğ‘using quantization function ğ‘„ğ‘, we will express the quantization error at precision ğ‘, Î”ğ‘ğ‘£, as follows: Î”ğ‘ğ‘£ ğ‘„ğ‘(ğ‘£) ğ‘£ (2) In order to perform FGMP quantization, we first develop a policy to determine which weights and activations need to be retained in higher precision to preserve model accuracy. While our experimen- tal results in this work leverage FP8 and NVFP4 as the higher and lower precision number formats, respectively, our precision assign- ment policy developed in this section is generalizable to any choice of high-precision and low-precision formats. We calibrate offline for a target sensitivity level above which the block is preserved in higher precision, and we apply a single threshold across different layers to allocate more high-precision blocks to more sensitive lay- ers. Finally, we develop a fine-grained weight clipping approach to improve the quantized representation of low-precision blocks with microscaling. Taken together, our methodology facilitates ac- curate fine-grained mixed-precision quantization with only a small portion of blocks retained in higher precision. 3.1 Precision Assignment Policy Our precision assignment policy determines which blocks to keep in higher precision in order to avoid degrading the accuracy of the model. We can avoid this accuracy degradation by minimizing the degradation in the model s loss L when the weights and activations are perturbed. We first consider how the loss is affected by pertur- bation in a single weight or activation value ğ‘£at a given layer ğ‘™. The loss can be expressed using Taylor expansion as follows (neglecting higher-order terms), where the gradient ğ‘”is the derivative of the loss with respect to the corresponding value, L ğ‘£: L(ğ‘„(ğ‘£)) L(ğ‘£) ğ‘” Î”ğ‘£ . . . (3) To minimize the expected degradation in model loss, we therefore study the minimization of the following expression: Eğ‘£ L(ğ‘£) L(ğ‘„(ğ‘£)) 2 Eğ‘£ ğ‘”2 (Î”ğ‘£)2 (4) Eğ‘£ ğ‘”2 Eğ‘£ (Î”ğ‘£)2 (5) Equation 5 holds under the assumption that ğ‘”and Î”ğ‘£are inde- pendent, meaning that the impact of perturbation in each weight or activation value on the model loss is independent of the pertur- bations in other values [25]. The expression Eğ‘£ ğ‘”2 corresponds to the diagonal Fisher information matrix. Empirically, this can be evaluated by averaging the squares of the gradients over a set of calibration data. For weight matrices, we calculate this by averag- ing the squared gradients over a sample dataset ğ·(in our case, 512 samples of sequence length 512 from the Wikitext-103 training set). Since activation values are dynamic during inference, we do not have access to their gradients at runtime. We instead use the aver- age of the square of the gradients for each input channel (computed offline on the calibration set) to estimate per-channel sensitivity for activations. For the remainder of the section, we will assume that 4 gradients have been calibrated, and consequently, expectations are implied. Considering a one-dimensional block v (either an activation or weight block in layer ğ‘™) containing ğ‘elements, we can express the impact ğ¼L of quantizing the block v on the model s loss as the sum of the sensitivity-weighted quantization error for each element ğ‘£ğ‘– in v (ignoring layer indices): ğ¼L(v) ğ‘ ğ‘– 1 ğ‘”2 ğ‘–(Î”ğ‘£ğ‘–)2 (6) The above metric is used as a ranking mechanism to identify the most sensitive blocks, which are then preferentially kept in higher precision. The increase in error for value ğ‘£when it is quantized in low precision ğ‘ğ‘™rather than high precision ğ‘â„can be expressed as follows: Î”ğ‘â„ ğ‘ğ‘™ğ‘£ Î”ğ‘ğ‘™ğ‘£ Î”ğ‘â„ğ‘£ (7) The impact on the final model output when a block v of size ğ‘ containing values ğ‘£ğ‘–is quantized in low precision rather than high precision is therefore: ğ¼ L(v) ğ‘ ğ‘– 1 ğ‘”2 ğ‘–(Î”ğ‘â„ ğ‘ğ‘™ğ‘£ğ‘–)2 (8) We use the impact score ğ¼ L(v) to identify the weight and activa- tion blocks for which low-precision quantization has the biggest impact on the model s output, and we preferentially preserve these blocks in higher precision. 3.2 Setting the Threshold for Precision Assignment In order to determine which blocks can be quantized to reduced precision, we need to determine the threshold for the importance score ğ¼ L(v), above which the block must be retained in higher precision. One option would be to compute the threshold for a target ratio of high-precision to low-precision blocks dynamically for each layer. However, sweeping over the activation tensor for layer ğ‘™online during inference to compute ğ¼ L(v) for each block and then computing the target threshold for that layer would lead to unacceptable inference overhead (since we would need to write out each output block from the previous operation in both precisions before we knew which precision was going to be used). It is there- fore preferable to calibrate offline to determine the threshold for activations for a target mixed-precision ratio. Namely, for a target mixed-precision ratio ğ‘…, we can assign ğ‘… of blocks to be in higher precision by setting the threshold ğ‘‡ğ‘™ğ‘œğ‘ğ‘ğ‘™to be the ğ‘…-th percentile of the impact score ğ¼ L(v), computed over all ğ½ğ‘™blocks v in a single weight or activation tensor W(ğ‘™) or X(ğ‘™): ğ‘‡ğ‘™ğ‘œğ‘ğ‘ğ‘™ ğ‘ƒğ‘…({ğ¼ L(v(ğ‘—))}ğ½ğ‘™ ğ‘— 1) (9) where ğ‘ƒğ‘…() denotes the ğ‘…-th percentile. Additionally, different layers in the network exhibit differing sensitivities in terms of their impacts on the model output. In order to allow our policy to adapt to varying sensitivities at different layers, instead of calibrating for a separate threshold for each layer, we instead set a single global threshold across the entire model (one for weights and one for activations). Since our sensitivity-weighted policy for precision assignment estimates the impact that quantizing the block will have on the final model output, it is already normalized across different layers (meaning that we can use the same threshold across different layers in the network even if they have different average magnitudes). This allows for preserving a different proportion of sensitive blocks for particular layers which have a greater or lesser impact on the final model loss. For a target mixed-precision ratio ğ‘…, we therefore set threshold ğ‘‡ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™to be the ğ‘…-th percentile of the impact score ğ¼ L(v), computed over all blocks across all weight or activation tensors W(ğ‘™) or X(ğ‘™) (across all layers): ğ‘‡ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ ğ‘ƒğ‘…({{ğ¼ L(v(ğ‘—))}ğ½ğ‘™ ğ‘— 1}ğ¿ ğ‘™ 1) (10) Note that this threshold is computed separately for weights and for activations since the input activation size will vary at inference time. Figure 7 in Section 5.3 visualizes how our global threshold pol- icy allows for retaining a greater portion of blocks in high precision for particular layers. 3.3 Fine-Grained Sensitivity-Weighted Clipping Improving accuracy with low-precision quantization can provide noticeable benefits even in the context of FGMP quantization, as it can allow us to retain a greater portion of blocks in low pre- cision without compromising accuracy. One method to improve low-precision quantization is clipping, where we reduce the rep- resentable range of numbers in order to give us better resolution within this range. In the context of low-precision quantization with microscaling, performing clipping by adjusting the per-block scale factors can provide significant benefits as it allows us to perform fine-grained adjustments to best represent the values in a given block. To improve the representation of blocks that are quantized to reduced precision, we therefore pursue clipping with the per-block scale factors. In particular, we develop a sensitivity- weighted approach for fine-grained clipping to accurately quantize low-precision blocks without perturbing the model loss. For a given block v with ğ‘elements in a weight layer W(ğ‘™), the objective is to minimize the squared quantization error in v weighted by the square of the gradients ğ‘”. We therefore want to choose the scaling factor ğ‘ which minimizes this objective: min ğ‘  ğ‘ ğ‘– 1 ğ‘”2 ğ‘–( ğ‘  Î”ğ‘£ğ‘–)2 (11) where ğ‘  Î”ğ‘£ğ‘–is the quantization error for element ğ‘£ğ‘–when the per-block scaling factor is ğ‘ . With the low-precision datatype used in this work, the per-block scale factors are restricted to FP8 values, and as such there are only a limited number of possible values for ğ‘ . We can therefore perform a brute-force search over possible values for ğ‘ in order to identify the scale factors which minimize the sensitivity-weighted quantization error. Note that we only apply sensitivity-weighted clipping offline for the weight matrices, and we use dynamic-max clipping online for activations, as it is not possible to efficiently compute the optimal scale factors for activations online during inference. 5 3.4 Additional Baseline Policies In order to justify our precision assignment policy, we compare with multiple potential baseline methods for selecting which blocks to retain in higher precision (the comparison is provided in Figure 6 in Section 5.3). Our first baseline policy (referred to as Quantization Error ) determines which blocks to retain in higher precision using only the quantization error for each block within a single layer ğ‘™. The error when a 1D block v of size ğ‘is quantized in low precision ğ‘ğ‘™rather than high precision ğ‘â„can be expressed as: ğ¼ ğ‘„ğ¸(v) ğ‘ ğ‘– 1 (Î”ğ‘â„ ğ‘ğ‘™ğ‘£ğ‘–)2 (12) where Î”ğ‘â„ ğ‘ğ‘™gives the increase in error from quantizing ele- ment ğ‘£ğ‘–to low precision rather than high precision. As a second baseline, we also include an Output Error policy, which minimizes the error at the layer output for layer ğ‘™in order to determine which blocks to retain in higher precision for that layer. For this policy, we first calibrate for the average squared input channel magnitude across the other tensor Q. For quantizing a block v in weight matrix W(ğ‘™), we compute the average squared magnitudes for the corresponding input channels in X(ğ‘™), and for quantizing a block v in activation matrix X(ğ‘™) we compute the average squared magnitudes for the corresponding input channels in W(ğ‘™). We compute the average squared magnitudes for the input channels in X(ğ‘™) statically using calibration data from the Wikitext- 103 training set. Using this policy, the error when a 1D block v of size ğ‘is quantized in low precision ğ‘ğ‘™rather than high precision ğ‘â„can be expressed as: ğ¼ ğ‘‚ğ¸(v) ğ‘ ğ‘– 1 avg(ğ‘„2 ğ‘–)(Î”ğ‘â„ ğ‘ğ‘™ğ‘£ğ‘–)2 (13) where avg(ğ‘„2 ğ‘–) is the average squared input channel magnitude in the other tensor. For both baseline policies, in order to set the mixed-precision ratio ğ‘…, we set the threshold ğ‘‡to be the ğ‘…-th percentile of ğ¼ ğ‘„ğ¸(v) ğ¼ ğ‘‚ğ¸(v) computed dynamically over all blocks v in a single weight or activation layer W(ğ‘™) or X(ğ‘™). 4 Hardware Support for Fine-Grained Mixed-Precision Quantization Hardware support is required to use low-precision datapaths for the majority of computations when we leverage FGMP quantization. Hardware support for quantizing activations to mixed-precision on- line during inference is also needed to realize the energy efficiency benefits of FGMP quantization, as these benefits would otherwise be negated by the overheads of performing mixed-precision quan- tization in software. Our proposed design (outlined in Figure 3a) consists of an array of processing elements (PEs), each of which contains a Vector Multiply-Accumulate (VMAC) datapath consist- ing of multiple parallel lanes that each compute one VMAC per cycle [29]. The hardware augmentations required for supporting FGMP quantization for both weights and activations includes i) dat- apath modifications to support FGMP which are outlined in Figure 3b, and ii) a post-processing unit (PPU) that efficiently quantizes activations to mixed precision during inference which is outlined in Figure 4. We use NVFP4 as our low precision datatype [21], which uses microscaling to improve the quantization accuracy in low pre- cision, as in [5, 14, 24]. NVFP4 uses a block size of 16 and performs quantization using FP4 (E2M1) representation for the values with FP8 (E4M3) microscaling factors. Throughout the remainder of this work, when we refer to a block being quantized in FP4, this block is quantized using the NVFP4 datatype. We set the microscaling block size and the FGMP block size (ğµğ‘†) equal to the VMAC vector length to improve hardware efficiency. We use a single metadata bit alongside each block to indicate whether the block contains high or low precision values. 4.1 Mixed-Precision Datapath Figure 3b outlines the multi-lane mixed-precision VMAC datapath which is required for performing efficient mixed-precision computa- tion. The datapath contains FP4, FP8, and mixed FP4 8 dot-product units. In a single cycle, the active dot-product unit in each lane (determined by inspecting the metadata bit associated with each weight and activation block) computes ğµğ‘†-wide dot products across ğ¿parallel lanes and adds this FP32 partial sum to the previously accumulated partial sum, thereby performing 2 ğµğ‘† ğ¿operations per cycle. The dot-product units with at least one FP4 input also perform the scale factor multiplication before adding the partial sum to the accumulated value. The remaining three dot-product units in each lane are inactive and are clock-gated or data-gated appropriately to minimize switching power. When computing a matrix multiplication between a weight tile A and an activation tile B, we assume a weight-stationary dataflow in which A is held sta- tionary and blocks in B are streamed in one per cycle and broadcast across all lanes. Note that we maintain the same math throughput per cycle regardless of the precision of the weights and inputs being consumed, which improves energy efficiency by maximizing tem- poral reuse of A and spatial reuse of B while avoiding complicated control logic. Our proposed design contains independent FP4 FP8 and FP8 FP4 dot-product units in each lane so that the ğ´input can be consistently held constant during the innermost temporal loop, improving energy efficiency at the cost of area overhead. Note that datapath components could be shared to reduce area; however, this would come at the cost of increased energy consumption. Using shared hardware for all four dot-product combinations would obvi- ate any energy gains of FGMP. Sharing hardware between two or three dot-product formats could still realize some (reduced) energy savings. 4.2 Post-Processing Activation Quantization Unit Figure 4 provides a diagram of our mixed-precision activation quan- tization unit, which is a post-processing unit that quantizes output activation blocks before writing them out to memory. In a typi- cal single-precision datapath, values are locally accumulated in high precision (e.g., FP32) before being sent to a PPU for scaling and quantization to the (single) activation data format. In contrast, our mixed-precision PPU must dynamically determine whether to quantize the accumulated values to FP8 or FP4 by using the average per-input channel Fisher information to compute the sensitivity- weighted sum of the quantization error for each output block. The 6 Weight Buffer Select DPath Weight Collector L Vector Lanes FP4 FP8 FP4 8 FP8 4 Weight Buffer Select DPath Weight Collector FP4 FP8 FP4 8 FP8 4 Vector MAC Datapaths A FP8? W FP8? A FP8? W FP8? Activation Collector x BS BS BS x x Sa Sw Activation Buffer Output Collector Output Collector Fully accumulated sums to PPU Partial sum back to datapath x x x BS Partial sum in Partial sum out Select DPath Select DPath b) Mixed-Precision, Multi- Lane Vector-MAC Datapath PE PE PE PE Vector Units (Contain Post-Processing Units) Processing Element Array Memory System a) Accelerator Architecture VU PPU VU PPU Figure 3: a) High-level accelerator architecture, consisting of a PE array (each of which contains a VMAC-based datapath), as well as one or more vector units which contain our post-processing activation quantization unit (outlined in detail in Figure 4). b) Datapath support for FGMP quantization with four dot-product units per lane to perform FGMP VMAC operations. The portion of the figure highlighted in gray is synthesized for hardware measurements. Quantize (FP4) Quantize (FP8) VMax Mixed-Precision Activation Quantization Unit (Q(Y)-Y)2 (Q(Y)-Y)2 BS BS FP4 8? FP4 8? - Quantized Output Activation Block Output Activation Block Mixed-Precision Activation Quantization Post-Processing Unit Per-Channel Sensitivity Information Get ğ-scale Threshold Figure 4: A diagram of the post-processing unit for quantizing activations to mixed-precision online during inference. activation block is then written out as either FP4 or FP8, depend- ing on whether the sum is greater than the configured threshold. The threshold is calibrated offline for a given model and is fixed throughout inference. Note that our approach for leveraging per- input channel metadata for weighting the parameters in a block in order to determine the block s importance is agnostic to how this metadata is computed. For example, this approach could also be used to run the baseline Output Error configuration described in Section 3.4, since it also uses statically calibrated per-input channel metadata to weight the values in each block when determining importance. 4.3 Experimental Approach We prototype our hardware implementation using SystemC and the Siemens Catapult High-Level Synthesis tool to generate RTL. RTL simulations are performed using Synopsys VCS with synthetic data sampled from an appropriately scaled Gaussian random dis- tribution, where the proportion of weight and activation blocks in FP4 and FP8 are independently adjusted to generate representa- tive input stimulus. We leverage the Synopsys Fusion Compiler for synthesis targeting a 1 GHz clock period in a 5 nm process, and we replay the test stimulus on the gate netlist to gather activity factors for power measurements using Synopsys PrimePower (TT 0.67 V corner). All measurements use a prototype design with ğ¿ 16 and ğµğ‘† 16, as in [5]. In order to collect energy estimates with a realistic workload, we need to leverage relevant estimates for the proportion of FP4 and FP8 blocks in different layers and for varying global threshold settings. As outlined in Figure 7 in Section 5.3, different layers have very different distributions of FP4 versus FP8 blocks, but measuring power for all possible layer configurations is runtime-intractable. In order to collect realistic data, we first profiled the portion of FP8 and FP4 blocks across different layers for both weights and activations. We collected this data for different global ratios of FP8 to FP4 blocks (from 90 to 10 ). We then treated each configuration as a set of features, normalized each feature, and performed K-means clustering to compute 100 representative configurations across all layers. After running power analysis on small kernels derived from each of these 100 configurations, we scaled up the results to match the shapes of each of the corresponding layers in order to estimate total energy consumption. 7 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 5.10 5.15 5.20 5.25 Perplexity Llama-2-7B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 4.62 4.64 4.66 4.68 4.70 4.72 4.74 4.76 Perplexity Llama-2-13B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 5.95 6.00 6.05 6.10 6.15 6.20 Perplexity Nemotron-4-15B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 10.0 10.1 10.2 10.3 10.4 Perplexity GPT3-1.3B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 7.40 7.45 7.50 7.55 7.60 Perplexity GPT3-8.3B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 6.54 6.56 6.58 6.60 6.62 6.64 6.66 6.68 6.70 Perplexity GPT3-22B Tradeoff Curve No Clip Clip NVFP4 1 Error FP8 BF16 Figure 5: Perplexity evaluation on Wikitext-103 for different models in the Llama-2, GPT3, and Nemotron model families. We report FGMP quantization results with and without our sensitivity-weighted weight clipping approach. Note that for GPT3-22B and Nemotron-4-15B, we did not apply sensitivity-weighted clipping as it did not yield perplexity improvements, and for Nemotron-4-15B, we exclude the BF16 baseline as it exhibited worse perplexity than FP8. Table 1: Perplexity on Wikitext-103 when applying sensitivity-weighted weight clipping SW-Clip") for Llama- 2-7B and Llama-2-13B. We report results for weight-only FP4 quantization with microscaling, both with and without sensitivity-weighted clipping. All configurations use BF16 for activations. Weight Precision Llama-2-7B Llama-2-13B BF16 5.06 4.61 FP4 5.18 4.69 FP4 (w SW-Clip) 5.13 4.67 Table 2: Average 5-shot evaluation results on MMLU for dif- ferent models in the Llama-2, GPT3, and Nemotron model families. Model Llama-2 GPT3 Nemotron-4 7B 13B 1.3B 8.3B 22B 15B BF16 0.458 0.551 0.247 0.252 0.387 0.643 FP8 0.462 0.550 0.249 0.248 0.383 0.647 FP4 0.430 0.533 0.258 0.258 0.371 0.634 90 FP4 0.448 0.543 0.252 0.253 0.382 0.637 70 FP4 0.451 0.547 0.256 0.251 0.382 0.642 5 Results We report a comprehensive accuracy evaluation of our quantiza- tion method and simulated energy measurements of our prototyped implementation in order to confirm the efficiency benefits of our approach. Sections 5.1 and 5.2 provide results for our methodology using perplexity as measured on Wikitext-103 and using down- stream task evaluation, respectively. Section 5.3 provides ablations for our precision assignment policy, as well as analysis for the distri- bution of sensitive blocks across different layers and the runtime for computing Fisher information for quantization. Section 5.4 presents detailed evaluation of our hardware implementation. 5.1 Perplexity Evaluation We evaluate our proposed mixed-precision block assignment pol- icy using models in the Llama-2, GPT3, and Nemotron families [3, 22, 28]. We used FP8 without microscaling and NVFP4 as the higher and lower precision data formats. Figure 5 shows the per- plexity on Wikitext-103 versus the percentage of blocks that are kept in FP8, including results with and without our sensitivity- weighted clipping approach. Our results show that with only a small percentage of blocks retained in FP8, we can attain significant accuracy improvements relative to quantizing all blocks to FP4. These results also demonstrate the perplexity improvements from incorporating our sensitivity-weighted weight clipping approach (particularly when a higher proportion of blocks are quantized in FP4). Figure 1 also provides comparisons against previous methods for the Llama-2-7B model, demonstrating how our method is able to 8 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 5.10 5.15 5.20 5.25 Perplexity Quantization Error Output Error FGMP (w o Global Threshold Clipping) FGMP (w o Clipping) FGMP NVFP4 FP8 BF16 Figure 6: Perplexity evaluation on Wikitext-103 with the Llama-2-7B model (lower is better) with different percent- ages of blocks retained in FP8. We provide an ablation of our approach against several baseline policies, and also highlight the perplexity benefits from using a single global thresh- old and from applying sensitivity-weighted clipping to the weights. attain improved perplexity for the same compression rate relative to prior methods. 5.2 Downstream Task Evaluation Tables 2 and 3 show the results of evaluating our methodology on downstream tasks. We compare FGMP against single-precision quantization on MMLU in Table 2 as well as on a selection of tasks from lm-eval-harness (RACE, Hellaswag, PIQA, Winogrande, and BoolQ) in Table 3 (with sensitivity-weighted weight clipping applied for Llama-2-7B 13B and GPT3-1.3B 8.3B). On MMLU, we observe 58-89 less accuracy degradation when we use FGMP with 70 of blocks in FP4, relative to the degradation observed when going from FP8 to FP4 quantization.3 On lm-eval-harness, we observe less than 0.4 average accuracy degradation for all models when leveraging FGMP with 70 of blocks in FP4 relative to FP8-only quantization. Overall, our results demonstrate that our method helps retain downstream task performance even with the majority of blocks quantized to reduced precision. 5.3 Ablation Studies Figure 6 compares our FGMP approach with several other meth- ods, including minimizing only the unweighted quantization er- ror ( Quantization Error ) and minimizing the quantization error weighted by the average magnitude of the elements in the other tensor in order to minimize the error for the output of that particu- lar layer ( Output Error ). Additionally, we provide a comparison with using a per-layer dynamically-computed threshold in order to determine which blocks for that layer should be in FP4 or FP8 3Note that for GPT-3 1.3B 8.3B, the baseline accuracy on MMLU was close to 25 , which is essentially random since each question is multiple choice with four possible answers. This means that the results on these two models cannot be used to differentiate between configurations, and we therefore excluded these models from this calculation. Table 3: Results on a selection of lm-eval-harness down- stream evaluation tasks for different models in the Llama-2, GPT3, and Nemotron model families. Llama-2-7B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.440 0.570 0.780 0.696 0.795 0.656 FP8 0.435 0.570 0.780 0.680 0.791 0.651 FP4 0.429 0.561 0.768 0.680 0.773 0.642 90 FP4 0.427 0.564 0.777 0.684 0.782 0.647 70 FP4 0.450 0.569 0.777 0.695 0.788 0.656 Llama-2-13B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.448 0.602 0.797 0.723 0.822 0.678 FP8 0.450 0.603 0.792 0.721 0.824 0.678 FP4 0.440 0.595 0.785 0.715 0.809 0.669 90 FP4 0.446 0.596 0.787 0.724 0.817 0.674 70 FP4 0.445 0.600 0.792 0.725 0.815 0.675 GPT3-1.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.363 0.439 0.742 0.591 0.648 0.557 FP8 0.365 0.439 0.737 0.580 0.642 0.552 FP4 0.359 0.429 0.731 0.577 0.642 0.548 90 FP4 0.362 0.433 0.727 0.590 0.644 0.551 70 FP4 0.360 0.435 0.739 0.594 0.637 0.553 GPT3-8.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.415 0.546 0.780 0.680 0.689 0.622 FP8 0.416 0.546 0.774 0.672 0.691 0.620 FP4 0.415 0.533 0.774 0.665 0.683 0.614 90 FP4 0.413 0.538 0.770 0.665 0.688 0.615 70 FP4 0.415 0.541 0.773 0.667 0.690 0.617 GPT3-22B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.434 0.579 0.789 0.707 0.746 0.651 FP8 0.434 0.577 0.790 0.702 0.751 0.651 FP4 0.441 0.568 0.788 0.699 0.745 0.648 90 FP4 0.441 0.573 0.784 0.707 0.740 0.649 70 FP4 0.435 0.575 0.792 0.700 0.740 0.648 Nemotron-4-15B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.471 0.620 0.812 0.756 0.787 0.689 FP8 0.482 0.625 0.813 0.757 0.736 0.683 FP4 0.457 0.615 0.791 0.743 0.689 0.659 90 FP4 0.478 0.618 0.807 0.753 0.720 0.675 70 FP4 0.471 0.623 0.809 0.770 0.751 0.684 ( FGMP (w o Global Threshold Clipping) ), and with applying the global threshold but not applying our sensitivity-weighted clip- ping method ( FGMP (w o Clipping) ). These results demonstrate the superiority of our sensitivity-weighted precision assignment policy, which factors in the impact of each parameter on the fi- nal model output when assigning blocks to different precisions. These results also highlight the benefits of our global threshold- ing approach for maintaining high accuracy with the majority of blocks in low precision, as well as the perplexity benefits of our sensitivity-weighted weight clipping method. Table 1 also provides additional evaluation for our sensitivity-weighted weight clipping approach, demonstrating the perplexity benefits in the weight-only quantization regime. We also provide analysis for the portion of blocks retained in high precision across different layers. Figure 7 shows the percentage of sensitive blocks that are retained in FP8 when using 90 FP4 blocks 9 Layer Number 0 20 40 60 80 100 Blocks in FP8 Activations QKV_proj O_proj FC1 FC2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Layer Number 0 10 20 30 40 50 Blocks in FP8 Weights QKV_proj O_proj FC1 FC2 Figure 7: Percentage of blocks retained in FP8 when using FGMP with sensitivity-weighted clipping targeting 90 FP4 10 FP8 for the Llama-2-7B model. We show the percentage of blocks in FP8 for all 32 QKV projection ( QKV_proj ), Output projection ( O_proj ), Fully Connected 1 ( FC1 ), and Fully Connected 2 ( FC2 ) layers. Figure 8: Memory savings for the Llama-2-7B model weights when using FGMP (shown for configurations with 70 and 90 of blocks in FP4). and 10 FP8 blocks (profiled using a sample of sequence length 4096 from the Wikitext-103 test set). This profiling demonstrates how our policy is able to adapt to differing sensitivities at different layers by allocating a larger or smaller portion of FP8 blocks to those layers (for example, retaining a greater portion of QKV projection layer activation blocks and Output projection layer weight blocks in FP8 at the early layers). Additionally, to better understand the quantization efficiency of our method, we profiled the runtime for collecting the weight and activation Fisher information matrices. For the Llama-2-7B model, we found that computing the Fisher information matrices (for 512 samples of sequence length 512) took less than 3 minutes on a single A100 GPU. Note that this is a one-time cost for calibrating the model and can be performed ahead of inference time. This demonstrates that our calibration procedure is lightweight and allows for quickly quantizing new models. 0 25 50 75 100 Weight FP8 0 25 50 75 100 Activations FP8 FP4 FP4 8 FP8 4 FP8 20 22 24 26 28 Datapath Energy (fJ op) Figure 9: Energy efficiency of the FGMP datapath processing different proportions of weights and activations in FP8. The four labeled points show the energy efficiency when only a single dot-product unit is active over the entire test. 5.4 Hardware Evaluation 5.4.1 Memory Savings. Figure 8 highlights the memory savings from leveraging our FGMP method for the Llama-2-7B model. We are able to attain 30 and 39 memory savings using our FGMP approach with 70 and 90 of blocks in FP4, respectively. We also provide a breakdown of memory consumption, showing both the overhead of microscaling for the FP4 format as well as the overhead of the FGMP metadata (the per-block bit to distinguish FP4 and FP8 blocks). 5.4.2 Energy Analysis. Figure 9 shows the energy efficiency of our FGMP datapaths for different percentages of FP8 blocks in both 10 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 Normalized Energy Consumption 5.075 5.100 5.125 5.150 5.175 5.200 5.225 5.250 5.275 Perplexity (Wikitext-103) FGMP NVFP4 FP8 1 Error FGMP (70 NVFP4) Figure 10: Perplexity on Wikitext-103 versus normalized en- ergy consumption using FGMP with sensitivity-weighted clipping with different percentages of blocks retained in FP8, using the Llama-2-7B model. weights and activations. When evaluated only with stimulus re- stricted to a single data format (the labeled boxes in the figure), the NVFP4 datapath consumes 33 less energy relative to the FP8 datapath, while the FP4 8 and FP8 4 (Weight Activation) datapaths consume 16 and 17 less energy than the FP8 baseline, respec- tively. The additional overheads of muxing between the different dot-product units at fine granularity imposes a small tax" to en- able FGMP, such that mostly FP8" data costs slightly more energy than 100 FP8 data. However, this overhead is small compared to the energy savings of performing lower-precision arithmetic when most blocks are quantized to FP4. We also measured the energy consumption of the mixed-precision activation quantization unit performing on-the-fly activation quan- tization under random stimulus. The energy consumption of per- forming quantization on a single block is 25.7 pJ. However, this operation need only be performed after reduction, so it is amor- tized over the dot product dimension of the input tensors, which is at least 4096 for the layers of the Llama-2-7B network. Accordingly, this energy cost is just 0.20 fJ op, which is less than 1 of the en- ergy cost of the dot products themselves and is therefore negligible when considering system energy consumption. Figure 10 shows the perplexity on Wikitext103 when using our approach under different energy consumption budgets. FP4 and FP8 baseline measurements are included for reference. This data shows how our implementation is able to attain high accuracy with low energy consumption. In particular, with less than 1 accuracy degradation relative to the FP8 baseline, we are able to attain 14 energy savings. These results highlight the efficiency gains from leveraging fine-grained mixed-precision quantization in order to enable accurate low-precision LLM inference. 5.4.3 Area Analysis. Table 4 shows the area consumption of our custom design. The area overhead is 3.5 from a standalone FP8 dat- apath, or a 2.2 overhead from a datapath supporting only coarse- grained mixed precision in FP8 and FP4. As noted in Section 4, we chose a hardware design point that spends area to maximize energy efficiency; area overhead could be reduced by sharing datapath Table 4: Area breakdown for the FGMP datapath and post- processing unit (post-synthesis area in a 5nm process). Data- path areas are reported for 16 lanes, and all values assume a block size of 16. Note that the PPU area can be amortized across multiple PEs (since it is only invoked when writ- ing out activations, which is performed infrequently). Note that FP8 NVFP4 refers to the datapath which supports FP8 weights and NVFP4 activations. Configuration Area (ğ‘¢ğ‘š2) FP8 Datapath 2995 NVFP4 Datapath 1811 FP8 NVFP4 Datapath 2669 NVFP4 FP8 Datapath 2630 FGMP Datapath 10356 FGMP PPU 8848 hardware. Furthermore, it is important to note that datapath area is often a small portion of the area of a DNN accelerator. For example, the datapath is only 11 of the area of a processing element in [27]. The processing elements are an even smaller portion of full system area, which tends to be memory-dominated. Additionally, the PPU has an 85 area overhead relative to the FGMP datapath with 16 lanes. However, the PPU overhead can be amortized by increasing the number of lanes, or by sharing a PPU across multiple PEs. Given ğ‘ƒPEs with ğ¿vector lanes per PE and ğ‘ˆPPUs, and assuming block size of 16, the time for the datapaths to process an (ğ‘€by ğ¾) (ğ¾by ğ‘) matrix (assuming a balanced pipeline) would be ğ‘€ ğ¿ ğ¾ 16 ğ‘ ğ‘ƒcycles, whereas the time for PPU processing would be ğ‘€ 16 ğ‘ ğ‘ˆcycles. For a typical Llama-2-7B matrix multiplication with 4K context length (4096 by 4096 4096 by 4096), a single PPU would be able to support up to 256 16-lane PEs without stalling; the area overhead of the PPU is therefore minimal when the cost is amortized across several PEs. 6 Conclusion In this paper, we propose a post-training methodology for fine- grained mixed-precision quantization with both weights and ac- tivations. This approach allows us to retain the performance of the base model with only a small percentage of blocks retained in higher precision. This is achieved by leveraging sensitivity infor- mation with respect to the final model output to determine which blocks should be preferentially retained in higher precision in order to preserve model accuracy. Additionally, we develop a sensitivity- weighted clipping approach for weights that significantly improves accuracy and allows for a greater portion of blocks to be quantized to low precision with minimal accuracy loss. In order to leverage the efficiency benefits of performing multiplications in reduced preci- sion, we propose custom hardware support for FGMP quantization at the dot product level. Our hardware implementation includes datapath support for performing dot products between two FP4 blocks, two FP8 blocks, or one FP4 and one FP8 block. Our hardware implementation also includes a mixed-precision activation quanti- zation unit that assigns activation blocks to low and high precision on-the-fly. Our approach facilitates FGMP quantization, attaining the benefits of low-precision quantization while preserving the accuracy of inference with the high-precision values. 11 References [1] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. 2023. Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259 (2023). [2] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. 2024. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems 37 (2024), 100213 100240. [3] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020). [4] Wanyun Cui and Qianle Wang. 2024. Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models. arXiv preprint arXiv:2404.02837 (2024). [5] Steve Dai, Rangha Venkatesan, Mark Ren, Brian Zimmer, William Dally, and Brucek Khailany. 2021. Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference. Proceedings of Machine Learning and Systems 3 (2021), 873 884. [6] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 (2022). [7] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. arXiv preprint arXiv:2306.03078 (2023). [8] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. Advances in neural information processing systems 33 (2020), 18518 18529. [9] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2019. Hawq: Hessian aware quantization of neural networks with mixed- precision. In Proceedings of the IEEE CVF International Conference on Computer Vision. 293 302. [10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 (2022). [11] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2023. Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization. In Proceedings of the 50th Annual International Symposium on Computer Architecture. 1 15. [12] Wei Huang, Haotong Qin, Yangdong Liu, Yawei Li, Xianglong Liu, Luca Benini, Michele Magno, and Xiaojuan Qi. 2024. SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models. arXiv preprint arXiv:2405.14917 (2024). [13] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, An- drew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2704 2713. [14] Ben Keller, Rangharajan Venkatesan, Steve Dai, Stephen G Tell, Brian Zimmer, William J Dally, C Thomas Gray, and Brucek Khailany. 2022. A 17 95.6 TOPS W deep learning inference accelerator with per-vector scaled 4-bit quantization for transformers in 5nm. In 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits). IEEE, 16 17. [15] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. SqueezeLLM: Dense-and-Sparse Quantization. arXiv preprint arXiv:2306.07629 (2023). [16] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. Proceedings of Machine Learning and Systems 6 (2024), 87 100. [17] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint arXiv:2306.00978 (2023). [18] Fangxin Liu, Ning Yang, Haomin Li, Zongwu Wang, Zhuoran Song, Songwen Pei, and Li Jiang. 2024. SPARK: Scalable and Precision-Aware Acceleration of Neural Networks via Efficient Encoding. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 1029 1042. [19] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. 2024. Spinquant: Llm quantization with learned rotations. arXiv preprint arXiv:2405.16406 (2024). [20] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. 2021. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295 (2021). [21] NVIDIA Corporation. 2021. BlockScaling: NVFP4 Datatype. NVIDIA Corpora- tion. BlockScaling.html NVIDIA cuDNN Frontend Documentation, Version 1.10.0. Accessed: 2025-04-05.. [22] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhun- wala, Ayush Dattagupta, et al. 2024. Nemotron-4 15B Technical Report. arXiv preprint arXiv:2402.16819 (2024). [23] Akshat Ramachandran, Souvik Kundu, and Tushar Krishna. 2024. MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quanti- zation. arXiv preprint arXiv:2411.05282 (2024). [24] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Kho- damoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. 2023. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537 (2023). [25] Charbel Sakr, Yongjune Kim, and Naresh Shanbhag. 2017. Analytical Guarantees on Numerical Precision of Deep Neural Networks. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 3007 3016. [26] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2023. Omniquant: Omni- directionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137 (2023). [27] Yakun Sophia Shao, Jason Cemons, Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, et al. 2021. Simba: scaling deep-learning inference with chiplet- based architecture. Commun. ACM 64, 6 (2021), 107 116. [28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas- mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos- ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [29] Rangharajan Venkatesan, Yakun Sophia Shao, Miaorong Wang, Jason Clemons, Steve Dai, Matthew Fojtik, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, et al. 2019. Magnet: A modular accelerator generator for neural networks. In 2019 IEEE ACM International Conference on Computer-Aided Design (ICCAD). IEEE, 1 8. [30] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. 2019. Haq: Hardware- aware automated quantization with mixed precision. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition. 8612 8620. [31] Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. 2018. Mixed precision quantization of convnets via differentiable neural architecture search. arXiv preprint arXiv:1812.00090 (2018). [32] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning. PMLR, 38087 38099. [33] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020. Gobo: Quantizing attention-based nlp models for low latency and energy ef- ficient inference. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 811 824. [34] Ben Zandonati, Adrian Alan Pol, Maurizio Pierini, Olya Sirkin, and Tal Kopetz. 2022. Fit: A metric for model sensitivity. arXiv preprint arXiv:2210.08502 (2022). [35] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. 2024. Atom: Low- bit quantization for efficient and accurate llm serving. Proceedings of Machine Learning and Systems 6 (2024), 196 209. 12\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2504.14152v1 [cs.AR] 19 Apr 2025 FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization for Hardware-Accelerated LLM Inference Coleman Hooper1,2 , Charbel Sakr1, Ben Keller1, Rangharajan Venkatesan1, Kurt Keutzer2, Sophia Shao2, Brucek Khailany1 1NVIDIA, 2University of California, Berkeley Abstract Quantization is a powerful tool to improve large language model (LLM) inference efficiency by utilizing more energy-efficient low- precision datapaths and reducing memory footprint. However, ac- curately quantizing LLM weights and activations to low preci- sion is challenging without degrading model accuracy. We pro- pose fine-grained mixed precision (FGMP) quantization, a post- training mixed-precision quantization methodology that maintains accuracy while quantizing the majority of weights and activations to reduced precision. We co-design hardware which exploits the mixed-precision data representation to achieve greater energy effi- ciency and which quantizes activations to mixed precision on the fly. Our work makes the following contributions: 1) We develop a policy that uses the perturbation in each value, weighted by the Fisher information, to select which weight and activation blocks to keep in higher precision. This approach preserves accuracy by identifying which weight and activation blocks need to be retained in higher precision to minimize the perturbation in the model loss. 2) We also propose a sensitivity-weighted clipping approach for fine-grained quantization which helps retain accuracy for blocks that are quantized to low precision. 3) We then propose hardware augmentations to leverage the efficiency benefits of FGMP quan- tization. Our hardware implementation encompasses i) datapath support for FGMP at block granularity, and ii) a mixed-precision activation quantization unit to assign activation blocks to high or low precision on the fly with minimal runtime and energy overhead.\n\n--- Segment 2 ---\n3) We then propose hardware augmentations to leverage the efficiency benefits of FGMP quan- tization. Our hardware implementation encompasses i) datapath support for FGMP at block granularity, and ii) a mixed-precision activation quantization unit to assign activation blocks to high or low precision on the fly with minimal runtime and energy overhead. Our design, prototyped using NVFP4 (an FP4 format with microscal- ing) as the low-precision datatype and FP8 as the high-precision datatype, facilitates efficient FGMP quantization, attaining 1 perplexity degradation on Wikitext-103 for the Llama-2-7B model relative to an all-FP8 baseline design while consuming 14 less energy during inference and requiring 30 less weight memory. 1 Introduction Large language models (LLMs) are scaling rapidly in both model size and computational requirements, which makes it challenging to efficiently perform inference using these networks. One approach for reducing both computational and memory requirements for LLM inference is quantization of model weights and activations to low-precision number formats. However, aggressive quantization of all weights and activations is challenging due to the presence of data particularly sensitive to perturbations, such as large-magnitude outliers in weights and activations. Quantization-aware training (QAT) may be employed to mitigate these perturbations [13, 20]; however, the costs of re-training and unavailability of training Work done during internship at NVIDIA Figure 1: Perplexity degradation (less is better) versus com- pression rate (higher is better), evaluated using Llama-2-7B on Wikitext-103 with 4-bit weight-activation quantization. Compression rate is computed as 16 divided by the aver- age bit width for weights and activations (assuming 4K con- text length). We group prior work into algorithm-only ap- proaches that use integer quantization ( Algo. ), prior work on microscaling quantization ( ğœ‡scale ), and prior methods for mixed-precision quantization ( MP ).\n\n--- Segment 3 ---\nWe group prior work into algorithm-only ap- proaches that use integer quantization ( Algo. ), prior work on microscaling quantization ( ğœ‡scale ), and prior methods for mixed-precision quantization ( MP ). Our method, FGMP, (shown here with 70 , 80 , and 90 of blocks in FP4) at- tains reduced perplexity degradation relative to existing post- training quantization methods.1 data with recent LLMs can make QAT difficult and motivates post- training quantization (PTQ) approaches [10, 16]. In this paper, we explore techniques to improve PTQ inference accuracy. Using low precision (e.g. 4-bit) quantization for weights and activations for LLMs is desirable since it allows the use of energy- efficient low-precision compute datapaths (in addition to mem- ory savings). However, accurate post-training quantization with weights and activations in 4-bit precision is challenging; prior works still have substantial accuracy degradation when quantizing to low precision for both weights and activations [2, 19, 26, 32]. The rea- son that these methods struggle to retain accuracy in aggressive regimes like 4-bit quantization is due to challenges with numerical outliers which skew the quantization range [6, 7, 15], as well as disproportionately sensitive values [15]. To address this challenge, previous works have applied mixed-precision quantization, where a mixture of high-precision and low-precision quantization is used to preserve accuracy by retaining particularly sensitive or hard to 1Note that 6-bit configurations are reported for [26, 32] since the corresponding W4A4 configurations have greater than 1 PPL loss, and [26, 35] quantize the KV cache activations in addition to the linear layers. 1 Dot Product Dimension Activation Weight FP4 FP8 Block Size BS Large impact on model output Small impact on model output 1 FP8 FP4 BS 1 a) FGMP Visualization b) Sample block precision assignments with our policy Figure 2: a) Diagram outlining our approach for FGMP quantization at block granularity. We quantize both weights and activations to mixed precision, preserving sensitive blocks in higher precision (FP8) and quantizing the rest to low precision (NVFP4).\n\n--- Segment 4 ---\n1 Dot Product Dimension Activation Weight FP4 FP8 Block Size BS Large impact on model output Small impact on model output 1 FP8 FP4 BS 1 a) FGMP Visualization b) Sample block precision assignments with our policy Figure 2: a) Diagram outlining our approach for FGMP quantization at block granularity. We quantize both weights and activations to mixed precision, preserving sensitive blocks in higher precision (FP8) and quantizing the rest to low precision (NVFP4). b) Visualization of a slice of the activation and weight precision assignments for Layer 7 Fully Connected 1 ( FC1 ) in the Llama-2-7B model using our policy with 10 of blocks in FP8 and with our sensitivity-weighted clipping method applied. The NVFP4 and FP8 blocks are interleaved in an unstructured manner. quantize parts of the network in higher precision [1, 35]. However, existing methods typically perform coarse-grained mixed precision (e.g., at the granularity of rows of a matrix [1, 35]). As oulined in Figure 2, the distribution of sensitive and hard-to-quantize values is unstructured in both LLM weights and activations; existing coarse- grained mixed-precision quantization methods cannot fully adapt to these unstructured distributions. Our work addresses the challenges with existing mixed-precision quantization methods by leveraging fine-grained mixed-precision (FGMP) quantization, in which we identify small, contiguous re- gions of hard-to-quantize weights and activations to be retained in a higher-precision data format while the majority of weights and activations are quantized to a lower-precision format. We pursue mixed-precision quantization at much finer block granularity. We define a block as a small ( 100 elements) 1-dimensional vector along the dot product dimension of each tensor (as in [12]). We perform FGMP quantization by independently assigning each block to either a higher-precision or lower-precision data format (see Figure 2). This provides improved accuracy since it allows for more flexible precision assignment, adapting to different distributions of sensitive values (both inter-layer and intra-layer).\n\n--- Segment 5 ---\nWe perform FGMP quantization by independently assigning each block to either a higher-precision or lower-precision data format (see Figure 2). This provides improved accuracy since it allows for more flexible precision assignment, adapting to different distributions of sensitive values (both inter-layer and intra-layer). However, although FGMP is a promising approach for maintaining model accuracy with the majority of values in reduced precision, it is challenging to leverage the potential efficiency benefits due to the difficulty of performing computation using different precisions for adjacent blocks. There are also distinct challenges with applying FGMP quantization to activations, as we need to be able to efficiently identify hard-to- quantize activation blocks online during inference to quantize the output activations of each layer. Even if we leveraged low-precision datapaths for the majority of computations, software implementa- tions for dynamically selecting precisions when quantizing blocks of activations may be impractical. Our work aims to address these challenges through hardware- software co-design in order to enable fine-grained mixed-precision quantization while supporting low-precision computation for the majority of blocks. We pursue FGMP quantization where the major- ity of LLM weights and activations are quantized to low precision (e.g. FP4), with only a small percentage of values kept in higher precision (e.g. FP8). As outlined in Figure 2, we quantize the weights and activations at block granularity. We use microscaling, which defines a fine-grained scaling factor as metadata for each block of data, to reduce quantization error for the blocks that are quantized to low precision. Microscaling has been shown to enable more ac- curate low-precision quantization [5, 24] with minimal hardware overhead. Specifically, we leverage NVFP4 [21] as our low-precision datatype in order to accurately quantize each block to reduced preci- sion. Additionally, we propose hardware augmentations to support FGMP at block granularity while performing the majority of compu- tation using low-precision datapaths. Figure 1 highlights how our methodology achieves reduced perplexity degradation for the same compression ratio relative to existing work on weight activation quantization.\n\n--- Segment 6 ---\nAdditionally, we propose hardware augmentations to support FGMP at block granularity while performing the majority of compu- tation using low-precision datapaths. Figure 1 highlights how our methodology achieves reduced perplexity degradation for the same compression ratio relative to existing work on weight activation quantization. As highlighted in Section 5.4, utilizing low-precision datapaths also provides distinct energy savings, with NVFP4 dat- apaths with microscaling consuming 33 less energy than FP8. Our prototype demonstrates the efficiency benefits of our FGMP methodology. Our work makes the following contributions: (1) We develop a policy to determine which blocks of weights and activations need to be retained in higher precision. We leverage sensitivity information with respect to the final model output to determine which blocks can be quantized to reduced precision without degrading the accuracy of the network. Importantly, we ensure that this saliency policy is computationally efficient and can be computed on-the-fly for activations, thereby allowing for fast mixed-precision activation quantization. (2) We also design a fine-grained weight clipping strategy to improve the representation capacity for the blocks that are retained in lower precision. We tune the scale factor 2 for each low-precision block to minimize the impact of quantizing the block on the model s output. (3) We co-design the hardware to accelerate FGMP quantiza- tion: i) We adapt a Vector Multiply-Accumulate (VMAC) datapath to allow for performing mixed-precision dot prod- ucts at 1D block granularity. Our datapath design supports FGMP quantization for both weights and activations. ii) We also design a mixed-precision activation quantization unit to assign activation blocks to low or high precision on the fly based on runtime statistics, thereby allowing for dynam- ically identifying and preserving regions of the activations that are challenging to quantize in higher precision. By leveraging our FGMP quantization method with NVFP4 and FP8 as the low and high precision number formats, we are able to attain the efficiency benefits of low-precision quantization for LLM inference while maintaining accuracy. For the Llama-2-7B model, we attain 14 energy savings in dot product computations during inference with 1 accuracy degradation (measured as perplexity on Wikitext-103).\n\n--- Segment 7 ---\nBy leveraging our FGMP quantization method with NVFP4 and FP8 as the low and high precision number formats, we are able to attain the efficiency benefits of low-precision quantization for LLM inference while maintaining accuracy. For the Llama-2-7B model, we attain 14 energy savings in dot product computations during inference with 1 accuracy degradation (measured as perplexity on Wikitext-103). Figure 1 also demonstrates how our method acheives less perplexity degradation for a target compres- sion ratio relative to prior methods. Our approach also requires 30 less parameter memory than serving the model in FP8, enabling the serving of a larger model with the same memory budget. 2 Related Work This section outlines prior work relevant to FGMP quantization. We first provide an overview of prior work on quantizing both weights and activations in LLMs. We then discuss previous efforts to leverage mixed-precision quantization to improve accuracy with low-precision quantization. Finally, we include a discussion of ex- isting hardware support for mixed-precision quantization. 2.1 LLM Quantization Quantization is a popular model compression method to improve inference efficiency. Most existing quantization strategies have em- ployed either integer or floating-point quantization [10, 17]. More complex non-uniform quantization schemes allow for more flexible quantizaton signpost placement compared to integer and floating- point quantization [15]; however, since quantization signposts are no longer uniformly placed, quantized values must first be dequan- tized and computation must be performed in higher precision. Prior work has also leveraged microscaling in order to enable accurate low-precision quantization [5, 24]. Microscaling formats leverage a fine-grained scaling factor for each block, which better preserves the values in the block by adjusting the represented range. Typical quantization methods for LLMs have performed either weight-only [10, 17] or weight-activation [1, 26] quantization. In the weight-only regime, prior methods have been able to maintain high accuracy while compressing the model to 4-bit quantization; how- ever, these approaches cannot utilize standard low-precision com- pute datapaths, as they still represented activations using higher precision. In the weight-activation regime, it is possible to leverage low-precision compute datapaths to attain higher energy efficiency.\n\n--- Segment 8 ---\nIn the weight-only regime, prior methods have been able to maintain high accuracy while compressing the model to 4-bit quantization; how- ever, these approaches cannot utilize standard low-precision com- pute datapaths, as they still represented activations using higher precision. In the weight-activation regime, it is possible to leverage low-precision compute datapaths to attain higher energy efficiency. However, existing methods have struggled to retain accuracy in this regime due to the challenges of accurately quantizing both weights and activations to low precision without retraining [19, 26, 32]. In this work, we aim to perform post-training weight-activation quantization in order to exploit low-precision compute units, while maintaining accuracy through retaining a small portion of blocks in higher precision. 2.2 Mixed-Precision Quantization Several works have noted the presence of distinct outliers in both weights and activations, which significantly increase quantization difficulty [4, 6, 7, 15]. Previous works have split LLM weight matri- ces into a dense low-precision matrix and an unstructured sparse high-precision matrix, which is used to accurately store numerical outliers and highly sensitive values [4, 7, 15]. Although these ap- proaches achieved high accuracy with low memory consumption for weight quantization, they cannot be easily applied for activa- tion quantization and they lead to efficiency overheads in use cases that are not memory bandwidth-bound. [12] employed fine-grained mixed-precision quantization at the 1D block granularity for LLM weights by leveraging sensitivity information, demonstrating the advantages of exploiting sensitivity information when determining which blocks to retain in higher precision. However, this work was restricted to weight-only quantization, and was therefore unable to leverage standard low-precision compute datapaths. There have also been previous works on coarse-grained struc- tured mixed precision, in which a fixed block of the matrix is kept in higher precision, and the remainder of the matrix is quantized to low precision. Several prior works have assigned different preci- sions to different layers (aiming to preserve more sensitive layers in higher precision) [8, 9, 30, 34]. Other works have performed structured mixed-precision quantization by preserving a subset of input channels in higher precision.\n\n--- Segment 9 ---\nSeveral prior works have assigned different preci- sions to different layers (aiming to preserve more sensitive layers in higher precision) [8, 9, 30, 34]. Other works have performed structured mixed-precision quantization by preserving a subset of input channels in higher precision. [6] preserved large-magnitude activation input channels (and the corresponding input channels in weights) in high precision, while quantizing the remaining values. [1, 35] reordered input channels in order to group large-magnitude activation channels together at the end of the matrix, and then left a portion of the input channels at the end of the matrix in higher precision for both weights and activations. These works were able to leverage low-precision datapaths for part of the computation since they quantize both weights and activations. However, these methods were unable to adapt to the unstructured nature of impor- tant values in weights and activations. In contrast, our fine-grained mixed-precision approach can adapt to unstructured sensitive val- ues in both weights and activations, while still performing the majority of computation using low precision datapaths. There have also been prior works which searched for the opti- mal mixed-precision configuration, either by using reinforcement learning [30] or by formulated the search as a Neural Architecture Search problem [31]. However, existing automatic mixed precision search methods are challenging to apply for LLMs, as these methods require repeated evaluation of the network with different precision configurations for each layer, which can be prohibitively expensive. Additionally, these approaches have previously been applied for layer wise mixed precision. With mixed precision at the 1D block granularity, the search space of potential precision assignments is much greater than the layer wise precision assignment problem. It is therefore infeasible to leverage existing automatic methods to search for the optimal fine-grained mixed precision assignment. 3 2.3 Hardware Support for Mixed-Precision Quantization Several prior works have investigated methods for adding archi- tecture support for representing numerical outliers in order to improve low-precision quantization. GOBO [33] combines outlier- aware quantization with non-uniform quantization for better low- precision representation. However, this approach requires sepa- rately loading outliers and dense values (as the sparse values are stored in a separate sparse matrix), as well as separate datapaths for outlier processing.\n\n--- Segment 10 ---\nGOBO [33] combines outlier- aware quantization with non-uniform quantization for better low- precision representation. However, this approach requires sepa- rately loading outliers and dense values (as the sparse values are stored in a separate sparse matrix), as well as separate datapaths for outlier processing. OLIVE [11] employs an outlier-aware encod- ing, storing outlier values across multiple positions (sacrificing the neighboring dense values). They repurposed one of the 16 repre- sentative 4-bit values to indicate whether an outlier is present in a neighboring value, which amounts to an overhead of 1 bit per 4 elements. This has the benefit of allowing for a structured data stor- age format, but it negatively impacts the representation of nearby values. One concurrent work, MicroScopiQ [23], also retains outlier values in higher precision by pruning out a portion of non-outlier values and repurposing the saved bits to represent the outliers in higher precision. SPARK [18] employs a variable-precision encod- ing scheme where large and small magnitude values are encoded with different precisions, and a bit at the start of each element indicates which precision is stored in that location. This requires per-element metadata (eg. a bit for each low-precision element) to mark whether that element is in low or high precision. Both OLIVE and SPARK [11, 18] employ decoder modules at the boundary of a systolic array in order to decode their compact data format into a format that can be fed into an array of processing elements. While these works perform decoding at the per-element level, we instead implement FGMP at block granularity. This dramatically reduces metadata overhead compared to the per-element approach by amor- tizing the cost of the metadata across all elements in a block (so for a block size of 16, it requires only a single bit per 16 elements) and only requires performing a comparison with a single bit per block rather than performing per-element comparisons. FGMP greatly improves the efficiency of the hardware implementation by amor- tizing control overhead and enabling the use of efficient vector multiply-accumulate operations, while still providing sufficiently fine granularity to maintain high accuracy.\n\n--- Segment 11 ---\nThis dramatically reduces metadata overhead compared to the per-element approach by amor- tizing the cost of the metadata across all elements in a block (so for a block size of 16, it requires only a single bit per 16 elements) and only requires performing a comparison with a single bit per block rather than performing per-element comparisons. FGMP greatly improves the efficiency of the hardware implementation by amor- tizing control overhead and enabling the use of efficient vector multiply-accumulate operations, while still providing sufficiently fine granularity to maintain high accuracy. 3 Fine-Grained Mixed-Precision Quantization In this work, we focus on joint weight activation quantization, specifically targeting the linear layers in LLM matrices, where a weight matrix W and activation matrix X are multiplied in order to compute the output Y, i.e. Y W X.2 The model is composed of ğ¿layers, and the weights and activations for layer ğ‘™will be denoted as W(ğ‘™) and X(ğ‘™), respectively. The model is trained by optimizing in order to reduce its loss L, which is a function of both the weights and the activations corresponding to a given input data, as well as the ground truth labels corresponding to this data: L({X(ğ‘™)}ğ¿ ğ‘™ 1, {W(ğ‘™)}ğ¿ ğ‘™ 1) (1) By minimizing the increase in the loss due to perturbation from quantizing the weights and activations, we can thereby minimize 2Throughout this work, we use capital boldface letters to refer to matrices and lower- case boldface letters to refer to 1D blocks. the perturbation in the model output, which is desirable for main- taining accuracy. When we quantize an activation or weight value ğ‘£to precision ğ‘using quantization function ğ‘„ğ‘, we will express the quantization error at precision ğ‘, Î”ğ‘ğ‘£, as follows: Î”ğ‘ğ‘£ ğ‘„ğ‘(ğ‘£) ğ‘£ (2) In order to perform FGMP quantization, we first develop a policy to determine which weights and activations need to be retained in higher precision to preserve model accuracy.\n\n--- Segment 12 ---\nthe perturbation in the model output, which is desirable for main- taining accuracy. When we quantize an activation or weight value ğ‘£to precision ğ‘using quantization function ğ‘„ğ‘, we will express the quantization error at precision ğ‘, Î”ğ‘ğ‘£, as follows: Î”ğ‘ğ‘£ ğ‘„ğ‘(ğ‘£) ğ‘£ (2) In order to perform FGMP quantization, we first develop a policy to determine which weights and activations need to be retained in higher precision to preserve model accuracy. While our experimen- tal results in this work leverage FP8 and NVFP4 as the higher and lower precision number formats, respectively, our precision assign- ment policy developed in this section is generalizable to any choice of high-precision and low-precision formats. We calibrate offline for a target sensitivity level above which the block is preserved in higher precision, and we apply a single threshold across different layers to allocate more high-precision blocks to more sensitive lay- ers. Finally, we develop a fine-grained weight clipping approach to improve the quantized representation of low-precision blocks with microscaling. Taken together, our methodology facilitates ac- curate fine-grained mixed-precision quantization with only a small portion of blocks retained in higher precision. 3.1 Precision Assignment Policy Our precision assignment policy determines which blocks to keep in higher precision in order to avoid degrading the accuracy of the model. We can avoid this accuracy degradation by minimizing the degradation in the model s loss L when the weights and activations are perturbed. We first consider how the loss is affected by pertur- bation in a single weight or activation value ğ‘£at a given layer ğ‘™. The loss can be expressed using Taylor expansion as follows (neglecting higher-order terms), where the gradient ğ‘”is the derivative of the loss with respect to the corresponding value, L ğ‘£: L(ğ‘„(ğ‘£)) L(ğ‘£) ğ‘” Î”ğ‘£ . . .\n\n--- Segment 13 ---\n. . (3) To minimize the expected degradation in model loss, we therefore study the minimization of the following expression: Eğ‘£ L(ğ‘£) L(ğ‘„(ğ‘£)) 2 Eğ‘£ ğ‘”2 (Î”ğ‘£)2 (4) Eğ‘£ ğ‘”2 Eğ‘£ (Î”ğ‘£)2 (5) Equation 5 holds under the assumption that ğ‘”and Î”ğ‘£are inde- pendent, meaning that the impact of perturbation in each weight or activation value on the model loss is independent of the pertur- bations in other values [25]. The expression Eğ‘£ ğ‘”2 corresponds to the diagonal Fisher information matrix. Empirically, this can be evaluated by averaging the squares of the gradients over a set of calibration data. For weight matrices, we calculate this by averag- ing the squared gradients over a sample dataset ğ·(in our case, 512 samples of sequence length 512 from the Wikitext-103 training set). Since activation values are dynamic during inference, we do not have access to their gradients at runtime. We instead use the aver- age of the square of the gradients for each input channel (computed offline on the calibration set) to estimate per-channel sensitivity for activations. For the remainder of the section, we will assume that 4 gradients have been calibrated, and consequently, expectations are implied. Considering a one-dimensional block v (either an activation or weight block in layer ğ‘™) containing ğ‘elements, we can express the impact ğ¼L of quantizing the block v on the model s loss as the sum of the sensitivity-weighted quantization error for each element ğ‘£ğ‘– in v (ignoring layer indices): ğ¼L(v) ğ‘ ğ‘– 1 ğ‘”2 ğ‘–(Î”ğ‘£ğ‘–)2 (6) The above metric is used as a ranking mechanism to identify the most sensitive blocks, which are then preferentially kept in higher precision.\n\n--- Segment 14 ---\nFor the remainder of the section, we will assume that 4 gradients have been calibrated, and consequently, expectations are implied. Considering a one-dimensional block v (either an activation or weight block in layer ğ‘™) containing ğ‘elements, we can express the impact ğ¼L of quantizing the block v on the model s loss as the sum of the sensitivity-weighted quantization error for each element ğ‘£ğ‘– in v (ignoring layer indices): ğ¼L(v) ğ‘ ğ‘– 1 ğ‘”2 ğ‘–(Î”ğ‘£ğ‘–)2 (6) The above metric is used as a ranking mechanism to identify the most sensitive blocks, which are then preferentially kept in higher precision. The increase in error for value ğ‘£when it is quantized in low precision ğ‘ğ‘™rather than high precision ğ‘â„can be expressed as follows: Î”ğ‘â„ ğ‘ğ‘™ğ‘£ Î”ğ‘ğ‘™ğ‘£ Î”ğ‘â„ğ‘£ (7) The impact on the final model output when a block v of size ğ‘ containing values ğ‘£ğ‘–is quantized in low precision rather than high precision is therefore: ğ¼ L(v) ğ‘ ğ‘– 1 ğ‘”2 ğ‘–(Î”ğ‘â„ ğ‘ğ‘™ğ‘£ğ‘–)2 (8) We use the impact score ğ¼ L(v) to identify the weight and activa- tion blocks for which low-precision quantization has the biggest impact on the model s output, and we preferentially preserve these blocks in higher precision. 3.2 Setting the Threshold for Precision Assignment In order to determine which blocks can be quantized to reduced precision, we need to determine the threshold for the importance score ğ¼ L(v), above which the block must be retained in higher precision. One option would be to compute the threshold for a target ratio of high-precision to low-precision blocks dynamically for each layer.\n\n--- Segment 15 ---\n3.2 Setting the Threshold for Precision Assignment In order to determine which blocks can be quantized to reduced precision, we need to determine the threshold for the importance score ğ¼ L(v), above which the block must be retained in higher precision. One option would be to compute the threshold for a target ratio of high-precision to low-precision blocks dynamically for each layer. However, sweeping over the activation tensor for layer ğ‘™online during inference to compute ğ¼ L(v) for each block and then computing the target threshold for that layer would lead to unacceptable inference overhead (since we would need to write out each output block from the previous operation in both precisions before we knew which precision was going to be used). It is there- fore preferable to calibrate offline to determine the threshold for activations for a target mixed-precision ratio. Namely, for a target mixed-precision ratio ğ‘…, we can assign ğ‘… of blocks to be in higher precision by setting the threshold ğ‘‡ğ‘™ğ‘œğ‘ğ‘ğ‘™to be the ğ‘…-th percentile of the impact score ğ¼ L(v), computed over all ğ½ğ‘™blocks v in a single weight or activation tensor W(ğ‘™) or X(ğ‘™): ğ‘‡ğ‘™ğ‘œğ‘ğ‘ğ‘™ ğ‘ƒğ‘…({ğ¼ L(v(ğ‘—))}ğ½ğ‘™ ğ‘— 1) (9) where ğ‘ƒğ‘…() denotes the ğ‘…-th percentile. Additionally, different layers in the network exhibit differing sensitivities in terms of their impacts on the model output. In order to allow our policy to adapt to varying sensitivities at different layers, instead of calibrating for a separate threshold for each layer, we instead set a single global threshold across the entire model (one for weights and one for activations). Since our sensitivity-weighted policy for precision assignment estimates the impact that quantizing the block will have on the final model output, it is already normalized across different layers (meaning that we can use the same threshold across different layers in the network even if they have different average magnitudes).\n\n--- Segment 16 ---\nIn order to allow our policy to adapt to varying sensitivities at different layers, instead of calibrating for a separate threshold for each layer, we instead set a single global threshold across the entire model (one for weights and one for activations). Since our sensitivity-weighted policy for precision assignment estimates the impact that quantizing the block will have on the final model output, it is already normalized across different layers (meaning that we can use the same threshold across different layers in the network even if they have different average magnitudes). This allows for preserving a different proportion of sensitive blocks for particular layers which have a greater or lesser impact on the final model loss. For a target mixed-precision ratio ğ‘…, we therefore set threshold ğ‘‡ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™to be the ğ‘…-th percentile of the impact score ğ¼ L(v), computed over all blocks across all weight or activation tensors W(ğ‘™) or X(ğ‘™) (across all layers): ğ‘‡ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ ğ‘ƒğ‘…({{ğ¼ L(v(ğ‘—))}ğ½ğ‘™ ğ‘— 1}ğ¿ ğ‘™ 1) (10) Note that this threshold is computed separately for weights and for activations since the input activation size will vary at inference time. Figure 7 in Section 5.3 visualizes how our global threshold pol- icy allows for retaining a greater portion of blocks in high precision for particular layers. 3.3 Fine-Grained Sensitivity-Weighted Clipping Improving accuracy with low-precision quantization can provide noticeable benefits even in the context of FGMP quantization, as it can allow us to retain a greater portion of blocks in low pre- cision without compromising accuracy. One method to improve low-precision quantization is clipping, where we reduce the rep- resentable range of numbers in order to give us better resolution within this range. In the context of low-precision quantization with microscaling, performing clipping by adjusting the per-block scale factors can provide significant benefits as it allows us to perform fine-grained adjustments to best represent the values in a given block.\n\n--- Segment 17 ---\nOne method to improve low-precision quantization is clipping, where we reduce the rep- resentable range of numbers in order to give us better resolution within this range. In the context of low-precision quantization with microscaling, performing clipping by adjusting the per-block scale factors can provide significant benefits as it allows us to perform fine-grained adjustments to best represent the values in a given block. To improve the representation of blocks that are quantized to reduced precision, we therefore pursue clipping with the per-block scale factors. In particular, we develop a sensitivity- weighted approach for fine-grained clipping to accurately quantize low-precision blocks without perturbing the model loss. For a given block v with ğ‘elements in a weight layer W(ğ‘™), the objective is to minimize the squared quantization error in v weighted by the square of the gradients ğ‘”. We therefore want to choose the scaling factor ğ‘ which minimizes this objective: min ğ‘  ğ‘ ğ‘– 1 ğ‘”2 ğ‘–( ğ‘  Î”ğ‘£ğ‘–)2 (11) where ğ‘  Î”ğ‘£ğ‘–is the quantization error for element ğ‘£ğ‘–when the per-block scaling factor is ğ‘ . With the low-precision datatype used in this work, the per-block scale factors are restricted to FP8 values, and as such there are only a limited number of possible values for ğ‘ . We can therefore perform a brute-force search over possible values for ğ‘ in order to identify the scale factors which minimize the sensitivity-weighted quantization error. Note that we only apply sensitivity-weighted clipping offline for the weight matrices, and we use dynamic-max clipping online for activations, as it is not possible to efficiently compute the optimal scale factors for activations online during inference. 5 3.4 Additional Baseline Policies In order to justify our precision assignment policy, we compare with multiple potential baseline methods for selecting which blocks to retain in higher precision (the comparison is provided in Figure 6 in Section 5.3). Our first baseline policy (referred to as Quantization Error ) determines which blocks to retain in higher precision using only the quantization error for each block within a single layer ğ‘™.\n\n--- Segment 18 ---\n5 3.4 Additional Baseline Policies In order to justify our precision assignment policy, we compare with multiple potential baseline methods for selecting which blocks to retain in higher precision (the comparison is provided in Figure 6 in Section 5.3). Our first baseline policy (referred to as Quantization Error ) determines which blocks to retain in higher precision using only the quantization error for each block within a single layer ğ‘™. The error when a 1D block v of size ğ‘is quantized in low precision ğ‘ğ‘™rather than high precision ğ‘â„can be expressed as: ğ¼ ğ‘„ğ¸(v) ğ‘ ğ‘– 1 (Î”ğ‘â„ ğ‘ğ‘™ğ‘£ğ‘–)2 (12) where Î”ğ‘â„ ğ‘ğ‘™gives the increase in error from quantizing ele- ment ğ‘£ğ‘–to low precision rather than high precision. As a second baseline, we also include an Output Error policy, which minimizes the error at the layer output for layer ğ‘™in order to determine which blocks to retain in higher precision for that layer. For this policy, we first calibrate for the average squared input channel magnitude across the other tensor Q. For quantizing a block v in weight matrix W(ğ‘™), we compute the average squared magnitudes for the corresponding input channels in X(ğ‘™), and for quantizing a block v in activation matrix X(ğ‘™) we compute the average squared magnitudes for the corresponding input channels in W(ğ‘™). We compute the average squared magnitudes for the input channels in X(ğ‘™) statically using calibration data from the Wikitext- 103 training set.\n\n--- Segment 19 ---\nFor quantizing a block v in weight matrix W(ğ‘™), we compute the average squared magnitudes for the corresponding input channels in X(ğ‘™), and for quantizing a block v in activation matrix X(ğ‘™) we compute the average squared magnitudes for the corresponding input channels in W(ğ‘™). We compute the average squared magnitudes for the input channels in X(ğ‘™) statically using calibration data from the Wikitext- 103 training set. Using this policy, the error when a 1D block v of size ğ‘is quantized in low precision ğ‘ğ‘™rather than high precision ğ‘â„can be expressed as: ğ¼ ğ‘‚ğ¸(v) ğ‘ ğ‘– 1 avg(ğ‘„2 ğ‘–)(Î”ğ‘â„ ğ‘ğ‘™ğ‘£ğ‘–)2 (13) where avg(ğ‘„2 ğ‘–) is the average squared input channel magnitude in the other tensor. For both baseline policies, in order to set the mixed-precision ratio ğ‘…, we set the threshold ğ‘‡to be the ğ‘…-th percentile of ğ¼ ğ‘„ğ¸(v) ğ¼ ğ‘‚ğ¸(v) computed dynamically over all blocks v in a single weight or activation layer W(ğ‘™) or X(ğ‘™). 4 Hardware Support for Fine-Grained Mixed-Precision Quantization Hardware support is required to use low-precision datapaths for the majority of computations when we leverage FGMP quantization. Hardware support for quantizing activations to mixed-precision on- line during inference is also needed to realize the energy efficiency benefits of FGMP quantization, as these benefits would otherwise be negated by the overheads of performing mixed-precision quan- tization in software. Our proposed design (outlined in Figure 3a) consists of an array of processing elements (PEs), each of which contains a Vector Multiply-Accumulate (VMAC) datapath consist- ing of multiple parallel lanes that each compute one VMAC per cycle [29].\n\n--- Segment 20 ---\nHardware support for quantizing activations to mixed-precision on- line during inference is also needed to realize the energy efficiency benefits of FGMP quantization, as these benefits would otherwise be negated by the overheads of performing mixed-precision quan- tization in software. Our proposed design (outlined in Figure 3a) consists of an array of processing elements (PEs), each of which contains a Vector Multiply-Accumulate (VMAC) datapath consist- ing of multiple parallel lanes that each compute one VMAC per cycle [29]. The hardware augmentations required for supporting FGMP quantization for both weights and activations includes i) dat- apath modifications to support FGMP which are outlined in Figure 3b, and ii) a post-processing unit (PPU) that efficiently quantizes activations to mixed precision during inference which is outlined in Figure 4. We use NVFP4 as our low precision datatype [21], which uses microscaling to improve the quantization accuracy in low pre- cision, as in [5, 14, 24]. NVFP4 uses a block size of 16 and performs quantization using FP4 (E2M1) representation for the values with FP8 (E4M3) microscaling factors. Throughout the remainder of this work, when we refer to a block being quantized in FP4, this block is quantized using the NVFP4 datatype. We set the microscaling block size and the FGMP block size (ğµğ‘†) equal to the VMAC vector length to improve hardware efficiency. We use a single metadata bit alongside each block to indicate whether the block contains high or low precision values. 4.1 Mixed-Precision Datapath Figure 3b outlines the multi-lane mixed-precision VMAC datapath which is required for performing efficient mixed-precision computa- tion. The datapath contains FP4, FP8, and mixed FP4 8 dot-product units. In a single cycle, the active dot-product unit in each lane (determined by inspecting the metadata bit associated with each weight and activation block) computes ğµğ‘†-wide dot products across ğ¿parallel lanes and adds this FP32 partial sum to the previously accumulated partial sum, thereby performing 2 ğµğ‘† ğ¿operations per cycle.\n\n--- Segment 21 ---\nThe datapath contains FP4, FP8, and mixed FP4 8 dot-product units. In a single cycle, the active dot-product unit in each lane (determined by inspecting the metadata bit associated with each weight and activation block) computes ğµğ‘†-wide dot products across ğ¿parallel lanes and adds this FP32 partial sum to the previously accumulated partial sum, thereby performing 2 ğµğ‘† ğ¿operations per cycle. The dot-product units with at least one FP4 input also perform the scale factor multiplication before adding the partial sum to the accumulated value. The remaining three dot-product units in each lane are inactive and are clock-gated or data-gated appropriately to minimize switching power. When computing a matrix multiplication between a weight tile A and an activation tile B, we assume a weight-stationary dataflow in which A is held sta- tionary and blocks in B are streamed in one per cycle and broadcast across all lanes. Note that we maintain the same math throughput per cycle regardless of the precision of the weights and inputs being consumed, which improves energy efficiency by maximizing tem- poral reuse of A and spatial reuse of B while avoiding complicated control logic. Our proposed design contains independent FP4 FP8 and FP8 FP4 dot-product units in each lane so that the ğ´input can be consistently held constant during the innermost temporal loop, improving energy efficiency at the cost of area overhead. Note that datapath components could be shared to reduce area; however, this would come at the cost of increased energy consumption. Using shared hardware for all four dot-product combinations would obvi- ate any energy gains of FGMP. Sharing hardware between two or three dot-product formats could still realize some (reduced) energy savings. 4.2 Post-Processing Activation Quantization Unit Figure 4 provides a diagram of our mixed-precision activation quan- tization unit, which is a post-processing unit that quantizes output activation blocks before writing them out to memory. In a typi- cal single-precision datapath, values are locally accumulated in high precision (e.g., FP32) before being sent to a PPU for scaling and quantization to the (single) activation data format.\n\n--- Segment 22 ---\n4.2 Post-Processing Activation Quantization Unit Figure 4 provides a diagram of our mixed-precision activation quan- tization unit, which is a post-processing unit that quantizes output activation blocks before writing them out to memory. In a typi- cal single-precision datapath, values are locally accumulated in high precision (e.g., FP32) before being sent to a PPU for scaling and quantization to the (single) activation data format. In contrast, our mixed-precision PPU must dynamically determine whether to quantize the accumulated values to FP8 or FP4 by using the average per-input channel Fisher information to compute the sensitivity- weighted sum of the quantization error for each output block. The 6 Weight Buffer Select DPath Weight Collector L Vector Lanes FP4 FP8 FP4 8 FP8 4 Weight Buffer Select DPath Weight Collector FP4 FP8 FP4 8 FP8 4 Vector MAC Datapaths A FP8? W FP8? A FP8? W FP8? Activation Collector x BS BS BS x x Sa Sw Activation Buffer Output Collector Output Collector Fully accumulated sums to PPU Partial sum back to datapath x x x BS Partial sum in Partial sum out Select DPath Select DPath b) Mixed-Precision, Multi- Lane Vector-MAC Datapath PE PE PE PE Vector Units (Contain Post-Processing Units) Processing Element Array Memory System a) Accelerator Architecture VU PPU VU PPU Figure 3: a) High-level accelerator architecture, consisting of a PE array (each of which contains a VMAC-based datapath), as well as one or more vector units which contain our post-processing activation quantization unit (outlined in detail in Figure 4). b) Datapath support for FGMP quantization with four dot-product units per lane to perform FGMP VMAC operations. The portion of the figure highlighted in gray is synthesized for hardware measurements. Quantize (FP4) Quantize (FP8) VMax Mixed-Precision Activation Quantization Unit (Q(Y)-Y)2 (Q(Y)-Y)2 BS BS FP4 8? FP4 8?\n\n--- Segment 23 ---\nQuantize (FP4) Quantize (FP8) VMax Mixed-Precision Activation Quantization Unit (Q(Y)-Y)2 (Q(Y)-Y)2 BS BS FP4 8? FP4 8? - Quantized Output Activation Block Output Activation Block Mixed-Precision Activation Quantization Post-Processing Unit Per-Channel Sensitivity Information Get ğ-scale Threshold Figure 4: A diagram of the post-processing unit for quantizing activations to mixed-precision online during inference. activation block is then written out as either FP4 or FP8, depend- ing on whether the sum is greater than the configured threshold. The threshold is calibrated offline for a given model and is fixed throughout inference. Note that our approach for leveraging per- input channel metadata for weighting the parameters in a block in order to determine the block s importance is agnostic to how this metadata is computed. For example, this approach could also be used to run the baseline Output Error configuration described in Section 3.4, since it also uses statically calibrated per-input channel metadata to weight the values in each block when determining importance. 4.3 Experimental Approach We prototype our hardware implementation using SystemC and the Siemens Catapult High-Level Synthesis tool to generate RTL. RTL simulations are performed using Synopsys VCS with synthetic data sampled from an appropriately scaled Gaussian random dis- tribution, where the proportion of weight and activation blocks in FP4 and FP8 are independently adjusted to generate representa- tive input stimulus. We leverage the Synopsys Fusion Compiler for synthesis targeting a 1 GHz clock period in a 5 nm process, and we replay the test stimulus on the gate netlist to gather activity factors for power measurements using Synopsys PrimePower (TT 0.67 V corner). All measurements use a prototype design with ğ¿ 16 and ğµğ‘† 16, as in [5]. In order to collect energy estimates with a realistic workload, we need to leverage relevant estimates for the proportion of FP4 and FP8 blocks in different layers and for varying global threshold settings. As outlined in Figure 7 in Section 5.3, different layers have very different distributions of FP4 versus FP8 blocks, but measuring power for all possible layer configurations is runtime-intractable.\n\n--- Segment 24 ---\nIn order to collect energy estimates with a realistic workload, we need to leverage relevant estimates for the proportion of FP4 and FP8 blocks in different layers and for varying global threshold settings. As outlined in Figure 7 in Section 5.3, different layers have very different distributions of FP4 versus FP8 blocks, but measuring power for all possible layer configurations is runtime-intractable. In order to collect realistic data, we first profiled the portion of FP8 and FP4 blocks across different layers for both weights and activations. We collected this data for different global ratios of FP8 to FP4 blocks (from 90 to 10 ). We then treated each configuration as a set of features, normalized each feature, and performed K-means clustering to compute 100 representative configurations across all layers. After running power analysis on small kernels derived from each of these 100 configurations, we scaled up the results to match the shapes of each of the corresponding layers in order to estimate total energy consumption.\n\n--- Segment 25 ---\nWe then treated each configuration as a set of features, normalized each feature, and performed K-means clustering to compute 100 representative configurations across all layers. After running power analysis on small kernels derived from each of these 100 configurations, we scaled up the results to match the shapes of each of the corresponding layers in order to estimate total energy consumption. 7 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 5.10 5.15 5.20 5.25 Perplexity Llama-2-7B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 4.62 4.64 4.66 4.68 4.70 4.72 4.74 4.76 Perplexity Llama-2-13B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 5.95 6.00 6.05 6.10 6.15 6.20 Perplexity Nemotron-4-15B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 10.0 10.1 10.2 10.3 10.4 Perplexity GPT3-1.3B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 7.40 7.45 7.50 7.55 7.60 Perplexity GPT3-8.3B Tradeoff Curve 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 6.54 6.56 6.58 6.60 6.62 6.64 6.66 6.68 6.70 Perplexity GPT3-22B Tradeoff Curve No Clip Clip NVFP4 1 Error FP8 BF16 Figure 5: Perplexity evaluation on Wikitext-103 for different models in the Llama-2, GPT3, and Nemotron model families. We report FGMP quantization results with and without our sensitivity-weighted weight clipping approach. Note that for GPT3-22B and Nemotron-4-15B, we did not apply sensitivity-weighted clipping as it did not yield perplexity improvements, and for Nemotron-4-15B, we exclude the BF16 baseline as it exhibited worse perplexity than FP8.\n\n--- Segment 26 ---\nWe report FGMP quantization results with and without our sensitivity-weighted weight clipping approach. Note that for GPT3-22B and Nemotron-4-15B, we did not apply sensitivity-weighted clipping as it did not yield perplexity improvements, and for Nemotron-4-15B, we exclude the BF16 baseline as it exhibited worse perplexity than FP8. Table 1: Perplexity on Wikitext-103 when applying sensitivity-weighted weight clipping SW-Clip") for Llama- 2-7B and Llama-2-13B. We report results for weight-only FP4 quantization with microscaling, both with and without sensitivity-weighted clipping. All configurations use BF16 for activations. Weight Precision Llama-2-7B Llama-2-13B BF16 5.06 4.61 FP4 5.18 4.69 FP4 (w SW-Clip) 5.13 4.67 Table 2: Average 5-shot evaluation results on MMLU for dif- ferent models in the Llama-2, GPT3, and Nemotron model families. Model Llama-2 GPT3 Nemotron-4 7B 13B 1.3B 8.3B 22B 15B BF16 0.458 0.551 0.247 0.252 0.387 0.643 FP8 0.462 0.550 0.249 0.248 0.383 0.647 FP4 0.430 0.533 0.258 0.258 0.371 0.634 90 FP4 0.448 0.543 0.252 0.253 0.382 0.637 70 FP4 0.451 0.547 0.256 0.251 0.382 0.642 5 Results We report a comprehensive accuracy evaluation of our quantiza- tion method and simulated energy measurements of our prototyped implementation in order to confirm the efficiency benefits of our approach. Sections 5.1 and 5.2 provide results for our methodology using perplexity as measured on Wikitext-103 and using down- stream task evaluation, respectively. Section 5.3 provides ablations for our precision assignment policy, as well as analysis for the distri- bution of sensitive blocks across different layers and the runtime for computing Fisher information for quantization.\n\n--- Segment 27 ---\nSections 5.1 and 5.2 provide results for our methodology using perplexity as measured on Wikitext-103 and using down- stream task evaluation, respectively. Section 5.3 provides ablations for our precision assignment policy, as well as analysis for the distri- bution of sensitive blocks across different layers and the runtime for computing Fisher information for quantization. Section 5.4 presents detailed evaluation of our hardware implementation. 5.1 Perplexity Evaluation We evaluate our proposed mixed-precision block assignment pol- icy using models in the Llama-2, GPT3, and Nemotron families [3, 22, 28]. We used FP8 without microscaling and NVFP4 as the higher and lower precision data formats. Figure 5 shows the per- plexity on Wikitext-103 versus the percentage of blocks that are kept in FP8, including results with and without our sensitivity- weighted clipping approach. Our results show that with only a small percentage of blocks retained in FP8, we can attain significant accuracy improvements relative to quantizing all blocks to FP4. These results also demonstrate the perplexity improvements from incorporating our sensitivity-weighted weight clipping approach (particularly when a higher proportion of blocks are quantized in FP4). Figure 1 also provides comparisons against previous methods for the Llama-2-7B model, demonstrating how our method is able to 8 0 20 40 60 80 100 Percentage of Blocks in 4-bit Precision ( ) 5.10 5.15 5.20 5.25 Perplexity Quantization Error Output Error FGMP (w o Global Threshold Clipping) FGMP (w o Clipping) FGMP NVFP4 FP8 BF16 Figure 6: Perplexity evaluation on Wikitext-103 with the Llama-2-7B model (lower is better) with different percent- ages of blocks retained in FP8. We provide an ablation of our approach against several baseline policies, and also highlight the perplexity benefits from using a single global thresh- old and from applying sensitivity-weighted clipping to the weights. attain improved perplexity for the same compression rate relative to prior methods. 5.2 Downstream Task Evaluation Tables 2 and 3 show the results of evaluating our methodology on downstream tasks.\n\n--- Segment 28 ---\nattain improved perplexity for the same compression rate relative to prior methods. 5.2 Downstream Task Evaluation Tables 2 and 3 show the results of evaluating our methodology on downstream tasks. We compare FGMP against single-precision quantization on MMLU in Table 2 as well as on a selection of tasks from lm-eval-harness (RACE, Hellaswag, PIQA, Winogrande, and BoolQ) in Table 3 (with sensitivity-weighted weight clipping applied for Llama-2-7B 13B and GPT3-1.3B 8.3B). On MMLU, we observe 58-89 less accuracy degradation when we use FGMP with 70 of blocks in FP4, relative to the degradation observed when going from FP8 to FP4 quantization.3 On lm-eval-harness, we observe less than 0.4 average accuracy degradation for all models when leveraging FGMP with 70 of blocks in FP4 relative to FP8-only quantization. Overall, our results demonstrate that our method helps retain downstream task performance even with the majority of blocks quantized to reduced precision. 5.3 Ablation Studies Figure 6 compares our FGMP approach with several other meth- ods, including minimizing only the unweighted quantization er- ror ( Quantization Error ) and minimizing the quantization error weighted by the average magnitude of the elements in the other tensor in order to minimize the error for the output of that particu- lar layer ( Output Error ). Additionally, we provide a comparison with using a per-layer dynamically-computed threshold in order to determine which blocks for that layer should be in FP4 or FP8 3Note that for GPT-3 1.3B 8.3B, the baseline accuracy on MMLU was close to 25 , which is essentially random since each question is multiple choice with four possible answers. This means that the results on these two models cannot be used to differentiate between configurations, and we therefore excluded these models from this calculation. Table 3: Results on a selection of lm-eval-harness down- stream evaluation tasks for different models in the Llama-2, GPT3, and Nemotron model families.\n\n--- Segment 29 ---\nThis means that the results on these two models cannot be used to differentiate between configurations, and we therefore excluded these models from this calculation. Table 3: Results on a selection of lm-eval-harness down- stream evaluation tasks for different models in the Llama-2, GPT3, and Nemotron model families. Llama-2-7B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.440 0.570 0.780 0.696 0.795 0.656 FP8 0.435 0.570 0.780 0.680 0.791 0.651 FP4 0.429 0.561 0.768 0.680 0.773 0.642 90 FP4 0.427 0.564 0.777 0.684 0.782 0.647 70 FP4 0.450 0.569 0.777 0.695 0.788 0.656 Llama-2-13B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.448 0.602 0.797 0.723 0.822 0.678 FP8 0.450 0.603 0.792 0.721 0.824 0.678 FP4 0.440 0.595 0.785 0.715 0.809 0.669 90 FP4 0.446 0.596 0.787 0.724 0.817 0.674 70 FP4 0.445 0.600 0.792 0.725 0.815 0.675 GPT3-1.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.363 0.439 0.742 0.591 0.648 0.557 FP8 0.365 0.439 0.737 0.580 0.642 0.552 FP4 0.359 0.429 0.731 0.577 0.642 0.548 90 FP4 0.362 0.433 0.727 0.590 0.644 0.551 70 FP4 0.360 0.435 0.739 0.594 0.637 0.553 GPT3-8.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.415 0.546 0.780 0.680 0.689 0.622 FP8 0.416 0.546 0.774 0.672 0.691 0.620 FP4 0.415 0.533 0.774 0.665 0.683 0.614 90 FP4 0.413 0.538 0.770 0.665 0.688 0.615 70 FP4 0.415 0.541 0.773 0.667 0.690 0.617 GPT3-22B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.434 0.579 0.789 0.707 0.746 0.651 FP8 0.434 0.577 0.790 0.702 0.751 0.651 FP4 0.441 0.568 0.788 0.699 0.745 0.648 90 FP4 0.441 0.573 0.784 0.707 0.740 0.649 70 FP4 0.435 0.575 0.792 0.700 0.740 0.648 Nemotron-4-15B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.471 0.620 0.812 0.756 0.787 0.689 FP8 0.482 0.625 0.813 0.757 0.736 0.683 FP4 0.457 0.615 0.791 0.743 0.689 0.659 90 FP4 0.478 0.618 0.807 0.753 0.720 0.675 70 FP4 0.471 0.623 0.809 0.770 0.751 0.684 ( FGMP (w o Global Threshold Clipping) ), and with applying the global threshold but not applying our sensitivity-weighted clip- ping method ( FGMP (w o Clipping) ).\n\n--- Segment 30 ---\nTable 3: Results on a selection of lm-eval-harness down- stream evaluation tasks for different models in the Llama-2, GPT3, and Nemotron model families. Llama-2-7B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.440 0.570 0.780 0.696 0.795 0.656 FP8 0.435 0.570 0.780 0.680 0.791 0.651 FP4 0.429 0.561 0.768 0.680 0.773 0.642 90 FP4 0.427 0.564 0.777 0.684 0.782 0.647 70 FP4 0.450 0.569 0.777 0.695 0.788 0.656 Llama-2-13B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.448 0.602 0.797 0.723 0.822 0.678 FP8 0.450 0.603 0.792 0.721 0.824 0.678 FP4 0.440 0.595 0.785 0.715 0.809 0.669 90 FP4 0.446 0.596 0.787 0.724 0.817 0.674 70 FP4 0.445 0.600 0.792 0.725 0.815 0.675 GPT3-1.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.363 0.439 0.742 0.591 0.648 0.557 FP8 0.365 0.439 0.737 0.580 0.642 0.552 FP4 0.359 0.429 0.731 0.577 0.642 0.548 90 FP4 0.362 0.433 0.727 0.590 0.644 0.551 70 FP4 0.360 0.435 0.739 0.594 0.637 0.553 GPT3-8.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.415 0.546 0.780 0.680 0.689 0.622 FP8 0.416 0.546 0.774 0.672 0.691 0.620 FP4 0.415 0.533 0.774 0.665 0.683 0.614 90 FP4 0.413 0.538 0.770 0.665 0.688 0.615 70 FP4 0.415 0.541 0.773 0.667 0.690 0.617 GPT3-22B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.434 0.579 0.789 0.707 0.746 0.651 FP8 0.434 0.577 0.790 0.702 0.751 0.651 FP4 0.441 0.568 0.788 0.699 0.745 0.648 90 FP4 0.441 0.573 0.784 0.707 0.740 0.649 70 FP4 0.435 0.575 0.792 0.700 0.740 0.648 Nemotron-4-15B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.471 0.620 0.812 0.756 0.787 0.689 FP8 0.482 0.625 0.813 0.757 0.736 0.683 FP4 0.457 0.615 0.791 0.743 0.689 0.659 90 FP4 0.478 0.618 0.807 0.753 0.720 0.675 70 FP4 0.471 0.623 0.809 0.770 0.751 0.684 ( FGMP (w o Global Threshold Clipping) ), and with applying the global threshold but not applying our sensitivity-weighted clip- ping method ( FGMP (w o Clipping) ). These results demonstrate the superiority of our sensitivity-weighted precision assignment policy, which factors in the impact of each parameter on the fi- nal model output when assigning blocks to different precisions.\n\n--- Segment 31 ---\nLlama-2-7B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.440 0.570 0.780 0.696 0.795 0.656 FP8 0.435 0.570 0.780 0.680 0.791 0.651 FP4 0.429 0.561 0.768 0.680 0.773 0.642 90 FP4 0.427 0.564 0.777 0.684 0.782 0.647 70 FP4 0.450 0.569 0.777 0.695 0.788 0.656 Llama-2-13B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.448 0.602 0.797 0.723 0.822 0.678 FP8 0.450 0.603 0.792 0.721 0.824 0.678 FP4 0.440 0.595 0.785 0.715 0.809 0.669 90 FP4 0.446 0.596 0.787 0.724 0.817 0.674 70 FP4 0.445 0.600 0.792 0.725 0.815 0.675 GPT3-1.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.363 0.439 0.742 0.591 0.648 0.557 FP8 0.365 0.439 0.737 0.580 0.642 0.552 FP4 0.359 0.429 0.731 0.577 0.642 0.548 90 FP4 0.362 0.433 0.727 0.590 0.644 0.551 70 FP4 0.360 0.435 0.739 0.594 0.637 0.553 GPT3-8.3B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.415 0.546 0.780 0.680 0.689 0.622 FP8 0.416 0.546 0.774 0.672 0.691 0.620 FP4 0.415 0.533 0.774 0.665 0.683 0.614 90 FP4 0.413 0.538 0.770 0.665 0.688 0.615 70 FP4 0.415 0.541 0.773 0.667 0.690 0.617 GPT3-22B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.434 0.579 0.789 0.707 0.746 0.651 FP8 0.434 0.577 0.790 0.702 0.751 0.651 FP4 0.441 0.568 0.788 0.699 0.745 0.648 90 FP4 0.441 0.573 0.784 0.707 0.740 0.649 70 FP4 0.435 0.575 0.792 0.700 0.740 0.648 Nemotron-4-15B Precision RACE Hellaswag PIQA Winogrande BoolQ Average BF16 0.471 0.620 0.812 0.756 0.787 0.689 FP8 0.482 0.625 0.813 0.757 0.736 0.683 FP4 0.457 0.615 0.791 0.743 0.689 0.659 90 FP4 0.478 0.618 0.807 0.753 0.720 0.675 70 FP4 0.471 0.623 0.809 0.770 0.751 0.684 ( FGMP (w o Global Threshold Clipping) ), and with applying the global threshold but not applying our sensitivity-weighted clip- ping method ( FGMP (w o Clipping) ). These results demonstrate the superiority of our sensitivity-weighted precision assignment policy, which factors in the impact of each parameter on the fi- nal model output when assigning blocks to different precisions. These results also highlight the benefits of our global threshold- ing approach for maintaining high accuracy with the majority of blocks in low precision, as well as the perplexity benefits of our sensitivity-weighted weight clipping method.\n\n--- Segment 32 ---\nThese results demonstrate the superiority of our sensitivity-weighted precision assignment policy, which factors in the impact of each parameter on the fi- nal model output when assigning blocks to different precisions. These results also highlight the benefits of our global threshold- ing approach for maintaining high accuracy with the majority of blocks in low precision, as well as the perplexity benefits of our sensitivity-weighted weight clipping method. Table 1 also provides additional evaluation for our sensitivity-weighted weight clipping approach, demonstrating the perplexity benefits in the weight-only quantization regime. We also provide analysis for the portion of blocks retained in high precision across different layers. Figure 7 shows the percentage of sensitive blocks that are retained in FP8 when using 90 FP4 blocks 9 Layer Number 0 20 40 60 80 100 Blocks in FP8 Activations QKV_proj O_proj FC1 FC2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Layer Number 0 10 20 30 40 50 Blocks in FP8 Weights QKV_proj O_proj FC1 FC2 Figure 7: Percentage of blocks retained in FP8 when using FGMP with sensitivity-weighted clipping targeting 90 FP4 10 FP8 for the Llama-2-7B model. We show the percentage of blocks in FP8 for all 32 QKV projection ( QKV_proj ), Output projection ( O_proj ), Fully Connected 1 ( FC1 ), and Fully Connected 2 ( FC2 ) layers. Figure 8: Memory savings for the Llama-2-7B model weights when using FGMP (shown for configurations with 70 and 90 of blocks in FP4). and 10 FP8 blocks (profiled using a sample of sequence length 4096 from the Wikitext-103 test set). This profiling demonstrates how our policy is able to adapt to differing sensitivities at different layers by allocating a larger or smaller portion of FP8 blocks to those layers (for example, retaining a greater portion of QKV projection layer activation blocks and Output projection layer weight blocks in FP8 at the early layers). Additionally, to better understand the quantization efficiency of our method, we profiled the runtime for collecting the weight and activation Fisher information matrices.\n\n--- Segment 33 ---\nThis profiling demonstrates how our policy is able to adapt to differing sensitivities at different layers by allocating a larger or smaller portion of FP8 blocks to those layers (for example, retaining a greater portion of QKV projection layer activation blocks and Output projection layer weight blocks in FP8 at the early layers). Additionally, to better understand the quantization efficiency of our method, we profiled the runtime for collecting the weight and activation Fisher information matrices. For the Llama-2-7B model, we found that computing the Fisher information matrices (for 512 samples of sequence length 512) took less than 3 minutes on a single A100 GPU. Note that this is a one-time cost for calibrating the model and can be performed ahead of inference time. This demonstrates that our calibration procedure is lightweight and allows for quickly quantizing new models. 0 25 50 75 100 Weight FP8 0 25 50 75 100 Activations FP8 FP4 FP4 8 FP8 4 FP8 20 22 24 26 28 Datapath Energy (fJ op) Figure 9: Energy efficiency of the FGMP datapath processing different proportions of weights and activations in FP8. The four labeled points show the energy efficiency when only a single dot-product unit is active over the entire test. 5.4 Hardware Evaluation 5.4.1 Memory Savings. Figure 8 highlights the memory savings from leveraging our FGMP method for the Llama-2-7B model. We are able to attain 30 and 39 memory savings using our FGMP approach with 70 and 90 of blocks in FP4, respectively. We also provide a breakdown of memory consumption, showing both the overhead of microscaling for the FP4 format as well as the overhead of the FGMP metadata (the per-block bit to distinguish FP4 and FP8 blocks). 5.4.2 Energy Analysis.\n\n--- Segment 34 ---\nWe also provide a breakdown of memory consumption, showing both the overhead of microscaling for the FP4 format as well as the overhead of the FGMP metadata (the per-block bit to distinguish FP4 and FP8 blocks). 5.4.2 Energy Analysis. Figure 9 shows the energy efficiency of our FGMP datapaths for different percentages of FP8 blocks in both 10 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 Normalized Energy Consumption 5.075 5.100 5.125 5.150 5.175 5.200 5.225 5.250 5.275 Perplexity (Wikitext-103) FGMP NVFP4 FP8 1 Error FGMP (70 NVFP4) Figure 10: Perplexity on Wikitext-103 versus normalized en- ergy consumption using FGMP with sensitivity-weighted clipping with different percentages of blocks retained in FP8, using the Llama-2-7B model. weights and activations. When evaluated only with stimulus re- stricted to a single data format (the labeled boxes in the figure), the NVFP4 datapath consumes 33 less energy relative to the FP8 datapath, while the FP4 8 and FP8 4 (Weight Activation) datapaths consume 16 and 17 less energy than the FP8 baseline, respec- tively. The additional overheads of muxing between the different dot-product units at fine granularity imposes a small tax" to en- able FGMP, such that mostly FP8" data costs slightly more energy than 100 FP8 data. However, this overhead is small compared to the energy savings of performing lower-precision arithmetic when most blocks are quantized to FP4. We also measured the energy consumption of the mixed-precision activation quantization unit performing on-the-fly activation quan- tization under random stimulus. The energy consumption of per- forming quantization on a single block is 25.7 pJ. However, this operation need only be performed after reduction, so it is amor- tized over the dot product dimension of the input tensors, which is at least 4096 for the layers of the Llama-2-7B network.\n\n--- Segment 35 ---\nThe energy consumption of per- forming quantization on a single block is 25.7 pJ. However, this operation need only be performed after reduction, so it is amor- tized over the dot product dimension of the input tensors, which is at least 4096 for the layers of the Llama-2-7B network. Accordingly, this energy cost is just 0.20 fJ op, which is less than 1 of the en- ergy cost of the dot products themselves and is therefore negligible when considering system energy consumption. Figure 10 shows the perplexity on Wikitext103 when using our approach under different energy consumption budgets. FP4 and FP8 baseline measurements are included for reference. This data shows how our implementation is able to attain high accuracy with low energy consumption. In particular, with less than 1 accuracy degradation relative to the FP8 baseline, we are able to attain 14 energy savings. These results highlight the efficiency gains from leveraging fine-grained mixed-precision quantization in order to enable accurate low-precision LLM inference. 5.4.3 Area Analysis. Table 4 shows the area consumption of our custom design. The area overhead is 3.5 from a standalone FP8 dat- apath, or a 2.2 overhead from a datapath supporting only coarse- grained mixed precision in FP8 and FP4. As noted in Section 4, we chose a hardware design point that spends area to maximize energy efficiency; area overhead could be reduced by sharing datapath Table 4: Area breakdown for the FGMP datapath and post- processing unit (post-synthesis area in a 5nm process). Data- path areas are reported for 16 lanes, and all values assume a block size of 16. Note that the PPU area can be amortized across multiple PEs (since it is only invoked when writ- ing out activations, which is performed infrequently). Note that FP8 NVFP4 refers to the datapath which supports FP8 weights and NVFP4 activations. Configuration Area (ğ‘¢ğ‘š2) FP8 Datapath 2995 NVFP4 Datapath 1811 FP8 NVFP4 Datapath 2669 NVFP4 FP8 Datapath 2630 FGMP Datapath 10356 FGMP PPU 8848 hardware.\n\n--- Segment 36 ---\nNote that FP8 NVFP4 refers to the datapath which supports FP8 weights and NVFP4 activations. Configuration Area (ğ‘¢ğ‘š2) FP8 Datapath 2995 NVFP4 Datapath 1811 FP8 NVFP4 Datapath 2669 NVFP4 FP8 Datapath 2630 FGMP Datapath 10356 FGMP PPU 8848 hardware. Furthermore, it is important to note that datapath area is often a small portion of the area of a DNN accelerator. For example, the datapath is only 11 of the area of a processing element in [27]. The processing elements are an even smaller portion of full system area, which tends to be memory-dominated. Additionally, the PPU has an 85 area overhead relative to the FGMP datapath with 16 lanes. However, the PPU overhead can be amortized by increasing the number of lanes, or by sharing a PPU across multiple PEs. Given ğ‘ƒPEs with ğ¿vector lanes per PE and ğ‘ˆPPUs, and assuming block size of 16, the time for the datapaths to process an (ğ‘€by ğ¾) (ğ¾by ğ‘) matrix (assuming a balanced pipeline) would be ğ‘€ ğ¿ ğ¾ 16 ğ‘ ğ‘ƒcycles, whereas the time for PPU processing would be ğ‘€ 16 ğ‘ ğ‘ˆcycles. For a typical Llama-2-7B matrix multiplication with 4K context length (4096 by 4096 4096 by 4096), a single PPU would be able to support up to 256 16-lane PEs without stalling; the area overhead of the PPU is therefore minimal when the cost is amortized across several PEs. 6 Conclusion In this paper, we propose a post-training methodology for fine- grained mixed-precision quantization with both weights and ac- tivations. This approach allows us to retain the performance of the base model with only a small percentage of blocks retained in higher precision. This is achieved by leveraging sensitivity infor- mation with respect to the final model output to determine which blocks should be preferentially retained in higher precision in order to preserve model accuracy.\n\n--- Segment 37 ---\nThis approach allows us to retain the performance of the base model with only a small percentage of blocks retained in higher precision. This is achieved by leveraging sensitivity infor- mation with respect to the final model output to determine which blocks should be preferentially retained in higher precision in order to preserve model accuracy. Additionally, we develop a sensitivity- weighted clipping approach for weights that significantly improves accuracy and allows for a greater portion of blocks to be quantized to low precision with minimal accuracy loss. In order to leverage the efficiency benefits of performing multiplications in reduced preci- sion, we propose custom hardware support for FGMP quantization at the dot product level. Our hardware implementation includes datapath support for performing dot products between two FP4 blocks, two FP8 blocks, or one FP4 and one FP8 block. Our hardware implementation also includes a mixed-precision activation quanti- zation unit that assigns activation blocks to low and high precision on-the-fly. Our approach facilitates FGMP quantization, attaining the benefits of low-precision quantization while preserving the accuracy of inference with the high-precision values. 11 References [1] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. 2023. Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259 (2023). [2] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. 2024. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems 37 (2024), 100213 100240. [3] Tom B Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020). [4] Wanyun Cui and Qianle Wang. 2024. Cherry on Top: Parameter Heterogeneity and Quantization in Large Language Models. arXiv preprint arXiv:2404.02837 (2024).\n\n--- Segment 38 ---\nCherry on Top: Parameter Heterogeneity and Quantization in Large Language Models. arXiv preprint arXiv:2404.02837 (2024). [5] Steve Dai, Rangha Venkatesan, Mark Ren, Brian Zimmer, William Dally, and Brucek Khailany. 2021. Vs-quant: Per-vector scaled quantization for accurate low-precision neural network inference. Proceedings of Machine Learning and Systems 3 (2021), 873 884. [6] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 (2022). [7] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. 2023. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression. arXiv preprint arXiv:2306.03078 (2023). [8] Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Hawq-v2: Hessian aware trace-weighted quantization of neural networks. Advances in neural information processing systems 33 (2020), 18518 18529. [9] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2019. Hawq: Hessian aware quantization of neural networks with mixed- precision. In Proceedings of the IEEE CVF International Conference on Computer Vision. 293 302. [10] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 (2022).\n\n--- Segment 39 ---\nGptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 (2022). [11] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and Yuhao Zhu. 2023. Olive: Accelerating large language models via hardware-friendly outlier-victim pair quantization. In Proceedings of the 50th Annual International Symposium on Computer Architecture. 1 15. [12] Wei Huang, Haotong Qin, Yangdong Liu, Yawei Li, Xianglong Liu, Luca Benini, Michele Magno, and Xiaojuan Qi. 2024. SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models. arXiv preprint arXiv:2405.14917 (2024). [13] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, An- drew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition. 2704 2713. [14] Ben Keller, Rangharajan Venkatesan, Steve Dai, Stephen G Tell, Brian Zimmer, William J Dally, C Thomas Gray, and Brucek Khailany. 2022. A 17 95.6 TOPS W deep learning inference accelerator with per-vector scaled 4-bit quantization for transformers in 5nm. In 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits). IEEE, 16 17. [15] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. SqueezeLLM: Dense-and-Sparse Quantization. arXiv preprint arXiv:2306.07629 (2023).\n\n--- Segment 40 ---\nSqueezeLLM: Dense-and-Sparse Quantization. arXiv preprint arXiv:2306.07629 (2023). [16] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. Proceedings of Machine Learning and Systems 6 (2024), 87 100. [17] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint arXiv:2306.00978 (2023). [18] Fangxin Liu, Ning Yang, Haomin Li, Zongwu Wang, Zhuoran Song, Songwen Pei, and Li Jiang. 2024. SPARK: Scalable and Precision-Aware Acceleration of Neural Networks via Efficient Encoding. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA). IEEE, 1029 1042. [19] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. 2024. Spinquant: Llm quantization with learned rotations. arXiv preprint arXiv:2405.16406 (2024). [20] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. 2021. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295 (2021). [21] NVIDIA Corporation. 2021. BlockScaling: NVFP4 Datatype. NVIDIA Corpora- tion. BlockScaling.html NVIDIA cuDNN Frontend Documentation, Version 1.10.0.\n\n--- Segment 41 ---\nNVIDIA Corpora- tion. BlockScaling.html NVIDIA cuDNN Frontend Documentation, Version 1.10.0. Accessed: 2025-04-05.. [22] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhun- wala, Ayush Dattagupta, et al. 2024. Nemotron-4 15B Technical Report. arXiv preprint arXiv:2402.16819 (2024). [23] Akshat Ramachandran, Souvik Kundu, and Tushar Krishna. 2024. MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quanti- zation. arXiv preprint arXiv:2411.05282 (2024). [24] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Kho- damoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. 2023. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537 (2023). [25] Charbel Sakr, Yongjune Kim, and Naresh Shanbhag. 2017. Analytical Guarantees on Numerical Precision of Deep Neural Networks. In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 70), Doina Precup and Yee Whye Teh (Eds.). PMLR, 3007 3016. [26] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2023. Omniquant: Omni- directionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137 (2023).\n\n--- Segment 42 ---\nOmniquant: Omni- directionally calibrated quantization for large language models. arXiv preprint arXiv:2308.13137 (2023). [27] Yakun Sophia Shao, Jason Cemons, Rangharajan Venkatesan, Brian Zimmer, Matthew Fojtik, Nan Jiang, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, et al. 2021. Simba: scaling deep-learning inference with chiplet- based architecture. Commun. ACM 64, 6 (2021), 107 116. [28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas- mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos- ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [29] Rangharajan Venkatesan, Yakun Sophia Shao, Miaorong Wang, Jason Clemons, Steve Dai, Matthew Fojtik, Ben Keller, Alicia Klinefelter, Nathaniel Pinckney, Priyanka Raina, et al. 2019. Magnet: A modular accelerator generator for neural networks. In 2019 IEEE ACM International Conference on Computer-Aided Design (ICCAD). IEEE, 1 8. [30] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. 2019. Haq: Hardware- aware automated quantization with mixed precision. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition. 8612 8620. [31] Bichen Wu, Yanghan Wang, Peizhao Zhang, Yuandong Tian, Peter Vajda, and Kurt Keutzer. 2018. Mixed precision quantization of convnets via differentiable neural architecture search. arXiv preprint arXiv:1812.00090 (2018). [32] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models.\n\n--- Segment 43 ---\n2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning. PMLR, 38087 38099. [33] Ali Hadi Zadeh, Isak Edo, Omar Mohamed Awad, and Andreas Moshovos. 2020. Gobo: Quantizing attention-based nlp models for low latency and energy ef- ficient inference. In 2020 53rd Annual IEEE ACM International Symposium on Microarchitecture (MICRO). IEEE, 811 824. [34] Ben Zandonati, Adrian Alan Pol, Maurizio Pierini, Olya Sirkin, and Tal Kopetz. 2022. Fit: A metric for model sensitivity. arXiv preprint arXiv:2210.08502 (2022). [35] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. 2024. Atom: Low- bit quantization for efficient and accurate llm serving. Proceedings of Machine Learning and Systems 6 (2024), 196 209. 12\n\n