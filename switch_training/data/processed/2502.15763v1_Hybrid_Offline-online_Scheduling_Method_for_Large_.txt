=== ORIGINAL PDF: 2502.15763v1_Hybrid_Offline-online_Scheduling_Method_for_Large_.pdf ===\n\nRaw text length: 67475 characters\nCleaned text length: 66650 characters\nNumber of segments: 37\n\n=== CLEANED TEXT ===\n\n1 Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization Bowen Pang, Kai Li, Ruifeng She, and Feifan Wang, Member, IEEE Abstract With the development of large language models (LLMs), it has become increasingly important to optimize hardware usage and improve throughput. In this paper, we study the inference optimization of the serving system that deploys LLMs. To optimize system throughput and maximize hardware utilization, we formulate the inference optimization problem as a mixed-integer programming (MIP) model and propose a hybrid offline-online method as solution. The offline method improves large-scale inference systems by introducing a Minimizing Makespan Bin Packing Problem. We further provide a theoretical lower bound computation method. Then, we propose an online sorting and preemptive scheduling method to better utilize hardware. In the online iteration scheduling process, a Lagrangian method is applied to evaluate the cost efficiency of inserting prefill stages versus decode stages at each iteration and dynamically determine when to preempt decoding tasks and insert prefill tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K dataset demonstrate that system utilization improves from 80.2 to 89.1 , and the total inference time decreases from 201.00 to 190.58 seconds. A 100- cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0 on average. Finally, we discuss potential future extensions, including stochastic modeling, reinforcement learning-based schedulers, and dynamic decision-making strategies for system throughput and hardware utilization. Note to Practitioners This work provides optimization tools for enhancing the efficiency of LLM inference systems through advanced scheduling techniques. From the perspective of LLM inference service providers, improved hardware utilization can reduce operational costs by requiring less hardware to maintain the same level of service. From the user s perspective, reduced inference time translates to faster response times and improved service quality. Furthermore, the proposed scheduling techniques are adaptable to various LLM models, hardware platforms, and datasets, making them highly scalable and broadly applicable to real-world LLM inference scenarios. Index Terms Large Language Model, Inference System, On- line Scheduling, Mixed-integer Programming. I. INTRODUCTION Recent advancements in large language models (LLMs), including GPT-4, LLaMA, and Qwen, have significantly transformed the landscape of natural language processing by enabling more sophisticated text generation, comprehension, and interaction capabilities. These models serve as founda- tional technologies in a wide range of applications, such as chatbots, machine translation, and content creation. Despite their transformative potential, the inference process for LLMs B. Pang, K. Li, and R. She are with Noah s Ark Lab, Huawei. F. Wang is with the Department of Industrial Engineering, Tsinghua University, Beijing, 100084, China. E-mail: Computing Infrastructure Serving Configurations System resource model Configurations GPU computing ability Node size Network topology Memory Network bandwidth Layers Head Model type DP TP PP Service level Quant BS Requests scheduling Iteration scheduling basis Inference Service Input requests Offline requests scheduling Prefill queue Decode queue Scheduling strategy Online requests scheduling Online iteration scheduling KV cache Multi-objectives Service level SJF LLM workers Batching Profiler Response tokens Decode iteration Information collecting (optional) Fig. 1: Illustration of LLM inference service system is computationally intensive and resource-demanding, result- ing in high costs and increased latency. These factors pose significant challenges when scaling their deployment across various applications. Effective inference scheduling is crucial for optimizing resource usage, reducing operational costs, and ensuring high-quality service delivery. As illustrated in Fig. 1, the current inference service system comprises three components: system resource, serving con- figuration, and inference service. System resource determines the extent of computational resources available in the system including GPU computing ability, memory, node size, network bandwidth, etc. The serving configuration specifies the desired model deployment method and service level expectations, such as data tensor pipeline parallel settings, quantization, batch size, and scheduling configurations. The inference service processes input requests from users, utilizes hardware capa- bilities, fulfills user configurations, and returns responses to LLM users. The scheduling strategy module is the core of the inference service, managing prefill and decode queues, offline and online request scheduling, and iteration-level hardware management. The offline scheduling method is optional, only for inference tasks where all requests are known in advance. Conversely, the two online scheduling methods, i.e., requests scheduling and iteration scheduling, are more versatile and arXiv:2502.15763v1 [cs.DC] 14 Feb 2025 2 applicable to a wide range of scenarios. The online methods acquire information from the online profiler and dispatch requests and inference tasks to LLM workers. In practice hardware utilization includes up to 20 bubbles , which means the hardware is idle during inference service, and detail is later shown in Section IV. By designing an efficient LLM inference scheduling system to reduce these bubbles, computational resource consumption can be decreased, leading to reduced latency and increased throughput. For maintaining logic flow, detailed technical introductions to prefill and de- code operations are provided in Section III. Current works, such as vLLM [1] and ORCA [2], provide a robust foundation for LLM inference serving systems. The primary contributions of vLLM and ORCA to LLM inference are their enhancements in resource allocation and execution efficiency, which significantly increase throughput and de- crease latency in large-scale language model deployments. These improvements are achieved through advanced memory management and continuous batching techniques, which en- hance model parallelism and effectively leverage hardware resources [1], [2]. However, the state-of-the-art scheduler in vLLM predominantly employs a First-Come-First-Serve (FCFS) and prefill-first policy, prioritizing prefill tasks but not fully leveraging the scheduling system s potential. We identify the causes of low hardware utilization rates. From an offline scheduling perspective, client loads are unbalanced, and request sequencing can be improved. From an online schedul- ing angle, the prefill-first policy lacks parallel processing of multiple requests at prefill stage, and there are no dynamic sequencing adjustments or preemption methods. The optimization of LLM inference presents two major challenges. The first challenge pertains to the offline method, which involves managing a large number of requests from up to 200 parallel clients. This task is particularly time-consuming when using a mixed-integer programming (MIP) scheduling model. The second challenge is the need for much faster decision-making for online methods compared to traditional online scheduling problems. For example, in LLM inference scenarios, online decisions about whether to send prefill or decode requests to LLM workers typically occur every 50 milliseconds. In contrast, in healthcare or production systems, online decisions usually occur every thirty seconds or even several minutes. For online scheduling methods, the higher frequency of decision-making requires algorithms that are efficient and capable of delivering results within extremely short time frames. Accompanied by these challenges, developing a scheduling method for LLM inference could yield substantial benefits. According to NVIDIA Financial Results in 2024 [3], the revenue from GPU retail and cloud computation service has reached 60.9 billion per year. Improving computational re- source efficiency by 5 will earn up to 3.05 billion in revenue annually. Therefore, research in this area is crucial, especially for scheduling researchers. In this paper, we aim at enhance service system through- put and optimize hardware utilization. We define the LLM inference optimization problem using a MIP model as its uncertainty equivalence. Then, we propose a hybrid offline- online method as solution. This work is the first to define this problem from a scheduling perspective. To address the high real-time demands in LLM inference, we propose a hybrid offline-online scheduling method. In numerical exper- iments, we demonstrate that our method can improve system throughput by 5.46 and improve hardware utilization by 11.0 . A 100-cases study shows that our method outperforms baseline method consistently with an improvement of 8.0 on utilization rate. The major contributions of this work include the followings. To the best of our knowledge, we are the first to formulate a mathematical model that describe the scheduling problem in the area of LLM inference. We design a two-stage approach to manage both offline and online scheduling challenges. An Minimizing Makespan Bin Packing model is developed to efficiently solve the offline scheduling problem. Additionally, we introduce a sorting and preemption method to handle the online request scheduling problem, and we develop a Lagrangian-based heuristic technique for solving the online iteration scheduling issue. In the online scheduling module, our method can provide decisions within 5 milliseconds, meet- ing the real-time decision-making requirements of practical application in LLM inference. The remainder of the paper is structured as follows. We review literature on efficient LLM inference in Section II. The problem definition and model formulation are presented in Section III. Then, we illustrate the difficulty of solving this problem and introduce our hybrid offline-online scheduling method to provide a timely solution in Section IV. Numerical studies using real cases and 100 generated cases are presented in Section V. Finally, the conclusion and future directions are provided in Section VI. II. LITERATURE REVIEW This section provides an overview of existing research and prevalent methodologies in inference optimization for LLMs. Firstly, we introduce the general techniques commonly em- ployed in model serving, which can be seamlessly integrated with our scheduling strategies. Subsequently, we elucidate several classical techniques widely adopted in LLM inference systems, which constitute the cornerstone of our framework and methodologies. In the following, we succinctly introduce recent advancements in inference optimization. Finally, we examine scheduling methods within the existing operations research domain. As LLM inference falls within the broader scope of model serving, a variety of general inference optimization techniques can be effectively utilized. Model compression is one of the quintessential optimization strategies for reducing model size, encompassing techniques such as quantization [11], spar- sification [12], [13], and distillation [14]. In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [15] or grouped-query attention [16] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model. Nevertheless, both model compression and the design 3 TABLE I: Literature Review Work Throughput Latency Cache Management Dynamic Decision Uncertainty Request Management Iteration Management Batching Distributed Strategies Orca vLLM Sarathi-Serve [4] Llumnix [5] InfiniGen [6] dLoRA [7] VTC [8] FastServe [9] DistServe [10] MoonCake OURS of compact structures can alter model weights, potentially leading to a decline in accuracy. Instead of optimizing the model size, data parallelism (DP) and model parallelism aim to fully leverage the computational power of the devices. In DP [17], the model weights are replicated across multiple devices, allowing different inference requests to be processed in parallel on different devices. Model parallelism distributes the model weights across several devices to minimize the per- device memory footprint of the model weights. Consequently, each device can operate more efficiently by executing a smaller portion of the task. Several model parallelization methods exist, such as pipeline parallelism (PP) [18], tensor parallelism (TP) [19], sequence parallelism (SP) [20], and context paral- lelism (CP) [21]. Since our scheduling methods are orthogonal to the aforementioned techniques, both model optimization and parallelization strategies can be employed seamlessly in conjunction with our methods to enhance inference efficiency. In addition to general model serving techniques, the opti- mization of LLM inference serving systems primarily involves enhancing the model forward pass. Researchers have improved system efficiency from various perspectives, including kernel fusion, Key-Value (KV) cache management, request manage- ment, iteration management, batching, distributed strategies, etc. Here, we present several classic techniques that are prevalent in LLM inference serving systems. FlashAttention [22] amalgamates the operations of data transfer between hard- ware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding [23], [24] employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model. This technique enables the serving system to output multiple tokens in a single forward pass instead of one. Orca [2] pioneers continuous batching by aggregating different requests at the iteration level. Rather than awaiting the completion of an entire batch before starting the execution of new requests, continuous batching allows new requests to be inserted into the batch while other requests are still in progress. Inspired by memory management strategies in operating systems, vLLM [1] introduces PagedAttention, wherein the attention key and value vectors are stored as non-contiguous blocks in memory. Continuous batching and PagedAttention significantly increase overall GPU memory utilization during the execution of LLM. SarathiServe [4] introduces chunked-prefills, also known as dynamic SplitFuse or prefill-decode (PD) Fusion, batching together prefill and decode chunks to maximize both computation and bandwidth utilization. Since serving systems are commonly deployed on distributed platforms, numerous strategies have been pro- posed to exploit distributed characteristics. For example, recent works [10], [25], [26] advocate for separating prefill servers from decode servers, also known as PD Separation, due to the distinct computational and bandwidth characteristics of these two stages. For a comprehensive review of these techniques, we recommend [27] for further reference. These classic techniques form the foundation of inference services and our methods. Recently, with ongoing advancements in AI system re- search, numerous innovative inference techniques have been developed, particularly those related to schedulers. We present some representative works and highlight the differences be- tween these approaches and our method in TABLE I. Inspired by context switching across CPU cores, Llumnix [5] proposes a live migration mechanism and a dynamic scheduling policy to reschedule requests across multiple model instances of LLM deployed on GPU clusters, thereby enhancing load balancing and isolation. InfiniGen [6] addresses the challenge of large KV cache sizes for long-text generation by speculating and prefetching critical KV cache entries. For LoRA models, dLoRA [7] addresses the challenges of serving multiple LoRA models by dynamically merging and unmerging adapters with the base model and migrating requests and adapters between replicas. Sheng et al. [8] study the fairness problem in LLM serving concerning clients and proposes a novel scheduling algorithm called the virtual token counter (VTC). FastServe [9] proposed an innovative skip-join MLFQ scheduler to enable preemption during the autoregression process of LLM infer- ence. In distributed systems, DistServe [10] tackles the issues of PD interference and resource coupling by disaggregating prefill and decoding computation, and proposes placement algorithms to optimize resource allocation and parallelism strategies for different phases. Mooncake [28] also proposes a disaggregated architecture that separates the prefill and decode clusters and utilizes a KV cache-centric scheduler to manage the cache flow. While these innovative techniques attempt to address the issues faced by the scheduler to some extent, they scarcely model the scheduling problem formally 4 Client 1 Client 2 Bin 1 Bin 2 Bin 3 Prefill stage Decode stage Prefill stage Decode stage Decode stage Prefill stage Request Request Request Request Request Fig. 2: Illustration of LLM inference and are limited to solve the inference optimization problem theoretically. In the domain of operations research, scheduling is a well- established and extensively utilized approach. For instance, of- fline scheduling methods are applied in manufacturing systems [29], healthcare systems [30], and operations management systems [31]. To address real-time decision-making or multi- stage stochastic scenarios, online scheduling methods have been introduced in these systems [32], [33]. Nevertheless, none of the systems we examined achieve the rapid decision frequency down to 10 milliseconds that is observed in LLM inference systems. Furthermore, traditional stochastic programming models are not suitable for sequential decision- making problems where real-time information is continuously revealed. Therefore, it is imperative to develop and tailor traditional methods for application in this emergent domain of LLM inference. Research on LLM inference optimization can not only enhance hardware resource utilization in the LLM domain but also expand the repertoire available to existing operations research algorithms. III. PROBLEM FORMULATION A. Background of LLM inference techniques High efficiency and low latency are critical in LLM infer- ence, and they depend not only on performance of hardware, such as GPU, but also on how well the hardware is used. As pushing the limit of hardware computing power is costly and faced with slow progress, improved inference scheduling becomes a promising means of achieving the same outcome. Three factors are involved when designing inference schedul- ing methods: PD management approaches, cache management, and batching strategy. LLM inference alternately goes through two stages, i.e., prefill and decode stages. In the prefill stage, initial input to- kens are processed, and the LLM model s internal states, such as hidden states, are prepared. In the decode stage, the LLM model generates output tokens based on the context established during the prefill stage. The time lengths for prefill stage and decode stage are uncertain. The alternating process continues in parallel, until either the end of the sequence is reached or a predefined stopping criterion is satisfied. The process of LLM inference is illustrated in Fig. 2, where requests, each with a prefill phase and a decode phase, are sent to clients and processed in parallel. The whole process is divided into multiple bins, and each bin consists of a prefill stage and a prefill decode makespan Client 1 Client 2 Client 3 Client 4 Client 200 Up Stream: Clients sending batched P D requests to GPU node Down Stream: GPU node returning results processed by LLM GPU Node GPU 1 GPU 2 GPU 3 GPU 4 GPU 8 GPU 5 GPU 6 GPU 7 Fig. 3: Illustration of PD Competition decode stage. Requests in prefill phase can be served only when the process is in prefill stage, and the same requirement is applied to the decode phase and stage. A client may be idle as either the prefill phase or decode phase is completed but the process still stays in the same stage, causing compromised utilization of computing resources. A single LLM inference in practice typically contains a large number of requests processed by parallel clients. Thus, there is great potential in better utilizing computing resources in LLM inference, but the scheduling problem has a high complexity and, as a real- time decision making in milliseconds, the computing time is limited. The most commonly used approaches to managing prefill are provided below. PD Competition: As illustrated in Fig. 3, in this approach, either prefill or decode stage is processed at any given time for all clients on a single hardware node (typically consisting of 8 GPUs). PD Competition allows decode stage to be preempted by the prefill stage to enhance hardware utilization. PD Fusion: This approach integrates the prefill and de- code stages into a single cohesive operation, aimed at reducing overhead and enhancing throughput by stream- lining the inference pipeline. This approach also attempts to decrease latency through alignment of processes. How- ever, this integration compromises flexibility, restricting the ability to independently optimize each process or tailor responses to varying workload demands. PD Separation: This approach separates the prefill and decode stages across exclusive sets of GPUs. However, it introduces additional communication or coordination overhead, which increases latency if not properly man- aged. As a widely used approach, PD Competition has a high flexibility in effectively utilizing computing resources. Such an approach also allows an inference scheduling method to fit in and further enhance its performance. As is aforementioned, 5 inference scheduling for LLM inference is challenging. This study focuses on the inference scheduling under the PD Competition approach. The second factor that influences the efficiency of LLM inference is cache management. The KV cache is instrumental during the decoding stage by storing intermediate hidden states from preceding token generation steps. It allows the LLM model to reuse states, significantly accelerating the inference process. Despite its advantages, the KV cache requires to be properly managed. First, the KV Cache size increases with the length of input and output sequences and the number of LLM model layers. This cache size growth results in significant memory consumption, especially in the context of large-sized models and extended sequences. Effective KV cache man- agement avoids memory overflow and sustains high inference speed. Cache management may involve caching only the most relevant hidden states, while discarding or compressing less critical information to optimize resource use. Second, concur- rency issues should be addressed. The complexity of manag- ing the KV cache escalates with concurrent inference tasks. Ensuring consistent and conflict-free partitioning and access to the cache for each task is important for performance and accuracy of LLM inference. Besides, although the KV cache alleviates computational load during decoding, it introduces cache access and management overheads. Cache management requires taking into account overall latency. In this study, we proactively compute the KV cache and determine the optimal maximum number of parallel requests, equivalent to the number of clients handled in subsequent stages. Thus, we assume in the inference scheduling problem shown in Fig. 2 that the total number of clients is predetermined. The third factor that influences LLM inference efficiency is batching strategy. Continuous batching, which is introduced by ORCA [2], has emerged as an innovative technique to enhance the inference efficiency by dynamically aggregating incom- ing requests in real time. Unlike traditional static batching, which waits for a fixed number of requests before processing, continuous batching minimizes latency by reducing idle times between batch executions and adapting to fluctuating work- loads. It ensures efficient use of computational resources. By maximizing parallel processing capabilities and aligning with the model architecture s latency and throughput trade-offs, continuous batching significantly enhances the scalability and responsiveness of LLM deployments. Using the continuous batching technique, decoding is allowed to be preempted by the prefill stage. Such a case is shown by the first request of client 2 in Fig. 2, where the decode phase is separated by the prefill stage of bin 2 and is assigned to both the decode stages of bin 1 and 2. In this work, the decision-making process allows decoding to be preempted by the prefill stage, facilitated by the continuous batching technique. B. Inference scheduling problem description Inference scheduling aims to schedule inference requests using continuous batching and PD Competition, with the goal of minimizing the total inference time while adhering to operational constraints. The problem settings are given as follows and related notations are presented in TABLE II. 1) Requests and processing time: Let I {1, 2, } be the set of inference requests. Inference request i, for i I, has a fixed input token number N p i Z for prefill phase and output token number N d i Z for decode phase. The input token number is assumed to be known, but the output token number is unknown. Each request has a known prefilling time, linearly related to the total number of input tokens in a bin. The decoding time is approximately linearly related to the output token number. Let J {1, 2, } be the set of clients and T d be the decode time per token. The minimal unit of decoding time is equal to the amount of time that each client processes a single token, i.e., T d J . 2) Bin and batch size: Let K {1, 2, } be the set of bins. The inference process is divided into a total of K K bins. The batch size J represents the maximum number of requests that can be processed simultaneously in a batch. 3) Stages: There are two stages in hardware operation system: prefill and decode. Prefill and decode stages must alternate, ensuring that each stage is dedicated ex- clusively to one type of operation. At any given time, the system can perform either prefill or decode stages, but not both. At any time as the system starts a decode stage, the length of this operation is determined. Meanwhile, all requests processed at this stage will be sched- uled. The decision is made based on system state, including information of the duration of the current bin, type of requests being processed, and the set of remaining requests. It is illustrated in Fig. 4. 4) Assignment and allocation: Each request must be assigned to exactly one prefill stage for processing. For every request, the prefill phase must be com- pleted by the same client that will subsequently carry out its decode phase. Hence, both phases must be allocated to the same client. A client can process only one request at a time. Once a client begins processing a request, it remains occu- pied and cannot preempt tasks until both the prefill and decode stages of that request are completed. C. Deterministic equivalence In this hybrid offline-online problem, the offline decision component involves determining the assignment and sequence of given requests on clients. As a sequential decision-making problem, the online decision component focuses on determin- ing the length of each bin and the sequence of remaining requests in the future, with the aim of minimizing the total inference time. 6 TABLE II: Notation Table: Sets, Parameters and Decision Variables Sets Description I {1, 2, } Set of requests. J {1, 2, } Set of clients. K, Kp, Kd {1, 2, } Set of bins, prefill stages, and decode stages, respectively. K Kp Kd. L {1, 2, } Set of all possible levels for the prefill stage. Parameters I I Total number of requests. J J Total number of clients. K K Total number of bins. Np i Z Input token number of request i, for i I. Nd i Z Output token number of request i, for i I. Ncap l Z Maximum token capacity of level l, for l L. T d R Decode time per token. T p R Prefill time per token. T p l R Prefill time of level l, for l L. Decision Variables pi,j,k {0, 1} Assignment of prefill stage k to request i on client j for prefill phase, for i I, j J , and k Kp. di,j,k {0, 1} Assignment of decode stage k to request i on client j for decode phase, for i I, j J , and k Kd. ts,p k R Start time of the kth prefill stage, for k Kp. ts,d k R Start time of the kth decode stage, for k Kd. np k R Time length of the kth prefill stage, for k Kp. nd k R Time length of the kth decode stage, for k Kd. wi,j,k [0, 1] The proportion of the decoding phase of request i executed in the kth decode stage on client j, for i I, j J , and k Kd. xi,j {0, 1} The assignment of request i to client j, for i I and j J . yk,l {0, 1} Indicator variable specifying if the kth prefill stage is at level l, for k Kp and l L. tmax R Total inference time for all requests completed. Bin k Bin (k 1) Current time t Action: the time to end the current bin Action: the requests scheduled to the next bin and clients State: duration of current bin State: type of requests being processed State: set of remaining requests Fig. 4: Online scheduling Without considering uncertainty, the offline-online inference scheduling can be formulated as a deterministic equivalence, which is a form of an MIP model as follows. min tmax (1) s.t. tmax ts,d k nd k, for k Kd, (2) ts,p k ts,d k 1 nd k 1 0, for k 2, ..., K, (3) ts,d k ts,p k np k 0, for k 1, 2, ..., K, (4) np k X l L T p l yk,l, for k Kp, (5) X i I X j J N p i pi,j,k X l L N cap l yk,l, for k Kp, (6) X l L yk,l 1, for k Kp, (7) nd k T d X i I N d i wi,j,k, for j J , and k Kd, (8) di,j,k pi,j,k 0, for i I, j J , and k Kd, (9) M (2 di,j,k1 di,j,k2) k2 X k k1 di,j,k k2 k1 1, for i I, j J , k1, k2 Kd, and k1 k2, (10) M (pi,j,k1 1) di,j,k2 0, for i I, j J , k1 Kp, k2 Kd, and k1 k2, (11) X i I di,j,k 1, for j J , and k Kd, (12) X k Kd di,j,k K, for i I, and j J , (13) X k Kd wi,j,k xi,j, for i I, and j J , (14) X j J X k Kd wi,j,k 1, for i I, (15) X i I pi,j,k 1, j, k, (16) X k pi,j,k xi,j, for j J , and k Kp, (17) X j J xi,j 1, for i I, (18) 7 xi,j {0, 1}, for i I, and j J , (19) yk,l {0, 1}, for k Kp, and j J , (20) pi,j,k {0, 1}, for i I, j J , k Kp, (21) di,j,k {0, 1}, for i I, j J , k Kd, (22) wi,j,k [0, 1], for i I, j J , k Kd, (23) ts,p k , np k, for k Kp, (24) ts,d k , nd k, for k Kd. (25) The total inference time is denoted by tmax, which is min- imized in the objective function given in Eq. (1). We let Kd {1, 2, } be the set of decode stages. For any decode stage k Kd, let ts,d k R and nd k R denote the start time and time length of the kth decode stage, respectively, and thus its end time is ts,d k nd k . Eq. (2) requires that the total inference time is greater than or equal to the end time of any decode stage. Let K Z be the total number of bins, prefill stages, and also decode stages. Any prefill stage should start after the end of its previous decode stage, suggested by Eq. (3). For any prefill stage k Kp, let np k R be the time length of the kth prefill stage. Thus, Eq. (4) suggests that, within the same bin, the decode stage should start after the end of prefill stage. We use L {1, 2, } to represent the set of all possible levels for a prefill stage, and the time length of a prefill stage depends on the level. Let yk,l {0, 1}, for k Kp and l L, be an indicator variable. Prefill stage k is at level l, if yk,l 1. Otherwise, the prefill stage is not at level l. We denote by T p l R the prefill time of level l, for l L, and thus the time length of a prefill stage is determined in Eq. (5). We use N cap l Z to denote the maximum token capacity of level l, for l L, and let pi,j,k {0, 1} denote assignment of prefill stage k to request i on client j for prefill phase, for i I, j J , and k Kp. The relationship between pi,j,k and yk,l is given in Eq. (6). Eq. (7) suggests that any prefill stage can only be at one level in L. Different than the prefill phase, the decode phase of a request can be served in multiple decode stages. Thus, we introduce wi,j,k [0, 1] to represent the proportion of the decoding phase of request i executed in the kth decode stage on client j, for i I, j J , and k Kd. The time length of a decode stage is provided by Eq. (8). Let di,j,k {0, 1} be the assignment of decode stage k to request i on client j for decode phase, for i I, j J , and k Kd. Eqs. (9)-(11) together sets the rule that the decoding phase of a request must immediately follow the consecutive prefilling phase or its previous decoding phase. Eqs. (12)-(18) relate the assignment decisions among requests, clients, and bins. Eqs. (19)-(25) define the domain for each set of variables, respectively. Eqs (1)-(25) will hereafter be referred to as the original model . We attempted to solve the original model using com- mercial MIP solvers, such as Gurobi. However, we found it nearly impossible to solve such a large-scale problem directly. The number of requests, denoted by I , is around 1,000 in small cases, while in larger cases, it can be up to 10,000. The number of batch sizes or client numbers, denoted by J , can reach up to 200. The number of exclusive bins, denoted by K , often is on the same order of magnitude as I . To probe the solving cost, we solved a down-scaled toy case model with merely 100 requests and 20 clients, which took Gurobi more than 3,600 seconds to yield a near-optimal solution without fully closing the dual gap. Solving this problem in its original form is hence evidently impractical and cannot be solved in hours. Therefore, it is necessary to decompose this problem into stages and address it sequentially. IV. SOLUTION METHOD A. Method overview The original model, provided by Eqs. (1)-(25), demands a solution method capable of making decisions within 10 milliseconds in an asynchronous cycle, as the decoding time per client batch can be around 50 milliseconds. However, the original model is a large-scale MIP model with over 100,000 integer decision variables and constraints, making it difficult to solve even within several hours. Hence, it is vital to develop an efficient solution method that provides the best possible outcomes within the required time frame. As illustrated in Fig. 5, we propose a hybrid offline-online method that structures the scheduling process for large model inference into two main methods: offline requests assignment and online scheduling. Each method involves a subset of decision variables of the original model and provides timely solutions at each stage. In the figure, we illustrate how the offline-online information, as well as the decision making given by the scheduling models, is obtained and shared in the system. Offline Requests Scheduling: In this method, a prede- termined batch size of clients determines the number of parallel requests. Each client is allocated a balanced number of requests, resulting in an equitable task distribution. This method considers the assignment decisions in the original model as described in constraints (2), (12), and (15) (16). We isolate this part of the model to demonstrate offline training scenarios, such as RLHF (Reinforcement Learning with Hu- man Feedback) training. In this task, requests are typically given and known in advance. Users can manually send their request prompts to the LLMs and wait to receive the outputs. These tasks can implement offline request assignment methods to achieve better throughput. However, for most scenarios such us user using GPT, using the offline method is still limited since solving the MIP model usually takes 10 minutes or more, which cannot meet the rapid iteration requirements of the LLM online decision-making process. Therefore, it is necessary to develop an online method to fill this gap. Online Scheduling: The online scheduling process com- prises two major parts corresponding to two types of online decisions. The first part, online requests scheduling, deter- mines which requests are scheduled in the upcoming bin and identifies the next client to serve once a previous request is completed. The second part, online iteration scheduling, decides when to conclude the current decoding bin and start a preemptive prefilling bin to enhance overall utilization rates. In the online requests scheduling part, heuristic methods are employed to determine the optimal order for processing requests in real-time. This approach considers factors such as task priority, current system load, and anticipated resource 8 Offline information Online information Execution Inference Service Profiler Simulator Input requests Prefill Queue Decode Queue Original MIP Model Offline requests scheduling Online requests scheduling Online iteration scheduling LLM workers Decision Variables Fig. 5: Illustration of Solution Method availability. By effectively prioritizing requests, the system can minimize waiting times and maximize throughput under dynamic operational conditions. This method emphasizes the implementation of the relaxed solutions provided by job as- signment and illustrates the constraints (3) (11) in the original model. The online iteration scheduling part aims to minimize idle time on machines by strategically allocating computational re- sources. By dynamically adjusting prefilling and decoding task priorities based on real-time feedback and system constraints, this method enhances overall system efficiency and responsive- ness. This proactive scheduling approach minimizes machine idle time and optimizes the utilization of processing resources, thereby improving the overall performance of large language model inference tasks. This method underscores iteration- based optimization and considers the constraints (14) (15) and (17) (18) in the original model. The overarching goal of this structured approach is to mini- mize the total inference time for a specific benchmark, thereby maximizing throughput. By integrating thorough data analy- sis, efficient task allocation, and adaptive online scheduling strategies, this scheduling solution optimizes the performance of LLM inference processes. This holistic approach not only enhances system efficiency but also supports scalability and reliability in handling complex computational tasks. B. Offline requests scheduling theoretical lower bound As previously introduced, we begin by examining the offline request assignment decisions within the original model, with a specific focus on the constraints described in (2), (12), and (15 16). This part of the model is isolated to demonstrate offline training scenarios, such as RLHF training. In this sce- nario, we tackle the Minimizing Makespan Bin Packing Prob- lem to efficiently address the workload balancing challenge. We assume that the output length is predetermined, and that prefill decode stages do not conflict during problem-solving. Nevertheless, in practical applications and simulations used to evaluate performance, we adhere to these constraints by allocating workload to clients without affecting the uncertainty of output length. In the offline model outlined in Eqs. (26) (30), we introduce a new parameter, denoted by Ti R for i I, representing the estimated decode completion time for request i. We also introduce a new decision variable, denoted by tj R for j J , to indicate the total decoding time for client j. min max j J tj (26) s.t. X j J xi,j 1, for i I, (27) X i I xi,jTi tj, for j J , (28) xi,j {0, 1}, for i I, and j J , (29) tmax, tj R , for j J . (30) The offline model also provides a method to calculate the theoretical lower bound for a given set of requests, I. In this method, we assume that prefill and decode phases for all the requests can be separated into two groups, and we calculate the optimal inference time for each group. Let tp R and td R represent the optimal total prefill and total decode times for all the requests in set I, respectively. The value of td maxj J tj is obtained from the objective function value from Eqs. (26) (30). Let L arg maxl L N cap l , and then the largest prefill time across all 9 levels in T p l is denoted by T p L. N cap L is the number of maximum number of tokens that can be processed in T p L. Then, tp can be calculated by the following equation. tp T p L P i I N p i N cap L . (31) It yields a tight theoretical lower bound T LB as follows. T LB tp td . (32) C. Online requests and iteration scheduling In this online part of the LLM inference optimization problem, two critical considerations arise. First, we need to determine which request to send to an available client once the previous request is completed. Second, when a round of decoding stage is finished, we must decide whether to send a preemptive prefill stage or continue this decode stage to the LLM workers for the subsequent time frame. The first issue presents an online scheduling problem, as illustrated by Eqs. (14) (15) and (16) (17). The primary decision in this context is whether to select a new request to override the original assignment in order to achieve better machine utilization. A sorting and online preemptive method is illustrated in Algorithm 1. This online algorithm first selects the future available requests, denoted as Ij, for client j J . The set Ij is sorted by N p i N d i , that is, N p i1 N d i1 N p i2 N d i2 i1 i2 Ij. Then, for each client, the algorithm calculates future requests and counts the expected remaining tokens remain token(j) for j J to be processed. Idle clients then greedily select the longest request from busy clients to process. This algorithm utilizes offline information on request assignment to provide timely online assignment decisions. Algorithm 1 Sorting and Online Preemptive Method Require: {Ij {i xij 1}, j J } for client j in clients J do if queue for client j is empty and Ij then pop Ij to client j remain token(j) remain token(j) (N p i N d i ) else if max(remain token(j)) 0 then pop arg max(remain token) to client j remain token(j) remain token(j) (N p i N d i ) end if end for Continuing from the previous discussion, the second prob- lem involves a sequential decision-making process, as outlined by Eqs. (2) (11). The main challenge here is to deliver timely and efficient decisions in real time. As previously mentioned, each round of decoding takes approximately 50 milliseconds. Thus, it is essential to ensure that decisions are made within 10 milliseconds to sustain system efficiency. To achieve this, we employ the following method to integrate quick decision- making into the process. This aspect of decision-making corresponds to the following problem. min tmax s.t. tmax ts,d k nd k, for k Kd, (33) ts p,k (ts d,k 1 nd k 1) 0, for k 2, ..., K, (34) ts,d k (ts,p k np k) 0, for k Kp, (35) np k X l L T p l yk,l, for k Kp, (36) X i I,j J N p i pi,j,k X l L N cap l yk,l, for k Kp, (37) X l L yk,l 1, for k Kp, (38) nd k T d X i N d i wi,j,k, for j J , and k Kd, (39) di,j,k pi,j,k 0, for i I, j J , and k K. (40) By combining Eqs. (33) and (35), we derive that tmax maxk K ts,p k np k nd k . In the context of online decision- making, the start time ts,p k is typically influenced by the completion time of preceding tasks. The primary objective is to minimize the total time cost of prefill and decode stages. Consequently, we establish the following equation by integrating the calculations of np k and nd k from Eqs. (36) and (39). min tmax max k K ts,p k X l L T p l yk,l T d X i I,j J N d i wi,j,k . (41) In this problem, the time cost is divided into two components. The cost for adding a prefill task at any point is given by tmax yk,l X l L T p l , (42) and the cost for adding a decode task at the decision-making moment is expressed by tmax wi,j,k T d X i I N d i . (43) Thus, our heuristic method for deciding whether to dispatch a prefill or decode stage to the LLM worker involves comparing the prefill cost Cp P l T p l with the waited decode time Cd T d P i I,j J N d i wi,j,k. If Cp Cd, the algorithm advises continuing with a round of the decode task and waiting for additional prefill tasks; otherwise, the algorithm recommends executing a round of the prefill task. V. NUMERICAL EXPERIMENT A. Experiment settings and baseline Before the model is solved by our hybrid method, extensive analysis is conducted to evaluate the time taken by the decode and prefill stages on hardware. This analysis provides crucial insights into the computational demands and performance characteristics of each stage. By quantifying these metrics, 10 TABLE III: Experiment Settings Parameter Number Brief Description I 1319 The GSM8K dataset J 200 Due to hardware memory limit E(Np i ) 68.43 The GSM8K dataset input E(Nd i ) 344.83 The Llama 65B output T p 0.13 ms token The hardware performance on prefilling T d 0.21 ms token The hardware performance on decoding such as processing times and resource utilization, the data analysis establishes a solid foundation of empirical data. The data serve as reliable support for subsequent decision-making in optimizing scheduling strategies. The basic experiment setting is given in TABLE III. We demonstrate our improvements by utilizing the GSM8K dataset [34], a comprehensive collection of mathematical word problems specifically designed to assess the problem-solving and reasoning capabilities of language models. This dataset serves as a benchmark for evaluating both arithmetic and log- ical reasoning skills, essential attributes for advanced language models. Each problem in the dataset is intricately constructed to simulate real-world situations involving numerical rela- tionships, necessitating that models comprehend the problem contextually and perform accurate calculations to derive the correct solution. The GSM8K dataset comprises 1,319 unique problems as input requests, with an average input length of 68.43 tokens and a standard deviation of 25.04 tokens. For our experiments, we selected the LLaMA-65B language model due to its open- source nature and wide accessibility, making it a suitable candidate for academic and reproducible research. In our tests, the LLaMA-65B model generated responses averaging 344.83 tokens, with a standard deviation of 187.99 tokens. To ensure consistency and focus on quality responses, we constrained the maximum output length to 512 tokens during testing. Our computational setup is characterized by a robust hard- ware configuration, consisting of eight Ascend processing units, each equipped with a maximum memory capacity of 64 GB. This formidable hardware infrastructure is essential for facilitating the efficient processing and testing necessary for our experiments. Additionally, we have assessed the KV cache usage for input in this experiment, establishing baseline settings that are also utilized in practical applications. The current hardware, along with the LLM employed, imposes a memory constraint of 1024 blocks of KV cache. Each block can accommodate a maximum of 128 tokens. For the GSM-8k benchmark, the combined maximum input and output for each request requires five blocks. Consequently, this configuration limits us to a maximum of approximately 200 clients running concurrently, calculated by the expression 1024 5 200. In our experimental setup, we conduct an estimation of the operation time required for prefill and decode stages using over 400 data groups. We find that both prefill and decode times exhibit a linear relationship with the number of tokens involved. Specifically, the prefill time can be calculated as 0.13 milliseconds per token, plus a fixed overhead of 25 millisec- onds. For the decode process, the time required for each batch 0 25000 50000 75000 100000 125000 150000 175000 200000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 80.2 . Total inference time: 201.00 seconds. Fig. 6: Result Gantt: Baseline of clients can be estimated as 0.21 milliseconds per token, with an additional fixed overhead of 29 milliseconds. For instance, when processing a parallel batched decode stage involving 200 clients, where each client produces one token per round, the operation would take approximately 200 0.21 29 71 milliseconds. In the case of prefill stages, if a batch consists of inputs totaling 5,000 tokens, the estimated time required would be 5000 0.13 25 675 milliseconds. We present a Gantt chart in Fig. 6, generated from an experiment using real-world data and open source LLM, to illustrate the current state of online inference services with- out the implementation of our proposed method. This chart demonstrates that, in practical scenarios, a significant number of idle periods, or bubbles , occur when no scheduling strategy is employed. Furthermore, in offline scenarios, if the workload among clients is not evenly distributed, substantial machine idle time is observed after the early completion of some client s tasks. Our analysis of this Gantt chart reveals that the overall machine utilization rate is only 80.2 . B. Offline request scheduling result This offline request scheduling model given by Eqs. (26) (30) can be solved using open-source solver SCIP. Due to significantly reduced complexity, optimal solutions can be achieved within 20 minutes comparing to original problem which is not possible to be solved within hours. Although this offline model only addresses workload balancing using estimations of output length, its performance surpasses that of the original version. As illustrated in Fig. 7, the system shows a significant reduction of idle times, and machine utilization is enhanced to 85.5 . Comparing to the baseline method, this method provides a more balanced request assignment across clients and reduce bubbles . The total inference time can be reduced from 201.00 seconds to 197.08 seconds. Since solving the model still takes relatively long time, we list this method as optional and suggest practitioners use the offline model in typical scenarios such as RLHF training. 11 0 25000 50000 75000 100000 125000 150000 175000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 85.5 . Total inference time: 197.08 seconds. Fig. 7: Result Gantt: Offline Request Scheduling C. Online scheduling results Incorporating online requests and iteration scheduling meth- ods, as depicted in Fig. 9, results in a marked improvement in total inference time, showing reductions 190.58s compared to 201.00s in the baseline scenario. Additionally, machine utiliza- tion is enhanced to 89.06 . Comparing to offline scheduling method, these two online methods do not require additional computing and can be used for current online inference. In Fig. 8, we present the results obtained using only the online scheduling method, without employing the offline scheduling method. As shown, compared to the baseline, the utilization rate improves to 86.19 , and the total inference time decreases to 193.33 seconds. These results demonstrate that the online method performs well even in the absence of prior knowledge about requests. This scenario is common in the area of LLM inference. We also calculate the theoretical lower bound using Eq. (32). In the specified numerical case utilizing GSM8K, the theoretical bound is 180 seconds, in which T p 13 seconds and T d 167 seconds. In this scenario, we reduce the total inference time from 201.00 seconds in the baseline to 190.08 seconds with the hybrid online-offline method. The gap to the optimal value is thus reduced from 201 180 21 seconds to 190 180 10 seconds, representing a reduction of 52.4 in this primal dual gap. To better demonstrate the performance of our online scheduling methods, we present a numerical experiment in- volving 100 cases in Figs. 10 and 11. These cases are randomly generated with the input and output length distributions shown in TABLE III. As illustrated in the figures, despite some vari- ations across the 100 cases, our hybrid offline-online method consistently outperforms in both utilization and generation speed. The unit for generation speed is tokens per second, indicating how many tokens the LLM can generate each sec- ond. On average, our method achieves an 8.0 improvement in utilization and an increase of 100.63 tokens per second in generation speed. 0 25000 50000 75000 100000 125000 150000 175000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 86.19 . Total inference time: 193.33 seconds. Fig. 8: Result Gantt: Online only Scheduling 0 25000 50000 75000 100000 125000 150000 175000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 89.06 . Total inference time: 190.58 seconds. Fig. 9: Result Gantt: Offline Online Scheduling Fig. 10: Utilization rate with 100 cases 12 Fig. 11: Generate speed with 100 cases VI. CONCLUSION AND FUTURE WORK In this paper, we study the inference optimization problem in the service system when deploying LLMs. To enhance the system throughput and better utilize the hardware, we formulate an MIP model to describe this inference optimiza- tion problem. To the best of our knowledge, this is the first formulation of the problem from a scheduling perspective. To tackle the complex and high real-time demands of LLM inference, we introduce a hybrid offline-online method. In the offline method, we demonstrate how large-scale infer- ence systems can be improved using a Minimizing Makespan Bin Packing Problem and how a theoretical lower bound can be provided. In the online request scheduling and iteration scheduling methods, the solution time efficiency is crucial. We propose a sorting and online preemptive method to more effectively utilize clients that finish early. Then, we focus on the iteration scheduling component of the original model and employ a Lagrangian method to compare the costs of adding a prefill stage versus a decode stage at each iteration. We provide a time efficient heuristic method to determine when to insert a prefill task and interrupt ongoing decoding tasks. In real-world experiments, we deploy the LlaMA-65B LLM model and infer the GSM 8K dataset, which includes 1,319 unique math problems. Our offline model increases machine utilization rates from a baseline of 80.2 to 85.5 , and reduces the total inference time from 201 seconds to 197 seconds. Utilizing the online scheduling methods, the system utilization rate can be further increased to 89.1 , and the total inference time for the dataset can be reduced to 191 seconds. As demonstrated, if all our methods are implemented, system throughput can be improved by 5.46 , and hardware utilization can increase by 11.0 . A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0 on average. The future directions of this research can be extended along three key aspects: Stochastic Model Efficient Solution Method: The orig- inal MIP model we proposed is a deterministic equiva- lence formulation. While solving this model is already computationally challenging, developing a stochastic pro- gramming model could further enhance its accuracy by better accounting for uncertainties. Additionally, more efficient solution methods for tackling the original MIP model are needed to meet the millisecond-level real-time requirements of the decision-making process. Reinforcement Learning on Iteration Scheduling: the cur- rent iteration scheduling approach relies on a heuristic online method. Notably, the decision-making process in this method involves choosing between two options: prefill or decode. Since online state variables such as prefill task waiting time, the number of decoding clients, expected decoding time, and expected prefill time are relatively easy to derive, a simple reinforcement learning (RL) model could be trained to assist the scheduler in making decisions dynamically. Online Hardware Utilization Method: We observed dur- ing hardware experiments that the system s hardware is often underutilized when a static number of clients is employed, due to stochastic variations in the output length. In scenarios where dynamic and continuous batch- ing methods are applicable, investigating online decision- making for hardware utilization could further optimize performance. Specifically, determining the optimal num- ber of clients that can be allocated concurrently to the system at any given time could help enhance resource utilization and overall efficiency. REFERENCES [1] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, Efficient memory management for large language model serving with pagedattention, in Proceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 611 626. [2] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, Orca: A distributed serving system for {Transformer-Based} generative models, in 16th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 22), 2022, pp. 521 538. [3] NVIDIA, NVIDIA Announces Financial Results for Fourth Quarter and Fiscal 2024, details 2024 NVIDIA-Announces-Financial-Results-for-Fourth-Quarter- and-Fiscal-2024 , Feb. 2024, [Online; accessed 09-01-2025]. [4] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, A. Tumanov, and R. Ramjee, Taming throughput-latency tradeoff in llm inference with sarathi-serve, arXiv preprint arXiv:2403.02310, 2024. [5] B. Sun, Z. Huang, H. Zhao, W. Xiao, X. Zhang, Y. Li, and W. Lin, Llumnix: Dynamic scheduling for large language model serving, arXiv preprint arXiv:2406.03243, 2024. [6] W. Lee, J. Lee, J. Seo, and J. Sim, {InfiniGen}: Efficient generative inference of large language models with dynamic {KV} cache manage- ment, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 155 172. [7] B. Wu, R. Zhu, Z. Zhang, P. Sun, X. Liu, and X. Jin, {dLoRA}: Dynamically orchestrating requests and adapters for {LoRA}{LLM} serving, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 911 927. [8] Y. Sheng, S. Cao, D. Li, B. Zhu, Z. Li, D. Zhuo, J. E. Gonzalez, and I. Stoica, Fairness in serving large language models, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 965 988. [9] B. Wu, Y. Zhong, Z. Zhang, S. Liu, F. Liu, Y. Sun, G. Huang, X. Liu, and X. Jin, Fast distributed inference serving for large language models, arXiv preprint arXiv:2305.05920, 2023. [10] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang, {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 193 210. 13 [11] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, Quantization and training of neural networks for efficient integer-arithmetic-only inference, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 2704 2713. [12] R. Child, S. Gray, A. Radford, and I. Sutskever, Generating long sequences with sparse transformers, arXiv preprint arXiv:1904.10509, 2019. [13] G. Bai, Y. Li, C. Ling, K. Kim, and L. Zhao, Sparsellm: Towards global pruning of pre-trained language models, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [14] G. Hinton, Distilling the knowledge in a neural network, arXiv preprint arXiv:1503.02531, 2015. [15] N. Shazeer, Fast transformer decoding: One write-head is all you need, arXiv preprint arXiv:1911.02150, 2019. [16] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr on, and S. Sanghai, Gqa: Training generalized multi-query transformer models from multi-head checkpoints, arXiv preprint arXiv:2305.13245, 2023. [17] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro et al., Efficient large-scale language model training on gpu clusters using megatron-lm, in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021, pp. 1 15. [18] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu et al., Gpipe: Easy scaling with micro-batch pipeline parallelism, proceeding of Computer Science Computer Vision and Pattern Recognition, 2019. [19] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan- zaro, Megatron-lm: Training multi-billion parameter language models using model parallelism, arXiv preprint arXiv:1909.08053, 2019. [20] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro, Reducing activation recomputation in large transformer models, Proceedings of Machine Learning and Systems, vol. 5, pp. 341 353, 2023. [21] H. Liu, M. Zaharia, and P. Abbeel, Ring attention with blockwise transformers for near-infinite context, arXiv preprint arXiv:2310.01889, 2023. [22] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R e, Flashattention: Fast and memory-efficient exact attention with io-awareness, Advances in Neural Information Processing Systems, vol. 35, pp. 16 344 16 359, 2022. [23] Y. Leviathan, M. Kalman, and Y. Matias, Fast inference from transform- ers via speculative decoding, in International Conference on Machine Learning. PMLR, 2023, pp. 19 274 19 286. [24] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper, Accelerating large language model decoding with speculative sam- pling, arXiv preprint arXiv:2302.01318, 2023. [25] P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and R. Bianchini, Splitwise: Efficient generative llm inference using phase splitting, in 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 2024, pp. 118 132. [26] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng, C. Wang, S. Wang, Y. Bao et al., Inference without interference: Disaggre- gate llm inference for mixed downstream workloads, arXiv preprint arXiv:2401.11181, 2024. [27] Z. Zhou, X. Ning, K. Hong, T. Fu, J. Xu, S. Li, Y. Lou, L. Wang, Z. Yuan, X. Li et al., A survey on efficient inference for large language models, arXiv preprint arXiv:2404.14294, 2024. [28] R. Qin, Z. Li, W. He, M. Zhang, Y. Wu, W. Zheng, and X. Xu, Moon- cake: A kvcache-centric disaggregated architecture for llm serving, arXiv preprint arXiv:2407.00079, 2024. [29] F. Wang, S. Fathizadan, F. Ju, K. Rowe, and N. Hofmann, Print surface thermal modeling and layer time control for large-scale additive manu- facturing, IEEE Transactions on automation science and engineering, vol. 18, no. 1, pp. 244 254, 2020. [30] B. Pang, X. Xie, Y. Song, and L. Luo, Surgery scheduling under case cancellation and surgery duration uncertainty, IEEE Transactions on Automation Science and Engineering, vol. 16, no. 1, pp. 74 86, 2018. [31] S. A. Erdogan, A. Gose, and B. T. Denton, Online appointment sequencing and scheduling, IIE Transactions, vol. 47, no. 11, pp. 1267 1286, 2015. [32] B. Pang, X. Xie, F. Ju, and J. Pipe, A dynamic sequential decision- making model on mri real-time scheduling with simulation-based opti- mization, Health Care Management Science, vol. 25, no. 3, pp. 426 440, 2022. [33] K. Lee, F. Zheng, and M. L. Pinedo, Online scheduling of ordered flow shops, European Journal of Operational Research, vol. 272, no. 1, pp. 50 60, 2019. [34] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. Bowen Pang received his Bachelor s degree and Ph.D. degree from the Department of Industrial Engineering at Tsinghua University, Beijing, China, in 2016 and 2022, respectively. He is currently a researcher at Noah s Ark Lab, Huawei Technology. His research interests include modeling, analyzing, and solving problems in engineering systems, with applications in healthcare, supply chain, manufactur- ing, and artificial intelligence. He has been a member of IEEE, IISE, and INFORMS. Please feel free to contact his email if you are interested in LLM inference optimization area. Kai Li received the bachelor s degree from YingCai Honors College, University of Electronic Science and Technology of China, Chengdu, China, in 2015, and the Ph.D. degree from the Department of Com- puter Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, in 2021. He is currently a researcher at Noah s Ark Lab, Huawei Technology. His research interests include game theory, rein- forcement learning, and large language models. Ruifeng She received his B.S. and Ph.D. degrees from the Department of Civil Engineering of the University of Illinois at Urbana-Champaign, USA, in 2018 and 2023, respectively. He is currently a researcher at Noah s Ark Lab, Huawei Technology. His research interests include optimization of com- plex systems, reinforcement learning, and the inte- gration of operations research and machine learning. Feifan Wang received the bachelor s degree from the Department of Industrial Engineering, Zhejiang University of Technology, Hangzhou, China, in 2013, the master s degree from the Department of Industrial and Systems Engineering, Zhejiang Uni- versity, Hangzhou, China, in 2016, and the Ph.D. degree from the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA, in 2021. He is cur- rently an Assistant Professor with the Department of Industrial Engineering at Tsinghua University, Beijing, China. His research focuses on modeling, analysis, optimization, and control of complex systems, with applications in healthcare delivery systems and production systems. He is a member of IEEE, IISE, and INFORMS. He was a recipient of multiple awards, including the Design and Manufacturing Best Paper Award from the IISE Transactions, the Best Student Paper Award from IEEE CASE, and the Dean s Dissertation Award from ASU. He has twice been a finalist for the Best Paper Award on Healthcare Automation from IEEE CASE.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\n1 Hybrid Offline-online Scheduling Method for Large Language Model Inference Optimization Bowen Pang, Kai Li, Ruifeng She, and Feifan Wang, Member, IEEE Abstract With the development of large language models (LLMs), it has become increasingly important to optimize hardware usage and improve throughput. In this paper, we study the inference optimization of the serving system that deploys LLMs. To optimize system throughput and maximize hardware utilization, we formulate the inference optimization problem as a mixed-integer programming (MIP) model and propose a hybrid offline-online method as solution. The offline method improves large-scale inference systems by introducing a Minimizing Makespan Bin Packing Problem. We further provide a theoretical lower bound computation method. Then, we propose an online sorting and preemptive scheduling method to better utilize hardware. In the online iteration scheduling process, a Lagrangian method is applied to evaluate the cost efficiency of inserting prefill stages versus decode stages at each iteration and dynamically determine when to preempt decoding tasks and insert prefill tasks. Experiments using real-world data from the LLaMA-65B model and the GSM8K dataset demonstrate that system utilization improves from 80.2 to 89.1 , and the total inference time decreases from 201.00 to 190.58 seconds. A 100- cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0 on average. Finally, we discuss potential future extensions, including stochastic modeling, reinforcement learning-based schedulers, and dynamic decision-making strategies for system throughput and hardware utilization. Note to Practitioners This work provides optimization tools for enhancing the efficiency of LLM inference systems through advanced scheduling techniques. From the perspective of LLM inference service providers, improved hardware utilization can reduce operational costs by requiring less hardware to maintain the same level of service. From the user s perspective, reduced inference time translates to faster response times and improved service quality. Furthermore, the proposed scheduling techniques are adaptable to various LLM models, hardware platforms, and datasets, making them highly scalable and broadly applicable to real-world LLM inference scenarios. Index Terms Large Language Model, Inference System, On- line Scheduling, Mixed-integer Programming. I.\n\n--- Segment 2 ---\nIndex Terms Large Language Model, Inference System, On- line Scheduling, Mixed-integer Programming. I. INTRODUCTION Recent advancements in large language models (LLMs), including GPT-4, LLaMA, and Qwen, have significantly transformed the landscape of natural language processing by enabling more sophisticated text generation, comprehension, and interaction capabilities. These models serve as founda- tional technologies in a wide range of applications, such as chatbots, machine translation, and content creation. Despite their transformative potential, the inference process for LLMs B. Pang, K. Li, and R. She are with Noah s Ark Lab, Huawei. F. Wang is with the Department of Industrial Engineering, Tsinghua University, Beijing, 100084, China. E-mail: Computing Infrastructure Serving Configurations System resource model Configurations GPU computing ability Node size Network topology Memory Network bandwidth Layers Head Model type DP TP PP Service level Quant BS Requests scheduling Iteration scheduling basis Inference Service Input requests Offline requests scheduling Prefill queue Decode queue Scheduling strategy Online requests scheduling Online iteration scheduling KV cache Multi-objectives Service level SJF LLM workers Batching Profiler Response tokens Decode iteration Information collecting (optional) Fig. 1: Illustration of LLM inference service system is computationally intensive and resource-demanding, result- ing in high costs and increased latency. These factors pose significant challenges when scaling their deployment across various applications. Effective inference scheduling is crucial for optimizing resource usage, reducing operational costs, and ensuring high-quality service delivery. As illustrated in Fig. 1, the current inference service system comprises three components: system resource, serving con- figuration, and inference service. System resource determines the extent of computational resources available in the system including GPU computing ability, memory, node size, network bandwidth, etc. The serving configuration specifies the desired model deployment method and service level expectations, such as data tensor pipeline parallel settings, quantization, batch size, and scheduling configurations. The inference service processes input requests from users, utilizes hardware capa- bilities, fulfills user configurations, and returns responses to LLM users. The scheduling strategy module is the core of the inference service, managing prefill and decode queues, offline and online request scheduling, and iteration-level hardware management. The offline scheduling method is optional, only for inference tasks where all requests are known in advance.\n\n--- Segment 3 ---\nThe scheduling strategy module is the core of the inference service, managing prefill and decode queues, offline and online request scheduling, and iteration-level hardware management. The offline scheduling method is optional, only for inference tasks where all requests are known in advance. Conversely, the two online scheduling methods, i.e., requests scheduling and iteration scheduling, are more versatile and arXiv:2502.15763v1 [cs.DC] 14 Feb 2025 2 applicable to a wide range of scenarios. The online methods acquire information from the online profiler and dispatch requests and inference tasks to LLM workers. In practice hardware utilization includes up to 20 bubbles , which means the hardware is idle during inference service, and detail is later shown in Section IV. By designing an efficient LLM inference scheduling system to reduce these bubbles, computational resource consumption can be decreased, leading to reduced latency and increased throughput. For maintaining logic flow, detailed technical introductions to prefill and de- code operations are provided in Section III. Current works, such as vLLM [1] and ORCA [2], provide a robust foundation for LLM inference serving systems. The primary contributions of vLLM and ORCA to LLM inference are their enhancements in resource allocation and execution efficiency, which significantly increase throughput and de- crease latency in large-scale language model deployments. These improvements are achieved through advanced memory management and continuous batching techniques, which en- hance model parallelism and effectively leverage hardware resources [1], [2]. However, the state-of-the-art scheduler in vLLM predominantly employs a First-Come-First-Serve (FCFS) and prefill-first policy, prioritizing prefill tasks but not fully leveraging the scheduling system s potential. We identify the causes of low hardware utilization rates. From an offline scheduling perspective, client loads are unbalanced, and request sequencing can be improved. From an online schedul- ing angle, the prefill-first policy lacks parallel processing of multiple requests at prefill stage, and there are no dynamic sequencing adjustments or preemption methods. The optimization of LLM inference presents two major challenges. The first challenge pertains to the offline method, which involves managing a large number of requests from up to 200 parallel clients. This task is particularly time-consuming when using a mixed-integer programming (MIP) scheduling model.\n\n--- Segment 4 ---\nThe first challenge pertains to the offline method, which involves managing a large number of requests from up to 200 parallel clients. This task is particularly time-consuming when using a mixed-integer programming (MIP) scheduling model. The second challenge is the need for much faster decision-making for online methods compared to traditional online scheduling problems. For example, in LLM inference scenarios, online decisions about whether to send prefill or decode requests to LLM workers typically occur every 50 milliseconds. In contrast, in healthcare or production systems, online decisions usually occur every thirty seconds or even several minutes. For online scheduling methods, the higher frequency of decision-making requires algorithms that are efficient and capable of delivering results within extremely short time frames. Accompanied by these challenges, developing a scheduling method for LLM inference could yield substantial benefits. According to NVIDIA Financial Results in 2024 [3], the revenue from GPU retail and cloud computation service has reached 60.9 billion per year. Improving computational re- source efficiency by 5 will earn up to 3.05 billion in revenue annually. Therefore, research in this area is crucial, especially for scheduling researchers. In this paper, we aim at enhance service system through- put and optimize hardware utilization. We define the LLM inference optimization problem using a MIP model as its uncertainty equivalence. Then, we propose a hybrid offline- online method as solution. This work is the first to define this problem from a scheduling perspective. To address the high real-time demands in LLM inference, we propose a hybrid offline-online scheduling method. In numerical exper- iments, we demonstrate that our method can improve system throughput by 5.46 and improve hardware utilization by 11.0 . A 100-cases study shows that our method outperforms baseline method consistently with an improvement of 8.0 on utilization rate. The major contributions of this work include the followings. To the best of our knowledge, we are the first to formulate a mathematical model that describe the scheduling problem in the area of LLM inference. We design a two-stage approach to manage both offline and online scheduling challenges. An Minimizing Makespan Bin Packing model is developed to efficiently solve the offline scheduling problem. Additionally, we introduce a sorting and preemption method to handle the online request scheduling problem, and we develop a Lagrangian-based heuristic technique for solving the online iteration scheduling issue.\n\n--- Segment 5 ---\nAn Minimizing Makespan Bin Packing model is developed to efficiently solve the offline scheduling problem. Additionally, we introduce a sorting and preemption method to handle the online request scheduling problem, and we develop a Lagrangian-based heuristic technique for solving the online iteration scheduling issue. In the online scheduling module, our method can provide decisions within 5 milliseconds, meet- ing the real-time decision-making requirements of practical application in LLM inference. The remainder of the paper is structured as follows. We review literature on efficient LLM inference in Section II. The problem definition and model formulation are presented in Section III. Then, we illustrate the difficulty of solving this problem and introduce our hybrid offline-online scheduling method to provide a timely solution in Section IV. Numerical studies using real cases and 100 generated cases are presented in Section V. Finally, the conclusion and future directions are provided in Section VI. II. LITERATURE REVIEW This section provides an overview of existing research and prevalent methodologies in inference optimization for LLMs. Firstly, we introduce the general techniques commonly em- ployed in model serving, which can be seamlessly integrated with our scheduling strategies. Subsequently, we elucidate several classical techniques widely adopted in LLM inference systems, which constitute the cornerstone of our framework and methodologies. In the following, we succinctly introduce recent advancements in inference optimization. Finally, we examine scheduling methods within the existing operations research domain. As LLM inference falls within the broader scope of model serving, a variety of general inference optimization techniques can be effectively utilized. Model compression is one of the quintessential optimization strategies for reducing model size, encompassing techniques such as quantization [11], spar- sification [12], [13], and distillation [14]. In addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [15] or grouped-query attention [16] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model.\n\n--- Segment 6 ---\nIn addition, the design of more compact structures to replace the original ones is also common. For instance, employing multi-query attention [15] or grouped-query attention [16] in place of the original multi-head attention in Transformer architecture can reduce key-value heads, resulting in a more streamlined model. Nevertheless, both model compression and the design 3 TABLE I: Literature Review Work Throughput Latency Cache Management Dynamic Decision Uncertainty Request Management Iteration Management Batching Distributed Strategies Orca vLLM Sarathi-Serve [4] Llumnix [5] InfiniGen [6] dLoRA [7] VTC [8] FastServe [9] DistServe [10] MoonCake OURS of compact structures can alter model weights, potentially leading to a decline in accuracy. Instead of optimizing the model size, data parallelism (DP) and model parallelism aim to fully leverage the computational power of the devices. In DP [17], the model weights are replicated across multiple devices, allowing different inference requests to be processed in parallel on different devices. Model parallelism distributes the model weights across several devices to minimize the per- device memory footprint of the model weights. Consequently, each device can operate more efficiently by executing a smaller portion of the task. Several model parallelization methods exist, such as pipeline parallelism (PP) [18], tensor parallelism (TP) [19], sequence parallelism (SP) [20], and context paral- lelism (CP) [21]. Since our scheduling methods are orthogonal to the aforementioned techniques, both model optimization and parallelization strategies can be employed seamlessly in conjunction with our methods to enhance inference efficiency. In addition to general model serving techniques, the opti- mization of LLM inference serving systems primarily involves enhancing the model forward pass. Researchers have improved system efficiency from various perspectives, including kernel fusion, Key-Value (KV) cache management, request manage- ment, iteration management, batching, distributed strategies, etc. Here, we present several classic techniques that are prevalent in LLM inference serving systems. FlashAttention [22] amalgamates the operations of data transfer between hard- ware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding [23], [24] employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model.\n\n--- Segment 7 ---\nFlashAttention [22] amalgamates the operations of data transfer between hard- ware components within the attention mechanism to expedite operation execution without compromising model accuracy. Speculative decoding [23], [24] employs an auxiliary model to generate a preliminary draft, followed by a verification process executed by the main model. This technique enables the serving system to output multiple tokens in a single forward pass instead of one. Orca [2] pioneers continuous batching by aggregating different requests at the iteration level. Rather than awaiting the completion of an entire batch before starting the execution of new requests, continuous batching allows new requests to be inserted into the batch while other requests are still in progress. Inspired by memory management strategies in operating systems, vLLM [1] introduces PagedAttention, wherein the attention key and value vectors are stored as non-contiguous blocks in memory. Continuous batching and PagedAttention significantly increase overall GPU memory utilization during the execution of LLM. SarathiServe [4] introduces chunked-prefills, also known as dynamic SplitFuse or prefill-decode (PD) Fusion, batching together prefill and decode chunks to maximize both computation and bandwidth utilization. Since serving systems are commonly deployed on distributed platforms, numerous strategies have been pro- posed to exploit distributed characteristics. For example, recent works [10], [25], [26] advocate for separating prefill servers from decode servers, also known as PD Separation, due to the distinct computational and bandwidth characteristics of these two stages. For a comprehensive review of these techniques, we recommend [27] for further reference. These classic techniques form the foundation of inference services and our methods. Recently, with ongoing advancements in AI system re- search, numerous innovative inference techniques have been developed, particularly those related to schedulers. We present some representative works and highlight the differences be- tween these approaches and our method in TABLE I. Inspired by context switching across CPU cores, Llumnix [5] proposes a live migration mechanism and a dynamic scheduling policy to reschedule requests across multiple model instances of LLM deployed on GPU clusters, thereby enhancing load balancing and isolation. InfiniGen [6] addresses the challenge of large KV cache sizes for long-text generation by speculating and prefetching critical KV cache entries.\n\n--- Segment 8 ---\nInspired by context switching across CPU cores, Llumnix [5] proposes a live migration mechanism and a dynamic scheduling policy to reschedule requests across multiple model instances of LLM deployed on GPU clusters, thereby enhancing load balancing and isolation. InfiniGen [6] addresses the challenge of large KV cache sizes for long-text generation by speculating and prefetching critical KV cache entries. For LoRA models, dLoRA [7] addresses the challenges of serving multiple LoRA models by dynamically merging and unmerging adapters with the base model and migrating requests and adapters between replicas. Sheng et al. [8] study the fairness problem in LLM serving concerning clients and proposes a novel scheduling algorithm called the virtual token counter (VTC). FastServe [9] proposed an innovative skip-join MLFQ scheduler to enable preemption during the autoregression process of LLM infer- ence. In distributed systems, DistServe [10] tackles the issues of PD interference and resource coupling by disaggregating prefill and decoding computation, and proposes placement algorithms to optimize resource allocation and parallelism strategies for different phases. Mooncake [28] also proposes a disaggregated architecture that separates the prefill and decode clusters and utilizes a KV cache-centric scheduler to manage the cache flow. While these innovative techniques attempt to address the issues faced by the scheduler to some extent, they scarcely model the scheduling problem formally 4 Client 1 Client 2 Bin 1 Bin 2 Bin 3 Prefill stage Decode stage Prefill stage Decode stage Decode stage Prefill stage Request Request Request Request Request Fig. 2: Illustration of LLM inference and are limited to solve the inference optimization problem theoretically. In the domain of operations research, scheduling is a well- established and extensively utilized approach. For instance, of- fline scheduling methods are applied in manufacturing systems [29], healthcare systems [30], and operations management systems [31]. To address real-time decision-making or multi- stage stochastic scenarios, online scheduling methods have been introduced in these systems [32], [33]. Nevertheless, none of the systems we examined achieve the rapid decision frequency down to 10 milliseconds that is observed in LLM inference systems. Furthermore, traditional stochastic programming models are not suitable for sequential decision- making problems where real-time information is continuously revealed.\n\n--- Segment 9 ---\nNevertheless, none of the systems we examined achieve the rapid decision frequency down to 10 milliseconds that is observed in LLM inference systems. Furthermore, traditional stochastic programming models are not suitable for sequential decision- making problems where real-time information is continuously revealed. Therefore, it is imperative to develop and tailor traditional methods for application in this emergent domain of LLM inference. Research on LLM inference optimization can not only enhance hardware resource utilization in the LLM domain but also expand the repertoire available to existing operations research algorithms. III. PROBLEM FORMULATION A. Background of LLM inference techniques High efficiency and low latency are critical in LLM infer- ence, and they depend not only on performance of hardware, such as GPU, but also on how well the hardware is used. As pushing the limit of hardware computing power is costly and faced with slow progress, improved inference scheduling becomes a promising means of achieving the same outcome. Three factors are involved when designing inference schedul- ing methods: PD management approaches, cache management, and batching strategy. LLM inference alternately goes through two stages, i.e., prefill and decode stages. In the prefill stage, initial input to- kens are processed, and the LLM model s internal states, such as hidden states, are prepared. In the decode stage, the LLM model generates output tokens based on the context established during the prefill stage. The time lengths for prefill stage and decode stage are uncertain. The alternating process continues in parallel, until either the end of the sequence is reached or a predefined stopping criterion is satisfied. The process of LLM inference is illustrated in Fig. 2, where requests, each with a prefill phase and a decode phase, are sent to clients and processed in parallel. The whole process is divided into multiple bins, and each bin consists of a prefill stage and a prefill decode makespan Client 1 Client 2 Client 3 Client 4 Client 200 Up Stream: Clients sending batched P D requests to GPU node Down Stream: GPU node returning results processed by LLM GPU Node GPU 1 GPU 2 GPU 3 GPU 4 GPU 8 GPU 5 GPU 6 GPU 7 Fig. 3: Illustration of PD Competition decode stage. Requests in prefill phase can be served only when the process is in prefill stage, and the same requirement is applied to the decode phase and stage.\n\n--- Segment 10 ---\n3: Illustration of PD Competition decode stage. Requests in prefill phase can be served only when the process is in prefill stage, and the same requirement is applied to the decode phase and stage. A client may be idle as either the prefill phase or decode phase is completed but the process still stays in the same stage, causing compromised utilization of computing resources. A single LLM inference in practice typically contains a large number of requests processed by parallel clients. Thus, there is great potential in better utilizing computing resources in LLM inference, but the scheduling problem has a high complexity and, as a real- time decision making in milliseconds, the computing time is limited. The most commonly used approaches to managing prefill are provided below. PD Competition: As illustrated in Fig. 3, in this approach, either prefill or decode stage is processed at any given time for all clients on a single hardware node (typically consisting of 8 GPUs). PD Competition allows decode stage to be preempted by the prefill stage to enhance hardware utilization. PD Fusion: This approach integrates the prefill and de- code stages into a single cohesive operation, aimed at reducing overhead and enhancing throughput by stream- lining the inference pipeline. This approach also attempts to decrease latency through alignment of processes. How- ever, this integration compromises flexibility, restricting the ability to independently optimize each process or tailor responses to varying workload demands. PD Separation: This approach separates the prefill and decode stages across exclusive sets of GPUs. However, it introduces additional communication or coordination overhead, which increases latency if not properly man- aged. As a widely used approach, PD Competition has a high flexibility in effectively utilizing computing resources. Such an approach also allows an inference scheduling method to fit in and further enhance its performance. As is aforementioned, 5 inference scheduling for LLM inference is challenging. This study focuses on the inference scheduling under the PD Competition approach. The second factor that influences the efficiency of LLM inference is cache management. The KV cache is instrumental during the decoding stage by storing intermediate hidden states from preceding token generation steps. It allows the LLM model to reuse states, significantly accelerating the inference process. Despite its advantages, the KV cache requires to be properly managed. First, the KV Cache size increases with the length of input and output sequences and the number of LLM model layers. This cache size growth results in significant memory consumption, especially in the context of large-sized models and extended sequences.\n\n--- Segment 11 ---\nFirst, the KV Cache size increases with the length of input and output sequences and the number of LLM model layers. This cache size growth results in significant memory consumption, especially in the context of large-sized models and extended sequences. Effective KV cache man- agement avoids memory overflow and sustains high inference speed. Cache management may involve caching only the most relevant hidden states, while discarding or compressing less critical information to optimize resource use. Second, concur- rency issues should be addressed. The complexity of manag- ing the KV cache escalates with concurrent inference tasks. Ensuring consistent and conflict-free partitioning and access to the cache for each task is important for performance and accuracy of LLM inference. Besides, although the KV cache alleviates computational load during decoding, it introduces cache access and management overheads. Cache management requires taking into account overall latency. In this study, we proactively compute the KV cache and determine the optimal maximum number of parallel requests, equivalent to the number of clients handled in subsequent stages. Thus, we assume in the inference scheduling problem shown in Fig. 2 that the total number of clients is predetermined. The third factor that influences LLM inference efficiency is batching strategy. Continuous batching, which is introduced by ORCA [2], has emerged as an innovative technique to enhance the inference efficiency by dynamically aggregating incom- ing requests in real time. Unlike traditional static batching, which waits for a fixed number of requests before processing, continuous batching minimizes latency by reducing idle times between batch executions and adapting to fluctuating work- loads. It ensures efficient use of computational resources. By maximizing parallel processing capabilities and aligning with the model architecture s latency and throughput trade-offs, continuous batching significantly enhances the scalability and responsiveness of LLM deployments. Using the continuous batching technique, decoding is allowed to be preempted by the prefill stage. Such a case is shown by the first request of client 2 in Fig. 2, where the decode phase is separated by the prefill stage of bin 2 and is assigned to both the decode stages of bin 1 and 2. In this work, the decision-making process allows decoding to be preempted by the prefill stage, facilitated by the continuous batching technique. B.\n\n--- Segment 12 ---\nIn this work, the decision-making process allows decoding to be preempted by the prefill stage, facilitated by the continuous batching technique. B. Inference scheduling problem description Inference scheduling aims to schedule inference requests using continuous batching and PD Competition, with the goal of minimizing the total inference time while adhering to operational constraints. The problem settings are given as follows and related notations are presented in TABLE II. 1) Requests and processing time: Let I {1, 2, } be the set of inference requests. Inference request i, for i I, has a fixed input token number N p i Z for prefill phase and output token number N d i Z for decode phase. The input token number is assumed to be known, but the output token number is unknown. Each request has a known prefilling time, linearly related to the total number of input tokens in a bin. The decoding time is approximately linearly related to the output token number. Let J {1, 2, } be the set of clients and T d be the decode time per token. The minimal unit of decoding time is equal to the amount of time that each client processes a single token, i.e., T d J . 2) Bin and batch size: Let K {1, 2, } be the set of bins. The inference process is divided into a total of K K bins. The batch size J represents the maximum number of requests that can be processed simultaneously in a batch. 3) Stages: There are two stages in hardware operation system: prefill and decode. Prefill and decode stages must alternate, ensuring that each stage is dedicated ex- clusively to one type of operation. At any given time, the system can perform either prefill or decode stages, but not both. At any time as the system starts a decode stage, the length of this operation is determined. Meanwhile, all requests processed at this stage will be sched- uled. The decision is made based on system state, including information of the duration of the current bin, type of requests being processed, and the set of remaining requests. It is illustrated in Fig. 4. 4) Assignment and allocation: Each request must be assigned to exactly one prefill stage for processing. For every request, the prefill phase must be com- pleted by the same client that will subsequently carry out its decode phase. Hence, both phases must be allocated to the same client.\n\n--- Segment 13 ---\nFor every request, the prefill phase must be com- pleted by the same client that will subsequently carry out its decode phase. Hence, both phases must be allocated to the same client. A client can process only one request at a time. Once a client begins processing a request, it remains occu- pied and cannot preempt tasks until both the prefill and decode stages of that request are completed. C. Deterministic equivalence In this hybrid offline-online problem, the offline decision component involves determining the assignment and sequence of given requests on clients. As a sequential decision-making problem, the online decision component focuses on determin- ing the length of each bin and the sequence of remaining requests in the future, with the aim of minimizing the total inference time. 6 TABLE II: Notation Table: Sets, Parameters and Decision Variables Sets Description I {1, 2, } Set of requests. J {1, 2, } Set of clients. K, Kp, Kd {1, 2, } Set of bins, prefill stages, and decode stages, respectively. K Kp Kd. L {1, 2, } Set of all possible levels for the prefill stage. Parameters I I Total number of requests. J J Total number of clients. K K Total number of bins. Np i Z Input token number of request i, for i I. Nd i Z Output token number of request i, for i I. Ncap l Z Maximum token capacity of level l, for l L. T d R Decode time per token. T p R Prefill time per token. T p l R Prefill time of level l, for l L. Decision Variables pi,j,k {0, 1} Assignment of prefill stage k to request i on client j for prefill phase, for i I, j J , and k Kp. di,j,k {0, 1} Assignment of decode stage k to request i on client j for decode phase, for i I, j J , and k Kd. ts,p k R Start time of the kth prefill stage, for k Kp. ts,d k R Start time of the kth decode stage, for k Kd. np k R Time length of the kth prefill stage, for k Kp. nd k R Time length of the kth decode stage, for k Kd.\n\n--- Segment 14 ---\nnp k R Time length of the kth prefill stage, for k Kp. nd k R Time length of the kth decode stage, for k Kd. wi,j,k [0, 1] The proportion of the decoding phase of request i executed in the kth decode stage on client j, for i I, j J , and k Kd. xi,j {0, 1} The assignment of request i to client j, for i I and j J . yk,l {0, 1} Indicator variable specifying if the kth prefill stage is at level l, for k Kp and l L. tmax R Total inference time for all requests completed. Bin k Bin (k 1) Current time t Action: the time to end the current bin Action: the requests scheduled to the next bin and clients State: duration of current bin State: type of requests being processed State: set of remaining requests Fig. 4: Online scheduling Without considering uncertainty, the offline-online inference scheduling can be formulated as a deterministic equivalence, which is a form of an MIP model as follows. min tmax (1) s.t.\n\n--- Segment 15 ---\n4: Online scheduling Without considering uncertainty, the offline-online inference scheduling can be formulated as a deterministic equivalence, which is a form of an MIP model as follows. min tmax (1) s.t. tmax ts,d k nd k, for k Kd, (2) ts,p k ts,d k 1 nd k 1 0, for k 2, ..., K, (3) ts,d k ts,p k np k 0, for k 1, 2, ..., K, (4) np k X l L T p l yk,l, for k Kp, (5) X i I X j J N p i pi,j,k X l L N cap l yk,l, for k Kp, (6) X l L yk,l 1, for k Kp, (7) nd k T d X i I N d i wi,j,k, for j J , and k Kd, (8) di,j,k pi,j,k 0, for i I, j J , and k Kd, (9) M (2 di,j,k1 di,j,k2) k2 X k k1 di,j,k k2 k1 1, for i I, j J , k1, k2 Kd, and k1 k2, (10) M (pi,j,k1 1) di,j,k2 0, for i I, j J , k1 Kp, k2 Kd, and k1 k2, (11) X i I di,j,k 1, for j J , and k Kd, (12) X k Kd di,j,k K, for i I, and j J , (13) X k Kd wi,j,k xi,j, for i I, and j J , (14) X j J X k Kd wi,j,k 1, for i I, (15) X i I pi,j,k 1, j, k, (16) X k pi,j,k xi,j, for j J , and k Kp, (17) X j J xi,j 1, for i I, (18) 7 xi,j {0, 1}, for i I, and j J , (19) yk,l {0, 1}, for k Kp, and j J , (20) pi,j,k {0, 1}, for i I, j J , k Kp, (21) di,j,k {0, 1}, for i I, j J , k Kd, (22) wi,j,k [0, 1], for i I, j J , k Kd, (23) ts,p k , np k, for k Kp, (24) ts,d k , nd k, for k Kd.\n\n--- Segment 16 ---\nmin tmax (1) s.t. tmax ts,d k nd k, for k Kd, (2) ts,p k ts,d k 1 nd k 1 0, for k 2, ..., K, (3) ts,d k ts,p k np k 0, for k 1, 2, ..., K, (4) np k X l L T p l yk,l, for k Kp, (5) X i I X j J N p i pi,j,k X l L N cap l yk,l, for k Kp, (6) X l L yk,l 1, for k Kp, (7) nd k T d X i I N d i wi,j,k, for j J , and k Kd, (8) di,j,k pi,j,k 0, for i I, j J , and k Kd, (9) M (2 di,j,k1 di,j,k2) k2 X k k1 di,j,k k2 k1 1, for i I, j J , k1, k2 Kd, and k1 k2, (10) M (pi,j,k1 1) di,j,k2 0, for i I, j J , k1 Kp, k2 Kd, and k1 k2, (11) X i I di,j,k 1, for j J , and k Kd, (12) X k Kd di,j,k K, for i I, and j J , (13) X k Kd wi,j,k xi,j, for i I, and j J , (14) X j J X k Kd wi,j,k 1, for i I, (15) X i I pi,j,k 1, j, k, (16) X k pi,j,k xi,j, for j J , and k Kp, (17) X j J xi,j 1, for i I, (18) 7 xi,j {0, 1}, for i I, and j J , (19) yk,l {0, 1}, for k Kp, and j J , (20) pi,j,k {0, 1}, for i I, j J , k Kp, (21) di,j,k {0, 1}, for i I, j J , k Kd, (22) wi,j,k [0, 1], for i I, j J , k Kd, (23) ts,p k , np k, for k Kp, (24) ts,d k , nd k, for k Kd. (25) The total inference time is denoted by tmax, which is min- imized in the objective function given in Eq.\n\n--- Segment 17 ---\ntmax ts,d k nd k, for k Kd, (2) ts,p k ts,d k 1 nd k 1 0, for k 2, ..., K, (3) ts,d k ts,p k np k 0, for k 1, 2, ..., K, (4) np k X l L T p l yk,l, for k Kp, (5) X i I X j J N p i pi,j,k X l L N cap l yk,l, for k Kp, (6) X l L yk,l 1, for k Kp, (7) nd k T d X i I N d i wi,j,k, for j J , and k Kd, (8) di,j,k pi,j,k 0, for i I, j J , and k Kd, (9) M (2 di,j,k1 di,j,k2) k2 X k k1 di,j,k k2 k1 1, for i I, j J , k1, k2 Kd, and k1 k2, (10) M (pi,j,k1 1) di,j,k2 0, for i I, j J , k1 Kp, k2 Kd, and k1 k2, (11) X i I di,j,k 1, for j J , and k Kd, (12) X k Kd di,j,k K, for i I, and j J , (13) X k Kd wi,j,k xi,j, for i I, and j J , (14) X j J X k Kd wi,j,k 1, for i I, (15) X i I pi,j,k 1, j, k, (16) X k pi,j,k xi,j, for j J , and k Kp, (17) X j J xi,j 1, for i I, (18) 7 xi,j {0, 1}, for i I, and j J , (19) yk,l {0, 1}, for k Kp, and j J , (20) pi,j,k {0, 1}, for i I, j J , k Kp, (21) di,j,k {0, 1}, for i I, j J , k Kd, (22) wi,j,k [0, 1], for i I, j J , k Kd, (23) ts,p k , np k, for k Kp, (24) ts,d k , nd k, for k Kd. (25) The total inference time is denoted by tmax, which is min- imized in the objective function given in Eq. (1).\n\n--- Segment 18 ---\n(25) The total inference time is denoted by tmax, which is min- imized in the objective function given in Eq. (1). We let Kd {1, 2, } be the set of decode stages. For any decode stage k Kd, let ts,d k R and nd k R denote the start time and time length of the kth decode stage, respectively, and thus its end time is ts,d k nd k . Eq. (2) requires that the total inference time is greater than or equal to the end time of any decode stage. Let K Z be the total number of bins, prefill stages, and also decode stages. Any prefill stage should start after the end of its previous decode stage, suggested by Eq. (3). For any prefill stage k Kp, let np k R be the time length of the kth prefill stage. Thus, Eq. (4) suggests that, within the same bin, the decode stage should start after the end of prefill stage. We use L {1, 2, } to represent the set of all possible levels for a prefill stage, and the time length of a prefill stage depends on the level. Let yk,l {0, 1}, for k Kp and l L, be an indicator variable. Prefill stage k is at level l, if yk,l 1. Otherwise, the prefill stage is not at level l. We denote by T p l R the prefill time of level l, for l L, and thus the time length of a prefill stage is determined in Eq. (5). We use N cap l Z to denote the maximum token capacity of level l, for l L, and let pi,j,k {0, 1} denote assignment of prefill stage k to request i on client j for prefill phase, for i I, j J , and k Kp. The relationship between pi,j,k and yk,l is given in Eq. (6). Eq. (7) suggests that any prefill stage can only be at one level in L. Different than the prefill phase, the decode phase of a request can be served in multiple decode stages.\n\n--- Segment 19 ---\nEq. (7) suggests that any prefill stage can only be at one level in L. Different than the prefill phase, the decode phase of a request can be served in multiple decode stages. Thus, we introduce wi,j,k [0, 1] to represent the proportion of the decoding phase of request i executed in the kth decode stage on client j, for i I, j J , and k Kd. The time length of a decode stage is provided by Eq. (8). Let di,j,k {0, 1} be the assignment of decode stage k to request i on client j for decode phase, for i I, j J , and k Kd. Eqs. (9)-(11) together sets the rule that the decoding phase of a request must immediately follow the consecutive prefilling phase or its previous decoding phase. Eqs. (12)-(18) relate the assignment decisions among requests, clients, and bins. Eqs. (19)-(25) define the domain for each set of variables, respectively. Eqs (1)-(25) will hereafter be referred to as the original model . We attempted to solve the original model using com- mercial MIP solvers, such as Gurobi. However, we found it nearly impossible to solve such a large-scale problem directly. The number of requests, denoted by I , is around 1,000 in small cases, while in larger cases, it can be up to 10,000. The number of batch sizes or client numbers, denoted by J , can reach up to 200. The number of exclusive bins, denoted by K , often is on the same order of magnitude as I . To probe the solving cost, we solved a down-scaled toy case model with merely 100 requests and 20 clients, which took Gurobi more than 3,600 seconds to yield a near-optimal solution without fully closing the dual gap. Solving this problem in its original form is hence evidently impractical and cannot be solved in hours. Therefore, it is necessary to decompose this problem into stages and address it sequentially. IV. SOLUTION METHOD A. Method overview The original model, provided by Eqs. (1)-(25), demands a solution method capable of making decisions within 10 milliseconds in an asynchronous cycle, as the decoding time per client batch can be around 50 milliseconds.\n\n--- Segment 20 ---\nMethod overview The original model, provided by Eqs. (1)-(25), demands a solution method capable of making decisions within 10 milliseconds in an asynchronous cycle, as the decoding time per client batch can be around 50 milliseconds. However, the original model is a large-scale MIP model with over 100,000 integer decision variables and constraints, making it difficult to solve even within several hours. Hence, it is vital to develop an efficient solution method that provides the best possible outcomes within the required time frame. As illustrated in Fig. 5, we propose a hybrid offline-online method that structures the scheduling process for large model inference into two main methods: offline requests assignment and online scheduling. Each method involves a subset of decision variables of the original model and provides timely solutions at each stage. In the figure, we illustrate how the offline-online information, as well as the decision making given by the scheduling models, is obtained and shared in the system. Offline Requests Scheduling: In this method, a prede- termined batch size of clients determines the number of parallel requests. Each client is allocated a balanced number of requests, resulting in an equitable task distribution. This method considers the assignment decisions in the original model as described in constraints (2), (12), and (15) (16). We isolate this part of the model to demonstrate offline training scenarios, such as RLHF (Reinforcement Learning with Hu- man Feedback) training. In this task, requests are typically given and known in advance. Users can manually send their request prompts to the LLMs and wait to receive the outputs. These tasks can implement offline request assignment methods to achieve better throughput. However, for most scenarios such us user using GPT, using the offline method is still limited since solving the MIP model usually takes 10 minutes or more, which cannot meet the rapid iteration requirements of the LLM online decision-making process. Therefore, it is necessary to develop an online method to fill this gap. Online Scheduling: The online scheduling process com- prises two major parts corresponding to two types of online decisions. The first part, online requests scheduling, deter- mines which requests are scheduled in the upcoming bin and identifies the next client to serve once a previous request is completed. The second part, online iteration scheduling, decides when to conclude the current decoding bin and start a preemptive prefilling bin to enhance overall utilization rates.\n\n--- Segment 21 ---\nThe first part, online requests scheduling, deter- mines which requests are scheduled in the upcoming bin and identifies the next client to serve once a previous request is completed. The second part, online iteration scheduling, decides when to conclude the current decoding bin and start a preemptive prefilling bin to enhance overall utilization rates. In the online requests scheduling part, heuristic methods are employed to determine the optimal order for processing requests in real-time. This approach considers factors such as task priority, current system load, and anticipated resource 8 Offline information Online information Execution Inference Service Profiler Simulator Input requests Prefill Queue Decode Queue Original MIP Model Offline requests scheduling Online requests scheduling Online iteration scheduling LLM workers Decision Variables Fig. 5: Illustration of Solution Method availability. By effectively prioritizing requests, the system can minimize waiting times and maximize throughput under dynamic operational conditions. This method emphasizes the implementation of the relaxed solutions provided by job as- signment and illustrates the constraints (3) (11) in the original model. The online iteration scheduling part aims to minimize idle time on machines by strategically allocating computational re- sources. By dynamically adjusting prefilling and decoding task priorities based on real-time feedback and system constraints, this method enhances overall system efficiency and responsive- ness. This proactive scheduling approach minimizes machine idle time and optimizes the utilization of processing resources, thereby improving the overall performance of large language model inference tasks. This method underscores iteration- based optimization and considers the constraints (14) (15) and (17) (18) in the original model. The overarching goal of this structured approach is to mini- mize the total inference time for a specific benchmark, thereby maximizing throughput. By integrating thorough data analy- sis, efficient task allocation, and adaptive online scheduling strategies, this scheduling solution optimizes the performance of LLM inference processes. This holistic approach not only enhances system efficiency but also supports scalability and reliability in handling complex computational tasks. B. Offline requests scheduling theoretical lower bound As previously introduced, we begin by examining the offline request assignment decisions within the original model, with a specific focus on the constraints described in (2), (12), and (15 16). This part of the model is isolated to demonstrate offline training scenarios, such as RLHF training. In this sce- nario, we tackle the Minimizing Makespan Bin Packing Prob- lem to efficiently address the workload balancing challenge.\n\n--- Segment 22 ---\nThis part of the model is isolated to demonstrate offline training scenarios, such as RLHF training. In this sce- nario, we tackle the Minimizing Makespan Bin Packing Prob- lem to efficiently address the workload balancing challenge. We assume that the output length is predetermined, and that prefill decode stages do not conflict during problem-solving. Nevertheless, in practical applications and simulations used to evaluate performance, we adhere to these constraints by allocating workload to clients without affecting the uncertainty of output length. In the offline model outlined in Eqs. (26) (30), we introduce a new parameter, denoted by Ti R for i I, representing the estimated decode completion time for request i. We also introduce a new decision variable, denoted by tj R for j J , to indicate the total decoding time for client j. min max j J tj (26) s.t. X j J xi,j 1, for i I, (27) X i I xi,jTi tj, for j J , (28) xi,j {0, 1}, for i I, and j J , (29) tmax, tj R , for j J . (30) The offline model also provides a method to calculate the theoretical lower bound for a given set of requests, I. In this method, we assume that prefill and decode phases for all the requests can be separated into two groups, and we calculate the optimal inference time for each group. Let tp R and td R represent the optimal total prefill and total decode times for all the requests in set I, respectively. The value of td maxj J tj is obtained from the objective function value from Eqs. (26) (30). Let L arg maxl L N cap l , and then the largest prefill time across all 9 levels in T p l is denoted by T p L. N cap L is the number of maximum number of tokens that can be processed in T p L. Then, tp can be calculated by the following equation. tp T p L P i I N p i N cap L . (31) It yields a tight theoretical lower bound T LB as follows. T LB tp td . (32) C. Online requests and iteration scheduling In this online part of the LLM inference optimization problem, two critical considerations arise.\n\n--- Segment 23 ---\nT LB tp td . (32) C. Online requests and iteration scheduling In this online part of the LLM inference optimization problem, two critical considerations arise. First, we need to determine which request to send to an available client once the previous request is completed. Second, when a round of decoding stage is finished, we must decide whether to send a preemptive prefill stage or continue this decode stage to the LLM workers for the subsequent time frame. The first issue presents an online scheduling problem, as illustrated by Eqs. (14) (15) and (16) (17). The primary decision in this context is whether to select a new request to override the original assignment in order to achieve better machine utilization. A sorting and online preemptive method is illustrated in Algorithm 1. This online algorithm first selects the future available requests, denoted as Ij, for client j J . The set Ij is sorted by N p i N d i , that is, N p i1 N d i1 N p i2 N d i2 i1 i2 Ij. Then, for each client, the algorithm calculates future requests and counts the expected remaining tokens remain token(j) for j J to be processed. Idle clients then greedily select the longest request from busy clients to process. This algorithm utilizes offline information on request assignment to provide timely online assignment decisions. Algorithm 1 Sorting and Online Preemptive Method Require: {Ij {i xij 1}, j J } for client j in clients J do if queue for client j is empty and Ij then pop Ij to client j remain token(j) remain token(j) (N p i N d i ) else if max(remain token(j)) 0 then pop arg max(remain token) to client j remain token(j) remain token(j) (N p i N d i ) end if end for Continuing from the previous discussion, the second prob- lem involves a sequential decision-making process, as outlined by Eqs. (2) (11). The main challenge here is to deliver timely and efficient decisions in real time. As previously mentioned, each round of decoding takes approximately 50 milliseconds. Thus, it is essential to ensure that decisions are made within 10 milliseconds to sustain system efficiency. To achieve this, we employ the following method to integrate quick decision- making into the process.\n\n--- Segment 24 ---\nThus, it is essential to ensure that decisions are made within 10 milliseconds to sustain system efficiency. To achieve this, we employ the following method to integrate quick decision- making into the process. This aspect of decision-making corresponds to the following problem. min tmax s.t. tmax ts,d k nd k, for k Kd, (33) ts p,k (ts d,k 1 nd k 1) 0, for k 2, ..., K, (34) ts,d k (ts,p k np k) 0, for k Kp, (35) np k X l L T p l yk,l, for k Kp, (36) X i I,j J N p i pi,j,k X l L N cap l yk,l, for k Kp, (37) X l L yk,l 1, for k Kp, (38) nd k T d X i N d i wi,j,k, for j J , and k Kd, (39) di,j,k pi,j,k 0, for i I, j J , and k K. (40) By combining Eqs. (33) and (35), we derive that tmax maxk K ts,p k np k nd k . In the context of online decision- making, the start time ts,p k is typically influenced by the completion time of preceding tasks. The primary objective is to minimize the total time cost of prefill and decode stages. Consequently, we establish the following equation by integrating the calculations of np k and nd k from Eqs. (36) and (39). min tmax max k K ts,p k X l L T p l yk,l T d X i I,j J N d i wi,j,k . (41) In this problem, the time cost is divided into two components. The cost for adding a prefill task at any point is given by tmax yk,l X l L T p l , (42) and the cost for adding a decode task at the decision-making moment is expressed by tmax wi,j,k T d X i I N d i .\n\n--- Segment 25 ---\n(41) In this problem, the time cost is divided into two components. The cost for adding a prefill task at any point is given by tmax yk,l X l L T p l , (42) and the cost for adding a decode task at the decision-making moment is expressed by tmax wi,j,k T d X i I N d i . (43) Thus, our heuristic method for deciding whether to dispatch a prefill or decode stage to the LLM worker involves comparing the prefill cost Cp P l T p l with the waited decode time Cd T d P i I,j J N d i wi,j,k. If Cp Cd, the algorithm advises continuing with a round of the decode task and waiting for additional prefill tasks; otherwise, the algorithm recommends executing a round of the prefill task. V. NUMERICAL EXPERIMENT A. Experiment settings and baseline Before the model is solved by our hybrid method, extensive analysis is conducted to evaluate the time taken by the decode and prefill stages on hardware. This analysis provides crucial insights into the computational demands and performance characteristics of each stage. By quantifying these metrics, 10 TABLE III: Experiment Settings Parameter Number Brief Description I 1319 The GSM8K dataset J 200 Due to hardware memory limit E(Np i ) 68.43 The GSM8K dataset input E(Nd i ) 344.83 The Llama 65B output T p 0.13 ms token The hardware performance on prefilling T d 0.21 ms token The hardware performance on decoding such as processing times and resource utilization, the data analysis establishes a solid foundation of empirical data. The data serve as reliable support for subsequent decision-making in optimizing scheduling strategies. The basic experiment setting is given in TABLE III. We demonstrate our improvements by utilizing the GSM8K dataset [34], a comprehensive collection of mathematical word problems specifically designed to assess the problem-solving and reasoning capabilities of language models. This dataset serves as a benchmark for evaluating both arithmetic and log- ical reasoning skills, essential attributes for advanced language models. Each problem in the dataset is intricately constructed to simulate real-world situations involving numerical rela- tionships, necessitating that models comprehend the problem contextually and perform accurate calculations to derive the correct solution.\n\n--- Segment 26 ---\nThis dataset serves as a benchmark for evaluating both arithmetic and log- ical reasoning skills, essential attributes for advanced language models. Each problem in the dataset is intricately constructed to simulate real-world situations involving numerical rela- tionships, necessitating that models comprehend the problem contextually and perform accurate calculations to derive the correct solution. The GSM8K dataset comprises 1,319 unique problems as input requests, with an average input length of 68.43 tokens and a standard deviation of 25.04 tokens. For our experiments, we selected the LLaMA-65B language model due to its open- source nature and wide accessibility, making it a suitable candidate for academic and reproducible research. In our tests, the LLaMA-65B model generated responses averaging 344.83 tokens, with a standard deviation of 187.99 tokens. To ensure consistency and focus on quality responses, we constrained the maximum output length to 512 tokens during testing. Our computational setup is characterized by a robust hard- ware configuration, consisting of eight Ascend processing units, each equipped with a maximum memory capacity of 64 GB. This formidable hardware infrastructure is essential for facilitating the efficient processing and testing necessary for our experiments. Additionally, we have assessed the KV cache usage for input in this experiment, establishing baseline settings that are also utilized in practical applications. The current hardware, along with the LLM employed, imposes a memory constraint of 1024 blocks of KV cache. Each block can accommodate a maximum of 128 tokens. For the GSM-8k benchmark, the combined maximum input and output for each request requires five blocks. Consequently, this configuration limits us to a maximum of approximately 200 clients running concurrently, calculated by the expression 1024 5 200. In our experimental setup, we conduct an estimation of the operation time required for prefill and decode stages using over 400 data groups. We find that both prefill and decode times exhibit a linear relationship with the number of tokens involved. Specifically, the prefill time can be calculated as 0.13 milliseconds per token, plus a fixed overhead of 25 millisec- onds. For the decode process, the time required for each batch 0 25000 50000 75000 100000 125000 150000 175000 200000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 80.2 . Total inference time: 201.00 seconds. Fig.\n\n--- Segment 27 ---\nTotal inference time: 201.00 seconds. Fig. 6: Result Gantt: Baseline of clients can be estimated as 0.21 milliseconds per token, with an additional fixed overhead of 29 milliseconds. For instance, when processing a parallel batched decode stage involving 200 clients, where each client produces one token per round, the operation would take approximately 200 0.21 29 71 milliseconds. In the case of prefill stages, if a batch consists of inputs totaling 5,000 tokens, the estimated time required would be 5000 0.13 25 675 milliseconds. We present a Gantt chart in Fig. 6, generated from an experiment using real-world data and open source LLM, to illustrate the current state of online inference services with- out the implementation of our proposed method. This chart demonstrates that, in practical scenarios, a significant number of idle periods, or bubbles , occur when no scheduling strategy is employed. Furthermore, in offline scenarios, if the workload among clients is not evenly distributed, substantial machine idle time is observed after the early completion of some client s tasks. Our analysis of this Gantt chart reveals that the overall machine utilization rate is only 80.2 . B. Offline request scheduling result This offline request scheduling model given by Eqs. (26) (30) can be solved using open-source solver SCIP. Due to significantly reduced complexity, optimal solutions can be achieved within 20 minutes comparing to original problem which is not possible to be solved within hours. Although this offline model only addresses workload balancing using estimations of output length, its performance surpasses that of the original version. As illustrated in Fig. 7, the system shows a significant reduction of idle times, and machine utilization is enhanced to 85.5 . Comparing to the baseline method, this method provides a more balanced request assignment across clients and reduce bubbles . The total inference time can be reduced from 201.00 seconds to 197.08 seconds. Since solving the model still takes relatively long time, we list this method as optional and suggest practitioners use the offline model in typical scenarios such as RLHF training. 11 0 25000 50000 75000 100000 125000 150000 175000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 85.5 . Total inference time: 197.08 seconds. Fig.\n\n--- Segment 28 ---\nTotal inference time: 197.08 seconds. Fig. 7: Result Gantt: Offline Request Scheduling C. Online scheduling results Incorporating online requests and iteration scheduling meth- ods, as depicted in Fig. 9, results in a marked improvement in total inference time, showing reductions 190.58s compared to 201.00s in the baseline scenario. Additionally, machine utiliza- tion is enhanced to 89.06 . Comparing to offline scheduling method, these two online methods do not require additional computing and can be used for current online inference. In Fig. 8, we present the results obtained using only the online scheduling method, without employing the offline scheduling method. As shown, compared to the baseline, the utilization rate improves to 86.19 , and the total inference time decreases to 193.33 seconds. These results demonstrate that the online method performs well even in the absence of prior knowledge about requests. This scenario is common in the area of LLM inference. We also calculate the theoretical lower bound using Eq. (32). In the specified numerical case utilizing GSM8K, the theoretical bound is 180 seconds, in which T p 13 seconds and T d 167 seconds. In this scenario, we reduce the total inference time from 201.00 seconds in the baseline to 190.08 seconds with the hybrid online-offline method. The gap to the optimal value is thus reduced from 201 180 21 seconds to 190 180 10 seconds, representing a reduction of 52.4 in this primal dual gap. To better demonstrate the performance of our online scheduling methods, we present a numerical experiment in- volving 100 cases in Figs. 10 and 11. These cases are randomly generated with the input and output length distributions shown in TABLE III. As illustrated in the figures, despite some vari- ations across the 100 cases, our hybrid offline-online method consistently outperforms in both utilization and generation speed. The unit for generation speed is tokens per second, indicating how many tokens the LLM can generate each sec- ond. On average, our method achieves an 8.0 improvement in utilization and an increase of 100.63 tokens per second in generation speed. 0 25000 50000 75000 100000 125000 150000 175000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 86.19 . Total inference time: 193.33 seconds. Fig.\n\n--- Segment 29 ---\nTotal inference time: 193.33 seconds. Fig. 8: Result Gantt: Online only Scheduling 0 25000 50000 75000 100000 125000 150000 175000 Time(ms) 0 25 50 75 100 125 150 175 200 Client Total Inference Time prefill decode Utilization rate: 89.06 . Total inference time: 190.58 seconds. Fig. 9: Result Gantt: Offline Online Scheduling Fig. 10: Utilization rate with 100 cases 12 Fig. 11: Generate speed with 100 cases VI. CONCLUSION AND FUTURE WORK In this paper, we study the inference optimization problem in the service system when deploying LLMs. To enhance the system throughput and better utilize the hardware, we formulate an MIP model to describe this inference optimiza- tion problem. To the best of our knowledge, this is the first formulation of the problem from a scheduling perspective. To tackle the complex and high real-time demands of LLM inference, we introduce a hybrid offline-online method. In the offline method, we demonstrate how large-scale infer- ence systems can be improved using a Minimizing Makespan Bin Packing Problem and how a theoretical lower bound can be provided. In the online request scheduling and iteration scheduling methods, the solution time efficiency is crucial. We propose a sorting and online preemptive method to more effectively utilize clients that finish early. Then, we focus on the iteration scheduling component of the original model and employ a Lagrangian method to compare the costs of adding a prefill stage versus a decode stage at each iteration. We provide a time efficient heuristic method to determine when to insert a prefill task and interrupt ongoing decoding tasks. In real-world experiments, we deploy the LlaMA-65B LLM model and infer the GSM 8K dataset, which includes 1,319 unique math problems. Our offline model increases machine utilization rates from a baseline of 80.2 to 85.5 , and reduces the total inference time from 201 seconds to 197 seconds. Utilizing the online scheduling methods, the system utilization rate can be further increased to 89.1 , and the total inference time for the dataset can be reduced to 191 seconds. As demonstrated, if all our methods are implemented, system throughput can be improved by 5.46 , and hardware utilization can increase by 11.0 .\n\n--- Segment 30 ---\nUtilizing the online scheduling methods, the system utilization rate can be further increased to 89.1 , and the total inference time for the dataset can be reduced to 191 seconds. As demonstrated, if all our methods are implemented, system throughput can be improved by 5.46 , and hardware utilization can increase by 11.0 . A 100-cases study shows that our method consistently outperforms the baseline method and improves the utilization rate by 8.0 on average. The future directions of this research can be extended along three key aspects: Stochastic Model Efficient Solution Method: The orig- inal MIP model we proposed is a deterministic equiva- lence formulation. While solving this model is already computationally challenging, developing a stochastic pro- gramming model could further enhance its accuracy by better accounting for uncertainties. Additionally, more efficient solution methods for tackling the original MIP model are needed to meet the millisecond-level real-time requirements of the decision-making process. Reinforcement Learning on Iteration Scheduling: the cur- rent iteration scheduling approach relies on a heuristic online method. Notably, the decision-making process in this method involves choosing between two options: prefill or decode. Since online state variables such as prefill task waiting time, the number of decoding clients, expected decoding time, and expected prefill time are relatively easy to derive, a simple reinforcement learning (RL) model could be trained to assist the scheduler in making decisions dynamically. Online Hardware Utilization Method: We observed dur- ing hardware experiments that the system s hardware is often underutilized when a static number of clients is employed, due to stochastic variations in the output length. In scenarios where dynamic and continuous batch- ing methods are applicable, investigating online decision- making for hardware utilization could further optimize performance. Specifically, determining the optimal num- ber of clients that can be allocated concurrently to the system at any given time could help enhance resource utilization and overall efficiency. REFERENCES [1] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, Efficient memory management for large language model serving with pagedattention, in Proceedings of the 29th Symposium on Operating Systems Principles, 2023, pp. 611 626. [2] G.-I.\n\n--- Segment 31 ---\n611 626. [2] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, Orca: A distributed serving system for {Transformer-Based} generative models, in 16th USENIX Symposium on Operating Systems Design and Imple- mentation (OSDI 22), 2022, pp. 521 538. [3] NVIDIA, NVIDIA Announces Financial Results for Fourth Quarter and Fiscal 2024, details 2024 NVIDIA-Announces-Financial-Results-for-Fourth-Quarter- and-Fiscal-2024 , Feb. 2024, [Online; accessed 09-01-2025]. [4] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B. S. Gulavani, A. Tumanov, and R. Ramjee, Taming throughput-latency tradeoff in llm inference with sarathi-serve, arXiv preprint arXiv:2403.02310, 2024. [5] B. Sun, Z. Huang, H. Zhao, W. Xiao, X. Zhang, Y. Li, and W. Lin, Llumnix: Dynamic scheduling for large language model serving, arXiv preprint arXiv:2406.03243, 2024. [6] W. Lee, J. Lee, J. Seo, and J. Sim, {InfiniGen}: Efficient generative inference of large language models with dynamic {KV} cache manage- ment, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 155 172. [7] B. Wu, R. Zhu, Z. Zhang, P. Sun, X. Liu, and X. Jin, {dLoRA}: Dynamically orchestrating requests and adapters for {LoRA}{LLM} serving, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 911 927.\n\n--- Segment 32 ---\n[7] B. Wu, R. Zhu, Z. Zhang, P. Sun, X. Liu, and X. Jin, {dLoRA}: Dynamically orchestrating requests and adapters for {LoRA}{LLM} serving, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 911 927. [8] Y. Sheng, S. Cao, D. Li, B. Zhu, Z. Li, D. Zhuo, J. E. Gonzalez, and I. Stoica, Fairness in serving large language models, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 965 988. [9] B. Wu, Y. Zhong, Z. Zhang, S. Liu, F. Liu, Y. Sun, G. Huang, X. Liu, and X. Jin, Fast distributed inference serving for large language models, arXiv preprint arXiv:2305.05920, 2023. [10] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang, {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving, in 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), 2024, pp. 193 210. 13 [11] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, Quantization and training of neural networks for efficient integer-arithmetic-only inference, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 2704 2713. [12] R. Child, S. Gray, A. Radford, and I. Sutskever, Generating long sequences with sparse transformers, arXiv preprint arXiv:1904.10509, 2019. [13] G. Bai, Y. Li, C. Ling, K. Kim, and L. Zhao, Sparsellm: Towards global pruning of pre-trained language models, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.\n\n--- Segment 33 ---\n[12] R. Child, S. Gray, A. Radford, and I. Sutskever, Generating long sequences with sparse transformers, arXiv preprint arXiv:1904.10509, 2019. [13] G. Bai, Y. Li, C. Ling, K. Kim, and L. Zhao, Sparsellm: Towards global pruning of pre-trained language models, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [14] G. Hinton, Distilling the knowledge in a neural network, arXiv preprint arXiv:1503.02531, 2015. [15] N. Shazeer, Fast transformer decoding: One write-head is all you need, arXiv preprint arXiv:1911.02150, 2019. [16] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr on, and S. Sanghai, Gqa: Training generalized multi-query transformer models from multi-head checkpoints, arXiv preprint arXiv:2305.13245, 2023. [17] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro et al., Efficient large-scale language model training on gpu clusters using megatron-lm, in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2021, pp. 1 15. [18] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu et al., Gpipe: Easy scaling with micro-batch pipeline parallelism, proceeding of Computer Science Computer Vision and Pattern Recognition, 2019.\n\n--- Segment 34 ---\n1 15. [18] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu et al., Gpipe: Easy scaling with micro-batch pipeline parallelism, proceeding of Computer Science Computer Vision and Pattern Recognition, 2019. [19] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan- zaro, Megatron-lm: Training multi-billion parameter language models using model parallelism, arXiv preprint arXiv:1909.08053, 2019. [20] V. A. Korthikanti, J. Casper, S. Lym, L. McAfee, M. Andersch, M. Shoeybi, and B. Catanzaro, Reducing activation recomputation in large transformer models, Proceedings of Machine Learning and Systems, vol. 5, pp. 341 353, 2023. [21] H. Liu, M. Zaharia, and P. Abbeel, Ring attention with blockwise transformers for near-infinite context, arXiv preprint arXiv:2310.01889, 2023. [22] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R e, Flashattention: Fast and memory-efficient exact attention with io-awareness, Advances in Neural Information Processing Systems, vol. 35, pp. 16 344 16 359, 2022. [23] Y. Leviathan, M. Kalman, and Y. Matias, Fast inference from transform- ers via speculative decoding, in International Conference on Machine Learning. PMLR, 2023, pp. 19 274 19 286. [24] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper, Accelerating large language model decoding with speculative sam- pling, arXiv preprint arXiv:2302.01318, 2023.\n\n--- Segment 35 ---\n[24] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and J. Jumper, Accelerating large language model decoding with speculative sam- pling, arXiv preprint arXiv:2302.01318, 2023. [25] P. Patel, E. Choukse, C. Zhang, A. Shah, I. Goiri, S. Maleki, and R. Bianchini, Splitwise: Efficient generative llm inference using phase splitting, in 2024 ACM IEEE 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 2024, pp. 118 132. [26] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng, C. Wang, S. Wang, Y. Bao et al., Inference without interference: Disaggre- gate llm inference for mixed downstream workloads, arXiv preprint arXiv:2401.11181, 2024. [27] Z. Zhou, X. Ning, K. Hong, T. Fu, J. Xu, S. Li, Y. Lou, L. Wang, Z. Yuan, X. Li et al., A survey on efficient inference for large language models, arXiv preprint arXiv:2404.14294, 2024. [28] R. Qin, Z. Li, W. He, M. Zhang, Y. Wu, W. Zheng, and X. Xu, Moon- cake: A kvcache-centric disaggregated architecture for llm serving, arXiv preprint arXiv:2407.00079, 2024. [29] F. Wang, S. Fathizadan, F. Ju, K. Rowe, and N. Hofmann, Print surface thermal modeling and layer time control for large-scale additive manu- facturing, IEEE Transactions on automation science and engineering, vol. 18, no. 1, pp. 244 254, 2020. [30] B. Pang, X. Xie, Y. Song, and L. Luo, Surgery scheduling under case cancellation and surgery duration uncertainty, IEEE Transactions on Automation Science and Engineering, vol. 16, no. 1, pp. 74 86, 2018.\n\n--- Segment 36 ---\n1, pp. 74 86, 2018. [31] S. A. Erdogan, A. Gose, and B. T. Denton, Online appointment sequencing and scheduling, IIE Transactions, vol. 47, no. 11, pp. 1267 1286, 2015. [32] B. Pang, X. Xie, F. Ju, and J. Pipe, A dynamic sequential decision- making model on mri real-time scheduling with simulation-based opti- mization, Health Care Management Science, vol. 25, no. 3, pp. 426 440, 2022. [33] K. Lee, F. Zheng, and M. L. Pinedo, Online scheduling of ordered flow shops, European Journal of Operational Research, vol. 272, no. 1, pp. 50 60, 2019. [34] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., Training verifiers to solve math word problems, arXiv preprint arXiv:2110.14168, 2021. Bowen Pang received his Bachelor s degree and Ph.D. degree from the Department of Industrial Engineering at Tsinghua University, Beijing, China, in 2016 and 2022, respectively. He is currently a researcher at Noah s Ark Lab, Huawei Technology. His research interests include modeling, analyzing, and solving problems in engineering systems, with applications in healthcare, supply chain, manufactur- ing, and artificial intelligence. He has been a member of IEEE, IISE, and INFORMS. Please feel free to contact his email if you are interested in LLM inference optimization area. Kai Li received the bachelor s degree from YingCai Honors College, University of Electronic Science and Technology of China, Chengdu, China, in 2015, and the Ph.D. degree from the Department of Com- puter Science and Engineering, Shanghai Jiao Tong University, Shanghai, China, in 2021. He is currently a researcher at Noah s Ark Lab, Huawei Technology. His research interests include game theory, rein- forcement learning, and large language models. Ruifeng She received his B.S.\n\n--- Segment 37 ---\nHis research interests include game theory, rein- forcement learning, and large language models. Ruifeng She received his B.S. and Ph.D. degrees from the Department of Civil Engineering of the University of Illinois at Urbana-Champaign, USA, in 2018 and 2023, respectively. He is currently a researcher at Noah s Ark Lab, Huawei Technology. His research interests include optimization of com- plex systems, reinforcement learning, and the inte- gration of operations research and machine learning. Feifan Wang received the bachelor s degree from the Department of Industrial Engineering, Zhejiang University of Technology, Hangzhou, China, in 2013, the master s degree from the Department of Industrial and Systems Engineering, Zhejiang Uni- versity, Hangzhou, China, in 2016, and the Ph.D. degree from the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA, in 2021. He is cur- rently an Assistant Professor with the Department of Industrial Engineering at Tsinghua University, Beijing, China. His research focuses on modeling, analysis, optimization, and control of complex systems, with applications in healthcare delivery systems and production systems. He is a member of IEEE, IISE, and INFORMS. He was a recipient of multiple awards, including the Design and Manufacturing Best Paper Award from the IISE Transactions, the Best Student Paper Award from IEEE CASE, and the Dean s Dissertation Award from ASU. He has twice been a finalist for the Best Paper Award on Healthcare Automation from IEEE CASE.\n\n