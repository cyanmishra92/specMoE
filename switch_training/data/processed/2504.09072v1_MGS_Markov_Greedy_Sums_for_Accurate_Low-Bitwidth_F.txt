=== ORIGINAL PDF: 2504.09072v1_MGS_Markov_Greedy_Sums_for_Accurate_Low-Bitwidth_F.pdf ===\n\nRaw text length: 53240 characters\nCleaned text length: 51539 characters\nNumber of segments: 34\n\n=== CLEANED TEXT ===\n\nMGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation Vikas Natesh Harvard University Cambridge, MA, USA H.T. Kung Harvard University Cambridge, MA, USA David Kong Harvard University Cambridge, MA, USA Abstract We offer a novel approach, MGS (Markov Greedy Sums), to im- prove the accuracy of low-bitwidth floating-point dot products in neural network computations. In conventional 32-bit floating- point summation, adding values with different exponents may lead to loss of precision in the mantissa of the smaller term, which is right-shifted to align with the larger term s exponent. Such shift- ing (a.k.a. swamping ) is a significant source of numerical errors in accumulation when implementing low-bitwidth dot products (e.g., 8-bit floating point) as the mantissa has a small number of bits. We avoid most swamping errors by arranging the terms in dot product summation based on their exponents and summing the mantissas without overflowing the low-bitwidth accumulator. We design, analyze, and implement the algorithm to minimize 8-bit floating point error at inference time for several neural networks. In contrast to traditional sequential summation, our method has signif- icantly lowered numerical errors, achieving classification accuracy on par with high-precision floating-point baselines for multiple image classification tasks. Our dMAC hardware units can reduce power consumption by up to 34.1 relative to conventional MAC units. 1 Introduction Quantization has become a ubiquitous optimization for compress- ing deep neural networks (DNNs) on both low-power edge devices [20, 29, 35, 37] as well as large-scale training and inference sys- tems made up of many GPUs [34]. Low-power devices for tinyML typically have small local memories [5] and often lack support for efficient floating-point computation [3, 37]. Hence, integer quanti- zation is, by default, necessary on such systems, and most tinyML models are quantized to 8 bits or less. Meanwhile, large generative AI workloads push GPU-based training and inference clusters to the limits of available memory, bandwidth, and computation power. To this end, low-bitwidth formats such as brain float-16 (bfloat16) [43], block floating point (BFP) [44], and 8-bit floating-point (FP8) have been implemented in various hardware [1, 2]. Such formats have been successful in reducing memory footprint, memory accesses, computation time, and power consumption [1, 2, 31] When performing quantized matrix multiplications, dot prod- ucts are typically accumulated into wider registers. For instance, partial products in FP8 may be accumulated in FP16 or FP32 to ensure numerical accuracy. Reducing accumulator bitwidth can reduce bandwidth and energy usage while increasing inference throughput [11 13, 33]. However, if the partial product sum over- flows the accumulator, its value may be clipped to a finite range. This introduces numerical errors into the final matrix result that degrade model accuracy and limit how much one can reduce the -3 4 14 2 -12 4 -4 1 -7 2 4 -4 s1 15 s2 -9 s3 -5 s1 s2 s3 (a) Markov Greedy Sum (MGS) . . . Exp Mant (b) Using MGS In FP8 Accumulation Exponent DEMUX Normalize Round . . . Narrow Accum Wide Accum Left Shifter Figure 1: (a) Example of Markov Greedy Sums (MGS). In (a), we sum 12 integers into a narrow accumulator (green box) until the sum 𝑠𝑖overflows the range [-15, 15]. Then, we accu- mulate 𝑠𝑖into a wider accumulator (red box). The underlined red values are those that would have caused an overflow of the narrow accumulator, noting that 15 2 15 and -9-7 -15. In (b), we accumulate FP8 values by separating them by their exponents into 16 groups, summing the mantissas into separate narrow accumulators indexed by the exponent, and using a wide accumulator upon overflow. MGS amortizes the cost of aligning (shifting) FP8 mantissas over many sums. accumulator bitwidth. In addition, there is the swamping problem [23] that causes loss of precision due to mantissa shift when floating point numbers need to align their exponents before summation. Prior works have attempted to reduce overflow in narrow ac- cumulators by regularizing the loss function [33] or by control- ling weight magnitude during training [11 13]. While these ap- proaches succeed in reducing overflows, they impose restrictive constraints on weights during training that may reduce model ac- curacy [11, 12, 33]. Moreover, existing networks require expensive re-training to satisfy such constraints. Other works reorder dot product summations to avoid the majority of overflows when using narrow accumulators [32]. However, reordering requires additional 1 arXiv:2504.09072v1 [cs.AR] 12 Apr 2025 Vikas Natesh, H.T. Kung, and David Kong sort permute operations as well as memory for temporary storage and is difficult to optimize on existing hardware. We propose Markov Greedy Summation (MGS), a novel approach to enable low-precision accumulation in neural network dot prod- ucts without the need for retraining or summation reordering. We analyze overflows during neural network inference and model the value of the partial sum in dot products as a Markov process to derive the expected dot product length without overflow. Our key insight is that based on the statistical properties of weight and activation distributions, we can sum many partial products in re- duced precision before overflow occurs. MGS is greedy in the sense that it uses a narrow accumulator to accumulate as many values as possible while falling back on a wider accumulator when the rare overflow occurs. Leveraging this insight, we design dual-multiply- accumulate (dMAC) hardware units that use narrow accumulators for the majority of sums. Our method uses a narrower average accumulator bitwidth compared to prior works when performing DNN computations. Our dMAC units consume up to 34.1 less energy than conventional integer and floating-point MACs that use wide accumulators for all summations. Figure 1 provides an overview of MGS applied to integer and FP8 summation. The novel contributions of this paper are: Analysis of dot product overflows in integer and floating- point quantized neural networks (Section 3 ). Dual-MAC hardware architecture (dMAC) and algorithm for avoiding overflows when using narrow accumulators in integer and FP8 dot products (Sections and 4 and 5). Evaluation of the resulting methods regarding model accu- racy and accumulator compression for classification tasks. We emulate our integer and FP8 dMAC units on both CPU and GPU hardware and evaluate task accuracy and accu- mulator compression for multiple DNNs (Section 6). Energy consumption and area evaluation of dMAC units compared to conventional integer and floating point MACs when implemented in a 7nm process node. (Section 6) 2 Background We present background on both integer and floating-point quanti- zation of DNNs and some prior work on avoiding overflow during DNN execution. 2.1 Integer Quantization We consider the uniform quantization of weight and activation ma- trices per-tensor from FP32 to 𝑏-bit signed values [25]. The floating- point values in a matrix 𝑀have a range 𝑅 𝑚𝑎𝑥(𝑋) 𝑚𝑖𝑛(𝑋). To map values in 𝑀to integers in [0, 2𝑏 1], we partition 𝑅into 2𝑏 1 uniform intervals of length 𝑠𝑥 𝑅 2𝑏 1, also called the scale factor. For example, we can map a FP32 activation 𝑥to a value 𝑥𝑞in [0, 2𝑏 1] using the equation 𝑥𝑞 𝑟𝑜𝑢𝑛𝑑( 𝑥𝑓 𝑠𝑥). If the range is asymmetric around zero, we shift 𝑥𝑞by an offset 𝑜𝑥 2𝑏 1 𝑟𝑜𝑢𝑛𝑑( 𝑚𝑖𝑛(𝑋) 𝑠𝑥 ) into the range [ 2𝑏 1, 2𝑏 1 1], guaranteeing that the FP32 value for 0 maps to an integer. We can obtain the approximate FP32 representation of a quantized activa- tion𝑥𝑞by reversing the effect of the scale and offset via the equation 𝑥 𝑠𝑥(𝑥𝑞 𝑜𝑥). Quantized dot products are then performed using the FP32 approximations. 𝑠𝑧(𝑧 𝑜𝑧) 𝐾 𝑖 1 𝑠𝑤(𝑤𝑞 𝑖 𝑜𝑤)𝑠𝑥(𝑥𝑞 𝑖 𝑜𝑥) where 𝑠𝑤, 𝑜𝑤, 𝑠𝑧, and 𝑜𝑧represent the quantization parameters of weights 𝑤and output activations 𝑧. Floating point scale factors are factored out and normalized to an integer representation, while weights are typically symmetric around zero with 𝑜𝑤 0 [18, 25, 28, 41]. As a result, several terms under the summation disappear, and the majority of computation arises from the integer dot product 𝑧 Í𝐾 𝑖 1 𝑤𝑞 𝑖𝑥𝑞 𝑖. When FP32 weights and activations are quantized to low-precision (e.g., 8-bit), the computation cost of multiplications 𝑤𝑞 𝑖𝑥𝑞 𝑖decreases significantly. However, the compute bottleneck transitions to the 𝐾dot product summations, as these accumulations are typically executed in higher precision, such as 32-bit, to avoid overflow of the accumulator. For example, assume we accumulate using a 𝑝-bit register where each partial product 𝑤𝑞 𝑖𝑥𝑞 𝑖is 2𝑏-bits and 𝑝 2𝑏. This leaves 𝑝 2𝑏bits leftover for precision during accumulation. Hence, the dot product overflows when 𝐾 2𝑝 2𝑏. However, if we use a narrow accumulator 𝑝 2𝑏, overflow may occur during any of the 𝐾partial sums, leading to inaccurate dot product and poor model accuracy. Previous works enable the use of narrow accumulators in DNN computations by retraining the network to reduce partial sum mag- nitude [11, 12, 33] or algorithmically avoiding most overflows [32]. In practice, ML frameworks for quantized DNNs avoid overflow by either using high-precision accumulators (e.g., 32-64 bits) or clipping partial results into a finite range (saturation arithmetic) as they are accumulated [4, 6, 19]. Clipping is cheap to implement in hardware or software, allowing for a modest reduction in accu- mulator precision, e.g., from 32 to 16 bits. However, for narrower bitwidths ( 16), clipping severely degrades numerical accuracy and task performance [11, 32]. 000 0101 1000 0101 1111 0001 1000 0101 1111 0001 1000 0101 0000 0101 1000 0101 0000 0101 1111 Swamped Bits (a) Prepend Leading 1 (b) Subtract Exponents (c) Align Mantissa (Shift) (d) Sum Mantissas 1 111 0001 1 A B 4 (shift amt.) _ 1000 0101 -0.250000 -0.029297 -0.25000 -0.28125 Sign Exponent Mantissa -0.28125 Figure 2: An example of mantissa bit swamping when adding two E4M3 values with different exponents, 𝐴 0.25 and 𝐵 0.029297, while using a narrow 4-bit accumulator. The exponent bias in E4M3 is 7. A s exponent of 5 is larger than B s exponent 1 (b), causing B s mantissa to be shifted left by 5-1 4 bits (c). Since the entire mantissa shifts out, B is treated as zero, and the final result is 0.25, differing from the closest FP8 result of -0.28125 (d). 2 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation 2.2 Floating-Point Quantization FP32 DNN weights and activations may be quantized to lower- precision floating-point formats such as bfloat16, BFP, FP16, or FP8. In particular, FP8 formats for both inference and training have been developed and implemented on several commercial AI accelerators, such as Nvidia H100 GPUs and Intel Gaudi2 [1, 2, 31]. Such formats are now widely used and can achieve baseline FP32 performance on large AI workloads, such as LLMs [14, 34]. FP8 summation involves several steps as shown in Figure 2. Consider the E4M3 format with one sign bit, four exponent bits, and three mantissa bits [31]. When adding two FP8 values with different exponents, the lower order bits of the smaller value are shifted out ( swamped ) due to right-shifting to align exponents with the larger value. This leads to a loss of precision in the final sum. In contrast to floating-point formats with wide mantissas, narrow formats such as E4M3 suffer from a significant loss in numerical accuracy due to swamping. Commercial hardware such as the H100 avoids swamping by accumulating FP8 partial sums in a wider precision such as FP16 or FP32 [2]. There are several classical algorithms for reducing swamping error in floating point summation, including pairwise summation [23] and Kahan summation [26]. Although Kahan summation has higher accuracy, it requires several extra floating point operations to maintain the compensated error term. Meanwhile, pairwise sum- mation is efficient to implement but suffers from large error in narrow floating-point formats. Figure 3 illustrates the need for high-precision accumulation of FP8 dot products. Using several summation algorithms, we perform dot products between two Gaussian vectors in FP8 precision (4-bit mantissa accumulator) and plot the numerical error relative to the baseline FP32 accumulation (24-bit mantissa accumulator). Sequen- tial summation loses all accuracy after only 200 sums. Pairwise summation is significantly more accurate than sequential summa- tion but still exhibits up to 50 error for longer dot products. In Section 5.2, we discuss how to accumulate FP8 mantissas in low- precision for a majority of sums while attaining numerical accuracy on-par with FP32 accumulation. 3 Analysis of Dot-Product Overflows We begin by providing an analytical framework for reasoning about overflows. We define two types of integer overflow and discuss multiple algorithms for avoiding them. Definition 3.1 (Transient Overflow). Overflow that may occur at any point during the sequential summation of 𝑘integers 𝑋 {𝑥1,𝑥2, ...,𝑥𝑘} when using a 𝑏-bit accumulator. Definition 3.2 (Persistent Overflow). Overflow that occurs when the final sum 𝑦 Í𝑘 𝑖 1 𝑥𝑖overflows a 𝑏-bit accumulator. Note that transient overflows may occur even when there is no persistent overflow. We aim to minimize these transient overflows. 3.1 Avoiding All Overflows Several prior works aim to avoid both persistent and transient over- flows entirely by retraining the neural network such that partial sums are always within the accumulator bounds. A2Q [11] and 200 400 600 800 1000 Dot Product Length 0 10 20 30 40 50 60 70 80 90 100 Relative Error ( ) Relative Error of FP8 Gaussian Vector Dot Products for Different Summations Algos Sequential Pairwise MGS w Narrow Accum Figure 3: Error, relative to FP32 precision, of Gaussian vec- tor dot products performed in FP8 precision. We execute each algorithm using solely a narrow accumulator and clip par- tial sums upon overflow. All algorithms exhibit significant errors due to the swamping of lower order bits when using reduced-precision accumulators. MGS has lower error than pairwise summation by separating partial product mantissas by exponent and accumulating them in separate narrow ac- cumulators. This means that dot product errors result only from clipping overflows. However, the 35 error of MGS, when restricted to a narrow accumulator, is unacceptable for DNN applications. A2Q [12] eliminate the possibility of both transient and persis- tent overflows by constraining the weight vector s L1-norm during quantization-aware training (QAT). They first bound the dot prod- uct result : 𝐾 𝑖 1 𝑤𝑖𝑥𝑖 𝐾 𝑖 1 𝑤𝑖 𝑥𝑖 2𝑝 1 1 In the worst case, all activations are maximal 𝑥𝑖 2𝑏 1 and the weight L1-norm may be bounded such that: 𝑘 𝑖 1 𝑤𝑖 w 1 2𝑝 1 1 2𝑏 1 This bound acts as an L1-regularizer and pulls most weight values toward zero, ensuring that partial sums never grow beyond 𝑝bits. L1 regularization promotes unstructured sparsity in the weight matrices, reducing the model size and enabling acceleration by skipping zero computations. However, network sparsification may reduce model accuracy [30] Meanwhile, retraining a pre-trained DNN to satisfy accumulator constraints may alter properties of the pre-trained model, such as algorithmic fairness guarantees [42]. We find that enforcing strict bounds on weight magnitude is not necessary for using narrow accumulators. 3.2 Avoiding Transient Overflows Persistent overflow is a true overflow where the final result is simply too large for the accumulator. Transient overflows are temporary and arise when a partial sum overflows but where the final sum may not actually overflow the accumulator. Hence, in the absence of persistent overflow, we should be able to eliminate transient overflows by reordering the summation. 3 Vikas Natesh, H.T. Kung, and David Kong Theorem 3.3. Let 𝑋 {𝑥1,𝑥2, ...,𝑥𝑘} be a list of 𝑘signed integers, where each 𝑥𝑖is represented using 𝑛bits. Let 𝑦 Í𝑘 𝑖 1 𝑥𝑖be the sum of all elements in 𝑋, representable using 𝑚 𝑛 1 bits without persistent overflow (i.e., 2𝑚 1 𝑦 2𝑚 1 1). Then, there exists an ordering of summation for 𝑋that avoids transient overflow when using an 𝑚-bit accumulator. Proof. Suppose 𝑘 2. The list 𝑋 {𝑥1,𝑥2} contains 𝑛-bit numbers, and its sum 𝑥1 𝑥2 (or 𝑥2 𝑥1) can require at most 𝑛 1 bits. Since 𝑚 𝑛 1, the sum does not overflow 𝑚-bits, and the theorem holds for 𝑘 2. Let 𝑙 2 and assume the theorem holds 𝑘 𝑙, i.e., there exists an ordering of𝑋 {𝑥1,𝑥2, ...,𝑥𝑙} such that the sum of elements of𝑋 w.r.t. said ordering avoids transient overflow (inductive hypothesis). Denote this ordering by the index set 𝛼𝑙and the so-ordered list by 𝑋𝛼𝑙. Suppose that 𝑘 𝑙 1 and 𝑋 𝑋𝛼𝑙 𝑥𝑘. Then, 𝑦 𝑘 𝑖 1 𝑥𝑖 𝑥𝑘 𝑖 𝛼𝑙 𝑥𝑖 By the inductive hypothesis, the second term in the sum, denoted by ˆ𝑦 Í 𝑖 𝛼𝑙𝑥𝑖, avoids transient overflow. Since ˆ𝑦is an 𝑚-bit signed integer, and 𝑥𝑘is an 𝑛-bit signed integer with 𝑛 𝑚 1, the sum 𝑦 𝑥𝑘 ˆ𝑦is represented by at most 𝑚bits. Therefore, a feasible ordering to avoid transient overflow is 𝛼𝑘 {𝛼𝑙,𝑘}. Thus, by induction, our theorem must hold for any 𝑘 2. The proof shows how to construct a summation sequence with- out transient overflow by building the right permutation sequence at each step. One example of such an algorithm is first to sort the 𝑘 values, divide them into a list of negative values and a list of positive values, and repeatedly form the sum of the largest positive and most negative values. We can then take the resulting list, with length at least 𝑘 2, and apply the algorithm recursively until a single pair of values remains. This method is guaranteed to avoid transient overflow while using the narrowest possible accumulator as the running sum increases monotonically. Performing summations in a sorted order is also beneficial for retaining floating point accuracy since adding pairs of values of similar magnitude reduces the num- ber of bits swamped in the smaller value [16]. However, sorting before adding becomes expensive in DNN applications where dot product lengths may exceed 4096. AGS is a recent method to avoid transient overflow by reorder- ing in integer-quantized DNNs [32]. AGS first splits the sequence by sign into a positive list and negative list, then alternates sum- ming values from either the negative or positive list depending on whether the accumulator overflows its maximum or minimum value, respectively. This allows AGS to avoid transient overflows while also avoiding sorting and using only an extra bit for over- flow detection. However, AGS may require additional registers or memory to buffer partial products. For example, once an overflow is detected, AGS may need to buffer several positive values while waiting for a negative partial product to arrive. The extra mem- ory requirements may overwhelm the benefits of using a narrow accumulator, challenging AGS hardware implementation. 4 Markov Greedy Summation In this section, we detail how our proposed MGS avoids all over- flows while using narrow accumulators for the majority of sum- mations. We show that since weights and activations are normally distributed or half-normally distributed, the chance of overflow during summation is actually low. We then derive the expected number of summations before overflow by modeling the running sum as a random walk. 0 10 20 30 40 50 Dot Product Length 0.0 0.2 0.4 0.6 0.8 Pr(Overflow) (a) Prob. of Overflow For Different Accum Bitwidths accum 8 accum 9 accum 10 accum 11 accum 12 accum 13 0 5 10 15 20 25 30 Layer Index 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 9.2 Average Accum Bitwidth (b) Average Accumulator Bitwidth During MobileNetV2 Inference Figure 4: (a) We estimate the probability of overflow based on the model described in Section 4.1, when performing dot product at different accumulator bitwidths. 5-bit Gauss- ian weights in the range[-15,15] are multiplied with 7-bit Gaussian activations in [-63,63] to yield partial products 𝑍 𝑁(0,𝑘 𝜎𝑤𝜎𝑥). We set 𝜎of weights and data such that the extreme values lie 3 𝜎 s away from the mean 0, i.e., 𝜎𝑤 15 3 5 and 𝜎𝑥 63 3 21. The figure shows that despite 7 5 12-bit partial products, we can use accumulators with 12 bits for most sums before overflow. For example, there is only a 12 chance of overflow when summing 10 elements in a narrow 10-bit accumulator. In (b), we plot the average accumulator bitwidth when running MobileNetv2 inference with 5-bit weights and 7-bit activations. Although one would expect that at least 5 7 12 bits are required to prevent overflow, the average accumulator bitwidth required varies between 7 and 10 bits. 4.1 Estimating the Probability of Integer Overflow We consider 𝑏-bit quantized neural network dot products 𝑍 Í𝑘 𝑖 1 𝑤𝑖𝑥𝑖with weights and activations in the range [ 2𝑏 1, 2𝑏 1]. Weight and input activation vectors 𝑤and 𝑥are truncated, zero- centered i.i.d normal distributions 𝑁(𝜇𝑤 0, 𝜎𝑤) and 𝑁(𝜇𝑥 0, 𝜎𝑥), respectively. Input activations may also be half-normal dis- tributions due to ReLU operations in the previous layer. The partial products 𝑝𝑖 𝑤𝑖𝑥𝑖are i.i.d product-normal distributions with 𝜇𝑝 0 and 𝜎2𝑝 (𝜎2𝑤 𝜇2𝑤)(𝜎2𝑥 𝜇2𝑥) 𝜇2𝑤𝜇2𝑥 𝜎2𝑤𝜎2𝑥. The sum- ming of partial products can be represented by the random variable 𝑍 Í𝑘 𝑖 1 𝑝𝑖. By the central limit theorem (CLT), for large enough 𝑘, 𝑍 𝑁(0,𝑘 𝜎𝑤𝜎𝑥). This enables us to approximate the probability of overflow given a particular dot product length 𝑘and accumulator bitwidth 𝑎. 𝑃𝑟( 𝑍 2𝑎 1) 2Φ 2𝑎 1 𝜎𝑤𝜎𝑥 𝑘 4 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation where Φ is the CDF of the standard normal distribution. Figure 4a displays the probability of overflow for different vector lengths and accumulator bitwidths when performing dot product with 5-bit weights and 7-bit activations. The figure shows that for relatively long dot products, such as 10 or 15 elements, the chance of overflow is relatively low, even for narrow accumulators. In Figure 4b, we empirically observe that the average accumulator bitwidth is small across DNN layers, suggesting that wide accumulators may not be needed for a majority of sums. 4.2 Computing the Expected Number of Overflows The approximation above provides a loose bound showing that overflow is relatively rare, even with narrow accumulators. We can derive the expected number of summations before overflow more precisely by modeling summation as a random walk, specifically a Markov chain with a single absorbing state representing overflow. To illustrate the idea, consider the summation of integers from the range [-2,2] using an accumulator that can only hold values in [-2,2]. In each step, we randomly select an integer from the range [-2,2] and add it to the accumulator. We stop when the accumulator overflows out of range [-2,2], i.e., we enter the absorbing state. Once entered, the process cannot leave the absorbing state. Hence, the random walk will eventually end as the accumulator is permanently absorbed into an overflow state. A 6 6 transition matrix 𝑃repre- sents the probabilities of entering different states given the current sum, with each row summing to 1. P Input State -2 -1 0 1 2 Ovfl Output State -2 1 5 -1 1 5 0 1 5 1 0 2 0 Ovfl 2 5 1 5 1 5 1 5 1 5 0 1 5 1 5 1 5 1 5 1 5 1 5 0 0 1 5 1 5 1 5 1 5 1 5 0 0 1 5 1 5 1 5 2 5 0 0 0 0 0 1 For example, the 5th row represents the probability of different output states given the starting accumulator value of 2. The value 2 may be summed with either 1 or 2 with probability 2 5 as both are equally likely to be the next state (uniform random draws). Since 2 1 3 and 2 2 4 both overflow the accumulator, we enter the overflow state (ovfl column) with probability 2 5. The last row shows that if we start in an overflow state, we will remain in that state surely. We can represent the transition matrix 𝑃in a blocked form: 𝑃 𝑄 𝑅 0 𝐼 where 0 is a zero matrix and 𝐼is the identity matrix. 𝑅represents the transitions from transient states to absorbing states, and 𝑄 represents the transitions between transient states. To compute the transition probabilities after 𝑘steps, we simply multiply 𝑃by itself 𝑘times. 𝑃𝑘 𝑄𝑘 𝑅 𝑄𝑅 ... 𝑄𝑘𝑅 0 𝐼 0 (𝐼 𝑄) 1𝑅 0 𝐼 𝑄𝑘 0 reflects the fact that the random walk will eventually end, i.e., eventually there is zero probability of being in a non-absorbed state. The fundamental matrix of the Markov chain 𝑁 (𝐼 𝑄) 1 represents the expected number of visits to non-absorbing state 𝑗 [ 2, 2] starting from non-absorbing state 𝑖 [ 2, 2], before absorption. The accumulator starts with the value 0, varies across different non-absorbing states with each partial sum, and eventually overflows. The expected number of steps to reach overflow is simply the sum of the entries in row 3 of 𝑁, corresponding to the state 0. This sum represents the total expected number of visits to all non- absorbing states before absorption, i.e., the total expected number of sums we may perform before overflow. We apply our random walk model to dot product accumulation when executing quantized MobileNetV2 inference on Imagenet1K with 5-bit weights and 7-bit activations [15, 36]. Since weights and activations may deviate slightly from normal, we compute transition probabilities using their empirical distributions during DNN inference. Figure 5 plots the empirical versus modeled average summation length before overflow in a 1x1 convolution layer in the 13th residual block. When summing partial products derived from multiplying 5-bit weight and 7-bit activations, we expect that 5 7 12 bit accumulation is required. However, Figure 5 shows one may use a narrow 9-bit accumulator to sum 10 values before needing to use a wider accumulator, on average. 15 10 5 0 5 10 15 0 2000 4000 6000 8000 Weight Distribution 60 65 70 75 80 85 90 0 5000 10000 15000 20000 25000 Input Activation Distribution 600 400 200 0 200 400 600 0 20 40 60 80 Partial Product Distribution 5 6 7 8 9 10 Accum Bitwidth 0 10 20 30 40 50 Mean Summation Length Modelled vs Measured Summation Length Before Overflow in MobileNetV2 Measured During Inference Modelled by Random Walk Figure 5: Plotting the empirical measured average dot prod- uct length versus expected dot product length based on our random walk model. 5-bit Weights follow a normal distri- bution in the range [-15, 15], while 7-bit activations have a half-normal distribution in the range [0,127] after ReLU. Note that the plot shows that with the accumulation bitwidth equal to 10, we do not expect overflow at a summation length of about 32. In contrast, a naive analysis would conclude that 17 5 7 5 bits are required to avoid overflows, noting that 5 log2 32. 5 Vikas Natesh, H.T. Kung, and David Kong 5 Dual-Accumulator MAC Design In this section, we describe the hardware for dual-multiply-accumulate (dMAC) units, leveraging our observation that the majority of dot product sums do not overflow when using narrow accumulators. We first introduce the dMAC for integer dot products and then show how this design enables narrow accumulation in FP8. 5.1 Integer dMAC The integer dMAC unit uses a narrow adder (green in Figure 6) for most summations and a wide adder (red) to handle partial sums that overflow the narrow adder. It has a slightly higher area over- head than a conventional MAC unit, containing two adders and additional overflow handling logic. However, dMAC consumes sig- nificantly less dynamic power by exploiting the low overflow rate in DNN dot products. In addition, we clock-gate the wider accumu- lator to reduce dynamic power usage further when not performing wide accumulations. Figure 6 displays our integer dMAC design when multiplying 4-bit weights and activations using 8-bit and 32-bit adders. After multiplication, the product 𝑝is accumulated in an 8-bit register 𝑎8. If the 8-bit adder s carry-out overflow flag is set, we accumulate 𝑎8 in the wider 32-bit register 𝑎32 instead and write 𝑝to 𝑎8. Once all the partial products have been accumulated, we add 𝑎8 and 𝑎32 and return the output. 32-bit adder 1 0 a32 a8 p GND Multiplier act 4 weight 4 8 8 8 8-bit adder 1 1 0 8 8 8 Dual-MAC (dMAC) done 1 0 1 32 32 GND OUT Oflow Figure 6: Dual accumulator MAC hardware unit (dMAC) with output-stationary behavior. In this example, 4-bit weights and data arrive for multiplication and summation with an 8-bit accumulator 𝑎8. If an overflow occurs (oflow 1), 𝑎8 is summed into the wider 32-bit accumulator 𝑎32, and the 8-bit partial product is written to 𝑎8. Upon completing the dot product (done 1), we return the sum of 𝑎8 and 𝑎32. 5.2 8-bit Floating-Point dMAC Existing hardware for FP8 MAC operations accumulate partial prod- ucts in higher precision such as FP32 [1, 2] This not only requires the use of wide mantissa adders but also FP8- FP32 data conver- sions and FP32 normalizations. Figure 7 provides a high-level view of the difference between our hardware and existing FP8 MAC units. We show that using dMACs for mantissa accumulation can avoid several expensive operations in wide registers while maintaining numerical accuracy. Figure 8 displays our FP8 dMAC design. A new weight and activa- tion in the E4M3 format arrive each cycle. After multiplication and FP8 Multiply FP8 ---- FP32 FP32 Normalize Round FP32 Accumulation FP32 Normalize Round FP8 Multiply FP8 Normalize Round FP8 Accumulation dMAC-FP8 FP8 Figure 7: High-level view of operations in our dMAC-FP8 unit versus conventional FP8 MACs. The gray boxes represent the operations that must occur every time a pair of values arrives. Conventional FP8 incurs overhead from data conversion and wide accumulation and normalization operations. In con- trast, dMAC-FP8 performs the majority of computation in narrower precision while amortizing the cost of normaliza- tion across multiple partial summations. rounding, the partial product sign bit converts the 4-bit mantissa (with leading 1) to 5-bit signed 2 s complement. Using a narrow 5-bit adder, we then accumulate the mantissa into one of 16 5-bit registers based on its 4-bit exponent, which ranges from 0 to 15. By accumulating mantissas of the same exponent in the same register, we avoid the shifting operations required when adding two FP8 values with differing exponents while also avoiding numerical error from swamping (see 2). When the 5-bit adder overflows, we left-shift the accumulator by its exponent and accumulate into a wide 32-bit accumulator. Left-shifting by the exponent forces the 5-bit accumulator value to have exponent zero, allowing partial sums with different expo- nents to be added into the same wide register without error. Since overflows are rare, dMAC amortizes the shifting cost of mantissa alignment over several summations instead of between each pair of elements as in conventional FP32. Once the dot product is complete, the values in each accumulator register are left shifted by their respective exponent and summed into the 32-bit accumulator. This 16 shift add operation is only performed once per dot product. Finally, the result is normalized, rounded, and returned. 5.3 Subnormal Gating Zero-gating hardware, such as those proposed in [45] and [8] do not perform multiplication when a zero operand is loaded. With zero- gating approaches, processing elements are forced to remain idle, saving computation energy. Meanwhile, zero-skipping approaches such as [21] and [9] avoid loading zero operands from memory altogether. We exploit the range of possible FP8 products to avoid MAC operations on zeros and small inputs to the dMAC unit. Given the 256 possible values in the FP8 range, the number of possible partial product pairs in FP8 is 256 2 32640. Of these possible pairs, 1280 lead to product magnitudes that are too small to be represented in the FP8 subnormal range and ultimately round to zero, i.e., 𝑤 𝑥 2 9. Moreover, as weights and activations may be 6 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation Exp Mant Exponent DEMUX Normalize Round . . . 16 Narrow Accumulator Registers for Each Exponent 1 Wide Accumulator Register Register Narrow Adder Wide Adder Left Shifter Oflow Figure 8: FP8 dMAC hardware unit. We display the FP8 ac- cumulation step in the computation. A new partial product arrives every cycle. Partial product mantissas are written to one of 16 registers such that they are always accumulated with other values of the same exponent, thereby preventing swamping while enabling the use of the narrow accumulator (green) for most summations. When the narrow accumula- tor overflows, it is left-shifted and summed with the wide accumulator (red) to prevent precision loss. normally distributed with mean 0, several partial products will be 0 or close to 0. We implement logic to check whether input exponents are small enough that the product lies outside the subnormal range. We then skip these MAC operations for additional dynamic power savings. 6 Evaluation Our evaluation is divided into two parts. In Section 6.2, we compare MGS to state-of-the-art (SOTA) works in accumulator compres- sion when performing inference on various image classification networks. We show that MGS can significantly reduce accumula- tor bitwidth while achieving accuracy on par with FP32 baselines, without retraining. Then, in Section 6.4, we implement dMAC units for INT8 and FP8 in a 7nm node and measure power consumption relative to conventional MACs. dMACs can reduce inference power consumption by up to 34.1 . 6.1 FP8 Emulation Library Prior works have addressed the difficulty of efficiently profiling overflows due to lack of support in standard deep learning frame- works [11, 33]. We have built a C CUDA library to emulate dMAC FP8 on both CPUs and GPUs to run experiments quickly. We ex- tend PyTorch s quantization framework with custom linear and convolution layers implementing MGS for INT8 and FP8 quanti- zation to measure the impact on model accuracy. We unroll dot product computations, allowing users to vary weight, activation, and accumulator bitwidths and evaluate overflow solutions such as MGS, clipping, or wraparound arithmetic. Table 1: Imagenet1K Top-1 Accuracy Unit MobileNetV2 ResNet-18 ViT-Small Baseline (FP32) 71.6 70.58 80.08 INT8 69.7 68.45 79.17 FP8 71.04 70.12 80.02 dMAC 71.10 70.11 80.07 6.2 Reducing Accumulator Bitwidth In this section, we evaluate the ability of MGS to enable low- resolution accumulation while maintaining FP32 model accuracy in MobileNetV2 [24], ResNet-18 [22], and ViT [17] on CIFAR10 [27] and ImageNet [15]. While our evaluation focuses on FP8 inference, MGS may also be applied to other data formats to reduce accumu- lator bitwidth, e.g., during training with the E5M2 FP8 datatype. 6.2.1 8-Bit Integer Quantized DNNs. We sweep the design space by varying weight and activations from 5 to 8 bits while varying the accumulator bitwidth from 8 to 20. We select the best-performing models with the lowest required accumulator bitwidth to generate a Pareto frontier. For models on the frontier, we use our software li- brary to evaluate accuracy compared to SOTA methods A2Q, A2Q , AGS, and overflow clipping [4, 6, 11, 12, 19, 32]. Figure 9 shows that MGS can push the accumulator bit width lower than A2Q while also maintaining task performance. The magenta lines show that clipping transient overflows within dot products can limit how much we may reduce accumulator bitwidth. AGS accurately avoids transient overflows but clips persistent over- flows, leading to accuracy drops at lower bitwidth where clipping becomes more prevalent. 6.2.2 8-Bit Floating Point Quantized DNNs. We evaluate MGS when performing FP8 inference using our target models. We employ the E4M3 datatype and fix the narrow accumulator bitwidth at five signed bits. Table 1 shows that MGS accuracy is on par with SOTA methods for FP8. This is expected since MGS always falls back to using a wide accumulator upon overflow and does not lose accuracy due to swamping. 6.3 dMAC FPGA Prototyping To estimate resource utilization for the MAC units, we prototype the designs on an AMD Virtex-7 VC707 FPGA. Table 2 compares the resource utilization of the FPGA MAC unit implementations in terms of look-up tables (LUTs) and flip-flops (FFs). The area of our INT8 dMAC is slightly higher than the conventional INT8 MAC since our unit contains an extra adder and overflow handling logic to use both accumulators. Although some dMAC designs may have higher resource utilization, they always achieve power savings as detailed in the accurate power, performance, and area characterization of ASIC implementations of the MAC units detailed in Section 6.4. 6.4 dMAC ASIC Physical Implementation In addition to comparing the FPGA implementation of the designs, we compare the ASIC implementations for accurate power, perfor- mance and area characterization. To compare the performance of 7 Vikas Natesh, H.T. Kung, and David Kong 9 10 11 12 13 14 15 16 17 18 19 20 Accumulator Bit Width 75.0 77.5 80.0 82.5 85.0 87.5 90.0 92.5 Top-1 Accuracy ( ) (a) MobileNetV2 on CIFAR-10 MGS AGS A2Q A2Q Clip FP32 Baseline 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Accumulator Bit Width 60 65 70 75 80 85 90 95 Top-1 Accuracy ( ) (b) ResNet-18 on CIFAR-10 MGS AGS A2Q Clip FP32 Baseline 10 11 12 13 14 15 16 17 18 19 20 Accumulator Bit Width 60 65 70 75 80 85 90 Top-1 Accuracy ( ) (c) ViT-Small on CIFAR-10 MGS AGS Clip FP32 Baseline Figure 9: Comparing INT8 MGS to SOTA methods for low-precision accumulation during quantized inference using several models. We sweep weight and activation bitwidths from 5 to 8 bits while varying the accumulator from 8 to 20 bits. We then plot the best-performing models with the lowest required accumulator bitwidth. Since MGS uses both narrow and wide accumulators during the dot product, we plot the average accumulator bitwidth when running MGS. In principle, MGS can indefinitely reduce the narrow accumulator bitwidth as it always falls back on the wide accumulator. However, we stop reducing accumulator bitwidth for MGS when additional reduction increases the average bitwidth (using the wide accumulator more often) and instead start clipping those overflows. By using narrow accumulators for the majority of sums, MGS can reduce accumulator bitwidth beyond the SOTA. Table 2: FPGA MAC unit resource utilization comparison Unit FPGA LUTs FPGA FFs INT8 MAC 107 81 INT8 dMAC 126 79 FP8 MAC 457 335 FP8 dMAC (w o skipping) 165 143 FP8 dMAC (w skipping) 180 143 each MAC unit, we perform the full physical implementation of the designs at the 7 nm node using the ASAP7 PDK [10] with a 0.7 V supply voltage. Figure 10 shows the layouts and dimensions of the MAC units and their respective areas. To balance switching speed and power consumption, we implement the design using standard threshold voltage transistors targeting a clock frequency of 500 MHz. We synthesize the designs using Cadence Genus [7], perform implementation using Cadence Innovus, run gate-level simulations with representative weights for each workload using Synopsys VCS [40], and characterize power consumption based on transient behavior using Cadence Voltus. The power comparison of the designs is shown in Table 3. As expected, the static leakage power scales with area. Comparing INT8 dMAC with INT8 MAC (with traces from MobileNetV2), we see that although the area for INT8 dMAC is 14.5 larger and leak- age power is 16.4 higher from additional logic, we still achieve a 15.4 total power savings. The FP8 dMAC unit without skipping consumes the least area, the FP8 dMAC unit with skipping con- sumes slightly more area for skipping logic, and the baseline FP8 MAC unit consumes the most area. When comparing total power for FP8 (with traces from ViT), the dMAC achieves 33.6 and 34.1 total power savings without and with skipping, respectively. Our zero-skipping approach implements logic to check input exponents before skipping state machine stages within the full MAC operation. Further savings could be made with more aggressive power-saving Figure 10: Layouts and dimensions of ASIC implementations of the MAC units at the 7 nm node: (a) is INT8 MAC, (b) is INT8 dMAC, (c) is FP8 MAC, (d) is FP8 dMAC without skipping, and (e) is FP8 dMAC with skipping. techniques such as power-gating (where unused parts of the circuit are turned off) [39] or dynamic voltage and frequency scaling [38]. 7 Conclusion The paper introduced MGS to reduce the required accumulation bitwidth in performing dot products that form the bulk of DNN computations. Based on the statistical properties of weight and acti- vation distributions, we can sum many partial products in reduced precision before overflow occurs. Specifically, MGS uses a narrow accumulator to accumulate as many values as possible while falling back on a wider accumulator when the rare overflow occurs. We have designed dual-multiply-accumulate (dMAC) hardware units that use narrow accumulators for most sums, resulting in a nar- rower average accumulator bitwidth compared to prior works. Our 8 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation Table 3: ASIC MAC unit power results at 500 MHz showing that 15.4 , 33.6 , and 34.1 savings are achieved for INT8, FP8 without skipping, and FP8 with skipping, respectively. Unit Dynamic Power (𝜇W) Static Power (𝜇W) Total Power (𝜇W) Power Saving INT8 MAC 27.41 0.073 27.48 baseline INT8 dMAC 23.16 0.085 23.25 15.4 FP8 MAC 97.12 0.249 97.37 baseline FP8 dMAC (w o skipping) 64.44 0.226 64.66 33.6 FP8 dMAC (w skipping) 63.92 0.232 64.15 34.1 dMAC units consume significantly less power than conventional integer and floating-point MACs that use wide accumulators for all summations while achieving classification accuracy on par with high-precision floating-point baselines for multiple image classifi- cation tasks. References [1] 2022. Habana Gaudi2 White Paper. Gaudi2 20White 20Paper.pdf [2] 2022. NVIDIA H100 Tensor Core GPU Architecture White Paper. https: resources.nvidia.com en-us-tensor-core [3] 2023. GAP8 IoT Application Processor. gap8_mcu_ai . [4] Arm Limited 2022. ARM CMSIS Library. Arm Limited. com tools-and-software embedded cmsis [5] Arm Limited 2022. Cortex-M4 Technical Reference Manual. Arm Limited. https: developer.arm.com documentation ddi0439 b [6] Arm Limited 2024. Arm Neon technology, the Advanced SIMD (Single Instruction Multiple Data) architecture extension for implementation of the Armv8-A or Armv8- R architecture profiles. Arm Limited. instruction-sets intrinsics [7] Cadence Design Systems. 2024. Cadence Innovus Implementation Sys- tem. soc-implementation-and-floorplanning innovus-implementation-system.html [8] Yu-Hsin Chen, Joel Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architec- ture for Energy-Efficient Dataflow for Convolutional Neural Networks. In 2016 ACM IEEE 43rd Annual International Symposium on Computer Architecture (ISCA). 367 379. [9] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. 2019. Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (June 2019), 292 308. [10] Lawrence T Clark, Vinay Vashishtha, Lucian Shifren, Aditya Gujja, Saurabh Sinha, Brian Cline, Chandarasekaran Ramamurthy, and Greg Yeric. 2016. ASAP7: A 7-nm finFET predictive process design kit. Microelectronics Journal 53 (2016), 105 115. [11] Ian Colbert, Alessandro Pappalardo, and Jakoba Petri-Koenig. 2023. A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance. 2023 IEEE CVF International Conference on Computer Vision (ICCV) (2023), 16943 16952. [12] Ian Colbert, Alessandro Pappalardo, Jakoba Petri-Koenig, and Yaman Umuroglu. 2024. A2Q : improving accumulator-aware weight quantization. In Proceedings of the 41st International Conference on Machine Learning (Vienna, Austria) (ICML 24). JMLR.org, Article 369, 17 pages. [13] Barry de Bruin, Zoran Zivkovic, and Henk Corporaal. 2020. Quantization of deep neural networks for accumulator-constrained processors. Microprocess. Microsystems 72 (2020). [14] DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] 2501.12948 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im- ageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition. 248 255. 2009.5206848 [16] L. C. W. Dixon and D. J. Mills. 1994. Effect of rounding errors on the variable metric method. J. Optim. Theory Appl. 80, 1 (Jan. 1994), 175 179. org 10.1007 BF02196600 [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv abs 2010.11929 (2020). [18] PyTorch Foundation. [n. d.]. PyTorch Quantization. [19] Angelo Garofalo, Manuele Rusci, Francesco Conti, Davide Rossi, and Luca Benini. 2019. PULP-NN: accelerating quantized neural networks on parallel ultra-low- power RISC-V processors. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 378, 2164 (Dec. 2019), 20190155. [20] Jorge Gomez, Saavan Patel, Syed Shakib Sarwar, Ziyun Li, Raffaele Capoc- cia, Zhao Wang, Reid Pinkham, Andrew Berkovich, Tsung-Hsun Tsai, Barbara De Salvo, et al. 2022. Distributed On-Sensor Compute System for AR VR Devices: A Semi-Analytical Simulation Framework for Power Estimation. arXiv preprint arXiv:2203.07474 (2022). [21] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J. Dally. 2016. EIE: Efficient Inference Engine on Compressed Deep Neural Network. ACM SIGARCH Computer Architecture News 44, 3 (June 2016), 243 254. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. [23] Nicholas J. Higham. 1993. The Accuracy of Floating Point Summation. SIAM Journal on Scientific Computing 14, 4 (1993), 783 799. 0914050 arXiv: [24] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Wei- jun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mo- bileNets: Efficient Convolutional Neural Networks for Mobile Vision Applica- tions. arXiv:1704.04861 [cs.CV] [25] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, An- drew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [26] W. Kahan. 1965. Pracniques: further remarks on reducing truncation errors. Commun. ACM 8, 1 (Jan. 1965), 40. [27] Alex Krizhevsky. [n. d.]. CIFAR-10 and CIFAR-100 Datasets. kriz cifar.html. [28] H. T. Kung, Bradley McDanel, and Sai Qian Zhang. 2020. Term quantization: furthering quantization at run time. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (Atlanta, Georgia) (SC 20). IEEE Press, Article 96, 14 pages. [29] Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. 2020. MCUNet: Tiny Deep Learning on IoT Devices. In Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS 20). Curran Associates Inc., Red Hook, NY, USA, 11711 11722. [30] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir. 2020. Prov- ing the Lottery Ticket Hypothesis: Pruning is All You Need. ArXiv abs 2002.00585 (2020). [31] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. 2022. FP8 Formats for Deep Learning. arXiv:2209.05433 [cs.LG] [32] Vikas Natesh and H. T. Kung. 2025. Alternating Greedy Schedules: Enabling Low-Bitwidth Accumulation of Dot Products in Neural Network Computations. In 2025 IEEE International Symposium on Circuits and Systems (ISCAS). 1 5. [33] Renkun Ni, Hong-Min Chu, Oscar Castañeda, Ping-yeh Chiang, Christoph Studer, and Tom Goldstein. 2021. WrapNet: Neural Net Inference with Ultra-Low- Precision Arithmetic. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https: openreview.net forum?id 3SqrRe8FWQ- [34] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng. 2023. FP8-LM: Training FP8 Large Language Models. arXiv:2310.18313 [cs.LG] [35] Andrew Sabot, Vikas Natesh, H. T. Kung, and Wei-Te Ting. 2023. MEMA Runtime Framework: Minimizing External Memory Accesses for TinyML on Microcontrollers. TinyML Research Symposium 2023 abs 2304.05544 (2023). arXiv:2304.05544 9 Vikas Natesh, H.T. Kung, and David Kong [36] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottle- necks. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. Computer Vision Foundation IEEE Computer Society, 4510 4520. [37] Moritz Scherer, Manuel Eggimann, Alfio Di Mauro, Arpan Suravi Prasad, Francesco Conti, Davide Rossi, Jorge Tomás Gómez, Ziyun Li, Syed Shakib Sarwar, Zhao Wang, Barbara De Salvo, and Luca Benini. 2023. Siracusa: A Low-Power On-Sensor RISC-V SoC for Extended Reality Visual Processing in 16nm CMOS. In ESSCIRC 2023- IEEE 49th European Solid State Circuits Conference (ESSCIRC). 217 220. [38] G. Semeraro, G. Magklis, R. Balasubramonian, D.H. Albonesi, S. Dwarkadas, and M.L. Scott. 2002. Energy-efficient processor design using multiple clock domains with dynamic voltage and frequency scaling. In Proceedings Eighth International Symposium on High Performance Computer Architecture. 29 40. [39] Youngsoo Shin, Jun Seomun, Kyu-Myung Choi, and Takayasu Sakurai. 2010. Power gating: Circuits, design methodologies, and best practice for standard-cell VLSI designs. ACM Transactions on Design Automation of Electronic Systems (TODAES) 15, 4 (2010), 1 37. [40] Synopsys. 2021. Synopsys VCS Simulation Solution. com content dam synopsys gated-assets verification vcs-ds.pdf [41] Tensorflow. [n. d.]. Tensorflow Lite Quantization. 10432 [42] Cuong Tran, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022. Prun- ing has a disparate impact on model accuracy. In Proceedings of the 36th In- ternational Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 1283, 13 pages. [43] Shibo Wang and Pankaj Kanwar. 2019. BFloat16: The secret to high performance on Cloud TPUs. bfloat16-the-secret-to-high-performance-on-cloud-tpus. Accessed: 2025-02-02. [44] James Hardy Wilkinson. 1964. Rounding Errors in Algebraic Processes. https: trace.tennessee.edu cgi viewcontent.cgi?article 1053 context utk_harlan [45] Lin Ye, Jinghao Ye, Masao Yanagisawa, and Youhua Shi. 2019. A Zero-Gating Processing Element Design for Low-Power Deep Convolutional Neural Networks. In 2019 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS). 317 320. 10\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nMGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation Vikas Natesh Harvard University Cambridge, MA, USA H.T. Kung Harvard University Cambridge, MA, USA David Kong Harvard University Cambridge, MA, USA Abstract We offer a novel approach, MGS (Markov Greedy Sums), to im- prove the accuracy of low-bitwidth floating-point dot products in neural network computations. In conventional 32-bit floating- point summation, adding values with different exponents may lead to loss of precision in the mantissa of the smaller term, which is right-shifted to align with the larger term s exponent. Such shift- ing (a.k.a. swamping ) is a significant source of numerical errors in accumulation when implementing low-bitwidth dot products (e.g., 8-bit floating point) as the mantissa has a small number of bits. We avoid most swamping errors by arranging the terms in dot product summation based on their exponents and summing the mantissas without overflowing the low-bitwidth accumulator. We design, analyze, and implement the algorithm to minimize 8-bit floating point error at inference time for several neural networks. In contrast to traditional sequential summation, our method has signif- icantly lowered numerical errors, achieving classification accuracy on par with high-precision floating-point baselines for multiple image classification tasks. Our dMAC hardware units can reduce power consumption by up to 34.1 relative to conventional MAC units. 1 Introduction Quantization has become a ubiquitous optimization for compress- ing deep neural networks (DNNs) on both low-power edge devices [20, 29, 35, 37] as well as large-scale training and inference sys- tems made up of many GPUs [34]. Low-power devices for tinyML typically have small local memories [5] and often lack support for efficient floating-point computation [3, 37]. Hence, integer quanti- zation is, by default, necessary on such systems, and most tinyML models are quantized to 8 bits or less. Meanwhile, large generative AI workloads push GPU-based training and inference clusters to the limits of available memory, bandwidth, and computation power.\n\n--- Segment 2 ---\nHence, integer quanti- zation is, by default, necessary on such systems, and most tinyML models are quantized to 8 bits or less. Meanwhile, large generative AI workloads push GPU-based training and inference clusters to the limits of available memory, bandwidth, and computation power. To this end, low-bitwidth formats such as brain float-16 (bfloat16) [43], block floating point (BFP) [44], and 8-bit floating-point (FP8) have been implemented in various hardware [1, 2]. Such formats have been successful in reducing memory footprint, memory accesses, computation time, and power consumption [1, 2, 31] When performing quantized matrix multiplications, dot prod- ucts are typically accumulated into wider registers. For instance, partial products in FP8 may be accumulated in FP16 or FP32 to ensure numerical accuracy. Reducing accumulator bitwidth can reduce bandwidth and energy usage while increasing inference throughput [11 13, 33]. However, if the partial product sum over- flows the accumulator, its value may be clipped to a finite range. This introduces numerical errors into the final matrix result that degrade model accuracy and limit how much one can reduce the -3 4 14 2 -12 4 -4 1 -7 2 4 -4 s1 15 s2 -9 s3 -5 s1 s2 s3 (a) Markov Greedy Sum (MGS) . . . Exp Mant (b) Using MGS In FP8 Accumulation Exponent DEMUX Normalize Round . . . Narrow Accum Wide Accum Left Shifter Figure 1: (a) Example of Markov Greedy Sums (MGS). In (a), we sum 12 integers into a narrow accumulator (green box) until the sum 𝑠𝑖overflows the range [-15, 15]. Then, we accu- mulate 𝑠𝑖into a wider accumulator (red box). The underlined red values are those that would have caused an overflow of the narrow accumulator, noting that 15 2 15 and -9-7 -15. In (b), we accumulate FP8 values by separating them by their exponents into 16 groups, summing the mantissas into separate narrow accumulators indexed by the exponent, and using a wide accumulator upon overflow.\n\n--- Segment 3 ---\nThe underlined red values are those that would have caused an overflow of the narrow accumulator, noting that 15 2 15 and -9-7 -15. In (b), we accumulate FP8 values by separating them by their exponents into 16 groups, summing the mantissas into separate narrow accumulators indexed by the exponent, and using a wide accumulator upon overflow. MGS amortizes the cost of aligning (shifting) FP8 mantissas over many sums. accumulator bitwidth. In addition, there is the swamping problem [23] that causes loss of precision due to mantissa shift when floating point numbers need to align their exponents before summation. Prior works have attempted to reduce overflow in narrow ac- cumulators by regularizing the loss function [33] or by control- ling weight magnitude during training [11 13]. While these ap- proaches succeed in reducing overflows, they impose restrictive constraints on weights during training that may reduce model ac- curacy [11, 12, 33]. Moreover, existing networks require expensive re-training to satisfy such constraints. Other works reorder dot product summations to avoid the majority of overflows when using narrow accumulators [32]. However, reordering requires additional 1 arXiv:2504.09072v1 [cs.AR] 12 Apr 2025 Vikas Natesh, H.T. Kung, and David Kong sort permute operations as well as memory for temporary storage and is difficult to optimize on existing hardware. We propose Markov Greedy Summation (MGS), a novel approach to enable low-precision accumulation in neural network dot prod- ucts without the need for retraining or summation reordering. We analyze overflows during neural network inference and model the value of the partial sum in dot products as a Markov process to derive the expected dot product length without overflow. Our key insight is that based on the statistical properties of weight and activation distributions, we can sum many partial products in re- duced precision before overflow occurs. MGS is greedy in the sense that it uses a narrow accumulator to accumulate as many values as possible while falling back on a wider accumulator when the rare overflow occurs. Leveraging this insight, we design dual-multiply- accumulate (dMAC) hardware units that use narrow accumulators for the majority of sums.\n\n--- Segment 4 ---\nMGS is greedy in the sense that it uses a narrow accumulator to accumulate as many values as possible while falling back on a wider accumulator when the rare overflow occurs. Leveraging this insight, we design dual-multiply- accumulate (dMAC) hardware units that use narrow accumulators for the majority of sums. Our method uses a narrower average accumulator bitwidth compared to prior works when performing DNN computations. Our dMAC units consume up to 34.1 less energy than conventional integer and floating-point MACs that use wide accumulators for all summations. Figure 1 provides an overview of MGS applied to integer and FP8 summation. The novel contributions of this paper are: Analysis of dot product overflows in integer and floating- point quantized neural networks (Section 3 ). Dual-MAC hardware architecture (dMAC) and algorithm for avoiding overflows when using narrow accumulators in integer and FP8 dot products (Sections and 4 and 5). Evaluation of the resulting methods regarding model accu- racy and accumulator compression for classification tasks. We emulate our integer and FP8 dMAC units on both CPU and GPU hardware and evaluate task accuracy and accu- mulator compression for multiple DNNs (Section 6). Energy consumption and area evaluation of dMAC units compared to conventional integer and floating point MACs when implemented in a 7nm process node. (Section 6) 2 Background We present background on both integer and floating-point quanti- zation of DNNs and some prior work on avoiding overflow during DNN execution. 2.1 Integer Quantization We consider the uniform quantization of weight and activation ma- trices per-tensor from FP32 to 𝑏-bit signed values [25]. The floating- point values in a matrix 𝑀have a range 𝑅 𝑚𝑎𝑥(𝑋) 𝑚𝑖𝑛(𝑋). To map values in 𝑀to integers in [0, 2𝑏 1], we partition 𝑅into 2𝑏 1 uniform intervals of length 𝑠𝑥 𝑅 2𝑏 1, also called the scale factor.\n\n--- Segment 5 ---\nThe floating- point values in a matrix 𝑀have a range 𝑅 𝑚𝑎𝑥(𝑋) 𝑚𝑖𝑛(𝑋). To map values in 𝑀to integers in [0, 2𝑏 1], we partition 𝑅into 2𝑏 1 uniform intervals of length 𝑠𝑥 𝑅 2𝑏 1, also called the scale factor. For example, we can map a FP32 activation 𝑥to a value 𝑥𝑞in [0, 2𝑏 1] using the equation 𝑥𝑞 𝑟𝑜𝑢𝑛𝑑( 𝑥𝑓 𝑠𝑥). If the range is asymmetric around zero, we shift 𝑥𝑞by an offset 𝑜𝑥 2𝑏 1 𝑟𝑜𝑢𝑛𝑑( 𝑚𝑖𝑛(𝑋) 𝑠𝑥 ) into the range [ 2𝑏 1, 2𝑏 1 1], guaranteeing that the FP32 value for 0 maps to an integer. We can obtain the approximate FP32 representation of a quantized activa- tion𝑥𝑞by reversing the effect of the scale and offset via the equation 𝑥 𝑠𝑥(𝑥𝑞 𝑜𝑥). Quantized dot products are then performed using the FP32 approximations. 𝑠𝑧(𝑧 𝑜𝑧) 𝐾 𝑖 1 𝑠𝑤(𝑤𝑞 𝑖 𝑜𝑤)𝑠𝑥(𝑥𝑞 𝑖 𝑜𝑥) where 𝑠𝑤, 𝑜𝑤, 𝑠𝑧, and 𝑜𝑧represent the quantization parameters of weights 𝑤and output activations 𝑧.\n\n--- Segment 6 ---\nQuantized dot products are then performed using the FP32 approximations. 𝑠𝑧(𝑧 𝑜𝑧) 𝐾 𝑖 1 𝑠𝑤(𝑤𝑞 𝑖 𝑜𝑤)𝑠𝑥(𝑥𝑞 𝑖 𝑜𝑥) where 𝑠𝑤, 𝑜𝑤, 𝑠𝑧, and 𝑜𝑧represent the quantization parameters of weights 𝑤and output activations 𝑧. Floating point scale factors are factored out and normalized to an integer representation, while weights are typically symmetric around zero with 𝑜𝑤 0 [18, 25, 28, 41]. As a result, several terms under the summation disappear, and the majority of computation arises from the integer dot product 𝑧 Í𝐾 𝑖 1 𝑤𝑞 𝑖𝑥𝑞 𝑖. When FP32 weights and activations are quantized to low-precision (e.g., 8-bit), the computation cost of multiplications 𝑤𝑞 𝑖𝑥𝑞 𝑖decreases significantly. However, the compute bottleneck transitions to the 𝐾dot product summations, as these accumulations are typically executed in higher precision, such as 32-bit, to avoid overflow of the accumulator. For example, assume we accumulate using a 𝑝-bit register where each partial product 𝑤𝑞 𝑖𝑥𝑞 𝑖is 2𝑏-bits and 𝑝 2𝑏. This leaves 𝑝 2𝑏bits leftover for precision during accumulation. Hence, the dot product overflows when 𝐾 2𝑝 2𝑏. However, if we use a narrow accumulator 𝑝 2𝑏, overflow may occur during any of the 𝐾partial sums, leading to inaccurate dot product and poor model accuracy.\n\n--- Segment 7 ---\nHence, the dot product overflows when 𝐾 2𝑝 2𝑏. However, if we use a narrow accumulator 𝑝 2𝑏, overflow may occur during any of the 𝐾partial sums, leading to inaccurate dot product and poor model accuracy. Previous works enable the use of narrow accumulators in DNN computations by retraining the network to reduce partial sum mag- nitude [11, 12, 33] or algorithmically avoiding most overflows [32]. In practice, ML frameworks for quantized DNNs avoid overflow by either using high-precision accumulators (e.g., 32-64 bits) or clipping partial results into a finite range (saturation arithmetic) as they are accumulated [4, 6, 19]. Clipping is cheap to implement in hardware or software, allowing for a modest reduction in accu- mulator precision, e.g., from 32 to 16 bits. However, for narrower bitwidths ( 16), clipping severely degrades numerical accuracy and task performance [11, 32]. 000 0101 1000 0101 1111 0001 1000 0101 1111 0001 1000 0101 0000 0101 1000 0101 0000 0101 1111 Swamped Bits (a) Prepend Leading 1 (b) Subtract Exponents (c) Align Mantissa (Shift) (d) Sum Mantissas 1 111 0001 1 A B 4 (shift amt.) _ 1000 0101 -0.250000 -0.029297 -0.25000 -0.28125 Sign Exponent Mantissa -0.28125 Figure 2: An example of mantissa bit swamping when adding two E4M3 values with different exponents, 𝐴 0.25 and 𝐵 0.029297, while using a narrow 4-bit accumulator. The exponent bias in E4M3 is 7. A s exponent of 5 is larger than B s exponent 1 (b), causing B s mantissa to be shifted left by 5-1 4 bits (c). Since the entire mantissa shifts out, B is treated as zero, and the final result is 0.25, differing from the closest FP8 result of -0.28125 (d).\n\n--- Segment 8 ---\nA s exponent of 5 is larger than B s exponent 1 (b), causing B s mantissa to be shifted left by 5-1 4 bits (c). Since the entire mantissa shifts out, B is treated as zero, and the final result is 0.25, differing from the closest FP8 result of -0.28125 (d). 2 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation 2.2 Floating-Point Quantization FP32 DNN weights and activations may be quantized to lower- precision floating-point formats such as bfloat16, BFP, FP16, or FP8. In particular, FP8 formats for both inference and training have been developed and implemented on several commercial AI accelerators, such as Nvidia H100 GPUs and Intel Gaudi2 [1, 2, 31]. Such formats are now widely used and can achieve baseline FP32 performance on large AI workloads, such as LLMs [14, 34]. FP8 summation involves several steps as shown in Figure 2. Consider the E4M3 format with one sign bit, four exponent bits, and three mantissa bits [31]. When adding two FP8 values with different exponents, the lower order bits of the smaller value are shifted out ( swamped ) due to right-shifting to align exponents with the larger value. This leads to a loss of precision in the final sum. In contrast to floating-point formats with wide mantissas, narrow formats such as E4M3 suffer from a significant loss in numerical accuracy due to swamping. Commercial hardware such as the H100 avoids swamping by accumulating FP8 partial sums in a wider precision such as FP16 or FP32 [2]. There are several classical algorithms for reducing swamping error in floating point summation, including pairwise summation [23] and Kahan summation [26]. Although Kahan summation has higher accuracy, it requires several extra floating point operations to maintain the compensated error term. Meanwhile, pairwise sum- mation is efficient to implement but suffers from large error in narrow floating-point formats. Figure 3 illustrates the need for high-precision accumulation of FP8 dot products.\n\n--- Segment 9 ---\nMeanwhile, pairwise sum- mation is efficient to implement but suffers from large error in narrow floating-point formats. Figure 3 illustrates the need for high-precision accumulation of FP8 dot products. Using several summation algorithms, we perform dot products between two Gaussian vectors in FP8 precision (4-bit mantissa accumulator) and plot the numerical error relative to the baseline FP32 accumulation (24-bit mantissa accumulator). Sequen- tial summation loses all accuracy after only 200 sums. Pairwise summation is significantly more accurate than sequential summa- tion but still exhibits up to 50 error for longer dot products. In Section 5.2, we discuss how to accumulate FP8 mantissas in low- precision for a majority of sums while attaining numerical accuracy on-par with FP32 accumulation. 3 Analysis of Dot-Product Overflows We begin by providing an analytical framework for reasoning about overflows. We define two types of integer overflow and discuss multiple algorithms for avoiding them. Definition 3.1 (Transient Overflow). Overflow that may occur at any point during the sequential summation of 𝑘integers 𝑋 {𝑥1,𝑥2, ...,𝑥𝑘} when using a 𝑏-bit accumulator. Definition 3.2 (Persistent Overflow). Overflow that occurs when the final sum 𝑦 Í𝑘 𝑖 1 𝑥𝑖overflows a 𝑏-bit accumulator. Note that transient overflows may occur even when there is no persistent overflow. We aim to minimize these transient overflows. 3.1 Avoiding All Overflows Several prior works aim to avoid both persistent and transient over- flows entirely by retraining the neural network such that partial sums are always within the accumulator bounds. A2Q [11] and 200 400 600 800 1000 Dot Product Length 0 10 20 30 40 50 60 70 80 90 100 Relative Error ( ) Relative Error of FP8 Gaussian Vector Dot Products for Different Summations Algos Sequential Pairwise MGS w Narrow Accum Figure 3: Error, relative to FP32 precision, of Gaussian vec- tor dot products performed in FP8 precision. We execute each algorithm using solely a narrow accumulator and clip par- tial sums upon overflow.\n\n--- Segment 10 ---\nA2Q [11] and 200 400 600 800 1000 Dot Product Length 0 10 20 30 40 50 60 70 80 90 100 Relative Error ( ) Relative Error of FP8 Gaussian Vector Dot Products for Different Summations Algos Sequential Pairwise MGS w Narrow Accum Figure 3: Error, relative to FP32 precision, of Gaussian vec- tor dot products performed in FP8 precision. We execute each algorithm using solely a narrow accumulator and clip par- tial sums upon overflow. All algorithms exhibit significant errors due to the swamping of lower order bits when using reduced-precision accumulators. MGS has lower error than pairwise summation by separating partial product mantissas by exponent and accumulating them in separate narrow ac- cumulators. This means that dot product errors result only from clipping overflows. However, the 35 error of MGS, when restricted to a narrow accumulator, is unacceptable for DNN applications. A2Q [12] eliminate the possibility of both transient and persis- tent overflows by constraining the weight vector s L1-norm during quantization-aware training (QAT). They first bound the dot prod- uct result : 𝐾 𝑖 1 𝑤𝑖𝑥𝑖 𝐾 𝑖 1 𝑤𝑖 𝑥𝑖 2𝑝 1 1 In the worst case, all activations are maximal 𝑥𝑖 2𝑏 1 and the weight L1-norm may be bounded such that: 𝑘 𝑖 1 𝑤𝑖 w 1 2𝑝 1 1 2𝑏 1 This bound acts as an L1-regularizer and pulls most weight values toward zero, ensuring that partial sums never grow beyond 𝑝bits. L1 regularization promotes unstructured sparsity in the weight matrices, reducing the model size and enabling acceleration by skipping zero computations. However, network sparsification may reduce model accuracy [30] Meanwhile, retraining a pre-trained DNN to satisfy accumulator constraints may alter properties of the pre-trained model, such as algorithmic fairness guarantees [42]. We find that enforcing strict bounds on weight magnitude is not necessary for using narrow accumulators.\n\n--- Segment 11 ---\nHowever, network sparsification may reduce model accuracy [30] Meanwhile, retraining a pre-trained DNN to satisfy accumulator constraints may alter properties of the pre-trained model, such as algorithmic fairness guarantees [42]. We find that enforcing strict bounds on weight magnitude is not necessary for using narrow accumulators. 3.2 Avoiding Transient Overflows Persistent overflow is a true overflow where the final result is simply too large for the accumulator. Transient overflows are temporary and arise when a partial sum overflows but where the final sum may not actually overflow the accumulator. Hence, in the absence of persistent overflow, we should be able to eliminate transient overflows by reordering the summation. 3 Vikas Natesh, H.T. Kung, and David Kong Theorem 3.3. Let 𝑋 {𝑥1,𝑥2, ...,𝑥𝑘} be a list of 𝑘signed integers, where each 𝑥𝑖is represented using 𝑛bits. Let 𝑦 Í𝑘 𝑖 1 𝑥𝑖be the sum of all elements in 𝑋, representable using 𝑚 𝑛 1 bits without persistent overflow (i.e., 2𝑚 1 𝑦 2𝑚 1 1). Then, there exists an ordering of summation for 𝑋that avoids transient overflow when using an 𝑚-bit accumulator. Proof. Suppose 𝑘 2. The list 𝑋 {𝑥1,𝑥2} contains 𝑛-bit numbers, and its sum 𝑥1 𝑥2 (or 𝑥2 𝑥1) can require at most 𝑛 1 bits. Since 𝑚 𝑛 1, the sum does not overflow 𝑚-bits, and the theorem holds for 𝑘 2. Let 𝑙 2 and assume the theorem holds 𝑘 𝑙, i.e., there exists an ordering of𝑋 {𝑥1,𝑥2, ...,𝑥𝑙} such that the sum of elements of𝑋 w.r.t.\n\n--- Segment 12 ---\nSince 𝑚 𝑛 1, the sum does not overflow 𝑚-bits, and the theorem holds for 𝑘 2. Let 𝑙 2 and assume the theorem holds 𝑘 𝑙, i.e., there exists an ordering of𝑋 {𝑥1,𝑥2, ...,𝑥𝑙} such that the sum of elements of𝑋 w.r.t. said ordering avoids transient overflow (inductive hypothesis). Denote this ordering by the index set 𝛼𝑙and the so-ordered list by 𝑋𝛼𝑙. Suppose that 𝑘 𝑙 1 and 𝑋 𝑋𝛼𝑙 𝑥𝑘. Then, 𝑦 𝑘 𝑖 1 𝑥𝑖 𝑥𝑘 𝑖 𝛼𝑙 𝑥𝑖 By the inductive hypothesis, the second term in the sum, denoted by ˆ𝑦 Í 𝑖 𝛼𝑙𝑥𝑖, avoids transient overflow. Since ˆ𝑦is an 𝑚-bit signed integer, and 𝑥𝑘is an 𝑛-bit signed integer with 𝑛 𝑚 1, the sum 𝑦 𝑥𝑘 ˆ𝑦is represented by at most 𝑚bits. Therefore, a feasible ordering to avoid transient overflow is 𝛼𝑘 {𝛼𝑙,𝑘}. Thus, by induction, our theorem must hold for any 𝑘 2. The proof shows how to construct a summation sequence with- out transient overflow by building the right permutation sequence at each step. One example of such an algorithm is first to sort the 𝑘 values, divide them into a list of negative values and a list of positive values, and repeatedly form the sum of the largest positive and most negative values. We can then take the resulting list, with length at least 𝑘 2, and apply the algorithm recursively until a single pair of values remains.\n\n--- Segment 13 ---\nOne example of such an algorithm is first to sort the 𝑘 values, divide them into a list of negative values and a list of positive values, and repeatedly form the sum of the largest positive and most negative values. We can then take the resulting list, with length at least 𝑘 2, and apply the algorithm recursively until a single pair of values remains. This method is guaranteed to avoid transient overflow while using the narrowest possible accumulator as the running sum increases monotonically. Performing summations in a sorted order is also beneficial for retaining floating point accuracy since adding pairs of values of similar magnitude reduces the num- ber of bits swamped in the smaller value [16]. However, sorting before adding becomes expensive in DNN applications where dot product lengths may exceed 4096. AGS is a recent method to avoid transient overflow by reorder- ing in integer-quantized DNNs [32]. AGS first splits the sequence by sign into a positive list and negative list, then alternates sum- ming values from either the negative or positive list depending on whether the accumulator overflows its maximum or minimum value, respectively. This allows AGS to avoid transient overflows while also avoiding sorting and using only an extra bit for over- flow detection. However, AGS may require additional registers or memory to buffer partial products. For example, once an overflow is detected, AGS may need to buffer several positive values while waiting for a negative partial product to arrive. The extra mem- ory requirements may overwhelm the benefits of using a narrow accumulator, challenging AGS hardware implementation. 4 Markov Greedy Summation In this section, we detail how our proposed MGS avoids all over- flows while using narrow accumulators for the majority of sum- mations. We show that since weights and activations are normally distributed or half-normally distributed, the chance of overflow during summation is actually low. We then derive the expected number of summations before overflow by modeling the running sum as a random walk. 0 10 20 30 40 50 Dot Product Length 0.0 0.2 0.4 0.6 0.8 Pr(Overflow) (a) Prob.\n\n--- Segment 14 ---\nWe then derive the expected number of summations before overflow by modeling the running sum as a random walk. 0 10 20 30 40 50 Dot Product Length 0.0 0.2 0.4 0.6 0.8 Pr(Overflow) (a) Prob. of Overflow For Different Accum Bitwidths accum 8 accum 9 accum 10 accum 11 accum 12 accum 13 0 5 10 15 20 25 30 Layer Index 7.6 7.8 8.0 8.2 8.4 8.6 8.8 9.0 9.2 Average Accum Bitwidth (b) Average Accumulator Bitwidth During MobileNetV2 Inference Figure 4: (a) We estimate the probability of overflow based on the model described in Section 4.1, when performing dot product at different accumulator bitwidths. 5-bit Gauss- ian weights in the range[-15,15] are multiplied with 7-bit Gaussian activations in [-63,63] to yield partial products 𝑍 𝑁(0,𝑘 𝜎𝑤𝜎𝑥). We set 𝜎of weights and data such that the extreme values lie 3 𝜎 s away from the mean 0, i.e., 𝜎𝑤 15 3 5 and 𝜎𝑥 63 3 21. The figure shows that despite 7 5 12-bit partial products, we can use accumulators with 12 bits for most sums before overflow. For example, there is only a 12 chance of overflow when summing 10 elements in a narrow 10-bit accumulator. In (b), we plot the average accumulator bitwidth when running MobileNetv2 inference with 5-bit weights and 7-bit activations. Although one would expect that at least 5 7 12 bits are required to prevent overflow, the average accumulator bitwidth required varies between 7 and 10 bits. 4.1 Estimating the Probability of Integer Overflow We consider 𝑏-bit quantized neural network dot products 𝑍 Í𝑘 𝑖 1 𝑤𝑖𝑥𝑖with weights and activations in the range [ 2𝑏 1, 2𝑏 1].\n\n--- Segment 15 ---\nAlthough one would expect that at least 5 7 12 bits are required to prevent overflow, the average accumulator bitwidth required varies between 7 and 10 bits. 4.1 Estimating the Probability of Integer Overflow We consider 𝑏-bit quantized neural network dot products 𝑍 Í𝑘 𝑖 1 𝑤𝑖𝑥𝑖with weights and activations in the range [ 2𝑏 1, 2𝑏 1]. Weight and input activation vectors 𝑤and 𝑥are truncated, zero- centered i.i.d normal distributions 𝑁(𝜇𝑤 0, 𝜎𝑤) and 𝑁(𝜇𝑥 0, 𝜎𝑥), respectively. Input activations may also be half-normal dis- tributions due to ReLU operations in the previous layer. The partial products 𝑝𝑖 𝑤𝑖𝑥𝑖are i.i.d product-normal distributions with 𝜇𝑝 0 and 𝜎2𝑝 (𝜎2𝑤 𝜇2𝑤)(𝜎2𝑥 𝜇2𝑥) 𝜇2𝑤𝜇2𝑥 𝜎2𝑤𝜎2𝑥. The sum- ming of partial products can be represented by the random variable 𝑍 Í𝑘 𝑖 1 𝑝𝑖. By the central limit theorem (CLT), for large enough 𝑘, 𝑍 𝑁(0,𝑘 𝜎𝑤𝜎𝑥).\n\n--- Segment 16 ---\nThe sum- ming of partial products can be represented by the random variable 𝑍 Í𝑘 𝑖 1 𝑝𝑖. By the central limit theorem (CLT), for large enough 𝑘, 𝑍 𝑁(0,𝑘 𝜎𝑤𝜎𝑥). This enables us to approximate the probability of overflow given a particular dot product length 𝑘and accumulator bitwidth 𝑎. 𝑃𝑟( 𝑍 2𝑎 1) 2Φ 2𝑎 1 𝜎𝑤𝜎𝑥 𝑘 4 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation where Φ is the CDF of the standard normal distribution. Figure 4a displays the probability of overflow for different vector lengths and accumulator bitwidths when performing dot product with 5-bit weights and 7-bit activations. The figure shows that for relatively long dot products, such as 10 or 15 elements, the chance of overflow is relatively low, even for narrow accumulators. In Figure 4b, we empirically observe that the average accumulator bitwidth is small across DNN layers, suggesting that wide accumulators may not be needed for a majority of sums. 4.2 Computing the Expected Number of Overflows The approximation above provides a loose bound showing that overflow is relatively rare, even with narrow accumulators. We can derive the expected number of summations before overflow more precisely by modeling summation as a random walk, specifically a Markov chain with a single absorbing state representing overflow. To illustrate the idea, consider the summation of integers from the range [-2,2] using an accumulator that can only hold values in [-2,2]. In each step, we randomly select an integer from the range [-2,2] and add it to the accumulator. We stop when the accumulator overflows out of range [-2,2], i.e., we enter the absorbing state. Once entered, the process cannot leave the absorbing state. Hence, the random walk will eventually end as the accumulator is permanently absorbed into an overflow state.\n\n--- Segment 17 ---\nOnce entered, the process cannot leave the absorbing state. Hence, the random walk will eventually end as the accumulator is permanently absorbed into an overflow state. A 6 6 transition matrix 𝑃repre- sents the probabilities of entering different states given the current sum, with each row summing to 1. P Input State -2 -1 0 1 2 Ovfl Output State -2 1 5 -1 1 5 0 1 5 1 0 2 0 Ovfl 2 5 1 5 1 5 1 5 1 5 0 1 5 1 5 1 5 1 5 1 5 1 5 0 0 1 5 1 5 1 5 1 5 1 5 0 0 1 5 1 5 1 5 2 5 0 0 0 0 0 1 For example, the 5th row represents the probability of different output states given the starting accumulator value of 2. The value 2 may be summed with either 1 or 2 with probability 2 5 as both are equally likely to be the next state (uniform random draws). Since 2 1 3 and 2 2 4 both overflow the accumulator, we enter the overflow state (ovfl column) with probability 2 5. The last row shows that if we start in an overflow state, we will remain in that state surely. We can represent the transition matrix 𝑃in a blocked form: 𝑃 𝑄 𝑅 0 𝐼 where 0 is a zero matrix and 𝐼is the identity matrix. 𝑅represents the transitions from transient states to absorbing states, and 𝑄 represents the transitions between transient states. To compute the transition probabilities after 𝑘steps, we simply multiply 𝑃by itself 𝑘times. 𝑃𝑘 𝑄𝑘 𝑅 𝑄𝑅 ... 𝑄𝑘𝑅 0 𝐼 0 (𝐼 𝑄) 1𝑅 0 𝐼 𝑄𝑘 0 reflects the fact that the random walk will eventually end, i.e., eventually there is zero probability of being in a non-absorbed state.\n\n--- Segment 18 ---\nTo compute the transition probabilities after 𝑘steps, we simply multiply 𝑃by itself 𝑘times. 𝑃𝑘 𝑄𝑘 𝑅 𝑄𝑅 ... 𝑄𝑘𝑅 0 𝐼 0 (𝐼 𝑄) 1𝑅 0 𝐼 𝑄𝑘 0 reflects the fact that the random walk will eventually end, i.e., eventually there is zero probability of being in a non-absorbed state. The fundamental matrix of the Markov chain 𝑁 (𝐼 𝑄) 1 represents the expected number of visits to non-absorbing state 𝑗 [ 2, 2] starting from non-absorbing state 𝑖 [ 2, 2], before absorption. The accumulator starts with the value 0, varies across different non-absorbing states with each partial sum, and eventually overflows. The expected number of steps to reach overflow is simply the sum of the entries in row 3 of 𝑁, corresponding to the state 0. This sum represents the total expected number of visits to all non- absorbing states before absorption, i.e., the total expected number of sums we may perform before overflow. We apply our random walk model to dot product accumulation when executing quantized MobileNetV2 inference on Imagenet1K with 5-bit weights and 7-bit activations [15, 36]. Since weights and activations may deviate slightly from normal, we compute transition probabilities using their empirical distributions during DNN inference. Figure 5 plots the empirical versus modeled average summation length before overflow in a 1x1 convolution layer in the 13th residual block. When summing partial products derived from multiplying 5-bit weight and 7-bit activations, we expect that 5 7 12 bit accumulation is required. However, Figure 5 shows one may use a narrow 9-bit accumulator to sum 10 values before needing to use a wider accumulator, on average.\n\n--- Segment 19 ---\nWhen summing partial products derived from multiplying 5-bit weight and 7-bit activations, we expect that 5 7 12 bit accumulation is required. However, Figure 5 shows one may use a narrow 9-bit accumulator to sum 10 values before needing to use a wider accumulator, on average. 15 10 5 0 5 10 15 0 2000 4000 6000 8000 Weight Distribution 60 65 70 75 80 85 90 0 5000 10000 15000 20000 25000 Input Activation Distribution 600 400 200 0 200 400 600 0 20 40 60 80 Partial Product Distribution 5 6 7 8 9 10 Accum Bitwidth 0 10 20 30 40 50 Mean Summation Length Modelled vs Measured Summation Length Before Overflow in MobileNetV2 Measured During Inference Modelled by Random Walk Figure 5: Plotting the empirical measured average dot prod- uct length versus expected dot product length based on our random walk model. 5-bit Weights follow a normal distri- bution in the range [-15, 15], while 7-bit activations have a half-normal distribution in the range [0,127] after ReLU. Note that the plot shows that with the accumulation bitwidth equal to 10, we do not expect overflow at a summation length of about 32. In contrast, a naive analysis would conclude that 17 5 7 5 bits are required to avoid overflows, noting that 5 log2 32. 5 Vikas Natesh, H.T. Kung, and David Kong 5 Dual-Accumulator MAC Design In this section, we describe the hardware for dual-multiply-accumulate (dMAC) units, leveraging our observation that the majority of dot product sums do not overflow when using narrow accumulators. We first introduce the dMAC for integer dot products and then show how this design enables narrow accumulation in FP8. 5.1 Integer dMAC The integer dMAC unit uses a narrow adder (green in Figure 6) for most summations and a wide adder (red) to handle partial sums that overflow the narrow adder. It has a slightly higher area over- head than a conventional MAC unit, containing two adders and additional overflow handling logic. However, dMAC consumes sig- nificantly less dynamic power by exploiting the low overflow rate in DNN dot products. In addition, we clock-gate the wider accumu- lator to reduce dynamic power usage further when not performing wide accumulations.\n\n--- Segment 20 ---\nHowever, dMAC consumes sig- nificantly less dynamic power by exploiting the low overflow rate in DNN dot products. In addition, we clock-gate the wider accumu- lator to reduce dynamic power usage further when not performing wide accumulations. Figure 6 displays our integer dMAC design when multiplying 4-bit weights and activations using 8-bit and 32-bit adders. After multiplication, the product 𝑝is accumulated in an 8-bit register 𝑎8. If the 8-bit adder s carry-out overflow flag is set, we accumulate 𝑎8 in the wider 32-bit register 𝑎32 instead and write 𝑝to 𝑎8. Once all the partial products have been accumulated, we add 𝑎8 and 𝑎32 and return the output. 32-bit adder 1 0 a32 a8 p GND Multiplier act 4 weight 4 8 8 8 8-bit adder 1 1 0 8 8 8 Dual-MAC (dMAC) done 1 0 1 32 32 GND OUT Oflow Figure 6: Dual accumulator MAC hardware unit (dMAC) with output-stationary behavior. In this example, 4-bit weights and data arrive for multiplication and summation with an 8-bit accumulator 𝑎8. If an overflow occurs (oflow 1), 𝑎8 is summed into the wider 32-bit accumulator 𝑎32, and the 8-bit partial product is written to 𝑎8. Upon completing the dot product (done 1), we return the sum of 𝑎8 and 𝑎32. 5.2 8-bit Floating-Point dMAC Existing hardware for FP8 MAC operations accumulate partial prod- ucts in higher precision such as FP32 [1, 2] This not only requires the use of wide mantissa adders but also FP8- FP32 data conver- sions and FP32 normalizations. Figure 7 provides a high-level view of the difference between our hardware and existing FP8 MAC units. We show that using dMACs for mantissa accumulation can avoid several expensive operations in wide registers while maintaining numerical accuracy. Figure 8 displays our FP8 dMAC design. A new weight and activa- tion in the E4M3 format arrive each cycle.\n\n--- Segment 21 ---\nFigure 8 displays our FP8 dMAC design. A new weight and activa- tion in the E4M3 format arrive each cycle. After multiplication and FP8 Multiply FP8 ---- FP32 FP32 Normalize Round FP32 Accumulation FP32 Normalize Round FP8 Multiply FP8 Normalize Round FP8 Accumulation dMAC-FP8 FP8 Figure 7: High-level view of operations in our dMAC-FP8 unit versus conventional FP8 MACs. The gray boxes represent the operations that must occur every time a pair of values arrives. Conventional FP8 incurs overhead from data conversion and wide accumulation and normalization operations. In con- trast, dMAC-FP8 performs the majority of computation in narrower precision while amortizing the cost of normaliza- tion across multiple partial summations. rounding, the partial product sign bit converts the 4-bit mantissa (with leading 1) to 5-bit signed 2 s complement. Using a narrow 5-bit adder, we then accumulate the mantissa into one of 16 5-bit registers based on its 4-bit exponent, which ranges from 0 to 15. By accumulating mantissas of the same exponent in the same register, we avoid the shifting operations required when adding two FP8 values with differing exponents while also avoiding numerical error from swamping (see 2). When the 5-bit adder overflows, we left-shift the accumulator by its exponent and accumulate into a wide 32-bit accumulator. Left-shifting by the exponent forces the 5-bit accumulator value to have exponent zero, allowing partial sums with different expo- nents to be added into the same wide register without error. Since overflows are rare, dMAC amortizes the shifting cost of mantissa alignment over several summations instead of between each pair of elements as in conventional FP32. Once the dot product is complete, the values in each accumulator register are left shifted by their respective exponent and summed into the 32-bit accumulator. This 16 shift add operation is only performed once per dot product. Finally, the result is normalized, rounded, and returned. 5.3 Subnormal Gating Zero-gating hardware, such as those proposed in [45] and [8] do not perform multiplication when a zero operand is loaded. With zero- gating approaches, processing elements are forced to remain idle, saving computation energy.\n\n--- Segment 22 ---\n5.3 Subnormal Gating Zero-gating hardware, such as those proposed in [45] and [8] do not perform multiplication when a zero operand is loaded. With zero- gating approaches, processing elements are forced to remain idle, saving computation energy. Meanwhile, zero-skipping approaches such as [21] and [9] avoid loading zero operands from memory altogether. We exploit the range of possible FP8 products to avoid MAC operations on zeros and small inputs to the dMAC unit. Given the 256 possible values in the FP8 range, the number of possible partial product pairs in FP8 is 256 2 32640. Of these possible pairs, 1280 lead to product magnitudes that are too small to be represented in the FP8 subnormal range and ultimately round to zero, i.e., 𝑤 𝑥 2 9. Moreover, as weights and activations may be 6 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation Exp Mant Exponent DEMUX Normalize Round . . . 16 Narrow Accumulator Registers for Each Exponent 1 Wide Accumulator Register Register Narrow Adder Wide Adder Left Shifter Oflow Figure 8: FP8 dMAC hardware unit. We display the FP8 ac- cumulation step in the computation. A new partial product arrives every cycle. Partial product mantissas are written to one of 16 registers such that they are always accumulated with other values of the same exponent, thereby preventing swamping while enabling the use of the narrow accumulator (green) for most summations. When the narrow accumula- tor overflows, it is left-shifted and summed with the wide accumulator (red) to prevent precision loss. normally distributed with mean 0, several partial products will be 0 or close to 0. We implement logic to check whether input exponents are small enough that the product lies outside the subnormal range. We then skip these MAC operations for additional dynamic power savings. 6 Evaluation Our evaluation is divided into two parts. In Section 6.2, we compare MGS to state-of-the-art (SOTA) works in accumulator compres- sion when performing inference on various image classification networks. We show that MGS can significantly reduce accumula- tor bitwidth while achieving accuracy on par with FP32 baselines, without retraining.\n\n--- Segment 23 ---\nIn Section 6.2, we compare MGS to state-of-the-art (SOTA) works in accumulator compres- sion when performing inference on various image classification networks. We show that MGS can significantly reduce accumula- tor bitwidth while achieving accuracy on par with FP32 baselines, without retraining. Then, in Section 6.4, we implement dMAC units for INT8 and FP8 in a 7nm node and measure power consumption relative to conventional MACs. dMACs can reduce inference power consumption by up to 34.1 . 6.1 FP8 Emulation Library Prior works have addressed the difficulty of efficiently profiling overflows due to lack of support in standard deep learning frame- works [11, 33]. We have built a C CUDA library to emulate dMAC FP8 on both CPUs and GPUs to run experiments quickly. We ex- tend PyTorch s quantization framework with custom linear and convolution layers implementing MGS for INT8 and FP8 quanti- zation to measure the impact on model accuracy. We unroll dot product computations, allowing users to vary weight, activation, and accumulator bitwidths and evaluate overflow solutions such as MGS, clipping, or wraparound arithmetic. Table 1: Imagenet1K Top-1 Accuracy Unit MobileNetV2 ResNet-18 ViT-Small Baseline (FP32) 71.6 70.58 80.08 INT8 69.7 68.45 79.17 FP8 71.04 70.12 80.02 dMAC 71.10 70.11 80.07 6.2 Reducing Accumulator Bitwidth In this section, we evaluate the ability of MGS to enable low- resolution accumulation while maintaining FP32 model accuracy in MobileNetV2 [24], ResNet-18 [22], and ViT [17] on CIFAR10 [27] and ImageNet [15]. While our evaluation focuses on FP8 inference, MGS may also be applied to other data formats to reduce accumu- lator bitwidth, e.g., during training with the E5M2 FP8 datatype. 6.2.1 8-Bit Integer Quantized DNNs. We sweep the design space by varying weight and activations from 5 to 8 bits while varying the accumulator bitwidth from 8 to 20.\n\n--- Segment 24 ---\n6.2.1 8-Bit Integer Quantized DNNs. We sweep the design space by varying weight and activations from 5 to 8 bits while varying the accumulator bitwidth from 8 to 20. We select the best-performing models with the lowest required accumulator bitwidth to generate a Pareto frontier. For models on the frontier, we use our software li- brary to evaluate accuracy compared to SOTA methods A2Q, A2Q , AGS, and overflow clipping [4, 6, 11, 12, 19, 32]. Figure 9 shows that MGS can push the accumulator bit width lower than A2Q while also maintaining task performance. The magenta lines show that clipping transient overflows within dot products can limit how much we may reduce accumulator bitwidth. AGS accurately avoids transient overflows but clips persistent over- flows, leading to accuracy drops at lower bitwidth where clipping becomes more prevalent. 6.2.2 8-Bit Floating Point Quantized DNNs. We evaluate MGS when performing FP8 inference using our target models. We employ the E4M3 datatype and fix the narrow accumulator bitwidth at five signed bits. Table 1 shows that MGS accuracy is on par with SOTA methods for FP8. This is expected since MGS always falls back to using a wide accumulator upon overflow and does not lose accuracy due to swamping. 6.3 dMAC FPGA Prototyping To estimate resource utilization for the MAC units, we prototype the designs on an AMD Virtex-7 VC707 FPGA. Table 2 compares the resource utilization of the FPGA MAC unit implementations in terms of look-up tables (LUTs) and flip-flops (FFs). The area of our INT8 dMAC is slightly higher than the conventional INT8 MAC since our unit contains an extra adder and overflow handling logic to use both accumulators. Although some dMAC designs may have higher resource utilization, they always achieve power savings as detailed in the accurate power, performance, and area characterization of ASIC implementations of the MAC units detailed in Section 6.4. 6.4 dMAC ASIC Physical Implementation In addition to comparing the FPGA implementation of the designs, we compare the ASIC implementations for accurate power, perfor- mance and area characterization. To compare the performance of 7 Vikas Natesh, H.T.\n\n--- Segment 25 ---\n6.4 dMAC ASIC Physical Implementation In addition to comparing the FPGA implementation of the designs, we compare the ASIC implementations for accurate power, perfor- mance and area characterization. To compare the performance of 7 Vikas Natesh, H.T. Kung, and David Kong 9 10 11 12 13 14 15 16 17 18 19 20 Accumulator Bit Width 75.0 77.5 80.0 82.5 85.0 87.5 90.0 92.5 Top-1 Accuracy ( ) (a) MobileNetV2 on CIFAR-10 MGS AGS A2Q A2Q Clip FP32 Baseline 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Accumulator Bit Width 60 65 70 75 80 85 90 95 Top-1 Accuracy ( ) (b) ResNet-18 on CIFAR-10 MGS AGS A2Q Clip FP32 Baseline 10 11 12 13 14 15 16 17 18 19 20 Accumulator Bit Width 60 65 70 75 80 85 90 Top-1 Accuracy ( ) (c) ViT-Small on CIFAR-10 MGS AGS Clip FP32 Baseline Figure 9: Comparing INT8 MGS to SOTA methods for low-precision accumulation during quantized inference using several models. We sweep weight and activation bitwidths from 5 to 8 bits while varying the accumulator from 8 to 20 bits. We then plot the best-performing models with the lowest required accumulator bitwidth. Since MGS uses both narrow and wide accumulators during the dot product, we plot the average accumulator bitwidth when running MGS. In principle, MGS can indefinitely reduce the narrow accumulator bitwidth as it always falls back on the wide accumulator. However, we stop reducing accumulator bitwidth for MGS when additional reduction increases the average bitwidth (using the wide accumulator more often) and instead start clipping those overflows. By using narrow accumulators for the majority of sums, MGS can reduce accumulator bitwidth beyond the SOTA.\n\n--- Segment 26 ---\nHowever, we stop reducing accumulator bitwidth for MGS when additional reduction increases the average bitwidth (using the wide accumulator more often) and instead start clipping those overflows. By using narrow accumulators for the majority of sums, MGS can reduce accumulator bitwidth beyond the SOTA. Table 2: FPGA MAC unit resource utilization comparison Unit FPGA LUTs FPGA FFs INT8 MAC 107 81 INT8 dMAC 126 79 FP8 MAC 457 335 FP8 dMAC (w o skipping) 165 143 FP8 dMAC (w skipping) 180 143 each MAC unit, we perform the full physical implementation of the designs at the 7 nm node using the ASAP7 PDK [10] with a 0.7 V supply voltage. Figure 10 shows the layouts and dimensions of the MAC units and their respective areas. To balance switching speed and power consumption, we implement the design using standard threshold voltage transistors targeting a clock frequency of 500 MHz. We synthesize the designs using Cadence Genus [7], perform implementation using Cadence Innovus, run gate-level simulations with representative weights for each workload using Synopsys VCS [40], and characterize power consumption based on transient behavior using Cadence Voltus. The power comparison of the designs is shown in Table 3. As expected, the static leakage power scales with area. Comparing INT8 dMAC with INT8 MAC (with traces from MobileNetV2), we see that although the area for INT8 dMAC is 14.5 larger and leak- age power is 16.4 higher from additional logic, we still achieve a 15.4 total power savings. The FP8 dMAC unit without skipping consumes the least area, the FP8 dMAC unit with skipping con- sumes slightly more area for skipping logic, and the baseline FP8 MAC unit consumes the most area. When comparing total power for FP8 (with traces from ViT), the dMAC achieves 33.6 and 34.1 total power savings without and with skipping, respectively. Our zero-skipping approach implements logic to check input exponents before skipping state machine stages within the full MAC operation.\n\n--- Segment 27 ---\nWhen comparing total power for FP8 (with traces from ViT), the dMAC achieves 33.6 and 34.1 total power savings without and with skipping, respectively. Our zero-skipping approach implements logic to check input exponents before skipping state machine stages within the full MAC operation. Further savings could be made with more aggressive power-saving Figure 10: Layouts and dimensions of ASIC implementations of the MAC units at the 7 nm node: (a) is INT8 MAC, (b) is INT8 dMAC, (c) is FP8 MAC, (d) is FP8 dMAC without skipping, and (e) is FP8 dMAC with skipping. techniques such as power-gating (where unused parts of the circuit are turned off) [39] or dynamic voltage and frequency scaling [38]. 7 Conclusion The paper introduced MGS to reduce the required accumulation bitwidth in performing dot products that form the bulk of DNN computations. Based on the statistical properties of weight and acti- vation distributions, we can sum many partial products in reduced precision before overflow occurs. Specifically, MGS uses a narrow accumulator to accumulate as many values as possible while falling back on a wider accumulator when the rare overflow occurs. We have designed dual-multiply-accumulate (dMAC) hardware units that use narrow accumulators for most sums, resulting in a nar- rower average accumulator bitwidth compared to prior works. Our 8 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation Table 3: ASIC MAC unit power results at 500 MHz showing that 15.4 , 33.6 , and 34.1 savings are achieved for INT8, FP8 without skipping, and FP8 with skipping, respectively.\n\n--- Segment 28 ---\nWe have designed dual-multiply-accumulate (dMAC) hardware units that use narrow accumulators for most sums, resulting in a nar- rower average accumulator bitwidth compared to prior works. Our 8 MGS: Markov Greedy Sums for Accurate Low-Bitwidth Floating-Point Accumulation Table 3: ASIC MAC unit power results at 500 MHz showing that 15.4 , 33.6 , and 34.1 savings are achieved for INT8, FP8 without skipping, and FP8 with skipping, respectively. Unit Dynamic Power (𝜇W) Static Power (𝜇W) Total Power (𝜇W) Power Saving INT8 MAC 27.41 0.073 27.48 baseline INT8 dMAC 23.16 0.085 23.25 15.4 FP8 MAC 97.12 0.249 97.37 baseline FP8 dMAC (w o skipping) 64.44 0.226 64.66 33.6 FP8 dMAC (w skipping) 63.92 0.232 64.15 34.1 dMAC units consume significantly less power than conventional integer and floating-point MACs that use wide accumulators for all summations while achieving classification accuracy on par with high-precision floating-point baselines for multiple image classifi- cation tasks. References [1] 2022. Habana Gaudi2 White Paper. Gaudi2 20White 20Paper.pdf [2] 2022. NVIDIA H100 Tensor Core GPU Architecture White Paper. https: resources.nvidia.com en-us-tensor-core [3] 2023. GAP8 IoT Application Processor. gap8_mcu_ai . [4] Arm Limited 2022. ARM CMSIS Library. Arm Limited. com tools-and-software embedded cmsis [5] Arm Limited 2022. Cortex-M4 Technical Reference Manual. Arm Limited. https: developer.arm.com documentation ddi0439 b [6] Arm Limited 2024. Arm Neon technology, the Advanced SIMD (Single Instruction Multiple Data) architecture extension for implementation of the Armv8-A or Armv8- R architecture profiles. Arm Limited. instruction-sets intrinsics [7] Cadence Design Systems. 2024. Cadence Innovus Implementation Sys- tem.\n\n--- Segment 29 ---\n2024. Cadence Innovus Implementation Sys- tem. soc-implementation-and-floorplanning innovus-implementation-system.html [8] Yu-Hsin Chen, Joel Emer, and Vivienne Sze. 2016. Eyeriss: A Spatial Architec- ture for Energy-Efficient Dataflow for Convolutional Neural Networks. In 2016 ACM IEEE 43rd Annual International Symposium on Computer Architecture (ISCA). 367 379. [9] Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze. 2019. Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices. IEEE Journal on Emerging and Selected Topics in Circuits and Systems 9, 2 (June 2019), 292 308. [10] Lawrence T Clark, Vinay Vashishtha, Lucian Shifren, Aditya Gujja, Saurabh Sinha, Brian Cline, Chandarasekaran Ramamurthy, and Greg Yeric. 2016. ASAP7: A 7-nm finFET predictive process design kit. Microelectronics Journal 53 (2016), 105 115. [11] Ian Colbert, Alessandro Pappalardo, and Jakoba Petri-Koenig. 2023. A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance. 2023 IEEE CVF International Conference on Computer Vision (ICCV) (2023), 16943 16952. [12] Ian Colbert, Alessandro Pappalardo, Jakoba Petri-Koenig, and Yaman Umuroglu. 2024. A2Q : improving accumulator-aware weight quantization. In Proceedings of the 41st International Conference on Machine Learning (Vienna, Austria) (ICML 24). JMLR.org, Article 369, 17 pages. [13] Barry de Bruin, Zoran Zivkovic, and Henk Corporaal. 2020. Quantization of deep neural networks for accumulator-constrained processors. Microprocess. Microsystems 72 (2020). [14] DeepSeek-AI. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.\n\n--- Segment 30 ---\n2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948 [cs.CL] 2501.12948 [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Im- ageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition. 248 255. 2009.5206848 [16] L. C. W. Dixon and D. J. Mills. 1994. Effect of rounding errors on the variable metric method. J. Optim. Theory Appl. 80, 1 (Jan. 1994), 175 179. org 10.1007 BF02196600 [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv abs 2010.11929 (2020). [18] PyTorch Foundation. [n. d.]. PyTorch Quantization. [19] Angelo Garofalo, Manuele Rusci, Francesco Conti, Davide Rossi, and Luca Benini. 2019. PULP-NN: accelerating quantized neural networks on parallel ultra-low- power RISC-V processors. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 378, 2164 (Dec. 2019), 20190155. [20] Jorge Gomez, Saavan Patel, Syed Shakib Sarwar, Ziyun Li, Raffaele Capoc- cia, Zhao Wang, Reid Pinkham, Andrew Berkovich, Tsung-Hsun Tsai, Barbara De Salvo, et al. 2022. Distributed On-Sensor Compute System for AR VR Devices: A Semi-Analytical Simulation Framework for Power Estimation. arXiv preprint arXiv:2203.07474 (2022).\n\n--- Segment 31 ---\nDistributed On-Sensor Compute System for AR VR Devices: A Semi-Analytical Simulation Framework for Power Estimation. arXiv preprint arXiv:2203.07474 (2022). [21] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, and William J. Dally. 2016. EIE: Efficient Inference Engine on Compressed Deep Neural Network. ACM SIGARCH Computer Architecture News 44, 3 (June 2016), 243 254. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep Residual Learning for Image Recognition. [23] Nicholas J. Higham. 1993. The Accuracy of Floating Point Summation. SIAM Journal on Scientific Computing 14, 4 (1993), 783 799. 0914050 arXiv: [24] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Wei- jun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mo- bileNets: Efficient Convolutional Neural Networks for Mobile Vision Applica- tions. arXiv:1704.04861 [cs.CV] [25] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, An- drew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [26] W. Kahan. 1965. Pracniques: further remarks on reducing truncation errors. Commun. ACM 8, 1 (Jan. 1965), 40. [27] Alex Krizhevsky. [n. d.]. CIFAR-10 and CIFAR-100 Datasets. kriz cifar.html. [28] H. T. Kung, Bradley McDanel, and Sai Qian Zhang. 2020. Term quantization: furthering quantization at run time. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (Atlanta, Georgia) (SC 20). IEEE Press, Article 96, 14 pages.\n\n--- Segment 32 ---\nIn Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (Atlanta, Georgia) (SC 20). IEEE Press, Article 96, 14 pages. [29] Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. 2020. MCUNet: Tiny Deep Learning on IoT Devices. In Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS 20). Curran Associates Inc., Red Hook, NY, USA, 11711 11722. [30] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir. 2020. Prov- ing the Lottery Ticket Hypothesis: Pruning is All You Need. ArXiv abs 2002.00585 (2020). [31] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. 2022. FP8 Formats for Deep Learning. arXiv:2209.05433 [cs.LG] [32] Vikas Natesh and H. T. Kung. 2025. Alternating Greedy Schedules: Enabling Low-Bitwidth Accumulation of Dot Products in Neural Network Computations. In 2025 IEEE International Symposium on Circuits and Systems (ISCAS). 1 5. [33] Renkun Ni, Hong-Min Chu, Oscar Castañeda, Ping-yeh Chiang, Christoph Studer, and Tom Goldstein. 2021. WrapNet: Neural Net Inference with Ultra-Low- Precision Arithmetic. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\n\n--- Segment 33 ---\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https: openreview.net forum?id 3SqrRe8FWQ- [34] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng. 2023. FP8-LM: Training FP8 Large Language Models. arXiv:2310.18313 [cs.LG] [35] Andrew Sabot, Vikas Natesh, H. T. Kung, and Wei-Te Ting. 2023. MEMA Runtime Framework: Minimizing External Memory Accesses for TinyML on Microcontrollers. TinyML Research Symposium 2023 abs 2304.05544 (2023). arXiv:2304.05544 9 Vikas Natesh, H.T. Kung, and David Kong [36] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottle- necks. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. Computer Vision Foundation IEEE Computer Society, 4510 4520. [37] Moritz Scherer, Manuel Eggimann, Alfio Di Mauro, Arpan Suravi Prasad, Francesco Conti, Davide Rossi, Jorge Tomás Gómez, Ziyun Li, Syed Shakib Sarwar, Zhao Wang, Barbara De Salvo, and Luca Benini. 2023. Siracusa: A Low-Power On-Sensor RISC-V SoC for Extended Reality Visual Processing in 16nm CMOS. In ESSCIRC 2023- IEEE 49th European Solid State Circuits Conference (ESSCIRC). 217 220.\n\n--- Segment 34 ---\nIn ESSCIRC 2023- IEEE 49th European Solid State Circuits Conference (ESSCIRC). 217 220. [38] G. Semeraro, G. Magklis, R. Balasubramonian, D.H. Albonesi, S. Dwarkadas, and M.L. Scott. 2002. Energy-efficient processor design using multiple clock domains with dynamic voltage and frequency scaling. In Proceedings Eighth International Symposium on High Performance Computer Architecture. 29 40. [39] Youngsoo Shin, Jun Seomun, Kyu-Myung Choi, and Takayasu Sakurai. 2010. Power gating: Circuits, design methodologies, and best practice for standard-cell VLSI designs. ACM Transactions on Design Automation of Electronic Systems (TODAES) 15, 4 (2010), 1 37. [40] Synopsys. 2021. Synopsys VCS Simulation Solution. com content dam synopsys gated-assets verification vcs-ds.pdf [41] Tensorflow. [n. d.]. Tensorflow Lite Quantization. 10432 [42] Cuong Tran, Ferdinando Fioretto, Jung-Eun Kim, and Rakshit Naidu. 2022. Prun- ing has a disparate impact on model accuracy. In Proceedings of the 36th In- ternational Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 22). Curran Associates Inc., Red Hook, NY, USA, Article 1283, 13 pages. [43] Shibo Wang and Pankaj Kanwar. 2019. BFloat16: The secret to high performance on Cloud TPUs. bfloat16-the-secret-to-high-performance-on-cloud-tpus. Accessed: 2025-02-02. [44] James Hardy Wilkinson. 1964. Rounding Errors in Algebraic Processes. https: trace.tennessee.edu cgi viewcontent.cgi?article 1053 context utk_harlan [45] Lin Ye, Jinghao Ye, Masao Yanagisawa, and Youhua Shi. 2019. A Zero-Gating Processing Element Design for Low-Power Deep Convolutional Neural Networks. In 2019 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS). 317 320. 10\n\n