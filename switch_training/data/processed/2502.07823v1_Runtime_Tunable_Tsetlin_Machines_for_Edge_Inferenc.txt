=== ORIGINAL PDF: 2502.07823v1_Runtime_Tunable_Tsetlin_Machines_for_Edge_Inferenc.pdf ===\n\nRaw text length: 37202 characters\nCleaned text length: 36307 characters\nNumber of segments: 23\n\n=== CLEANED TEXT ===\n\nRuntime Tunable Tsetlin Machines for Edge Inference on eFPGAs Tousif Rahman Gang Mao Bob Pattison Sidharth Maheshwari Marcos Sartori , Adrian Wheeldon , Rishad Shafik Alex Yakovlev Newcastle University, Newcastle upon Tyne, UK Indian Institute of Technology Jammu, Jammu, India Literal Labs, Newcastle upon Tyne, UK Indicates equal contribution. Correspondence: ABSTRACT Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of hardware accelerators of edge Machine Learning (ML) applications at a lower power budget compared with traditional FPGA platforms. However, the limited eFPGA logic and memory significantly constrain compute capabilities and model size. As such, ML application deployment on eFPGAs is in direct contrast with the most recent FPGA approaches developing architecture-specific implementations and maximizing throughput over resource frugal- ity. This paper focuses on the opposite side of this trade-off: the proposed eFPGA accelerator focuses on minimizing resource usage and allowing flexibility for on-field recalibration over throughput. This allows for runtime changes in model size, architecture, and input data dimensionality without offline resynthesis. This is made possible through the use of a bitwise compressed inference archi- tecture of the Tsetlin Machine (TM) algorithm. TM compute does not require any multiplication operations, being limited to only bitwise AND, OR, NOT, summations and additions. Additionally, TM model compression allows the entire model to fit within the on- chip block RAM of the eFPGA. The paper uses this accelerator to propose a strategy for runtime model tuning in the field. The pro- posed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer registers than the current most resource-fugal design and achieves up to 129x energy reduction compared with low-power microcontrollers running the same ML application. KEYWORDS Tsetlin Machine, System-on-Chip, Embedded FPGA, Inference Ac- celerator, Machine Learning, Edge inference ACM Reference Format: Tousif Rahman, Gang Mao, Bob Pattison, Sidharth Maheshwari, Marcos Sartori, Adrian Wheeldon, Rishad Shafik, Alex Yakovlev. 2025. Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs. In Proceedings of tinyML Research Symposium. ACM, New York, NY, USA, 7 pages. 1 INTRODUCTION Deploying Machine Learning (ML) applications on the edge in- volves finding an adequate trade-off between the performance of the ML model, in terms of accuracy and the compute and memory constraints of the edge device [20, 25]. The trade-off is made more challenging when considering the compute cost of floating-point multiply-accumulate (MAC) arithmetic and 32-bit model sizes of Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner author(s). tinyML Research Symposium, April 2025, Austin, TX 2025 Copyright held by the owner author(s). Figure 1: Comparing the proposed design (3480 LUTs configuration) to state-of-the-art accelerator automation flows targeting FPGAs. All accelerators were designed for MNIST. Each vertical line indicates the max LUTs of an off-the-shelf eFPGA platform. This work uses 2.5x fewer LUTs than the next closest work (MATADOR). Figure 2: Core components of the Tsetlin Machine: Input conversion to Boolean literals, the Tsetlin Automata (TA) and Clause compute. traditional Deep Neural Networks (DNNs). Fortunately, these chal- lenges can be somewhat alleviated by quantizing DNN weights and data, often to a single bit producing Binary Neural Networks (BNNs) [2, 5, 6, 17, 28]. This means that the MAC is simplified to XNOR and popcount operations and weights in memory now use 1-bit each. This approach is particularly effective when trans- lated to accelerators targeting Field Programmable Gate Arrays (FPGAs). The most recent works follow one of two strategies: ef- ficiently mapping quantized computation to the FPGA s Look-up- Tables (LUTs) by converting the memory elements into custom compute [2, 18, 23, 26]. Or, they develop architecture-specific ap- proaches for the ML model that are centered around a parameterized compute engine [5, 17, 22, 28]. Often both types of approach are also wrapped in design automation flows. Fig. 1 shows recent state-of-the-art automation flows that can generate accelerators for FPGAs, including this work. The figure shows that the chosen trade-off in current FPGA accelerator work is maximizing inference throughput through custom designs at the expense of LUTs. In doing so, most works are unable to deploy their accelerators onto smaller, cheaper, and more power efficient embed- ded FPGA (eFPGA) platforms, even for a simple MNIST [7] applica- tion (70K LUTs for PolyLUT [2] and 260K LUTs using hls4ml [17]). This paper focuses on the alternative side of this trade-off: mini- mizing resource utilization as much as possible to target smaller arXiv:2502.07823v1 [cs.AR] 10 Feb 2025 tinyML Research Symposium, April 2025, Austin, TX Rahman and Mao et al. Figure 3: 1: The class sum compute in the original TM algorithm. 2: The impact of Includes and Excludes in the Clause Output computation - showing that excludes become redundant during inference. 3: The traversal of a trained TM model using when only considering included TAs. 4: The encoding instruction used to create a compressed TM model adapted from the approach used by [15]. eFPGA platforms and, crucially, favoring flexibility at the cost of longer latency. The proposed accelerator introduces flexibility through real-time reconfiguration, which will be referred to as run- time tunability henceforth, of the ML model size and architecture, as well as input data size that allow adaptability to new workloads without offline resynthesis to generate a new bitstream and repro- gram the eFPGA. This flexibility is made possible by the recent ML algorithm upon which the proposed accelerator is built called the Tsetlin Machine (TM) [8]. Unlike current DNN quantization approaches, the TM is inherently logic based; it does not require quantization, as the main computation is already bitwise (AND, OR and NOT). Fig. 2 shows the fundamental computation components of the TM used to generate a Clause Output. Boolean Inputs: Starting at the top of Fig. 2, the input data (Input Datapoint) is converted into Boolean Literals. For small edge applications, this is simply the binary representation of the data, referred to as Boolean features, and their complements (seen in red). This process is called Booleanization. Clause Computation: Each Boolean literal interacts with its own learning element called a Tsetlin Automata (TA). The TA is a finite state machine with states corresponding to one of two actions: Include or Exclude producing a 1 or 0 respectively. The model training process finds optimum Include Exclude state for each TA (detailed in [8, 9, 21]). The actions of each TA are inverted and OR ed with their Boolean literal, they are all then AND ed to form the 1-bit Clause Output (seen in green). TM Inference Architecture: Fig 3.1 shows the architecture of the TM. In a multi-class classification problem with M classes, each class has Cl Clauses where each clause will have its own set of TAs, as seen in Fig. 2, and generate a 1-bit Clause output. Clauses in each class have a polarity 1 -1 (seen in dark green). Each clause output is multiplied by its respective polarity and summed to generate the class sum for each class. The argmax of these class sums produces the predicted class. The size and architecture of the TM is controlled by the number of Boolean literals, the number of clauses, and the number of classes. For example, in keeping with the MNIST example, if MNIST has 784 Boolean features, it will have 1568 Boolean literals. If there are 200 clauses per class (10 classes in MNIST) then altogether this TM model will have 3,136,000 TAs. This paper exploits the compute and structural simplicity of the TM, along with the sparsity and redundancy of the model, to make the following contributions: Implementation: An LUT frugal, real-time architecture adaptable accelerator. This functionality alone differentiates the proposed design from current FPGA works. In addition, customization of the design is offered if supported by the eFPGA s resources. Runtime Tunability: An implementation flow for all pro- posed accelerator design configurations and a strategy to pe- riodically update the accelerator in real-time without resyn- thesis once deployed if re-calibration is required. The rest of the paper is organized as follows: Section II focuses on the sparsity and compression opportunities in the TM algorithm. Section III develops these ideas into the proposed accelerator design and its different configurations, as well as the flow for generating the implementation and its on-field runtime tunability. Section IV benchmarks the accelerator against comparable work, and Section V concludes the paper. 2 SPARSE TSETLIN MACHINES For inference, the state inside each TA in a TM model is represented only by a 1-bit Include or Exclude action (see Fig. 2 if needed). During TM training, this leads to a very sparse model where the number of Excludes vastly outnumbers the Includes [1, 3, 15]. This sparsity can be exploited when examining Fig 3.2. It shows the impact of an Include and an Exclude on the clause output. The Exclude eliminates the contribution of an input Boolean literal (X Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs tinyML Research Symposium, April 2025, Austin, TX Figure 4: Overview of the Proposed Accelerator (Base Version): 1: An incoming data stream to the accelerator; the header packet of the stream is used for configuration. 2: The bit-fields of the header when the data stream contains instructions (the TM model) 3: The bit-field of the header when the data stream contains input Boolean features. 4: The instruction fetching and decoding process. 5: Selecting Boolean literals that match the TA Include actions. 6: Accumulators for the clause outputs and class sums when performing the compressed inference. {0,1}). However, an Include propagates the Boolean literal. This means only TAs that are Includes are needed for TM inference. Compression: The accelerator design builds on this Include- only approach by adapting a compression scheme similar to [15] which claims around 99 model compression for small edge datasets. Continuing with the MNIST example, if there are 3,136,000 TA ac- tions in total, only around 17,000 will be Include actions. A visual overview of this approach can be seen in Fig. 3.3 showing the struc- ture of a trained TM model. Within each class, in each row are the TA actions for each Boolean literal that makes up a clause (These TA actions are written as f and f to signify the Boolean feature and its complement that the TA action corresponds to). Therefore, each row represents a clause output, and class sums can be formed using each clause s corresponding polarity ( -). Continuing with Fig. 3.3, a little blue box indicates where a TA action is an Include. The model can therefore be traversed iteratively using only these TAs for inference. This is shown with the blue arrow that indicates the iteration path over all the TA Include actions for each class. In most TM applications, the TAs with Include actions make up around 1 of the total model. Therefore, this iteration path is much shorter than computing every TA for every clause in every class in the model. Include Instruction Encoding: The compressed model only needs these included actions. However, each TA Include action (little blue box) now needs to be encoded with information about its respective Boolean literal, clause, clause polarity and class. This is achieved through a 16-bit encoding called the Include Instruction Encoding (Fig 3.4). This instruction encoding contains the neces- sary information to jump from one TA Include action to another so the inference can be done directly using this compressed model. The jump is controlled with an offset bit field, Offset(O) (seen in blue), indicating the number of TAs until the next TA Include action. For each action, the encoding toggles the two MSB bits ( -, CC) each time a clause changes, and the LSB (L) indicates whether the Boolean Literal is the feature f or its complement f. This paper also adds an extra bit E that toggles when changing classes. Compressed inference forms the backbone of the proposed accelerator archi- tecture and is used by other authors dealing with training TMs to become more sparse [1] or exploiting this sparsity for TM inference - in particular targeting micro-controllers (MCU) [3, 15] claiming substantial speed-ups (up to 5700x) compared to embedded BNNs. 3 DESIGN AND RUNTIME TUNABILITY The proposed architecture performs TM inference using the com- pressed instructions described in the previous section. Through Fig. 4 the main attributes of the accelerator design will be high- lighted: Real-time adaptability of the model and task, customization options, and resource frugality: Programming for Real-time Adaptability: The accelerator can be reconfigured for different TM models and input data sizes in real-time using a data stream as seen in Fig. 4.1. The data packets start with a header to configure the architecture. The header is either an Instruction Header (Fig. 4.2) to send a new TM model, or a Feature Header (Fig. 4.3) to send Boolean features for inference. Headers can be configured as 16,32 or 64-bits. The MSB bit indicates that this is a new stream and resets the accelerator. The second bit from the MSB bit indicates whether the following data packets will contain either the Include instructions (Instruction Header), or Boolean Features for inference (Feature Header). The remainder of the Instruction Header contains the model architecture parameters: the number of classes and the number of clauses. This is used later in the accumulation counters (Clause Counter and Class Counter). The remainder of the Feature header contains the number of Inference data packets the accelerator will need to process to generate classifications. Real-time architecture change is made possible due to the simplicity of the TM structure. Only three parameters are sufficient to update the accelerator to a new TM model size (instruction number in Instruction Header) or a new task and new data dimensionality (Class number in Instruction Header and Inference packet number in Data Header). tinyML Research Symposium, April 2025, Austin, TX Rahman and Mao et al. Figure 5: Timing diagrams of the programming, inference and execution cycle of an instruction (1). The instruction execution cycle (2) is a per-core process - it would be the same for a multi-core version of the accelerator. Memory Customization Options: While the accelerator is general-purpose, the reconfigurability of the eFPGA allows for some customization options when it comes to using memory. So far, the paper has only discussed LUT frugality; however, now it is important to introduce Block RAM (BRAM) frugality. The level of compression is possible through the Include-only approach allow- ing nearly all TM models for edge applications to fit well within the BRAM of the smallest Xilinx chips. The eFPGA gives users freedom to configure memory depths for greater runtime tunability options when deployed. However, this comes at the expense of more LUT, Flip Flop (FF), and power usage and running at a lower frequency. However, it allows users to have greater customization than an equivalent fixed-memory ASIC. This is seen through Fig. 6 where vertical lines represent the minimum memory required for edge-scale datasets for which this accelerator design is appropriate (discussed in more detail in Section IV). These datasets give an indication of the scale of problems this accelerator will be expected to be used for, for example, multivariate sensor data, wearable and telemetry data (e.g. activity detection) and small audio (e.g. keyword detection) and computer vision clas- sification problems (e.g. object classification). These problems do not place as high a priority on throughput - unlike the datasets that PolyLUT and LogicNets have targeted like UNSW-NB15 [16]. Compressed Inference and batching support: Once the in- structions have been loaded into memory, the accelerator can begin the compressed inference computation. This starts by fetching an in- struction from the Instruction Memory and decoding it (Fig. 4.4). The instruction will contain the offset to select the appropriate Boolean feature(s) (if in batched mode) from the Feature Memory. Notice in Fig. 4.5, the Literal Select box, the Offset is 4 and the 4th element in the Feature Memory is selected. The L bit is then used to identify which Boolean literal is included from this Boolean feature (either f or f). The next step is to evaluate the clause output. The selected literal (in the blue) is AND ed with the clause output registers. Notice that there are 32 of the same literal (Lùë†), this is due to the batching support of the accelerator, and 32 datapoints can be computed at once (the same literal but for 32 datapoints). This clause output continues to be updated by ANDing with in- cluded Boolean literals until CC and - toggle to say that the clause is completed. The clause output and its polarity are then added to the class sum. The E bit toggles for when classes change. In this way, the full TM model can be iterated in the compressed domain. Figure 6: Customization options for memory depths for the base configuration (implemented on Artix A7-35T). Figure 7: Block diagram for the multi-core design of the accelerator. Each Inference Core is the base version seen in Fig. 4. After iterating through all classes, the argmax is taken from the class sums, and the output FIFO is filled with up to 32 classifications (if in batched mode). This process can be seen through the timing diagram of the instruction execution cycle in Fig. 5. Fig. 5 shows the relevant parts of the instructions that are used by each computation stage and the pipelining of this execution cycle. Each instruction takes a minimum of four clock cycles to execute. Configurations (Standalone, Single Core and Multi-Core: Building on the memory customization options possible with eFP- GAs, it is also possible to implement different configurations of the base architecture presented in Fig. 4. Three configurations are of- fered, the standalone accelerator, an AXI-Stream (AXIS) interfaced single core, and an AXIS connected multi-core. The multi-core ar- chitecture is presented in Fig. 7. The AXIS interfacing allows the Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs tinyML Research Symposium, April 2025, Austin, TX Table 1: Resource usage of the three proposed accelerator configu- rations against MATADOR (MTDR) for CIFAR, KWS and MNIST Accelerator Con- figurations eFPGA chip No. LUTs (LUT-6) No. FFs No. BRAMs Freq (MHz) Base (B) A7035 1340 2228 14 200 Single Core (S) Z7020 3480 5154 43 100 Multi-Core (M) Z7020 9814 10909 43 100 MTDR (CIFAR) Z7020 3867 33212 3 50 MTDR (KWS) Z7021 6063 10658 3 50 MTDR (MNIST) Z7020 8709 17440 3 50 use of a processor for pre-processing if this is not already done by the edge sensor. Developers may choose the configuration most suitable for their application and ML pipeline. The multi-core de- sign instances base inference cores, but each instruction memory is now loaded with instructions corresponding to non-overlapped classes but the same features into feature memory. The AXIS inter- face will split the instruction stream and write them into different cores. This allows for class-level parallelism and reduced latency at the cost of more resource usage. Runtime tunability: One of the main functionalities of the ar- chitecture is its flexibility to change the model and the task. There may be instances where data set on which the model was trained no longer adequately reflects the actual data (edge sensor readings may vary subject to aging, temperature, humidity, etc... [13]). Fig. 8 shows a potential system for real-time recalibration. First, a one- time implementation of the accelerator is required. developers can customize memory, batch mode, number of cores and whether it is standalone or AXIS interfaced. Once deployed, the accelerator per- forms real-time inference from edge sensor data. However, on the same local network (or directly connected) is a Model Training Node. The simplicity of the TM training algorithm leads to fast conver- gence and energy-efficient training implementations [12, 21]. The authors in [12] demonstrate that Tsetlin machines can be trained very well on small compute nodes like Raspberry Pis. Therefore, this type of node may train on an updating dataset and periodi- cally reprogram the accelerator with a new model if needed. Users can also run a hyperparameter search to update the architecture if needed, or even add an additional class to the classification task. The TM only has two hyperparameters for training and the authors in [21] have highlighted the reduced complexity in the TM archi- tecture search space compared with DNNs. The key advantage is this Raspberry Pi node does not require FPGA synthesis tools to re- configure the proposed accelerator to a new model or task - unlike current architecture-specific FPGA approaches [2, 5, 18, 23, 28]. 4 EVALUATION To evaluate the proposed architecture, there are two critical ques- tions to consider: Question 1: How significant is the sacrifice in latency and energy of the proposed architecture compared to its closest comparable custom FPGA implementation? Question 2: How much better is the recalibration approach on an eFPGA com- pared to if the same compressed TA-Include only algorithm was developed as software for the processors of low-power off-the-shelf micro-controllers? Addressing Question 1: Include-only encoding based compres- sion is just one way to compress the TM. Another Include-only Figure 8: Configuration options for the initial deployment and the proposed system for on-field re-calibration and task update. Figure 9: Energy (E) and latency (L) of the proposed accelerator de- signs (B, S, M) against MATADOR (MTDR) and the same compressed instruction algorithm on the STM32Disco MCU (RDRS). Single dat- apoint energy and latencies are shown with the hatched bar and batched energy and latency are shown with the solid bar. MATADOR does not support batch mode, only single datapoint data is reported. approach for TMs has already been exploited in an FPGA infer- ence implementation [18]. The FPGA approach (MATADOR) is seen in Fig. 1, it uses the Include TA actions to directly synthesize the model specific clause expressions in each class. The authors of MATADOR use Include-only sparsity to generate the closest comparable trade-off in terms of latency and performance per LUT. It uses the fewest LUTs of all comparable approaches and is the fastest of the existing TM accelerators [22]. However, it follows the design rationale of its contemporaries in Fig. 1 as it creates model- specific accelerators. As such, the authors of FINN [5], hls4ml [17] and MATADOR offer open-source end-to-end automation flows to implement these designs, all requiring resynthesis every time. This work uses MATADOR s automation flow to replicate the TM model training for MNIST, CIFAR 2 [11] (2 classes: vehicles and ani- mals) and Google Speech Commands [27] (Keyword Spotting with 6 words: yes, no, up, down, left, right - referred to as KWS 6) using the same TM architectures, as mentioned in MATADOR, result in the tinyML Research Symposium, April 2025, Austin, TX Rahman and Mao et al. Table 2: Latency and energy comparisons of the proposed accelerators vs an Espressif ESP32 software version of the algorithm. Latency (us) Energy (uJ) Dataset Acc Design Batch Single Data Point Throughput (inf s) Batch Single data point xSpeedups xEnergy Reduction Base (B) 7.44 0.23 4303968 2.610 0.082 245.3 22.9 Single Core (S) 14.87 0.46 2151984 21.279 0.665 122.7 2.8 5-Core (M) 7.64 0.24 4188482 11.429 0.357 238.7 5.2 EMG [10] 87 ESP32 1824.00 57.00 17544 59.791 1.868 - - Base (B) 37.80 1.18 846561 13.268 0.415 490.2 109.4 Single Core (S) 75.60 2.36 423280 108.184 3.381 245.1 13.4 5-Core (M) 27.10 0.85 1180812 40.542 1.267 683.7 35.8 Human Activity [19] 84 ESP32 18528.00 579.00 1727 1451.113 45.347 - - Base (B) 42.87 1.34 746530 15.046 0.470 58.2 13.0 Single Core (S) 85.73 2.68 373265 122.680 3.834 29.1 1.6 5-Core (M) 28.26 0.88 1132343 42.277 1.321 88.3 4.6 Gesture Phase [14] 89 ESP32 2496.00 78.00 12821 195.487 6.109 1.0 1.0 Base (B) 83.05 2.60 385310 29.151 0.911 578.8 129.1 Single Core (S) 166.1 5.19 192655 237.689 7.428 289.4 15.8 5-Core (M) 50.1 1.57 638723 74.950 2.342 959.4 50.2 Sensorless Drives [4] 86 ESP32 48,068.27 1502.13 666 3764.707 117.647 - - Base (B) 30.115 0.94 1062593 10.570 0.330 544.8 121.6 Single Core (S) 60.23 1.88 531297 86.189 2.693 272.4 14.9 5-Core (M) 57.57 1.80 555845 86.125 2.691 285.0 14.9 Gas Sensor Array Drift [24] 90 ESP32 16,407.33 512.73 1950 1285.022 40.157 - - same accuracy. The accelerator configurations are given in Table 1. The proposed base B accelerator is the most resource efficient for LUTs and FFs and operates at the highest frequency. Additionally, on the same eFPGA chip, the Single Core (S) uses 2.5x fewer LUTs and 3.38x fewer FFs than MATADOR for MNIST. BRAMs for B, S and M are over-provisioned for more tunability later. The models were compressed into 16-bit instructions and used to program the proposed accelerator configurations. Fig. 9 shows the energy and latency of the configurations compared to MATADOR. The num- bers in red indicate the speed-up and energy reduction compared to a software implementation of the compressed Include encoded inference approach running on an STM32Disco MCU (RDRS) as presented by [15] claiming upto 5700x speedup compared with embedded BNNs - MCU comparisons will be discussed in more detail in Question 2. All B, S, M configurations are within one order of magnitude of the MATADOR results; in the case of CIFAR 2, B is the most energy efficient. The key point to note is that MATA- DOR is fixed to these latencies and energies; however, a real-time recalibration to a smaller model would improve the B, S, M results without resynthesis. Ultimately, the key point becomes application, if the trained model is always a good representation of the data it will infer, then MATADOR is a better option. If the data is subject to drift, or there is need for personalization or sensor reading degra- dation or environmental fluctuation, and there is an opportunity to use a system like Fig. 8, then the proposed accelerators are more viable. They will adapt in situ during deployment. Addressing Question 2: The flexibility to change model size and architecture at runtime makes the accelerator comparable to small RISC processors. Question 1 already demonstrated the energy and latency advantages against an Arm-based STM32Disco. This section explores how it compares to another even smaller, cheaper low-power MCU - the Espressif ESP32. This is done to understand whether the eFPGA is the best platform to be used in a system like Fig. 8. Once again, the ESP32 runs the same compressed model inference, but as a software task on the processor. This time, the applications are chosen for their suitability for run-time recalibra- tion (Table II). The EMG dataset involves classifying myographic signals sent from a bracelet to an edge inference node [10]. This requires recalibration for user personalization, the same is true for Gesture Phase [14] and Human Activity detection [19], they are all used to classify user movements. Sensorless Drives [4] involves diagnosing faulty components in electric current drive signals. Gas Sensor Array Drift uses chemical sensors to classify different gas concentrations [24]. Both applications are subject to environmen- tal changes and component aging. Across all the datasets, all the proposed accelerators give better latency and energy efficiency (see speedup and energy reduction with respect to the ESP32 implemen- tation). For Sensorless Drives the 5-Core M configuration gives the best energy performance. 5 CONCLUSION This work explored the possibilities of LUT frugal and flexible accelerator development for applications that require runtime recal- ibration. The Tsetlin Machine algorithm leads to minimal bitwise compute, and its sparsity enables the instruction-based compression to allow models to fit well with the BRAMs of of-the-shelf eFPGA platforms. While current FPGA works leverage reconfigurability to build the fastest possible custom architectures, this work uses it to allow customization options in memory, batching, interconnect and number of cores. The proposed accelerator implementations offer far better energy efficiency than the low-power MCUs running the same compressed model inference. 6 ACKNOWLEDGMENTS This work is supported by the UK Research and Innovation (UKRI) Engineering and Physical Sciences Research Council (EPSRC) under grant EP X039943 1 and grant reference: studentship-2926263. Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs tinyML Research Symposium, April 2025, Austin, TX REFERENCES [1] K. Darshana Abeyrathna, Ahmed A. O. Abouzeid, Bimal Bhattarai, Charul Giri, Sondre Glimsdal, Ole-Christoffer Granmo, Lei Jiao, Rupsa Saha, Jivitesh Sharma, Svein A. Tunheim, and Xuan Zhang. 2023. Building concise logical patterns by constraining tsetlin machine clause size. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (Macao, P.R.China) (IJCAI 23). Article 378, 9 pages. [2] Marta Andronic and George A. Constantinides. 2023. PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference. In 2023 International Conference on Field Programmable Technology (ICFPT). 60 68. [3] Abu Bakar, Tousif Rahman, Rishad Shafik, Fahim Kawsar, and Alessandro Mon- tanari. 2023. Adaptive Intelligence for Batteryless Sensors Using Software- Accelerated Tsetlin Machines. In Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems (SenSys 22). 236 249. 1145 3560905.3568512 [4] Martyna Bator. 2013. Dataset for Sensorless Drive Diagnosis. UCI Machine Learning Repository. DOI: [5] Michaela Blott, Thomas B. Preu√üer, Nicholas J. Fraser, Giulio Gambardella, Ken- neth O brien, Yaman Umuroglu, Miriam Leeser, and Kees Vissers. 2018. FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks. ACM Trans. Reconfigurable Technol. Syst. 11, 3, Article 16 (dec 2018), 23 pages. [6] Francesco Conti, Pasquale Davide Schiavone, and Luca Benini. 2018. XNOR Neural Engine: A Hardware Accelerator IP for 21.6-fJ op Binary Neural Network Inference. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 37, 11 (Nov. 2018), 2940 2951. [7] Li Deng. 2012. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine 29, 6 (2012), 141 142. [8] Ole-Christoffer Granmo. 2018. The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. CoRR abs 1804.01508 (2018). arXiv:1804.01508 [9] Lei Jiao, Xuan Zhang, Ole-Christoffer Granmo, and Kuruge Darshana Abeyrathna. 2023. On the Convergence of Tsetlin Machines for the XOR Operator. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 5 (2023), 6072 6085. [10] N. Krilova, I. Kastalskiy, V. Kazantsev, V.A. Makarov, and S. Lobov. 2018. EMG Data for Gestures. UCI Machine Learning Repository. DOI: [11] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Technical Report. University of Toronto. [12] Jie Lei, Tousif Rahman, Rishad Shafik, Adrian Wheeldon, Alex Yakovlev, Ole- Christoffer Granmo, Fahim Kawsar, and Akhil Mathur. 2021. Low-Power Audio Keyword Spotting Using Tsetlin Machines. Journal of Low Power Electronics and Applications 11, 2 (2021). [13] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Jo√£o Gama, and Guangquan Zhang. 2019. Learning under Concept Drift: A Review. IEEE Transactions on Knowledge and Data Engineering 31, 12 (2019), 2346 2363. 2876857 [14] Renata Madeo, Priscilla Wagner, and Sarajane Peres. 2013. Gesture Phase Segmen- tation. UCI Machine Learning Repository. DOI: [15] Sidharth Maheshwari, Tousif Rahman, Rishad Shafik, Alex Yakovlev, Ashur Rafiev, Lei Jiao, and Ole-Christoffer Granmo. 2023. REDRESS: Generating Compressed Models for Edge Inference Using Tsetlin Machines. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023), 1 16. 2023.3268415 [16] Nour Moustafa and Jill Slay. 2015. UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set). In 2015 Military Communications and Information Systems Conference (MilCIS). 1 6. [17] Jennifer Ngadiuba, Vladimir Loncar, Maurizio Pierini, Sioni Summers, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris, Dylan Rankin, Sergo Jindari- ani, Mia Liu, Kevin Pedro, Nhan Tran, Edward Kreinar, Sheila Sagear, Zhenbin Wu, and Duc Hoang. 2020. Compressing deep neural networks on FPGAs to binary and ternary precision with hls4ml. Machine Learning: Science and Technology 2, 1 (dec 2020), 015001. [18] Tousif Rahman, Gang Mao, Sidharth Maheshwari, Rishad Shafik, and Alex Yakovlev. 2024. MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications. In 2024 Design, Automation and Test in Europe Conference and Exhibition (DATE). 1 6. 2024.10546779 [19] Jorge Reyes-Ortiz, Davide Anguita, Alessandro Ghio, Luca Oneto, and Xavier Parra. 2013. Human Activity Recognition Using Smartphones. UCI Machine Learning Repository. DOI: [20] Md. Maruf Hossain Shuvo, Syed Kamrul Islam, Jianlin Cheng, and Bashir I. Morshed. 2023. Efficient Acceleration of Deep Learning Inference on Resource- Constrained Edge Devices: A Review. Proc. IEEE 111, 1 (2023), 42 91. https: doi.org 10.1109 JPROC.2022.3226481 [21] Olga Tarasyuk, Anatoliy Gorbenko, Tousif Rahman, Rishad Shafik, Alex Yakovlev, Ole-Christoffer Granmo, and Lei Jiao. 2023. Systematic Search for Optimal Hyper- parameters of the Tsetlin Machine on MNIST Dataset. In Second International Symposium on the Tsetlin Machine. [22] Svein Anders Tunheim, Lei Jiao, Rishad Shafik, Alex Yakovlev, and Ole-Christoffer Granmo. 2022. A Convolutional Tsetlin Machine-based Field Programmable Gate Array Accelerator for Image Classification. In 2022 International Symposium on the Tsetlin Machine (ISTM). 21 28. [23] Yaman Umuroglu, Yash Akhauri, Nicholas James Fraser, and Michaela Blott. 2020. LogicNets: Co-Designed Neural Networks and Circuits for Extreme-Throughput Applications. In 2020 30th International Conference on Field-Programmable Logic and Applications (FPL). 291 297. [24] Alexander Vergara. 2012. Gas Sensor Array Drift Dataset. UCI Machine Learning Repository. DOI: [25] Marian Verhelst and Bert Moons. 2017. Embedded Deep Neural Network Processing: Algorithmic and Processor Techniques Bring Deep Learning to IoT and Edge Devices. IEEE Solid-State Circuits Magazine 9, 4 (2017), 55 65. [26] Erwei Wang, James J. Davis, Peter Y. K. Cheung, and George A. Constantinides. 2019. LUTNet: Rethinking Inference in FPGA Soft Logic. In 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). 26 34. [27] Pete Warden. 2018. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209 (2018). [28] Yichi Zhang, Junhao Pan, Xinheng Liu, Hongzheng Chen, Deming Chen, and Zhiru Zhang. 2021. FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations. In The 2021 ACM SIGDA International Symposium on Field-Programmable Gate Arrays (Virtual Event, USA) (FPGA 21). Association for Computing Machinery, New York, NY, USA, 171 182.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nRuntime Tunable Tsetlin Machines for Edge Inference on eFPGAs Tousif Rahman Gang Mao Bob Pattison Sidharth Maheshwari Marcos Sartori , Adrian Wheeldon , Rishad Shafik Alex Yakovlev Newcastle University, Newcastle upon Tyne, UK Indian Institute of Technology Jammu, Jammu, India Literal Labs, Newcastle upon Tyne, UK Indicates equal contribution. Correspondence: ABSTRACT Embedded Field-Programmable Gate Arrays (eFPGAs) allow for the design of hardware accelerators of edge Machine Learning (ML) applications at a lower power budget compared with traditional FPGA platforms. However, the limited eFPGA logic and memory significantly constrain compute capabilities and model size. As such, ML application deployment on eFPGAs is in direct contrast with the most recent FPGA approaches developing architecture-specific implementations and maximizing throughput over resource frugal- ity. This paper focuses on the opposite side of this trade-off: the proposed eFPGA accelerator focuses on minimizing resource usage and allowing flexibility for on-field recalibration over throughput. This allows for runtime changes in model size, architecture, and input data dimensionality without offline resynthesis. This is made possible through the use of a bitwise compressed inference archi- tecture of the Tsetlin Machine (TM) algorithm. TM compute does not require any multiplication operations, being limited to only bitwise AND, OR, NOT, summations and additions. Additionally, TM model compression allows the entire model to fit within the on- chip block RAM of the eFPGA. The paper uses this accelerator to propose a strategy for runtime model tuning in the field. The pro- posed approach uses 2.5x fewer Look-up-Tables (LUTs) and 3.38x fewer registers than the current most resource-fugal design and achieves up to 129x energy reduction compared with low-power microcontrollers running the same ML application. KEYWORDS Tsetlin Machine, System-on-Chip, Embedded FPGA, Inference Ac- celerator, Machine Learning, Edge inference ACM Reference Format: Tousif Rahman, Gang Mao, Bob Pattison, Sidharth Maheshwari, Marcos Sartori, Adrian Wheeldon, Rishad Shafik, Alex Yakovlev. 2025.\n\n--- Segment 2 ---\nKEYWORDS Tsetlin Machine, System-on-Chip, Embedded FPGA, Inference Ac- celerator, Machine Learning, Edge inference ACM Reference Format: Tousif Rahman, Gang Mao, Bob Pattison, Sidharth Maheshwari, Marcos Sartori, Adrian Wheeldon, Rishad Shafik, Alex Yakovlev. 2025. Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs. In Proceedings of tinyML Research Symposium. ACM, New York, NY, USA, 7 pages. 1 INTRODUCTION Deploying Machine Learning (ML) applications on the edge in- volves finding an adequate trade-off between the performance of the ML model, in terms of accuracy and the compute and memory constraints of the edge device [20, 25]. The trade-off is made more challenging when considering the compute cost of floating-point multiply-accumulate (MAC) arithmetic and 32-bit model sizes of Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner author(s). tinyML Research Symposium, April 2025, Austin, TX 2025 Copyright held by the owner author(s). Figure 1: Comparing the proposed design (3480 LUTs configuration) to state-of-the-art accelerator automation flows targeting FPGAs. All accelerators were designed for MNIST. Each vertical line indicates the max LUTs of an off-the-shelf eFPGA platform. This work uses 2.5x fewer LUTs than the next closest work (MATADOR). Figure 2: Core components of the Tsetlin Machine: Input conversion to Boolean literals, the Tsetlin Automata (TA) and Clause compute. traditional Deep Neural Networks (DNNs). Fortunately, these chal- lenges can be somewhat alleviated by quantizing DNN weights and data, often to a single bit producing Binary Neural Networks (BNNs) [2, 5, 6, 17, 28].\n\n--- Segment 3 ---\ntraditional Deep Neural Networks (DNNs). Fortunately, these chal- lenges can be somewhat alleviated by quantizing DNN weights and data, often to a single bit producing Binary Neural Networks (BNNs) [2, 5, 6, 17, 28]. This means that the MAC is simplified to XNOR and popcount operations and weights in memory now use 1-bit each. This approach is particularly effective when trans- lated to accelerators targeting Field Programmable Gate Arrays (FPGAs). The most recent works follow one of two strategies: ef- ficiently mapping quantized computation to the FPGA s Look-up- Tables (LUTs) by converting the memory elements into custom compute [2, 18, 23, 26]. Or, they develop architecture-specific ap- proaches for the ML model that are centered around a parameterized compute engine [5, 17, 22, 28]. Often both types of approach are also wrapped in design automation flows. Fig. 1 shows recent state-of-the-art automation flows that can generate accelerators for FPGAs, including this work. The figure shows that the chosen trade-off in current FPGA accelerator work is maximizing inference throughput through custom designs at the expense of LUTs. In doing so, most works are unable to deploy their accelerators onto smaller, cheaper, and more power efficient embed- ded FPGA (eFPGA) platforms, even for a simple MNIST [7] applica- tion (70K LUTs for PolyLUT [2] and 260K LUTs using hls4ml [17]). This paper focuses on the alternative side of this trade-off: mini- mizing resource utilization as much as possible to target smaller arXiv:2502.07823v1 [cs.AR] 10 Feb 2025 tinyML Research Symposium, April 2025, Austin, TX Rahman and Mao et al. Figure 3: 1: The class sum compute in the original TM algorithm. 2: The impact of Includes and Excludes in the Clause Output computation - showing that excludes become redundant during inference. 3: The traversal of a trained TM model using when only considering included TAs. 4: The encoding instruction used to create a compressed TM model adapted from the approach used by [15]. eFPGA platforms and, crucially, favoring flexibility at the cost of longer latency.\n\n--- Segment 4 ---\n4: The encoding instruction used to create a compressed TM model adapted from the approach used by [15]. eFPGA platforms and, crucially, favoring flexibility at the cost of longer latency. The proposed accelerator introduces flexibility through real-time reconfiguration, which will be referred to as run- time tunability henceforth, of the ML model size and architecture, as well as input data size that allow adaptability to new workloads without offline resynthesis to generate a new bitstream and repro- gram the eFPGA. This flexibility is made possible by the recent ML algorithm upon which the proposed accelerator is built called the Tsetlin Machine (TM) [8]. Unlike current DNN quantization approaches, the TM is inherently logic based; it does not require quantization, as the main computation is already bitwise (AND, OR and NOT). Fig. 2 shows the fundamental computation components of the TM used to generate a Clause Output. Boolean Inputs: Starting at the top of Fig. 2, the input data (Input Datapoint) is converted into Boolean Literals. For small edge applications, this is simply the binary representation of the data, referred to as Boolean features, and their complements (seen in red). This process is called Booleanization. Clause Computation: Each Boolean literal interacts with its own learning element called a Tsetlin Automata (TA). The TA is a finite state machine with states corresponding to one of two actions: Include or Exclude producing a 1 or 0 respectively. The model training process finds optimum Include Exclude state for each TA (detailed in [8, 9, 21]). The actions of each TA are inverted and OR ed with their Boolean literal, they are all then AND ed to form the 1-bit Clause Output (seen in green). TM Inference Architecture: Fig 3.1 shows the architecture of the TM. In a multi-class classification problem with M classes, each class has Cl Clauses where each clause will have its own set of TAs, as seen in Fig. 2, and generate a 1-bit Clause output. Clauses in each class have a polarity 1 -1 (seen in dark green). Each clause output is multiplied by its respective polarity and summed to generate the class sum for each class. The argmax of these class sums produces the predicted class.\n\n--- Segment 5 ---\nEach clause output is multiplied by its respective polarity and summed to generate the class sum for each class. The argmax of these class sums produces the predicted class. The size and architecture of the TM is controlled by the number of Boolean literals, the number of clauses, and the number of classes. For example, in keeping with the MNIST example, if MNIST has 784 Boolean features, it will have 1568 Boolean literals. If there are 200 clauses per class (10 classes in MNIST) then altogether this TM model will have 3,136,000 TAs. This paper exploits the compute and structural simplicity of the TM, along with the sparsity and redundancy of the model, to make the following contributions: Implementation: An LUT frugal, real-time architecture adaptable accelerator. This functionality alone differentiates the proposed design from current FPGA works. In addition, customization of the design is offered if supported by the eFPGA s resources. Runtime Tunability: An implementation flow for all pro- posed accelerator design configurations and a strategy to pe- riodically update the accelerator in real-time without resyn- thesis once deployed if re-calibration is required. The rest of the paper is organized as follows: Section II focuses on the sparsity and compression opportunities in the TM algorithm. Section III develops these ideas into the proposed accelerator design and its different configurations, as well as the flow for generating the implementation and its on-field runtime tunability. Section IV benchmarks the accelerator against comparable work, and Section V concludes the paper. 2 SPARSE TSETLIN MACHINES For inference, the state inside each TA in a TM model is represented only by a 1-bit Include or Exclude action (see Fig. 2 if needed). During TM training, this leads to a very sparse model where the number of Excludes vastly outnumbers the Includes [1, 3, 15]. This sparsity can be exploited when examining Fig 3.2. It shows the impact of an Include and an Exclude on the clause output. The Exclude eliminates the contribution of an input Boolean literal (X Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs tinyML Research Symposium, April 2025, Austin, TX Figure 4: Overview of the Proposed Accelerator (Base Version): 1: An incoming data stream to the accelerator; the header packet of the stream is used for configuration.\n\n--- Segment 6 ---\nIt shows the impact of an Include and an Exclude on the clause output. The Exclude eliminates the contribution of an input Boolean literal (X Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs tinyML Research Symposium, April 2025, Austin, TX Figure 4: Overview of the Proposed Accelerator (Base Version): 1: An incoming data stream to the accelerator; the header packet of the stream is used for configuration. 2: The bit-fields of the header when the data stream contains instructions (the TM model) 3: The bit-field of the header when the data stream contains input Boolean features. 4: The instruction fetching and decoding process. 5: Selecting Boolean literals that match the TA Include actions. 6: Accumulators for the clause outputs and class sums when performing the compressed inference. {0,1}). However, an Include propagates the Boolean literal. This means only TAs that are Includes are needed for TM inference. Compression: The accelerator design builds on this Include- only approach by adapting a compression scheme similar to [15] which claims around 99 model compression for small edge datasets. Continuing with the MNIST example, if there are 3,136,000 TA ac- tions in total, only around 17,000 will be Include actions. A visual overview of this approach can be seen in Fig. 3.3 showing the struc- ture of a trained TM model. Within each class, in each row are the TA actions for each Boolean literal that makes up a clause (These TA actions are written as f and f to signify the Boolean feature and its complement that the TA action corresponds to). Therefore, each row represents a clause output, and class sums can be formed using each clause s corresponding polarity ( -). Continuing with Fig. 3.3, a little blue box indicates where a TA action is an Include. The model can therefore be traversed iteratively using only these TAs for inference. This is shown with the blue arrow that indicates the iteration path over all the TA Include actions for each class. In most TM applications, the TAs with Include actions make up around 1 of the total model. Therefore, this iteration path is much shorter than computing every TA for every clause in every class in the model. Include Instruction Encoding: The compressed model only needs these included actions.\n\n--- Segment 7 ---\nTherefore, this iteration path is much shorter than computing every TA for every clause in every class in the model. Include Instruction Encoding: The compressed model only needs these included actions. However, each TA Include action (little blue box) now needs to be encoded with information about its respective Boolean literal, clause, clause polarity and class. This is achieved through a 16-bit encoding called the Include Instruction Encoding (Fig 3.4). This instruction encoding contains the neces- sary information to jump from one TA Include action to another so the inference can be done directly using this compressed model. The jump is controlled with an offset bit field, Offset(O) (seen in blue), indicating the number of TAs until the next TA Include action. For each action, the encoding toggles the two MSB bits ( -, CC) each time a clause changes, and the LSB (L) indicates whether the Boolean Literal is the feature f or its complement f. This paper also adds an extra bit E that toggles when changing classes. Compressed inference forms the backbone of the proposed accelerator archi- tecture and is used by other authors dealing with training TMs to become more sparse [1] or exploiting this sparsity for TM inference - in particular targeting micro-controllers (MCU) [3, 15] claiming substantial speed-ups (up to 5700x) compared to embedded BNNs. 3 DESIGN AND RUNTIME TUNABILITY The proposed architecture performs TM inference using the com- pressed instructions described in the previous section. Through Fig. 4 the main attributes of the accelerator design will be high- lighted: Real-time adaptability of the model and task, customization options, and resource frugality: Programming for Real-time Adaptability: The accelerator can be reconfigured for different TM models and input data sizes in real-time using a data stream as seen in Fig. 4.1. The data packets start with a header to configure the architecture. The header is either an Instruction Header (Fig. 4.2) to send a new TM model, or a Feature Header (Fig. 4.3) to send Boolean features for inference. Headers can be configured as 16,32 or 64-bits. The MSB bit indicates that this is a new stream and resets the accelerator.\n\n--- Segment 8 ---\nHeaders can be configured as 16,32 or 64-bits. The MSB bit indicates that this is a new stream and resets the accelerator. The second bit from the MSB bit indicates whether the following data packets will contain either the Include instructions (Instruction Header), or Boolean Features for inference (Feature Header). The remainder of the Instruction Header contains the model architecture parameters: the number of classes and the number of clauses. This is used later in the accumulation counters (Clause Counter and Class Counter). The remainder of the Feature header contains the number of Inference data packets the accelerator will need to process to generate classifications. Real-time architecture change is made possible due to the simplicity of the TM structure. Only three parameters are sufficient to update the accelerator to a new TM model size (instruction number in Instruction Header) or a new task and new data dimensionality (Class number in Instruction Header and Inference packet number in Data Header). tinyML Research Symposium, April 2025, Austin, TX Rahman and Mao et al. Figure 5: Timing diagrams of the programming, inference and execution cycle of an instruction (1). The instruction execution cycle (2) is a per-core process - it would be the same for a multi-core version of the accelerator. Memory Customization Options: While the accelerator is general-purpose, the reconfigurability of the eFPGA allows for some customization options when it comes to using memory. So far, the paper has only discussed LUT frugality; however, now it is important to introduce Block RAM (BRAM) frugality. The level of compression is possible through the Include-only approach allow- ing nearly all TM models for edge applications to fit well within the BRAM of the smallest Xilinx chips. The eFPGA gives users freedom to configure memory depths for greater runtime tunability options when deployed. However, this comes at the expense of more LUT, Flip Flop (FF), and power usage and running at a lower frequency. However, it allows users to have greater customization than an equivalent fixed-memory ASIC. This is seen through Fig. 6 where vertical lines represent the minimum memory required for edge-scale datasets for which this accelerator design is appropriate (discussed in more detail in Section IV).\n\n--- Segment 9 ---\nThis is seen through Fig. 6 where vertical lines represent the minimum memory required for edge-scale datasets for which this accelerator design is appropriate (discussed in more detail in Section IV). These datasets give an indication of the scale of problems this accelerator will be expected to be used for, for example, multivariate sensor data, wearable and telemetry data (e.g. activity detection) and small audio (e.g. keyword detection) and computer vision clas- sification problems (e.g. object classification). These problems do not place as high a priority on throughput - unlike the datasets that PolyLUT and LogicNets have targeted like UNSW-NB15 [16]. Compressed Inference and batching support: Once the in- structions have been loaded into memory, the accelerator can begin the compressed inference computation. This starts by fetching an in- struction from the Instruction Memory and decoding it (Fig. 4.4). The instruction will contain the offset to select the appropriate Boolean feature(s) (if in batched mode) from the Feature Memory. Notice in Fig. 4.5, the Literal Select box, the Offset is 4 and the 4th element in the Feature Memory is selected. The L bit is then used to identify which Boolean literal is included from this Boolean feature (either f or f). The next step is to evaluate the clause output. The selected literal (in the blue) is AND ed with the clause output registers. Notice that there are 32 of the same literal (Lùë†), this is due to the batching support of the accelerator, and 32 datapoints can be computed at once (the same literal but for 32 datapoints). This clause output continues to be updated by ANDing with in- cluded Boolean literals until CC and - toggle to say that the clause is completed. The clause output and its polarity are then added to the class sum. The E bit toggles for when classes change. In this way, the full TM model can be iterated in the compressed domain. Figure 6: Customization options for memory depths for the base configuration (implemented on Artix A7-35T). Figure 7: Block diagram for the multi-core design of the accelerator. Each Inference Core is the base version seen in Fig. 4.\n\n--- Segment 10 ---\nEach Inference Core is the base version seen in Fig. 4. After iterating through all classes, the argmax is taken from the class sums, and the output FIFO is filled with up to 32 classifications (if in batched mode). This process can be seen through the timing diagram of the instruction execution cycle in Fig. 5. Fig. 5 shows the relevant parts of the instructions that are used by each computation stage and the pipelining of this execution cycle. Each instruction takes a minimum of four clock cycles to execute. Configurations (Standalone, Single Core and Multi-Core: Building on the memory customization options possible with eFP- GAs, it is also possible to implement different configurations of the base architecture presented in Fig. 4. Three configurations are of- fered, the standalone accelerator, an AXI-Stream (AXIS) interfaced single core, and an AXIS connected multi-core. The multi-core ar- chitecture is presented in Fig. 7. The AXIS interfacing allows the Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs tinyML Research Symposium, April 2025, Austin, TX Table 1: Resource usage of the three proposed accelerator configu- rations against MATADOR (MTDR) for CIFAR, KWS and MNIST Accelerator Con- figurations eFPGA chip No. LUTs (LUT-6) No. FFs No. BRAMs Freq (MHz) Base (B) A7035 1340 2228 14 200 Single Core (S) Z7020 3480 5154 43 100 Multi-Core (M) Z7020 9814 10909 43 100 MTDR (CIFAR) Z7020 3867 33212 3 50 MTDR (KWS) Z7021 6063 10658 3 50 MTDR (MNIST) Z7020 8709 17440 3 50 use of a processor for pre-processing if this is not already done by the edge sensor. Developers may choose the configuration most suitable for their application and ML pipeline. The multi-core de- sign instances base inference cores, but each instruction memory is now loaded with instructions corresponding to non-overlapped classes but the same features into feature memory. The AXIS inter- face will split the instruction stream and write them into different cores.\n\n--- Segment 11 ---\nThe multi-core de- sign instances base inference cores, but each instruction memory is now loaded with instructions corresponding to non-overlapped classes but the same features into feature memory. The AXIS inter- face will split the instruction stream and write them into different cores. This allows for class-level parallelism and reduced latency at the cost of more resource usage. Runtime tunability: One of the main functionalities of the ar- chitecture is its flexibility to change the model and the task. There may be instances where data set on which the model was trained no longer adequately reflects the actual data (edge sensor readings may vary subject to aging, temperature, humidity, etc... [13]). Fig. 8 shows a potential system for real-time recalibration. First, a one- time implementation of the accelerator is required. developers can customize memory, batch mode, number of cores and whether it is standalone or AXIS interfaced. Once deployed, the accelerator per- forms real-time inference from edge sensor data. However, on the same local network (or directly connected) is a Model Training Node. The simplicity of the TM training algorithm leads to fast conver- gence and energy-efficient training implementations [12, 21]. The authors in [12] demonstrate that Tsetlin machines can be trained very well on small compute nodes like Raspberry Pis. Therefore, this type of node may train on an updating dataset and periodi- cally reprogram the accelerator with a new model if needed. Users can also run a hyperparameter search to update the architecture if needed, or even add an additional class to the classification task. The TM only has two hyperparameters for training and the authors in [21] have highlighted the reduced complexity in the TM archi- tecture search space compared with DNNs. The key advantage is this Raspberry Pi node does not require FPGA synthesis tools to re- configure the proposed accelerator to a new model or task - unlike current architecture-specific FPGA approaches [2, 5, 18, 23, 28]. 4 EVALUATION To evaluate the proposed architecture, there are two critical ques- tions to consider: Question 1: How significant is the sacrifice in latency and energy of the proposed architecture compared to its closest comparable custom FPGA implementation?\n\n--- Segment 12 ---\nThe key advantage is this Raspberry Pi node does not require FPGA synthesis tools to re- configure the proposed accelerator to a new model or task - unlike current architecture-specific FPGA approaches [2, 5, 18, 23, 28]. 4 EVALUATION To evaluate the proposed architecture, there are two critical ques- tions to consider: Question 1: How significant is the sacrifice in latency and energy of the proposed architecture compared to its closest comparable custom FPGA implementation? Question 2: How much better is the recalibration approach on an eFPGA com- pared to if the same compressed TA-Include only algorithm was developed as software for the processors of low-power off-the-shelf micro-controllers? Addressing Question 1: Include-only encoding based compres- sion is just one way to compress the TM. Another Include-only Figure 8: Configuration options for the initial deployment and the proposed system for on-field re-calibration and task update. Figure 9: Energy (E) and latency (L) of the proposed accelerator de- signs (B, S, M) against MATADOR (MTDR) and the same compressed instruction algorithm on the STM32Disco MCU (RDRS). Single dat- apoint energy and latencies are shown with the hatched bar and batched energy and latency are shown with the solid bar. MATADOR does not support batch mode, only single datapoint data is reported. approach for TMs has already been exploited in an FPGA infer- ence implementation [18]. The FPGA approach (MATADOR) is seen in Fig. 1, it uses the Include TA actions to directly synthesize the model specific clause expressions in each class. The authors of MATADOR use Include-only sparsity to generate the closest comparable trade-off in terms of latency and performance per LUT. It uses the fewest LUTs of all comparable approaches and is the fastest of the existing TM accelerators [22]. However, it follows the design rationale of its contemporaries in Fig. 1 as it creates model- specific accelerators. As such, the authors of FINN [5], hls4ml [17] and MATADOR offer open-source end-to-end automation flows to implement these designs, all requiring resynthesis every time.\n\n--- Segment 13 ---\n1 as it creates model- specific accelerators. As such, the authors of FINN [5], hls4ml [17] and MATADOR offer open-source end-to-end automation flows to implement these designs, all requiring resynthesis every time. This work uses MATADOR s automation flow to replicate the TM model training for MNIST, CIFAR 2 [11] (2 classes: vehicles and ani- mals) and Google Speech Commands [27] (Keyword Spotting with 6 words: yes, no, up, down, left, right - referred to as KWS 6) using the same TM architectures, as mentioned in MATADOR, result in the tinyML Research Symposium, April 2025, Austin, TX Rahman and Mao et al. Table 2: Latency and energy comparisons of the proposed accelerators vs an Espressif ESP32 software version of the algorithm.\n\n--- Segment 14 ---\nThis work uses MATADOR s automation flow to replicate the TM model training for MNIST, CIFAR 2 [11] (2 classes: vehicles and ani- mals) and Google Speech Commands [27] (Keyword Spotting with 6 words: yes, no, up, down, left, right - referred to as KWS 6) using the same TM architectures, as mentioned in MATADOR, result in the tinyML Research Symposium, April 2025, Austin, TX Rahman and Mao et al. Table 2: Latency and energy comparisons of the proposed accelerators vs an Espressif ESP32 software version of the algorithm. Latency (us) Energy (uJ) Dataset Acc Design Batch Single Data Point Throughput (inf s) Batch Single data point xSpeedups xEnergy Reduction Base (B) 7.44 0.23 4303968 2.610 0.082 245.3 22.9 Single Core (S) 14.87 0.46 2151984 21.279 0.665 122.7 2.8 5-Core (M) 7.64 0.24 4188482 11.429 0.357 238.7 5.2 EMG [10] 87 ESP32 1824.00 57.00 17544 59.791 1.868 - - Base (B) 37.80 1.18 846561 13.268 0.415 490.2 109.4 Single Core (S) 75.60 2.36 423280 108.184 3.381 245.1 13.4 5-Core (M) 27.10 0.85 1180812 40.542 1.267 683.7 35.8 Human Activity [19] 84 ESP32 18528.00 579.00 1727 1451.113 45.347 - - Base (B) 42.87 1.34 746530 15.046 0.470 58.2 13.0 Single Core (S) 85.73 2.68 373265 122.680 3.834 29.1 1.6 5-Core (M) 28.26 0.88 1132343 42.277 1.321 88.3 4.6 Gesture Phase [14] 89 ESP32 2496.00 78.00 12821 195.487 6.109 1.0 1.0 Base (B) 83.05 2.60 385310 29.151 0.911 578.8 129.1 Single Core (S) 166.1 5.19 192655 237.689 7.428 289.4 15.8 5-Core (M) 50.1 1.57 638723 74.950 2.342 959.4 50.2 Sensorless Drives [4] 86 ESP32 48,068.27 1502.13 666 3764.707 117.647 - - Base (B) 30.115 0.94 1062593 10.570 0.330 544.8 121.6 Single Core (S) 60.23 1.88 531297 86.189 2.693 272.4 14.9 5-Core (M) 57.57 1.80 555845 86.125 2.691 285.0 14.9 Gas Sensor Array Drift [24] 90 ESP32 16,407.33 512.73 1950 1285.022 40.157 - - same accuracy.\n\n--- Segment 15 ---\nTable 2: Latency and energy comparisons of the proposed accelerators vs an Espressif ESP32 software version of the algorithm. Latency (us) Energy (uJ) Dataset Acc Design Batch Single Data Point Throughput (inf s) Batch Single data point xSpeedups xEnergy Reduction Base (B) 7.44 0.23 4303968 2.610 0.082 245.3 22.9 Single Core (S) 14.87 0.46 2151984 21.279 0.665 122.7 2.8 5-Core (M) 7.64 0.24 4188482 11.429 0.357 238.7 5.2 EMG [10] 87 ESP32 1824.00 57.00 17544 59.791 1.868 - - Base (B) 37.80 1.18 846561 13.268 0.415 490.2 109.4 Single Core (S) 75.60 2.36 423280 108.184 3.381 245.1 13.4 5-Core (M) 27.10 0.85 1180812 40.542 1.267 683.7 35.8 Human Activity [19] 84 ESP32 18528.00 579.00 1727 1451.113 45.347 - - Base (B) 42.87 1.34 746530 15.046 0.470 58.2 13.0 Single Core (S) 85.73 2.68 373265 122.680 3.834 29.1 1.6 5-Core (M) 28.26 0.88 1132343 42.277 1.321 88.3 4.6 Gesture Phase [14] 89 ESP32 2496.00 78.00 12821 195.487 6.109 1.0 1.0 Base (B) 83.05 2.60 385310 29.151 0.911 578.8 129.1 Single Core (S) 166.1 5.19 192655 237.689 7.428 289.4 15.8 5-Core (M) 50.1 1.57 638723 74.950 2.342 959.4 50.2 Sensorless Drives [4] 86 ESP32 48,068.27 1502.13 666 3764.707 117.647 - - Base (B) 30.115 0.94 1062593 10.570 0.330 544.8 121.6 Single Core (S) 60.23 1.88 531297 86.189 2.693 272.4 14.9 5-Core (M) 57.57 1.80 555845 86.125 2.691 285.0 14.9 Gas Sensor Array Drift [24] 90 ESP32 16,407.33 512.73 1950 1285.022 40.157 - - same accuracy. The accelerator configurations are given in Table 1.\n\n--- Segment 16 ---\nLatency (us) Energy (uJ) Dataset Acc Design Batch Single Data Point Throughput (inf s) Batch Single data point xSpeedups xEnergy Reduction Base (B) 7.44 0.23 4303968 2.610 0.082 245.3 22.9 Single Core (S) 14.87 0.46 2151984 21.279 0.665 122.7 2.8 5-Core (M) 7.64 0.24 4188482 11.429 0.357 238.7 5.2 EMG [10] 87 ESP32 1824.00 57.00 17544 59.791 1.868 - - Base (B) 37.80 1.18 846561 13.268 0.415 490.2 109.4 Single Core (S) 75.60 2.36 423280 108.184 3.381 245.1 13.4 5-Core (M) 27.10 0.85 1180812 40.542 1.267 683.7 35.8 Human Activity [19] 84 ESP32 18528.00 579.00 1727 1451.113 45.347 - - Base (B) 42.87 1.34 746530 15.046 0.470 58.2 13.0 Single Core (S) 85.73 2.68 373265 122.680 3.834 29.1 1.6 5-Core (M) 28.26 0.88 1132343 42.277 1.321 88.3 4.6 Gesture Phase [14] 89 ESP32 2496.00 78.00 12821 195.487 6.109 1.0 1.0 Base (B) 83.05 2.60 385310 29.151 0.911 578.8 129.1 Single Core (S) 166.1 5.19 192655 237.689 7.428 289.4 15.8 5-Core (M) 50.1 1.57 638723 74.950 2.342 959.4 50.2 Sensorless Drives [4] 86 ESP32 48,068.27 1502.13 666 3764.707 117.647 - - Base (B) 30.115 0.94 1062593 10.570 0.330 544.8 121.6 Single Core (S) 60.23 1.88 531297 86.189 2.693 272.4 14.9 5-Core (M) 57.57 1.80 555845 86.125 2.691 285.0 14.9 Gas Sensor Array Drift [24] 90 ESP32 16,407.33 512.73 1950 1285.022 40.157 - - same accuracy. The accelerator configurations are given in Table 1. The proposed base B accelerator is the most resource efficient for LUTs and FFs and operates at the highest frequency.\n\n--- Segment 17 ---\nThe accelerator configurations are given in Table 1. The proposed base B accelerator is the most resource efficient for LUTs and FFs and operates at the highest frequency. Additionally, on the same eFPGA chip, the Single Core (S) uses 2.5x fewer LUTs and 3.38x fewer FFs than MATADOR for MNIST. BRAMs for B, S and M are over-provisioned for more tunability later. The models were compressed into 16-bit instructions and used to program the proposed accelerator configurations. Fig. 9 shows the energy and latency of the configurations compared to MATADOR. The num- bers in red indicate the speed-up and energy reduction compared to a software implementation of the compressed Include encoded inference approach running on an STM32Disco MCU (RDRS) as presented by [15] claiming upto 5700x speedup compared with embedded BNNs - MCU comparisons will be discussed in more detail in Question 2. All B, S, M configurations are within one order of magnitude of the MATADOR results; in the case of CIFAR 2, B is the most energy efficient. The key point to note is that MATA- DOR is fixed to these latencies and energies; however, a real-time recalibration to a smaller model would improve the B, S, M results without resynthesis. Ultimately, the key point becomes application, if the trained model is always a good representation of the data it will infer, then MATADOR is a better option. If the data is subject to drift, or there is need for personalization or sensor reading degra- dation or environmental fluctuation, and there is an opportunity to use a system like Fig. 8, then the proposed accelerators are more viable. They will adapt in situ during deployment. Addressing Question 2: The flexibility to change model size and architecture at runtime makes the accelerator comparable to small RISC processors. Question 1 already demonstrated the energy and latency advantages against an Arm-based STM32Disco. This section explores how it compares to another even smaller, cheaper low-power MCU - the Espressif ESP32. This is done to understand whether the eFPGA is the best platform to be used in a system like Fig. 8. Once again, the ESP32 runs the same compressed model inference, but as a software task on the processor.\n\n--- Segment 18 ---\n8. Once again, the ESP32 runs the same compressed model inference, but as a software task on the processor. This time, the applications are chosen for their suitability for run-time recalibra- tion (Table II). The EMG dataset involves classifying myographic signals sent from a bracelet to an edge inference node [10]. This requires recalibration for user personalization, the same is true for Gesture Phase [14] and Human Activity detection [19], they are all used to classify user movements. Sensorless Drives [4] involves diagnosing faulty components in electric current drive signals. Gas Sensor Array Drift uses chemical sensors to classify different gas concentrations [24]. Both applications are subject to environmen- tal changes and component aging. Across all the datasets, all the proposed accelerators give better latency and energy efficiency (see speedup and energy reduction with respect to the ESP32 implemen- tation). For Sensorless Drives the 5-Core M configuration gives the best energy performance. 5 CONCLUSION This work explored the possibilities of LUT frugal and flexible accelerator development for applications that require runtime recal- ibration. The Tsetlin Machine algorithm leads to minimal bitwise compute, and its sparsity enables the instruction-based compression to allow models to fit well with the BRAMs of of-the-shelf eFPGA platforms. While current FPGA works leverage reconfigurability to build the fastest possible custom architectures, this work uses it to allow customization options in memory, batching, interconnect and number of cores. The proposed accelerator implementations offer far better energy efficiency than the low-power MCUs running the same compressed model inference. 6 ACKNOWLEDGMENTS This work is supported by the UK Research and Innovation (UKRI) Engineering and Physical Sciences Research Council (EPSRC) under grant EP X039943 1 and grant reference: studentship-2926263.\n\n--- Segment 19 ---\nThe proposed accelerator implementations offer far better energy efficiency than the low-power MCUs running the same compressed model inference. 6 ACKNOWLEDGMENTS This work is supported by the UK Research and Innovation (UKRI) Engineering and Physical Sciences Research Council (EPSRC) under grant EP X039943 1 and grant reference: studentship-2926263. Runtime Tunable Tsetlin Machines for Edge Inference on eFPGAs tinyML Research Symposium, April 2025, Austin, TX REFERENCES [1] K. Darshana Abeyrathna, Ahmed A. O. Abouzeid, Bimal Bhattarai, Charul Giri, Sondre Glimsdal, Ole-Christoffer Granmo, Lei Jiao, Rupsa Saha, Jivitesh Sharma, Svein A. Tunheim, and Xuan Zhang. 2023. Building concise logical patterns by constraining tsetlin machine clause size. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence (Macao, P.R.China) (IJCAI 23). Article 378, 9 pages. [2] Marta Andronic and George A. Constantinides. 2023. PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference. In 2023 International Conference on Field Programmable Technology (ICFPT). 60 68. [3] Abu Bakar, Tousif Rahman, Rishad Shafik, Fahim Kawsar, and Alessandro Mon- tanari. 2023. Adaptive Intelligence for Batteryless Sensors Using Software- Accelerated Tsetlin Machines. In Proceedings of the 20th ACM Conference on Embedded Networked Sensor Systems (SenSys 22). 236 249. 1145 3560905.3568512 [4] Martyna Bator. 2013. Dataset for Sensorless Drive Diagnosis. UCI Machine Learning Repository. DOI: [5] Michaela Blott, Thomas B. Preu√üer, Nicholas J. Fraser, Giulio Gambardella, Ken- neth O brien, Yaman Umuroglu, Miriam Leeser, and Kees Vissers. 2018. FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks. ACM Trans.\n\n--- Segment 20 ---\nFINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks. ACM Trans. Reconfigurable Technol. Syst. 11, 3, Article 16 (dec 2018), 23 pages. [6] Francesco Conti, Pasquale Davide Schiavone, and Luca Benini. 2018. XNOR Neural Engine: A Hardware Accelerator IP for 21.6-fJ op Binary Neural Network Inference. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems 37, 11 (Nov. 2018), 2940 2951. [7] Li Deng. 2012. The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine 29, 6 (2012), 141 142. [8] Ole-Christoffer Granmo. 2018. The Tsetlin Machine - A Game Theoretic Bandit Driven Approach to Optimal Pattern Recognition with Propositional Logic. CoRR abs 1804.01508 (2018). arXiv:1804.01508 [9] Lei Jiao, Xuan Zhang, Ole-Christoffer Granmo, and Kuruge Darshana Abeyrathna. 2023. On the Convergence of Tsetlin Machines for the XOR Operator. IEEE Transactions on Pattern Analysis and Machine Intelligence 45, 5 (2023), 6072 6085. [10] N. Krilova, I. Kastalskiy, V. Kazantsev, V.A. Makarov, and S. Lobov. 2018. EMG Data for Gestures. UCI Machine Learning Repository. DOI: [11] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Technical Report. University of Toronto. [12] Jie Lei, Tousif Rahman, Rishad Shafik, Adrian Wheeldon, Alex Yakovlev, Ole- Christoffer Granmo, Fahim Kawsar, and Akhil Mathur. 2021. Low-Power Audio Keyword Spotting Using Tsetlin Machines. Journal of Low Power Electronics and Applications 11, 2 (2021). [13] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Jo√£o Gama, and Guangquan Zhang. 2019. Learning under Concept Drift: A Review.\n\n--- Segment 21 ---\n2019. Learning under Concept Drift: A Review. IEEE Transactions on Knowledge and Data Engineering 31, 12 (2019), 2346 2363. 2876857 [14] Renata Madeo, Priscilla Wagner, and Sarajane Peres. 2013. Gesture Phase Segmen- tation. UCI Machine Learning Repository. DOI: [15] Sidharth Maheshwari, Tousif Rahman, Rishad Shafik, Alex Yakovlev, Ashur Rafiev, Lei Jiao, and Ole-Christoffer Granmo. 2023. REDRESS: Generating Compressed Models for Edge Inference Using Tsetlin Machines. IEEE Transactions on Pattern Analysis and Machine Intelligence (2023), 1 16. 2023.3268415 [16] Nour Moustafa and Jill Slay. 2015. UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set). In 2015 Military Communications and Information Systems Conference (MilCIS). 1 6. [17] Jennifer Ngadiuba, Vladimir Loncar, Maurizio Pierini, Sioni Summers, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris, Dylan Rankin, Sergo Jindari- ani, Mia Liu, Kevin Pedro, Nhan Tran, Edward Kreinar, Sheila Sagear, Zhenbin Wu, and Duc Hoang. 2020. Compressing deep neural networks on FPGAs to binary and ternary precision with hls4ml. Machine Learning: Science and Technology 2, 1 (dec 2020), 015001. [18] Tousif Rahman, Gang Mao, Sidharth Maheshwari, Rishad Shafik, and Alex Yakovlev. 2024. MATADOR: Automated System-on-Chip Tsetlin Machine Design Generation for Edge Applications. In 2024 Design, Automation and Test in Europe Conference and Exhibition (DATE). 1 6. 2024.10546779 [19] Jorge Reyes-Ortiz, Davide Anguita, Alessandro Ghio, Luca Oneto, and Xavier Parra. 2013. Human Activity Recognition Using Smartphones. UCI Machine Learning Repository. DOI: [20] Md.\n\n--- Segment 22 ---\nUCI Machine Learning Repository. DOI: [20] Md. Maruf Hossain Shuvo, Syed Kamrul Islam, Jianlin Cheng, and Bashir I. Morshed. 2023. Efficient Acceleration of Deep Learning Inference on Resource- Constrained Edge Devices: A Review. Proc. IEEE 111, 1 (2023), 42 91. https: doi.org 10.1109 JPROC.2022.3226481 [21] Olga Tarasyuk, Anatoliy Gorbenko, Tousif Rahman, Rishad Shafik, Alex Yakovlev, Ole-Christoffer Granmo, and Lei Jiao. 2023. Systematic Search for Optimal Hyper- parameters of the Tsetlin Machine on MNIST Dataset. In Second International Symposium on the Tsetlin Machine. [22] Svein Anders Tunheim, Lei Jiao, Rishad Shafik, Alex Yakovlev, and Ole-Christoffer Granmo. 2022. A Convolutional Tsetlin Machine-based Field Programmable Gate Array Accelerator for Image Classification. In 2022 International Symposium on the Tsetlin Machine (ISTM). 21 28. [23] Yaman Umuroglu, Yash Akhauri, Nicholas James Fraser, and Michaela Blott. 2020. LogicNets: Co-Designed Neural Networks and Circuits for Extreme-Throughput Applications. In 2020 30th International Conference on Field-Programmable Logic and Applications (FPL). 291 297. [24] Alexander Vergara. 2012. Gas Sensor Array Drift Dataset. UCI Machine Learning Repository. DOI: [25] Marian Verhelst and Bert Moons. 2017. Embedded Deep Neural Network Processing: Algorithmic and Processor Techniques Bring Deep Learning to IoT and Edge Devices. IEEE Solid-State Circuits Magazine 9, 4 (2017), 55 65. [26] Erwei Wang, James J. Davis, Peter Y. K. Cheung, and George A. Constantinides. 2019. LUTNet: Rethinking Inference in FPGA Soft Logic. In 2019 IEEE 27th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM). 26 34. [27] Pete Warden. 2018. Speech commands: A dataset for limited-vocabulary speech recognition.\n\n--- Segment 23 ---\n2018. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint arXiv:1804.03209 (2018). [28] Yichi Zhang, Junhao Pan, Xinheng Liu, Hongzheng Chen, Deming Chen, and Zhiru Zhang. 2021. FracBNN: Accurate and FPGA-Efficient Binary Neural Networks with Fractional Activations. In The 2021 ACM SIGDA International Symposium on Field-Programmable Gate Arrays (Virtual Event, USA) (FPGA 21). Association for Computing Machinery, New York, NY, USA, 171 182.\n\n