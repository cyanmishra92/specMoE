=== ORIGINAL PDF: 2505.02314v1_NeuroSim_V15_Improved_Software_Backbone_for_Benchm.pdf ===\n\nRaw text length: 73100 characters\nCleaned text length: 72549 characters\nNumber of segments: 41\n\n=== CLEANED TEXT ===\n\narXiv:2505.02314v1 [cs.AR] 5 May 2025 1 NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities James Read Graduate Student Member, IEEE, Ming-Yen Lee Graduate Student Member, IEEE, Wei-Hsing Huang , Yuan-Chun Luo Graduate Student Member, IEEE, Anni Lu Graduate Student Member, IEEE, Shimeng Yu Fellow, IEEE, Abstract The exponential growth of artificial intelligence (AI) applications has exposed the inefficiency of conventional von Neumann architectures, where frequent data transfers between compute units and memory create significant energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses this challenge by performing multiply-accumulate (MAC) op- erations directly in the memory arrays, substantially reducing data movement. However, designing robust ACIM accelerators requires accurate modeling of device- and circuit-level non- idealities. In this work, we present NeuroSim V1.5, introducing several key advances: (1) seamless integration with TensorRT s post-training quantization flow enabling support for more neural networks including transformers, (2) a flexible noise injection methodology built on pre-characterized statistical models, mak- ing it straightforward to incorporate data from SPICE simu- lations or silicon measurements, (3) expanded device support including emerging non-volatile capacitive memories, and (4) up to 6.5 faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The combination of these capabilities uniquely enables systematic design space exploration across both accuracy and hardware efficiency metrics. Through multiple case studies, we demonstrate optimization of critical design parameters while maintaining network accuracy. By bridging high-fidelity noise modeling with efficient simulation, NeuroSim V1.5 advances the design and validation of next-generation ACIM accelerators. All NeuroSim versions are available open-source at Index Terms Compute-in-memory, AI accelerator, hard- ware software co-design, open-source, benchmark I. INTRODUCTION T HE exponential growth in AI applications has exposed a critical challenge in energy-efficient computing. The fun- damental limitation lies in the mismatch between the memory- centric workloads such as in machine learning algorithms and the processing-centric architecture in contemporary hardware platforms. This mismatch causes massive data movement between memory and processing units to consume more than five times the energy of computation [1]. The challenges stem from the data-intensive nature of deep neural networks (DNNs), where a relatively small number of operations is performed per fetched data; this causes system performance to be limited by memory bandwidth, as This work is supported in part by PRISM, one of the SRC DARPA JUMP 2.0 Centers. J. Read acknowledges support from the DoD s SCALE program. The authors are with the School of Electrical and Computer Engi- neering, Georgia Institute of Technology, Atlanta, GA, 30332 USA (email: noted by the roofline model [2]. Compute-in-memory (CIM) has emerged as a promising paradigm to tackle this bot- tleneck by co-locating data storage and multiply-accumulate (MAC) operations in the same physical arrays, thereby reduc- ing repeated weight fetches. In digital computing-in-memory (DCIM), memory arrays (typically made of modified SRAM bit cells) are augmented with digital multipliers and adder trees for partial-sum accumulation, delivering robust performance with the overhead of adder trees [3]. By contrast, ACIM leverages the intrinsic electrical behavior of memory cells to carry out multiplications directly in the current or charge domain. Specifically, weight values are encoded as conductance (or capacitance), and when an input voltage is applied to a row, the resultant column current (or charge) naturally represents the multiplication of input and weight. Summing all such column outputs yields the MAC result in the analog domain and then being converted back to the digital domain through analog-to-digital converters (ADCs) [4], [5]. A variety of emerging non-volatile memory (NVM) devices have been utilized for these ACIM arrays, including resistive random-access memory (RRAM) [6] [9], phase-change memory (PCM) [10] [12], ferroelectric field- effect transistors (FeFET) [13] [15], and more recently non- volatile capacitors (nvCap) [16] [18]. ACIM implementations can also rely on modified SRAM bit cells, tapping into current- based summation [19] [24], or incorporate on-chip capacitors to accumulate charge [25], [26] or using them for time-domain computing [27]. Several of these memory cells are depicted in a generic CIM macro with peripheral circuits as shown in Fig. 1. Over the past few years, both DCIM and ACIM have been demonstrated in silicon with memory capacities up to a few megabytes [28] sufficient for many on-chip inference scenarios such as computer vision tasks involving millions of parameters. DCIM generally offers the robustness of digital processing, making it attractive for high-performance comput- ing, whereas ACIM excels in energy efficiency, an essential requirement for power-constrained edge devices. ACIM accelerators can theoretically surpass 100 TOPS W [3], [9], [19], [26] far exceeding the 1 10 TOPS W range of contemporary von Neumann platforms largely because they eliminate most weight-reloading costs by performing the MAC operations directly within the memory arrays. In this paper, one operation is defined as an 8b 8b MAC. The key challenge 2 Fig. 1. Overview of NeuroSim V1.5: This update revamped the software backbone to support arbitrary neural network models and provide a more intuitive noise simulation workflow tailored to device and circuit experts. NeuroSim V1.5 enables users to evaluate inference accuracy across diverse architectures, including custom convolutional neural networks and transformers, while simulating various device types and noise sources. The platform uniquely supports power, performance, and area (PPA) estimations, facilitating comprehensive system-level design space exploration. Key new features include optimized noise modeling, compatibility with advanced non-volatile memory technologies, and improved runtime performance. in ACIM is to manage inherently noisy analog computations to maintain the inference accuracy. These noise sources span multiple abstraction layers. At the device level, fabrication imperfections lead to device-to-device (D2D) variations and stuck-at-faults. Memory states can also drift over time, degrading compute accuracy [29] [32]. At the circuit level, thermal noise and process variations in peripheral circuits degrade sensing margins, resulting in errors in the ADC processes [18], [33]. At the system level, partial sums may be quantized at a low bit precision, compounding errors as layer outputs are passed from one stage to another [34] [36]. Together, these effects can reduce the inference accuracy of DNNs run on ACIM chips. Consequently, modeling and ultimately mitigating these non-idealities is critical for the design of robust ACIM architectures. These multi-layered noise effects create complex interac- tions between device characteristics, circuit design choices, and neural network requirements. Therefore optimizing ACIM accelerators requires systematic exploration across this design space to balance inference accuracy with hardware efficiency. Such an exploration requires simulation frameworks that can evaluate both accuracy and hardware metrics while maintain- ing practical runtimes even for modern large-scale networks. Direct SPICE simulation of analog computing arrays is prohibitively expensive for large neural networks, making it impractical to evaluate broad parameter spaces with modern datasets. While tools like CrossSim [37] and AIHWKit [38] offer detailed device modeling and noise-aware training re- spectively, they lack integrated hardware analysis capabilities and require extensive engineering effort to support new devices or circuits. Other frameworks like CIMLoop [39] focus on hardware metrics without considering accuracy under device and circuit non-idealities. These limitations make it difficult to systematically explore and optimize ACIM designs across both software accuracy and hardware efficiency metrics. NeuroSim [40] addresses these challenges through an in- tegrated framework that combines behavioral simulation of CIM operation with automated hardware analysis. Building on this foundation, we present NeuroSim V1.5, which introduces several key advances to enable comprehensive design space exploration: 1) Seamless integration with TensorRT to support arbitrary pre-trained models, while automating quantization. 2) A flexible noise modeling methodology built on pre- characterized statistical models, enabling rapid evalua- tion of diverse device and circuit designs. 3) Support for emerging technologies like charge-based non-volatile capacitive memories (nvCap) and emerging algorithms like vision transformer (ViT). 4) Significantly improved runtime performance - up to 6.5 faster than previous versions of NeuroSim. These capabilities maintain seamless compatibility with NeuroSim s existing power, performance, and area (PPA) anal- ysis framework, creating a unique environment for systematic CIM accelerator design optimization. The framework achieves runtime only two to three times that of software-only standard GPU inference. The remainder of this paper is organized as follows: Section II provides technical background on ACIM systems and ex- isting simulation approaches. Section III details our improved framework architecture and generic noise modeling method- ology. Section IV presents results from various case studies including inference accuracy analysis of several devices and compute circuits using data from SPICE simulations and tape- out measurements [7], [18], [33], [41] [43]. We also provide design space exploration for a RRAM based ACIM chip to demonstrate the framework s adaptability and improved running speed. Finally, Section VI discusses implications for future larger-scale AI model benchmarking and concludes the paper. 3 II. BACKGROUND INFORMATION A. CIM Array Architecture and Operation ACIM performs matrix computations by utilizing physical properties of memory arrays. The fundamental building block is the crossbar array, where memory elements are arranged at the intersections of wordlines (WLs) and bitlines (BLs). This crossbar arrangement is uniquely suited for parallel MAC operations because each row s input signal (voltage) is simultaneously applied to all memory devices along that row. The resulting outputs (currents or charges) are summed along each column, naturally implementing the dot product. Consequently, large-scale MAC operations can be performed in one analog step , vastly increasing the throughput com- pared to digital architectures that must shuttle data back and forth between separate memory and logic units. To date, two primary methods of analog computation have emerged. Resistive arrays constructed with RRAM, PCM, FeFET that encode neural network weights as conductance states (G), performing MAC operations in the current domain: IBLi X j GijVj (1) where IBLi represents the analog matrix-vector multiplica- tion (MVM) output current on the ith bitline, and j indexes across the wordlines. Capacitive arrays utilize non-volatile capacitors (nvCap), where the weight is represented by the programmable capaci- tance of the ferroelectric or charge-trap memory cell [44] [45], thus performing MAC operations in the charge domain: QBLi X j CijVj (2) Alternative charge-based computation approaches utilize fixed capacitors in conjunction with SRAM cells, where the stored digital weight modulates charge accumulation through charge sharing mechanisms [46], [47]. DCIM has emerged as another promising approach [3], where SRAM arrays are augmented with digital multipliers and adder trees to perform integer MAC operations directly within memory. These systems are attractive due to their compatibility with advanced CMOS technology scaling and robust digital operation. However, the digital processing over- head - particularly from the complex adder trees required for partial sum accumulation - impacts array density. Additionally, SRAM leakage power becomes more significant in advanced technology nodes. Both the array scalability and leakage power create challenges for running complete neural networks on- chip. Each approach is finding its niche, while ACIM remains near the theoretical limit of efficiency for neural network ap- plications. In ACIM designs, the choice of memory technology influences key array characteristics like precision, reliability, and energy efficiency - topics we will explore in detail when discussing noise modeling methodology. B. Device and Circuit Level Considerations The precision and reliability of analog computing operations depend heavily on the properties of individual memory cells. Key device parameters include: On Off Ratio: The ratio between maximum and minimum conductance capacitance states defines the usable dynamic range for weight storage. Finite on off ratios introduce non- zero current charge contribution from cells programmed to the off states, which accumulates across array columns and can de- grade computation accuracy. This effect becomes particularly significant in larger arrays where hundreds of devices in the off-state contribute to the output. To address this, array designs typically include additional dummy columns programmed to off states and are supported in NeuroSim [40], whose outputs are subtracted from the computational columns to cancel the accumulated off-state current charge. While this technique ef- fectively eliminates the impact of finite on off ratio, it slightly reduces the available dynamic range for computation since part of the range must be reserved for cancellation. This results in smaller sensing margins and potentially smaller signal-to-noise ratio (SNR). Therefore, high on off ratios are still a desired characteristic for ACIM memory devices. Number of States: Supporting high-precision weights re- quires either multi-level memory cells (MLC) or bit slicing with multiple lower-precision binary cells. For MLC devices, the number of reliably programmable states determines the weight precision. RRAM, PCM, and nvCap typically achieve 1-4 bits per cell [8], [11], [28], while FeFET have demon- strated 5 bits [14], [48]. At the circuit level, array size is limited mainly by sensing margins, where reliable output detection requires sufficient separation between computational states. This becomes chal- lenging with larger arrays due to increased noise accumulation and reduced signal levels from parasitic effects. Critical peripheral circuits include: WL BL drivers to supply the input and output signals. ADCs for digitizing array outputs. Shift-and-add circuits for managing bit-precise computa- tions. Accumulation units on the PE level for partial sum aggregation. Multiplexers for data routing and array access. The digital circuit blocks in the periphery can be verified through conventional VLSI design methodologies. Therefore, behavioral modeling efforts focus primarily on the analog computational elements. C. Precision and Data Representation Neural network parameters must be carefully mapped to CIM arrays while managing precision and computational accu- racy. This mapping presents two key challenges: representing multi-bit input activations and storing high-precision weights. For input activations, two main approaches exist: 1) Using digital-to-analog converters (DACs) to represent each input value as an analog voltage. While straight- forward, this requires complex multi-bit DACs for every row of the array, introducing significant area and energy 4 Fig. 2. Procedure for ACIM behavioral simulation in NeuroSim V1.5. (1) Inputs and weights are quantized from floating point to integer. (2) Inputs and weights unfolded and (3) mapped to compute-in-memory arrays. (4) Analog MACs are calculated with analytical circuit models. (5) Noise is injected either to memory devices or to digital MAC outputs. (6) ADC outputs are shifted and added. (7) Outputs from memory arrays are accumulated and concatenated. (8) Accumulated outputs are folded into output tensors. (9) Outputs are de-quantized from integer back to floating-point. overhead. The I-V or C-V nonlinearity of the underlying memory devices may also introduces the errors in the input representation. 2) Processing each bit of the digital input serially, with outputs shifted and added according to bit significance. This eliminates DAC overhead but requires multiple computational cycles. For weight storage, two methods are available: 1) Bit slicing across multiple lower-precision cells. For example, an 8-bit weight can be distributed across cells of various precisions - from a single 8-bit cell to eight 1-bit cells, with intermediate combinations possible. When slicing the weight among multiple cells, each cell is placed in an adjacent column, with the outputs of each column scaled by its significance post-ADC and added with the other scaled outputs. This operation is referred to as shift-add and is described in Fig. 2 and in section III. Using analog shift-add before the ADCs has also been explored [44], [49], [50], but is not explicitly supported in NeuroSim V1.5. While NeuroSim s behavioral simulator supports both input representation methods, the PPA estimator only supports the bit-serial approach due to its practical hardware implementa- tion. In CIM, the fundamental MAC operation is performed in the integer domain, necessitating quantization of the inputs and weights of each layer in the network from floating-point to integer. The analog multiplication and accumulation processes generate integer results, which are digitized through ADCs. Finally, the integer outputs are de-quantized back to floating- point for further processing. Some accelerators explore quan- tizing the entire network to integers [51], [52], requiring spe- cialized approximations for operations like normalization and activation functions. This approach is particularly applicable to transformers, which require more complex operators like GELU and layer normalization. D. Hierarchical System Architecture The organization of ACIM accelerators stems from fun- damental constraints in both device technology and neural network structure. Memory array sizes are typically limited to dimensions between 32 32 and 256 256 cells due to sensing margins and manufacturing considerations. However, neural network layers often require much larger matrix operations. This dimensional mismatch necessitates a hierarchical decom- position of network operations. The organization of the chip hierarchy in NeuroSim is as follows: At the base level, synaptic arrays perform MAC op- erations through crossbar structures. These arrays are grouped into processing elements (PEs), which implement weight- stationary, spatial mapping of convolutional operations. For example, when mapping 3D convolutional kernels to 2D array structures, multiple arrays within a PE process different kernel depths in parallel, with their outputs appropriately accumu- lated. PEs are further aggregated into computational tiles, each capable of processing a portion of a neural network layer. The tile architecture incorporates buffers for activation storage and accumulation circuits for combining partial results from multiple PEs. This organization enables efficient processing of large layers by distributing computations across multiple tiles while maintaining data locality. At the chip level, multiple tiles operate together through either an H-tree or X-Y bus interconnect structure. This highest level of hierarchy includes global buffers for inter-tile data movement and accumulation circuits for combining results from distributed computations. The entire system implements layer-wise pipelining, where 5 different tiles process consecutive layers of the network si- multaneously to maximize throughput. To balance processing speed between layers of different sizes, weight duplication can be employed within tiles [53]. For instance, if a layer s weight matrix is smaller than the tile s capacity, the weights can be duplicated to process multiple input vectors simultaneously, improving throughput. This hierarchical structure enables flexible mapping of di- verse neural network architectures while managing the phys- ical constraints of ACIM arrays. To model this hierarchy, NeuroSim employs analytical models for circuit blocks derived from standard cell libraries in advanced logic nodes. The previously released V1.4 [54] expanded technology support to the leading-edge 1 nm node, introducing stacked nanosheet transistors and studying the scaling path toward a future A5 node with complementary-FET (CFET) technology. Thanks to NeuroSim s software hardware co-design philosophy and open-source development model, the tool enables broader research on emerging device technologies, array architectures, on-chip training [55], and advanced packaging [56] [58]. For a detailed analysis of the hierarchy in NeuroSim, readers are referred to [40]. III. SIMULATION METHODOLOGY A. Overview of Design Methodology Designing ACIM accelerators requires optimization across multiple abstraction layers - from device characteristics to system architectures. NeuroSim V1.5 provides an integrated framework to evaluate these designs through two main com- ponents shown in Fig. 1 working jointly: 1) A behavioral simulator that maps and quantizes neu- ral networks for ACIM implementation and evaluates inference accuracy while accounting for hardware non- idealities. 2) A hardware analyzer that estimates system-level energy, latency, and area through detailed circuit models of array peripherals and interconnects. In this update, our first contribution is a comprehensive re-design of the behavioral simulator. Second, we update the hardware analyzer to support the emerging nvCap-based CIM, complete with circuit models for the new charge- domain compute mode. In the behavioral simulator, we have fundamentally enhanced how we model compute-in-memory operations for accuracy estimation. A key approach is that we leverage pre-characterized statistical models derived from SPICE simulations or silicon measurements to ensure ac- curacy, while utilizing GPU-accelerated PyTorch operations to achieve the throughput needed for large-scale inference simulation. This separation allows us to maintain high fidelity while dramatically improving simulation efficiency. The behavioral simulator supports two primary simulation modes: Device expert mode: Model memory array physics directly, simulating conductance variations, stuck faults, and temporal drift to isolate the effect of memory technologies on network accuracy. Circuit expert mode: Model memory arrays with a sta- tistical approach using pre-characterized noise distributions from SPICE simulations or silicon measurements, enabling rapid evaluation of aggregated non-idealities from devices and circuits. Through integration with TensorRT s post-training quanti- zation flow, NeuroSim V1.5 automatically maps pre-trained neural networks to the corresponding ACIM architecture. The following sections detail the quantization methodol- ogy, array modeling approach, and specific techniques for incorporating device and circuit-level non-idealities into the simulation framework. B. Neural Network Quantization and Mapping 1) Quantization: NeuroSim V1.5 adopts NVIDIA s Ten- sorRT quantization infrastructure for converting neural net- works from floating-point to integer representations. This differs from previous NeuroSim versions which used in-house WAGE quantization [59] - an integer-training method that requires retraining the network. TensorRT instead provides post-training quantization through automatic operator replace- ment, supporting quantization of pre-trained networks using operators like linear layers, convolutions, and pooling without modification. TensorRT supports configurable bit widths for both inputs and weights, which we use to evaluate different precision settings (e.g., 8b 8b, 6b 6b). The framework determines optimal scaling factors through either max or histogram calibration during a brief calibra- tion phase. This phase typically requires only 2 batches of calibration data (e.g., several hundred images) to analyze the distribution of values in both input tensors and weight matrices. Max calibration simply uses the maximum observed value across these tensors to set the scaling factor. Histogram calibration creates a histogram of the observed tensor values and sets the scale factor based on the cumulative distribution function (CDF) - specifically, choosing the value at which the CDF reaches a user-specified percentile (typically 99.99 ). This helps exclude outliers that would otherwise force a larger dynamic range. The key advantages of adopting TensorRT over WAGE quantization are direct support for arbitrary pre- trained models without retraining and the automated support for multiple operator types through operator replacement. 2) Network to Array Mapping: Physical constraints of ACIM arrays necessitate careful partitioning of neural network operations. For a linear layer with input dimension N and output dimension M, the quantized weight matrix must be mapped to multiple arrays based on both the memory array dimensions and the precision requirements. The number of memory cells required per weight depends on the bit precision of the cells (bcell). For example, to represent an 8-bit weight using 1-bit cells, we need Ncell 8 1 8 cells per weight. This bit-slicing requirement effectively multiplies the width dimension of our mapping by Ncell. As shown for a linear layer in Fig. 3, these weight components are arranged in adjacent columns, with each column corresponding to a different bit significance. Memory array size further constrains the mapping. With a memory array height of R rows and C columns, the N M 6 weight matrix must be partitioned into N R rows of arrays. The total number of arrays needed to map the weights is equal to N R MNcell C . Mapping convolutional layers to memory arrays requires additional unfolding operations, of which multiple methods can be implemented. For a more detailed description, readers are referred to [40]. The input feature map is unfolded and mapped to the arrays according to the selected input scheme. In Fig. 2 we show the bit-serial operation, where the feature map is processed separately for each bit of the quantized input. Outputs from vertically-aligned arrays are accumulated, as these represent partial sums from the same output neurons. Outputs from horizontally aligned arrays are shifted and added according to their bit significance, then concatenated to form the complete layer output. 3) Support for Transformers: For transformer architec- tures, NeuroSim V1.5 adopts a hybrid approach that maps compatible operations to CIM arrays while implementing other operations in the digital domain. The framework maps linear layers (including query, key, value projections and MLP layers) to CIM arrays using the same methodology as standard neural networks. These linear layers comprise a significant portion of the total computation in transformer models. How- ever, dynamic operations such as attention score computation, layer normalization, and activation functions must still be computed. We elect to compute these dynamic operations in floating- point compute units due to CIM hardware constraints. Non- volatile memory-based CIM is optimal for weight-stationary computations that do not require frequent weight updates. This is due to limited write endurance and slow programming speed common in these devices. Therefore, operators like self- attention that require unique input and weight matrices each in- ference are not well-suited for non-volatile CIM. Digital CIM implementations using SRAM could potentially handle these dynamic operations, however they would still require careful quantization of the query, key, and value matrices to main- tain compatibility with integer-only computation [60]. Recent work on integer-only transformers [51], [52] has demonstrated promising approaches for full integer quantization, though further research is needed to evaluate the accuracy-efficiency tradeoffs of the hardware. The current hardware estimation portion of NeuroSim does not include circuit models for floating-point functional units needed for self-attention computation, layer normalization, or GELU activation functions. Future versions will explore integrating these components, potentially through integer ap- proximations that maintain accuracy while enabling efficient hardware implementation. C. ACIM Behavioral Modeling 1) Memory Array Operation: Building on TensorRT s operator replacement strategy, NeuroSim V1.5 substitutes stan- dard neural network layers with specialized CIM operators that model array-level computation. When a PyTorch model is loaded, linear and convolutional layers are automatically replaced with cim.Linear and cim.Conv modules that partition Fig. 3. NeuroSim V1.5 flowchart. Device expert mode: user provides device data (memory state variation, stuck-at-faults, state drift). Circuit expert mode: user provides statistical data on MAC outputs from SPICE Monte-Carlo simulations or tapeout measurements. operations according to array constraints and simulate analog computation as described in Fig. 3. The framework incorporates two separate simulation modes for modeling non-idealities: device expert mode for measured device-level device variation or calibrated aging models, and circuit expert mode using statistical models derived from SPICE or silicon measurements. These modes cannot be used simultaneously as circuit-level noise inherently captures device-level variations. 2) Device and Circuit Non-Idealities: In device expert mode, integer weights are first mapped to conductance or capacitance states based on the target memory technology. The framework then simulates analog multiplication through Ohm s law (I GV) for resistive arrays or charge-based computation (Q CV) for capacitive arrays. Input values are treated as idealized voltages, and outputs are computed by summing the resulting currents or charges along each column. To capture all possible input and weight bit-slicing combina- tions, including both bit-serial and DAC-based input schemes, we use the following generalized equation: y Ncell X i Nin X j (2i bcell)(2j PDAC)(Wi xj) (3) where: bin is the bit significance of the inputs. PDAC is DAC precision (PDAC 1 for bit-serial). Nin is the number of input cycles bin PDAC bw is the bit significance of the weights. bcell is the number of bits per weight cell (e.g. bw 4 for 4-bit MLC) Ncell is the number of cells per weight bw bcell Wi represents the i-th bit slice of the weight matrix xj represents the j-th bit slice of the input vector For example, when using 4-bit MLC (bcell 4) and 8-bit weights (bw 8), Ncell 2 so i ranges from 0 to 1 and the weight scales for each iteration are 2i bcell 20 4 1 and 7 2i bcell 21 4 16. When using bit-serial mode (PDAC 1) to represent 8-bit activations (bin 8), Nin 8 so j ranges from 0 to 7 with input scales increasing by powers of 2. Each sum in equation 3 corresponds to a for-loop in the behavioral simulation. Consequently, the nested loop becomes the critical piece of code to optimize. Using PyTorch s GPU- accelerated tensor operations, we can calculate the output y of every memory array on the CIM chip design in parallel, which greatly reduces the overall runtime of NeuroSim. Shown in Fig. 3, in device expert mode, three primary categories of device variations are modeled: Device-to-Device (D2D) Variation: For each conductance (or capacitance) state i, the programmed value follows a Gaussian distribution: G N(Gmeani, σi) (4) where Gmeani is the target conductance for state i and σi is the user-specified standard deviation. Separate varia- tion parameters are specified for each level i. Users pro- vide these parameters as comma-separated tuples, with one variation value per memory state. Users store these in the file mem states.csv that is then read by the framework automatically. Stuck-at-Faults (SAF): Memory cells can become per- manently fixed to their minimum or maximum states due to manufacturing defects or programming failures. This is modeled by randomly setting cells to minimum or maximum states according to user-specified probabilities before weight mapping. Unlike temporal variations, this permanent modifi- cation is applied once during initialization. Temporal Drift: Time-dependent conductance changes fol- low: G(t) G0(t t0)v (5) where G0 is initial conductance, t is retention time, v is the drift coefficient, and t0 is a reference time. The framework supports two drift modes: random drifting, or drifting towards a fixed state. The simulation treats arrays as frozen at the specified time point t for inference calculations. In circuit expert mode, statistical models are derived from Monte Carlo circuit analysis or silicon measurements. For SPICE-based characterization, users should: Create a complete array model including peripheral cir- cuits in a circuit simulator. Run Monte Carlo simulations incorporating device vari- ations (threshold voltage, device sizing), circuit-level ef- fects (thermal noise, parasitic), and ADC quantization. Generate statistical distributions by collecting multiple simulation results for each possible MAC output value. For silicon-based characterization: 1) Collect repeated measurements across multiple array columns. 2) Compare expected vs. actual outputs over thousands of trials. 3) Build output distributions capturing combined effects of all noise sources. The resulting models provide mean and standard deviation pairs for each possible MAC output value. The data can be displayed in a Confusion Matrix like those shown in Fig. 3. Users provide these parameters as comma-separated tuples, with one variation value per output level. Users store these in the file output noise.csv . During behavioral simula- tion, the critical loop (Eq. 3 can be skipped, as the circuit behavior is modeled by the statistical output distribution. Therefore, we can calculate ideal partial sums with negligible runtime overhead. After ideal partial sums are calculated, the framework samples from the MAC distributions to efficiently inject noise into the outputs. This statistical approach enables rapid evaluation and SPICE-level accuracy through empirically validated noise characteristics (see case studies in Section IV.D). TABLE I DESIGN SPACE EXPLORATION PARAMETERS Parameter Range Values Technology Node 22nm Device RRAM HRS LRS 40kΩ 3kΩ Network Dataset ResNet-18 CIFAR-100 Array Dimensions 32 32 - 256 256 Input Weight Precision 6b, 8b Input Processing bit-serial Memory Cell Precision 1b 4b ADC Precision 3b 11b The RRAM parameters are configured based on Intel s 22nm FinFET platform [6] D. Performance Analysis Framework To enable accurate power and performance estimation, NeuroSim V1.5 implements a trace-based interface between its behavioral simulation and hardware evaluation components. During inference simulation in PyTorch, the framework saves quantized input and weight values as decimal traces in CSV format (shown in Fig. 1. These traces capture the actual data patterns that would flow through the hardware, accounting for all quantization effects and bit slicing operations. The hardware estimator (written in C ) automatically generates a floorplan for the entire accelerator based on the network architecture and user-specified parameters. It creates detailed transistor-level circuit models for all components (memory arrays, ADCs, interconnects, peripherals, accumula- tors, etc.), calibrated using SPICE simulations and data from previous tapeouts [7]. These models enable accurate hardware performance estimation at advanced technology nodes. Using traces from the software component, the estimator calculates detailed energy consumption for each array. The bit-serial input scheme is explicitly modeled - each input value is processed one bit at a time, with the framework tracking energy consumption for each operation. Weight values from the traces are converted to corresponding conductance or ca- pacitance states based on parameters specified in the hardware configuration file. In NeuroSim V1.5, we extended the hardware analyzer to support charge-based computation with non-volatile capacitive 8 memories. This required the addition of new circuit models for nvCap arrays, charge-to-voltage conversion using an amplifier [61], and control circuits for the unique charge-based compu- tation. The hardware estimator uses these models along with device parameters specified in the configuration file to accurately evaluate PPA metrics for nvCap-based accelerators while maintaining compatibility with the existing trace-based inter- face between behavioral simulation and hardware analysis. E. Evaluation Setup Our experimental evaluation consists of three main com- ponents: design space exploration of hardware configura- tions, inference accuracy analysis under various device non- idealities, and runtime performance comparison across open- source platforms. The DNN algorithms and proposed Neu- roSim framework are implemented using PyTorch, with all simulations performed on a single NVIDIA RTX A6000 GPU with 48GB GDDR6. 1) Design Space Exploration: For the design space explo- ration, we examine the trade-offs between hardware efficiency and inference accuracy across a range of architectural param- eters. Table I summarizes the key parameters and their ranges used in the design space exploration study. The memory device characteristics are based on Intel s 22nm RRAM platform [6]. We process inputs using the bit-serial approach (1b per cycle) to maintain consistency with the hardware estimator and activate all rows in parallel to maximize throughput. The ADC precision requirements are determined by the dynamic range of array outputs, which depends on three key factors: memory cell precision (bcell), DAC precision (PDAC), and array rows (R). For a given array configuration, the maximum output value can be calculated as: outmax R(2PDAC 1)(2bc 1) (6) The minimum required ADC precision (PADC) to capture this full range without quantization loss is: PADC log2(outmax) (7) In our design space exploration, we use binary inputs (1b DAC) and we evaluate two ADC precision settings for each configuration: 1) Full precision matching the calculated dynamic range (PADC). 2) Reduced precision (PADC 1 and PADC 2) to study the impact of quantization noise. Excluding device and circuit noise, the inference accu- racy is solely affected by the input weight quantization from floating-point to fixed-point representation and the quanti- zation of array outputs depending on the ADC precision. Design parameters were swept using automated scripts to explore the trade-offs in accuracy and performance considering input weight ADC quantization and array size. When reducing ADC precision below the ideal precision, we adopt a clipping approach, where the sensing margins for each analog output state remain the same regardless of ADC precision, and outputs larger than the precision of the ADC are clipped to the maximum ADC output based on its precision. We find that this method results in comparable accuracy to other, non-linear quantization methods while being the most practical to implement in hardware. It is important to note that higher precision ADCs require smaller sensing margins between levels, which in practice could lead to increased susceptibility to circuit noise and higher error rates. The design-space analysis focuses solely on quantization effects, as detailed circuit-level noise char- acterization data is not available for the multi-bit memory configurations under study. For a complete understanding of system-level reliability, future work should incorporate both quantization effects and circuit-level noise measurements. 2) Device Noise Case Studies: For analyzing inference accuracy under device non-idealities, we standardize the hard- ware configuration to isolate the effects of various noise sources and variations. Table II presents the fixed parameters used across all device non-ideality studies. TABLE II DEVICE NON-IDEALITIES CASE STUDY PARAMETERS Parameter Value Technology Node 22nm Device RRAM HRS LRS 40kΩ 3kΩ Networks Datasets VGG8 CIFAR-10 ResNet-18 CIFAR-100 ResNet-50 ImageNet 1k Swin-T ImageNet 1k Array Dimensions 128 128 Activated rows 128 Input Weight Precision 8b 8b Input Processing bit-serial Memory Cell Precision 1b - 4b ADC Precision full precision Device Noise Sources D2D, SAF, state drift ADC precision is calculated using equations (6) and (7) In these case studies, we analyze different sources of device variation including device-to-device variation, stuck- at-faults, and state drift across multiple DNNs and datasets to demonstrate support for multiple networks and noise types. 3) Circuit Noise Case Studies: Next, we demonstrate the power of our improved noise modeling by using circuit noise data characterized both from SPICE characterization and tapeout measurements. With this improved modeling, direct comparisons between different devices, compute circuits, and DNNs can be performed. The table in Fig. 8 summarizes the key characteristics of each platform, with parameters drawn directly from their respective publications. Mean and standard deviation for each output state are supplied in a comma separated format (.csv file), where each row of the file corresponds to the mean for each given output (post-ADC) and the standard deviation of the output. 9 4) Runtime Evaluation: To characterize the improvements in runtime over previous versions of NeuroSim, we record the runtime of the inference accuracy portion under several key conditions. The main parameters that influence the runtime are the size of the network and dataset used the MLC level, precision of the DAC if one is used, and the input and weight precision. IV. RESULTS AND CASE STUDIES A. Design Space Exploration We demonstrate NeuroSim V1.5 s capability for compre- hensive design space exploration using a 22nm RRAM [6] platform as our baseline technology. Following the parameters outlined in Table I, we evaluate the fundamental tradeoffs between inference accuracy and hardware efficiency across architectural choices including array size, precision, and data representation. Our analysis focuses on architectural optimiza- tion without device circuit noise effects, as these effects are technology-specific and addressed separately in subsequent noise modeling studies. TABLE III DEFAULT 22NM RRAM PPA (RESNET18 CIFAR100) TOPS TOPS mm2 TOPS W FPS Accuracy 11.6 0.013 21.3 7770 75.57 Example performance using 22nm RRAM [6] and default settings in NeuroSim: 128 128 array size, 7-bit ADC, 8b 8b input weight precision. Fig. 4. Example design space exploration for 22nm RRAM. Each point represents a unique configuration. Bit-serial input was used for every design. Designs with highest TOPS W use array sizes of 32 32 and 64 64. We provide example performance metrics for the default configuration in NeuroSim using the selected RRAM device. As shown in Table III, we achieve 11.6 TOPS at 21.3 TOPS W while processing 7,770 frames per second for CIFAR-100 images. Through systematic exploration of the design space, 4 2 10 5 15 7 20 10 40 20 0 20 40 60 80 100 Classification Accuracy ( ) Device to device varaition (HRS LRS) VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet Software Accuracy Fig. 5. Inference accuracy of VGG8 on CIFAR-10, ResNet18 on CIFAR-100, and ResNet50 on ImageNet using V1.5 across different RRAM (single-level cell) variations. we can find designs that significantly outperform this baseline. Results are shown in Fig. 4. From this exploration, we identify several key trends that characterize optimal CIM architectures: 1) ADC Precision: The Pareto-optimal designs cluster in the 5 8 bit ADC precision range. Our analysis shows that ADC precision can generally be reduced by 1-bit from the lossless precision without significant accuracy degradation, and by 2-bits in certain configurations. 2) Array Dimensions: Optimal designs utilize array sizes between 32 32 and 128 128 cells. Smaller ar- rays reduce ADC precision requirements but necessitate additional accumulation circuitry, while larger arrays necessitate larger ADCs, incurring costs in latency and energy. The designs that achieve 40 TOPS W used array sizes of either 32 32 or 64 64. 3) Memory Cell Precision: 2-bit and 3-bit MLC configura- tions dominate the Pareto frontier, offering an effective balance between storage density and programming reli- ability. These configurations achieve the highest energy efficiency (TOPS W) for any given accuracy target. The interplay between these parameters creates distinct architectural sweet spots. For example, the 8b 8b 64 6 2 con- figuration (input weight array size ADC MLC) achieves 75 accuracy while maintaining high energy efficiency. Designs with reduced precision (6b 6b) can push efficiency above 40 TOPS W at the cost of 5 accuracy degradation. The ability to rapidly evaluate these configurations through NeuroSim V1.5 enables the optimization of architectural parameters based on specific device and circuit designs. Next, we demonstrate the improved noise-modeling capa- bilities in the new behavioral simulator. B. Noise Modeling Case Studies 1) D2D Variation: We evaluated inference accuracy across multiple neural networks using NeuroSim V1.5 s comprehen- sive noise modeling capabilities, including D2D variation, memory state drifting, stuck-at-faults and circuit-level MAC output noise. To systematically assess accuracy degradation, 10 we selected network architectures of increasing complexity: VGG8 for CIFAR-10, ResNet18 for CIFAR-100, ResNet50 for ImageNet, and Swin Transformer Tiny for ImageNet. Following established methodology [62], we excluded the first convolutional layer from quantization in all simulations. Figure 5 reveals that accuracy degradation becomes more severe with increasing network complexity and dataset dif- ficulty. VGG8 on CIFAR-10 maintains reasonable accuracy until LRS variation reaches 20 . In contrast, ResNet18 on CIFAR-100 and ResNet50 on ImageNet show significant ac- curacy drops at just 10 and 5 LRS variation, respectively. Most notably, Swin Transformer Tiny demonstrates the lowest tolerance to device-level variations, highlighting the need for additional research into improving transformer model robust- ness for analog computing implementations. 101 103 105 107 40 50 60 70 80 90 Classification Accuracy ( ) Retention time (sec) Random drifting Drifting to Gmax Drifting to Gmin Software Accuracy Fig. 6. Inference accuracy of VGG8 on CIFAR-10 regarding random device conductance drifting, drifting to Gmax, and drifting to Gmin with 1b cells. 2) Conductance Drift: Fig. 6 examines how three different types of conductance drift (modeled by Equation 5) affect inference accuracy. First, random drift shows better accuracy retention compared to drifting toward minimum or maximum conductance. This improved resilience occurs because cells at minimum conductance cannot drift lower, while cells at maximum conductance cannot drift higher, effectively limiting the total conductance change possible under random drift. Second, drifting toward maximum conductance maintains bet- ter accuracy than drifting toward minimum conductance. This asymmetry arises from Equation 5, where the drift coefficient v determines the drift direction. For the same retention time t and drifting speed, drifting toward minimum conductance results in a larger conductance difference compared to drifting toward maximum, leading to a larger deviation of MAC outputs. 3) Stuck-at-Faults: NeuroSim V1.5 adds modeling ca- pabilities for stuck-at-fault (SAF) errors, extending beyond the D2D variation and drift modeling present in V1.4. Fig. 7 demonstrates how SAF errors impact inference accuracy across different networks, with SAF rates bounded by realistic RRAM fabrication constraints: 1.75 for LRS and 9.0 for HRS [42]. Our analysis reveals several key findings. First, both 1-bit and 2-bit cells show similar accuracy degradation at equivalent SAF rates, suggesting that cell precision does not significantly influence SAF sensitivity. However, network complexity strongly affects SAF tolerance - ResNet-50 on ImageNet loses accuracy even at minimal SAF rates, while VGG8 and ResNet18 maintain limited functionality only at the lowest SAF rates. Notably, SAF errors cause more severe accuracy degradation compared to other noise sources like D2D variation or conductance drift. This heightened sensitivity to SAF errors aligns with previous research findings [42], highlighting the critical importance of minimizing stuck faults during device fabrication. 4) MAC Output Noise: circuit-level noise sources during array computation can be captured as statistical variations in the post-ADC MAC outputs. To demonstrate NeuroSim V1.5 s flexible noise modeling capabilities, we evaluated inference accuracy across four different CIM macros, incorporating both SPICE-characterized circuit data and silicon measurements from fabricated chips. CIM A and B [43] employ 2b FeFET technology with different computing modes: CIM A performs current-mode computation with FeFETs directly, while CIM B implements charge-based computation using FeFET-modulated capacitors. Their noise characteristics were derived from Monte-Carlo SPICE simulations. CIM C [7] provides real silicon validation through direct measurements from a fabricated RRAM-based CIM chip. For these three designs, each MAC output level is characterized by unique mean and standard deviation values. CIM D [18], [33] uses 28 nm nvCap with charge-mode computation, where SPICE simulations revealed thermal noise as the dominant source, here, a uniform variation across output levels was used. Figs. 8(a-d) illustrate the statistical distribution of actual versus ideal MAC outputs for representative cases. While CIM A, B, and D show tighter error bounds compared to CIM C, all designs achieve similar accuracy on smaller networks. Fig. 8(e) compares inference accuracy across architectures of in- creasing complexity. VGG8 and ResNet18 maintain reasonable accuracy across all implementations, with CIM C showing slightly higher degradation ( 2 ) due to increased output variation. For ResNet50, only CIM B and D - which achieve lower standard deviations in their MAC errors - maintain high accuracy. The Swin Transformer exhibits particular sensitivity to output noise, with only the nvCap-based CIM D achieving software-comparable performance. These results validate NeuroSim V1.5 s noise modeling framework as a versatile platform for evaluating CIM designs 0.5 0.1 2.0 0.5 4.0 1.0 8.0 1.5 9.0 1.75 0 20 40 60 80 100 Classification Accuracy ( ) Rate of stuck at HRS ( ) LRS ( ) VGG8 CIFAR-10 1b cell VGG8 CIFAR-10 2b cell ResNet18 CIFAR-100 1b cell ResNet18 CIFAR-100 2b cell Software Accuracy Fig. 7. Inference accuracy of VGG8 and ResNet18 under different stuck-at- fault rates. 11 0 20 40 60 80 100 0 20 40 60 80 100 Mean Actual MAC Output Ideal MAC Output CIM A CIM B 89.7 75.6 80.0 80.1 CIM C CIM D 0 20 40 60 80 100 0 20 40 60 80 100 Mean Actual MAC Output Ideal MAC Output (a) (b) (c) (d) VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet 0 20 40 60 80 100 Device Computing Mode Activated Rows ADC Precision Average Noise Std Max. Noise Std CIM A[43] 2b FeFET Current 32 7b 0.75 1.1 CIM B[43] 2b FeFET Charge 32 7b 0.24 0.57 CIM C[41] 1b RRAM Current 8 3b 0.28 0.54 CIM D[18, 33] 1b nvCap Charge 32 5b 0.08 0.1 Accuracy ( ) Software Accuracy (e) 2b FeFET Current-based CIM 0 2 4 6 8 0 2 4 6 8 Mean Actual MAC Output Ideal MAC Output 2b FeFET Charge-based CIM 2b FeFET Current-based CIM 1b RRAM Current-based CIM 0 8 16 24 32 0 8 16 24 32 Mean Actual MAC Output Ideal MAC Output 2b FeFET Charge-based CIM 2b FeFET Current-based CIM 1b nvCap Charge-based CIM (b) (d) Fig. 8. (a) Actual MAC output with 2b FeFET current domain CIM [43] under 50mV Vth variation; (b) Actual MAC output with 2b FeFET charge domain CIM [43] under 50mV Vth variation; (c) Actual MAC output of RRAM CIM tape-out [41]; (d) Actual MAC output with 1b nvCap charge domain CIM [18], [33] under 50mV Vth variation; (e) Inference accuracy of various neural network algorithms across CIM macro A-D. using either simulation data or silicon measurements. By providing a standardized methodology for capturing MAC output statistics, the framework enables direct comparison of different technologies and compute circuits under realistic operating conditions. C. Runtime Evaluation and Simulator Comparison We first compare NeuroSim V1.5 against existing open- source ACIM simulation frameworks to highlight key capabili- ties and performance improvements (Table IV). CrossSim [37] provides detailed device-level accuracy simulation but faces significant runtime overhead when modeling noise effects. AIHWKit [63] specializes in noise-aware training but offers limited support for general noise modeling and architectural exploration. While NeuroSim V1.4 [54] includes PPA analysis and has some noise modeling, it was constrained by slower inference speeds and limited network support. The runtime is primarily influenced by the precision param- eters that determine computation complexity through Equation TABLE IV COMPARISON OF DNN ACIM SIMULATORS DNN CIM Simulator CrossSim [37] AIHWKit [63] NeuroSim V1.4 [54] NeuroSim V1.5 Supported devices PCM, RRAM, EcRAM, DRAM RRAM, PCM, Flash SRAM, RRAM, FeFET SRAM, RRAM, FeFET, nvCap Supported network types Linear, Conv Linear, Conv, Recurrent, Transformer Linear, Conv, Recurrent Linear, Conv, Recurrent, Transformer Custom Compute Circuit Support No No No Yes Bit Slicing Support Yes No Yes Yes PPA Support No No Yes Yes PPA evaluation excluded. 3: DAC precision affects the number of input cycles, cell bit- precision (MLC) determines the number of weight compo- nents, and noise modeling adds statistical sampling overhead. With 8b 8b None configuration (DAC MLC Noise) shown in Table V, V1.5 achieves a baseline runtime of 0.95 ms image for VGG8 and CIFAR-10. When using 1b cells to represent 8b weights, runtime increases 4 to 4.1 ms image due to additional bit-slicing operations. Adding device noise results in negligible runtime increase and adding per-MAC output noise increases runtime to only 5.1 ms image. NeuroSim V1.5 achieves this dramatic improvement in simulation efficiency through three key advances: a PyTorch- optimized ACIM behavioral simulator, noise modeling with pre-characterized statistical noise modeling and TensorRT in- tegration for efficient quantization. As shown in Fig. 9, these optimizations deliver a 6.51 speedup over V1.4 for CIFAR- 10 inference with 1-bit cells. VGG8 CIFAR-10 VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet 0.00 0.05 0.10 0.15 0.20 0.25 0.30 1b cell 2b cell 4b cell Runtime (Seconds Image) V1.4 V1.5 6.51x Fig. 9. Inference runtime of different neural network algorithms using V1.4 and V1.5 across 1b 2b 4b RRAM cell states without device non-idealities and noise. Tables V and VI further demonstrate V1.5 s efficient noise modeling capabilities on larger networks towards CIFAR-100 and ImageNet datasets. Compared to CrossSim, which shows a significant slowdown when incorporating device noise, V1.5 s statistical approach adds minimal overhead - just 1.3 for uniform noise and 3.1 for output-dependent noise simula- 12 tion. This efficiency stems from our pre-characterized noise distribution approach, eliminating the need for detailed circuit simulation during inference discussed in section III-B. TABLE V INFERENCE RUNTIME (SECONDS IMAGE) COMPARISON OF VGG8 ON CIFAR-10 (8B ACTIVATION, 8B WEIGHT) BETWEEN NEUROSIM V1.4 AND NEUROSIM V1.5 DAC Precision MLC Noise NeuroSim V1.4 [54] NeuroSim V1.5 8b 8b None 0.00095 8b 1b None 0.0041 1b 8b None 0.042 0.0048 1b 1b None 0.17 0.027 8b 8b Device Noise 0.00095 8b 1b Device Noise 0.0041 1b 8b Device Noise 0.042 0.0048 1b 1b Device Noise 0.17 0.027 8b 8b Output noise 0.0011 8b 1b Output noise 0.0051 1b 8b Output noise 0.0059 1b 1b Output noise 0.039,0.085 128 128 array size 0.039: same noise on each MAC output, 0.085: individual noise on each MAC output These advances establish NeuroSim V1.5 as a comprehen- sive development platform that bridges detailed device circuit- level analysis with system-level architectural exploration. The framework s efficiency and adaptability make it particularly valuable as the field moves toward more complex network architectures requiring extensive accuracy and performance optimization. TABLE VI INFERENCE RUNTIME (SECONDS IMAGE) COMPARISON OF RESNET50 ON IMAGENET (8B ACTIVATION, 8B WEIGHT) BETWEEN CROSSSIM, AIHWKIT, AND NEUROSIM V1.5 DAC Precision MLC Noise CrossSim [37] AIHWKit [63] NeuroSim V1.5 8b 8b None 0.3 0.0025 0.0085 8b 1b None 1.1 0.033 1b 8b None 1 0.04 1b 1b None 5.8 0.24 8b 8b Device Noise 9.8 0.003 0.0085 8b 1b Device Noise 200 0.033 1b 8b Device Noise 45 0.041 1b 1b Device Noise 1220 0.24 8b 8b Output noise 0.0025 0.0093 8b 1b Output noise 0.039 1b 8b Output noise 0.047 1b 1b Output noise 0.29,0.61 128 128 array size 0.29: same noise on each MAC output, 0.61: individual noise on each MAC output V. CONCLUSION The growing complexity of AI models places increasing demands on CIM accelerator design, requiring tools that can efficiently evaluate both accuracy and hardware implemen- tation trade-offs. This work presents NeuroSim V1.5, which combines accuracy analysis with power, performance, and area estimation to support systematic design space exploration of CIM architectures. The framework s integration with TensorRT enables evalu- ation of pre-trained networks without modification, while our flexible noise modeling methodology supports both detailed device-level analysis and efficient MAC-level noise injection, validated against published SPICE simulations and silicon measurements. These capabilities, combined with up to 6.5 faster runtime compared to NeuroSim V1.4, enable more extensive design space exploration than previous versions. Through our case studies with a variety of device tech- nologies and neural network topologies, we demonstrate how NeuroSim V1.5 can provide insights into CIM design trade- offs: 1) Optimal designs must balance array size, ADC preci- sion, and multi-level cell capability. 2) Network complexity influences sensitivity to device- level variations, particularly in larger networks like ResNet-50 and Swin-T. 3) Charge-domain computation (e.g., based on nvCap) is a promising device for future large-scale ACIM accelera- tion. Looking forward, the emergence of large language models (LLMs) with billions to trillions of parameters presents new challenges beyond on-chip memory capacity. These models will require GB-level high bandwidth memory (HBM) and process-in-memory (PIM) architectures that place compute logic near DRAM dies. While direct LLM and PIM support is beyond NeuroSim V1.5 s current scope, we have initiated efforts toward these capabilities through our work on 3D- stacked DRAM on TPU-like architectures [64], [65]. REFERENCES [1] C. E. Tripp, et al., Measuring the energy consumption and efficiency of deep neural networks: An empirical analysis and design recommenda- tions, arXiv:2403.08151, Mar. 2024. [2] S. Williams, et al., Roofline: An insightful visual performance model for multicore architectures, Commun. ACM, vol. 52, no. 4, pp. 65 76, Apr. 1 2009. [3] Y.-D. Chih, et al., 16.4 An 89 TOPS W and 16.3TOPS mm2 all- digital SRAM-based full-precision compute-in memory macro in 22nm for machine-learning edge applications, 2021 IEEE Interna-tional Solid- State Circuits Conference (ISSCC), vol. 64, pp. 252 254, Feb. 2021. [4] S. Yu, Neuro-inspired computing with emerging non-volatile memories, Proc. IEEE, vol. 106, pp. 260 285, Feb. 2018. [5] N. Verma, et al., In-memory computing: Advances and prospects, IEEE Solid-State Circuits Mag., vol. 11, no. 3, pp. 43 55, 2019. [6] P. Jain, et al., 13.2 A 3.6Mb 10.1Mb mm2 Embedded non-volatile ReRAM macro in 22nm FinFET technology with adaptive form- ing set reset schemes yielding down to 0.5V with sensing time of 5ns at 0.7V, in 2019 IEEE International Solid-State Circuits Conference - (ISSCC), Feb. 2019, pp. 212 214. [7] W. Li, et al., A 40nm MLC-RRAM compute-in-memory macro with sparsity control, on-chip write-verify, and temperature-independent ADC references, IEEE J. Solid-State Circuits, vol. 57, no. 9, pp. 2868 2877, 2022. 13 [8] C.-X. Xue, et al., 24.1 a 1MB multibit ReRAM computing-in-memory macro with 14.6ns parallel MAC computing time for CNN based AI edge processors, in 2019 IEEE International Solid-State Circuits Conference - (ISSCC), 2019, pp. 388 390. [9] Q. Liu, et al., 33.2 a fully integrated analog reram based 78.4tops w compute-in-memory chip with fully parallel mac computing, in 2020 IEEE International Solid-State Circuits Conference - (ISSCC), 2020, pp. 500 502. [10] M. Le Gallo, et al., A 64-core mixed-signal in-memory compute chip based on phase-change memory for deep neural network inference, Nat. Electron., vol. 6, pp. 680 693, Sep. 2023. [11] V. Joshi, M. Le Gallo, S. Haefeli, I. Boybat, S. R. Nandakumar, C. Piveteau, et al., Accurate deep neural network inference using compu- tational phase-change memory, Nat. Commun., vol. 11, no. 1, p. 2473, May 18 2020. [12] C. Zhou et al., AnalogNets: ML-HW co-design of noise-robust TinyML models and always-on analog compute-in-memory accelerator, arXiv:2111.06503, Nov. 2021. [13] H. Mulaosmanovic, et al., Novel ferroelectric FET based synapse for neuromorphic systems, in 2017 Symposium on VLSI Technology, Jun. 2017. [14] M. Jerry, et al., Ferroelectric FET analog synapse for acceleration of deep neural network training, in 2017 IEEE International Electron Devices Meeting (IEDM), Dec. 2017. [15] S. De, et al., Demonstration of multiply-accumulate operation with 28 nm FeFET crossbar array, IEEE Electron Device Lett., vol. 43, no. 12, pp. 2081 2084, Dec. 2022. [16] S. Yu, et al., Nonvolatile capacitive synapse: Device candidates for charge domain compute-in-memory, IEEE Electron Devices Mag., vol. 1, no. 2, pp. 23 32, 2023. [17] Y.-C. Luo, et al., Experimental demonstration of non-volatile capacitive crossbar array for in-memory computing, in 2021 IEEE International Electron Devices Meeting (IEDM), Dec. 2021, pp. 1 4. [18] T.-H. Kim, et al., Tunable non-volatile gate-to-source drain capacitance of fefet for capacitive synapse, IEEE Electron Device Letters, vol. 44, no. 10, pp. 1628 1631, 2023. [19] M. E. Sinangil, et al., A 7-nm compute-in-memory SRAM macro supporting multi-bit input, weight and output and achieving 351 TOPS W and 372.4 GOPS, IEEE J. Solid-State Circuits, vol. 56, no. 1, pp. 188 198, Jan. 2021. [20] X. Si, et al., 24.5 a twin-8T SRAM computation-in-memory macro for multiple-bit cnn-based machine learning, 2019 IEEE International Solid-State Circuits Conference - (ISSCC), pp. 396 398, 2019. [21] X. Si, et al., 15.5 a 28nm 64kb 6T SRAM computing-in-memory macro with 8b mac operation for ai edge chips, in 2020 IEEE International Solid-State Circuits Conference - (ISSCC), 2020, pp. 246 248. [22] J.-W. Su, et al., 15.2 a 28nm 64kb inference-training two-way trans- pose multibit 6T SRAM compute-in-memory macro for ai edge chips, in 2020 IEEE International Solid-State Circuits Conference - (ISSCC), 2020, pp. 240 242. [23] X. Si, et al., A local computing cell and 6T SRAM-based computing- in-memory macro with 8-b mac operation for edge ai chips, IEEE J. Solid-State Circuits, vol. 56, no. 9, pp. 2817 2831, 2021. [24] J.-W. Su, et al., Two-way transpose multibit 6T SRAM computing-in- memory macro for inference-training ai edge chips, IEEE J. Solid-State Circuits, vol. 57, no. 2, pp. 609 624, 2022. [25] H. Valavi, et al., A 64-tile 2.4-Mb in-memory-computing CNN accel- erator employing charge-domain compute, IEEE J. Solid-State Circuits, vol. 54, no. 6, pp. 1789 1799, Jun. 2019. [26] J. Lee, et al., Fully row column-parallel in-memory computing SRAM macro employing capacitor-based mixed-signal computation with 5-b Inputs, in 2021 Symposium on VLSI Circuits, Jun. 2021. [27] Y. Kong, et al., Evaluation platform of time-domain computing-in- memory circuits, IEEE Trans. Circuits Syst. II Express Briefs, vol. 70, no. 3, pp. 1174 1178, Mar. 2023. [28] W. Wan, R. Kubendran, C. Schaefer, S. B. Eryilmaz, W. Zhang, D. Wu, et al., A compute-in-memory chip based on resistive random-access memory, Nature, vol. 608, no. 7923, pp. 504 512, Aug. 2022. [29] C. Sung, S. Lim, H. Kim, T. Kim, K. Moon, J. Song, et al., Effect of conductance linearity and multi-level cell characteristics of TaOx-based synapse device on pattern recognition accuracy of neuromorphic system, Nanotechnology, vol. 29, no. 11, p. 115203, Mar. 16 2018. [30] W. Wu, et al., A methodology to improve linearity of analog rram for neuromorphic computing, in 2018 IEEE symposium on VLSI technol- ogy. IEEE, 2018, pp. 103 104. [31] X. Sun, et al., Impact of Non-ideal characteristics of resistive synaptic devices on implementing convolutional neural networks, IEEE J. Emerg. Sel. Top. Circuits Syst., vol. 9, no. 3, p. 570-579, Aug. 2019. [32] T. P. Xiao, et al., On the accuracy of analog neural network inference accelerators, IEEE Circuits Syst. Mag., vol. 22, no. 4, p. 26-48 Jan. 2022. [33] Y.-C. Luo, et al., A cross-layer framework for design space and variation analysis of non-volatile ferroelectric capacitor-based compute- in-memory accelerators, in 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), Jan. 2024, pp. 159 164. [34] M. Spear, et al., The impact of analog-to-digital converter architecture and variability on analog neural network accuracy, IEEE J. Explor. Solid- State Comput. Devices Circuits, Jan. 2023. [35] S. Huang, et al., Hardware-aware quantization mapping strategies for compute-in-memory accelerators, ACM Trans. Des. Autom. Electron. Syst., vol. 28, no. 3, pp. 1-23, Mar. 2023. [36] T. Andrulis, et al., RAELLA: reforming the arithmetic for efficient, low-resolution, and low-loss analog PIM: no retraining required! In Proceedings of the 50th Annual International Symposium on Computer Architecture, Jun. 2023. [37] T. Xiao, et al., Crosssim: gpu-accelerated simulation of analog neural networks., Sandia National Lab.(SNL-NM), Albuquerque, NM (United States), Tech. Rep., 2021. [Online]. Available: [38] M. J. Rasch, et al., A flexible and fast PyTorch toolkit for simulating training and inference on analog crossbar arrays, in 2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS), Jun. 2021. [39] T. Andrulis, et al., CiMLoop: a flexible, accurate, and fast compute-in- memory modeling tool, arXiv:2405.07259, May 2024. [40] X. Peng, et al., DNN NeuroSim: an end-to-end benchmarking frame- work for compute-in-memory accelerators with versatile device technolo- gies, in 2019 IEEE International Electron Devices Meeting (IEDM), Dec. 2019. [41] J. Read et al., Enabling long-term robustness in RRAM-based compute- in-memory edge devices, in 2023 IEEE International Symposium on Circuits and Systems (ISCAS), May 2023, pp. 1 5. [42] Z. He, et al., Noise injection adaption: End-to-end reram crossbar non- ideal effect adaption for neural network mapping, in Proceedings of the 56th Annual Design Automation Conference 2019, 2019, pp. 1 6. [43] Y. Xu, et al., Multi-level cell sensing inspired robust and energy- efficient charge domain compute-in-memory array with ferroelectric fet, in 2024 IEEE International Electron Devices Meeting (IEDM), Dec. 2024. [44] J. Hur, et al., Nonvolatile capacitive crossbar array for in-memory computing, Adv. Intell. Syst., vol. 4, no. 8, 2022. [45] C.-K. Kim et al., Capacitive synaptor with gate surrounding semi- conductor pillar structure and overturned charge injection for compute- in-memory, Advanced Intelligent Systems, pp. 400-371, Aug. 2024. [46] J.-W. Su, et al., A 8-b-precision 6T SRAM computing-in-memory macro using segmented-bitline charge-sharing scheme for ai edge chips, IEEE J. Solid-State Circuits, vol. 58, no. 3, pp. 877 892, 2023. [47] H. Jia, et al., 15.1 A programmable neural-network inference accelera- tor based on scalable in-memory computing, in 2021 IEEE International Solid-State Circuits Conference (ISSCC), vol. 64, Feb. 2021, pp. 236 238. [48] K. A. Aabrar, et al., A thousand state superlattice(SL) FEFET analog weight cell, in 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits), Jun. 2022, pp. 242 243. [49] H. Jiang, et al., ENNA: an efficient neural network accelerator design based on ADC-free compute-in-memory subarrays, IEEE Trans. Circuits Syst. I Regul. Pap., vol. 70, no. 1, pp. 353 - 363, Jan. 2023. [50] A. Guo, et al., 34.3 A 22nm 64kb lightning-like hybrid computing- in-memory macro with a compressed adder tree and analog-storage quantizers for transformer and CNNs, in 2024 IEEE International Solid- State Circuits Conference (ISSCC), vol. 67, Feb. 2024, pp. 570 572. [51] S. Kim, et al., I-bert: Integer-only bert quantization, in International conference on machine learning, 2021, pp. 5506 5518. [52] Z. Li, et al., I-ViT: Integer-only quantization for efficient vision trans- former inference, IEEE International Conference on Computer Vision, 2022. [53] X. Peng, et al., Optimizing weight mapping and data flow for convolu- tional neural networks on RRAM based processing-in-memory architec- ture, in 2019 IEEE International Symposium on Circuits and Systems (ISCAS), 2019, pp. 1 5. [54] J. Lee, et al., NeuroSim V1.4: Extending technology support for digital compute-in-memory toward 1nm node, IEEE Trans. Circuits Syst. I Regul. Pap., vol. 71, no. 4, pp. 1733 1744, Apr. 2024. 14 [55] X. Peng, et al., DNN NeuroSim V2.0: An end-to-end benchmarking framework for compute-in-memory accelerators for on-chip training, IEEE Trans. Comput. Aided Des. Integr. Circ. Syst., vol. 40, no. 11, pp. 2306 - 2319, Nov. 2021. [56] X. Peng, et al., Heterogeneous 3-D integration of multitier compute- in-memory accelerators: An electrical-thermal co-design, IEEE Trans. Electron Dev., Sep, pp. 1 8, 2021. [57] M. Manley, et al., Co-optimization of power delivery network design for 3D heterogeneous integration of RRAM-based compute in-memory accelerators, IEEE J. Explor. Solid-State Comput. Devices Circuits, 2025. (JxCDC) [58] W. Li, et al., H3DAtten: Heterogeneous 3-D integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self- attention, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 31, no. 10, pp. 1592 1602, Oct. 2023, Conference Name: IEEE Transactions on Very Large Scale Integration (VLSI) Systems. [59] S. Wu, et al., Training and inference with integers in deep neural networks, arXiv:1802.04680, 2018. [60] Y. Luo, et al., H3D-Transformer: A heterogeneous 3D (H3D) comput- ing platform for transformer model acceleration on edge devices, ACM Trans. Des. Autom. Electron. Syst., vol. 29, no. 3, Apr. 2024, [61] Y.-C. Luo, et al., Design of non-volatile capacitive crossbar array for in-memory computing, in 2021 IEEE International Memory Workshop (IMW), May 2021, pp. 1 4. [62] S. Zhou, et al., Dorefa-net: training low bitwidth convolutional neural networks with low bitwidth gradients, arXiv preprint arXiv:, 2016. [63] M. L. Gallo, et al., Using the IBM analog in-memory hardware acceleration kit for neural network training and inference, APL Mach. Learn., vol. 1, no. 4, Dec. 2023. [64] J. Sharda, et al., Accelerator design using 3D stacked capacitorless DRAM for large language models, in 2024 IEEE 6th International Con- ference on AI Circuits and Systems (AICAS), Apr. 2024, pp. 487 491. [65] P.-K. Hsu, et al., Monolithic 3D stackable DRAM: An enabling technology for large language model acceleration, IEEE Nanotechnol. Mag., 2025.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2505.02314v1 [cs.AR] 5 May 2025 1 NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities James Read Graduate Student Member, IEEE, Ming-Yen Lee Graduate Student Member, IEEE, Wei-Hsing Huang , Yuan-Chun Luo Graduate Student Member, IEEE, Anni Lu Graduate Student Member, IEEE, Shimeng Yu Fellow, IEEE, Abstract The exponential growth of artificial intelligence (AI) applications has exposed the inefficiency of conventional von Neumann architectures, where frequent data transfers between compute units and memory create significant energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses this challenge by performing multiply-accumulate (MAC) op- erations directly in the memory arrays, substantially reducing data movement. However, designing robust ACIM accelerators requires accurate modeling of device- and circuit-level non- idealities. In this work, we present NeuroSim V1.5, introducing several key advances: (1) seamless integration with TensorRT s post-training quantization flow enabling support for more neural networks including transformers, (2) a flexible noise injection methodology built on pre-characterized statistical models, mak- ing it straightforward to incorporate data from SPICE simu- lations or silicon measurements, (3) expanded device support including emerging non-volatile capacitive memories, and (4) up to 6.5 faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The combination of these capabilities uniquely enables systematic design space exploration across both accuracy and hardware efficiency metrics. Through multiple case studies, we demonstrate optimization of critical design parameters while maintaining network accuracy. By bridging high-fidelity noise modeling with efficient simulation, NeuroSim V1.5 advances the design and validation of next-generation ACIM accelerators. All NeuroSim versions are available open-source at Index Terms Compute-in-memory, AI accelerator, hard- ware software co-design, open-source, benchmark I. INTRODUCTION T HE exponential growth in AI applications has exposed a critical challenge in energy-efficient computing. The fun- damental limitation lies in the mismatch between the memory- centric workloads such as in machine learning algorithms and the processing-centric architecture in contemporary hardware platforms.\n\n--- Segment 2 ---\nINTRODUCTION T HE exponential growth in AI applications has exposed a critical challenge in energy-efficient computing. The fun- damental limitation lies in the mismatch between the memory- centric workloads such as in machine learning algorithms and the processing-centric architecture in contemporary hardware platforms. This mismatch causes massive data movement between memory and processing units to consume more than five times the energy of computation [1]. The challenges stem from the data-intensive nature of deep neural networks (DNNs), where a relatively small number of operations is performed per fetched data; this causes system performance to be limited by memory bandwidth, as This work is supported in part by PRISM, one of the SRC DARPA JUMP 2.0 Centers. J. Read acknowledges support from the DoD s SCALE program. The authors are with the School of Electrical and Computer Engi- neering, Georgia Institute of Technology, Atlanta, GA, 30332 USA (email: noted by the roofline model [2]. Compute-in-memory (CIM) has emerged as a promising paradigm to tackle this bot- tleneck by co-locating data storage and multiply-accumulate (MAC) operations in the same physical arrays, thereby reduc- ing repeated weight fetches. In digital computing-in-memory (DCIM), memory arrays (typically made of modified SRAM bit cells) are augmented with digital multipliers and adder trees for partial-sum accumulation, delivering robust performance with the overhead of adder trees [3]. By contrast, ACIM leverages the intrinsic electrical behavior of memory cells to carry out multiplications directly in the current or charge domain. Specifically, weight values are encoded as conductance (or capacitance), and when an input voltage is applied to a row, the resultant column current (or charge) naturally represents the multiplication of input and weight. Summing all such column outputs yields the MAC result in the analog domain and then being converted back to the digital domain through analog-to-digital converters (ADCs) [4], [5].\n\n--- Segment 3 ---\nSpecifically, weight values are encoded as conductance (or capacitance), and when an input voltage is applied to a row, the resultant column current (or charge) naturally represents the multiplication of input and weight. Summing all such column outputs yields the MAC result in the analog domain and then being converted back to the digital domain through analog-to-digital converters (ADCs) [4], [5]. A variety of emerging non-volatile memory (NVM) devices have been utilized for these ACIM arrays, including resistive random-access memory (RRAM) [6] [9], phase-change memory (PCM) [10] [12], ferroelectric field- effect transistors (FeFET) [13] [15], and more recently non- volatile capacitors (nvCap) [16] [18]. ACIM implementations can also rely on modified SRAM bit cells, tapping into current- based summation [19] [24], or incorporate on-chip capacitors to accumulate charge [25], [26] or using them for time-domain computing [27]. Several of these memory cells are depicted in a generic CIM macro with peripheral circuits as shown in Fig. 1. Over the past few years, both DCIM and ACIM have been demonstrated in silicon with memory capacities up to a few megabytes [28] sufficient for many on-chip inference scenarios such as computer vision tasks involving millions of parameters. DCIM generally offers the robustness of digital processing, making it attractive for high-performance comput- ing, whereas ACIM excels in energy efficiency, an essential requirement for power-constrained edge devices. ACIM accelerators can theoretically surpass 100 TOPS W [3], [9], [19], [26] far exceeding the 1 10 TOPS W range of contemporary von Neumann platforms largely because they eliminate most weight-reloading costs by performing the MAC operations directly within the memory arrays. In this paper, one operation is defined as an 8b 8b MAC. The key challenge 2 Fig. 1. Overview of NeuroSim V1.5: This update revamped the software backbone to support arbitrary neural network models and provide a more intuitive noise simulation workflow tailored to device and circuit experts. NeuroSim V1.5 enables users to evaluate inference accuracy across diverse architectures, including custom convolutional neural networks and transformers, while simulating various device types and noise sources.\n\n--- Segment 4 ---\nOverview of NeuroSim V1.5: This update revamped the software backbone to support arbitrary neural network models and provide a more intuitive noise simulation workflow tailored to device and circuit experts. NeuroSim V1.5 enables users to evaluate inference accuracy across diverse architectures, including custom convolutional neural networks and transformers, while simulating various device types and noise sources. The platform uniquely supports power, performance, and area (PPA) estimations, facilitating comprehensive system-level design space exploration. Key new features include optimized noise modeling, compatibility with advanced non-volatile memory technologies, and improved runtime performance. in ACIM is to manage inherently noisy analog computations to maintain the inference accuracy. These noise sources span multiple abstraction layers. At the device level, fabrication imperfections lead to device-to-device (D2D) variations and stuck-at-faults. Memory states can also drift over time, degrading compute accuracy [29] [32]. At the circuit level, thermal noise and process variations in peripheral circuits degrade sensing margins, resulting in errors in the ADC processes [18], [33]. At the system level, partial sums may be quantized at a low bit precision, compounding errors as layer outputs are passed from one stage to another [34] [36]. Together, these effects can reduce the inference accuracy of DNNs run on ACIM chips. Consequently, modeling and ultimately mitigating these non-idealities is critical for the design of robust ACIM architectures. These multi-layered noise effects create complex interac- tions between device characteristics, circuit design choices, and neural network requirements. Therefore optimizing ACIM accelerators requires systematic exploration across this design space to balance inference accuracy with hardware efficiency. Such an exploration requires simulation frameworks that can evaluate both accuracy and hardware metrics while maintain- ing practical runtimes even for modern large-scale networks. Direct SPICE simulation of analog computing arrays is prohibitively expensive for large neural networks, making it impractical to evaluate broad parameter spaces with modern datasets. While tools like CrossSim [37] and AIHWKit [38] offer detailed device modeling and noise-aware training re- spectively, they lack integrated hardware analysis capabilities and require extensive engineering effort to support new devices or circuits. Other frameworks like CIMLoop [39] focus on hardware metrics without considering accuracy under device and circuit non-idealities.\n\n--- Segment 5 ---\nWhile tools like CrossSim [37] and AIHWKit [38] offer detailed device modeling and noise-aware training re- spectively, they lack integrated hardware analysis capabilities and require extensive engineering effort to support new devices or circuits. Other frameworks like CIMLoop [39] focus on hardware metrics without considering accuracy under device and circuit non-idealities. These limitations make it difficult to systematically explore and optimize ACIM designs across both software accuracy and hardware efficiency metrics. NeuroSim [40] addresses these challenges through an in- tegrated framework that combines behavioral simulation of CIM operation with automated hardware analysis. Building on this foundation, we present NeuroSim V1.5, which introduces several key advances to enable comprehensive design space exploration: 1) Seamless integration with TensorRT to support arbitrary pre-trained models, while automating quantization. 2) A flexible noise modeling methodology built on pre- characterized statistical models, enabling rapid evalua- tion of diverse device and circuit designs. 3) Support for emerging technologies like charge-based non-volatile capacitive memories (nvCap) and emerging algorithms like vision transformer (ViT). 4) Significantly improved runtime performance - up to 6.5 faster than previous versions of NeuroSim. These capabilities maintain seamless compatibility with NeuroSim s existing power, performance, and area (PPA) anal- ysis framework, creating a unique environment for systematic CIM accelerator design optimization. The framework achieves runtime only two to three times that of software-only standard GPU inference. The remainder of this paper is organized as follows: Section II provides technical background on ACIM systems and ex- isting simulation approaches. Section III details our improved framework architecture and generic noise modeling method- ology. Section IV presents results from various case studies including inference accuracy analysis of several devices and compute circuits using data from SPICE simulations and tape- out measurements [7], [18], [33], [41] [43]. We also provide design space exploration for a RRAM based ACIM chip to demonstrate the framework s adaptability and improved running speed. Finally, Section VI discusses implications for future larger-scale AI model benchmarking and concludes the paper. 3 II. BACKGROUND INFORMATION A. CIM Array Architecture and Operation ACIM performs matrix computations by utilizing physical properties of memory arrays.\n\n--- Segment 6 ---\n3 II. BACKGROUND INFORMATION A. CIM Array Architecture and Operation ACIM performs matrix computations by utilizing physical properties of memory arrays. The fundamental building block is the crossbar array, where memory elements are arranged at the intersections of wordlines (WLs) and bitlines (BLs). This crossbar arrangement is uniquely suited for parallel MAC operations because each row s input signal (voltage) is simultaneously applied to all memory devices along that row. The resulting outputs (currents or charges) are summed along each column, naturally implementing the dot product. Consequently, large-scale MAC operations can be performed in one analog step , vastly increasing the throughput com- pared to digital architectures that must shuttle data back and forth between separate memory and logic units. To date, two primary methods of analog computation have emerged. Resistive arrays constructed with RRAM, PCM, FeFET that encode neural network weights as conductance states (G), performing MAC operations in the current domain: IBLi X j GijVj (1) where IBLi represents the analog matrix-vector multiplica- tion (MVM) output current on the ith bitline, and j indexes across the wordlines. Capacitive arrays utilize non-volatile capacitors (nvCap), where the weight is represented by the programmable capaci- tance of the ferroelectric or charge-trap memory cell [44] [45], thus performing MAC operations in the charge domain: QBLi X j CijVj (2) Alternative charge-based computation approaches utilize fixed capacitors in conjunction with SRAM cells, where the stored digital weight modulates charge accumulation through charge sharing mechanisms [46], [47]. DCIM has emerged as another promising approach [3], where SRAM arrays are augmented with digital multipliers and adder trees to perform integer MAC operations directly within memory. These systems are attractive due to their compatibility with advanced CMOS technology scaling and robust digital operation. However, the digital processing over- head - particularly from the complex adder trees required for partial sum accumulation - impacts array density. Additionally, SRAM leakage power becomes more significant in advanced technology nodes. Both the array scalability and leakage power create challenges for running complete neural networks on- chip. Each approach is finding its niche, while ACIM remains near the theoretical limit of efficiency for neural network ap- plications.\n\n--- Segment 7 ---\nBoth the array scalability and leakage power create challenges for running complete neural networks on- chip. Each approach is finding its niche, while ACIM remains near the theoretical limit of efficiency for neural network ap- plications. In ACIM designs, the choice of memory technology influences key array characteristics like precision, reliability, and energy efficiency - topics we will explore in detail when discussing noise modeling methodology. B. Device and Circuit Level Considerations The precision and reliability of analog computing operations depend heavily on the properties of individual memory cells. Key device parameters include: On Off Ratio: The ratio between maximum and minimum conductance capacitance states defines the usable dynamic range for weight storage. Finite on off ratios introduce non- zero current charge contribution from cells programmed to the off states, which accumulates across array columns and can de- grade computation accuracy. This effect becomes particularly significant in larger arrays where hundreds of devices in the off-state contribute to the output. To address this, array designs typically include additional dummy columns programmed to off states and are supported in NeuroSim [40], whose outputs are subtracted from the computational columns to cancel the accumulated off-state current charge. While this technique ef- fectively eliminates the impact of finite on off ratio, it slightly reduces the available dynamic range for computation since part of the range must be reserved for cancellation. This results in smaller sensing margins and potentially smaller signal-to-noise ratio (SNR). Therefore, high on off ratios are still a desired characteristic for ACIM memory devices. Number of States: Supporting high-precision weights re- quires either multi-level memory cells (MLC) or bit slicing with multiple lower-precision binary cells. For MLC devices, the number of reliably programmable states determines the weight precision. RRAM, PCM, and nvCap typically achieve 1-4 bits per cell [8], [11], [28], while FeFET have demon- strated 5 bits [14], [48]. At the circuit level, array size is limited mainly by sensing margins, where reliable output detection requires sufficient separation between computational states. This becomes chal- lenging with larger arrays due to increased noise accumulation and reduced signal levels from parasitic effects. Critical peripheral circuits include: WL BL drivers to supply the input and output signals. ADCs for digitizing array outputs. Shift-and-add circuits for managing bit-precise computa- tions.\n\n--- Segment 8 ---\nADCs for digitizing array outputs. Shift-and-add circuits for managing bit-precise computa- tions. Accumulation units on the PE level for partial sum aggregation. Multiplexers for data routing and array access. The digital circuit blocks in the periphery can be verified through conventional VLSI design methodologies. Therefore, behavioral modeling efforts focus primarily on the analog computational elements. C. Precision and Data Representation Neural network parameters must be carefully mapped to CIM arrays while managing precision and computational accu- racy. This mapping presents two key challenges: representing multi-bit input activations and storing high-precision weights. For input activations, two main approaches exist: 1) Using digital-to-analog converters (DACs) to represent each input value as an analog voltage. While straight- forward, this requires complex multi-bit DACs for every row of the array, introducing significant area and energy 4 Fig. 2. Procedure for ACIM behavioral simulation in NeuroSim V1.5. (1) Inputs and weights are quantized from floating point to integer. (2) Inputs and weights unfolded and (3) mapped to compute-in-memory arrays. (4) Analog MACs are calculated with analytical circuit models. (5) Noise is injected either to memory devices or to digital MAC outputs. (6) ADC outputs are shifted and added. (7) Outputs from memory arrays are accumulated and concatenated. (8) Accumulated outputs are folded into output tensors. (9) Outputs are de-quantized from integer back to floating-point. overhead. The I-V or C-V nonlinearity of the underlying memory devices may also introduces the errors in the input representation. 2) Processing each bit of the digital input serially, with outputs shifted and added according to bit significance. This eliminates DAC overhead but requires multiple computational cycles. For weight storage, two methods are available: 1) Bit slicing across multiple lower-precision cells. For example, an 8-bit weight can be distributed across cells of various precisions - from a single 8-bit cell to eight 1-bit cells, with intermediate combinations possible. When slicing the weight among multiple cells, each cell is placed in an adjacent column, with the outputs of each column scaled by its significance post-ADC and added with the other scaled outputs.\n\n--- Segment 9 ---\nFor example, an 8-bit weight can be distributed across cells of various precisions - from a single 8-bit cell to eight 1-bit cells, with intermediate combinations possible. When slicing the weight among multiple cells, each cell is placed in an adjacent column, with the outputs of each column scaled by its significance post-ADC and added with the other scaled outputs. This operation is referred to as shift-add and is described in Fig. 2 and in section III. Using analog shift-add before the ADCs has also been explored [44], [49], [50], but is not explicitly supported in NeuroSim V1.5. While NeuroSim s behavioral simulator supports both input representation methods, the PPA estimator only supports the bit-serial approach due to its practical hardware implementa- tion. In CIM, the fundamental MAC operation is performed in the integer domain, necessitating quantization of the inputs and weights of each layer in the network from floating-point to integer. The analog multiplication and accumulation processes generate integer results, which are digitized through ADCs. Finally, the integer outputs are de-quantized back to floating- point for further processing. Some accelerators explore quan- tizing the entire network to integers [51], [52], requiring spe- cialized approximations for operations like normalization and activation functions. This approach is particularly applicable to transformers, which require more complex operators like GELU and layer normalization. D. Hierarchical System Architecture The organization of ACIM accelerators stems from fun- damental constraints in both device technology and neural network structure. Memory array sizes are typically limited to dimensions between 32 32 and 256 256 cells due to sensing margins and manufacturing considerations. However, neural network layers often require much larger matrix operations. This dimensional mismatch necessitates a hierarchical decom- position of network operations. The organization of the chip hierarchy in NeuroSim is as follows: At the base level, synaptic arrays perform MAC op- erations through crossbar structures. These arrays are grouped into processing elements (PEs), which implement weight- stationary, spatial mapping of convolutional operations. For example, when mapping 3D convolutional kernels to 2D array structures, multiple arrays within a PE process different kernel depths in parallel, with their outputs appropriately accumu- lated. PEs are further aggregated into computational tiles, each capable of processing a portion of a neural network layer.\n\n--- Segment 10 ---\nFor example, when mapping 3D convolutional kernels to 2D array structures, multiple arrays within a PE process different kernel depths in parallel, with their outputs appropriately accumu- lated. PEs are further aggregated into computational tiles, each capable of processing a portion of a neural network layer. The tile architecture incorporates buffers for activation storage and accumulation circuits for combining partial results from multiple PEs. This organization enables efficient processing of large layers by distributing computations across multiple tiles while maintaining data locality. At the chip level, multiple tiles operate together through either an H-tree or X-Y bus interconnect structure. This highest level of hierarchy includes global buffers for inter-tile data movement and accumulation circuits for combining results from distributed computations. The entire system implements layer-wise pipelining, where 5 different tiles process consecutive layers of the network si- multaneously to maximize throughput. To balance processing speed between layers of different sizes, weight duplication can be employed within tiles [53]. For instance, if a layer s weight matrix is smaller than the tile s capacity, the weights can be duplicated to process multiple input vectors simultaneously, improving throughput. This hierarchical structure enables flexible mapping of di- verse neural network architectures while managing the phys- ical constraints of ACIM arrays. To model this hierarchy, NeuroSim employs analytical models for circuit blocks derived from standard cell libraries in advanced logic nodes. The previously released V1.4 [54] expanded technology support to the leading-edge 1 nm node, introducing stacked nanosheet transistors and studying the scaling path toward a future A5 node with complementary-FET (CFET) technology. Thanks to NeuroSim s software hardware co-design philosophy and open-source development model, the tool enables broader research on emerging device technologies, array architectures, on-chip training [55], and advanced packaging [56] [58]. For a detailed analysis of the hierarchy in NeuroSim, readers are referred to [40]. III. SIMULATION METHODOLOGY A. Overview of Design Methodology Designing ACIM accelerators requires optimization across multiple abstraction layers - from device characteristics to system architectures. NeuroSim V1.5 provides an integrated framework to evaluate these designs through two main com- ponents shown in Fig. 1 working jointly: 1) A behavioral simulator that maps and quantizes neu- ral networks for ACIM implementation and evaluates inference accuracy while accounting for hardware non- idealities.\n\n--- Segment 11 ---\nNeuroSim V1.5 provides an integrated framework to evaluate these designs through two main com- ponents shown in Fig. 1 working jointly: 1) A behavioral simulator that maps and quantizes neu- ral networks for ACIM implementation and evaluates inference accuracy while accounting for hardware non- idealities. 2) A hardware analyzer that estimates system-level energy, latency, and area through detailed circuit models of array peripherals and interconnects. In this update, our first contribution is a comprehensive re-design of the behavioral simulator. Second, we update the hardware analyzer to support the emerging nvCap-based CIM, complete with circuit models for the new charge- domain compute mode. In the behavioral simulator, we have fundamentally enhanced how we model compute-in-memory operations for accuracy estimation. A key approach is that we leverage pre-characterized statistical models derived from SPICE simulations or silicon measurements to ensure ac- curacy, while utilizing GPU-accelerated PyTorch operations to achieve the throughput needed for large-scale inference simulation. This separation allows us to maintain high fidelity while dramatically improving simulation efficiency. The behavioral simulator supports two primary simulation modes: Device expert mode: Model memory array physics directly, simulating conductance variations, stuck faults, and temporal drift to isolate the effect of memory technologies on network accuracy. Circuit expert mode: Model memory arrays with a sta- tistical approach using pre-characterized noise distributions from SPICE simulations or silicon measurements, enabling rapid evaluation of aggregated non-idealities from devices and circuits. Through integration with TensorRT s post-training quanti- zation flow, NeuroSim V1.5 automatically maps pre-trained neural networks to the corresponding ACIM architecture. The following sections detail the quantization methodol- ogy, array modeling approach, and specific techniques for incorporating device and circuit-level non-idealities into the simulation framework. B. Neural Network Quantization and Mapping 1) Quantization: NeuroSim V1.5 adopts NVIDIA s Ten- sorRT quantization infrastructure for converting neural net- works from floating-point to integer representations. This differs from previous NeuroSim versions which used in-house WAGE quantization [59] - an integer-training method that requires retraining the network.\n\n--- Segment 12 ---\nB. Neural Network Quantization and Mapping 1) Quantization: NeuroSim V1.5 adopts NVIDIA s Ten- sorRT quantization infrastructure for converting neural net- works from floating-point to integer representations. This differs from previous NeuroSim versions which used in-house WAGE quantization [59] - an integer-training method that requires retraining the network. TensorRT instead provides post-training quantization through automatic operator replace- ment, supporting quantization of pre-trained networks using operators like linear layers, convolutions, and pooling without modification. TensorRT supports configurable bit widths for both inputs and weights, which we use to evaluate different precision settings (e.g., 8b 8b, 6b 6b). The framework determines optimal scaling factors through either max or histogram calibration during a brief calibra- tion phase. This phase typically requires only 2 batches of calibration data (e.g., several hundred images) to analyze the distribution of values in both input tensors and weight matrices. Max calibration simply uses the maximum observed value across these tensors to set the scaling factor. Histogram calibration creates a histogram of the observed tensor values and sets the scale factor based on the cumulative distribution function (CDF) - specifically, choosing the value at which the CDF reaches a user-specified percentile (typically 99.99 ). This helps exclude outliers that would otherwise force a larger dynamic range. The key advantages of adopting TensorRT over WAGE quantization are direct support for arbitrary pre- trained models without retraining and the automated support for multiple operator types through operator replacement. 2) Network to Array Mapping: Physical constraints of ACIM arrays necessitate careful partitioning of neural network operations. For a linear layer with input dimension N and output dimension M, the quantized weight matrix must be mapped to multiple arrays based on both the memory array dimensions and the precision requirements. The number of memory cells required per weight depends on the bit precision of the cells (bcell). For example, to represent an 8-bit weight using 1-bit cells, we need Ncell 8 1 8 cells per weight. This bit-slicing requirement effectively multiplies the width dimension of our mapping by Ncell. As shown for a linear layer in Fig. 3, these weight components are arranged in adjacent columns, with each column corresponding to a different bit significance. Memory array size further constrains the mapping.\n\n--- Segment 13 ---\n3, these weight components are arranged in adjacent columns, with each column corresponding to a different bit significance. Memory array size further constrains the mapping. With a memory array height of R rows and C columns, the N M 6 weight matrix must be partitioned into N R rows of arrays. The total number of arrays needed to map the weights is equal to N R MNcell C . Mapping convolutional layers to memory arrays requires additional unfolding operations, of which multiple methods can be implemented. For a more detailed description, readers are referred to [40]. The input feature map is unfolded and mapped to the arrays according to the selected input scheme. In Fig. 2 we show the bit-serial operation, where the feature map is processed separately for each bit of the quantized input. Outputs from vertically-aligned arrays are accumulated, as these represent partial sums from the same output neurons. Outputs from horizontally aligned arrays are shifted and added according to their bit significance, then concatenated to form the complete layer output. 3) Support for Transformers: For transformer architec- tures, NeuroSim V1.5 adopts a hybrid approach that maps compatible operations to CIM arrays while implementing other operations in the digital domain. The framework maps linear layers (including query, key, value projections and MLP layers) to CIM arrays using the same methodology as standard neural networks. These linear layers comprise a significant portion of the total computation in transformer models. How- ever, dynamic operations such as attention score computation, layer normalization, and activation functions must still be computed. We elect to compute these dynamic operations in floating- point compute units due to CIM hardware constraints. Non- volatile memory-based CIM is optimal for weight-stationary computations that do not require frequent weight updates. This is due to limited write endurance and slow programming speed common in these devices. Therefore, operators like self- attention that require unique input and weight matrices each in- ference are not well-suited for non-volatile CIM. Digital CIM implementations using SRAM could potentially handle these dynamic operations, however they would still require careful quantization of the query, key, and value matrices to main- tain compatibility with integer-only computation [60].\n\n--- Segment 14 ---\nTherefore, operators like self- attention that require unique input and weight matrices each in- ference are not well-suited for non-volatile CIM. Digital CIM implementations using SRAM could potentially handle these dynamic operations, however they would still require careful quantization of the query, key, and value matrices to main- tain compatibility with integer-only computation [60]. Recent work on integer-only transformers [51], [52] has demonstrated promising approaches for full integer quantization, though further research is needed to evaluate the accuracy-efficiency tradeoffs of the hardware. The current hardware estimation portion of NeuroSim does not include circuit models for floating-point functional units needed for self-attention computation, layer normalization, or GELU activation functions. Future versions will explore integrating these components, potentially through integer ap- proximations that maintain accuracy while enabling efficient hardware implementation. C. ACIM Behavioral Modeling 1) Memory Array Operation: Building on TensorRT s operator replacement strategy, NeuroSim V1.5 substitutes stan- dard neural network layers with specialized CIM operators that model array-level computation. When a PyTorch model is loaded, linear and convolutional layers are automatically replaced with cim.Linear and cim.Conv modules that partition Fig. 3. NeuroSim V1.5 flowchart. Device expert mode: user provides device data (memory state variation, stuck-at-faults, state drift). Circuit expert mode: user provides statistical data on MAC outputs from SPICE Monte-Carlo simulations or tapeout measurements. operations according to array constraints and simulate analog computation as described in Fig. 3. The framework incorporates two separate simulation modes for modeling non-idealities: device expert mode for measured device-level device variation or calibrated aging models, and circuit expert mode using statistical models derived from SPICE or silicon measurements. These modes cannot be used simultaneously as circuit-level noise inherently captures device-level variations. 2) Device and Circuit Non-Idealities: In device expert mode, integer weights are first mapped to conductance or capacitance states based on the target memory technology. The framework then simulates analog multiplication through Ohm s law (I GV) for resistive arrays or charge-based computation (Q CV) for capacitive arrays.\n\n--- Segment 15 ---\n2) Device and Circuit Non-Idealities: In device expert mode, integer weights are first mapped to conductance or capacitance states based on the target memory technology. The framework then simulates analog multiplication through Ohm s law (I GV) for resistive arrays or charge-based computation (Q CV) for capacitive arrays. Input values are treated as idealized voltages, and outputs are computed by summing the resulting currents or charges along each column. To capture all possible input and weight bit-slicing combina- tions, including both bit-serial and DAC-based input schemes, we use the following generalized equation: y Ncell X i Nin X j (2i bcell)(2j PDAC)(Wi xj) (3) where: bin is the bit significance of the inputs. PDAC is DAC precision (PDAC 1 for bit-serial). Nin is the number of input cycles bin PDAC bw is the bit significance of the weights. bcell is the number of bits per weight cell (e.g. bw 4 for 4-bit MLC) Ncell is the number of cells per weight bw bcell Wi represents the i-th bit slice of the weight matrix xj represents the j-th bit slice of the input vector For example, when using 4-bit MLC (bcell 4) and 8-bit weights (bw 8), Ncell 2 so i ranges from 0 to 1 and the weight scales for each iteration are 2i bcell 20 4 1 and 7 2i bcell 21 4 16. When using bit-serial mode (PDAC 1) to represent 8-bit activations (bin 8), Nin 8 so j ranges from 0 to 7 with input scales increasing by powers of 2. Each sum in equation 3 corresponds to a for-loop in the behavioral simulation. Consequently, the nested loop becomes the critical piece of code to optimize. Using PyTorch s GPU- accelerated tensor operations, we can calculate the output y of every memory array on the CIM chip design in parallel, which greatly reduces the overall runtime of NeuroSim. Shown in Fig.\n\n--- Segment 16 ---\nUsing PyTorch s GPU- accelerated tensor operations, we can calculate the output y of every memory array on the CIM chip design in parallel, which greatly reduces the overall runtime of NeuroSim. Shown in Fig. 3, in device expert mode, three primary categories of device variations are modeled: Device-to-Device (D2D) Variation: For each conductance (or capacitance) state i, the programmed value follows a Gaussian distribution: G N(Gmeani, σi) (4) where Gmeani is the target conductance for state i and σi is the user-specified standard deviation. Separate varia- tion parameters are specified for each level i. Users pro- vide these parameters as comma-separated tuples, with one variation value per memory state. Users store these in the file mem states.csv that is then read by the framework automatically. Stuck-at-Faults (SAF): Memory cells can become per- manently fixed to their minimum or maximum states due to manufacturing defects or programming failures. This is modeled by randomly setting cells to minimum or maximum states according to user-specified probabilities before weight mapping. Unlike temporal variations, this permanent modifi- cation is applied once during initialization. Temporal Drift: Time-dependent conductance changes fol- low: G(t) G0(t t0)v (5) where G0 is initial conductance, t is retention time, v is the drift coefficient, and t0 is a reference time. The framework supports two drift modes: random drifting, or drifting towards a fixed state. The simulation treats arrays as frozen at the specified time point t for inference calculations. In circuit expert mode, statistical models are derived from Monte Carlo circuit analysis or silicon measurements. For SPICE-based characterization, users should: Create a complete array model including peripheral cir- cuits in a circuit simulator. Run Monte Carlo simulations incorporating device vari- ations (threshold voltage, device sizing), circuit-level ef- fects (thermal noise, parasitic), and ADC quantization. Generate statistical distributions by collecting multiple simulation results for each possible MAC output value. For silicon-based characterization: 1) Collect repeated measurements across multiple array columns. 2) Compare expected vs. actual outputs over thousands of trials. 3) Build output distributions capturing combined effects of all noise sources.\n\n--- Segment 17 ---\n2) Compare expected vs. actual outputs over thousands of trials. 3) Build output distributions capturing combined effects of all noise sources. The resulting models provide mean and standard deviation pairs for each possible MAC output value. The data can be displayed in a Confusion Matrix like those shown in Fig. 3. Users provide these parameters as comma-separated tuples, with one variation value per output level. Users store these in the file output noise.csv . During behavioral simula- tion, the critical loop (Eq. 3 can be skipped, as the circuit behavior is modeled by the statistical output distribution. Therefore, we can calculate ideal partial sums with negligible runtime overhead. After ideal partial sums are calculated, the framework samples from the MAC distributions to efficiently inject noise into the outputs. This statistical approach enables rapid evaluation and SPICE-level accuracy through empirically validated noise characteristics (see case studies in Section IV.D). TABLE I DESIGN SPACE EXPLORATION PARAMETERS Parameter Range Values Technology Node 22nm Device RRAM HRS LRS 40kΩ 3kΩ Network Dataset ResNet-18 CIFAR-100 Array Dimensions 32 32 - 256 256 Input Weight Precision 6b, 8b Input Processing bit-serial Memory Cell Precision 1b 4b ADC Precision 3b 11b The RRAM parameters are configured based on Intel s 22nm FinFET platform [6] D. Performance Analysis Framework To enable accurate power and performance estimation, NeuroSim V1.5 implements a trace-based interface between its behavioral simulation and hardware evaluation components. During inference simulation in PyTorch, the framework saves quantized input and weight values as decimal traces in CSV format (shown in Fig. 1. These traces capture the actual data patterns that would flow through the hardware, accounting for all quantization effects and bit slicing operations. The hardware estimator (written in C ) automatically generates a floorplan for the entire accelerator based on the network architecture and user-specified parameters. It creates detailed transistor-level circuit models for all components (memory arrays, ADCs, interconnects, peripherals, accumula- tors, etc. ), calibrated using SPICE simulations and data from previous tapeouts [7]. These models enable accurate hardware performance estimation at advanced technology nodes. Using traces from the software component, the estimator calculates detailed energy consumption for each array.\n\n--- Segment 18 ---\nThese models enable accurate hardware performance estimation at advanced technology nodes. Using traces from the software component, the estimator calculates detailed energy consumption for each array. The bit-serial input scheme is explicitly modeled - each input value is processed one bit at a time, with the framework tracking energy consumption for each operation. Weight values from the traces are converted to corresponding conductance or ca- pacitance states based on parameters specified in the hardware configuration file. In NeuroSim V1.5, we extended the hardware analyzer to support charge-based computation with non-volatile capacitive 8 memories. This required the addition of new circuit models for nvCap arrays, charge-to-voltage conversion using an amplifier [61], and control circuits for the unique charge-based compu- tation. The hardware estimator uses these models along with device parameters specified in the configuration file to accurately evaluate PPA metrics for nvCap-based accelerators while maintaining compatibility with the existing trace-based inter- face between behavioral simulation and hardware analysis. E. Evaluation Setup Our experimental evaluation consists of three main com- ponents: design space exploration of hardware configura- tions, inference accuracy analysis under various device non- idealities, and runtime performance comparison across open- source platforms. The DNN algorithms and proposed Neu- roSim framework are implemented using PyTorch, with all simulations performed on a single NVIDIA RTX A6000 GPU with 48GB GDDR6. 1) Design Space Exploration: For the design space explo- ration, we examine the trade-offs between hardware efficiency and inference accuracy across a range of architectural param- eters. Table I summarizes the key parameters and their ranges used in the design space exploration study. The memory device characteristics are based on Intel s 22nm RRAM platform [6]. We process inputs using the bit-serial approach (1b per cycle) to maintain consistency with the hardware estimator and activate all rows in parallel to maximize throughput. The ADC precision requirements are determined by the dynamic range of array outputs, which depends on three key factors: memory cell precision (bcell), DAC precision (PDAC), and array rows (R).\n\n--- Segment 19 ---\nWe process inputs using the bit-serial approach (1b per cycle) to maintain consistency with the hardware estimator and activate all rows in parallel to maximize throughput. The ADC precision requirements are determined by the dynamic range of array outputs, which depends on three key factors: memory cell precision (bcell), DAC precision (PDAC), and array rows (R). For a given array configuration, the maximum output value can be calculated as: outmax R(2PDAC 1)(2bc 1) (6) The minimum required ADC precision (PADC) to capture this full range without quantization loss is: PADC log2(outmax) (7) In our design space exploration, we use binary inputs (1b DAC) and we evaluate two ADC precision settings for each configuration: 1) Full precision matching the calculated dynamic range (PADC). 2) Reduced precision (PADC 1 and PADC 2) to study the impact of quantization noise. Excluding device and circuit noise, the inference accu- racy is solely affected by the input weight quantization from floating-point to fixed-point representation and the quanti- zation of array outputs depending on the ADC precision. Design parameters were swept using automated scripts to explore the trade-offs in accuracy and performance considering input weight ADC quantization and array size. When reducing ADC precision below the ideal precision, we adopt a clipping approach, where the sensing margins for each analog output state remain the same regardless of ADC precision, and outputs larger than the precision of the ADC are clipped to the maximum ADC output based on its precision. We find that this method results in comparable accuracy to other, non-linear quantization methods while being the most practical to implement in hardware. It is important to note that higher precision ADCs require smaller sensing margins between levels, which in practice could lead to increased susceptibility to circuit noise and higher error rates. The design-space analysis focuses solely on quantization effects, as detailed circuit-level noise char- acterization data is not available for the multi-bit memory configurations under study. For a complete understanding of system-level reliability, future work should incorporate both quantization effects and circuit-level noise measurements. 2) Device Noise Case Studies: For analyzing inference accuracy under device non-idealities, we standardize the hard- ware configuration to isolate the effects of various noise sources and variations.\n\n--- Segment 20 ---\nFor a complete understanding of system-level reliability, future work should incorporate both quantization effects and circuit-level noise measurements. 2) Device Noise Case Studies: For analyzing inference accuracy under device non-idealities, we standardize the hard- ware configuration to isolate the effects of various noise sources and variations. Table II presents the fixed parameters used across all device non-ideality studies. TABLE II DEVICE NON-IDEALITIES CASE STUDY PARAMETERS Parameter Value Technology Node 22nm Device RRAM HRS LRS 40kΩ 3kΩ Networks Datasets VGG8 CIFAR-10 ResNet-18 CIFAR-100 ResNet-50 ImageNet 1k Swin-T ImageNet 1k Array Dimensions 128 128 Activated rows 128 Input Weight Precision 8b 8b Input Processing bit-serial Memory Cell Precision 1b - 4b ADC Precision full precision Device Noise Sources D2D, SAF, state drift ADC precision is calculated using equations (6) and (7) In these case studies, we analyze different sources of device variation including device-to-device variation, stuck- at-faults, and state drift across multiple DNNs and datasets to demonstrate support for multiple networks and noise types. 3) Circuit Noise Case Studies: Next, we demonstrate the power of our improved noise modeling by using circuit noise data characterized both from SPICE characterization and tapeout measurements. With this improved modeling, direct comparisons between different devices, compute circuits, and DNNs can be performed. The table in Fig. 8 summarizes the key characteristics of each platform, with parameters drawn directly from their respective publications. Mean and standard deviation for each output state are supplied in a comma separated format (.csv file), where each row of the file corresponds to the mean for each given output (post-ADC) and the standard deviation of the output. 9 4) Runtime Evaluation: To characterize the improvements in runtime over previous versions of NeuroSim, we record the runtime of the inference accuracy portion under several key conditions. The main parameters that influence the runtime are the size of the network and dataset used the MLC level, precision of the DAC if one is used, and the input and weight precision. IV. RESULTS AND CASE STUDIES A. Design Space Exploration We demonstrate NeuroSim V1.5 s capability for compre- hensive design space exploration using a 22nm RRAM [6] platform as our baseline technology.\n\n--- Segment 21 ---\nRESULTS AND CASE STUDIES A. Design Space Exploration We demonstrate NeuroSim V1.5 s capability for compre- hensive design space exploration using a 22nm RRAM [6] platform as our baseline technology. Following the parameters outlined in Table I, we evaluate the fundamental tradeoffs between inference accuracy and hardware efficiency across architectural choices including array size, precision, and data representation. Our analysis focuses on architectural optimiza- tion without device circuit noise effects, as these effects are technology-specific and addressed separately in subsequent noise modeling studies. TABLE III DEFAULT 22NM RRAM PPA (RESNET18 CIFAR100) TOPS TOPS mm2 TOPS W FPS Accuracy 11.6 0.013 21.3 7770 75.57 Example performance using 22nm RRAM [6] and default settings in NeuroSim: 128 128 array size, 7-bit ADC, 8b 8b input weight precision. Fig. 4. Example design space exploration for 22nm RRAM. Each point represents a unique configuration. Bit-serial input was used for every design. Designs with highest TOPS W use array sizes of 32 32 and 64 64. We provide example performance metrics for the default configuration in NeuroSim using the selected RRAM device. As shown in Table III, we achieve 11.6 TOPS at 21.3 TOPS W while processing 7,770 frames per second for CIFAR-100 images. Through systematic exploration of the design space, 4 2 10 5 15 7 20 10 40 20 0 20 40 60 80 100 Classification Accuracy ( ) Device to device varaition (HRS LRS) VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet Software Accuracy Fig. 5. Inference accuracy of VGG8 on CIFAR-10, ResNet18 on CIFAR-100, and ResNet50 on ImageNet using V1.5 across different RRAM (single-level cell) variations. we can find designs that significantly outperform this baseline. Results are shown in Fig. 4. From this exploration, we identify several key trends that characterize optimal CIM architectures: 1) ADC Precision: The Pareto-optimal designs cluster in the 5 8 bit ADC precision range.\n\n--- Segment 22 ---\n4. From this exploration, we identify several key trends that characterize optimal CIM architectures: 1) ADC Precision: The Pareto-optimal designs cluster in the 5 8 bit ADC precision range. Our analysis shows that ADC precision can generally be reduced by 1-bit from the lossless precision without significant accuracy degradation, and by 2-bits in certain configurations. 2) Array Dimensions: Optimal designs utilize array sizes between 32 32 and 128 128 cells. Smaller ar- rays reduce ADC precision requirements but necessitate additional accumulation circuitry, while larger arrays necessitate larger ADCs, incurring costs in latency and energy. The designs that achieve 40 TOPS W used array sizes of either 32 32 or 64 64. 3) Memory Cell Precision: 2-bit and 3-bit MLC configura- tions dominate the Pareto frontier, offering an effective balance between storage density and programming reli- ability. These configurations achieve the highest energy efficiency (TOPS W) for any given accuracy target. The interplay between these parameters creates distinct architectural sweet spots. For example, the 8b 8b 64 6 2 con- figuration (input weight array size ADC MLC) achieves 75 accuracy while maintaining high energy efficiency. Designs with reduced precision (6b 6b) can push efficiency above 40 TOPS W at the cost of 5 accuracy degradation. The ability to rapidly evaluate these configurations through NeuroSim V1.5 enables the optimization of architectural parameters based on specific device and circuit designs. Next, we demonstrate the improved noise-modeling capa- bilities in the new behavioral simulator. B. Noise Modeling Case Studies 1) D2D Variation: We evaluated inference accuracy across multiple neural networks using NeuroSim V1.5 s comprehen- sive noise modeling capabilities, including D2D variation, memory state drifting, stuck-at-faults and circuit-level MAC output noise. To systematically assess accuracy degradation, 10 we selected network architectures of increasing complexity: VGG8 for CIFAR-10, ResNet18 for CIFAR-100, ResNet50 for ImageNet, and Swin Transformer Tiny for ImageNet. Following established methodology [62], we excluded the first convolutional layer from quantization in all simulations. Figure 5 reveals that accuracy degradation becomes more severe with increasing network complexity and dataset dif- ficulty.\n\n--- Segment 23 ---\nFollowing established methodology [62], we excluded the first convolutional layer from quantization in all simulations. Figure 5 reveals that accuracy degradation becomes more severe with increasing network complexity and dataset dif- ficulty. VGG8 on CIFAR-10 maintains reasonable accuracy until LRS variation reaches 20 . In contrast, ResNet18 on CIFAR-100 and ResNet50 on ImageNet show significant ac- curacy drops at just 10 and 5 LRS variation, respectively. Most notably, Swin Transformer Tiny demonstrates the lowest tolerance to device-level variations, highlighting the need for additional research into improving transformer model robust- ness for analog computing implementations. 101 103 105 107 40 50 60 70 80 90 Classification Accuracy ( ) Retention time (sec) Random drifting Drifting to Gmax Drifting to Gmin Software Accuracy Fig. 6. Inference accuracy of VGG8 on CIFAR-10 regarding random device conductance drifting, drifting to Gmax, and drifting to Gmin with 1b cells. 2) Conductance Drift: Fig. 6 examines how three different types of conductance drift (modeled by Equation 5) affect inference accuracy. First, random drift shows better accuracy retention compared to drifting toward minimum or maximum conductance. This improved resilience occurs because cells at minimum conductance cannot drift lower, while cells at maximum conductance cannot drift higher, effectively limiting the total conductance change possible under random drift. Second, drifting toward maximum conductance maintains bet- ter accuracy than drifting toward minimum conductance. This asymmetry arises from Equation 5, where the drift coefficient v determines the drift direction. For the same retention time t and drifting speed, drifting toward minimum conductance results in a larger conductance difference compared to drifting toward maximum, leading to a larger deviation of MAC outputs. 3) Stuck-at-Faults: NeuroSim V1.5 adds modeling ca- pabilities for stuck-at-fault (SAF) errors, extending beyond the D2D variation and drift modeling present in V1.4. Fig. 7 demonstrates how SAF errors impact inference accuracy across different networks, with SAF rates bounded by realistic RRAM fabrication constraints: 1.75 for LRS and 9.0 for HRS [42]. Our analysis reveals several key findings. First, both 1-bit and 2-bit cells show similar accuracy degradation at equivalent SAF rates, suggesting that cell precision does not significantly influence SAF sensitivity.\n\n--- Segment 24 ---\nOur analysis reveals several key findings. First, both 1-bit and 2-bit cells show similar accuracy degradation at equivalent SAF rates, suggesting that cell precision does not significantly influence SAF sensitivity. However, network complexity strongly affects SAF tolerance - ResNet-50 on ImageNet loses accuracy even at minimal SAF rates, while VGG8 and ResNet18 maintain limited functionality only at the lowest SAF rates. Notably, SAF errors cause more severe accuracy degradation compared to other noise sources like D2D variation or conductance drift. This heightened sensitivity to SAF errors aligns with previous research findings [42], highlighting the critical importance of minimizing stuck faults during device fabrication. 4) MAC Output Noise: circuit-level noise sources during array computation can be captured as statistical variations in the post-ADC MAC outputs. To demonstrate NeuroSim V1.5 s flexible noise modeling capabilities, we evaluated inference accuracy across four different CIM macros, incorporating both SPICE-characterized circuit data and silicon measurements from fabricated chips. CIM A and B [43] employ 2b FeFET technology with different computing modes: CIM A performs current-mode computation with FeFETs directly, while CIM B implements charge-based computation using FeFET-modulated capacitors. Their noise characteristics were derived from Monte-Carlo SPICE simulations. CIM C [7] provides real silicon validation through direct measurements from a fabricated RRAM-based CIM chip. For these three designs, each MAC output level is characterized by unique mean and standard deviation values. CIM D [18], [33] uses 28 nm nvCap with charge-mode computation, where SPICE simulations revealed thermal noise as the dominant source, here, a uniform variation across output levels was used. Figs. 8(a-d) illustrate the statistical distribution of actual versus ideal MAC outputs for representative cases. While CIM A, B, and D show tighter error bounds compared to CIM C, all designs achieve similar accuracy on smaller networks. Fig. 8(e) compares inference accuracy across architectures of in- creasing complexity. VGG8 and ResNet18 maintain reasonable accuracy across all implementations, with CIM C showing slightly higher degradation ( 2 ) due to increased output variation. For ResNet50, only CIM B and D - which achieve lower standard deviations in their MAC errors - maintain high accuracy.\n\n--- Segment 25 ---\nVGG8 and ResNet18 maintain reasonable accuracy across all implementations, with CIM C showing slightly higher degradation ( 2 ) due to increased output variation. For ResNet50, only CIM B and D - which achieve lower standard deviations in their MAC errors - maintain high accuracy. The Swin Transformer exhibits particular sensitivity to output noise, with only the nvCap-based CIM D achieving software-comparable performance. These results validate NeuroSim V1.5 s noise modeling framework as a versatile platform for evaluating CIM designs 0.5 0.1 2.0 0.5 4.0 1.0 8.0 1.5 9.0 1.75 0 20 40 60 80 100 Classification Accuracy ( ) Rate of stuck at HRS ( ) LRS ( ) VGG8 CIFAR-10 1b cell VGG8 CIFAR-10 2b cell ResNet18 CIFAR-100 1b cell ResNet18 CIFAR-100 2b cell Software Accuracy Fig. 7. Inference accuracy of VGG8 and ResNet18 under different stuck-at- fault rates. 11 0 20 40 60 80 100 0 20 40 60 80 100 Mean Actual MAC Output Ideal MAC Output CIM A CIM B 89.7 75.6 80.0 80.1 CIM C CIM D 0 20 40 60 80 100 0 20 40 60 80 100 Mean Actual MAC Output Ideal MAC Output (a) (b) (c) (d) VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet 0 20 40 60 80 100 Device Computing Mode Activated Rows ADC Precision Average Noise Std Max.\n\n--- Segment 26 ---\nInference accuracy of VGG8 and ResNet18 under different stuck-at- fault rates. 11 0 20 40 60 80 100 0 20 40 60 80 100 Mean Actual MAC Output Ideal MAC Output CIM A CIM B 89.7 75.6 80.0 80.1 CIM C CIM D 0 20 40 60 80 100 0 20 40 60 80 100 Mean Actual MAC Output Ideal MAC Output (a) (b) (c) (d) VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet 0 20 40 60 80 100 Device Computing Mode Activated Rows ADC Precision Average Noise Std Max. Noise Std CIM A[43] 2b FeFET Current 32 7b 0.75 1.1 CIM B[43] 2b FeFET Charge 32 7b 0.24 0.57 CIM C[41] 1b RRAM Current 8 3b 0.28 0.54 CIM D[18, 33] 1b nvCap Charge 32 5b 0.08 0.1 Accuracy ( ) Software Accuracy (e) 2b FeFET Current-based CIM 0 2 4 6 8 0 2 4 6 8 Mean Actual MAC Output Ideal MAC Output 2b FeFET Charge-based CIM 2b FeFET Current-based CIM 1b RRAM Current-based CIM 0 8 16 24 32 0 8 16 24 32 Mean Actual MAC Output Ideal MAC Output 2b FeFET Charge-based CIM 2b FeFET Current-based CIM 1b nvCap Charge-based CIM (b) (d) Fig. 8. (a) Actual MAC output with 2b FeFET current domain CIM [43] under 50mV Vth variation; (b) Actual MAC output with 2b FeFET charge domain CIM [43] under 50mV Vth variation; (c) Actual MAC output of RRAM CIM tape-out [41]; (d) Actual MAC output with 1b nvCap charge domain CIM [18], [33] under 50mV Vth variation; (e) Inference accuracy of various neural network algorithms across CIM macro A-D. using either simulation data or silicon measurements.\n\n--- Segment 27 ---\n8. (a) Actual MAC output with 2b FeFET current domain CIM [43] under 50mV Vth variation; (b) Actual MAC output with 2b FeFET charge domain CIM [43] under 50mV Vth variation; (c) Actual MAC output of RRAM CIM tape-out [41]; (d) Actual MAC output with 1b nvCap charge domain CIM [18], [33] under 50mV Vth variation; (e) Inference accuracy of various neural network algorithms across CIM macro A-D. using either simulation data or silicon measurements. By providing a standardized methodology for capturing MAC output statistics, the framework enables direct comparison of different technologies and compute circuits under realistic operating conditions. C. Runtime Evaluation and Simulator Comparison We first compare NeuroSim V1.5 against existing open- source ACIM simulation frameworks to highlight key capabili- ties and performance improvements (Table IV). CrossSim [37] provides detailed device-level accuracy simulation but faces significant runtime overhead when modeling noise effects. AIHWKit [63] specializes in noise-aware training but offers limited support for general noise modeling and architectural exploration. While NeuroSim V1.4 [54] includes PPA analysis and has some noise modeling, it was constrained by slower inference speeds and limited network support. The runtime is primarily influenced by the precision param- eters that determine computation complexity through Equation TABLE IV COMPARISON OF DNN ACIM SIMULATORS DNN CIM Simulator CrossSim [37] AIHWKit [63] NeuroSim V1.4 [54] NeuroSim V1.5 Supported devices PCM, RRAM, EcRAM, DRAM RRAM, PCM, Flash SRAM, RRAM, FeFET SRAM, RRAM, FeFET, nvCap Supported network types Linear, Conv Linear, Conv, Recurrent, Transformer Linear, Conv, Recurrent Linear, Conv, Recurrent, Transformer Custom Compute Circuit Support No No No Yes Bit Slicing Support Yes No Yes Yes PPA Support No No Yes Yes PPA evaluation excluded. 3: DAC precision affects the number of input cycles, cell bit- precision (MLC) determines the number of weight compo- nents, and noise modeling adds statistical sampling overhead.\n\n--- Segment 28 ---\nThe runtime is primarily influenced by the precision param- eters that determine computation complexity through Equation TABLE IV COMPARISON OF DNN ACIM SIMULATORS DNN CIM Simulator CrossSim [37] AIHWKit [63] NeuroSim V1.4 [54] NeuroSim V1.5 Supported devices PCM, RRAM, EcRAM, DRAM RRAM, PCM, Flash SRAM, RRAM, FeFET SRAM, RRAM, FeFET, nvCap Supported network types Linear, Conv Linear, Conv, Recurrent, Transformer Linear, Conv, Recurrent Linear, Conv, Recurrent, Transformer Custom Compute Circuit Support No No No Yes Bit Slicing Support Yes No Yes Yes PPA Support No No Yes Yes PPA evaluation excluded. 3: DAC precision affects the number of input cycles, cell bit- precision (MLC) determines the number of weight compo- nents, and noise modeling adds statistical sampling overhead. With 8b 8b None configuration (DAC MLC Noise) shown in Table V, V1.5 achieves a baseline runtime of 0.95 ms image for VGG8 and CIFAR-10. When using 1b cells to represent 8b weights, runtime increases 4 to 4.1 ms image due to additional bit-slicing operations. Adding device noise results in negligible runtime increase and adding per-MAC output noise increases runtime to only 5.1 ms image. NeuroSim V1.5 achieves this dramatic improvement in simulation efficiency through three key advances: a PyTorch- optimized ACIM behavioral simulator, noise modeling with pre-characterized statistical noise modeling and TensorRT in- tegration for efficient quantization. As shown in Fig. 9, these optimizations deliver a 6.51 speedup over V1.4 for CIFAR- 10 inference with 1-bit cells. VGG8 CIFAR-10 VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet 0.00 0.05 0.10 0.15 0.20 0.25 0.30 1b cell 2b cell 4b cell Runtime (Seconds Image) V1.4 V1.5 6.51x Fig. 9.\n\n--- Segment 29 ---\nVGG8 CIFAR-10 VGG8 CIFAR-10 ResNet18 CIFAR-100 ResNet50 ImageNet Swin_t ImageNet 0.00 0.05 0.10 0.15 0.20 0.25 0.30 1b cell 2b cell 4b cell Runtime (Seconds Image) V1.4 V1.5 6.51x Fig. 9. Inference runtime of different neural network algorithms using V1.4 and V1.5 across 1b 2b 4b RRAM cell states without device non-idealities and noise. Tables V and VI further demonstrate V1.5 s efficient noise modeling capabilities on larger networks towards CIFAR-100 and ImageNet datasets. Compared to CrossSim, which shows a significant slowdown when incorporating device noise, V1.5 s statistical approach adds minimal overhead - just 1.3 for uniform noise and 3.1 for output-dependent noise simula- 12 tion. This efficiency stems from our pre-characterized noise distribution approach, eliminating the need for detailed circuit simulation during inference discussed in section III-B.\n\n--- Segment 30 ---\nCompared to CrossSim, which shows a significant slowdown when incorporating device noise, V1.5 s statistical approach adds minimal overhead - just 1.3 for uniform noise and 3.1 for output-dependent noise simula- 12 tion. This efficiency stems from our pre-characterized noise distribution approach, eliminating the need for detailed circuit simulation during inference discussed in section III-B. TABLE V INFERENCE RUNTIME (SECONDS IMAGE) COMPARISON OF VGG8 ON CIFAR-10 (8B ACTIVATION, 8B WEIGHT) BETWEEN NEUROSIM V1.4 AND NEUROSIM V1.5 DAC Precision MLC Noise NeuroSim V1.4 [54] NeuroSim V1.5 8b 8b None 0.00095 8b 1b None 0.0041 1b 8b None 0.042 0.0048 1b 1b None 0.17 0.027 8b 8b Device Noise 0.00095 8b 1b Device Noise 0.0041 1b 8b Device Noise 0.042 0.0048 1b 1b Device Noise 0.17 0.027 8b 8b Output noise 0.0011 8b 1b Output noise 0.0051 1b 8b Output noise 0.0059 1b 1b Output noise 0.039,0.085 128 128 array size 0.039: same noise on each MAC output, 0.085: individual noise on each MAC output These advances establish NeuroSim V1.5 as a comprehen- sive development platform that bridges detailed device circuit- level analysis with system-level architectural exploration. The framework s efficiency and adaptability make it particularly valuable as the field moves toward more complex network architectures requiring extensive accuracy and performance optimization.\n\n--- Segment 31 ---\nTABLE V INFERENCE RUNTIME (SECONDS IMAGE) COMPARISON OF VGG8 ON CIFAR-10 (8B ACTIVATION, 8B WEIGHT) BETWEEN NEUROSIM V1.4 AND NEUROSIM V1.5 DAC Precision MLC Noise NeuroSim V1.4 [54] NeuroSim V1.5 8b 8b None 0.00095 8b 1b None 0.0041 1b 8b None 0.042 0.0048 1b 1b None 0.17 0.027 8b 8b Device Noise 0.00095 8b 1b Device Noise 0.0041 1b 8b Device Noise 0.042 0.0048 1b 1b Device Noise 0.17 0.027 8b 8b Output noise 0.0011 8b 1b Output noise 0.0051 1b 8b Output noise 0.0059 1b 1b Output noise 0.039,0.085 128 128 array size 0.039: same noise on each MAC output, 0.085: individual noise on each MAC output These advances establish NeuroSim V1.5 as a comprehen- sive development platform that bridges detailed device circuit- level analysis with system-level architectural exploration. The framework s efficiency and adaptability make it particularly valuable as the field moves toward more complex network architectures requiring extensive accuracy and performance optimization. TABLE VI INFERENCE RUNTIME (SECONDS IMAGE) COMPARISON OF RESNET50 ON IMAGENET (8B ACTIVATION, 8B WEIGHT) BETWEEN CROSSSIM, AIHWKIT, AND NEUROSIM V1.5 DAC Precision MLC Noise CrossSim [37] AIHWKit [63] NeuroSim V1.5 8b 8b None 0.3 0.0025 0.0085 8b 1b None 1.1 0.033 1b 8b None 1 0.04 1b 1b None 5.8 0.24 8b 8b Device Noise 9.8 0.003 0.0085 8b 1b Device Noise 200 0.033 1b 8b Device Noise 45 0.041 1b 1b Device Noise 1220 0.24 8b 8b Output noise 0.0025 0.0093 8b 1b Output noise 0.039 1b 8b Output noise 0.047 1b 1b Output noise 0.29,0.61 128 128 array size 0.29: same noise on each MAC output, 0.61: individual noise on each MAC output V. CONCLUSION The growing complexity of AI models places increasing demands on CIM accelerator design, requiring tools that can efficiently evaluate both accuracy and hardware implemen- tation trade-offs.\n\n--- Segment 32 ---\nThe framework s efficiency and adaptability make it particularly valuable as the field moves toward more complex network architectures requiring extensive accuracy and performance optimization. TABLE VI INFERENCE RUNTIME (SECONDS IMAGE) COMPARISON OF RESNET50 ON IMAGENET (8B ACTIVATION, 8B WEIGHT) BETWEEN CROSSSIM, AIHWKIT, AND NEUROSIM V1.5 DAC Precision MLC Noise CrossSim [37] AIHWKit [63] NeuroSim V1.5 8b 8b None 0.3 0.0025 0.0085 8b 1b None 1.1 0.033 1b 8b None 1 0.04 1b 1b None 5.8 0.24 8b 8b Device Noise 9.8 0.003 0.0085 8b 1b Device Noise 200 0.033 1b 8b Device Noise 45 0.041 1b 1b Device Noise 1220 0.24 8b 8b Output noise 0.0025 0.0093 8b 1b Output noise 0.039 1b 8b Output noise 0.047 1b 1b Output noise 0.29,0.61 128 128 array size 0.29: same noise on each MAC output, 0.61: individual noise on each MAC output V. CONCLUSION The growing complexity of AI models places increasing demands on CIM accelerator design, requiring tools that can efficiently evaluate both accuracy and hardware implemen- tation trade-offs. This work presents NeuroSim V1.5, which combines accuracy analysis with power, performance, and area estimation to support systematic design space exploration of CIM architectures. The framework s integration with TensorRT enables evalu- ation of pre-trained networks without modification, while our flexible noise modeling methodology supports both detailed device-level analysis and efficient MAC-level noise injection, validated against published SPICE simulations and silicon measurements. These capabilities, combined with up to 6.5 faster runtime compared to NeuroSim V1.4, enable more extensive design space exploration than previous versions. Through our case studies with a variety of device tech- nologies and neural network topologies, we demonstrate how NeuroSim V1.5 can provide insights into CIM design trade- offs: 1) Optimal designs must balance array size, ADC preci- sion, and multi-level cell capability.\n\n--- Segment 33 ---\nThese capabilities, combined with up to 6.5 faster runtime compared to NeuroSim V1.4, enable more extensive design space exploration than previous versions. Through our case studies with a variety of device tech- nologies and neural network topologies, we demonstrate how NeuroSim V1.5 can provide insights into CIM design trade- offs: 1) Optimal designs must balance array size, ADC preci- sion, and multi-level cell capability. 2) Network complexity influences sensitivity to device- level variations, particularly in larger networks like ResNet-50 and Swin-T. 3) Charge-domain computation (e.g., based on nvCap) is a promising device for future large-scale ACIM accelera- tion. Looking forward, the emergence of large language models (LLMs) with billions to trillions of parameters presents new challenges beyond on-chip memory capacity. These models will require GB-level high bandwidth memory (HBM) and process-in-memory (PIM) architectures that place compute logic near DRAM dies. While direct LLM and PIM support is beyond NeuroSim V1.5 s current scope, we have initiated efforts toward these capabilities through our work on 3D- stacked DRAM on TPU-like architectures [64], [65]. REFERENCES [1] C. E. Tripp, et al., Measuring the energy consumption and efficiency of deep neural networks: An empirical analysis and design recommenda- tions, arXiv:2403.08151, Mar. 2024. [2] S. Williams, et al., Roofline: An insightful visual performance model for multicore architectures, Commun. ACM, vol. 52, no. 4, pp. 65 76, Apr. 1 2009. [3] Y.-D. Chih, et al., 16.4 An 89 TOPS W and 16.3TOPS mm2 all- digital SRAM-based full-precision compute-in memory macro in 22nm for machine-learning edge applications, 2021 IEEE Interna-tional Solid- State Circuits Conference (ISSCC), vol. 64, pp. 252 254, Feb. 2021. [4] S. Yu, Neuro-inspired computing with emerging non-volatile memories, Proc. IEEE, vol. 106, pp. 260 285, Feb. 2018.\n\n--- Segment 34 ---\n106, pp. 260 285, Feb. 2018. [5] N. Verma, et al., In-memory computing: Advances and prospects, IEEE Solid-State Circuits Mag., vol. 11, no. 3, pp. 43 55, 2019. [6] P. Jain, et al., 13.2 A 3.6Mb 10.1Mb mm2 Embedded non-volatile ReRAM macro in 22nm FinFET technology with adaptive form- ing set reset schemes yielding down to 0.5V with sensing time of 5ns at 0.7V, in 2019 IEEE International Solid-State Circuits Conference - (ISSCC), Feb. 2019, pp. 212 214. [7] W. Li, et al., A 40nm MLC-RRAM compute-in-memory macro with sparsity control, on-chip write-verify, and temperature-independent ADC references, IEEE J. Solid-State Circuits, vol. 57, no. 9, pp. 2868 2877, 2022. 13 [8] C.-X. Xue, et al., 24.1 a 1MB multibit ReRAM computing-in-memory macro with 14.6ns parallel MAC computing time for CNN based AI edge processors, in 2019 IEEE International Solid-State Circuits Conference - (ISSCC), 2019, pp. 388 390. [9] Q. Liu, et al., 33.2 a fully integrated analog reram based 78.4tops w compute-in-memory chip with fully parallel mac computing, in 2020 IEEE International Solid-State Circuits Conference - (ISSCC), 2020, pp. 500 502. [10] M. Le Gallo, et al., A 64-core mixed-signal in-memory compute chip based on phase-change memory for deep neural network inference, Nat. Electron., vol. 6, pp. 680 693, Sep. 2023. [11] V. Joshi, M. Le Gallo, S. Haefeli, I. Boybat, S. R. Nandakumar, C. Piveteau, et al., Accurate deep neural network inference using compu- tational phase-change memory, Nat. Commun., vol. 11, no. 1, p. 2473, May 18 2020.\n\n--- Segment 35 ---\n11, no. 1, p. 2473, May 18 2020. [12] C. Zhou et al., AnalogNets: ML-HW co-design of noise-robust TinyML models and always-on analog compute-in-memory accelerator, arXiv:2111.06503, Nov. 2021. [13] H. Mulaosmanovic, et al., Novel ferroelectric FET based synapse for neuromorphic systems, in 2017 Symposium on VLSI Technology, Jun. 2017. [14] M. Jerry, et al., Ferroelectric FET analog synapse for acceleration of deep neural network training, in 2017 IEEE International Electron Devices Meeting (IEDM), Dec. 2017. [15] S. De, et al., Demonstration of multiply-accumulate operation with 28 nm FeFET crossbar array, IEEE Electron Device Lett., vol. 43, no. 12, pp. 2081 2084, Dec. 2022. [16] S. Yu, et al., Nonvolatile capacitive synapse: Device candidates for charge domain compute-in-memory, IEEE Electron Devices Mag., vol. 1, no. 2, pp. 23 32, 2023. [17] Y.-C. Luo, et al., Experimental demonstration of non-volatile capacitive crossbar array for in-memory computing, in 2021 IEEE International Electron Devices Meeting (IEDM), Dec. 2021, pp. 1 4. [18] T.-H. Kim, et al., Tunable non-volatile gate-to-source drain capacitance of fefet for capacitive synapse, IEEE Electron Device Letters, vol. 44, no. 10, pp. 1628 1631, 2023. [19] M. E. Sinangil, et al., A 7-nm compute-in-memory SRAM macro supporting multi-bit input, weight and output and achieving 351 TOPS W and 372.4 GOPS, IEEE J. Solid-State Circuits, vol. 56, no. 1, pp. 188 198, Jan. 2021. [20] X. Si, et al., 24.5 a twin-8T SRAM computation-in-memory macro for multiple-bit cnn-based machine learning, 2019 IEEE International Solid-State Circuits Conference - (ISSCC), pp. 396 398, 2019.\n\n--- Segment 36 ---\n[20] X. Si, et al., 24.5 a twin-8T SRAM computation-in-memory macro for multiple-bit cnn-based machine learning, 2019 IEEE International Solid-State Circuits Conference - (ISSCC), pp. 396 398, 2019. [21] X. Si, et al., 15.5 a 28nm 64kb 6T SRAM computing-in-memory macro with 8b mac operation for ai edge chips, in 2020 IEEE International Solid-State Circuits Conference - (ISSCC), 2020, pp. 246 248. [22] J.-W. Su, et al., 15.2 a 28nm 64kb inference-training two-way trans- pose multibit 6T SRAM compute-in-memory macro for ai edge chips, in 2020 IEEE International Solid-State Circuits Conference - (ISSCC), 2020, pp. 240 242. [23] X. Si, et al., A local computing cell and 6T SRAM-based computing- in-memory macro with 8-b mac operation for edge ai chips, IEEE J. Solid-State Circuits, vol. 56, no. 9, pp. 2817 2831, 2021. [24] J.-W. Su, et al., Two-way transpose multibit 6T SRAM computing-in- memory macro for inference-training ai edge chips, IEEE J. Solid-State Circuits, vol. 57, no. 2, pp. 609 624, 2022. [25] H. Valavi, et al., A 64-tile 2.4-Mb in-memory-computing CNN accel- erator employing charge-domain compute, IEEE J. Solid-State Circuits, vol. 54, no. 6, pp. 1789 1799, Jun. 2019. [26] J. Lee, et al., Fully row column-parallel in-memory computing SRAM macro employing capacitor-based mixed-signal computation with 5-b Inputs, in 2021 Symposium on VLSI Circuits, Jun. 2021. [27] Y. Kong, et al., Evaluation platform of time-domain computing-in- memory circuits, IEEE Trans. Circuits Syst. II Express Briefs, vol. 70, no. 3, pp. 1174 1178, Mar. 2023.\n\n--- Segment 37 ---\n1174 1178, Mar. 2023. [28] W. Wan, R. Kubendran, C. Schaefer, S. B. Eryilmaz, W. Zhang, D. Wu, et al., A compute-in-memory chip based on resistive random-access memory, Nature, vol. 608, no. 7923, pp. 504 512, Aug. 2022. [29] C. Sung, S. Lim, H. Kim, T. Kim, K. Moon, J. Song, et al., Effect of conductance linearity and multi-level cell characteristics of TaOx-based synapse device on pattern recognition accuracy of neuromorphic system, Nanotechnology, vol. 29, no. 11, p. 115203, Mar. 16 2018. [30] W. Wu, et al., A methodology to improve linearity of analog rram for neuromorphic computing, in 2018 IEEE symposium on VLSI technol- ogy. IEEE, 2018, pp. 103 104. [31] X. Sun, et al., Impact of Non-ideal characteristics of resistive synaptic devices on implementing convolutional neural networks, IEEE J. Emerg. Sel. Top. Circuits Syst., vol. 9, no. 3, p. 570-579, Aug. 2019. [32] T. P. Xiao, et al., On the accuracy of analog neural network inference accelerators, IEEE Circuits Syst. Mag., vol. 22, no. 4, p. 26-48 Jan. 2022. [33] Y.-C. Luo, et al., A cross-layer framework for design space and variation analysis of non-volatile ferroelectric capacitor-based compute- in-memory accelerators, in 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), Jan. 2024, pp. 159 164. [34] M. Spear, et al., The impact of analog-to-digital converter architecture and variability on analog neural network accuracy, IEEE J. Explor. Solid- State Comput. Devices Circuits, Jan. 2023. [35] S. Huang, et al., Hardware-aware quantization mapping strategies for compute-in-memory accelerators, ACM Trans. Des. Autom. Electron. Syst., vol. 28, no. 3, pp. 1-23, Mar.\n\n--- Segment 38 ---\n3, pp. 1-23, Mar. 2023. [36] T. Andrulis, et al., RAELLA: reforming the arithmetic for efficient, low-resolution, and low-loss analog PIM: no retraining required! In Proceedings of the 50th Annual International Symposium on Computer Architecture, Jun. 2023. [37] T. Xiao, et al., Crosssim: gpu-accelerated simulation of analog neural networks., Sandia National Lab. (SNL-NM), Albuquerque, NM (United States), Tech. Rep., 2021. [Online]. Available: [38] M. J. Rasch, et al., A flexible and fast PyTorch toolkit for simulating training and inference on analog crossbar arrays, in 2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS), Jun. 2021. [39] T. Andrulis, et al., CiMLoop: a flexible, accurate, and fast compute-in- memory modeling tool, arXiv:2405.07259, May 2024. [40] X. Peng, et al., DNN NeuroSim: an end-to-end benchmarking frame- work for compute-in-memory accelerators with versatile device technolo- gies, in 2019 IEEE International Electron Devices Meeting (IEDM), Dec. 2019. [41] J. Read et al., Enabling long-term robustness in RRAM-based compute- in-memory edge devices, in 2023 IEEE International Symposium on Circuits and Systems (ISCAS), May 2023, pp. 1 5. [42] Z. He, et al., Noise injection adaption: End-to-end reram crossbar non- ideal effect adaption for neural network mapping, in Proceedings of the 56th Annual Design Automation Conference 2019, 2019, pp. 1 6. [43] Y. Xu, et al., Multi-level cell sensing inspired robust and energy- efficient charge domain compute-in-memory array with ferroelectric fet, in 2024 IEEE International Electron Devices Meeting (IEDM), Dec. 2024. [44] J. Hur, et al., Nonvolatile capacitive crossbar array for in-memory computing, Adv. Intell. Syst., vol. 4, no. 8, 2022.\n\n--- Segment 39 ---\n4, no. 8, 2022. [45] C.-K. Kim et al., Capacitive synaptor with gate surrounding semi- conductor pillar structure and overturned charge injection for compute- in-memory, Advanced Intelligent Systems, pp. 400-371, Aug. 2024. [46] J.-W. Su, et al., A 8-b-precision 6T SRAM computing-in-memory macro using segmented-bitline charge-sharing scheme for ai edge chips, IEEE J. Solid-State Circuits, vol. 58, no. 3, pp. 877 892, 2023. [47] H. Jia, et al., 15.1 A programmable neural-network inference accelera- tor based on scalable in-memory computing, in 2021 IEEE International Solid-State Circuits Conference (ISSCC), vol. 64, Feb. 2021, pp. 236 238. [48] K. A. Aabrar, et al., A thousand state superlattice(SL) FEFET analog weight cell, in 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits), Jun. 2022, pp. 242 243. [49] H. Jiang, et al., ENNA: an efficient neural network accelerator design based on ADC-free compute-in-memory subarrays, IEEE Trans. Circuits Syst. I Regul. Pap., vol. 70, no. 1, pp. 353 - 363, Jan. 2023. [50] A. Guo, et al., 34.3 A 22nm 64kb lightning-like hybrid computing- in-memory macro with a compressed adder tree and analog-storage quantizers for transformer and CNNs, in 2024 IEEE International Solid- State Circuits Conference (ISSCC), vol. 67, Feb. 2024, pp. 570 572. [51] S. Kim, et al., I-bert: Integer-only bert quantization, in International conference on machine learning, 2021, pp. 5506 5518. [52] Z. Li, et al., I-ViT: Integer-only quantization for efficient vision trans- former inference, IEEE International Conference on Computer Vision, 2022.\n\n--- Segment 40 ---\n5506 5518. [52] Z. Li, et al., I-ViT: Integer-only quantization for efficient vision trans- former inference, IEEE International Conference on Computer Vision, 2022. [53] X. Peng, et al., Optimizing weight mapping and data flow for convolu- tional neural networks on RRAM based processing-in-memory architec- ture, in 2019 IEEE International Symposium on Circuits and Systems (ISCAS), 2019, pp. 1 5. [54] J. Lee, et al., NeuroSim V1.4: Extending technology support for digital compute-in-memory toward 1nm node, IEEE Trans. Circuits Syst. I Regul. Pap., vol. 71, no. 4, pp. 1733 1744, Apr. 2024. 14 [55] X. Peng, et al., DNN NeuroSim V2.0: An end-to-end benchmarking framework for compute-in-memory accelerators for on-chip training, IEEE Trans. Comput. Aided Des. Integr. Circ. Syst., vol. 40, no. 11, pp. 2306 - 2319, Nov. 2021. [56] X. Peng, et al., Heterogeneous 3-D integration of multitier compute- in-memory accelerators: An electrical-thermal co-design, IEEE Trans. Electron Dev., Sep, pp. 1 8, 2021. [57] M. Manley, et al., Co-optimization of power delivery network design for 3D heterogeneous integration of RRAM-based compute in-memory accelerators, IEEE J. Explor. Solid-State Comput. Devices Circuits, 2025. (JxCDC) [58] W. Li, et al., H3DAtten: Heterogeneous 3-D integrated hybrid analog and digital compute-in-memory accelerator for vision transformer self- attention, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 31, no. 10, pp. 1592 1602, Oct. 2023, Conference Name: IEEE Transactions on Very Large Scale Integration (VLSI) Systems. [59] S. Wu, et al., Training and inference with integers in deep neural networks, arXiv:1802.04680, 2018.\n\n--- Segment 41 ---\n1592 1602, Oct. 2023, Conference Name: IEEE Transactions on Very Large Scale Integration (VLSI) Systems. [59] S. Wu, et al., Training and inference with integers in deep neural networks, arXiv:1802.04680, 2018. [60] Y. Luo, et al., H3D-Transformer: A heterogeneous 3D (H3D) comput- ing platform for transformer model acceleration on edge devices, ACM Trans. Des. Autom. Electron. Syst., vol. 29, no. 3, Apr. 2024, [61] Y.-C. Luo, et al., Design of non-volatile capacitive crossbar array for in-memory computing, in 2021 IEEE International Memory Workshop (IMW), May 2021, pp. 1 4. [62] S. Zhou, et al., Dorefa-net: training low bitwidth convolutional neural networks with low bitwidth gradients, arXiv preprint arXiv:, 2016. [63] M. L. Gallo, et al., Using the IBM analog in-memory hardware acceleration kit for neural network training and inference, APL Mach. Learn., vol. 1, no. 4, Dec. 2023. [64] J. Sharda, et al., Accelerator design using 3D stacked capacitorless DRAM for large language models, in 2024 IEEE 6th International Con- ference on AI Circuits and Systems (AICAS), Apr. 2024, pp. 487 491. [65] P.-K. Hsu, et al., Monolithic 3D stackable DRAM: An enabling technology for large language model acceleration, IEEE Nanotechnol. Mag., 2025.\n\n