=== ORIGINAL PDF: 2507.06549v1_Deep-Learning-Based_Pre-Layout_Parasitic_Capacitan.pdf ===\n\nRaw text length: 35644 characters\nCleaned text length: 35223 characters\nNumber of segments: 27\n\n=== CLEANED TEXT ===\n\nDeep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs Shan Shen , Dingcheng Yang , Yuyang Xie , Chunyan Pei , Wenjian Yu , Bei Yu Department Computer Science Technology, BNRist, Tsinghua University, Beijing, China Department of Computer Science Engineering, The Chinese University of Hong Kong, Hong Kong SAR ABSTRACT To achieve higher system energy efficiency, SRAM in SoCs is of- ten customized. The parasitic effects cause notable discrepancies between pre-layout and post-layout circuit simulations, leading to difficulty in converging design parameters and excessive design it- erations. Is it possible to well predict the parasitics based on the pre-layout circuit, so as to perform parasitic-aware pre-layout simu- lation? In this work, we propose a deep-learning-based 2-stage model to accurately predict these parasitics in pre-layout stages. The model combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron (MLP) regressors, effectively managing class imbalance of the net parasitics in SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant internal net samples and integrate subcircuit information into the graph to abstract the hierarchical structure of schematics. Experiments on 4 real SRAM designs show that our approach not only surpasses the state-of-the-art model in parasitic prediction by a maximum of 19X reduction of error but also significantly boosts the simulation process by up to 598X speedup. 1 INTRODUCTION With the rapid expansion of intelligent Internet of Things (IoT) devices, contemporary System-on-Chips (SoCs) are increasingly fo- cused on enhancing energy efficiency. This is essential for extending the standby time of battery-powered devices. In order to optimize the energy efficiency of on-chip memory, both academic and industrial researchers have proposed various high-energy-efficiency memory structures. These include near-threshold SRAM [15][2], compute-in- memory SRAM [22][23], and others. SRAM customization involves adjustments in the topology and size of the memory cell, peripheral circuits, timing, and controller design. To ensure the stability of the chip s functionality and its final yield, it is crucial to simulate and evaluate various performance metrics, such as read write delay, power consumption, and failure probabilities during the design pro- cedure [11]. Once the SRAM performance falls short of expectations, This work was partially supported by the National Science and Technology Major Project (2021ZD0114703), and NSFC under grant No. 62204141 and 62090025. W. Yu is the corresponding author. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner author(s). GLSVLSI 24, June 12 14, 2024, Clearwater, FL, USA 2024 Copyright held by the owner author(s). ACM ISBN 979-8-4007-0605-9 24 06. significant time and labor are required for iterative design modifi- cations. This process adds complexity to customizing the SRAM IP and results in prolonged design cycles. In traditional design workflows, designers proceed with circuit design and optimization based on pre-layout simulations. They then perform verification using post-layout simulations after completing the layout drawing. However, with advanced technologies adopt- ing smaller transistor sizes and lower operating voltages, there s a notable decrease in transistor driving ability. Consequently, the parasitic effect becomes too significant to be overlooked. This dis- parity leads to a substantial gap between pre-layout and post-layout simulation results, making it challenging to ensure the final circuit performance. Regrettably, the back-end design of customized SRAM is an arduous and time-intensive process. This is due to two primary reasons: (1) the high density of transistors often leads to violations of design rules; (2) the routing process is complicated, as it involves managing a substantial number of data wires within a constrained area. Consequently, even minor alterations to the schematic can necessitate extensive modifications in the layout. Recently, numerous studies [3, 7, 9, 12, 13, 17, 21] have employed machine learning as a potent tool for predicting parasitic effects in electronic design. However, training accurate ML-based models is often hindered by the class imbalance of parasitic capacitance. As illustrated in Figure 1, the distribution of net capacitance shows a prominent imbalance, with values ranging from 0.01 fF to 100 pF. There are over 106 nets in the second bin of SP8192W SRAM, predominantly internal connections in memory cells. This imbalance presents two problems: (1) the training process becomes inefficient, as the majority of nets are easy negatives that offer little to no valuable learning signal; (2) these easy negatives can dominate the training process, potentially leading to the development of ineffective models. In this work, we aim to address this challenge and enhance the accuracy of a deep-learning-based model for parasitic prediction. The contributions of this work are outlined as follows. We propose a unique 2-stage model, consisting of a Graph Neu- ral Network (GNN) classifier and multiple Multi-Layer Percep- tron (MLP) regressors, and the corresponding training strategy. We implement Focal Loss [8] as the loss function of the classifier to further reduce the overwhelming effect of easy negatives. Subcircuit information is also integrated into the graph, effec- tively mirroring the hierarchical structure found in schematics. Our experimental results demonstrate that the 2-stage model achieves an accuracy improvement ranging from 2.5X to 19X over the state-of-the-art model [13], while also reducing both training and inference time. The proposed method stands out from existing parasitic predic- tion models that primarily focus on small-scale analog circuits, arXiv:2507.06549v1 [cs.LG] 9 Jul 2025 10 2 10 1 100 101 102 103 104 Capacitance (fF) 10 0 10 1 10 2 10 3 10 4 10 5 10 6 Count (a) 10 2 10 1 100 101 102 103 104 Capacitance (fF) 10 1 10 2 10 3 10 4 10 5 10 6 Count (b) 10 2 10 1 100 101 102 103 104 Capacitance (fF) 10 1 10 2 10 3 10 4 10 5 10 6 Count (c) Figure 1: Distributions of the parasitic net capacitance of (a) Ultra8T SRAM, (b) Sandwich-RAM [22], and (c) SP8192W SRAM [1]. as it targets large-scale memory circuits. By simulating the schematic netlist with the predicted parasitics, we achieve a significant speedup, up to 586X, compared to the simulations using the post-layout netlist. This approach substantially ben- efits the large-scale memory circuit design. The versatility of the proposed method also makes it naturally suitable for other downstream tasks. This includes design space exploration and expedited design updates. The paper is organized as follows. Section 2 reviews the related work in the field and Section 3 introduces some fundamental concepts of GNNs. Section 4 presents the proposed model for net capacitance prediction, and Section 5 describes the over workflow. Section 6 gives experimental results derived from real SRAM designs. Section 7 concludes the whole paper. 2 RELATED WORKS In recent years, a lot of attention has been paid to machine learning as an effective method for parasitic prediction. Shook et al. [17] trans- formed the front-end netlist (schematic) of analog IP designs into a star topology and used a random forest model to predict the equiva- lent resistance and capacitance of each net. Another work, named ParaGraph [13], converts circuit schematics into graphs and utilizes graph neural network (GNN) techniques to predict net capacitance and device layout parameters. It leverages a complex aggregation procedure to compute node embedding, which is a combination of graph convolutional network (GCN) [6], GraphSage [5], relation GCN (RGCN) [14], and graph attention network (GAT) [18]. It also includes the ensemble modeling technique to improve the prediction accuracy via training 3 different sub-models for different magnitudes of the capacitance value. Net samples with a ground truth larger than the maximum predicted value of the sub-model are ignored dur- ing training, which increases prediction accuracy within a specified range within each model. The sub-model with larger capacitance prediction is more preferred than that with small capacitance. Li et al [7] adopted a ParaGraph-like model to predict the parasitics and guide optimization of the voltage-controlled oscillator (VCO). Liu et al. [9] proposed an improved surrogate performance model us- ing parasitic graph embeddings generated by ParaGraph [13]. Then the surrogate model was integrated into a Bayesian optimization workflow to automate transistor sizing. Machine learning-based capacitance extraction is also studied for the scenario of monolithic 3D (M3D) IC design. Pentapati et al. [12] proposed a regression model based on augmented decision tree learn- ing to better predict 3D net parasitics. In another scenario, Yang et al. [21] proposed a convolutional neural network capacitance model (CNN-Cap) for the pattern-matching-based capacitance extraction. The method is able to accurately compute the capacitances of 2D patterns with a variable number of conductors. It should be pointed out that although [7, 9, 13, 17] are for pre- dicting net capacitances at the pre-layout stage, they are all focused on analog circuits. Compared to the analog circuit, large-scale high- density memory circuits suffer from a more severe imbalance of training data. Besides, authors in [12] and [3], etc. assume the lay- out is partially ready, which is not purely based on the pre-layout schematic. In this work, we focus on predicting the net capacitances at the pre-layout stage to expedite the customized SRAM design. 3 PRELIMINARIES A graph G (V, E) is a structure used to represent entities and their relations. It consists of two sets, the set of nodes V (also called vertices) and the set of edges E (also called arcs). Each node is associ- ated with a vector of features 𝑥𝑣 (𝑥1, ...,𝑥𝑑) with dimension 𝑑. The 𝑛 V node features form a matrix 𝑋 R𝑛 𝑑. An edge (𝑢, 𝑣) E, represented as 𝑒𝑢,𝑣, connecting a pair of nodes 𝑢and 𝑣indicates that there is a relation between them. The edge can also have a feature vector 𝑥𝑒 (𝑥1, ...,𝑥𝑐) with dimension 𝑐and 𝑚 E features form a matrix 𝑋𝑒 R𝑚 𝑐. The neighborhood of a node N(𝑣) is defined as N(𝑣) {𝑢 V (𝑢, 𝑣) E}. Graphs can be either homogeneous or heterogeneous. In a homogeneous graph, all the nodes represent instances of the same type and all the edges represent relations of the same type. In contrast, in a heterogeneous graph, the nodes and edges can be of different types. GNN is a kind of neural network that operates directly on data structured as graphs, without losing structural and feature informa- tion [4]. Having an input graph, a GNN aims to learn the embedding vectors per node, defined as ℎ𝑢, 𝑢 V, which encodes the neighbor- hood information of each node [20]. The message passing between nodes is assumed as the most generic GNN layer [10]. Given a graph structure, message passing updates the edge embeddings ℎ𝑒 (𝑢,𝑣) with ℎ𝑒 (𝑢,𝑣) 𝜙(ℎ𝑢,ℎ𝑣,𝑥𝑒 (𝑢,𝑣)), (1) where 𝜙( ) is an arbitrary, non-linear, differentiable function that aggregates its inputs, and 𝑥𝑒 (𝑢,𝑣) R𝑐is the initial edge feature vector. After the edge embedding is obtained, and defining 𝑥𝑢 R𝑑 as the feature vector for the starting node, the node representation 2 is updated by ℎ 𝑢 𝜙 (ℎ𝑢, 𝑣 𝑁(𝑢) ℎ𝑒 (𝑢,𝑣),𝑥𝑢). (2) The graph embeddings learned by GNNs can be used as inputs to other ML models for building an end-to-end framework. There are three levels of tasks for such a framework: node, edge, and graph [20]. In the node-level task, the regression or classification problem of the nodes is of concern. 4 PARASITIC CAPACITANCE PREDICTION A 2-stage deep-learning model is proposed in this section. It contains a GNN-based classifier and 5 MLP regressors. We first introduce the conversion of schematics, then the feature extraction, and last the model architecture and training strategy. 4.1 Conversion of Circuit Netlist to Graph In order to reflect the modularization in circuit design, the schematic netlist will be modeled as a heterogeneous graph G (V, E) in this work. A node in the graph corresponds to a net, a device, or a sub- circuit instance in the circuit. Figure 2 shows an example of a buffer containing three types of node sets V VNET VDEV VSUB. A green circle represents a net (𝑣 VNET), connecting to devices; an orange square represents an instance of the transistor device (𝑣 VDEV), which can also be other types of devices such as a capacitor, a resistor, and a diode; a blue triangle represents an instance of the subcircuit (𝑣 VSUB), comprised of multiple nets and device instances. We set all edges in the graph as undirected. The advantage of the undi- rected graph is that feature information can be transferred between different nodes in a shallow network. Compared to the graphs only comprising nodes representing nets and devices in [13], the node of the subcircuit instance in this work can reflect the hierarchical structure in schematics and generalize the local features. net 1 device 1 device inst. 2 net 2 net 3 device inst. 3 device inst. 4 .SUBCKT inv in out M1 out in VSS VSS nch W 0.1u L 0.03u M2 out in VDD VDD pch W 0.2u L 0.03u .ENDS .SUBCKT buffer net1 net3 Xinst1 net1 net2 inv Xinst2 net2 net3 inv .ENDS Sub. inst.1 Sub. inst.0 Sub. inst.2 Figure 2: Example of converting a circuit to a graph. 4.2 Acquisition of Training Data In the post-layout netlist (SPF files) generated by RC extraction, the net parasitics form a complex 𝜋-type RC network. As the net capacitance is of concern, we generate the lumped sum of capacitance (Ceff) for each net from the post-layout netlist and use it as the ground-truth label for training the 2-stage model. We extract three types of feature vectors from the schematic netlist. The definitions of different feature elements are listed in Table 1. Device nodes need to extract different features according to their device types. Compared to existing works, we collect more features for different nodes. For example, feature elements of a tran- sistor device include the multiplier (𝑀𝑚𝑜𝑠), channel length (𝐿), width (𝑊), etc., while other feature elements (such as 𝑀𝑟𝑒𝑠, 𝐿𝑟𝑒𝑠, and𝑊𝑟𝑒𝑠) Table 1: Feature definitions of nets, device instances, and sub- circuit instances Type Notation Definition Index Net 𝑁𝑚𝑜𝑠 of connected transistors 0 𝑁𝑔 of connected gate terminals 1 𝑁𝑠𝑑 of connected source drain terminals 2 𝑁𝑏 of connected base terminals 3 𝑊𝑡𝑜𝑡 Total width of connected transistor 4 𝐿𝑡𝑜𝑡 Total length of connected transistor 5 𝑁𝑐𝑎𝑝 of connected capacitors 6 𝐿𝑟𝑡𝑜𝑡 Total length of connected capacitors 7 𝑁𝑟𝑡𝑜𝑡 Total of connected capacitor fingers 8 𝑁𝑟𝑒𝑠 of connected resistors 9 𝑊𝑡𝑜𝑡_𝑟𝑒𝑠 Total width of connected resistors 10 𝐿𝑡𝑜𝑡_𝑟𝑒𝑠 Total length of connected resistors 11 𝑁𝑝𝑜𝑟𝑡 of connected ports 12 Device Instance 𝑀𝑚𝑜𝑠 Multiplier of transistors 0 𝐿 Length of the transistor 1 𝑊 Width of the transistor 2 𝑀𝑟𝑒𝑠 Multiplier of connected resistors 3 𝐿𝑟𝑒𝑠 Length of resistor 4 𝑊𝑟𝑒𝑠 Width of resistor 5 𝑀𝑐𝑎𝑝 Multiplier of connected capacitor 6 𝐿𝑟 Length of capacitor 7 𝑁𝑟 of capacitor fingers 8 𝑁𝑝 of ports in the device instance 9 𝑇 Type code of the device instance 10 Sub- circuit Instance 𝑁𝑝𝑜𝑟𝑡 of ports in the device instance 0 𝑁𝑑 of ports in the device instance 1 𝑁𝑛 of nets in the subcircuit instance 2 𝐿𝑣𝑙 Hierarchy level of the instance 3 are zeros. All features are normalized by the maximum value in the dataset to achieve better numerical stability. 4.3 Two-Stage Model Building The distribution of Ceff is extremely imbalanced (Figure 1). The large number of nets with small capacitance, called easy negatives, results in the minority samples being prone to be mispredicted. However, those nets with large Ceff are usually important ports or clock nets, which are likely to affect the timing analysis results. Therefore, we divide the training data (net nodes) into 5 categories and label them with 𝑡 T {0, 1, 2, 3, 4} according to the magnitude of their parasitic capacitance, i.e., {(0.01 fF, 0.1 fF], (0.1 fF, 1 fF], (1 fF, 10 fF], (10 fF,100 fF], (100 fF, )}. GNN is more suitable for classification tasks. In order to do the capacitance regression with GNN, we build a 2-stage model based on GNN and multilayer perceptron (MLP). The model is mainly di- vided into the net classification and the net capacitance regression, as shown in Figure 3. Feature vectors (different dimensions) of three types of nodes are projected to a common space through 3 different projectors individually so that we can easily transform a heteroge- neous graph into a homogeneous one. It is convenient for subsequent training of different GNN models. Next, the projected feature vectors are input into the GNN MLP model to classify the net nodes into different categories. Embeddings of net nodes generated by the GNN model will be fed into a 3-layer MLP to predict the label 𝑡of a net node. Note that only the net nodes have labels and their embeddings are retained at this time, while that of the other two types of nodes 3 will be masked. To reduce both the training and inference time, our proposed method leverages simple GNN models but uses a delicate training strategy and loss function. In the 2nd stage, capacitance regression is performed for each category individually. The input of the regression model is a con- catenated vector containing node embeddings and the original net feature vector. The target vector is the effective net capacitance Ceff. The training flow is also divided into 2 stages. We first use feature vectors and labels of net nodes to train and validate the classifier. Once the classifier is obtained, according to the classification result, net nodes in the training set are grouped into different sub-sets and are used to train their own regressors with the corresponding targets Ceff, respectively. The test set is invisible during the whole training process. In the 1st stage, we adopt Focal Loss [8] with a weight factor 𝛼 [0, 1] which applies a modulating term to the cross-entropy loss in order to focus learning on hard examples and down-weight the numerous easy negatives. L 𝑣 VNET 𝐹𝐿𝑣 𝑣 VNET 𝛼𝑡(1 𝑝𝑣(𝑡))𝛾log(𝑝𝑣(𝑡)), (3) where 𝛾 0 is a tunable focusing parameter, 𝑝𝑣(𝑡) [0, 1] is the model s estimated probability for the net node 𝑣 VNET to be in class 𝑡 T. Here we set 𝛼to be 𝛼𝑡 1 𝑓𝑡 , (4) where 𝑓𝑡is the proportion of class 𝑡in the entire data samples. It can also be set to other values according to the classification results to increase the contribution of categories with fewer data to the total loss function. In the 2nd stage, we use the mean value of the squared percentage error as the loss function in the regression training, i.e. L𝑡 1 𝑁𝑡 𝑣 VNET 𝑡 (𝑦𝑣 ˆ𝑦𝑣 𝑦𝑣 ) 2 , (5) where symbol 𝑦denotes the target value and ˆ𝑦the predicted value. 𝑁𝑡is the number of nets with predicted label 𝑡. 5 OVERALL WORKFLOW The workflow of the proposed method is illustrated in Figure 4. Training data are collected from the pre-layout schematic and the post-layout netlist by matching net names. During training, the input of the 2-stage model includes feature matrices from 3 types of nodes, GNN 3- Layer MLP Device inst. feat. Net embed. Projected net feat. Net feat. Projected device inst. feat. Subcircuit inst. feat. h[0] h[N] 2- Layer MLP h[0] h[N] 2- Layer MLP h[0] h[N] 2- Layer MLP 4-Layer MLP 4-Layer MLP 4-Layer MLP 4-Layer MLP 4-Layer MLP Ceff Ceff Ceff Ceff Ceff Projected subcircuit inst. feat. Concat. (Net feat., Net embed.) ① ② Class 0 (Ceff 0.1fF) Class 1 (0.1 Ceff 1.0fF) Class 2 (1 Ceff 10fF) Class 3 (10 Ceff 100fF) Class 4 (Ceff 100fF) Figure 3: Structure of the proposed 2-stage model including net classification stage and capacitance regression stage. Net Class. Cap. Regr. 2-Stage Model Downstream tasks (Sim.) Ceff Labels Graph Net matching Features Predicted Ceff Features Training Validation Inference Application Post-layout Netlist Pre-layout Netlist Pre-layout Netlist Figure 4: The overall workflow of the proposed method. a graph converted from the schematic, a class label vector, and a net capacitance vector. The output of the model is the predicted Ceff. The regressors of stage 2 can be trained in a parallel manner to further speed up the model training. During the inference, the design s graph and the features of nets are fed into the model. The predicted net capacitance from the model is back-annotated into a netlist for downstream tasks. 6 EXPERIMENTAL RESULTS Experiments are conducted to show the effectiveness of the proposed method with 4 SRAM designs. All ML models are implemented based on DGL library [19] written in PyTorch. The post-layout netlists with full parasitics are extracted by StarRC. All experiments are run on a server with 40 Intel Xeon Silver CPUs with 128GB memory. Table 2: SRAM designs used by this work. SRAM Designs SSRAM Ultra8T Sandwich SP8192W of Nets 19902 861842 1160940 2342588 of Device Inst. 57417 2325092 2665422 6984821 of Subcircuit Inst. 9965 315466 428498 39885 Total of Nodes 87284 3502400 4254860 9367294 of Edges 134926 13392268 13254854 32009072 6.1 Dataset We prepare the full schematic netlists and the post-layout SPF netlists to extract the feature vectors and net capacitance respectively. Ta- ble 2 summarizes the SRAM design examples utilized by this work (net parasitic capacitance distributions are illustrated in Figure 1). All designs are under 28nm CMOS technology. SSRAM [15] is a small design with high-energy efficiency with a timing-speculation tech- nique. The Sandwich-RAM [22] is made up of half of digital circuits for computing and half of memory arrays, forming a sandwich-like structure. Ultra8T SRAM [16] is a multi-voltage design with a wide range of operation voltages, which contains analog circuits. SP8192W SRAM is based on a single port 6T cell structure generated by the SRAM compiler [1] with tremendously high density. We leverage the full neighbor sampling method provided by DGL, which ran- domly splits the test set from the original large graph and returns a new subgraph. This ensures the invisibility of nodes in the test set during model training. The train validation test set split ratio for each design is 0.6 0.2 0.2. 6.2 Model Settings In the 1st stage, we adopt three mainstream GNN structures, includ- ing a 3-layer graph attention network (GAT) [18], a 3-layer graph 4 convolutional networks (GCN) [6], and 2-layer GraphSAGE [5], in net classification. GraphSAGE can be implemented with different types of aggregators in (1) and (2), and we use mean and pool in our comparison. We set all layer widths to 64. The feature projection MLP has 2 linear layers and the classification MLP has 3 linear layers. The activation function is ReLU and the dropout is 0.1 for all layers. The batch normalization (BN) is turned on. The GAT-based classifier needs to turn on the layer normalization and turn off the BN to get reasonable accuracy. We use a cosine annealing schedule to adjust the learning rate based on the number of executed epochs. The learn- ing rate ranges from 1E-3 to 1E-4. The weight decay parameter is set to 5E-4. The performance metrics include accuracy, and the F1 macro score (the best value is 1.0 for all metrics). F1 macro is the unweighted mean of the F1 score of each class. The training process is run 10 times for each model and the performance metrics are averaged from the best epochs. In the 2nd stage, each regressor is a 4-layer MLP with hidden layer widths {128, 128, 64}. The input width is the sum of the node embedding width 64 and the original net feature width 13 while the output width is 1. The activation function is ReLU and the dropout is 0.5. The learning rate is set to 1E-3 and the weight decay is set to 5E-4. The following mean absolute percentage error (MAPE) is used as the performance metric: MAPE 100 𝑁 𝑁 𝑖 𝑦𝑖 ˆ𝑦𝑖 𝑦𝑖 . (6) We also implement ParaGraph [13]. ParaGraph is comprised of 3 sub-models to predict the net capacitance ranging from (0.01fF, 1fF], (1fF, 10fF], (10fF, 10pF], respectively. Each submodel has a 32- dimension width, a 5-layer GNN model, and a 4-layer MLP regressor. 6.3 Classification and Regression Results Table 3 shows the classification results using different GNN mod- els. The GraphSAGE-based classifier has the best accuracy across all design cases. The attention-based classifier has relatively lower F1 macro scores than others. Besides, SP8192W SRAM has special characteristics since all classifiers have reduced scores in this case. It can be explained by the imbalanced Ceff distribution in Figure 1, where the number of easy negatives from class 1 is over 100X larger than that of other classes. In Figure 5, we compare the classification results with different training strategies. After changing the loss function from cross en- tropy to Focal loss, the F1 macro scores are improved significantly. GAT GCN GraphSAGE_mean GraphSAGE_pool 0.0 0.2 0.4 0.6 0.8 1.0 F1 Macro w o FL w o sub. inst. w o FL w sub. inst. w FL w o sub. inst. w FL w sub. inst. Figure 5: Performance comparison of the GNN-based classi- fiers trained by different strategies. Moreover, by introducing the subcircuit instance nodes in the graph, the hierarchical information improves the performance of the GAT- based, GCN, GraphSAGE_mean-based classifiers, but slightly degen- erates that of the GraphSAGE_pool-based model. We also find the existence of subcircuit nodes helps GCN and GraphSAGE_mean to converge more quickly during training. Table 3 also compares the accuracy of different ML-based regres- sion models. ParaGraph s prediction errors are listed in the last column, as it does not incorporate the net classification stage. Para- Graph has the worst prediction accuracy, over 30 MAPEs for the first 3 design cases. This is because it fails to solve the class imbal- ance in the dataset. The None row from each design case represents the 5 pure regression models without introducing any classifica- tion and feature projection. They are trained individually according to the ground truth labels of samples. The input of the regressors is just the original features of net nodes. Compared to the None model, GNN-based models have lower predicted errors across all net classes. For the GraphSAGE model, using a mean aggregator is a good choice in our experiments and achieves better accuracy. The GCN-base and the GraphSAGE_mean-based regression models have similar accuracy across all design cases. The minimum and maximum MAPE reductions of the proposed 2-stage models against ParaGraph are 2.5X in SP8192W and 19X in Ultra8T. Notice that for SP8192W SRAM, the MAPE increases for all models when predicting Ceff of the nets in class 0 and class 2. This is due to a lot of false positives residing in these 2 classes (see the low F1 macro scores) as noise. 6.4 Other Comparisons Figure 6 shows the power consumption collected from the pre-layout simulation, the post-layout simulation, and the proposed method. With the predicted Ceff, the performance error of the pre-layout simu- lation is largely reduced from 57.24 to 17.77 on average. Moreover, since our method only uses schematics, the maximum simulation speedup reaches 586X for Ultra8T while the minimum speedup is 19.66X for SSRAM compared to the post-layout simulation. Table 4 further lists the scales of different models, and the train- ing inference time. ParaGraph [13] exhibits the largest training and inference time due to the complicated aggregation function that requires an attention operation for each edge type. In general, the proposed GraphSAGE_mean-based model is the most efficient model. The proposed models have a larger number of trainable parameters than ParaGraph due to the existence of the 5 regressors. SSRAM Ultra8T Sandwich SP8192W 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Power (mW) Post-layout Pre-layout Pre-layout with Ceff 0 100 200 300 400 500 600 700 800 Speedup Figure 6: Simulated power consumption using different netlists, and the speedup of the proposed method. 5 Table 3: The classification accuracy, F1 macro, and mean absolute percentage error (MAPE) of the regressors with different GNN classifiers on the test set. (The figures meaning the best performance are highlighted) Design Case Choice of Classifier Class Acc. F1 Macro Class 0 Class 1 Class 2 Class 3 Class 4 All tested nets SSRAM ParaGraph[13] - - - - - - - 33.53 None - - 21.73 5.53 28.84 6.59 - 6.95 GAT 93.34 0.83 26.15 0.88 26.37 1.68 - 4.30 GCN 95.38 0.88 15.58 2.22 19.00 5.66 - 4.05 GraphSAGE_mean 97.98 0.93 24.19 1.37 14.09 6.35 - 3.13 GraphSAGE_pool 99.22 0.97 14.11 8.70 31.94 1.34 - 9.66 Ultra8T ParaGraph[13] - - - - - - - 32.01 None - - 4.12 11.59 28.81 20.79 9.49 9.78 GAT 96.61 0.90 2.57 1.30 17.10 5.25 45.07 2.67 GCN 98.36 0.94 1.15 1.59 6.11 4.89 9.10 1.68 GraphSAGE_mean 99.91 0.99 0.95 1.87 6.77 3.56 5.63 1.72 GraphSAGE_pool 98.29 0.96 3.10 9.89 14.07 5.92 10.03 7.74 Sandwich ParaGraph[13] - - - - - - - 34.98 None - - 23.57 25.05 29.01 55.78 8.20 25.07 GAT 87.04 0.72 15.92 10.08 20.43 30.08 35.28 13.60 GCN 97.99 0.97 3.83 6.98 10.12 10.46 6.71 6.25 GraphSAGE_mean 98.72 0.98 3.11 5.96 8.56 8.21 8.49 5.32 GraphSAGE_pool 96.33 0.95 3.84 7.91 10.99 14.07 8.06 6.83 SP8192W ParaGraph[13] - - - - - - - 4.61 None - - 26.68 1.14 38.27 6.74 55.75 1.62 GAT 99.17 0.72 33.34 0.36 25.95 2.71 9.34 0.87 GCN 99.22 0.76 26.32 0.26 22.70 2.74 21.18 0.81 GraphSAGE_mean 99.72 0.88 20.59 0.73 12.36 2.34 3.99 1.04 GraphSAGE_pool 99.40 0.82 24.38 1.31 26.66 5.33 6.10 1.82 Table 4: Storage and time overhead of different GNN-based models. Info. ParaGraph[13] GAT GCN GraphSAGE_mean GraphSAGE_pool of params 141,987 212,362 212,618 216,650 224,970 train. time(h) 21.59 16.43 13.41 7.65 15.94 infer. time(s) 27.10 17.28 5.58 4.07 6.08 7 CONCLUSION This paper presents a novel method to train a 2-stage model based on GNN and MLP for predicting parasitic capacitances in SRAM designs. This model well handles the class imbalance of net parasitics in SRAMs, and thus outperforms the existing state-of-the-art model. In the future, the proposed method will be extended to complete RC prediction and integrated into circuit optimization algorithms of energy-efficient SRAM design. REFERENCES [1] ARM. 2023. Artisan Embedded Memory IP. silicon-ip-physical embedded-memory [2] Yung-Chen Chien and Jinn-Shyan Wang. 2018. A 0.2 V 32-Kb 10T SRAM with 41 nW standby power for IoT applications. IEEE Trans. Circuits Syst. I 65, 8 (2018), 2443 2454. [3] Weibing Gong, Wenjian Yu, Yongqiang Lü, Qiming Tang, Qiang Zhou, and Yici Cai. 2010. A parasitic extraction method of VLSI interconnects for pre-route timing analysis. In Proc. Int. Conf. on Commun., Circuits and Syst. (ICCCAS). 871 875. [4] Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for learning in graph domains. In Proc. Int. Joint Conf. on Neural Netw. (IJCNN). 729 734. [5] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in Neural Information Process. Syst. 30 (2017). [6] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [7] Chenfeng Li, Dezhong Hu, and Xiaoyan Zhang. 2023. Pre-Layout Parasitic-Aware Design Optimizing for RF Circuits Using Graph Neural Network. Electronics 12, 2 (2023), 465. [8] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In Proc. Int. Conf. on Comput. Vision (ICCV). 2980 2988. [9] Mingjie Liu, Walker J Turner, George F Kokai, Brucek Khailany, David Z Pan, and Haoxing Ren. 2021. Parasitic-aware analog circuit sizing with graph neural networks and Bayesian optimization. In Proc. DATE. 1372 1377. [10] Daniela Sánchez Lopera, Lorenzo Servadei, Gamze Naz Kiprit, Souvik Hazra, Robert Wille, and Wolfgang Ecker. 2021. A survey of graph neural networks for electronic design automation. In Proc. MLCAD. 1 6. [11] Saibal Mukhopadhyay, Hamid Mahmoodi, and Kaushik Roy. 2005. Modeling of failure probability and statistical design of SRAM array for yield enhancement in nanoscaled CMOS. IEEE Trans. Comput.-Aided Design Integr. Circuits Syst. 24, 12 (2005), 1859 1880. [12] Sai Surya Kiran Pentapati, Bon Woong Ku, and Sung Kyu Lim. 2021. ML-based wire RC prediction in monolithic 3D ICs with an application to full-chip optimization. In Proc. ISPD. 75 82. [13] Haoxing Ren, George F Kokai, Walker J Turner, and Ting-Sheng Ku. 2020. Para- Graph: Layout parasitics and device parameter prediction using graph neural networks. In Proc. DAC. 1 6. [14] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. June 3 7, 2018. Modeling relational data with graph con- volutional networks. In Proc. European Semantic Web Conference (ESWC), Heraklion, Crete, Greece. 593 607. [15] Shan Shen, Tianxiang Shao, Xiaojing Shang, Yichen Guo, Ming Ling, Jun Yang, and Longxing Shi. 2019. TS cache: A fast cache with timing-speculation mechanism under low supply voltages. IEEE Trans. VLSI Syst. 28, 1 (2019), 252 262. [16] Shan Shen, Hao Xu, Yongliang Zhou, Ming Ling, and Wenjian Yu. 2023. Ultra8T: A Sub-Threshold 8T SRAM with Leakage Detection. arXiv preprint arXiv:2306.08936 (2023). [17] Brett Shook, Prateek Bhansali, Chandramouli Kashyap, Chirayu Amin, and Sid- dhartha Joshi. 2020. MLParest: Machine learning based parasitic estimation for custom circuit design. In Proc. DAC. 1 6. [18] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [19] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, et al. 2019. Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv preprint arXiv:1909.01315 (2019). [20] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE Trans. Neural Netw. Learn. Syst. 32, 1 (2020), 4 24. [21] Dingcheng Yang, Wenjian Yu, Yuanbo Guo, and Wenjie Liang. 2021. CNN-Cap: Effective convolutional neural network based capacitance models for full-chip parasitic extraction. In Proc. ICCAD. 1 9. [22] Jun Yang, Yuyao Kong, Zhen Wang, Yan Liu, Bo Wang, Shouyi Yin, and Longxin Shi. 2019. 24.4 sandwich-RAM: An energy-efficient in-memory BWN architecture with pulse-width modulation. In Proc. Int. Solid-State Circuits Conf. (ISSCC). 394 396. [23] Chengshuo Yu, Taegeun Yoo, Kevin Tshun Chuan Chai, Tony Tae-Hyoung Kim, and Bongjin Kim. 2022. A 65-nm 8T SRAM compute-in-memory macro with column ADCs for processing neural networks. IEEE J. Solid-State Circuits 57, 11 (2022), 3466 3476. 6\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nDeep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs Shan Shen , Dingcheng Yang , Yuyang Xie , Chunyan Pei , Wenjian Yu , Bei Yu Department Computer Science Technology, BNRist, Tsinghua University, Beijing, China Department of Computer Science Engineering, The Chinese University of Hong Kong, Hong Kong SAR ABSTRACT To achieve higher system energy efficiency, SRAM in SoCs is of- ten customized. The parasitic effects cause notable discrepancies between pre-layout and post-layout circuit simulations, leading to difficulty in converging design parameters and excessive design it- erations. Is it possible to well predict the parasitics based on the pre-layout circuit, so as to perform parasitic-aware pre-layout simu- lation? In this work, we propose a deep-learning-based 2-stage model to accurately predict these parasitics in pre-layout stages. The model combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron (MLP) regressors, effectively managing class imbalance of the net parasitics in SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant internal net samples and integrate subcircuit information into the graph to abstract the hierarchical structure of schematics. Experiments on 4 real SRAM designs show that our approach not only surpasses the state-of-the-art model in parasitic prediction by a maximum of 19X reduction of error but also significantly boosts the simulation process by up to 598X speedup. 1 INTRODUCTION With the rapid expansion of intelligent Internet of Things (IoT) devices, contemporary System-on-Chips (SoCs) are increasingly fo- cused on enhancing energy efficiency. This is essential for extending the standby time of battery-powered devices. In order to optimize the energy efficiency of on-chip memory, both academic and industrial researchers have proposed various high-energy-efficiency memory structures. These include near-threshold SRAM [15][2], compute-in- memory SRAM [22][23], and others. SRAM customization involves adjustments in the topology and size of the memory cell, peripheral circuits, timing, and controller design.\n\n--- Segment 2 ---\nThese include near-threshold SRAM [15][2], compute-in- memory SRAM [22][23], and others. SRAM customization involves adjustments in the topology and size of the memory cell, peripheral circuits, timing, and controller design. To ensure the stability of the chip s functionality and its final yield, it is crucial to simulate and evaluate various performance metrics, such as read write delay, power consumption, and failure probabilities during the design pro- cedure [11]. Once the SRAM performance falls short of expectations, This work was partially supported by the National Science and Technology Major Project (2021ZD0114703), and NSFC under grant No. 62204141 and 62090025. W. Yu is the corresponding author. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner author(s). GLSVLSI 24, June 12 14, 2024, Clearwater, FL, USA 2024 Copyright held by the owner author(s). ACM ISBN 979-8-4007-0605-9 24 06. significant time and labor are required for iterative design modifi- cations. This process adds complexity to customizing the SRAM IP and results in prolonged design cycles. In traditional design workflows, designers proceed with circuit design and optimization based on pre-layout simulations. They then perform verification using post-layout simulations after completing the layout drawing. However, with advanced technologies adopt- ing smaller transistor sizes and lower operating voltages, there s a notable decrease in transistor driving ability. Consequently, the parasitic effect becomes too significant to be overlooked. This dis- parity leads to a substantial gap between pre-layout and post-layout simulation results, making it challenging to ensure the final circuit performance. Regrettably, the back-end design of customized SRAM is an arduous and time-intensive process. This is due to two primary reasons: (1) the high density of transistors often leads to violations of design rules; (2) the routing process is complicated, as it involves managing a substantial number of data wires within a constrained area.\n\n--- Segment 3 ---\nRegrettably, the back-end design of customized SRAM is an arduous and time-intensive process. This is due to two primary reasons: (1) the high density of transistors often leads to violations of design rules; (2) the routing process is complicated, as it involves managing a substantial number of data wires within a constrained area. Consequently, even minor alterations to the schematic can necessitate extensive modifications in the layout. Recently, numerous studies [3, 7, 9, 12, 13, 17, 21] have employed machine learning as a potent tool for predicting parasitic effects in electronic design. However, training accurate ML-based models is often hindered by the class imbalance of parasitic capacitance. As illustrated in Figure 1, the distribution of net capacitance shows a prominent imbalance, with values ranging from 0.01 fF to 100 pF. There are over 106 nets in the second bin of SP8192W SRAM, predominantly internal connections in memory cells. This imbalance presents two problems: (1) the training process becomes inefficient, as the majority of nets are easy negatives that offer little to no valuable learning signal; (2) these easy negatives can dominate the training process, potentially leading to the development of ineffective models. In this work, we aim to address this challenge and enhance the accuracy of a deep-learning-based model for parasitic prediction. The contributions of this work are outlined as follows. We propose a unique 2-stage model, consisting of a Graph Neu- ral Network (GNN) classifier and multiple Multi-Layer Percep- tron (MLP) regressors, and the corresponding training strategy. We implement Focal Loss [8] as the loss function of the classifier to further reduce the overwhelming effect of easy negatives. Subcircuit information is also integrated into the graph, effec- tively mirroring the hierarchical structure found in schematics. Our experimental results demonstrate that the 2-stage model achieves an accuracy improvement ranging from 2.5X to 19X over the state-of-the-art model [13], while also reducing both training and inference time.\n\n--- Segment 4 ---\nSubcircuit information is also integrated into the graph, effec- tively mirroring the hierarchical structure found in schematics. Our experimental results demonstrate that the 2-stage model achieves an accuracy improvement ranging from 2.5X to 19X over the state-of-the-art model [13], while also reducing both training and inference time. The proposed method stands out from existing parasitic predic- tion models that primarily focus on small-scale analog circuits, arXiv:2507.06549v1 [cs.LG] 9 Jul 2025 10 2 10 1 100 101 102 103 104 Capacitance (fF) 10 0 10 1 10 2 10 3 10 4 10 5 10 6 Count (a) 10 2 10 1 100 101 102 103 104 Capacitance (fF) 10 1 10 2 10 3 10 4 10 5 10 6 Count (b) 10 2 10 1 100 101 102 103 104 Capacitance (fF) 10 1 10 2 10 3 10 4 10 5 10 6 Count (c) Figure 1: Distributions of the parasitic net capacitance of (a) Ultra8T SRAM, (b) Sandwich-RAM [22], and (c) SP8192W SRAM [1]. as it targets large-scale memory circuits. By simulating the schematic netlist with the predicted parasitics, we achieve a significant speedup, up to 586X, compared to the simulations using the post-layout netlist. This approach substantially ben- efits the large-scale memory circuit design. The versatility of the proposed method also makes it naturally suitable for other downstream tasks. This includes design space exploration and expedited design updates. The paper is organized as follows. Section 2 reviews the related work in the field and Section 3 introduces some fundamental concepts of GNNs. Section 4 presents the proposed model for net capacitance prediction, and Section 5 describes the over workflow. Section 6 gives experimental results derived from real SRAM designs. Section 7 concludes the whole paper. 2 RELATED WORKS In recent years, a lot of attention has been paid to machine learning as an effective method for parasitic prediction. Shook et al. [17] trans- formed the front-end netlist (schematic) of analog IP designs into a star topology and used a random forest model to predict the equiva- lent resistance and capacitance of each net.\n\n--- Segment 5 ---\nShook et al. [17] trans- formed the front-end netlist (schematic) of analog IP designs into a star topology and used a random forest model to predict the equiva- lent resistance and capacitance of each net. Another work, named ParaGraph [13], converts circuit schematics into graphs and utilizes graph neural network (GNN) techniques to predict net capacitance and device layout parameters. It leverages a complex aggregation procedure to compute node embedding, which is a combination of graph convolutional network (GCN) [6], GraphSage [5], relation GCN (RGCN) [14], and graph attention network (GAT) [18]. It also includes the ensemble modeling technique to improve the prediction accuracy via training 3 different sub-models for different magnitudes of the capacitance value. Net samples with a ground truth larger than the maximum predicted value of the sub-model are ignored dur- ing training, which increases prediction accuracy within a specified range within each model. The sub-model with larger capacitance prediction is more preferred than that with small capacitance. Li et al [7] adopted a ParaGraph-like model to predict the parasitics and guide optimization of the voltage-controlled oscillator (VCO). Liu et al. [9] proposed an improved surrogate performance model us- ing parasitic graph embeddings generated by ParaGraph [13]. Then the surrogate model was integrated into a Bayesian optimization workflow to automate transistor sizing. Machine learning-based capacitance extraction is also studied for the scenario of monolithic 3D (M3D) IC design. Pentapati et al. [12] proposed a regression model based on augmented decision tree learn- ing to better predict 3D net parasitics. In another scenario, Yang et al. [21] proposed a convolutional neural network capacitance model (CNN-Cap) for the pattern-matching-based capacitance extraction. The method is able to accurately compute the capacitances of 2D patterns with a variable number of conductors. It should be pointed out that although [7, 9, 13, 17] are for pre- dicting net capacitances at the pre-layout stage, they are all focused on analog circuits. Compared to the analog circuit, large-scale high- density memory circuits suffer from a more severe imbalance of training data.\n\n--- Segment 6 ---\nIt should be pointed out that although [7, 9, 13, 17] are for pre- dicting net capacitances at the pre-layout stage, they are all focused on analog circuits. Compared to the analog circuit, large-scale high- density memory circuits suffer from a more severe imbalance of training data. Besides, authors in [12] and [3], etc. assume the lay- out is partially ready, which is not purely based on the pre-layout schematic. In this work, we focus on predicting the net capacitances at the pre-layout stage to expedite the customized SRAM design. 3 PRELIMINARIES A graph G (V, E) is a structure used to represent entities and their relations. It consists of two sets, the set of nodes V (also called vertices) and the set of edges E (also called arcs). Each node is associ- ated with a vector of features 𝑥𝑣 (𝑥1, ...,𝑥𝑑) with dimension 𝑑. The 𝑛 V node features form a matrix 𝑋 R𝑛 𝑑. An edge (𝑢, 𝑣) E, represented as 𝑒𝑢,𝑣, connecting a pair of nodes 𝑢and 𝑣indicates that there is a relation between them. The edge can also have a feature vector 𝑥𝑒 (𝑥1, ...,𝑥𝑐) with dimension 𝑐and 𝑚 E features form a matrix 𝑋𝑒 R𝑚 𝑐. The neighborhood of a node N(𝑣) is defined as N(𝑣) {𝑢 V (𝑢, 𝑣) E}. Graphs can be either homogeneous or heterogeneous. In a homogeneous graph, all the nodes represent instances of the same type and all the edges represent relations of the same type. In contrast, in a heterogeneous graph, the nodes and edges can be of different types. GNN is a kind of neural network that operates directly on data structured as graphs, without losing structural and feature informa- tion [4].\n\n--- Segment 7 ---\nIn contrast, in a heterogeneous graph, the nodes and edges can be of different types. GNN is a kind of neural network that operates directly on data structured as graphs, without losing structural and feature informa- tion [4]. Having an input graph, a GNN aims to learn the embedding vectors per node, defined as ℎ𝑢, 𝑢 V, which encodes the neighbor- hood information of each node [20]. The message passing between nodes is assumed as the most generic GNN layer [10]. Given a graph structure, message passing updates the edge embeddings ℎ𝑒 (𝑢,𝑣) with ℎ𝑒 (𝑢,𝑣) 𝜙(ℎ𝑢,ℎ𝑣,𝑥𝑒 (𝑢,𝑣)), (1) where 𝜙( ) is an arbitrary, non-linear, differentiable function that aggregates its inputs, and 𝑥𝑒 (𝑢,𝑣) R𝑐is the initial edge feature vector. After the edge embedding is obtained, and defining 𝑥𝑢 R𝑑 as the feature vector for the starting node, the node representation 2 is updated by ℎ 𝑢 𝜙 (ℎ𝑢, 𝑣 𝑁(𝑢) ℎ𝑒 (𝑢,𝑣),𝑥𝑢). (2) The graph embeddings learned by GNNs can be used as inputs to other ML models for building an end-to-end framework. There are three levels of tasks for such a framework: node, edge, and graph [20]. In the node-level task, the regression or classification problem of the nodes is of concern. 4 PARASITIC CAPACITANCE PREDICTION A 2-stage deep-learning model is proposed in this section. It contains a GNN-based classifier and 5 MLP regressors. We first introduce the conversion of schematics, then the feature extraction, and last the model architecture and training strategy.\n\n--- Segment 8 ---\nIt contains a GNN-based classifier and 5 MLP regressors. We first introduce the conversion of schematics, then the feature extraction, and last the model architecture and training strategy. 4.1 Conversion of Circuit Netlist to Graph In order to reflect the modularization in circuit design, the schematic netlist will be modeled as a heterogeneous graph G (V, E) in this work. A node in the graph corresponds to a net, a device, or a sub- circuit instance in the circuit. Figure 2 shows an example of a buffer containing three types of node sets V VNET VDEV VSUB. A green circle represents a net (𝑣 VNET), connecting to devices; an orange square represents an instance of the transistor device (𝑣 VDEV), which can also be other types of devices such as a capacitor, a resistor, and a diode; a blue triangle represents an instance of the subcircuit (𝑣 VSUB), comprised of multiple nets and device instances. We set all edges in the graph as undirected. The advantage of the undi- rected graph is that feature information can be transferred between different nodes in a shallow network. Compared to the graphs only comprising nodes representing nets and devices in [13], the node of the subcircuit instance in this work can reflect the hierarchical structure in schematics and generalize the local features. net 1 device 1 device inst. 2 net 2 net 3 device inst. 3 device inst. 4 .SUBCKT inv in out M1 out in VSS VSS nch W 0.1u L 0.03u M2 out in VDD VDD pch W 0.2u L 0.03u .ENDS .SUBCKT buffer net1 net3 Xinst1 net1 net2 inv Xinst2 net2 net3 inv .ENDS Sub. inst.1 Sub. inst.0 Sub. inst.2 Figure 2: Example of converting a circuit to a graph. 4.2 Acquisition of Training Data In the post-layout netlist (SPF files) generated by RC extraction, the net parasitics form a complex 𝜋-type RC network.\n\n--- Segment 9 ---\ninst.2 Figure 2: Example of converting a circuit to a graph. 4.2 Acquisition of Training Data In the post-layout netlist (SPF files) generated by RC extraction, the net parasitics form a complex 𝜋-type RC network. As the net capacitance is of concern, we generate the lumped sum of capacitance (Ceff) for each net from the post-layout netlist and use it as the ground-truth label for training the 2-stage model. We extract three types of feature vectors from the schematic netlist. The definitions of different feature elements are listed in Table 1. Device nodes need to extract different features according to their device types. Compared to existing works, we collect more features for different nodes.\n\n--- Segment 10 ---\nDevice nodes need to extract different features according to their device types. Compared to existing works, we collect more features for different nodes. For example, feature elements of a tran- sistor device include the multiplier (𝑀𝑚𝑜𝑠), channel length (𝐿), width (𝑊), etc., while other feature elements (such as 𝑀𝑟𝑒𝑠, 𝐿𝑟𝑒𝑠, and𝑊𝑟𝑒𝑠) Table 1: Feature definitions of nets, device instances, and sub- circuit instances Type Notation Definition Index Net 𝑁𝑚𝑜𝑠 of connected transistors 0 𝑁𝑔 of connected gate terminals 1 𝑁𝑠𝑑 of connected source drain terminals 2 𝑁𝑏 of connected base terminals 3 𝑊𝑡𝑜𝑡 Total width of connected transistor 4 𝐿𝑡𝑜𝑡 Total length of connected transistor 5 𝑁𝑐𝑎𝑝 of connected capacitors 6 𝐿𝑟𝑡𝑜𝑡 Total length of connected capacitors 7 𝑁𝑟𝑡𝑜𝑡 Total of connected capacitor fingers 8 𝑁𝑟𝑒𝑠 of connected resistors 9 𝑊𝑡𝑜𝑡_𝑟𝑒𝑠 Total width of connected resistors 10 𝐿𝑡𝑜𝑡_𝑟𝑒𝑠 Total length of connected resistors 11 𝑁𝑝𝑜𝑟𝑡 of connected ports 12 Device Instance 𝑀𝑚𝑜𝑠 Multiplier of transistors 0 𝐿 Length of the transistor 1 𝑊 Width of the transistor 2 𝑀𝑟𝑒𝑠 Multiplier of connected resistors 3 𝐿𝑟𝑒𝑠 Length of resistor 4 𝑊𝑟𝑒𝑠 Width of resistor 5 𝑀𝑐𝑎𝑝 Multiplier of connected capacitor 6 𝐿𝑟 Length of capacitor 7 𝑁𝑟 of capacitor fingers 8 𝑁𝑝 of ports in the device instance 9 𝑇 Type code of the device instance 10 Sub- circuit Instance 𝑁𝑝𝑜𝑟𝑡 of ports in the device instance 0 𝑁𝑑 of ports in the device instance 1 𝑁𝑛 of nets in the subcircuit instance 2 𝐿𝑣𝑙 Hierarchy level of the instance 3 are zeros.\n\n--- Segment 11 ---\nCompared to existing works, we collect more features for different nodes. For example, feature elements of a tran- sistor device include the multiplier (𝑀𝑚𝑜𝑠), channel length (𝐿), width (𝑊), etc., while other feature elements (such as 𝑀𝑟𝑒𝑠, 𝐿𝑟𝑒𝑠, and𝑊𝑟𝑒𝑠) Table 1: Feature definitions of nets, device instances, and sub- circuit instances Type Notation Definition Index Net 𝑁𝑚𝑜𝑠 of connected transistors 0 𝑁𝑔 of connected gate terminals 1 𝑁𝑠𝑑 of connected source drain terminals 2 𝑁𝑏 of connected base terminals 3 𝑊𝑡𝑜𝑡 Total width of connected transistor 4 𝐿𝑡𝑜𝑡 Total length of connected transistor 5 𝑁𝑐𝑎𝑝 of connected capacitors 6 𝐿𝑟𝑡𝑜𝑡 Total length of connected capacitors 7 𝑁𝑟𝑡𝑜𝑡 Total of connected capacitor fingers 8 𝑁𝑟𝑒𝑠 of connected resistors 9 𝑊𝑡𝑜𝑡_𝑟𝑒𝑠 Total width of connected resistors 10 𝐿𝑡𝑜𝑡_𝑟𝑒𝑠 Total length of connected resistors 11 𝑁𝑝𝑜𝑟𝑡 of connected ports 12 Device Instance 𝑀𝑚𝑜𝑠 Multiplier of transistors 0 𝐿 Length of the transistor 1 𝑊 Width of the transistor 2 𝑀𝑟𝑒𝑠 Multiplier of connected resistors 3 𝐿𝑟𝑒𝑠 Length of resistor 4 𝑊𝑟𝑒𝑠 Width of resistor 5 𝑀𝑐𝑎𝑝 Multiplier of connected capacitor 6 𝐿𝑟 Length of capacitor 7 𝑁𝑟 of capacitor fingers 8 𝑁𝑝 of ports in the device instance 9 𝑇 Type code of the device instance 10 Sub- circuit Instance 𝑁𝑝𝑜𝑟𝑡 of ports in the device instance 0 𝑁𝑑 of ports in the device instance 1 𝑁𝑛 of nets in the subcircuit instance 2 𝐿𝑣𝑙 Hierarchy level of the instance 3 are zeros. All features are normalized by the maximum value in the dataset to achieve better numerical stability.\n\n--- Segment 12 ---\nFor example, feature elements of a tran- sistor device include the multiplier (𝑀𝑚𝑜𝑠), channel length (𝐿), width (𝑊), etc., while other feature elements (such as 𝑀𝑟𝑒𝑠, 𝐿𝑟𝑒𝑠, and𝑊𝑟𝑒𝑠) Table 1: Feature definitions of nets, device instances, and sub- circuit instances Type Notation Definition Index Net 𝑁𝑚𝑜𝑠 of connected transistors 0 𝑁𝑔 of connected gate terminals 1 𝑁𝑠𝑑 of connected source drain terminals 2 𝑁𝑏 of connected base terminals 3 𝑊𝑡𝑜𝑡 Total width of connected transistor 4 𝐿𝑡𝑜𝑡 Total length of connected transistor 5 𝑁𝑐𝑎𝑝 of connected capacitors 6 𝐿𝑟𝑡𝑜𝑡 Total length of connected capacitors 7 𝑁𝑟𝑡𝑜𝑡 Total of connected capacitor fingers 8 𝑁𝑟𝑒𝑠 of connected resistors 9 𝑊𝑡𝑜𝑡_𝑟𝑒𝑠 Total width of connected resistors 10 𝐿𝑡𝑜𝑡_𝑟𝑒𝑠 Total length of connected resistors 11 𝑁𝑝𝑜𝑟𝑡 of connected ports 12 Device Instance 𝑀𝑚𝑜𝑠 Multiplier of transistors 0 𝐿 Length of the transistor 1 𝑊 Width of the transistor 2 𝑀𝑟𝑒𝑠 Multiplier of connected resistors 3 𝐿𝑟𝑒𝑠 Length of resistor 4 𝑊𝑟𝑒𝑠 Width of resistor 5 𝑀𝑐𝑎𝑝 Multiplier of connected capacitor 6 𝐿𝑟 Length of capacitor 7 𝑁𝑟 of capacitor fingers 8 𝑁𝑝 of ports in the device instance 9 𝑇 Type code of the device instance 10 Sub- circuit Instance 𝑁𝑝𝑜𝑟𝑡 of ports in the device instance 0 𝑁𝑑 of ports in the device instance 1 𝑁𝑛 of nets in the subcircuit instance 2 𝐿𝑣𝑙 Hierarchy level of the instance 3 are zeros. All features are normalized by the maximum value in the dataset to achieve better numerical stability. 4.3 Two-Stage Model Building The distribution of Ceff is extremely imbalanced (Figure 1).\n\n--- Segment 13 ---\nAll features are normalized by the maximum value in the dataset to achieve better numerical stability. 4.3 Two-Stage Model Building The distribution of Ceff is extremely imbalanced (Figure 1). The large number of nets with small capacitance, called easy negatives, results in the minority samples being prone to be mispredicted. However, those nets with large Ceff are usually important ports or clock nets, which are likely to affect the timing analysis results. Therefore, we divide the training data (net nodes) into 5 categories and label them with 𝑡 T {0, 1, 2, 3, 4} according to the magnitude of their parasitic capacitance, i.e., {(0.01 fF, 0.1 fF], (0.1 fF, 1 fF], (1 fF, 10 fF], (10 fF,100 fF], (100 fF, )}. GNN is more suitable for classification tasks. In order to do the capacitance regression with GNN, we build a 2-stage model based on GNN and multilayer perceptron (MLP). The model is mainly di- vided into the net classification and the net capacitance regression, as shown in Figure 3. Feature vectors (different dimensions) of three types of nodes are projected to a common space through 3 different projectors individually so that we can easily transform a heteroge- neous graph into a homogeneous one. It is convenient for subsequent training of different GNN models. Next, the projected feature vectors are input into the GNN MLP model to classify the net nodes into different categories. Embeddings of net nodes generated by the GNN model will be fed into a 3-layer MLP to predict the label 𝑡of a net node. Note that only the net nodes have labels and their embeddings are retained at this time, while that of the other two types of nodes 3 will be masked. To reduce both the training and inference time, our proposed method leverages simple GNN models but uses a delicate training strategy and loss function. In the 2nd stage, capacitance regression is performed for each category individually. The input of the regression model is a con- catenated vector containing node embeddings and the original net feature vector. The target vector is the effective net capacitance Ceff. The training flow is also divided into 2 stages.\n\n--- Segment 14 ---\nThe target vector is the effective net capacitance Ceff. The training flow is also divided into 2 stages. We first use feature vectors and labels of net nodes to train and validate the classifier. Once the classifier is obtained, according to the classification result, net nodes in the training set are grouped into different sub-sets and are used to train their own regressors with the corresponding targets Ceff, respectively. The test set is invisible during the whole training process. In the 1st stage, we adopt Focal Loss [8] with a weight factor 𝛼 [0, 1] which applies a modulating term to the cross-entropy loss in order to focus learning on hard examples and down-weight the numerous easy negatives. L 𝑣 VNET 𝐹𝐿𝑣 𝑣 VNET 𝛼𝑡(1 𝑝𝑣(𝑡))𝛾log(𝑝𝑣(𝑡)), (3) where 𝛾 0 is a tunable focusing parameter, 𝑝𝑣(𝑡) [0, 1] is the model s estimated probability for the net node 𝑣 VNET to be in class 𝑡 T. Here we set 𝛼to be 𝛼𝑡 1 𝑓𝑡 , (4) where 𝑓𝑡is the proportion of class 𝑡in the entire data samples. It can also be set to other values according to the classification results to increase the contribution of categories with fewer data to the total loss function. In the 2nd stage, we use the mean value of the squared percentage error as the loss function in the regression training, i.e. L𝑡 1 𝑁𝑡 𝑣 VNET 𝑡 (𝑦𝑣 ˆ𝑦𝑣 𝑦𝑣 ) 2 , (5) where symbol 𝑦denotes the target value and ˆ𝑦the predicted value. 𝑁𝑡is the number of nets with predicted label 𝑡. 5 OVERALL WORKFLOW The workflow of the proposed method is illustrated in Figure 4.\n\n--- Segment 15 ---\n𝑁𝑡is the number of nets with predicted label 𝑡. 5 OVERALL WORKFLOW The workflow of the proposed method is illustrated in Figure 4. Training data are collected from the pre-layout schematic and the post-layout netlist by matching net names. During training, the input of the 2-stage model includes feature matrices from 3 types of nodes, GNN 3- Layer MLP Device inst. feat. Net embed. Projected net feat. Net feat. Projected device inst. feat. Subcircuit inst. feat. h[0] h[N] 2- Layer MLP h[0] h[N] 2- Layer MLP h[0] h[N] 2- Layer MLP 4-Layer MLP 4-Layer MLP 4-Layer MLP 4-Layer MLP 4-Layer MLP Ceff Ceff Ceff Ceff Ceff Projected subcircuit inst. feat. Concat. (Net feat., Net embed.) ① ② Class 0 (Ceff 0.1fF) Class 1 (0.1 Ceff 1.0fF) Class 2 (1 Ceff 10fF) Class 3 (10 Ceff 100fF) Class 4 (Ceff 100fF) Figure 3: Structure of the proposed 2-stage model including net classification stage and capacitance regression stage. Net Class. Cap. Regr. 2-Stage Model Downstream tasks (Sim.) Ceff Labels Graph Net matching Features Predicted Ceff Features Training Validation Inference Application Post-layout Netlist Pre-layout Netlist Pre-layout Netlist Figure 4: The overall workflow of the proposed method. a graph converted from the schematic, a class label vector, and a net capacitance vector. The output of the model is the predicted Ceff. The regressors of stage 2 can be trained in a parallel manner to further speed up the model training. During the inference, the design s graph and the features of nets are fed into the model. The predicted net capacitance from the model is back-annotated into a netlist for downstream tasks. 6 EXPERIMENTAL RESULTS Experiments are conducted to show the effectiveness of the proposed method with 4 SRAM designs. All ML models are implemented based on DGL library [19] written in PyTorch.\n\n--- Segment 16 ---\n6 EXPERIMENTAL RESULTS Experiments are conducted to show the effectiveness of the proposed method with 4 SRAM designs. All ML models are implemented based on DGL library [19] written in PyTorch. The post-layout netlists with full parasitics are extracted by StarRC. All experiments are run on a server with 40 Intel Xeon Silver CPUs with 128GB memory. Table 2: SRAM designs used by this work. SRAM Designs SSRAM Ultra8T Sandwich SP8192W of Nets 19902 861842 1160940 2342588 of Device Inst. 57417 2325092 2665422 6984821 of Subcircuit Inst. 9965 315466 428498 39885 Total of Nodes 87284 3502400 4254860 9367294 of Edges 134926 13392268 13254854 32009072 6.1 Dataset We prepare the full schematic netlists and the post-layout SPF netlists to extract the feature vectors and net capacitance respectively. Ta- ble 2 summarizes the SRAM design examples utilized by this work (net parasitic capacitance distributions are illustrated in Figure 1). All designs are under 28nm CMOS technology. SSRAM [15] is a small design with high-energy efficiency with a timing-speculation tech- nique. The Sandwich-RAM [22] is made up of half of digital circuits for computing and half of memory arrays, forming a sandwich-like structure. Ultra8T SRAM [16] is a multi-voltage design with a wide range of operation voltages, which contains analog circuits. SP8192W SRAM is based on a single port 6T cell structure generated by the SRAM compiler [1] with tremendously high density. We leverage the full neighbor sampling method provided by DGL, which ran- domly splits the test set from the original large graph and returns a new subgraph. This ensures the invisibility of nodes in the test set during model training. The train validation test set split ratio for each design is 0.6 0.2 0.2. 6.2 Model Settings In the 1st stage, we adopt three mainstream GNN structures, includ- ing a 3-layer graph attention network (GAT) [18], a 3-layer graph 4 convolutional networks (GCN) [6], and 2-layer GraphSAGE [5], in net classification.\n\n--- Segment 17 ---\nThe train validation test set split ratio for each design is 0.6 0.2 0.2. 6.2 Model Settings In the 1st stage, we adopt three mainstream GNN structures, includ- ing a 3-layer graph attention network (GAT) [18], a 3-layer graph 4 convolutional networks (GCN) [6], and 2-layer GraphSAGE [5], in net classification. GraphSAGE can be implemented with different types of aggregators in (1) and (2), and we use mean and pool in our comparison. We set all layer widths to 64. The feature projection MLP has 2 linear layers and the classification MLP has 3 linear layers. The activation function is ReLU and the dropout is 0.1 for all layers. The batch normalization (BN) is turned on. The GAT-based classifier needs to turn on the layer normalization and turn off the BN to get reasonable accuracy. We use a cosine annealing schedule to adjust the learning rate based on the number of executed epochs. The learn- ing rate ranges from 1E-3 to 1E-4. The weight decay parameter is set to 5E-4. The performance metrics include accuracy, and the F1 macro score (the best value is 1.0 for all metrics). F1 macro is the unweighted mean of the F1 score of each class. The training process is run 10 times for each model and the performance metrics are averaged from the best epochs. In the 2nd stage, each regressor is a 4-layer MLP with hidden layer widths {128, 128, 64}. The input width is the sum of the node embedding width 64 and the original net feature width 13 while the output width is 1. The activation function is ReLU and the dropout is 0.5. The learning rate is set to 1E-3 and the weight decay is set to 5E-4. The following mean absolute percentage error (MAPE) is used as the performance metric: MAPE 100 𝑁 𝑁 𝑖 𝑦𝑖 ˆ𝑦𝑖 𝑦𝑖 . (6) We also implement ParaGraph [13].\n\n--- Segment 18 ---\nThe following mean absolute percentage error (MAPE) is used as the performance metric: MAPE 100 𝑁 𝑁 𝑖 𝑦𝑖 ˆ𝑦𝑖 𝑦𝑖 . (6) We also implement ParaGraph [13]. ParaGraph is comprised of 3 sub-models to predict the net capacitance ranging from (0.01fF, 1fF], (1fF, 10fF], (10fF, 10pF], respectively. Each submodel has a 32- dimension width, a 5-layer GNN model, and a 4-layer MLP regressor. 6.3 Classification and Regression Results Table 3 shows the classification results using different GNN mod- els. The GraphSAGE-based classifier has the best accuracy across all design cases. The attention-based classifier has relatively lower F1 macro scores than others. Besides, SP8192W SRAM has special characteristics since all classifiers have reduced scores in this case. It can be explained by the imbalanced Ceff distribution in Figure 1, where the number of easy negatives from class 1 is over 100X larger than that of other classes. In Figure 5, we compare the classification results with different training strategies. After changing the loss function from cross en- tropy to Focal loss, the F1 macro scores are improved significantly. GAT GCN GraphSAGE_mean GraphSAGE_pool 0.0 0.2 0.4 0.6 0.8 1.0 F1 Macro w o FL w o sub. inst. w o FL w sub. inst. w FL w o sub. inst. w FL w sub. inst. Figure 5: Performance comparison of the GNN-based classi- fiers trained by different strategies. Moreover, by introducing the subcircuit instance nodes in the graph, the hierarchical information improves the performance of the GAT- based, GCN, GraphSAGE_mean-based classifiers, but slightly degen- erates that of the GraphSAGE_pool-based model. We also find the existence of subcircuit nodes helps GCN and GraphSAGE_mean to converge more quickly during training. Table 3 also compares the accuracy of different ML-based regres- sion models.\n\n--- Segment 19 ---\nWe also find the existence of subcircuit nodes helps GCN and GraphSAGE_mean to converge more quickly during training. Table 3 also compares the accuracy of different ML-based regres- sion models. ParaGraph s prediction errors are listed in the last column, as it does not incorporate the net classification stage. Para- Graph has the worst prediction accuracy, over 30 MAPEs for the first 3 design cases. This is because it fails to solve the class imbal- ance in the dataset. The None row from each design case represents the 5 pure regression models without introducing any classifica- tion and feature projection. They are trained individually according to the ground truth labels of samples. The input of the regressors is just the original features of net nodes. Compared to the None model, GNN-based models have lower predicted errors across all net classes. For the GraphSAGE model, using a mean aggregator is a good choice in our experiments and achieves better accuracy. The GCN-base and the GraphSAGE_mean-based regression models have similar accuracy across all design cases. The minimum and maximum MAPE reductions of the proposed 2-stage models against ParaGraph are 2.5X in SP8192W and 19X in Ultra8T. Notice that for SP8192W SRAM, the MAPE increases for all models when predicting Ceff of the nets in class 0 and class 2. This is due to a lot of false positives residing in these 2 classes (see the low F1 macro scores) as noise. 6.4 Other Comparisons Figure 6 shows the power consumption collected from the pre-layout simulation, the post-layout simulation, and the proposed method. With the predicted Ceff, the performance error of the pre-layout simu- lation is largely reduced from 57.24 to 17.77 on average. Moreover, since our method only uses schematics, the maximum simulation speedup reaches 586X for Ultra8T while the minimum speedup is 19.66X for SSRAM compared to the post-layout simulation. Table 4 further lists the scales of different models, and the train- ing inference time. ParaGraph [13] exhibits the largest training and inference time due to the complicated aggregation function that requires an attention operation for each edge type. In general, the proposed GraphSAGE_mean-based model is the most efficient model.\n\n--- Segment 20 ---\nParaGraph [13] exhibits the largest training and inference time due to the complicated aggregation function that requires an attention operation for each edge type. In general, the proposed GraphSAGE_mean-based model is the most efficient model. The proposed models have a larger number of trainable parameters than ParaGraph due to the existence of the 5 regressors. SSRAM Ultra8T Sandwich SP8192W 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Power (mW) Post-layout Pre-layout Pre-layout with Ceff 0 100 200 300 400 500 600 700 800 Speedup Figure 6: Simulated power consumption using different netlists, and the speedup of the proposed method. 5 Table 3: The classification accuracy, F1 macro, and mean absolute percentage error (MAPE) of the regressors with different GNN classifiers on the test set. (The figures meaning the best performance are highlighted) Design Case Choice of Classifier Class Acc.\n\n--- Segment 21 ---\n5 Table 3: The classification accuracy, F1 macro, and mean absolute percentage error (MAPE) of the regressors with different GNN classifiers on the test set. (The figures meaning the best performance are highlighted) Design Case Choice of Classifier Class Acc. F1 Macro Class 0 Class 1 Class 2 Class 3 Class 4 All tested nets SSRAM ParaGraph[13] - - - - - - - 33.53 None - - 21.73 5.53 28.84 6.59 - 6.95 GAT 93.34 0.83 26.15 0.88 26.37 1.68 - 4.30 GCN 95.38 0.88 15.58 2.22 19.00 5.66 - 4.05 GraphSAGE_mean 97.98 0.93 24.19 1.37 14.09 6.35 - 3.13 GraphSAGE_pool 99.22 0.97 14.11 8.70 31.94 1.34 - 9.66 Ultra8T ParaGraph[13] - - - - - - - 32.01 None - - 4.12 11.59 28.81 20.79 9.49 9.78 GAT 96.61 0.90 2.57 1.30 17.10 5.25 45.07 2.67 GCN 98.36 0.94 1.15 1.59 6.11 4.89 9.10 1.68 GraphSAGE_mean 99.91 0.99 0.95 1.87 6.77 3.56 5.63 1.72 GraphSAGE_pool 98.29 0.96 3.10 9.89 14.07 5.92 10.03 7.74 Sandwich ParaGraph[13] - - - - - - - 34.98 None - - 23.57 25.05 29.01 55.78 8.20 25.07 GAT 87.04 0.72 15.92 10.08 20.43 30.08 35.28 13.60 GCN 97.99 0.97 3.83 6.98 10.12 10.46 6.71 6.25 GraphSAGE_mean 98.72 0.98 3.11 5.96 8.56 8.21 8.49 5.32 GraphSAGE_pool 96.33 0.95 3.84 7.91 10.99 14.07 8.06 6.83 SP8192W ParaGraph[13] - - - - - - - 4.61 None - - 26.68 1.14 38.27 6.74 55.75 1.62 GAT 99.17 0.72 33.34 0.36 25.95 2.71 9.34 0.87 GCN 99.22 0.76 26.32 0.26 22.70 2.74 21.18 0.81 GraphSAGE_mean 99.72 0.88 20.59 0.73 12.36 2.34 3.99 1.04 GraphSAGE_pool 99.40 0.82 24.38 1.31 26.66 5.33 6.10 1.82 Table 4: Storage and time overhead of different GNN-based models.\n\n--- Segment 22 ---\n(The figures meaning the best performance are highlighted) Design Case Choice of Classifier Class Acc. F1 Macro Class 0 Class 1 Class 2 Class 3 Class 4 All tested nets SSRAM ParaGraph[13] - - - - - - - 33.53 None - - 21.73 5.53 28.84 6.59 - 6.95 GAT 93.34 0.83 26.15 0.88 26.37 1.68 - 4.30 GCN 95.38 0.88 15.58 2.22 19.00 5.66 - 4.05 GraphSAGE_mean 97.98 0.93 24.19 1.37 14.09 6.35 - 3.13 GraphSAGE_pool 99.22 0.97 14.11 8.70 31.94 1.34 - 9.66 Ultra8T ParaGraph[13] - - - - - - - 32.01 None - - 4.12 11.59 28.81 20.79 9.49 9.78 GAT 96.61 0.90 2.57 1.30 17.10 5.25 45.07 2.67 GCN 98.36 0.94 1.15 1.59 6.11 4.89 9.10 1.68 GraphSAGE_mean 99.91 0.99 0.95 1.87 6.77 3.56 5.63 1.72 GraphSAGE_pool 98.29 0.96 3.10 9.89 14.07 5.92 10.03 7.74 Sandwich ParaGraph[13] - - - - - - - 34.98 None - - 23.57 25.05 29.01 55.78 8.20 25.07 GAT 87.04 0.72 15.92 10.08 20.43 30.08 35.28 13.60 GCN 97.99 0.97 3.83 6.98 10.12 10.46 6.71 6.25 GraphSAGE_mean 98.72 0.98 3.11 5.96 8.56 8.21 8.49 5.32 GraphSAGE_pool 96.33 0.95 3.84 7.91 10.99 14.07 8.06 6.83 SP8192W ParaGraph[13] - - - - - - - 4.61 None - - 26.68 1.14 38.27 6.74 55.75 1.62 GAT 99.17 0.72 33.34 0.36 25.95 2.71 9.34 0.87 GCN 99.22 0.76 26.32 0.26 22.70 2.74 21.18 0.81 GraphSAGE_mean 99.72 0.88 20.59 0.73 12.36 2.34 3.99 1.04 GraphSAGE_pool 99.40 0.82 24.38 1.31 26.66 5.33 6.10 1.82 Table 4: Storage and time overhead of different GNN-based models. Info.\n\n--- Segment 23 ---\nF1 Macro Class 0 Class 1 Class 2 Class 3 Class 4 All tested nets SSRAM ParaGraph[13] - - - - - - - 33.53 None - - 21.73 5.53 28.84 6.59 - 6.95 GAT 93.34 0.83 26.15 0.88 26.37 1.68 - 4.30 GCN 95.38 0.88 15.58 2.22 19.00 5.66 - 4.05 GraphSAGE_mean 97.98 0.93 24.19 1.37 14.09 6.35 - 3.13 GraphSAGE_pool 99.22 0.97 14.11 8.70 31.94 1.34 - 9.66 Ultra8T ParaGraph[13] - - - - - - - 32.01 None - - 4.12 11.59 28.81 20.79 9.49 9.78 GAT 96.61 0.90 2.57 1.30 17.10 5.25 45.07 2.67 GCN 98.36 0.94 1.15 1.59 6.11 4.89 9.10 1.68 GraphSAGE_mean 99.91 0.99 0.95 1.87 6.77 3.56 5.63 1.72 GraphSAGE_pool 98.29 0.96 3.10 9.89 14.07 5.92 10.03 7.74 Sandwich ParaGraph[13] - - - - - - - 34.98 None - - 23.57 25.05 29.01 55.78 8.20 25.07 GAT 87.04 0.72 15.92 10.08 20.43 30.08 35.28 13.60 GCN 97.99 0.97 3.83 6.98 10.12 10.46 6.71 6.25 GraphSAGE_mean 98.72 0.98 3.11 5.96 8.56 8.21 8.49 5.32 GraphSAGE_pool 96.33 0.95 3.84 7.91 10.99 14.07 8.06 6.83 SP8192W ParaGraph[13] - - - - - - - 4.61 None - - 26.68 1.14 38.27 6.74 55.75 1.62 GAT 99.17 0.72 33.34 0.36 25.95 2.71 9.34 0.87 GCN 99.22 0.76 26.32 0.26 22.70 2.74 21.18 0.81 GraphSAGE_mean 99.72 0.88 20.59 0.73 12.36 2.34 3.99 1.04 GraphSAGE_pool 99.40 0.82 24.38 1.31 26.66 5.33 6.10 1.82 Table 4: Storage and time overhead of different GNN-based models. Info. ParaGraph[13] GAT GCN GraphSAGE_mean GraphSAGE_pool of params 141,987 212,362 212,618 216,650 224,970 train.\n\n--- Segment 24 ---\nInfo. ParaGraph[13] GAT GCN GraphSAGE_mean GraphSAGE_pool of params 141,987 212,362 212,618 216,650 224,970 train. time(h) 21.59 16.43 13.41 7.65 15.94 infer. time(s) 27.10 17.28 5.58 4.07 6.08 7 CONCLUSION This paper presents a novel method to train a 2-stage model based on GNN and MLP for predicting parasitic capacitances in SRAM designs. This model well handles the class imbalance of net parasitics in SRAMs, and thus outperforms the existing state-of-the-art model. In the future, the proposed method will be extended to complete RC prediction and integrated into circuit optimization algorithms of energy-efficient SRAM design. REFERENCES [1] ARM. 2023. Artisan Embedded Memory IP. silicon-ip-physical embedded-memory [2] Yung-Chen Chien and Jinn-Shyan Wang. 2018. A 0.2 V 32-Kb 10T SRAM with 41 nW standby power for IoT applications. IEEE Trans. Circuits Syst. I 65, 8 (2018), 2443 2454. [3] Weibing Gong, Wenjian Yu, Yongqiang Lü, Qiming Tang, Qiang Zhou, and Yici Cai. 2010. A parasitic extraction method of VLSI interconnects for pre-route timing analysis. In Proc. Int. Conf. on Commun., Circuits and Syst. (ICCCAS). 871 875. [4] Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A new model for learning in graph domains. In Proc. Int. Joint Conf. on Neural Netw. (IJCNN). 729 734. [5] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in Neural Information Process. Syst. 30 (2017). [6] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).\n\n--- Segment 25 ---\nSemi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907 (2016). [7] Chenfeng Li, Dezhong Hu, and Xiaoyan Zhang. 2023. Pre-Layout Parasitic-Aware Design Optimizing for RF Circuits Using Graph Neural Network. Electronics 12, 2 (2023), 465. [8] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. Focal loss for dense object detection. In Proc. Int. Conf. on Comput. Vision (ICCV). 2980 2988. [9] Mingjie Liu, Walker J Turner, George F Kokai, Brucek Khailany, David Z Pan, and Haoxing Ren. 2021. Parasitic-aware analog circuit sizing with graph neural networks and Bayesian optimization. In Proc. DATE. 1372 1377. [10] Daniela Sánchez Lopera, Lorenzo Servadei, Gamze Naz Kiprit, Souvik Hazra, Robert Wille, and Wolfgang Ecker. 2021. A survey of graph neural networks for electronic design automation. In Proc. MLCAD. 1 6. [11] Saibal Mukhopadhyay, Hamid Mahmoodi, and Kaushik Roy. 2005. Modeling of failure probability and statistical design of SRAM array for yield enhancement in nanoscaled CMOS. IEEE Trans. Comput.-Aided Design Integr. Circuits Syst. 24, 12 (2005), 1859 1880. [12] Sai Surya Kiran Pentapati, Bon Woong Ku, and Sung Kyu Lim. 2021. ML-based wire RC prediction in monolithic 3D ICs with an application to full-chip optimization. In Proc. ISPD. 75 82. [13] Haoxing Ren, George F Kokai, Walker J Turner, and Ting-Sheng Ku. 2020. Para- Graph: Layout parasitics and device parameter prediction using graph neural networks. In Proc. DAC. 1 6. [14] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling.\n\n--- Segment 26 ---\n1 6. [14] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling. June 3 7, 2018. Modeling relational data with graph con- volutional networks. In Proc. European Semantic Web Conference (ESWC), Heraklion, Crete, Greece. 593 607. [15] Shan Shen, Tianxiang Shao, Xiaojing Shang, Yichen Guo, Ming Ling, Jun Yang, and Longxing Shi. 2019. TS cache: A fast cache with timing-speculation mechanism under low supply voltages. IEEE Trans. VLSI Syst. 28, 1 (2019), 252 262. [16] Shan Shen, Hao Xu, Yongliang Zhou, Ming Ling, and Wenjian Yu. 2023. Ultra8T: A Sub-Threshold 8T SRAM with Leakage Detection. arXiv preprint arXiv:2306.08936 (2023). [17] Brett Shook, Prateek Bhansali, Chandramouli Kashyap, Chirayu Amin, and Sid- dhartha Joshi. 2020. MLParest: Machine learning based parasitic estimation for custom circuit design. In Proc. DAC. 1 6. [18] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903 (2017). [19] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, et al. 2019. Deep graph library: A graph-centric, highly-performant package for graph neural networks. arXiv preprint arXiv:1909.01315 (2019). [20] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE Trans. Neural Netw. Learn. Syst. 32, 1 (2020), 4 24.\n\n--- Segment 27 ---\nSyst. 32, 1 (2020), 4 24. [21] Dingcheng Yang, Wenjian Yu, Yuanbo Guo, and Wenjie Liang. 2021. CNN-Cap: Effective convolutional neural network based capacitance models for full-chip parasitic extraction. In Proc. ICCAD. 1 9. [22] Jun Yang, Yuyao Kong, Zhen Wang, Yan Liu, Bo Wang, Shouyi Yin, and Longxin Shi. 2019. 24.4 sandwich-RAM: An energy-efficient in-memory BWN architecture with pulse-width modulation. In Proc. Int. Solid-State Circuits Conf. (ISSCC). 394 396. [23] Chengshuo Yu, Taegeun Yoo, Kevin Tshun Chuan Chai, Tony Tae-Hyoung Kim, and Bongjin Kim. 2022. A 65-nm 8T SRAM compute-in-memory macro with column ADCs for processing neural networks. IEEE J. Solid-State Circuits 57, 11 (2022), 3466 3476. 6\n\n