=== ORIGINAL PDF: 2506.00424v2_COGNATE_Acceleration_of_Sparse_Tensor_Programs_on_.pdf ===\n\nRaw text length: 78448 characters\nCleaned text length: 78053 characters\nNumber of segments: 44\n\n=== CLEANED TEXT ===\n\narXiv:2506.00424v2 [cs.LG] 15 Jun 2025 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Chamika Sudusinghe 1 Gerasimos Gerogiannis 1 Damitha Lenadora 1 Charles Block 1 Josep Torrellas 1 Charith Mendis 1 Abstract Sparse tensor programs are essential in deep learn- ing and graph analytics, driving the need for op- timized processing. To meet this demand, spe- cialized hardware accelerators are being devel- oped. Optimizing these programs for acceler- ators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffec- tive for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general- purpose hardware (e.g., CPUs) to train cost mod- els, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5 of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE out- performs existing techniques, achieving average speedups of 1.47 (up to 5.46 ) for SpMM and 1.39 (up to 4.22 ) for SDDMM. 1. Introduction Sparse tensor programs have gained increased significance with the recent advancements in sparse deep learning and graph analytics (Beltagy et al., 2020; Ye Ji, 2021; Child et al., 2019; Dao et al., 2021) workloads. As a result, many hand-crafted performance optimization techniques have been suggested to improve the performance of sparse 1University of Illinois Urbana-Champaign, USA. Correspon- dence to: Chamika Sudusinghe Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). kernels (Kjolstad et al., 2017; Ye et al., 2023; Hong et al., 2019; Jiang et al., 2020). However, the varying non-zero distributions in input sparse matrices have made it diffi- cult to develop performance optimizations for sparse tensor programs that consistently work well across diverse inputs. To overcome this challenge, machine learning (ML)-based program optimization techniques have been introduced to optimize sparse tensor programs on established hardware platforms (e.g., CPUs) (Won et al., 2023; Yang et al., 2023). These techniques adaptively select a program configuration based on the input sparse matrix features. For example, WACO (Won et al., 2023) introduces learned cost models to predict the runtime cost of programs under different sparse matrices and program configurations. Then, search-based techniques are used to automatically find the optimal pro- gram configuration using the cost model s output. Overall, these ML-based techniques show superior performance and adaptability across a diverse range of inputs compared to manually crafted performance optimization techniques. Recently, on the hardware front, new domain-specific ma- chines specifically designed for sparse operations are emerg- ing (Pal et al., 2018; Hegde et al., 2019; Aananthakrish- nan et al., 2023; Gerogiannis et al., 2023; Li et al., 2023; Mu noz-Mart ınez et al., 2023; Jin et al., 2024). These ma- chines, known as hardware accelerators, offer significant speedups over established hardware platforms. Similar to CPUs, sparse accelerators also provide various program configurations (Gerogiannis et al., 2023; Jin et al., 2024; Gerogiannis et al., 2024), which must be configured by soft- ware to achieve the hardware s full potential. It is important that this potential is tested during early-stage hardware de- velopment (i.e. before the actual chip is available) to inform better hardware design decisions. For example, hardware ar- chitects face the risk of overprovisioning hardware resources (e.g., increasing cache size) to address inefficiencies that could potentially be resolved through improved software strategies (e.g., adopting a better tiling strategy). Therefore, it is crucial to automatically select the optimal program con- figuration during the design space exploration (DSE) phase of accelerator development. However, finding the best program configuration for a given 1 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning early-stage hardware accelerator is challenging. This is because software developers have access only to expen- sive simulators and, therefore, cannot utilize ML-based cost model development and auto-tuning techniques commonly used for established hardware platforms, which rely on large supervised datasets. Such datasets often include hundreds of thousands of labeled examples (Won et al., 2023). Unfor- tunately, the time needed to collect datasets of similar scale for emerging accelerators relying on simulators is many orders of magnitude longer than the real execution of the program on the actual chip. For example, it can take up to two weeks to collect a single data point using the simulator of the state-of-the-art SPADE sparse accelerator (Gerogian- nis et al., 2023). At the same time, the same program would take less than a second to execute on the real chip once fabri- cated. Collecting large datasets would require huge clusters running simulations for months or even years. Therefore, in order to bring the same benefits of ML-based optimizations to accelerator platforms at their early stages, we need to rethink learned cost model construction techniques that are data-frugal and highly sample-efficient. Emerging Hardware Dataset Fine-tune on Target General-Purpose Hardware Dataset Runtimes Program Configs Train on Source Runtime Execution Simulator of a Dedicated Sparse Accelerator Predicted Best Config Sparse Matrix Sparse Matrices Runtimes Program Configs Sparse Matrices Target Cost Model Source Cost Model Sparse Matrix Figure 1: Transfer learning pipeline of COGNATE. 5 100 1000 of Matrices 1.0 1.1 1.2 1.3 1.4 1.5 Geomean Speedup WACO FM WACO FA Ours (Top-1) Optimal: 1.55 Baseline: 1.0 Figure 2: Performance of existing systems for SpMM on SPADE; WacoNet with feature augmentation (WACO FA), and feature mapping (WACO FM). Inspired by the suc- cess of transfer learn- ing in other domains (Weiss et al., 2016; Zhuang et al., 2020), re- searchers have proposed many techniques to re- duce data requirements for training cost mod- els (Sasaki et al., 2022; Zheng et al., 2021). These techniques lever- age knowledge trans- ferred from cost models learned on one hardware platform (source) to an- other (target) using the ubiquitous pre-train and fine-tune paradigm (Krizhevsky et al., 2012). Such techniques have shown to reduce the data requirement for the target platform. Therefore, using such techniques to transfer learn cost models from general- purpose hardware to emerging accelerators can reduce the data requirements from expensive simulations (Figure 1). However, we notice that most prior works have achieved ef- fective knowledge transfer only between hardware platforms of the same type (e.g. CPU-to-CPU, GPU-to-GPU) (Sasaki et al., 2022; Won et al., 2023; Zheng et al., 2021). Trans- ferring between hardware of different types (e.g. CPU-to- accelerator) poses unique challenges: Heterogeneous program configuration spaces. The pro- gram configurations for emerging sparse accelerators, which serve as the input feature space for cost models, can dif- fer significantly from those of general-purpose hardware. For example, emerging sparse accelerators have software- managed buffers instead of hardware-managed caches and specialized, rather than general-purpose, pipelines. This causes a disparity in program configuration spaces for general-purpose hardware and emerging accelerators, mak- ing it challenging to naively apply transfer learning. Exist- ing heterogeneous transfer learning techniques (Liang et al., 2019), such as feature augmentation (Daum e III, 2009; Duan et al., 2012), can be a viable approach. However, these tech- niques often produce feature representations that are too sparse for the cost model to effectively learn, specifically when accommodating a diverse set of program configura- tion across different hardware platforms. Figure 2 shows the results of applying popular heterogeneous transfer learning techniques feature augmentation (FA) and feature map- ping (FM) to a learned cost model, WACO (Won et al., 2023). Even when using data samples from 1000 matrices for fine-tuning on the SPADE accelerator, the best configura- tions found under WACO FA and WACO FM are far from optimal. Therefore, we need better techniques to handle the heterogeneity of program configurations across hardware. High sample efficiency requirement. Existing transfer learning solutions for learned cost models operating in ho- mogeneous feature spaces typically require at least 25 of the original dataset used in a non-transfer learning setup to achieve competitive performance on the target hardware plat- form (Sasaki et al., 2022). The target dataset requirement for these solutions can further increase due to the heterogeneous input feature spaces between general-purpose hardware and emerging accelerators. This makes it infeasible to adopt ex- isting solutions in their current form for accelerators in early design stages. Therefore, we need data-frugal techniques. COGNATE. In this paper, we present COGNATE, a novel framework for developing learned cost models that enable effective knowledge transfer (Figure 1) overcoming these challenges. COGNATE uses WACO s cost model architec- ture (Won et al., 2023) as the base model (WacoNet) but incorporates key changes to make it amenable for transfer learning. This enables the discovery of better program con- figurations that are closer to the optimal (Figure 2), while requiring significantly less fine-tuning data samples. COGNATE is centered around two key principles intro- duced in (Neyshabur et al., 2020): feature reuse and the 2 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning capture of low-level statistical information. We observe that, even though the program configurations between general- purpose hardware and accelerators are heterogeneous, there are certain feature spaces that can be mapped due to their similarities. Motivated by this observation, we propose an approximate mapping of comparable code optimizations, effectively segregating the feature space generated by pro- gram configuration into homogeneous and heterogeneous components. This allows feature reuse across the source and target platforms. The heterogeneous components repre- sent non-mappable hardware specific parameters that can be disparate across different platforms. Such components can introduce challenges during transfer learning due to negative transfer. To separately encode the heterogeneous feature spaces, we introduce a novel latent space representation of the heterogeneous input feature space using an auto- encoder. This novel formulation of the feature space allows us to effectively reuse features while minimizing the impact of negative transfer. Further, we observe that certain layers of WacoNet do not contribute heavily to the final prediction and this over-parameterization can hinder transferability due to over-fitting. To mitigate this, COGNATE modifies WacoNet by reducing the number of layers and extracting features at various depths and scales, effectively allowing the model to capture low-level statistical information. We evaluate COGNATE on two widely used sparse opera- tions, Sparse Matrix-Matrix Multiplication (SpMM) and Sampled Dense-Dense Matrix Multiplication (SDDMM). Starting with a CPU as the source hardware platform, we transfer program program configurations to the the state-of- the art SPADE (Gerogiannis et al., 2023) emerging sparse accelerator. SPADE s open ISA and vast set of possible pro- gram configurations make it an ideal target platform for our evaluation. Further, to demonstrate the generalizability of COGNATE, we evaluate our techniques for a second target accelerator an NVIDIA A100 GPU. Our results show that COGNATE outperforms existing transfer learning techniques by 28.44 , achieving an aver- age speedup of 1.47 (up to 5.46 ) for SpMM and 1.39 (up to 4.22 ) for SDDMM on SPADE. On the A100 GPU, it attains an average speedup of 1.17 (up to 1.61 ) for SpMM and 1.15 (up to 1.49 ) for SDDMM. In summary, this paper makes the following contributions. We introduce techniques to segregate and encode the ho- mogeneous (approximate mapping of comparable code optimizations) and heterogeneous (latent representa- tion using an auto-encoder) components of program configurations across different hardware platforms. We introduce COGNATE, a novel data-frugal framework for developing learned cost models that are amenable to few-shot fine-tuning across different hardware platforms, leveraging the above techniques. We evaluate and show that COGNATE produces highly accurate transfer learned cost models for emerging sparse accelerators with minimal data collection overhead. Fur- thermore, we perform extensive experiments and ablation studies to demonstrate its benefits and generalizability. 2. Background and Related Work 2.1. Sparse Tensor Programs Sparse tensor programs perform computational tasks that involve tensors where most of the elements are zero. These computations are optimized to efficiently process only the non-zero values. We describe two operations frequently used in these computations below. Sparse Matrix-Matrix Multiplication (SpMM) is the op- eration of multiplying a sparse matrix A RM K with a dense matrix B RK N, resulting in an output matrix D RM N. The SpMM operation can be expressed as Di,k P j Ai,j Bj,k, where Ai,j 0. Sampled Dense-Dense Matrix Multiplication (SDDMM) is an operation that involves the multiplication of two dense matrices, followed by an elementwise multiplica- tion with a sparse matrix. Given a sparse matrix A RM N, a sparse output matrix D RM N, and two dense matrices B RM K and C RK N, SD- DMM operation can be expressed as Di,k Ai,k P j (Bi,j Cj,k) , where Ai,k 0. 2.2. Sparse Tensor Programming Systems Table 1: Program configuration parameters (Config Params) available across CPU, GPU, and SPADE. Config Params CPU GPU SPADE Type Loop Strip-mining Numerical Loop Reordering Categorical Format Reordering Categorical Loop Binding Categorical Loop Unrolling Categorical Tiling Numerical Barrier Binary Cache Bypassing Binary Matrix Reordering Binary A sparse tensor programming system supports a range of code optimizations that modify the structure of the code to enhance performance. The effectiveness of these code optimizations depends on assigning specific values to the parameters of the program configuration. By tuning these parameters, we can significantly impact the runtime perfor- mance of sparse operations. Table 1 outlines the parameters available for program configurations across different hard- ware platforms explored in this work (code optimizations are detailed in Appendix B). The execution strategy for sparse tensor programs depends on both the hardware platform and the corresponding programming system used. In this 3 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning work, for CPU execution (source platform), we use TACO (Kjolstad et al., 2017), a domain-specific language and a compiler designed for sparse tensor algebra. Considering our target accelerators, SPADE has its own tile-based open instruction set architecture (ISA) to leverage different vari- ations of SpMM and SDDMM operations. For GPU, we employ SparseTIR (Ye et al., 2023), a sparse tensor com- pilation framework developed as an enhancement to TVM Tensor IR (Chen et al., 2018a). 2.3. ML-based cost models Learned Cost Models. Cost models act as fast cost- effective proxies for executing workloads on real hardware. Their primary goal is to accurately estimate the execution time of workloads as they would perform on real hardware. To achieve this, these cost models can be trained on data sam- ples with various program configurations and then be used to predict the program configuration that will deliver the op- timal performance. Hence, generally, the training objective of cost models is tied with minimizing t CM t , where t is the runtime of the true optimal program configuration and t CM is the runtime of the best program configuration suggested by the cost model (accuracy objective)(detailed Appendix A). Finding the best configuration suggested by the cost model is usually done using auxiliary intelligent search techniques such as simulated annealing, Monte Carlo tree search, and reinforcement learning. There have been numerous works on learned cost models to predict the run- time of workloads targeting different hardware platforms (Chen et al., 2018b; Adams et al., 2019). These techniques range from simple XGBoost (Chen Guestrin, 2016) based cost models (Chen et al., 2018b;a) to sophisticated deep neu- ral network based models (Baghdadi et al., 2021; Kaufman et al., 2021; Zhai et al., 2023; Zheng et al., 2020). WACO s Cost Model. WACO (Figure 3(a)) (Won et al., 2023) introduced a learned cost model specifically built for sparse tensor programs, which utilizes sparsity patterns as raw input data. WACO s cost model employs submanifold sparse convolution networks (SCNN) (Graham Van der Maaten, 2017) to extract features using an input featurizer. It leverages a neural network-based program embedder to capture the impact of code optimizations on sparse opera- tions by encoding program configurations into embeddings. These program embeddings are merged with the extracted sparsity pattern features produced by the input featurizer. The merged inputs are then processed through a multi-layer perceptron predictor to estimate the execution cost. Transfer Learning. Transfer learning is a technique that leverages knowledge gained from a task in a source domain to improve the performance of a related task in a target domain, where data collection can often be challenging (Bozinovski, 2020). There have been many successful ex- amples of transfer learning techniques in a wide range of fields (Weiss et al., 2016). Transfer learning can be catego- rized into two main types: homogeneous transfer learning (Zhuang et al., 2020), where the input and label spaces are the same, and heterogeneous transfer learning (Day Khoshgoftaar, 2017), where either one or both can be dif- ferent. In program optimization, transfer learning has been successfully used to transfer cost models learned from one hardware platform to another, primarily in homogeneous settings, to minimize the target domain data requirements (Zheng et al., 2021; Ryu Sung, 2021; Sasaki et al., 2022). In this work, we seek to minimize the target domain data requirement during fine-tuning (Shen et al., 2021), by tar- geting heterogeneous input feature spaces present between general-purpose hardware and emerging sparse accelerators (data-collection objective) (detailed in Appendix A). 3. Our Methodology: COGNATE Autoencoder Latent Encoder Configuration Mapper Sparse Convolution Input Featurizer (3) Sparse Latent Config Sparse Matrix Program Configurations Homogenous Heterogenous Predictor WACO Program Embedder Sparse Convolution Input Featurizer Sparse Config Predictor Program Configurations (a) (b) (4) Runtime Cost (1) (2) COGNATE Figure 3: A comparative overview of the enhanced cost model design in COGNATE (b) alongside WACO s cost model design (a), highlighting key differences. Here, we present COGNATE, a novel framework to design data-frugal learned cost models to accelerate the execution of sparse tensor programs on emerging hardware. The fol- lowing subsections outline our contributions toward achiev- ing the objectives set forth in Section 2.3; maximizing the accuracy while minimizing the data collection overhead. 3.1. Enhancements to Enable Transfer Learning We build upon WACO considering it as our base model by refining its architecture to better facilitate transfer learning across diverse hardware platforms. These enhancements represent contributions that are orthogonal to WACO s orig- inal scope. Our improved cost model design (Figure 3(b)) is structured around four key components: configuration mapper, input featurizer, latent encoder, and predictor. The configuration mapper (Figure 3(b)(1)) and latent encoder (Figure 3(b)(2)) replace the program embedder in WACO, while the input featurizer (Figure 3(b)(3)) has been modi- fied to more effectively capture low-level information from sparsity patterns. Both configuration mapper and input fea- turizer remain consistent across hardware platforms, serving as the components that enable efficient knowledge transfer. Configuration Mapper (FM). The configuration mapper captures homogeneity across hardware platforms by pro- cessing program configurations (cj) and their parameters to identify similarities in code optimizations across various 4 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning platforms. We designed it to approximately map similar con- figuration parameters across different hardware platforms (described in Section 3.2 and Section E) to a unified feature space. This is achieved by using explicit mapping func- tions. The resulting parameters are subsequently passed through a multi-layer perceptron (MLP) to produce the final configuration vector pj. In this work, we approximate the code optimizations loop strip-mining and loop reordering as pj FM(ϕ( ), π( ), cj). using the mapping functions ϕ and π, as introduced in Section 3.2. Input Featurizer (IFE). Matrices with identical dimen- sions and non-zero elements can exhibit vastly different sparsity patterns, making it difficult to extract meaning- ful features based only on statistical properties. Building on WACO s input featurizer (Won et al., 2023), we mod- ify the network architecture (Figure 3) to more effectively capture low-level information from sparsity patterns. Our network consists of 12 SCNN layers (compared to 14 layers in WACO), arranged in 4 blocks, each containing 3 sparse convolution layers. At the end of each block, we apply max pooling to condense spatial information. We increase the number of channels across blocks up to 256, whereas WACO had them fixed at 32. These additional channels enables our design to capture hierarchical features more effectively throughout the network compared to WACO. For a given sparse matrix M, our input featurizer generates a sparse feature vector sM, expressed as sM IFE(M). Latent Encoder (LE). We handle the heterogeneity of program configurations across hardware platforms using per-target autoencoders that compress the heterogeneous components of the configurations into compact latent rep- resentations (described in Section 3.3). An autoencoder is trained for each target sparse primitive pair. During both training and inference, the latent encoder LE processes a configuration (cj), transforming it into a latent representa- tion zj LE(cj), that encapsulates the unique characteris- tics of the program configuration. Predictor (P). As the final component of the cost model, the predictor (Figure 3(b)(4)) integrates the three feature vectors from the preceding components into a single uni- fied vector, encapsulating all key information about the sparsity pattern and program configuration. This unified vector (sM pj zj) is passed through a multi-layer percep- tron (MLP) to eventually output a scalar value representing the predicted execution cost, which can be expressed as ˆrM,cj P(pj sM zj). 3.2. Exploiting Homogeneity: Approximate Mapping of Comparable Code Optimizations Different hardware platforms often use distinct program- ming systems, leading to variations in how code optimiza- tions are parameterized (Figure 1). Further, an optimization available in one platform may not be directly available on another, requiring the combination of multiple other code op- timizations to replicate the same impact. For example, loop strip-mining optimization on CPUs can be closely approxi- mated by collectively applying barrier and tiling optimiza- tions in SPADE. By mapping the effects of these optimiza- tions using their program configuration parameters, we can expose patterns that facilitate effective knowledge transfer across hardware platforms. In this section, we present our approaches for approximately mapping loop strip-mining, barrier, and tiling optimizations between CPU and SPADE, and loop reordering optimization between CPU and GPU. Loop strip-mining is a code optimization that decomposes large software loops into smaller segments to optimize com- putations for memory utilization and cache performance. In our context, it is applied to loops iterating over the in- dices i, j, and k of matrices in SpMM and SDDMM sparse operations (Section 2.1), where parameters I, J, and K are used to split these loops into outer and inner segments, resulting in a loop nest of six decomposed loops. The re- sulting loop segments are {i1, i2, j1, j2, k1, k2} and their execution order is denoted by ω. In SPADE, we approx- imate this using barrier and tiling optimizations. Tiling decomposes a matrix into smaller blocks to optimize data reuse in the local memory, while barrier controls the ex- ecution order of tiles. For example, enabling barrier opti- mization pauses the tiles scheduled by a control processing element until all previous tiles have been completed (Gero- giannis et al., 2023). Similar to strip-mining parameters, the tiling parameters for i, j, and k indices of matrices are represented in SPADE as pcol, prow, dsplit, respectively, while barrier is represented by b, where b 1 if barrier is enabled, and b 0 otherwise. Intuitively, tiling divides computations into smaller blocks, while barriers control syn- chronization during execution. By enabling and disabling barriers for various tiling configurations, we can dictate the order of computation. This resembles loop strip-mining and reordering in CPUs, where optimizing loop execution enhances performance and cache utilization. We can approx- imately map tiling and barrier parameters to the correspond- ing strip-mining parameters using the mapping function ϕ : {pcol, prow, ssplit, b} {I, J, K, ω} as follows: ϕ(pcol, prow, ssplit, b) (I, J, K, ω) I pcol, J prow, K ssplit; ω ( [k2, j2, i2, i1, j1, k1] if b 1 [k2, i2, j2, i1, j1, k1] if b 0 Loop reordering is a code optimization that adjusts the exe- cution order (ω) of loops to improve cache efficiency and facilitate parallel processing. It is often applied after loop strip-mining. Here, we examine how it can be approximated for both CPU (a1) and GPU (a3). In CPU, loop strip-mining results in six decomposed loops {i1, i2, j1, j2, k1, k2}. Similarly, in GPU, loop strip-mining produces six loop seg- ments, but the loop structure differs {i1, i2, j, k1, k2, k3} 5 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning SpMM (SPADE) SDDMM (SPADE) SpMM (GPU) SDDMM (GPU) Sparse Tensor Operations 0.00 0.25 0.50 0.75 1.00 1.25 1.50 Geomean Speedup 1.04 1.05 0.52 0.64 1.09 1.07 0.55 0.67 1.29 1.19 0.72 0.89 0.71 0.59 0.07 0.09 1.40 1.27 1.03 1.07 1.47 1.39 1.17 1.15 1.55 1.44 1.25 1.22 WACO FA WACO FM No Transfer Zero-Shot (CPU) COGNATE (Top-1) COGNATE (Top-5) Optimal Figure 4: Geomean speedups of COGNATE and other techniques, normalized to the baseline. due to architectural differences. We approximate them us- ing Ω( ) function that determines the index of a loop seg- ment and a mapping function πai : {i1, i2, . . . , k2, ωai} {i1, i2, . . . , k3, ω ai} as follows: πa1(i1, i2, j1, j2, k1, k2, ωa1) i1, i2, j1, j2, k1, k2, k3, ω a1 ; k3 1, Ωa1(k3) Ωa1(k2) 1 πa3(i1, i2, j, k1, k2, k3, ωa3) i1, i2, j, j , k1, k2, k3, ω a3 ; j 1, Ωa3(j ) Ωa3(j) 1 3.3. Mitigating Heterogeneity: Latent Encoding of Hardware-specific Code Optimizations While we can use the strategies described in Section 3.2 to approximate code optimizations with homogeneity, such techniques are not applicable to hardware-specific optimiza- tions. An existing approach for representing hardware- specific optimizations across hardware platforms is to en- code them using feature augmentation. However, this results in excessively sparse feature vectors, as code optimizations that are not applicable to a selected hardware platform are zeroed out. Training models on such sparse feature vectors often leads to sub-optimal performance (Figure 4). To address this limitation, we propose indexing the parame- ters of the heterogeneous component of the program config- urations for each platform ai using low-dimensional latent representations. Specifically, we train an autoencoder AEai to learn a latent representation zj for each configuration cj Cai. This is accomplished by determining the value ranges for all parameters of the heterogeneous component in the program configurations, followed by training an autoen- coder to learn an unsupervised embedding of this parame- terization. Once trained, we use the encoder LEai in AEai, which takes each configuration (cj) as input and transforms it into its corresponding latent representation zj, where zj is a fixed-size vector. By compressing configurations from different hardware platforms each with varying param- eters and ranges into fixed-size vectors, we standardize the input for hardware-specific optimizations into the cost model. This compression significantly simplifies the model compared to feature augmentation, as the cost model now processes fewer input features, reducing its computational complexity. With the hardware specific optimizations now represented in a unified latent feature space, it becomes possible to identify and leverage similarities in their impact on performance during fine-tuning. Finally, this approach facilitates the seamless integration of emerging hardware platforms into COGNATE, as we can extend COGNATE to support new target hardware platforms by training new au- toencoders and relying on few-shot fine-tuning, eliminating the need to retrain the source model from scratch (detailed in Appendix C). As long as the overall structure of the sparse tensor program remains consistent, COGNATE can quickly adapt by using a small number of new performance samples. In contrast, traditional cost model development approaches would require re-evaluating a large number of configurations (Won et al., 2023). 4. Evaluation 4.1. Dataset, Training and Evaluation Setup Dataset. Our experiments were conducted using real-world sparse matrices sourced from the SuiteSparse Matrix Collec- tion (Davis Hu, 2011). This dataset has been widely used in previous work (Pal et al., 2018; Hong et al., 2019; Jiang et al., 2020; Gerogiannis et al., 2023; Won et al., 2023) and covers a broad spectrum of domains, ensuring a realistic and comprehensive evaluation of COGNATE s performance. To collect the training dataset, we performed the sparse op- erations SpMM and SDDMM on three distinct hardware platforms: an Intel Xeon Gold 6348 CPU with 1TB of RAM, an NVIDIA A100 GPU paired with an Intel Xeon Platinum 8358, and SPADE, a simulated sparse accelera- tor with 32 processing elements operating at 0.8GHz. To ensure practical feasibility across hardware platforms, the program configuration search space was constrained to 256 configurations for SPADE and approximately 300 configura- tions for SparseTIR (GPU). We gathered data samples using 1,500 matrices for each hardware platform, with up to 1,000 matrices used for model training under various scenarios and the remainder was set aside for validation. For each matrix, we randomly sampled 100 program configurations per hardware platform to have diverse and representative training datasets across all experiments. 6 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Program Configuration Search Space for SPADE. The program configuration search space considered for the SPADE accelerator was derived from a combination of key tunable parameters including tiling, synchronization barriers, cache bypassing, and matrix reordering. As sum- marized in Table 1, the parameters for barrier insertion, cache bypassing, and matrix reordering are binary (i.e., ei- ther enabled or disabled). Tiling is controlled by three numerical parameters: the number of row panels, column panels, and the split factor. These values were chosen to resemble those explored in the original SPADE work (Gero- giannis et al., 2023), as those values were expected to show more significant performance deviations for differ- ent sparse matrices. Specifically, we used 4 values for row panels {4, 32, 256, 2048}, 4 values for column pan- els {1024, 16384, 65536, NUM MATRIX COLS} (where NUM MATRIX COLS depends on the input matrix) and 2 values for the split factor {32, 256}. Although COGNATE is designed to perform under limited data availability, we conducted extensive data collection to rigorously evaluate and justify its effectiveness. This in- cluded a range of experiments and ablation studies, some of which required performance data samples from up to 1,000 matrices for training. Altogether, this effort demanded ap- proximately 4 million CPU hours. Despite the constrained nature of the search space (256 program configurations), it took nearly three months to complete the dataset cura- tion, even though the simulations were parallelized across multiple machines. Hence, exhaustively evaluating a larger, unconstrained program configuration space would be com- putationally infeasible. This underscores the need for data- efficient methods like COGNATE, which are designed to operate effectively even under limited data availability. Baselines and Implementation. We executed SpMM and SDDMM on CPU, GPU, and SPADE using the respective programming systems introduced in Section 2.2. We used the default optimizations of these programming systems as our baselines. We implemented COGNATE in PyTorch, utilizing MinkowskiEngine (Choy et al., 2019) to handle sparse convolution. Separate models were developed for SpMM and SDDMM to conduct precise performance pre- dictions. We focused on these two sparse operations because they are the only operations currently supported natively by both the SPADE accelerator (Gerogiannis et al., 2023) and the SparseTIR framework (Ye et al., 2023). Cost Model Evaluation. We evaluated COGNATE s per- formance on 715 real-world matrices from the SuiteSparse Matrix Collection, ensuring that none of the evaluation data samples overlapped with the training set. For each matrix, we predicted the runtime cost across all program configura- tions and selected the top-1 and top-5. Then we executed the sparse operations with the selected program configurations on the target platform and identified the one with the short- est runtime. We then compared our results to the normalized runtime of the baseline executions, WacoNet with feature augmentation, and WacoNet with feature mapping by cal- culating the geometric mean (geomean) speedups across matrices to quantify COGNATE s overall effectiveness. Pre-training and Fine-tuning Procedure. The matrices for pre-training were randomly selected from the training set while ensuring a balanced representation of their dimen- sions and sparsity. To achieve this, we first grouped the matrices into five bins based on the number of rows: 8192, 32,768, 65,536, 131,072, and 131,072. From each group, we randomly sampled matrices, ensuring the selected subset collectively spanned a diverse range of sparsity lev- els. We empirically demonstrate in Section 4.4 (Figure 11) that training the source model with 100 matrices strikes a good balance. We use this setting for our headline result (Figure 4). Once the source model was pre-trained, we performed few-shot fine-tuning on accelerators using data samples from only 5 matrices. This choice was guided by empirical observations, aiming to strike a good balance be- tween transfer learning effectiveness and the cost of data collection. As shown in Section 4.4 (Figure 12), this set- ting offers the best trade-off between our objectives for accuracy and data collection (detailed in Appendix A). Fur- ther, the same set of matrices were used for evaluating the non-transfer learning baselines, enabling consistent and fair comparisons across all experimental settings. 4.2. Transferability to SPADE 0 100 200 300 400 500 600 700 Matrix 1 2 3 4 5 Geomean Speedup Speedup Optimal: 1.55 COGNATE (Top-5): 1.40 Baseline: 1.0 Figure 5: COGNATE per-matrix speedups (SpMM). Figure 4 illustrates the geomean speedups achieved using multiple techniques: zero-shot inference from the source model (zero-shot), a model trained exclusively on the target hardware using the fine-tuning dataset (no transfer), Wa- coNet with feature augmentation (WACO FA), WacoNet with feature mapping (WACO FM), and COGNATE s per- formance for both the top-1 and top-5 (k-best) predicted program configurations. Our results show that COGNATE consistently outperformed other techniques across both sparse operations and hardware platforms. Specifically for SPADE, COGNATE (Top-1) achieved an average speedup of 1.40 for SpMM, reaching 90 of the optimal speedup of 1.55 . When expanding COGNATE (Top-5), it deliv- ered an average speedup of 1.47 , achieving 95 of the 7 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning optimal speedup. Note that the optimal speedup was deter- mined by exhaustively evaluating all program configurations within the defined constrained search space for each matrix in the test set, and selecting the fastest configuration per matrix to compute the geometric mean. Similarly, for SD- DMM in SPADE, COGNATE (Top-1) achieved an average speedup of 1.27 and COGNATE (Top-5) achieved an aver- age speedup of 1.39 . This emphasizes COGNATE s ability to consistently find near-optimal program configurations with minimal fine-tuning across multiple sparse operations. The speedup gained for zero-shot inference from the source model was significantly lower than the baseline. In contrast, fine-tuning on a few data samples from SPADE led to signifi- cant performance gains showing COGNATE s effectiveness in knowledge transfer. 0 5 10 15 20 25 30 Epochs 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Accuracy Loss Train Loss(PRL) Kendall's TAU Validation Loss(PRL) Ordered Pair Accracy(OPA) Figure 6: Loss and accuracy during training. 4.3. Transferability to GPU COGNATE is generalizable and is not only applicable to one target hardware platform. To showcase COGNATE s capability, we extended our evaluation to a GPU acceler- ator (NVIDIA A100) (Figure 4). The speedup trends on GPU aligned with those observed on SPADE, reinforcing the effectiveness of COGNATE. COGNATE (Top-1) de- livered an average speedup of 1.03 and COGNATE (Top- 5) yielded an average speedup of 1.17 for SpMM, with the optimal achievable speedup being 1.25 . In compari- son, cuSPARSE SpMM (Naumov et al., 2010) achieved a lower average speedup of 1.01 . For SDDMM, COGNATE (Top-1) resulted in an average speedup of 1.07 , while COGNATE (Top-1) yielded a 1.15 speedup, with the op- timal being 1.22 . Zero-shot inference on the GPU was significantly worse compared to Zero-shot for SPADE, with speedups falling well below the baseline. This discrepancy is likely due to the inherent architectural differences be- tween the CPU and GPU architectures. Further, to assess COGNATE s scalability, we conducted preliminary exper- iments on an end-to-end GNN workload on GPU. Using the transient sparse matrix from our test set (178,866 rows columns (nodes), 961,368 non-zeros) and GraphSAGE model configured with three hidden layers and 256 hidden features, COGNATE achieved a 1.30 speedup for inference and a 1.28 speedup for training over the default SparseTIR (Ye et al., 2023) implementation. These results demon- strate the potential of COGNATE to scale effectively on real-world workloads and deliver consistent performance. Comparison with Other Transfer Learning Techniques. For comparisons, we modified WacoNet to support feature augmentation and feature mapping, as it is not inherently optimized for heterogeneous transfer. Despite these modi- fications, COGNATE consistently outperformed both. For SpMM on SPADE, WACO FA had an average speedup of 1.04 , while WACO FM resulted in a slightly higher average speedup of 1.09 . In comparison, COGNATE deliv- ered an average speedup of 1.40 , outperforming its closest alternative (WACO FM) by 28.44 . The sub-optimal per- formance of WACO FA and WACO FM can be attributed to the increased sparsity in the feature space due to fea- ture augmentation and their limited capacity to effectively mitigate the heterogeneity. 1.0 1.2 1.4 1.6 Geomean Speedup 1.01 1.16 1.26 1.40 1.55 w o LE w o FM w o IFE COGNATE Optimal Figure 7: Ablation study of the model components. 1.0 1.2 1.4 1.6 Geomean Speedup 1.321.341.36 1.40 1.55 LSTM GRU TF COGNATE Optimal Figure 8: Design choices of the predictor. 4.4. Additional Experiments for SpMM on SPADE Speedup Performance. Figure 5 shows the speedups achieved by COGNATE (Top-1) across all evaluated matri- ces. Matrices where the baseline outperformed COGNATE are indicated below the y 1 dotted line. While the baseline outperformed COGNATE on a few matrices, the overall results demonstrate that COGNATE delivered substantial speedups (as high as 5.46x) for the majority of the dataset. Cost Model Accuracy. Figure 6 shows the accuracy of COGNATE s cost model across training epochs using Pair- wise Ranking Loss (PRL), Ordered Pair Accuracy (OPA), and Kendall s Tau (K-Tau). The steady decline in PRL for both training and validation loss indicates that the model effectively learns to rank program configurations without over-fitting. OPA and K-Tau steadily improved to 0.80 and 0.61, indicating effective training. Component-Level Contributions. The effectiveness of our cost model relies on the inclusion of all components, each contributing uniquely to the overall performance. As illus- trated in Figure 7, the exclusion of individual components leads to a noticeable decline in speedups. For example, ex- cluding the input featurizer (IFE) causes a decline from 1.40x to 1.26x. Similarly, omitting the configuration map- per (FM) leads to a further decline to 1.16x, and excluding latent encoder (LE) lowers speedup to 1.01x. This em- phasizes that each component contributes uniquely to the model s high performance, and all need to act synergistically to maximize the benefits of knowledge transfer. 8 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning 1.0 1.2 1.4 1.6 Geomean Speedup 1.01 1.04 1.31 1.40 1.55 VAE FA PCA COGNATE Optimal Figure 9: Selection of auto- encoders for COGNATE. 1.0 1.2 1.4 1.6 Geomean Speedup 1.29 1.38 1.43 1.40 1.55 NT 5 NT 100 NT 1000 COGNATE Optimal Figure 10: Data overhead w o transfer learning. Selection of MLP Predictor. As shown in Figure 3, the MLP predictor from WACO s base cost model was retained in our enhanced design. Figure 8 provides a comparative analysis of alternative predictors, including LSTM, GRU, and Transformer (TF). The results demonstrate that our proposed cost model design outperforms the alternatives, with the TF predictor achieving the next best performance with 1.36 speedup. These findings highlight that an MLP predictor is sufficient to deliver robust performance with limited data. In contrast, the suboptimal performance of the TF predictor can be attributed to the limited dataset, as the high simulation costs associated with emerging hardware make it challenging to collect larger datasets for fine-tuning. Selection of Auto-Encoders. Figure 9 shows our inves- tigation into various methods for handling the heteroge- neous components of program configurations. We evaluated choices ranging from conventional feature augmentation (FA) to principal component analysis (PCA), auto-encoders, and variational auto-encoders (VAE). Our findings reveal that auto-encoders were the most effective for capturing het- erogeneous optimizations in a latent space. This was evident from the lower validation loss observed during the training of the auto-encoders to learn the latent representations. Data Collection Overhead w o Transfer Learning. Figure 10 shows that without transfer learning, the overhead of data collection becomes significant on emerging hardware due to the high costs of running simulations. For example, models trained exclusively on SPADE would require 20 200 more target data samples (collected using 100 1000 matrices) to match or surpass the speedups achieved through COGNATE via transfer learning. 1.0 1.2 1.4 1.6 Geomean Speedup 1.07 1.21 1.40 1.36 1.19 1.55 CPU 5 CPU 20 COGNATE CPU 500 CPU 1000 Optimal Figure 11: Impact of nega- tive transfer for fine-tuning. 1.0 1.2 1.4 1.6 Geomean Speedup 1.00 1.401.411.421.43 1.55 Baseline COGNATE TL 100 TL 1000 NT 1000 Optimal Figure 12: Impact of number of samples for fine-tuning. Impact of Negative Transfer. Figure 11 shows that using a large dataset to train the source model (e.g., data samples from 1000 matrices) does not necessarily lead to better out- comes. As the size of the training dataset increases, the model becomes overly specialized to the source platform, diminishing its adaptability during fine-tuning. To inves- tigate this effect, we trained source models on datasets of varying sizes (5, 20, 100, 500, and 1,000 matrices) and eval- uated their transferability to our target platform (SPADE) using few-shot fine-tuning on just 5 matrices (Figure 11). Our results show that training on the CPU (source) with data samples from 100 matrices and fine-tuning on SPADE (target) with data samples from 5 matrices produces the best results. In contrast, training the source model with data from 1,000 matrices yields sub-optimal performance due to overfitting to source-specific characteristics. This highlights the importance of carefully selecting the size of the source training dataset to avoid over-specialization and minimize the impact of negative transfer. Number of Samples in Fine-Tuning. In Figure 12, we show COGNATE s performance as fine-tuning data samples increase. Despite fine-tuning on data from 1,000 matrices, the maximum speedup saturates at 1.42 . We can achieve a comparable speedup of 1.40 with data from 5 matrices, which shows the diminishing returns associated with larger datasets. Further, the non-transfer learning setup achieved a marginally higher speedup of 1.43 when using data from 1,000 matrices. However, considering the significant data collection overhead, these marginal improvements are not practically justifiable. To further assess sensitivity to the size of the fine-tuning dataset, we conducted additional ex- periments using 3 and 7 matrices. The resulting speedups were 1.30 and 1.41 , respectively, compared to 1.40 with 5 matrices. While using only 3 matrices led to a noticeable drop in performance, increasing to 7 did not yield a sig- nificant gain but required substantially more data samples, incurring several days of additional data collection time. These results suggest that using 5 matrices strikes a practi- cal balance between data collection cost and performance, while demonstrating that COGNATE is relatively robust to small variations in dataset size. 5. Conclusion In this paper, we introduced COGNATE, a novel frame- work to develop data-frugal learned cost models to optimize sparse tensor programs for emerging hardware platforms. COGNATE leverages a unique technique that capitalizes on the homogeneity of input features across different platforms while effectively mitigating heterogeneity. This enables COGNATE to train cost models using low-cost data sam- ples from widely accessible general-purpose hardware (such as CPUs) and then fine-tune them for emerging hardware platforms with few-shot learning. Our results demonstrate that COGNATE is able to achieve near-optimal accuracy while maintaining significant sample efficiency. 9 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Acknowledgements The work is partially supported by the National Science Foundation Graduate Research Fellowship, ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Cor- poration (SRC) program sponsored by DARPA, by NSF un- der the grants CCF-2338739 and CCF-2316233, by DARPA under the grant D24AP00295-00 and by generous gifts from Qualcomm. Any opinion, findings, and conclusions or rec- ommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the NSF or DARPA. Impact Statement The goal of this work is to accelerate the execution of sparse tensor programs in the domain of emerging sparse accel- erators through the application of machine learning-based techniques. Experiments demonstrate that our approach exhibits potential in benefiting early-stage accelerator devel- opment by enabling data-efficient design space exploration. There may be potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Aananthakrishnan, S., Abedin, S., Cav e, V., Checconi, F., Du Bois, K., Eyerman, S., Fryman, J. B., Heirman, W., Howard, J., Hur, I., et al. The intel programmable and integrated unified memory architecture graph analytics processor. IEEE Micro, 43(5):78 87, 2023. Adams, A., Ma, K., Anderson, L., Baghdadi, R., Li, T.- M., Gharbi, M., Steiner, B., Johnson, S., Fatahalian, K., Durand, F., et al. Learning to optimize halide with tree search and random programs. ACM Transactions on Graphics (TOG), 38(4):1 12, 2019. Baghdadi, R., Merouani, M., Leghettas, M.-H., Abdous, K., Arbaoui, T., Benatchba, K., et al. A deep learning based cost model for automatic code optimization. Proceedings of Machine Learning and Systems, 3:181 193, 2021. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Bozinovski, S. Reminder of the first paper on transfer learn- ing in neural networks, 1976. Informatica, 44(3), 2020. Chen, T. and Guestrin, C. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd inter- national conference on knowledge discovery and data mining, pp. 785 794, 2016. Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan, M., Wang, L., Hu, Y., Ceze, L., et al. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Sys- tems Design and Implementation (OSDI 18), pp. 578 594, 2018a. Chen, T., Zheng, L., Yan, E., Jiang, Z., Moreau, T., Ceze, L., Guestrin, C., and Krishnamurthy, A. Learning to opti- mize tensor programs. Advances in Neural Information Processing Systems, 31, 2018b. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choy, C., Gwak, J., and Savarese, S. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition, pp. 3075 3084, 2019. Dao, T., Chen, B., Liang, K., Yang, J., Song, Z., Rudra, A., and Re, C. Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint arXiv:2112.00029, 2021. Daum e III, H. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009. Davis, T. A. and Hu, Y. The university of florida sparse matrix collection. ACM Transactions on Mathematical Software (TOMS), 38(1):1 25, 2011. Day, O. and Khoshgoftaar, T. M. A survey on heterogeneous transfer learning. Journal of Big Data, 4:1 42, 2017. Duan, L., Xu, D., and Tsang, I. Learning with augmented features for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660, 2012. Gerogiannis, G., Yesil, S., Lenadora, D., Cao, D., Mendis, C., and Torrellas, J. Spade: A flexible and scalable ac- celerator for spmm and sddmm. In Proceedings of the 50th Annual International Symposium on Computer Ar- chitecture, ISCA 23, New York, NY, USA, 2023. Associ- ation for Computing Machinery. ISBN 9798400700958. doi: 10.1145 3579371.3589054. URL org 10.1145 3579371.3589054. Gerogiannis, G., Aananthakrishnan, S., Torrellas, J., and Hur, I. Hottiles: Accelerating spmm with heterogeneous accelerator architectures. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 1012 1028. IEEE, 2024. Graham, B. and Van der Maaten, L. Submanifold sparse con- volutional networks. arXiv preprint arXiv:1706.01307, 2017. 10 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Hegde, K., Asghari-Moghaddam, H., Pellauer, M., Crago, N., Jaleel, A., Solomonik, E., Emer, J., and Fletcher, C. W. Extensor: An accelerator for sparse tensor algebra. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, pp. 319 333, 2019. Hong, C., Sukumaran-Rajam, A., Nisa, I., Singh, K., and Sadayappan, P. Adaptive sparse tiling for sparse matrix multiplication. In Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming, pp. 300 314, 2019. Jiang, P., Hong, C., and Agrawal, G. A novel data trans- formation and execution strategy for accelerating sparse matrix multiplication on gpus. In Proceedings of the 25th ACM SIGPLAN symposium on principles and practice of parallel programming, pp. 376 388, 2020. Jin, H., Yue, Z., Zhao, Z., Du, Y., Deng, C., Srivastava, N., and Zhang, Z. Vesper: A versatile sparse linear algebra accelerator with configurable compute patterns. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. Kaufman, S., Phothilimthana, P., Zhou, Y., Mendis, C., Roy, S., Sabne, A., and Burrows, M. A learned performance model for tensor processing units. Proceedings of Ma- chine Learning and Systems, 3:387 400, 2021. Kjolstad, F., Kamil, S., Chou, S., Lugato, D., and Ama- rasinghe, S. The tensor algebra compiler. Proc. ACM Program. Lang., 1(OOPSLA), oct 2017. doi: 10.1145 3133901. URL 1145 3133901. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207 1216, Stan- ford, CA, 2000. Morgan Kaufmann. Li, Z., Li, J., Chen, T., Niu, D., Zheng, H., Xie, Y., and Gao, M. Spada: Accelerating sparse matrix multiplication with adaptive dataflow. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pp. 747 761, 2023. Liang, H., Fu, W., and Yi, F. A survey of recent advances in transfer learning. In 2019 IEEE 19th International Con- ference on Communication Technology (ICCT), pp. 1516 1523, 2019. doi: 10.1109 ICCT46805.2019.8947072. Mu noz-Mart ınez, F., Garg, R., Pellauer, M., Abell an, J. L., Acacio, M. E., and Krishna, T. Flexagon: A multi- dataflow sparse-sparse matrix multiplication accelerator for efficient dnn processing. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Vol- ume 3, pp. 252 265, 2023. Naumov, M., Chien, L., Vandermersch, P., and Kapasi, U. Cusparse library. In GPU Technology Conference, vol- ume 12, 2010. Neyshabur, B., Sedghi, H., and Zhang, C. What is being transferred in transfer learning? Advances in neural information processing systems, 33:512 523, 2020. Pal, S., Beaumont, J., Park, D.-H., Amarnath, A., Feng, S., Chakrabarti, C., Kim, H.-S., Blaauw, D., Mudge, T., and Dreslinski, R. Outerspace: An outer product based sparse matrix multiplication accelerator. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 724 736. IEEE, 2018. Ryu, J. and Sung, H. Metatune: Meta-learning based cost model for fast and efficient auto-tuning frameworks. arXiv preprint arXiv:2102.04199, 2021. Sasaki, Y., Takahashi, K., Shimomura, Y., and Takizawa, H. A cost model for compilers based on transfer learning. In 2022 IEEE International Parallel and Distributed Pro- cessing Symposium Workshops (IPDPSW), pp. 942 951. IEEE, 2022. Shen, Z., Liu, Z., Qin, J., Savvides, M., and Cheng, K.-T. Partial is better than all: Revisiting fine-tuning strategy for few-shot learning. In Proceedings of the AAAI confer- ence on artificial intelligence, volume 35, pp. 9594 9602, 2021. Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of transfer learning. Journal of Big data, 3:1 40, 2016. Won, J., Mendis, C., Emer, J. S., and Amarasinghe, S. Waco: Learning workload-aware co-optimization of the format and schedule of a sparse tensor program. In Proceedings of the 28th ACM International Confer- ence on Architectural Support for Programming Lan- guages and Operating Systems, Volume 2, ASPLOS 2023, pp. 920 934, New York, NY, USA, 2023. Associa- tion for Computing Machinery. ISBN 9781450399166. doi: 10.1145 3575693.3575742. URL org 10.1145 3575693.3575742. Yang, H., Liu, Y., Luan, Z., Gan, L., Yang, G., and Qian, D. Input-aware sparse tensor storage format selection for optimizing mttkrp. Computer, 56(08):4 7, aug 2023. ISSN 1558-0814. doi: 10.1109 MC.2023.3279447. 11 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Ye, Y. and Ji, S. Sparse graph attention networks. IEEE Transactions on Knowledge and Data Engineering, 35 (1):905 916, 2021. Ye, Z., Lai, R., Shao, J., Chen, T., and Ceze, L. Sparse- tir: Composable abstractions for sparse compilation in deep learning. ASPLOS 2023, pp. 660 678, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450399180. doi: 10.1145 3582016.3582047. URL 1145 3582016.3582047. Zhai, Y., Zhang, Y., Liu, S., Chu, X., Peng, J., Ji, J., and Zhang, Y. Tlp: A deep learning-based cost model for tensor program tuning. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pp. 833 845, 2023. Zheng, L., Jia, C., Sun, M., Wu, Z., Yu, C. H., Haj-Ali, A., Wang, Y., Yang, J., Zhuo, D., Sen, K., et al. Ansor: Gen- erating {High-Performance} tensor programs for deep learning. In 14th USENIX symposium on operating sys- tems design and implementation (OSDI 20), pp. 863 879, 2020. Zheng, L., Liu, R., Shao, J., Chen, T., Gonzalez, J. E., Sto- ica, I., and Ali, A. H. Tenset: A large-scale program performance dataset for learned tensor compilers. In Thirty-fifth Conference on Neural Information Process- ing Systems Datasets and Benchmarks Track (Round 1), 2021. URL id aIfp8kLuvc9. Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43 76, 2020. 12 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning A. Problem Formulation In this work, our aim is to build accurate learned cost models for emerging hardware platforms to enable faster identification of optimal program configurations. A key challenge is the need to maximize the accuracy of the cost model (accuracy objective) while using as few expensive (i.e. collected through simulation) data samples as possible (data collection objective). We first formalize the program optimization objective and then tie it with the cost model objectives. A.1. Program Optimization Selection The goal of program optimization in sparse tensor programs is to select the optimal program configuration for a given hardware platform and input sparsity pattern from the total configuration space. Let configuration space Ca be the set {c1, c2, . . . , cma} of all valid program configurations for a given hardware platform a (ma Z ). For example, for CPU, a valid configuration from CCP U is a tuple of program configuration parameters for loop strip-mining, loop reordering, and format reordering (Table 1). The optimal program configuration minimizes the execution time of a sparse tensor program. For an input sparse matrix (sparsity pattern) M, the optimal program configuration on platform a can be given as, c arg minci Ca Ta(M, ci), where Ta is the execution time function for platform a (ground truth runtime). The execution time for the optimal program configuration is given by t Ta(Ml, c ). A.2. Cost Model Performance and Data Efficiency Objectives We approximate the ground truth runtime Ta using learned cost models. Usually, these cost models are trained with one objective: to achieve high accuracy. However, due to the high cost of simulation in emerging hardware, we also want to minimize the amount of data samples required from these platforms for model training. We formalize these two objectives as follows. Data Collection Objective (DCE). Let Da {(Ml, ci), ti i ma , l Z } be the dataset collected from hardware platform a, and let βa represent the average cost of collecting a single data sample from the platform. Our objective is to minDa βa Da . Accuracy Objective. Let CMa (which approximates Ta) be the learned cost model trained on dataset Da. If the best program configuration returned by the cost model (c CMa) has an actual execution time t CMa, our objective is to min t CMa t , where t is the execution time for the optimal configuration. For a set of input sparse matrices {M1, M2, . . . , Mk}, our objective can be extended to minimizing the Absolute Percentage Error (APE) across all matrices: APE 1 k k X l 1 t CMa,Ml t Ml t Ml 100 where t CMa,Ml denotes the execution times for the predicted best program configuration for the input sparse matrix Ml and t Ml denote the optimal program configurations for the same matrix. A.3. Evaluations for Cost Model Objectives To evaluate the cost model objectives, we conducted the following experiments for SpMM on SPADE. For simplicity in the calculations, we set βCPU 1 and βSPADE 1000. However, a CPU execution typically takes milliseconds, while a SPADE execution can extend up to two weeks. We explored 11 distinct models across four different categories, differentiated by the number of data samples they were trained on, while the cost model architecture remained the same. Category I consists of models (NT d) trained exclusively on data samples from d matrices executed on SPADE. Category II includes transfer-learned models (TL d), which were pre-trained with data samples from 100 matrices on CPU (10,000 data samples) and then fine-tuned on SPADE with data samples from d matrices. Category III consists of models (CPU d) pre-trained with varying numbers of data samples from d matrices on CPU and then fine-tuned on data samples from 5 matrices on SPADE. Finally, we did zero-shot inference (Zero-Shot) from a model pre-trained on CPU with data samples from 100 matrices without additional fine-tuning on SPADE. Models trained exclusively on SPADE data samples (NT d) generally exhibit increasing speedup and decreasing APE as the number of SPADE data samples increases. For example, NT 1000, trained on 100,000 SPADE data samples, achieves the highest speedup of 1.43 and an APE of 7.06. However, the data collection overhead for these models rises significantly with the number of SPADE samples, making the use of them impractical due to the long simulation times. In contrast, the TL 13 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning models, which are pre-trained on CPU data and fine-tuned on SPADE samples, demonstrate an excellent balance between speedup, APE, and DCE. TL 5 model, for instance, delivers a competitive speedup of 1.40 and a low APE of 7.28, while maintaining an excellent DCE of 0.51. Model Data Samples COGNATE (Top-1) Speedup APE DCE 106 CPU SPADE NT 5 - 500 1.29 15.02 0.50 NT 100 - 10000 1.38 9.42 10.00 NT 1000 - 100000 1.43 7.06 100.00 TL 5 (CPU 100) 10000 500 1.40 9.58 0.51 TL 100 10000 10000 1.41 8.74 10.01 TL 1000 10000 100000 1.42 7.28 100.01 CPU 5 500 500 1.07 27.80 0.50 CPU 20 2000 500 1.21 19.35 0.50 CPU 500 50000 500 1.36 16.34 0.55 CPU 1000 100000 500 1.19 36.00 0.60 Zero-Shot (CPU) 10000 - 0.71 46.22 0.01 Table 2: Comparison of cost model performance with varying data samples from CPU and SPADE. A.4. Learning Objective Our objective is to train a cost model that effectively learns to identify a program configuration that minimizes the runtime of a sparse operation for a given sparsity pattern. To achieve this, we begin by training our cost model to learn the relative rankings of program configurations during execution, enabling it to accurately identify optimal configurations based on their performance. This objective improves robustness to noise and runtime scale variance, which are common in early-stage accelerator performance data, as the model focuses on preserving relative orderings. This also enables us to efficiently integrate our cost model with a search technique to efficiently select the top-k (k-best) program configurations from the configuration space. Furthermore, prior work (Kaufman et al., 2021) has shown that training with ranking loss significantly improves a model s ability to identify optimal configurations. We use the pairwise ranking loss as our learning objective (implemented using margin ranking loss) to rank program configurations based on their true performance differences. For a given input matrix M, the pairwise ranking loss (L) across all program configuration pairs can be defined as L P (c1,c2) max(0, 1 (ˆrM,c1 ˆrM,c2)) δtrue; δtrue sign(tM,c1 tM,c2) where ˆrM,c1 and ˆrM,c2 are the predicted scores for configurations c1 and c2, respectively; tM,c1 and tM,c2 represent their actual runtimes; and δtrue signifies the true performance difference where sign(x) returns 1 if x 0, -1 if x 0, and 0 if x 0. This ensures that the model is penalized when the predicted ranking does not align with the true ranking. By minimizing this loss (L), COGNATE improves its ability to accurately rank and identify the top-k program configurations. This also contributes to achieving our accuracy objective (Section A.2). B. Code Optimizations Across Hardware Platforms Loop strip-mining: Breaks down large software loops into smaller segments to optimize cache utilization. Loop reordering: Adjusts the execution order of loops to improve cache efficiency. Typically, it is applied after loop strip-mining. Format reordering: Reorganizes the data structure layout of sparsity patterns in memory to optimize memory access patterns Parallelization: Distributes tensor computations across multiple threads or processors to run tasks simultaneously. Loop binding: Assigns specific loop iterations to threads for parallel processing. Loop unrolling: Executes multiple iterations of a loop in a single iteration, reducing loop control overhead and boosting execution speed. 14 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Tiling: Decomposes a matrix into smaller blocks to optimize data reuse in the local memory and improve cache efficiency. Barrier: Applying a barrier would ensure all threads finish processing their current tile (synchronized) before progressing to the next stage. Cache bypassing: Capability of bypassing caches to to reduce cache pollution. Matrix reordering: Enhances data locality by reordering the input matrix. C. Generalizability Our approximated code mappings are designed to enable well-established code optimizations that remain effective across emerging hardware platforms. For instance, we expect loop transformations (e.g. loop strip-mining, loop reordering, tiling, etc) to be implemented regardless of the underlying hardware platform although the exact implementation may be slightly different. Any hardware-specific optimizations (code optimizations that cannot be mapped) are included in the heterogeneous component using the latent encoder. Since the dimension of the latent embedding is fixed, we can effectively finetune using an already pre-trained model. To elaborate, let us have a qualitative discussion and explore the intuition behind incorporating approximate mapping of these loop transformations into sparse accelerators, using Intel PIUMA (Gerogiannis et al., 2024; Aananthakrishnan et al., 2023) and Vesper (Jin et al., 2024) as examples. These mappings are conceptually aligned with those we applied to SPADE and GPU, highlighting the general applicability of our approach across diverse hardware backends. Intel PIUMA (Gerogiannis et al., 2024; Aananthakrishnan et al., 2023) is a configurable accelerator that has a RISC ISA making it CPU-programmable. This enables it to employ code optimizations that are available in CPUs. Hence, it is possible to implement SpMM and SDDMM sparse operations (kernels) with code optimizations such as loop reordering with a one-to-one mapping. Similarly, loop strip-ming and tiling can be mapped. However, similar to SPADE, where we accounted for the barrier optimization in the mapping process, one would need to consider the PIUMA scratchpad reuse . Vesper (Jin et al., 2024) is another reconfigurable accelerator designed for sparse computations, supporting three dataflow models implemented through distinct loop traversal orders. While the authors refer to the use of tiling, they do not provide implementation details, source code, or a description of the tile size selection mechanism. Based on standard tiling practices, we can approximate Vesper s approach using loop stripping and loop reordering within our representation. Intel PIUMA is proprietary, and Vesper s source code was not available. This made it infeasible to test our hypothesis on these accelerators. Hence, our current evaluation focuses only on two examples (SPADE and NVIDIA A100) primarily due to practical constraints. However, it should be emphasized that COGNATE was designed with hardware-agnostic principles in mind. We believe that as a wider range of accelerators becomes accessible to the research community, and as sparse compilation frameworks like SparseTIR (Ye et al., 2023) evolve, COGNATE can be extended with minimal changes. Assuming these accelerators were available, the data collection process would still be highly time-consuming, likely requiring millions of machine hours to gather sufficient data for training, validation, and testing. For example, collecting performance data or training, validation, and testing across all matrices and experimental settings for SPADE required approximately 4 million CPU hours. Despite parallelizing experiments across multiple machines, each with 64 CPU cores, this process spanned nearly three months. While extending the evaluation to additional hardware platforms remains an important direction, it was beyond the practical scope of this work given resource constraints. Further, frequent changes in emerging hardware may require updates to configuration mappings and fine-tuning, this challenge is significantly mitigated by our approach in COGNATE. As long as the entire kernel does not change or the newly introduced optimizations are heterogeneous, updating the mappings is relatively straight forward. However, relying solely on simulations would require rerunning them for a large number of configurations each time a change is made, resulting in significant computational and time costs. In contrast, our transfer learning-based approach significantly reduces the cost and time of running simulations. By collecting only a few data samples and fine-tuning the model, we can efficiently adapt to hardware changes without the need for extensive simulations. Hence, this approach not only reduces maintenance complexity but also accelerates the design process, making it more feasible to handle frequent and timely updates in emerging hardware. 15 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning D. Cost Models for Early-Stage Sparse Accelerator Design With the flexibility of recent accelerators to have software programmable kernels (Gerogiannis et al., 2023; Jin et al., 2024; Gerogiannis et al., 2024), integration of cost models and heuristics into the DSE pipeline has become an up and coming area (Gerogiannis et al., 2024; Jin et al., 2024). For example, Vesper is a recent work that had integrated an analytical model to a configurable sparse accelerator to enable higher throughput (Jin et al., 2024). HotTiles is another work that uses an analytical model to predict the performance of different accelerator processing elements (PEs) that accommodate intra-matrix heterogeneity (Gerogiannis et al., 2024). Further, in HotTiles, the authors acknowledged that a more accurate model could have enabled making better design decisions during the early stages. We believe that our proposed data-driven cost model framework, COGNATE, addresses this gap (resulting in speedups close to optimal) while complementing expert-driven strategies to enable more informed and better design decisions. This would effectively replace the analytical approaches with a data driven approach. The primary overhead associated with our approach arises from the need to gather data points to fine-tune the cost model. This overhead is minimal compared to the effort required for an expert to iteratively optimize a kernel for sparse workloads, where kernel performance is highly input-sensitive due to diverse sparsity patterns. E. An Example of Code Optimization Notations Used in Approximate Mappings Here, we provide an additional explanation and an illustrative example to clarify the notation used in the code op- timization mapping functions presented in Section 3.2. These are designed to approximate how high-level schedule configurations in SPADE are translated into low-level loop representations in CPU. The following example demon- strates how a sparse matrix-matrix multiplication (SpMM) configuration in SPADE is mapped into its corresponding loop-level representation using the defined notation. Consider the following high-level configuration for the SpMM oper- ation in SPADE: name, row panels, column panels, split, barrier, bypass, reorder, time 144, 4, 1024, 1, 0, 0, 0, 38.83592. Here, row panels, column panels, and split define the tiling strategy, while the binary flags barrier, bypass, and reorder indicate the use of additional code op- timizations. Using our mapping functions, this configuration is mapped into the following loop-level represen- tation: name, i split, j split, k split, loop 1, ..., loop 7, barrier, bypass, reorder, time 144, 4, 1024, 32, 6, 7, 2, 4, 1, 3, 5, 0, 0, 0, 38.83592. In this mapped form, the tiling parameters are converted to i split, j split, and k split, which define how the loop indices are partitioned across the three dimensions. The sequence loop 1 through loop 7 encodes the execution order of the nested loops, and the binary flags are retained to preserve platform-specific scheduling decisions. This example demonstrates how our framework captures key aspects of tiling structure, loop ordering, and other scheduling optimizations. F. Hyperparamters Table 3: Hyperparameters for model training fine-tuning Hyperparameter Value Learning Rate 0.0001 Batch Size 32 Optimizer Adam Number of Epochs 100 Loss Function MarginRankingLoss Table 4: Hyperparameters for the autoencoders Hyperparameter Value Learning Rate 0.001 Batch Size 32 Optimizer Adam Number of Epochs 1000 Loss Function MSE 16 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Table 5: Composition of layers in the Input Featurizer (IFE) Layer Description Layer 1 MinkowskiConvolution (in channels, 32, kernel size 5) Layer 2 MinkowskiConvolution (32, 32, kernel size 3) Layer 3 MinkowskiConvolution (32, 64, kernel size 3) MinkowskiMaxPooling Layer 4 MinkowskiConvolution (64, 64, kernel size 3) Layer 5 MinkowskiConvolution (64, 64, kernel size 3) Layer 6 MinkowskiConvolution (64, 128, kernel size 3) MinkowskiMaxPooling Layer 7 MinkowskiConvolution (128, 128, kernel size 3) Layer 8 MinkowskiConvolution (128, 128, kernel size 3) Layer 9 MinkowskiConvolution (128, 256, kernel size 3)MinkowskiMaxPooling Layer 10 MinkowskiConvolution (256, 256, kernel size 3) Layer 11 MinkowskiConvolution (256, 256, kernel size 3) Layer 12 MinkowskiConvolution (256, 256, kernel size 3) Global Pooling Layer MinkowskiGlobalAvgPooling Table 6: Composition of layers in the Predictor (P) Component Layer Input Size Output Size Matrix Embedding (x) 128 128 Configuration Embedding (y) 53 64 Latent Embedding (z) 64 64 Concatenation (xyz) 128 64 192 Predictor Layer 1 192 128 Predictor Layer 2 128 64 Predictor Layer 3 64 1 17 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning G. Additional Results 0 100 200 300 400 500 600 700 Matrix 1 2 3 4 5 Geomean Speedup Speedup Optimal: 1.55 COGNATE (Top-5): 1.47 Baseline: 1.0 Figure 13: COGNATE (Top-5) per-matrix speedups (SpMM) 0 100 200 300 400 500 600 700 Matrix 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Geomean Speedup Speedup Optimal: 1.44 COGNATE (Top-1): 1.27 Baseline: 1.0 Figure 14: COGNATE (Top-1) per-matrix speedups (SDDMM) 0 100 200 300 400 500 600 700 Matrix 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Geomean Speedup Speedup Optimal: 1.44 COGNATE (Top-5): 1.39 Baseline: 1.0 Figure 15: COGNATE (Top-5) per-matrix speedups (SDDMM) 18\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2506.00424v2 [cs.LG] 15 Jun 2025 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Chamika Sudusinghe 1 Gerasimos Gerogiannis 1 Damitha Lenadora 1 Charles Block 1 Josep Torrellas 1 Charith Mendis 1 Abstract Sparse tensor programs are essential in deep learn- ing and graph analytics, driving the need for op- timized processing. To meet this demand, spe- cialized hardware accelerators are being devel- oped. Optimizing these programs for acceler- ators is challenging for two reasons: program performance is highly sensitive to variations in sparse inputs, and early-stage accelerators rely on expensive simulators. Therefore, ML-based cost models used for optimizing such programs on general-purpose hardware are often ineffec- tive for early-stage accelerators, as they require large datasets for proper training. To this end, we introduce COGNATE, a novel framework that leverages inexpensive data samples from general- purpose hardware (e.g., CPUs) to train cost mod- els, followed by few-shot fine-tuning on emerging hardware. COGNATE exploits the homogeneity of input features across hardware platforms while effectively mitigating heterogeneity, enabling cost model training with just 5 of the data samples needed by accelerator-specific models to achieve comparable performance. We conduct extensive experiments to demonstrate that COGNATE out- performs existing techniques, achieving average speedups of 1.47 (up to 5.46 ) for SpMM and 1.39 (up to 4.22 ) for SDDMM. 1. Introduction Sparse tensor programs have gained increased significance with the recent advancements in sparse deep learning and graph analytics (Beltagy et al., 2020; Ye Ji, 2021; Child et al., 2019; Dao et al., 2021) workloads. As a result, many hand-crafted performance optimization techniques have been suggested to improve the performance of sparse 1University of Illinois Urbana-Champaign, USA. Correspon- dence to: Chamika Sudusinghe Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s).\n\n--- Segment 2 ---\nPMLR 267, 2025. Copyright 2025 by the author(s). kernels (Kjolstad et al., 2017; Ye et al., 2023; Hong et al., 2019; Jiang et al., 2020). However, the varying non-zero distributions in input sparse matrices have made it diffi- cult to develop performance optimizations for sparse tensor programs that consistently work well across diverse inputs. To overcome this challenge, machine learning (ML)-based program optimization techniques have been introduced to optimize sparse tensor programs on established hardware platforms (e.g., CPUs) (Won et al., 2023; Yang et al., 2023). These techniques adaptively select a program configuration based on the input sparse matrix features. For example, WACO (Won et al., 2023) introduces learned cost models to predict the runtime cost of programs under different sparse matrices and program configurations. Then, search-based techniques are used to automatically find the optimal pro- gram configuration using the cost model s output. Overall, these ML-based techniques show superior performance and adaptability across a diverse range of inputs compared to manually crafted performance optimization techniques. Recently, on the hardware front, new domain-specific ma- chines specifically designed for sparse operations are emerg- ing (Pal et al., 2018; Hegde et al., 2019; Aananthakrish- nan et al., 2023; Gerogiannis et al., 2023; Li et al., 2023; Mu noz-Mart ınez et al., 2023; Jin et al., 2024). These ma- chines, known as hardware accelerators, offer significant speedups over established hardware platforms. Similar to CPUs, sparse accelerators also provide various program configurations (Gerogiannis et al., 2023; Jin et al., 2024; Gerogiannis et al., 2024), which must be configured by soft- ware to achieve the hardware s full potential. It is important that this potential is tested during early-stage hardware de- velopment (i.e. before the actual chip is available) to inform better hardware design decisions. For example, hardware ar- chitects face the risk of overprovisioning hardware resources (e.g., increasing cache size) to address inefficiencies that could potentially be resolved through improved software strategies (e.g., adopting a better tiling strategy).\n\n--- Segment 3 ---\nbefore the actual chip is available) to inform better hardware design decisions. For example, hardware ar- chitects face the risk of overprovisioning hardware resources (e.g., increasing cache size) to address inefficiencies that could potentially be resolved through improved software strategies (e.g., adopting a better tiling strategy). Therefore, it is crucial to automatically select the optimal program con- figuration during the design space exploration (DSE) phase of accelerator development. However, finding the best program configuration for a given 1 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning early-stage hardware accelerator is challenging. This is because software developers have access only to expen- sive simulators and, therefore, cannot utilize ML-based cost model development and auto-tuning techniques commonly used for established hardware platforms, which rely on large supervised datasets. Such datasets often include hundreds of thousands of labeled examples (Won et al., 2023). Unfor- tunately, the time needed to collect datasets of similar scale for emerging accelerators relying on simulators is many orders of magnitude longer than the real execution of the program on the actual chip. For example, it can take up to two weeks to collect a single data point using the simulator of the state-of-the-art SPADE sparse accelerator (Gerogian- nis et al., 2023). At the same time, the same program would take less than a second to execute on the real chip once fabri- cated. Collecting large datasets would require huge clusters running simulations for months or even years. Therefore, in order to bring the same benefits of ML-based optimizations to accelerator platforms at their early stages, we need to rethink learned cost model construction techniques that are data-frugal and highly sample-efficient. Emerging Hardware Dataset Fine-tune on Target General-Purpose Hardware Dataset Runtimes Program Configs Train on Source Runtime Execution Simulator of a Dedicated Sparse Accelerator Predicted Best Config Sparse Matrix Sparse Matrices Runtimes Program Configs Sparse Matrices Target Cost Model Source Cost Model Sparse Matrix Figure 1: Transfer learning pipeline of COGNATE.\n\n--- Segment 4 ---\nTherefore, in order to bring the same benefits of ML-based optimizations to accelerator platforms at their early stages, we need to rethink learned cost model construction techniques that are data-frugal and highly sample-efficient. Emerging Hardware Dataset Fine-tune on Target General-Purpose Hardware Dataset Runtimes Program Configs Train on Source Runtime Execution Simulator of a Dedicated Sparse Accelerator Predicted Best Config Sparse Matrix Sparse Matrices Runtimes Program Configs Sparse Matrices Target Cost Model Source Cost Model Sparse Matrix Figure 1: Transfer learning pipeline of COGNATE. 5 100 1000 of Matrices 1.0 1.1 1.2 1.3 1.4 1.5 Geomean Speedup WACO FM WACO FA Ours (Top-1) Optimal: 1.55 Baseline: 1.0 Figure 2: Performance of existing systems for SpMM on SPADE; WacoNet with feature augmentation (WACO FA), and feature mapping (WACO FM). Inspired by the suc- cess of transfer learn- ing in other domains (Weiss et al., 2016; Zhuang et al., 2020), re- searchers have proposed many techniques to re- duce data requirements for training cost mod- els (Sasaki et al., 2022; Zheng et al., 2021). These techniques lever- age knowledge trans- ferred from cost models learned on one hardware platform (source) to an- other (target) using the ubiquitous pre-train and fine-tune paradigm (Krizhevsky et al., 2012). Such techniques have shown to reduce the data requirement for the target platform. Therefore, using such techniques to transfer learn cost models from general- purpose hardware to emerging accelerators can reduce the data requirements from expensive simulations (Figure 1). However, we notice that most prior works have achieved ef- fective knowledge transfer only between hardware platforms of the same type (e.g. CPU-to-CPU, GPU-to-GPU) (Sasaki et al., 2022; Won et al., 2023; Zheng et al., 2021). Trans- ferring between hardware of different types (e.g. CPU-to- accelerator) poses unique challenges: Heterogeneous program configuration spaces.\n\n--- Segment 5 ---\nTrans- ferring between hardware of different types (e.g. CPU-to- accelerator) poses unique challenges: Heterogeneous program configuration spaces. The pro- gram configurations for emerging sparse accelerators, which serve as the input feature space for cost models, can dif- fer significantly from those of general-purpose hardware. For example, emerging sparse accelerators have software- managed buffers instead of hardware-managed caches and specialized, rather than general-purpose, pipelines. This causes a disparity in program configuration spaces for general-purpose hardware and emerging accelerators, mak- ing it challenging to naively apply transfer learning. Exist- ing heterogeneous transfer learning techniques (Liang et al., 2019), such as feature augmentation (Daum e III, 2009; Duan et al., 2012), can be a viable approach. However, these tech- niques often produce feature representations that are too sparse for the cost model to effectively learn, specifically when accommodating a diverse set of program configura- tion across different hardware platforms. Figure 2 shows the results of applying popular heterogeneous transfer learning techniques feature augmentation (FA) and feature map- ping (FM) to a learned cost model, WACO (Won et al., 2023). Even when using data samples from 1000 matrices for fine-tuning on the SPADE accelerator, the best configura- tions found under WACO FA and WACO FM are far from optimal. Therefore, we need better techniques to handle the heterogeneity of program configurations across hardware. High sample efficiency requirement. Existing transfer learning solutions for learned cost models operating in ho- mogeneous feature spaces typically require at least 25 of the original dataset used in a non-transfer learning setup to achieve competitive performance on the target hardware plat- form (Sasaki et al., 2022). The target dataset requirement for these solutions can further increase due to the heterogeneous input feature spaces between general-purpose hardware and emerging accelerators. This makes it infeasible to adopt ex- isting solutions in their current form for accelerators in early design stages. Therefore, we need data-frugal techniques. COGNATE. In this paper, we present COGNATE, a novel framework for developing learned cost models that enable effective knowledge transfer (Figure 1) overcoming these challenges.\n\n--- Segment 6 ---\nCOGNATE. In this paper, we present COGNATE, a novel framework for developing learned cost models that enable effective knowledge transfer (Figure 1) overcoming these challenges. COGNATE uses WACO s cost model architec- ture (Won et al., 2023) as the base model (WacoNet) but incorporates key changes to make it amenable for transfer learning. This enables the discovery of better program con- figurations that are closer to the optimal (Figure 2), while requiring significantly less fine-tuning data samples. COGNATE is centered around two key principles intro- duced in (Neyshabur et al., 2020): feature reuse and the 2 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning capture of low-level statistical information. We observe that, even though the program configurations between general- purpose hardware and accelerators are heterogeneous, there are certain feature spaces that can be mapped due to their similarities. Motivated by this observation, we propose an approximate mapping of comparable code optimizations, effectively segregating the feature space generated by pro- gram configuration into homogeneous and heterogeneous components. This allows feature reuse across the source and target platforms. The heterogeneous components repre- sent non-mappable hardware specific parameters that can be disparate across different platforms. Such components can introduce challenges during transfer learning due to negative transfer. To separately encode the heterogeneous feature spaces, we introduce a novel latent space representation of the heterogeneous input feature space using an auto- encoder. This novel formulation of the feature space allows us to effectively reuse features while minimizing the impact of negative transfer. Further, we observe that certain layers of WacoNet do not contribute heavily to the final prediction and this over-parameterization can hinder transferability due to over-fitting. To mitigate this, COGNATE modifies WacoNet by reducing the number of layers and extracting features at various depths and scales, effectively allowing the model to capture low-level statistical information. We evaluate COGNATE on two widely used sparse opera- tions, Sparse Matrix-Matrix Multiplication (SpMM) and Sampled Dense-Dense Matrix Multiplication (SDDMM). Starting with a CPU as the source hardware platform, we transfer program program configurations to the the state-of- the art SPADE (Gerogiannis et al., 2023) emerging sparse accelerator.\n\n--- Segment 7 ---\nWe evaluate COGNATE on two widely used sparse opera- tions, Sparse Matrix-Matrix Multiplication (SpMM) and Sampled Dense-Dense Matrix Multiplication (SDDMM). Starting with a CPU as the source hardware platform, we transfer program program configurations to the the state-of- the art SPADE (Gerogiannis et al., 2023) emerging sparse accelerator. SPADE s open ISA and vast set of possible pro- gram configurations make it an ideal target platform for our evaluation. Further, to demonstrate the generalizability of COGNATE, we evaluate our techniques for a second target accelerator an NVIDIA A100 GPU. Our results show that COGNATE outperforms existing transfer learning techniques by 28.44 , achieving an aver- age speedup of 1.47 (up to 5.46 ) for SpMM and 1.39 (up to 4.22 ) for SDDMM on SPADE. On the A100 GPU, it attains an average speedup of 1.17 (up to 1.61 ) for SpMM and 1.15 (up to 1.49 ) for SDDMM. In summary, this paper makes the following contributions. We introduce techniques to segregate and encode the ho- mogeneous (approximate mapping of comparable code optimizations) and heterogeneous (latent representa- tion using an auto-encoder) components of program configurations across different hardware platforms. We introduce COGNATE, a novel data-frugal framework for developing learned cost models that are amenable to few-shot fine-tuning across different hardware platforms, leveraging the above techniques. We evaluate and show that COGNATE produces highly accurate transfer learned cost models for emerging sparse accelerators with minimal data collection overhead. Fur- thermore, we perform extensive experiments and ablation studies to demonstrate its benefits and generalizability. 2. Background and Related Work 2.1. Sparse Tensor Programs Sparse tensor programs perform computational tasks that involve tensors where most of the elements are zero. These computations are optimized to efficiently process only the non-zero values. We describe two operations frequently used in these computations below.\n\n--- Segment 8 ---\nThese computations are optimized to efficiently process only the non-zero values. We describe two operations frequently used in these computations below. Sparse Matrix-Matrix Multiplication (SpMM) is the op- eration of multiplying a sparse matrix A RM K with a dense matrix B RK N, resulting in an output matrix D RM N. The SpMM operation can be expressed as Di,k P j Ai,j Bj,k, where Ai,j 0. Sampled Dense-Dense Matrix Multiplication (SDDMM) is an operation that involves the multiplication of two dense matrices, followed by an elementwise multiplica- tion with a sparse matrix. Given a sparse matrix A RM N, a sparse output matrix D RM N, and two dense matrices B RM K and C RK N, SD- DMM operation can be expressed as Di,k Ai,k P j (Bi,j Cj,k) , where Ai,k 0. 2.2. Sparse Tensor Programming Systems Table 1: Program configuration parameters (Config Params) available across CPU, GPU, and SPADE. Config Params CPU GPU SPADE Type Loop Strip-mining Numerical Loop Reordering Categorical Format Reordering Categorical Loop Binding Categorical Loop Unrolling Categorical Tiling Numerical Barrier Binary Cache Bypassing Binary Matrix Reordering Binary A sparse tensor programming system supports a range of code optimizations that modify the structure of the code to enhance performance. The effectiveness of these code optimizations depends on assigning specific values to the parameters of the program configuration. By tuning these parameters, we can significantly impact the runtime perfor- mance of sparse operations. Table 1 outlines the parameters available for program configurations across different hard- ware platforms explored in this work (code optimizations are detailed in Appendix B). The execution strategy for sparse tensor programs depends on both the hardware platform and the corresponding programming system used. In this 3 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning work, for CPU execution (source platform), we use TACO (Kjolstad et al., 2017), a domain-specific language and a compiler designed for sparse tensor algebra. Considering our target accelerators, SPADE has its own tile-based open instruction set architecture (ISA) to leverage different vari- ations of SpMM and SDDMM operations.\n\n--- Segment 9 ---\nIn this 3 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning work, for CPU execution (source platform), we use TACO (Kjolstad et al., 2017), a domain-specific language and a compiler designed for sparse tensor algebra. Considering our target accelerators, SPADE has its own tile-based open instruction set architecture (ISA) to leverage different vari- ations of SpMM and SDDMM operations. For GPU, we employ SparseTIR (Ye et al., 2023), a sparse tensor com- pilation framework developed as an enhancement to TVM Tensor IR (Chen et al., 2018a). 2.3. ML-based cost models Learned Cost Models. Cost models act as fast cost- effective proxies for executing workloads on real hardware. Their primary goal is to accurately estimate the execution time of workloads as they would perform on real hardware. To achieve this, these cost models can be trained on data sam- ples with various program configurations and then be used to predict the program configuration that will deliver the op- timal performance. Hence, generally, the training objective of cost models is tied with minimizing t CM t , where t is the runtime of the true optimal program configuration and t CM is the runtime of the best program configuration suggested by the cost model (accuracy objective)(detailed Appendix A). Finding the best configuration suggested by the cost model is usually done using auxiliary intelligent search techniques such as simulated annealing, Monte Carlo tree search, and reinforcement learning. There have been numerous works on learned cost models to predict the run- time of workloads targeting different hardware platforms (Chen et al., 2018b; Adams et al., 2019). These techniques range from simple XGBoost (Chen Guestrin, 2016) based cost models (Chen et al., 2018b;a) to sophisticated deep neu- ral network based models (Baghdadi et al., 2021; Kaufman et al., 2021; Zhai et al., 2023; Zheng et al., 2020). WACO s Cost Model. WACO (Figure 3(a)) (Won et al., 2023) introduced a learned cost model specifically built for sparse tensor programs, which utilizes sparsity patterns as raw input data.\n\n--- Segment 10 ---\nWACO s Cost Model. WACO (Figure 3(a)) (Won et al., 2023) introduced a learned cost model specifically built for sparse tensor programs, which utilizes sparsity patterns as raw input data. WACO s cost model employs submanifold sparse convolution networks (SCNN) (Graham Van der Maaten, 2017) to extract features using an input featurizer. It leverages a neural network-based program embedder to capture the impact of code optimizations on sparse opera- tions by encoding program configurations into embeddings. These program embeddings are merged with the extracted sparsity pattern features produced by the input featurizer. The merged inputs are then processed through a multi-layer perceptron predictor to estimate the execution cost. Transfer Learning. Transfer learning is a technique that leverages knowledge gained from a task in a source domain to improve the performance of a related task in a target domain, where data collection can often be challenging (Bozinovski, 2020). There have been many successful ex- amples of transfer learning techniques in a wide range of fields (Weiss et al., 2016). Transfer learning can be catego- rized into two main types: homogeneous transfer learning (Zhuang et al., 2020), where the input and label spaces are the same, and heterogeneous transfer learning (Day Khoshgoftaar, 2017), where either one or both can be dif- ferent. In program optimization, transfer learning has been successfully used to transfer cost models learned from one hardware platform to another, primarily in homogeneous settings, to minimize the target domain data requirements (Zheng et al., 2021; Ryu Sung, 2021; Sasaki et al., 2022). In this work, we seek to minimize the target domain data requirement during fine-tuning (Shen et al., 2021), by tar- geting heterogeneous input feature spaces present between general-purpose hardware and emerging sparse accelerators (data-collection objective) (detailed in Appendix A). 3.\n\n--- Segment 11 ---\nIn this work, we seek to minimize the target domain data requirement during fine-tuning (Shen et al., 2021), by tar- geting heterogeneous input feature spaces present between general-purpose hardware and emerging sparse accelerators (data-collection objective) (detailed in Appendix A). 3. Our Methodology: COGNATE Autoencoder Latent Encoder Configuration Mapper Sparse Convolution Input Featurizer (3) Sparse Latent Config Sparse Matrix Program Configurations Homogenous Heterogenous Predictor WACO Program Embedder Sparse Convolution Input Featurizer Sparse Config Predictor Program Configurations (a) (b) (4) Runtime Cost (1) (2) COGNATE Figure 3: A comparative overview of the enhanced cost model design in COGNATE (b) alongside WACO s cost model design (a), highlighting key differences. Here, we present COGNATE, a novel framework to design data-frugal learned cost models to accelerate the execution of sparse tensor programs on emerging hardware. The fol- lowing subsections outline our contributions toward achiev- ing the objectives set forth in Section 2.3; maximizing the accuracy while minimizing the data collection overhead. 3.1. Enhancements to Enable Transfer Learning We build upon WACO considering it as our base model by refining its architecture to better facilitate transfer learning across diverse hardware platforms. These enhancements represent contributions that are orthogonal to WACO s orig- inal scope. Our improved cost model design (Figure 3(b)) is structured around four key components: configuration mapper, input featurizer, latent encoder, and predictor. The configuration mapper (Figure 3(b)(1)) and latent encoder (Figure 3(b)(2)) replace the program embedder in WACO, while the input featurizer (Figure 3(b)(3)) has been modi- fied to more effectively capture low-level information from sparsity patterns. Both configuration mapper and input fea- turizer remain consistent across hardware platforms, serving as the components that enable efficient knowledge transfer. Configuration Mapper (FM). The configuration mapper captures homogeneity across hardware platforms by pro- cessing program configurations (cj) and their parameters to identify similarities in code optimizations across various 4 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning platforms.\n\n--- Segment 12 ---\nConfiguration Mapper (FM). The configuration mapper captures homogeneity across hardware platforms by pro- cessing program configurations (cj) and their parameters to identify similarities in code optimizations across various 4 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning platforms. We designed it to approximately map similar con- figuration parameters across different hardware platforms (described in Section 3.2 and Section E) to a unified feature space. This is achieved by using explicit mapping func- tions. The resulting parameters are subsequently passed through a multi-layer perceptron (MLP) to produce the final configuration vector pj. In this work, we approximate the code optimizations loop strip-mining and loop reordering as pj FM(ϕ( ), π( ), cj). using the mapping functions ϕ and π, as introduced in Section 3.2. Input Featurizer (IFE). Matrices with identical dimen- sions and non-zero elements can exhibit vastly different sparsity patterns, making it difficult to extract meaning- ful features based only on statistical properties. Building on WACO s input featurizer (Won et al., 2023), we mod- ify the network architecture (Figure 3) to more effectively capture low-level information from sparsity patterns. Our network consists of 12 SCNN layers (compared to 14 layers in WACO), arranged in 4 blocks, each containing 3 sparse convolution layers. At the end of each block, we apply max pooling to condense spatial information. We increase the number of channels across blocks up to 256, whereas WACO had them fixed at 32. These additional channels enables our design to capture hierarchical features more effectively throughout the network compared to WACO. For a given sparse matrix M, our input featurizer generates a sparse feature vector sM, expressed as sM IFE(M). Latent Encoder (LE). We handle the heterogeneity of program configurations across hardware platforms using per-target autoencoders that compress the heterogeneous components of the configurations into compact latent rep- resentations (described in Section 3.3). An autoencoder is trained for each target sparse primitive pair.\n\n--- Segment 13 ---\nWe handle the heterogeneity of program configurations across hardware platforms using per-target autoencoders that compress the heterogeneous components of the configurations into compact latent rep- resentations (described in Section 3.3). An autoencoder is trained for each target sparse primitive pair. During both training and inference, the latent encoder LE processes a configuration (cj), transforming it into a latent representa- tion zj LE(cj), that encapsulates the unique characteris- tics of the program configuration. Predictor (P). As the final component of the cost model, the predictor (Figure 3(b)(4)) integrates the three feature vectors from the preceding components into a single uni- fied vector, encapsulating all key information about the sparsity pattern and program configuration. This unified vector (sM pj zj) is passed through a multi-layer percep- tron (MLP) to eventually output a scalar value representing the predicted execution cost, which can be expressed as ˆrM,cj P(pj sM zj). 3.2. Exploiting Homogeneity: Approximate Mapping of Comparable Code Optimizations Different hardware platforms often use distinct program- ming systems, leading to variations in how code optimiza- tions are parameterized (Figure 1). Further, an optimization available in one platform may not be directly available on another, requiring the combination of multiple other code op- timizations to replicate the same impact. For example, loop strip-mining optimization on CPUs can be closely approxi- mated by collectively applying barrier and tiling optimiza- tions in SPADE. By mapping the effects of these optimiza- tions using their program configuration parameters, we can expose patterns that facilitate effective knowledge transfer across hardware platforms. In this section, we present our approaches for approximately mapping loop strip-mining, barrier, and tiling optimizations between CPU and SPADE, and loop reordering optimization between CPU and GPU. Loop strip-mining is a code optimization that decomposes large software loops into smaller segments to optimize com- putations for memory utilization and cache performance.\n\n--- Segment 14 ---\nIn this section, we present our approaches for approximately mapping loop strip-mining, barrier, and tiling optimizations between CPU and SPADE, and loop reordering optimization between CPU and GPU. Loop strip-mining is a code optimization that decomposes large software loops into smaller segments to optimize com- putations for memory utilization and cache performance. In our context, it is applied to loops iterating over the in- dices i, j, and k of matrices in SpMM and SDDMM sparse operations (Section 2.1), where parameters I, J, and K are used to split these loops into outer and inner segments, resulting in a loop nest of six decomposed loops. The re- sulting loop segments are {i1, i2, j1, j2, k1, k2} and their execution order is denoted by ω. In SPADE, we approx- imate this using barrier and tiling optimizations. Tiling decomposes a matrix into smaller blocks to optimize data reuse in the local memory, while barrier controls the ex- ecution order of tiles. For example, enabling barrier opti- mization pauses the tiles scheduled by a control processing element until all previous tiles have been completed (Gero- giannis et al., 2023). Similar to strip-mining parameters, the tiling parameters for i, j, and k indices of matrices are represented in SPADE as pcol, prow, dsplit, respectively, while barrier is represented by b, where b 1 if barrier is enabled, and b 0 otherwise. Intuitively, tiling divides computations into smaller blocks, while barriers control syn- chronization during execution. By enabling and disabling barriers for various tiling configurations, we can dictate the order of computation. This resembles loop strip-mining and reordering in CPUs, where optimizing loop execution enhances performance and cache utilization.\n\n--- Segment 15 ---\nBy enabling and disabling barriers for various tiling configurations, we can dictate the order of computation. This resembles loop strip-mining and reordering in CPUs, where optimizing loop execution enhances performance and cache utilization. We can approx- imately map tiling and barrier parameters to the correspond- ing strip-mining parameters using the mapping function ϕ : {pcol, prow, ssplit, b} {I, J, K, ω} as follows: ϕ(pcol, prow, ssplit, b) (I, J, K, ω) I pcol, J prow, K ssplit; ω ( [k2, j2, i2, i1, j1, k1] if b 1 [k2, i2, j2, i1, j1, k1] if b 0 Loop reordering is a code optimization that adjusts the exe- cution order (ω) of loops to improve cache efficiency and facilitate parallel processing. It is often applied after loop strip-mining. Here, we examine how it can be approximated for both CPU (a1) and GPU (a3). In CPU, loop strip-mining results in six decomposed loops {i1, i2, j1, j2, k1, k2}.\n\n--- Segment 16 ---\nHere, we examine how it can be approximated for both CPU (a1) and GPU (a3). In CPU, loop strip-mining results in six decomposed loops {i1, i2, j1, j2, k1, k2}. Similarly, in GPU, loop strip-mining produces six loop seg- ments, but the loop structure differs {i1, i2, j, k1, k2, k3} 5 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning SpMM (SPADE) SDDMM (SPADE) SpMM (GPU) SDDMM (GPU) Sparse Tensor Operations 0.00 0.25 0.50 0.75 1.00 1.25 1.50 Geomean Speedup 1.04 1.05 0.52 0.64 1.09 1.07 0.55 0.67 1.29 1.19 0.72 0.89 0.71 0.59 0.07 0.09 1.40 1.27 1.03 1.07 1.47 1.39 1.17 1.15 1.55 1.44 1.25 1.22 WACO FA WACO FM No Transfer Zero-Shot (CPU) COGNATE (Top-1) COGNATE (Top-5) Optimal Figure 4: Geomean speedups of COGNATE and other techniques, normalized to the baseline. due to architectural differences. We approximate them us- ing Ω( ) function that determines the index of a loop seg- ment and a mapping function πai : {i1, i2, . . . , k2, ωai} {i1, i2, . . .\n\n--- Segment 17 ---\n. . , k3, ω ai} as follows: πa1(i1, i2, j1, j2, k1, k2, ωa1) i1, i2, j1, j2, k1, k2, k3, ω a1 ; k3 1, Ωa1(k3) Ωa1(k2) 1 πa3(i1, i2, j, k1, k2, k3, ωa3) i1, i2, j, j , k1, k2, k3, ω a3 ; j 1, Ωa3(j ) Ωa3(j) 1 3.3. Mitigating Heterogeneity: Latent Encoding of Hardware-specific Code Optimizations While we can use the strategies described in Section 3.2 to approximate code optimizations with homogeneity, such techniques are not applicable to hardware-specific optimiza- tions. An existing approach for representing hardware- specific optimizations across hardware platforms is to en- code them using feature augmentation. However, this results in excessively sparse feature vectors, as code optimizations that are not applicable to a selected hardware platform are zeroed out. Training models on such sparse feature vectors often leads to sub-optimal performance (Figure 4). To address this limitation, we propose indexing the parame- ters of the heterogeneous component of the program config- urations for each platform ai using low-dimensional latent representations. Specifically, we train an autoencoder AEai to learn a latent representation zj for each configuration cj Cai. This is accomplished by determining the value ranges for all parameters of the heterogeneous component in the program configurations, followed by training an autoen- coder to learn an unsupervised embedding of this parame- terization. Once trained, we use the encoder LEai in AEai, which takes each configuration (cj) as input and transforms it into its corresponding latent representation zj, where zj is a fixed-size vector. By compressing configurations from different hardware platforms each with varying param- eters and ranges into fixed-size vectors, we standardize the input for hardware-specific optimizations into the cost model.\n\n--- Segment 18 ---\nOnce trained, we use the encoder LEai in AEai, which takes each configuration (cj) as input and transforms it into its corresponding latent representation zj, where zj is a fixed-size vector. By compressing configurations from different hardware platforms each with varying param- eters and ranges into fixed-size vectors, we standardize the input for hardware-specific optimizations into the cost model. This compression significantly simplifies the model compared to feature augmentation, as the cost model now processes fewer input features, reducing its computational complexity. With the hardware specific optimizations now represented in a unified latent feature space, it becomes possible to identify and leverage similarities in their impact on performance during fine-tuning. Finally, this approach facilitates the seamless integration of emerging hardware platforms into COGNATE, as we can extend COGNATE to support new target hardware platforms by training new au- toencoders and relying on few-shot fine-tuning, eliminating the need to retrain the source model from scratch (detailed in Appendix C). As long as the overall structure of the sparse tensor program remains consistent, COGNATE can quickly adapt by using a small number of new performance samples. In contrast, traditional cost model development approaches would require re-evaluating a large number of configurations (Won et al., 2023). 4. Evaluation 4.1. Dataset, Training and Evaluation Setup Dataset. Our experiments were conducted using real-world sparse matrices sourced from the SuiteSparse Matrix Collec- tion (Davis Hu, 2011). This dataset has been widely used in previous work (Pal et al., 2018; Hong et al., 2019; Jiang et al., 2020; Gerogiannis et al., 2023; Won et al., 2023) and covers a broad spectrum of domains, ensuring a realistic and comprehensive evaluation of COGNATE s performance. To collect the training dataset, we performed the sparse op- erations SpMM and SDDMM on three distinct hardware platforms: an Intel Xeon Gold 6348 CPU with 1TB of RAM, an NVIDIA A100 GPU paired with an Intel Xeon Platinum 8358, and SPADE, a simulated sparse accelera- tor with 32 processing elements operating at 0.8GHz. To ensure practical feasibility across hardware platforms, the program configuration search space was constrained to 256 configurations for SPADE and approximately 300 configura- tions for SparseTIR (GPU).\n\n--- Segment 19 ---\nTo collect the training dataset, we performed the sparse op- erations SpMM and SDDMM on three distinct hardware platforms: an Intel Xeon Gold 6348 CPU with 1TB of RAM, an NVIDIA A100 GPU paired with an Intel Xeon Platinum 8358, and SPADE, a simulated sparse accelera- tor with 32 processing elements operating at 0.8GHz. To ensure practical feasibility across hardware platforms, the program configuration search space was constrained to 256 configurations for SPADE and approximately 300 configura- tions for SparseTIR (GPU). We gathered data samples using 1,500 matrices for each hardware platform, with up to 1,000 matrices used for model training under various scenarios and the remainder was set aside for validation. For each matrix, we randomly sampled 100 program configurations per hardware platform to have diverse and representative training datasets across all experiments. 6 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Program Configuration Search Space for SPADE. The program configuration search space considered for the SPADE accelerator was derived from a combination of key tunable parameters including tiling, synchronization barriers, cache bypassing, and matrix reordering. As sum- marized in Table 1, the parameters for barrier insertion, cache bypassing, and matrix reordering are binary (i.e., ei- ther enabled or disabled). Tiling is controlled by three numerical parameters: the number of row panels, column panels, and the split factor. These values were chosen to resemble those explored in the original SPADE work (Gero- giannis et al., 2023), as those values were expected to show more significant performance deviations for differ- ent sparse matrices. Specifically, we used 4 values for row panels {4, 32, 256, 2048}, 4 values for column pan- els {1024, 16384, 65536, NUM MATRIX COLS} (where NUM MATRIX COLS depends on the input matrix) and 2 values for the split factor {32, 256}. Although COGNATE is designed to perform under limited data availability, we conducted extensive data collection to rigorously evaluate and justify its effectiveness. This in- cluded a range of experiments and ablation studies, some of which required performance data samples from up to 1,000 matrices for training. Altogether, this effort demanded ap- proximately 4 million CPU hours.\n\n--- Segment 20 ---\nThis in- cluded a range of experiments and ablation studies, some of which required performance data samples from up to 1,000 matrices for training. Altogether, this effort demanded ap- proximately 4 million CPU hours. Despite the constrained nature of the search space (256 program configurations), it took nearly three months to complete the dataset cura- tion, even though the simulations were parallelized across multiple machines. Hence, exhaustively evaluating a larger, unconstrained program configuration space would be com- putationally infeasible. This underscores the need for data- efficient methods like COGNATE, which are designed to operate effectively even under limited data availability. Baselines and Implementation. We executed SpMM and SDDMM on CPU, GPU, and SPADE using the respective programming systems introduced in Section 2.2. We used the default optimizations of these programming systems as our baselines. We implemented COGNATE in PyTorch, utilizing MinkowskiEngine (Choy et al., 2019) to handle sparse convolution. Separate models were developed for SpMM and SDDMM to conduct precise performance pre- dictions. We focused on these two sparse operations because they are the only operations currently supported natively by both the SPADE accelerator (Gerogiannis et al., 2023) and the SparseTIR framework (Ye et al., 2023). Cost Model Evaluation. We evaluated COGNATE s per- formance on 715 real-world matrices from the SuiteSparse Matrix Collection, ensuring that none of the evaluation data samples overlapped with the training set. For each matrix, we predicted the runtime cost across all program configura- tions and selected the top-1 and top-5. Then we executed the sparse operations with the selected program configurations on the target platform and identified the one with the short- est runtime. We then compared our results to the normalized runtime of the baseline executions, WacoNet with feature augmentation, and WacoNet with feature mapping by cal- culating the geometric mean (geomean) speedups across matrices to quantify COGNATE s overall effectiveness. Pre-training and Fine-tuning Procedure. The matrices for pre-training were randomly selected from the training set while ensuring a balanced representation of their dimen- sions and sparsity.\n\n--- Segment 21 ---\nPre-training and Fine-tuning Procedure. The matrices for pre-training were randomly selected from the training set while ensuring a balanced representation of their dimen- sions and sparsity. To achieve this, we first grouped the matrices into five bins based on the number of rows: 8192, 32,768, 65,536, 131,072, and 131,072. From each group, we randomly sampled matrices, ensuring the selected subset collectively spanned a diverse range of sparsity lev- els. We empirically demonstrate in Section 4.4 (Figure 11) that training the source model with 100 matrices strikes a good balance. We use this setting for our headline result (Figure 4). Once the source model was pre-trained, we performed few-shot fine-tuning on accelerators using data samples from only 5 matrices. This choice was guided by empirical observations, aiming to strike a good balance be- tween transfer learning effectiveness and the cost of data collection. As shown in Section 4.4 (Figure 12), this set- ting offers the best trade-off between our objectives for accuracy and data collection (detailed in Appendix A). Fur- ther, the same set of matrices were used for evaluating the non-transfer learning baselines, enabling consistent and fair comparisons across all experimental settings. 4.2. Transferability to SPADE 0 100 200 300 400 500 600 700 Matrix 1 2 3 4 5 Geomean Speedup Speedup Optimal: 1.55 COGNATE (Top-5): 1.40 Baseline: 1.0 Figure 5: COGNATE per-matrix speedups (SpMM). Figure 4 illustrates the geomean speedups achieved using multiple techniques: zero-shot inference from the source model (zero-shot), a model trained exclusively on the target hardware using the fine-tuning dataset (no transfer), Wa- coNet with feature augmentation (WACO FA), WacoNet with feature mapping (WACO FM), and COGNATE s per- formance for both the top-1 and top-5 (k-best) predicted program configurations. Our results show that COGNATE consistently outperformed other techniques across both sparse operations and hardware platforms.\n\n--- Segment 22 ---\nFigure 4 illustrates the geomean speedups achieved using multiple techniques: zero-shot inference from the source model (zero-shot), a model trained exclusively on the target hardware using the fine-tuning dataset (no transfer), Wa- coNet with feature augmentation (WACO FA), WacoNet with feature mapping (WACO FM), and COGNATE s per- formance for both the top-1 and top-5 (k-best) predicted program configurations. Our results show that COGNATE consistently outperformed other techniques across both sparse operations and hardware platforms. Specifically for SPADE, COGNATE (Top-1) achieved an average speedup of 1.40 for SpMM, reaching 90 of the optimal speedup of 1.55 . When expanding COGNATE (Top-5), it deliv- ered an average speedup of 1.47 , achieving 95 of the 7 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning optimal speedup. Note that the optimal speedup was deter- mined by exhaustively evaluating all program configurations within the defined constrained search space for each matrix in the test set, and selecting the fastest configuration per matrix to compute the geometric mean. Similarly, for SD- DMM in SPADE, COGNATE (Top-1) achieved an average speedup of 1.27 and COGNATE (Top-5) achieved an aver- age speedup of 1.39 . This emphasizes COGNATE s ability to consistently find near-optimal program configurations with minimal fine-tuning across multiple sparse operations. The speedup gained for zero-shot inference from the source model was significantly lower than the baseline. In contrast, fine-tuning on a few data samples from SPADE led to signifi- cant performance gains showing COGNATE s effectiveness in knowledge transfer. 0 5 10 15 20 25 30 Epochs 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Accuracy Loss Train Loss(PRL) Kendall's TAU Validation Loss(PRL) Ordered Pair Accracy(OPA) Figure 6: Loss and accuracy during training. 4.3. Transferability to GPU COGNATE is generalizable and is not only applicable to one target hardware platform.\n\n--- Segment 23 ---\n4.3. Transferability to GPU COGNATE is generalizable and is not only applicable to one target hardware platform. To showcase COGNATE s capability, we extended our evaluation to a GPU acceler- ator (NVIDIA A100) (Figure 4). The speedup trends on GPU aligned with those observed on SPADE, reinforcing the effectiveness of COGNATE. COGNATE (Top-1) de- livered an average speedup of 1.03 and COGNATE (Top- 5) yielded an average speedup of 1.17 for SpMM, with the optimal achievable speedup being 1.25 . In compari- son, cuSPARSE SpMM (Naumov et al., 2010) achieved a lower average speedup of 1.01 . For SDDMM, COGNATE (Top-1) resulted in an average speedup of 1.07 , while COGNATE (Top-1) yielded a 1.15 speedup, with the op- timal being 1.22 . Zero-shot inference on the GPU was significantly worse compared to Zero-shot for SPADE, with speedups falling well below the baseline. This discrepancy is likely due to the inherent architectural differences be- tween the CPU and GPU architectures. Further, to assess COGNATE s scalability, we conducted preliminary exper- iments on an end-to-end GNN workload on GPU. Using the transient sparse matrix from our test set (178,866 rows columns (nodes), 961,368 non-zeros) and GraphSAGE model configured with three hidden layers and 256 hidden features, COGNATE achieved a 1.30 speedup for inference and a 1.28 speedup for training over the default SparseTIR (Ye et al., 2023) implementation. These results demon- strate the potential of COGNATE to scale effectively on real-world workloads and deliver consistent performance. Comparison with Other Transfer Learning Techniques. For comparisons, we modified WacoNet to support feature augmentation and feature mapping, as it is not inherently optimized for heterogeneous transfer. Despite these modi- fications, COGNATE consistently outperformed both. For SpMM on SPADE, WACO FA had an average speedup of 1.04 , while WACO FM resulted in a slightly higher average speedup of 1.09 .\n\n--- Segment 24 ---\nDespite these modi- fications, COGNATE consistently outperformed both. For SpMM on SPADE, WACO FA had an average speedup of 1.04 , while WACO FM resulted in a slightly higher average speedup of 1.09 . In comparison, COGNATE deliv- ered an average speedup of 1.40 , outperforming its closest alternative (WACO FM) by 28.44 . The sub-optimal per- formance of WACO FA and WACO FM can be attributed to the increased sparsity in the feature space due to fea- ture augmentation and their limited capacity to effectively mitigate the heterogeneity. 1.0 1.2 1.4 1.6 Geomean Speedup 1.01 1.16 1.26 1.40 1.55 w o LE w o FM w o IFE COGNATE Optimal Figure 7: Ablation study of the model components. 1.0 1.2 1.4 1.6 Geomean Speedup 1.321.341.36 1.40 1.55 LSTM GRU TF COGNATE Optimal Figure 8: Design choices of the predictor. 4.4. Additional Experiments for SpMM on SPADE Speedup Performance. Figure 5 shows the speedups achieved by COGNATE (Top-1) across all evaluated matri- ces. Matrices where the baseline outperformed COGNATE are indicated below the y 1 dotted line. While the baseline outperformed COGNATE on a few matrices, the overall results demonstrate that COGNATE delivered substantial speedups (as high as 5.46x) for the majority of the dataset. Cost Model Accuracy. Figure 6 shows the accuracy of COGNATE s cost model across training epochs using Pair- wise Ranking Loss (PRL), Ordered Pair Accuracy (OPA), and Kendall s Tau (K-Tau). The steady decline in PRL for both training and validation loss indicates that the model effectively learns to rank program configurations without over-fitting. OPA and K-Tau steadily improved to 0.80 and 0.61, indicating effective training. Component-Level Contributions. The effectiveness of our cost model relies on the inclusion of all components, each contributing uniquely to the overall performance. As illus- trated in Figure 7, the exclusion of individual components leads to a noticeable decline in speedups.\n\n--- Segment 25 ---\nThe effectiveness of our cost model relies on the inclusion of all components, each contributing uniquely to the overall performance. As illus- trated in Figure 7, the exclusion of individual components leads to a noticeable decline in speedups. For example, ex- cluding the input featurizer (IFE) causes a decline from 1.40x to 1.26x. Similarly, omitting the configuration map- per (FM) leads to a further decline to 1.16x, and excluding latent encoder (LE) lowers speedup to 1.01x. This em- phasizes that each component contributes uniquely to the model s high performance, and all need to act synergistically to maximize the benefits of knowledge transfer. 8 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning 1.0 1.2 1.4 1.6 Geomean Speedup 1.01 1.04 1.31 1.40 1.55 VAE FA PCA COGNATE Optimal Figure 9: Selection of auto- encoders for COGNATE. 1.0 1.2 1.4 1.6 Geomean Speedup 1.29 1.38 1.43 1.40 1.55 NT 5 NT 100 NT 1000 COGNATE Optimal Figure 10: Data overhead w o transfer learning. Selection of MLP Predictor. As shown in Figure 3, the MLP predictor from WACO s base cost model was retained in our enhanced design. Figure 8 provides a comparative analysis of alternative predictors, including LSTM, GRU, and Transformer (TF). The results demonstrate that our proposed cost model design outperforms the alternatives, with the TF predictor achieving the next best performance with 1.36 speedup. These findings highlight that an MLP predictor is sufficient to deliver robust performance with limited data. In contrast, the suboptimal performance of the TF predictor can be attributed to the limited dataset, as the high simulation costs associated with emerging hardware make it challenging to collect larger datasets for fine-tuning. Selection of Auto-Encoders. Figure 9 shows our inves- tigation into various methods for handling the heteroge- neous components of program configurations. We evaluated choices ranging from conventional feature augmentation (FA) to principal component analysis (PCA), auto-encoders, and variational auto-encoders (VAE).\n\n--- Segment 26 ---\nFigure 9 shows our inves- tigation into various methods for handling the heteroge- neous components of program configurations. We evaluated choices ranging from conventional feature augmentation (FA) to principal component analysis (PCA), auto-encoders, and variational auto-encoders (VAE). Our findings reveal that auto-encoders were the most effective for capturing het- erogeneous optimizations in a latent space. This was evident from the lower validation loss observed during the training of the auto-encoders to learn the latent representations. Data Collection Overhead w o Transfer Learning. Figure 10 shows that without transfer learning, the overhead of data collection becomes significant on emerging hardware due to the high costs of running simulations. For example, models trained exclusively on SPADE would require 20 200 more target data samples (collected using 100 1000 matrices) to match or surpass the speedups achieved through COGNATE via transfer learning. 1.0 1.2 1.4 1.6 Geomean Speedup 1.07 1.21 1.40 1.36 1.19 1.55 CPU 5 CPU 20 COGNATE CPU 500 CPU 1000 Optimal Figure 11: Impact of nega- tive transfer for fine-tuning. 1.0 1.2 1.4 1.6 Geomean Speedup 1.00 1.401.411.421.43 1.55 Baseline COGNATE TL 100 TL 1000 NT 1000 Optimal Figure 12: Impact of number of samples for fine-tuning. Impact of Negative Transfer. Figure 11 shows that using a large dataset to train the source model (e.g., data samples from 1000 matrices) does not necessarily lead to better out- comes. As the size of the training dataset increases, the model becomes overly specialized to the source platform, diminishing its adaptability during fine-tuning. To inves- tigate this effect, we trained source models on datasets of varying sizes (5, 20, 100, 500, and 1,000 matrices) and eval- uated their transferability to our target platform (SPADE) using few-shot fine-tuning on just 5 matrices (Figure 11). Our results show that training on the CPU (source) with data samples from 100 matrices and fine-tuning on SPADE (target) with data samples from 5 matrices produces the best results.\n\n--- Segment 27 ---\nTo inves- tigate this effect, we trained source models on datasets of varying sizes (5, 20, 100, 500, and 1,000 matrices) and eval- uated their transferability to our target platform (SPADE) using few-shot fine-tuning on just 5 matrices (Figure 11). Our results show that training on the CPU (source) with data samples from 100 matrices and fine-tuning on SPADE (target) with data samples from 5 matrices produces the best results. In contrast, training the source model with data from 1,000 matrices yields sub-optimal performance due to overfitting to source-specific characteristics. This highlights the importance of carefully selecting the size of the source training dataset to avoid over-specialization and minimize the impact of negative transfer. Number of Samples in Fine-Tuning. In Figure 12, we show COGNATE s performance as fine-tuning data samples increase. Despite fine-tuning on data from 1,000 matrices, the maximum speedup saturates at 1.42 . We can achieve a comparable speedup of 1.40 with data from 5 matrices, which shows the diminishing returns associated with larger datasets. Further, the non-transfer learning setup achieved a marginally higher speedup of 1.43 when using data from 1,000 matrices. However, considering the significant data collection overhead, these marginal improvements are not practically justifiable. To further assess sensitivity to the size of the fine-tuning dataset, we conducted additional ex- periments using 3 and 7 matrices. The resulting speedups were 1.30 and 1.41 , respectively, compared to 1.40 with 5 matrices. While using only 3 matrices led to a noticeable drop in performance, increasing to 7 did not yield a sig- nificant gain but required substantially more data samples, incurring several days of additional data collection time. These results suggest that using 5 matrices strikes a practi- cal balance between data collection cost and performance, while demonstrating that COGNATE is relatively robust to small variations in dataset size. 5. Conclusion In this paper, we introduced COGNATE, a novel frame- work to develop data-frugal learned cost models to optimize sparse tensor programs for emerging hardware platforms. COGNATE leverages a unique technique that capitalizes on the homogeneity of input features across different platforms while effectively mitigating heterogeneity.\n\n--- Segment 28 ---\nConclusion In this paper, we introduced COGNATE, a novel frame- work to develop data-frugal learned cost models to optimize sparse tensor programs for emerging hardware platforms. COGNATE leverages a unique technique that capitalizes on the homogeneity of input features across different platforms while effectively mitigating heterogeneity. This enables COGNATE to train cost models using low-cost data sam- ples from widely accessible general-purpose hardware (such as CPUs) and then fine-tune them for emerging hardware platforms with few-shot learning. Our results demonstrate that COGNATE is able to achieve near-optimal accuracy while maintaining significant sample efficiency. 9 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Acknowledgements The work is partially supported by the National Science Foundation Graduate Research Fellowship, ACE, one of the seven centers in JUMP 2.0, a Semiconductor Research Cor- poration (SRC) program sponsored by DARPA, by NSF un- der the grants CCF-2338739 and CCF-2316233, by DARPA under the grant D24AP00295-00 and by generous gifts from Qualcomm. Any opinion, findings, and conclusions or rec- ommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the NSF or DARPA. Impact Statement The goal of this work is to accelerate the execution of sparse tensor programs in the domain of emerging sparse accel- erators through the application of machine learning-based techniques. Experiments demonstrate that our approach exhibits potential in benefiting early-stage accelerator devel- opment by enabling data-efficient design space exploration. There may be potential societal consequences of our work, none of which we feel must be specifically highlighted here. References Aananthakrishnan, S., Abedin, S., Cav e, V., Checconi, F., Du Bois, K., Eyerman, S., Fryman, J. B., Heirman, W., Howard, J., Hur, I., et al. The intel programmable and integrated unified memory architecture graph analytics processor. IEEE Micro, 43(5):78 87, 2023.\n\n--- Segment 29 ---\nThe intel programmable and integrated unified memory architecture graph analytics processor. IEEE Micro, 43(5):78 87, 2023. Adams, A., Ma, K., Anderson, L., Baghdadi, R., Li, T.- M., Gharbi, M., Steiner, B., Johnson, S., Fatahalian, K., Durand, F., et al. Learning to optimize halide with tree search and random programs. ACM Transactions on Graphics (TOG), 38(4):1 12, 2019. Baghdadi, R., Merouani, M., Leghettas, M.-H., Abdous, K., Arbaoui, T., Benatchba, K., et al. A deep learning based cost model for automatic code optimization. Proceedings of Machine Learning and Systems, 3:181 193, 2021. Beltagy, I., Peters, M. E., and Cohan, A. Long- former: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Bozinovski, S. Reminder of the first paper on transfer learn- ing in neural networks, 1976. Informatica, 44(3), 2020. Chen, T. and Guestrin, C. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd acm sigkdd inter- national conference on knowledge discovery and data mining, pp. 785 794, 2016. Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., Cowan, M., Wang, L., Hu, Y., Ceze, L., et al. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Sys- tems Design and Implementation (OSDI 18), pp. 578 594, 2018a. Chen, T., Zheng, L., Yan, E., Jiang, Z., Moreau, T., Ceze, L., Guestrin, C., and Krishnamurthy, A. Learning to opti- mize tensor programs. Advances in Neural Information Processing Systems, 31, 2018b. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers.\n\n--- Segment 30 ---\nAdvances in Neural Information Processing Systems, 31, 2018b. Child, R., Gray, S., Radford, A., and Sutskever, I. Gen- erating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choy, C., Gwak, J., and Savarese, S. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition, pp. 3075 3084, 2019. Dao, T., Chen, B., Liang, K., Yang, J., Song, Z., Rudra, A., and Re, C. Pixelated butterfly: Simple and efficient sparse training for neural network models. arXiv preprint arXiv:2112.00029, 2021. Daum e III, H. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009. Davis, T. A. and Hu, Y. The university of florida sparse matrix collection. ACM Transactions on Mathematical Software (TOMS), 38(1):1 25, 2011. Day, O. and Khoshgoftaar, T. M. A survey on heterogeneous transfer learning. Journal of Big Data, 4:1 42, 2017. Duan, L., Xu, D., and Tsang, I. Learning with augmented features for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660, 2012. Gerogiannis, G., Yesil, S., Lenadora, D., Cao, D., Mendis, C., and Torrellas, J. Spade: A flexible and scalable ac- celerator for spmm and sddmm. In Proceedings of the 50th Annual International Symposium on Computer Ar- chitecture, ISCA 23, New York, NY, USA, 2023. Associ- ation for Computing Machinery. ISBN 9798400700958. doi: 10.1145 3579371.3589054. URL org 10.1145 3579371.3589054.\n\n--- Segment 31 ---\nISBN 9798400700958. doi: 10.1145 3579371.3589054. URL org 10.1145 3579371.3589054. Gerogiannis, G., Aananthakrishnan, S., Torrellas, J., and Hur, I. Hottiles: Accelerating spmm with heterogeneous accelerator architectures. In 2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 1012 1028. IEEE, 2024. Graham, B. and Van der Maaten, L. Submanifold sparse con- volutional networks. arXiv preprint arXiv:1706.01307, 2017. 10 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Hegde, K., Asghari-Moghaddam, H., Pellauer, M., Crago, N., Jaleel, A., Solomonik, E., Emer, J., and Fletcher, C. W. Extensor: An accelerator for sparse tensor algebra. In Proceedings of the 52nd Annual IEEE ACM International Symposium on Microarchitecture, pp. 319 333, 2019. Hong, C., Sukumaran-Rajam, A., Nisa, I., Singh, K., and Sadayappan, P. Adaptive sparse tiling for sparse matrix multiplication. In Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming, pp. 300 314, 2019. Jiang, P., Hong, C., and Agrawal, G. A novel data trans- formation and execution strategy for accelerating sparse matrix multiplication on gpus. In Proceedings of the 25th ACM SIGPLAN symposium on principles and practice of parallel programming, pp. 376 388, 2020. Jin, H., Yue, Z., Zhao, Z., Du, Y., Deng, C., Srivastava, N., and Zhang, Z. Vesper: A versatile sparse linear algebra accelerator with configurable compute patterns. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. Kaufman, S., Phothilimthana, P., Zhou, Y., Mendis, C., Roy, S., Sabne, A., and Burrows, M. A learned performance model for tensor processing units.\n\n--- Segment 32 ---\nIEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. Kaufman, S., Phothilimthana, P., Zhou, Y., Mendis, C., Roy, S., Sabne, A., and Burrows, M. A learned performance model for tensor processing units. Proceedings of Ma- chine Learning and Systems, 3:387 400, 2021. Kjolstad, F., Kamil, S., Chou, S., Lugato, D., and Ama- rasinghe, S. The tensor algebra compiler. Proc. ACM Program. Lang., 1(OOPSLA), oct 2017. doi: 10.1145 3133901. URL 1145 3133901. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. Langley, P. Crafting papers on machine learning. In Langley, P. (ed. ), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207 1216, Stan- ford, CA, 2000. Morgan Kaufmann. Li, Z., Li, J., Chen, T., Niu, D., Zheng, H., Xie, Y., and Gao, M. Spada: Accelerating sparse matrix multiplication with adaptive dataflow. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pp. 747 761, 2023. Liang, H., Fu, W., and Yi, F. A survey of recent advances in transfer learning. In 2019 IEEE 19th International Con- ference on Communication Technology (ICCT), pp. 1516 1523, 2019. doi: 10.1109 ICCT46805.2019.8947072. Mu noz-Mart ınez, F., Garg, R., Pellauer, M., Abell an, J. L., Acacio, M. E., and Krishna, T. Flexagon: A multi- dataflow sparse-sparse matrix multiplication accelerator for efficient dnn processing. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Vol- ume 3, pp.\n\n--- Segment 33 ---\nMu noz-Mart ınez, F., Garg, R., Pellauer, M., Abell an, J. L., Acacio, M. E., and Krishna, T. Flexagon: A multi- dataflow sparse-sparse matrix multiplication accelerator for efficient dnn processing. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Vol- ume 3, pp. 252 265, 2023. Naumov, M., Chien, L., Vandermersch, P., and Kapasi, U. Cusparse library. In GPU Technology Conference, vol- ume 12, 2010. Neyshabur, B., Sedghi, H., and Zhang, C. What is being transferred in transfer learning? Advances in neural information processing systems, 33:512 523, 2020. Pal, S., Beaumont, J., Park, D.-H., Amarnath, A., Feng, S., Chakrabarti, C., Kim, H.-S., Blaauw, D., Mudge, T., and Dreslinski, R. Outerspace: An outer product based sparse matrix multiplication accelerator. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), pp. 724 736. IEEE, 2018. Ryu, J. and Sung, H. Metatune: Meta-learning based cost model for fast and efficient auto-tuning frameworks. arXiv preprint arXiv:2102.04199, 2021. Sasaki, Y., Takahashi, K., Shimomura, Y., and Takizawa, H. A cost model for compilers based on transfer learning. In 2022 IEEE International Parallel and Distributed Pro- cessing Symposium Workshops (IPDPSW), pp. 942 951. IEEE, 2022. Shen, Z., Liu, Z., Qin, J., Savvides, M., and Cheng, K.-T. Partial is better than all: Revisiting fine-tuning strategy for few-shot learning. In Proceedings of the AAAI confer- ence on artificial intelligence, volume 35, pp. 9594 9602, 2021. Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of transfer learning.\n\n--- Segment 34 ---\n9594 9602, 2021. Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of transfer learning. Journal of Big data, 3:1 40, 2016. Won, J., Mendis, C., Emer, J. S., and Amarasinghe, S. Waco: Learning workload-aware co-optimization of the format and schedule of a sparse tensor program. In Proceedings of the 28th ACM International Confer- ence on Architectural Support for Programming Lan- guages and Operating Systems, Volume 2, ASPLOS 2023, pp. 920 934, New York, NY, USA, 2023. Associa- tion for Computing Machinery. ISBN 9781450399166. doi: 10.1145 3575693.3575742. URL org 10.1145 3575693.3575742. Yang, H., Liu, Y., Luan, Z., Gan, L., Yang, G., and Qian, D. Input-aware sparse tensor storage format selection for optimizing mttkrp. Computer, 56(08):4 7, aug 2023. ISSN 1558-0814. doi: 10.1109 MC.2023.3279447. 11 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Ye, Y. and Ji, S. Sparse graph attention networks. IEEE Transactions on Knowledge and Data Engineering, 35 (1):905 916, 2021. Ye, Z., Lai, R., Shao, J., Chen, T., and Ceze, L. Sparse- tir: Composable abstractions for sparse compilation in deep learning. ASPLOS 2023, pp. 660 678, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9781450399180. doi: 10.1145 3582016.3582047. URL 1145 3582016.3582047. Zhai, Y., Zhang, Y., Liu, S., Chu, X., Peng, J., Ji, J., and Zhang, Y. Tlp: A deep learning-based cost model for tensor program tuning. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pp. 833 845, 2023.\n\n--- Segment 35 ---\nIn Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, pp. 833 845, 2023. Zheng, L., Jia, C., Sun, M., Wu, Z., Yu, C. H., Haj-Ali, A., Wang, Y., Yang, J., Zhuo, D., Sen, K., et al. Ansor: Gen- erating {High-Performance} tensor programs for deep learning. In 14th USENIX symposium on operating sys- tems design and implementation (OSDI 20), pp. 863 879, 2020. Zheng, L., Liu, R., Shao, J., Chen, T., Gonzalez, J. E., Sto- ica, I., and Ali, A. H. Tenset: A large-scale program performance dataset for learned tensor compilers. In Thirty-fifth Conference on Neural Information Process- ing Systems Datasets and Benchmarks Track (Round 1), 2021. URL id aIfp8kLuvc9. Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43 76, 2020. 12 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning A. Problem Formulation In this work, our aim is to build accurate learned cost models for emerging hardware platforms to enable faster identification of optimal program configurations. A key challenge is the need to maximize the accuracy of the cost model (accuracy objective) while using as few expensive (i.e. collected through simulation) data samples as possible (data collection objective). We first formalize the program optimization objective and then tie it with the cost model objectives. A.1. Program Optimization Selection The goal of program optimization in sparse tensor programs is to select the optimal program configuration for a given hardware platform and input sparsity pattern from the total configuration space. Let configuration space Ca be the set {c1, c2, . . . , cma} of all valid program configurations for a given hardware platform a (ma Z ).\n\n--- Segment 36 ---\n. , cma} of all valid program configurations for a given hardware platform a (ma Z ). For example, for CPU, a valid configuration from CCP U is a tuple of program configuration parameters for loop strip-mining, loop reordering, and format reordering (Table 1). The optimal program configuration minimizes the execution time of a sparse tensor program. For an input sparse matrix (sparsity pattern) M, the optimal program configuration on platform a can be given as, c arg minci Ca Ta(M, ci), where Ta is the execution time function for platform a (ground truth runtime). The execution time for the optimal program configuration is given by t Ta(Ml, c ). A.2. Cost Model Performance and Data Efficiency Objectives We approximate the ground truth runtime Ta using learned cost models. Usually, these cost models are trained with one objective: to achieve high accuracy. However, due to the high cost of simulation in emerging hardware, we also want to minimize the amount of data samples required from these platforms for model training. We formalize these two objectives as follows. Data Collection Objective (DCE). Let Da {(Ml, ci), ti i ma , l Z } be the dataset collected from hardware platform a, and let βa represent the average cost of collecting a single data sample from the platform. Our objective is to minDa βa Da . Accuracy Objective. Let CMa (which approximates Ta) be the learned cost model trained on dataset Da. If the best program configuration returned by the cost model (c CMa) has an actual execution time t CMa, our objective is to min t CMa t , where t is the execution time for the optimal configuration. For a set of input sparse matrices {M1, M2, . . . , Mk}, our objective can be extended to minimizing the Absolute Percentage Error (APE) across all matrices: APE 1 k k X l 1 t CMa,Ml t Ml t Ml 100 where t CMa,Ml denotes the execution times for the predicted best program configuration for the input sparse matrix Ml and t Ml denote the optimal program configurations for the same matrix. A.3. Evaluations for Cost Model Objectives To evaluate the cost model objectives, we conducted the following experiments for SpMM on SPADE. For simplicity in the calculations, we set βCPU 1 and βSPADE 1000.\n\n--- Segment 37 ---\nEvaluations for Cost Model Objectives To evaluate the cost model objectives, we conducted the following experiments for SpMM on SPADE. For simplicity in the calculations, we set βCPU 1 and βSPADE 1000. However, a CPU execution typically takes milliseconds, while a SPADE execution can extend up to two weeks. We explored 11 distinct models across four different categories, differentiated by the number of data samples they were trained on, while the cost model architecture remained the same. Category I consists of models (NT d) trained exclusively on data samples from d matrices executed on SPADE. Category II includes transfer-learned models (TL d), which were pre-trained with data samples from 100 matrices on CPU (10,000 data samples) and then fine-tuned on SPADE with data samples from d matrices. Category III consists of models (CPU d) pre-trained with varying numbers of data samples from d matrices on CPU and then fine-tuned on data samples from 5 matrices on SPADE. Finally, we did zero-shot inference (Zero-Shot) from a model pre-trained on CPU with data samples from 100 matrices without additional fine-tuning on SPADE. Models trained exclusively on SPADE data samples (NT d) generally exhibit increasing speedup and decreasing APE as the number of SPADE data samples increases. For example, NT 1000, trained on 100,000 SPADE data samples, achieves the highest speedup of 1.43 and an APE of 7.06. However, the data collection overhead for these models rises significantly with the number of SPADE samples, making the use of them impractical due to the long simulation times. In contrast, the TL 13 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning models, which are pre-trained on CPU data and fine-tuned on SPADE samples, demonstrate an excellent balance between speedup, APE, and DCE. TL 5 model, for instance, delivers a competitive speedup of 1.40 and a low APE of 7.28, while maintaining an excellent DCE of 0.51.\n\n--- Segment 38 ---\nIn contrast, the TL 13 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning models, which are pre-trained on CPU data and fine-tuned on SPADE samples, demonstrate an excellent balance between speedup, APE, and DCE. TL 5 model, for instance, delivers a competitive speedup of 1.40 and a low APE of 7.28, while maintaining an excellent DCE of 0.51. Model Data Samples COGNATE (Top-1) Speedup APE DCE 106 CPU SPADE NT 5 - 500 1.29 15.02 0.50 NT 100 - 10000 1.38 9.42 10.00 NT 1000 - 100000 1.43 7.06 100.00 TL 5 (CPU 100) 10000 500 1.40 9.58 0.51 TL 100 10000 10000 1.41 8.74 10.01 TL 1000 10000 100000 1.42 7.28 100.01 CPU 5 500 500 1.07 27.80 0.50 CPU 20 2000 500 1.21 19.35 0.50 CPU 500 50000 500 1.36 16.34 0.55 CPU 1000 100000 500 1.19 36.00 0.60 Zero-Shot (CPU) 10000 - 0.71 46.22 0.01 Table 2: Comparison of cost model performance with varying data samples from CPU and SPADE. A.4. Learning Objective Our objective is to train a cost model that effectively learns to identify a program configuration that minimizes the runtime of a sparse operation for a given sparsity pattern. To achieve this, we begin by training our cost model to learn the relative rankings of program configurations during execution, enabling it to accurately identify optimal configurations based on their performance. This objective improves robustness to noise and runtime scale variance, which are common in early-stage accelerator performance data, as the model focuses on preserving relative orderings. This also enables us to efficiently integrate our cost model with a search technique to efficiently select the top-k (k-best) program configurations from the configuration space. Furthermore, prior work (Kaufman et al., 2021) has shown that training with ranking loss significantly improves a model s ability to identify optimal configurations. We use the pairwise ranking loss as our learning objective (implemented using margin ranking loss) to rank program configurations based on their true performance differences.\n\n--- Segment 39 ---\nFurthermore, prior work (Kaufman et al., 2021) has shown that training with ranking loss significantly improves a model s ability to identify optimal configurations. We use the pairwise ranking loss as our learning objective (implemented using margin ranking loss) to rank program configurations based on their true performance differences. For a given input matrix M, the pairwise ranking loss (L) across all program configuration pairs can be defined as L P (c1,c2) max(0, 1 (ˆrM,c1 ˆrM,c2)) δtrue; δtrue sign(tM,c1 tM,c2) where ˆrM,c1 and ˆrM,c2 are the predicted scores for configurations c1 and c2, respectively; tM,c1 and tM,c2 represent their actual runtimes; and δtrue signifies the true performance difference where sign(x) returns 1 if x 0, -1 if x 0, and 0 if x 0. This ensures that the model is penalized when the predicted ranking does not align with the true ranking. By minimizing this loss (L), COGNATE improves its ability to accurately rank and identify the top-k program configurations. This also contributes to achieving our accuracy objective (Section A.2). B. Code Optimizations Across Hardware Platforms Loop strip-mining: Breaks down large software loops into smaller segments to optimize cache utilization. Loop reordering: Adjusts the execution order of loops to improve cache efficiency. Typically, it is applied after loop strip-mining. Format reordering: Reorganizes the data structure layout of sparsity patterns in memory to optimize memory access patterns Parallelization: Distributes tensor computations across multiple threads or processors to run tasks simultaneously. Loop binding: Assigns specific loop iterations to threads for parallel processing. Loop unrolling: Executes multiple iterations of a loop in a single iteration, reducing loop control overhead and boosting execution speed. 14 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Tiling: Decomposes a matrix into smaller blocks to optimize data reuse in the local memory and improve cache efficiency. Barrier: Applying a barrier would ensure all threads finish processing their current tile (synchronized) before progressing to the next stage. Cache bypassing: Capability of bypassing caches to to reduce cache pollution.\n\n--- Segment 40 ---\nBarrier: Applying a barrier would ensure all threads finish processing their current tile (synchronized) before progressing to the next stage. Cache bypassing: Capability of bypassing caches to to reduce cache pollution. Matrix reordering: Enhances data locality by reordering the input matrix. C. Generalizability Our approximated code mappings are designed to enable well-established code optimizations that remain effective across emerging hardware platforms. For instance, we expect loop transformations (e.g. loop strip-mining, loop reordering, tiling, etc) to be implemented regardless of the underlying hardware platform although the exact implementation may be slightly different. Any hardware-specific optimizations (code optimizations that cannot be mapped) are included in the heterogeneous component using the latent encoder. Since the dimension of the latent embedding is fixed, we can effectively finetune using an already pre-trained model. To elaborate, let us have a qualitative discussion and explore the intuition behind incorporating approximate mapping of these loop transformations into sparse accelerators, using Intel PIUMA (Gerogiannis et al., 2024; Aananthakrishnan et al., 2023) and Vesper (Jin et al., 2024) as examples. These mappings are conceptually aligned with those we applied to SPADE and GPU, highlighting the general applicability of our approach across diverse hardware backends. Intel PIUMA (Gerogiannis et al., 2024; Aananthakrishnan et al., 2023) is a configurable accelerator that has a RISC ISA making it CPU-programmable. This enables it to employ code optimizations that are available in CPUs. Hence, it is possible to implement SpMM and SDDMM sparse operations (kernels) with code optimizations such as loop reordering with a one-to-one mapping. Similarly, loop strip-ming and tiling can be mapped. However, similar to SPADE, where we accounted for the barrier optimization in the mapping process, one would need to consider the PIUMA scratchpad reuse . Vesper (Jin et al., 2024) is another reconfigurable accelerator designed for sparse computations, supporting three dataflow models implemented through distinct loop traversal orders. While the authors refer to the use of tiling, they do not provide implementation details, source code, or a description of the tile size selection mechanism.\n\n--- Segment 41 ---\nVesper (Jin et al., 2024) is another reconfigurable accelerator designed for sparse computations, supporting three dataflow models implemented through distinct loop traversal orders. While the authors refer to the use of tiling, they do not provide implementation details, source code, or a description of the tile size selection mechanism. Based on standard tiling practices, we can approximate Vesper s approach using loop stripping and loop reordering within our representation. Intel PIUMA is proprietary, and Vesper s source code was not available. This made it infeasible to test our hypothesis on these accelerators. Hence, our current evaluation focuses only on two examples (SPADE and NVIDIA A100) primarily due to practical constraints. However, it should be emphasized that COGNATE was designed with hardware-agnostic principles in mind. We believe that as a wider range of accelerators becomes accessible to the research community, and as sparse compilation frameworks like SparseTIR (Ye et al., 2023) evolve, COGNATE can be extended with minimal changes. Assuming these accelerators were available, the data collection process would still be highly time-consuming, likely requiring millions of machine hours to gather sufficient data for training, validation, and testing. For example, collecting performance data or training, validation, and testing across all matrices and experimental settings for SPADE required approximately 4 million CPU hours. Despite parallelizing experiments across multiple machines, each with 64 CPU cores, this process spanned nearly three months. While extending the evaluation to additional hardware platforms remains an important direction, it was beyond the practical scope of this work given resource constraints. Further, frequent changes in emerging hardware may require updates to configuration mappings and fine-tuning, this challenge is significantly mitigated by our approach in COGNATE. As long as the entire kernel does not change or the newly introduced optimizations are heterogeneous, updating the mappings is relatively straight forward. However, relying solely on simulations would require rerunning them for a large number of configurations each time a change is made, resulting in significant computational and time costs. In contrast, our transfer learning-based approach significantly reduces the cost and time of running simulations. By collecting only a few data samples and fine-tuning the model, we can efficiently adapt to hardware changes without the need for extensive simulations.\n\n--- Segment 42 ---\nIn contrast, our transfer learning-based approach significantly reduces the cost and time of running simulations. By collecting only a few data samples and fine-tuning the model, we can efficiently adapt to hardware changes without the need for extensive simulations. Hence, this approach not only reduces maintenance complexity but also accelerates the design process, making it more feasible to handle frequent and timely updates in emerging hardware. 15 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning D. Cost Models for Early-Stage Sparse Accelerator Design With the flexibility of recent accelerators to have software programmable kernels (Gerogiannis et al., 2023; Jin et al., 2024; Gerogiannis et al., 2024), integration of cost models and heuristics into the DSE pipeline has become an up and coming area (Gerogiannis et al., 2024; Jin et al., 2024). For example, Vesper is a recent work that had integrated an analytical model to a configurable sparse accelerator to enable higher throughput (Jin et al., 2024). HotTiles is another work that uses an analytical model to predict the performance of different accelerator processing elements (PEs) that accommodate intra-matrix heterogeneity (Gerogiannis et al., 2024). Further, in HotTiles, the authors acknowledged that a more accurate model could have enabled making better design decisions during the early stages. We believe that our proposed data-driven cost model framework, COGNATE, addresses this gap (resulting in speedups close to optimal) while complementing expert-driven strategies to enable more informed and better design decisions. This would effectively replace the analytical approaches with a data driven approach. The primary overhead associated with our approach arises from the need to gather data points to fine-tune the cost model. This overhead is minimal compared to the effort required for an expert to iteratively optimize a kernel for sparse workloads, where kernel performance is highly input-sensitive due to diverse sparsity patterns. E. An Example of Code Optimization Notations Used in Approximate Mappings Here, we provide an additional explanation and an illustrative example to clarify the notation used in the code op- timization mapping functions presented in Section 3.2. These are designed to approximate how high-level schedule configurations in SPADE are translated into low-level loop representations in CPU.\n\n--- Segment 43 ---\nE. An Example of Code Optimization Notations Used in Approximate Mappings Here, we provide an additional explanation and an illustrative example to clarify the notation used in the code op- timization mapping functions presented in Section 3.2. These are designed to approximate how high-level schedule configurations in SPADE are translated into low-level loop representations in CPU. The following example demon- strates how a sparse matrix-matrix multiplication (SpMM) configuration in SPADE is mapped into its corresponding loop-level representation using the defined notation. Consider the following high-level configuration for the SpMM oper- ation in SPADE: name, row panels, column panels, split, barrier, bypass, reorder, time 144, 4, 1024, 1, 0, 0, 0, 38.83592. Here, row panels, column panels, and split define the tiling strategy, while the binary flags barrier, bypass, and reorder indicate the use of additional code op- timizations. Using our mapping functions, this configuration is mapped into the following loop-level represen- tation: name, i split, j split, k split, loop 1, ..., loop 7, barrier, bypass, reorder, time 144, 4, 1024, 32, 6, 7, 2, 4, 1, 3, 5, 0, 0, 0, 38.83592. In this mapped form, the tiling parameters are converted to i split, j split, and k split, which define how the loop indices are partitioned across the three dimensions. The sequence loop 1 through loop 7 encodes the execution order of the nested loops, and the binary flags are retained to preserve platform-specific scheduling decisions. This example demonstrates how our framework captures key aspects of tiling structure, loop ordering, and other scheduling optimizations.\n\n--- Segment 44 ---\nThe sequence loop 1 through loop 7 encodes the execution order of the nested loops, and the binary flags are retained to preserve platform-specific scheduling decisions. This example demonstrates how our framework captures key aspects of tiling structure, loop ordering, and other scheduling optimizations. F. Hyperparamters Table 3: Hyperparameters for model training fine-tuning Hyperparameter Value Learning Rate 0.0001 Batch Size 32 Optimizer Adam Number of Epochs 100 Loss Function MarginRankingLoss Table 4: Hyperparameters for the autoencoders Hyperparameter Value Learning Rate 0.001 Batch Size 32 Optimizer Adam Number of Epochs 1000 Loss Function MSE 16 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning Table 5: Composition of layers in the Input Featurizer (IFE) Layer Description Layer 1 MinkowskiConvolution (in channels, 32, kernel size 5) Layer 2 MinkowskiConvolution (32, 32, kernel size 3) Layer 3 MinkowskiConvolution (32, 64, kernel size 3) MinkowskiMaxPooling Layer 4 MinkowskiConvolution (64, 64, kernel size 3) Layer 5 MinkowskiConvolution (64, 64, kernel size 3) Layer 6 MinkowskiConvolution (64, 128, kernel size 3) MinkowskiMaxPooling Layer 7 MinkowskiConvolution (128, 128, kernel size 3) Layer 8 MinkowskiConvolution (128, 128, kernel size 3) Layer 9 MinkowskiConvolution (128, 256, kernel size 3)MinkowskiMaxPooling Layer 10 MinkowskiConvolution (256, 256, kernel size 3) Layer 11 MinkowskiConvolution (256, 256, kernel size 3) Layer 12 MinkowskiConvolution (256, 256, kernel size 3) Global Pooling Layer MinkowskiGlobalAvgPooling Table 6: Composition of layers in the Predictor (P) Component Layer Input Size Output Size Matrix Embedding (x) 128 128 Configuration Embedding (y) 53 64 Latent Embedding (z) 64 64 Concatenation (xyz) 128 64 192 Predictor Layer 1 192 128 Predictor Layer 2 128 64 Predictor Layer 3 64 1 17 COGNATE: Acceleration of Sparse Tensor Programs on Emerging Hardware using Transfer Learning G. Additional Results 0 100 200 300 400 500 600 700 Matrix 1 2 3 4 5 Geomean Speedup Speedup Optimal: 1.55 COGNATE (Top-5): 1.47 Baseline: 1.0 Figure 13: COGNATE (Top-5) per-matrix speedups (SpMM) 0 100 200 300 400 500 600 700 Matrix 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Geomean Speedup Speedup Optimal: 1.44 COGNATE (Top-1): 1.27 Baseline: 1.0 Figure 14: COGNATE (Top-1) per-matrix speedups (SDDMM) 0 100 200 300 400 500 600 700 Matrix 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Geomean Speedup Speedup Optimal: 1.44 COGNATE (Top-5): 1.39 Baseline: 1.0 Figure 15: COGNATE (Top-5) per-matrix speedups (SDDMM) 18\n\n