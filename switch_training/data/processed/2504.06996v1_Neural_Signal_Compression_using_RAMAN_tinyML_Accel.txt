=== ORIGINAL PDF: 2504.06996v1_Neural_Signal_Compression_using_RAMAN_tinyML_Accel.pdf ===\n\nRaw text length: 78457 characters\nCleaned text length: 76557 characters\nNumber of segments: 56\n\n=== CLEANED TEXT ===\n\ni Neural Signal Compression using RAMAN tinyML Accelerator for BCI Applications Adithya Krishna, Sohan Debnath, André van Schaik, Mahesh Mehendale and Chetan Singh Thakur Abstract High-quality, multi-channel neural recording is in- dispensable for neuroscience research and clinical applications. Large-scale brain recordings often produce vast amounts of data that must be wirelessly transmitted for subsequent offline analysis and decoding, especially in brain-computer interfaces (BCIs) utilizing high-density intracortical recordings with hundreds or thousands of electrodes. However, transmitting raw neural data presents significant challenges due to limited communi- cation bandwidth and resultant excessive heating. To address this challenge, we propose a neural signal compression scheme utilizing Convolutional Autoencoders (CAEs), which achieves a compression ratio of up to 150 for compressing local field potentials (LFPs). The CAE encoder section is implemented on RAMAN, an energy-efficient tinyML accelerator designed for edge computing, and subsequently deployed on an Efinix Ti60 FPGA with 37.3k LUTs and 8.6k register utilization. RAMAN leverages sparsity in activation and weights through zero skip- ping, gating, and weight compression techniques. Additionally, we employ hardware-software co-optimization by pruning CAE encoder model parameters using a hardware-aware balanced stochastic pruning strategy, resolving workload imbalance issues and eliminating indexing overhead to reduce parameter storage requirements by up to 32.4 . Using the proposed compact depthwise separable convolutional autoencoder (DS-CAE) model, the compressed neural data from RAMAN is reconstructed offline with superior signal-to-noise and distortion ratios (SNDR) of 22.6 dB and 27.4 dB, along with R2 scores of 0.81 and 0.94, respectively, evaluated on two monkey neural recordings. Keywords Convolutional neural networks (CNNs), deep learn- ing, hardware acceleration, sparse processing, machine learning, Convolutional Autoencoders (CAEs), tinyML, edge computing, stochastic processing. I. INTRODUCTION In recent years, the Brain-Computer Interface (BCI) has garnered significant attention for facilitating direct commu- nication between the human brain and external devices [1] [4]. BCIs have emerged as a revolutionary tool for advancing our understanding of the brain and are increasingly being utilized across various clinical applications [5] [7], providing inventive solutions for communication [8], control [1], [9], and rehabilitation [10] [13]. Ongoing improvements in signal A. Krishna is with the Department of Electronic Systems Engineering, Indian Institute of Science, Bangalore - 560012, India, and also with the Inter- national Centre for Neuromorphic Systems, The MARCS Institute, Western Sydney University, Australia. S. Debnath, M. Mehendale, and C. S. Thakur (Email: are with the Department of Electronic Systems Engineering, Indian Institute of Science, Bangalore - 560012, India. A. van Schaik is with the International Centre for Neuromorphic Systems, The MARCS Institute, Western Sydney University, Australia. This work was supported by the Pratiksha Trust grant BCD - FG SMCH-22- 2106 and INAE grant INAE 121 AKF 48 (SAP code - SP INAE-23-0001). Corresponding author processing, machine learning algorithms, and neurotechnology pave the way for BCIs to revolutionize healthcare, human- computer interaction, and beyond. BCIs can potentially re- store lost sensory abilities, such as vision and hearing [14] [16] through stimulation and regain lost motor functions for individuals with motor impairments, such as those caused by conditions like ALS or spinal cord injury [17], [18]. In clinical settings, BCIs can interpret the user s intentions from brain activity and utilize this information to control the person s limb or an assistive device, such as a prosthetic arm or a computer cursor, necessitating simultaneous recordings from a relatively large population of neurons to achieve acceptable prediction accuracy. Intracortical neural recording systems have evolved significantly, progressing from widely used recording arrays like the Utah Array [19], which supports up to a hundred recording sites, to high-density electrodes such as [20], Neuralink [21] and Neuropixel [22]. These state-of- the-art intracortical neural recording systems have thousands of recording sites, producing massive amounts of neural data that require wireless transmission for offline signal analysis and decoding. Continuous wireless transmission of raw neural data poses a significant challenge due to constrained communication bandwidth (low data rates) and leads to excessive heating. For instance, a system comprising 1024 channels, sampled at 30 kS s with 16 bits per sample, generates 491.52 Mb s of data that must be wirelessly transmitted. To mitigate this, the data undergoes compression before transmission over wireless channels. Compressed sensing (CS) is one such prominent method that reduces the data dimensionality by multiplying the input signal X of dimension (1 M) with a sensing matrix of size (M N) to produce the compressed output signal Y of dimension (1 N), with compression ratio (CR) M N(M N) [23], [24]. Reconstruction of the compressed signal to its original form utilizes CS-based recon- struction algorithms [25], with the emerging interest in deep learning-based methods [24]. Additionally, various lossless compression techniques, categorized into dictionary-based and statistical-based schemes, offer further compression options for neural data. Dictionary-based compression [26] [29] relies on creating a dictionary of frequently occurring patterns in the data and replacing them with shorter codes or references to entries in the dictionary. Statistical compression utilizes statistical models to represent and encode data more efficiently. One standard method of statistical compression is Huffman coding [30], [31]. In Huffman coding, symbols with higher fre- quencies are assigned shorter codewords, while symbols with lower frequencies are assigned longer codewords. This ensures arXiv:2504.06996v1 [cs.AR] 9 Apr 2025 ii that more common symbols are represented with fewer bits, leading to overall compression. Lossless compression schemes like Huffman coding ensure that the original input signal is accurately reconstructed from the compressed data without any reconstruction errors. In contrast, lossy compression methods, such as compressed sensing (CS), can achieve significantly higher compression ratios but at the expense of introducing reconstruction errors. Additionally, several spike compression schemes have been proposed [32] [35], with associated data transformations to better represent signal space [36]. These methods focus exclusively on compressing spike waveforms for applications employing single-unit activities (SUAs) and multi-unit activities (MUAs). In contrast, our work focuses on compressing local field potentials (LFPs), where spike detection, extraction, and sorting methods are not applicable. LFPs, which represent slower voltage variations ( 300 Hz) and have lower spatial resolution compared to SUAs and MUAs, are more stable due to factors like electrode drift, neuron drop-out, and scarring at the electrode-tissue interface over time [37]. While MUA-based BCIs primarily decode motor cortex activity, LFP-based BCIs target higher-level cognitive regions like the posterior parietal cortex, enabling advanced cognitive decoding [38]. Studies have shown that LFP signals can reliably decode neural activity; for example, a study demonstrated that movement intentions, kinematic trajectories, and movement types, can be accurately predicted from LFP signals [39]. In this work, we propose a convolutional autoencoder-based neural signal compression scheme for compressing the LFPs. Autoencoders are neural networks used for the unsupervised learning of efficient codings or representations of the input data [40] [43]. They consist of two main components: an encoder and a decoder. The encoder network maps the in- put data to a lower-dimensional latent space representation, also known as a codeword or encoding, which effectively compresses the neural signal. The decoder network takes the encoded representation produced by the encoder and attempts to reconstruct the original input signal from it. The encoder network is implemented using RAMAN, a Re-configurable and spArse tinyML Accelerator for infereNce proposed in [44] for edge computing applications to compress the input data to a latent space. A. Our Contributions The main contributions of this paper can be summarized as follows: 1) We propose a neural signal compression scheme using convolutional autoencoders (CAEs) that achieves superior compression ratios compared to existing LFP compres- sion methods, as detailed in Section II and Section IV-C. We evaluate the performance of various CAE architectures based on SNDR and R2 scores, employing depth-wise separable convolutions to reduce Multiply- Accumulate (MAC) operations. 2) We employ RAMAN, a low-power, compact tinyML accelerator designed for edge computing [44], to im- plement the CAE encoder. RAMAN leverages sparsity in activations and weights to reduce latency, memory storage, and power consumption. We assess RAMAN s performance with MobileNetV1-based convolutional au- toencoder models and introduce our custom, compact models, DS-CAE1 and DS-CAE2 (Depthwise Separable Convolutional Autoencoders), which achieve reasonable SNDR and R2 scores with minimal model size and MAC count. 3) We employ a novel hardware-aware balanced stochastic weight pruning scheme incorporating Linear Feedback Shift Registers (LFSRs) to reduce parameter memory requirements by up to 32.4 . 4) We employ a memory optimization technique involving overlapping input and output activations on the same memory space, that reduces the peak activation memory storage by 37.5 . The specific implementation details can be found in Section III-B. The rest of the paper is organized as follows: Section II introduces the proposed neural signal compression scheme utilizing CAE and explains the fundamentals of autoencoders. Section III describes the RAMAN architecture, its features, and the hardware-software co-optimization approach used to handle workload imbalances and minimize parameter mem- ory requirements. This involves employing a hardware-aware balanced stochastic pruning scheme utilizing LFSRs. Section IV presents the FPGA implementation results, assesses model performance using SNDR and R2 score metrics across various CAE topologies, and compares the stochastic pruning scheme ADC RAMAN Neural Amp Intracortical Recordings Digitized and compressed neural data Bandwidth Constraints De- compress Decoding Analysis Control Head Unit In Silico Fig. 1: A system-level overview of the proposed neural signal compression scheme utilizing convolutional autoencoders. iii with the conventional magnitude-based pruning scheme. It also highlights the advantages of the stochastic pruning scheme in terms of memory size reduction. Finally, Section V concludes the paper. II. PROPOSED COMPRESSION SCHEME USING CONVOLUTIONAL AUTOENCODERS The proposed neural signal compression scheme is de- picted in Fig. 1. It comprises a signal conditioning module equipped with a neural amplifier and an analog-to-digital converter (ADC) for amplifying and digitizing the neural signal. Subsequently, the digitized neural signal is fed to the RAMAN tinyML accelerator for compression using convo- lutional autoencoders. All signal processing steps, including acquisition, amplification, digitization, and compression, occur in a head unit mounted on the head. The compressed data is transmitted wirelessly, while the decompression and decoding are conducted offline. A. Convolutional autoencoders Encoder (on RAMAN) Compressed Vector Convolutional Autoencoder Decoder (Off-chip) Input Neural Signal filters Decoder Transposed Convolutions Reconstructed Neural Signal filters 1 1 Depthwise (DW) Pointwise (PW) Encoder DWS Convolutions 1 1 Fig. 2: A convolutional autoencoder with an encoder to convert the input neural signal window of size C Tw to a compressed vector (in latent space) of size 1 1 γ and a decoder to recover back the original signal from the compressed vector. The encoder section employs depthwise separable (DWS) con- volutions, while the decoder utilizes transposed convolutions. Convolutional autoencoders (CAEs) are a type of neural network architecture that combines the principles of convo- lutional neural networks (CNNs) with autoencoder structures. They are designed to learn efficient representations of input by encoding it into a lower-dimensional latent space and then reconstructing the original input from this representation as shown in Fig. 2. The input neural signal was divided into 50 ms windows at a 2 KHz sampling rate, giving 100 samples per window (Tw). The CAE input is a 2D matrix sized C Tw, where C is the number of recording sites channels set to 96 in our implementation. The encoder output is a vector of shape 1 1 γ, representing the compressed data. Thus, the CR achieved by the proposed compression scheme is C Tw γ. The CAEs are composed of two main parts: an encoder and a decoder. 1) Encoder Compression: The encoder part of a CAE typ- ically consists of convolutional layers followed by pooling layers. These layers extract essential features from the input data and reduce its dimensionality. The encoding process can be represented as: z f(W x b) (1) where, x is the input data, W represents the convolutional filters, b is the bias, denotes the convolution operation, f is the activation function, such as ReLU (Rectified Linear Unit), applied element-wise and z is the encoded representation (also called latent or compressed representation) of the input data. In our design, we employ depthwise separable (DWS) convolutions to minimize the number of operations. The DWS convolutions split the convolution process into a depth-wise and a point-wise convolution. The depthwise convolution operation can be represented as: OAh,w,m bm Kh 1 X i 0 Kw 1 X j 0 Wi,j,m IAh sh i,w sw j,m where W is the depthwise convolutional kernel of size Kh Kw M. The mth filter in W is applied to the mth channel in the input activation (IA) tensor to produce the mth channel of the filtered output activation (OA) tensor. The strides along the height and width are denoted by sh and sw, respectively. The pointwise convolution operation can be expressed as: OAh,w,n bn M 1 X m 0 Wm,n IAh,w,m where M is the number of input channels, and N is the number of filters or output channels. Compared to the standard con- volutions [45], the DWS convolutions reduce the computation by: 1 N 1 Kh Kw 2) Decoder Decompression: The decoder part of a CAE consists of upsampling or transposed convolutional layers. The purpose of the decoder is to reconstruct the original input data from the encoded representation. The decoding process can be represented as: x g(W z b ) (2) where, z is the encoded representation obtained from the encoder, W represents the decoding filters, b is the bias, g is the activation function, x is the reconstructed output data. We use transposed convolutions for decoding the compressed data, which can be represented as: OAh,w,n bn M 1 X m 0 hb X i ha wb X j wa Wi,j,m,n IA h i sh , w j sw ,m 1Q iv where ha max 0, h sh(H 1) , hb min (h, Kh 1), wa max 0, w sw(W 1) , and wb min (w, Kw 1). H and W represent the height and width of the input activation tensor (IA), respectively. Additionally, 1Q, the indicator func- tion of the statement Q h i sh Z w j sw Z , yields 1 if Q is true, and 0 otherwise. Here Z denotes the set of integers. Similarly, the depthwise transposed convolution operation can be expressed as: OAh,w,m bm hb X i ha wb X j wa Wi,j,m IA h i sh , w j sw ,m 1Q The objective of the CAE is to minimize the reconstruction error between the input data and the reconstructed output. A standard loss function used for this purpose is the Mean Absolute Error (MAE) loss, given by: L 1 P P X i 1 xi x i (3) where P is the number of data samples, xi and x i are the ith input and reconstructed output data samples, respectively. By minimizing this loss function, the CAE learns to encode the input data into a lower-dimensional representation while retaining important information for accurate reconstruction. III. HARDWARE ARCHITECTURE This section introduces RAMAN architecture, features, and hardware-aware balanced stochastic pruning scheme. A. Top-Level Architecture The top-level architecture of the RAMAN accelerator uti- lized to deploy the encoder of CAE for neural data compres- sion is depicted in Fig. 3. The architecture comprises compute, memory, and control subsystems. The computing subsystem comprises a processing element (PE) array for performing MAC operations, an activation sparsity engine to exploit sparsity in activations, and a post-processing module (PPM) responsible for the rectified linear unit (ReLU) activation, quantization, pooling, bias addition, and residual addition. The memory subsystem consists of a global memory to store activations and parameters and a cache to exploit temporal data reuse. The control subsystem includes a top-level controller that schedules and sequences different operations and issues commands to various processing and memory blocks. A com- prehensive description of the architecture is presented in [44]. In this paper, we introduce a novel stochastic pruning scheme for weight pruning, that eliminates the indexing overhead of the compressed weights compared to our previous implemen- tation that utilizes conventional magnitude-based pruning [7], [44], [46], [47]. B. RAMAN features The architectural features of the RAMAN tinyML acceler- ator tailored for edge computing are outlined as follows: 1) Sparsity: RAMAN exploits both input activation and weight sparsity to reduce latency, memory access, and Activation Parameter Cache Global Memory PE PE Array PE PE PE PE PE PE PE PE PE PE PE Post Processing Activation Sparsity Engine Top-Level Controller Instruction Memory Layer Config. Inst. Stream IAs OAs Parameters Column Router Row Router LFSR Block Fig. 3: Top-Level architecture of the RAMAN accelerator. storage. RAMAN can skip the processing cycles with zero activations and weights to minimize the processing latency. The sparse weight matrices are compressed and stored in the memory. The stochastic pruning scheme pre- sented in Section III-C generates the compressed weight indices on the fly, eliminating the need for explicit index storage in memory. 2) Dataflow: RAMAN employs Gustavson s inspired [48] dataflow with optimal input and output activation reuse to reduce memory access. The dataflow reduces the partial sums (Psums) within the processing element to eliminate the Psum writeback traffic. 3) Peak Activation Memory Reduction: The state-of-the-art accelerators typically segment the Input Activations (IAs) and Output Activations (OAs) within the memory, with the total activation memory being the sum of IA and OA memory spaces. In our approach, we eliminate logical partitioning, enabling OAs to directly overwrite the IA memory space, as illustrated in Figure 4. This is achieved by pre-fetching the IA tile into the cache, making the original content in the activation memory redundant, and enabling OA overwriting within the same memory space. A detailed explanation can be found in [44]. Cache Compute IA Tile 1 OA Tile 1 IA Tile 2 IA Tile K IA OA Overlapping IA Tile 1 IA Tile 2 OA Tile 1 IA Tile K Act. Mem No IA OA Overlapping Vs. IA OA Overwrite OA on IA mem. space OA stored in a diff. location Prefetch IA tile to cache IA OA Fig. 4: Peak activation memory reduction using IA OA mem- ory overlapping. 4) Programmability: RAMAN is capable of supporting var- ious types of neural network topologies, including stan- v Seed 112 8 50 -1 -100 44 112 0 50 0 -100 0 Magnitude Pruning 50 0 Idx: 1 2 3 4 5 0 1 2 3 4 5 112 8 50 -1 -100 44 0 8 50 0 0 44 Stochastic Pruning 50 0 Idx: 1 2 3 4 5 0 1 2 3 4 5 Re-train Re-train Weight Weight 112 50 -100 Compression Val Idx 8 50 44 Compression Val LFSR Idx 1, 3, 5 (Generated on-the-fly) Store Val Idx in Memory Store only Val in Memory 0 3 4 Fig. 5: Illustration of stochastic pruning compared with conventional magnitude-based pruning. The magnitude-based pruning retains the stronger weights and prunes the rest. The sparse weight vector is then compressed and stored as a value (Val) and index (Idx) pair. On the other hand, stochastic pruning retains the weight values whose indices are covered by LFSR-generated pseudo-random sequence. Since the indices are generated on the fly during inference, only the weight values are stored in the memory, eliminating the need for explicit index storage. The LFSR s seed (initial state of flip flops) and structure (feedback polynomial) are kept constant during training and inference. dard CNN models as well as separable convolutions. The separable convolution models comprise depth-wise (DW) and point-wise (PW) layers, which effectively reduce MAC operations compared to standard convolutions [49]. Additionally, RAMAN can handle operations such as max pooling, average pooling, and fully connected (FC) layers. C. Hardware-aware Balanced Stochastic Pruning We present a novel hardware-aware balanced stochastic weight pruning scheme aimed at reducing memory storage, access, and latency. Traditionally, magnitude-based pruning removes weaker synaptic weights while retaining stronger ones. However, this approach involves storing both indices and non-zero weight values, leading to additional indexing overhead as shown in Fig. 5. Using magnitude-based pruning, our previous implementation [7], [44] utilized 8-bit weights and 4-bit indices to store compressed non-zero weights. In this work, we introduce a stochastic pruning approach that eliminates index storage in memory, storing only the non- zero weight values. Karimzadeh et al. [50] propose a similar pruning strategy based on the LFSR-generated pseudo-random sequences, primarily focusing on software implementation and analysis. However, their study does not extend to exploring the hardware feasibility of integrating this strategy into an actual ML accelerator. During the training phase, this scheme generates a pseudo- random sequence (PRS) serving as indices to sparsify the weight matrix. The synaptic weights corresponding to the indices covered by the PRS are retained, while the remaining weights are forced to zero. Subsequently, the un-pruned weight values (covered by the PRS) are retrained in the subsequent step. The pseudo-random sequence is generated using a linear feedback shift register (LFSR), offering a straightforward implementation during inference. This method enables the on- the-fly generation of indices without the need to store them in n 16 1 16 NZ 12 NZ 1 M N filters N M n 16 n 16 Pruning (25 ) 25 Pruning (12:16) 50 Pruning (8:16) 75 Pruning (4:16) Balanced Pruning PE Array Θ: 4 (75 Pruning) Θ: 8 (50 Pruning) Θ: 12 (25 Pruning) a b c p 0 a 0 0 8 NZ 8 NZ 8 NZ 8 NZ 8 NZ 8 NZ 8 NZ 12 NZ 12 NZ 12 NZ 12 NZ 12 NZ 12 NZ 12 NZ 4 NZ 4 NZ 4 NZ 4 NZ 4 NZ 4 NZ 4 NZ LFSR Idx: 0,3,4.... 0 1 2 15 d e 3 4 PE PE PE PE PE PE PE PE PE PE PE PE 1 M 2 Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ 1 2 Idx Val LFSR Generated Uniform non-zero (NZ) weight distribution 1 M 2 n 16 n 16 1 M 2 1 M 2 Fig. 6: The balanced pruning strategy for different pruning percentages. Depending on the pruning percentage, each PE receives an equal number of non-zero weight elements, leading to a uniform workload across PEs. The number of non-zero weight elements (Θ) in a given tile after pruning is fixed for a given pruning percentage set at 4, 8, and 12 for 75 , 50 , and 25 pruning, respectively. The indices of the compressed weights are generated by the LFSR and not stored explicitly in the memory. memory, thereby minimizing storage overhead. Additionally, implementing the logic for the LFSR incurs only a minimal increase in area and resource utilization, as demonstrated in Section IV-A. During deployment, only the non-zero weight values are stored in the RAMAN parameter memory. The same seed and LFSR structure employed during training are used to generate vi an identical PRS sequence during inference. The non-zero weight values are then read from memory and matched with the indices generated by the LFSR. This matching ensures that each non-zero weight value is correctly multiplied by its corresponding activation value. Ultimately, the indices generated by the LFSR guarantee that all non-zero weights stored in the RAMAN memory are covered, facilitating the computation of the final output. Additionally, the balanced pruning strategy ensures a uni- form distribution of non-zero weights across weight tiles, effectively mitigating workload imbalances. Without this strat- egy, non-zero weights would be unevenly distributed among various weight tiles processed by different processing elements (PEs), resulting in workload discrepancies and performance limitations imposed by the PE handling the heaviest load. The operation of the balanced pruning strategy is illustrated in Fig. 6. Initially, N filters with an input channel dimension of M are represented as a weight matrix of size M N. Subsequently, the weight matrix is divided into tiles of size 1 n, where n is determined by the depth of the PE register-file (RF), set to 16 in our design. Each 1 n tile is then pruned based on the required sparsity level to have Θ non-zeros, where Θ is a function of the pruning percentage. For example, for 25 , 50 , and 75 pruning percentages, Θ is set to 12, 8, and 4, respectively. The weight values with indices not covered by LFSR-generated PRS are pruned. Furthermore, the LFSR- generated indices ensure that the same number of weights are pruned in each tile, resulting in structured sparsity that can be efficiently leveraged in RAMAN. Following balanced pruning, each tile contains an equal number of non-zero weights pro- cessed by different PE columns, achieving uniform workload distribution across the PE array. Each processing element (PE) contains four multiply-accumulate (MAC) units, executing four MAC operations in a single cycle and necessitating four non-zero weights per cycle. Thus, we employ four 4-bit LFSRs to generate four random indices simultaneously. With 4 MACs per PE, processing each weight tile requires Θ 4 cycles and, consequently, Θ 4 cycles to generate Θ indices using the 4 LFSRs. The initial seed of each LFSR is selected to ensure that unique Θ indices are generated for each 1 n weight tile. This approach prevents repeated indices within a tile, thereby ensuring the desired Θ non-zero elements are produced. The detailed PE architecture and operation are presented in [44]. Using LFSRs for generating PRS offers several advantages: 1) LFSRs can be implemented using a small number of flip- flops and XOR gates, making them highly compact in terms of hardware resources. 2) LFSRs generate pseudo-random se- quences on-the-fly without any memory footprint. 3) The PRS generated by LFSRs are deterministic and repeatable, meaning that the same initial state (seed) and the feedback polynomial will always produce the same sequence. This property is desirable for our use case to ensure the same PRS generation for both training and inference. 4) LFSRs typically consume less power compared to some other methods of generating pseudo-random sequences, especially when implemented in hardware. Moreover, in our scenario, the need for costly additional memory access to retrieve indices is eliminated as LFSR logic replaces index memory storage. IV. RESULTS In this section, we present the FPGA implementation results of RAMAN and evaluate the performance of CAE models across various topologies and sparsity levels. We compare the stochastic pruning scheme with the standard magnitude pruning scheme. Furthermore, we compare our neural signal compression scheme employing CAEs with existing literature. A. FPGA Implementation The RAMAN accelerator was implemented on Efinix Tita- nium Ti60 FPGA, incorporating the stochastic weight pruning scheme presented in Section III-C. The hardware specifications of the RAMAN architecture and resource utilization details are presented in Table I evaluated for a custom DS-CAE1 model and the MobileNetV1-based autoencoder model with a width multiplier of 0.25x, represented as MobileNetV1- CAE(0.25x). The specific MobileNetV1-CAE and DS-CAE model architectures are provided in Table IIa and IIb. The encoder model of the CAE utilizes CONV, DW, PW, and pooling layers, amounting to a total of 2.234 million MAC operations for the DS-CAE1 model and 22.91 million MACs for the MobileNetV1-CAE(0.25x) model. The PW layer ac- counts for the majority of these MAC operations. RAMAN employs 12 processing elements, with each PE containing four MAC units. The register-file memory utilization is 0.896 KB. The PE array utilizes 52 (0.576 KB) of the registers, as each PE consists of a 16 24b Register File (RF) to store the Psums. Additionally, RAMAN offers the flexibility to decrease the PE RF width to 20b or even lower, depending on the application, thereby reducing register utilization. The post- processing module (PPM) stores post-processing parameters in registers, resulting in 32 (0.32 KB) register utilization. The power consumption is estimated to be 47.91 mW (3.11 mW dynamic power 44.79 mW static power) at a 2 MHz clock for the DS-CAE1 model, and 53.97 mW (8.97 mW dynamic power 45 mW static power) at a 7 MHz clock for the MobileNetV1-CAE(0.25x) model. Notably, owing to the FPGA implementation, the static power dissipa- tion is significant. Meanwhile, application-specific integrated circuits (ASICs) typically offer reduced power consumption and greater energy efficiency than FPGAs, making them more suitable for implantable systems. Nevertheless, our primary objective in this research was to validate the RAMAN archi- tecture for neural signal compression using CAE. Therefore, we initially utilized FPGAs before transitioning to a full- fledged ASIC implementation. Furthermore, the power usage of our neural signal compression approach utilizing RAMAN on FPGA falls comfortably within the established power limit of the mountable device on the head [51]. The architecture utilizes 37.3k 4-input look-up tables (LUTs) and 8.6k flip-flops (FFs), which are mapped as eXchangeable Logic and Routing (XLR) cells on the Efinix FPGA [52]. Table I illustrates that the logic overhead associated with the LFSR for enabling stochastic pruning is minimal with 32 LUTs and 20 FFs totaling 46 XLR cells. The activations and weights of the encoder network were quantized to 8 bits, while the Psums generated after the MAC operation were represented with 24 bits. Consequently, vii TABLE I: Specifications and resource utilization. Platform Efinix Ti60 Layers Supported CONV, DW, PW, FC and Max Average pooling. Number of PEs 12 (4 MACs PE) Reg-file Memory 0.896 KB Clock Rate 2-7 MHz Precision Weights Activations: 8b fixed point, Partial-sums: 24b fixed point. Power (mW) 47.91 (Dynamic: 3.11, Static: 44.79) 2 MHz 53.97 (Dynamic: 8.97, Static: 45) 7 MHz XLR cells 52.3k (86 util.), 37.3k LUTs 8.6k FFs LFSR logic: 46 XLR cells (32 LUTs 20 FFs) DSPs 61 (38.12 util.) Memory Blocks (10 Kb Blocks) 92 (Param.: 10, Act.: 48, Cache: 27, Rest: 7) 151 (Param.: 69, Act.: 48, Cache: 27, Rest: 7) MAC Operations (in Millions) 2.234 (CONV: 15.47 , DW: 12.92 , PW: 71.22 , Pool: 0.39 ) 22.91 (CONV: 1.51 , DW: 8.18 , PW: 90.29 , Pool: 0.02 ) Latency (ms) 45.47 2 MHz, 47.82 7 MHz Parameter Memory (KB) Baseline floating point: 45.76 , 8b Quantized 75 PW Stochastic Pruning: 6.19 Baseline floating point: 841.92 , 8b Quantized 75 PW Stochastic Pruning: 76.08 XLR: eXchangeable Logic and Routing (XLR) cell, DSP: Digital Signal Processor DS-CAE1 model, MobileNetV1-CAE(0.25x) model the parameter memory requirement decreased from 45.76 KB to 6.19 KB (7.4x reduction) and from 841.92 KB to 76.08 KB (11x reduction) after 8-bit quantization and 75 point-wise stochastic weight pruning for the DS-CAE1 and MobileNetV1-CAE(0.25x) models, respectively. In addition, the IA and OA memory overlapping scheme outlined in [44] reduced the peak activation memory of the MobileNetV1- CAE(0.25x) model from 76.8 KB to 48 KB, representing a 37.5 reduction. The global memory, comprising parameter memory and activation memory, along with the cache, were mapped to 10 Kb block memory units available in the Efinix Ti60 FPGA [52]. The latency of RAMAN for a single input inference to generate the compressed representation is mea- sured at 45.47 ms at a clock rate of 2 MHz and 47.82 ms at a clock rate of 7 MHz for the DS-CAE1 and MobileNetV1- CAE(0.25x) models, respectively. Since the input window spans 50 ms, the achieved latency can effectively process the input within a given period. The achieved throughput of the RAMAN accelerator is 98.3 MOp s for the DS-CAE1 model at 2 MHz and 958.3 MOp s for the MobileNetV1-CAE(0.25x) model at 7 MHz. B. Dataset Description We utilized the dataset [53], comprising neural recordings from the motor cortex of two macaque monkeys, K and L, during an instructed reach and grasp task. Neural activity was captured using 10-by-10 Utah electrode arrays with only 96 active electrodes. Each recording session includes two recordings, one for each monkey. The signals were sampled at 30 kHz. In this work, we are interested in the local field potential (LFP) signals, which are the aggregate synaptic activities of populations of neurons. Given that the frequencies of interest in LFPs typically range from 0.1 to 300 Hz, the sampling rate can be substantially reduced compared to what is necessary for spike-based processing, which typically falls between 1 and 2 kS s. Thus, the signals were downsampled to 2 kS s after applying a low-pass filter with a cut-off frequency of 1 KHz. The training, validation, and test sets were split into time windows of 50 ms, each corresponding to 100 samples per window at a sampling rate of 2 KHz. Input to the CAE is a 2D matrix of size 96 (Channels) x100 (samples per window). From each recording session, the first 80 of the recording is used for training, the next 10 for validation, and the final 10 for testing. The training is conducted offline and the entire implementation pipeline can be divided into three phases: 1) Training phase: During this phase, the RAMAN encoder is disabled and the uncompressed data is transmitted for offline training. Since real-time processing is not necessary, data can be recorded in local memory (such as an SD card) and transmitted at a lower data rate (channel- wise). The entire encoder and decoder stack of the CAE is trained offline, and the encoder parameters obtained after quantization, pruning, and model compression are stored in on-chip RAMAN memory. Additionally, the encoder model topology is encoded and stored as instructions in the instruction memory for programming RAMAN. The training phase occurs only initially. 2) Deployment phase: During this phase, compressed data from RAMAN is transmitted for real-time offline de- coding or analysis. The encoder model parameters and instructions stored in RAMAN are utilized for real-time data encoding (compression). 3) Calibration phase: During this phase, the model is cali- brated to compensate for electrode drifts and other non- idealities over time. It has been reported that only about 10 of the new raw data is required for calibration, whereas the training phase necessitates 80 of the new data. Since the data size is small, it can be transmitted at a lower bandwidth and does not need to be in real-time. This phase can occur regularly for the periodic calibration of the model, and it has been reported that model calibra- tion achieves similar performance to complete re-training [54]. RAMAN encoder parameters remain static, and only the decoder is trained offline eliminating the need for training within the head unit. Several factors influence the calibration frequency, including the type of electrode, the implantation site, and the specifics of the experimental conditions. The model s performance can be empirically assessed for calibration by periodically transmitting the raw signal and comparing the reconstruction error be- tween the original raw signal and the decompressed signal from the decoder. C. Model Performance We experimented with six models. Among them, four are MobileNetV1 [49] based autoencoder models with width mul- viii TABLE II: Architecture of the models. Stage Type Stride M N Output Size Encoder Conv (3 3) s2 1 32 48 50 32 Conv dws (3 3) s1 32 64 48 50 64 Conv dws (3 3) s2 64 128 24 25 128 Conv dws (3 3) s1 128 128 24 25 128 Conv dws (3 3) s2 128 256 12 13 256 Conv dws (3 3) s1 256 256 12 13 256 Conv dws (3 3) s1 256 512 12 13 512 5 Conv dws (3 3) s1 512 512 12 13 512 Conv dws (3 3) s2 512 1024 6 7 1024 Conv dws (3 3) s1 1024 1024 6 7 1024 Avg Pool (6 7) s1 1024 1024 1 1 1024 Decoder ConvTranspose dw (6 7) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s2 1024 512 12 13 512 5 ConvTranspose (3 3) s1 512 512 12 13 512 ConvTranspose (3 3) s1 512 256 12 13 256 ConvTranspose (3 3) s1 256 256 12 13 256 ConvTranspose (3 3) s2 256 128 24 25 128 ConvTranspose (3 3) s1 128 128 24 25 128 ConvTranspose (3 3) s2 128 64 48 50 64 ConvTranspose (3 3) s1 64 32 48 50 32 ConvTranspose (3 3) s2 32 1 96 100 1 (a) MobileNetV1-CAE(1x) model. Stage Type Stride M N Output Size Encoder Conv (3 3) s2 1 16 48 50 16 Conv dws (3 3) s2 16 16 24 25 16 Conv dws (3 3) s2 16 64 12 13 64 n Conv dws (3 3) s1 64 64 12 13 64 Avg Pool (12 13) s1 64 64 1 1 64 Decoder ConvTranspose dw (12 13) s1 64 64 12 13 64 n ConvTranspose (3 3) s1 64 64 12 13 64 ConvTranspose (3 3) s2 64 16 24 25 16 ConvTranspose (3 3) s2 16 16 48 50 16 ConvTranspose (3 3) s2 16 1 96 100 1 (b) DS-CAE models. A Conv dws (3 3) layer consists of a Depthwise convolu- tional layer with filter shape 3 3 M, followed by a Pointwise convolutional layer with filter shape 1 1 M N. Compressed representation of the input signal. The filter shape of a ConvTranspose dw (Kh Kw) layer is Kh Kw M. The filter shape of a ConvTranspose (3 3) layer is 3 3 M N. n 2 for DS-CAE1 and n 1 for DS-CAE2. tipliers 1, 0.75, 0.5, and 0.25, and the others are custom depth- wise separable convolutional autoencoder models DS-CAE1 and DS-CAE2. The MobileNetV1-based CAE was chosen as a baseline model for comparison because it inherently employs depthwise separable convolutions, for which RAMAN is tuned and optimized. Additionally, MobileNet significantly reduces the number of parameters and computations in the network through depthwise separable convolutions, leading to reduced storage, latency, and memory access requirements. This reduc- tion is critical for deployment on edge devices for applications such as BCI. The architectures of the MobileNetV1-CAE model with width multiplier 1 and the two DS-CAE models are presented in Table IIa and IIb, respectively. For the MobileNetV1-CAE models with a width multiplier other than 1, the number of channels in each layer is multiplied by the width multiplier and rounded to the nearest integer which is greater than or equal to it and divisible by 16 as shown in (4). Nl,w Nl w 16 16 (4) where Nl,w and Nl are the number of channels in layer l for MobileNetV1-CAE model with width multiplier w and 1, respectively. The PyTorch framework is used to train the models. To demonstrate the benefits of employing stochastic pruning over magnitude-based pruning, we conducted training of the models using two pruning methods separately. The results are depicted in the Table III. For magnitude-based pruning, the baseline floating-point model is trained for 500 epochs. Then, pruning is applied in the order of 25 , 50 , and 75 weight sparsity. After each pruning step, the model is trained for 100 epochs with the corresponding weight sparsity level. In the case of stochastic pruning, since the prune mask is known beforehand for all the sparsity levels, the models are pruned with that specific mask at the beginning and then trained for 500 epochs. The pruned 32-bit floating point models are then quantized to 8-bit weights, and quantization-aware training (QAT) is performed for 50 more epochs using integer-only arithmetic [55]. We employed batch normalization folding [56] to enhance the efficiency of our model, thereby reducing computational overhead while maintaining performance. We trained the models using Adam optimizer and 1cycle learning rate scheduler [57], setting a maximum learning rate of 0.01 and using a mini-batch size of 128. The mean absolute error (MAE) between the input and reconstructed signals was considered as the loss function. The ability of the models to reconstruct the input signals is evaluated using two metrics, viz. signal-to-noise and distortion ratio (SNDR) and R2 score [54]. SNDR is defined as: SNDR 20 log10 x 2 x ˆx 2 (5) where x and ˆx are original and reconstructed signals respec- tively, and 2 is the L2-norm operator. Whereas R2 score is calculated as: R2 1 P i (xi ˆxi)2 P i (xi x)2 (6) where xi and ˆxi are the i-th input and reconstructed sample respectively, and x is the mean of the input signals. Fig. 7 illustrates the ablation results of our neural network models for both 32-bit floating point and 8-bit quantized versions with 0 , 25 , 50 , 75 weight sparsity. Fig. 7a and 7b presents the SNDR and R2 scores respectively for monkey K, while Fig. 7c and 7d present the same for monkey L. We have used different colors to represent different model architectures, and bit-widths are differentiated by varying the marker shapes. The size of the markers is proportional to ix 0 25 50 75 Sparsity ( ) 22.0 22.5 23.0 23.5 24.0 24.5 25.0 25.5 26.0 SNDR (dB) 1000 kB 100 kB 10 kB Monkey K 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (a) 0 25 50 75 Sparsity ( ) 0.78 0.80 0.82 0.84 0.86 0.88 0.90 R2 Score 1000 kB 100 kB 10 kB Monkey K 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (b) 0 25 50 75 Sparsity ( ) 26.0 26.5 27.0 27.5 28.0 28.5 29.0 SNDR (dB) 1000 kB 100 kB 10 kB Monkey L 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (c) 0 25 50 75 Sparsity ( ) 0.92 0.93 0.94 0.95 0.96 R2 Score 1000 kB 100 kB 10 kB Monkey L 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (d) Fig. 7: Ablation study for different model architectures with different pruning percentages and bit-widths. (a) SNDR for monkey K, (b) R2-score for monkey K, (c) SNDR for monkey L and (d) R2-score for monkey L. Marker size is proportional to the size of the encoder part of the model. The MobileNetV1-CAE(0.25x) and DS-CAE1 models with a bit-width of 8 and 75 pruning percentage are deployed on RAMAN for FPGA evaluation (highlighted by green and red boxes, respectively). the respective encoder parameter size. It is observed from the plot that the custom DS-CAE1 model with 8-bit quantization and 75 sparsity exhibits comparable performance as the MobileNetV1-CAE models despite a 99.95 reduction in parameter size as compared to the MobileNetV1-CAE(1x) model with 32-bit floating point weights and 0 sparsity. Table III presents a comparison between models pruned with stochastic pruning and those pruned with magnitude-based pruning. The analysis shows that both types of pruning result in nearly equivalent SNDR and R2 scores. However, stochastic pruning reduces the parameter size because it does not require storing the compressed weight indices explicitly in memory. For instance, at 75 sparsity, the memory size reduction for the MobileNetV1-CAE(0.25x) and DS-CAE1 models was 24.2 and 15.7 , respectively. The MobileNetV1-CAE(1x) model achieved the highest parameter memory size reduction of 32.4 . In Fig. 8, we show the original signals and their reconstruc- tions along with the absolute error using the MobileNetV1- CAE(0.25x) and DS-CAE1 models with 8-bit quantization and 75 pruning. These reconstructions were obtained for the channels that have the best, medium, and worst SNDRs. The input to the models has a dimension of 96 100, and the encoder output of the MobileNetV1-CAE(0.25x) and DS- CAE1 models are 1 1 256 and 1 1 64 (cf. Table IIa and IIb), respectively. Therefore, the MobileNetV1-CAE(0.25x) model has a CR of (96 100) 256 37.5, while the DS-CAE1 model achieves a CR of (96 100) 64 150. We investigated how CAE models generalize across diverse monkey recordings. Using data from monkeys K and L, we trained CAE models on a combined dataset with 80 of the recordings from each monkey and evaluated their performance on individual test sets. Table IV compares models trained separately on monkeys K and L with those trained on the x TABLE III: Comparison between stochastic pruning and standard magnitude-based pruning for 8-bit quantization. Model Sparsity ( ) Monkey K Monkey L Stochastic Pruning Magnitude-based Pruning Stochastic Pruning Magnitude-based Pruning SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) MobileNetV1- CAE(1.00x) 25 25.23 2.53 0.90 0.08 2456.384 25.20 2.50 0.89 0.08 3633.728 28.22 2.37 0.95 0.18 2456.384 28.19 2.37 0.94 0.17 3633.728 50 25.22 2.54 0.89 0.08 1671.488 25.20 2.51 0.89 0.08 2456.384 28.24 2.38 0.95 0.17 1671.488 28.18 2.40 0.94 0.18 2456.384 75 25.05 2.48 0.89 0.09 886.592 25.14 2.50 0.89 0.08 1279.04 28.27 2.35 0.95 0.17 886.592 28.18 2.41 0.94 0.18 1279.04 MobileNetV1- CAE(0.25x) 25 24.33 2.21 0.87 0.10 173.36 23.94 2.02 0.86 0.14 246.32 28.63 2.34 0.95 0.15 173.36 28.48 2.28 0.95 0.17 246.32 50 24.18 2.21 0.87 0.13 124.72 23.91 2.03 0.86 0.14 173.36 28.48 2.29 0.95 0.14 124.72 28.55 2.29 0.95 0.16 173.36 75 24.37 2.19 0.87 0.10 76.08 24.03 2.05 0.86 0.14 100.4 28.49 2.28 0.95 0.15 76.08 28.57 2.31 0.95 0.15 100.4 DS-CAE1 25 22.78 2.38 0.82 0.16 10.8 22.75 2.35 0.81 0.14 14.256 27.55 2.42 0.94 0.15 10.8 27.78 2.51 0.94 0.16 14.256 50 22.70 2.21 0.81 0.19 8.496 22.69 2.30 0.81 0.14 10.8 27.55 2.52 0.94 0.15 8.496 27.78 2.53 0.94 0.16 10.8 75 22.61 2.21 0.81 0.13 6.192 22.38 2.18 0.80 0.14 7.344 27.43 2.41 0.94 0.13 6.192 27.61 2.50 0.94 0.15 7.344 The parameter size of the encoder part. 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 23 SNDR 33.87 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 15 SNDR 29.55 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 76 SNDR 20.98 dB Original Reconstructed Absolute Error Mob(0.25x) Monkey K (CR 37.5) (a) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 22 SNDR 38.10 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 9 SNDR 28.22 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 43 SNDR 24.22 dB Original Reconstructed Absolute Error Mob(0.25x) Monkey L (CR 37.5) (b) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 23 SNDR 32.70 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 12 SNDR 24.20 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 76 SNDR 19.01 dB Original Reconstructed Absolute Error DS-CAE1 Monkey K (CR 150.0) (c) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 22 SNDR 38.36 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 57 SNDR 27.24 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 24 SNDR 24.44 dB Original Reconstructed Absolute Error DS-CAE1 Monkey L (CR 150.0) (d) Fig. 8: Original and reconstructed signals along with their absolute differences for MobileNetV1-CAE(0.25x) and DS-CAE1 model using 8-bit quantization and 75 sparsity. The channels with the best, medium, and worst SNDRs are shown. combined dataset, using MobileNetV1-CAE(0.25x) and DS- CAE1 models across various weight sparsity values. Notably, models trained on the combined dataset showed similar or improved performance compared to those trained on individual datasets, particularly for the MobileNetV1-CAE(0.25x) model. Additionally, we tested the performance of models trained on one monkey (e.g., monkey K) and applied to the other (e.g., monkey L). To avoid overfitting, we trained the model with 80 of the recordings from monkey K (or L) and 5 from monkey L (or K), then tested it on the remaining data of monkey L (or K). Using the MobileNetV1-CAE(0.25x) model, we observed 13-14 reductions in SNDR and 17- 18 reductions in R2 score for monkey K, and 8-16 reductions in SNDR and 3-9 reductions in R2 score for monkey L across different sparsity levels, compared to the baseline models trained on the combined dataset (80 of the recording from each monkey). For the DS-CAE1 model, we observed 7-8 reductions in SNDR and 10-13 reductions in R2 score for monkey K, and 7-8 reductions in SNDR and 6-7 reductions in R2 score for monkey L across different sparsity levels. This suggests that CAEs can learn effectively even when exposed to a small portion of unseen data during training. D. Comparison with prior works Table V compares the proposed neural signal compression scheme employing RAMAN with existing works. [25], [58], [59], [63] employ the compressed sensing scheme for data compression. Shoaran et al. [25] propose an analog domain im- xi TABLE IV: Comparison of performance of 8-bit quantized models trained on individual and combined dataset. Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 23.72 2.02 0.85 0.14 28.40 2.25 0.95 0.17 Combined 24.26 2.01 0.87 0.13 29.46 2.33 0.96 0.14 25 Individual 24.33 2.21 0.87 0.10 28.63 2.34 0.95 0.15 Combined 24.35 2.07 0.87 0.11 29.52 2.17 0.96 0.13 50 Individual 24.18 2.21 0.87 0.13 28.48 2.29 0.95 0.14 Combined 24.30 2.03 0.87 0.15 29.55 2.24 0.96 0.14 75 Individual 24.37 2.19 0.87 0.10 28.49 2.28 0.95 0.15 Combined 24.47 1.99 0.88 0.13 29.49 2.22 0.96 0.15 (a) MobileNetV1-CAE(0.25x) Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 22.72 2.35 0.81 0.14 27.71 2.47 0.94 0.17 Combined 22.65 2.24 0.81 0.14 27.32 2.35 0.93 0.17 25 Individual 22.78 2.38 0.82 0.16 27.55 2.42 0.94 0.15 Combined 22.56 2.17 0.81 0.16 27.33 2.39 0.93 0.17 50 Individual 22.70 2.21 0.81 0.19 27.55 2.52 0.94 0.15 Combined 22.53 2.15 0.81 0.16 27.35 2.45 0.93 0.15 75 Individual 22.61 2.21 0.81 0.13 27.43 2.41 0.94 0.13 Combined 22.28 2.03 0.79 0.17 27.08 2.43 0.93 0.20 (b) DS-CAE1 Training set of either Monkey K or Monkey L, evaluation is done on the test set of the same monkey. The training set of both Monkey K and L are combined together. TABLE V: Comparison of the proposed neural signal compression scheme with existing works. Shoaran et al. [25] Li et al. [58] Liu et al. [59] Park et al. [60] Khazaei et al. [61] Valencia et al. [54] Turcotte et al. [62] Shrivastwa et al. [63] Our Work Platform ASIC 180-nm ASIC 130-nm ASIC 180-nm ASIC 180-nm ASIC 130-nm ASIC 180-nm Xilinx Spartan-6 FPGA Xilinx Virtex-7 FPGA Efinix Ti60 FPGA Signal Type EEG Spike LFP LFP LFP LFP Spike ECoG LFP Compression Algorithm CS CS CS DRR Hufmann Coding DRR AE DWT CS CAE Precision 10b 10b 10b 10b 10b I P:16b, O P:10b 16b 16b W: 8b, Act.: 8b Compression Ratio 16 10 8-16 4.3-5.8 2 19.2 4.17 4 150 , 37.5 SNDR (dB) 21.8 N A 9.78 N A N A 19 3 17 N A 22.61 2.21 [K], 27.43 2.41 [L] 24.37 2.19 [K], 28.49 2.28 [L] R2 Score N A N A N A N A N A 0.72 0.23[K] 0.93 0.09[L] N A N A 0.81 0.13 [K], 0.94 0.13 [L] 0.87 0.10 [K], 0.95 0.15 [L] [K]Monkey K recordings, [L]Monkey L recordings obtained from dataset [53]. DS-CAE1 model, MobileNetV1-CAE(0.25x) model plementation of the CS algorithm supporting a CR of up to 16, achieving an SNDR of 21.8 dB. The design was implemented using ASIC 180-nm process technology with an area of 0.008 mm2 per channel and power of 0.95 µW per channel. Li et al. [58] introduce a Minimum Euclidean or Manhattan Distance Cluster-based (MDC) deterministic compressed sensing matrix for compressing multi-channel neural signals, achieving a CR of 10. The design was fabricated on 130-nm CMOS with a core area of 0.03 mm2 per channel and power 12.5 µW per channel. Liu et al. [59] present a fully integrated wireless neural signal acquisition system with an integrated compressed sensing processor fabricated using 180-nm CMOS technology with a power of 3.2 µW per channel. They achieve a CR of 8-16 with 9.78 dB SNDR. Park et al. [60] and Khazaei et al. [61] present lossless compression by employing the dynamic range reduction (DRR) technique. Park et al. [60] exploit the spatial and temporal correlation of neural signals to reduce the dynamic range of LFPs. Additionally, Huffman encoding was applied to compress the LFP signals, achieving an overall average CR of 4.3-5.8. The prototype chip was fabricated using 180-nm CMOS technology with an area of 0.098 mm2 per channel and a power of 15.35 µW per channel. Khazaei et al. [61] propose a lossless data reduction scheme by eliminating spatial redundancy across parallel recording channels, achieving a CR of 2. The design was fabricated using TSMC 130-nm CMOS technology, occupying a silicon area of 0.004 mm2 per channel, and dissipating 6.4 µW of power per channel. Valencia et al. [54] propose an autoencoder-based compression digital architecture for the efficient transmission of LFP neural signals. The compression method outlined in [54] differs from our proposed approach. They utilize standard autoencoders (AEs) with dense layers to compress the spatial (channels) domain from 96 to 8. In contrast, our method employs a convolutional autoencoder that compresses both spatial and temporal domains, resulting in a higher compression ratio. Additionally, [54] utilizes 16-bit input data samples and 10-bit compressed outputs, achieving an overall CR of 19.2 (96 16 (8 10)). Their design was implemented using 180-nm CMOS process technology with an area of 0.002 mm2 per channel. To benchmark our results, we use the same dataset [53] as [54], and it s evident that our SNDR and R2 scores are superior, even at a high CR of 150 with a dynamic xii power of 32.39 µW per channel for the DS-CAE1 model at 2 MHz. Additionally, several FPGA-based implementations have been proposed in the literature. Turcotte et al. [62] employs a four-level discrete wavelet transform (DWT) to compress neural data. Their system, comprising a spike detection core, threshold estimation core, and wavelet compression, was im- plemented on a Xilinx Spartan-6 FPGA. This design achieves a CR of 4.17 with an SNDR of 17 dB consuming power of 5 mW per channel. Shrivastwa et al. [63] utilize a combination of compressed sensing and neural networks to compress and reconstruct ECoG signals, respectively. Their design, imple- mented on a Xilinx Virtex-7 FPGA with 285.5k LUTs and 22.18k registers, achieves a CR of up to 4. In practical BCI applications where subsequent decoding and processing can reasonably tolerate reconstruction and wireless transmission errors, employing lossy methods such as CAE, AE, or CS with a higher CR presents a more feasible approach. Specifically, the proposed CAE-based compression scheme achieves a significantly higher compression ratio than previous methods due to compression in both spatial and temporal domains while maintaining good SNDR and R2 scores. On the contrary, lossless compression schemes involve reducing the dynamic range of neural signals and encoding them using Huffman coding. While this approach reconstructs the original signal accurately, it typically achieves a very low compression ratio. Additionally, various spike compression methods dedicated to high-density brain-implantable microsystems have been proposed [32] [35], [64]. Chen et al. [33] introduced an online neural signal processor (NSP) for spike detection, feature extraction using first and second derivative extrema, and complex spike clustering using the Geo-OSort algorithm. This clustering algorithm involves threshold calculation and Euclidean distance estimation, taking into account the geo- metric information of the high-density probe to reduce com- plexity. Their NSP, fabricated using a 22 nm FDSOI CMOS process, achieved a compression ratio (CR) of 982 with an assumption of 10 spikes s channel. Shaeri and Sodagar [34] utilized the Discrete Haar Wavelet Transform (DHWT) and spike extraction, proposing a novel data framing scheme to compress the spike waveforms. Their design, fabricated in a 130 nm CMOS process, achieved a CR of 903. Mohan et al. [35] employed a neuromorphic approach to spike compression inspired by DVS sensors [65]. They used a delta modulator to encode input spike waveforms into ON OFF pulses, proposing two transmission modes: All pulse mode (APM) and Pulse count mode (PCM), and exploring the trade-offs between them. They also examined address event representation (AER) for asynchronous data transfer to prevent pulse loss due to collisions, achieving a CR of 40.37 at a 62 Hz firing rate per channel with 90 spike detection accuracy. Nekoui and Sodagar [64] proposed a spike compression scheme through selective downsampling at the implant side, reconstructing the spike offline using third-order polynomial curve fitting. This design, implemented in 130-nm CMOS technology, achieved a CR of 446.5. In another study, Shaeri and Sodagar [32] presented a framework for on-implant spike sorting based on salient feature extraction, maximizing the geometric mean for spike wave-shape isolation. Their systems include an online spike sorting module configured by a shadow spike sorter block on an external module. Generally, spike compression offers superior CR since only the spike events (with an average firing rate of 40-60 Hz) are detected, extracted, compressed, and transmitted. In contrast, our work focuses on compressing local field potentials (LFPs), which are a different signal modality compared to spike waveforms. LFPs are usually low-frequency components of the neural signal (typically 300 Hz), and the information is encoded in the raw signal itself rather than in spikes. Therefore, in our proposed compression scheme, we compress the raw neural signal as opposed to extracting and compressing spike waveforms. The CR achieved by the proposed compression scheme is superior to the existing LFP compression methods [54], [59] [61]. V. CONCLUSIONS This paper introduces a novel neural signal compression scheme employing convolutional autoencoders. The encoder section of the CAE underwent several hardware-software co- optimizations and was subsequently deployed on the RAMAN tinyML accelerator specifically designed for edge computing applications. RAMAN leveraged weight and activation spar- sity to reduce latency, memory usage, and power consumption. A novel hardware-aware stochastic pruning technique was em- ployed to address workload imbalance issues across multiple parallel processing elements and reduce the indexing overhead associated with compressed weight storage, resulting in up to a 32.4 reduction in parameter memory requirements. Furthermore, since RAMAN inherently supports a wide range of neural network topologies, including standard con- volutions, depth-wise convolutions, point-wise convolutions, pooling layers, and dense layers, the encoder of the CAE was constructed based on depth-wise separable convolutional layers to minimize the number of MAC operations. The proposed CAE-based scheme performs compression in both spatial (channel) and temporal domains, achieving a superior compression ratio of up to 150. The CAE encoder model was pruned using the stochastic pruning scheme and quantized to 8 bits before deployment on the RAMAN tinyML accelerator. RAMAN was implemented on the Efinix Ti60 FPGA with 52.3k XLR cells and 61 DSP units, and the compressed neural data obtained at the output of RAMAN was decoded offline. Compared to recently reported compression algorithms, our scheme achieves superior reconstruction qual- ity, with signal-to-noise and distortion ratios of 22.6 dB and 27.4 dB and R2 scores of 0.81 and 0.94 for the two monkey neural recordings employing a compact custom-designed DS- CAE1 model. VI. ACKNOWLEDGEMENTS The authors would like to express their sincere gratitude and appreciation to their colleagues, Srikanth Rohit Nudu- rupati, Chandana D G, Hitesh Pavan Oleti, Anand Chauhan, Shankaranarayanan H, and Ashwin Rajesh for their invaluable help throughout this work. xiii REFERENCES [1] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M. Vaughan, Brain computer interfaces for communication and control, Clinical Neurophysiology, vol. 113, no. 6, pp. 767 791, 2002. [Online]. Available: S1388245702000573 [2] L. F. Nicolas-Alonso and J. Gomez-Gil, Brain computer interfaces, a review, Sensors (Basel), vol. 12, no. 2, pp. 1211 1279, Jan. 2012. [3] U. Chaudhary, N. Birbaumer, and A. Ramos-Murguialday, Brain computer interfaces for communication and rehabilitation, Nature Reviews Neurology, vol. 12, no. 9, pp. 513 525, 2016. [Online]. Available: [4] P. Demarest, N. Rustamov, J. Swift, T. Xie, M. Adamek, H. Cho, E. Wilson, Z. Han, A. Belsten, N. Luczak, P. Brunner, S. Haroutounian, and E. C. Leuthardt, A novel theta-controlled vibrotactile brain computer interface to treat chronic pain: a pilot study, Scientific Reports, vol. 14, no. 1, p. 3433, 2024. [Online]. Available: [5] J. N. Mak and J. R. Wolpaw, Clinical Applications of Brain-Computer Interfaces: Current State and Future Prospects, IEEE Reviews in Biomedical Engineering, vol. 2, pp. 187 199, 2009. [6] J. J. Shih, D. J. Krusienski, and J. R. Wolpaw, Brain-computer interfaces in medicine, Mayo Clinic Proceedings, vol. 87, no. 3, pp. 268 279, Mar 2012. [7] A. Krishna, V. Ramanathan, S. S. Yadav, S. Shah, A. van Schaik, M. Mehendale, and C. S. Thakur, A Sparsity-driven tinyML Accelerator for Decoding Hand Kinematics in Brain-Computer Interfaces, in 2023 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2023, pp. 1 5. [8] D. J. McFarland and J. R. Wolpaw, Brain-Computer Interfaces for Communication and Control, Communications of the ACM, vol. 54, no. 5, pp. 60 66, 2011. [9] K. Karas, L. Pozzi, A. Pedrocchi, F. Braghin, and L. Roveda, Brain-computer interface for robot control with eye artifacts for assistive applications, Scientific Reports, vol. 13, no. 1, p. 17512, 2023. [Online]. Available: [10] M. A. Cervera, S. R. Soekadar, J. Ushiba, J. D. R. Millán, M. Liu, N. Birbaumer, and G. Garipelli, Brain-computer interfaces for post- stroke motor rehabilitation: a meta-analysis, Annals of Clinical and Translational Neurology, vol. 5, no. 5, pp. 651 663, 2018. [11] M. Sebastián-Romagosa, W. Cho, R. Ortner, N. Murovec, T. Von Oertzen, K. Kamada, B. Z. Allison, and C. Guger, Brain Computer Interface Treatment for Motor Rehabilitation of Upper Extremity of Stroke Patients A Feasibility Study, Frontiers in Neuroscience, vol. 14, 2020. [Online]. Available: frontiersin.org journals neuroscience articles 10.3389 fnins.2020.591435 [12] R. Mane, T. Chouhan, and C. Guan, BCI for stroke rehabilitation: motor and beyond, Journal of Neural Engineering, vol. 17, no. 4, p. 041001, 2020. [13] P. D. E. Baniqued, E. C. Stanyer, M. Awais, A. Alazmani, A. E. Jackson, M. A. Mon-Williams, F. Mushtaq, and R. J. Holt, Brain computer interface robotics for hand rehabilitation after stroke: a systematic review, Journal of NeuroEngineering and Rehabilitation, vol. 18, no. 1, p. 15, 2021. [Online]. Available: [14] H.-J. Hwang, V. Y. Ferreria, D. Ulrich, T. Kilic, X. Chatziliadis, B. Blankertz, and M. Treder, A Gaze Independent Brain-Computer Interface Based on Visual Stimulation through Closed Eyelids, Scientific Reports, vol. 5, no. 1, p. 15890, 2015. [Online]. Available: [15] S. Niketeghad and N. Pouratian, Brain Machine Interfaces for Vision Restoration: The Current State of Cortical Visual Prosthetics, Neurotherapeutics, vol. 16, no. 1, pp. 134 143, 2019. [Online]. Available: [16] Z. Wang, N. Shi, Y. Zhang, N. Zheng, H. Li, Y. Jiao, J. Cheng, Y. Wang, X. Zhang, Y. Chen, Y. Chen, H. Wang, T. Xie, Y. Wang, Y. Ma, X. Gao, and X. Feng, Conformal in-ear bioelectronics for visual and auditory brain-computer interfaces, Nature Communications, vol. 14, no. 1, p. 4213, 2023. [Online]. Available: [17] M. J. Vansteensel, E. G. Pels, M. G. Bleichner, M. P. Branco, T. Denison, Z. V. Freudenburg, P. Gosselaar, S. Leinders, T. H. Ottens, M. A. Van Den Boom, P. C. Van Rijen, E. J. Aarnoutse, and N. F. Ramsey, Fully Implanted Brain Computer Interface in a Locked-In Patient with ALS, New England Journal of Medicine, vol. 375, no. 21, pp. 2060 2066, 2016, pMID: 27959736. [Online]. Available: [18] U. Chaudhary, B. Xia, S. Silvoni, L. G. Cohen, and N. Birbaumer, Brain-Computer Interface-Based Communication in the Completely Locked-In State, PLoS biology, vol. 15, no. 1, p. e1002593, Jan 2017. [Online]. Available: [19] R. R. Harrison, The Design of Integrated Circuits to Observe Brain Activity, Proceedings of the IEEE, vol. 96, pp. 1203 1216, 2008. [Online]. Available: [20] T. Oxley, N. Opie, S. John, and et al., Minimally invasive endovascular stent-electrode array for high-fidelity, chronic recordings of cortical neural activity, Nature Biotechnology, vol. 34, pp. 320 327, 2016. [21] E. Musk, An Integrated Brain-Machine Interface Platform With Thousands of Channels, J Med Internet Res, vol. 21, no. 10, p. e16194, Oct 2019. [Online]. Available: [22] N. A. Steinmetz, C. Aydin, and et al., Neuropixels 2.0: A Miniaturized High-Density Probe for Stable, Long-Term Brain Recordings, Science (New York, N.Y.), vol. 372, no. 6539, p. eabf4588, Apr 2021, copyright 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. [Online]. Available: [23] A. Wang, Z. Jin, C. Song, and W. Xu, Adaptive compressed sensing architecture in wireless brain-computer interface, in Proceedings of the 52nd Annual Design Automation Conference, ser. DAC 15. New York, NY, USA: Association for Computing Machinery, 2015. [Online]. Available: [24] R. R. Shrivastwa, V. Pudi, C. Duo, R. So, A. Chattopadhyay, and G. Cun- tai, A Brain Computer Interface Framework Based on Compressive Sensing and Deep Learning, IEEE Consumer Electronics Magazine, vol. 9, no. 3, pp. 90 96, 2020. [25] M. Shoaran, M. H. Kamal, C. Pollo, P. Vandergheynst, and A. Schmid, Compact Low-Power Cortical Recording Architecture for Compressive Multichannel Data Acquisition, IEEE Transactions on Biomedical Circuits and Systems, vol. 8, no. 6, pp. 857 870, 2014. [26] K. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T.-W. Lee, and T. J. Sejnowski, Dictionary Learning Algorithms for Sparse Representation, Neural Computation, vol. 15, no. 2, pp. 349 396, 02 2003. [Online]. Available: [27] H. Daou and F. Labeau, Dynamic Dictionary for Combined EEG Compression and Seizure Detection, IEEE Journal of Biomedical and Health Informatics, vol. 18, no. 1, pp. 247 256, 2014. [28] V. Vadori, E. Grisan, and M. Rossi, Biomedical signal compression with time- and subject-adaptive dictionary for wearable devices, in 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 2016, pp. 1 6. [29] J. Qian, P. Tiwari, S. P. Gochhayat, and H. M. Pandey, A Noble Double- Dictionary-Based ECG Compression Technique for IoTH, IEEE Inter- net of Things Journal, vol. 7, no. 10, pp. 10 160 10 170, 2020. [30] U. Bihr, H. Xu, C. Bulach, M. Lorenz, J. Anders, and M. Ortmanns, Real-time data compression of neural spikes, in 2014 IEEE 12th International New Circuits and Systems Conference (NEWCAS), 2014, pp. 436 439. [31] O. W. Savolainen and T. G. Constandinou, Lossless Compression of Intracortical Extracellular Neural Recordings using Non-Adaptive Huffman Encoding, in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine Biology Society (EMBC), 2020, pp. 4318 4321. [32] M. Shaeri and A. M. Sodagar, A framework for on-implant spike sorting based on salient feature selection, Nature Communications, vol. 11, no. 1, p. 3278, Jun 2020. [Online]. Available: https: doi.org 10.1038 s41467-020-17031-9 [33] Y. Chen, B. Tacca, Y. Chen, D. Biswas, G. Gielen, F. Catthoor, M. Verhelst, and C. Mora Lopez, An Online-Spike-Sorting IC Using Unsupervised Geometry-Aware OSort Clustering for Efficient Embedded Neural-Signal Processing, IEEE Journal of Solid-State Circuits, vol. 58, no. 11, pp. 2990 3002, 2023. [34] M. A. Shaeri and A. M. Sodagar, A Method for Compression of Intra-Cortically-Recorded Neural Signals Dedicated to Implantable Brain Machine Interfaces, IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 23, no. 3, pp. 485 497, 2015. [35] V. Mohan, W. P. Tay, and A. Basu, Architectural Exploration of Neu- romorphic Compression-based Neural Sensing for Next-Gen Wireless implantable-BMI, in 2023 IEEE International Symposium on Circuits and Systems (ISCAS), 2023, pp. 1 5. xiv [36] M. Shaeri and A. M. Sodagar, Data Transformation in the Processing of Neuronal Signals: A Powerful Tool to Illuminate Informative Contents, IEEE Reviews in Biomedical Engineering, vol. 16, pp. 611 626, 2023. [37] J. W. Salatino, K. A. Ludwig, T. D. Y. Kozai, and E. K. Purcell, Glial responses to implanted electrodes in the brain, Nature Biomedical Engineering, vol. 2, no. 1, pp. 52 52, Jan 2018. [Online]. Available: [38] S. Musallam, B. D. Corneil, B. Greger, H. Scherberger, and R. A. Andersen, Cognitive Control Signals for Neural Prosthetics, Science, vol. 305, no. 5681, pp. 258 262, 2004. [Online]. Available: [39] T. Aflalo, S. Kellis, C. Klaes, B. Lee, Y. Shi, K. Pejsa, K. Shanfield, S. Hayes-Jackson, M. Aisen, C. Heck, C. Liu, and R. A. Andersen, Decoding motor imagery from the posterior parietal cortex of a tetraplegic human, Science, vol. 348, no. 6237, pp. 906 910, 2015. [Online]. Available: aaa5417 [40] P. Baldi, Autoencoders, unsupervised learning and deep architectures, in Proceedings of the 2011 International Conference on Unsuper- vised and Transfer Learning Workshop - Volume 27, ser. UTLW 11. JMLR.org, 2011, p. 37 50. [41] J. Zhai, S. Zhang, J. Chen, and Q. He, Autoencoder and Its Various Variants, in 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE Press, 2018, p. 415 419. [Online]. Available: [42] P. Li, Y. Pei, and J. Li, A comprehensive survey on design and application of autoencoder in deep learning, Applied Soft Computing, vol. 138, p. 110176, 2023. [Online]. Available: https: www.sciencedirect.com science article pii S1568494623001941 [43] D. Bank, N. Koenigstein, and R. Giryes, Autoencoders. Cham: Springer International Publishing, 2023, pp. 353 374. [Online]. Available: [44] A. Krishna, S. Rohit Nudurupati, D. G. Chandana, P. Dwivedi, A. van Schaik, M. Mehendale, and C. S. Thakur, RAMAN: A Reconfigurable and Sparse tinyML Accelerator for Inference on Edge, IEEE Internet of Things Journal, vol. 11, no. 14, pp. 24 831 24 845, 2024. [45] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, in Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, ser. NIPS 12. Red Hook, NY, USA: Curran Associates Inc., 2012, p. 1097 1105. [46] A. Krishna, H. P. Oleti, A. Chauhan, H. Shankaranarayanan, A. van Schaik, M. Mehendale, and C. Singh Thakur, Live Demonstration: Audio Inference using Neuromorphic Cochlea on RAMAN Accelerator, in 2023 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2023, pp. 1 1. [47] A. Krishna, A. Rajesh, H. P. Oleti, A. Chauhan, S. H, A. Van Schaik, M. Mehendale, and C. S. Thakur, Live Demonstration: Real-time audio and visual inference on the RAMAN TinyML accelerator, in 2024 IEEE International Symposium on Circuits and Systems (ISCAS), 2024, pp. 1 1. [48] G. Zhang, N. Attaluri, J. S. Emer, and D. Sanchez, Gamma: Leveraging Gustavson s Algorithm to Accelerate Sparse Matrix Multiplication, in Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS 2021. New York, NY, USA: Association for Computing Machinery, 2021, p. 687 701. [Online]. Available: [49] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, arXiv, 2017. [50] F. Karimzadeh, N. Cao, B. Crafton, J. Romberg, and A. Raychowdhury, A Hardware-Friendly Approach Towards Sparse Neural Networks Based on LFSR-Generated Pseudo-Random Sequences, IEEE Trans- actions on Circuits and Systems I: Regular Papers, vol. 68, no. 2, pp. 751 764, 2021. [51] C. Serrano-Amenos, P. Heydari, C. Y. Liu, A. H. Do, and Z. Nenadic, Power Budget of a Skull Unit in a Fully-Implantable Brain-Computer Interface: Bio-Heat Model, IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 31, pp. 4029 4039, 2023. [52] Efinix, TI60 Data Sheet, Online, 2023. [Online]. Available: [53] T. Brochier, L. Zehl, Y. Hao, M. Duret, J. Sprenger, M. Denker, S. Grün, and A. Riehle, Massively parallel multi-electrode recordings of macaque motor cortex during an instructed delayed reach-to-grasp task, 2017. [Online]. Available: [54] D. Valencia, P. P. Mercier, and A. Alimohammad, Efficient In Vivo Neural Signal Compression Using an Autoencoder-based Neural Net- work, IEEE Transactions on Biomedical Circuits and Systems, pp. 1 12, 2024. [55] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [56] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y. Wang, M. W. Mahoney, and K. Keutzer, HAWQV3: Dyadic Neural Network Quantization, in ICML, 2021. [57] L. N. Smith and N. Topin, Super-convergence: Very fast training of neural networks using large learning rates, in Artificial intelligence and machine learning for multi-domain operations applications, vol. 11006. SPIE, 2019, pp. 369 386. [58] N. Li, M. Osborn, G. Wang, and M. Sawan, A digital multichannel neural signal processing system using compressed sensing, Digital Signal Processing, vol. 55, pp. 64 77, 2016. [Online]. Available: [59] X. Liu, M. Zhang, T. Xiong, A. G. Richardson, T. H. Lucas, P. S. Chin, R. Etienne-Cummings, T. D. Tran, and J. Van der Spiegel, A Fully Integrated Wireless Compressed Sensing Neural Signal Acquisition System for Chronic Recording and Brain Machine Interface, IEEE Transactions on Biomedical Circuits and Systems, vol. 10, no. 4, pp. 874 883, 2016. [60] S.-Y. Park, J. Cho, K. Lee, and E. Yoon, Dynamic Power Reduction in Scalable Neural Recording Interface Using Spatiotemporal Correlation and Temporal Sparsity of Neural Signals, IEEE Journal of Solid-State Circuits, vol. 53, no. 4, pp. 1102 1114, 2018. [61] Y. Khazaei, A. A. Shahkooh, and A. M. Sodagar, Spatial Redundancy Reduction in Multi-Channel Implantable Neural Recording Microsys- tems, in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine Biology Society (EMBC), 2020, pp. 898 901. [62] G. Gagnon-Turcotte, Y. LeChasseur, C. Bories, Y. De Koninck, and B. Gosselin, A wireless optogenetic headstage with multichannel neural signal compression, in 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2015, pp. 1 4. [63] R. R. Shrivastwa, V. Pudi, and A. Chattopadhyay, An FPGA-Based Brain Computer Interfacing Using Compressive Sensing and Machine Learning, in 2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2018, pp. 726 731. [64] M. Nekoui and A. M. Sodagar, Spike Compression through Selec- tive Downsampling and Piecewise Curve Fitting Dedicated to Neural Recording Brain Implants, in 2022 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2022, pp. 50 54. [65] P. Lichtsteiner, C. Posch, and T. Delbruck, A 128 128 120 dB 15 µs Latency Asynchronous Temporal Contrast Vision Sensor, IEEE Journal of Solid-State Circuits, vol. 43, no. 2, pp. 566 576, 2008.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\ni Neural Signal Compression using RAMAN tinyML Accelerator for BCI Applications Adithya Krishna, Sohan Debnath, André van Schaik, Mahesh Mehendale and Chetan Singh Thakur Abstract High-quality, multi-channel neural recording is in- dispensable for neuroscience research and clinical applications. Large-scale brain recordings often produce vast amounts of data that must be wirelessly transmitted for subsequent offline analysis and decoding, especially in brain-computer interfaces (BCIs) utilizing high-density intracortical recordings with hundreds or thousands of electrodes. However, transmitting raw neural data presents significant challenges due to limited communi- cation bandwidth and resultant excessive heating. To address this challenge, we propose a neural signal compression scheme utilizing Convolutional Autoencoders (CAEs), which achieves a compression ratio of up to 150 for compressing local field potentials (LFPs). The CAE encoder section is implemented on RAMAN, an energy-efficient tinyML accelerator designed for edge computing, and subsequently deployed on an Efinix Ti60 FPGA with 37.3k LUTs and 8.6k register utilization. RAMAN leverages sparsity in activation and weights through zero skip- ping, gating, and weight compression techniques. Additionally, we employ hardware-software co-optimization by pruning CAE encoder model parameters using a hardware-aware balanced stochastic pruning strategy, resolving workload imbalance issues and eliminating indexing overhead to reduce parameter storage requirements by up to 32.4 . Using the proposed compact depthwise separable convolutional autoencoder (DS-CAE) model, the compressed neural data from RAMAN is reconstructed offline with superior signal-to-noise and distortion ratios (SNDR) of 22.6 dB and 27.4 dB, along with R2 scores of 0.81 and 0.94, respectively, evaluated on two monkey neural recordings. Keywords Convolutional neural networks (CNNs), deep learn- ing, hardware acceleration, sparse processing, machine learning, Convolutional Autoencoders (CAEs), tinyML, edge computing, stochastic processing. I. INTRODUCTION In recent years, the Brain-Computer Interface (BCI) has garnered significant attention for facilitating direct commu- nication between the human brain and external devices [1] [4].\n\n--- Segment 2 ---\nI. INTRODUCTION In recent years, the Brain-Computer Interface (BCI) has garnered significant attention for facilitating direct commu- nication between the human brain and external devices [1] [4]. BCIs have emerged as a revolutionary tool for advancing our understanding of the brain and are increasingly being utilized across various clinical applications [5] [7], providing inventive solutions for communication [8], control [1], [9], and rehabilitation [10] [13]. Ongoing improvements in signal A. Krishna is with the Department of Electronic Systems Engineering, Indian Institute of Science, Bangalore - 560012, India, and also with the Inter- national Centre for Neuromorphic Systems, The MARCS Institute, Western Sydney University, Australia. S. Debnath, M. Mehendale, and C. S. Thakur (Email: are with the Department of Electronic Systems Engineering, Indian Institute of Science, Bangalore - 560012, India. A. van Schaik is with the International Centre for Neuromorphic Systems, The MARCS Institute, Western Sydney University, Australia. This work was supported by the Pratiksha Trust grant BCD - FG SMCH-22- 2106 and INAE grant INAE 121 AKF 48 (SAP code - SP INAE-23-0001). Corresponding author processing, machine learning algorithms, and neurotechnology pave the way for BCIs to revolutionize healthcare, human- computer interaction, and beyond. BCIs can potentially re- store lost sensory abilities, such as vision and hearing [14] [16] through stimulation and regain lost motor functions for individuals with motor impairments, such as those caused by conditions like ALS or spinal cord injury [17], [18]. In clinical settings, BCIs can interpret the user s intentions from brain activity and utilize this information to control the person s limb or an assistive device, such as a prosthetic arm or a computer cursor, necessitating simultaneous recordings from a relatively large population of neurons to achieve acceptable prediction accuracy. Intracortical neural recording systems have evolved significantly, progressing from widely used recording arrays like the Utah Array [19], which supports up to a hundred recording sites, to high-density electrodes such as [20], Neuralink [21] and Neuropixel [22].\n\n--- Segment 3 ---\nIn clinical settings, BCIs can interpret the user s intentions from brain activity and utilize this information to control the person s limb or an assistive device, such as a prosthetic arm or a computer cursor, necessitating simultaneous recordings from a relatively large population of neurons to achieve acceptable prediction accuracy. Intracortical neural recording systems have evolved significantly, progressing from widely used recording arrays like the Utah Array [19], which supports up to a hundred recording sites, to high-density electrodes such as [20], Neuralink [21] and Neuropixel [22]. These state-of- the-art intracortical neural recording systems have thousands of recording sites, producing massive amounts of neural data that require wireless transmission for offline signal analysis and decoding. Continuous wireless transmission of raw neural data poses a significant challenge due to constrained communication bandwidth (low data rates) and leads to excessive heating. For instance, a system comprising 1024 channels, sampled at 30 kS s with 16 bits per sample, generates 491.52 Mb s of data that must be wirelessly transmitted. To mitigate this, the data undergoes compression before transmission over wireless channels. Compressed sensing (CS) is one such prominent method that reduces the data dimensionality by multiplying the input signal X of dimension (1 M) with a sensing matrix of size (M N) to produce the compressed output signal Y of dimension (1 N), with compression ratio (CR) M N(M N) [23], [24]. Reconstruction of the compressed signal to its original form utilizes CS-based recon- struction algorithms [25], with the emerging interest in deep learning-based methods [24]. Additionally, various lossless compression techniques, categorized into dictionary-based and statistical-based schemes, offer further compression options for neural data. Dictionary-based compression [26] [29] relies on creating a dictionary of frequently occurring patterns in the data and replacing them with shorter codes or references to entries in the dictionary. Statistical compression utilizes statistical models to represent and encode data more efficiently. One standard method of statistical compression is Huffman coding [30], [31]. In Huffman coding, symbols with higher fre- quencies are assigned shorter codewords, while symbols with lower frequencies are assigned longer codewords. This ensures arXiv:2504.06996v1 [cs.AR] 9 Apr 2025 ii that more common symbols are represented with fewer bits, leading to overall compression.\n\n--- Segment 4 ---\nIn Huffman coding, symbols with higher fre- quencies are assigned shorter codewords, while symbols with lower frequencies are assigned longer codewords. This ensures arXiv:2504.06996v1 [cs.AR] 9 Apr 2025 ii that more common symbols are represented with fewer bits, leading to overall compression. Lossless compression schemes like Huffman coding ensure that the original input signal is accurately reconstructed from the compressed data without any reconstruction errors. In contrast, lossy compression methods, such as compressed sensing (CS), can achieve significantly higher compression ratios but at the expense of introducing reconstruction errors. Additionally, several spike compression schemes have been proposed [32] [35], with associated data transformations to better represent signal space [36]. These methods focus exclusively on compressing spike waveforms for applications employing single-unit activities (SUAs) and multi-unit activities (MUAs). In contrast, our work focuses on compressing local field potentials (LFPs), where spike detection, extraction, and sorting methods are not applicable. LFPs, which represent slower voltage variations ( 300 Hz) and have lower spatial resolution compared to SUAs and MUAs, are more stable due to factors like electrode drift, neuron drop-out, and scarring at the electrode-tissue interface over time [37]. While MUA-based BCIs primarily decode motor cortex activity, LFP-based BCIs target higher-level cognitive regions like the posterior parietal cortex, enabling advanced cognitive decoding [38]. Studies have shown that LFP signals can reliably decode neural activity; for example, a study demonstrated that movement intentions, kinematic trajectories, and movement types, can be accurately predicted from LFP signals [39]. In this work, we propose a convolutional autoencoder-based neural signal compression scheme for compressing the LFPs. Autoencoders are neural networks used for the unsupervised learning of efficient codings or representations of the input data [40] [43]. They consist of two main components: an encoder and a decoder. The encoder network maps the in- put data to a lower-dimensional latent space representation, also known as a codeword or encoding, which effectively compresses the neural signal. The decoder network takes the encoded representation produced by the encoder and attempts to reconstruct the original input signal from it.\n\n--- Segment 5 ---\nThe encoder network maps the in- put data to a lower-dimensional latent space representation, also known as a codeword or encoding, which effectively compresses the neural signal. The decoder network takes the encoded representation produced by the encoder and attempts to reconstruct the original input signal from it. The encoder network is implemented using RAMAN, a Re-configurable and spArse tinyML Accelerator for infereNce proposed in [44] for edge computing applications to compress the input data to a latent space. A. Our Contributions The main contributions of this paper can be summarized as follows: 1) We propose a neural signal compression scheme using convolutional autoencoders (CAEs) that achieves superior compression ratios compared to existing LFP compres- sion methods, as detailed in Section II and Section IV-C. We evaluate the performance of various CAE architectures based on SNDR and R2 scores, employing depth-wise separable convolutions to reduce Multiply- Accumulate (MAC) operations. 2) We employ RAMAN, a low-power, compact tinyML accelerator designed for edge computing [44], to im- plement the CAE encoder. RAMAN leverages sparsity in activations and weights to reduce latency, memory storage, and power consumption. We assess RAMAN s performance with MobileNetV1-based convolutional au- toencoder models and introduce our custom, compact models, DS-CAE1 and DS-CAE2 (Depthwise Separable Convolutional Autoencoders), which achieve reasonable SNDR and R2 scores with minimal model size and MAC count. 3) We employ a novel hardware-aware balanced stochastic weight pruning scheme incorporating Linear Feedback Shift Registers (LFSRs) to reduce parameter memory requirements by up to 32.4 . 4) We employ a memory optimization technique involving overlapping input and output activations on the same memory space, that reduces the peak activation memory storage by 37.5 . The specific implementation details can be found in Section III-B. The rest of the paper is organized as follows: Section II introduces the proposed neural signal compression scheme utilizing CAE and explains the fundamentals of autoencoders. Section III describes the RAMAN architecture, its features, and the hardware-software co-optimization approach used to handle workload imbalances and minimize parameter mem- ory requirements.\n\n--- Segment 6 ---\nThe rest of the paper is organized as follows: Section II introduces the proposed neural signal compression scheme utilizing CAE and explains the fundamentals of autoencoders. Section III describes the RAMAN architecture, its features, and the hardware-software co-optimization approach used to handle workload imbalances and minimize parameter mem- ory requirements. This involves employing a hardware-aware balanced stochastic pruning scheme utilizing LFSRs. Section IV presents the FPGA implementation results, assesses model performance using SNDR and R2 score metrics across various CAE topologies, and compares the stochastic pruning scheme ADC RAMAN Neural Amp Intracortical Recordings Digitized and compressed neural data Bandwidth Constraints De- compress Decoding Analysis Control Head Unit In Silico Fig. 1: A system-level overview of the proposed neural signal compression scheme utilizing convolutional autoencoders. iii with the conventional magnitude-based pruning scheme. It also highlights the advantages of the stochastic pruning scheme in terms of memory size reduction. Finally, Section V concludes the paper. II. PROPOSED COMPRESSION SCHEME USING CONVOLUTIONAL AUTOENCODERS The proposed neural signal compression scheme is de- picted in Fig. 1. It comprises a signal conditioning module equipped with a neural amplifier and an analog-to-digital converter (ADC) for amplifying and digitizing the neural signal. Subsequently, the digitized neural signal is fed to the RAMAN tinyML accelerator for compression using convo- lutional autoencoders. All signal processing steps, including acquisition, amplification, digitization, and compression, occur in a head unit mounted on the head. The compressed data is transmitted wirelessly, while the decompression and decoding are conducted offline. A. Convolutional autoencoders Encoder (on RAMAN) Compressed Vector Convolutional Autoencoder Decoder (Off-chip) Input Neural Signal filters Decoder Transposed Convolutions Reconstructed Neural Signal filters 1 1 Depthwise (DW) Pointwise (PW) Encoder DWS Convolutions 1 1 Fig. 2: A convolutional autoencoder with an encoder to convert the input neural signal window of size C Tw to a compressed vector (in latent space) of size 1 1 γ and a decoder to recover back the original signal from the compressed vector.\n\n--- Segment 7 ---\nA. Convolutional autoencoders Encoder (on RAMAN) Compressed Vector Convolutional Autoencoder Decoder (Off-chip) Input Neural Signal filters Decoder Transposed Convolutions Reconstructed Neural Signal filters 1 1 Depthwise (DW) Pointwise (PW) Encoder DWS Convolutions 1 1 Fig. 2: A convolutional autoencoder with an encoder to convert the input neural signal window of size C Tw to a compressed vector (in latent space) of size 1 1 γ and a decoder to recover back the original signal from the compressed vector. The encoder section employs depthwise separable (DWS) con- volutions, while the decoder utilizes transposed convolutions. Convolutional autoencoders (CAEs) are a type of neural network architecture that combines the principles of convo- lutional neural networks (CNNs) with autoencoder structures. They are designed to learn efficient representations of input by encoding it into a lower-dimensional latent space and then reconstructing the original input from this representation as shown in Fig. 2. The input neural signal was divided into 50 ms windows at a 2 KHz sampling rate, giving 100 samples per window (Tw). The CAE input is a 2D matrix sized C Tw, where C is the number of recording sites channels set to 96 in our implementation. The encoder output is a vector of shape 1 1 γ, representing the compressed data. Thus, the CR achieved by the proposed compression scheme is C Tw γ. The CAEs are composed of two main parts: an encoder and a decoder. 1) Encoder Compression: The encoder part of a CAE typ- ically consists of convolutional layers followed by pooling layers. These layers extract essential features from the input data and reduce its dimensionality. The encoding process can be represented as: z f(W x b) (1) where, x is the input data, W represents the convolutional filters, b is the bias, denotes the convolution operation, f is the activation function, such as ReLU (Rectified Linear Unit), applied element-wise and z is the encoded representation (also called latent or compressed representation) of the input data. In our design, we employ depthwise separable (DWS) convolutions to minimize the number of operations.\n\n--- Segment 8 ---\nThe encoding process can be represented as: z f(W x b) (1) where, x is the input data, W represents the convolutional filters, b is the bias, denotes the convolution operation, f is the activation function, such as ReLU (Rectified Linear Unit), applied element-wise and z is the encoded representation (also called latent or compressed representation) of the input data. In our design, we employ depthwise separable (DWS) convolutions to minimize the number of operations. The DWS convolutions split the convolution process into a depth-wise and a point-wise convolution. The depthwise convolution operation can be represented as: OAh,w,m bm Kh 1 X i 0 Kw 1 X j 0 Wi,j,m IAh sh i,w sw j,m where W is the depthwise convolutional kernel of size Kh Kw M. The mth filter in W is applied to the mth channel in the input activation (IA) tensor to produce the mth channel of the filtered output activation (OA) tensor. The strides along the height and width are denoted by sh and sw, respectively. The pointwise convolution operation can be expressed as: OAh,w,n bn M 1 X m 0 Wm,n IAh,w,m where M is the number of input channels, and N is the number of filters or output channels. Compared to the standard con- volutions [45], the DWS convolutions reduce the computation by: 1 N 1 Kh Kw 2) Decoder Decompression: The decoder part of a CAE consists of upsampling or transposed convolutional layers. The purpose of the decoder is to reconstruct the original input data from the encoded representation. The decoding process can be represented as: x g(W z b ) (2) where, z is the encoded representation obtained from the encoder, W represents the decoding filters, b is the bias, g is the activation function, x is the reconstructed output data.\n\n--- Segment 9 ---\nThe purpose of the decoder is to reconstruct the original input data from the encoded representation. The decoding process can be represented as: x g(W z b ) (2) where, z is the encoded representation obtained from the encoder, W represents the decoding filters, b is the bias, g is the activation function, x is the reconstructed output data. We use transposed convolutions for decoding the compressed data, which can be represented as: OAh,w,n bn M 1 X m 0 hb X i ha wb X j wa Wi,j,m,n IA h i sh , w j sw ,m 1Q iv where ha max 0, h sh(H 1) , hb min (h, Kh 1), wa max 0, w sw(W 1) , and wb min (w, Kw 1). H and W represent the height and width of the input activation tensor (IA), respectively. Additionally, 1Q, the indicator func- tion of the statement Q h i sh Z w j sw Z , yields 1 if Q is true, and 0 otherwise. Here Z denotes the set of integers. Similarly, the depthwise transposed convolution operation can be expressed as: OAh,w,m bm hb X i ha wb X j wa Wi,j,m IA h i sh , w j sw ,m 1Q The objective of the CAE is to minimize the reconstruction error between the input data and the reconstructed output. A standard loss function used for this purpose is the Mean Absolute Error (MAE) loss, given by: L 1 P P X i 1 xi x i (3) where P is the number of data samples, xi and x i are the ith input and reconstructed output data samples, respectively. By minimizing this loss function, the CAE learns to encode the input data into a lower-dimensional representation while retaining important information for accurate reconstruction. III. HARDWARE ARCHITECTURE This section introduces RAMAN architecture, features, and hardware-aware balanced stochastic pruning scheme. A. Top-Level Architecture The top-level architecture of the RAMAN accelerator uti- lized to deploy the encoder of CAE for neural data compres- sion is depicted in Fig. 3. The architecture comprises compute, memory, and control subsystems.\n\n--- Segment 10 ---\n3. The architecture comprises compute, memory, and control subsystems. The computing subsystem comprises a processing element (PE) array for performing MAC operations, an activation sparsity engine to exploit sparsity in activations, and a post-processing module (PPM) responsible for the rectified linear unit (ReLU) activation, quantization, pooling, bias addition, and residual addition. The memory subsystem consists of a global memory to store activations and parameters and a cache to exploit temporal data reuse. The control subsystem includes a top-level controller that schedules and sequences different operations and issues commands to various processing and memory blocks. A com- prehensive description of the architecture is presented in [44]. In this paper, we introduce a novel stochastic pruning scheme for weight pruning, that eliminates the indexing overhead of the compressed weights compared to our previous implemen- tation that utilizes conventional magnitude-based pruning [7], [44], [46], [47]. B. RAMAN features The architectural features of the RAMAN tinyML acceler- ator tailored for edge computing are outlined as follows: 1) Sparsity: RAMAN exploits both input activation and weight sparsity to reduce latency, memory access, and Activation Parameter Cache Global Memory PE PE Array PE PE PE PE PE PE PE PE PE PE PE Post Processing Activation Sparsity Engine Top-Level Controller Instruction Memory Layer Config. Inst. Stream IAs OAs Parameters Column Router Row Router LFSR Block Fig. 3: Top-Level architecture of the RAMAN accelerator. storage. RAMAN can skip the processing cycles with zero activations and weights to minimize the processing latency. The sparse weight matrices are compressed and stored in the memory. The stochastic pruning scheme pre- sented in Section III-C generates the compressed weight indices on the fly, eliminating the need for explicit index storage in memory. 2) Dataflow: RAMAN employs Gustavson s inspired [48] dataflow with optimal input and output activation reuse to reduce memory access. The dataflow reduces the partial sums (Psums) within the processing element to eliminate the Psum writeback traffic. 3) Peak Activation Memory Reduction: The state-of-the-art accelerators typically segment the Input Activations (IAs) and Output Activations (OAs) within the memory, with the total activation memory being the sum of IA and OA memory spaces.\n\n--- Segment 11 ---\nThe dataflow reduces the partial sums (Psums) within the processing element to eliminate the Psum writeback traffic. 3) Peak Activation Memory Reduction: The state-of-the-art accelerators typically segment the Input Activations (IAs) and Output Activations (OAs) within the memory, with the total activation memory being the sum of IA and OA memory spaces. In our approach, we eliminate logical partitioning, enabling OAs to directly overwrite the IA memory space, as illustrated in Figure 4. This is achieved by pre-fetching the IA tile into the cache, making the original content in the activation memory redundant, and enabling OA overwriting within the same memory space. A detailed explanation can be found in [44]. Cache Compute IA Tile 1 OA Tile 1 IA Tile 2 IA Tile K IA OA Overlapping IA Tile 1 IA Tile 2 OA Tile 1 IA Tile K Act. Mem No IA OA Overlapping Vs. IA OA Overwrite OA on IA mem. space OA stored in a diff. location Prefetch IA tile to cache IA OA Fig. 4: Peak activation memory reduction using IA OA mem- ory overlapping. 4) Programmability: RAMAN is capable of supporting var- ious types of neural network topologies, including stan- v Seed 112 8 50 -1 -100 44 112 0 50 0 -100 0 Magnitude Pruning 50 0 Idx: 1 2 3 4 5 0 1 2 3 4 5 112 8 50 -1 -100 44 0 8 50 0 0 44 Stochastic Pruning 50 0 Idx: 1 2 3 4 5 0 1 2 3 4 5 Re-train Re-train Weight Weight 112 50 -100 Compression Val Idx 8 50 44 Compression Val LFSR Idx 1, 3, 5 (Generated on-the-fly) Store Val Idx in Memory Store only Val in Memory 0 3 4 Fig. 5: Illustration of stochastic pruning compared with conventional magnitude-based pruning. The magnitude-based pruning retains the stronger weights and prunes the rest. The sparse weight vector is then compressed and stored as a value (Val) and index (Idx) pair. On the other hand, stochastic pruning retains the weight values whose indices are covered by LFSR-generated pseudo-random sequence.\n\n--- Segment 12 ---\nThe sparse weight vector is then compressed and stored as a value (Val) and index (Idx) pair. On the other hand, stochastic pruning retains the weight values whose indices are covered by LFSR-generated pseudo-random sequence. Since the indices are generated on the fly during inference, only the weight values are stored in the memory, eliminating the need for explicit index storage. The LFSR s seed (initial state of flip flops) and structure (feedback polynomial) are kept constant during training and inference. dard CNN models as well as separable convolutions. The separable convolution models comprise depth-wise (DW) and point-wise (PW) layers, which effectively reduce MAC operations compared to standard convolutions [49]. Additionally, RAMAN can handle operations such as max pooling, average pooling, and fully connected (FC) layers. C. Hardware-aware Balanced Stochastic Pruning We present a novel hardware-aware balanced stochastic weight pruning scheme aimed at reducing memory storage, access, and latency. Traditionally, magnitude-based pruning removes weaker synaptic weights while retaining stronger ones. However, this approach involves storing both indices and non-zero weight values, leading to additional indexing overhead as shown in Fig. 5. Using magnitude-based pruning, our previous implementation [7], [44] utilized 8-bit weights and 4-bit indices to store compressed non-zero weights. In this work, we introduce a stochastic pruning approach that eliminates index storage in memory, storing only the non- zero weight values. Karimzadeh et al. [50] propose a similar pruning strategy based on the LFSR-generated pseudo-random sequences, primarily focusing on software implementation and analysis. However, their study does not extend to exploring the hardware feasibility of integrating this strategy into an actual ML accelerator. During the training phase, this scheme generates a pseudo- random sequence (PRS) serving as indices to sparsify the weight matrix. The synaptic weights corresponding to the indices covered by the PRS are retained, while the remaining weights are forced to zero. Subsequently, the un-pruned weight values (covered by the PRS) are retrained in the subsequent step. The pseudo-random sequence is generated using a linear feedback shift register (LFSR), offering a straightforward implementation during inference.\n\n--- Segment 13 ---\nSubsequently, the un-pruned weight values (covered by the PRS) are retrained in the subsequent step. The pseudo-random sequence is generated using a linear feedback shift register (LFSR), offering a straightforward implementation during inference. This method enables the on- the-fly generation of indices without the need to store them in n 16 1 16 NZ 12 NZ 1 M N filters N M n 16 n 16 Pruning (25 ) 25 Pruning (12:16) 50 Pruning (8:16) 75 Pruning (4:16) Balanced Pruning PE Array Θ: 4 (75 Pruning) Θ: 8 (50 Pruning) Θ: 12 (25 Pruning) a b c p 0 a 0 0 8 NZ 8 NZ 8 NZ 8 NZ 8 NZ 8 NZ 8 NZ 12 NZ 12 NZ 12 NZ 12 NZ 12 NZ 12 NZ 12 NZ 4 NZ 4 NZ 4 NZ 4 NZ 4 NZ 4 NZ 4 NZ LFSR Idx: 0,3,4.... 0 1 2 15 d e 3 4 PE PE PE PE PE PE PE PE PE PE PE PE 1 M 2 Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ NZ Θ 1 2 Idx Val LFSR Generated Uniform non-zero (NZ) weight distribution 1 M 2 n 16 n 16 1 M 2 1 M 2 Fig. 6: The balanced pruning strategy for different pruning percentages. Depending on the pruning percentage, each PE receives an equal number of non-zero weight elements, leading to a uniform workload across PEs. The number of non-zero weight elements (Θ) in a given tile after pruning is fixed for a given pruning percentage set at 4, 8, and 12 for 75 , 50 , and 25 pruning, respectively. The indices of the compressed weights are generated by the LFSR and not stored explicitly in the memory. memory, thereby minimizing storage overhead. Additionally, implementing the logic for the LFSR incurs only a minimal increase in area and resource utilization, as demonstrated in Section IV-A. During deployment, only the non-zero weight values are stored in the RAMAN parameter memory. The same seed and LFSR structure employed during training are used to generate vi an identical PRS sequence during inference.\n\n--- Segment 14 ---\nDuring deployment, only the non-zero weight values are stored in the RAMAN parameter memory. The same seed and LFSR structure employed during training are used to generate vi an identical PRS sequence during inference. The non-zero weight values are then read from memory and matched with the indices generated by the LFSR. This matching ensures that each non-zero weight value is correctly multiplied by its corresponding activation value. Ultimately, the indices generated by the LFSR guarantee that all non-zero weights stored in the RAMAN memory are covered, facilitating the computation of the final output. Additionally, the balanced pruning strategy ensures a uni- form distribution of non-zero weights across weight tiles, effectively mitigating workload imbalances. Without this strat- egy, non-zero weights would be unevenly distributed among various weight tiles processed by different processing elements (PEs), resulting in workload discrepancies and performance limitations imposed by the PE handling the heaviest load. The operation of the balanced pruning strategy is illustrated in Fig. 6. Initially, N filters with an input channel dimension of M are represented as a weight matrix of size M N. Subsequently, the weight matrix is divided into tiles of size 1 n, where n is determined by the depth of the PE register-file (RF), set to 16 in our design. Each 1 n tile is then pruned based on the required sparsity level to have Θ non-zeros, where Θ is a function of the pruning percentage. For example, for 25 , 50 , and 75 pruning percentages, Θ is set to 12, 8, and 4, respectively. The weight values with indices not covered by LFSR-generated PRS are pruned. Furthermore, the LFSR- generated indices ensure that the same number of weights are pruned in each tile, resulting in structured sparsity that can be efficiently leveraged in RAMAN. Following balanced pruning, each tile contains an equal number of non-zero weights pro- cessed by different PE columns, achieving uniform workload distribution across the PE array. Each processing element (PE) contains four multiply-accumulate (MAC) units, executing four MAC operations in a single cycle and necessitating four non-zero weights per cycle. Thus, we employ four 4-bit LFSRs to generate four random indices simultaneously.\n\n--- Segment 15 ---\nEach processing element (PE) contains four multiply-accumulate (MAC) units, executing four MAC operations in a single cycle and necessitating four non-zero weights per cycle. Thus, we employ four 4-bit LFSRs to generate four random indices simultaneously. With 4 MACs per PE, processing each weight tile requires Θ 4 cycles and, consequently, Θ 4 cycles to generate Θ indices using the 4 LFSRs. The initial seed of each LFSR is selected to ensure that unique Θ indices are generated for each 1 n weight tile. This approach prevents repeated indices within a tile, thereby ensuring the desired Θ non-zero elements are produced. The detailed PE architecture and operation are presented in [44]. Using LFSRs for generating PRS offers several advantages: 1) LFSRs can be implemented using a small number of flip- flops and XOR gates, making them highly compact in terms of hardware resources. 2) LFSRs generate pseudo-random se- quences on-the-fly without any memory footprint. 3) The PRS generated by LFSRs are deterministic and repeatable, meaning that the same initial state (seed) and the feedback polynomial will always produce the same sequence. This property is desirable for our use case to ensure the same PRS generation for both training and inference. 4) LFSRs typically consume less power compared to some other methods of generating pseudo-random sequences, especially when implemented in hardware. Moreover, in our scenario, the need for costly additional memory access to retrieve indices is eliminated as LFSR logic replaces index memory storage. IV. RESULTS In this section, we present the FPGA implementation results of RAMAN and evaluate the performance of CAE models across various topologies and sparsity levels. We compare the stochastic pruning scheme with the standard magnitude pruning scheme. Furthermore, we compare our neural signal compression scheme employing CAEs with existing literature. A. FPGA Implementation The RAMAN accelerator was implemented on Efinix Tita- nium Ti60 FPGA, incorporating the stochastic weight pruning scheme presented in Section III-C.\n\n--- Segment 16 ---\nFurthermore, we compare our neural signal compression scheme employing CAEs with existing literature. A. FPGA Implementation The RAMAN accelerator was implemented on Efinix Tita- nium Ti60 FPGA, incorporating the stochastic weight pruning scheme presented in Section III-C. The hardware specifications of the RAMAN architecture and resource utilization details are presented in Table I evaluated for a custom DS-CAE1 model and the MobileNetV1-based autoencoder model with a width multiplier of 0.25x, represented as MobileNetV1- CAE(0.25x). The specific MobileNetV1-CAE and DS-CAE model architectures are provided in Table IIa and IIb. The encoder model of the CAE utilizes CONV, DW, PW, and pooling layers, amounting to a total of 2.234 million MAC operations for the DS-CAE1 model and 22.91 million MACs for the MobileNetV1-CAE(0.25x) model. The PW layer ac- counts for the majority of these MAC operations. RAMAN employs 12 processing elements, with each PE containing four MAC units. The register-file memory utilization is 0.896 KB. The PE array utilizes 52 (0.576 KB) of the registers, as each PE consists of a 16 24b Register File (RF) to store the Psums. Additionally, RAMAN offers the flexibility to decrease the PE RF width to 20b or even lower, depending on the application, thereby reducing register utilization. The post- processing module (PPM) stores post-processing parameters in registers, resulting in 32 (0.32 KB) register utilization. The power consumption is estimated to be 47.91 mW (3.11 mW dynamic power 44.79 mW static power) at a 2 MHz clock for the DS-CAE1 model, and 53.97 mW (8.97 mW dynamic power 45 mW static power) at a 7 MHz clock for the MobileNetV1-CAE(0.25x) model. Notably, owing to the FPGA implementation, the static power dissipa- tion is significant. Meanwhile, application-specific integrated circuits (ASICs) typically offer reduced power consumption and greater energy efficiency than FPGAs, making them more suitable for implantable systems.\n\n--- Segment 17 ---\nNotably, owing to the FPGA implementation, the static power dissipa- tion is significant. Meanwhile, application-specific integrated circuits (ASICs) typically offer reduced power consumption and greater energy efficiency than FPGAs, making them more suitable for implantable systems. Nevertheless, our primary objective in this research was to validate the RAMAN archi- tecture for neural signal compression using CAE. Therefore, we initially utilized FPGAs before transitioning to a full- fledged ASIC implementation. Furthermore, the power usage of our neural signal compression approach utilizing RAMAN on FPGA falls comfortably within the established power limit of the mountable device on the head [51]. The architecture utilizes 37.3k 4-input look-up tables (LUTs) and 8.6k flip-flops (FFs), which are mapped as eXchangeable Logic and Routing (XLR) cells on the Efinix FPGA [52]. Table I illustrates that the logic overhead associated with the LFSR for enabling stochastic pruning is minimal with 32 LUTs and 20 FFs totaling 46 XLR cells. The activations and weights of the encoder network were quantized to 8 bits, while the Psums generated after the MAC operation were represented with 24 bits. Consequently, vii TABLE I: Specifications and resource utilization. Platform Efinix Ti60 Layers Supported CONV, DW, PW, FC and Max Average pooling. Number of PEs 12 (4 MACs PE) Reg-file Memory 0.896 KB Clock Rate 2-7 MHz Precision Weights Activations: 8b fixed point, Partial-sums: 24b fixed point. Power (mW) 47.91 (Dynamic: 3.11, Static: 44.79) 2 MHz 53.97 (Dynamic: 8.97, Static: 45) 7 MHz XLR cells 52.3k (86 util. ), 37.3k LUTs 8.6k FFs LFSR logic: 46 XLR cells (32 LUTs 20 FFs) DSPs 61 (38.12 util.) Memory Blocks (10 Kb Blocks) 92 (Param. : 10, Act. : 48, Cache: 27, Rest: 7) 151 (Param. : 69, Act.\n\n--- Segment 18 ---\n: 48, Cache: 27, Rest: 7) 151 (Param. : 69, Act. : 48, Cache: 27, Rest: 7) MAC Operations (in Millions) 2.234 (CONV: 15.47 , DW: 12.92 , PW: 71.22 , Pool: 0.39 ) 22.91 (CONV: 1.51 , DW: 8.18 , PW: 90.29 , Pool: 0.02 ) Latency (ms) 45.47 2 MHz, 47.82 7 MHz Parameter Memory (KB) Baseline floating point: 45.76 , 8b Quantized 75 PW Stochastic Pruning: 6.19 Baseline floating point: 841.92 , 8b Quantized 75 PW Stochastic Pruning: 76.08 XLR: eXchangeable Logic and Routing (XLR) cell, DSP: Digital Signal Processor DS-CAE1 model, MobileNetV1-CAE(0.25x) model the parameter memory requirement decreased from 45.76 KB to 6.19 KB (7.4x reduction) and from 841.92 KB to 76.08 KB (11x reduction) after 8-bit quantization and 75 point-wise stochastic weight pruning for the DS-CAE1 and MobileNetV1-CAE(0.25x) models, respectively. In addition, the IA and OA memory overlapping scheme outlined in [44] reduced the peak activation memory of the MobileNetV1- CAE(0.25x) model from 76.8 KB to 48 KB, representing a 37.5 reduction. The global memory, comprising parameter memory and activation memory, along with the cache, were mapped to 10 Kb block memory units available in the Efinix Ti60 FPGA [52]. The latency of RAMAN for a single input inference to generate the compressed representation is mea- sured at 45.47 ms at a clock rate of 2 MHz and 47.82 ms at a clock rate of 7 MHz for the DS-CAE1 and MobileNetV1- CAE(0.25x) models, respectively. Since the input window spans 50 ms, the achieved latency can effectively process the input within a given period.\n\n--- Segment 19 ---\nThe latency of RAMAN for a single input inference to generate the compressed representation is mea- sured at 45.47 ms at a clock rate of 2 MHz and 47.82 ms at a clock rate of 7 MHz for the DS-CAE1 and MobileNetV1- CAE(0.25x) models, respectively. Since the input window spans 50 ms, the achieved latency can effectively process the input within a given period. The achieved throughput of the RAMAN accelerator is 98.3 MOp s for the DS-CAE1 model at 2 MHz and 958.3 MOp s for the MobileNetV1-CAE(0.25x) model at 7 MHz. B. Dataset Description We utilized the dataset [53], comprising neural recordings from the motor cortex of two macaque monkeys, K and L, during an instructed reach and grasp task. Neural activity was captured using 10-by-10 Utah electrode arrays with only 96 active electrodes. Each recording session includes two recordings, one for each monkey. The signals were sampled at 30 kHz. In this work, we are interested in the local field potential (LFP) signals, which are the aggregate synaptic activities of populations of neurons. Given that the frequencies of interest in LFPs typically range from 0.1 to 300 Hz, the sampling rate can be substantially reduced compared to what is necessary for spike-based processing, which typically falls between 1 and 2 kS s. Thus, the signals were downsampled to 2 kS s after applying a low-pass filter with a cut-off frequency of 1 KHz. The training, validation, and test sets were split into time windows of 50 ms, each corresponding to 100 samples per window at a sampling rate of 2 KHz. Input to the CAE is a 2D matrix of size 96 (Channels) x100 (samples per window). From each recording session, the first 80 of the recording is used for training, the next 10 for validation, and the final 10 for testing. The training is conducted offline and the entire implementation pipeline can be divided into three phases: 1) Training phase: During this phase, the RAMAN encoder is disabled and the uncompressed data is transmitted for offline training. Since real-time processing is not necessary, data can be recorded in local memory (such as an SD card) and transmitted at a lower data rate (channel- wise).\n\n--- Segment 20 ---\nThe training is conducted offline and the entire implementation pipeline can be divided into three phases: 1) Training phase: During this phase, the RAMAN encoder is disabled and the uncompressed data is transmitted for offline training. Since real-time processing is not necessary, data can be recorded in local memory (such as an SD card) and transmitted at a lower data rate (channel- wise). The entire encoder and decoder stack of the CAE is trained offline, and the encoder parameters obtained after quantization, pruning, and model compression are stored in on-chip RAMAN memory. Additionally, the encoder model topology is encoded and stored as instructions in the instruction memory for programming RAMAN. The training phase occurs only initially. 2) Deployment phase: During this phase, compressed data from RAMAN is transmitted for real-time offline de- coding or analysis. The encoder model parameters and instructions stored in RAMAN are utilized for real-time data encoding (compression). 3) Calibration phase: During this phase, the model is cali- brated to compensate for electrode drifts and other non- idealities over time. It has been reported that only about 10 of the new raw data is required for calibration, whereas the training phase necessitates 80 of the new data. Since the data size is small, it can be transmitted at a lower bandwidth and does not need to be in real-time. This phase can occur regularly for the periodic calibration of the model, and it has been reported that model calibra- tion achieves similar performance to complete re-training [54]. RAMAN encoder parameters remain static, and only the decoder is trained offline eliminating the need for training within the head unit. Several factors influence the calibration frequency, including the type of electrode, the implantation site, and the specifics of the experimental conditions. The model s performance can be empirically assessed for calibration by periodically transmitting the raw signal and comparing the reconstruction error be- tween the original raw signal and the decompressed signal from the decoder. C. Model Performance We experimented with six models. Among them, four are MobileNetV1 [49] based autoencoder models with width mul- viii TABLE II: Architecture of the models.\n\n--- Segment 21 ---\nC. Model Performance We experimented with six models. Among them, four are MobileNetV1 [49] based autoencoder models with width mul- viii TABLE II: Architecture of the models. Stage Type Stride M N Output Size Encoder Conv (3 3) s2 1 32 48 50 32 Conv dws (3 3) s1 32 64 48 50 64 Conv dws (3 3) s2 64 128 24 25 128 Conv dws (3 3) s1 128 128 24 25 128 Conv dws (3 3) s2 128 256 12 13 256 Conv dws (3 3) s1 256 256 12 13 256 Conv dws (3 3) s1 256 512 12 13 512 5 Conv dws (3 3) s1 512 512 12 13 512 Conv dws (3 3) s2 512 1024 6 7 1024 Conv dws (3 3) s1 1024 1024 6 7 1024 Avg Pool (6 7) s1 1024 1024 1 1 1024 Decoder ConvTranspose dw (6 7) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s2 1024 512 12 13 512 5 ConvTranspose (3 3) s1 512 512 12 13 512 ConvTranspose (3 3) s1 512 256 12 13 256 ConvTranspose (3 3) s1 256 256 12 13 256 ConvTranspose (3 3) s2 256 128 24 25 128 ConvTranspose (3 3) s1 128 128 24 25 128 ConvTranspose (3 3) s2 128 64 48 50 64 ConvTranspose (3 3) s1 64 32 48 50 32 ConvTranspose (3 3) s2 32 1 96 100 1 (a) MobileNetV1-CAE(1x) model.\n\n--- Segment 22 ---\nAmong them, four are MobileNetV1 [49] based autoencoder models with width mul- viii TABLE II: Architecture of the models. Stage Type Stride M N Output Size Encoder Conv (3 3) s2 1 32 48 50 32 Conv dws (3 3) s1 32 64 48 50 64 Conv dws (3 3) s2 64 128 24 25 128 Conv dws (3 3) s1 128 128 24 25 128 Conv dws (3 3) s2 128 256 12 13 256 Conv dws (3 3) s1 256 256 12 13 256 Conv dws (3 3) s1 256 512 12 13 512 5 Conv dws (3 3) s1 512 512 12 13 512 Conv dws (3 3) s2 512 1024 6 7 1024 Conv dws (3 3) s1 1024 1024 6 7 1024 Avg Pool (6 7) s1 1024 1024 1 1 1024 Decoder ConvTranspose dw (6 7) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s2 1024 512 12 13 512 5 ConvTranspose (3 3) s1 512 512 12 13 512 ConvTranspose (3 3) s1 512 256 12 13 256 ConvTranspose (3 3) s1 256 256 12 13 256 ConvTranspose (3 3) s2 256 128 24 25 128 ConvTranspose (3 3) s1 128 128 24 25 128 ConvTranspose (3 3) s2 128 64 48 50 64 ConvTranspose (3 3) s1 64 32 48 50 32 ConvTranspose (3 3) s2 32 1 96 100 1 (a) MobileNetV1-CAE(1x) model. Stage Type Stride M N Output Size Encoder Conv (3 3) s2 1 16 48 50 16 Conv dws (3 3) s2 16 16 24 25 16 Conv dws (3 3) s2 16 64 12 13 64 n Conv dws (3 3) s1 64 64 12 13 64 Avg Pool (12 13) s1 64 64 1 1 64 Decoder ConvTranspose dw (12 13) s1 64 64 12 13 64 n ConvTranspose (3 3) s1 64 64 12 13 64 ConvTranspose (3 3) s2 64 16 24 25 16 ConvTranspose (3 3) s2 16 16 48 50 16 ConvTranspose (3 3) s2 16 1 96 100 1 (b) DS-CAE models.\n\n--- Segment 23 ---\nStage Type Stride M N Output Size Encoder Conv (3 3) s2 1 32 48 50 32 Conv dws (3 3) s1 32 64 48 50 64 Conv dws (3 3) s2 64 128 24 25 128 Conv dws (3 3) s1 128 128 24 25 128 Conv dws (3 3) s2 128 256 12 13 256 Conv dws (3 3) s1 256 256 12 13 256 Conv dws (3 3) s1 256 512 12 13 512 5 Conv dws (3 3) s1 512 512 12 13 512 Conv dws (3 3) s2 512 1024 6 7 1024 Conv dws (3 3) s1 1024 1024 6 7 1024 Avg Pool (6 7) s1 1024 1024 1 1 1024 Decoder ConvTranspose dw (6 7) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s1 1024 1024 6 7 1024 ConvTranspose (3 3) s2 1024 512 12 13 512 5 ConvTranspose (3 3) s1 512 512 12 13 512 ConvTranspose (3 3) s1 512 256 12 13 256 ConvTranspose (3 3) s1 256 256 12 13 256 ConvTranspose (3 3) s2 256 128 24 25 128 ConvTranspose (3 3) s1 128 128 24 25 128 ConvTranspose (3 3) s2 128 64 48 50 64 ConvTranspose (3 3) s1 64 32 48 50 32 ConvTranspose (3 3) s2 32 1 96 100 1 (a) MobileNetV1-CAE(1x) model. Stage Type Stride M N Output Size Encoder Conv (3 3) s2 1 16 48 50 16 Conv dws (3 3) s2 16 16 24 25 16 Conv dws (3 3) s2 16 64 12 13 64 n Conv dws (3 3) s1 64 64 12 13 64 Avg Pool (12 13) s1 64 64 1 1 64 Decoder ConvTranspose dw (12 13) s1 64 64 12 13 64 n ConvTranspose (3 3) s1 64 64 12 13 64 ConvTranspose (3 3) s2 64 16 24 25 16 ConvTranspose (3 3) s2 16 16 48 50 16 ConvTranspose (3 3) s2 16 1 96 100 1 (b) DS-CAE models. A Conv dws (3 3) layer consists of a Depthwise convolu- tional layer with filter shape 3 3 M, followed by a Pointwise convolutional layer with filter shape 1 1 M N. Compressed representation of the input signal.\n\n--- Segment 24 ---\nStage Type Stride M N Output Size Encoder Conv (3 3) s2 1 16 48 50 16 Conv dws (3 3) s2 16 16 24 25 16 Conv dws (3 3) s2 16 64 12 13 64 n Conv dws (3 3) s1 64 64 12 13 64 Avg Pool (12 13) s1 64 64 1 1 64 Decoder ConvTranspose dw (12 13) s1 64 64 12 13 64 n ConvTranspose (3 3) s1 64 64 12 13 64 ConvTranspose (3 3) s2 64 16 24 25 16 ConvTranspose (3 3) s2 16 16 48 50 16 ConvTranspose (3 3) s2 16 1 96 100 1 (b) DS-CAE models. A Conv dws (3 3) layer consists of a Depthwise convolu- tional layer with filter shape 3 3 M, followed by a Pointwise convolutional layer with filter shape 1 1 M N. Compressed representation of the input signal. The filter shape of a ConvTranspose dw (Kh Kw) layer is Kh Kw M. The filter shape of a ConvTranspose (3 3) layer is 3 3 M N. n 2 for DS-CAE1 and n 1 for DS-CAE2. tipliers 1, 0.75, 0.5, and 0.25, and the others are custom depth- wise separable convolutional autoencoder models DS-CAE1 and DS-CAE2. The MobileNetV1-based CAE was chosen as a baseline model for comparison because it inherently employs depthwise separable convolutions, for which RAMAN is tuned and optimized. Additionally, MobileNet significantly reduces the number of parameters and computations in the network through depthwise separable convolutions, leading to reduced storage, latency, and memory access requirements. This reduc- tion is critical for deployment on edge devices for applications such as BCI. The architectures of the MobileNetV1-CAE model with width multiplier 1 and the two DS-CAE models are presented in Table IIa and IIb, respectively. For the MobileNetV1-CAE models with a width multiplier other than 1, the number of channels in each layer is multiplied by the width multiplier and rounded to the nearest integer which is greater than or equal to it and divisible by 16 as shown in (4).\n\n--- Segment 25 ---\nThe architectures of the MobileNetV1-CAE model with width multiplier 1 and the two DS-CAE models are presented in Table IIa and IIb, respectively. For the MobileNetV1-CAE models with a width multiplier other than 1, the number of channels in each layer is multiplied by the width multiplier and rounded to the nearest integer which is greater than or equal to it and divisible by 16 as shown in (4). Nl,w Nl w 16 16 (4) where Nl,w and Nl are the number of channels in layer l for MobileNetV1-CAE model with width multiplier w and 1, respectively. The PyTorch framework is used to train the models. To demonstrate the benefits of employing stochastic pruning over magnitude-based pruning, we conducted training of the models using two pruning methods separately. The results are depicted in the Table III. For magnitude-based pruning, the baseline floating-point model is trained for 500 epochs. Then, pruning is applied in the order of 25 , 50 , and 75 weight sparsity. After each pruning step, the model is trained for 100 epochs with the corresponding weight sparsity level. In the case of stochastic pruning, since the prune mask is known beforehand for all the sparsity levels, the models are pruned with that specific mask at the beginning and then trained for 500 epochs. The pruned 32-bit floating point models are then quantized to 8-bit weights, and quantization-aware training (QAT) is performed for 50 more epochs using integer-only arithmetic [55]. We employed batch normalization folding [56] to enhance the efficiency of our model, thereby reducing computational overhead while maintaining performance. We trained the models using Adam optimizer and 1cycle learning rate scheduler [57], setting a maximum learning rate of 0.01 and using a mini-batch size of 128. The mean absolute error (MAE) between the input and reconstructed signals was considered as the loss function. The ability of the models to reconstruct the input signals is evaluated using two metrics, viz. signal-to-noise and distortion ratio (SNDR) and R2 score [54].\n\n--- Segment 26 ---\nThe ability of the models to reconstruct the input signals is evaluated using two metrics, viz. signal-to-noise and distortion ratio (SNDR) and R2 score [54]. SNDR is defined as: SNDR 20 log10 x 2 x ˆx 2 (5) where x and ˆx are original and reconstructed signals respec- tively, and 2 is the L2-norm operator. Whereas R2 score is calculated as: R2 1 P i (xi ˆxi)2 P i (xi x)2 (6) where xi and ˆxi are the i-th input and reconstructed sample respectively, and x is the mean of the input signals. Fig. 7 illustrates the ablation results of our neural network models for both 32-bit floating point and 8-bit quantized versions with 0 , 25 , 50 , 75 weight sparsity. Fig. 7a and 7b presents the SNDR and R2 scores respectively for monkey K, while Fig. 7c and 7d present the same for monkey L. We have used different colors to represent different model architectures, and bit-widths are differentiated by varying the marker shapes.\n\n--- Segment 27 ---\n7a and 7b presents the SNDR and R2 scores respectively for monkey K, while Fig. 7c and 7d present the same for monkey L. We have used different colors to represent different model architectures, and bit-widths are differentiated by varying the marker shapes. The size of the markers is proportional to ix 0 25 50 75 Sparsity ( ) 22.0 22.5 23.0 23.5 24.0 24.5 25.0 25.5 26.0 SNDR (dB) 1000 kB 100 kB 10 kB Monkey K 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (a) 0 25 50 75 Sparsity ( ) 0.78 0.80 0.82 0.84 0.86 0.88 0.90 R2 Score 1000 kB 100 kB 10 kB Monkey K 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (b) 0 25 50 75 Sparsity ( ) 26.0 26.5 27.0 27.5 28.0 28.5 29.0 SNDR (dB) 1000 kB 100 kB 10 kB Monkey L 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (c) 0 25 50 75 Sparsity ( ) 0.92 0.93 0.94 0.95 0.96 R2 Score 1000 kB 100 kB 10 kB Monkey L 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (d) Fig.\n\n--- Segment 28 ---\n7c and 7d present the same for monkey L. We have used different colors to represent different model architectures, and bit-widths are differentiated by varying the marker shapes. The size of the markers is proportional to ix 0 25 50 75 Sparsity ( ) 22.0 22.5 23.0 23.5 24.0 24.5 25.0 25.5 26.0 SNDR (dB) 1000 kB 100 kB 10 kB Monkey K 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (a) 0 25 50 75 Sparsity ( ) 0.78 0.80 0.82 0.84 0.86 0.88 0.90 R2 Score 1000 kB 100 kB 10 kB Monkey K 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (b) 0 25 50 75 Sparsity ( ) 26.0 26.5 27.0 27.5 28.0 28.5 29.0 SNDR (dB) 1000 kB 100 kB 10 kB Monkey L 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (c) 0 25 50 75 Sparsity ( ) 0.92 0.93 0.94 0.95 0.96 R2 Score 1000 kB 100 kB 10 kB Monkey L 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (d) Fig. 7: Ablation study for different model architectures with different pruning percentages and bit-widths.\n\n--- Segment 29 ---\nThe size of the markers is proportional to ix 0 25 50 75 Sparsity ( ) 22.0 22.5 23.0 23.5 24.0 24.5 25.0 25.5 26.0 SNDR (dB) 1000 kB 100 kB 10 kB Monkey K 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (a) 0 25 50 75 Sparsity ( ) 0.78 0.80 0.82 0.84 0.86 0.88 0.90 R2 Score 1000 kB 100 kB 10 kB Monkey K 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (b) 0 25 50 75 Sparsity ( ) 26.0 26.5 27.0 27.5 28.0 28.5 29.0 SNDR (dB) 1000 kB 100 kB 10 kB Monkey L 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (c) 0 25 50 75 Sparsity ( ) 0.92 0.93 0.94 0.95 0.96 R2 Score 1000 kB 100 kB 10 kB Monkey L 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 32 bit 8 bit Mob(1.00x) Mob(0.75x) Mob(0.50x) Mob(0.25x) DS-CAE1 DS-CAE2 (d) Fig. 7: Ablation study for different model architectures with different pruning percentages and bit-widths. (a) SNDR for monkey K, (b) R2-score for monkey K, (c) SNDR for monkey L and (d) R2-score for monkey L. Marker size is proportional to the size of the encoder part of the model.\n\n--- Segment 30 ---\n7: Ablation study for different model architectures with different pruning percentages and bit-widths. (a) SNDR for monkey K, (b) R2-score for monkey K, (c) SNDR for monkey L and (d) R2-score for monkey L. Marker size is proportional to the size of the encoder part of the model. The MobileNetV1-CAE(0.25x) and DS-CAE1 models with a bit-width of 8 and 75 pruning percentage are deployed on RAMAN for FPGA evaluation (highlighted by green and red boxes, respectively). the respective encoder parameter size. It is observed from the plot that the custom DS-CAE1 model with 8-bit quantization and 75 sparsity exhibits comparable performance as the MobileNetV1-CAE models despite a 99.95 reduction in parameter size as compared to the MobileNetV1-CAE(1x) model with 32-bit floating point weights and 0 sparsity. Table III presents a comparison between models pruned with stochastic pruning and those pruned with magnitude-based pruning. The analysis shows that both types of pruning result in nearly equivalent SNDR and R2 scores. However, stochastic pruning reduces the parameter size because it does not require storing the compressed weight indices explicitly in memory. For instance, at 75 sparsity, the memory size reduction for the MobileNetV1-CAE(0.25x) and DS-CAE1 models was 24.2 and 15.7 , respectively. The MobileNetV1-CAE(1x) model achieved the highest parameter memory size reduction of 32.4 . In Fig. 8, we show the original signals and their reconstruc- tions along with the absolute error using the MobileNetV1- CAE(0.25x) and DS-CAE1 models with 8-bit quantization and 75 pruning. These reconstructions were obtained for the channels that have the best, medium, and worst SNDRs. The input to the models has a dimension of 96 100, and the encoder output of the MobileNetV1-CAE(0.25x) and DS- CAE1 models are 1 1 256 and 1 1 64 (cf. Table IIa and IIb), respectively.\n\n--- Segment 31 ---\nThe input to the models has a dimension of 96 100, and the encoder output of the MobileNetV1-CAE(0.25x) and DS- CAE1 models are 1 1 256 and 1 1 64 (cf. Table IIa and IIb), respectively. Therefore, the MobileNetV1-CAE(0.25x) model has a CR of (96 100) 256 37.5, while the DS-CAE1 model achieves a CR of (96 100) 64 150. We investigated how CAE models generalize across diverse monkey recordings. Using data from monkeys K and L, we trained CAE models on a combined dataset with 80 of the recordings from each monkey and evaluated their performance on individual test sets. Table IV compares models trained separately on monkeys K and L with those trained on the x TABLE III: Comparison between stochastic pruning and standard magnitude-based pruning for 8-bit quantization.\n\n--- Segment 32 ---\nUsing data from monkeys K and L, we trained CAE models on a combined dataset with 80 of the recordings from each monkey and evaluated their performance on individual test sets. Table IV compares models trained separately on monkeys K and L with those trained on the x TABLE III: Comparison between stochastic pruning and standard magnitude-based pruning for 8-bit quantization. Model Sparsity ( ) Monkey K Monkey L Stochastic Pruning Magnitude-based Pruning Stochastic Pruning Magnitude-based Pruning SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) MobileNetV1- CAE(1.00x) 25 25.23 2.53 0.90 0.08 2456.384 25.20 2.50 0.89 0.08 3633.728 28.22 2.37 0.95 0.18 2456.384 28.19 2.37 0.94 0.17 3633.728 50 25.22 2.54 0.89 0.08 1671.488 25.20 2.51 0.89 0.08 2456.384 28.24 2.38 0.95 0.17 1671.488 28.18 2.40 0.94 0.18 2456.384 75 25.05 2.48 0.89 0.09 886.592 25.14 2.50 0.89 0.08 1279.04 28.27 2.35 0.95 0.17 886.592 28.18 2.41 0.94 0.18 1279.04 MobileNetV1- CAE(0.25x) 25 24.33 2.21 0.87 0.10 173.36 23.94 2.02 0.86 0.14 246.32 28.63 2.34 0.95 0.15 173.36 28.48 2.28 0.95 0.17 246.32 50 24.18 2.21 0.87 0.13 124.72 23.91 2.03 0.86 0.14 173.36 28.48 2.29 0.95 0.14 124.72 28.55 2.29 0.95 0.16 173.36 75 24.37 2.19 0.87 0.10 76.08 24.03 2.05 0.86 0.14 100.4 28.49 2.28 0.95 0.15 76.08 28.57 2.31 0.95 0.15 100.4 DS-CAE1 25 22.78 2.38 0.82 0.16 10.8 22.75 2.35 0.81 0.14 14.256 27.55 2.42 0.94 0.15 10.8 27.78 2.51 0.94 0.16 14.256 50 22.70 2.21 0.81 0.19 8.496 22.69 2.30 0.81 0.14 10.8 27.55 2.52 0.94 0.15 8.496 27.78 2.53 0.94 0.16 10.8 75 22.61 2.21 0.81 0.13 6.192 22.38 2.18 0.80 0.14 7.344 27.43 2.41 0.94 0.13 6.192 27.61 2.50 0.94 0.15 7.344 The parameter size of the encoder part.\n\n--- Segment 33 ---\nTable IV compares models trained separately on monkeys K and L with those trained on the x TABLE III: Comparison between stochastic pruning and standard magnitude-based pruning for 8-bit quantization. Model Sparsity ( ) Monkey K Monkey L Stochastic Pruning Magnitude-based Pruning Stochastic Pruning Magnitude-based Pruning SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) MobileNetV1- CAE(1.00x) 25 25.23 2.53 0.90 0.08 2456.384 25.20 2.50 0.89 0.08 3633.728 28.22 2.37 0.95 0.18 2456.384 28.19 2.37 0.94 0.17 3633.728 50 25.22 2.54 0.89 0.08 1671.488 25.20 2.51 0.89 0.08 2456.384 28.24 2.38 0.95 0.17 1671.488 28.18 2.40 0.94 0.18 2456.384 75 25.05 2.48 0.89 0.09 886.592 25.14 2.50 0.89 0.08 1279.04 28.27 2.35 0.95 0.17 886.592 28.18 2.41 0.94 0.18 1279.04 MobileNetV1- CAE(0.25x) 25 24.33 2.21 0.87 0.10 173.36 23.94 2.02 0.86 0.14 246.32 28.63 2.34 0.95 0.15 173.36 28.48 2.28 0.95 0.17 246.32 50 24.18 2.21 0.87 0.13 124.72 23.91 2.03 0.86 0.14 173.36 28.48 2.29 0.95 0.14 124.72 28.55 2.29 0.95 0.16 173.36 75 24.37 2.19 0.87 0.10 76.08 24.03 2.05 0.86 0.14 100.4 28.49 2.28 0.95 0.15 76.08 28.57 2.31 0.95 0.15 100.4 DS-CAE1 25 22.78 2.38 0.82 0.16 10.8 22.75 2.35 0.81 0.14 14.256 27.55 2.42 0.94 0.15 10.8 27.78 2.51 0.94 0.16 14.256 50 22.70 2.21 0.81 0.19 8.496 22.69 2.30 0.81 0.14 10.8 27.55 2.52 0.94 0.15 8.496 27.78 2.53 0.94 0.16 10.8 75 22.61 2.21 0.81 0.13 6.192 22.38 2.18 0.80 0.14 7.344 27.43 2.41 0.94 0.13 6.192 27.61 2.50 0.94 0.15 7.344 The parameter size of the encoder part. 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 23 SNDR 33.87 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 15 SNDR 29.55 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 76 SNDR 20.98 dB Original Reconstructed Absolute Error Mob(0.25x) Monkey K (CR 37.5) (a) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 22 SNDR 38.10 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 9 SNDR 28.22 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 43 SNDR 24.22 dB Original Reconstructed Absolute Error Mob(0.25x) Monkey L (CR 37.5) (b) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 23 SNDR 32.70 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 12 SNDR 24.20 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 76 SNDR 19.01 dB Original Reconstructed Absolute Error DS-CAE1 Monkey K (CR 150.0) (c) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 22 SNDR 38.36 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 57 SNDR 27.24 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 24 SNDR 24.44 dB Original Reconstructed Absolute Error DS-CAE1 Monkey L (CR 150.0) (d) Fig.\n\n--- Segment 34 ---\nModel Sparsity ( ) Monkey K Monkey L Stochastic Pruning Magnitude-based Pruning Stochastic Pruning Magnitude-based Pruning SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) SNDR (dB) R2 Score Size (kB) MobileNetV1- CAE(1.00x) 25 25.23 2.53 0.90 0.08 2456.384 25.20 2.50 0.89 0.08 3633.728 28.22 2.37 0.95 0.18 2456.384 28.19 2.37 0.94 0.17 3633.728 50 25.22 2.54 0.89 0.08 1671.488 25.20 2.51 0.89 0.08 2456.384 28.24 2.38 0.95 0.17 1671.488 28.18 2.40 0.94 0.18 2456.384 75 25.05 2.48 0.89 0.09 886.592 25.14 2.50 0.89 0.08 1279.04 28.27 2.35 0.95 0.17 886.592 28.18 2.41 0.94 0.18 1279.04 MobileNetV1- CAE(0.25x) 25 24.33 2.21 0.87 0.10 173.36 23.94 2.02 0.86 0.14 246.32 28.63 2.34 0.95 0.15 173.36 28.48 2.28 0.95 0.17 246.32 50 24.18 2.21 0.87 0.13 124.72 23.91 2.03 0.86 0.14 173.36 28.48 2.29 0.95 0.14 124.72 28.55 2.29 0.95 0.16 173.36 75 24.37 2.19 0.87 0.10 76.08 24.03 2.05 0.86 0.14 100.4 28.49 2.28 0.95 0.15 76.08 28.57 2.31 0.95 0.15 100.4 DS-CAE1 25 22.78 2.38 0.82 0.16 10.8 22.75 2.35 0.81 0.14 14.256 27.55 2.42 0.94 0.15 10.8 27.78 2.51 0.94 0.16 14.256 50 22.70 2.21 0.81 0.19 8.496 22.69 2.30 0.81 0.14 10.8 27.55 2.52 0.94 0.15 8.496 27.78 2.53 0.94 0.16 10.8 75 22.61 2.21 0.81 0.13 6.192 22.38 2.18 0.80 0.14 7.344 27.43 2.41 0.94 0.13 6.192 27.61 2.50 0.94 0.15 7.344 The parameter size of the encoder part. 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 23 SNDR 33.87 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 15 SNDR 29.55 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 76 SNDR 20.98 dB Original Reconstructed Absolute Error Mob(0.25x) Monkey K (CR 37.5) (a) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 22 SNDR 38.10 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 9 SNDR 28.22 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 43 SNDR 24.22 dB Original Reconstructed Absolute Error Mob(0.25x) Monkey L (CR 37.5) (b) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 23 SNDR 32.70 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 12 SNDR 24.20 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 76 SNDR 19.01 dB Original Reconstructed Absolute Error DS-CAE1 Monkey K (CR 150.0) (c) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 22 SNDR 38.36 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 57 SNDR 27.24 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 24 SNDR 24.44 dB Original Reconstructed Absolute Error DS-CAE1 Monkey L (CR 150.0) (d) Fig. 8: Original and reconstructed signals along with their absolute differences for MobileNetV1-CAE(0.25x) and DS-CAE1 model using 8-bit quantization and 75 sparsity.\n\n--- Segment 35 ---\n0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 23 SNDR 33.87 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 15 SNDR 29.55 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 76 SNDR 20.98 dB Original Reconstructed Absolute Error Mob(0.25x) Monkey K (CR 37.5) (a) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 22 SNDR 38.10 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 9 SNDR 28.22 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 43 SNDR 24.22 dB Original Reconstructed Absolute Error Mob(0.25x) Monkey L (CR 37.5) (b) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 23 SNDR 32.70 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 12 SNDR 24.20 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 76 SNDR 19.01 dB Original Reconstructed Absolute Error DS-CAE1 Monkey K (CR 150.0) (c) 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 22 SNDR 38.36 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 57 SNDR 27.24 dB 0.0 12.5 25.0 37.5 50.0 Time (s) Signal Channel 24 SNDR 24.44 dB Original Reconstructed Absolute Error DS-CAE1 Monkey L (CR 150.0) (d) Fig. 8: Original and reconstructed signals along with their absolute differences for MobileNetV1-CAE(0.25x) and DS-CAE1 model using 8-bit quantization and 75 sparsity. The channels with the best, medium, and worst SNDRs are shown.\n\n--- Segment 36 ---\n8: Original and reconstructed signals along with their absolute differences for MobileNetV1-CAE(0.25x) and DS-CAE1 model using 8-bit quantization and 75 sparsity. The channels with the best, medium, and worst SNDRs are shown. combined dataset, using MobileNetV1-CAE(0.25x) and DS- CAE1 models across various weight sparsity values. Notably, models trained on the combined dataset showed similar or improved performance compared to those trained on individual datasets, particularly for the MobileNetV1-CAE(0.25x) model. Additionally, we tested the performance of models trained on one monkey (e.g., monkey K) and applied to the other (e.g., monkey L). To avoid overfitting, we trained the model with 80 of the recordings from monkey K (or L) and 5 from monkey L (or K), then tested it on the remaining data of monkey L (or K). Using the MobileNetV1-CAE(0.25x) model, we observed 13-14 reductions in SNDR and 17- 18 reductions in R2 score for monkey K, and 8-16 reductions in SNDR and 3-9 reductions in R2 score for monkey L across different sparsity levels, compared to the baseline models trained on the combined dataset (80 of the recording from each monkey). For the DS-CAE1 model, we observed 7-8 reductions in SNDR and 10-13 reductions in R2 score for monkey K, and 7-8 reductions in SNDR and 6-7 reductions in R2 score for monkey L across different sparsity levels. This suggests that CAEs can learn effectively even when exposed to a small portion of unseen data during training. D. Comparison with prior works Table V compares the proposed neural signal compression scheme employing RAMAN with existing works. [25], [58], [59], [63] employ the compressed sensing scheme for data compression. Shoaran et al. [25] propose an analog domain im- xi TABLE IV: Comparison of performance of 8-bit quantized models trained on individual and combined dataset.\n\n--- Segment 37 ---\nShoaran et al. [25] propose an analog domain im- xi TABLE IV: Comparison of performance of 8-bit quantized models trained on individual and combined dataset. Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 23.72 2.02 0.85 0.14 28.40 2.25 0.95 0.17 Combined 24.26 2.01 0.87 0.13 29.46 2.33 0.96 0.14 25 Individual 24.33 2.21 0.87 0.10 28.63 2.34 0.95 0.15 Combined 24.35 2.07 0.87 0.11 29.52 2.17 0.96 0.13 50 Individual 24.18 2.21 0.87 0.13 28.48 2.29 0.95 0.14 Combined 24.30 2.03 0.87 0.15 29.55 2.24 0.96 0.14 75 Individual 24.37 2.19 0.87 0.10 28.49 2.28 0.95 0.15 Combined 24.47 1.99 0.88 0.13 29.49 2.22 0.96 0.15 (a) MobileNetV1-CAE(0.25x) Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 22.72 2.35 0.81 0.14 27.71 2.47 0.94 0.17 Combined 22.65 2.24 0.81 0.14 27.32 2.35 0.93 0.17 25 Individual 22.78 2.38 0.82 0.16 27.55 2.42 0.94 0.15 Combined 22.56 2.17 0.81 0.16 27.33 2.39 0.93 0.17 50 Individual 22.70 2.21 0.81 0.19 27.55 2.52 0.94 0.15 Combined 22.53 2.15 0.81 0.16 27.35 2.45 0.93 0.15 75 Individual 22.61 2.21 0.81 0.13 27.43 2.41 0.94 0.13 Combined 22.28 2.03 0.79 0.17 27.08 2.43 0.93 0.20 (b) DS-CAE1 Training set of either Monkey K or Monkey L, evaluation is done on the test set of the same monkey.\n\n--- Segment 38 ---\n[25] propose an analog domain im- xi TABLE IV: Comparison of performance of 8-bit quantized models trained on individual and combined dataset. Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 23.72 2.02 0.85 0.14 28.40 2.25 0.95 0.17 Combined 24.26 2.01 0.87 0.13 29.46 2.33 0.96 0.14 25 Individual 24.33 2.21 0.87 0.10 28.63 2.34 0.95 0.15 Combined 24.35 2.07 0.87 0.11 29.52 2.17 0.96 0.13 50 Individual 24.18 2.21 0.87 0.13 28.48 2.29 0.95 0.14 Combined 24.30 2.03 0.87 0.15 29.55 2.24 0.96 0.14 75 Individual 24.37 2.19 0.87 0.10 28.49 2.28 0.95 0.15 Combined 24.47 1.99 0.88 0.13 29.49 2.22 0.96 0.15 (a) MobileNetV1-CAE(0.25x) Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 22.72 2.35 0.81 0.14 27.71 2.47 0.94 0.17 Combined 22.65 2.24 0.81 0.14 27.32 2.35 0.93 0.17 25 Individual 22.78 2.38 0.82 0.16 27.55 2.42 0.94 0.15 Combined 22.56 2.17 0.81 0.16 27.33 2.39 0.93 0.17 50 Individual 22.70 2.21 0.81 0.19 27.55 2.52 0.94 0.15 Combined 22.53 2.15 0.81 0.16 27.35 2.45 0.93 0.15 75 Individual 22.61 2.21 0.81 0.13 27.43 2.41 0.94 0.13 Combined 22.28 2.03 0.79 0.17 27.08 2.43 0.93 0.20 (b) DS-CAE1 Training set of either Monkey K or Monkey L, evaluation is done on the test set of the same monkey. The training set of both Monkey K and L are combined together.\n\n--- Segment 39 ---\nSparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 23.72 2.02 0.85 0.14 28.40 2.25 0.95 0.17 Combined 24.26 2.01 0.87 0.13 29.46 2.33 0.96 0.14 25 Individual 24.33 2.21 0.87 0.10 28.63 2.34 0.95 0.15 Combined 24.35 2.07 0.87 0.11 29.52 2.17 0.96 0.13 50 Individual 24.18 2.21 0.87 0.13 28.48 2.29 0.95 0.14 Combined 24.30 2.03 0.87 0.15 29.55 2.24 0.96 0.14 75 Individual 24.37 2.19 0.87 0.10 28.49 2.28 0.95 0.15 Combined 24.47 1.99 0.88 0.13 29.49 2.22 0.96 0.15 (a) MobileNetV1-CAE(0.25x) Sparsity ( ) Training Dataset Monkey K Monkey L SNDR (dB) R2 Score SNDR (dB) R2 Score 0 Individual 22.72 2.35 0.81 0.14 27.71 2.47 0.94 0.17 Combined 22.65 2.24 0.81 0.14 27.32 2.35 0.93 0.17 25 Individual 22.78 2.38 0.82 0.16 27.55 2.42 0.94 0.15 Combined 22.56 2.17 0.81 0.16 27.33 2.39 0.93 0.17 50 Individual 22.70 2.21 0.81 0.19 27.55 2.52 0.94 0.15 Combined 22.53 2.15 0.81 0.16 27.35 2.45 0.93 0.15 75 Individual 22.61 2.21 0.81 0.13 27.43 2.41 0.94 0.13 Combined 22.28 2.03 0.79 0.17 27.08 2.43 0.93 0.20 (b) DS-CAE1 Training set of either Monkey K or Monkey L, evaluation is done on the test set of the same monkey. The training set of both Monkey K and L are combined together. TABLE V: Comparison of the proposed neural signal compression scheme with existing works.\n\n--- Segment 40 ---\nThe training set of both Monkey K and L are combined together. TABLE V: Comparison of the proposed neural signal compression scheme with existing works. Shoaran et al. [25] Li et al. [58] Liu et al. [59] Park et al. [60] Khazaei et al. [61] Valencia et al. [54] Turcotte et al. [62] Shrivastwa et al. [63] Our Work Platform ASIC 180-nm ASIC 130-nm ASIC 180-nm ASIC 180-nm ASIC 130-nm ASIC 180-nm Xilinx Spartan-6 FPGA Xilinx Virtex-7 FPGA Efinix Ti60 FPGA Signal Type EEG Spike LFP LFP LFP LFP Spike ECoG LFP Compression Algorithm CS CS CS DRR Hufmann Coding DRR AE DWT CS CAE Precision 10b 10b 10b 10b 10b I P:16b, O P:10b 16b 16b W: 8b, Act. : 8b Compression Ratio 16 10 8-16 4.3-5.8 2 19.2 4.17 4 150 , 37.5 SNDR (dB) 21.8 N A 9.78 N A N A 19 3 17 N A 22.61 2.21 [K], 27.43 2.41 [L] 24.37 2.19 [K], 28.49 2.28 [L] R2 Score N A N A N A N A N A 0.72 0.23[K] 0.93 0.09[L] N A N A 0.81 0.13 [K], 0.94 0.13 [L] 0.87 0.10 [K], 0.95 0.15 [L] [K]Monkey K recordings, [L]Monkey L recordings obtained from dataset [53]. DS-CAE1 model, MobileNetV1-CAE(0.25x) model plementation of the CS algorithm supporting a CR of up to 16, achieving an SNDR of 21.8 dB. The design was implemented using ASIC 180-nm process technology with an area of 0.008 mm2 per channel and power of 0.95 µW per channel. Li et al.\n\n--- Segment 41 ---\nThe design was implemented using ASIC 180-nm process technology with an area of 0.008 mm2 per channel and power of 0.95 µW per channel. Li et al. [58] introduce a Minimum Euclidean or Manhattan Distance Cluster-based (MDC) deterministic compressed sensing matrix for compressing multi-channel neural signals, achieving a CR of 10. The design was fabricated on 130-nm CMOS with a core area of 0.03 mm2 per channel and power 12.5 µW per channel. Liu et al. [59] present a fully integrated wireless neural signal acquisition system with an integrated compressed sensing processor fabricated using 180-nm CMOS technology with a power of 3.2 µW per channel. They achieve a CR of 8-16 with 9.78 dB SNDR. Park et al. [60] and Khazaei et al. [61] present lossless compression by employing the dynamic range reduction (DRR) technique. Park et al. [60] exploit the spatial and temporal correlation of neural signals to reduce the dynamic range of LFPs. Additionally, Huffman encoding was applied to compress the LFP signals, achieving an overall average CR of 4.3-5.8. The prototype chip was fabricated using 180-nm CMOS technology with an area of 0.098 mm2 per channel and a power of 15.35 µW per channel. Khazaei et al. [61] propose a lossless data reduction scheme by eliminating spatial redundancy across parallel recording channels, achieving a CR of 2. The design was fabricated using TSMC 130-nm CMOS technology, occupying a silicon area of 0.004 mm2 per channel, and dissipating 6.4 µW of power per channel. Valencia et al. [54] propose an autoencoder-based compression digital architecture for the efficient transmission of LFP neural signals. The compression method outlined in [54] differs from our proposed approach. They utilize standard autoencoders (AEs) with dense layers to compress the spatial (channels) domain from 96 to 8. In contrast, our method employs a convolutional autoencoder that compresses both spatial and temporal domains, resulting in a higher compression ratio. Additionally, [54] utilizes 16-bit input data samples and 10-bit compressed outputs, achieving an overall CR of 19.2 (96 16 (8 10)).\n\n--- Segment 42 ---\nIn contrast, our method employs a convolutional autoencoder that compresses both spatial and temporal domains, resulting in a higher compression ratio. Additionally, [54] utilizes 16-bit input data samples and 10-bit compressed outputs, achieving an overall CR of 19.2 (96 16 (8 10)). Their design was implemented using 180-nm CMOS process technology with an area of 0.002 mm2 per channel. To benchmark our results, we use the same dataset [53] as [54], and it s evident that our SNDR and R2 scores are superior, even at a high CR of 150 with a dynamic xii power of 32.39 µW per channel for the DS-CAE1 model at 2 MHz. Additionally, several FPGA-based implementations have been proposed in the literature. Turcotte et al. [62] employs a four-level discrete wavelet transform (DWT) to compress neural data. Their system, comprising a spike detection core, threshold estimation core, and wavelet compression, was im- plemented on a Xilinx Spartan-6 FPGA. This design achieves a CR of 4.17 with an SNDR of 17 dB consuming power of 5 mW per channel. Shrivastwa et al. [63] utilize a combination of compressed sensing and neural networks to compress and reconstruct ECoG signals, respectively. Their design, imple- mented on a Xilinx Virtex-7 FPGA with 285.5k LUTs and 22.18k registers, achieves a CR of up to 4. In practical BCI applications where subsequent decoding and processing can reasonably tolerate reconstruction and wireless transmission errors, employing lossy methods such as CAE, AE, or CS with a higher CR presents a more feasible approach. Specifically, the proposed CAE-based compression scheme achieves a significantly higher compression ratio than previous methods due to compression in both spatial and temporal domains while maintaining good SNDR and R2 scores. On the contrary, lossless compression schemes involve reducing the dynamic range of neural signals and encoding them using Huffman coding. While this approach reconstructs the original signal accurately, it typically achieves a very low compression ratio. Additionally, various spike compression methods dedicated to high-density brain-implantable microsystems have been proposed [32] [35], [64]. Chen et al.\n\n--- Segment 43 ---\nAdditionally, various spike compression methods dedicated to high-density brain-implantable microsystems have been proposed [32] [35], [64]. Chen et al. [33] introduced an online neural signal processor (NSP) for spike detection, feature extraction using first and second derivative extrema, and complex spike clustering using the Geo-OSort algorithm. This clustering algorithm involves threshold calculation and Euclidean distance estimation, taking into account the geo- metric information of the high-density probe to reduce com- plexity. Their NSP, fabricated using a 22 nm FDSOI CMOS process, achieved a compression ratio (CR) of 982 with an assumption of 10 spikes s channel. Shaeri and Sodagar [34] utilized the Discrete Haar Wavelet Transform (DHWT) and spike extraction, proposing a novel data framing scheme to compress the spike waveforms. Their design, fabricated in a 130 nm CMOS process, achieved a CR of 903. Mohan et al. [35] employed a neuromorphic approach to spike compression inspired by DVS sensors [65]. They used a delta modulator to encode input spike waveforms into ON OFF pulses, proposing two transmission modes: All pulse mode (APM) and Pulse count mode (PCM), and exploring the trade-offs between them. They also examined address event representation (AER) for asynchronous data transfer to prevent pulse loss due to collisions, achieving a CR of 40.37 at a 62 Hz firing rate per channel with 90 spike detection accuracy. Nekoui and Sodagar [64] proposed a spike compression scheme through selective downsampling at the implant side, reconstructing the spike offline using third-order polynomial curve fitting. This design, implemented in 130-nm CMOS technology, achieved a CR of 446.5. In another study, Shaeri and Sodagar [32] presented a framework for on-implant spike sorting based on salient feature extraction, maximizing the geometric mean for spike wave-shape isolation. Their systems include an online spike sorting module configured by a shadow spike sorter block on an external module. Generally, spike compression offers superior CR since only the spike events (with an average firing rate of 40-60 Hz) are detected, extracted, compressed, and transmitted.\n\n--- Segment 44 ---\nTheir systems include an online spike sorting module configured by a shadow spike sorter block on an external module. Generally, spike compression offers superior CR since only the spike events (with an average firing rate of 40-60 Hz) are detected, extracted, compressed, and transmitted. In contrast, our work focuses on compressing local field potentials (LFPs), which are a different signal modality compared to spike waveforms. LFPs are usually low-frequency components of the neural signal (typically 300 Hz), and the information is encoded in the raw signal itself rather than in spikes. Therefore, in our proposed compression scheme, we compress the raw neural signal as opposed to extracting and compressing spike waveforms. The CR achieved by the proposed compression scheme is superior to the existing LFP compression methods [54], [59] [61]. V. CONCLUSIONS This paper introduces a novel neural signal compression scheme employing convolutional autoencoders. The encoder section of the CAE underwent several hardware-software co- optimizations and was subsequently deployed on the RAMAN tinyML accelerator specifically designed for edge computing applications. RAMAN leveraged weight and activation spar- sity to reduce latency, memory usage, and power consumption. A novel hardware-aware stochastic pruning technique was em- ployed to address workload imbalance issues across multiple parallel processing elements and reduce the indexing overhead associated with compressed weight storage, resulting in up to a 32.4 reduction in parameter memory requirements. Furthermore, since RAMAN inherently supports a wide range of neural network topologies, including standard con- volutions, depth-wise convolutions, point-wise convolutions, pooling layers, and dense layers, the encoder of the CAE was constructed based on depth-wise separable convolutional layers to minimize the number of MAC operations. The proposed CAE-based scheme performs compression in both spatial (channel) and temporal domains, achieving a superior compression ratio of up to 150. The CAE encoder model was pruned using the stochastic pruning scheme and quantized to 8 bits before deployment on the RAMAN tinyML accelerator. RAMAN was implemented on the Efinix Ti60 FPGA with 52.3k XLR cells and 61 DSP units, and the compressed neural data obtained at the output of RAMAN was decoded offline.\n\n--- Segment 45 ---\nThe CAE encoder model was pruned using the stochastic pruning scheme and quantized to 8 bits before deployment on the RAMAN tinyML accelerator. RAMAN was implemented on the Efinix Ti60 FPGA with 52.3k XLR cells and 61 DSP units, and the compressed neural data obtained at the output of RAMAN was decoded offline. Compared to recently reported compression algorithms, our scheme achieves superior reconstruction qual- ity, with signal-to-noise and distortion ratios of 22.6 dB and 27.4 dB and R2 scores of 0.81 and 0.94 for the two monkey neural recordings employing a compact custom-designed DS- CAE1 model. VI. ACKNOWLEDGEMENTS The authors would like to express their sincere gratitude and appreciation to their colleagues, Srikanth Rohit Nudu- rupati, Chandana D G, Hitesh Pavan Oleti, Anand Chauhan, Shankaranarayanan H, and Ashwin Rajesh for their invaluable help throughout this work. xiii REFERENCES [1] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M. Vaughan, Brain computer interfaces for communication and control, Clinical Neurophysiology, vol. 113, no. 6, pp. 767 791, 2002. [Online]. Available: S1388245702000573 [2] L. F. Nicolas-Alonso and J. Gomez-Gil, Brain computer interfaces, a review, Sensors (Basel), vol. 12, no. 2, pp. 1211 1279, Jan. 2012. [3] U. Chaudhary, N. Birbaumer, and A. Ramos-Murguialday, Brain computer interfaces for communication and rehabilitation, Nature Reviews Neurology, vol. 12, no. 9, pp. 513 525, 2016. [Online]. Available: [4] P. Demarest, N. Rustamov, J.\n\n--- Segment 46 ---\n[Online]. Available: [4] P. Demarest, N. Rustamov, J. Swift, T. Xie, M. Adamek, H. Cho, E. Wilson, Z. Han, A. Belsten, N. Luczak, P. Brunner, S. Haroutounian, and E. C. Leuthardt, A novel theta-controlled vibrotactile brain computer interface to treat chronic pain: a pilot study, Scientific Reports, vol. 14, no. 1, p. 3433, 2024. [Online]. Available: [5] J. N. Mak and J. R. Wolpaw, Clinical Applications of Brain-Computer Interfaces: Current State and Future Prospects, IEEE Reviews in Biomedical Engineering, vol. 2, pp. 187 199, 2009. [6] J. J. Shih, D. J. Krusienski, and J. R. Wolpaw, Brain-computer interfaces in medicine, Mayo Clinic Proceedings, vol. 87, no. 3, pp. 268 279, Mar 2012. [7] A. Krishna, V. Ramanathan, S. S. Yadav, S. Shah, A. van Schaik, M. Mehendale, and C. S. Thakur, A Sparsity-driven tinyML Accelerator for Decoding Hand Kinematics in Brain-Computer Interfaces, in 2023 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2023, pp. 1 5. [8] D. J. McFarland and J. R. Wolpaw, Brain-Computer Interfaces for Communication and Control, Communications of the ACM, vol. 54, no. 5, pp. 60 66, 2011. [9] K. Karas, L. Pozzi, A. Pedrocchi, F. Braghin, and L. Roveda, Brain-computer interface for robot control with eye artifacts for assistive applications, Scientific Reports, vol. 13, no. 1, p. 17512, 2023. [Online].\n\n--- Segment 47 ---\n1, p. 17512, 2023. [Online]. Available: [10] M. A. Cervera, S. R. Soekadar, J. Ushiba, J. D. R. Millán, M. Liu, N. Birbaumer, and G. Garipelli, Brain-computer interfaces for post- stroke motor rehabilitation: a meta-analysis, Annals of Clinical and Translational Neurology, vol. 5, no. 5, pp. 651 663, 2018. [11] M. Sebastián-Romagosa, W. Cho, R. Ortner, N. Murovec, T. Von Oertzen, K. Kamada, B. Z. Allison, and C. Guger, Brain Computer Interface Treatment for Motor Rehabilitation of Upper Extremity of Stroke Patients A Feasibility Study, Frontiers in Neuroscience, vol. 14, 2020. [Online]. Available: frontiersin.org journals neuroscience articles 10.3389 fnins.2020.591435 [12] R. Mane, T. Chouhan, and C. Guan, BCI for stroke rehabilitation: motor and beyond, Journal of Neural Engineering, vol. 17, no. 4, p. 041001, 2020. [13] P. D. E. Baniqued, E. C. Stanyer, M. Awais, A. Alazmani, A. E. Jackson, M. A. Mon-Williams, F. Mushtaq, and R. J. Holt, Brain computer interface robotics for hand rehabilitation after stroke: a systematic review, Journal of NeuroEngineering and Rehabilitation, vol. 18, no. 1, p. 15, 2021. [Online]. Available: [14] H.-J. Hwang, V. Y. Ferreria, D. Ulrich, T. Kilic, X. Chatziliadis, B. Blankertz, and M. Treder, A Gaze Independent Brain-Computer Interface Based on Visual Stimulation through Closed Eyelids, Scientific Reports, vol. 5, no. 1, p. 15890, 2015. [Online].\n\n--- Segment 48 ---\n1, p. 15890, 2015. [Online]. Available: [15] S. Niketeghad and N. Pouratian, Brain Machine Interfaces for Vision Restoration: The Current State of Cortical Visual Prosthetics, Neurotherapeutics, vol. 16, no. 1, pp. 134 143, 2019. [Online]. Available: [16] Z. Wang, N. Shi, Y. Zhang, N. Zheng, H. Li, Y. Jiao, J. Cheng, Y. Wang, X. Zhang, Y. Chen, Y. Chen, H. Wang, T. Xie, Y. Wang, Y. Ma, X. Gao, and X. Feng, Conformal in-ear bioelectronics for visual and auditory brain-computer interfaces, Nature Communications, vol. 14, no. 1, p. 4213, 2023. [Online]. Available: [17] M. J. Vansteensel, E. G. Pels, M. G. Bleichner, M. P. Branco, T. Denison, Z. V. Freudenburg, P. Gosselaar, S. Leinders, T. H. Ottens, M. A. Van Den Boom, P. C. Van Rijen, E. J. Aarnoutse, and N. F. Ramsey, Fully Implanted Brain Computer Interface in a Locked-In Patient with ALS, New England Journal of Medicine, vol. 375, no. 21, pp. 2060 2066, 2016, pMID: 27959736. [Online]. Available: [18] U. Chaudhary, B. Xia, S. Silvoni, L. G. Cohen, and N. Birbaumer, Brain-Computer Interface-Based Communication in the Completely Locked-In State, PLoS biology, vol. 15, no. 1, p. e1002593, Jan 2017. [Online]. Available: [19] R. R. Harrison, The Design of Integrated Circuits to Observe Brain Activity, Proceedings of the IEEE, vol. 96, pp. 1203 1216, 2008. [Online].\n\n--- Segment 49 ---\n1203 1216, 2008. [Online]. Available: [20] T. Oxley, N. Opie, S. John, and et al., Minimally invasive endovascular stent-electrode array for high-fidelity, chronic recordings of cortical neural activity, Nature Biotechnology, vol. 34, pp. 320 327, 2016. [21] E. Musk, An Integrated Brain-Machine Interface Platform With Thousands of Channels, J Med Internet Res, vol. 21, no. 10, p. e16194, Oct 2019. [Online]. Available: [22] N. A. Steinmetz, C. Aydin, and et al., Neuropixels 2.0: A Miniaturized High-Density Probe for Stable, Long-Term Brain Recordings, Science (New York, N.Y.), vol. 372, no. 6539, p. eabf4588, Apr 2021, copyright 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. [Online]. Available: [23] A. Wang, Z. Jin, C. Song, and W. Xu, Adaptive compressed sensing architecture in wireless brain-computer interface, in Proceedings of the 52nd Annual Design Automation Conference, ser. DAC 15. New York, NY, USA: Association for Computing Machinery, 2015. [Online]. Available: [24] R. R. Shrivastwa, V. Pudi, C. Duo, R. So, A. Chattopadhyay, and G. Cun- tai, A Brain Computer Interface Framework Based on Compressive Sensing and Deep Learning, IEEE Consumer Electronics Magazine, vol. 9, no. 3, pp. 90 96, 2020. [25] M. Shoaran, M. H. Kamal, C. Pollo, P. Vandergheynst, and A. Schmid, Compact Low-Power Cortical Recording Architecture for Compressive Multichannel Data Acquisition, IEEE Transactions on Biomedical Circuits and Systems, vol. 8, no. 6, pp. 857 870, 2014.\n\n--- Segment 50 ---\n6, pp. 857 870, 2014. [26] K. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T.-W. Lee, and T. J. Sejnowski, Dictionary Learning Algorithms for Sparse Representation, Neural Computation, vol. 15, no. 2, pp. 349 396, 02 2003. [Online]. Available: [27] H. Daou and F. Labeau, Dynamic Dictionary for Combined EEG Compression and Seizure Detection, IEEE Journal of Biomedical and Health Informatics, vol. 18, no. 1, pp. 247 256, 2014. [28] V. Vadori, E. Grisan, and M. Rossi, Biomedical signal compression with time- and subject-adaptive dictionary for wearable devices, in 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP), 2016, pp. 1 6. [29] J. Qian, P. Tiwari, S. P. Gochhayat, and H. M. Pandey, A Noble Double- Dictionary-Based ECG Compression Technique for IoTH, IEEE Inter- net of Things Journal, vol. 7, no. 10, pp. 10 160 10 170, 2020. [30] U. Bihr, H. Xu, C. Bulach, M. Lorenz, J. Anders, and M. Ortmanns, Real-time data compression of neural spikes, in 2014 IEEE 12th International New Circuits and Systems Conference (NEWCAS), 2014, pp. 436 439. [31] O. W. Savolainen and T. G. Constandinou, Lossless Compression of Intracortical Extracellular Neural Recordings using Non-Adaptive Huffman Encoding, in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine Biology Society (EMBC), 2020, pp. 4318 4321. [32] M. Shaeri and A. M. Sodagar, A framework for on-implant spike sorting based on salient feature selection, Nature Communications, vol. 11, no. 1, p. 3278, Jun 2020. [Online].\n\n--- Segment 51 ---\n1, p. 3278, Jun 2020. [Online]. Available: https: doi.org 10.1038 s41467-020-17031-9 [33] Y. Chen, B. Tacca, Y. Chen, D. Biswas, G. Gielen, F. Catthoor, M. Verhelst, and C. Mora Lopez, An Online-Spike-Sorting IC Using Unsupervised Geometry-Aware OSort Clustering for Efficient Embedded Neural-Signal Processing, IEEE Journal of Solid-State Circuits, vol. 58, no. 11, pp. 2990 3002, 2023. [34] M. A. Shaeri and A. M. Sodagar, A Method for Compression of Intra-Cortically-Recorded Neural Signals Dedicated to Implantable Brain Machine Interfaces, IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 23, no. 3, pp. 485 497, 2015. [35] V. Mohan, W. P. Tay, and A. Basu, Architectural Exploration of Neu- romorphic Compression-based Neural Sensing for Next-Gen Wireless implantable-BMI, in 2023 IEEE International Symposium on Circuits and Systems (ISCAS), 2023, pp. 1 5. xiv [36] M. Shaeri and A. M. Sodagar, Data Transformation in the Processing of Neuronal Signals: A Powerful Tool to Illuminate Informative Contents, IEEE Reviews in Biomedical Engineering, vol. 16, pp. 611 626, 2023. [37] J. W. Salatino, K. A. Ludwig, T. D. Y. Kozai, and E. K. Purcell, Glial responses to implanted electrodes in the brain, Nature Biomedical Engineering, vol. 2, no. 1, pp. 52 52, Jan 2018. [Online]. Available: [38] S. Musallam, B. D. Corneil, B. Greger, H. Scherberger, and R. A. Andersen, Cognitive Control Signals for Neural Prosthetics, Science, vol. 305, no. 5681, pp. 258 262, 2004. [Online].\n\n--- Segment 52 ---\n258 262, 2004. [Online]. Available: [39] T. Aflalo, S. Kellis, C. Klaes, B. Lee, Y. Shi, K. Pejsa, K. Shanfield, S. Hayes-Jackson, M. Aisen, C. Heck, C. Liu, and R. A. Andersen, Decoding motor imagery from the posterior parietal cortex of a tetraplegic human, Science, vol. 348, no. 6237, pp. 906 910, 2015. [Online]. Available: aaa5417 [40] P. Baldi, Autoencoders, unsupervised learning and deep architectures, in Proceedings of the 2011 International Conference on Unsuper- vised and Transfer Learning Workshop - Volume 27, ser. UTLW 11. JMLR.org, 2011, p. 37 50. [41] J. Zhai, S. Zhang, J. Chen, and Q. He, Autoencoder and Its Various Variants, in 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE Press, 2018, p. 415 419. [Online]. Available: [42] P. Li, Y. Pei, and J. Li, A comprehensive survey on design and application of autoencoder in deep learning, Applied Soft Computing, vol. 138, p. 110176, 2023. [Online]. Available: https: www.sciencedirect.com science article pii S1568494623001941 [43] D. Bank, N. Koenigstein, and R. Giryes, Autoencoders. Cham: Springer International Publishing, 2023, pp. 353 374. [Online]. Available: [44] A. Krishna, S. Rohit Nudurupati, D. G. Chandana, P. Dwivedi, A. van Schaik, M. Mehendale, and C. S. Thakur, RAMAN: A Reconfigurable and Sparse tinyML Accelerator for Inference on Edge, IEEE Internet of Things Journal, vol. 11, no. 14, pp. 24 831 24 845, 2024.\n\n--- Segment 53 ---\n14, pp. 24 831 24 845, 2024. [45] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ImageNet Classification with Deep Convolutional Neural Networks, in Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 1, ser. NIPS 12. Red Hook, NY, USA: Curran Associates Inc., 2012, p. 1097 1105. [46] A. Krishna, H. P. Oleti, A. Chauhan, H. Shankaranarayanan, A. van Schaik, M. Mehendale, and C. Singh Thakur, Live Demonstration: Audio Inference using Neuromorphic Cochlea on RAMAN Accelerator, in 2023 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2023, pp. 1 1. [47] A. Krishna, A. Rajesh, H. P. Oleti, A. Chauhan, S. H, A. Van Schaik, M. Mehendale, and C. S. Thakur, Live Demonstration: Real-time audio and visual inference on the RAMAN TinyML accelerator, in 2024 IEEE International Symposium on Circuits and Systems (ISCAS), 2024, pp. 1 1. [48] G. Zhang, N. Attaluri, J. S. Emer, and D. Sanchez, Gamma: Leveraging Gustavson s Algorithm to Accelerate Sparse Matrix Multiplication, in Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS 2021. New York, NY, USA: Association for Computing Machinery, 2021, p. 687 701. [Online]. Available: [49] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, arXiv, 2017.\n\n--- Segment 54 ---\n[Online]. Available: [49] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications, arXiv, 2017. [50] F. Karimzadeh, N. Cao, B. Crafton, J. Romberg, and A. Raychowdhury, A Hardware-Friendly Approach Towards Sparse Neural Networks Based on LFSR-Generated Pseudo-Random Sequences, IEEE Trans- actions on Circuits and Systems I: Regular Papers, vol. 68, no. 2, pp. 751 764, 2021. [51] C. Serrano-Amenos, P. Heydari, C. Y. Liu, A. H. Do, and Z. Nenadic, Power Budget of a Skull Unit in a Fully-Implantable Brain-Computer Interface: Bio-Heat Model, IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 31, pp. 4029 4039, 2023. [52] Efinix, TI60 Data Sheet, Online, 2023. [Online]. Available: [53] T. Brochier, L. Zehl, Y. Hao, M. Duret, J. Sprenger, M. Denker, S. Grün, and A. Riehle, Massively parallel multi-electrode recordings of macaque motor cortex during an instructed delayed reach-to-grasp task, 2017. [Online]. Available: [54] D. Valencia, P. P. Mercier, and A. Alimohammad, Efficient In Vivo Neural Signal Compression Using an Autoencoder-based Neural Net- work, IEEE Transactions on Biomedical Circuits and Systems, pp. 1 12, 2024. [55] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\n\n--- Segment 55 ---\n1 12, 2024. [55] B. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko, Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [56] Z. Yao, Z. Dong, Z. Zheng, A. Gholami, J. Yu, E. Tan, L. Wang, Q. Huang, Y. Wang, M. W. Mahoney, and K. Keutzer, HAWQV3: Dyadic Neural Network Quantization, in ICML, 2021. [57] L. N. Smith and N. Topin, Super-convergence: Very fast training of neural networks using large learning rates, in Artificial intelligence and machine learning for multi-domain operations applications, vol. 11006. SPIE, 2019, pp. 369 386. [58] N. Li, M. Osborn, G. Wang, and M. Sawan, A digital multichannel neural signal processing system using compressed sensing, Digital Signal Processing, vol. 55, pp. 64 77, 2016. [Online]. Available: [59] X. Liu, M. Zhang, T. Xiong, A. G. Richardson, T. H. Lucas, P. S. Chin, R. Etienne-Cummings, T. D. Tran, and J. Van der Spiegel, A Fully Integrated Wireless Compressed Sensing Neural Signal Acquisition System for Chronic Recording and Brain Machine Interface, IEEE Transactions on Biomedical Circuits and Systems, vol. 10, no. 4, pp. 874 883, 2016. [60] S.-Y. Park, J. Cho, K. Lee, and E. Yoon, Dynamic Power Reduction in Scalable Neural Recording Interface Using Spatiotemporal Correlation and Temporal Sparsity of Neural Signals, IEEE Journal of Solid-State Circuits, vol. 53, no. 4, pp. 1102 1114, 2018. [61] Y. Khazaei, A.\n\n--- Segment 56 ---\n1102 1114, 2018. [61] Y. Khazaei, A. A. Shahkooh, and A. M. Sodagar, Spatial Redundancy Reduction in Multi-Channel Implantable Neural Recording Microsys- tems, in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine Biology Society (EMBC), 2020, pp. 898 901. [62] G. Gagnon-Turcotte, Y. LeChasseur, C. Bories, Y. De Koninck, and B. Gosselin, A wireless optogenetic headstage with multichannel neural signal compression, in 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2015, pp. 1 4. [63] R. R. Shrivastwa, V. Pudi, and A. Chattopadhyay, An FPGA-Based Brain Computer Interfacing Using Compressive Sensing and Machine Learning, in 2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), 2018, pp. 726 731. [64] M. Nekoui and A. M. Sodagar, Spike Compression through Selec- tive Downsampling and Piecewise Curve Fitting Dedicated to Neural Recording Brain Implants, in 2022 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2022, pp. 50 54. [65] P. Lichtsteiner, C. Posch, and T. Delbruck, A 128 128 120 dB 15 µs Latency Asynchronous Temporal Contrast Vision Sensor, IEEE Journal of Solid-State Circuits, vol. 43, no. 2, pp. 566 576, 2008.\n\n