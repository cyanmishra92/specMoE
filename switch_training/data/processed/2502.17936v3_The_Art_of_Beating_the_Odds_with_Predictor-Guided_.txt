=== ORIGINAL PDF: 2502.17936v3_The_Art_of_Beating_the_Odds_with_Predictor-Guided_.pdf ===\n\nRaw text length: 12654 characters\nCleaned text length: 12595 characters\nNumber of segments: 8\n\n=== CLEANED TEXT ===\n\nThe Art of Beating the Odds with Predictor-Guided Random Design Space Exploration Felix Arnold Huawei, Switzerland Maxence Bouvier Huawei, Switzerland Ryan Amaudruz Huawei, Switzerland Renzo Andri Huawei, Switzerland Lukas Cavigelli Huawei, Switzerland Abstract This work introduces an innovative method for improving combinational digital circuits through random exploration in MIG-based synthesis. High-quality circuits are crucial for performance, power, and cost, making this a critical area of active research. Our approach incorporates next-state prediction and iterative selection, significantly accelerating the synthesis process. This novel method achieves up to 14 synthesis speedup and up to 20.94 better MIG minimization on the EPFL Combinational Benchmark Suite compared to state-of-the-art techniques. We further explore various predictor models and show that increased prediction accuracy does not guarantee an equivalent increase in synthesis quality of results or speedup, observing that randomness remains a desirable factor. Index Terms Logic High-level Synthesis, AI and Machine Learning I. INTRODUCTION In recent years, substantial efforts have been made to transition the synthesis of combinational circuits from proprietary to open-source EDA tools. Still, these tools lag behind commercial alternatives in terms of area, power, timing particularly for well-known arithmetic blocks. All-in-one tools, such as YOSYS [1], often perform minimal combinational module minimization, while more specialized software like ABC [2] and Mockturtle (MT) [3] require high expertise to configure synthesis scripts and algorithms. S.-Y. Lee et al. [4] recently introduced a random-search-based Design Space Exploration (DSE) framework for Major-Inverter-Gate (MIG) minimization, achieving remarkable improvements over the State-of-The-Art (SoTA). How- ever, random search typically involves numerous trial-and-error iter- ations to find an effective algorithmic sequence. F. Faez et al. [5] have explored prediction-based synthesis and demonstrated Quality of Results (QoR) improvements and speedups, but they did not report the absolute minimal circuit size achieved. We propose a method built upon the framework of [4] that accelerates the DSE by incorporating next-state prediction and iterative selection. II. METHOD The original system [4] decomposes the full optimization process into a sequence of actions (steps) performed by the Mockturtle (MT) software [3]. At each step, one of thirty possible recipes, drawn from six distinct synthesis scripts with varying parameters, is randomly selected and applied to the circuit. Our method introduces three mod- ules that collaborate to optimize the synthesis QoR. The Prediction Module (PrM) and Policy Module (PoM) optimize the sequence of recipes (see Fig. 1b) to achieve faster circuit minimization. The Inter- Iteration Selection Module (IISM) periodically restarts the synthesis from the best circuit found so far (Fig. 1a). Recipe Selection Process: A recipe is an atomic action that the MT tool performs during a unitary step. The PrM predicts the resulting circuit size after applying all possible recipes to the current circuit. Based on the predictions, the PoM selects the recipe that MT should apply next. In this work, we evaluate several PrM models, including statistical and decoder-based causal auto-regressive Transformer [6]. In the PoM, the predicted metrics associated with Felix Arnold and Maxence Bouvier are co-first authors. each recipe are passed through a SoftMax function with temperature to obtain probabilities. These probabilities are used to sample the next action, as illustrated in Fig. 1c. Data Collection: To train the predictor models, we collect data by applying uniformly sampled recipes for a certain number of steps. At each step, we record the recipe index along with the resulting size-related metrics (LUT6, transistor, and MIG node counts, etc.). Inter-Iteration Selection Process: After a fixed number of steps, the IISM selects the best circuit. All chains are restarted from the selected design, initiating a new iteration. Parallel chains start from the same circuit and proceed with an individual sequence of recipes. III. EXPERIMENTS In a first step, we choose to optimize the ALU control unit (control.v) from the EPFL Benchmark [7], as its small size allowed for fast iteration. In this section, the target metric used is the number of transistors, as reported by YOSYS [1] after a technology independent abc step. Data collection is done with 500 runs, each with a single chain of 5000 steps. Unless otherwise stated, for all data points of each experiment presented below, we perform 200 runs, each with 1000 steps across iterations and chains. The real compute time can differ, as the PrM and recipe execution durations can vary. A single MT script takes 0.065 s on average (0.27 s including metrics extraction) on a 32-core 2.8 GHz x86 64 server CPU. Prediction Accuracy: We investigate several prediction models: 1) The statistical model predicts transistor variations based on the mean change observed for each recipe in the training set; 2) Other models incorporate metrics derived from previous circuits, with context lengths ranging up to 20 past states for the transformer-based model. Table I summarizes the Root Mean Square Error (RMSE) achieved on a validation set by different models across various configurations (lower is better). We note that higher prediction accuracy does not necessarily lead to higher QoR. TABLE I: RMSE of the PrM, speedup and minimum size achieved. Configuration RMSE Min. Trans. Uniform (Baseline) - 1.00 354 Statistical, 1SA 19.4 5.48 344 MLP, 1SA 17.5 5.71 348 Statistical, 2SA 23.6 9.60 346 Transformer, 2SA 18.0 9.00 340 Statistical, 1SA IISM 19.2 10.60 340 Temperature Effect: Higher SoftMax temperature results in a more uniform distribution during sampling, whereas lower temper- atures favor the predicted top-performing recipes. To investigate the impact of the parameter on the QoR we perform a sweep (see Fig. 2a). We see that there is an optimum around T 5, where significantly better results can be achieved compared to the uniform distribution (baseline). This suggests selecting top-performing recipes improves QoR, but keeping some degree of randomness is essential. arXiv:2502.17936v3 [cs.LG] 8 Apr 2025 ITERATION i 1 ITERATION i-1 ITERATION i IISM None Keep Best CHAIN STEP STEP ... ... CHAIN STEP STEP ... ... CHAIN STEP STEP ... ... CHAIN STEP STEP ... ... CHAIN STEP STEP ... ... (a) STEP Output Circuit POLICY MODULE SoftMax w. Temperature PREDICTION MODULE Model (b) None Statistical Transformer BDT PREDICTIONS PAST CONTEXT Past Metrics Past Recipes PREDICTION MODULE POLICY MODULE FUTURE RECIPES SELECTED RECIPE(S) r0,0 r0,1 r1,0 r1,1 rn,0 rn,1 m0,0 m0,1 m1,0 m1,1 mn,0 mn,1 ri,0 ri,1 Model (c) Input Circuit MOCKTURTLE SOFTWARE[3] Sampling Selected Recipe(s) Prob. 1SA 2SA Pred. Fig. 1: Overview of the proposed prediction-based DSE framework. (a) Multiple parallel chains and IISM. (b) Unitary steps with recipe selection and MT recipe processing. (c) Detailed view of recipe performance prediction and recipe selection by the PrM and PoM. Fig. 2: Effect of the temperature on the speedup factor for various target values for ctrl.v. Statistical model. a) 1SA, b) 2SA. 1 5 10 20 Parallel Chains 10 20 50 100 500 1000 Chain Length 336 338 340 338 344 340 334 338 342 336 340 336 340 336 340 340 346 340 348 346 350 344 348 352 Minimium Trans. Count 1 5 10 20 Parallel Chains 10 20 50 100 500 1000 415.5 413.6 414.1 412.1 420.6 418.1 418.2 418.5 424.8 423.4 423.2 423.6 427.1 426.1 425.9 425.8 430.0 430.0 430.1 429.9 430.7 430.7 430.6 430.6 Mean Trans. Count 1 5 10 20 Parallel Chains 10 20 50 100 500 1000 414 414 414 414 418 418 418 418 422 422 422 422 424 424 424 424 426 426 426 426 428 428 428 428 Median Trans. Count Fig. 3: Effect of chain length and number of parallel chains on achieved minimum, mean and median transistor counts. Speedup: We define synthesis speed as the average number of runs required to reach a target QoR. Speedup is the ratio of a method s speed to the baseline speed. Fig. 2a shows that by using the statistical model, equivalent QoR is achieved in fewer runs on average. This speedup is more pronounced for lower target values (higher QoR). Table I shows the speedup reached at a target transistor count of 365. Furthermore, without IISM, circuits with the lowest transistor count could only be obtained with the prediction model approach. Two-Step Ahead Predictions (2SA): We further extend our method to 2SA predictions, where the PrM and PoM select the pair of recipes for the two next steps. Table I and Fig. 2 show that 2SA achieves drastically better speedups than 1SA. For a target transistor count of 360, 2SA prediction reaches up to 14 speedup. IISM Effect: We compare 1SA prediction with and without IISM in Table I. With chain length 50 and 1 parallel chain, we observe the speedup increases from 5.48 to 10.60. We also perform a grid search over the number of parallel chains and chain length with uniform sampling (see Fig. 3). For this experiment, we keep the total number of steps per configuration constant by varying the number of runs. We find that an increase in parallel chains is beneficial, which has been verified for different benchmark designs. However, the optimal chain length varies widely across designs. Also, while a configuration maximizing the QoR across many steps usually achieves high-quality, it does not always yield the absolute optimum (338 transistors count vs. absolute minimum of 334 in Fig. 3). IV. RESULTS In the following, we deploy the EPFL Combinational Benchmark [7]. Starting with the original Verilog implementation, we apply our method targeting MIG nodes minimization, and Table II presents the resulting minimal circuit. We demonstrate an improvement of up to 20.94 in MIG minimization on the max.v circuit. As the duration of each step grows dramatically with the design size (from 5 ms to 21 min for some recipes), we have not yet applied the full method on larger designs. And-Inverter Graph (AIG) node counts of the MIG optimized circuits are added for reference, but have not been optimized for. The synthesized designs are available on GitHub. TABLE II: Best QoR achieved on the EPFL Benchmark [7] Bench. MIG Nodes Depth AIG Nodes [8] [4] Ours Ours [9] Ours adder 384 384 384 129 384 bar 2433 1906 1800 15 2696 div 12462 12368 12434 2316 19250 23012 hyp 115541 115539 131487 8991 209460 235073 log2 22010 22008 23291 217 30522 30247 max 2190 1939 1533 260 2676 multiplier 17112 17112 18515 141 25371 26381 sin 3870 3869 3842 118 4987 5102 sqrt 12357 12247 19430 5722 19706 19020 square 8138 8089 8132 128 17010 17838 arbiter 6711 792 685 36 879 783 cavlc 492 374 329 15 483 415 ctrl 74 60 58 9 71 dec 304 304 304 3 304 i2c 871 636 618 21 710 755 int2float 172 115 107 14 145 mem ctrl 32097 6886 6250 36 7644 7102 priority 406 337 315 14 375 router 147 97 92 14 96 95 voter 4555 3894 3849 41 9817 8854 V. CONCLUSION The proposed method shows a 14 synthesis speedup and up to 20.94 better MIG minimization through the integration of predictive models and iterative selection strategies. Hyperparameter optimization remains crucial, as the best configuration varies among circuits. Since a considerable number of steps are required to achieve the best QoR, minimization of large designs remains challenging. We believe that further QoR improvement can be achieved with reinforcement learning methods. As the PrM, PoM, and IISM are originally independent of the MT software, our method could be deployed with other tools. REFERENCES [1] C. Wolf, Yosys Open SYnthesis Suite. Online. [2] R. Brayton and A. Mishchenko, ABC: An Academic Industrial-Strength Verification Tool, in Computer Aided Verification, 2010. doi. [3] H. Riener et al., Scalable Generic Logic Synthesis: One Approach to Rule Them All, in DAC, 2019. doi. [4] S.-Y. Lee et al., Late Breaking Results: Majority-Inverter Graph Mini- mization by Design Space Exploration, in DAC, 2024. doi. [5] F. Faez et al., MTLSO: A Multi-Task Learning Approach for Logic Synthesis Optimization, in Proceedings of the ASPDAC, 2024. [6] A. Vaswani et al., Attention Is All You Need, 2023 (v7). arXiv. [7] Luca Amar u, et al., The EPFL combinational benchmark suite, in Proceedings of the IWLS, 2015. [8] A. Tempia Calvino et al., Scalable Logic Rewriting Using Don t Cares, in DATE, 2024. [9] E. Testa et al., Scalable Boolean Methods in a Modern Synthesis Flow, in DATE, 2019.\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nThe Art of Beating the Odds with Predictor-Guided Random Design Space Exploration Felix Arnold Huawei, Switzerland Maxence Bouvier Huawei, Switzerland Ryan Amaudruz Huawei, Switzerland Renzo Andri Huawei, Switzerland Lukas Cavigelli Huawei, Switzerland Abstract This work introduces an innovative method for improving combinational digital circuits through random exploration in MIG-based synthesis. High-quality circuits are crucial for performance, power, and cost, making this a critical area of active research. Our approach incorporates next-state prediction and iterative selection, significantly accelerating the synthesis process. This novel method achieves up to 14 synthesis speedup and up to 20.94 better MIG minimization on the EPFL Combinational Benchmark Suite compared to state-of-the-art techniques. We further explore various predictor models and show that increased prediction accuracy does not guarantee an equivalent increase in synthesis quality of results or speedup, observing that randomness remains a desirable factor. Index Terms Logic High-level Synthesis, AI and Machine Learning I. INTRODUCTION In recent years, substantial efforts have been made to transition the synthesis of combinational circuits from proprietary to open-source EDA tools. Still, these tools lag behind commercial alternatives in terms of area, power, timing particularly for well-known arithmetic blocks. All-in-one tools, such as YOSYS [1], often perform minimal combinational module minimization, while more specialized software like ABC [2] and Mockturtle (MT) [3] require high expertise to configure synthesis scripts and algorithms. S.-Y. Lee et al. [4] recently introduced a random-search-based Design Space Exploration (DSE) framework for Major-Inverter-Gate (MIG) minimization, achieving remarkable improvements over the State-of-The-Art (SoTA). How- ever, random search typically involves numerous trial-and-error iter- ations to find an effective algorithmic sequence. F. Faez et al. [5] have explored prediction-based synthesis and demonstrated Quality of Results (QoR) improvements and speedups, but they did not report the absolute minimal circuit size achieved. We propose a method built upon the framework of [4] that accelerates the DSE by incorporating next-state prediction and iterative selection. II.\n\n--- Segment 2 ---\nWe propose a method built upon the framework of [4] that accelerates the DSE by incorporating next-state prediction and iterative selection. II. METHOD The original system [4] decomposes the full optimization process into a sequence of actions (steps) performed by the Mockturtle (MT) software [3]. At each step, one of thirty possible recipes, drawn from six distinct synthesis scripts with varying parameters, is randomly selected and applied to the circuit. Our method introduces three mod- ules that collaborate to optimize the synthesis QoR. The Prediction Module (PrM) and Policy Module (PoM) optimize the sequence of recipes (see Fig. 1b) to achieve faster circuit minimization. The Inter- Iteration Selection Module (IISM) periodically restarts the synthesis from the best circuit found so far (Fig. 1a). Recipe Selection Process: A recipe is an atomic action that the MT tool performs during a unitary step. The PrM predicts the resulting circuit size after applying all possible recipes to the current circuit. Based on the predictions, the PoM selects the recipe that MT should apply next. In this work, we evaluate several PrM models, including statistical and decoder-based causal auto-regressive Transformer [6]. In the PoM, the predicted metrics associated with Felix Arnold and Maxence Bouvier are co-first authors. each recipe are passed through a SoftMax function with temperature to obtain probabilities. These probabilities are used to sample the next action, as illustrated in Fig. 1c. Data Collection: To train the predictor models, we collect data by applying uniformly sampled recipes for a certain number of steps. At each step, we record the recipe index along with the resulting size-related metrics (LUT6, transistor, and MIG node counts, etc.). Inter-Iteration Selection Process: After a fixed number of steps, the IISM selects the best circuit. All chains are restarted from the selected design, initiating a new iteration. Parallel chains start from the same circuit and proceed with an individual sequence of recipes. III. EXPERIMENTS In a first step, we choose to optimize the ALU control unit (control.v) from the EPFL Benchmark [7], as its small size allowed for fast iteration. In this section, the target metric used is the number of transistors, as reported by YOSYS [1] after a technology independent abc step.\n\n--- Segment 3 ---\nEXPERIMENTS In a first step, we choose to optimize the ALU control unit (control.v) from the EPFL Benchmark [7], as its small size allowed for fast iteration. In this section, the target metric used is the number of transistors, as reported by YOSYS [1] after a technology independent abc step. Data collection is done with 500 runs, each with a single chain of 5000 steps. Unless otherwise stated, for all data points of each experiment presented below, we perform 200 runs, each with 1000 steps across iterations and chains. The real compute time can differ, as the PrM and recipe execution durations can vary. A single MT script takes 0.065 s on average (0.27 s including metrics extraction) on a 32-core 2.8 GHz x86 64 server CPU. Prediction Accuracy: We investigate several prediction models: 1) The statistical model predicts transistor variations based on the mean change observed for each recipe in the training set; 2) Other models incorporate metrics derived from previous circuits, with context lengths ranging up to 20 past states for the transformer-based model. Table I summarizes the Root Mean Square Error (RMSE) achieved on a validation set by different models across various configurations (lower is better). We note that higher prediction accuracy does not necessarily lead to higher QoR. TABLE I: RMSE of the PrM, speedup and minimum size achieved. Configuration RMSE Min. Trans. Uniform (Baseline) - 1.00 354 Statistical, 1SA 19.4 5.48 344 MLP, 1SA 17.5 5.71 348 Statistical, 2SA 23.6 9.60 346 Transformer, 2SA 18.0 9.00 340 Statistical, 1SA IISM 19.2 10.60 340 Temperature Effect: Higher SoftMax temperature results in a more uniform distribution during sampling, whereas lower temper- atures favor the predicted top-performing recipes. To investigate the impact of the parameter on the QoR we perform a sweep (see Fig. 2a). We see that there is an optimum around T 5, where significantly better results can be achieved compared to the uniform distribution (baseline). This suggests selecting top-performing recipes improves QoR, but keeping some degree of randomness is essential.\n\n--- Segment 4 ---\nWe see that there is an optimum around T 5, where significantly better results can be achieved compared to the uniform distribution (baseline). This suggests selecting top-performing recipes improves QoR, but keeping some degree of randomness is essential. arXiv:2502.17936v3 [cs.LG] 8 Apr 2025 ITERATION i 1 ITERATION i-1 ITERATION i IISM None Keep Best CHAIN STEP STEP ... ... CHAIN STEP STEP ... ... CHAIN STEP STEP ... ... CHAIN STEP STEP ... ... CHAIN STEP STEP ... ... (a) STEP Output Circuit POLICY MODULE SoftMax w. Temperature PREDICTION MODULE Model (b) None Statistical Transformer BDT PREDICTIONS PAST CONTEXT Past Metrics Past Recipes PREDICTION MODULE POLICY MODULE FUTURE RECIPES SELECTED RECIPE(S) r0,0 r0,1 r1,0 r1,1 rn,0 rn,1 m0,0 m0,1 m1,0 m1,1 mn,0 mn,1 ri,0 ri,1 Model (c) Input Circuit MOCKTURTLE SOFTWARE[3] Sampling Selected Recipe(s) Prob. 1SA 2SA Pred. Fig. 1: Overview of the proposed prediction-based DSE framework. (a) Multiple parallel chains and IISM. (b) Unitary steps with recipe selection and MT recipe processing. (c) Detailed view of recipe performance prediction and recipe selection by the PrM and PoM. Fig. 2: Effect of the temperature on the speedup factor for various target values for ctrl.v. Statistical model. a) 1SA, b) 2SA. 1 5 10 20 Parallel Chains 10 20 50 100 500 1000 Chain Length 336 338 340 338 344 340 334 338 342 336 340 336 340 336 340 340 346 340 348 346 350 344 348 352 Minimium Trans. Count 1 5 10 20 Parallel Chains 10 20 50 100 500 1000 415.5 413.6 414.1 412.1 420.6 418.1 418.2 418.5 424.8 423.4 423.2 423.6 427.1 426.1 425.9 425.8 430.0 430.0 430.1 429.9 430.7 430.7 430.6 430.6 Mean Trans.\n\n--- Segment 5 ---\n1 5 10 20 Parallel Chains 10 20 50 100 500 1000 Chain Length 336 338 340 338 344 340 334 338 342 336 340 336 340 336 340 340 346 340 348 346 350 344 348 352 Minimium Trans. Count 1 5 10 20 Parallel Chains 10 20 50 100 500 1000 415.5 413.6 414.1 412.1 420.6 418.1 418.2 418.5 424.8 423.4 423.2 423.6 427.1 426.1 425.9 425.8 430.0 430.0 430.1 429.9 430.7 430.7 430.6 430.6 Mean Trans. Count 1 5 10 20 Parallel Chains 10 20 50 100 500 1000 414 414 414 414 418 418 418 418 422 422 422 422 424 424 424 424 426 426 426 426 428 428 428 428 Median Trans. Count Fig. 3: Effect of chain length and number of parallel chains on achieved minimum, mean and median transistor counts. Speedup: We define synthesis speed as the average number of runs required to reach a target QoR. Speedup is the ratio of a method s speed to the baseline speed. Fig. 2a shows that by using the statistical model, equivalent QoR is achieved in fewer runs on average. This speedup is more pronounced for lower target values (higher QoR). Table I shows the speedup reached at a target transistor count of 365. Furthermore, without IISM, circuits with the lowest transistor count could only be obtained with the prediction model approach. Two-Step Ahead Predictions (2SA): We further extend our method to 2SA predictions, where the PrM and PoM select the pair of recipes for the two next steps. Table I and Fig. 2 show that 2SA achieves drastically better speedups than 1SA. For a target transistor count of 360, 2SA prediction reaches up to 14 speedup. IISM Effect: We compare 1SA prediction with and without IISM in Table I. With chain length 50 and 1 parallel chain, we observe the speedup increases from 5.48 to 10.60. We also perform a grid search over the number of parallel chains and chain length with uniform sampling (see Fig. 3). For this experiment, we keep the total number of steps per configuration constant by varying the number of runs. We find that an increase in parallel chains is beneficial, which has been verified for different benchmark designs. However, the optimal chain length varies widely across designs.\n\n--- Segment 6 ---\nWe find that an increase in parallel chains is beneficial, which has been verified for different benchmark designs. However, the optimal chain length varies widely across designs. Also, while a configuration maximizing the QoR across many steps usually achieves high-quality, it does not always yield the absolute optimum (338 transistors count vs. absolute minimum of 334 in Fig. 3). IV. RESULTS In the following, we deploy the EPFL Combinational Benchmark [7]. Starting with the original Verilog implementation, we apply our method targeting MIG nodes minimization, and Table II presents the resulting minimal circuit. We demonstrate an improvement of up to 20.94 in MIG minimization on the max.v circuit. As the duration of each step grows dramatically with the design size (from 5 ms to 21 min for some recipes), we have not yet applied the full method on larger designs. And-Inverter Graph (AIG) node counts of the MIG optimized circuits are added for reference, but have not been optimized for. The synthesized designs are available on GitHub. TABLE II: Best QoR achieved on the EPFL Benchmark [7] Bench.\n\n--- Segment 7 ---\nThe synthesized designs are available on GitHub. TABLE II: Best QoR achieved on the EPFL Benchmark [7] Bench. MIG Nodes Depth AIG Nodes [8] [4] Ours Ours [9] Ours adder 384 384 384 129 384 bar 2433 1906 1800 15 2696 div 12462 12368 12434 2316 19250 23012 hyp 115541 115539 131487 8991 209460 235073 log2 22010 22008 23291 217 30522 30247 max 2190 1939 1533 260 2676 multiplier 17112 17112 18515 141 25371 26381 sin 3870 3869 3842 118 4987 5102 sqrt 12357 12247 19430 5722 19706 19020 square 8138 8089 8132 128 17010 17838 arbiter 6711 792 685 36 879 783 cavlc 492 374 329 15 483 415 ctrl 74 60 58 9 71 dec 304 304 304 3 304 i2c 871 636 618 21 710 755 int2float 172 115 107 14 145 mem ctrl 32097 6886 6250 36 7644 7102 priority 406 337 315 14 375 router 147 97 92 14 96 95 voter 4555 3894 3849 41 9817 8854 V. CONCLUSION The proposed method shows a 14 synthesis speedup and up to 20.94 better MIG minimization through the integration of predictive models and iterative selection strategies. Hyperparameter optimization remains crucial, as the best configuration varies among circuits. Since a considerable number of steps are required to achieve the best QoR, minimization of large designs remains challenging. We believe that further QoR improvement can be achieved with reinforcement learning methods. As the PrM, PoM, and IISM are originally independent of the MT software, our method could be deployed with other tools. REFERENCES [1] C. Wolf, Yosys Open SYnthesis Suite. Online. [2] R. Brayton and A. Mishchenko, ABC: An Academic Industrial-Strength Verification Tool, in Computer Aided Verification, 2010. doi. [3] H. Riener et al., Scalable Generic Logic Synthesis: One Approach to Rule Them All, in DAC, 2019. doi. [4] S.-Y.\n\n--- Segment 8 ---\n[3] H. Riener et al., Scalable Generic Logic Synthesis: One Approach to Rule Them All, in DAC, 2019. doi. [4] S.-Y. Lee et al., Late Breaking Results: Majority-Inverter Graph Mini- mization by Design Space Exploration, in DAC, 2024. doi. [5] F. Faez et al., MTLSO: A Multi-Task Learning Approach for Logic Synthesis Optimization, in Proceedings of the ASPDAC, 2024. [6] A. Vaswani et al., Attention Is All You Need, 2023 (v7). arXiv. [7] Luca Amar u, et al., The EPFL combinational benchmark suite, in Proceedings of the IWLS, 2015. [8] A. Tempia Calvino et al., Scalable Logic Rewriting Using Don t Cares, in DATE, 2024. [9] E. Testa et al., Scalable Boolean Methods in a Modern Synthesis Flow, in DATE, 2019.\n\n