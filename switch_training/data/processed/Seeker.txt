=== ORIGINAL PDF: Seeker.pdf ===\n\nRaw text length: 84097 characters\nCleaned text length: 83354 characters\nNumber of segments: 50\n\n=== CLEANED TEXT ===\n\nSynergistic and Efficient Edge-Host Communication for Energy Harvesting Wireless Sensor Networks Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandmeir, Vijaykrishnan Narayanan, Chita R Das {cyan, jms1257, mtk2, vijaykrishnan.narayanan, The Pennsylvania State University ABSTRACT There is an increasing demand for intelligent processing on ultra-low-power internet of things (IoT) device. Recent works have shown substantial efficiency boosts by execut- ing inferences directly on the IoT device (node) rather than transmitting data. However, the computation and power de- mands of Deep Neural Network (DNN)-based inference pose significant challenges in an energy-harvesting wireless sen- sor network (EH-WSN). Moreover, these tasks often require responses from multiple physically distributed EH sensor nodes, which impose crucial system optimization challenges in addition to per-node constraints. To address these chal- lenges, we propose Seeker, a hardware-software co-design approach for increasing on-sensor computation, reducing communication volume, and maximizing inference comple- tion, without violating the quality of service, in EH-WSNs co- ordinated by a mobile device. Seeker uses a store-and-execute approach to complete a subset of inferences on the EH sensor node, reducing communication with the mobile host. Fur- ther, for those inferences unfinished because of the harvested energy constraints, it leverages task-aware coreset construc- tion to efficiently communicate compact features to the host device. We evaluate Seeker for human activity recognition, as well as predictive maintenance and show 8.9 reduc- tion in communication data volume with 86.8 accuracy, surpassing the 81.2 accuracy of the state-of-the-art. 1 INTRODUCTION Innovations in low-power computing, artificial intelligence, and communication technologies have given rise to the gen- eration of intelligently connected devices that constitute the Internet of Things (IoT). Wireless sensor networks (WSNs), one of the prominent classes of IoT deployments, is currently dominating and expected to be pervasive impacting many application spaces [13] including, but not limited to, body area network [22, 47], industrial monitoring [34], predic- tive maintenance [70], commercial satellites[15] and smart farming [61]. Moreover, these WSNs are and further will be participating in producing rapid inferences to support the increasingly complex tasks enabled by machine learning (ML) algorithms [47, 68], often tweaked towards edge de- ployments, and applications of such edge-analytics is also ex- ploding with the user and market demands. This represents a particularly challenging tension between energy availability and desired functionality, because the form factor constraints of the WSNs fundamentally limit active power, energy re- serves, compute and communication capabilities. For many WSNs, their participation in inference tasks has traditionally been limited to data collection and transmission, sometimes with modest preprocessing. While several studies have shown the benefits of performing more inference closer to the point of data collection [23, 30, 31, 40, 56] and have ap- plied these techniques to more powerful edge devices, their form-factor-imposed limited energy storage, low-power op- eration points, and deployment scenarios have been a major impediment in executing compute-intensive inference tasks directly on such platforms. In contrast, communicating the data, often after little preprocessing, although popular, is not cheap in terms of power requirement, and often poses a challenge for remotely deployed and ultra low power WSNs. Prior works, trying to tackle this conflict between computa- tion, communication, power-requirement and quality of ser- vice (QoS), have pursued three major approaches: inference effort partitioning optimizations [22, 30, 31, 50], mitigation of energy provisioning limitations [24, 40, 43, 47, 56], and minimizing communication overheads [32, 33, 36, 37, 45]. One of the most emerging line of work aims to solve the energy provisioning problem at the edge by integrating en- ergy harvesting (EH) to the sensor nodes while making them more capable performing complex compute intermittently, which has given rise to energy harvesting wireless sensor networks (EH-WSNs). Specifically, recent works [24, 43] pro- posed EH, along with compiler runtime optimizations and leveraging non-volatile processors (NVP) [40, 56], to increase local compute at the edge. EH as a solution has been partic- ularly interesting as a means to address the sustainability issue of battery backing trillions of future devices. More importantly, EH can help us build sustainable distributed sensing monitoring infrastructure at virtually inaccessible places like oil-wells, mines, and even satellite orbits [14, 49]. However, harvested energy is fickle in nature, and typically arXiv:2408.14379v1 [cs.AR] 26 Aug 2024 harvested sources only deliver scant microwatts of power (see Figure 1b for an overview). The sporadic nature of har- vested energy and the lossy nature of EH based storage and charging circuits calls for using the harvested energy di- rectly to perform intermittent compute rather than storing energy for some distant future use. On this front, recent works [43, 44, 47] have specifically optimized DNN infer- ence execution at the EH-edge nodes by utilizing adaptive dynamic check pointing, intelligent scheduling and ensem- ble learning. Given the limitations of the EH budget, such approaches typically end up dropping many samples and not inferring from them locally. Importantly, they are of- ten incapable of transmitting the raw data due to a lack of sufficient energy; for sensing tasks with modest inference requirements, performing inference and transmitting the result can take less energy than transmitting raw data. How- ever, to unleash the remote deployment, and sustainable, yet pervasive, computing capabilities WSNs, development of ef- ficient energy harvesting WSNs (EH-WSNs), both for sensing and edge-analytics, plays an essential role. These wireless sensing (EH or otherwise) devices have long relied on compression techniques to mitigate data com- munication overheads [32, 33, 45]. However, when applied to low-dimensional sensor data, classical lossy compression techniques tend to discard or distort some important fea- tures, which significantly degrades the inference accuracy. To mitigate the shortcomings of classical compression tech- niques, recent works [7, 36, 37] propose using coresets, a data representation technique from computational geometry that preserves important, representative features when building a compressed form of the data, and thereby reducing the pay- load size while preserving data integrity for efficient edge communication. Although, with the help of coresets, one can efficiently offload minimal input representations to a more compute-capable device, performing accurate inference on coresets is non-trivial due to their low-dimensional nature. From the aforementioned challenges, it is evident that we need a concoction of both hardware-driven and software optimized solutions to build next-generation EH-WSNs with the ability to perform fine-grained intermittent computing, while ensuring efficient network communication. Towards this, we propose Seeker, a novel approach that leverages and extends coresets to efficiently execute DNN inference across a set of EH sensor nodes and a host mobile device. Seeker fo- cuses on building an efficient EH-WSN which can collabora- tively work to maximize the inferences performed at the EH- edge nodes. Furthermore, it then applies innovative coreset techniques to efficiently and intelligently offload unfinished compute tasks to a more capable host to further increase the inferences that can be performed. Particularly, Seeker aug- ments its coreset formation with application-awareness to form an energy aware, dynamically configured, and feature preserving payload with minimal communication footprint. Seeker provides hardware acceleration support for coreset formation to make them computationally efficient, adaptive, and accuracy-preserving specifically for EH-WSNs. The fol- lowing are the primary contributions of our work: Efficient Communication: We enable low data volume communication by developing extensions to traditional coresets that enhances their applicability to EH-WSN in- ference scenarios. Specifically, we introduce an activity- aware coreset construction technique to dynamically adapt to both activity and the available harvested energy, while conserving maximum features of the data. This reduces the communication payload size by 8.9 . We also propose a recoverable coreset construction technique, which helps reconstruct the original data from the compressed form with minimum (as low as 0.02 ) accuracy loss. Efficient Computation: We augment a state-of-the-art EH-sensor node with quantized DNNs to increase the num- ber of accurate inferences at the edge (by up to 40 ). We leverage data memoization to skip unnecessary compute saving inference execution time and energy. Efficient Hardware: We propose simple, low power, and low latency hardware to efficiently build coresets, further increasing the number of samples that can be inferred or transmitted under EH budget, and thereby significantly improving the accuracy over the state-of-the-art ( 5 ). We develop a non-volatile hardware accelerator, with mul- tiple quantization support, for efficient DNN inference. Adaptability: Although Seeker is meant for EH-WSNs, the coreset based data representation can easily be used in any commercial device for efficient communication. Detailed Evaluation: We provide a detailed evaluation of our system and the proposed hardware design. Our evalu- ations show that, even when powered by an unreliable EH source, Seeker s coreset-based optimizations result in bet- ter accuracy than that of a fully-powered system running a state-of-the-art classifier optimized for energy efficiency. Specifically, Seeker reaches 86.8 top-1 accuracy in com- parison to the 81.2 accuracy of the baseline system. 2 BACKGROUND AND MOTIVATION In this section, we provide a background of the current state- of-the-art in performing sensing and computations on EH- WSNs. We also describe the challenges in enabling complex compute on such devices and the need for hardware-software co-design to enable specialized intermittent computing in EH-WSNs. Finally, we define the scope of our work and focus on the problem specifics while alluding to probable solutions. Figure 1a shows the basic building blocks of an energy har- vesting sensing computing unit. The harvested energy is 2 typically stored in either an intermediate storage like a (su- per) capacitor [22], or used for charging. For building scalable and sustainable infrastructure of battery-free EH-WSNs, the former is more feasible and will be our focus for this work. The fickle nature of harvested energy has posed a major chal- lenge in performing any useful computation, as any useful forward progress gets lost when the traditional computing systems lose power. To tackle this, a significant amount of work has been done on check-pointing, and compiler level tweaks, which help maximize the forward progress on such devices [39, 43, 44]. These solutions operate on augmented commercial off the shelf micro-controllers [66], or specialized products with traditional architecture [62], and rely on their efficient prediction of power failure. While these software optimizations and judicious use of persistent storage works for smaller workloads like keyword spotting (e.g "Ok Google" detection), they are inefficient for complex workloads (e.g. multi-sensor HAR, predictive maintenance etc.). These software-based solutions exhibit inefficiencies with respect to energy and time due to performing multiple save- and-restore cycles [23, 56]: while some of these operations are necessary, unnecessary checkpoints will also be conser- vatively performed to ensure forward-progress. Therefore, recent works [39 42, 56] propose the use of a NVP, where the non-volatility of the hardware itself takes care of saving and resuming the program execution. This reduces software overheads and latencies for handling power emergencies and hence can guarantee better QoS for complex and longer tasks even when power is deeply unreliable. Using an NVP and multiple harvested energy sources Qiu et al. [56] demon- strates the possibility of performing complex DNN inference at the EH-Sensor itself. While an NVP ensures safe check- pointing for a given computation, current edge scenarios may require a device to be simultaneously performing multi- ple functionalities [3 5, 25] and might be at energy scarcity. As a result, it is difficult to reliably run these complex tasks standalone on the edge device. Current devices adapt in one of three ways: 1) Send all the sensor data to a connected host device, or cloud, to offload the compute and act only as a sens- ing and display device; 2) Process data on the device itself, potentially dropping or delaying tasks due to energy short- falls; 3) A mix of the two models, where some computations do happen on the device while others are offloaded to balance compute, energy, and communication resources; and typically, the latter is preferred, but it is non-trivial to find the optimal balance between what is to be done on the edge, what to be offloaded [20, 31, 65, 69], and how to efficiently offload. Need for Specialized Hardware: One of the major chal- lenges in deploying learning tasks using EH-WSNs is to find the proper hardware platform. The current commercial- of-the-shelf (CotS) hardware capable of performing such Harvester Ambient Energy AC-DC Converter Impedance Matching DC-DC Converter Controller Power Management Conditioning Energy Storage Sensor Compute (a) High-level overview of an energy harvesting sys- tem. Communicate elsewhere 0-50 50-500 500-1k 1k-10k 10k Harvested available power in the sensor node (µW) Compute at the edge COTS high-end wearables (bat.) Bonito (multiple) Origin (RF) ResiRCA (RF Piezo Solar) Chinchilla (RF) Ideal Solution (Any) COTS cheap wearables (bat.) Seeker (RF) All compute at edge Partial compute at edge Design space of Seeker (b) Current state-of-the-art of EH-WSN. Figure 1: A primer on energy harvesting systems: Fig- ure 1a shows the basic building blocks of an EH node equipped with sensing and computation. Some of the units change according to the harvested energy source. Figure 1b shows the capabilities of the current SOTA. The size of the circle representing the solutions de- picts the compute capabilities of the sensor nodes, the shade shows the available power, and their position on the axes approximates the amount of compute done on the node and the amount of reliability on external communication. The power source is denoted in (Red) (notations used in Figure 1b: COTS: Commercial-off- the-shelf, Bat.: Battery, Bonito [22], Chinchilla [43], ResiRCA [56], Origin [47]) compute are not energy efficient to run with all modali- ties of harvested energy since all of them do not have the same energy income (see Figure 1b). For example, there has been significant work on enabling solar powered smart farm- ing [63, 67], but the same can not be done for smart manufac- turing due to the lack of solar exposure and the low fidelity of the available EH sources such as vibration and RF (from WiFi or other sources). To estimate the required energy, we ran simple HAR inferences (optimized version of [26] for edge deployment using [68]) on an Adafruit ItsyBitsy nRF52840 Express - Bluetooth LE [2] and found it to be consuming from 550mJ to 1.6J of energy (depending on the quantiza- tion). Compared to this, body movement and WiFi sources (the possible modalities of harvesting for HAR) harvests in order of milliwatts [22, 56], making it almost impossible to have a feasible EH-WSN deployment, with the capabilities to perform modest learning tasks, using the CotS. Therefore, there has been a significant body of work [40, 42, 47, 56] on developing appropriate next generation hardware (most of 3 them on simulation). Although, we evaluate and show the communication cost savings of Seeker on the battery backed CoTS hardware, we propose possible (simulated) hardware accelerator designs to fully deploying a EH-WSN capable of performing inference, compression and communication in harvested energy budget. Complex Compute on EH-WSNs: To quantify the scope performing complex compute using EH-WSNs, we took hu- man activity recognition (HAR) as a workload1, and per- formed experiments on the MHEALTH data-set [9, 10] (see Section 5 for data-set details) using the DNNs proposed in [26, 60], an energy harvesting friendly DNN hardware accelerator [56] (to ensure that we are using the state of the art EH-WSN hardware) and recently proposed HAR- specific optimizations for EH systems [47]. Our analysis (see Figure 2a) shows that the state-of-the-art system still only finishes 58.7 of the inferences scheduled on a sensor. Al- though accuracy can increase by further tuning duty-cycle, as shown in Figure 2b, the returns are diminishing, and in- definite increase of duty cycle is also not an option as that might lead to skipping important data to infer. We observe that the system used in [47] does not aggressively employ quantization, which is a commonly used technique [64] to reduce both compute and transmission energy in DNN tasks. Our analysis, as shown in Figure 2c, shows accuracy as a function of quantization (we took the approach of perform- ing post training quantization and fine-tuned the DNN to work with reduced bit precision instead of training the DNN from scratch with a reduced precision). The quantized DNNs benefit from lower compute and memory footprints, but need specialized fine-tuning and often suffer from lower ac- curacy. Similarly, other approximation-via-data-reduction techniques, such as sub-sampling, did not perform inference with a desirable accuracy. Collectively, the aforementioned figures demonstrate that the harvested energy budget is insuf- ficient to perform all inferences with acceptable accuracy on currently proposed EH-WSN systems. Therefore, to complete all the scheduled computations, and thereby to improve ac- curacy, the system must rely on another device (e.g. a mobile phone), where sufficient resources are available to complete any remaining inference, if the data can be sent from the sensor. Said coordinating device completes the rest of the computations and finally, aggregates them with the ones completed in the sensor nodes. The challenge here is to send 1Throughout the paper we evaluate many of our motivation results using HAR as a workload as it is one such application, where the (EH-)WSN, used as body area network, fits perfectly with RF or body movement as the har- vesting source. HAR has the nuances of human introduced unpredictability and sensor induced noises. HAR has been pervasive enough given the rise of smart wearables and has been studied well enough to have ample access to resources to make a judicious evaluation. Further, we also evaluate one more emerging application from predictive maintenance domain. Algorithm Compression Ratio Accuracy Loss ( ) Fourier Decomposition 3 - 5 9.1 - 18.3 DCT 3 - 5 5.8 - 16.2 DWT 3 - 6 5.3 - 12.7 Coreset 3 - 10 0.02 - 0.76 Table 1: Accuracy trade-off of different compression techniques: Low-dimensional data loses important fea- tures under lossy compression, dropping inference ac- curacy significantly compared to the original data. De- tails on Coreset are available on Section 4. (Notations used: DCT: Discrete Cosine Transform, DWT: Discrete Wavelet Transform. 29.14 44.76 52.29 58.67 70.86 55.24 47.71 41.33 0 20 40 60 80 100 RR3 RR6 RR9 RR12 Scheduled Computation Completed Failed (a) Completion with ERR 0 20 40 60 80 100 Walking Climbing Cycling Running Jogging Jumping Accuracy ( ) RR3 RR6 RR9 RR12 Baseline (b) Accuracy of ERR 0 10 20 30 40 50 60 70 80 Walking Climbing Cycling Running Jogging Jumping Accuracy ( ) Quantization Level 16b Quantization Level 12b Quantization Level 8b (c) Accuracy vs quantiza- tions 0 20 40 60 80 100 Walking Climbing Cycling Running Jogging Jumping Accuracy ( ) Sampling with Probability Weighted Sampling Baseline (d) Accuracy vs sub- sampling Figure 2: Accuracy comparison of various classical node-level optimization techniques. The Extended- Round-Robin policy (ERR) [47] takes a store-and- execute approach, and the number associated repre- sents the ratio of store cycles vs execute cycles (e.g. RR3 is 3 store cycles followed by 1 execute cycle). The Baseline model is a fully powered system with no energy restrictions, and the quantized model runs on harvested energy using a RR12 policy. the data efficiently, since communication is an expensive task and especially challenging for EH-WSNs [22] thanks to their fickle and ultra-low energy budget. The obvious solution is to reduce the communication data volume by compress- ing the data before transmitting. This also reduces energy footprint and the probability of data packet loss. Challenges with Data Compression: Using standard compression algorithms, like discrete cosine transform, dis- crete wavelet transform, and Fourier decomposition etc., to minimize the communication overhead is not a viable solu- tion [45]. This is partly because we need a very high compres- sion ratio with very low power. Secondly, these compression algorithms are not context-aware, and hence lose relevant 4 features during the process of compression resulting in de- graded inference accuracy (refer Table 1 for details). A key insight is that, while these compression techniques work very well for high dimensional data (e.g. images), inference on low-dimensional sensor data (such as inertial measure- ment unit or IMU vibration data) is much more sensitive to lossy compression as separating between features might be difficult to do. And we will not achieve a sufficient com- pression ratio from lossless approaches either. Therefore, the standard data compression techniques are not very useful, let alone their energy efficient (such as quantized versions [33]) counterparts. For data compression in EH-WSNs, we need the compression algorithm to be 1 light weight (for energy efficiency), 2 feature preserving (for higher accuracy), 3 having a high compression ratio (for communication effi- ciency), and 4 context agnostic (for better generalization); i.e., our deployment scenario demands a smaller representa- tive form of the data that still preserves enough application- specific features to perform meaningful classifications in a given DNN. Why Coresets? The aforementioned requirements moti- vate us to consider coresets for forming representations of the original data. Coresets, primarily used in computational geometry [7], have been recently used [36, 37] for machine learning and sensor networks. Since coresets were designed to preserve the geometry of the data, we believe that they can be crafted to preserve features, and therefore be use- ful for performing accurate inference in subsequent stages, and thereby satisfying 2 . Furthermore, constructing core- sets do not need any application information, i.e. they are application data agnostic and can represent any form of data (IMU [36], Image [55], DNN feature map [17, 38, 46, 52]). This fulfils requirement 4 . They are also an effective way to con- struct a representation of the data set with high compression ratios [8, 17] without incurring unacceptable accuracy losses and thus useful for achieving 3 . For the DNNs in question, coresets can achieve sufficient compression ratios to make communication energy-competitive with computation, as well as opening up new opportunities for optimizing DNN in- ference on the coreset, rather than original data. Finally, most of the coreset construction algorithms are simple (hence can achieve 1 ) and do not need complex operations (like cosine, exponential, etc. [7, 8, 36, 37], they can also be quantized [37] to further reduce their computation and memory footprints. Motivated by this, we explore possibilities of designing an efficient synergistic sensor-host ecosystem (involving the EH-WSNs and host), where we try to maximize the compute at the sensor nodes, yet for the incomplete tasks, we use coresets to compress and send the data to the host where the rest of the computations could occur. H S C M H: Harvest S C: Sense Compute M: Communicate EH Sense Compute EH Sense Compute EH Sense Compute Decompress Infer Ensemble Sensor state transition sensor Host Legend: Figure 3: An example of EH Sensor-Host ecosystem - the sensor transitions between multiple states and executes the compute as store and execute fashion [47]. The host receives the data in compressed form for the unfinished portion, decompresses it, runs inference and finally ensembles the results from multiple sensors to improve accuracy and robustness. 3 DESIGN SPACE EXPLORATION Since data communication in a sensor host ecosystem (Fig- ure 3) consumes substantial power, we rely on coresets as an efficient way to lossily communicate the features with minimal information degradation. The coreset construction techniques need to be extremely lightweight while preserv- ing key features to justify the computation-communication trade-offs in energy and latency. To this end, we explore two different kinds of coreset construction techniques. 3.1 Coreset Construction Techniques Coreset Construction Using Importance Sampling: An easy way to build a representation from a data distribution is to perform importance sampling [7, 8], i.e. give more importance in choosing the data which are unique and, in our case, contribute significant to the inference (i.e. having a high enough magnitude in the frequency response of the sensor signal). The intuition is that any importance sampling scheme produces an unbiased estimator [8]. To preserve the temporal and frequency features, we ensure sampling data which are far enough from each other to build a better representation. The entire process of importance sampling uses simple arithmetic operations and is therefore viable in energy-scarce situations. The host can take the sub-sampled data and perform inference. The caveat is to have a model trained on the sub-sampled data, which can be done as an one-time step. Although the sub-sampling might lead to poor inference accuracy, in our experiments, with iso-compression ratio, importance sampling based coresets still outperforms classical compression techniques. Figure 4 shows a toy example of importance sampling in a 2D data set. Observe that the selected points (in red) are approximating the original distribution. 5 r2 r1 r4 r3 r5 Original Data Coreset with imp-sampling Coreset with Clustering Figure 4: A toy example of the coreset construction techniques in Seeker. Imp-sampling uses a probability based importance sampling; clustering preserves the geometric shape of the original data. In each case, the points values in red are communicated to the host. Coreset Construction Using Clustering: Although im- portance sapling based coreset construction is computation- ally inexpensive, it suffers from accuracy loss because it doesn t explicitly preserve the intricate structure of the data points. To address this, we also utilize coreset construction using k-means clustering [8, 36, 37], which separates the data points into a set of k (or fewer) N-spherical clusters and represents the geometric shape of the data by using the cluster centers and cluster radii (Fig. 4). These are then communicated to the host device for inference. Since clus- tering better preserves the geometry of the distribution, we observe that inferences with coresets constructed using clus- tering are more accurate than using importance sampling, and therefore can be preferred over the former whenever there is enough energy. 3.2 Communication vs Accuracy We can tune the aforementioned coreset construction tech- niques allow a variable number of features depending on the available energy, i.e. for importance sampling, we can limit the number of points to choose, and similarly, for clustering we can limit both the number of clusters and the number of iterations. However, even after preserving important fea- tures, the constructed corests are lossy representation of the original data. Therefore, when performing inference on the compressed coresets representation, the inference ac- curacy goes down, albeit not significant compared to other lossy compression methods (we can again refer to Table 1 for the relevant comparisons). This leaves an optimization space in trading between communication cost vs. accuracy, i.e. whether to construct strict and low-volume coresets and lose accuracy or to preserve maximum data points and pay for the communication cost. We perform an analysis on the MHELATH [9, 10] data set (we take a overlapping moving window of 60 data points sampled at 50Hz from 3 different IMUs, overlap size: 30 data points) to find a trade-off between the coreset size (directly related to the communication cost) and the inference accuracy. Empirically, we observe that ac- curately preserving the features for each class requires 20 data points using importance sampling or 12 clusters (see Fig. 6) using clustering based techniques. Going above 12 clusters did not significantly improve accuracy. This further motivates us to look for opportunities in the data distribution to improve the compression ratio. As the DNN models were designed to infer on the full data, we retrain the DNN models to recognize the compressed rep- resentation of the data and infer directly from that (both from the importance sampling and clustering). As the coreset for- mation algorithms are fairly simple [7, 8, 36, 37], it does not take much latency or energy to convert the raw sensor data into the coreset form even while using a commercial-off- the-shelf micro-controller (like TI MSP430FR5969 [66]). This allows the EH-sensor to opt for coreset formation followed by data communication to the host device as an energy-viable alternative to local DNN inference on the original data. In our example case, transmitting the raw data (60 data points, 32bit floating point data type) needs 240 Bytes of data trans- fer, and with coreset construction and quantization we can limit it to 36 Bytes (for 12 clusters, each cluster center is represented by 2 Bytes of data, and radius represented by 1 Byte data), thereby reducing the data communication volume by 85 . The host runs inference on the compressed data to detect the activity (with an accuracy of 76 ). However, due to this reduced accuracy, the sensor only takes this option iff it does not have enough energy to perform the inference at the edge device (either in the 16bit or 12bit variant of the DNN - more details on DNN design is presented in Sec- tion 4). This raises a question: is it possible to generate a more useful approximation, via reconstruction, of the data that we lost while forming the coresets? This problem has not been explored in details, as coresets are typically considered as an 𝛼 approximate representation of the data (𝛼being the error approximation parameter) [7] and never needed proper recovery. However, thanks to the low dimensional nature of many sensor data, reconstruction of original data from coresets becomes an essential step. 3.2.1 Data Memoization: Given our focus on ultra low power energy harvesting devices, any opportunities to re- duce computation and communication can noticeably aug- ment the performance and efficiency of the entire system. We look into data memoization as one such opportunity. For two instances of the same class, there should be a very high correlation in the sensor data. We empirically measure this by testing for correlation between the sensor signatures of different classes. Conservatively, we choose a correlation coefficient 0.95 to predict that the two activities are the same, and hence skip the inference altogether and just com- municate only the results to the host. We store ground truth sensor data pattern for all possible labels, and when new data arrives, we find the correlation of the sampled data against the ground truth data, and if any of the correlation coefficient 6 Power-Pred Decision Logic (MCU) Correlation Sensor Data 16bit DNN (x-bar) 12bit DNN (x-bar Coreset: Imp Smp Clust. Wireless Communication H S C M Harvestor Sensor Node EH Sense Compute EH Sense Compute EH Sense Compute Host Seeker Ecosystem Coreset Reconstruct DNN for Recovery DNN for Inference Ensemble Engine Cluster Recovery Classfied Results Figure 5: Overall system design of Seeker comes out to be 0.95, we choose to ignore further infer- ence computation and only communicate the classification result to the host for further processing. Note that choosing the correlation threshold entirely depends on the application and user preference. 3.2.2 Recoverable Coreset Construction: The primary reason the accuracy of inferring on coreset data is lower than that of the original model is the loss of features. Typically, the sensor data are low dimensional, and hence even with a good quality of coreset construction, it is difficult to preserve all the features. However, while inferring at the host, if we are able to recover the data or reconstruct it with minimum error, the accuracy can easily be increased. Clustering Coreset Recovery: Clustering preserves the geometry of the original data by representing them as a set of N-spherical clusters represented with a center and a ra- dius. In the process of coreset construction we only preserve the coordinates of the centers and the radii of the clusters, and hence miss the coordinates of the points inside the clus- ters. However, any random distribution of the lost points in the cluster could provide us with a 2𝑟 approximate repre- sentation of the original distribution (where 𝑟is the radius of the cluster; refer Figure 7a for a toy example). However, to achieve this, we need some extra information about the clus- ters. The standard method of clustering-based coreset con- struction keeps the cluster center and cluster radius, which gives the geometrical shape of the entire data. Extending this with the point count for each cluster allows for recon- struction of data in the original form that can be processed by DNNs trained on full-size data. These reconstructed data sets can be synthesized simply by uniformly distributing the points within each cluster. Although the intra-cluster data distribution will be different from the original, it will still preserve the overall geometry with a certain degree of approximation which the DNN could learn to accommodate. Experimentally, on the MHELATH dataset, we observe that inferring on the synthesized reconstructions of cluster 0 20 40 60 80 100 Walking Climbing Cycling Running Jogging Jumping Accuracy k 12 (Baseline) K 15 k 10 k 8 k 6 Figure 6: Accuracy with different clusters (k). Clustering Recovery Original Data Coreset Recovered Data (a) Recovering a cluster with uniform random re-distribution. Latent Space G Generator Noise D Discriminator Generated Sample Actual Sample Recovered Signal Close to actual? Finetune (b) Recovering a sub-sampling with GAN. Figure 7: Recovering data from the coresets. based coresets can achieve an accuracy of 85 . The recon- struction feature at the host comes with little to no overhead for the host (given the host devices have considerably more compute than the sensor nodes). The addition of the recovery parameter (number of points per cluster) needs 4 more bits (in our experiments, we never observe any clusters having more than 16 data points) of data per cluster, bringing the total data communication volume to 42 Bytes, which is still a significant 5.7 less in comparison to the original 240 Bytes needed to communicate the raw data in our setup. However, since clustering based coreset construction is more expensive than the importance sampling based coreset construction, it is not always possible to build a recoverable coreset at the edge, unless we figure out a to recover the lost points while we perform importance sampling. Importance Sampling Coreset Recovery: Unlike cluster- ing, when we construct a coreset with importance sampling, we typically have no information regarding the lost data points. We hypothesize that the dropped sample should con- tain, although not important, sensor specific artifacts. And these artifact must have some pattern, if modeled correctly, could represent the lost data. Towards this, we designed and trained a generative adversarial network (GAN, see Figure 7b for the structural details) to recover the lost samples of the importance sampling. As training parameters, we provide some statistical parameters (specifically mean and variance) 7 of the signal and random noise to the generator, and the generator generates the lost signals. The discriminator tries to discriminate between the actual data and the synthesized data. We fine-tune the network until the discriminator is fooled sufficiently to distinguish between the original data and the recovered data. Considering the fact that we do have access to the sensor data to train the learning algorithm, we can use the same data to train the GAN and with sufficient data, the discriminator could generate the lost signal with minimum error. Our experiments show that the deviation from the original signal, in most cases, is limited to 15 . However, in some pathological cases, the error at times goes close to 60 , and we believe them to be generated artifacts which are common side effects of the GANs[11]. Our experi- ments suggests that inference on the GAN recovered signal is almost as good as (about 2 4 difference in accuracy) the inference on the recovered cluster signal. The recovery policy can be implemented as a simple generator network in the host. Although, the training of the GAN is complex and involves multiple networks as well as hyper-parameters tun- ing, the generator network itself is very small (few hundred thousands of parameters depending on the sensor data). 4 DESIGN IMPLEMENTATION OF SEEKER By leveraging the coreset construction techniques discussed in Section 3, we design Seeker: A synergistic sensor host ecosys- tem. Figure 5 gives a pictorial representation of the overall design of Seeker and its various components. Seeker lever- ages the concept of NVP, and employs a flexible store and execute method using the state of the art ReRAM crossbar architecture [47] to perform inference at the edge. It aug- ments the sensor nodes with two different quantized DNNs (16 bit and 12 bit) to increase the number of completed in- ferences at the sensor node itself. Prior studies [56, 64, 68] and our empirical analysis on the quantization vs accuracy trade-offs (see Fig. 2c) indicate the 16 and 12bit precision to maximize the accuracy of the inferences while minimizing the energy consumption. Moreover, we also implement the memoization option so that it does not have to repeat infer- ences if it encounters similar data, thereby saving substantial energy as well as delivering results with extremely low la- tency. However, even with all these optimizations, due to the fickle nature of EH, Seeker cannot finish all the inferences at the edge and must communicate with a host device. To minimize the data communication overhead between the sensor-node and the host device, Seeker utilizes coresets to build representative, yet compressed, forms of the data. To cater towards the fickle EH budget, we use the two dif- ferent coreset construction techniques, described in Section 3: a cheaper, less accurate formation (importance sampling) and a more expensive, yet accurate formation (K-means). Trans- mitting coresets rather than raw data greatly improves the energy efficiency of communication to the host, when re- quired, and effectively increases the number of completed inferences, thereby increasing overall accuracy. Depending on the incoming data and the EH budget, the sensor decides whether to skip compute, perform an inference at the edge, or form a coreset to offload the inference to the host. The host, after obtaining information from multiple sensors, per- forms any further required computation and uses ensemble learning [47] to give an accurate classification result. Note that, unlike prior EH-WSN systems [47], the role of the host device here is not limited to just result aggregation; rather, the host participates and performs inference when the sen- sors do not have enough energy and choose to communicate the data (in the form of coresets) to the host. In this section, we will explain, in detail, the overall execution workflow of the Seeker system, followed by the the detailed design of the hardware support to maximize its energy efficiency. 4.1 Decision Flow: From Sensors to the Host Figure 8 depicts a flow chat showing the decision process taken in the sensor nodes to navigate between each compo- nents. Each sensor has a data buffer that collects the data points for classification (implemented using a 60 3 FIFO structure of 4Byte cells to store the floating point data. The 3 caters towards the multiple channels of the sensor. The moving window is designed using a counter to shift the streaming data.) The sensor also stores one ground truth trace for each activity. The sensor computes the correlation ( 1a ) between the stored ground truth and the current data. If the correlation coefficient is 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑( 1b ) the sensor skips all computation and sends the result to the host. Oth- erwise, the sensor prioritizes local computation and, with the help of a moving average power predictor [47], predicts whether it can finish the quantized DNN inference with the combination of stored energy and expected income ( 2a and 2b ). If energy is insufficient for DNN inference, the sensor will use coreset formation to communicate the important features to the host, which completes the inference. Since the clustering based coreset is typically more accurate then those formed by importance sampling, the former is pre- ferred, when possible. We increase the frequency of cluster- based formation by using custom, energy efficient hardware. With the help of an activity-aware and recoverable coreset construction and low-power hardware design, we can effi- ciently communicate inferences or compressed data to the host device with minimum power and latency overheads. Seeker, accounting for the available energy budget, con- siders the following decisions: D0: Test for data similarity using correlation, and if similarity is found then communi- cate the results to the host; D1: DNN at sensor with raw data Communicate the results to the host; D2: Try Quantized 8 Start Current Data Last Data Correlation Power Predictor Control Quantized DNN? Clustering send? Importance Sampling Send? DNN Inference No Similar? Send Importance Sampling Coreset Construction Clustering Coreset Construction AAC Last Classification Result End Yes Yes No Yes No Yes No 1a 2a 1b 2b Figure 8: Decision flow of Seeker. Strategy Sensor Energy Comm. Energy Total Energy Avg Acc ( ) D0 0.54 8.27 8.81 D1 29.23 8.27 37.5 80.03 D2 16.58 8.27 24.85 77.37 D3 1.07 15.97 17.04 78.30 D4 0.87 15.97 16.84 85.30 raw data 70.16 70.16 87.23 Table 2: Energy breakdown of different Seeker strate- gies (in 𝜇Joules). The accuracy reported is the average case over 1000 inferences. DNN inference and communicate the results to the host; D3: Clustering based coreset construction at the sensor, and com- municate the coreset to the host; host runs DNN inference on the reconstructed data; and D4: Importance sampling based coreset construction at the sensor and communicate the coresets to the host; host recovers the original data with the pre-programmed generator, and performs inference with the recovered data. Table 2 lists the energy requirements of each of these decisions. 4.2 Efficient Hardware Accelerator Energy harvesting brings challenges in both average power levels and power variability. Performing DNN inference un- der such conditions often limits exploitation of inherent DNN parallelism within the energy budget. Therefore, many prior works use custon DNN accelerators, typically based on (non- volatile) resistive RAM (Re-RAM) based [47, 56] crossbar architecture, to perform DNN inference on EH-sensor nodes. Seeker s inference engine follows the design proposed in ResiRCA [56] and modifies it to cater towards new quanti- zation requirements: We have two different instances of the Re-RAM crossbar in our system - one for the 16bit model and one for the 12-bit model. The nonvolatile nature of the Re- RAMs helps in performing intermittent computing with the harvested energy. Moreover, techniques like loop tiling and partial sums [56] can further break down the computation to maximize forward progress with minimum granularity. While DNN inference is already accelerated in the sensor node, the addition of a general-purpose processor for coreset 0.00 0.50 1.00 1.50 2.00 2.50 3.00 0.00 2.00 4.00 6.00 8.00 10.00 5 6 7 8 9 10 11 12 15 Accelerator energy in µJ M4 Energy in mJ Clusters Clustering Energy in M4 (mJ) Clustering Energy in Accelerator (uJ) Figure 9: Energy consumption comparison of the clus- tering accelerator vs the SoTA hardware computation would be energy inefficient. Specifically, the computation requirement is fixed and does not require the overheads to support generality. Therefore, we add a low power, low latency coreset construction hardware. Compar- ing the energy consumption of the proposed clustering accel- erator with Adafruit ItsyBitsy nRF52840 Express - Bluetooth LE [2] suggests the accelerator to be 3.7 103 times en- ergy efficient (refer Figure 9). Both the coreset construction algorithms follow a sequence of multiply and add subtract operations followed by averaging, and hence can be simply designed with few logic units. Moreover, as we are operating with lower data volume, these operations can be parallel (for example, the clustering hardware simultaneously works on all the cluster formations). The bigger challenge is posed by the requirement of a variable number of iterations for these algorithms to converge, the number of clusters samples re- quired, etc. To efficiently design the hardware and configure its parameters, we run several experiments and empirically arrive at the following conclusions: 1. The clustering finishes within 4 iterations and, for importance sampling, it takes up to 7 iterations. 2. None of the clusters have more than 16 points during any clustering. 3. We need not store all the points in either cases at every iteration, rather the clustering hardware needs to store the sum, the radii of the clusters, the number of points per cluster, and the importance sampling hardware needs just the points. 4. Storing the radii helps in easily selecting the points in the subsequent iterations. 5 EVALUATION AND RESULTS In this section, we describe our methodology to evaluate Seeker. We start with implementing Seeker using the CotS Adafruit ItsyBitsy nRF52840 Express [2] as the sensor com- pute node and a Google Pixel 6 Pro as the host node. Further, we describe how the design performs in simulated state-of- the-art hardware accelerator specifically designed for EH purpose. We look into two different applications: multi sen- sor human activity recognition (HAR), and bearing fault detection for predictive maintenance and compare the re- sults with multiple baselines designed for ultra-low-power as well as EH environments. 9 0 10 20 30 40 50 60 70 80 50 55 60 65 70 75 80 85 90 Raw Data DCT DWT 11 Clusters 12 Clusters IS (20 pts) Energy Consumed (mJ) Accuracy ( ) Accuracy at Host Energy Consumed (mJ) 1.00 2.61 2.86 6.49 5.71 6.00 Compression Ratio w.r.t. raw data Figure 10: Seeker vs other compression techniques 5.1 Seeker on Commercial Hardware Although Seeker is designed for EH-WSNs, the efficient com- munication mechanism for low dimensional sensor data can still be useful for the current commercial devices. Most of the ultra-low-power ( 30𝑚𝑊) micro-controllers are not equipped with complex multiply-accumulate units to effi- ciently perform DNN computations, and hence are suited to collect, compress and send the data to a host (Pixel 6 Pro) and then the host decompresses (or recovers) the data and performs the inference. We used the inertial measurement unit data of MHEALTH [10] dataset as the sensor data, which is pre-processed and compressed at the Adafruit compute node and then sent over Bluetooth low-energy to the host. We used Circuit Python [12] and Mu Editor [51] to imple- ment the compression and the communication algorithms in the Adafruit board, and TensorFlow lite [1] to deploy the DNN inference at the host. We evaluate the efficiency, both in terms of compression ratio, energy consumption and accuracy preservation, of the recoverable clustering and recoverable importance sampling algorithms against three other popular methods: 1) sending raw data without com- pression; 2) compression using DCT; 3) Compression using DWT. We measure the energy consumption and inference accuracy over 1000 iterations to provide an average fair es- timate. As depicted in Figure 10, Seeker out performs both DCT and DWT in compression ratio, and the recovery fea- ture of Seeker helps preserving inference accuracy close to the original raw data. 5.2 Seeker for Activity Recognition Human Activity Recognition (HAR) using body area net- work is becoming mainstream on most of the warble devices. Moreover, the pervasive nature of HAR along with ample opportunities to harvest energy, makes HAR on body area network quite interesting. Therefore, as a case study, we simulate an entirely EH body area network using all the components of Seeker; specifically, to leverage intermittent computing using EH only, we simulate HAR on the hardware described in Section 4.2. This includes three different sen- sors located at left ankle, right arm, and chest. Each sensor has (i) sensing element a.k.a Inertial Measurement Unit that collects acceleration data), (ii) two DNN Re-RAM crossbar (16bit 12bit) built using XB-SIM [21], (iii) two coreset com- putation engines synthesized using Design Compiler [16], (iv) an energy harvester unit which is modeled after real- world energy harvester trace data obtained from the works by Qiu et al.[56] and Geissdoerfer et al. [22] (the specifics of the energy-harvesting mechanism producing the power trace are beyond the scope of this work.t), (v) a simple mov- ing average power predictor power predictor and, (vi) low energy communication unit which uses IEEE 802.15.6. We model the communication energy based on the current state- of-the-art low energy communication systems [22, 59]. We utilize a simulation driven approach as multiple components, including the crossbar, coreset engine etc., are specialized hardware that are not commercially available. System devel- opment in the ultra-low-power space fundamentally spans the device technology, microarchitecture, architecture, and networking fields, and understanding the design space of next-generation EH-WSNs requires incorporating proposed advances from all areas. The crossbar simulator [21, 56] accu- rately measure the power consumption and the latency of the operations, and the same is true for Design Compiler s mod- eling of the coreset engine. The simulation tools used in our experiment are widely used and accepted in both industry and research. We evaluate our simulation using two differ- ent datasets, MHEALTH [9, 10], and PAMAP2 [57, 58]. The coreset re-construction GAN and DNN models are trained and quantized using tensorflow [1]. Furthermore, we also leverage the temporal nature of HAR by designing a dynamic coreset construction algorithm. Activity Aware Coreset Construction: In our experi- ments on HAR sensor data, we observe that not all the activ- ities are equally complex and hence may or may not need a certain number of clusters to represent every feature. While activities like walking and running do not lose much accu- racy even when represented with as low as eight clusters, complex activities are more sensitive and need more number of clusters to preserve their geometry. As the communication overhead depends on the number of clusters, which, in turn, depends on the complexity of the activity, we propose an activity aware clustering which ensures that coresets for the current activity are represented with just sufficient number of clusters to preserve accuracy. We determine the number of clusters required as a function of current energy availability and accuracy trade off of using a lesser number of clusters. However, naively framed, this approach requires knowledge of what activity is being performed in order to encode the data that will be used to perform inference to determine what activity is being performed. To break this circular de- pendency, we take inspiration from prior work in HAR [47], and use the highly stable temporal continuity of human ac- tivity (relative to the tens of milliseconds timescales for HAR 10 0 0.05 0.1 0.15 0.2 k 15 k 12 k 10 k 8 Activity Aware Coresets (HAR) Energy Aware Coresets (Bearing) Fixed Coresets Data Volume normalized to raw data (a) Data volume with dynamic coresets 0.5867 0.4106 0.2837 0.9139 0.8516 0.6076 0.8742 0.8073 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 WiFi Office Wifi Home Piezo Daily Movement Completion in Origin (HAR) Completion in Seeker (HAR) Completion in Seeker (Bearing) N A (b) Fraction of inferences completed with different EH sources 6.18 6.96 7.12 21.76 19.46 12.57 31.27 28.76 22.56 34.16 32.95 35.19 6.63 11.87 22.56 0 10 20 30 40 50 60 70 80 90 100 Wifi Office Wifi Home Piezo Daily Movement Power Trace: Memoization DNN 16bit DNN 12 Bit Clustering Importance Sampling (c) Distribution of compute off-load to different components Figure 11: Accuracy and communication efficiency of Seeker with different data sets and its sensitivity towards various EH sources. -0.45 -0.24 -0.02 -0.07 -0.14 -0.19 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 0 50 100 Walking Climbing Cycling Running Jogging Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (a) Accuracy with MHEALTH dataset -0.26 -0.19 -0.76 -0.28 -0.04 -0.80 -0.60 -0.40 -0.20 0.00 0 20 40 60 80 100 Walking Climbing Cycling Running Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (b) Accuracy with PAMAP2 dataset Figure 12: Accuracy and communication efficiency of Seeker with different data sets and sensitivity study. inferences) to predict the current activity based on previously completed local inferences. We use temporal continuity to our advantage, and make sure that if the system does not have enough energy to form the default 12 clusters, it will re- sort to forming a smaller number of clusters with minimum accuracy loss. We implement a small lookup table to carry the information on accuracy trade-off for different activities with respect to the number of clusters used to form the core- sets (similar to the data represented Fig. 6). We observe that AAC communicates about 11 data compared to sending the full raw data (refer Figure 11a). Note that we only resort to activity awareness while forming coresets using clustering, as importance sampling based coreset construction does not require much energy. Furthermore, dropping the number of samples in importance sampling method, even with recovery, significantly hampers the accuracy, which is not the case for recoverable-clustering based coreset construction. Baseline: We choose three points of comparison for our ac- curacy evaluation. Baseline-1 (Baseline: Large DNN) con- sists of a full precision (without any pruning) DNN built on the lines of [26]. Baseline-2 (Baseline: EAP) optimizes (us- ing [68]) Baseline-1 to design a power-aware DNN tuned to for the average harvested power of our EH source. Baseline- 3 (Baseline: Origin) uses the system design proposed in [47]. Baseline-1 and Baseline-2 run on fully powered systems where as Baseline-3 runs on the same EH source as Seeker. For communication, we consider a system which transmits the entire raw data to the host as the baseline. Analysis of Results on HAR: Figure 17a and 17b show the accuracy of various policies described in Section 4, along with the accuracy of Seeker - which applies all policies to- gether, along with ensemble learning. Figure 11a shows the normalized data communication volume with different num- bers of clusters, along with activity-aware clustering. We make the following observations: Seeker at Edge Finishes More Work: Equipped with multi- ple decision options, Seeker could finish close to 60 (58.67 using one of the RE sources; refer Figure 11b) of the in- ferences at the edge itself, thanks to the two efficient and quantized DNNs and the correlation engine. Both the DNNs share the load of the inference depending on the available energy, while correlation engine gets rid of close to 6 of the redundant compute (refer Figure 11c). Seeker at Edge Efficiently Offloads: For the unfinished compute, Seeker converts the data into coresets for commu- nicating them to the host with minimum payload footprint. The activity aware coresets, thanks to their dynamic nature, reduces the communication volume 8.9 (refer to Figure 11a) compared to sending the raw data, and up to 3 compared to the classical compression techniques. The Recovered Coresets Give Accurate Inference: Even with a specialized DNN tranined with coreset data, com- pressed coresets give less accuracy, evidently because of the loss of features during the coreset formation. However, with the reconstruction (via GAN or cluster redistribution), the accuracy reaches 86.8 compared to 76.4 for the former. The GAN modeled the lost signals with a very correlation ( 0.9 in most cases and 0.6 in some of the worst cases). Seeker is Close to a Fully Powered System: Seeker, thanks to synergistic computation, achieves 87.05 accuracy with 11 1.74 0.56 0.86 1.98 0.66 0.00 0.50 1.00 1.50 2.00 2.50 0 20 40 60 80 100 0.007mm 0.014mm 0.021mm 0.028mm GeoMean Error Accuracy ( ) Fault Diameter Cluster: Compressed Cluster: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered Figure 13: Accuracy of Seeker on Bearing Data set MHEALTH (0.18 less accurate than a DNN running at full precision with full power) data set and similarly reaches 74.2 accuracy on the PAMAP2 data set. This gives us 7 more accuracy than [47] on MHEALTH and 3 more accuracy for PAMAP2 dataset. The accuracy improvement is because of three reasons: 1. the DNNs at the edge are more fine-tuned towards delivering accurate results and are 1.5 more accurate than the prior state-of-the-art [47]; 2. the recovered coresets imitate the original data with a great accuracy, and hence the inference accuracy at the host is as good as it would have been with the original data. 3. because Seeker could finish more number of inferences (either at the edge or by offloading to a host), greatly reducing the scheduled task from 40 to 5 in best case, 8 in worst case, and 6.15 in average case (all experiments on RF power trace). To compare with a battery operated energy optimized (to the average energy harvested by the RF sources) system, Seeker is 5.89 more accurate on MHEALTH dataset, and 4.98 more accurate on PAMAP2 dataset. Sensitivity Study: Our experiments with other EH sources show the versatility of Seeker, which outperforms a HAR clas- sifier designed for EH [47] across multiple (piezo-electric, RF) harvested energy sources and demonstrate that it can easily be scaled to work with any number of sensors. Fig. 11b shows the comparison of scheduled inferences completed while using Seeker and the state-of-the-art [47]. Further, we demonstrate how Seeker leverages all the proposed design components (including memoization, DNN inference and coreset) to complete maximum compute at the edge and of- fload minimum to the host device. Fig. 11c shows the compute breakdown among components under different EH sources. 5.3 Seeker for Predictive Maintenance With the advent of automation and industry 4.0 [34, 70], predictive maintenance is once of the most sorted after prob- lems in industrial IoT domain. Predictive maintenance is a condition-driven preventive maintenance program where, instead of replying on failure to schedule maintenance activ- ities, predictive maintenance uses direct monitoring of the plant to preemptively schedule maintenance and possibly take measures to prevent them from occurring [48]. For con- tinuous monitoring, the machines are typically fitted with sensors (vibration, force, magnetic, acoustic etc.), and the plant performs continuous analytics on those sensor data for preemptively maintenance scheduling. Vibration based con- dition monitoring is one of the most common scenarios [53]. Since there are multiple machines, and each machine is fitted with one or more sensors, this is a perfect example of wire- less sensor network. The rise of Industry 4.0 [34] has lead to an exponential explosion of such sensors in industries, especially in remote or hostile locations, calling for energy harvesting as a solution, and hence need for EH-WSNs. Baseline: Towards this we take Case Western Bearing Fault data set [53] where the vibrations from the different bear- ings are collected to analyse the fault patterns (e.g. size of a crack in the bearing with respect to operating load, speed etc.). There has been a large body of work [18, 27, 28, 35] in industrial engineering domain to develop DNN classifer for this task. In our experiments, we took inspirations from the work of [18, 27] to build a classifier, and applied further opti- mizations, as we did for HAR evaluation, to make the DNN edge-friendly. We also tweaked AAC to be energy aware only, i.e. the number of clusters formed depends only the energy available. We redesigned (few changes in the hyper parameters) and trained the GAN to adapt to the bearing data for recovering the importance sampling coresets. Results: As the base design of Seeker can adapt to any sen- sor based communication, most of the arguments made for HAR still holds true in the case of bearing fault detection. As depicted on Figure 11a and 11b, Seeker reduces the com- munication overhead by 7 while finishing 80 of the scheduled compute using WiFi sources. Further, as shown in Figure 13, Seeker, on an average, delivers an accuracy of 84.73 which is only 0.66 less than a fully powered system. It is note worthy that the bearing data is much susceptible to the real-world nuances and machine part interactions yet, the accuracy of fault prediction is extremely essential to- wards the production continuity and quality. To portray an example of scale, for a typical grinding job in a manufac- turing industry that takes about 8.2 seconds [6], and the 5 improvement we observe (in Figure 13) over the prior state of the art [47] impacts 46𝑘parts per year per machine (work- ing 8 hours day). Therefore, in large scale industries, both saving communication overhead while maximizing accuracy directly impact the economics of production. 6 CONCLUSION As systems utilizing energy harvesting edge devices are tasked with increasingly complex tasks, like HAR or pre- dictive maintenance, both system and node designs must respond with targeted efficiency-maximizing optimizations. Our proposal, Seeker, synergizes EH sensor nodes host de- vices by intelligently distributing computations among them, while significantly minimizing the communication over- heads. Our experiments show that, by leveraging coreset techniques in data reduction and tuning these techniques for 12 application-aware properties, Seeker can reduce the commu- nication overhead by 8.9 , while providing better accuracy (86.8 ), even when limited to harvested power, than state-of- the-art energy-optimized DNNs running on a fully powered device (81.2 ). Furthermore, it also outperforms the state of the art system designed specifically for EH-WSNs. Col- lectively, the optimizations in Seeker reduce communication traffic, while improving inference accuracy, demonstrating the potential of holistic system node application optimiza- tion for the current and future generation of (energy harvest- ing) wireless sensor nodes. REFERENCES [1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin- cent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi- aoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org. [2] Adafruit ItsyBitsy nRF52840 Express Bluetooth LE 2022. Adafruit ItsyBitsy nRF52840 Express Bluetooth LE. [3] Apple Watch 2019. Apple Watch Series 5 teardown reveals bigger battery. 5-teardown-reveals-bigger-battery. [4] Apple Watch - ECG 2020. Taking an ECG with the ECG app on Apple Watch Series 4 or later. [5] Apple Watch - Fall Detection 2020. Use fall detection with Apple Watch. [6] Automating the Grinding Process 2013. Automating the Grinding Process. the-grinding-process . [7] Olivier Bachem, Mario Lucic, and Andreas Krause. 2015. Coresets for nonparametric estimation-the case of DP-means. In International Conference on Machine Learning. PMLR, 209 217. [8] Olivier Bachem, Mario Lucic, and Andreas Krause. 2017. Practical core- set constructions for machine learning. arXiv preprint arXiv:1703.06476 (2017). [9] Oresti Baños, Rafael García, Juan Antonio Holgado Terriza, Miguel Damas, Héctor Pomares, Ignacio Rojas Ruiz, Alejandro Saez, and Clau- dia Villalonga. 2014. mHealthDroid: A Novel Framework for Agile Development of Mobile Health Applications. In IWAAL. Springer. [10] Oresti Banos, Claudia Villalonga, Rafael García, Alejandro Saez, Miguel Damas, Juan Holgado-Terriza, Sungyong Lee, Hector Pomares, and Ignacio Rojas. 2015. Design, implementation and validation of a novel open framework for agile development of mobile health applications. BioMedical Engineering OnLine (2015). [11] David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Stro- belt, Bolei Zhou, and Antonio Torralba. 2019. Seeing what a gan cannot generate. In Proceedings of the IEEE CVF International Conference on Computer Vision. 4502 4511. [12] Circuit Python 2022. Circuit Python. [13] U Cisco. 2020. Cisco annual internet report (2018 2023) white paper. [14] Bradley Denby and Brandon Lucia. 2019. Orbital edge computing: Machine inference in space. IEEE Computer Architecture Letters 18, 1 (2019), 59 62. [15] Brad Denby, Emily Ruppel, Vaibhav Singh, Shize Che, Chad Taylor, Fayyaz Zaidi, Swarun Kumar, Zac Manchester, and Brandon Lucia. 2022. Tartan Artibeus: A Batteryless, Computational Satellite Research Platform. (2022). [16] Design Compiler: Concurrent Timing, Area, Power, and Test Optimiza- tion 2022. Design Compiler: Concurrent Timing, Area, Power, and Test Optimization. signoff rtl-synthesis-test dc-ultra.html. [17] Abhimanyu Dubey, Moitreya Chatterjee, and Narendra Ahuja. 2018. Coreset-based neural network compression. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV). 454 470. [18] Levent Eren, Turker Ince, and Serkan Kiranyaz. 2019. A generic intel- ligent bearing fault diagnosis system using compact adaptive 1D CNN classifier. Journal of Signal Processing Systems 91, 2 (2019), 179 189. [19] Levent Eren, Turker Ince, and Serkan Kiranyaz. 2019. A generic intel- ligent bearing fault diagnosis system using compact adaptive 1D CNN classifier. Journal of Signal Processing Systems 91, 2 (2019), 179 189. [20] Amir Erfan Eshratifar and Massoud Pedram. 2018. Energy and perfor- mance efficient computation offloading for deep neural networks in a mobile cloud computing environment. In Proceedings of the 2018 on Great Lakes Symposium on VLSI. 111 116. [21] Xiang Fei, Youhui Zhang, and Weimin Zheng. 2020. XB-SIM: A sim- ulation framework for modeling and exploration of ReRAM-based CNN acceleration design. Tsinghua Science and Technology 26, 3 (2020), 322 334. [22] Kai Geissdoerfer and Marco Zimmerling. 2022. Learning to Com- municate Effectively Between Battery-free Devices. In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22). USENIX Association, Renton, WA, 419 435. org conference nsdi22 presentation geissdoerfer [23] Graham Gobieski, Brandon Lucia, and Nathan Beckmann. 2019. Intelli- gence Beyond the Edge: Inference on Intermittent Embedded Systems. In ASPLOS, Iris Bahar, Maurice Herlihy, Emmett Witchel, and Alvin R. Lebeck (Eds.). ACM. [24] Graham Gobieski, Brandon Lucia, and Nathan Beckmann. 2019. Intelli- gence Beyond the Edge: Inference on Intermittent Embedded Systems. In ASPLOS. ACM. [25] Google Assistant for Wearables 2020. Google Assistant for Wearables. [26] S. Ha and S. Choi. 2016. Convolutional neural networks for human ac- tivity recognition using multiple accelerometer and gyroscope sensors. In IJCNN. [27] Seungmin Han and Jongpil Jeong. 2020. An weighted CNN ensemble model with small amount of data for bearing fault diagnosis. Procedia Computer Science 175 (2020), 88 95. [28] Duy-Tang Hoang and Hee-Jun Kang. 2017. Convolutional neural network based bearing fault diagnosis. In International conference on intelligent computing. Springer, 105 111. [29] Duy-Tang Hoang and Hee-Jun Kang. 2017. Convolutional neural network based bearing fault diagnosis. In International conference on intelligent computing. Springer, 105 111. [30] Chuang Hu, Wei Bao, Dan Wang, and Fengming Liu. 2019. Dynamic adaptive DNN surgery for inference acceleration on the edge. In IEEE INFOCOM 2019-IEEE Conference on Computer Communications. IEEE, 1423 1431. [31] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor Mudge, Jason Mars, and Lingjia Tang. 2017. Neurosurgeon: Collabora- tive intelligence between the cloud and mobile edge. ACM SIGARCH 13 Computer Architecture News 45, 1 (2017), 615 629. [32] Naoto Kimura and Shahram Latifi. 2005. A survey on data compression in wireless sensor networks. In International Conference on Information Technology: Coding and Computing (ITCC 05)-Volume II, Vol. 2. IEEE, 8 13. [33] Julius Kusuma, Lance Doherty, and Kannan Ramchandran. 2001. Dis- tributed compression for sensor networks. In Proceedings 2001 Inter- national Conference on Image Processing (Cat. No. 01CH37205), Vol. 1. IEEE, 82 85. [34] Heiner Lasi, Peter Fettke, Hans-Georg Kemper, Thomas Feld, and Michael Hoffmann. 2014. Industry 4.0. Business information systems engineering 6, 4 (2014), 239 242. [35] Dean Lee, Vincent Siu, Rick Cruz, and Charles Yetman. 2016. Con- volutional neural net and bearing fault analysis. In Proceedings of the International Conference on Data Science (ICDATA). The Steering Com- mittee of The World Congress in Computer Science, Computer ..., 194. [36] Hanlin Lu, Changchang Liu, Ting He, Shiqiang Wang, and Kevin S. Chan. 2020. Sharing Models or Coresets: A Study based on Membership Inference Attack. arXiv:2007.02977 [cs.LG] [37] H. Lu, C. Liu, S. Wang, T. He, V. Narayanan, K. S. Chan, and S. Pasteris. 2020. Joint Coreset Construction and Quantization for Distributed Machine Learning. In 2020 IFIP Networking Conference (Networking). [38] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. 2017. Training Gaussian Mixture Models at Scale via Coresets. J. Mach. Learn. Res. 18, 1 (Jan. 2017), 5885 5909. [39] K. Ma, X. Li, J. Li, Y. Liu, Y. Xie, J. Sampson, M. T. Kandemir, and V. Narayanan. 2017. Incidental Computing on IoT Nonvolatile Processors. In MICRO. [40] K. Ma, X. Li, S. Li, Y. Liu, J. J. Sampson, Y. Xie, and V. Narayanan. 2015. Nonvolatile Processor Architecture Exploration for Energy-Harvesting Applications. IEEE Micro 35, 5 (2015), 32 40. [41] K. Ma, X. Li, S. R. Srinivasa, Y. Liu, J. Sampson, Y. Xie, and V. Narayanan. 2017. Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors. In 2017 (ASP-DAC). 678 683. [42] Kaisheng Ma, Xueqing Li, Karthik Swaminathan, Yang Zheng, Shuangchen Li, Yongpan Liu, Yuan Xie, John (Jack) Morgan Sampson, and Vijaykrishnan Narayanan. 2016. Nonvolatile Processor Architec- tures: Efficient, Reliable Progress with Unstable Power. IEEE Micro 36, 3 (2016), 72 83. [43] Kiwan Maeng and Brandon Lucia. 2018. Adaptive Dynamic Check- pointing for Safe Efficient Intermittent Computing. In OSDI. USENIX Association. [44] Kiwan Maeng and Brandon Lucia. 2018. Adaptive Dynamic Check- pointing for Safe Efficient Intermittent Computing. In OSDI, Andrea C. Arpaci-Dusseau and Geoff Voelker (Eds.). USENIX Association. [45] Francesco Marcelloni and Massimo Vecchio. 2008. A simple algorithm for data compression in wireless sensor networks. IEEE communica- tions letters (2008). [46] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. 2020. Coresets for Data-efficient Training of Machine Learning Models. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, Virtual, 6950 6960. [47] Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. 2021. Origin: Enabling On-Device Intel- ligence for Human Activity Recognition Using Energy Harvesting Wireless Sensor Networks. In DATE. [48] R Keith Mobley. 2002. An introduction to predictive maintenance. Else- vier. [49] Reem E Mohamed, Ahmed I Saleh, Maher Abdelrazzak, and Ahmed S Samra. 2018. Survey on wireless sensor network applications and energy efficient routing protocols. Wireless Personal Communications 101, 2 (2018), 1019 1055. [50] Thaha Mohammed, Carlee Joe-Wong, Rohit Babbar, and Mario Di Francesco. 2020. Distributed inference acceleration with adap- tive DNN partitioning and offloading. In IEEE INFOCOM 2020-IEEE Conference on Computer Communications. IEEE, 854 863. [51] Mu Editor 2022. Code with Mu a simple Python editor for beginner programmers. [52] Ben Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, and Dan Feldman. 2019. Data-independent neural pruning via coresets. arXiv preprint arXiv:1907.04018 (2019). [53] Dhiraj Neupane and Jongwon Seok. 2020. Bearing fault detection and diagnosis using case western reserve university dataset with deep learning approaches: A review. IEEE Access 8 (2020), 93155 93178. [54] Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu, Zeng Huang, Sitao Xiang, Shunsuke Saito, Pushmeet Kohli, and Hao Li. 2017. Re- alistic dynamic facial textures from a single image using gans. In Proceedings of the IEEE International Conference on Computer Vision. 5429 5438. [55] Rohan Paul, Dan Feldman, Daniela Rus, and Paul Newman. 2014. Visual precis generation using coresets. In 2014 IEEE International Conference on Robotics and Automation (ICRA). 1304 1311. 1109 ICRA.2014.6907021 [56] K. Qiu, N. Jao, M. Zhao, C. S. Mishra, G. Gudukbay, S. Jose, J. Samp- son, M. T. Kandemir, and V. Narayanan. 2020. ResiRCA: A Resilient Energy Harvesting ReRAM Crossbar-Based Accelerator for Intelligent Embedded Processors. In 2020 HPCA. 315 327. [57] Attila Reiss and Didier Stricker. 2012. Creating and benchmarking a new dataset for physical activity monitoring. In PETRA, Fillia Makedon (Ed.). ACM. [58] Attila Reiss and Didier Stricker. 2012. Introducing a New Benchmarked Dataset for Activity Monitoring. In ISWC. IEEE. [59] Mohammad Rostami, Jeremy Gummeson, Ali Kiaghadi, and Deepak Ganesan. 2018. Polymorphic radios: A new design paradigm for ultra- low power communication. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication. 446 460. [60] Fernando Moya Rueda, René Grzeszick, Gernot A. Fink, Sascha Feld- horst, and Michael ten Hompel. 2018. Convolutional Neural Networks for Human Activity Recognition Using Body-Worn Sensors. Informat- ics (2018). [61] Rachid Saadane, Abdellah Chehri, Seunggil Jeon, et al. 2022. AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities. Sus- tainable Energy Technologies and Assessments 52 (2022), 102093. [62] Alanson P Sample, Daniel J Yeager, Pauline S Powledge, Alexander V Mamishev, and Joshua R Smith. 2008. Design of an RFID-based battery- free programmable sensing platform. IEEE transactions on instrumen- tation and measurement 57, 11 (2008), 2608 2615. [63] Himanshu Sharma, Ahteshamul Haque, and Zainul Abdin Jaffery. 2019. Maximization of wireless sensor network lifetime using solar energy harvesting for smart agriculture monitoring. Ad Hoc Networks 94 (2019), 101966. [64] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2020. DRQ: dynamic region-based quantization for deep neural network acceleration. In ISCA. IEEE. [65] Ben Taylor, Vicent Sanz Marco, Willy Wolff, Yehia Elkhatib, and Zheng Wang. 2018. Adaptive deep learning model selection on embedded systems. ACM SIGPLAN Notices 53, 6 (2018), 31 43. [66] Texas Instrument Micro-controller with FRAM 2022. 16 MHz MCU with 64KB FRAM, 2KB SRAM, AES, 12-bit ADC, comparator, DMA, 14 UART SPI I2C, timer. [67] Rambabu Vatti, Nagarjuna Vatti, K Mahender, Prasanna Lakshmi Vatti, and B Krishnaveni. 2020. Solar energy harvesting for smart farming using nanomaterial and machine learning. In IOP Conference Series: Materials Science and Engineering, Vol. 981. IOP Publishing, 032009. [68] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. 2018. NetAdapt: Platform- Aware Neural Network Adaptation for Mobile Applications. In ECCV. [69] Zhuoran Zhao, Kamyar Mirzazad Barijough, and Andreas Gerstlauer. 2018. DeepThings: Distributed adaptive deep learning inference on resource-constrained IoT edge clusters. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems 37, 11 (2018), 2348 2359. [70] Tiago Zonta, Cristiano André da Costa, Rodrigo da Rosa Righi, Miro- mar Jose de Lima, Eduardo Silveira da Trindade, and Guann Pyng Li. 2020. Predictive maintenance in the Industry 4.0: A systematic litera- ture review. Computers Industrial Engineering 150 (2020), 106889. 15 0 20 40 60 0 0.1 0.2 0.3 0.4 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 Reconstruction Error FFT Amplitude Frequency (Hz) Reconstructed Original Error Figure 14: An example of generator based coreset re- covery Figure 15: completion of the inference at the edge for bearing fault data with different EH source. Figure 16: Communication data volume with different number of clusters. Component Spec Power Area(mm2) SRAM Buffers 1kB 256 8kB 256 64kB 16 256kB 10.372W 117.164 MAC Unit (8 8) 256 8.46W 32.72 Adder Tree and Comparator 16 16bit 256 2.4W 21.556 Control 0.96W 12.2 Host Cortex A78 series 11W Design at 592MHz with Synopsys AED 32nm library Total 256 tiles 33.192W 183.64 Table 3: Area and power estimation of our design. A APPENDIX A.1 Reconstructing Importance Sampling Coreset As discussed in Section 3.2.2, we use GANs to recover the data we lost while performing importance sampling. This was motivated from the observation that as we selected more number of points in importance sampling, the accuracy of the inference on the compressed data increased significantly (at times by 2 ). Hence, the points which were not selected while performing importance sampling still had some impor- tance and can be represented as a function containing the low level nuances of the activity performed and the sensor state. The challenge was to learn this function, i.e. to de- vice a transformation function which can mimic the sensor signal given the aactivity and the sensor states. A similar problem, in terms of generating faces, paintings etc. given some latent space has already been solved using GANs [54]. Motivated by this, we designed a GANs to regenerate the lost data points while performing importance sampling. The latent space takes the activity, and the first and second order moments of the data sample to recreate the signal, and the Discriminator tried to distinguish between the generated sig- nal and the actual signal. The generator is tuned repeatedly until the discriminator could not distinguish the original and the generated signal. The GAN modeled the lost signals with a very high correlation ( 0.9 in most cases and 0.6 in some of the worst cases (refer Figure 14 for an example). In rare cases (once in over 2000 cases), the generator induced arti- facts which could result in wrong classifications. However, this error could be rectified with further fine tuning. A.2 More Results on Bearing Fault Data We repeated our experiments with similar experimental setup on the bearing fault data set [53]. The bearing fault data is sampled at a much higher frequency (48KHz) than the HAR data, and hence require a larger DNN, larger num- ber of importance sampling, and more number of clusters. We took the learning from multiple domain specific litera- tures [19, 29, 53] to isolate the frequency regions specific to the fault pattern to minimize the computations. But, because of the larger data volume, the number of computations per- formed at the edge diminished significantly (refer Figure 15). We also conducted an empirical study on number of clusters required, and found out that the bearing set data needs about 15 to 20 clusters to maintain the inference accuracy. The data volume communicated for different number of clusters is represented in Figure 13. B APPENDIX 16 -0.45 -0.24 -0.02 -0.07 -0.14 -0.19 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 0 50 100 Walking Climbing Cycling Running Jogging Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (a) Accuracy with MHEALTH dataset -0.26 -0.19 -0.76 -0.28 -0.04 -0.80 -0.60 -0.40 -0.20 0.00 0 20 40 60 80 100 Walking Climbing Cycling Running Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (b) Accuracy with PAMAP2 dataset Figure 17: Accuracy and communication efficiency of Seeker with different data sets and sensitivity study. 17\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nSynergistic and Efficient Edge-Host Communication for Energy Harvesting Wireless Sensor Networks Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandmeir, Vijaykrishnan Narayanan, Chita R Das {cyan, jms1257, mtk2, vijaykrishnan.narayanan, The Pennsylvania State University ABSTRACT There is an increasing demand for intelligent processing on ultra-low-power internet of things (IoT) device. Recent works have shown substantial efficiency boosts by execut- ing inferences directly on the IoT device (node) rather than transmitting data. However, the computation and power de- mands of Deep Neural Network (DNN)-based inference pose significant challenges in an energy-harvesting wireless sen- sor network (EH-WSN). Moreover, these tasks often require responses from multiple physically distributed EH sensor nodes, which impose crucial system optimization challenges in addition to per-node constraints. To address these chal- lenges, we propose Seeker, a hardware-software co-design approach for increasing on-sensor computation, reducing communication volume, and maximizing inference comple- tion, without violating the quality of service, in EH-WSNs co- ordinated by a mobile device. Seeker uses a store-and-execute approach to complete a subset of inferences on the EH sensor node, reducing communication with the mobile host. Fur- ther, for those inferences unfinished because of the harvested energy constraints, it leverages task-aware coreset construc- tion to efficiently communicate compact features to the host device. We evaluate Seeker for human activity recognition, as well as predictive maintenance and show 8.9 reduc- tion in communication data volume with 86.8 accuracy, surpassing the 81.2 accuracy of the state-of-the-art. 1 INTRODUCTION Innovations in low-power computing, artificial intelligence, and communication technologies have given rise to the gen- eration of intelligently connected devices that constitute the Internet of Things (IoT). Wireless sensor networks (WSNs), one of the prominent classes of IoT deployments, is currently dominating and expected to be pervasive impacting many application spaces [13] including, but not limited to, body area network [22, 47], industrial monitoring [34], predic- tive maintenance [70], commercial satellites[15] and smart farming [61].\n\n--- Segment 2 ---\n1 INTRODUCTION Innovations in low-power computing, artificial intelligence, and communication technologies have given rise to the gen- eration of intelligently connected devices that constitute the Internet of Things (IoT). Wireless sensor networks (WSNs), one of the prominent classes of IoT deployments, is currently dominating and expected to be pervasive impacting many application spaces [13] including, but not limited to, body area network [22, 47], industrial monitoring [34], predic- tive maintenance [70], commercial satellites[15] and smart farming [61]. Moreover, these WSNs are and further will be participating in producing rapid inferences to support the increasingly complex tasks enabled by machine learning (ML) algorithms [47, 68], often tweaked towards edge de- ployments, and applications of such edge-analytics is also ex- ploding with the user and market demands. This represents a particularly challenging tension between energy availability and desired functionality, because the form factor constraints of the WSNs fundamentally limit active power, energy re- serves, compute and communication capabilities. For many WSNs, their participation in inference tasks has traditionally been limited to data collection and transmission, sometimes with modest preprocessing. While several studies have shown the benefits of performing more inference closer to the point of data collection [23, 30, 31, 40, 56] and have ap- plied these techniques to more powerful edge devices, their form-factor-imposed limited energy storage, low-power op- eration points, and deployment scenarios have been a major impediment in executing compute-intensive inference tasks directly on such platforms. In contrast, communicating the data, often after little preprocessing, although popular, is not cheap in terms of power requirement, and often poses a challenge for remotely deployed and ultra low power WSNs. Prior works, trying to tackle this conflict between computa- tion, communication, power-requirement and quality of ser- vice (QoS), have pursued three major approaches: inference effort partitioning optimizations [22, 30, 31, 50], mitigation of energy provisioning limitations [24, 40, 43, 47, 56], and minimizing communication overheads [32, 33, 36, 37, 45].\n\n--- Segment 3 ---\nIn contrast, communicating the data, often after little preprocessing, although popular, is not cheap in terms of power requirement, and often poses a challenge for remotely deployed and ultra low power WSNs. Prior works, trying to tackle this conflict between computa- tion, communication, power-requirement and quality of ser- vice (QoS), have pursued three major approaches: inference effort partitioning optimizations [22, 30, 31, 50], mitigation of energy provisioning limitations [24, 40, 43, 47, 56], and minimizing communication overheads [32, 33, 36, 37, 45]. One of the most emerging line of work aims to solve the energy provisioning problem at the edge by integrating en- ergy harvesting (EH) to the sensor nodes while making them more capable performing complex compute intermittently, which has given rise to energy harvesting wireless sensor networks (EH-WSNs). Specifically, recent works [24, 43] pro- posed EH, along with compiler runtime optimizations and leveraging non-volatile processors (NVP) [40, 56], to increase local compute at the edge. EH as a solution has been partic- ularly interesting as a means to address the sustainability issue of battery backing trillions of future devices. More importantly, EH can help us build sustainable distributed sensing monitoring infrastructure at virtually inaccessible places like oil-wells, mines, and even satellite orbits [14, 49]. However, harvested energy is fickle in nature, and typically arXiv:2408.14379v1 [cs.AR] 26 Aug 2024 harvested sources only deliver scant microwatts of power (see Figure 1b for an overview). The sporadic nature of har- vested energy and the lossy nature of EH based storage and charging circuits calls for using the harvested energy di- rectly to perform intermittent compute rather than storing energy for some distant future use. On this front, recent works [43, 44, 47] have specifically optimized DNN infer- ence execution at the EH-edge nodes by utilizing adaptive dynamic check pointing, intelligent scheduling and ensem- ble learning. Given the limitations of the EH budget, such approaches typically end up dropping many samples and not inferring from them locally. Importantly, they are of- ten incapable of transmitting the raw data due to a lack of sufficient energy; for sensing tasks with modest inference requirements, performing inference and transmitting the result can take less energy than transmitting raw data.\n\n--- Segment 4 ---\nGiven the limitations of the EH budget, such approaches typically end up dropping many samples and not inferring from them locally. Importantly, they are of- ten incapable of transmitting the raw data due to a lack of sufficient energy; for sensing tasks with modest inference requirements, performing inference and transmitting the result can take less energy than transmitting raw data. How- ever, to unleash the remote deployment, and sustainable, yet pervasive, computing capabilities WSNs, development of ef- ficient energy harvesting WSNs (EH-WSNs), both for sensing and edge-analytics, plays an essential role. These wireless sensing (EH or otherwise) devices have long relied on compression techniques to mitigate data com- munication overheads [32, 33, 45]. However, when applied to low-dimensional sensor data, classical lossy compression techniques tend to discard or distort some important fea- tures, which significantly degrades the inference accuracy. To mitigate the shortcomings of classical compression tech- niques, recent works [7, 36, 37] propose using coresets, a data representation technique from computational geometry that preserves important, representative features when building a compressed form of the data, and thereby reducing the pay- load size while preserving data integrity for efficient edge communication. Although, with the help of coresets, one can efficiently offload minimal input representations to a more compute-capable device, performing accurate inference on coresets is non-trivial due to their low-dimensional nature. From the aforementioned challenges, it is evident that we need a concoction of both hardware-driven and software optimized solutions to build next-generation EH-WSNs with the ability to perform fine-grained intermittent computing, while ensuring efficient network communication. Towards this, we propose Seeker, a novel approach that leverages and extends coresets to efficiently execute DNN inference across a set of EH sensor nodes and a host mobile device. Seeker fo- cuses on building an efficient EH-WSN which can collabora- tively work to maximize the inferences performed at the EH- edge nodes. Furthermore, it then applies innovative coreset techniques to efficiently and intelligently offload unfinished compute tasks to a more capable host to further increase the inferences that can be performed.\n\n--- Segment 5 ---\nSeeker fo- cuses on building an efficient EH-WSN which can collabora- tively work to maximize the inferences performed at the EH- edge nodes. Furthermore, it then applies innovative coreset techniques to efficiently and intelligently offload unfinished compute tasks to a more capable host to further increase the inferences that can be performed. Particularly, Seeker aug- ments its coreset formation with application-awareness to form an energy aware, dynamically configured, and feature preserving payload with minimal communication footprint. Seeker provides hardware acceleration support for coreset formation to make them computationally efficient, adaptive, and accuracy-preserving specifically for EH-WSNs. The fol- lowing are the primary contributions of our work: Efficient Communication: We enable low data volume communication by developing extensions to traditional coresets that enhances their applicability to EH-WSN in- ference scenarios. Specifically, we introduce an activity- aware coreset construction technique to dynamically adapt to both activity and the available harvested energy, while conserving maximum features of the data. This reduces the communication payload size by 8.9 . We also propose a recoverable coreset construction technique, which helps reconstruct the original data from the compressed form with minimum (as low as 0.02 ) accuracy loss. Efficient Computation: We augment a state-of-the-art EH-sensor node with quantized DNNs to increase the num- ber of accurate inferences at the edge (by up to 40 ). We leverage data memoization to skip unnecessary compute saving inference execution time and energy. Efficient Hardware: We propose simple, low power, and low latency hardware to efficiently build coresets, further increasing the number of samples that can be inferred or transmitted under EH budget, and thereby significantly improving the accuracy over the state-of-the-art ( 5 ). We develop a non-volatile hardware accelerator, with mul- tiple quantization support, for efficient DNN inference. Adaptability: Although Seeker is meant for EH-WSNs, the coreset based data representation can easily be used in any commercial device for efficient communication. Detailed Evaluation: We provide a detailed evaluation of our system and the proposed hardware design.\n\n--- Segment 6 ---\nAdaptability: Although Seeker is meant for EH-WSNs, the coreset based data representation can easily be used in any commercial device for efficient communication. Detailed Evaluation: We provide a detailed evaluation of our system and the proposed hardware design. Our evalu- ations show that, even when powered by an unreliable EH source, Seeker s coreset-based optimizations result in bet- ter accuracy than that of a fully-powered system running a state-of-the-art classifier optimized for energy efficiency. Specifically, Seeker reaches 86.8 top-1 accuracy in com- parison to the 81.2 accuracy of the baseline system. 2 BACKGROUND AND MOTIVATION In this section, we provide a background of the current state- of-the-art in performing sensing and computations on EH- WSNs. We also describe the challenges in enabling complex compute on such devices and the need for hardware-software co-design to enable specialized intermittent computing in EH-WSNs. Finally, we define the scope of our work and focus on the problem specifics while alluding to probable solutions. Figure 1a shows the basic building blocks of an energy har- vesting sensing computing unit. The harvested energy is 2 typically stored in either an intermediate storage like a (su- per) capacitor [22], or used for charging. For building scalable and sustainable infrastructure of battery-free EH-WSNs, the former is more feasible and will be our focus for this work. The fickle nature of harvested energy has posed a major chal- lenge in performing any useful computation, as any useful forward progress gets lost when the traditional computing systems lose power. To tackle this, a significant amount of work has been done on check-pointing, and compiler level tweaks, which help maximize the forward progress on such devices [39, 43, 44]. These solutions operate on augmented commercial off the shelf micro-controllers [66], or specialized products with traditional architecture [62], and rely on their efficient prediction of power failure. While these software optimizations and judicious use of persistent storage works for smaller workloads like keyword spotting (e.g "Ok Google" detection), they are inefficient for complex workloads (e.g. multi-sensor HAR, predictive maintenance etc.).\n\n--- Segment 7 ---\nWhile these software optimizations and judicious use of persistent storage works for smaller workloads like keyword spotting (e.g "Ok Google" detection), they are inefficient for complex workloads (e.g. multi-sensor HAR, predictive maintenance etc.). These software-based solutions exhibit inefficiencies with respect to energy and time due to performing multiple save- and-restore cycles [23, 56]: while some of these operations are necessary, unnecessary checkpoints will also be conser- vatively performed to ensure forward-progress. Therefore, recent works [39 42, 56] propose the use of a NVP, where the non-volatility of the hardware itself takes care of saving and resuming the program execution. This reduces software overheads and latencies for handling power emergencies and hence can guarantee better QoS for complex and longer tasks even when power is deeply unreliable. Using an NVP and multiple harvested energy sources Qiu et al. [56] demon- strates the possibility of performing complex DNN inference at the EH-Sensor itself. While an NVP ensures safe check- pointing for a given computation, current edge scenarios may require a device to be simultaneously performing multi- ple functionalities [3 5, 25] and might be at energy scarcity. As a result, it is difficult to reliably run these complex tasks standalone on the edge device. Current devices adapt in one of three ways: 1) Send all the sensor data to a connected host device, or cloud, to offload the compute and act only as a sens- ing and display device; 2) Process data on the device itself, potentially dropping or delaying tasks due to energy short- falls; 3) A mix of the two models, where some computations do happen on the device while others are offloaded to balance compute, energy, and communication resources; and typically, the latter is preferred, but it is non-trivial to find the optimal balance between what is to be done on the edge, what to be offloaded [20, 31, 65, 69], and how to efficiently offload. Need for Specialized Hardware: One of the major chal- lenges in deploying learning tasks using EH-WSNs is to find the proper hardware platform.\n\n--- Segment 8 ---\nCurrent devices adapt in one of three ways: 1) Send all the sensor data to a connected host device, or cloud, to offload the compute and act only as a sens- ing and display device; 2) Process data on the device itself, potentially dropping or delaying tasks due to energy short- falls; 3) A mix of the two models, where some computations do happen on the device while others are offloaded to balance compute, energy, and communication resources; and typically, the latter is preferred, but it is non-trivial to find the optimal balance between what is to be done on the edge, what to be offloaded [20, 31, 65, 69], and how to efficiently offload. Need for Specialized Hardware: One of the major chal- lenges in deploying learning tasks using EH-WSNs is to find the proper hardware platform. The current commercial- of-the-shelf (CotS) hardware capable of performing such Harvester Ambient Energy AC-DC Converter Impedance Matching DC-DC Converter Controller Power Management Conditioning Energy Storage Sensor Compute (a) High-level overview of an energy harvesting sys- tem. Communicate elsewhere 0-50 50-500 500-1k 1k-10k 10k Harvested available power in the sensor node (µW) Compute at the edge COTS high-end wearables (bat.) Bonito (multiple) Origin (RF) ResiRCA (RF Piezo Solar) Chinchilla (RF) Ideal Solution (Any) COTS cheap wearables (bat.) Seeker (RF) All compute at edge Partial compute at edge Design space of Seeker (b) Current state-of-the-art of EH-WSN. Figure 1: A primer on energy harvesting systems: Fig- ure 1a shows the basic building blocks of an EH node equipped with sensing and computation. Some of the units change according to the harvested energy source. Figure 1b shows the capabilities of the current SOTA. The size of the circle representing the solutions de- picts the compute capabilities of the sensor nodes, the shade shows the available power, and their position on the axes approximates the amount of compute done on the node and the amount of reliability on external communication.\n\n--- Segment 9 ---\nFigure 1b shows the capabilities of the current SOTA. The size of the circle representing the solutions de- picts the compute capabilities of the sensor nodes, the shade shows the available power, and their position on the axes approximates the amount of compute done on the node and the amount of reliability on external communication. The power source is denoted in (Red) (notations used in Figure 1b: COTS: Commercial-off- the-shelf, Bat. : Battery, Bonito [22], Chinchilla [43], ResiRCA [56], Origin [47]) compute are not energy efficient to run with all modali- ties of harvested energy since all of them do not have the same energy income (see Figure 1b). For example, there has been significant work on enabling solar powered smart farm- ing [63, 67], but the same can not be done for smart manufac- turing due to the lack of solar exposure and the low fidelity of the available EH sources such as vibration and RF (from WiFi or other sources). To estimate the required energy, we ran simple HAR inferences (optimized version of [26] for edge deployment using [68]) on an Adafruit ItsyBitsy nRF52840 Express - Bluetooth LE [2] and found it to be consuming from 550mJ to 1.6J of energy (depending on the quantiza- tion). Compared to this, body movement and WiFi sources (the possible modalities of harvesting for HAR) harvests in order of milliwatts [22, 56], making it almost impossible to have a feasible EH-WSN deployment, with the capabilities to perform modest learning tasks, using the CotS. Therefore, there has been a significant body of work [40, 42, 47, 56] on developing appropriate next generation hardware (most of 3 them on simulation). Although, we evaluate and show the communication cost savings of Seeker on the battery backed CoTS hardware, we propose possible (simulated) hardware accelerator designs to fully deploying a EH-WSN capable of performing inference, compression and communication in harvested energy budget.\n\n--- Segment 10 ---\nTherefore, there has been a significant body of work [40, 42, 47, 56] on developing appropriate next generation hardware (most of 3 them on simulation). Although, we evaluate and show the communication cost savings of Seeker on the battery backed CoTS hardware, we propose possible (simulated) hardware accelerator designs to fully deploying a EH-WSN capable of performing inference, compression and communication in harvested energy budget. Complex Compute on EH-WSNs: To quantify the scope performing complex compute using EH-WSNs, we took hu- man activity recognition (HAR) as a workload1, and per- formed experiments on the MHEALTH data-set [9, 10] (see Section 5 for data-set details) using the DNNs proposed in [26, 60], an energy harvesting friendly DNN hardware accelerator [56] (to ensure that we are using the state of the art EH-WSN hardware) and recently proposed HAR- specific optimizations for EH systems [47]. Our analysis (see Figure 2a) shows that the state-of-the-art system still only finishes 58.7 of the inferences scheduled on a sensor. Al- though accuracy can increase by further tuning duty-cycle, as shown in Figure 2b, the returns are diminishing, and in- definite increase of duty cycle is also not an option as that might lead to skipping important data to infer. We observe that the system used in [47] does not aggressively employ quantization, which is a commonly used technique [64] to reduce both compute and transmission energy in DNN tasks. Our analysis, as shown in Figure 2c, shows accuracy as a function of quantization (we took the approach of perform- ing post training quantization and fine-tuned the DNN to work with reduced bit precision instead of training the DNN from scratch with a reduced precision). The quantized DNNs benefit from lower compute and memory footprints, but need specialized fine-tuning and often suffer from lower ac- curacy. Similarly, other approximation-via-data-reduction techniques, such as sub-sampling, did not perform inference with a desirable accuracy. Collectively, the aforementioned figures demonstrate that the harvested energy budget is insuf- ficient to perform all inferences with acceptable accuracy on currently proposed EH-WSN systems.\n\n--- Segment 11 ---\nSimilarly, other approximation-via-data-reduction techniques, such as sub-sampling, did not perform inference with a desirable accuracy. Collectively, the aforementioned figures demonstrate that the harvested energy budget is insuf- ficient to perform all inferences with acceptable accuracy on currently proposed EH-WSN systems. Therefore, to complete all the scheduled computations, and thereby to improve ac- curacy, the system must rely on another device (e.g. a mobile phone), where sufficient resources are available to complete any remaining inference, if the data can be sent from the sensor. Said coordinating device completes the rest of the computations and finally, aggregates them with the ones completed in the sensor nodes. The challenge here is to send 1Throughout the paper we evaluate many of our motivation results using HAR as a workload as it is one such application, where the (EH-)WSN, used as body area network, fits perfectly with RF or body movement as the har- vesting source. HAR has the nuances of human introduced unpredictability and sensor induced noises. HAR has been pervasive enough given the rise of smart wearables and has been studied well enough to have ample access to resources to make a judicious evaluation. Further, we also evaluate one more emerging application from predictive maintenance domain. Algorithm Compression Ratio Accuracy Loss ( ) Fourier Decomposition 3 - 5 9.1 - 18.3 DCT 3 - 5 5.8 - 16.2 DWT 3 - 6 5.3 - 12.7 Coreset 3 - 10 0.02 - 0.76 Table 1: Accuracy trade-off of different compression techniques: Low-dimensional data loses important fea- tures under lossy compression, dropping inference ac- curacy significantly compared to the original data. De- tails on Coreset are available on Section 4. (Notations used: DCT: Discrete Cosine Transform, DWT: Discrete Wavelet Transform.\n\n--- Segment 12 ---\nDe- tails on Coreset are available on Section 4. (Notations used: DCT: Discrete Cosine Transform, DWT: Discrete Wavelet Transform. 29.14 44.76 52.29 58.67 70.86 55.24 47.71 41.33 0 20 40 60 80 100 RR3 RR6 RR9 RR12 Scheduled Computation Completed Failed (a) Completion with ERR 0 20 40 60 80 100 Walking Climbing Cycling Running Jogging Jumping Accuracy ( ) RR3 RR6 RR9 RR12 Baseline (b) Accuracy of ERR 0 10 20 30 40 50 60 70 80 Walking Climbing Cycling Running Jogging Jumping Accuracy ( ) Quantization Level 16b Quantization Level 12b Quantization Level 8b (c) Accuracy vs quantiza- tions 0 20 40 60 80 100 Walking Climbing Cycling Running Jogging Jumping Accuracy ( ) Sampling with Probability Weighted Sampling Baseline (d) Accuracy vs sub- sampling Figure 2: Accuracy comparison of various classical node-level optimization techniques. The Extended- Round-Robin policy (ERR) [47] takes a store-and- execute approach, and the number associated repre- sents the ratio of store cycles vs execute cycles (e.g. RR3 is 3 store cycles followed by 1 execute cycle). The Baseline model is a fully powered system with no energy restrictions, and the quantized model runs on harvested energy using a RR12 policy. the data efficiently, since communication is an expensive task and especially challenging for EH-WSNs [22] thanks to their fickle and ultra-low energy budget. The obvious solution is to reduce the communication data volume by compress- ing the data before transmitting. This also reduces energy footprint and the probability of data packet loss. Challenges with Data Compression: Using standard compression algorithms, like discrete cosine transform, dis- crete wavelet transform, and Fourier decomposition etc., to minimize the communication overhead is not a viable solu- tion [45]. This is partly because we need a very high compres- sion ratio with very low power. Secondly, these compression algorithms are not context-aware, and hence lose relevant 4 features during the process of compression resulting in de- graded inference accuracy (refer Table 1 for details). A key insight is that, while these compression techniques work very well for high dimensional data (e.g.\n\n--- Segment 13 ---\nSecondly, these compression algorithms are not context-aware, and hence lose relevant 4 features during the process of compression resulting in de- graded inference accuracy (refer Table 1 for details). A key insight is that, while these compression techniques work very well for high dimensional data (e.g. images), inference on low-dimensional sensor data (such as inertial measure- ment unit or IMU vibration data) is much more sensitive to lossy compression as separating between features might be difficult to do. And we will not achieve a sufficient com- pression ratio from lossless approaches either. Therefore, the standard data compression techniques are not very useful, let alone their energy efficient (such as quantized versions [33]) counterparts. For data compression in EH-WSNs, we need the compression algorithm to be 1 light weight (for energy efficiency), 2 feature preserving (for higher accuracy), 3 having a high compression ratio (for communication effi- ciency), and 4 context agnostic (for better generalization); i.e., our deployment scenario demands a smaller representa- tive form of the data that still preserves enough application- specific features to perform meaningful classifications in a given DNN. Why Coresets? The aforementioned requirements moti- vate us to consider coresets for forming representations of the original data. Coresets, primarily used in computational geometry [7], have been recently used [36, 37] for machine learning and sensor networks. Since coresets were designed to preserve the geometry of the data, we believe that they can be crafted to preserve features, and therefore be use- ful for performing accurate inference in subsequent stages, and thereby satisfying 2 . Furthermore, constructing core- sets do not need any application information, i.e. they are application data agnostic and can represent any form of data (IMU [36], Image [55], DNN feature map [17, 38, 46, 52]). This fulfils requirement 4 . They are also an effective way to con- struct a representation of the data set with high compression ratios [8, 17] without incurring unacceptable accuracy losses and thus useful for achieving 3 . For the DNNs in question, coresets can achieve sufficient compression ratios to make communication energy-competitive with computation, as well as opening up new opportunities for optimizing DNN in- ference on the coreset, rather than original data.\n\n--- Segment 14 ---\nThey are also an effective way to con- struct a representation of the data set with high compression ratios [8, 17] without incurring unacceptable accuracy losses and thus useful for achieving 3 . For the DNNs in question, coresets can achieve sufficient compression ratios to make communication energy-competitive with computation, as well as opening up new opportunities for optimizing DNN in- ference on the coreset, rather than original data. Finally, most of the coreset construction algorithms are simple (hence can achieve 1 ) and do not need complex operations (like cosine, exponential, etc. [7, 8, 36, 37], they can also be quantized [37] to further reduce their computation and memory footprints. Motivated by this, we explore possibilities of designing an efficient synergistic sensor-host ecosystem (involving the EH-WSNs and host), where we try to maximize the compute at the sensor nodes, yet for the incomplete tasks, we use coresets to compress and send the data to the host where the rest of the computations could occur. H S C M H: Harvest S C: Sense Compute M: Communicate EH Sense Compute EH Sense Compute EH Sense Compute Decompress Infer Ensemble Sensor state transition sensor Host Legend: Figure 3: An example of EH Sensor-Host ecosystem - the sensor transitions between multiple states and executes the compute as store and execute fashion [47]. The host receives the data in compressed form for the unfinished portion, decompresses it, runs inference and finally ensembles the results from multiple sensors to improve accuracy and robustness. 3 DESIGN SPACE EXPLORATION Since data communication in a sensor host ecosystem (Fig- ure 3) consumes substantial power, we rely on coresets as an efficient way to lossily communicate the features with minimal information degradation. The coreset construction techniques need to be extremely lightweight while preserv- ing key features to justify the computation-communication trade-offs in energy and latency. To this end, we explore two different kinds of coreset construction techniques. 3.1 Coreset Construction Techniques Coreset Construction Using Importance Sampling: An easy way to build a representation from a data distribution is to perform importance sampling [7, 8], i.e. give more importance in choosing the data which are unique and, in our case, contribute significant to the inference (i.e.\n\n--- Segment 15 ---\n3.1 Coreset Construction Techniques Coreset Construction Using Importance Sampling: An easy way to build a representation from a data distribution is to perform importance sampling [7, 8], i.e. give more importance in choosing the data which are unique and, in our case, contribute significant to the inference (i.e. having a high enough magnitude in the frequency response of the sensor signal). The intuition is that any importance sampling scheme produces an unbiased estimator [8]. To preserve the temporal and frequency features, we ensure sampling data which are far enough from each other to build a better representation. The entire process of importance sampling uses simple arithmetic operations and is therefore viable in energy-scarce situations. The host can take the sub-sampled data and perform inference. The caveat is to have a model trained on the sub-sampled data, which can be done as an one-time step. Although the sub-sampling might lead to poor inference accuracy, in our experiments, with iso-compression ratio, importance sampling based coresets still outperforms classical compression techniques. Figure 4 shows a toy example of importance sampling in a 2D data set. Observe that the selected points (in red) are approximating the original distribution. 5 r2 r1 r4 r3 r5 Original Data Coreset with imp-sampling Coreset with Clustering Figure 4: A toy example of the coreset construction techniques in Seeker. Imp-sampling uses a probability based importance sampling; clustering preserves the geometric shape of the original data. In each case, the points values in red are communicated to the host. Coreset Construction Using Clustering: Although im- portance sapling based coreset construction is computation- ally inexpensive, it suffers from accuracy loss because it doesn t explicitly preserve the intricate structure of the data points. To address this, we also utilize coreset construction using k-means clustering [8, 36, 37], which separates the data points into a set of k (or fewer) N-spherical clusters and represents the geometric shape of the data by using the cluster centers and cluster radii (Fig. 4). These are then communicated to the host device for inference.\n\n--- Segment 16 ---\n4). These are then communicated to the host device for inference. Since clus- tering better preserves the geometry of the distribution, we observe that inferences with coresets constructed using clus- tering are more accurate than using importance sampling, and therefore can be preferred over the former whenever there is enough energy. 3.2 Communication vs Accuracy We can tune the aforementioned coreset construction tech- niques allow a variable number of features depending on the available energy, i.e. for importance sampling, we can limit the number of points to choose, and similarly, for clustering we can limit both the number of clusters and the number of iterations. However, even after preserving important fea- tures, the constructed corests are lossy representation of the original data. Therefore, when performing inference on the compressed coresets representation, the inference ac- curacy goes down, albeit not significant compared to other lossy compression methods (we can again refer to Table 1 for the relevant comparisons). This leaves an optimization space in trading between communication cost vs. accuracy, i.e. whether to construct strict and low-volume coresets and lose accuracy or to preserve maximum data points and pay for the communication cost. We perform an analysis on the MHELATH [9, 10] data set (we take a overlapping moving window of 60 data points sampled at 50Hz from 3 different IMUs, overlap size: 30 data points) to find a trade-off between the coreset size (directly related to the communication cost) and the inference accuracy. Empirically, we observe that ac- curately preserving the features for each class requires 20 data points using importance sampling or 12 clusters (see Fig. 6) using clustering based techniques. Going above 12 clusters did not significantly improve accuracy. This further motivates us to look for opportunities in the data distribution to improve the compression ratio. As the DNN models were designed to infer on the full data, we retrain the DNN models to recognize the compressed rep- resentation of the data and infer directly from that (both from the importance sampling and clustering). As the coreset for- mation algorithms are fairly simple [7, 8, 36, 37], it does not take much latency or energy to convert the raw sensor data into the coreset form even while using a commercial-off- the-shelf micro-controller (like TI MSP430FR5969 [66]).\n\n--- Segment 17 ---\nAs the DNN models were designed to infer on the full data, we retrain the DNN models to recognize the compressed rep- resentation of the data and infer directly from that (both from the importance sampling and clustering). As the coreset for- mation algorithms are fairly simple [7, 8, 36, 37], it does not take much latency or energy to convert the raw sensor data into the coreset form even while using a commercial-off- the-shelf micro-controller (like TI MSP430FR5969 [66]). This allows the EH-sensor to opt for coreset formation followed by data communication to the host device as an energy-viable alternative to local DNN inference on the original data. In our example case, transmitting the raw data (60 data points, 32bit floating point data type) needs 240 Bytes of data trans- fer, and with coreset construction and quantization we can limit it to 36 Bytes (for 12 clusters, each cluster center is represented by 2 Bytes of data, and radius represented by 1 Byte data), thereby reducing the data communication volume by 85 . The host runs inference on the compressed data to detect the activity (with an accuracy of 76 ). However, due to this reduced accuracy, the sensor only takes this option iff it does not have enough energy to perform the inference at the edge device (either in the 16bit or 12bit variant of the DNN - more details on DNN design is presented in Sec- tion 4). This raises a question: is it possible to generate a more useful approximation, via reconstruction, of the data that we lost while forming the coresets? This problem has not been explored in details, as coresets are typically considered as an 𝛼 approximate representation of the data (𝛼being the error approximation parameter) [7] and never needed proper recovery. However, thanks to the low dimensional nature of many sensor data, reconstruction of original data from coresets becomes an essential step. 3.2.1 Data Memoization: Given our focus on ultra low power energy harvesting devices, any opportunities to re- duce computation and communication can noticeably aug- ment the performance and efficiency of the entire system. We look into data memoization as one such opportunity. For two instances of the same class, there should be a very high correlation in the sensor data.\n\n--- Segment 18 ---\nWe look into data memoization as one such opportunity. For two instances of the same class, there should be a very high correlation in the sensor data. We empirically measure this by testing for correlation between the sensor signatures of different classes. Conservatively, we choose a correlation coefficient 0.95 to predict that the two activities are the same, and hence skip the inference altogether and just com- municate only the results to the host. We store ground truth sensor data pattern for all possible labels, and when new data arrives, we find the correlation of the sampled data against the ground truth data, and if any of the correlation coefficient 6 Power-Pred Decision Logic (MCU) Correlation Sensor Data 16bit DNN (x-bar) 12bit DNN (x-bar Coreset: Imp Smp Clust. Wireless Communication H S C M Harvestor Sensor Node EH Sense Compute EH Sense Compute EH Sense Compute Host Seeker Ecosystem Coreset Reconstruct DNN for Recovery DNN for Inference Ensemble Engine Cluster Recovery Classfied Results Figure 5: Overall system design of Seeker comes out to be 0.95, we choose to ignore further infer- ence computation and only communicate the classification result to the host for further processing. Note that choosing the correlation threshold entirely depends on the application and user preference. 3.2.2 Recoverable Coreset Construction: The primary reason the accuracy of inferring on coreset data is lower than that of the original model is the loss of features. Typically, the sensor data are low dimensional, and hence even with a good quality of coreset construction, it is difficult to preserve all the features. However, while inferring at the host, if we are able to recover the data or reconstruct it with minimum error, the accuracy can easily be increased. Clustering Coreset Recovery: Clustering preserves the geometry of the original data by representing them as a set of N-spherical clusters represented with a center and a ra- dius. In the process of coreset construction we only preserve the coordinates of the centers and the radii of the clusters, and hence miss the coordinates of the points inside the clus- ters.\n\n--- Segment 19 ---\nClustering Coreset Recovery: Clustering preserves the geometry of the original data by representing them as a set of N-spherical clusters represented with a center and a ra- dius. In the process of coreset construction we only preserve the coordinates of the centers and the radii of the clusters, and hence miss the coordinates of the points inside the clus- ters. However, any random distribution of the lost points in the cluster could provide us with a 2𝑟 approximate repre- sentation of the original distribution (where 𝑟is the radius of the cluster; refer Figure 7a for a toy example). However, to achieve this, we need some extra information about the clus- ters. The standard method of clustering-based coreset con- struction keeps the cluster center and cluster radius, which gives the geometrical shape of the entire data. Extending this with the point count for each cluster allows for recon- struction of data in the original form that can be processed by DNNs trained on full-size data. These reconstructed data sets can be synthesized simply by uniformly distributing the points within each cluster. Although the intra-cluster data distribution will be different from the original, it will still preserve the overall geometry with a certain degree of approximation which the DNN could learn to accommodate. Experimentally, on the MHELATH dataset, we observe that inferring on the synthesized reconstructions of cluster 0 20 40 60 80 100 Walking Climbing Cycling Running Jogging Jumping Accuracy k 12 (Baseline) K 15 k 10 k 8 k 6 Figure 6: Accuracy with different clusters (k). Clustering Recovery Original Data Coreset Recovered Data (a) Recovering a cluster with uniform random re-distribution. Latent Space G Generator Noise D Discriminator Generated Sample Actual Sample Recovered Signal Close to actual? Finetune (b) Recovering a sub-sampling with GAN. Figure 7: Recovering data from the coresets. based coresets can achieve an accuracy of 85 . The recon- struction feature at the host comes with little to no overhead for the host (given the host devices have considerably more compute than the sensor nodes).\n\n--- Segment 20 ---\nbased coresets can achieve an accuracy of 85 . The recon- struction feature at the host comes with little to no overhead for the host (given the host devices have considerably more compute than the sensor nodes). The addition of the recovery parameter (number of points per cluster) needs 4 more bits (in our experiments, we never observe any clusters having more than 16 data points) of data per cluster, bringing the total data communication volume to 42 Bytes, which is still a significant 5.7 less in comparison to the original 240 Bytes needed to communicate the raw data in our setup. However, since clustering based coreset construction is more expensive than the importance sampling based coreset construction, it is not always possible to build a recoverable coreset at the edge, unless we figure out a to recover the lost points while we perform importance sampling. Importance Sampling Coreset Recovery: Unlike cluster- ing, when we construct a coreset with importance sampling, we typically have no information regarding the lost data points. We hypothesize that the dropped sample should con- tain, although not important, sensor specific artifacts. And these artifact must have some pattern, if modeled correctly, could represent the lost data. Towards this, we designed and trained a generative adversarial network (GAN, see Figure 7b for the structural details) to recover the lost samples of the importance sampling. As training parameters, we provide some statistical parameters (specifically mean and variance) 7 of the signal and random noise to the generator, and the generator generates the lost signals. The discriminator tries to discriminate between the actual data and the synthesized data. We fine-tune the network until the discriminator is fooled sufficiently to distinguish between the original data and the recovered data. Considering the fact that we do have access to the sensor data to train the learning algorithm, we can use the same data to train the GAN and with sufficient data, the discriminator could generate the lost signal with minimum error. Our experiments show that the deviation from the original signal, in most cases, is limited to 15 . However, in some pathological cases, the error at times goes close to 60 , and we believe them to be generated artifacts which are common side effects of the GANs[11]. Our experi- ments suggests that inference on the GAN recovered signal is almost as good as (about 2 4 difference in accuracy) the inference on the recovered cluster signal.\n\n--- Segment 21 ---\nHowever, in some pathological cases, the error at times goes close to 60 , and we believe them to be generated artifacts which are common side effects of the GANs[11]. Our experi- ments suggests that inference on the GAN recovered signal is almost as good as (about 2 4 difference in accuracy) the inference on the recovered cluster signal. The recovery policy can be implemented as a simple generator network in the host. Although, the training of the GAN is complex and involves multiple networks as well as hyper-parameters tun- ing, the generator network itself is very small (few hundred thousands of parameters depending on the sensor data). 4 DESIGN IMPLEMENTATION OF SEEKER By leveraging the coreset construction techniques discussed in Section 3, we design Seeker: A synergistic sensor host ecosys- tem. Figure 5 gives a pictorial representation of the overall design of Seeker and its various components. Seeker lever- ages the concept of NVP, and employs a flexible store and execute method using the state of the art ReRAM crossbar architecture [47] to perform inference at the edge. It aug- ments the sensor nodes with two different quantized DNNs (16 bit and 12 bit) to increase the number of completed in- ferences at the sensor node itself. Prior studies [56, 64, 68] and our empirical analysis on the quantization vs accuracy trade-offs (see Fig. 2c) indicate the 16 and 12bit precision to maximize the accuracy of the inferences while minimizing the energy consumption. Moreover, we also implement the memoization option so that it does not have to repeat infer- ences if it encounters similar data, thereby saving substantial energy as well as delivering results with extremely low la- tency. However, even with all these optimizations, due to the fickle nature of EH, Seeker cannot finish all the inferences at the edge and must communicate with a host device. To minimize the data communication overhead between the sensor-node and the host device, Seeker utilizes coresets to build representative, yet compressed, forms of the data. To cater towards the fickle EH budget, we use the two dif- ferent coreset construction techniques, described in Section 3: a cheaper, less accurate formation (importance sampling) and a more expensive, yet accurate formation (K-means).\n\n--- Segment 22 ---\nTo minimize the data communication overhead between the sensor-node and the host device, Seeker utilizes coresets to build representative, yet compressed, forms of the data. To cater towards the fickle EH budget, we use the two dif- ferent coreset construction techniques, described in Section 3: a cheaper, less accurate formation (importance sampling) and a more expensive, yet accurate formation (K-means). Trans- mitting coresets rather than raw data greatly improves the energy efficiency of communication to the host, when re- quired, and effectively increases the number of completed inferences, thereby increasing overall accuracy. Depending on the incoming data and the EH budget, the sensor decides whether to skip compute, perform an inference at the edge, or form a coreset to offload the inference to the host. The host, after obtaining information from multiple sensors, per- forms any further required computation and uses ensemble learning [47] to give an accurate classification result. Note that, unlike prior EH-WSN systems [47], the role of the host device here is not limited to just result aggregation; rather, the host participates and performs inference when the sen- sors do not have enough energy and choose to communicate the data (in the form of coresets) to the host. In this section, we will explain, in detail, the overall execution workflow of the Seeker system, followed by the the detailed design of the hardware support to maximize its energy efficiency. 4.1 Decision Flow: From Sensors to the Host Figure 8 depicts a flow chat showing the decision process taken in the sensor nodes to navigate between each compo- nents. Each sensor has a data buffer that collects the data points for classification (implemented using a 60 3 FIFO structure of 4Byte cells to store the floating point data. The 3 caters towards the multiple channels of the sensor. The moving window is designed using a counter to shift the streaming data.) The sensor also stores one ground truth trace for each activity. The sensor computes the correlation ( 1a ) between the stored ground truth and the current data. If the correlation coefficient is 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑( 1b ) the sensor skips all computation and sends the result to the host.\n\n--- Segment 23 ---\nThe sensor computes the correlation ( 1a ) between the stored ground truth and the current data. If the correlation coefficient is 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑( 1b ) the sensor skips all computation and sends the result to the host. Oth- erwise, the sensor prioritizes local computation and, with the help of a moving average power predictor [47], predicts whether it can finish the quantized DNN inference with the combination of stored energy and expected income ( 2a and 2b ). If energy is insufficient for DNN inference, the sensor will use coreset formation to communicate the important features to the host, which completes the inference. Since the clustering based coreset is typically more accurate then those formed by importance sampling, the former is pre- ferred, when possible. We increase the frequency of cluster- based formation by using custom, energy efficient hardware. With the help of an activity-aware and recoverable coreset construction and low-power hardware design, we can effi- ciently communicate inferences or compressed data to the host device with minimum power and latency overheads. Seeker, accounting for the available energy budget, con- siders the following decisions: D0: Test for data similarity using correlation, and if similarity is found then communi- cate the results to the host; D1: DNN at sensor with raw data Communicate the results to the host; D2: Try Quantized 8 Start Current Data Last Data Correlation Power Predictor Control Quantized DNN? Clustering send? Importance Sampling Send? DNN Inference No Similar? Send Importance Sampling Coreset Construction Clustering Coreset Construction AAC Last Classification Result End Yes Yes No Yes No Yes No 1a 2a 1b 2b Figure 8: Decision flow of Seeker. Strategy Sensor Energy Comm.\n\n--- Segment 24 ---\nSend Importance Sampling Coreset Construction Clustering Coreset Construction AAC Last Classification Result End Yes Yes No Yes No Yes No 1a 2a 1b 2b Figure 8: Decision flow of Seeker. Strategy Sensor Energy Comm. Energy Total Energy Avg Acc ( ) D0 0.54 8.27 8.81 D1 29.23 8.27 37.5 80.03 D2 16.58 8.27 24.85 77.37 D3 1.07 15.97 17.04 78.30 D4 0.87 15.97 16.84 85.30 raw data 70.16 70.16 87.23 Table 2: Energy breakdown of different Seeker strate- gies (in 𝜇Joules). The accuracy reported is the average case over 1000 inferences. DNN inference and communicate the results to the host; D3: Clustering based coreset construction at the sensor, and com- municate the coreset to the host; host runs DNN inference on the reconstructed data; and D4: Importance sampling based coreset construction at the sensor and communicate the coresets to the host; host recovers the original data with the pre-programmed generator, and performs inference with the recovered data. Table 2 lists the energy requirements of each of these decisions. 4.2 Efficient Hardware Accelerator Energy harvesting brings challenges in both average power levels and power variability. Performing DNN inference un- der such conditions often limits exploitation of inherent DNN parallelism within the energy budget. Therefore, many prior works use custon DNN accelerators, typically based on (non- volatile) resistive RAM (Re-RAM) based [47, 56] crossbar architecture, to perform DNN inference on EH-sensor nodes. Seeker s inference engine follows the design proposed in ResiRCA [56] and modifies it to cater towards new quanti- zation requirements: We have two different instances of the Re-RAM crossbar in our system - one for the 16bit model and one for the 12-bit model. The nonvolatile nature of the Re- RAMs helps in performing intermittent computing with the harvested energy. Moreover, techniques like loop tiling and partial sums [56] can further break down the computation to maximize forward progress with minimum granularity.\n\n--- Segment 25 ---\nThe nonvolatile nature of the Re- RAMs helps in performing intermittent computing with the harvested energy. Moreover, techniques like loop tiling and partial sums [56] can further break down the computation to maximize forward progress with minimum granularity. While DNN inference is already accelerated in the sensor node, the addition of a general-purpose processor for coreset 0.00 0.50 1.00 1.50 2.00 2.50 3.00 0.00 2.00 4.00 6.00 8.00 10.00 5 6 7 8 9 10 11 12 15 Accelerator energy in µJ M4 Energy in mJ Clusters Clustering Energy in M4 (mJ) Clustering Energy in Accelerator (uJ) Figure 9: Energy consumption comparison of the clus- tering accelerator vs the SoTA hardware computation would be energy inefficient. Specifically, the computation requirement is fixed and does not require the overheads to support generality. Therefore, we add a low power, low latency coreset construction hardware. Compar- ing the energy consumption of the proposed clustering accel- erator with Adafruit ItsyBitsy nRF52840 Express - Bluetooth LE [2] suggests the accelerator to be 3.7 103 times en- ergy efficient (refer Figure 9). Both the coreset construction algorithms follow a sequence of multiply and add subtract operations followed by averaging, and hence can be simply designed with few logic units. Moreover, as we are operating with lower data volume, these operations can be parallel (for example, the clustering hardware simultaneously works on all the cluster formations). The bigger challenge is posed by the requirement of a variable number of iterations for these algorithms to converge, the number of clusters samples re- quired, etc. To efficiently design the hardware and configure its parameters, we run several experiments and empirically arrive at the following conclusions: 1. The clustering finishes within 4 iterations and, for importance sampling, it takes up to 7 iterations. 2. None of the clusters have more than 16 points during any clustering. 3. We need not store all the points in either cases at every iteration, rather the clustering hardware needs to store the sum, the radii of the clusters, the number of points per cluster, and the importance sampling hardware needs just the points. 4. Storing the radii helps in easily selecting the points in the subsequent iterations.\n\n--- Segment 26 ---\n4. Storing the radii helps in easily selecting the points in the subsequent iterations. 5 EVALUATION AND RESULTS In this section, we describe our methodology to evaluate Seeker. We start with implementing Seeker using the CotS Adafruit ItsyBitsy nRF52840 Express [2] as the sensor com- pute node and a Google Pixel 6 Pro as the host node. Further, we describe how the design performs in simulated state-of- the-art hardware accelerator specifically designed for EH purpose. We look into two different applications: multi sen- sor human activity recognition (HAR), and bearing fault detection for predictive maintenance and compare the re- sults with multiple baselines designed for ultra-low-power as well as EH environments. 9 0 10 20 30 40 50 60 70 80 50 55 60 65 70 75 80 85 90 Raw Data DCT DWT 11 Clusters 12 Clusters IS (20 pts) Energy Consumed (mJ) Accuracy ( ) Accuracy at Host Energy Consumed (mJ) 1.00 2.61 2.86 6.49 5.71 6.00 Compression Ratio w.r.t. raw data Figure 10: Seeker vs other compression techniques 5.1 Seeker on Commercial Hardware Although Seeker is designed for EH-WSNs, the efficient com- munication mechanism for low dimensional sensor data can still be useful for the current commercial devices. Most of the ultra-low-power ( 30𝑚𝑊) micro-controllers are not equipped with complex multiply-accumulate units to effi- ciently perform DNN computations, and hence are suited to collect, compress and send the data to a host (Pixel 6 Pro) and then the host decompresses (or recovers) the data and performs the inference. We used the inertial measurement unit data of MHEALTH [10] dataset as the sensor data, which is pre-processed and compressed at the Adafruit compute node and then sent over Bluetooth low-energy to the host. We used Circuit Python [12] and Mu Editor [51] to imple- ment the compression and the communication algorithms in the Adafruit board, and TensorFlow lite [1] to deploy the DNN inference at the host.\n\n--- Segment 27 ---\nWe used the inertial measurement unit data of MHEALTH [10] dataset as the sensor data, which is pre-processed and compressed at the Adafruit compute node and then sent over Bluetooth low-energy to the host. We used Circuit Python [12] and Mu Editor [51] to imple- ment the compression and the communication algorithms in the Adafruit board, and TensorFlow lite [1] to deploy the DNN inference at the host. We evaluate the efficiency, both in terms of compression ratio, energy consumption and accuracy preservation, of the recoverable clustering and recoverable importance sampling algorithms against three other popular methods: 1) sending raw data without com- pression; 2) compression using DCT; 3) Compression using DWT. We measure the energy consumption and inference accuracy over 1000 iterations to provide an average fair es- timate. As depicted in Figure 10, Seeker out performs both DCT and DWT in compression ratio, and the recovery fea- ture of Seeker helps preserving inference accuracy close to the original raw data. 5.2 Seeker for Activity Recognition Human Activity Recognition (HAR) using body area net- work is becoming mainstream on most of the warble devices. Moreover, the pervasive nature of HAR along with ample opportunities to harvest energy, makes HAR on body area network quite interesting. Therefore, as a case study, we simulate an entirely EH body area network using all the components of Seeker; specifically, to leverage intermittent computing using EH only, we simulate HAR on the hardware described in Section 4.2. This includes three different sen- sors located at left ankle, right arm, and chest. Each sensor has (i) sensing element a.k.a Inertial Measurement Unit that collects acceleration data), (ii) two DNN Re-RAM crossbar (16bit 12bit) built using XB-SIM [21], (iii) two coreset com- putation engines synthesized using Design Compiler [16], (iv) an energy harvester unit which is modeled after real- world energy harvester trace data obtained from the works by Qiu et al. [56] and Geissdoerfer et al.\n\n--- Segment 28 ---\nEach sensor has (i) sensing element a.k.a Inertial Measurement Unit that collects acceleration data), (ii) two DNN Re-RAM crossbar (16bit 12bit) built using XB-SIM [21], (iii) two coreset com- putation engines synthesized using Design Compiler [16], (iv) an energy harvester unit which is modeled after real- world energy harvester trace data obtained from the works by Qiu et al. [56] and Geissdoerfer et al. [22] (the specifics of the energy-harvesting mechanism producing the power trace are beyond the scope of this work.t), (v) a simple mov- ing average power predictor power predictor and, (vi) low energy communication unit which uses IEEE 802.15.6. We model the communication energy based on the current state- of-the-art low energy communication systems [22, 59]. We utilize a simulation driven approach as multiple components, including the crossbar, coreset engine etc., are specialized hardware that are not commercially available. System devel- opment in the ultra-low-power space fundamentally spans the device technology, microarchitecture, architecture, and networking fields, and understanding the design space of next-generation EH-WSNs requires incorporating proposed advances from all areas. The crossbar simulator [21, 56] accu- rately measure the power consumption and the latency of the operations, and the same is true for Design Compiler s mod- eling of the coreset engine. The simulation tools used in our experiment are widely used and accepted in both industry and research. We evaluate our simulation using two differ- ent datasets, MHEALTH [9, 10], and PAMAP2 [57, 58]. The coreset re-construction GAN and DNN models are trained and quantized using tensorflow [1]. Furthermore, we also leverage the temporal nature of HAR by designing a dynamic coreset construction algorithm. Activity Aware Coreset Construction: In our experi- ments on HAR sensor data, we observe that not all the activ- ities are equally complex and hence may or may not need a certain number of clusters to represent every feature.\n\n--- Segment 29 ---\nFurthermore, we also leverage the temporal nature of HAR by designing a dynamic coreset construction algorithm. Activity Aware Coreset Construction: In our experi- ments on HAR sensor data, we observe that not all the activ- ities are equally complex and hence may or may not need a certain number of clusters to represent every feature. While activities like walking and running do not lose much accu- racy even when represented with as low as eight clusters, complex activities are more sensitive and need more number of clusters to preserve their geometry. As the communication overhead depends on the number of clusters, which, in turn, depends on the complexity of the activity, we propose an activity aware clustering which ensures that coresets for the current activity are represented with just sufficient number of clusters to preserve accuracy. We determine the number of clusters required as a function of current energy availability and accuracy trade off of using a lesser number of clusters. However, naively framed, this approach requires knowledge of what activity is being performed in order to encode the data that will be used to perform inference to determine what activity is being performed.\n\n--- Segment 30 ---\nWe determine the number of clusters required as a function of current energy availability and accuracy trade off of using a lesser number of clusters. However, naively framed, this approach requires knowledge of what activity is being performed in order to encode the data that will be used to perform inference to determine what activity is being performed. To break this circular de- pendency, we take inspiration from prior work in HAR [47], and use the highly stable temporal continuity of human ac- tivity (relative to the tens of milliseconds timescales for HAR 10 0 0.05 0.1 0.15 0.2 k 15 k 12 k 10 k 8 Activity Aware Coresets (HAR) Energy Aware Coresets (Bearing) Fixed Coresets Data Volume normalized to raw data (a) Data volume with dynamic coresets 0.5867 0.4106 0.2837 0.9139 0.8516 0.6076 0.8742 0.8073 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 WiFi Office Wifi Home Piezo Daily Movement Completion in Origin (HAR) Completion in Seeker (HAR) Completion in Seeker (Bearing) N A (b) Fraction of inferences completed with different EH sources 6.18 6.96 7.12 21.76 19.46 12.57 31.27 28.76 22.56 34.16 32.95 35.19 6.63 11.87 22.56 0 10 20 30 40 50 60 70 80 90 100 Wifi Office Wifi Home Piezo Daily Movement Power Trace: Memoization DNN 16bit DNN 12 Bit Clustering Importance Sampling (c) Distribution of compute off-load to different components Figure 11: Accuracy and communication efficiency of Seeker with different data sets and its sensitivity towards various EH sources.\n\n--- Segment 31 ---\nHowever, naively framed, this approach requires knowledge of what activity is being performed in order to encode the data that will be used to perform inference to determine what activity is being performed. To break this circular de- pendency, we take inspiration from prior work in HAR [47], and use the highly stable temporal continuity of human ac- tivity (relative to the tens of milliseconds timescales for HAR 10 0 0.05 0.1 0.15 0.2 k 15 k 12 k 10 k 8 Activity Aware Coresets (HAR) Energy Aware Coresets (Bearing) Fixed Coresets Data Volume normalized to raw data (a) Data volume with dynamic coresets 0.5867 0.4106 0.2837 0.9139 0.8516 0.6076 0.8742 0.8073 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 WiFi Office Wifi Home Piezo Daily Movement Completion in Origin (HAR) Completion in Seeker (HAR) Completion in Seeker (Bearing) N A (b) Fraction of inferences completed with different EH sources 6.18 6.96 7.12 21.76 19.46 12.57 31.27 28.76 22.56 34.16 32.95 35.19 6.63 11.87 22.56 0 10 20 30 40 50 60 70 80 90 100 Wifi Office Wifi Home Piezo Daily Movement Power Trace: Memoization DNN 16bit DNN 12 Bit Clustering Importance Sampling (c) Distribution of compute off-load to different components Figure 11: Accuracy and communication efficiency of Seeker with different data sets and its sensitivity towards various EH sources. -0.45 -0.24 -0.02 -0.07 -0.14 -0.19 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 0 50 100 Walking Climbing Cycling Running Jogging Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (a) Accuracy with MHEALTH dataset -0.26 -0.19 -0.76 -0.28 -0.04 -0.80 -0.60 -0.40 -0.20 0.00 0 20 40 60 80 100 Walking Climbing Cycling Running Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (b) Accuracy with PAMAP2 dataset Figure 12: Accuracy and communication efficiency of Seeker with different data sets and sensitivity study.\n\n--- Segment 32 ---\nTo break this circular de- pendency, we take inspiration from prior work in HAR [47], and use the highly stable temporal continuity of human ac- tivity (relative to the tens of milliseconds timescales for HAR 10 0 0.05 0.1 0.15 0.2 k 15 k 12 k 10 k 8 Activity Aware Coresets (HAR) Energy Aware Coresets (Bearing) Fixed Coresets Data Volume normalized to raw data (a) Data volume with dynamic coresets 0.5867 0.4106 0.2837 0.9139 0.8516 0.6076 0.8742 0.8073 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 WiFi Office Wifi Home Piezo Daily Movement Completion in Origin (HAR) Completion in Seeker (HAR) Completion in Seeker (Bearing) N A (b) Fraction of inferences completed with different EH sources 6.18 6.96 7.12 21.76 19.46 12.57 31.27 28.76 22.56 34.16 32.95 35.19 6.63 11.87 22.56 0 10 20 30 40 50 60 70 80 90 100 Wifi Office Wifi Home Piezo Daily Movement Power Trace: Memoization DNN 16bit DNN 12 Bit Clustering Importance Sampling (c) Distribution of compute off-load to different components Figure 11: Accuracy and communication efficiency of Seeker with different data sets and its sensitivity towards various EH sources. -0.45 -0.24 -0.02 -0.07 -0.14 -0.19 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 0 50 100 Walking Climbing Cycling Running Jogging Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (a) Accuracy with MHEALTH dataset -0.26 -0.19 -0.76 -0.28 -0.04 -0.80 -0.60 -0.40 -0.20 0.00 0 20 40 60 80 100 Walking Climbing Cycling Running Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (b) Accuracy with PAMAP2 dataset Figure 12: Accuracy and communication efficiency of Seeker with different data sets and sensitivity study. inferences) to predict the current activity based on previously completed local inferences.\n\n--- Segment 33 ---\n-0.45 -0.24 -0.02 -0.07 -0.14 -0.19 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 0 50 100 Walking Climbing Cycling Running Jogging Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (a) Accuracy with MHEALTH dataset -0.26 -0.19 -0.76 -0.28 -0.04 -0.80 -0.60 -0.40 -0.20 0.00 0 20 40 60 80 100 Walking Climbing Cycling Running Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (b) Accuracy with PAMAP2 dataset Figure 12: Accuracy and communication efficiency of Seeker with different data sets and sensitivity study. inferences) to predict the current activity based on previously completed local inferences. We use temporal continuity to our advantage, and make sure that if the system does not have enough energy to form the default 12 clusters, it will re- sort to forming a smaller number of clusters with minimum accuracy loss. We implement a small lookup table to carry the information on accuracy trade-off for different activities with respect to the number of clusters used to form the core- sets (similar to the data represented Fig. 6). We observe that AAC communicates about 11 data compared to sending the full raw data (refer Figure 11a). Note that we only resort to activity awareness while forming coresets using clustering, as importance sampling based coreset construction does not require much energy. Furthermore, dropping the number of samples in importance sampling method, even with recovery, significantly hampers the accuracy, which is not the case for recoverable-clustering based coreset construction. Baseline: We choose three points of comparison for our ac- curacy evaluation. Baseline-1 (Baseline: Large DNN) con- sists of a full precision (without any pruning) DNN built on the lines of [26].\n\n--- Segment 34 ---\nBaseline: We choose three points of comparison for our ac- curacy evaluation. Baseline-1 (Baseline: Large DNN) con- sists of a full precision (without any pruning) DNN built on the lines of [26]. Baseline-2 (Baseline: EAP) optimizes (us- ing [68]) Baseline-1 to design a power-aware DNN tuned to for the average harvested power of our EH source. Baseline- 3 (Baseline: Origin) uses the system design proposed in [47]. Baseline-1 and Baseline-2 run on fully powered systems where as Baseline-3 runs on the same EH source as Seeker. For communication, we consider a system which transmits the entire raw data to the host as the baseline. Analysis of Results on HAR: Figure 17a and 17b show the accuracy of various policies described in Section 4, along with the accuracy of Seeker - which applies all policies to- gether, along with ensemble learning. Figure 11a shows the normalized data communication volume with different num- bers of clusters, along with activity-aware clustering. We make the following observations: Seeker at Edge Finishes More Work: Equipped with multi- ple decision options, Seeker could finish close to 60 (58.67 using one of the RE sources; refer Figure 11b) of the in- ferences at the edge itself, thanks to the two efficient and quantized DNNs and the correlation engine. Both the DNNs share the load of the inference depending on the available energy, while correlation engine gets rid of close to 6 of the redundant compute (refer Figure 11c). Seeker at Edge Efficiently Offloads: For the unfinished compute, Seeker converts the data into coresets for commu- nicating them to the host with minimum payload footprint. The activity aware coresets, thanks to their dynamic nature, reduces the communication volume 8.9 (refer to Figure 11a) compared to sending the raw data, and up to 3 compared to the classical compression techniques. The Recovered Coresets Give Accurate Inference: Even with a specialized DNN tranined with coreset data, com- pressed coresets give less accuracy, evidently because of the loss of features during the coreset formation.\n\n--- Segment 35 ---\nThe activity aware coresets, thanks to their dynamic nature, reduces the communication volume 8.9 (refer to Figure 11a) compared to sending the raw data, and up to 3 compared to the classical compression techniques. The Recovered Coresets Give Accurate Inference: Even with a specialized DNN tranined with coreset data, com- pressed coresets give less accuracy, evidently because of the loss of features during the coreset formation. However, with the reconstruction (via GAN or cluster redistribution), the accuracy reaches 86.8 compared to 76.4 for the former. The GAN modeled the lost signals with a very correlation ( 0.9 in most cases and 0.6 in some of the worst cases). Seeker is Close to a Fully Powered System: Seeker, thanks to synergistic computation, achieves 87.05 accuracy with 11 1.74 0.56 0.86 1.98 0.66 0.00 0.50 1.00 1.50 2.00 2.50 0 20 40 60 80 100 0.007mm 0.014mm 0.021mm 0.028mm GeoMean Error Accuracy ( ) Fault Diameter Cluster: Compressed Cluster: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered Figure 13: Accuracy of Seeker on Bearing Data set MHEALTH (0.18 less accurate than a DNN running at full precision with full power) data set and similarly reaches 74.2 accuracy on the PAMAP2 data set. This gives us 7 more accuracy than [47] on MHEALTH and 3 more accuracy for PAMAP2 dataset. The accuracy improvement is because of three reasons: 1. the DNNs at the edge are more fine-tuned towards delivering accurate results and are 1.5 more accurate than the prior state-of-the-art [47]; 2. the recovered coresets imitate the original data with a great accuracy, and hence the inference accuracy at the host is as good as it would have been with the original data. 3. because Seeker could finish more number of inferences (either at the edge or by offloading to a host), greatly reducing the scheduled task from 40 to 5 in best case, 8 in worst case, and 6.15 in average case (all experiments on RF power trace).\n\n--- Segment 36 ---\nThe accuracy improvement is because of three reasons: 1. the DNNs at the edge are more fine-tuned towards delivering accurate results and are 1.5 more accurate than the prior state-of-the-art [47]; 2. the recovered coresets imitate the original data with a great accuracy, and hence the inference accuracy at the host is as good as it would have been with the original data. 3. because Seeker could finish more number of inferences (either at the edge or by offloading to a host), greatly reducing the scheduled task from 40 to 5 in best case, 8 in worst case, and 6.15 in average case (all experiments on RF power trace). To compare with a battery operated energy optimized (to the average energy harvested by the RF sources) system, Seeker is 5.89 more accurate on MHEALTH dataset, and 4.98 more accurate on PAMAP2 dataset. Sensitivity Study: Our experiments with other EH sources show the versatility of Seeker, which outperforms a HAR clas- sifier designed for EH [47] across multiple (piezo-electric, RF) harvested energy sources and demonstrate that it can easily be scaled to work with any number of sensors. Fig. 11b shows the comparison of scheduled inferences completed while using Seeker and the state-of-the-art [47]. Further, we demonstrate how Seeker leverages all the proposed design components (including memoization, DNN inference and coreset) to complete maximum compute at the edge and of- fload minimum to the host device. Fig. 11c shows the compute breakdown among components under different EH sources. 5.3 Seeker for Predictive Maintenance With the advent of automation and industry 4.0 [34, 70], predictive maintenance is once of the most sorted after prob- lems in industrial IoT domain. Predictive maintenance is a condition-driven preventive maintenance program where, instead of replying on failure to schedule maintenance activ- ities, predictive maintenance uses direct monitoring of the plant to preemptively schedule maintenance and possibly take measures to prevent them from occurring [48]. For con- tinuous monitoring, the machines are typically fitted with sensors (vibration, force, magnetic, acoustic etc. ), and the plant performs continuous analytics on those sensor data for preemptively maintenance scheduling. Vibration based con- dition monitoring is one of the most common scenarios [53].\n\n--- Segment 37 ---\n), and the plant performs continuous analytics on those sensor data for preemptively maintenance scheduling. Vibration based con- dition monitoring is one of the most common scenarios [53]. Since there are multiple machines, and each machine is fitted with one or more sensors, this is a perfect example of wire- less sensor network. The rise of Industry 4.0 [34] has lead to an exponential explosion of such sensors in industries, especially in remote or hostile locations, calling for energy harvesting as a solution, and hence need for EH-WSNs. Baseline: Towards this we take Case Western Bearing Fault data set [53] where the vibrations from the different bear- ings are collected to analyse the fault patterns (e.g. size of a crack in the bearing with respect to operating load, speed etc.). There has been a large body of work [18, 27, 28, 35] in industrial engineering domain to develop DNN classifer for this task. In our experiments, we took inspirations from the work of [18, 27] to build a classifier, and applied further opti- mizations, as we did for HAR evaluation, to make the DNN edge-friendly. We also tweaked AAC to be energy aware only, i.e. the number of clusters formed depends only the energy available. We redesigned (few changes in the hyper parameters) and trained the GAN to adapt to the bearing data for recovering the importance sampling coresets. Results: As the base design of Seeker can adapt to any sen- sor based communication, most of the arguments made for HAR still holds true in the case of bearing fault detection. As depicted on Figure 11a and 11b, Seeker reduces the com- munication overhead by 7 while finishing 80 of the scheduled compute using WiFi sources. Further, as shown in Figure 13, Seeker, on an average, delivers an accuracy of 84.73 which is only 0.66 less than a fully powered system. It is note worthy that the bearing data is much susceptible to the real-world nuances and machine part interactions yet, the accuracy of fault prediction is extremely essential to- wards the production continuity and quality.\n\n--- Segment 38 ---\nFurther, as shown in Figure 13, Seeker, on an average, delivers an accuracy of 84.73 which is only 0.66 less than a fully powered system. It is note worthy that the bearing data is much susceptible to the real-world nuances and machine part interactions yet, the accuracy of fault prediction is extremely essential to- wards the production continuity and quality. To portray an example of scale, for a typical grinding job in a manufac- turing industry that takes about 8.2 seconds [6], and the 5 improvement we observe (in Figure 13) over the prior state of the art [47] impacts 46𝑘parts per year per machine (work- ing 8 hours day). Therefore, in large scale industries, both saving communication overhead while maximizing accuracy directly impact the economics of production. 6 CONCLUSION As systems utilizing energy harvesting edge devices are tasked with increasingly complex tasks, like HAR or pre- dictive maintenance, both system and node designs must respond with targeted efficiency-maximizing optimizations. Our proposal, Seeker, synergizes EH sensor nodes host de- vices by intelligently distributing computations among them, while significantly minimizing the communication over- heads. Our experiments show that, by leveraging coreset techniques in data reduction and tuning these techniques for 12 application-aware properties, Seeker can reduce the commu- nication overhead by 8.9 , while providing better accuracy (86.8 ), even when limited to harvested power, than state-of- the-art energy-optimized DNNs running on a fully powered device (81.2 ). Furthermore, it also outperforms the state of the art system designed specifically for EH-WSNs. Col- lectively, the optimizations in Seeker reduce communication traffic, while improving inference accuracy, demonstrating the potential of holistic system node application optimiza- tion for the current and future generation of (energy harvest- ing) wireless sensor nodes.\n\n--- Segment 39 ---\nFurthermore, it also outperforms the state of the art system designed specifically for EH-WSNs. Col- lectively, the optimizations in Seeker reduce communication traffic, while improving inference accuracy, demonstrating the potential of holistic system node application optimiza- tion for the current and future generation of (energy harvest- ing) wireless sensor nodes. REFERENCES [1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin- cent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xi- aoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org. [2] Adafruit ItsyBitsy nRF52840 Express Bluetooth LE 2022. Adafruit ItsyBitsy nRF52840 Express Bluetooth LE. [3] Apple Watch 2019. Apple Watch Series 5 teardown reveals bigger battery. 5-teardown-reveals-bigger-battery. [4] Apple Watch - ECG 2020. Taking an ECG with the ECG app on Apple Watch Series 4 or later. [5] Apple Watch - Fall Detection 2020. Use fall detection with Apple Watch. [6] Automating the Grinding Process 2013. Automating the Grinding Process. the-grinding-process . [7] Olivier Bachem, Mario Lucic, and Andreas Krause. 2015. Coresets for nonparametric estimation-the case of DP-means. In International Conference on Machine Learning. PMLR, 209 217.\n\n--- Segment 40 ---\nIn International Conference on Machine Learning. PMLR, 209 217. [8] Olivier Bachem, Mario Lucic, and Andreas Krause. 2017. Practical core- set constructions for machine learning. arXiv preprint arXiv:1703.06476 (2017). [9] Oresti Baños, Rafael García, Juan Antonio Holgado Terriza, Miguel Damas, Héctor Pomares, Ignacio Rojas Ruiz, Alejandro Saez, and Clau- dia Villalonga. 2014. mHealthDroid: A Novel Framework for Agile Development of Mobile Health Applications. In IWAAL. Springer. [10] Oresti Banos, Claudia Villalonga, Rafael García, Alejandro Saez, Miguel Damas, Juan Holgado-Terriza, Sungyong Lee, Hector Pomares, and Ignacio Rojas. 2015. Design, implementation and validation of a novel open framework for agile development of mobile health applications. BioMedical Engineering OnLine (2015). [11] David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Stro- belt, Bolei Zhou, and Antonio Torralba. 2019. Seeing what a gan cannot generate. In Proceedings of the IEEE CVF International Conference on Computer Vision. 4502 4511. [12] Circuit Python 2022. Circuit Python. [13] U Cisco. 2020. Cisco annual internet report (2018 2023) white paper. [14] Bradley Denby and Brandon Lucia. 2019. Orbital edge computing: Machine inference in space. IEEE Computer Architecture Letters 18, 1 (2019), 59 62. [15] Brad Denby, Emily Ruppel, Vaibhav Singh, Shize Che, Chad Taylor, Fayyaz Zaidi, Swarun Kumar, Zac Manchester, and Brandon Lucia. 2022. Tartan Artibeus: A Batteryless, Computational Satellite Research Platform. (2022). [16] Design Compiler: Concurrent Timing, Area, Power, and Test Optimiza- tion 2022. Design Compiler: Concurrent Timing, Area, Power, and Test Optimization. signoff rtl-synthesis-test dc-ultra.html.\n\n--- Segment 41 ---\nDesign Compiler: Concurrent Timing, Area, Power, and Test Optimization. signoff rtl-synthesis-test dc-ultra.html. [17] Abhimanyu Dubey, Moitreya Chatterjee, and Narendra Ahuja. 2018. Coreset-based neural network compression. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV). 454 470. [18] Levent Eren, Turker Ince, and Serkan Kiranyaz. 2019. A generic intel- ligent bearing fault diagnosis system using compact adaptive 1D CNN classifier. Journal of Signal Processing Systems 91, 2 (2019), 179 189. [19] Levent Eren, Turker Ince, and Serkan Kiranyaz. 2019. A generic intel- ligent bearing fault diagnosis system using compact adaptive 1D CNN classifier. Journal of Signal Processing Systems 91, 2 (2019), 179 189. [20] Amir Erfan Eshratifar and Massoud Pedram. 2018. Energy and perfor- mance efficient computation offloading for deep neural networks in a mobile cloud computing environment. In Proceedings of the 2018 on Great Lakes Symposium on VLSI. 111 116. [21] Xiang Fei, Youhui Zhang, and Weimin Zheng. 2020. XB-SIM: A sim- ulation framework for modeling and exploration of ReRAM-based CNN acceleration design. Tsinghua Science and Technology 26, 3 (2020), 322 334. [22] Kai Geissdoerfer and Marco Zimmerling. 2022. Learning to Com- municate Effectively Between Battery-free Devices. In 19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22). USENIX Association, Renton, WA, 419 435. org conference nsdi22 presentation geissdoerfer [23] Graham Gobieski, Brandon Lucia, and Nathan Beckmann. 2019. Intelli- gence Beyond the Edge: Inference on Intermittent Embedded Systems. In ASPLOS, Iris Bahar, Maurice Herlihy, Emmett Witchel, and Alvin R. Lebeck (Eds.). ACM. [24] Graham Gobieski, Brandon Lucia, and Nathan Beckmann. 2019. Intelli- gence Beyond the Edge: Inference on Intermittent Embedded Systems. In ASPLOS.\n\n--- Segment 42 ---\nIntelli- gence Beyond the Edge: Inference on Intermittent Embedded Systems. In ASPLOS. ACM. [25] Google Assistant for Wearables 2020. Google Assistant for Wearables. [26] S. Ha and S. Choi. 2016. Convolutional neural networks for human ac- tivity recognition using multiple accelerometer and gyroscope sensors. In IJCNN. [27] Seungmin Han and Jongpil Jeong. 2020. An weighted CNN ensemble model with small amount of data for bearing fault diagnosis. Procedia Computer Science 175 (2020), 88 95. [28] Duy-Tang Hoang and Hee-Jun Kang. 2017. Convolutional neural network based bearing fault diagnosis. In International conference on intelligent computing. Springer, 105 111. [29] Duy-Tang Hoang and Hee-Jun Kang. 2017. Convolutional neural network based bearing fault diagnosis. In International conference on intelligent computing. Springer, 105 111. [30] Chuang Hu, Wei Bao, Dan Wang, and Fengming Liu. 2019. Dynamic adaptive DNN surgery for inference acceleration on the edge. In IEEE INFOCOM 2019-IEEE Conference on Computer Communications. IEEE, 1423 1431. [31] Yiping Kang, Johann Hauswald, Cao Gao, Austin Rovinski, Trevor Mudge, Jason Mars, and Lingjia Tang. 2017. Neurosurgeon: Collabora- tive intelligence between the cloud and mobile edge. ACM SIGARCH 13 Computer Architecture News 45, 1 (2017), 615 629. [32] Naoto Kimura and Shahram Latifi. 2005. A survey on data compression in wireless sensor networks. In International Conference on Information Technology: Coding and Computing (ITCC 05)-Volume II, Vol. 2. IEEE, 8 13. [33] Julius Kusuma, Lance Doherty, and Kannan Ramchandran. 2001. Dis- tributed compression for sensor networks. In Proceedings 2001 Inter- national Conference on Image Processing (Cat. No. 01CH37205), Vol. 1. IEEE, 82 85. [34] Heiner Lasi, Peter Fettke, Hans-Georg Kemper, Thomas Feld, and Michael Hoffmann. 2014. Industry 4.0. Business information systems engineering 6, 4 (2014), 239 242.\n\n--- Segment 43 ---\n2014. Industry 4.0. Business information systems engineering 6, 4 (2014), 239 242. [35] Dean Lee, Vincent Siu, Rick Cruz, and Charles Yetman. 2016. Con- volutional neural net and bearing fault analysis. In Proceedings of the International Conference on Data Science (ICDATA). The Steering Com- mittee of The World Congress in Computer Science, Computer ..., 194. [36] Hanlin Lu, Changchang Liu, Ting He, Shiqiang Wang, and Kevin S. Chan. 2020. Sharing Models or Coresets: A Study based on Membership Inference Attack. arXiv:2007.02977 [cs.LG] [37] H. Lu, C. Liu, S. Wang, T. He, V. Narayanan, K. S. Chan, and S. Pasteris. 2020. Joint Coreset Construction and Quantization for Distributed Machine Learning. In 2020 IFIP Networking Conference (Networking). [38] Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. 2017. Training Gaussian Mixture Models at Scale via Coresets. J. Mach. Learn. Res. 18, 1 (Jan. 2017), 5885 5909. [39] K. Ma, X. Li, J. Li, Y. Liu, Y. Xie, J. Sampson, M. T. Kandemir, and V. Narayanan. 2017. Incidental Computing on IoT Nonvolatile Processors. In MICRO. [40] K. Ma, X. Li, S. Li, Y. Liu, J. J. Sampson, Y. Xie, and V. Narayanan. 2015. Nonvolatile Processor Architecture Exploration for Energy-Harvesting Applications. IEEE Micro 35, 5 (2015), 32 40. [41] K. Ma, X. Li, S. R. Srinivasa, Y. Liu, J. Sampson, Y. Xie, and V. Narayanan. 2017. Spendthrift: Machine learning based resource and frequency scaling for ambient energy harvesting nonvolatile processors. In 2017 (ASP-DAC). 678 683.\n\n--- Segment 44 ---\nIn 2017 (ASP-DAC). 678 683. [42] Kaisheng Ma, Xueqing Li, Karthik Swaminathan, Yang Zheng, Shuangchen Li, Yongpan Liu, Yuan Xie, John (Jack) Morgan Sampson, and Vijaykrishnan Narayanan. 2016. Nonvolatile Processor Architec- tures: Efficient, Reliable Progress with Unstable Power. IEEE Micro 36, 3 (2016), 72 83. [43] Kiwan Maeng and Brandon Lucia. 2018. Adaptive Dynamic Check- pointing for Safe Efficient Intermittent Computing. In OSDI. USENIX Association. [44] Kiwan Maeng and Brandon Lucia. 2018. Adaptive Dynamic Check- pointing for Safe Efficient Intermittent Computing. In OSDI, Andrea C. Arpaci-Dusseau and Geoff Voelker (Eds.). USENIX Association. [45] Francesco Marcelloni and Massimo Vecchio. 2008. A simple algorithm for data compression in wireless sensor networks. IEEE communica- tions letters (2008). [46] Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec. 2020. Coresets for Data-efficient Training of Machine Learning Models. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daumé III and Aarti Singh (Eds.). PMLR, Virtual, 6950 6960. [47] Cyan Subhra Mishra, Jack Sampson, Mahmut Taylan Kandemir, and Vijaykrishnan Narayanan. 2021. Origin: Enabling On-Device Intel- ligence for Human Activity Recognition Using Energy Harvesting Wireless Sensor Networks. In DATE. [48] R Keith Mobley. 2002. An introduction to predictive maintenance. Else- vier. [49] Reem E Mohamed, Ahmed I Saleh, Maher Abdelrazzak, and Ahmed S Samra. 2018. Survey on wireless sensor network applications and energy efficient routing protocols. Wireless Personal Communications 101, 2 (2018), 1019 1055. [50] Thaha Mohammed, Carlee Joe-Wong, Rohit Babbar, and Mario Di Francesco. 2020.\n\n--- Segment 45 ---\n[50] Thaha Mohammed, Carlee Joe-Wong, Rohit Babbar, and Mario Di Francesco. 2020. Distributed inference acceleration with adap- tive DNN partitioning and offloading. In IEEE INFOCOM 2020-IEEE Conference on Computer Communications. IEEE, 854 863. [51] Mu Editor 2022. Code with Mu a simple Python editor for beginner programmers. [52] Ben Mussay, Margarita Osadchy, Vladimir Braverman, Samson Zhou, and Dan Feldman. 2019. Data-independent neural pruning via coresets. arXiv preprint arXiv:1907.04018 (2019). [53] Dhiraj Neupane and Jongwon Seok. 2020. Bearing fault detection and diagnosis using case western reserve university dataset with deep learning approaches: A review. IEEE Access 8 (2020), 93155 93178. [54] Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu, Zeng Huang, Sitao Xiang, Shunsuke Saito, Pushmeet Kohli, and Hao Li. 2017. Re- alistic dynamic facial textures from a single image using gans. In Proceedings of the IEEE International Conference on Computer Vision. 5429 5438. [55] Rohan Paul, Dan Feldman, Daniela Rus, and Paul Newman. 2014. Visual precis generation using coresets. In 2014 IEEE International Conference on Robotics and Automation (ICRA). 1304 1311. 1109 ICRA.2014.6907021 [56] K. Qiu, N. Jao, M. Zhao, C. S. Mishra, G. Gudukbay, S. Jose, J. Samp- son, M. T. Kandemir, and V. Narayanan. 2020. ResiRCA: A Resilient Energy Harvesting ReRAM Crossbar-Based Accelerator for Intelligent Embedded Processors. In 2020 HPCA. 315 327. [57] Attila Reiss and Didier Stricker. 2012. Creating and benchmarking a new dataset for physical activity monitoring. In PETRA, Fillia Makedon (Ed.). ACM. [58] Attila Reiss and Didier Stricker. 2012. Introducing a New Benchmarked Dataset for Activity Monitoring. In ISWC. IEEE.\n\n--- Segment 46 ---\nIn ISWC. IEEE. [59] Mohammad Rostami, Jeremy Gummeson, Ali Kiaghadi, and Deepak Ganesan. 2018. Polymorphic radios: A new design paradigm for ultra- low power communication. In Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication. 446 460. [60] Fernando Moya Rueda, René Grzeszick, Gernot A. Fink, Sascha Feld- horst, and Michael ten Hompel. 2018. Convolutional Neural Networks for Human Activity Recognition Using Body-Worn Sensors. Informat- ics (2018). [61] Rachid Saadane, Abdellah Chehri, Seunggil Jeon, et al. 2022. AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities. Sus- tainable Energy Technologies and Assessments 52 (2022), 102093. [62] Alanson P Sample, Daniel J Yeager, Pauline S Powledge, Alexander V Mamishev, and Joshua R Smith. 2008. Design of an RFID-based battery- free programmable sensing platform. IEEE transactions on instrumen- tation and measurement 57, 11 (2008), 2608 2615. [63] Himanshu Sharma, Ahteshamul Haque, and Zainul Abdin Jaffery. 2019. Maximization of wireless sensor network lifetime using solar energy harvesting for smart agriculture monitoring. Ad Hoc Networks 94 (2019), 101966. [64] Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li Jiang, Naifeng Jing, and Xiaoyao Liang. 2020. DRQ: dynamic region-based quantization for deep neural network acceleration. In ISCA. IEEE. [65] Ben Taylor, Vicent Sanz Marco, Willy Wolff, Yehia Elkhatib, and Zheng Wang. 2018. Adaptive deep learning model selection on embedded systems. ACM SIGPLAN Notices 53, 6 (2018), 31 43. [66] Texas Instrument Micro-controller with FRAM 2022.\n\n--- Segment 47 ---\nACM SIGPLAN Notices 53, 6 (2018), 31 43. [66] Texas Instrument Micro-controller with FRAM 2022. 16 MHz MCU with 64KB FRAM, 2KB SRAM, AES, 12-bit ADC, comparator, DMA, 14 UART SPI I2C, timer. [67] Rambabu Vatti, Nagarjuna Vatti, K Mahender, Prasanna Lakshmi Vatti, and B Krishnaveni. 2020. Solar energy harvesting for smart farming using nanomaterial and machine learning. In IOP Conference Series: Materials Science and Engineering, Vol. 981. IOP Publishing, 032009. [68] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. 2018. NetAdapt: Platform- Aware Neural Network Adaptation for Mobile Applications. In ECCV. [69] Zhuoran Zhao, Kamyar Mirzazad Barijough, and Andreas Gerstlauer. 2018. DeepThings: Distributed adaptive deep learning inference on resource-constrained IoT edge clusters. IEEE Transactions on Computer- Aided Design of Integrated Circuits and Systems 37, 11 (2018), 2348 2359. [70] Tiago Zonta, Cristiano André da Costa, Rodrigo da Rosa Righi, Miro- mar Jose de Lima, Eduardo Silveira da Trindade, and Guann Pyng Li. 2020. Predictive maintenance in the Industry 4.0: A systematic litera- ture review. Computers Industrial Engineering 150 (2020), 106889. 15 0 20 40 60 0 0.1 0.2 0.3 0.4 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 Reconstruction Error FFT Amplitude Frequency (Hz) Reconstructed Original Error Figure 14: An example of generator based coreset re- covery Figure 15: completion of the inference at the edge for bearing fault data with different EH source. Figure 16: Communication data volume with different number of clusters.\n\n--- Segment 48 ---\n15 0 20 40 60 0 0.1 0.2 0.3 0.4 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 Reconstruction Error FFT Amplitude Frequency (Hz) Reconstructed Original Error Figure 14: An example of generator based coreset re- covery Figure 15: completion of the inference at the edge for bearing fault data with different EH source. Figure 16: Communication data volume with different number of clusters. Component Spec Power Area(mm2) SRAM Buffers 1kB 256 8kB 256 64kB 16 256kB 10.372W 117.164 MAC Unit (8 8) 256 8.46W 32.72 Adder Tree and Comparator 16 16bit 256 2.4W 21.556 Control 0.96W 12.2 Host Cortex A78 series 11W Design at 592MHz with Synopsys AED 32nm library Total 256 tiles 33.192W 183.64 Table 3: Area and power estimation of our design. A APPENDIX A.1 Reconstructing Importance Sampling Coreset As discussed in Section 3.2.2, we use GANs to recover the data we lost while performing importance sampling. This was motivated from the observation that as we selected more number of points in importance sampling, the accuracy of the inference on the compressed data increased significantly (at times by 2 ). Hence, the points which were not selected while performing importance sampling still had some impor- tance and can be represented as a function containing the low level nuances of the activity performed and the sensor state. The challenge was to learn this function, i.e. to de- vice a transformation function which can mimic the sensor signal given the aactivity and the sensor states. A similar problem, in terms of generating faces, paintings etc. given some latent space has already been solved using GANs [54]. Motivated by this, we designed a GANs to regenerate the lost data points while performing importance sampling. The latent space takes the activity, and the first and second order moments of the data sample to recreate the signal, and the Discriminator tried to distinguish between the generated sig- nal and the actual signal. The generator is tuned repeatedly until the discriminator could not distinguish the original and the generated signal.\n\n--- Segment 49 ---\nThe latent space takes the activity, and the first and second order moments of the data sample to recreate the signal, and the Discriminator tried to distinguish between the generated sig- nal and the actual signal. The generator is tuned repeatedly until the discriminator could not distinguish the original and the generated signal. The GAN modeled the lost signals with a very high correlation ( 0.9 in most cases and 0.6 in some of the worst cases (refer Figure 14 for an example). In rare cases (once in over 2000 cases), the generator induced arti- facts which could result in wrong classifications. However, this error could be rectified with further fine tuning. A.2 More Results on Bearing Fault Data We repeated our experiments with similar experimental setup on the bearing fault data set [53]. The bearing fault data is sampled at a much higher frequency (48KHz) than the HAR data, and hence require a larger DNN, larger num- ber of importance sampling, and more number of clusters. We took the learning from multiple domain specific litera- tures [19, 29, 53] to isolate the frequency regions specific to the fault pattern to minimize the computations. But, because of the larger data volume, the number of computations per- formed at the edge diminished significantly (refer Figure 15). We also conducted an empirical study on number of clusters required, and found out that the bearing set data needs about 15 to 20 clusters to maintain the inference accuracy. The data volume communicated for different number of clusters is represented in Figure 13.\n\n--- Segment 50 ---\nWe also conducted an empirical study on number of clusters required, and found out that the bearing set data needs about 15 to 20 clusters to maintain the inference accuracy. The data volume communicated for different number of clusters is represented in Figure 13. B APPENDIX 16 -0.45 -0.24 -0.02 -0.07 -0.14 -0.19 -0.50 -0.40 -0.30 -0.20 -0.10 0.00 0 50 100 Walking Climbing Cycling Running Jogging Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (a) Accuracy with MHEALTH dataset -0.26 -0.19 -0.76 -0.28 -0.04 -0.80 -0.60 -0.40 -0.20 0.00 0 20 40 60 80 100 Walking Climbing Cycling Running Jumping Geo Mean Error Accuracy ( ) Coreset: Compressed Coreset: Reconstructed Reconstructed with Larger Model Seeker Baseline: EAP Baseline: Origin Baseline: Large DNN Error vs Fully Powered (b) Accuracy with PAMAP2 dataset Figure 17: Accuracy and communication efficiency of Seeker with different data sets and sensitivity study. 17\n\n