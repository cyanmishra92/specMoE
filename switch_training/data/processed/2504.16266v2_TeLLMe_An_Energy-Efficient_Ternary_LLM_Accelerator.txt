=== ORIGINAL PDF: 2504.16266v2_TeLLMe_An_Energy-Efficient_Ternary_LLM_Accelerator.pdf ===\n\nRaw text length: 43214 characters\nCleaned text length: 42657 characters\nNumber of segments: 29\n\n=== CLEANED TEXT ===\n\nTeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefill and Decode on Edge FPGAs Ye Qiao , Zhiheng Chen , Yifan Zhang, Yian Wang, Sitao Huang Department of Electrical Engineering and Computer Science University of California, Irvine, USA {yeq6, zhihenc5, yifanz58, yianw11, Abstract Deploying large language models (LLMs) on edge platforms is challenged by their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as 1.58 bits with minimal accuracy loss, edge deployment is still constrained by limited on-chip resources, power budgets, and the often-neglected latency of the prefill phase. We present TeLLMe, the first ternary LLM accelerator for low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and autoregressive decoding using 1.58-bit weights and 8-bit activations. Our contributions include: (1) a table-lookup matrix engine for ternary matmul that merges grouped activations with online precomputation to minimize resource use; (2) a fused, bandwidth-efficient attention module featuring a reversed reordering scheme to accelerate prefill; and (3) a tightly integrated normalization and quanti- zation dequantization unit optimized for ultra-low-bit inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens s throughput over 1,024-token contexts and prefill latencies of 0.55 1.15 s for 64 128 token prompts, marking a significant energy-efficiency advance and establishing a new edge FPGA benchmark for generative AI. I. INTRODUCTION Large Language Models (LLMs) have achieved remarkable progress in recent years, powering state-of-the-art performance in natural language processing tasks such as machine transla- tion, code generation, question answering, and conversational AI. Models like GPT-3[1], LLaMA[2], and DeepSeek-R1[3] have shown that increasing model size significantly improves generalization and task performance. However, this scaling comes with substantial costs in terms of computational de- mands, memory usage, and energy consumption. Edge deployment of LLMs, i.e., running these models on low-power, resource-constrained devices such as embedded systems, FPGAs, or mobile SoCs, is a critical enabler for privacy-preserving, latency-sensitive, and autonomous appli- cations. However, such a deployment remains challenging due to the gap between LLM complexity and the limited memory bandwidth, memory capacity, compute capacity, and power budgets on edge platforms. To bridge this gap, recent research has focused on extreme model compression, particularly through low-bit quantization[4], [5]. Pioneering work such as BitNet [6] demonstrated that Transformer models can be trained with 1- bit weights, while BitNet-1.58 [7] and DeepSeek [8] extend Equal contribution this to ternary quantization (t 1, 0, 1u), achieving near- parity with full-precision models. These innovations signifi- cantly reduce model size and energy cost, making LLMs more viable for edge execution. However, deploying these compressed models on real hard- ware, especially FPGAs, presents unique challenges. Unlike cloud-scale GPUs, edge FPGAs have strict constraints on on- chip memory (BRAM URAM), external DRAM bandwidth, and energy budgets. Furthermore, the requirements of autore- gressive decoding (such as growing key-value (KV) caches, long context handling, and latency sensitivity) exacerbate these limitations. While most prior works focus either on model quantization or software acceleration, there is still no a systematic hardware-software co-optimization solution that fully exploit the benefits of extreme low bitwidth LLMs while meeting the computing demands of edge inference. One critical yet often overlooked issue in edge LLM deploy- ment is the disproportionate emphasis on decoding throughput, while prefill latency remains largely ignored. For example, [9] demonstrates efficient LLM decoding on embedded FPGAs but neglects the prefill stage entirely. However, prefill latency is not merely a technical detail, it is a primary bottleneck for user experience and safety in latency-sensitive edge AI applications. While prefill overhead may be negligible in cloud environments, on-device deployment places it squarely on the critical execution path. Despite its importance, prefill optimization remains significantly underexplored and demands more serious attention from the community. To address these limitations, we present TeLLMe the Tenary Large Language Model Edge Accelerator the first edge FPGA-based accelerator specifically designed for ternary LLM inference with full support for both prefill and decoding stages. TeLLMe enables low-latency, energy-efficient deploy- ment of LLMs on resource-constrained platforms by target- ing cost-effective FPGAs such as AMD KV260. It supports ternary-quantized weights (1.58-bit) and 8-bit activations. Our design co-optimizes compute, memory, and scheduling efficiency, key contributions are as follows: We develop the first end-to-end edge FPGA accelerator for ternary LLMs supporting both prefill and decoding stages. We propose Table-lookup-based ternary matmul, an effi- cient and resource-saving matrix multiplication unit that arXiv:2504.16266v2 [cs.AR] 24 Apr 2025 specially optimized for FPGA, reusing grouped activa- tions and online computation for ternary matmuls across projection and FFN layers. We introduce a fused attention unit for prefill and de- coding, incorporating a novel reversed attention mecha- nism and fully fused pipeline to minimize off-chip data movement, avoid redundant masked computation, and guarantee the parallelism at the same time. We deliver up to 9.51 tokens sec generation in up to 1024 token contexts while consuming less than 7W power, outperforming mobile SoCs with much lower power budgets. TeLLMe achieves a prefill latency from 0.55s to 1.15s for prompt sizes of 64 to 128 tokens and delivers up to 9.51 tokens s decoding throughput with support for 1024-token context lengths on edge FPGAs, all while operating under 7 watts of power consumption. This marks a significant ad- vancement over existing mobile-edge devices and prior FPGA- based accelerators, which typically require higher precision and greater power budgets. To the best of our knowledge, TeLLMe is the first ac- celerator to provide end-to-end support for ternary LLM inference including both prefill and decoding stages on real FPGA hardware, establishing a new baseline for energy- efficient, low-latency generative AI at the edge. II. BACKGROUND AND RELATED WORK A. LLM Basic A typical LLM, such as LLaMA, is composed of multiple identical transformer blocks, each containing an attention module followed by a multilayer perceptron (MLP) module, as illustrated in Fig. 1. Within each attention module, three linear projections are used to compute the query (Q), key (K), and value (V) representations. These are then processed by a multi-head attention mechanism that incorporates both the current QKV tensors and historical KV cache. The MLP module consists of an up-projection and down-projection layer, along with an additional gating projection applied to the up- projection output. The generative inference of LLMs is typically divided into two stages: the prefill phase and the decode phase (generation), as shown in Figure 1. During the prefill phase, the entire prompt is processed through the full model stack to produce the first output token. This phase involves parallel computation across multiple input tokens and is dominated by compute-intensive matrix matrix multiplications, particularly within the linear projection layers. In contrast, the decode phase proceeds in an autoregressive fashion, generating one token at a time by feeding the previously generated token back into the model. This phase is typically memory-bound due to its reliance on KV cache lookup and smaller matrix vector operations. Following the observations in Chen et al.[10] , while FPGAs are generally less efficient than GPUs during the compute- heavy prefill stage, they exhibit competitive advantages during the memory-intensive decode phase. In this work, we priori- tize optimizing the decode phase of LLM inference to fully leverage the strengths of FPGA architectures. B. Binary, Ternary, and Low-Bit Quantized Transformers Model quantization is a key technique for compressing LLMs to run on constrained devices. Most conventional quan- tization approaches target 8-bit or 4-bit representations, but recent work has pushed the boundary down to the binary regime. BitNet [6] introduced a method for training Transformers from scratch using 1-bit weights. Despite extreme quantiza- tion, BitNet maintained competitive perplexity through custom scaling and layer-wise normalization strategies. Building on this, BitNet-1.58 [7] introduced ternary weight representations (t 1, 0, 1u), striking a balance between expressiveness and compression. Both approaches highlight the potential of binary LLMs in terms of storage, throughput, and energy efficiency. Similarly, FBI-LLM [11] and OneBit [12] demonstrate fully binarized models trained using autoregressive distillation, achieving promising results on open-domain generation tasks. DeepSeek-R1 [8] presents a hybrid quantization strategy ap- plying ternary quantization to Mixture-of-Expert (MoE) layers, achieving up to 80 model size reduction on a 671B model. Beyond training-time quantization, post-training quantization (PTQ) also plays a role. BitDistiller [13] combines self- distillation with quantization-aware techniques to push 3- bit and 2-bit LLM performance to new levels. QuIP [14] introduces 2-bit quantization with incoherence processing and rounding guarantees. These works focus on algorithmic aspects. In contrast, TeLLMe provides a hardware-aligned solution for deploying such models in practice, offering both matmul reuse and pipeline fusion for edge execution. C. Edge-Focused LLM Acceleration on FPGA Deploying Transformers on FPGAs is challenging due to limited bandwidth and logic resources. Several works have explored quantized Transformer accelerators on embedded FPGAs. T-MAC [15] implements a table-lookup-based (TL-based) matrix multiplication (matmul) kernel for CPUs using low-bit weights and high-bit activations. It achieves notable perfor- mance on Apple M2 and Raspberry Pi 5, but being software- based, it lacks the deep hardware-level optimization required for maximum efficiency. Li et al. [9] successfully implemented a 4-bit quantized LLaMA2-7B model on the AMD KV260 platform. Although the model weights are quantized to 4-bit, the decoding compu- tations rely on unquantized FP16 activations, thereby requiring all operations to be conducted in FP16 and preventing the use of more efficient 4-bit computation units. Moreover, hardware acceleration is limited to the decoding stage and does not address the computational demands of the prefilling stage. LlamaF [16] targets LLaMA2-style models with int8 quan- tization on ZCU102. It leverages pipelined matrix-vector Quant Quant Deq Deq Deq K Proj Q Proj V Proj Output Proj Down Proj RMS Norm RMS Norm Softmax Input Embedding be N LM Head Linear Bitnet Transformer Decoder Block Happy Input Embedding to N LM Head Linear Bitnet Transformer Decoder Block be Input Embedding N LM Head Linear Bitnet Transformer Decoder Block to Rope Rope K Cache V Cache Quant De-quant Deq Deq Up Proj Gate Proj RMS Norm Element-wise Prefill (GEMM) Generation (GVMM) Make the choice MHA FFN TTFT TOPT TOPT Fig. 1: Breakdown of TeLLMe 1.58-bit Model Inference Process with Prefill and Generation units and asynchronous scheduling but does not address the demands of long-context decoding and ignore prefill state entirely. Edge-MoE [17] introduces a memory-efficient MoE vision transformer accelerator using dynamic task-level sparsity. A key technique is the specialized reordering to enable the data reuse of attention computation, which is the idea we extend in TeLLMe s prefill module. SECDA [18] SECDA designs the MatMul accelerator sup- porting block floating-point quantized operations on PYNQ, reducing latency by 11x compared to dual-core Arm NEON- based CPU execution for the TinyLlama model. However, the token per second is only 0.58, which means 2 seconds for one token generation. Compared to these, our work is the first to unify binary weight inference, prefill decoding support, and FPGA-level memory hierarchy optimization into one cohesive design. III. TELLME HARDWARE DESIGN As shown in Fig. 3, the accelerator design primarily consists of the following modules: (1) a table-look-up-based ternary matrix multiplication for both the decoding and prefill passes; (2) a specialized reverse reorder for prefill attention; (3) a unified decoding attention and language model head (LM Head); and (4) specialized functional units. A. Table-lookup-based Ternary MatMul on FPGA Table-lookup-based (TL-based) matrix multiplication (mat- mul) is a highly efficient method for ternary matmul in CPUs, leveraging specialized ARM NEON and AVX instructions for 128 256-bit table look-up operations. However, its limited table size often incurs frequent memory accesses and increased latency [15]. FPGAs offer an ideal solution by exploiting their intrinsic lookup table units (LUT) resources to support larger tables, yet prior FPGA works [19] primarily focus on module and design automation level optimizations rather than comprehensive dataflow or scheduling strategies. In this work, we present a TL-based ternary matmul implementation on FPGAs, coupled with an in-depth exploration of efficient pipeline scheduling to maximize performance. 1) Algorithm background: In general, the bit-wise oper- ation for mixed-precision matrix multiplication can be ex- pressed as follows: A b W A b n 1 ÿ i 0 2iWi n 1 ÿ i 0 2iA b Wi (1) Considering the ternary matrix multiplication with W P t 1, 0, 1u, the above equation can be simplified to: A b W A b W0, Wternary P t 1, 0, 1u (2) In this case, simple summation and subtraction can replace the multiplication process. However, the method of selecting -1, 0, and 1 to determine the summation and subtraction may not be optimal, as there are only a limited number of combinations of -1, 0, and 1, leading to repetitive computations for the corresponding A entries. Furthermore, when increasing computation parallelism by duplicating the selection unit of the adding and subtracting path, the resource consumption of the selection may exceed that of the TL tables themselves. This is because the multiple reading ports of the on-chip distributed RAM unit can support multiple accesses to the TL tables, requiring only additional buffers for addressing. The supportive ablation study will be presented in the next subsection. Algorithm 1: TL-based Ternary Matmul Input : A: Input activation stream (shape rMsrNs); Widx Offline preprocess(W): Offline-preprocessed weight indices (shape rN{pT GqsrKs); Output: O: Output activation stream (shape rMsrKs); Initialize:; TL TABLErNsr3Gs Ð 0 ; Table for all signed combinations A BLOCKrT ˆ Gs Ð 0 ; Activation buffer O BLOCKrKs Ð 0 ; Output vector accumulator for i Ð 0 to M 1 do for j Ð 0 to N 1 step T ˆ G do Load activation block for p Ð 0 to T ˆ G 1 do A BLOCKrps Ð A.read(); end for Set up values of TL_TABLE for t Ð 0 to T 1 do val1...G Ð A BLOCKrt ˆ G : pt 1q ˆ G 1s; TL TABLE set uppval1...Gq; end for Process hidden dimension for m Ð 0 to K step Q do for n Ð 0 to Q 1 do idx vec Ð B Y j T ˆG ]ı rm ns; for t Ð 0 to N 1 do TL TABLE idx Ð idx vecrts; O BLOCKrm ns Ð O BLOCKrm ns TL TABLErtsrTL TABLE idxs; end for end for end for end for Write output for p Ð 0 to K 1 do O.writepC BLOCKrpsq; O BLOCKrps Ð 0; end for end for Function Offline preprocess(W): return Encode every G value as an index in the matrix and pack every T values as a index vector idx vec; Function TL TABLE set up(val1...G): return return all 3G add and subtract combination; As described in Algorithm 1 and Fig. 2, the TL-based matmul can be divided into two stages: (1) preprocessing the weights into groups sized G, and (2) performing the online ternary matrix multiplication computation. Output row buffer Offline weight preprocessing 8 6 5 2 3 11 -2 4 6 1 5 6 2 4 5 6 4 2 2:LUT setup loop Look up table ADD SUBSTRACT 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 Group size Index vector length Feteched Index vectors Look up table Look up table 1:Look-up loop 3: Token Loop ACCUMULATOR index values return values Online Looking up and Accumulation ADD SUBSTRACT ADD SUBSTRACT Fig. 2: Dataflow and architecture of TL-based ternary matMul (G 4) In the preprocessing stage, assume that every G 3 ternary values are packed into a single index for TL table addressing, resulting in 3G 3 ˆ 3 ˆ 3 27 combinations. The index representation for this packing requires log2 27 5 bits. Let A P NMˆN and W P ternaryNˆK. The preprocessing of the weights involves encoding every group of G 3 ternary values into a 5-bit packed index. In the online stage, we perform vector-wise tiled matrix multiplication, where A and B. The first step is to establish the TL table using the precompute unit, which consists of 3G 27 sets of adders and subtractors to cover all possible combinations of dynamic activations. The packed-up indexes are then used to address and select the corresponding values from the TL table. Finally, the selected values are accumulated to generate a single vector O P N1ˆK in the output matrix. 2) Multi-TL-table dataflow design: In our design, we care- fully optimize the dataflow for TL-based matrix multiplication, as illustrated in Fig. 2. Assume we have a total of T tables. To better vectorize the TL-based matrix multiplication, the consecutive T indices can be grouped into an index vec- tor, enabling simultaneous access to different look-up tables. The weight index vector matrix can be rewritten as Widx P t5B, Tu N T ˆG ˆK. Widx are loaded onto the on-chip URAM. Regarding the scheduling, the innermost loop first performs vector operations to establish the T look-up tables simultane- ously, based on the first T ˆ G entries of the A matrix. Then, leveraging the multiple reading traits of the URAM [20], Q W index vectors are processed in parallel for TL table addressing, returning QˆT outcomes. The corresponding TL table return values are then accumulated into an output buffer of size K. The K index vectors on each row of W are traversed in steps of Q. The TL table addressing and accumulation process can be fully pipelined with an interval of one cycle, as there are Top-level control TL matmul unit Linear layer weights BRAM Decode loader Prefill loader Softmax score unit Decode loader LM head loader Quant Dequant Fused unit loader Reverse scheduler ROPE RMSnorm Element-wise add Element-wise mul Quant DRAM Special Function Unit Quant Online softmax unit Reverse Attention Engine Tenary Matmul Engine Decoding Attention Engine On-chip decoding hidden state BRAM Fig. 3: System architecture of TeLLMe. no inter-iteration dependencies. After the first T ˆ G entries of the A matrix are processed, the M values of the row of A are traversed in steps of T ˆ G in the intermediate loop. Finally, the outermost loop traverses the different row vectors in A, corresponding to the tokens in the prefill stage of the LLM. As for comparison, the LUT consumption of different matmul unit design methods is also presented in Table I. The configuration for the TL-based matmul is set as G 3, T 32, and Q 16. All levels of parallelism for the modules are set to be the same. Our design consumes 52094 LUTs, while the naive implementation, which selects whether to add or subtract, requires 7905 more LUTs. Another approach involves storing half of the possible combinations (13 out of 27) instead of all combinations and using the index to determine whether the value should be negative. This approach results in a smaller distributed RAM size, aiming to save LUTs. However, after synthesizing, it consumed 9209 more LUTs. TABLE I: LUT Consumption Comparison of Different Mat- mul Unit Design Methods Approach LUT Consumption Difference TL-based(Our Design) 52,094 Naive Implementation 59,999 7,905 Partial Storage 61,303 9,209 B. Specialized Reverse Reorder for Prefill Attention 1) Prefill Challenge on edge FPGA: Prefill is one of the most challenging components for edge FPGAs. This part of LLM requires significant resources and bandwidth for multi-token computation, especially for attention computation, which involves softmax and matrix-to-matrix multi-head op- erations with a complexity of N 2. Given the limited memory bandwidth and finite computational units, the computation order of prefill attention must be carefully scheduled to meet these requirements. Otherwise, it may be constrained by the bandwidth limitations of the edge FPGA, as shown in the naive attention scheduling in Fig. 5. In the context of edge FPGA vision transformers, [17] proposed a state-of-the-art dense attention scheduling strategy, Attention map for dense attention scheduling Attention map for naive attention scheduling Attention map for reverse reordering and kernel fusion Fig. 4: The visualization of scheduling on the attention map (number of computation core p 4, beige stands for attention mask). 1 2 N 4 1 Load Load Iter Compute Compute Compute Compute N 2 4 No data resuse for the vectors Some of the computed values are invalid because of casual masks Fig. 5: Naive attention scheduling (p 4). as shown in Fig. 6 and Fig. 4, which takes into account the reuse of the Q values across different tokens. However, unlike vision transformers, LLMs use a causal attention mask. As a result, the dense scheduling approach wastes computational resources on zero masks. Furthermore, the fusion of operations such as Q b K, softmax, and S b V can reduce the additional accesses to DRAM. The state-of-the-art kernel fusion implementation for resource-abundant GPUs is Flash Attention. However, GPU- optimized computation is not suitable for FPGAs, as GPUs have many more computational cores and much larger on-chip SRAM compared to the on-chip BRAM URAM available on FPGAs. To address these challenges, we propose the reverse attention method, which utilizes fused attention and reverse reorder scheduling, specifically tailored for edge FPGAs. TABLE II: Comparison of different attention approaches. Approach Data Block Load Iteration Count Bandwidth Reverse Scheduling (Our Design) N2 2p N 2 N2 2p N 2 1 naive scheduling N2 N N2 p p dense scheduling [17] N2 p N p 1 N2 p p 1 1 2) Reverse Attention: The reverse attention scheduling is depicted in Fig. 7. Assume that the current length of the prefill Load Load Iter 1 2 3 4 N 1 N 2 N 3 N 4 Compute Compute Compute Compute N 2 4 N 2 4 1 N 2 4 2 N 2 4 3 Some of the computed values are invalid because of casual masks Fig. 6: Dense attention scheduling (p 4). tokens is N, with 1 ă i ď N and 1 ă j ď N representing the current token indices for q and k, v, respectively. There are a total of h heads. The kernel fusion computation can be considered a special case of Flash Attention V2 [21] when the block size is equal to 1. The head-wise formula for the case with two consecutive Load Load Load Compute Compute Compute Compute Iter 1 2 3 4 N 1 N 2 N 3 N 4 N(4 N) 8 N(4 N) 8 - 1 N(4 N) 8 - 2 N(4 N) 8 - 3 vectors are fully reused and transfered data blocks is less compared to naive scheduling Avoid invalid masked computation and relevant data transfer compared to dense scheduling Fig. 7: Reverse attention scheduling (p 4). blocks can be written as follows: mp1q sp1q ℓp1q esp1q mp1q op1q esp1q mp1qvp1q mp2q max mp1q, sp2q m ℓp2q emp1q mp2qℓp1q esp2q mp2q esp1q m esp2q m ℓ pp2q esp2q mp2q{ℓp2q op2q op1q{emp1q mp2q esp2q mp2qvp2q esp1q mvp1q esp2q mvp2q op2q op2q{ℓp2q o (3) where m denotes the maximum value, s qi b ki represents the MAC result of the vector dot product, ℓis the denominator factor, and o is the numerator vector. The upper index indicates the current step in the kernel fusion computation. Regarding scheduling, instead of starting from the first token q1, our schedule begins from qN 1. Specifically, the level of parallelism is set to p (as illustrated in the figure for p 4). The factor p also implies that the on-chip BRAM can store p tokens of qi. In each iteration, one qi token is loaded onto the on-chip memory (with the first batch loading qN to qN 3). Simultaneously, the corresponding kj and vj tokens are loaded for computation. After all N kj and vj tokens have been loaded and the fused-kernel computation is completed, the next iteration will evict p kj and vj tokens, starting from kN 3 and vN 3, to avoid redundant computations arising from the causal attention mask. The iteration continues until all 1 ă i ď N, 1 ă j ď N are traversed. In this approach, the only required input buffers are for p qi tokens, one kj, and one vj. Additionally, the intermediate buffers include: hˆp multi-head MAC intermediate results s, h ˆ p multi-head previous max values m, and h ˆ p intermediate denominators ℓ. After introducing the reverse attention process, a compari- son between naive attention, dense attention in Edge Moe [17], and our proposed reverse attention is provided in Table II. The comparison indicates that the iteration count for reverse attention is the lowest, and the required bandwidth remains constant. Furthermore, the redundant outcome rate of the computation core can be directly assessed from the attention map in Fig. 4. The naive and dense scheduling approaches do not account for the causal mask, leading to many redundant computed values. C. Hardware Specialization and Reuse for Decoding-Phase Attention and LM Head The previous section focuses on optimizing the Prefill phase in LLM inference. Since we consider efficient end-to-end LLM inference on single edge Device, both Prefill and Decoding phases require efficient implementation to maximize global performance. The attention mechanism is a core component in both phases, but its computational characteristic differs. In the prefill phase, calculating attention involves operations on matrices representing the entire input sequence. In the decoding phase, it involves operations between the vector representation of the single new token and the cached matrices of keys and values. This difference presents an opportunity for hardware specialization. In the decoding phase, a single new token is generated per step. Let the total sequence length (prompt already generated tokens) be M. The query q is now a 1 ˆ N vector corresponding to the new token. The key (Kcache) and value (Vcache) matrices contain the cached representations of all M previous tokens and are of dimensions M d. The core attention computation involves: 1. Calculating attention scores: q ˆ KT cache (a 1 ˆ N vector multiplied by a N ˆ M matrix, resulting in a 1 ˆ M score vector). 2. Applying softmax to the scores. Multiplying the resulting 1 ˆ M vector by Vcache (an M ˆ N matrix) to get the 1 ˆ N output vector. The computation involves primarily matrix-vector and vector-vector operations. The computational load per step (OpNdq) is significantly lower than the total prefill computation. However, this phase requires fetching the large Kcache and Vcache matrices from memory (e.g., off-chip DRAM) in every step. Consequently, the decoding phase is often memory-bandwidth bound, especially as the sequence length M grows. Since computation is less intensive and latency is dominated by memory access (fetching the KV cache), massive parallelism is inefficient and wastes resources. A more sequential or lower-parallelism computation unit is sufficient. This approach significantly reduces the required on- chip hardware resources (e.g., number of PEs, buffer sizes) compared to the prefill unit. The profiling result in Figure 8 clearly shows that the attention in the decoding phase is memory-bounded, while the prefill phase is compute-bounded. This demonstrates the necessity of lightweight decoding im- plementation to save on-chip resources for the Prefill phase. Fig. 8: Characterization of Attention Module during Pre- fill Decoding Phase 1) Reuse of Decoding Attention for LM Head: Following the processing thr-ough N transformer blocks in typical LLM architectures like LLaMA, the final step before token genera- tion involves the LM Head. This component performs a crucial linear projection, mapping the final hidden state output from the last transformer block to a vector of logits representing the probability distribution over the entire vocabulary. For inference, particularly during the auto-regressive decoding phase, where one token is generated at a time, the input to the LM Head is the final hidden state vector corresponding to the token being predicted. This vector has dimensions r1, Ns, where N represents the hidden state dimension (e.g., N 1536 for some models). The LM Head uses a large weight matrix of dimensions rN, V s, where V is the vocab- ulary size (e.g., V 32000), to compute the output logit vector of dimensions r1, V s. The core computation is thus a matrix-vector multiplication: r1, Ns ˆ rN, V s Ñ r1, V s. This computation (O(HV)) has similar characteristics to decoding attention: it s a matrix-vector operation with limited data reuse opportunities, heavily reliant on fetching a large matrix (the Kcache or the LM Head weights), making it memory-bound (especially as V " H). Given this similarity, we reuse the decoding-phase atten- tion hardware to execute the LM Head computation. This eliminates the need for a dedicated LM Head unit, yielding substantial area and power savings. The performance impact is negligible because the LM Head executes only once per generated token, whereas attention occurs N times (once per layer). Reusing the Decoding-phase Attention hardware for both attention and the LM Head precludes a fully fused attention pipeline within it. We therefore adopt a decoupled execution model for attention sub-steps during decoding: (1) Atten- tion Score Computation: s q ˆ KT cache . (2) Softmax: p softmaxpsq. (3) Value Aggregation: Compute the final attention output o p ˆ Vcache. This is efficient because the intermediate score probability vector (1 M) is small enough to be buffered on-chip BRAM with minimal latency penalty. On the contrary, for the Prefill phase, the intermediate N N attention score matrix would be far too large for practical on-chip buffering. Thus it requires a fully fused attention pipeline that integrates attention score calculation, softmax, and value aggregation in a single, uninterrupted hardware pass to minimize off-chip data movement. D. Implementation and Optimization of Special Function Units Besides the core modules discussed in previous sections, LLM inference also relies on several essential special func- tions, including Quantization Dequantization, RMSNorm, and Activation Function. These operations are computationally less intensive, therefore, our strategy focuses on lightweight hardware implementations and operator fusion to minimize overhead. For efficient data handling, vector data for these modules is processed in 256-bit packets, aligning with AXI bus width. Quantization Dequantization: The activations need to be quantized before the ternary Linear modules. We employ Absmax Quantization, which involves two passes: (1) finding the maximum absolute value to compute the scale factor, and (2) applying this scale element-wise. Figure 1 shows that quantization follows the RMSNorm. We fuse these operations to reduce data movement. The dequantization is fused into the Linear output pipeline. RMSNorm: RMSNorm involves two passes. The first cal- culates the Root Mean Square of the input x: RMSpxq b 1 n řN i 1 x2 i . The second pass normalizes the input by di- viding by the RMS value and multiplying by a learned scaling parameter γ. Recognizing that both RMSNorm and Absmax Quantization involve a two-pass traverse, we fuse these four logical steps into two optimized hardware passes. This significantly minimizes the data movement. Activation function: The element-wise SiLU activation (x 1 1 e x ) is required after the Gate projection in Feed- Forward Network (FFN) block. The SiLU is pipelined and fused directly into the preceding Linear module, effectively hiding its latency. IV. EXPERIMENTAL RESULTS AND ANALYSIS A. Experiment Setup We implement the TeLLMe accelerator using high-level synthesis C C in Vitis HLS and Vivado 2023.1. We eval- uate our design on Kria KV260 (Zynq UltraScale XCK26 MPSoC). To ensure timing closure, we use 250 MHz for the final bitstream generation. B. LLM Inference Performance and Resource Breakdown Figure 9 shows the key metrics in LLM inference, including Decoding Throughput (token generation speed) and Prefill Time (time-to-first-token). We evaluate TeLLMe under differ- ent configurations [prompt size, generate size], note that the total tokens prompt size generate size. TeLLMe achieves ą 9 tokens s in 512 context lengths and 8 tokens s in 1024 context lengths. For prompt size ă 128, TeLLMe achieves TABLE III: Comparison of FPGA-based LLM Accelerators Work Device LUT FF BRAM DSP MHz Power (W) BW (GB s) Model Throughput (tokens s) Accelerate Prefill? SECDA [18] PYNQ TinyLLaMA W4 0.58 LlamaF [16] ZCU102 164K 171K 223 528 205 5.08 21.3 TinyLLaMA W8 1.50 Li et al. [9] KV260 78K 105K 36.5 291 300 6.57 19.2 LLaMA2-7B W4 4.90 TeLLMe(Ours) KV260 108K 155K 206 356 250 6.72 19.2 Bitnet W1.58 9.51 Not directly comparable since our TeLLMe have additional logic to accelerate prefill stage 1s Prefill size. TeLLMe demonstrates practical viability and deployment potential in real-world applications. The resource breakdown is shown in Table IV. Regarding BRAM usage, most of it is consumed by the top-level AXI buffer. DSP resources are mainly utilized by the Attention modules due to their INT8 precision. LUTs are primarily consumed by the TL-based matmul unit for the TL tables. URAM is used by the matmul weight buffer to support ping- pong operations. Fig. 9: TeLLMe LLM Inference Performance TABLE IV: Resource Consumption Breakdown Module BRAM DSP FF LUT URAM Control Data Transfer 120 0 24973 5897 Attention (Prefill Phase) 46 122 25629 33069 Attention (Decoding Phase) 24 134 17465 7028 TL-based Matmul Unit 0 0 35765 52094 48 RMSNorm 16 28 6202 5933 Misc (Add, Mul, RoPE, etc) 0 72 45804 4973 Total 206 356 155838 108994 48 (71 ) (28 ) (66 ) (93 ) (75 ) C. Comparison with Existing Edge FPGA Work Table III presents a comparison between TeLLMe and prior FPGA-based LLM accelerators. Despite differences in model scale and quantization schemes, TeLLMe achieves a peak decoding throughput of 9.51 tokens per second representing up to a 16.4ˆ improvement over previous work while supporting both prefill and decoding stages on a single edge FPGA device. As highlighted in the table, none of the existing edge FPGA-based solutions implement on-device prefill, often citing its computational intensity as unsuitable for resource- constrained hardware. While we acknowledge the challenges associated with prefill on FPGAs, we argue that full on- device support is essential for a complete and self-contained TABLE V: Performance Comparison with Mobile CPU Device Category Decode (tokens s) Time-to- first-token (s) Prefill (tokens s) Model Size (MB) Snapdragon 8 Gen 3 1B BF16 (baseline) 19.2 1.0 60.3 2358 1B SpinQuant 50.2 0.3 260.5 1083 1B QLoRA 45.8 0.3 252.0 1127 3B BF16 (baseline) 7.6 3.0 21.2 6129 3B SpinQuant 19.7 0.7 89.7 2435 3B QLoRA 18.5 0.7 88.8 2529 KV260 FPGA 0.7B TeLLMe 9.51 0.55 116.4 257 Time-to-first-token (prefill delay) is measured with a prompt length 64 edge deployment. Relying on external hosts to perform prefill introduces additional concerns regarding system complexity, data privacy, and scalability. Our detailed prefill performance is presented in Figure 9 and Table V. D. Comparison with Mobile CPU Despite the significant technological disparity between KV260 and modern mobile SoCs, our TeLLMe design demon- strates highly competitive performance in key inference met- rics. As shown in Table V, TeLLMe achieves a prefill latency of 0.55 seconds comparable to the 0.3 0.7 seconds observed on the Qualcomm Snapdragon 8 Gen 3, a device fabricated in an advanced 4nm process with integrated LPDDR5x memory and substantially higher bandwidth. In contrast, the KV260 is based on a 16nm process and relies on DDR4 mem- ory with much lower bandwidth. This makes our ability to match prefill performance particularly notable, as prefill is typically compute-bound and less amenable to acceleration on bandwidth-constrained FPGAs. This result highlights the ef- fectiveness of our architectural optimizations including TL- based tenary matmul, a bandwidth-efficient attention module with fused operation and a Reversed Attention reordering scheme to accelerate prefill. While TeLLMe s decoding throughput (9.51 tokens s) lags behind that of mobile SoCs, this gap is largely attributable to the KV260 s limited external memory bandwidth, which disproportionately affects the memory-bound decode phase. We emphasize that this limitation is architectural rather than algorithmic; our design scales favorably to higher-bandwidth platforms such as HBM-enabled FPGAs or custom ASICs. Taken together, these results validate TeLLMe as the first binary LLM accelerator on edge FPGA to support full in- ference including both prefill and decoding with energy efficiency and architectural flexibility that position it well for future edge AI deployments. V. CONCLUSION We introduced TeLLMe, the first end-to-end FPGA ac- celerator optimized for ternary LLM inference across both prefill and decoding stages. By co-optimizing compute, mem- ory, and scheduling, TeLLMe employs a table-lookup-based matmul engine that reuses grouped activations and online precomputations across projection and feedforward layers for efficient ternary matrix operations. A fused attention module with reversed attention and Flash Attention-style kernel fusion reduces bandwidth demands, eliminates redundant masked operations, and supports parallelism. Running under 7 W, TeLLMe achieves up to 9.51 tokens s and supports 1024- token contexts, outperforming mobile SoCs at significantly lower power. It delivers prefill latencies of 0.55 1.15 s for prompts of 64 128 tokens. To our knowledge, TeLLMe is the first real-hardware FPGA accelerator to fully support ternary LLMs end-to-end, establishing a new benchmark for efficient, low-latency edge inference. REFERENCES [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod- els are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020. [2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi ere, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [3] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [4] Y. Qiao, M. Alnemari, and N. Bagherzadeh, A two-stage efficient 3- d cnn framework for eeg based emotion recognition, in 2022 IEEE International Conference on Industrial Technology (ICIT). IEEE, 2022, pp. 1 8. [5] A. Ding, Y. Qiao, and N. Bagherzadeh, Bnn an ideal architecture for acceleration with resistive in memory computation, IEEE Transactions on Emerging Topics in Computing, vol. 11, no. 2, pp. 281 291, 2023. [6] H. Wang, S. Ma, L. Dong, and et al., Bitnet: Scaling 1-bit transformers for large language models, arXiv preprint arXiv:2310.11453, 2023. [7] S. Ma, H. Wang, L. Ma, and et al., The era of 1-bit llms: All large language models are in 1.58 bits, arXiv preprint arXiv:2402.17764, 2024. [8] D. Guo, D. Yang, H. Zhang, and et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [9] J. Li, T. Li, G. Shen, D. Zhao, Q. Zhang, and Y. Zeng, Pushing up to the limit of memory bandwidth and capacity utilization for efficient llm decoding on embedded fpga, arXiv preprint arXiv:2502.10659, 2025. [10] H. Chen, J. Zhang, Y. Du, S. Xiang, Z. Yue, N. Zhang, Y. Cai, and Z. Zhang, Understanding the potential of fpga-based spatial accel- eration for large language model inference, ACM Transactions on Reconfigurable Technology and Systems, vol. 18, no. 1, pp. 1 29, 2024. [11] L. Ma, M. Sun, and Z. Shen, Fbi-llm: Scaling up fully bina- rized llms from scratch via autoregressive distillation, arXiv preprint arXiv:2407.07093, 2024. [12] Y. Xu, X. Han, Z. Yang, and et al., Onebit: Towards extremely low-bit large language models, in Advances in Neural Information Processing Systems (NeurIPS), 2024. [13] D. Du, Y. Zhang, S. Cao, and et al., Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [14] J. Chee, Y. Cai, V. Kuleshov, and C. De Sa, Quip: 2-bit quan- tization of large language models with guarantees, arXiv preprint arXiv:2307.13304, 2023. [15] J. Wei, S. Cao, T. Cao, and et al., T-mac: Cpu renaissance via table lookup for low-bit llm deployment on edge, arXiv preprint arXiv:2407.00088, 2024. [16] H. Xu, Y. Li, and S. Ji, Llamaf: An efficient llama2 architecture accelerator on embedded fpgas, arXiv preprint arXiv:2409.11424, 2024. [17] R. Sarkar, H. Liang, Z. Fan, Z. Wang, and C. Hao, Edge-moe: Memory- efficient multi-task vision transformer architecture with task-level spar- sity via mixture-of-experts, in IEEE ACM International Conference on Computer-Aided Design (ICCAD), 2023, pp. 1 9. [18] J. Haris, R. Saha, W. Hu, and J. Cano, Designing efficient llm accelerators for edge devices, arXiv preprint arXiv:2408.00462, 2024, accessed: 2025-04-20. [Online]. Available: 00462 [19] D. Gerlinghoff, B. Choong, R. Goh, W.-F. Wong, and T. Luo, Table- lookup mac: Scalable processing of quantised neural networks in fpga soft logic, 04 2024, pp. 235 245. [20] AMD, Uram storage bind, 2024. [Online]. Available: amd.com r 2024.2-English ug1399-vitis-hls pragma-HLS-bind storage [21] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R e, Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. [Online]. Available:\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nTeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefill and Decode on Edge FPGAs Ye Qiao , Zhiheng Chen , Yifan Zhang, Yian Wang, Sitao Huang Department of Electrical Engineering and Computer Science University of California, Irvine, USA {yeq6, zhihenc5, yifanz58, yianw11, Abstract Deploying large language models (LLMs) on edge platforms is challenged by their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as 1.58 bits with minimal accuracy loss, edge deployment is still constrained by limited on-chip resources, power budgets, and the often-neglected latency of the prefill phase. We present TeLLMe, the first ternary LLM accelerator for low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and autoregressive decoding using 1.58-bit weights and 8-bit activations. Our contributions include: (1) a table-lookup matrix engine for ternary matmul that merges grouped activations with online precomputation to minimize resource use; (2) a fused, bandwidth-efficient attention module featuring a reversed reordering scheme to accelerate prefill; and (3) a tightly integrated normalization and quanti- zation dequantization unit optimized for ultra-low-bit inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens s throughput over 1,024-token contexts and prefill latencies of 0.55 1.15 s for 64 128 token prompts, marking a significant energy-efficiency advance and establishing a new edge FPGA benchmark for generative AI. I. INTRODUCTION Large Language Models (LLMs) have achieved remarkable progress in recent years, powering state-of-the-art performance in natural language processing tasks such as machine transla- tion, code generation, question answering, and conversational AI. Models like GPT-3[1], LLaMA[2], and DeepSeek-R1[3] have shown that increasing model size significantly improves generalization and task performance. However, this scaling comes with substantial costs in terms of computational de- mands, memory usage, and energy consumption.\n\n--- Segment 2 ---\nModels like GPT-3[1], LLaMA[2], and DeepSeek-R1[3] have shown that increasing model size significantly improves generalization and task performance. However, this scaling comes with substantial costs in terms of computational de- mands, memory usage, and energy consumption. Edge deployment of LLMs, i.e., running these models on low-power, resource-constrained devices such as embedded systems, FPGAs, or mobile SoCs, is a critical enabler for privacy-preserving, latency-sensitive, and autonomous appli- cations. However, such a deployment remains challenging due to the gap between LLM complexity and the limited memory bandwidth, memory capacity, compute capacity, and power budgets on edge platforms. To bridge this gap, recent research has focused on extreme model compression, particularly through low-bit quantization[4], [5]. Pioneering work such as BitNet [6] demonstrated that Transformer models can be trained with 1- bit weights, while BitNet-1.58 [7] and DeepSeek [8] extend Equal contribution this to ternary quantization (t 1, 0, 1u), achieving near- parity with full-precision models. These innovations signifi- cantly reduce model size and energy cost, making LLMs more viable for edge execution. However, deploying these compressed models on real hard- ware, especially FPGAs, presents unique challenges. Unlike cloud-scale GPUs, edge FPGAs have strict constraints on on- chip memory (BRAM URAM), external DRAM bandwidth, and energy budgets. Furthermore, the requirements of autore- gressive decoding (such as growing key-value (KV) caches, long context handling, and latency sensitivity) exacerbate these limitations. While most prior works focus either on model quantization or software acceleration, there is still no a systematic hardware-software co-optimization solution that fully exploit the benefits of extreme low bitwidth LLMs while meeting the computing demands of edge inference. One critical yet often overlooked issue in edge LLM deploy- ment is the disproportionate emphasis on decoding throughput, while prefill latency remains largely ignored. For example, [9] demonstrates efficient LLM decoding on embedded FPGAs but neglects the prefill stage entirely.\n\n--- Segment 3 ---\nOne critical yet often overlooked issue in edge LLM deploy- ment is the disproportionate emphasis on decoding throughput, while prefill latency remains largely ignored. For example, [9] demonstrates efficient LLM decoding on embedded FPGAs but neglects the prefill stage entirely. However, prefill latency is not merely a technical detail, it is a primary bottleneck for user experience and safety in latency-sensitive edge AI applications. While prefill overhead may be negligible in cloud environments, on-device deployment places it squarely on the critical execution path. Despite its importance, prefill optimization remains significantly underexplored and demands more serious attention from the community. To address these limitations, we present TeLLMe the Tenary Large Language Model Edge Accelerator the first edge FPGA-based accelerator specifically designed for ternary LLM inference with full support for both prefill and decoding stages. TeLLMe enables low-latency, energy-efficient deploy- ment of LLMs on resource-constrained platforms by target- ing cost-effective FPGAs such as AMD KV260. It supports ternary-quantized weights (1.58-bit) and 8-bit activations. Our design co-optimizes compute, memory, and scheduling efficiency, key contributions are as follows: We develop the first end-to-end edge FPGA accelerator for ternary LLMs supporting both prefill and decoding stages. We propose Table-lookup-based ternary matmul, an effi- cient and resource-saving matrix multiplication unit that arXiv:2504.16266v2 [cs.AR] 24 Apr 2025 specially optimized for FPGA, reusing grouped activa- tions and online computation for ternary matmuls across projection and FFN layers. We introduce a fused attention unit for prefill and de- coding, incorporating a novel reversed attention mecha- nism and fully fused pipeline to minimize off-chip data movement, avoid redundant masked computation, and guarantee the parallelism at the same time. We deliver up to 9.51 tokens sec generation in up to 1024 token contexts while consuming less than 7W power, outperforming mobile SoCs with much lower power budgets.\n\n--- Segment 4 ---\nWe introduce a fused attention unit for prefill and de- coding, incorporating a novel reversed attention mecha- nism and fully fused pipeline to minimize off-chip data movement, avoid redundant masked computation, and guarantee the parallelism at the same time. We deliver up to 9.51 tokens sec generation in up to 1024 token contexts while consuming less than 7W power, outperforming mobile SoCs with much lower power budgets. TeLLMe achieves a prefill latency from 0.55s to 1.15s for prompt sizes of 64 to 128 tokens and delivers up to 9.51 tokens s decoding throughput with support for 1024-token context lengths on edge FPGAs, all while operating under 7 watts of power consumption. This marks a significant ad- vancement over existing mobile-edge devices and prior FPGA- based accelerators, which typically require higher precision and greater power budgets. To the best of our knowledge, TeLLMe is the first ac- celerator to provide end-to-end support for ternary LLM inference including both prefill and decoding stages on real FPGA hardware, establishing a new baseline for energy- efficient, low-latency generative AI at the edge. II. BACKGROUND AND RELATED WORK A. LLM Basic A typical LLM, such as LLaMA, is composed of multiple identical transformer blocks, each containing an attention module followed by a multilayer perceptron (MLP) module, as illustrated in Fig. 1. Within each attention module, three linear projections are used to compute the query (Q), key (K), and value (V) representations. These are then processed by a multi-head attention mechanism that incorporates both the current QKV tensors and historical KV cache. The MLP module consists of an up-projection and down-projection layer, along with an additional gating projection applied to the up- projection output. The generative inference of LLMs is typically divided into two stages: the prefill phase and the decode phase (generation), as shown in Figure 1. During the prefill phase, the entire prompt is processed through the full model stack to produce the first output token. This phase involves parallel computation across multiple input tokens and is dominated by compute-intensive matrix matrix multiplications, particularly within the linear projection layers.\n\n--- Segment 5 ---\nDuring the prefill phase, the entire prompt is processed through the full model stack to produce the first output token. This phase involves parallel computation across multiple input tokens and is dominated by compute-intensive matrix matrix multiplications, particularly within the linear projection layers. In contrast, the decode phase proceeds in an autoregressive fashion, generating one token at a time by feeding the previously generated token back into the model. This phase is typically memory-bound due to its reliance on KV cache lookup and smaller matrix vector operations. Following the observations in Chen et al. [10] , while FPGAs are generally less efficient than GPUs during the compute- heavy prefill stage, they exhibit competitive advantages during the memory-intensive decode phase. In this work, we priori- tize optimizing the decode phase of LLM inference to fully leverage the strengths of FPGA architectures. B. Binary, Ternary, and Low-Bit Quantized Transformers Model quantization is a key technique for compressing LLMs to run on constrained devices. Most conventional quan- tization approaches target 8-bit or 4-bit representations, but recent work has pushed the boundary down to the binary regime. BitNet [6] introduced a method for training Transformers from scratch using 1-bit weights. Despite extreme quantiza- tion, BitNet maintained competitive perplexity through custom scaling and layer-wise normalization strategies. Building on this, BitNet-1.58 [7] introduced ternary weight representations (t 1, 0, 1u), striking a balance between expressiveness and compression. Both approaches highlight the potential of binary LLMs in terms of storage, throughput, and energy efficiency. Similarly, FBI-LLM [11] and OneBit [12] demonstrate fully binarized models trained using autoregressive distillation, achieving promising results on open-domain generation tasks. DeepSeek-R1 [8] presents a hybrid quantization strategy ap- plying ternary quantization to Mixture-of-Expert (MoE) layers, achieving up to 80 model size reduction on a 671B model. Beyond training-time quantization, post-training quantization (PTQ) also plays a role. BitDistiller [13] combines self- distillation with quantization-aware techniques to push 3- bit and 2-bit LLM performance to new levels.\n\n--- Segment 6 ---\nBeyond training-time quantization, post-training quantization (PTQ) also plays a role. BitDistiller [13] combines self- distillation with quantization-aware techniques to push 3- bit and 2-bit LLM performance to new levels. QuIP [14] introduces 2-bit quantization with incoherence processing and rounding guarantees. These works focus on algorithmic aspects. In contrast, TeLLMe provides a hardware-aligned solution for deploying such models in practice, offering both matmul reuse and pipeline fusion for edge execution. C. Edge-Focused LLM Acceleration on FPGA Deploying Transformers on FPGAs is challenging due to limited bandwidth and logic resources. Several works have explored quantized Transformer accelerators on embedded FPGAs. T-MAC [15] implements a table-lookup-based (TL-based) matrix multiplication (matmul) kernel for CPUs using low-bit weights and high-bit activations. It achieves notable perfor- mance on Apple M2 and Raspberry Pi 5, but being software- based, it lacks the deep hardware-level optimization required for maximum efficiency. Li et al. [9] successfully implemented a 4-bit quantized LLaMA2-7B model on the AMD KV260 platform. Although the model weights are quantized to 4-bit, the decoding compu- tations rely on unquantized FP16 activations, thereby requiring all operations to be conducted in FP16 and preventing the use of more efficient 4-bit computation units. Moreover, hardware acceleration is limited to the decoding stage and does not address the computational demands of the prefilling stage. LlamaF [16] targets LLaMA2-style models with int8 quan- tization on ZCU102.\n\n--- Segment 7 ---\nMoreover, hardware acceleration is limited to the decoding stage and does not address the computational demands of the prefilling stage. LlamaF [16] targets LLaMA2-style models with int8 quan- tization on ZCU102. It leverages pipelined matrix-vector Quant Quant Deq Deq Deq K Proj Q Proj V Proj Output Proj Down Proj RMS Norm RMS Norm Softmax Input Embedding be N LM Head Linear Bitnet Transformer Decoder Block Happy Input Embedding to N LM Head Linear Bitnet Transformer Decoder Block be Input Embedding N LM Head Linear Bitnet Transformer Decoder Block to Rope Rope K Cache V Cache Quant De-quant Deq Deq Up Proj Gate Proj RMS Norm Element-wise Prefill (GEMM) Generation (GVMM) Make the choice MHA FFN TTFT TOPT TOPT Fig. 1: Breakdown of TeLLMe 1.58-bit Model Inference Process with Prefill and Generation units and asynchronous scheduling but does not address the demands of long-context decoding and ignore prefill state entirely. Edge-MoE [17] introduces a memory-efficient MoE vision transformer accelerator using dynamic task-level sparsity. A key technique is the specialized reordering to enable the data reuse of attention computation, which is the idea we extend in TeLLMe s prefill module. SECDA [18] SECDA designs the MatMul accelerator sup- porting block floating-point quantized operations on PYNQ, reducing latency by 11x compared to dual-core Arm NEON- based CPU execution for the TinyLlama model. However, the token per second is only 0.58, which means 2 seconds for one token generation. Compared to these, our work is the first to unify binary weight inference, prefill decoding support, and FPGA-level memory hierarchy optimization into one cohesive design. III. TELLME HARDWARE DESIGN As shown in Fig. 3, the accelerator design primarily consists of the following modules: (1) a table-look-up-based ternary matrix multiplication for both the decoding and prefill passes; (2) a specialized reverse reorder for prefill attention; (3) a unified decoding attention and language model head (LM Head); and (4) specialized functional units.\n\n--- Segment 8 ---\nTELLME HARDWARE DESIGN As shown in Fig. 3, the accelerator design primarily consists of the following modules: (1) a table-look-up-based ternary matrix multiplication for both the decoding and prefill passes; (2) a specialized reverse reorder for prefill attention; (3) a unified decoding attention and language model head (LM Head); and (4) specialized functional units. A. Table-lookup-based Ternary MatMul on FPGA Table-lookup-based (TL-based) matrix multiplication (mat- mul) is a highly efficient method for ternary matmul in CPUs, leveraging specialized ARM NEON and AVX instructions for 128 256-bit table look-up operations. However, its limited table size often incurs frequent memory accesses and increased latency [15]. FPGAs offer an ideal solution by exploiting their intrinsic lookup table units (LUT) resources to support larger tables, yet prior FPGA works [19] primarily focus on module and design automation level optimizations rather than comprehensive dataflow or scheduling strategies. In this work, we present a TL-based ternary matmul implementation on FPGAs, coupled with an in-depth exploration of efficient pipeline scheduling to maximize performance. 1) Algorithm background: In general, the bit-wise oper- ation for mixed-precision matrix multiplication can be ex- pressed as follows: A b W A b n 1 ÿ i 0 2iWi n 1 ÿ i 0 2iA b Wi (1) Considering the ternary matrix multiplication with W P t 1, 0, 1u, the above equation can be simplified to: A b W A b W0, Wternary P t 1, 0, 1u (2) In this case, simple summation and subtraction can replace the multiplication process. However, the method of selecting -1, 0, and 1 to determine the summation and subtraction may not be optimal, as there are only a limited number of combinations of -1, 0, and 1, leading to repetitive computations for the corresponding A entries. Furthermore, when increasing computation parallelism by duplicating the selection unit of the adding and subtracting path, the resource consumption of the selection may exceed that of the TL tables themselves.\n\n--- Segment 9 ---\nHowever, the method of selecting -1, 0, and 1 to determine the summation and subtraction may not be optimal, as there are only a limited number of combinations of -1, 0, and 1, leading to repetitive computations for the corresponding A entries. Furthermore, when increasing computation parallelism by duplicating the selection unit of the adding and subtracting path, the resource consumption of the selection may exceed that of the TL tables themselves. This is because the multiple reading ports of the on-chip distributed RAM unit can support multiple accesses to the TL tables, requiring only additional buffers for addressing. The supportive ablation study will be presented in the next subsection.\n\n--- Segment 10 ---\nThis is because the multiple reading ports of the on-chip distributed RAM unit can support multiple accesses to the TL tables, requiring only additional buffers for addressing. The supportive ablation study will be presented in the next subsection. Algorithm 1: TL-based Ternary Matmul Input : A: Input activation stream (shape rMsrNs); Widx Offline preprocess(W): Offline-preprocessed weight indices (shape rN{pT GqsrKs); Output: O: Output activation stream (shape rMsrKs); Initialize:; TL TABLErNsr3Gs Ð 0 ; Table for all signed combinations A BLOCKrT ˆ Gs Ð 0 ; Activation buffer O BLOCKrKs Ð 0 ; Output vector accumulator for i Ð 0 to M 1 do for j Ð 0 to N 1 step T ˆ G do Load activation block for p Ð 0 to T ˆ G 1 do A BLOCKrps Ð A.read(); end for Set up values of TL_TABLE for t Ð 0 to T 1 do val1...G Ð A BLOCKrt ˆ G : pt 1q ˆ G 1s; TL TABLE set uppval1...Gq; end for Process hidden dimension for m Ð 0 to K step Q do for n Ð 0 to Q 1 do idx vec Ð B Y j T ˆG ]ı rm ns; for t Ð 0 to N 1 do TL TABLE idx Ð idx vecrts; O BLOCKrm ns Ð O BLOCKrm ns TL TABLErtsrTL TABLE idxs; end for end for end for end for Write output for p Ð 0 to K 1 do O.writepC BLOCKrpsq; O BLOCKrps Ð 0; end for end for Function Offline preprocess(W): return Encode every G value as an index in the matrix and pack every T values as a index vector idx vec; Function TL TABLE set up(val1...G): return return all 3G add and subtract combination; As described in Algorithm 1 and Fig.\n\n--- Segment 11 ---\nThe supportive ablation study will be presented in the next subsection. Algorithm 1: TL-based Ternary Matmul Input : A: Input activation stream (shape rMsrNs); Widx Offline preprocess(W): Offline-preprocessed weight indices (shape rN{pT GqsrKs); Output: O: Output activation stream (shape rMsrKs); Initialize:; TL TABLErNsr3Gs Ð 0 ; Table for all signed combinations A BLOCKrT ˆ Gs Ð 0 ; Activation buffer O BLOCKrKs Ð 0 ; Output vector accumulator for i Ð 0 to M 1 do for j Ð 0 to N 1 step T ˆ G do Load activation block for p Ð 0 to T ˆ G 1 do A BLOCKrps Ð A.read(); end for Set up values of TL_TABLE for t Ð 0 to T 1 do val1...G Ð A BLOCKrt ˆ G : pt 1q ˆ G 1s; TL TABLE set uppval1...Gq; end for Process hidden dimension for m Ð 0 to K step Q do for n Ð 0 to Q 1 do idx vec Ð B Y j T ˆG ]ı rm ns; for t Ð 0 to N 1 do TL TABLE idx Ð idx vecrts; O BLOCKrm ns Ð O BLOCKrm ns TL TABLErtsrTL TABLE idxs; end for end for end for end for Write output for p Ð 0 to K 1 do O.writepC BLOCKrpsq; O BLOCKrps Ð 0; end for end for Function Offline preprocess(W): return Encode every G value as an index in the matrix and pack every T values as a index vector idx vec; Function TL TABLE set up(val1...G): return return all 3G add and subtract combination; As described in Algorithm 1 and Fig. 2, the TL-based matmul can be divided into two stages: (1) preprocessing the weights into groups sized G, and (2) performing the online ternary matrix multiplication computation.\n\n--- Segment 12 ---\nAlgorithm 1: TL-based Ternary Matmul Input : A: Input activation stream (shape rMsrNs); Widx Offline preprocess(W): Offline-preprocessed weight indices (shape rN{pT GqsrKs); Output: O: Output activation stream (shape rMsrKs); Initialize:; TL TABLErNsr3Gs Ð 0 ; Table for all signed combinations A BLOCKrT ˆ Gs Ð 0 ; Activation buffer O BLOCKrKs Ð 0 ; Output vector accumulator for i Ð 0 to M 1 do for j Ð 0 to N 1 step T ˆ G do Load activation block for p Ð 0 to T ˆ G 1 do A BLOCKrps Ð A.read(); end for Set up values of TL_TABLE for t Ð 0 to T 1 do val1...G Ð A BLOCKrt ˆ G : pt 1q ˆ G 1s; TL TABLE set uppval1...Gq; end for Process hidden dimension for m Ð 0 to K step Q do for n Ð 0 to Q 1 do idx vec Ð B Y j T ˆG ]ı rm ns; for t Ð 0 to N 1 do TL TABLE idx Ð idx vecrts; O BLOCKrm ns Ð O BLOCKrm ns TL TABLErtsrTL TABLE idxs; end for end for end for end for Write output for p Ð 0 to K 1 do O.writepC BLOCKrpsq; O BLOCKrps Ð 0; end for end for Function Offline preprocess(W): return Encode every G value as an index in the matrix and pack every T values as a index vector idx vec; Function TL TABLE set up(val1...G): return return all 3G add and subtract combination; As described in Algorithm 1 and Fig. 2, the TL-based matmul can be divided into two stages: (1) preprocessing the weights into groups sized G, and (2) performing the online ternary matrix multiplication computation. Output row buffer Offline weight preprocessing 8 6 5 2 3 11 -2 4 6 1 5 6 2 4 5 6 4 2 2:LUT setup loop Look up table ADD SUBSTRACT 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 Group size Index vector length Feteched Index vectors Look up table Look up table 1:Look-up loop 3: Token Loop ACCUMULATOR index values return values Online Looking up and Accumulation ADD SUBSTRACT ADD SUBSTRACT Fig.\n\n--- Segment 13 ---\n2, the TL-based matmul can be divided into two stages: (1) preprocessing the weights into groups sized G, and (2) performing the online ternary matrix multiplication computation. Output row buffer Offline weight preprocessing 8 6 5 2 3 11 -2 4 6 1 5 6 2 4 5 6 4 2 2:LUT setup loop Look up table ADD SUBSTRACT 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 1 1 -1 -1 0 1 1 -1 1 1 -1 0 1 -1 1 1 -1 1 Group size Index vector length Feteched Index vectors Look up table Look up table 1:Look-up loop 3: Token Loop ACCUMULATOR index values return values Online Looking up and Accumulation ADD SUBSTRACT ADD SUBSTRACT Fig. 2: Dataflow and architecture of TL-based ternary matMul (G 4) In the preprocessing stage, assume that every G 3 ternary values are packed into a single index for TL table addressing, resulting in 3G 3 ˆ 3 ˆ 3 27 combinations. The index representation for this packing requires log2 27 5 bits. Let A P NMˆN and W P ternaryNˆK. The preprocessing of the weights involves encoding every group of G 3 ternary values into a 5-bit packed index. In the online stage, we perform vector-wise tiled matrix multiplication, where A and B. The first step is to establish the TL table using the precompute unit, which consists of 3G 27 sets of adders and subtractors to cover all possible combinations of dynamic activations. The packed-up indexes are then used to address and select the corresponding values from the TL table. Finally, the selected values are accumulated to generate a single vector O P N1ˆK in the output matrix. 2) Multi-TL-table dataflow design: In our design, we care- fully optimize the dataflow for TL-based matrix multiplication, as illustrated in Fig. 2. Assume we have a total of T tables.\n\n--- Segment 14 ---\n2. Assume we have a total of T tables. To better vectorize the TL-based matrix multiplication, the consecutive T indices can be grouped into an index vec- tor, enabling simultaneous access to different look-up tables. The weight index vector matrix can be rewritten as Widx P t5B, Tu N T ˆG ˆK. Widx are loaded onto the on-chip URAM. Regarding the scheduling, the innermost loop first performs vector operations to establish the T look-up tables simultane- ously, based on the first T ˆ G entries of the A matrix. Then, leveraging the multiple reading traits of the URAM [20], Q W index vectors are processed in parallel for TL table addressing, returning QˆT outcomes. The corresponding TL table return values are then accumulated into an output buffer of size K. The K index vectors on each row of W are traversed in steps of Q. The TL table addressing and accumulation process can be fully pipelined with an interval of one cycle, as there are Top-level control TL matmul unit Linear layer weights BRAM Decode loader Prefill loader Softmax score unit Decode loader LM head loader Quant Dequant Fused unit loader Reverse scheduler ROPE RMSnorm Element-wise add Element-wise mul Quant DRAM Special Function Unit Quant Online softmax unit Reverse Attention Engine Tenary Matmul Engine Decoding Attention Engine On-chip decoding hidden state BRAM Fig. 3: System architecture of TeLLMe. no inter-iteration dependencies. After the first T ˆ G entries of the A matrix are processed, the M values of the row of A are traversed in steps of T ˆ G in the intermediate loop. Finally, the outermost loop traverses the different row vectors in A, corresponding to the tokens in the prefill stage of the LLM. As for comparison, the LUT consumption of different matmul unit design methods is also presented in Table I. The configuration for the TL-based matmul is set as G 3, T 32, and Q 16. All levels of parallelism for the modules are set to be the same. Our design consumes 52094 LUTs, while the naive implementation, which selects whether to add or subtract, requires 7905 more LUTs.\n\n--- Segment 15 ---\nAll levels of parallelism for the modules are set to be the same. Our design consumes 52094 LUTs, while the naive implementation, which selects whether to add or subtract, requires 7905 more LUTs. Another approach involves storing half of the possible combinations (13 out of 27) instead of all combinations and using the index to determine whether the value should be negative. This approach results in a smaller distributed RAM size, aiming to save LUTs. However, after synthesizing, it consumed 9209 more LUTs. TABLE I: LUT Consumption Comparison of Different Mat- mul Unit Design Methods Approach LUT Consumption Difference TL-based(Our Design) 52,094 Naive Implementation 59,999 7,905 Partial Storage 61,303 9,209 B. Specialized Reverse Reorder for Prefill Attention 1) Prefill Challenge on edge FPGA: Prefill is one of the most challenging components for edge FPGAs. This part of LLM requires significant resources and bandwidth for multi-token computation, especially for attention computation, which involves softmax and matrix-to-matrix multi-head op- erations with a complexity of N 2. Given the limited memory bandwidth and finite computational units, the computation order of prefill attention must be carefully scheduled to meet these requirements. Otherwise, it may be constrained by the bandwidth limitations of the edge FPGA, as shown in the naive attention scheduling in Fig. 5. In the context of edge FPGA vision transformers, [17] proposed a state-of-the-art dense attention scheduling strategy, Attention map for dense attention scheduling Attention map for naive attention scheduling Attention map for reverse reordering and kernel fusion Fig. 4: The visualization of scheduling on the attention map (number of computation core p 4, beige stands for attention mask). 1 2 N 4 1 Load Load Iter Compute Compute Compute Compute N 2 4 No data resuse for the vectors Some of the computed values are invalid because of casual masks Fig. 5: Naive attention scheduling (p 4). as shown in Fig. 6 and Fig. 4, which takes into account the reuse of the Q values across different tokens. However, unlike vision transformers, LLMs use a causal attention mask. As a result, the dense scheduling approach wastes computational resources on zero masks.\n\n--- Segment 16 ---\nHowever, unlike vision transformers, LLMs use a causal attention mask. As a result, the dense scheduling approach wastes computational resources on zero masks. Furthermore, the fusion of operations such as Q b K, softmax, and S b V can reduce the additional accesses to DRAM. The state-of-the-art kernel fusion implementation for resource-abundant GPUs is Flash Attention. However, GPU- optimized computation is not suitable for FPGAs, as GPUs have many more computational cores and much larger on-chip SRAM compared to the on-chip BRAM URAM available on FPGAs. To address these challenges, we propose the reverse attention method, which utilizes fused attention and reverse reorder scheduling, specifically tailored for edge FPGAs. TABLE II: Comparison of different attention approaches. Approach Data Block Load Iteration Count Bandwidth Reverse Scheduling (Our Design) N2 2p N 2 N2 2p N 2 1 naive scheduling N2 N N2 p p dense scheduling [17] N2 p N p 1 N2 p p 1 1 2) Reverse Attention: The reverse attention scheduling is depicted in Fig. 7. Assume that the current length of the prefill Load Load Iter 1 2 3 4 N 1 N 2 N 3 N 4 Compute Compute Compute Compute N 2 4 N 2 4 1 N 2 4 2 N 2 4 3 Some of the computed values are invalid because of casual masks Fig. 6: Dense attention scheduling (p 4). tokens is N, with 1 ă i ď N and 1 ă j ď N representing the current token indices for q and k, v, respectively. There are a total of h heads. The kernel fusion computation can be considered a special case of Flash Attention V2 [21] when the block size is equal to 1. The head-wise formula for the case with two consecutive Load Load Load Compute Compute Compute Compute Iter 1 2 3 4 N 1 N 2 N 3 N 4 N(4 N) 8 N(4 N) 8 - 1 N(4 N) 8 - 2 N(4 N) 8 - 3 vectors are fully reused and transfered data blocks is less compared to naive scheduling Avoid invalid masked computation and relevant data transfer compared to dense scheduling Fig. 7: Reverse attention scheduling (p 4).\n\n--- Segment 17 ---\nThe head-wise formula for the case with two consecutive Load Load Load Compute Compute Compute Compute Iter 1 2 3 4 N 1 N 2 N 3 N 4 N(4 N) 8 N(4 N) 8 - 1 N(4 N) 8 - 2 N(4 N) 8 - 3 vectors are fully reused and transfered data blocks is less compared to naive scheduling Avoid invalid masked computation and relevant data transfer compared to dense scheduling Fig. 7: Reverse attention scheduling (p 4). blocks can be written as follows: mp1q sp1q ℓp1q esp1q mp1q op1q esp1q mp1qvp1q mp2q max mp1q, sp2q m ℓp2q emp1q mp2qℓp1q esp2q mp2q esp1q m esp2q m ℓ pp2q esp2q mp2q{ℓp2q op2q op1q{emp1q mp2q esp2q mp2qvp2q esp1q mvp1q esp2q mvp2q op2q op2q{ℓp2q o (3) where m denotes the maximum value, s qi b ki represents the MAC result of the vector dot product, ℓis the denominator factor, and o is the numerator vector. The upper index indicates the current step in the kernel fusion computation. Regarding scheduling, instead of starting from the first token q1, our schedule begins from qN 1. Specifically, the level of parallelism is set to p (as illustrated in the figure for p 4). The factor p also implies that the on-chip BRAM can store p tokens of qi. In each iteration, one qi token is loaded onto the on-chip memory (with the first batch loading qN to qN 3). Simultaneously, the corresponding kj and vj tokens are loaded for computation. After all N kj and vj tokens have been loaded and the fused-kernel computation is completed, the next iteration will evict p kj and vj tokens, starting from kN 3 and vN 3, to avoid redundant computations arising from the causal attention mask. The iteration continues until all 1 ă i ď N, 1 ă j ď N are traversed.\n\n--- Segment 18 ---\nAfter all N kj and vj tokens have been loaded and the fused-kernel computation is completed, the next iteration will evict p kj and vj tokens, starting from kN 3 and vN 3, to avoid redundant computations arising from the causal attention mask. The iteration continues until all 1 ă i ď N, 1 ă j ď N are traversed. In this approach, the only required input buffers are for p qi tokens, one kj, and one vj. Additionally, the intermediate buffers include: hˆp multi-head MAC intermediate results s, h ˆ p multi-head previous max values m, and h ˆ p intermediate denominators ℓ. After introducing the reverse attention process, a compari- son between naive attention, dense attention in Edge Moe [17], and our proposed reverse attention is provided in Table II. The comparison indicates that the iteration count for reverse attention is the lowest, and the required bandwidth remains constant. Furthermore, the redundant outcome rate of the computation core can be directly assessed from the attention map in Fig. 4. The naive and dense scheduling approaches do not account for the causal mask, leading to many redundant computed values. C. Hardware Specialization and Reuse for Decoding-Phase Attention and LM Head The previous section focuses on optimizing the Prefill phase in LLM inference. Since we consider efficient end-to-end LLM inference on single edge Device, both Prefill and Decoding phases require efficient implementation to maximize global performance. The attention mechanism is a core component in both phases, but its computational characteristic differs. In the prefill phase, calculating attention involves operations on matrices representing the entire input sequence. In the decoding phase, it involves operations between the vector representation of the single new token and the cached matrices of keys and values. This difference presents an opportunity for hardware specialization. In the decoding phase, a single new token is generated per step. Let the total sequence length (prompt already generated tokens) be M. The query q is now a 1 ˆ N vector corresponding to the new token. The key (Kcache) and value (Vcache) matrices contain the cached representations of all M previous tokens and are of dimensions M d. The core attention computation involves: 1.\n\n--- Segment 19 ---\nLet the total sequence length (prompt already generated tokens) be M. The query q is now a 1 ˆ N vector corresponding to the new token. The key (Kcache) and value (Vcache) matrices contain the cached representations of all M previous tokens and are of dimensions M d. The core attention computation involves: 1. Calculating attention scores: q ˆ KT cache (a 1 ˆ N vector multiplied by a N ˆ M matrix, resulting in a 1 ˆ M score vector). 2. Applying softmax to the scores. Multiplying the resulting 1 ˆ M vector by Vcache (an M ˆ N matrix) to get the 1 ˆ N output vector. The computation involves primarily matrix-vector and vector-vector operations. The computational load per step (OpNdq) is significantly lower than the total prefill computation. However, this phase requires fetching the large Kcache and Vcache matrices from memory (e.g., off-chip DRAM) in every step. Consequently, the decoding phase is often memory-bandwidth bound, especially as the sequence length M grows. Since computation is less intensive and latency is dominated by memory access (fetching the KV cache), massive parallelism is inefficient and wastes resources. A more sequential or lower-parallelism computation unit is sufficient. This approach significantly reduces the required on- chip hardware resources (e.g., number of PEs, buffer sizes) compared to the prefill unit. The profiling result in Figure 8 clearly shows that the attention in the decoding phase is memory-bounded, while the prefill phase is compute-bounded. This demonstrates the necessity of lightweight decoding im- plementation to save on-chip resources for the Prefill phase. Fig. 8: Characterization of Attention Module during Pre- fill Decoding Phase 1) Reuse of Decoding Attention for LM Head: Following the processing thr-ough N transformer blocks in typical LLM architectures like LLaMA, the final step before token genera- tion involves the LM Head. This component performs a crucial linear projection, mapping the final hidden state output from the last transformer block to a vector of logits representing the probability distribution over the entire vocabulary.\n\n--- Segment 20 ---\n8: Characterization of Attention Module during Pre- fill Decoding Phase 1) Reuse of Decoding Attention for LM Head: Following the processing thr-ough N transformer blocks in typical LLM architectures like LLaMA, the final step before token genera- tion involves the LM Head. This component performs a crucial linear projection, mapping the final hidden state output from the last transformer block to a vector of logits representing the probability distribution over the entire vocabulary. For inference, particularly during the auto-regressive decoding phase, where one token is generated at a time, the input to the LM Head is the final hidden state vector corresponding to the token being predicted. This vector has dimensions r1, Ns, where N represents the hidden state dimension (e.g., N 1536 for some models). The LM Head uses a large weight matrix of dimensions rN, V s, where V is the vocab- ulary size (e.g., V 32000), to compute the output logit vector of dimensions r1, V s. The core computation is thus a matrix-vector multiplication: r1, Ns ˆ rN, V s Ñ r1, V s. This computation (O(HV)) has similar characteristics to decoding attention: it s a matrix-vector operation with limited data reuse opportunities, heavily reliant on fetching a large matrix (the Kcache or the LM Head weights), making it memory-bound (especially as V " H). Given this similarity, we reuse the decoding-phase atten- tion hardware to execute the LM Head computation. This eliminates the need for a dedicated LM Head unit, yielding substantial area and power savings. The performance impact is negligible because the LM Head executes only once per generated token, whereas attention occurs N times (once per layer). Reusing the Decoding-phase Attention hardware for both attention and the LM Head precludes a fully fused attention pipeline within it. We therefore adopt a decoupled execution model for attention sub-steps during decoding: (1) Atten- tion Score Computation: s q ˆ KT cache . (2) Softmax: p softmaxpsq. (3) Value Aggregation: Compute the final attention output o p ˆ Vcache. This is efficient because the intermediate score probability vector (1 M) is small enough to be buffered on-chip BRAM with minimal latency penalty.\n\n--- Segment 21 ---\n(3) Value Aggregation: Compute the final attention output o p ˆ Vcache. This is efficient because the intermediate score probability vector (1 M) is small enough to be buffered on-chip BRAM with minimal latency penalty. On the contrary, for the Prefill phase, the intermediate N N attention score matrix would be far too large for practical on-chip buffering. Thus it requires a fully fused attention pipeline that integrates attention score calculation, softmax, and value aggregation in a single, uninterrupted hardware pass to minimize off-chip data movement. D. Implementation and Optimization of Special Function Units Besides the core modules discussed in previous sections, LLM inference also relies on several essential special func- tions, including Quantization Dequantization, RMSNorm, and Activation Function. These operations are computationally less intensive, therefore, our strategy focuses on lightweight hardware implementations and operator fusion to minimize overhead. For efficient data handling, vector data for these modules is processed in 256-bit packets, aligning with AXI bus width. Quantization Dequantization: The activations need to be quantized before the ternary Linear modules. We employ Absmax Quantization, which involves two passes: (1) finding the maximum absolute value to compute the scale factor, and (2) applying this scale element-wise. Figure 1 shows that quantization follows the RMSNorm. We fuse these operations to reduce data movement. The dequantization is fused into the Linear output pipeline. RMSNorm: RMSNorm involves two passes. The first cal- culates the Root Mean Square of the input x: RMSpxq b 1 n řN i 1 x2 i . The second pass normalizes the input by di- viding by the RMS value and multiplying by a learned scaling parameter γ. Recognizing that both RMSNorm and Absmax Quantization involve a two-pass traverse, we fuse these four logical steps into two optimized hardware passes. This significantly minimizes the data movement. Activation function: The element-wise SiLU activation (x 1 1 e x ) is required after the Gate projection in Feed- Forward Network (FFN) block. The SiLU is pipelined and fused directly into the preceding Linear module, effectively hiding its latency. IV. EXPERIMENTAL RESULTS AND ANALYSIS A.\n\n--- Segment 22 ---\nIV. EXPERIMENTAL RESULTS AND ANALYSIS A. Experiment Setup We implement the TeLLMe accelerator using high-level synthesis C C in Vitis HLS and Vivado 2023.1. We eval- uate our design on Kria KV260 (Zynq UltraScale XCK26 MPSoC). To ensure timing closure, we use 250 MHz for the final bitstream generation. B. LLM Inference Performance and Resource Breakdown Figure 9 shows the key metrics in LLM inference, including Decoding Throughput (token generation speed) and Prefill Time (time-to-first-token). We evaluate TeLLMe under differ- ent configurations [prompt size, generate size], note that the total tokens prompt size generate size. TeLLMe achieves ą 9 tokens s in 512 context lengths and 8 tokens s in 1024 context lengths. For prompt size ă 128, TeLLMe achieves TABLE III: Comparison of FPGA-based LLM Accelerators Work Device LUT FF BRAM DSP MHz Power (W) BW (GB s) Model Throughput (tokens s) Accelerate Prefill? SECDA [18] PYNQ TinyLLaMA W4 0.58 LlamaF [16] ZCU102 164K 171K 223 528 205 5.08 21.3 TinyLLaMA W8 1.50 Li et al. [9] KV260 78K 105K 36.5 291 300 6.57 19.2 LLaMA2-7B W4 4.90 TeLLMe(Ours) KV260 108K 155K 206 356 250 6.72 19.2 Bitnet W1.58 9.51 Not directly comparable since our TeLLMe have additional logic to accelerate prefill stage 1s Prefill size. TeLLMe demonstrates practical viability and deployment potential in real-world applications. The resource breakdown is shown in Table IV. Regarding BRAM usage, most of it is consumed by the top-level AXI buffer. DSP resources are mainly utilized by the Attention modules due to their INT8 precision. LUTs are primarily consumed by the TL-based matmul unit for the TL tables. URAM is used by the matmul weight buffer to support ping- pong operations. Fig.\n\n--- Segment 23 ---\nURAM is used by the matmul weight buffer to support ping- pong operations. Fig. 9: TeLLMe LLM Inference Performance TABLE IV: Resource Consumption Breakdown Module BRAM DSP FF LUT URAM Control Data Transfer 120 0 24973 5897 Attention (Prefill Phase) 46 122 25629 33069 Attention (Decoding Phase) 24 134 17465 7028 TL-based Matmul Unit 0 0 35765 52094 48 RMSNorm 16 28 6202 5933 Misc (Add, Mul, RoPE, etc) 0 72 45804 4973 Total 206 356 155838 108994 48 (71 ) (28 ) (66 ) (93 ) (75 ) C. Comparison with Existing Edge FPGA Work Table III presents a comparison between TeLLMe and prior FPGA-based LLM accelerators. Despite differences in model scale and quantization schemes, TeLLMe achieves a peak decoding throughput of 9.51 tokens per second representing up to a 16.4ˆ improvement over previous work while supporting both prefill and decoding stages on a single edge FPGA device. As highlighted in the table, none of the existing edge FPGA-based solutions implement on-device prefill, often citing its computational intensity as unsuitable for resource- constrained hardware. While we acknowledge the challenges associated with prefill on FPGAs, we argue that full on- device support is essential for a complete and self-contained TABLE V: Performance Comparison with Mobile CPU Device Category Decode (tokens s) Time-to- first-token (s) Prefill (tokens s) Model Size (MB) Snapdragon 8 Gen 3 1B BF16 (baseline) 19.2 1.0 60.3 2358 1B SpinQuant 50.2 0.3 260.5 1083 1B QLoRA 45.8 0.3 252.0 1127 3B BF16 (baseline) 7.6 3.0 21.2 6129 3B SpinQuant 19.7 0.7 89.7 2435 3B QLoRA 18.5 0.7 88.8 2529 KV260 FPGA 0.7B TeLLMe 9.51 0.55 116.4 257 Time-to-first-token (prefill delay) is measured with a prompt length 64 edge deployment.\n\n--- Segment 24 ---\nAs highlighted in the table, none of the existing edge FPGA-based solutions implement on-device prefill, often citing its computational intensity as unsuitable for resource- constrained hardware. While we acknowledge the challenges associated with prefill on FPGAs, we argue that full on- device support is essential for a complete and self-contained TABLE V: Performance Comparison with Mobile CPU Device Category Decode (tokens s) Time-to- first-token (s) Prefill (tokens s) Model Size (MB) Snapdragon 8 Gen 3 1B BF16 (baseline) 19.2 1.0 60.3 2358 1B SpinQuant 50.2 0.3 260.5 1083 1B QLoRA 45.8 0.3 252.0 1127 3B BF16 (baseline) 7.6 3.0 21.2 6129 3B SpinQuant 19.7 0.7 89.7 2435 3B QLoRA 18.5 0.7 88.8 2529 KV260 FPGA 0.7B TeLLMe 9.51 0.55 116.4 257 Time-to-first-token (prefill delay) is measured with a prompt length 64 edge deployment. Relying on external hosts to perform prefill introduces additional concerns regarding system complexity, data privacy, and scalability. Our detailed prefill performance is presented in Figure 9 and Table V. D. Comparison with Mobile CPU Despite the significant technological disparity between KV260 and modern mobile SoCs, our TeLLMe design demon- strates highly competitive performance in key inference met- rics. As shown in Table V, TeLLMe achieves a prefill latency of 0.55 seconds comparable to the 0.3 0.7 seconds observed on the Qualcomm Snapdragon 8 Gen 3, a device fabricated in an advanced 4nm process with integrated LPDDR5x memory and substantially higher bandwidth. In contrast, the KV260 is based on a 16nm process and relies on DDR4 mem- ory with much lower bandwidth. This makes our ability to match prefill performance particularly notable, as prefill is typically compute-bound and less amenable to acceleration on bandwidth-constrained FPGAs.\n\n--- Segment 25 ---\nIn contrast, the KV260 is based on a 16nm process and relies on DDR4 mem- ory with much lower bandwidth. This makes our ability to match prefill performance particularly notable, as prefill is typically compute-bound and less amenable to acceleration on bandwidth-constrained FPGAs. This result highlights the ef- fectiveness of our architectural optimizations including TL- based tenary matmul, a bandwidth-efficient attention module with fused operation and a Reversed Attention reordering scheme to accelerate prefill. While TeLLMe s decoding throughput (9.51 tokens s) lags behind that of mobile SoCs, this gap is largely attributable to the KV260 s limited external memory bandwidth, which disproportionately affects the memory-bound decode phase. We emphasize that this limitation is architectural rather than algorithmic; our design scales favorably to higher-bandwidth platforms such as HBM-enabled FPGAs or custom ASICs. Taken together, these results validate TeLLMe as the first binary LLM accelerator on edge FPGA to support full in- ference including both prefill and decoding with energy efficiency and architectural flexibility that position it well for future edge AI deployments. V. CONCLUSION We introduced TeLLMe, the first end-to-end FPGA ac- celerator optimized for ternary LLM inference across both prefill and decoding stages. By co-optimizing compute, mem- ory, and scheduling, TeLLMe employs a table-lookup-based matmul engine that reuses grouped activations and online precomputations across projection and feedforward layers for efficient ternary matrix operations. A fused attention module with reversed attention and Flash Attention-style kernel fusion reduces bandwidth demands, eliminates redundant masked operations, and supports parallelism. Running under 7 W, TeLLMe achieves up to 9.51 tokens s and supports 1024- token contexts, outperforming mobile SoCs at significantly lower power. It delivers prefill latencies of 0.55 1.15 s for prompts of 64 128 tokens. To our knowledge, TeLLMe is the first real-hardware FPGA accelerator to fully support ternary LLMs end-to-end, establishing a new benchmark for efficient, low-latency edge inference.\n\n--- Segment 26 ---\nIt delivers prefill latencies of 0.55 1.15 s for prompts of 64 128 tokens. To our knowledge, TeLLMe is the first real-hardware FPGA accelerator to fully support ternary LLMs end-to-end, establishing a new benchmark for efficient, low-latency edge inference. REFERENCES [1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., Language mod- els are few-shot learners, Advances in neural information processing systems, vol. 33, pp. 1877 1901, 2020. [2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi ere, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [3] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [4] Y. Qiao, M. Alnemari, and N. Bagherzadeh, A two-stage efficient 3- d cnn framework for eeg based emotion recognition, in 2022 IEEE International Conference on Industrial Technology (ICIT). IEEE, 2022, pp. 1 8. [5] A. Ding, Y. Qiao, and N. Bagherzadeh, Bnn an ideal architecture for acceleration with resistive in memory computation, IEEE Transactions on Emerging Topics in Computing, vol. 11, no. 2, pp. 281 291, 2023.\n\n--- Segment 27 ---\n2, pp. 281 291, 2023. [6] H. Wang, S. Ma, L. Dong, and et al., Bitnet: Scaling 1-bit transformers for large language models, arXiv preprint arXiv:2310.11453, 2023. [7] S. Ma, H. Wang, L. Ma, and et al., The era of 1-bit llms: All large language models are in 1.58 bits, arXiv preprint arXiv:2402.17764, 2024. [8] D. Guo, D. Yang, H. Zhang, and et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [9] J. Li, T. Li, G. Shen, D. Zhao, Q. Zhang, and Y. Zeng, Pushing up to the limit of memory bandwidth and capacity utilization for efficient llm decoding on embedded fpga, arXiv preprint arXiv:2502.10659, 2025. [10] H. Chen, J. Zhang, Y. Du, S. Xiang, Z. Yue, N. Zhang, Y. Cai, and Z. Zhang, Understanding the potential of fpga-based spatial accel- eration for large language model inference, ACM Transactions on Reconfigurable Technology and Systems, vol. 18, no. 1, pp. 1 29, 2024. [11] L. Ma, M. Sun, and Z. Shen, Fbi-llm: Scaling up fully bina- rized llms from scratch via autoregressive distillation, arXiv preprint arXiv:2407.07093, 2024. [12] Y. Xu, X. Han, Z. Yang, and et al., Onebit: Towards extremely low-bit large language models, in Advances in Neural Information Processing Systems (NeurIPS), 2024. [13] D. Du, Y. Zhang, S. Cao, and et al., Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL), 2024.\n\n--- Segment 28 ---\n[12] Y. Xu, X. Han, Z. Yang, and et al., Onebit: Towards extremely low-bit large language models, in Advances in Neural Information Processing Systems (NeurIPS), 2024. [13] D. Du, Y. Zhang, S. Cao, and et al., Bitdistiller: Unleashing the potential of sub-4-bit llms via self-distillation, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [14] J. Chee, Y. Cai, V. Kuleshov, and C. De Sa, Quip: 2-bit quan- tization of large language models with guarantees, arXiv preprint arXiv:2307.13304, 2023. [15] J. Wei, S. Cao, T. Cao, and et al., T-mac: Cpu renaissance via table lookup for low-bit llm deployment on edge, arXiv preprint arXiv:2407.00088, 2024. [16] H. Xu, Y. Li, and S. Ji, Llamaf: An efficient llama2 architecture accelerator on embedded fpgas, arXiv preprint arXiv:2409.11424, 2024. [17] R. Sarkar, H. Liang, Z. Fan, Z. Wang, and C. Hao, Edge-moe: Memory- efficient multi-task vision transformer architecture with task-level spar- sity via mixture-of-experts, in IEEE ACM International Conference on Computer-Aided Design (ICCAD), 2023, pp. 1 9. [18] J. Haris, R. Saha, W. Hu, and J. Cano, Designing efficient llm accelerators for edge devices, arXiv preprint arXiv:2408.00462, 2024, accessed: 2025-04-20. [Online]. Available: 00462 [19] D. Gerlinghoff, B. Choong, R. Goh, W.-F. Wong, and T. Luo, Table- lookup mac: Scalable processing of quantised neural networks in fpga soft logic, 04 2024, pp. 235 245. [20] AMD, Uram storage bind, 2024. [Online].\n\n--- Segment 29 ---\n[20] AMD, Uram storage bind, 2024. [Online]. Available: amd.com r 2024.2-English ug1399-vitis-hls pragma-HLS-bind storage [21] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R e, Flashattention: Fast and memory-efficient exact attention with io-awareness, 2022. [Online]. Available:\n\n