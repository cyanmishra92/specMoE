=== ORIGINAL PDF: 2504.16269v2_COBRA_Algorithm-Architecture_Co-optimized_Binary_T.pdf ===\n\nRaw text length: 48259 characters\nCleaned text length: 47614 characters\nNumber of segments: 35\n\n=== CLEANED TEXT ===\n\nCOBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference Ye Qiao , Zhiheng Chen , Yian Wang, Yifan Zhang, Yunzhe Deng, Sitao Huang Department of Electrical Engineering and Computer Science University of California, Irvine, USA {yeq6, zhihenc5, yianw11, yifanz58, yunzhd1, Abstract Transformer-based models have demonstrated su- perior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communi- cation limit their deployment to edge platforms for local, secure inference. Binary transformers offer a compact, low-complexity solution for edge deployment with reduced bandwidth needs and acceptable accuracy. However, existing binary transformers per- form inefficiently on current hardware due to the lack of binary specific optimizations. To address this, we introduce COBRA, an algorithm-architecture co-optimized binary Transformer accel- erator for edge computing. COBRA features a real 1-bit binary multiplication unit, enabling matrix operations with -1, 0, and 1 values, surpassing ternary methods. With further hardware- friendly optimizations in the attention block, COBRA achieves up to 3,894.7 GOPS throughput and 448.7 GOPS Watt energy efficiency on edge FPGAs, delivering a 311 energy efficiency improvement over GPUs and a 3.5 throughput improvement over the state-of-the-art binary accelerator, with only negligible inference accuracy degradation. I. INTRODUCTION In recent years, transformer-based models have become foundational architectures across multiple domains, achieving state-of-the-art performance in tasks such as natural language processing [1], computer vision [2], and others [3], [4]. These models excel at capturing complex patterns through self- attention mechanisms and extensive parametrization, often involving billions of parameters to achieve superior perfor- mance. However, the increasing size of these models intro- duces significant computational, memory, and communication challenges, restricting their deployment to a wide range of devices, especially resource-constrained devices. For many edge applications, such resource demands are impractical, making the deployment of full-scale transformers on edge devices particularly difficult, especially in scenarios requiring real-time inference. To address these challenges, model compression techniques like quantization [5] and pruning [6] have been proposed. Quantization reduces computational requirements by lower- ing numerical bitwidths, though it can lead to performance degradation. Despite these advances, quantized models often remain inefficient on edge devices due to the lack of effective acceleration on existing hardware. Equal contribution Binary transformers, an extreme form of quantized trans- formers, represent all weights and activations using binary val- ues like t 1, 1u or t0, 1u. These models significantly reduce computational complexity, model size, and communication bandwidth by replacing expensive floating-point operations with efficient bitwise logical operations [7], [8], [9], [10]. This approach enables faster execution with lower power consumption, making binary transformers a promising solution for energy efficient edge deployment. Despite these advantages, binary transformer models face significant challenges in implementation on existing hardware. The primary issue is the lack of optimized hardware units for binary matrix multiplication, as most processors and acceler- ators are designed for integer and floating-point computation, which are inefficient for binary operations. Furthermore, previ- ously proposed binary transformer models are often not well- suited for hardware optimization. For instance, BinaryBERT [11] and BiBERT [12] either do not fully binarize the model or retain some ternary representations. Similarly, BiT [13] employs both t 1, 1u and t0, 1u binarization schemes (though not for the same operation) and uses the original softmax function, which imposes a significant performance burden on hardware acceleration design, as will be discussed in a later section. To overcome these limitations, we propose COBRA, a hardware software co-designed Binary Transformer Acceler- ator optimized for edge FPGAs. COBRA introduces several innovations, including the Shifted Polarized Softmax (SPS) for hardware-efficient attention, a true 1-bit binary multiplication method for 1, 0, and 1 matrices, and integer packing to maximize bandwidth. Additional optimizations, such as efficient popcount units, operation fusion, processing element reuse, and parallelism tuning, further enhance throughput, latency, and resource efficiency. Our evaluations show that COBRA operates efficiently on both mid-range edge FPGA (ZCU102) and low-power edge FPGA (KV260), making it portable and well-suited for resource-limited edge deployment on various devices. The contributions of this work are sum- marized as follows: We propose COBRA, an algorithm-architecture co- optimized hardware accelerator for efficient inference of binary Transformer models. COBRA achieves 3.5ÀÜ speedup over state-of-the-art binary transformer acceler- arXiv:2504.16269v2 [cs.AR] 24 Apr 2025 ators. At hardware architecture level, we propose a real 1-bit binary matrix multiplication engine, RBMM. It achieves high computational efficiency with real 1-bit operations, using bitwise XNOR and popcount operations with a unique don t care (DC) count mechanism. RBMM is designed to support serveral variations of binary opera- tions, making it reusable and resource-efficient for edge devices, and it could be extended to other binary models. At algorithm level, we propose a hardware-friendly binary attention mechanism, shifted polarized softmax, SPS, which enables efficient hardware implementation with negligible impact on transformer models inference ac- curacy. At system level, we propose a series of optimizations including 6:3 compressor-based popcount, quantization- fused multiplication, and long bitwidth datapack repre- sentation that further enhance the system performance of COBRA. II. BACKGROUND AND RELATED WORK A. Binary Transformer 1) Binary Transformer Basic: The binarized Transformer represents an extreme case of transformer quantization, where full-precision real number weights and activations Wr, Ar are approximated with binary values Wb, Ab as follows: Wr, Ar Œ±Wb, Œ≥Ab where Wb SignpWrq, Œ± 1 n}Wr}, n is the total number of elements in Wr. Activations are approximated with a similar way. Here, Œ± and Œ≥ are scaling factors, which may vary across different approaches. For example, the state-of-the-art binary transformer BiT [13] uses these scaling factors in the initialization, and further trains them as part of the model weights during updates. Our work adopts a similar strategy. Without considering bias, the originial full-precision matrix multiplication can be approximated with binary operations as: Wr b Ar Popcount pXNORpWb, Abqq Œ±Œ≥ where b is matrix multiplication operator and popcount re- turns the number of 1 s in a binary format number. 2) Binary Transformer Designs: Most transformer quanti- zation efforts focus on 8-bit or 4-bit integer quantization due to the inherent difficulty of quantizing the attention block. Only a few studies address extreme low-bit transformers. For instance, TernaryBERT [14] proposed ternarizing the weights of the BERT model and fine-tuning it with a small dataset, achieving reasonable results. BinaryBERT [11] was the first to quantize both weights and activations of the BERT base model to binary, although it maintained a 2-bit embedding layer. This approach begins by initializing the network with weights from a fully trained half-sized ternary model, then proceeds by splitting weights to construct the binary model, followed by fine-tuning with knowledge distillation. Bin. (0,1) Binarize Binarize Binarize Bin. Bin. Bin. Linear Linear Linear Linear Linear Linear ReLU Layer Norm Layer Norm Softmax Binarize (0,1) Shifted Polarized Softmax Replace Input Embedding Input N Task Specific Linear Binary Transformer Encoder Block Output RBMM Operation Modes (Intrinsics) Legend (Details in Section III-B4) M2 M1 M3 F1 M4 F2 Fig. 1: General structure of binarized BERT with our Shifted Polarized Softmax (SPS) and Real Binary RBMM Engine Built on BinaryBERT, two later works BiBERT [12] and BiT [13] further binarized all weights, activations, and em- beddings. They compensate performance loss and maximize representational capability through direct matching, precision- progressive distillation, a two-set binarization scheme, and an elastic binary activation function with learnable parameters. These efforts yielded substantial model accuracy. However, the lack of hardware considerations, such as efficient softmax implementation and compatibility with two-set binarization schemes, limits the ability of these models to be accelerated efficiently with hardware, posing challenges for specialized accelerator design. B. Hardware Acceleration of Binary Transformers 1) Binary CNN Accelerators Are Not Suitable for Trans- formers: While previous works have proposed accelerators for binary convolutional neural networks (CNNs) [8], [15], [16], they are unsuitable for binary transformers due to differences in computation irregularity, data dependency, and memory access patterns. Transformers rely on fine-grained parallelism across long sequence dimensions for self-attention and large matrix multiplications, unlike the structured parallelism of CNNs. Furthermore, binary transformers require two specific types of binary schemes to function effectively and achieve acceptable accuracy a challenge not present in binary CNNs. 2) Binary Transformer Acceleration: Hardware accelera- tion for binary transformer networks presents unique chal- lenges compared to full-precision networks. Fig. 1 illustrates the general structure of a binarized BERT model. Binary Transformer models require two types of binarization schemes, t 1, 1ubt 1, 1u and t 1, 1ubt0, 1u, and involve two distinct matrix multiplication operations: linear layer and attention computation. Some existing works aim to enhance the efficiency of binary transformers. VAQF [17] introduced a binary vision Trans- former (ViT) accelerator that exploits the speedup potential of binarization by converting multiplication into bitwise logical operations, as discussed in an earlier section. However, it only supports one execution mode and does not consider 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1.0 0.9 0.8 0.1 0.2 0.1 0.1 1.0 0.7 0.3 0.1 0.1 0.3 0.6 1.0 0.7 0.1 0.2 0.2 0.2 0.5 1.0 0.9 0.3 0.2 0.1 0.1 0.7 1.0 0.6 0.1 0.1 0.7 0.4 0.8 1.0 ùúÜùëñ [ùúÜùëñ,1, ùúÜùëñ,2, , ùúÜùëñ,ùëò] Original BiT Binary Attention Attention Map Distortion Rate Original BiT (Sofmax-Round-Clip) COBRA SPS Binary Attention Argmin Update 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 Similarity MSE Fig. 2: Shifted Polarized Softmax (SPS) Search the matrix multiplication of binarized attention. BETA [18] employs a Compressor Tree Loop to create an accelerator specifically for fully binary transformers, supporting both matrix multiplication types and achieving improved efficiency and throughput on edge platforms. BAT [19] offers a co-design approach, creating a custom binarized transformer model with a specialized hardware accelerator for edge deployment. However, they rely on encoding ternary values t 1, 0, 1u as two-bit numbers through lookup tables, which is not a real binary representation, therefore it did not fully leverage the unique advantages of binary models. In summary, no existing hardware acceleration works fully leverage the potential of binary transformers, nor address the primary performance (latency) bottleneck for binary acceler- ator design: the softmax operations. Our proposed RBMM engine, along with the Shifted Polarized Softmax (SPS), addresses these limitations and unlocks the full potential of binary transformers for edge deployment. 3) Attention Mechanism Acceleration: The self-attention, constitutes the cornerstone of transformer models, enabling the dynamic weighting of input sequence elements based on their relative importance. However, this capability incurs a significant computational cost: attention mechanisms exhibit quadratic time and space complexity relative to sequence length. Flash Attention [20] represents a prominent GPU- optimized approach to accelerating attention computations. Its architecture leverages HBM and SRAM on GPUs, employing a tiled computation strategy that integrates the softmax and matrix multiplication operations. Nevertheless, this method is not directly adaptable to edge FPGAs, which are constrained by the limited bandwidth of DDR memory and limited MB- level on-chip BRAM and URAM. III. COBRA METHODOLOGY A. Hardware Friendly Binary Attention Mechanism Design 1) Real Binary Attention: BiBERT [12] and BiT [13] fol- lowed Eq. (1) to implement softmax in their binary attentions. This approach overlooks hardware-level overhead of softmax, leading to suboptimal computation speed, hardware costs, and power efficiency, making it impractical for edge deploy- ment. Furthermore, the use of softmax requires exponential computation and produces non-binary values in the attention mechanism, preventing BiT from achieving a fully binarized transformer model and complicating efficient hardware ac- celeration. In detail, BiT calculates a vanilla attention score (including softmax operation) and applies elastic binarization as Eq. (2), effectively redistributing the attention score to binary domain. Softmaxpziq exppziq ≈ôn j 1 exppzjq (1) Att probBiT clip round Softmax QbKJ b ?dk Œ± , 0, 1 (2) where clipp , 0, 1q clamps input elements into r0, 1s, and roundp q rounds input to the nearest integer. However, both the softmax operation and the elastic binarization function rely on many floating-point operations, which dominate the computational complexity of binary models and significantly compromise the hardware acceleration. To overcome these limitations, we propose Shifted Polarized Softmax (SPS) as a direct replacement for softmax and bina- rization in attention computation. SPS generates binary-state outputs directly, without relying on any floating-point arith- metic, thereby eliminating the need for a separate binarization step and further improving hardware efficiency. 2) Shifted Polarized Softmax (SPS): Conventional softmax scales input values into a continuous probability distribution over the range (0,1) and the quantization function rescale the distribution back to binary form, the SPS function in- stead polarizes values directly into binary states based on a learnable threshold, capturing the core purpose of soft- max emphasizing stronger signals without the need for continuous probabilistic scaling, as shown in Eq. (3). SPSpzq 1, if z ƒõ Œªi,k 0, otherwise (3) where i indicate ith attention block and k represent Kth attention head in the attention block. Att probSP S SPS ÀÜQbKJ b ?dk (4) As discussed in [12], for the attention mechanism to capture crucial elements in the binary representation, maximizing the information entropy of binarized attention score is a must and a binarized attention score matrix can serve the purpose. Hence, we propose SPS to replace Eq. (2) by search for a series of head-wise thresholds Œª P h ÀÜ n that further compensate the information loss for not having softmax normalization for attention score. SPS approximates the effects of softmax plus elastic binarization function while significantly reducing computational overhead, making it more compatible with hardware constraints. Our Final attention probability can be found as Eq. (4). TABLE I: Accuracy of COBRA vs. Others. All binary transformer models use W1A1 (1-bit weights and activations). Variations MNLI-m mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Average Relative Perf. Original BERT[1] 84.9 85.5 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.9 129.1 BinaryBERT[11] 35.6 35.3 66.5 51.5 53.2 0 6.1 68.3 52.7 41.0 55.6 BiBERT[12] 69.5 71.1 84.9 78.3 87.5 25.4 34.7 72.5 49.8 63.8 89.7 BiT[13] 75.6 76.3 84.7 82.8 88.0 27.4 69.8 77.4 52.7 71.0 100 COBRA (Layer) 67.6 68.7 79.03 70.1 85.7 25.4 66.3 74.3 53.4 65.1 93.8 COBRA (Row) - - 71.5 87.1 26.6 - - 53.0 - - COBRA (Head) 71.3 72.1 83.3 73.2 86.9 27.9 69.2 80.2 53.4 68.2 98.2 BiT [13] was selected as the baseline, as it represents the state-of-the-art in binary BERT models. BiT Attention Map (One Head) SPS Attention Map (One Head) Difference (a) Heatmap Correlation Comparison 0 25 50 75 100 125 0 20 40 60 80 100 120 Cosine Similarity (One Head) 0.3 0.4 0.5 0.6 0.7 0.8 0.9 (b) Cosine Similarity Compari- son 3 4 5 6 7 8 1e 5 7.61e 1 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 1e 5 4.991e 1 PCA Projection (c) PCA Projection Comparison 0 20 40 60 80 100 120 0 2 4 6 Row Norms Comparison (One Head) BiT Map SPS Map (d) Attention Row Norm Comparison Fig. 3: Similarity and Correlation Comparisons Between BiT (with Regular Softmax) and SPS Attention. 3) SPS Thresholds Search and Model Training: Figure 2 illustrates our method and search strategy for determining the SPS thresholds. Suppose two attention maps, A1 and A2, are obtained from the same layer Layeri. We define the Channel Distortion Rate (CDR) as the mean squared error (MSE) between these two maps: Distortion 1 n i√ø n 1 pA1 A2q2 (5) The thresholds Œª P RhÀÜn are predefined within the range r0, 1s with a search granularity of 0.05 and an initial value of 0(regular sign function). Our objective is to identify the optimal thresholds that minimize the output discrepancy be- tween BiT s softmax-based attention (Eq. 2) and our SPS- based attention (Eq. 4) for a given attention layer. Formally, we solve: Œª arg min Œª pDistortion pAtt probBiT, Att probSPSpŒªqqq (6) Here, Œª represents the per-head threshold. To avoid overfitting to any specific task, we uniformly sample 10 of each dataset (benchmark) to construct a small calibration set for the search process. This process identifies a unique threshold for each attention head in every attention block and is highly efficient, requiring only about five minutes in our experiments. We further evaluate different threshold granularities per layer, per head, and per row of the attention map which will be discussed in the next section. Balancing effectiveness and efficiency, we adopt the head-wise threshold configuration in our final design. This decision is well-supported, as different attention heads often capture distinct relational patterns among tokens and contribute to diverse representations, thereby en- hancing the model s expressivity. Figure 3 presents multiple similarity and correlation mea- surements between BiT (with Softmax) and SPS attention, demonstrating the validity of our SPS approximation. Finally, to further enhance COBRA s performance, we fix the searched thresholds Œª and fine-tune the model weights using the original training data to compensate for the information loss introduced by the SPS approximation. 4) SPS Results Analysis: Table I presents the accuracy performance of our binary COBRA model on the GLUE benchmarks [21], compared against the original BERT and other binary BERT variants. Our model outperforms both BinaryBERT [11] and BiBERT [12] by a significant margin, with only a less than 2 minor average performance drop compared to the state-of-the-art BiT [13]. Despite this small accuracy reduction, our model relies exclusively on true bi- nary operations and provides substantial hardware advantages, which will be further discussed in Section IV-C. We also evaluate different granularities of the SPS threshold. The attention layer-wise threshold performs adequately, with minimal overhead and the shortest search time. However, as previously discussed, the head-wise threshold yields the best performance, incurring similarly negligible overhead com- pared to the layer-wise case, and the search can be completed relatively quickly. Additionally, we conducted experiments using a more fine-grained row-wise threshold at the attention map. The results indicate that it does not offer meaningful Unsigned (0,1) (-1, 1) 0 1 0 0 0 1 AND 0 0 0 0 0 1 Don't care count (Number of "0"s) 4 RBVM Number of "1"s - Number of "-1"s 2 popcount - data width Don't care count 0 2-bit vector 6-bit binary datapack 0 1 0 0 0 1 1 -1 1 -1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 -5 1 -4-1-2 3 previous RBMM that computes Number of "-1"s Number of "0"s Number of "1"s popcount 1 Signed (-1,1) (-1, 1) 0 1 0 0 0 1 XNOR 0 0 0 1 0 1 2-bit vector 6-bit binary datapack -1 1 -1-1-1 1 1 -1 1 -1 1 1 1 0 1 0 1 1 data width - popcount 4 Number of "-1"s Number of "1"s popcount 2 RBVM Number of "1"s - Number of "-1"s 2 popcount - data width -2 data width - popcount 5 1 1 1 1 binarization Fig. 4: A 6-bit example of our RBVM. ùë® ùë´ùíÇùíïùíÇùíëùíÇùíÑùíå ùë© ùë´ùíÇùíïùíÇùíëùíÇùíÑùíåùüè ùë© ùë´ùíÇùíïùíÇùíëùíÇùíÑùíåùëµùíëùíÜ A B On Chip Memory RBMM Engine ùë∂ùíñùíïùíëùíñùíïùüêùë® ùë∂ùíñùíïùíëùíñùíïùüêùë© Off-Chip DDR RAM A B M2 M3 Output Return DC Return LayerNorm Unit Local Buffer AXI BUS ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùëµùíëùíÜ Npe ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùüê ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùüè COBRA Control Internal Read Write Control Don t care INPUT bias Threshold COBRA Control RBMM Engine On Chip Memory Fig. 5: COBRA Hardware Architecture Overview. performance improvements while introducing more parameters to the model. It also increases the search time by over 20 . Therefore, we adopt the head-wise threshold as the default configuration for the SPS function in our COBRA models. Additionally, all threshold granularity configurations are fully supported by our proposed accelerator architecture without incurring any extra computation time. B. COBRA Hardware Accelerator Design 1) Real 1-bit Binary Matrix Multiplication (RBMM): To fully leverage the benefits of binary representations, we propose an efficient algorithm, RBMM, for real binary matrix multiplication. Consider matrices A ra1, , am, , aMsJ P tp0, 1q p 1, 1quMÀÜN, B rb1, , bp, , bP s P tp 1, 1quNÀÜP , and multiplication result C A b B P NMÀÜP . To optimize binary computations while eliminating sign bit management in ternary multiplica- tion, we propose a unified representation for matrices A and B, where 1 is encoded as 0 while preserving 1 . For computational efficiency, the bit-packing strategy is adopted where elements am are packed into N-bit am datapack, and similarly, bp, are packed into N-bit bp. Then, the real binary vector dot product (RBVM), denoted as ambp am b bm, can be derived as (steps omitted for brevity): ambbp 2 popcountpam bpq N, am Ptp 1, 1qu 2 popcountpam bpq N Œ¥m, am Ptp0, 1qu (7) where N represents the data bitwidth of the datapack, pop- count counts the number of 1 s in the vector, and denote bit-wise logical operations XNOR and AND respectively, and Œ¥m denotes the number of zeros in the unsigned datapack am ( don t care count, DC). Fig. 4 illustrates our RBVM algorithm with a 6-bit toy example (768 bits in our actual design). The algorithm determines the count of 1 s through popcount operations, from which the number of 1 s can be derived. For the p0, 1q binarization scheme, the number of 0 s are considered. Moreover, N can be fused with ƒõ T binarization in Eq. (3), which will be detailed in the Section III-B4. Although this approach seemingly requires an addi- tional popcount operation to determine Œ¥m, this computation can be efficiently integrated into the previous quantization- fused RBMM as shown in Fig. 4. Specifically, when a 0 quantization value is calculated, the row s Œ¥m is incremented by 1. Besides, the RBVM operation exhibits compositional properties when column row vectors are subdivided into S smaller vectors, where am ram1, ..., ams, ..., amSs and bp rbp1, ..., bps, ..., bpSsJ. The vector dot product property is preserved as follows, ambp S√ø s 1 amsbps S√ø s 1 ams b bps, (8) where am concatpam1, ..., ams, ..., amSq and bp concatpbp1, ..., bps, ..., bpSq. This decomposition enables the RBMM engine to support both smaller-sized RBMM oper- ations and larger configurations, making the RBMM engine reusable for both multi-head attention (MHA) and feed- forward network (FFN). 2) Quantization-fused RBMM: On top of our RBMM de- sign, we further fuse quantization into it and create the quantization-fused RBMM engine. The unsigned and signed binary quantization schemes can then be mathematically ex- pressed as: cb ij clip round cij Œ≤j Œ± , 0, 1 , (0, 1) scheme sign cij Œ≤j Œ± , (-1, 1) scheme (9) where cij is an element from C, cb ij is the binarized value, Œ± P N denotes the scaling factor, Œ≤i is an element from the shift vector Œ≤ P N1ÀÜP and the sign of zero is deemed as 1 . Since the unified hardware representation for 1 and 0 has been applied as mentioned earlier, the two binarization schemes can be unified as, cb ij 1, cij Œ∏j ƒõ 0 0, cij Œ∏j ƒÉ 0 , Œ∏j r 1 2Œ± Œ≤j , (0, 1) Œ≤j, (-1, 1) (10) dh -bit OR OR a1 b1 MODE CTRL 6:3 3 Popcount Unit 6:3 6:3 6:3 6:3 6:3 Compressor 6:3 Every 36 bits [2:0] [2:0] 6:3 6 [0] [1] [2] [2:0] 6:3 6:3 2 1 1 1 s in 36 bits (log2dh 1)-bit threshold data width sign 0 Atten Mask CTRL DC HEAD 0 MODE CTRL HEAD PE ACC PATH CONCAT PATH OUT Shifted Polarized Softmax(SPS) dh DC INPUT d dh -bit a2 b2 a1 b1 RBMM PE CONCAT DC RETURN bias MODE CTRL OUTPUT RETURN MODE CTRL DC INPUT 0 0 M3 F2 Œî M4 F2 M2 Œî Œî F1 Œî F2 Œî M3 F2 Œî M2 DC FULL XNOR AND Unit bias Bo -bit Bo -bit (log2dh 1)-bit 1-bit b1 bh b2 HEAD PE AND Popcount Unit Shifted Polarized Softmax a1 a2 ah datapacks ùõøùõø1 ùõøùõøùüêùüê . . . ùõøùõø . . ùõøùõøùëëùëë . . ùúÉùúÉ1 ùúÉùúÉùüêùüê . . . ùúÉùúÉ . . ùúÉùúÉùëëùëë .. HEAD PE Fig. 6: The Architecture of RBMM Processing Elements (M1 4 and F1 2 are operation modes of RBMM engine). where Œ∏j is an element from the bias vector, and r is the round operation. Furthermore, the unsigned binarization is only employed after the ReLU operation within the feed- forward network (FFN), as illustrated in Fig. 5. From a hardware implementation perspective, these operations can be efficiently merged due to the overlapped range between the conditions cij ƒõ rp 1 2Œ± Œ≤jq and cij ƒõ 0 in the ReLU function. Consequently, the Œ∏j is set as maxp0, rp 1 2Œ± Œ≤jqq. 3) Overall Hardware Architecture: As illustrated in Fig. 5, the accelerator architecture consists of five major components: an RBMM engine, a LayerNorm unit, a COBRA configuration control unit, a data packing conversion unit, and a master AXI interface. For clarity, we define the following notation: d represents the hidden dimension, h denotes the number of heads, dh d{h indicates the size per head, FFsize Rd (R ƒÖ 1) represents the FFN hidden dimension, l (l ƒÉ d) denotes the sequence length, and Npe represents the number of Processing Elements (PEs). The numbers in the multiplexer indicate the RBMM modes (detailed in the next Section III-B4), while represents all other modes. The notation (n x ÀÜ y ÀÜ z) represents the RBMM where n is the number of matrix multiplications, the first matrix has dimensions x ÀÜ y, the second matrix has dimensions y ÀÜ z, and the result matrix has dimensions x ÀÜ z. The RBMM engine supports multiple configurations (i.e., operation modes) of quantization-fused matrix multiplication operations. We will discuss the operation modes of RBMM in the next Section III-B4. The LayerNorm unit implements the normalization function using the 16-bit fixed-point scaling fac- tors and value for computation to optimize hardware efficiency. The COBRA control unit manages the RBMM configuration parameters and orchestrates the overall computation flow. The data packing conversion unit efficiently transforms interme- diate matrices into binary datapack formats. External DDR memory accesses like weight matrix fetching and intermediate array storage are facilitated via the AXI high-performance master interface. 4) RBMM Engine Design: The RBMM engine is designed and optimized for efficient matrix multiplication of various matrix sizes in both MHA and FFN modules. Therefore we only need one single RBMM engine to handle all the matrix multiplications in the Transformer. This flexible design greatly optimize the area and power efficiency. The engine integrates activation functions and quantization operations directly, with behavior determined by the RBMM mode. As illustrated in Fig. 5, for each invocation of the RBMM engine, it produces Npe RBVM results. The number of invo- cations is different in different mode configurations which is set by the COBRA controller. Each invocation of the engine is scheduled at an interval of one clock cycle, i.e., the initiation interval (II) of the pipeline is 1, which means the latency of RBMM Engine execution in each invocation is perfectly overlapped, and the modules inside RBMM are fully pipelined. The RBMM Engine accepts one d-bit row datapack from matrix A with its associated 1 ÀÜ l DC INPUT vector, Npe d-bit column datapacks from matrix B, and a quantization 1 ÀÜ d bias vector as inputs. Each RBMM PE processes a single row datapack and column datapack pair, computing their RBVM result and updating the DC RETURN which includes h DC HEADs and a DC FULL after the ACC PATH. The DC RETURN will be treated as DC INPUT in the subsequent RBMM if (0, 1) binarization scheme is being used. Within the RBMM Engine shown in Fig. 6, each RBMM PE consists of h HEAD PEs that compute RBVM results for dh- bit head datapacks, and the final d-bit datapack RBVM result is obtained by accumulating these head-level computations as formulated in Eq. (8). As for the popcount unit in HEAD PE, 64 3-bit ROM-based 6:3 compressors can be used to compute the population count of every 36 bits. Initially, six compressors determine the number of ones in each group of six input bits. In the subsequent stage, the six 3-bit outputs from these compressors are combined using another round of compressors by feeding in the third bits, second bits, and first bits separately. Finally, a shift-and-add operation is performed to accumulate the results of the third, second, and first bits, yielding the total popcount. For FFN computations, since the hidden size is FFsize Rd, the RBMM PE return value is accumulated with the previous output. As for bit width, the HEAD PE result requires plog2 dh 1q bits, while the RBMM Engine output uses maxplog2 FFsize 1, hq Bo bits, enabling efficient multi-head computation where binary multi-head attention results can be concatenated into a k-bit return value. What s more, the quantized binary RBVM output in this case set all Bo-bit to represent 0 or 1 and when it comes to the integer output, all bits are utilized to represent the integer. Finally, the address of the return output value is controlled by the Read Write unit. TABLE II: COBRA Hardware Performance Evaluation and Comparison against Previous Works Platform GPU FPGA RTX 3090 FQ-BERT[22] COSA[23] TransFRU[24] BETA[18] BAT[19] KV260 ZCU102 Network BERT BiT COBRA FQ-BERT BERT BERT BiT BinaryBERT BiBERT BMT COBRA Npe 16 COBRA Npe 32 Precision W16A16 W1A1 W1A1 W4 8A8 W8A8 W4A4 W1A1 W1A1 W1A1 W1A2 W1A1 W1A1 Throughput (GOPS) 749.1 484.26 503.14 22.74 1556.5 1223.90 1240.98 1387.59 1436.07 1122.40 846.28 3894.74 Power (W) 350.00 350.00 350.00 9.80 31.3 45.46 7.18 7.95 8.20 5.47 4.07 8.68 Energy Efficiency (GOPS W) 2.14 1.38 1.44 2.32 49.7 26.9 172.41 174.59 175.23 207.7 207.93 448.70 TABLE III: FPGA Resource Utilization Comparison. means not reported or don t have the kind of resource. Work FQ-BERT [22] COSA [23] TransFRU [24] BETA [18] BAT [19] Our Work Platform ZCU102 XCVU13P Alveo U280 ZCU102 ZCU102 KV260 ZCU102 Network FQ-BERT BERT BERT BiT BiBERT BinaryBERT BMT COBRA Npe 16 COBRA Npe 32 LUT 123K (45 ) 1147K (66 ) 633K (49 ) 191K (70 ) 146K (53 ) 86K (74 ) 122K (45 ) FF 124K (23 ) 526K (15 ) ( ) 88K (16 ) 136K (25 ) 105K (45 ) 162K (30 ) DSP 1751 (70 ) 8192(est.) (67 ) 5016 (56 ) 64 (2.5 ) 1024 (41 ) 45 (3.6 ) 78 (3.1 ) BRAM 419 (46 ) 786 (29 ) ( ) 543 (60 ) 114 (12.5 ) 142.5 (98 ) 691.5 (76 ) URAM 0 (0 ) ( ) 51 (80 ) Our RBMM engine supports six operation modes as follows. We visualize the places of using each mode in the Transformer model in Fig. 1, and the hardware design in Fig. 5 and Fig. 6. M1. MHA Q K V Computation (l ÀÜ d ÀÜ d): The Q K V matrix feeds to Matrix A and the corresponding weight matrix serves as Matrix B. The HEAD PE activates the ACC PATH the threshold data width value is set to dh and the RBMM PE produces quantized binary outputs where output ƒõ bias. M2. MHA Attention Score (h l ÀÜ dh ÀÜ l): The Q serves as Matrix A, while the K serves as Matrix B. The h heads are processed simultaneously as they appear consecutively in the input datapacks. The HEAD PE enables the CONCAT PATH. Each head s result undergoes threshold comparison in SPS and attention mask application through iterative comparison of the current row and column indices. Note that the attention mask can support multiple mechanisms, including padding masks, causal masks, and others. This is achieved by checking whether the current loop index exceeds a value, which deter- mines whether the attention mask is applied to the subsequent elements. The threshold data width value is adjusted to T dh. The RBMM PE outputs the h-bit concatenated value and h updated DC HEADs for each head. M3. MHA Context Output (h l ÀÜ l ÀÜ dh): Matrix A is the attention score matrix of one head, and Matrix B is the transposed V l-bit datapacks. The lower l bits of the d bit are valid. The threshold data width value is modified to l{h, and the bias is added with l mod h. The HEAD PE activates the ACC PATH. The RBMM PE adds the DC INPUT because the attention score matrix is (0, 1) value. The output is quantized with bias, and the h head output addresses are arranged continuously to form an l ÀÜ d matrix. M4. MHA Linear Layer (l ÀÜ d ÀÜ d): This layer shares the configuration of mode M1 but produces integer output for the following LayerNorm operation instead of the quantized binary output value. F1. FFN Linear Layer I (l ÀÜ d ÀÜ FFsize) and F2. FFN Linear Layer II (l ÀÜ FFsize ÀÜ d): In the first FFN layer, the unsigned quantization bias is applied to represent the ReLU and binarization operation as described in Eq. (10). The DC FULL is updated to count the zeros after ReLU. The second FFN hidden layer involves the DC INPUT for summation and returns integer values accumulated with previous output values. The threshold data width values are set to dh. While these larger RBMMs seem to require a larger buffer of size lÀÜFFsize, the consecutive RBMMs are optimized as follows: E ReLUpX b Yq b Z ReLUpX b rY1, . . . , Yr, . . . , YRsq b rZ1, . . . , Zr, . . . , ZRsJ ReLUpX b Y1q b Z1 ReLUpX b YRq b ZR, (11) where X P tp 1, 1qulÀÜd, Y P tp 1, 1qudÀÜF Fsize, Z P tp 1, 1quF FsizeÀÜd and ReLU denotes the ReLU operation with unsigned binarization. This optimization requires only R iterations of ReLUpX b Yrq and ReLUpX b Yrq b Zr operations, each involving an l ÀÜ d ÀÜ d RBMM. In this way, only two buffers of size l ÀÜ d are required. Moreover, for the corner case of resource-limited platforms like KV260 that cannot support two buffers, the hardware design adheres to the original computational flow and utilizes off-chip DDR memory to store the additional R intermediate matrices. 5) On-chip and Off-chip Memory: Considering the ineffi- cient off-chip memory access throughputs of edge platforms, on-chip memory management requires careful optimization to meet hardware resource constraints. The binary-format Matrix A and Matrix B are buffered entirely in BRAM or URAM due to their relatively small size. The DC INPUT vector and quantization bias vector are also maintained in on- chip memory. In contrast, the weight and bias for different RBMM and transformer layers are fetched from off-chip memory and loaded to Matrix B and bias vector. However, the output matrix necessitates integer representation to support LayerNorm operations. Thus, a minimum of three Bo-bit matrices with dimensions lÀÜd are required to store the Q K V projection output matrices and support the two intermediate matrices in mode F1 and F2. These matrices are allocated to off-chip DDR memory or on-chip BRAM URAM based on available resources. For instance, with parameters l 512 and d 768 using Bo 13-bit integer representation, the KV260 platform can only accommodate one output matrix in on-chip memory, necessitating off-chip DDR allocation for the remaining matrices. This configuration leads to performance TABLE IV: Resource Breakdown for COBRA (Npe 32). COBRA Npe 32 LUT FF DSP BRAM RBMM Engine 42926 (34.96 ) 62782 (38.66 ) 0 0 LayerNorm Unit 14055 (11.45 ) 13505 (8.32 ) 78 (100 ) 16 (2.31 ) Data Packing Conversion Units 27187 (22.14 ) 52707 (32.46 ) 0 0 On-chip Memory, AXI Bus and COBRA CTRL 38634 (31.46 ) 33399 (20.57 ) 0 675.5 (97.67 ) Total 122802 162393 78 691.5 Data Packing Conversion RBMM Engine Layer Norm COBRA Control AXI Interface On-Chip Memory Fig. 7: COBRA Accelerator Implementation Layout (ZCU102) degradation due to the inefficient DDR read write operations through the AXI master interface on edge platforms. In con- trast, the ZCU102, with its larger memory capacity, can buffer all three intermediate output matrices in on-chip memory, enabling optimal performance. IV. EXPERIMENTAL RESULTS AND ANALYSIS A. Experimental Setup We implement COBRA accelerator using high-level synthe- sis C C in Vitis HLS and Vivado 2023.1. We evaluate our design on AMD KV260 (Zynq UltraScale XCK26 MPSoC) and AMDZCU102 (Zynq UltraScale XCZU9EG MPSoC) boards. For Vitis HLS synthesis, we set the target frequency to 333MHz, achieving a maximum of 389 MHz on the KV260 and 339 MHz on the ZCU102. To ensure timing closure, we use 300 MHz for the final bitstream generation. We evaluate COBRA with BERT-base model: the maximum sequence length l 512, the hidden dimension d 768, the number of head h 12, the FFN hidden dimension FFsize 4 d 3072 and 12 transformer layers in total. B. COBRA Accelerator Evaluation and Comparison As shown in Table II, our FPGA implementation of CO- BRA achieves energy efficiency improvements of 150.7 and 144.4 on the KV260 platform compared to the BiT [13] and COBRA GPU baselines (PyTorch GPU), respectively. On the ZCU102 platform, it achieves energy efficiency im- provements of 325.1 and 311.6 , respectively. The resource utilization shown in Table III reveals that the configuration with Npe 16 on KV260 consumes the least LUT resources while maintaining energy efficiency comparable to BETA [18] TABLE V: Impact of Each Proposed Optimization. Design Throughput (GOPS) Power (W) Energy Efficiency (GOPS W) Hardware Resource LUT DSP COBRA Npe 32 3894.74 8.68 448.70 122K 78 w o SPS 6.91 (564ÀÜ) 13.41 ( 4.7) 0.52 (862.9ÀÜ) 158K ( 36K) 104 ( 26) w o 6:3 compressor based popcount 3873.91 (1.005ÀÜ) 12.18 ( 3.5) 318.06 (1.4ÀÜ) 134k ( 12K) 1614 ( 1536) w o pipeline for RBMM Engine 794.32 (4.9ÀÜ) 7.98 (-0.7) 99.54 (4.5ÀÜ) 125K ( 3K) 78 and BAT [19] implementations. These demonstrate the adapt- ability and efficiency of our hardware architecture on resource- constrained edge devices. The Npe 32 COBRA implementa- tion on mid-end ZCU102 delivers the best performance in both energy efficiency and throughput among all evaluated imple- mentations. Specifically, it achieves 3.5 throughput and 2.2 energy efficiency improvements, compared to BAT [19], the state-of-the-art binary transformer accelerator. Furthermore, our design on the low-power ZCU102 platform achieves 2.5ÀÜ higher throughput than COSA [23] and TransFRU [24] designs on high-performance datacenter-grade FPGAs, while our de- sign consumes significantly lower power and has reduced LUT or DSP resource utilization compared to theirs. The resource breakdown of COBRA Npe 32 is shown in Table IV. Our RBMM Engine design uses 42,926 LUTs for efficient quantization-fused RBMM. The LayerNorm unit occupies 78 DSPs for computing the standard deviation and normalization. Notably, the data packing conversion units consume significant resources due to the LUT-based intermediate buffers in the transpose operation for V output matrix to datapacks (details in the M3. MHA Context Output section). Fig. 7 shows the implementation layout of the COBRA accelerator on the ZCU102 FPGA. Major components are highlighted and labeled in the figure. C. Ablation Study of COBRA Hardware Design We performed an ablation study on our proposed optimiza- tion techniques to better understand their impact on the overall accelerator performance. Table V shows the results. 1) SPS instead of Softmax: As discussed in Section III-A2, the Softmax hardware module is inefficient for hardware. In contrast, our proposed SPS-fused RBMM achieves a through- put improvement of 564ÀÜ compared to the original Softmax unit, which requires additional 36K LUTs and 26 DSPs for the computation of attention scores across h heads after RBMM. 2) 6:3 Compressor-Based Popcount instead of CPU- Optimized Popcount: The design of the Popcount unit is a critical consideration for the RBMM Engine, as it is replicated h times in each RBMM PE. The CPU-optimized popcount, as introduced in [25], leads to additional 12K LUTs and 1536 DSPs due to the inclusion of multipliers. 3) Pipeline Execution of RBMM Engine instead of Serial Execution: As outlined in Section III-B4, the components within the RBMM Engine are fully pipelined, and the engine s initiation interval (II) is one clock cycle. If pipeline scheduling is disabled, the RBMM Engine s execution after each call is serialized, resulting in only 20 of the throughput achieved by the pipelined version. V. CONCLUSION In this paper, we propose COBRA, a co-designed Binary Transformer Accelerator optimized for edge FPGAs. Our approach incorporates a hardware-friendly Shifted Polarized Softmax (SPS) design, a Real 1-bit Binary Matrix Multi- plication Engine (RBMM), and an adaptive computational flow, enabling us to fully harness the potential of binary transformer acceleration. COBRA achieves a 3.5 improve- ment in throughput and a 2.2 gain in energy efficiency with negligible accuracy degradation compared to state-of-the-art binary transformer accelerators. These on-board evaluation results highlight the performance of our design, enabling highly efficient edge inference of transformer models. REFERENCES [1] J. D. M.-W. C. Kenton and L. K. Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of naacL-HLT, vol. 1. Minneapolis, Minnesota, 2019, p. 2. [2] D. Alexey, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv: 2010.11929, 2020. [3] H. Kaeley, Y. Qiao, and N. Bagherzadeh, Support for stock trend prediction using transformers and sentiment analysis, arXiv preprint arXiv:2305.14368, 2023. [4] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, Progprompt: Generating situated robot task plans using large language models, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 523 11 530. [5] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat, Q8bert: Quantized 8bit bert, in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 36 39. [6] W. Kwon, S. Kim, M. W. Mahoney, J. Hassoun, K. Keutzer, and A. Gholami, A fast post-training pruning framework for transform- ers, Advances in Neural Information Processing Systems, vol. 35, pp. 24 101 24 116, 2022. [7] R. Zhao, W. Song, W. Zhang, T. Xing, J.-H. Lin, M. Srivastava, R. Gupta, and Z. Zhang, Accelerating binarized convolutional neural networks with software-programmable fpgas, in Proceedings of the 2017 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA 17. New York, NY, USA: Association for Computing Machinery, 2017, p. 15 24. [Online]. Available: [8] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, Fracbnn: Accurate and fpga-efficient binary neural networks with fractional activations, in The 2021 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA 21. New York, NY, USA: Association for Computing Machinery, 2021, p. 171 182. [Online]. Available: [9] Y. Qiao, M. Alnemari, and N. Bagherzadeh, A two-stage efficient 3- d cnn framework for eeg based emotion recognition, in 2022 IEEE International Conference on Industrial Technology (ICIT). IEEE, 2022, pp. 1 8. [10] A. Ding, Y. Qiao, and N. Bagherzadeh, Bnn an ideal architecture for acceleration with resistive in memory computation, IEEE Transactions on Emerging Topics in Computing, vol. 11, no. 2, pp. 281 291, 2023. [11] H. Bai, W. Zhang, L. Hou, L. Shang, J. Jin, X. Jiang, Q. Liu, M. Lyu, and I. King, BinaryBERT: Pushing the limit of bert quantization, arXiv preprint arXiv:2012.15701, 2020. [12] H. Qin, Y. Ding, M. Zhang, Q. Yan, A. Liu, Q. Dang, Z. Liu, and X. Liu, BiBERT: Accurate fully binarized BERT, arXiv preprint arXiv:2203.06390, 2022. [13] Z. Liu, B. Oguz, A. Pappu, L. Xiao, S. Yih, M. Li, R. Krishnamoorthi, and Y. Mehdad, BiT: Robustly binarized multi-distilled transformer, Advances in neural information processing systems, vol. 35, pp. 14 303 14 316, 2022. [14] W. Zhang, L. Hou, Y. Yin, L. Shang, X. Chen, X. Jiang, and Q. Liu, TernaryBERT: Distillation-aware ultra-low bit BERT, arXiv preprint arXiv:2009.12812, 2020. [15] R. Andri, L. Cavigelli, D. Rossi, and L. Benini, Yodann: An architecture for ultralow power binary-weight cnn acceleration, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 37, no. 1, pp. 48 60, 2017. [16] A. Al Bahou, G. Karunaratne, R. Andri, L. Cavigelli, and L. Benini, Xnorbin: A 95 top s w hardware accelerator for binary convolutional neural networks, in 2018 IEEE Symposium in Low-Power and High- Speed Chips (COOL CHIPS). IEEE, 2018, pp. 1 3. [17] M. Sun, H. Ma, G. Kang, Y. Jiang, T. Chen, X. Ma, Z. Wang, and Y. Wang, Vaqf: Fully automatic software-hardware co-design frame- work for low-bit vision transformer, arXiv preprint arXiv:2201.06618, 2022. [18] Y. Ji, C. Fang, and Z. Wang, BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge, arXiv preprint arXiv:2401.11851, 2024. [19] Y. Ji, C. Fang, S. Ma, H. Shao, and Z. Wang, Co-designing binarized transformer and hardware accelerator for efficient end-to-end edge deployment, arXiv preprint arXiv:2407.12070, 2024. [20] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R e, Flashattention: fast and memory-efficient exact attention with io-awareness, in Proceedings of the 36th International Conference on Neural Information Processing Systems, ser. NIPS 22. Red Hook, NY, USA: Curran Associates Inc., 2022. [21] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding, 2019. [Online]. Available: [22] Z. Liu, G. Li, and J. Cheng, Hardware acceleration of fully quan- tized bert for efficient natural language processing, in 2021 Design, Automation Test in Europe Conference Exhibition (DATE), 2021, pp. 513 516. [23] Z. Wang, G. Wang, H. Jiang, N. Xu, and G. He, Cosa:co-operative systolic arrays for multi-head attention mechanism in neural network using hybrid data reuse and fusion methodologies, in 2023 60th ACM IEEE Design Automation Conference (DAC), 2023, pp. 1 6. [24] H. Wang, Y. Bai, J. Yu, and K. Wang, Transfru: Efficient deployment of transformers on fpga with full resource utilization, in 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), 2024, pp. 521 526. [25] Wikipedia, Hamming weight wikipedia, the free encyclopedia. [Online]. Available: weight\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nCOBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference Ye Qiao , Zhiheng Chen , Yian Wang, Yifan Zhang, Yunzhe Deng, Sitao Huang Department of Electrical Engineering and Computer Science University of California, Irvine, USA {yeq6, zhihenc5, yianw11, yifanz58, yunzhd1, Abstract Transformer-based models have demonstrated su- perior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communi- cation limit their deployment to edge platforms for local, secure inference. Binary transformers offer a compact, low-complexity solution for edge deployment with reduced bandwidth needs and acceptable accuracy. However, existing binary transformers per- form inefficiently on current hardware due to the lack of binary specific optimizations. To address this, we introduce COBRA, an algorithm-architecture co-optimized binary Transformer accel- erator for edge computing. COBRA features a real 1-bit binary multiplication unit, enabling matrix operations with -1, 0, and 1 values, surpassing ternary methods. With further hardware- friendly optimizations in the attention block, COBRA achieves up to 3,894.7 GOPS throughput and 448.7 GOPS Watt energy efficiency on edge FPGAs, delivering a 311 energy efficiency improvement over GPUs and a 3.5 throughput improvement over the state-of-the-art binary accelerator, with only negligible inference accuracy degradation. I. INTRODUCTION In recent years, transformer-based models have become foundational architectures across multiple domains, achieving state-of-the-art performance in tasks such as natural language processing [1], computer vision [2], and others [3], [4]. These models excel at capturing complex patterns through self- attention mechanisms and extensive parametrization, often involving billions of parameters to achieve superior perfor- mance. However, the increasing size of these models intro- duces significant computational, memory, and communication challenges, restricting their deployment to a wide range of devices, especially resource-constrained devices. For many edge applications, such resource demands are impractical, making the deployment of full-scale transformers on edge devices particularly difficult, especially in scenarios requiring real-time inference.\n\n--- Segment 2 ---\nHowever, the increasing size of these models intro- duces significant computational, memory, and communication challenges, restricting their deployment to a wide range of devices, especially resource-constrained devices. For many edge applications, such resource demands are impractical, making the deployment of full-scale transformers on edge devices particularly difficult, especially in scenarios requiring real-time inference. To address these challenges, model compression techniques like quantization [5] and pruning [6] have been proposed. Quantization reduces computational requirements by lower- ing numerical bitwidths, though it can lead to performance degradation. Despite these advances, quantized models often remain inefficient on edge devices due to the lack of effective acceleration on existing hardware. Equal contribution Binary transformers, an extreme form of quantized trans- formers, represent all weights and activations using binary val- ues like t 1, 1u or t0, 1u. These models significantly reduce computational complexity, model size, and communication bandwidth by replacing expensive floating-point operations with efficient bitwise logical operations [7], [8], [9], [10]. This approach enables faster execution with lower power consumption, making binary transformers a promising solution for energy efficient edge deployment. Despite these advantages, binary transformer models face significant challenges in implementation on existing hardware. The primary issue is the lack of optimized hardware units for binary matrix multiplication, as most processors and acceler- ators are designed for integer and floating-point computation, which are inefficient for binary operations. Furthermore, previ- ously proposed binary transformer models are often not well- suited for hardware optimization. For instance, BinaryBERT [11] and BiBERT [12] either do not fully binarize the model or retain some ternary representations. Similarly, BiT [13] employs both t 1, 1u and t0, 1u binarization schemes (though not for the same operation) and uses the original softmax function, which imposes a significant performance burden on hardware acceleration design, as will be discussed in a later section. To overcome these limitations, we propose COBRA, a hardware software co-designed Binary Transformer Acceler- ator optimized for edge FPGAs. COBRA introduces several innovations, including the Shifted Polarized Softmax (SPS) for hardware-efficient attention, a true 1-bit binary multiplication method for 1, 0, and 1 matrices, and integer packing to maximize bandwidth.\n\n--- Segment 3 ---\nTo overcome these limitations, we propose COBRA, a hardware software co-designed Binary Transformer Acceler- ator optimized for edge FPGAs. COBRA introduces several innovations, including the Shifted Polarized Softmax (SPS) for hardware-efficient attention, a true 1-bit binary multiplication method for 1, 0, and 1 matrices, and integer packing to maximize bandwidth. Additional optimizations, such as efficient popcount units, operation fusion, processing element reuse, and parallelism tuning, further enhance throughput, latency, and resource efficiency. Our evaluations show that COBRA operates efficiently on both mid-range edge FPGA (ZCU102) and low-power edge FPGA (KV260), making it portable and well-suited for resource-limited edge deployment on various devices. The contributions of this work are sum- marized as follows: We propose COBRA, an algorithm-architecture co- optimized hardware accelerator for efficient inference of binary Transformer models. COBRA achieves 3.5ÀÜ speedup over state-of-the-art binary transformer acceler- arXiv:2504.16269v2 [cs.AR] 24 Apr 2025 ators. At hardware architecture level, we propose a real 1-bit binary matrix multiplication engine, RBMM. It achieves high computational efficiency with real 1-bit operations, using bitwise XNOR and popcount operations with a unique don t care (DC) count mechanism. RBMM is designed to support serveral variations of binary opera- tions, making it reusable and resource-efficient for edge devices, and it could be extended to other binary models. At algorithm level, we propose a hardware-friendly binary attention mechanism, shifted polarized softmax, SPS, which enables efficient hardware implementation with negligible impact on transformer models inference ac- curacy. At system level, we propose a series of optimizations including 6:3 compressor-based popcount, quantization- fused multiplication, and long bitwidth datapack repre- sentation that further enhance the system performance of COBRA. II.\n\n--- Segment 4 ---\nAt system level, we propose a series of optimizations including 6:3 compressor-based popcount, quantization- fused multiplication, and long bitwidth datapack repre- sentation that further enhance the system performance of COBRA. II. BACKGROUND AND RELATED WORK A. Binary Transformer 1) Binary Transformer Basic: The binarized Transformer represents an extreme case of transformer quantization, where full-precision real number weights and activations Wr, Ar are approximated with binary values Wb, Ab as follows: Wr, Ar Œ±Wb, Œ≥Ab where Wb SignpWrq, Œ± 1 n}Wr}, n is the total number of elements in Wr. Activations are approximated with a similar way. Here, Œ± and Œ≥ are scaling factors, which may vary across different approaches. For example, the state-of-the-art binary transformer BiT [13] uses these scaling factors in the initialization, and further trains them as part of the model weights during updates. Our work adopts a similar strategy. Without considering bias, the originial full-precision matrix multiplication can be approximated with binary operations as: Wr b Ar Popcount pXNORpWb, Abqq Œ±Œ≥ where b is matrix multiplication operator and popcount re- turns the number of 1 s in a binary format number. 2) Binary Transformer Designs: Most transformer quanti- zation efforts focus on 8-bit or 4-bit integer quantization due to the inherent difficulty of quantizing the attention block. Only a few studies address extreme low-bit transformers. For instance, TernaryBERT [14] proposed ternarizing the weights of the BERT model and fine-tuning it with a small dataset, achieving reasonable results. BinaryBERT [11] was the first to quantize both weights and activations of the BERT base model to binary, although it maintained a 2-bit embedding layer. This approach begins by initializing the network with weights from a fully trained half-sized ternary model, then proceeds by splitting weights to construct the binary model, followed by fine-tuning with knowledge distillation. Bin. (0,1) Binarize Binarize Binarize Bin. Bin. Bin.\n\n--- Segment 5 ---\nBin. Bin. Linear Linear Linear Linear Linear Linear ReLU Layer Norm Layer Norm Softmax Binarize (0,1) Shifted Polarized Softmax Replace Input Embedding Input N Task Specific Linear Binary Transformer Encoder Block Output RBMM Operation Modes (Intrinsics) Legend (Details in Section III-B4) M2 M1 M3 F1 M4 F2 Fig. 1: General structure of binarized BERT with our Shifted Polarized Softmax (SPS) and Real Binary RBMM Engine Built on BinaryBERT, two later works BiBERT [12] and BiT [13] further binarized all weights, activations, and em- beddings. They compensate performance loss and maximize representational capability through direct matching, precision- progressive distillation, a two-set binarization scheme, and an elastic binary activation function with learnable parameters. These efforts yielded substantial model accuracy. However, the lack of hardware considerations, such as efficient softmax implementation and compatibility with two-set binarization schemes, limits the ability of these models to be accelerated efficiently with hardware, posing challenges for specialized accelerator design. B. Hardware Acceleration of Binary Transformers 1) Binary CNN Accelerators Are Not Suitable for Trans- formers: While previous works have proposed accelerators for binary convolutional neural networks (CNNs) [8], [15], [16], they are unsuitable for binary transformers due to differences in computation irregularity, data dependency, and memory access patterns. Transformers rely on fine-grained parallelism across long sequence dimensions for self-attention and large matrix multiplications, unlike the structured parallelism of CNNs. Furthermore, binary transformers require two specific types of binary schemes to function effectively and achieve acceptable accuracy a challenge not present in binary CNNs. 2) Binary Transformer Acceleration: Hardware accelera- tion for binary transformer networks presents unique chal- lenges compared to full-precision networks. Fig. 1 illustrates the general structure of a binarized BERT model. Binary Transformer models require two types of binarization schemes, t 1, 1ubt 1, 1u and t 1, 1ubt0, 1u, and involve two distinct matrix multiplication operations: linear layer and attention computation. Some existing works aim to enhance the efficiency of binary transformers.\n\n--- Segment 6 ---\nBinary Transformer models require two types of binarization schemes, t 1, 1ubt 1, 1u and t 1, 1ubt0, 1u, and involve two distinct matrix multiplication operations: linear layer and attention computation. Some existing works aim to enhance the efficiency of binary transformers. VAQF [17] introduced a binary vision Trans- former (ViT) accelerator that exploits the speedup potential of binarization by converting multiplication into bitwise logical operations, as discussed in an earlier section. However, it only supports one execution mode and does not consider 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1.0 0.9 0.8 0.1 0.2 0.1 0.1 1.0 0.7 0.3 0.1 0.1 0.3 0.6 1.0 0.7 0.1 0.2 0.2 0.2 0.5 1.0 0.9 0.3 0.2 0.1 0.1 0.7 1.0 0.6 0.1 0.1 0.7 0.4 0.8 1.0 ùúÜùëñ [ùúÜùëñ,1, ùúÜùëñ,2, , ùúÜùëñ,ùëò] Original BiT Binary Attention Attention Map Distortion Rate Original BiT (Sofmax-Round-Clip) COBRA SPS Binary Attention Argmin Update 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 Similarity MSE Fig. 2: Shifted Polarized Softmax (SPS) Search the matrix multiplication of binarized attention. BETA [18] employs a Compressor Tree Loop to create an accelerator specifically for fully binary transformers, supporting both matrix multiplication types and achieving improved efficiency and throughput on edge platforms. BAT [19] offers a co-design approach, creating a custom binarized transformer model with a specialized hardware accelerator for edge deployment.\n\n--- Segment 7 ---\nBETA [18] employs a Compressor Tree Loop to create an accelerator specifically for fully binary transformers, supporting both matrix multiplication types and achieving improved efficiency and throughput on edge platforms. BAT [19] offers a co-design approach, creating a custom binarized transformer model with a specialized hardware accelerator for edge deployment. However, they rely on encoding ternary values t 1, 0, 1u as two-bit numbers through lookup tables, which is not a real binary representation, therefore it did not fully leverage the unique advantages of binary models. In summary, no existing hardware acceleration works fully leverage the potential of binary transformers, nor address the primary performance (latency) bottleneck for binary acceler- ator design: the softmax operations. Our proposed RBMM engine, along with the Shifted Polarized Softmax (SPS), addresses these limitations and unlocks the full potential of binary transformers for edge deployment. 3) Attention Mechanism Acceleration: The self-attention, constitutes the cornerstone of transformer models, enabling the dynamic weighting of input sequence elements based on their relative importance. However, this capability incurs a significant computational cost: attention mechanisms exhibit quadratic time and space complexity relative to sequence length. Flash Attention [20] represents a prominent GPU- optimized approach to accelerating attention computations. Its architecture leverages HBM and SRAM on GPUs, employing a tiled computation strategy that integrates the softmax and matrix multiplication operations. Nevertheless, this method is not directly adaptable to edge FPGAs, which are constrained by the limited bandwidth of DDR memory and limited MB- level on-chip BRAM and URAM. III. COBRA METHODOLOGY A. Hardware Friendly Binary Attention Mechanism Design 1) Real Binary Attention: BiBERT [12] and BiT [13] fol- lowed Eq. (1) to implement softmax in their binary attentions. This approach overlooks hardware-level overhead of softmax, leading to suboptimal computation speed, hardware costs, and power efficiency, making it impractical for edge deploy- ment. Furthermore, the use of softmax requires exponential computation and produces non-binary values in the attention mechanism, preventing BiT from achieving a fully binarized transformer model and complicating efficient hardware ac- celeration. In detail, BiT calculates a vanilla attention score (including softmax operation) and applies elastic binarization as Eq.\n\n--- Segment 8 ---\nFurthermore, the use of softmax requires exponential computation and produces non-binary values in the attention mechanism, preventing BiT from achieving a fully binarized transformer model and complicating efficient hardware ac- celeration. In detail, BiT calculates a vanilla attention score (including softmax operation) and applies elastic binarization as Eq. (2), effectively redistributing the attention score to binary domain. Softmaxpziq exppziq ≈ôn j 1 exppzjq (1) Att probBiT clip round Softmax QbKJ b ?dk Œ± , 0, 1 (2) where clipp , 0, 1q clamps input elements into r0, 1s, and roundp q rounds input to the nearest integer. However, both the softmax operation and the elastic binarization function rely on many floating-point operations, which dominate the computational complexity of binary models and significantly compromise the hardware acceleration. To overcome these limitations, we propose Shifted Polarized Softmax (SPS) as a direct replacement for softmax and bina- rization in attention computation. SPS generates binary-state outputs directly, without relying on any floating-point arith- metic, thereby eliminating the need for a separate binarization step and further improving hardware efficiency. 2) Shifted Polarized Softmax (SPS): Conventional softmax scales input values into a continuous probability distribution over the range (0,1) and the quantization function rescale the distribution back to binary form, the SPS function in- stead polarizes values directly into binary states based on a learnable threshold, capturing the core purpose of soft- max emphasizing stronger signals without the need for continuous probabilistic scaling, as shown in Eq. (3). SPSpzq 1, if z ƒõ Œªi,k 0, otherwise (3) where i indicate ith attention block and k represent Kth attention head in the attention block. Att probSP S SPS ÀÜQbKJ b ?dk (4) As discussed in [12], for the attention mechanism to capture crucial elements in the binary representation, maximizing the information entropy of binarized attention score is a must and a binarized attention score matrix can serve the purpose. Hence, we propose SPS to replace Eq.\n\n--- Segment 9 ---\nAtt probSP S SPS ÀÜQbKJ b ?dk (4) As discussed in [12], for the attention mechanism to capture crucial elements in the binary representation, maximizing the information entropy of binarized attention score is a must and a binarized attention score matrix can serve the purpose. Hence, we propose SPS to replace Eq. (2) by search for a series of head-wise thresholds Œª P h ÀÜ n that further compensate the information loss for not having softmax normalization for attention score. SPS approximates the effects of softmax plus elastic binarization function while significantly reducing computational overhead, making it more compatible with hardware constraints. Our Final attention probability can be found as Eq. (4). TABLE I: Accuracy of COBRA vs. Others. All binary transformer models use W1A1 (1-bit weights and activations). Variations MNLI-m mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Average Relative Perf. Original BERT[1] 84.9 85.5 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.9 129.1 BinaryBERT[11] 35.6 35.3 66.5 51.5 53.2 0 6.1 68.3 52.7 41.0 55.6 BiBERT[12] 69.5 71.1 84.9 78.3 87.5 25.4 34.7 72.5 49.8 63.8 89.7 BiT[13] 75.6 76.3 84.7 82.8 88.0 27.4 69.8 77.4 52.7 71.0 100 COBRA (Layer) 67.6 68.7 79.03 70.1 85.7 25.4 66.3 74.3 53.4 65.1 93.8 COBRA (Row) - - 71.5 87.1 26.6 - - 53.0 - - COBRA (Head) 71.3 72.1 83.3 73.2 86.9 27.9 69.2 80.2 53.4 68.2 98.2 BiT [13] was selected as the baseline, as it represents the state-of-the-art in binary BERT models.\n\n--- Segment 10 ---\nVariations MNLI-m mm QQP QNLI SST-2 CoLA STS-B MRPC RTE Average Relative Perf. Original BERT[1] 84.9 85.5 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.9 129.1 BinaryBERT[11] 35.6 35.3 66.5 51.5 53.2 0 6.1 68.3 52.7 41.0 55.6 BiBERT[12] 69.5 71.1 84.9 78.3 87.5 25.4 34.7 72.5 49.8 63.8 89.7 BiT[13] 75.6 76.3 84.7 82.8 88.0 27.4 69.8 77.4 52.7 71.0 100 COBRA (Layer) 67.6 68.7 79.03 70.1 85.7 25.4 66.3 74.3 53.4 65.1 93.8 COBRA (Row) - - 71.5 87.1 26.6 - - 53.0 - - COBRA (Head) 71.3 72.1 83.3 73.2 86.9 27.9 69.2 80.2 53.4 68.2 98.2 BiT [13] was selected as the baseline, as it represents the state-of-the-art in binary BERT models. BiT Attention Map (One Head) SPS Attention Map (One Head) Difference (a) Heatmap Correlation Comparison 0 25 50 75 100 125 0 20 40 60 80 100 120 Cosine Similarity (One Head) 0.3 0.4 0.5 0.6 0.7 0.8 0.9 (b) Cosine Similarity Compari- son 3 4 5 6 7 8 1e 5 7.61e 1 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 1e 5 4.991e 1 PCA Projection (c) PCA Projection Comparison 0 20 40 60 80 100 120 0 2 4 6 Row Norms Comparison (One Head) BiT Map SPS Map (d) Attention Row Norm Comparison Fig.\n\n--- Segment 11 ---\nOriginal BERT[1] 84.9 85.5 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.9 129.1 BinaryBERT[11] 35.6 35.3 66.5 51.5 53.2 0 6.1 68.3 52.7 41.0 55.6 BiBERT[12] 69.5 71.1 84.9 78.3 87.5 25.4 34.7 72.5 49.8 63.8 89.7 BiT[13] 75.6 76.3 84.7 82.8 88.0 27.4 69.8 77.4 52.7 71.0 100 COBRA (Layer) 67.6 68.7 79.03 70.1 85.7 25.4 66.3 74.3 53.4 65.1 93.8 COBRA (Row) - - 71.5 87.1 26.6 - - 53.0 - - COBRA (Head) 71.3 72.1 83.3 73.2 86.9 27.9 69.2 80.2 53.4 68.2 98.2 BiT [13] was selected as the baseline, as it represents the state-of-the-art in binary BERT models. BiT Attention Map (One Head) SPS Attention Map (One Head) Difference (a) Heatmap Correlation Comparison 0 25 50 75 100 125 0 20 40 60 80 100 120 Cosine Similarity (One Head) 0.3 0.4 0.5 0.6 0.7 0.8 0.9 (b) Cosine Similarity Compari- son 3 4 5 6 7 8 1e 5 7.61e 1 4.2 4.4 4.6 4.8 5.0 5.2 5.4 5.6 1e 5 4.991e 1 PCA Projection (c) PCA Projection Comparison 0 20 40 60 80 100 120 0 2 4 6 Row Norms Comparison (One Head) BiT Map SPS Map (d) Attention Row Norm Comparison Fig. 3: Similarity and Correlation Comparisons Between BiT (with Regular Softmax) and SPS Attention. 3) SPS Thresholds Search and Model Training: Figure 2 illustrates our method and search strategy for determining the SPS thresholds.\n\n--- Segment 12 ---\n3: Similarity and Correlation Comparisons Between BiT (with Regular Softmax) and SPS Attention. 3) SPS Thresholds Search and Model Training: Figure 2 illustrates our method and search strategy for determining the SPS thresholds. Suppose two attention maps, A1 and A2, are obtained from the same layer Layeri. We define the Channel Distortion Rate (CDR) as the mean squared error (MSE) between these two maps: Distortion 1 n i√ø n 1 pA1 A2q2 (5) The thresholds Œª P RhÀÜn are predefined within the range r0, 1s with a search granularity of 0.05 and an initial value of 0(regular sign function). Our objective is to identify the optimal thresholds that minimize the output discrepancy be- tween BiT s softmax-based attention (Eq. 2) and our SPS- based attention (Eq. 4) for a given attention layer. Formally, we solve: Œª arg min Œª pDistortion pAtt probBiT, Att probSPSpŒªqqq (6) Here, Œª represents the per-head threshold. To avoid overfitting to any specific task, we uniformly sample 10 of each dataset (benchmark) to construct a small calibration set for the search process. This process identifies a unique threshold for each attention head in every attention block and is highly efficient, requiring only about five minutes in our experiments. We further evaluate different threshold granularities per layer, per head, and per row of the attention map which will be discussed in the next section. Balancing effectiveness and efficiency, we adopt the head-wise threshold configuration in our final design. This decision is well-supported, as different attention heads often capture distinct relational patterns among tokens and contribute to diverse representations, thereby en- hancing the model s expressivity. Figure 3 presents multiple similarity and correlation mea- surements between BiT (with Softmax) and SPS attention, demonstrating the validity of our SPS approximation. Finally, to further enhance COBRA s performance, we fix the searched thresholds Œª and fine-tune the model weights using the original training data to compensate for the information loss introduced by the SPS approximation.\n\n--- Segment 13 ---\nFigure 3 presents multiple similarity and correlation mea- surements between BiT (with Softmax) and SPS attention, demonstrating the validity of our SPS approximation. Finally, to further enhance COBRA s performance, we fix the searched thresholds Œª and fine-tune the model weights using the original training data to compensate for the information loss introduced by the SPS approximation. 4) SPS Results Analysis: Table I presents the accuracy performance of our binary COBRA model on the GLUE benchmarks [21], compared against the original BERT and other binary BERT variants. Our model outperforms both BinaryBERT [11] and BiBERT [12] by a significant margin, with only a less than 2 minor average performance drop compared to the state-of-the-art BiT [13]. Despite this small accuracy reduction, our model relies exclusively on true bi- nary operations and provides substantial hardware advantages, which will be further discussed in Section IV-C. We also evaluate different granularities of the SPS threshold. The attention layer-wise threshold performs adequately, with minimal overhead and the shortest search time. However, as previously discussed, the head-wise threshold yields the best performance, incurring similarly negligible overhead com- pared to the layer-wise case, and the search can be completed relatively quickly. Additionally, we conducted experiments using a more fine-grained row-wise threshold at the attention map.\n\n--- Segment 14 ---\nHowever, as previously discussed, the head-wise threshold yields the best performance, incurring similarly negligible overhead com- pared to the layer-wise case, and the search can be completed relatively quickly. Additionally, we conducted experiments using a more fine-grained row-wise threshold at the attention map. The results indicate that it does not offer meaningful Unsigned (0,1) (-1, 1) 0 1 0 0 0 1 AND 0 0 0 0 0 1 Don't care count (Number of "0"s) 4 RBVM Number of "1"s - Number of "-1"s 2 popcount - data width Don't care count 0 2-bit vector 6-bit binary datapack 0 1 0 0 0 1 1 -1 1 -1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 -5 1 -4-1-2 3 previous RBMM that computes Number of "-1"s Number of "0"s Number of "1"s popcount 1 Signed (-1,1) (-1, 1) 0 1 0 0 0 1 XNOR 0 0 0 1 0 1 2-bit vector 6-bit binary datapack -1 1 -1-1-1 1 1 -1 1 -1 1 1 1 0 1 0 1 1 data width - popcount 4 Number of "-1"s Number of "1"s popcount 2 RBVM Number of "1"s - Number of "-1"s 2 popcount - data width -2 data width - popcount 5 1 1 1 1 binarization Fig. 4: A 6-bit example of our RBVM.\n\n--- Segment 15 ---\nThe results indicate that it does not offer meaningful Unsigned (0,1) (-1, 1) 0 1 0 0 0 1 AND 0 0 0 0 0 1 Don't care count (Number of "0"s) 4 RBVM Number of "1"s - Number of "-1"s 2 popcount - data width Don't care count 0 2-bit vector 6-bit binary datapack 0 1 0 0 0 1 1 -1 1 -1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 -5 1 -4-1-2 3 previous RBMM that computes Number of "-1"s Number of "0"s Number of "1"s popcount 1 Signed (-1,1) (-1, 1) 0 1 0 0 0 1 XNOR 0 0 0 1 0 1 2-bit vector 6-bit binary datapack -1 1 -1-1-1 1 1 -1 1 -1 1 1 1 0 1 0 1 1 data width - popcount 4 Number of "-1"s Number of "1"s popcount 2 RBVM Number of "1"s - Number of "-1"s 2 popcount - data width -2 data width - popcount 5 1 1 1 1 binarization Fig. 4: A 6-bit example of our RBVM. ùë® ùë´ùíÇùíïùíÇùíëùíÇùíÑùíå ùë© ùë´ùíÇùíïùíÇùíëùíÇùíÑùíåùüè ùë© ùë´ùíÇùíïùíÇùíëùíÇùíÑùíåùëµùíëùíÜ A B On Chip Memory RBMM Engine ùë∂ùíñùíïùíëùíñùíïùüêùë® ùë∂ùíñùíïùíëùíñùíïùüêùë© Off-Chip DDR RAM A B M2 M3 Output Return DC Return LayerNorm Unit Local Buffer AXI BUS ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùëµùíëùíÜ Npe ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùüê ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùüè COBRA Control Internal Read Write Control Don t care INPUT bias Threshold COBRA Control RBMM Engine On Chip Memory Fig.\n\n--- Segment 16 ---\n4: A 6-bit example of our RBVM. ùë® ùë´ùíÇùíïùíÇùíëùíÇùíÑùíå ùë© ùë´ùíÇùíïùíÇùíëùíÇùíÑùíåùüè ùë© ùë´ùíÇùíïùíÇùíëùíÇùíÑùíåùëµùíëùíÜ A B On Chip Memory RBMM Engine ùë∂ùíñùíïùíëùíñùíïùüêùë® ùë∂ùíñùíïùíëùíñùíïùüêùë© Off-Chip DDR RAM A B M2 M3 Output Return DC Return LayerNorm Unit Local Buffer AXI BUS ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùëµùíëùíÜ Npe ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùüê ùëπùë©ùë¥ùë¥ ùë∑ùë¨ùüè COBRA Control Internal Read Write Control Don t care INPUT bias Threshold COBRA Control RBMM Engine On Chip Memory Fig. 5: COBRA Hardware Architecture Overview. performance improvements while introducing more parameters to the model. It also increases the search time by over 20 . Therefore, we adopt the head-wise threshold as the default configuration for the SPS function in our COBRA models. Additionally, all threshold granularity configurations are fully supported by our proposed accelerator architecture without incurring any extra computation time. B. COBRA Hardware Accelerator Design 1) Real 1-bit Binary Matrix Multiplication (RBMM): To fully leverage the benefits of binary representations, we propose an efficient algorithm, RBMM, for real binary matrix multiplication. Consider matrices A ra1, , am, , aMsJ P tp0, 1q p 1, 1quMÀÜN, B rb1, , bp, , bP s P tp 1, 1quNÀÜP , and multiplication result C A b B P NMÀÜP .\n\n--- Segment 17 ---\nB. COBRA Hardware Accelerator Design 1) Real 1-bit Binary Matrix Multiplication (RBMM): To fully leverage the benefits of binary representations, we propose an efficient algorithm, RBMM, for real binary matrix multiplication. Consider matrices A ra1, , am, , aMsJ P tp0, 1q p 1, 1quMÀÜN, B rb1, , bp, , bP s P tp 1, 1quNÀÜP , and multiplication result C A b B P NMÀÜP . To optimize binary computations while eliminating sign bit management in ternary multiplica- tion, we propose a unified representation for matrices A and B, where 1 is encoded as 0 while preserving 1 . For computational efficiency, the bit-packing strategy is adopted where elements am are packed into N-bit am datapack, and similarly, bp, are packed into N-bit bp. Then, the real binary vector dot product (RBVM), denoted as ambp am b bm, can be derived as (steps omitted for brevity): ambbp 2 popcountpam bpq N, am Ptp 1, 1qu 2 popcountpam bpq N Œ¥m, am Ptp0, 1qu (7) where N represents the data bitwidth of the datapack, pop- count counts the number of 1 s in the vector, and denote bit-wise logical operations XNOR and AND respectively, and Œ¥m denotes the number of zeros in the unsigned datapack am ( don t care count, DC). Fig. 4 illustrates our RBVM algorithm with a 6-bit toy example (768 bits in our actual design). The algorithm determines the count of 1 s through popcount operations, from which the number of 1 s can be derived. For the p0, 1q binarization scheme, the number of 0 s are considered. Moreover, N can be fused with ƒõ T binarization in Eq. (3), which will be detailed in the Section III-B4. Although this approach seemingly requires an addi- tional popcount operation to determine Œ¥m, this computation can be efficiently integrated into the previous quantization- fused RBMM as shown in Fig. 4. Specifically, when a 0 quantization value is calculated, the row s Œ¥m is incremented by 1.\n\n--- Segment 18 ---\n4. Specifically, when a 0 quantization value is calculated, the row s Œ¥m is incremented by 1. Besides, the RBVM operation exhibits compositional properties when column row vectors are subdivided into S smaller vectors, where am ram1, ..., ams, ..., amSs and bp rbp1, ..., bps, ..., bpSsJ. The vector dot product property is preserved as follows, ambp S√ø s 1 amsbps S√ø s 1 ams b bps, (8) where am concatpam1, ..., ams, ..., amSq and bp concatpbp1, ..., bps, ..., bpSq. This decomposition enables the RBMM engine to support both smaller-sized RBMM oper- ations and larger configurations, making the RBMM engine reusable for both multi-head attention (MHA) and feed- forward network (FFN). 2) Quantization-fused RBMM: On top of our RBMM de- sign, we further fuse quantization into it and create the quantization-fused RBMM engine. The unsigned and signed binary quantization schemes can then be mathematically ex- pressed as: cb ij clip round cij Œ≤j Œ± , 0, 1 , (0, 1) scheme sign cij Œ≤j Œ± , (-1, 1) scheme (9) where cij is an element from C, cb ij is the binarized value, Œ± P N denotes the scaling factor, Œ≤i is an element from the shift vector Œ≤ P N1ÀÜP and the sign of zero is deemed as 1 .\n\n--- Segment 19 ---\n2) Quantization-fused RBMM: On top of our RBMM de- sign, we further fuse quantization into it and create the quantization-fused RBMM engine. The unsigned and signed binary quantization schemes can then be mathematically ex- pressed as: cb ij clip round cij Œ≤j Œ± , 0, 1 , (0, 1) scheme sign cij Œ≤j Œ± , (-1, 1) scheme (9) where cij is an element from C, cb ij is the binarized value, Œ± P N denotes the scaling factor, Œ≤i is an element from the shift vector Œ≤ P N1ÀÜP and the sign of zero is deemed as 1 . Since the unified hardware representation for 1 and 0 has been applied as mentioned earlier, the two binarization schemes can be unified as, cb ij 1, cij Œ∏j ƒõ 0 0, cij Œ∏j ƒÉ 0 , Œ∏j r 1 2Œ± Œ≤j , (0, 1) Œ≤j, (-1, 1) (10) dh -bit OR OR a1 b1 MODE CTRL 6:3 3 Popcount Unit 6:3 6:3 6:3 6:3 6:3 Compressor 6:3 Every 36 bits [2:0] [2:0] 6:3 6 [0] [1] [2] [2:0] 6:3 6:3 2 1 1 1 s in 36 bits (log2dh 1)-bit threshold data width sign 0 Atten Mask CTRL DC HEAD 0 MODE CTRL HEAD PE ACC PATH CONCAT PATH OUT Shifted Polarized Softmax(SPS) dh DC INPUT d dh -bit a2 b2 a1 b1 RBMM PE CONCAT DC RETURN bias MODE CTRL OUTPUT RETURN MODE CTRL DC INPUT 0 0 M3 F2 Œî M4 F2 M2 Œî Œî F1 Œî F2 Œî M3 F2 Œî M2 DC FULL XNOR AND Unit bias Bo -bit Bo -bit (log2dh 1)-bit 1-bit b1 bh b2 HEAD PE AND Popcount Unit Shifted Polarized Softmax a1 a2 ah datapacks ùõøùõø1 ùõøùõøùüêùüê . . . ùõøùõø . .\n\n--- Segment 20 ---\nùõøùõø . . ùõøùõøùëëùëë . . ùúÉùúÉ1 ùúÉùúÉùüêùüê . . . ùúÉùúÉ . . ùúÉùúÉùëëùëë .. HEAD PE Fig. 6: The Architecture of RBMM Processing Elements (M1 4 and F1 2 are operation modes of RBMM engine). where Œ∏j is an element from the bias vector, and r is the round operation. Furthermore, the unsigned binarization is only employed after the ReLU operation within the feed- forward network (FFN), as illustrated in Fig. 5. From a hardware implementation perspective, these operations can be efficiently merged due to the overlapped range between the conditions cij ƒõ rp 1 2Œ± Œ≤jq and cij ƒõ 0 in the ReLU function. Consequently, the Œ∏j is set as maxp0, rp 1 2Œ± Œ≤jqq. 3) Overall Hardware Architecture: As illustrated in Fig. 5, the accelerator architecture consists of five major components: an RBMM engine, a LayerNorm unit, a COBRA configuration control unit, a data packing conversion unit, and a master AXI interface. For clarity, we define the following notation: d represents the hidden dimension, h denotes the number of heads, dh d{h indicates the size per head, FFsize Rd (R ƒÖ 1) represents the FFN hidden dimension, l (l ƒÉ d) denotes the sequence length, and Npe represents the number of Processing Elements (PEs). The numbers in the multiplexer indicate the RBMM modes (detailed in the next Section III-B4), while represents all other modes. The notation (n x ÀÜ y ÀÜ z) represents the RBMM where n is the number of matrix multiplications, the first matrix has dimensions x ÀÜ y, the second matrix has dimensions y ÀÜ z, and the result matrix has dimensions x ÀÜ z. The RBMM engine supports multiple configurations (i.e., operation modes) of quantization-fused matrix multiplication operations. We will discuss the operation modes of RBMM in the next Section III-B4.\n\n--- Segment 21 ---\nThe RBMM engine supports multiple configurations (i.e., operation modes) of quantization-fused matrix multiplication operations. We will discuss the operation modes of RBMM in the next Section III-B4. The LayerNorm unit implements the normalization function using the 16-bit fixed-point scaling fac- tors and value for computation to optimize hardware efficiency. The COBRA control unit manages the RBMM configuration parameters and orchestrates the overall computation flow. The data packing conversion unit efficiently transforms interme- diate matrices into binary datapack formats. External DDR memory accesses like weight matrix fetching and intermediate array storage are facilitated via the AXI high-performance master interface. 4) RBMM Engine Design: The RBMM engine is designed and optimized for efficient matrix multiplication of various matrix sizes in both MHA and FFN modules. Therefore we only need one single RBMM engine to handle all the matrix multiplications in the Transformer. This flexible design greatly optimize the area and power efficiency. The engine integrates activation functions and quantization operations directly, with behavior determined by the RBMM mode. As illustrated in Fig. 5, for each invocation of the RBMM engine, it produces Npe RBVM results. The number of invo- cations is different in different mode configurations which is set by the COBRA controller. Each invocation of the engine is scheduled at an interval of one clock cycle, i.e., the initiation interval (II) of the pipeline is 1, which means the latency of RBMM Engine execution in each invocation is perfectly overlapped, and the modules inside RBMM are fully pipelined. The RBMM Engine accepts one d-bit row datapack from matrix A with its associated 1 ÀÜ l DC INPUT vector, Npe d-bit column datapacks from matrix B, and a quantization 1 ÀÜ d bias vector as inputs. Each RBMM PE processes a single row datapack and column datapack pair, computing their RBVM result and updating the DC RETURN which includes h DC HEADs and a DC FULL after the ACC PATH. The DC RETURN will be treated as DC INPUT in the subsequent RBMM if (0, 1) binarization scheme is being used. Within the RBMM Engine shown in Fig.\n\n--- Segment 22 ---\nThe DC RETURN will be treated as DC INPUT in the subsequent RBMM if (0, 1) binarization scheme is being used. Within the RBMM Engine shown in Fig. 6, each RBMM PE consists of h HEAD PEs that compute RBVM results for dh- bit head datapacks, and the final d-bit datapack RBVM result is obtained by accumulating these head-level computations as formulated in Eq. (8). As for the popcount unit in HEAD PE, 64 3-bit ROM-based 6:3 compressors can be used to compute the population count of every 36 bits. Initially, six compressors determine the number of ones in each group of six input bits. In the subsequent stage, the six 3-bit outputs from these compressors are combined using another round of compressors by feeding in the third bits, second bits, and first bits separately. Finally, a shift-and-add operation is performed to accumulate the results of the third, second, and first bits, yielding the total popcount. For FFN computations, since the hidden size is FFsize Rd, the RBMM PE return value is accumulated with the previous output. As for bit width, the HEAD PE result requires plog2 dh 1q bits, while the RBMM Engine output uses maxplog2 FFsize 1, hq Bo bits, enabling efficient multi-head computation where binary multi-head attention results can be concatenated into a k-bit return value. What s more, the quantized binary RBVM output in this case set all Bo-bit to represent 0 or 1 and when it comes to the integer output, all bits are utilized to represent the integer. Finally, the address of the return output value is controlled by the Read Write unit.\n\n--- Segment 23 ---\nWhat s more, the quantized binary RBVM output in this case set all Bo-bit to represent 0 or 1 and when it comes to the integer output, all bits are utilized to represent the integer. Finally, the address of the return output value is controlled by the Read Write unit. TABLE II: COBRA Hardware Performance Evaluation and Comparison against Previous Works Platform GPU FPGA RTX 3090 FQ-BERT[22] COSA[23] TransFRU[24] BETA[18] BAT[19] KV260 ZCU102 Network BERT BiT COBRA FQ-BERT BERT BERT BiT BinaryBERT BiBERT BMT COBRA Npe 16 COBRA Npe 32 Precision W16A16 W1A1 W1A1 W4 8A8 W8A8 W4A4 W1A1 W1A1 W1A1 W1A2 W1A1 W1A1 Throughput (GOPS) 749.1 484.26 503.14 22.74 1556.5 1223.90 1240.98 1387.59 1436.07 1122.40 846.28 3894.74 Power (W) 350.00 350.00 350.00 9.80 31.3 45.46 7.18 7.95 8.20 5.47 4.07 8.68 Energy Efficiency (GOPS W) 2.14 1.38 1.44 2.32 49.7 26.9 172.41 174.59 175.23 207.7 207.93 448.70 TABLE III: FPGA Resource Utilization Comparison. means not reported or don t have the kind of resource.\n\n--- Segment 24 ---\nTABLE II: COBRA Hardware Performance Evaluation and Comparison against Previous Works Platform GPU FPGA RTX 3090 FQ-BERT[22] COSA[23] TransFRU[24] BETA[18] BAT[19] KV260 ZCU102 Network BERT BiT COBRA FQ-BERT BERT BERT BiT BinaryBERT BiBERT BMT COBRA Npe 16 COBRA Npe 32 Precision W16A16 W1A1 W1A1 W4 8A8 W8A8 W4A4 W1A1 W1A1 W1A1 W1A2 W1A1 W1A1 Throughput (GOPS) 749.1 484.26 503.14 22.74 1556.5 1223.90 1240.98 1387.59 1436.07 1122.40 846.28 3894.74 Power (W) 350.00 350.00 350.00 9.80 31.3 45.46 7.18 7.95 8.20 5.47 4.07 8.68 Energy Efficiency (GOPS W) 2.14 1.38 1.44 2.32 49.7 26.9 172.41 174.59 175.23 207.7 207.93 448.70 TABLE III: FPGA Resource Utilization Comparison. means not reported or don t have the kind of resource. Work FQ-BERT [22] COSA [23] TransFRU [24] BETA [18] BAT [19] Our Work Platform ZCU102 XCVU13P Alveo U280 ZCU102 ZCU102 KV260 ZCU102 Network FQ-BERT BERT BERT BiT BiBERT BinaryBERT BMT COBRA Npe 16 COBRA Npe 32 LUT 123K (45 ) 1147K (66 ) 633K (49 ) 191K (70 ) 146K (53 ) 86K (74 ) 122K (45 ) FF 124K (23 ) 526K (15 ) ( ) 88K (16 ) 136K (25 ) 105K (45 ) 162K (30 ) DSP 1751 (70 ) 8192(est.)\n\n--- Segment 25 ---\nmeans not reported or don t have the kind of resource. Work FQ-BERT [22] COSA [23] TransFRU [24] BETA [18] BAT [19] Our Work Platform ZCU102 XCVU13P Alveo U280 ZCU102 ZCU102 KV260 ZCU102 Network FQ-BERT BERT BERT BiT BiBERT BinaryBERT BMT COBRA Npe 16 COBRA Npe 32 LUT 123K (45 ) 1147K (66 ) 633K (49 ) 191K (70 ) 146K (53 ) 86K (74 ) 122K (45 ) FF 124K (23 ) 526K (15 ) ( ) 88K (16 ) 136K (25 ) 105K (45 ) 162K (30 ) DSP 1751 (70 ) 8192(est.) (67 ) 5016 (56 ) 64 (2.5 ) 1024 (41 ) 45 (3.6 ) 78 (3.1 ) BRAM 419 (46 ) 786 (29 ) ( ) 543 (60 ) 114 (12.5 ) 142.5 (98 ) 691.5 (76 ) URAM 0 (0 ) ( ) 51 (80 ) Our RBMM engine supports six operation modes as follows. We visualize the places of using each mode in the Transformer model in Fig. 1, and the hardware design in Fig. 5 and Fig. 6. M1. MHA Q K V Computation (l ÀÜ d ÀÜ d): The Q K V matrix feeds to Matrix A and the corresponding weight matrix serves as Matrix B. The HEAD PE activates the ACC PATH the threshold data width value is set to dh and the RBMM PE produces quantized binary outputs where output ƒõ bias. M2. MHA Attention Score (h l ÀÜ dh ÀÜ l): The Q serves as Matrix A, while the K serves as Matrix B. The h heads are processed simultaneously as they appear consecutively in the input datapacks. The HEAD PE enables the CONCAT PATH. Each head s result undergoes threshold comparison in SPS and attention mask application through iterative comparison of the current row and column indices. Note that the attention mask can support multiple mechanisms, including padding masks, causal masks, and others.\n\n--- Segment 26 ---\nEach head s result undergoes threshold comparison in SPS and attention mask application through iterative comparison of the current row and column indices. Note that the attention mask can support multiple mechanisms, including padding masks, causal masks, and others. This is achieved by checking whether the current loop index exceeds a value, which deter- mines whether the attention mask is applied to the subsequent elements. The threshold data width value is adjusted to T dh. The RBMM PE outputs the h-bit concatenated value and h updated DC HEADs for each head. M3. MHA Context Output (h l ÀÜ l ÀÜ dh): Matrix A is the attention score matrix of one head, and Matrix B is the transposed V l-bit datapacks. The lower l bits of the d bit are valid. The threshold data width value is modified to l{h, and the bias is added with l mod h. The HEAD PE activates the ACC PATH. The RBMM PE adds the DC INPUT because the attention score matrix is (0, 1) value. The output is quantized with bias, and the h head output addresses are arranged continuously to form an l ÀÜ d matrix. M4. MHA Linear Layer (l ÀÜ d ÀÜ d): This layer shares the configuration of mode M1 but produces integer output for the following LayerNorm operation instead of the quantized binary output value. F1. FFN Linear Layer I (l ÀÜ d ÀÜ FFsize) and F2. FFN Linear Layer II (l ÀÜ FFsize ÀÜ d): In the first FFN layer, the unsigned quantization bias is applied to represent the ReLU and binarization operation as described in Eq. (10). The DC FULL is updated to count the zeros after ReLU. The second FFN hidden layer involves the DC INPUT for summation and returns integer values accumulated with previous output values. The threshold data width values are set to dh. While these larger RBMMs seem to require a larger buffer of size lÀÜFFsize, the consecutive RBMMs are optimized as follows: E ReLUpX b Yq b Z ReLUpX b rY1, . . . , Yr, . . . , YRsq b rZ1, . . . , Zr, . . .\n\n--- Segment 27 ---\n. . , ZRsJ ReLUpX b Y1q b Z1 ReLUpX b YRq b ZR, (11) where X P tp 1, 1qulÀÜd, Y P tp 1, 1qudÀÜF Fsize, Z P tp 1, 1quF FsizeÀÜd and ReLU denotes the ReLU operation with unsigned binarization. This optimization requires only R iterations of ReLUpX b Yrq and ReLUpX b Yrq b Zr operations, each involving an l ÀÜ d ÀÜ d RBMM. In this way, only two buffers of size l ÀÜ d are required. Moreover, for the corner case of resource-limited platforms like KV260 that cannot support two buffers, the hardware design adheres to the original computational flow and utilizes off-chip DDR memory to store the additional R intermediate matrices. 5) On-chip and Off-chip Memory: Considering the ineffi- cient off-chip memory access throughputs of edge platforms, on-chip memory management requires careful optimization to meet hardware resource constraints. The binary-format Matrix A and Matrix B are buffered entirely in BRAM or URAM due to their relatively small size. The DC INPUT vector and quantization bias vector are also maintained in on- chip memory. In contrast, the weight and bias for different RBMM and transformer layers are fetched from off-chip memory and loaded to Matrix B and bias vector. However, the output matrix necessitates integer representation to support LayerNorm operations. Thus, a minimum of three Bo-bit matrices with dimensions lÀÜd are required to store the Q K V projection output matrices and support the two intermediate matrices in mode F1 and F2. These matrices are allocated to off-chip DDR memory or on-chip BRAM URAM based on available resources. For instance, with parameters l 512 and d 768 using Bo 13-bit integer representation, the KV260 platform can only accommodate one output matrix in on-chip memory, necessitating off-chip DDR allocation for the remaining matrices. This configuration leads to performance TABLE IV: Resource Breakdown for COBRA (Npe 32).\n\n--- Segment 28 ---\nFor instance, with parameters l 512 and d 768 using Bo 13-bit integer representation, the KV260 platform can only accommodate one output matrix in on-chip memory, necessitating off-chip DDR allocation for the remaining matrices. This configuration leads to performance TABLE IV: Resource Breakdown for COBRA (Npe 32). COBRA Npe 32 LUT FF DSP BRAM RBMM Engine 42926 (34.96 ) 62782 (38.66 ) 0 0 LayerNorm Unit 14055 (11.45 ) 13505 (8.32 ) 78 (100 ) 16 (2.31 ) Data Packing Conversion Units 27187 (22.14 ) 52707 (32.46 ) 0 0 On-chip Memory, AXI Bus and COBRA CTRL 38634 (31.46 ) 33399 (20.57 ) 0 675.5 (97.67 ) Total 122802 162393 78 691.5 Data Packing Conversion RBMM Engine Layer Norm COBRA Control AXI Interface On-Chip Memory Fig. 7: COBRA Accelerator Implementation Layout (ZCU102) degradation due to the inefficient DDR read write operations through the AXI master interface on edge platforms. In con- trast, the ZCU102, with its larger memory capacity, can buffer all three intermediate output matrices in on-chip memory, enabling optimal performance. IV. EXPERIMENTAL RESULTS AND ANALYSIS A. Experimental Setup We implement COBRA accelerator using high-level synthe- sis C C in Vitis HLS and Vivado 2023.1. We evaluate our design on AMD KV260 (Zynq UltraScale XCK26 MPSoC) and AMDZCU102 (Zynq UltraScale XCZU9EG MPSoC) boards. For Vitis HLS synthesis, we set the target frequency to 333MHz, achieving a maximum of 389 MHz on the KV260 and 339 MHz on the ZCU102. To ensure timing closure, we use 300 MHz for the final bitstream generation. We evaluate COBRA with BERT-base model: the maximum sequence length l 512, the hidden dimension d 768, the number of head h 12, the FFN hidden dimension FFsize 4 d 3072 and 12 transformer layers in total.\n\n--- Segment 29 ---\nTo ensure timing closure, we use 300 MHz for the final bitstream generation. We evaluate COBRA with BERT-base model: the maximum sequence length l 512, the hidden dimension d 768, the number of head h 12, the FFN hidden dimension FFsize 4 d 3072 and 12 transformer layers in total. B. COBRA Accelerator Evaluation and Comparison As shown in Table II, our FPGA implementation of CO- BRA achieves energy efficiency improvements of 150.7 and 144.4 on the KV260 platform compared to the BiT [13] and COBRA GPU baselines (PyTorch GPU), respectively. On the ZCU102 platform, it achieves energy efficiency im- provements of 325.1 and 311.6 , respectively. The resource utilization shown in Table III reveals that the configuration with Npe 16 on KV260 consumes the least LUT resources while maintaining energy efficiency comparable to BETA [18] TABLE V: Impact of Each Proposed Optimization. Design Throughput (GOPS) Power (W) Energy Efficiency (GOPS W) Hardware Resource LUT DSP COBRA Npe 32 3894.74 8.68 448.70 122K 78 w o SPS 6.91 (564ÀÜ) 13.41 ( 4.7) 0.52 (862.9ÀÜ) 158K ( 36K) 104 ( 26) w o 6:3 compressor based popcount 3873.91 (1.005ÀÜ) 12.18 ( 3.5) 318.06 (1.4ÀÜ) 134k ( 12K) 1614 ( 1536) w o pipeline for RBMM Engine 794.32 (4.9ÀÜ) 7.98 (-0.7) 99.54 (4.5ÀÜ) 125K ( 3K) 78 and BAT [19] implementations. These demonstrate the adapt- ability and efficiency of our hardware architecture on resource- constrained edge devices. The Npe 32 COBRA implementa- tion on mid-end ZCU102 delivers the best performance in both energy efficiency and throughput among all evaluated imple- mentations. Specifically, it achieves 3.5 throughput and 2.2 energy efficiency improvements, compared to BAT [19], the state-of-the-art binary transformer accelerator.\n\n--- Segment 30 ---\nThe Npe 32 COBRA implementa- tion on mid-end ZCU102 delivers the best performance in both energy efficiency and throughput among all evaluated imple- mentations. Specifically, it achieves 3.5 throughput and 2.2 energy efficiency improvements, compared to BAT [19], the state-of-the-art binary transformer accelerator. Furthermore, our design on the low-power ZCU102 platform achieves 2.5ÀÜ higher throughput than COSA [23] and TransFRU [24] designs on high-performance datacenter-grade FPGAs, while our de- sign consumes significantly lower power and has reduced LUT or DSP resource utilization compared to theirs. The resource breakdown of COBRA Npe 32 is shown in Table IV. Our RBMM Engine design uses 42,926 LUTs for efficient quantization-fused RBMM. The LayerNorm unit occupies 78 DSPs for computing the standard deviation and normalization. Notably, the data packing conversion units consume significant resources due to the LUT-based intermediate buffers in the transpose operation for V output matrix to datapacks (details in the M3. MHA Context Output section). Fig. 7 shows the implementation layout of the COBRA accelerator on the ZCU102 FPGA. Major components are highlighted and labeled in the figure. C. Ablation Study of COBRA Hardware Design We performed an ablation study on our proposed optimiza- tion techniques to better understand their impact on the overall accelerator performance. Table V shows the results. 1) SPS instead of Softmax: As discussed in Section III-A2, the Softmax hardware module is inefficient for hardware. In contrast, our proposed SPS-fused RBMM achieves a through- put improvement of 564ÀÜ compared to the original Softmax unit, which requires additional 36K LUTs and 26 DSPs for the computation of attention scores across h heads after RBMM. 2) 6:3 Compressor-Based Popcount instead of CPU- Optimized Popcount: The design of the Popcount unit is a critical consideration for the RBMM Engine, as it is replicated h times in each RBMM PE. The CPU-optimized popcount, as introduced in [25], leads to additional 12K LUTs and 1536 DSPs due to the inclusion of multipliers.\n\n--- Segment 31 ---\n2) 6:3 Compressor-Based Popcount instead of CPU- Optimized Popcount: The design of the Popcount unit is a critical consideration for the RBMM Engine, as it is replicated h times in each RBMM PE. The CPU-optimized popcount, as introduced in [25], leads to additional 12K LUTs and 1536 DSPs due to the inclusion of multipliers. 3) Pipeline Execution of RBMM Engine instead of Serial Execution: As outlined in Section III-B4, the components within the RBMM Engine are fully pipelined, and the engine s initiation interval (II) is one clock cycle. If pipeline scheduling is disabled, the RBMM Engine s execution after each call is serialized, resulting in only 20 of the throughput achieved by the pipelined version. V. CONCLUSION In this paper, we propose COBRA, a co-designed Binary Transformer Accelerator optimized for edge FPGAs. Our approach incorporates a hardware-friendly Shifted Polarized Softmax (SPS) design, a Real 1-bit Binary Matrix Multi- plication Engine (RBMM), and an adaptive computational flow, enabling us to fully harness the potential of binary transformer acceleration. COBRA achieves a 3.5 improve- ment in throughput and a 2.2 gain in energy efficiency with negligible accuracy degradation compared to state-of-the-art binary transformer accelerators. These on-board evaluation results highlight the performance of our design, enabling highly efficient edge inference of transformer models. REFERENCES [1] J. D. M.-W. C. Kenton and L. K. Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, in Proceedings of naacL-HLT, vol. 1. Minneapolis, Minnesota, 2019, p. 2. [2] D. Alexey, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv: 2010.11929, 2020. [3] H. Kaeley, Y. Qiao, and N. Bagherzadeh, Support for stock trend prediction using transformers and sentiment analysis, arXiv preprint arXiv:2305.14368, 2023.\n\n--- Segment 32 ---\n[2] D. Alexey, An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv: 2010.11929, 2020. [3] H. Kaeley, Y. Qiao, and N. Bagherzadeh, Support for stock trend prediction using transformers and sentiment analysis, arXiv preprint arXiv:2305.14368, 2023. [4] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, Progprompt: Generating situated robot task plans using large language models, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11 523 11 530. [5] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat, Q8bert: Quantized 8bit bert, in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 36 39. [6] W. Kwon, S. Kim, M. W. Mahoney, J. Hassoun, K. Keutzer, and A. Gholami, A fast post-training pruning framework for transform- ers, Advances in Neural Information Processing Systems, vol. 35, pp. 24 101 24 116, 2022. [7] R. Zhao, W. Song, W. Zhang, T. Xing, J.-H. Lin, M. Srivastava, R. Gupta, and Z. Zhang, Accelerating binarized convolutional neural networks with software-programmable fpgas, in Proceedings of the 2017 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA 17. New York, NY, USA: Association for Computing Machinery, 2017, p. 15 24. [Online].\n\n--- Segment 33 ---\nNew York, NY, USA: Association for Computing Machinery, 2017, p. 15 24. [Online]. Available: [8] Y. Zhang, J. Pan, X. Liu, H. Chen, D. Chen, and Z. Zhang, Fracbnn: Accurate and fpga-efficient binary neural networks with fractional activations, in The 2021 ACM SIGDA International Symposium on Field-Programmable Gate Arrays, ser. FPGA 21. New York, NY, USA: Association for Computing Machinery, 2021, p. 171 182. [Online]. Available: [9] Y. Qiao, M. Alnemari, and N. Bagherzadeh, A two-stage efficient 3- d cnn framework for eeg based emotion recognition, in 2022 IEEE International Conference on Industrial Technology (ICIT). IEEE, 2022, pp. 1 8. [10] A. Ding, Y. Qiao, and N. Bagherzadeh, Bnn an ideal architecture for acceleration with resistive in memory computation, IEEE Transactions on Emerging Topics in Computing, vol. 11, no. 2, pp. 281 291, 2023. [11] H. Bai, W. Zhang, L. Hou, L. Shang, J. Jin, X. Jiang, Q. Liu, M. Lyu, and I. King, BinaryBERT: Pushing the limit of bert quantization, arXiv preprint arXiv:2012.15701, 2020. [12] H. Qin, Y. Ding, M. Zhang, Q. Yan, A. Liu, Q. Dang, Z. Liu, and X. Liu, BiBERT: Accurate fully binarized BERT, arXiv preprint arXiv:2203.06390, 2022. [13] Z. Liu, B. Oguz, A. Pappu, L. Xiao, S. Yih, M. Li, R. Krishnamoorthi, and Y. Mehdad, BiT: Robustly binarized multi-distilled transformer, Advances in neural information processing systems, vol. 35, pp. 14 303 14 316, 2022.\n\n--- Segment 34 ---\n35, pp. 14 303 14 316, 2022. [14] W. Zhang, L. Hou, Y. Yin, L. Shang, X. Chen, X. Jiang, and Q. Liu, TernaryBERT: Distillation-aware ultra-low bit BERT, arXiv preprint arXiv:2009.12812, 2020. [15] R. Andri, L. Cavigelli, D. Rossi, and L. Benini, Yodann: An architecture for ultralow power binary-weight cnn acceleration, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 37, no. 1, pp. 48 60, 2017. [16] A. Al Bahou, G. Karunaratne, R. Andri, L. Cavigelli, and L. Benini, Xnorbin: A 95 top s w hardware accelerator for binary convolutional neural networks, in 2018 IEEE Symposium in Low-Power and High- Speed Chips (COOL CHIPS). IEEE, 2018, pp. 1 3. [17] M. Sun, H. Ma, G. Kang, Y. Jiang, T. Chen, X. Ma, Z. Wang, and Y. Wang, Vaqf: Fully automatic software-hardware co-design frame- work for low-bit vision transformer, arXiv preprint arXiv:2201.06618, 2022. [18] Y. Ji, C. Fang, and Z. Wang, BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge, arXiv preprint arXiv:2401.11851, 2024. [19] Y. Ji, C. Fang, S. Ma, H. Shao, and Z. Wang, Co-designing binarized transformer and hardware accelerator for efficient end-to-end edge deployment, arXiv preprint arXiv:2407.12070, 2024. [20] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R e, Flashattention: fast and memory-efficient exact attention with io-awareness, in Proceedings of the 36th International Conference on Neural Information Processing Systems, ser. NIPS 22. Red Hook, NY, USA: Curran Associates Inc., 2022.\n\n--- Segment 35 ---\nNIPS 22. Red Hook, NY, USA: Curran Associates Inc., 2022. [21] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding, 2019. [Online]. Available: [22] Z. Liu, G. Li, and J. Cheng, Hardware acceleration of fully quan- tized bert for efficient natural language processing, in 2021 Design, Automation Test in Europe Conference Exhibition (DATE), 2021, pp. 513 516. [23] Z. Wang, G. Wang, H. Jiang, N. Xu, and G. He, Cosa:co-operative systolic arrays for multi-head attention mechanism in neural network using hybrid data reuse and fusion methodologies, in 2023 60th ACM IEEE Design Automation Conference (DAC), 2023, pp. 1 6. [24] H. Wang, Y. Bai, J. Yu, and K. Wang, Transfru: Efficient deployment of transformers on fpga with full resource utilization, in 2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC), 2024, pp. 521 526. [25] Wikipedia, Hamming weight wikipedia, the free encyclopedia. [Online]. Available: weight\n\n