=== ORIGINAL PDF: 2503.17513v1_Improving_Quantization_with_Post-Training_Model_Ex.pdf ===\n\nRaw text length: 28946 characters\nCleaned text length: 28763 characters\nNumber of segments: 20\n\n=== CLEANED TEXT ===\n\narXiv:2503.17513v1 [cs.LG] 21 Mar 2025 IMPROVING QUANTIZATION WITH POST-TRAINING MODEL EXPANSION Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott AMD ABSTRACT The size of a model has been a strong predictor of its quality, as well as its cost. As such, the trade- off between model cost and quality has been well-studied. Post-training optimizations like quan- tization and pruning have typically focused on reducing the overall volume of pre-trained models to reduce inference costs while maintaining model quality. However, recent advancements have in- troduced optimization techniques that, interestingly, expand models post-training, increasing model size to improve quality when reducing volume. For instance, to enable 4-bit weight and activation quantization, incoherence processing often necessitates inserting online Hadamard rotations in the compute graph, and preserving highly sensitive weights often calls for additional higher precision computations. However, if application requirements cannot be met, the prevailing solution is to re- lax quantization constraints. In contrast, we demonstrate post-training model expansion is a viable strategy to improve model quality within a quantization co-design space, and provide theoretical justiﬁcation. We show it is possible to progressively and selectively expand the size of a pre-trained large language model (LLM) to improve model quality without end-to-end retraining. In particular, when quantizing the weights and activations to 4 bits for Llama3 1B, we reduce the zero-shot accu- racy gap to full precision by an average of 3 relative to both QuaRot and SpinQuant with only 5 more parameters, which is still a 3.8 reduction in volume relative to a BF16 reference model. 1 Introduction Quantization plays a critical role in the deployment of large language models (LLMs), offering a means of bridging the gap between full-precision reference models and their low-precision counterparts. The primary objective of quantiza- tion is to minimize inference costs while maintaining model quality by reducing the bit width requirements of weights and activations. To this end, recent advancements in post-training quantization (PTQ) have enabled the practical use of 4-bit weights and activations during inference, effectively addressing challenges that are particularly pronounced in LLMs, such as mitigating the impact of outliers [1, 2] and managing non-uniform error sensitivity [3, 4]. Traditionally, the quantization design space has been navigated with a ﬁxed set of strategies; namely, the selection of bit width and data format [5, 6, 7], the addition or removal of zero-points [8, 9], and the granularity or optimization of scaling factors [10, 11]. Interestingly, parameter volume (i.e., model size bit width) is commonly viewed in one dimension; volume is primarily reduced via bit width reductions, and model size is rarely considered outside of scaling analyses [12, 13, 14]. However, in real-world applications where maintaining model quality is paramount, a modest model size increase of 5-10 is often an acceptable trade-off. For example, when deploying on hardware accelerators restricted to power-of-2 bit widths (e.g., GPUs), failing to meet an accuracy requirement at 4 bits necessitates reverting to 8-bit precision, a signiﬁcant step size increase in parameter volume. While increasing model size typically requires either ﬂexible supernetworks [15] or end-to-end re-training [14], we present post-training model expansion as an emerging strategy to reduce the gap between full-precision reference models and their quantized counterparts. Our proposal aligns with existing literature, which consistently demonstrates that increasing model size is generally an effective strategy for improving accuracy [16, 17, 18]. Moreover, a new trend is emerging; as further discussed in Section 2, recent studies have already introduced post-training quantization techniques that incidentally increase model size while still decreasing parameter volume. We argue that post-training model expansion offers a promising path to balancing the trade-off between model cost and quality. We propose a simple method to control model size expansion rates via online Hadamard transformations with theoretical justiﬁcation (see Section 3), and show that it can further improve the latest state-of-the-art weight-activation quantization algorithms, namely, QuaRot [1] and SpinQuant [2] (see Section 4). To facilitate further research and application, we have open-sourced our implementations in the Brevitas quantization library [19], complete with detailed instructions for reproduction. 2 Related Work GPTQ [12] is the most popular and often the most effective PTQ algorithm for LLM quantization, especially when considering the act-order trick [20], where columns are quantized in descending order of the diagonal of the Hessian. Recent techniques, such as QuaRot [1], FrameQuant [21], and QuIP [22, 23], heavily rely on incoherence processing, where orthogonal rotations are introduced to reduce outliers in both weights and activations. Unlike GPTQ, which alters weight values through iterative error correction, incoherence processing comes at the cost of adding online matrix multiplications to the compute graph (usually Hadamard matrices). Liu et al. [2] extend incoherence processing to gradient-based PTQ, showing one can optimize rotation matrices via Cayley transforms [24], which preserves the orthogonality of merged rotations at no extra inference cost compared to QuaRot but with additional calibration costs. Both QuaRot and SpinQuant start from the assumption that LLM quantization is hindered by the presence of outliers, a phenomenon studied in prior works [4]. In particular, Yu et al. [4] empirically identify weights that are extremely sensitive to quantization, and propose to keep them in higher precision; interestingly, these outliers can be linked to the activations outliers. Preserving high-precision weights limits the impact of quantization on model accuracy at the cost of high-precision matrix multiplications. Finally, Zhao et al. [3], although not applied in the LLM context, showed that it is possible to split the most difﬁcult channels to quantize in a layer, thus removing potential outliers and making quantization easier at the cost of slightly larger matrix multiplications. Each of these works incidentally increase model size to improve model quality while reducing model volume. We refer to this trend as post-training model expansion and propose a simple mechanism inspired by the intersection of these works. 3 Post-Training Model Expansion via Online Incoherence Processing Incoherence processing is known to reduce the negative impact of outliers when quantizing weights and activations to low-precision datatypes, even when using per-channel and per-token scaling factor granularities, respectively [1, 2]. These techniques exploit the following rotation invariance property in linear layers XT W (RX)T (RW ) XT R 1RW XT W , where, for inputs X and weights W , orthogonal rotation matrix R effectively rotates the latent space of an LLM before quantization, amortizing outliers across the rotated space before quantizing RX and RW . Note that, for all orthogonal matrices, its inverse can be computed through transposition. When compared to standard orthogonal matrix multiplications, Hadamard matrix multiplications are orders of magnitude faster, owing to their recursive structure and strictly bipolar values (all values are either { γ, γ}, for γ R). When used as rotations to exploit this invariance, ran- dom Hadamard matrices are known to yield more consistent improvements in compression quality when compared to rotations with random orthogonal matrices, as shown in [2]. These beneﬁts have led to the rapid adoption of Hadamard matrices in LLM quantization and motivated the development of optimized libraries such as Hadacore [25]. Further- more, many rotation matrices can be merged into the linear layers around the activation quantizer, as exploited by [1] and [2]; however, this is not always possible because of non-linearities in the network. Thus, in some circumstances, the rotation matrix is unavoidably left in the compute graph and Q(RX)T Q(W ) is computed during inference, where Q is a known quantization function and W RW . We refer to this as online incoherence processing. We propose to extend the work of [1] and [2] via expanding the Hadamard matrices used during online incoherence processing, incidentally increasing model size post-training. Let X be a I D matrix of D input samples of I dimensions. Similarly, let the I O weight matrix W have I input channels and O output channels. Our proposal is to ﬁrst generate an expanded M M Hadamard matrix H where M I, and then select only the ﬁrst I output channels, yielding a M I matrix H, where γ 1 M. The left inverse of H can still be computed through transposition, allowing for fast matrix computations. By combining H with the weights, we obtain W HW as a M O matrix, effectively increasing the number of input channels from I to M; similarly, after the expanded Hadamard multiplication, the expanded input activation X HX is a M D matrix. Since W and X are linear combinations of the original W and X, respectively, this also effectively increases their bit rate during quantization. Interestingly, post-training model expansion incidentally expands the nullspace of the input activations, granting more opportunity to effectively hide quantization error during calibration, as supported by the following proposition. Proposition 3.1. Given that H RM I is full-column rank, then the expansion of X RI D to X HX RM D, where M I, increases the dimensionality of the nullspace of the input activations such that nullity( XT ) nullity(XT ) Proof. Recall that our expansion matrix H is M I where M I. For input activations X RI D, it follows that the expanded input activations X HX RM D. By the rank-nullity theorem, it follows that I rank(XT ) nullity(XT ), M rank( XT ) nullity( XT ) 2 By construction, H is full-column rank. It then follows that rank( X) rank( HX) rank(X), and therefore nullity( XT ) nullity(XT ) M I 0 Thus, there is a strictly positive increase in the dimensionality of the nullspace, concluding the proof. As our goal is to ﬁnd a quantization mapping Q such that XT W XT Q(W ) or, equivalently, that w Q(w) nullspace(XT ) (for every column w in W ), one could expect increasing the nullspace of the input feature space to help reduce the quantization error. In other words, though rotation does not alter the intrinsic dimensionality of the data, the higher redundancy in the projected space could be exploited by a carefully designed quantizer. The placement of these expanded Hadamard matrices is crucial to avoid an excessive increase in computational com- plexity without substantial beneﬁt in model quality. For this reason, we solely focus on the down projection layer (i.e., the last layer in the feed-forward block). Our reasoning is two-fold: (1) as shown in [4], this is one of the most sensitive layers to quantize, and (2) when following the placement of Hadamard matrices in [2], the down projection layer is naturally left with an online Hadamard matrix multiplication, which allows for ﬁne-grained control of the expansion process. Expanding other layers, like the QKV layers, would require either inserting more online Hadamard matrices or expanding all the layers connected by the same rotation matrix. While it is possible to add more online Hadamard matrices, and thus expand other layers than the down projection layer, this would come at the cost of even further increased model size and computational complexity due to the extra matrix multiplications. 4 Results Models Datasets. We evaluate our post-training model expansion technique on the Llama 3 family of models [26], focusing in particular on Llama 3.2 1B and 3B, and Llama 3.1 8B. We use WikiText2 [27] as the dataset for calibra- tion and evaluation with 128 training samples used for calibration and a sequence length of 2048 tokens, a common conﬁguration used in prior works [2, 12, 13]. For zero-shot evaluation, we considered the following tasks: ARC-easy (ARC-E) and ARC-challenge (ARC-C) [28], HellaSwag (HS) [29], PIQA [30], and Winogrande (Wino) [31]. We report the unnormalized results for all tests using Huggingface s LightEval library [32]. Quantization Details. We follow the same 4-bit weight and activation quantization paradigm proposed by Liu et. al [2]; in particular, we use per-channel symmetric weight quantization, per-token asymmetric activation quantization, and place Hadamard matrices in the same places in the compute graph, merging matrices in the linear layers when possible and having only one online Hadamard rotation for the down projection layer (see Section 3). However, rather than using random Hadamard matrices, we use a deterministic Hadamard matrix. We do not quantize the KV cache as it is does not directly impact expansion. We applied only GPTQ [12] for weight quantization and quantize weight columns in descending order of the diagonal of the Hessian (i.e., the act-order trick [20]) for better accuracy. Experiment Details. We perform two main sets of experiments, denoted as QuaRot and SpinQuant , which respec- tively re-implement the proposals from [1] and [2]. When evaluating Cayley optimization in SpinQuant , we used 800 training samples and the same training hyperparameters reported in [2]. All algorithms are implemented in Bre- vitas [19], and we provide instructions on how to replicate our results. For each of our experiments, we report the model sizes for the quantized variants and the ratio of expansion compared to the base quantized version. These are computed by counting the total number of parameters in the network. We exclude the last linear layer from this count since its parameters are shared with the embedding layers. Similarly, we report the number of input channels for the down projection layers, and their expansion ratios. The ratio 1.0 refers to the baseline quantization conﬁguration (i.e., QuaRot or SpinQuant ), with no online Hadamard expansion. Main Results. In Table 1, we can see that slightly expanding the down projection layers with QuaRot , and thus slightly increasing the model size, improves both perplexity and zero-shot performance, although there are quickly diminishing returns in perplexity improvements. In these experiments, optimization was performed using bfloat16 as the base datatype. Next, we show that combining Hadamard expansions with Cayley optimization for orthogonal rotation matrices can further improve these results. In these experiments, optimization was performed using float32 as the base datatype, instead of bfloat16. Results are provided in Table 2; we do not report the performance of the ﬂoat model, or the model size, which are identical to Table 1. As expected, Cayley-optimized rotations improves on the base Hadamard rotations. Furthermore, Cayley-optimized expanded rotations (i.e., SpinQuant ) consistently provides beneﬁts over QuaRot , as reported in [2], and narrows the gap between the quantized model and its high-precision counterpart. We again can see that larger expansions correlate with larger reductions in perplexity, with diminishing returns on zero-shot accuracy; however, more results may be needed to draw a conclusion. 3 Table 1: Expanding QuaRot for Llama3 models. We compare different post-training expansion rates for perplexity (lower is better) and several zero-shot tasks (higher is better). We also report the overall model size of the different quantized conﬁgurations and the expansion ratio compared to baseline quantized. Similarly, we report the input channels for the down projection layers and the expansion ratio. The model size includes quantization parameters. Model Algorithm Model Size (Ratio) Layer size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B Float - - 8.75 31.91 66.45 48.15 60.06 75.46 QuaRot 1.235B (1.00) 8192 (1.0) 14.00 25.25 55.68 40.78 53.51 66.15 1.294B (1.05) 8960 (1.1) 13.56 27.99 57.62 41.09 53.67 69.26 1.328B (1.08) 11008 (1.3) 13.56 27.82 54.92 40.69 55.64 66.81 3B Float - - 7.00 42.40 74.87 55.83 68.43 76.70 QuaRot 3.212B (1.00) 8192 (1.0) 9.63 36.09 68.18 48.46 58.56 71.43 3.367B (1.05) 9984 (1.2) 9.50 35.06 67.84 49.07 59.58 72.36 3.455B (1.08) 11008 (1.3) 9.50 36.00 69.02 48.74 59.19 72.41 8B Float - - 5.56 51.79 82.20 60.73 70.87 79.11 QuaRot 7.504B (1.00) 8192 (1.0) 7.75 43.60 75.80 53.53 63.20 75.40 8.041B (1.07) 18432 (1.3) 7.50 43.77 75.93 54.75 63.22 76.88 8.512B (1.13) 22016 (1.5) 7.50 43.17 76.85 54.20 64.33 74.46 Table 2: Expanding SpinQuant for Llama 3.2 1B. We evaluate the impact of different expansion factors on per- plexity (lower is better) and several zero-shot tasks (higher is better). Model Algorithm Layer Size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B SpinQuant 8192 (1.0) 13.30 27.13 54.92 40.58 53.20 67.41 8960 (1.1) 12.94 27.65 56.86 41.36 55.96 68.66 11008 (1.3) 12.71 26.54 57.07 41.48 55.33 67.35 Overﬁtting Hypothesis. We hypothesize that data-dependent algorithms, like GPTQ and Cayley optimization, can lead to overﬁtting on WikiText2. We perform two tests to investigate this hypothesis. First, we apply QuaRot without GPTQ and observe improved results in terms of perplexity and zero-shot accuracy when expanding the down projection layer by 1.3 relative to expansion by 1.1 , as shown in Table 3. Although GPTQ still improves performance in general, the interaction with expansion and GPTQ suggests overﬁtting. Second, we perform Cayley optimization with the learning rate reduced by 10 . As shown in Table 4, the average zero-shot performance improves, even though the perplexity results are considerably worse, supporting our overﬁtting theory. More results may be needed to draw a conclusion, which we leave for future work. The Trade-off Between Model Quality and Model Volume. Quantization often focuses on the amount of model degradation compared to some full-precision baseline. When quantization is applied to multiple models designed for the same problem (e.g., multiple LLMs trained on the same data), the effect of quantization can be placed into a wider context. For example, if there is some constraint at inference time, such as model volume, we can ask: "what is the most accurate model which ﬁts my requirements?" Using model volume as an example constraint, Figures 1 and 2 show the trade-off that can be made between volume and WikiText2 perplexity or average zero-shot evaluation, respectively. The ﬁgures show the results1 from Tables 1, 2 and 4, the different clusters in the ﬁgures correspond to the different models sizes; Llama 3.2 1B, 3.2 3B and 3.1 8B. For a given memory constraint (measured as model volume), the Pareto frontier provides the optimal model within our co-design space. Notably, SpinQuant results outperform QuaRot results for equivalent models (i.e., Llama 3.2 1B), otherwise, almost all quantized and ﬂoat models lie on the Pareto frontier. Interestingly, for the zero-shot results, the Llama 3.2 1B bfloat16 results are completely Pareto-dominated by the INT4 Llama 3.2 3B results. Finally, since all experiments quantize both weights and activations, we expect the trade- off between model volume and model quality would signiﬁcantly lean further towards quantized models if weight-only quantization were considered. 1Only the best results of Tables 2 and 4 were used for the SpinQuant results. 4 Table 3: Expanding QuaRot for Llama 3.2 1B without any data-dependent PTQ. We evaluate the impact of removing data-dependent PTQ algorithms on perplexity (lower is better) and several zero-shot tasks (higher is better). Model Algorithm Layer Size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B QuaRot 8960 (1.1) 19.75 22.87 46.71 36.51 51.14 64.96 11008 (1.3) 18.63 23.72 47.64 37.72 51.30 66.70 Table 4: Expanding SpinQuant for Llama 3.2 1B with reduced learning rate. We evaluate the impact of different expansion factors on perplexity (lower is better) and several zero-shot tasks (higher is better). The learning rate is reduced by a factor of 10 from the original SpinQuant proposal [2]. Model Algorithm Layer Size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B SpinQuant 8192 (1.0) 13.34 28.58 56.65 40.81 54.38 66.70 8960 (1.1) 13.35 27.73 56.77 40.83 54.61 67.63 11008 (1.3) 12.92 29.10 57.07 42.17 55.64 68.06 5 Conclusion Post-training model expansion broadens the quantization landscape. When accuracy requirements cannot be met, the most common strategy is to relax quantization constraints, for example going from per-tensor to per-channel (or even per-group) scaling or increasing bit widths. However, compilers and accelerators are often designed and optimized to work best with a speciﬁc set of datatypes. Therefore, as an alternative strategy, we demonstrate modest increases in model size post-training can improve the accuracy of quantized models within a ﬁxed quantization co-design space. Our simple post-training model expansion trick gives more ﬂexibility to deep learning engineers deploying LLMs, and our investigations open up promising new research questions around the potential for data-dependent quantization algorithms to overﬁt, and the general performance plateau of data-free quantization. Furthermore, while we focused our investigation on the expansion of all down projection layers in a model, one could precisely target speciﬁc layers based on their sensitivity to quantization; we leave such a study for future work. Finally, although Cayley optimization seems to consistently work well with Hadamard expansion, we plan to investigate the variability in zero-shot results that we observed when testing for different model expansion rates. 0 2 4 6 8 10 12 14 16 6 8 10 12 14 Model Volume (GB) Perplexity Pareto Frontier BF16 INT4 (QuaRot ) INT4 (SpinQuant ) Figure 1: Comparison between Wikitext2 per- plexity scores and model volume in GB. 0 2 4 6 8 10 12 14 16 45 50 55 60 65 70 Model Volume (GB) Avg. 0-Shot Accuracy ( ) Pareto Frontier BF16 INT4 (QuaRot ) INT4 (SpinQuant ) Figure 2: Comparison between geometric mean of 0-shot evaluation and model volume in GB. 5 References [1] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoe- ﬂer, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. [2] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. SpinQuant: LLM quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. [3] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543 7552. PMLR, 2019. [4] Mengxia Yu, De Wang, Qi Shan, and Alvin Wan. The super weight in large language models. arXiv preprint arXiv:2411.07191, 2024. [5] Mart Van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. Advances in neural information processing systems, 33:5741 5752, 2020. [6] Jorn Peters, Marios Fournarakis, Markus Nagel, Mart Van Baalen, and Tijmen Blankevoort. Qbitopt: Fast and accurate bitwidth reallocation during training. In Proceedings of the IEEE CVF international conference on computer vision, pages 1282 1291, 2023. [7] Mart Van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, et al. FP8 versus INT8 for efﬁcient deep learning inference. arXiv preprint arXiv:2303.17951, 2023. [8] Xinyu Zhang, Ian Colbert, and Srinjoy Das. Learning low-precision structured subnetworks using joint layerwise channel pruning and uniform quantization. Applied Sciences, 12(15):7829, 2022. [9] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efﬁcient neural network inference. In Low-power computer vision, pages 291 326. Chapman and Hall CRC, 2022. [10] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. [11] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. [12] Elias Frantar, Saleh Ashkboos, Torsten Hoeﬂer, and Dan Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [13] Ian Colbert, Fabian Grob, Giuseppe Franco, Jinjie Zhang, and Rayan Saab. Accumulator-aware post-training quantization. arXiv preprint arXiv:2409.17092, 2024. [14] Zechun Liu, Changsheng Zhao, Hanxian Huang, Sijia Chen, Jing Zhang, Jiawei Zhao, Scott Roy, Lisa Jin, Yunyang Xiong, Yangyang Shi, et al. ParetoQ: Scaling laws in extremely low-bit llm quantization. arXiv preprint arXiv:2502.02631, 2025. [15] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efﬁcient deployment. arXiv preprint arXiv:1908.09791, 2019. [16] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. [17] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019. [18] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [19] Giuseppe Franco, Alessandro Pappalardo, and Nicholas J Fraser. Xilinx brevitas, 2025. [20] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87 100, 2024. 6 [21] Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, and Vikas Singh. Framequant: Flexible low-bit quantization for transformers. arXiv preprint arXiv:2403.06082, 2024. [22] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. QuIP: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024. [23] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. QuIP : Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. [24] Jun Li, Li Fuxin, and Sinisa Todorovic. Efﬁcient riemannian optimization on the stiefel manifold via the cayley transform. arXiv preprint arXiv:2002.01113, 2020. [25] Hadacore: Tensor core accelerated hadamard transform kernel. [26] Aaron Grattaﬁori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [28] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [29] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019. [30] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 34, pages 7432 7439, 2020. [31] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial wino- grad schema challenge at scale. Communications of the ACM, 64(9):99 106, 2021. [32] Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. LightEval: A lightweight framework for llm evaluation, 2023. 7\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\narXiv:2503.17513v1 [cs.LG] 21 Mar 2025 IMPROVING QUANTIZATION WITH POST-TRAINING MODEL EXPANSION Giuseppe Franco, Pablo Monteagudo-Lago, Ian Colbert, Nicholas Fraser, Michaela Blott AMD ABSTRACT The size of a model has been a strong predictor of its quality, as well as its cost. As such, the trade- off between model cost and quality has been well-studied. Post-training optimizations like quan- tization and pruning have typically focused on reducing the overall volume of pre-trained models to reduce inference costs while maintaining model quality. However, recent advancements have in- troduced optimization techniques that, interestingly, expand models post-training, increasing model size to improve quality when reducing volume. For instance, to enable 4-bit weight and activation quantization, incoherence processing often necessitates inserting online Hadamard rotations in the compute graph, and preserving highly sensitive weights often calls for additional higher precision computations. However, if application requirements cannot be met, the prevailing solution is to re- lax quantization constraints. In contrast, we demonstrate post-training model expansion is a viable strategy to improve model quality within a quantization co-design space, and provide theoretical justiﬁcation. We show it is possible to progressively and selectively expand the size of a pre-trained large language model (LLM) to improve model quality without end-to-end retraining. In particular, when quantizing the weights and activations to 4 bits for Llama3 1B, we reduce the zero-shot accu- racy gap to full precision by an average of 3 relative to both QuaRot and SpinQuant with only 5 more parameters, which is still a 3.8 reduction in volume relative to a BF16 reference model. 1 Introduction Quantization plays a critical role in the deployment of large language models (LLMs), offering a means of bridging the gap between full-precision reference models and their low-precision counterparts. The primary objective of quantiza- tion is to minimize inference costs while maintaining model quality by reducing the bit width requirements of weights and activations.\n\n--- Segment 2 ---\n1 Introduction Quantization plays a critical role in the deployment of large language models (LLMs), offering a means of bridging the gap between full-precision reference models and their low-precision counterparts. The primary objective of quantiza- tion is to minimize inference costs while maintaining model quality by reducing the bit width requirements of weights and activations. To this end, recent advancements in post-training quantization (PTQ) have enabled the practical use of 4-bit weights and activations during inference, effectively addressing challenges that are particularly pronounced in LLMs, such as mitigating the impact of outliers [1, 2] and managing non-uniform error sensitivity [3, 4]. Traditionally, the quantization design space has been navigated with a ﬁxed set of strategies; namely, the selection of bit width and data format [5, 6, 7], the addition or removal of zero-points [8, 9], and the granularity or optimization of scaling factors [10, 11]. Interestingly, parameter volume (i.e., model size bit width) is commonly viewed in one dimension; volume is primarily reduced via bit width reductions, and model size is rarely considered outside of scaling analyses [12, 13, 14]. However, in real-world applications where maintaining model quality is paramount, a modest model size increase of 5-10 is often an acceptable trade-off. For example, when deploying on hardware accelerators restricted to power-of-2 bit widths (e.g., GPUs), failing to meet an accuracy requirement at 4 bits necessitates reverting to 8-bit precision, a signiﬁcant step size increase in parameter volume. While increasing model size typically requires either ﬂexible supernetworks [15] or end-to-end re-training [14], we present post-training model expansion as an emerging strategy to reduce the gap between full-precision reference models and their quantized counterparts. Our proposal aligns with existing literature, which consistently demonstrates that increasing model size is generally an effective strategy for improving accuracy [16, 17, 18]. Moreover, a new trend is emerging; as further discussed in Section 2, recent studies have already introduced post-training quantization techniques that incidentally increase model size while still decreasing parameter volume. We argue that post-training model expansion offers a promising path to balancing the trade-off between model cost and quality.\n\n--- Segment 3 ---\nMoreover, a new trend is emerging; as further discussed in Section 2, recent studies have already introduced post-training quantization techniques that incidentally increase model size while still decreasing parameter volume. We argue that post-training model expansion offers a promising path to balancing the trade-off between model cost and quality. We propose a simple method to control model size expansion rates via online Hadamard transformations with theoretical justiﬁcation (see Section 3), and show that it can further improve the latest state-of-the-art weight-activation quantization algorithms, namely, QuaRot [1] and SpinQuant [2] (see Section 4). To facilitate further research and application, we have open-sourced our implementations in the Brevitas quantization library [19], complete with detailed instructions for reproduction. 2 Related Work GPTQ [12] is the most popular and often the most effective PTQ algorithm for LLM quantization, especially when considering the act-order trick [20], where columns are quantized in descending order of the diagonal of the Hessian. Recent techniques, such as QuaRot [1], FrameQuant [21], and QuIP [22, 23], heavily rely on incoherence processing, where orthogonal rotations are introduced to reduce outliers in both weights and activations. Unlike GPTQ, which alters weight values through iterative error correction, incoherence processing comes at the cost of adding online matrix multiplications to the compute graph (usually Hadamard matrices). Liu et al. [2] extend incoherence processing to gradient-based PTQ, showing one can optimize rotation matrices via Cayley transforms [24], which preserves the orthogonality of merged rotations at no extra inference cost compared to QuaRot but with additional calibration costs. Both QuaRot and SpinQuant start from the assumption that LLM quantization is hindered by the presence of outliers, a phenomenon studied in prior works [4]. In particular, Yu et al. [4] empirically identify weights that are extremely sensitive to quantization, and propose to keep them in higher precision; interestingly, these outliers can be linked to the activations outliers. Preserving high-precision weights limits the impact of quantization on model accuracy at the cost of high-precision matrix multiplications. Finally, Zhao et al.\n\n--- Segment 4 ---\nPreserving high-precision weights limits the impact of quantization on model accuracy at the cost of high-precision matrix multiplications. Finally, Zhao et al. [3], although not applied in the LLM context, showed that it is possible to split the most difﬁcult channels to quantize in a layer, thus removing potential outliers and making quantization easier at the cost of slightly larger matrix multiplications. Each of these works incidentally increase model size to improve model quality while reducing model volume. We refer to this trend as post-training model expansion and propose a simple mechanism inspired by the intersection of these works. 3 Post-Training Model Expansion via Online Incoherence Processing Incoherence processing is known to reduce the negative impact of outliers when quantizing weights and activations to low-precision datatypes, even when using per-channel and per-token scaling factor granularities, respectively [1, 2]. These techniques exploit the following rotation invariance property in linear layers XT W (RX)T (RW ) XT R 1RW XT W , where, for inputs X and weights W , orthogonal rotation matrix R effectively rotates the latent space of an LLM before quantization, amortizing outliers across the rotated space before quantizing RX and RW . Note that, for all orthogonal matrices, its inverse can be computed through transposition. When compared to standard orthogonal matrix multiplications, Hadamard matrix multiplications are orders of magnitude faster, owing to their recursive structure and strictly bipolar values (all values are either { γ, γ}, for γ R). When used as rotations to exploit this invariance, ran- dom Hadamard matrices are known to yield more consistent improvements in compression quality when compared to rotations with random orthogonal matrices, as shown in [2]. These beneﬁts have led to the rapid adoption of Hadamard matrices in LLM quantization and motivated the development of optimized libraries such as Hadacore [25]. Further- more, many rotation matrices can be merged into the linear layers around the activation quantizer, as exploited by [1] and [2]; however, this is not always possible because of non-linearities in the network.\n\n--- Segment 5 ---\nThese beneﬁts have led to the rapid adoption of Hadamard matrices in LLM quantization and motivated the development of optimized libraries such as Hadacore [25]. Further- more, many rotation matrices can be merged into the linear layers around the activation quantizer, as exploited by [1] and [2]; however, this is not always possible because of non-linearities in the network. Thus, in some circumstances, the rotation matrix is unavoidably left in the compute graph and Q(RX)T Q(W ) is computed during inference, where Q is a known quantization function and W RW . We refer to this as online incoherence processing. We propose to extend the work of [1] and [2] via expanding the Hadamard matrices used during online incoherence processing, incidentally increasing model size post-training. Let X be a I D matrix of D input samples of I dimensions. Similarly, let the I O weight matrix W have I input channels and O output channels. Our proposal is to ﬁrst generate an expanded M M Hadamard matrix H where M I, and then select only the ﬁrst I output channels, yielding a M I matrix H, where γ 1 M. The left inverse of H can still be computed through transposition, allowing for fast matrix computations. By combining H with the weights, we obtain W HW as a M O matrix, effectively increasing the number of input channels from I to M; similarly, after the expanded Hadamard multiplication, the expanded input activation X HX is a M D matrix. Since W and X are linear combinations of the original W and X, respectively, this also effectively increases their bit rate during quantization. Interestingly, post-training model expansion incidentally expands the nullspace of the input activations, granting more opportunity to effectively hide quantization error during calibration, as supported by the following proposition. Proposition 3.1. Given that H RM I is full-column rank, then the expansion of X RI D to X HX RM D, where M I, increases the dimensionality of the nullspace of the input activations such that nullity( XT ) nullity(XT ) Proof. Recall that our expansion matrix H is M I where M I.\n\n--- Segment 6 ---\nGiven that H RM I is full-column rank, then the expansion of X RI D to X HX RM D, where M I, increases the dimensionality of the nullspace of the input activations such that nullity( XT ) nullity(XT ) Proof. Recall that our expansion matrix H is M I where M I. For input activations X RI D, it follows that the expanded input activations X HX RM D. By the rank-nullity theorem, it follows that I rank(XT ) nullity(XT ), M rank( XT ) nullity( XT ) 2 By construction, H is full-column rank. It then follows that rank( X) rank( HX) rank(X), and therefore nullity( XT ) nullity(XT ) M I 0 Thus, there is a strictly positive increase in the dimensionality of the nullspace, concluding the proof. As our goal is to ﬁnd a quantization mapping Q such that XT W XT Q(W ) or, equivalently, that w Q(w) nullspace(XT ) (for every column w in W ), one could expect increasing the nullspace of the input feature space to help reduce the quantization error. In other words, though rotation does not alter the intrinsic dimensionality of the data, the higher redundancy in the projected space could be exploited by a carefully designed quantizer. The placement of these expanded Hadamard matrices is crucial to avoid an excessive increase in computational com- plexity without substantial beneﬁt in model quality. For this reason, we solely focus on the down projection layer (i.e., the last layer in the feed-forward block). Our reasoning is two-fold: (1) as shown in [4], this is one of the most sensitive layers to quantize, and (2) when following the placement of Hadamard matrices in [2], the down projection layer is naturally left with an online Hadamard matrix multiplication, which allows for ﬁne-grained control of the expansion process. Expanding other layers, like the QKV layers, would require either inserting more online Hadamard matrices or expanding all the layers connected by the same rotation matrix.\n\n--- Segment 7 ---\nOur reasoning is two-fold: (1) as shown in [4], this is one of the most sensitive layers to quantize, and (2) when following the placement of Hadamard matrices in [2], the down projection layer is naturally left with an online Hadamard matrix multiplication, which allows for ﬁne-grained control of the expansion process. Expanding other layers, like the QKV layers, would require either inserting more online Hadamard matrices or expanding all the layers connected by the same rotation matrix. While it is possible to add more online Hadamard matrices, and thus expand other layers than the down projection layer, this would come at the cost of even further increased model size and computational complexity due to the extra matrix multiplications. 4 Results Models Datasets. We evaluate our post-training model expansion technique on the Llama 3 family of models [26], focusing in particular on Llama 3.2 1B and 3B, and Llama 3.1 8B. We use WikiText2 [27] as the dataset for calibra- tion and evaluation with 128 training samples used for calibration and a sequence length of 2048 tokens, a common conﬁguration used in prior works [2, 12, 13]. For zero-shot evaluation, we considered the following tasks: ARC-easy (ARC-E) and ARC-challenge (ARC-C) [28], HellaSwag (HS) [29], PIQA [30], and Winogrande (Wino) [31]. We report the unnormalized results for all tests using Huggingface s LightEval library [32]. Quantization Details. We follow the same 4-bit weight and activation quantization paradigm proposed by Liu et. al [2]; in particular, we use per-channel symmetric weight quantization, per-token asymmetric activation quantization, and place Hadamard matrices in the same places in the compute graph, merging matrices in the linear layers when possible and having only one online Hadamard rotation for the down projection layer (see Section 3). However, rather than using random Hadamard matrices, we use a deterministic Hadamard matrix. We do not quantize the KV cache as it is does not directly impact expansion.\n\n--- Segment 8 ---\nHowever, rather than using random Hadamard matrices, we use a deterministic Hadamard matrix. We do not quantize the KV cache as it is does not directly impact expansion. We applied only GPTQ [12] for weight quantization and quantize weight columns in descending order of the diagonal of the Hessian (i.e., the act-order trick [20]) for better accuracy. Experiment Details. We perform two main sets of experiments, denoted as QuaRot and SpinQuant , which respec- tively re-implement the proposals from [1] and [2]. When evaluating Cayley optimization in SpinQuant , we used 800 training samples and the same training hyperparameters reported in [2]. All algorithms are implemented in Bre- vitas [19], and we provide instructions on how to replicate our results. For each of our experiments, we report the model sizes for the quantized variants and the ratio of expansion compared to the base quantized version. These are computed by counting the total number of parameters in the network. We exclude the last linear layer from this count since its parameters are shared with the embedding layers. Similarly, we report the number of input channels for the down projection layers, and their expansion ratios. The ratio 1.0 refers to the baseline quantization conﬁguration (i.e., QuaRot or SpinQuant ), with no online Hadamard expansion. Main Results. In Table 1, we can see that slightly expanding the down projection layers with QuaRot , and thus slightly increasing the model size, improves both perplexity and zero-shot performance, although there are quickly diminishing returns in perplexity improvements. In these experiments, optimization was performed using bfloat16 as the base datatype. Next, we show that combining Hadamard expansions with Cayley optimization for orthogonal rotation matrices can further improve these results. In these experiments, optimization was performed using float32 as the base datatype, instead of bfloat16. Results are provided in Table 2; we do not report the performance of the ﬂoat model, or the model size, which are identical to Table 1. As expected, Cayley-optimized rotations improves on the base Hadamard rotations.\n\n--- Segment 9 ---\nResults are provided in Table 2; we do not report the performance of the ﬂoat model, or the model size, which are identical to Table 1. As expected, Cayley-optimized rotations improves on the base Hadamard rotations. Furthermore, Cayley-optimized expanded rotations (i.e., SpinQuant ) consistently provides beneﬁts over QuaRot , as reported in [2], and narrows the gap between the quantized model and its high-precision counterpart. We again can see that larger expansions correlate with larger reductions in perplexity, with diminishing returns on zero-shot accuracy; however, more results may be needed to draw a conclusion. 3 Table 1: Expanding QuaRot for Llama3 models. We compare different post-training expansion rates for perplexity (lower is better) and several zero-shot tasks (higher is better). We also report the overall model size of the different quantized conﬁgurations and the expansion ratio compared to baseline quantized. Similarly, we report the input channels for the down projection layers and the expansion ratio. The model size includes quantization parameters.\n\n--- Segment 10 ---\nSimilarly, we report the input channels for the down projection layers and the expansion ratio. The model size includes quantization parameters. Model Algorithm Model Size (Ratio) Layer size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B Float - - 8.75 31.91 66.45 48.15 60.06 75.46 QuaRot 1.235B (1.00) 8192 (1.0) 14.00 25.25 55.68 40.78 53.51 66.15 1.294B (1.05) 8960 (1.1) 13.56 27.99 57.62 41.09 53.67 69.26 1.328B (1.08) 11008 (1.3) 13.56 27.82 54.92 40.69 55.64 66.81 3B Float - - 7.00 42.40 74.87 55.83 68.43 76.70 QuaRot 3.212B (1.00) 8192 (1.0) 9.63 36.09 68.18 48.46 58.56 71.43 3.367B (1.05) 9984 (1.2) 9.50 35.06 67.84 49.07 59.58 72.36 3.455B (1.08) 11008 (1.3) 9.50 36.00 69.02 48.74 59.19 72.41 8B Float - - 5.56 51.79 82.20 60.73 70.87 79.11 QuaRot 7.504B (1.00) 8192 (1.0) 7.75 43.60 75.80 53.53 63.20 75.40 8.041B (1.07) 18432 (1.3) 7.50 43.77 75.93 54.75 63.22 76.88 8.512B (1.13) 22016 (1.5) 7.50 43.17 76.85 54.20 64.33 74.46 Table 2: Expanding SpinQuant for Llama 3.2 1B. We evaluate the impact of different expansion factors on per- plexity (lower is better) and several zero-shot tasks (higher is better).\n\n--- Segment 11 ---\nModel Algorithm Model Size (Ratio) Layer size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B Float - - 8.75 31.91 66.45 48.15 60.06 75.46 QuaRot 1.235B (1.00) 8192 (1.0) 14.00 25.25 55.68 40.78 53.51 66.15 1.294B (1.05) 8960 (1.1) 13.56 27.99 57.62 41.09 53.67 69.26 1.328B (1.08) 11008 (1.3) 13.56 27.82 54.92 40.69 55.64 66.81 3B Float - - 7.00 42.40 74.87 55.83 68.43 76.70 QuaRot 3.212B (1.00) 8192 (1.0) 9.63 36.09 68.18 48.46 58.56 71.43 3.367B (1.05) 9984 (1.2) 9.50 35.06 67.84 49.07 59.58 72.36 3.455B (1.08) 11008 (1.3) 9.50 36.00 69.02 48.74 59.19 72.41 8B Float - - 5.56 51.79 82.20 60.73 70.87 79.11 QuaRot 7.504B (1.00) 8192 (1.0) 7.75 43.60 75.80 53.53 63.20 75.40 8.041B (1.07) 18432 (1.3) 7.50 43.77 75.93 54.75 63.22 76.88 8.512B (1.13) 22016 (1.5) 7.50 43.17 76.85 54.20 64.33 74.46 Table 2: Expanding SpinQuant for Llama 3.2 1B. We evaluate the impact of different expansion factors on per- plexity (lower is better) and several zero-shot tasks (higher is better). Model Algorithm Layer Size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B SpinQuant 8192 (1.0) 13.30 27.13 54.92 40.58 53.20 67.41 8960 (1.1) 12.94 27.65 56.86 41.36 55.96 68.66 11008 (1.3) 12.71 26.54 57.07 41.48 55.33 67.35 Overﬁtting Hypothesis.\n\n--- Segment 12 ---\nWe evaluate the impact of different expansion factors on per- plexity (lower is better) and several zero-shot tasks (higher is better). Model Algorithm Layer Size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B SpinQuant 8192 (1.0) 13.30 27.13 54.92 40.58 53.20 67.41 8960 (1.1) 12.94 27.65 56.86 41.36 55.96 68.66 11008 (1.3) 12.71 26.54 57.07 41.48 55.33 67.35 Overﬁtting Hypothesis. We hypothesize that data-dependent algorithms, like GPTQ and Cayley optimization, can lead to overﬁtting on WikiText2. We perform two tests to investigate this hypothesis. First, we apply QuaRot without GPTQ and observe improved results in terms of perplexity and zero-shot accuracy when expanding the down projection layer by 1.3 relative to expansion by 1.1 , as shown in Table 3. Although GPTQ still improves performance in general, the interaction with expansion and GPTQ suggests overﬁtting. Second, we perform Cayley optimization with the learning rate reduced by 10 . As shown in Table 4, the average zero-shot performance improves, even though the perplexity results are considerably worse, supporting our overﬁtting theory. More results may be needed to draw a conclusion, which we leave for future work. The Trade-off Between Model Quality and Model Volume. Quantization often focuses on the amount of model degradation compared to some full-precision baseline. When quantization is applied to multiple models designed for the same problem (e.g., multiple LLMs trained on the same data), the effect of quantization can be placed into a wider context. For example, if there is some constraint at inference time, such as model volume, we can ask: "what is the most accurate model which ﬁts my requirements?" Using model volume as an example constraint, Figures 1 and 2 show the trade-off that can be made between volume and WikiText2 perplexity or average zero-shot evaluation, respectively.\n\n--- Segment 13 ---\nFor example, if there is some constraint at inference time, such as model volume, we can ask: "what is the most accurate model which ﬁts my requirements?" Using model volume as an example constraint, Figures 1 and 2 show the trade-off that can be made between volume and WikiText2 perplexity or average zero-shot evaluation, respectively. The ﬁgures show the results1 from Tables 1, 2 and 4, the different clusters in the ﬁgures correspond to the different models sizes; Llama 3.2 1B, 3.2 3B and 3.1 8B. For a given memory constraint (measured as model volume), the Pareto frontier provides the optimal model within our co-design space. Notably, SpinQuant results outperform QuaRot results for equivalent models (i.e., Llama 3.2 1B), otherwise, almost all quantized and ﬂoat models lie on the Pareto frontier. Interestingly, for the zero-shot results, the Llama 3.2 1B bfloat16 results are completely Pareto-dominated by the INT4 Llama 3.2 3B results. Finally, since all experiments quantize both weights and activations, we expect the trade- off between model volume and model quality would signiﬁcantly lean further towards quantized models if weight-only quantization were considered. 1Only the best results of Tables 2 and 4 were used for the SpinQuant results. 4 Table 3: Expanding QuaRot for Llama 3.2 1B without any data-dependent PTQ. We evaluate the impact of removing data-dependent PTQ algorithms on perplexity (lower is better) and several zero-shot tasks (higher is better). Model Algorithm Layer Size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B QuaRot 8960 (1.1) 19.75 22.87 46.71 36.51 51.14 64.96 11008 (1.3) 18.63 23.72 47.64 37.72 51.30 66.70 Table 4: Expanding SpinQuant for Llama 3.2 1B with reduced learning rate. We evaluate the impact of different expansion factors on perplexity (lower is better) and several zero-shot tasks (higher is better).\n\n--- Segment 14 ---\nModel Algorithm Layer Size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B QuaRot 8960 (1.1) 19.75 22.87 46.71 36.51 51.14 64.96 11008 (1.3) 18.63 23.72 47.64 37.72 51.30 66.70 Table 4: Expanding SpinQuant for Llama 3.2 1B with reduced learning rate. We evaluate the impact of different expansion factors on perplexity (lower is better) and several zero-shot tasks (higher is better). The learning rate is reduced by a factor of 10 from the original SpinQuant proposal [2]. Model Algorithm Layer Size (Ratio) Perplexity ARC-C ARC-E HS Wino PIQA 1B SpinQuant 8192 (1.0) 13.34 28.58 56.65 40.81 54.38 66.70 8960 (1.1) 13.35 27.73 56.77 40.83 54.61 67.63 11008 (1.3) 12.92 29.10 57.07 42.17 55.64 68.06 5 Conclusion Post-training model expansion broadens the quantization landscape. When accuracy requirements cannot be met, the most common strategy is to relax quantization constraints, for example going from per-tensor to per-channel (or even per-group) scaling or increasing bit widths. However, compilers and accelerators are often designed and optimized to work best with a speciﬁc set of datatypes. Therefore, as an alternative strategy, we demonstrate modest increases in model size post-training can improve the accuracy of quantized models within a ﬁxed quantization co-design space. Our simple post-training model expansion trick gives more ﬂexibility to deep learning engineers deploying LLMs, and our investigations open up promising new research questions around the potential for data-dependent quantization algorithms to overﬁt, and the general performance plateau of data-free quantization. Furthermore, while we focused our investigation on the expansion of all down projection layers in a model, one could precisely target speciﬁc layers based on their sensitivity to quantization; we leave such a study for future work.\n\n--- Segment 15 ---\nOur simple post-training model expansion trick gives more ﬂexibility to deep learning engineers deploying LLMs, and our investigations open up promising new research questions around the potential for data-dependent quantization algorithms to overﬁt, and the general performance plateau of data-free quantization. Furthermore, while we focused our investigation on the expansion of all down projection layers in a model, one could precisely target speciﬁc layers based on their sensitivity to quantization; we leave such a study for future work. Finally, although Cayley optimization seems to consistently work well with Hadamard expansion, we plan to investigate the variability in zero-shot results that we observed when testing for different model expansion rates. 0 2 4 6 8 10 12 14 16 6 8 10 12 14 Model Volume (GB) Perplexity Pareto Frontier BF16 INT4 (QuaRot ) INT4 (SpinQuant ) Figure 1: Comparison between Wikitext2 per- plexity scores and model volume in GB. 0 2 4 6 8 10 12 14 16 45 50 55 60 65 70 Model Volume (GB) Avg. 0-Shot Accuracy ( ) Pareto Frontier BF16 INT4 (QuaRot ) INT4 (SpinQuant ) Figure 2: Comparison between geometric mean of 0-shot evaluation and model volume in GB. 5 References [1] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoe- ﬂer, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. [2] Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. SpinQuant: LLM quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. [3] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting.\n\n--- Segment 16 ---\n[3] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural network quantization without retraining using outlier channel splitting. In International conference on machine learning, pages 7543 7552. PMLR, 2019. [4] Mengxia Yu, De Wang, Qi Shan, and Alvin Wan. The super weight in large language models. arXiv preprint arXiv:2411.07191, 2024. [5] Mart Van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning. Advances in neural information processing systems, 33:5741 5752, 2020. [6] Jorn Peters, Marios Fournarakis, Markus Nagel, Mart Van Baalen, and Tijmen Blankevoort. Qbitopt: Fast and accurate bitwidth reallocation during training. In Proceedings of the IEEE CVF international conference on computer vision, pages 1282 1291, 2023. [7] Mart Van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin, Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga, et al. FP8 versus INT8 for efﬁcient deep learning inference. arXiv preprint arXiv:2303.17951, 2023. [8] Xinyu Zhang, Ian Colbert, and Srinjoy Das. Learning low-precision structured subnetworks using joint layerwise channel pruning and uniform quantization. Applied Sciences, 12(15):7829, 2022. [9] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of quantization methods for efﬁcient neural network inference. In Low-power computer vision, pages 291 326. Chapman and Hall CRC, 2022. [10] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization.\n\n--- Segment 17 ---\n[10] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. [11] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023. [12] Elias Frantar, Saleh Ashkboos, Torsten Hoeﬂer, and Dan Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. [13] Ian Colbert, Fabian Grob, Giuseppe Franco, Jinjie Zhang, and Rayan Saab. Accumulator-aware post-training quantization. arXiv preprint arXiv:2409.17092, 2024. [14] Zechun Liu, Changsheng Zhao, Hanxian Huang, Sijia Chen, Jing Zhang, Jiawei Zhao, Scott Roy, Lisa Jin, Yunyang Xiong, Yangyang Shi, et al. ParetoQ: Scaling laws in extremely low-bit llm quantization. arXiv preprint arXiv:2502.02631, 2025. [15] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network and specialize it for efﬁcient deployment. arXiv preprint arXiv:1908.09791, 2019. [16] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.\n\n--- Segment 18 ---\nDeep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. [17] Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019. [18] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [19] Giuseppe Franco, Alessandro Pappalardo, and Nicholas J Fraser. Xilinx brevitas, 2025. [20] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87 100, 2024. 6 [21] Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, and Vikas Singh. Framequant: Flexible low-bit quantization for transformers. arXiv preprint arXiv:2403.06082, 2024. [22] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. QuIP: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36, 2024. [23] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. QuIP : Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. [24] Jun Li, Li Fuxin, and Sinisa Todorovic. Efﬁcient riemannian optimization on the stiefel manifold via the cayley transform. arXiv preprint arXiv:2002.01113, 2020.\n\n--- Segment 19 ---\nEfﬁcient riemannian optimization on the stiefel manifold via the cayley transform. arXiv preprint arXiv:2002.01113, 2020. [25] Hadacore: Tensor core accelerated hadamard transform kernel. [26] Aaron Grattaﬁori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [28] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [29] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really ﬁnish your sentence? arXiv preprint arXiv:1905.07830, 2019. [30] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artiﬁcial intelligence, volume 34, pages 7432 7439, 2020. [31] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial wino- grad schema challenge at scale. Communications of the ACM, 64(9):99 106, 2021. [32] Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. LightEval: A lightweight framework for llm evaluation, 2023.\n\n--- Segment 20 ---\n[32] Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. LightEval: A lightweight framework for llm evaluation, 2023. 7\n\n