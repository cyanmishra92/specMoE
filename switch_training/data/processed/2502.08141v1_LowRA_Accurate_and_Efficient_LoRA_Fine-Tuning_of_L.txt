=== ORIGINAL PDF: 2502.08141v1_LowRA_Accurate_and_Efficient_LoRA_Fine-Tuning_of_L.pdf ===\n\nRaw text length: 67809 characters\nCleaned text length: 66721 characters\nNumber of segments: 50\n\n=== CLEANED TEXT ===\n\nLowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits Zikai Zhou1, Qizheng Zhang1, Hermann Kumbong1, Kunle Olukotun1,2 1Department of Computer Science, Stanford University 2Department of Electrical Engineering, Stanford University February 13, 2025 Abstract Fine-tuning large language models (LLMs) is increasingly costly as models scale to hundreds of billions of parameters, and even parameter-efficient fine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce LowRA, the first framework to enable LoRA fine-tuning below 2 bits per parameter with minimal performance loss. LowRA optimizes fine-grained quantization mapping, threshold selection, and precision assignment while leveraging efficient CUDA kernels for scalable deployment. Extensive evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior performance precision trade-off above 2 bits and remains accurate down to 1.15 bits, reducing memory usage by up to 50 . Our results highlight the potential of ultra-low-bit LoRA fine-tuning for resource-constrained environments. 1 Introduction Fine-tuning large language models (LLM) can enhance their performance on particular tasks [50, 48, 55], and remove unwanted behaviors like hallucinations [15, 26] and harmful responses [3, 1]. Yet as models scale - e.g., Llama 3.1 with 405 billion parameters [10] and DeepSeek-V3 with 671 billion parameters [25] - the cost of fine-tuning soars [14]. To reduce fine-tuning costs, parameter-efficient fine-tuning (PEFT) methods [52, 14, 30, 27] freeze a model s core weights and insert small trainable modules. We focus on LoRA [14], which adds rank-decomposed adapters to cut computation and memory demands. Still, fine-tuning large models on a single GPU can exceed memory limits. Quantized LoRA approaches (e.g., QLoRA [9], LoftQ [23]) resolve this by quantizing the base weights with minimal loss in accuracy, enabling billions-parameter models to be fine-tuned on standard single GPUs or even mobile devices. Despite the success of quantized LoRA in cutting memory usage, all existing works focus on LoRA fine-tuning within the range of 2 to 4 bits (per parameter), many of which are incompatible with ultra-low-bit LoRA fine-tuning [46, 32, 9]. Further pushing down the bits (per parameter) requirement, e.g., below 2 bits, has profound implications in fine-tuning and deployment in ultra- low-resource scenarios like embedded devices [41, 4] and mobile phones [47, 43]. However, current methods face three fundamental limitations: L1: focus exclusively on coarse-grained quantization of the base model weights. 1 arXiv:2502.08141v1 [cs.LG] 12 Feb 2025 L2: leverage quantization functions (i.e., mappings and thresholds) that assume some fixed data distribution across the entire model weight. L3: rely on simulated quantization, with no system support for efficient low-bit quantization. To unleash the full potential of quantized LoRA fine-tuning, we present LowRA, an accurate and efficient framework that enables LoRA fine-tuning down to below 2 bits (per parameter). LowRA features three major components for each of the three challenges: (1) mapping threshold function search, (2) fine-grained precision assignment, and (3) CUDA-based kernels as quantization primitives. Addressing L1 and L2 requires extra care because LoRA base weights have to work with multiple sets of adapters in real-life settings [42, 36, 5]. This constraint demands a powerful, task-agnostic quantization technique. Furthermore, optimally assigning precisions at a fine-grained granularity for LLMs calls for a scalable, low-complexity solution to handle massive parameter spaces. LowRA meets the need through a hierarchical ILP(Integer Linear Programming)-based precision assigner for performing fine-grained mixed-precision. Moreover, LowRA provides a weighted Lloyd-Max [29, 31] formulation of mapping threshold search for groupwise normalization, and achieves strong practical performance through its efficient solution. We conduct extensive evaluation of LowRA across 4 choices of widely used base LLMs and 4 choices of natural language applications, and compare LowRA against state-of-the-art baselines. Evaluation results demonstrate that LowRA: (1) achieves a superior performance-precision trade-off above 2 bits (per parameter) compared to baselines, and is the first method to enable accurate, efficient LoRA fine-tuning below 2 bits, (2) enables substantial memory footprint reduction in fine-tuning, and (3) incurs minimal additional overhead even with newly added components. In summary, we make the following contributions: Identifying Key Limitations: We identify three core limitations in existing quantized LoRA approaches, highlighting opportunities to exploit fine-grained precision assignment and mappings thresholds. Design and Implementation of LowRA: We introduce LowRA, an accurate, end-to-end framework that applies fine-grained quantization to LoRA fine-tuning of LLMs. We detail its key design choices including a mappings thresholds learner, precision assigner, and practical programming primitives ensuring both efficiency and high performance. Better Performance-Precision Trade-Off: Comprehensive evaluations show that LowRA outperforms baselines in performance-precision trade-off, enabling an average 0.86-bit reduction per parameter without sacrificing performance. Memory Footprint Reduction: LowRA cuts memory usage by 30 50 during fine-tuning and deployment with minimal performance loss. Moreover, it enables fine-tuning and deploying LLMs in ultra-resource-constrained settings at as low as 1.15 bits. Open-Source Framework: We will open-source our framework and artifacts to spur further research in ultra-low-bit LoRA fine-tuning. 2 This paper is organized as follows: Section 2 introduces LoRA fine-tuning, quantization for LoRA, and three key limitations of existing quantized LoRA methods. Section 3 presents the LowRA end-to-end workflow, while Section 4 discusses its key design insights and benefits. Sections 5 and 6 detail the mapping threshold learner and the fine-grained precision assignment, respectively. Finally, we describe our evaluation setup, results, and takeaways in Section 7. 2 Background and Motivation Pretrained Weight (T1) A B Intelligently Initialized Low-rank Tensors (T5) Mapping and Threshold Learner (P1) Fine-grained Mappings and Thresholds Per-output-channel Mixed-Precision Downquant Residual (T4) A B Final Low-rank Tensors (T6) f FP32 FP16 BF16 4-bit 2-bit 1-bit Programs Tensors Precision Assigner (P2) (T2) Precs (T3) Output- channel- wise Quantize Kernel (P3) Low-Rank Initializer (P4) Further Finetuning (P5) Output- channel- wise Dequantize Kernel (P5.1) f Out In User DeÔ¨Åned Compression Ratio Figure 1: End-to-end workflow of LowRA. 2.1 Low-Rank Adaptation (LoRA) of LLMs Fine-tuning large language models (LLMs) allows us to adapt pre-trained LLMs to particular tasks or domains [50, 48, 55]. This process usually requires changing all model parameters, which can be prohibitively expensive (in terms of compute and memory) when the number of model parameters increases. Low-Rank Adaptation (LoRA) [14] tackles this by freezing the base weights and introducing a small set of trainable adapter parameters, drastically reducing memory and compute requirements for fine-tuning. 2.2 Quantization for LoRA Fine-Tuning Quantized LoRA fine-tuning further cuts memory usage by quantizing the base model weights without hurting performance. QLoRA [9] introduces a NormalFloat format to backpropagate through a 4-bit quantized backbone, while LoftQ [23] and ApiQ [24] jointly optimize quantized base weights and adapter initializations under a unified objective. These advances unlock fine-tuning and deployment of LLMs on low-resource platforms like embedded devices [41, 4] and mobile phones [47, 43]. 2.3 Limitations of Existing Quantized LoRA Methods Despite the early promise of recent work in quantized LoRA like QLoRA and LoftQ, we observe three major limitations that prevent us from fully realizing the potential of memory-efficient LLM fine-tuning. 3 L1: Coarse-Grained Precision Assignment. Existing approaches typically apply a single quantization precision to an entire weight matrix or multiple layers. For instance, QLoRA uses uniform 4-bit quantization across all base weights, while LoftQ adopts a layerwise mixed-precision scheme (e.g., higher precision for earlier layers, lower for later layers). Our findings ( 6) suggest that truly unlocking ultra-low-bit fine-tuning requires a finer-grained assignment strategy, potentially at the sub-layer or sub-matrix level. L2: Discrepancy in Data Distribution. Most quantized LoRA methods use a globally shared data format such as QLoRA s NormalFloat, which assumes a roughly normal distribution. However, Figure 2 reveals that groupwise normalization at a per-channel level often deviates significantly from a global normal distribution. To preserve accuracy, more localized quantization dequantization approaches are needed. 1.0 0.5 0.0 0.5 1.0 0 100 200 300 400 500 Layer 0 self_attn_v_proj Channel 1214 1.0 0.5 0.0 0.5 1.0 0 250 500 750 1000 1250 Layer 0 self_attn_q_proj Channel 2082 Histogram Normal PDF 1.0 0.5 0.0 0.5 1.0 0 100 200 300 Layer 0 self_attn_q_proj Channel 723 Normalized Value Frequency Figure 2: Distributions of normalized parameters in different output channels sampled from the first layer of Llama2-7b. L3: Lack of High-Performance Quantization Primitives. Most quantized LoRA methods and related quantization studies [23, 37, 40, 2] rely on simulated quantization 1, lacking native hardware support for sub-4-bit or flexible mixed-precision operations. For instance, LoftQ requires eight A100 GPUs even for smaller LLMs. Such reliance on simulation inflates resource requirements and impedes practical deployment (see Appendix J for related Github issues), as no existing system offers efficient low-bit or adaptive-precision kernels tailored to LoRA. 3 The LowRA Framework In this section, we provide an overview of the LowRA end-to-end workflow, illustrated in Figure 1. The process begins with the pretrained model weights (T1). We feed each layer of these weights into a dedicated mapping and thresholds learner (P1), which produces optimized per-output-channel mappings and thresholds, denoted (T2). These mappings and thresholds, along with the pretrained weights, are then processed by a two-step ILP quantizer (P2) to determine the optimal precision assignments (T3) for each output channel. Next, the output-channel-wise quantize kernel (P3), which supports custom quantization thresholds, uses the derived thresholds (T2) and the assigned precision levels (T3) to quantize the weights. We calculate the quantization errors arising from this step and apply low-rank tensor initialization (P4). Techniques for intelligent low-rank initialization include LoftQ [23] and PiSSa [32] (Appendix D) which generates low-rank tensors (T5) designed to absorb quantization errors 1using floating-point values to mimic discrete quantization levels throughout training or fine-tuning. 4 during initialization. In our implementation, we opt for LoftQ [23] as experiments show that they give better performance in the low-bit range. The resulting mixed-precision quantized weights (T4) and the initialized low-rank tensors (T5) are then passed to a fine-tuning module (P5), which relies on an output-channel-wise dequantize kernel (P5.1) to recover the base weights from their quantized form. Following the approach used in LoRA and QLoRA, the base weights (T4) remain frozen, and only the low-rank tensors (T5) are trained. Finally, we obtain the updated low-rank tensors (T6), which are output together with the quantized base weights (and associated quantization state) (T4). 4 Discussion about Design Choices In this section, we discuss various design choices in the LowRA framework, as well as system and hardware support. 4.1 Insights behind LowRA Design Choices Per-Output-Channel Quantization In LLMs, linear layers often exhibit substantially more variation across output channels than across input channels. For instance, in Llama2-7b [44], the average standard deviation along the output channel is 2.20 higher than along the input channel (see Appendix H). As a result, grouping parameters by output channel and assigning a unique precision to each group i.e., per-output-channel quantization more effectively captures their diverse distributions. Groupwise Normalization Each output channel may still exhibit significant internal variability even with per-output-channel quantization. To address this, groupwise normalization is often used to allow each group of elements share a separate scale. We follow QLoRA s design of using 64-element normalization scaled by the absmax (i.e., maximum absolute value) in each group [9]. Data-Free Post-Training Quantization Unlike quantization-aware training (QAT) [11, 51, 17, 39], our approach adds no overhead to fine-tuning. By automatically searching for quantization mappings and thresholds, it frees users from manual tuning [39, 54], saving both development and computation resources. Moreover, contrary to some methods that vary compression ratios over time [39, 51], LowRA maintains a consistent compression ratio, ensuring persistent memory savings during fine-tuning. Per-Output-Channel Thresholds and Mappings Figure 3 visualizes the roles of thresholds and mappings in the process of quantization. Thresholds refer to the boundary points ( bin edges ) that partition the continuous domain of normalized parameters into discrete intervals and thus specific bitstring encodings. Mappings, on the other hand, specify the representative values assigned to each encoded bitstring and thus the intervals. As discussed in 2.3, fine-grained designs of quantization mappings and thresholds could lead to significantly more accurate approximation and reconstruction of parameters. LowRA allows each output channel to adopt a different combination of mappings and thresholds for more precise fine-grained quantization. Data-Free One-Shot Post-Training Quantization Most quantization-aware training (QAT) techniques achieve higher task performance by incurring additional training overhead and learning task-specific quantization parameters [11, 51, 17, 39]. Similarly, many post-training quantization 5 -1.00 1.00 Thresholds Mappings Original Quantized -0.92 -0.31 0.46 0.78 -0.76 -0.12 0.63 Figure 3: Roles of mappings and thresholds in quantization. Circles represent thresholds whereas crosses represent mappings. Colored Triangles represent the process of converting a range of original unquantized real values - partitioned by thresholds - to the mapped values corresponding to each quantization level. methods require a calibration set for quantization scheme learning [24, 16]. In contrast, LowRA uses data-free one-shot post-training quantization, enabling reusable quantization schemes and quantized parameters, minimal hyperparameter tuning, and negligible fine-tuning overhead. This design is particularly suited to LoRA fine-tuning because: (1) Task-dependent learning is confined to the adapters, (2) LoRA base weights are often shared across multiple adapters, and (3) LoRA primarily targets resource-constrained fine-tuning scenarios. User-Defined Compression Ratios Quantized LoRA methods see heavy use in tight resource settings - e.g., limited-memory GPUs or on-device scenarios - where specifying a precise compression ratio is pivotal. By tailoring each parameter s bit precision, thresholds, and mappings, LowRA directly aligns compression with real-world resource budgets, ensuring feasibility and efficiency even under strict constraints. Furthermore, because LowRA fixes the ratio in a single pass, it obviates the extensive hyper-parameter tuning needed by alternative methods to find acceptable compression accuracy trade-offs [39, 54]. Using LoftQ as Low-Rank Intializer Researchers have found that the initialization of low-rank tensors is crucial to the effectiveness of LoRA fine-tuning, especially when it comes to ultra-low- bit quantized base weight [23, 32, 46]. LoftQ [23] and PiSSA [32] are two notable initialization techniques for quantized LoRA (see Appendix D for a detailed introduction). While PiSSA purports faster convergence than LoftQ, our experiments consistently show LoftQ outperforming PiSSA. As illustrated by the sample data points in Table 1, PiSSA fails to achieve reasonable task performance at lower bit ranges. This aligns with our intuition that performing quantization rather than SVD first allows the low-rank tensors to better absorb quantization errors. Our findings also corroborate points raised in the LoftQ appendix. Since our main objective is to enable lower-precision fine-tuning and deployment, we opt to use LoftQ as our low-rank initializer. Following the recommendation from LoftQ, we use five alternating steps for initialization. Adapting LowRA to Production Use Cases Many production use cases, e.g., batched inference in data centers, require fixed quantization mappings [22, 53]. LowRA can be adapted to such scenarios by keeping only the thresholds learnable, which is shown to be useful in enhancing model performance [28]. To maximize performance with task-agnostic reusable base weight, LowRA can be extended to use the same set of thresholds for multiple adapters but learn mappings for each downstream task. In other words, each adapter can be connected to the base weight together 6 Setup Llama-7b Llama-13b Method Dataset 2-bit 4-bit 2-bit 4-bit PiSSA WikiText-2 ( ) 1919.63 5.53 1825.68 5.05 LoftQ 8.63 5.26 7.27 4.79 Table 1: Perplexities of PiSSA and LoftQ as initialization methods on WikiText-2. Quantization is performed at 2 bits or 4 bits per parameter. Lower values indicate better performance ( ). with a dedicated base weight decoding mapping for that downstream task. Nevertheless, this would demand higher development and computation costs. In our implementation and experiments, we adopt the same set of learned thresholds and learned mappings for a single base weight for the proof of concept. 4.2 System and Hardware Support Building on the limitations noted in 2.3, we implement practical CUDA-based primitives that support both low-bit and mixed-precision LoRA fine-tuning with maximum flexibility (details in Appendix A). Notably, the added kernel generalization incurs only negligible overhead in end-to-end inference, as quantization dequantization constitutes a minimal portion of the total compute cost. 5 Mapping and Threshold Learner In this section, we introduce the mapping and threshold learner in LowRA. Because we want the final base weights to remain task-agnostic and thus reusable across multiple adapters, we adopt a simple approach that minimizes the mean squared error2 (MSE) in each output channel. As discussed in 4.1, one could learn separate decoding mappings for each downstream task (or adapter set), but at a higher fine-tuning cost. We therefore propose an efficient design for the mapping threshold learner that avoids this expense. Weighted Lloyd-Max Algorithm. We cast the problem of searching for the optimal quantization mappings and thresholds for each output channel to minimize MSE as a Weighted Lloyd-Max Problem. A detailed description of this algorithm can be found in Appendix G. Weighted Lloyd s for LoRA Quantization. As discussed in 4.1, we perform groupwise normalization to give more scale to quantization within each output channel. To recap, with groupwise normalization (Section 4.1), each block of weights is scaled by the block-wise maximum absolute value (absmax). Specifically, if we denote the set of original weights in a block by {xi} and its maximum absolute value by absmax, then we treat absmax as a per-block weight in the Weighted Lloyd-Max algorithm. By assigning them proportionally larger weights, the algorithm pays more attention to those blocks and adjusts their bin thresholds and centroids accordingly. Consequently, blocks whose values have smaller magnitudes (and thus smaller absmax) are penalized less, striking a balance across all blocks to minimize the overall quantization error in QLoRA. In our application of the Weighted Lloyd s algorithm to LoRA base weight quantization, we initialize the thresholds as those used by NormalFloats [9] for 2-bit and 4-bit precisions and use 2We define SE (Squared Error) as (ÀÜy y)2, i.e., the squared difference between the predicted and ground-truth values. Therefore, MSE is the mean of these squared errors over the dataset. 7 0.0 as the initial threshold for 1-bit quantization34. Then, at each iteration, we recompute the quantization mappings as the weighted centroids of the assigned data and recompute the thresholds as the midpoints between consecutive mapping (centroid) values5. We output the last-computed quantization mappings and thresholds when max iteration is reached or the MSE stops going down. In our current implementation, we take the average of all thresholds to preserve distribution and prevent instability in the interaction with the Low-Rank Initializer. 6 Mixed-Precision Quantization: Channelwise Precision Assign- ment In this section, we present how mixed-precision quantization assignment is conducted in LowRA. In light of the aforementioned task-agnostic requirement, a simple yet effective objective for defining this problem is the minimization of the overall Summed Square Error6 (SSE) considering the regular structure of transformer-based architectures [49, 10, 44]. Such formulation can serve as an effective proxy to retain more information for the harder-to-quantize channels in weights. One can observe that finding the optimal mixed-precision scheme (w.r.t. SSE) can be formulated as an ILP. However, due to the large number of output channels in LLMs, a direct solver-based approach becomes prohibitively expensive. For instance, solving more than five layers of LLaMA-2- 7B fails to finish within ten hours. To address this limitation, we propose a two-level ILP workflow (Figure 4, Algorithm 1) that retains the benefits of ILP-based methods while ensuring reasonable complexity. Algorithm 1 Channelwise Precision Assignment 1: Input: N, distinct w(1), . . . , w(K), partition {Ik}, MSE(i, p), total budget Btotal 2: Step 1. Compute Wk P i Ik wi, then Wsum PK k 1 Wk 3: Step 2. Bk Btotal Wk Wsum for k 1, . . . , K 4: for k 1 to K do 5: Step 3. Cluster channels in Ik (e.g. K-Means on MSE features) into Kk clusters 6: Step 4. Cluster-Level ILP: decide how many channels in each cluster get each bitwidth, subject to Bk 7: Step 5. Intra-Cluster ILP: within each cluster, assign specific channels to bitwidths 8: end for 9: Step 6. Combine final bitwidths into b1, . . . , bN 10: Output: (b1, . . . , bN), total SSE, actual bits used 6.1 Preprocessing for the Pipeline (Step 1-3) To preprocess channels for the hierarchical ILP pipeline, we first compute each channel s MSE under 1-, 2-, and 4-bit quantization. Next, we split channels by parameter count, which in LLMs typically yields two distinct sizes (e.g., 4096 and 11008 for LLaMA-2-7B). Within each group, we then apply three-dimensional K-Means clustering7 (based on the three computed MSE values), forming 128 3Separately defined as NormalFloats lack a 1-bit representation 4See the initial values in Appendix F 5In our implementation, we set number of iterations to 2 6We define SSE (Summed Square Error) as the sum of the squared differences between the original values and their quantized counterparts, i.e., P i xi bxi 2. 7We use 300 as the maximum number of iterations 8 clusters per group in our implementation. 6.2 Cluster-Level ILP (Step 4) Formed clusters first go through the following cluster-level ILP to be assigned budgets of 1-bit, 2-bit, and 4-bit channels. Consider C clusters, each with Sc channels c 1, . . . , C . Let P {1, 2, 4} be the available bit-precisions8. For each cluster c and precision p P, define: costc,p: mean quantization error (e.g., mean-squared error) per channel in cluster c if all channels in that cluster are assigned to precision p, scaled by the number of weight parameters per channel in that cluster 9. yc,p Z 0: decision variable representing the number of channels in cluster c that will be assigned precision p. We define a global bit-budget B (i.e., total permissible bits across all clusters). Let Œ≤(p) be the bit-precision value (e.g., Œ≤(1) 1, Œ≤(2) 2, Œ≤(4) 4). To enforce the bit budget, we multiply Œ≤(p) by the channel parameter count œâc for cluster c, and then by the number of channels yc,p. As we split channels based on the number of parameters within each channel, each channel in a cluster c shares the same œâc. Minimize C X c 1 X p P costc,p yc,p subject to X p P yc,p Sc, c 1, . . . , C, C X c 1 X p P Œ≤(p) œâc yc,p B, yc,p Z 0, 0 yc,p Sc. This formulation seeks to minimize the total weighted quantization error by choosing, for each cluster c, how many of its channels yc,p are assigned to each precision level p. The constraints ensure that every channel of a cluster is allocated exactly once, the total bits used do not exceed the overall budget B, and that the decision variables remain non-negative integers and do not exceed the number of channels in their respective clusters. 6.3 Intra-Cluster ILP (Step 5) Once the cluster-level ILP decides how many channels {yc,p} in each cluster c should be assigned to each bit precision p, a second ILP distributes these assignments to each channel within each cluster. Let Sc denote the total number of channels in cluster c. For channel i {1, . . . , Sc} in cluster c, we define the precomputed mean-squared error at precision p as MSE(i, p) (precomputed quantization error of channel i at precision p). We define binary decision variables xi,p 1 if channel i is assigned bit precision p; otherwise, xi,p 0. 8For LLaMA, restricting to 2 and 4 bits outperformed including 1 bit for bpp 2.0, so we adopt this configuration. 9Contrary to LoftQ s suggestion, per-layer cost weighting based on layer index proved suboptimal in our experiments. 9 Minimize Sc X i 1 X p P MSE(i, p) xi,p subject to X p P xi,p 1 i {1, . . . , Sc}, Sc X i 1 xi,p yc,p p P, xi,p {0, 1} i {1, . . . , Sc}, p P. This formulation constitutes the intra-cluster ILP. The objective minimizes the total quantization error, where MSE(i, p) is the precomputed mean-squared error for channel i at bit precision p. The first constraint ensures that each channel is assigned to exactly one precision. The second constraint enforces that the number of channels assigned to each precision p matches the counts yc,p determined by the cluster-level ILP. Finally, the binary constraint stipulates that each decision variable xi,p is either 0 or 1. This intra-cluster ILP enforces that the required number of channels (from the cluster-level ILP) is assigned to each bit precision and minimizes local MSE within the cluster. Efficiently Leveraging ILP Solvers. In Step 6, we collect the assigned per-channel precisions. By employing this two-step hierarchical approach, we capitalize on the strengths of ILP solvers while maintaining minimal computational overhead. 7 Evaluation We evaluate LowRA across four datasets spanning natural language generation, multi-turn conver- sation, and long-context text summarization, demonstrating that: Better performance at the same precision: LowRA outperforms all baselines below 4-bit and matches their performance at 4-bit ( 7.3). Same performance at lower precision: LowRA achieves comparable performance while reducing precision by 0.86 bits per parameter on average ( 7.3). First method to fine-tune LoRA under 2 bits: LowRA enables fine-tuning down to 1.75 bits on LLaMA-2-7B, LLaMA-2-13B, and BART-large, and 1.15 bits on LLaMA-30B ( 7.4). Substantial memory savings: LowRA reduces memory usage by 30 50 in fine-tuning and deployment, with minimal performance loss compared to QLoRA ( 7.5). Minimal overhead: The additional one-time preprocessing costs in LowRA are negligible (Appendix E). 7.1 Evaluation Setup Hardware Platform Experiments are conducted on NVIDIA A100 GPUs (80GB memory). Each LLaMA experiment runs on a single dedicated GPU. Each BART-large experiment runs two instances concurrently on a single GPU. Hyperparameters For a fair comparison, we use identical hyperparameters across all methods, consistent with QLoRA [9] and LoftQ [23]. Details on selected hyperparameters are in Appendix I. 10 Channelwise Param Counts Channelwise Quant Costs (MSEs) Channelwise Quant Costs (A) Channelwise Param Counts (A) Channelwise Quant Costs (B) Channelwise Param Counts (B) Split on Param Counts K-Means Clustering K-Means Clustering Cluster-level ILP Per-Cluster Precision Assignments Clusters Group A Clusters Group B Intra-cluster ILP Channelwise Precision Assignment Bit Budget Figure 4: Two-step ILP-based Workflow for Channelwise Precision Assignment Language Models We evaluate LowRA on a range of LLMs: LLaMA-2-7B, LLaMA-2-13B [45], BART-large [20], and LLaMA-30B [44] (to assess ultra-low-bit scalability). Datasets and Evaluation Metrics We use standard datasets across different NLP tasks: WikiText-2 [33] (language modeling, perplexity), Open-Assistant [19] (multi-turn conversation, perplexity), XSUM [35] (summarization, ROUGE scores), and CNN DailyMail [13] (summarization, ROUGE scores). Each dataset is evaluated using the standard metrics used in prior work. 7.2 Baselines QLoRA QLoRA originally employs a fixed 4-bit quantization for pretrained LLMs and does not support fine-tuning below 4 bits. To enable sub-4-bit QLoRA experiments, we follow the adaptation introduced in LoftQ. Additionally, QLoRA directly quantizes the pretrained weights while preserving their original distribution, initializing the low-rank tensors with zeros and small Gaussian noise. LoftQ LoftQ performs mixed-precision quantization (2-bit 4-bit) and jointly optimizes both quantized LLM weights and low-rank adapter initialization. We match LoftQ s effective batch 11 Method Bit LLaMA-2-7B LLaMA-2-13B BART-large WikiText-2 OASST1 WikiText-2 OASST1 XSUM CNN DailyMail ppl. acc. ppl. ppl. acc. ppl. ROUGE1 ROUGE2 ROUGEL QLoRA 4.00 6.22 0.583 3.52 4.79 0.628 3.25 39.07 16.31 31.09 41.18 18.32 27.58 LoftQ 4.00 5.26 0.613 3.48 4.79 0.628 3.23 40.34 17.06 31.92 41.12 18.29 27.54 LowRA 4.00 5.25 0.612 3.48 4.79 0.628 3.23 40.27 17.18 32.06 40.95 18.12 27.54 QLoRA 3.00 7.13 0.566 4.56 6.06 0.588 3.88 17.60 2.68 13.93 15.34 1.12 10.44 LoftQ 3.00 6.87 0.571 4.42 5.91 0.591 3.79 37.23 14.34 29.30 40.47 17.75 26.88 LowRA 3.00 5.84 0.593 3.87 5.24 0.611 3.50 38.84 15.68 30.57 40.85 18.12 27.23 QLoRA 2.50 8.05 0.546 5.17 6.84 0.568 4.36 15.33 1.97 12.55 13.68 1.04 9.99 LoftQ 2.50 7.72 0.552 4.98 6.70 0.572 4.21 34.48 12.26 27.05 39.81 17.19 26.57 LowRA 2.50 6.23 0.582 4.11 5.51 0.601 3.64 37.69 14.76 29.53 40.88 18.06 27.01 QLoRA 2.25 8.67 0.534 5.59 7.31 0.588 4.64 16.37 2.22 12.84 11.90 1.32 10.25 LoftQ 2.25 8.22 0.543 5.24 6.96 0.564 4.46 32.71 10.94 25.37 39.36 16.87 26.29 LowRA 2.25 6.40 0.578 4.21 5.66 0.597 3.73 37.29 14.36 29.12 41.01 18.19 27.23 QLoRA 2.00 9.17 0.526 6.07 7.64 0.551 5.02 DNC 4.84 0.00 4.36 LoftQ 2.00 8.63 0.536 5.68 7.27 0.558 4.75 31.89 10.18 24.59 38.88 16.49 25.85 LowRA 2.00 6.60 0.574 4.35 5.79 0.593 3.84 36.75 13.93 28.61 40.15 17.48 26.67 QLoRA 1.90 LoftQ 1.90 LowRA 1.90 7.13 0.562 4.94 6.16 0.583 4.22 34.05 11.74 26.49 39.19 16.84 26.35 QLoRA 1.80 LoftQ 1.80 LowRA 1.80 7.50 0.553 5.24 6.48 0.575 4.59 33.29 11.19 25.85 39.20 16.69 26.07 QLoRA 1.75 LoftQ 1.75 LowRA 1.75 7.76 0.548 5.43 6.65 0.569 4.76 33.09 11.05 25.69 38.54 16.38 25.99 Table 2: Performance comparison of different methods on LLaMA-2-7B, LLaMA-2-13B, and BART-large. means this method fails to support this level of precision. DNC means fine-tuning fails to converge. LowRA (using PEFT) not only outperforms QLoRA and LoftQ in terms of performance-precision trade-off, but also enables us to fine-tune LLMs in the sub-2-bit range. Both QLoRA and LoftQ use NormalFloats [9]. LoftQ results on Bart-Large are taken as the best of two strategies: (1) layers are ordered based sheerly on layer-index and (2) encoder layers are ordered before decoder layers. See Appendix K for detailed results. Also, see Appendix B for ablation analysis. sizes but observe discrepancies with its published results due to: (1) reproducibility constraints the original authors did not release experiment seeds or hyperparameters used in mixed-precision trainings (2) hardware differences Our experiments run on a single A100 GPU, whereas LoftQ was trained on 8 A100 GPUs with greater data parallelism - and (3) quantization implementation The original LoftQ experiments rely on simulated quantization, which introduces discrepancies in quantized model weights, as noted by the research community10. In contrast, we employ CUDA-kernel-based quantization and dequantization, ensuring more accurate and hardware-aligned results. 7.3 Analysis of Key Results Table 2 presents the performance comparison of LowRA against QLoRA and LoftQ. Better performance at the same precision: LowRA outperforms QLoRA and LoftQ across 10 12 all sub-4-bit precision levels. In particular, at challenging 2-bit quantization, LowRA achieves a perplexity reduction of: 2.21 (WikiText-2) 1.45 (Open-Assistant) over QLoRA, and 1.76 (WikiText-2) 1.12 (Open-Assistant) over LoftQ. Same performance at lower precision: LowRA enables fine-tuning with 0.98 bits (QLoRA) 0.76 bits (LoftQ) fewer per parameter (on average) without performance loss. For example, 2.5-bit LowRA on WikiText-2 (LLaMA-2-7B) matches 4-bit QLoRA; 1.9-bit LowRA on Open-Assistant (LLaMA-2-7B) matches 2.5-bit LoftQ. 7.4 Fine-Tuning LLMs with Ultra-Low Bits We further explore: (1) Fine-tuning larger LLMs, and (2) fine-tuning under ultra-low-bit precision. Results for LLaMA-30B at 1.15 and 1.25 bits are in Table 3. LowRA is the first method to enable accurate LoRA fine-tuning at ultra-low-bit levels: 1.75 bits on LLaMA-2-7B11, LLaMA-2-13B, and BART-large, or 1.15 bits on LLaMA-30B. 7.5 Memory Implications Following the analysis methodology in QLoRA [9], we evaluate the memory footprint of LowRA at different quantization precisions. Full visualizations are in Appendix C. Our results show that LowRA significantly reduces memory usage for both fine-tuning and inference, making ultra-low-bit LoRA practical on resource-constrained hardware. For inference, reducing precision from 4-bit to 2-bit leads to 40 lower memory usage on LLaMA-2-13B and 30 on LLaMA-2-7B (Figures 8). Compressing LLaMA-30B to 1.15 or 1.25 bits achieves even greater savings, reducing the memory footprint by 50 (Figure 9). For fine-tuning, LowRA also achieves substantial reductions. Moving from 4-bit to 2-bit precision cuts memory consumption by 30 on LLaMA-2-13B and 25 on LLaMA-2-7B (Figures 7). On LLaMA-30B, reducing precision to 1.15 or 1.25 bits per parameter leads to an estimated 45 reduction in fine-tuning memory usage, making it feasible to train larger models under stricter memory constraints. These memory savings are particularly impactful given that 4-bit QLoRA models are already highly compressed. By pushing below 2-bit precision with minimal performance loss, LowRA enables fine-tuning and deployment of LLMs on significantly smaller devices. For instance, a fine-tuned LLaMA-2-7B model can now be deployed on a Raspberry Pi 4 Model B (4GB RAM) [12], making on-device inference feasible even in extreme resource-constrained settings. More strikingly, LowRA is the first method to enable LLaMA-30B fine-tuning on a single NVIDIA Tesla T4 (16GB VRAM) [8], demonstrating its potential for democratizing large-scale LLM adaptation. Bit Dataset Method QLoRA LoftQ LowRA 1.25 WikiText-2 7.46 Open-Assistant 5.44 1.15 WikiText-2 8.00 Open-Assistant 5.73 Table 3: Performance of different methods on LLaMA-33B. LowRA allows us to fine-tune LLMs at a precision level of as low as 1.15 bits, without significantly sacrificing accuracy. 111.5-bit Llama-2-7B gives 9.38 perplexity on Wikitext2 13 8 Conclusion and Future Work As LLMs scale, fine-tuning remains computationally and memory intensive, even with parameter- efficient methods like LoRA. We introduced LowRA, the first framework to enable accurate LoRA fine-tuning below 2 bits per parameter. By addressing key limitations in quantized LoRA, LowRA leverages fine-grained precision assignment, adaptive quantization mappings, and optimized CUDA kernels to minimize memory overhead while preserving performance. Extensive evaluations show that LowRA achieves a superior performance precision trade-off above 2 bits and remains accurate at even 1.15 bits per parameter, reducing memory usage by up to 50 . This enables fine-tuning in ultra-resource-constrained environments, making LLMs more accessible for real-world applications. Looking ahead, LowRA paves the way for ultra-low-bit fine-tuning and deployment. We hope it inspires further research and brings efficient LLM fine-tuning and inference to mobile devices, embedded systems, and beyond. References [1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. [2] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020. [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [4] Yuji Chai, Mujin Kwen, David Brooks, and Gu-Yeon Wei. Flexquant: Elastic quantization framework for locally hosted llm on edge devices, 2025. [5] Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy. Punica: Multi-tenant lora serving. Proceedings of Machine Learning and Systems, 6:1 13, 2024. [6] NVIDIA Corporation. Tesla P100 GPU Accelerator. data-center tesla-p100 , 2016. Accessed: 2025-01-29. [7] NVIDIA Corporation. Tesla V100 GPU Accelerator Datasheet. content technologies volta pdf tesla-volta-v100-datasheet-letter-fnl-web.pdf, 2017. Accessed: 2025-01-29. [8] NVIDIA Corporation. NVIDIA T4 Virtualization Datasheet. com content dam en-zz Solutions design-visualization solutions resources documents1 Datasheet_NVIDIA_T4_Virtualization.pdf, 2021. Accessed: 2025-01-29. [9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 14 [11] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. [12] Raspberry Pi Foundation. Raspberry Pi 4 Model B. products raspberry-pi-4-model-b , 2019. Accessed: 2025-01-29. [13] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015. [14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [15] Minda Hu, Bowei He, Yufei Wang, Liangyou Li, Chen Ma, and Irwin King. Mitigating large language model hallucination with faithful finetuning. arXiv preprint arXiv:2406.11267, 2024. [16] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning, pages 4466 4475. PMLR, 2021. [17] Hyesung Jeon, Yulhwa Kim, and Jae-joon Kim. L4q: Parameter efficient quantization-aware training on large language models via lora-wise lsq. arXiv preprint arXiv:2402.04902, 2024. [18] Virginia Klema and Alan Laub. The singular value decomposition: Its computation and some applications. IEEE Transactions on automatic control, 25(2):164 176, 1980. [19] Andreas K opf, Yannic Kilcher, Dimitri von R utte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich ard Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024. [20] Mike Lewis. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [21] Mike Lewis and Facebook AI. BART-Large Model Card. bart-large, 2023. Accessed: [Jan 26th 2025]. [22] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024. [23] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models. arXiv preprint arXiv:2310.08659, 2023. [24] Baohao Liao, Christian Herold, Shahram Khadivi, and Christof Monz. Apiq: Finetuning of 2-bit quantized large language model. arXiv preprint arXiv:2402.05147, 2024. [25] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 15 [26] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023. [27] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950 1965, 2022. [28] Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform- to-uniform quantization: Towards accurate quantization via generalized straight-through estimation. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition, pages 4942 4952, 2022. [29] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129 137, 1982. [30] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv preprint arXiv:2110.07577, 2021. [31] Joel Max. Quantizing for minimum distortion. IRE Transactions on Information Theory, 6(1):7 12, 1960. [32] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024. [33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [34] Stuart Mitchell, Michael OSullivan, and Iain Dunning. Pulp: a linear programming toolkit for python. The University of Auckland, Auckland, New Zealand, 65:25, 2011. [35] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018. [36] Oleksiy Ostapenko, Zhan Su, Edoardo Maria Ponti, Laurent Charlin, Nicolas Le Roux, Matheus Pereira, Lucas Caccia, and Alessandro Sordoni. Towards modular llms by building and reusing a library of loras. arXiv preprint arXiv:2405.11157, 2024. [37] Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, and Michele Magno. Accurate lora-finetuning quantization of llms via information retention. arXiv preprint arXiv:2402.05445, 2024. [38] Matthew J Saltzman. Coin-or: an open-source library for optimization. Programming languages and systems in computational economics and finance, pages 3 32, 2002. [39] Pedro Savarese, Xin Yuan, Yanjing Li, and Michael Maire. Not all bits have equal value: Heterogeneous precisions via trainable noise. Advances in Neural Information Processing Systems, 35:35769 35782, 2022. 16 [40] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815 8821, 2020. [41] Xuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang Li, Ming Lin, Chao Wu, and Yanzhi Wang. Agile-quant: Activation-guided quantization for faster inference of llms on the edge, 2023. [42] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, et al. Slora: Scalable serving of thousands of lora adapters. Proceedings of Machine Learning and Systems, 6:296 311, 2024. [43] Fuwen Tan, Royson Lee, Lukasz Dudziak, Shell Xu Hu, Sourav Bhattacharya, Timothy Hospedales, Georgios Tzimiropoulos, and Brais Martinez. Mobilequant: Mobile-friendly quantization for on-device language models, 2024. [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ee Lacroix, Baptiste Rozi ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [46] Shaowen Wang, Linxi Yu, and Jian Li. Lora-ga: Low-rank adaptation with gradient approxi- mation. arXiv preprint arXiv:2407.05000, 2024. [47] Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, and Xipeng Qiu. Bitstack: Any-size compression of large language models in variable memory environments, 2025. [48] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600 nlp tasks. arXiv preprint arXiv:2204.07705, 2022. [49] A Waswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A Gomez, L Kaiser, and I Polosukhin. Attention is all you need. In NIPS, 2017. [50] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [51] Huanrui Yang, Lin Duan, Yiran Chen, and Hai Li. Bsq: Exploring bit-level sparsity for mixed-precision neural network quantization. arXiv preprint arXiv:2102.10462, 2021. [52] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. [53] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196 209, 2024. 17 [54] Cyrus Zhou, Vaughn Richard, Pedro Savarese, Zachary Hassman, Michael Maire, Michael DiBrino, and Yanjing Li. Sysmol: A hardware-software co-design framework for ultra-low and fine-grained mixed-precision neural networks. arXiv preprint arXiv:2311.14114, 2023. [55] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 18 A LowRA System Support for Low-Bit Fine-Grained LoRA Fine- tuning In this appendix, we provide an overview of the CUDA-based system support we built for supporting low-bit fine-grained quantization and dequantization for LoRA fine-tuning. We specifically introduce the Quantize and Dequantize Kernels for low-bit fine-grained LoRA fine-tuning. We integrate this into the bitsandbytes12 library for usability. 2 4 1 1 2 4 4 2 4 1 1 2 4 2 2 Write per-group absmax absmax array Original Weight Quantized Weight Precs absmax group Quant Decision Trees Feed Into Write to normalize Precs determine write element size Figure 5: Overview of Kernel for Low-Bit Fine-Grained Quantzation. Indexing logic omitted for simplicity. 2 4 1 1 2 4 4 2 4 1 1 2 4 2 2 Write per-group absmax absmax array Precs Precs determine read size Dequant Lookup Tables Index into Write to normalize Original Weight absmax group Quantized Weight scale Figure 6: Overview of Kernel for Low-Bit Fine-Grained Dequantization. Indexing logic omitted for simplicity. A.1 Quantization (Figure 5) During the quantization process, the kernel operates on each block of data from the weight tensor by loading it and computing its maximum magnitude, which is then stored and used for normalization. Next, each value in the block is quantized according to its channel s bit precision (defined by Precs) 12 19 and channel-specific decision boundaries (provided as arrays of decision trees). Finally, the packed, quantized results are written to the output buffer at the appropriate offset. A.2 Dequantization (Figure 6) During the dequantization process, each thread in the kernel calculates its offset into the quantized buffer based on the channel s bit precision (Precs) and then loads the necessary packed bytes. Using the block s absmax array, the thread rescales the dequantized values, which are obtained from a per-channel lookup table indexed by the unpacked quantized values. Depending on the precision (1, 2, or 4 bits), the kernel unpacks bits from the packed bytes, retrieves the corresponding float from the channel-specific lookup table, multiplies by absmax, and writes the final results into the output tensor. 20 B Ablation Studies In this appendix, we perform ablation studies on the different components of LowRA. In particular, we compare the fine-tuning results with QLoRA, LoftQ, ours with only the precision assigner, and ours with both the precision assigner and the mapping threshold searcher. Table 4 reports results of Bart-Large on XSUM and CNN DailyMail. Table 5 reports results of Llama-2-7B on Wikitext2. Technique Metric XSUM CNN DailyMail 2 2.25 2.5 3 4 2 2.25 2.5 3 4 QLoRA ROUGE1 DNC 16.3727 15.3282 17.6011 39.0742 4.8420 11.8987 13.6767 15.3374 41.1846 ROUGE2 DNC 2.2212 1.9712 2.6801 16.3124 0.0040 1.3188 1.0364 1.1239 18.3249 ROUGEL DNC 12.8367 12.5547 13.9294 31.0891 4.3608 10.2535 9.9929 10.4375 27.5773 LoftQ ROUGE1 31.8941 32.6153 33.7645 36.0222 40.3429 38.8866 39.3648 39.8138 40.4684 41.1247 ROUGE2 10.1775 10.7005 11.7335 13.4883 17.0615 16.4935 16.8711 17.1934 17.7529 18.2853 ROUGEL 24.5908 25.1533 26.2388 28.1558 31.9186 25.8534 26.2877 26.5726 26.8812 27.5372 Ours, PA Only ROUGE1 31.8941 36.4856 37.2861 38.1543 40.3429 38.8866 40.3669 40.6187 41.1820 41.1247 ROUGE2 10.1775 13.6310 14.3775 15.1856 17.0615 16.4935 17.6272 17.8600 18.3966 18.2853 ROUGEL 24.5908 28.4593 29.2036 29.9965 31.9186 25.8534 26.7442 26.9107 27.4180 27.5372 Ours, PA MTSearch ROUGE1 36.7454 37.2915 37.6897 38.8396 40.2669 40.1489 41.0133 40.8819 40.8470 40.9493 ROUGE2 13.9324 14.3641 14.7587 15.6822 17.1800 17.4843 18.1898 18.0609 18.1191 18.1223 ROUGEL 28.6133 29.1175 29.5275 30.5712 32.0614 26.6717 27.2268 27.0113 27.2309 27.5392 Table 4: Comparison of ROUGE scores with Bart-Large on XSUM and CNN DailyMail under different techniques and combinations of techniques. PA refers to the two-level precision assigner. MTSearch refers to mapping and threshold search. Technique Metric Wikitext2 1.5 1.75 1.8 1.9 2 2.25 2.5 3 4 QLoRA Perplexity - - - - 9.17 8.67 8.05 7.13 6.22 Accuracy - - - - 0.526 0.534 0.546 0.566 0.583 LoftQ Perlexity - - - - 8.63 8.22 7.72 6.87 5.26 Accuracy - - - - 0.536 0.543 0.552 0.571 0.613 PA Only Perplexity ND ND ND ND 8.63 8.22 7.75 6.75 5.26 Accuracy ND ND ND ND 0.536 0.542 0.550 0.570 0.613 PA MTSearch Perplexity 9.38 7.76 7.50 7.13 6.60 6.40 6.23 5.84 5.25 Accuracy 0.521 0.548 0.553 0.562 0.574 0.578 0.582 0.593 0.6123 Table 5: Comparison of perplexity and accuracy of Llama2-7B on the Wikitext2 dataset under different techniques and combinations of techniques. ND indicates no data, - indicates the techniques fail to support these precisions. PA refers to the two-level precision assigner. MTSearch refers to mapping and threshold search. We observe generally that the precision assigner gives a significant advantage for Bart-Large on summarization summarization tasks (Table 4), whereas the mapping threshold searcher yields significant gains for Llama-2-7B on Wikitext2 (Table 5). Interestingly, the precision assigner only yields minimal advantage over LoftQ for Llama-2-7B on Wikitext2. In terms of applying the mapping threshold searcher to Bart-Large (Table 4), we see a less significant gain in the 2.5-4.0 precision range in comparison to the gains in the 2.0-2.5 precision range. This is in line with our intuition as at higher precision there are more mappings and thresholds for capturing distributions during quantization and dequantization; thereby, the tolerance for a less accurate combination of mappings and thresholds is higher. We encourage future works to build upon LowRA and study the optimal precision assignment and mapping threshold search algorithms for different types of architectures (i.e., encoder-decoder, encoder-only, decoder-only). We will provide easy integration to new advances in our open-sourced repository. 21 C Memory Requirements In this appendix, we provide a detailed breakdown of memory footprints. Figure 7 shows the fine-tuning memory footprint decompositions for Llama-2-7B and Llama-2-13B. Figure 8 shows the inference memory decomposition for Llama-2-7B and Llama-2-13B. Figure 9 shows both inference and fine-tuning memory footprint decomposition for Llama-33B. For finetuning, we follow QLoRA s setup of using a batch size of 1 and sequence length of 512. Number labels on the bars are in MegaBytes (MB). Estimations are linear layer only (not attention). 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 3309 3348 3425 3502 3695 3888 4274 5046 306 306 306 306 306 306 306 306 288 288 288 288 288 288 288 288 1152 1152 1152 1152 1152 1152 1152 1152 Llama-2-7B 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 5073 5148 5300 5451 5829 6207 6964 8476 479 479 479 479 479 479 479 479 450 450 450 450 450 450 450 450 1800 1800 1800 1800 1800 1800 1800 1800 Llama-2-13B 5.5 6.0 6.5 7.0 7.5 Perplexity 4.75 5.00 5.25 5.50 5.75 6.00 6.25 6.50 Perplexity Model LoRA Weight Grad Optimizer Input Grad Perplexity Figure 7: Decomposition of finetuning memory footprint for Llama-2 7B and 13B under different bits per parameter. 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 3309 3348 3425 3502 3695 3888 4274 5046 306 306 306 306 306 306 306 306 Llama-2-7B 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 5073 5148 5300 5451 5829 6207 6964 8476 479 479 479 479 479 479 479 479 Llama-2-13B 5.5 6.0 6.5 7.0 7.5 Perplexity 4.75 5.00 5.25 5.50 5.75 6.00 6.25 6.50 Perplexity Model Adapters Figure 8: Decomposition of inference memory footprint for Llama-2 7B and 13B under different bits per parameter. 22 1.15 1.25 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 8395 8778 19302 932 932 932 Inference Memory Usage 1.15 1.25 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 8395 8778 19302 932 932 932 888 888 888 3510 3510 3510 Finetune Memory Usage Model LoRA Weight Grad Optimizer Input Grad Figure 9: Decomposition of memory footprint for Llama-33B under different bits per parameter. 23 D Low-Rank Initializers: LoftQ vs PiSSA Both LoftQ [23] and PiSSa [32] use an iterative two-step process to enhance LoRA finetuning by exploiting low-rank structure and quantizing the residual. In each iteration: 1. Low-Rank Decomposition: A singular value decomposition (SVD) of the current weight (or an updated version of it) is performed to factor out a low-rank approximation. 2. Residual Quantization: The remaining component (i.e., the difference between the original weight and the low-rank approximation) is quantized to preserve overall model capacity with fewer bits. The key distinction lies in how these two steps are ordered: LoftQ first quantizes the residual, which at the beginning is simply the full base weight. Thus, it initializes by treating the entire unmodified weight as a residual to be quantized and only then proceeds with the low-rank factorization in subsequent iterations. PiSSA starts by performing SVD on the unquantized base weight, extracting a low-rank representation before any quantization. Only after factoring out the low-rank component does PiSSa quantize the remaining residual. Below (Table 6) are the experimental results covering the full 2.0-to-4.0 range comparing these two initialization techniques. From our replication, LoftQ consistently outperforms PiSSa in terms of final task performance. We adopt LoftQ s mixed precision scheme as a result. Setup Llama-7b Llama-13b Method Dataset 2.0 2.25 2.5 3.0 4.0 2.0 2.25 2.5 3.0 4.0 PiSSA WikiText-2 ( ) 1919.63 795.88 1938.33 1397.26 5.53 1825.68 1822.62 1769.34 1549.48 5.05 LoftQ 8.63 8.22 7.72 6.87 5.26 7.27 6.96 5.7 5.91 4.79 Table 6: Perplexities of PiSSA and LoftQ as initialization methods on WikiText-2 covering full range from 2.0 to 4.0. Lower values indicate better performance ( ). 24 E Overheads of Mapping Thresholds Searcher and Precision As- signer In this appendix, we report the overhead incurred by LowRA s newly added components. Note that LowRA s mapping threshold learner and precision learner can both be done offline. The former only needs to be run for each model architecture, while the latter only needs to be run for each combination of model architecture and user-specified precision requirement. Mapping Threshold Learner Overhead We timed the overhead of our mapping threshold learner on a single NVIDIA A100 SXM GPU with GPU memory. For each precision, we ran 2 iterations of the Lloyd-Max algorithm, which we found sufficient to push down the MSE and enhance task performance. From the average of 10 runs, each run on Bart-Large, Llama2-7B, and Llama2-13B takes 111.46, 309.87, and 464.92 seconds, respectively. Solver Overhead We build our two-level ILP pipeline using the opensourced Coin-Or Branch and Cut (CBC) [38] solver via the Python-based modeling library PuLP [34]. The experiments were run on a server with 2x Intel Xeon Gold 6342 CPUs. The solver uses 8 threads in parallel. We timed the overhead of running this two-level ILP pipeline for each combination of model architecture and precision requirement. From the average of 10 runs, each run on Bart-Large, Llama2-7B, and Llama2-13B takes 48.99 seconds, 319.13 seconds, and 665.93 seconds, respectively. Note that an exact (i.e., one-step) solver for the same problem would fail to finish in a reasonable amount of time. 25 F Initialization of the Mapping and Threshold Learner We attach the mappings and thresholds we use for the weighted Lloyd-Max algorithm in Listing 1. Listing 1 Initializations for bit-precision mappings and thresholds. mappings_4bit_init torch.tensor( [[-1.0, -0.6961928 , -0.5250731 , -0.3949175 , -0.28444138 , -0.18477343 , -0.09105 , 0.0, 0.0795803 , 0.1609302 , 0.2461123 , 0.33791524 , 0.44070983 , 0.562617 , 0.72295684 , 1.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) thresholds_2bit_init torch.tensor( [[-0.5, 0.16895762 , 0.66895762]] , dtype torch.float32 , device device ). repeat(nchannels , 1) mappings_2bit_init torch.tensor( [[-1.0, 0.0, 0.3379 , 1.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) thresholds_1bit_init torch.tensor( [[0.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) mappings_1bit_init torch.tensor( [[-1.0, 1.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) 26 G Weighted Lloyd-Max Algorithm In this subsection, we briefly introduce the Weighted Lloyd-Max Algorithm, extended from the original Lloyd-Max algorithm [29, 31]. Let {xi}N i 1 Rd be data points with corresponding weights {wi}N i 1, wi 0. We seek to find K cluster centers {yj}K j 1 Rd minimizing the weighted mean-squared error: min {c(i)},{yj} N X i 1 wi xi yc(i) 2, where c(i) {1, . . . , K} is the cluster index assigned to xi. The weighted Lloyd s algorithm alternates between: 1. Assignment (E-step): Assign each data point xi to the cluster center closest in Euclidean distance: c(i) arg min 1 j K xi yj 2. (E-step) 2. Update (M-step): Recompute each cluster center yj as the weighted centroid of the points assigned to it: yj P i:c(i) j wi xi P i:c(i) j wi . (M-step) These steps are repeated until convergence or until a stopping criterion (e.g., a maximum number of iterations) is met. H Parameter Variances Along Output Channel Dimension vs Along Input Channel Dimension Figure 10 compares weight variance patterns across different layer types by examining the standard deviation along both input and output channel dimensions. We observe that the standard deviation is consistently higher along the output channel dimension than the input channel dimension. down_proj gate_proj k_proj o_proj q_proj up_proj v_proj 0 2 4 6 8 Avg. Std Dev x 10e-5 Out Dim In Dim Figure 10: Standard Deviations Along Output Channel Dimension vs Along Input Channel Dimen- sion, Grouped by Layer Type. 27 I Experiment Hyperparamter Setup In this appendix, we lay out the hyperparameters used for different experiments. Table 7 details the settings for Llama on the Wikitext-2 dataset. Table 8 provides the configuration used for fine-tuning Bart-Large on CNN DailyMail, and Table 9 does the same for XSUM. Finally, Table 10 describes the hyperparameters for Llama on the Open Assistant (oasst1) dataset. Hyperparameter Value model name or path meta-llama Llama-2-7b-hf data seed 42 evaluation strategy steps eval dataset size 1024 max eval samples 1000 per device eval batch size 4 dataloader num workers 3 lora r 64 lora alpha 64 lora modules all bf16 True warmup ratio 0.03 lr scheduler type cosine gradient checkpointing True dataset wikitext dataset config wikitext-2-raw-v1 per device train batch size 16 gradient accumulation steps 4 max steps 126 eval steps 20 learning rate 0.0003 adam beta2 0.999 max grad norm 0.3 weight decay 0.1 seed 0 block size 1024 Table 7: Hyperparameters used for all Llama experiments on Wikitext-2. 28 Hyperparameter Value learning rate 1e-4 seed 11 dataset name cnn dailymail dataset config 3.0.0 pad to max length True max source length 512 num train epochs 15 per device train batch size 8 per device eval batch size 32 gradient accumulation steps 32 model name or path facebook bart-large evaluation strategy epoch predict with generate True Table 8: Hyperparameters for fine-tuning Bart-Large on CNN DailyMail. Hyperparameter Value learning rate 1e-4 seed 11 dataset name xsum dataset config 3.0.0 pad to max length True max source length 512 num train epochs 25 per device train batch size 4 per device eval batch size 32 gradient accumulation steps 32 model name or path facebook bart-large evaluation strategy epoch Table 9: Hyperparameters for fine-tuning Bart-Large on XSUM. 29 Hyperparameter Value model name or path meta-llama Llama-2-7b-hf data seed 42 evaluation strategy steps eval dataset size 1024 max eval samples 500 per device eval batch size 1 max new tokens 32 dataloader num workers 3 group by length True logging strategy steps remove unused columns False lora r 64 lora alpha 64 lora modules all bf16 True warmup ratio 0.03 lr scheduler type constant gradient checkpointing True dataset oasst1 source max len 16 target max len 512 per device train batch size 4 gradient accumulation steps 4 max steps 1875 eval steps 200 learning rate 0.0002 adam beta2 0.999 max grad norm 0.3 lora dropout 0.1 weight decay 0.0 seed 0 Table 10: Hyperparameters used for all Llama experiments on Open Assistant (oasst1). 30 J Github Issues Related To The Lack of A Practical Quantization Primitive The use of NF fake quantization in LoftQ Discrepancy between real model weights and expected model weights due to fake quantization in LoftQ The use of NF fake quantization in LoftQ Unrealized GPU memory saving due to fake quantization in LoftQ Unrealized LLM model size reduction due to fake quantization in PiSSA K Detailed LoftQ Results on Bart-Large In this appendix, Table 11 presents results for LoftQ-based fine-tuning under two interpretations: (1) ordering layers by index (Layer Index), and (2) treating encoder layers as preceding decoder layers (Encoder First). For fairness, we select the best-performing interpretation as the LoftQ baseline. Note that for average precision 2.0 and 4.0, both interpretations lead to the same implementation. Interestingly, Encoder First consistently performs better on XSUM, whereas Layer Index generally achieves higher scores on CNN DailyMail. Setup Bart-Large Method Dataset 2.0 2.25 2.5 3.0 4.0 Layer Index XSUM( ) 31.89 10.18 24.59 32.62 10.70 25.15 33.76 11.73 26.24 36.02 13.49 28.16 40.34 17.06 31.92 Encoder First 32.72 10.94 25.38 34.49 12.26 27.05 37.23 14.34 29.31 Layer Index CNN DailyMail( ) 38.89 16.49 25.85 39.36 16.87 26.29 39.81 17.19 26.57 40.47 17.75 26.88 41.12 18.29 27.54 Encoder First 39.27 16.84 26.26 39.53 17.05 26.45 39.89 17.36 26.73 Table 11: ROUGE-1 ROUGE-2 ROUGE-L results of Bart-Large fine-tuned with LoftQ. Columns labeled 2.0, 2.25, 2.5, 3.0, and 4.0 refer to the average bits per parameter (bpp) or average precision. Higher scores indicate better performance. 31\n\n=== SEGMENTS ===\n\n--- Segment 1 ---\nLowRA: Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits Zikai Zhou1, Qizheng Zhang1, Hermann Kumbong1, Kunle Olukotun1,2 1Department of Computer Science, Stanford University 2Department of Electrical Engineering, Stanford University February 13, 2025 Abstract Fine-tuning large language models (LLMs) is increasingly costly as models scale to hundreds of billions of parameters, and even parameter-efficient fine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce LowRA, the first framework to enable LoRA fine-tuning below 2 bits per parameter with minimal performance loss. LowRA optimizes fine-grained quantization mapping, threshold selection, and precision assignment while leveraging efficient CUDA kernels for scalable deployment. Extensive evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior performance precision trade-off above 2 bits and remains accurate down to 1.15 bits, reducing memory usage by up to 50 . Our results highlight the potential of ultra-low-bit LoRA fine-tuning for resource-constrained environments. 1 Introduction Fine-tuning large language models (LLM) can enhance their performance on particular tasks [50, 48, 55], and remove unwanted behaviors like hallucinations [15, 26] and harmful responses [3, 1]. Yet as models scale - e.g., Llama 3.1 with 405 billion parameters [10] and DeepSeek-V3 with 671 billion parameters [25] - the cost of fine-tuning soars [14]. To reduce fine-tuning costs, parameter-efficient fine-tuning (PEFT) methods [52, 14, 30, 27] freeze a model s core weights and insert small trainable modules. We focus on LoRA [14], which adds rank-decomposed adapters to cut computation and memory demands. Still, fine-tuning large models on a single GPU can exceed memory limits. Quantized LoRA approaches (e.g., QLoRA [9], LoftQ [23]) resolve this by quantizing the base weights with minimal loss in accuracy, enabling billions-parameter models to be fine-tuned on standard single GPUs or even mobile devices.\n\n--- Segment 2 ---\nStill, fine-tuning large models on a single GPU can exceed memory limits. Quantized LoRA approaches (e.g., QLoRA [9], LoftQ [23]) resolve this by quantizing the base weights with minimal loss in accuracy, enabling billions-parameter models to be fine-tuned on standard single GPUs or even mobile devices. Despite the success of quantized LoRA in cutting memory usage, all existing works focus on LoRA fine-tuning within the range of 2 to 4 bits (per parameter), many of which are incompatible with ultra-low-bit LoRA fine-tuning [46, 32, 9]. Further pushing down the bits (per parameter) requirement, e.g., below 2 bits, has profound implications in fine-tuning and deployment in ultra- low-resource scenarios like embedded devices [41, 4] and mobile phones [47, 43]. However, current methods face three fundamental limitations: L1: focus exclusively on coarse-grained quantization of the base model weights. 1 arXiv:2502.08141v1 [cs.LG] 12 Feb 2025 L2: leverage quantization functions (i.e., mappings and thresholds) that assume some fixed data distribution across the entire model weight. L3: rely on simulated quantization, with no system support for efficient low-bit quantization. To unleash the full potential of quantized LoRA fine-tuning, we present LowRA, an accurate and efficient framework that enables LoRA fine-tuning down to below 2 bits (per parameter). LowRA features three major components for each of the three challenges: (1) mapping threshold function search, (2) fine-grained precision assignment, and (3) CUDA-based kernels as quantization primitives. Addressing L1 and L2 requires extra care because LoRA base weights have to work with multiple sets of adapters in real-life settings [42, 36, 5]. This constraint demands a powerful, task-agnostic quantization technique. Furthermore, optimally assigning precisions at a fine-grained granularity for LLMs calls for a scalable, low-complexity solution to handle massive parameter spaces. LowRA meets the need through a hierarchical ILP(Integer Linear Programming)-based precision assigner for performing fine-grained mixed-precision.\n\n--- Segment 3 ---\nFurthermore, optimally assigning precisions at a fine-grained granularity for LLMs calls for a scalable, low-complexity solution to handle massive parameter spaces. LowRA meets the need through a hierarchical ILP(Integer Linear Programming)-based precision assigner for performing fine-grained mixed-precision. Moreover, LowRA provides a weighted Lloyd-Max [29, 31] formulation of mapping threshold search for groupwise normalization, and achieves strong practical performance through its efficient solution. We conduct extensive evaluation of LowRA across 4 choices of widely used base LLMs and 4 choices of natural language applications, and compare LowRA against state-of-the-art baselines. Evaluation results demonstrate that LowRA: (1) achieves a superior performance-precision trade-off above 2 bits (per parameter) compared to baselines, and is the first method to enable accurate, efficient LoRA fine-tuning below 2 bits, (2) enables substantial memory footprint reduction in fine-tuning, and (3) incurs minimal additional overhead even with newly added components. In summary, we make the following contributions: Identifying Key Limitations: We identify three core limitations in existing quantized LoRA approaches, highlighting opportunities to exploit fine-grained precision assignment and mappings thresholds. Design and Implementation of LowRA: We introduce LowRA, an accurate, end-to-end framework that applies fine-grained quantization to LoRA fine-tuning of LLMs. We detail its key design choices including a mappings thresholds learner, precision assigner, and practical programming primitives ensuring both efficiency and high performance. Better Performance-Precision Trade-Off: Comprehensive evaluations show that LowRA outperforms baselines in performance-precision trade-off, enabling an average 0.86-bit reduction per parameter without sacrificing performance. Memory Footprint Reduction: LowRA cuts memory usage by 30 50 during fine-tuning and deployment with minimal performance loss. Moreover, it enables fine-tuning and deploying LLMs in ultra-resource-constrained settings at as low as 1.15 bits. Open-Source Framework: We will open-source our framework and artifacts to spur further research in ultra-low-bit LoRA fine-tuning. 2 This paper is organized as follows: Section 2 introduces LoRA fine-tuning, quantization for LoRA, and three key limitations of existing quantized LoRA methods.\n\n--- Segment 4 ---\nOpen-Source Framework: We will open-source our framework and artifacts to spur further research in ultra-low-bit LoRA fine-tuning. 2 This paper is organized as follows: Section 2 introduces LoRA fine-tuning, quantization for LoRA, and three key limitations of existing quantized LoRA methods. Section 3 presents the LowRA end-to-end workflow, while Section 4 discusses its key design insights and benefits. Sections 5 and 6 detail the mapping threshold learner and the fine-grained precision assignment, respectively. Finally, we describe our evaluation setup, results, and takeaways in Section 7. 2 Background and Motivation Pretrained Weight (T1) A B Intelligently Initialized Low-rank Tensors (T5) Mapping and Threshold Learner (P1) Fine-grained Mappings and Thresholds Per-output-channel Mixed-Precision Downquant Residual (T4) A B Final Low-rank Tensors (T6) f FP32 FP16 BF16 4-bit 2-bit 1-bit Programs Tensors Precision Assigner (P2) (T2) Precs (T3) Output- channel- wise Quantize Kernel (P3) Low-Rank Initializer (P4) Further Finetuning (P5) Output- channel- wise Dequantize Kernel (P5.1) f Out In User DeÔ¨Åned Compression Ratio Figure 1: End-to-end workflow of LowRA. 2.1 Low-Rank Adaptation (LoRA) of LLMs Fine-tuning large language models (LLMs) allows us to adapt pre-trained LLMs to particular tasks or domains [50, 48, 55]. This process usually requires changing all model parameters, which can be prohibitively expensive (in terms of compute and memory) when the number of model parameters increases. Low-Rank Adaptation (LoRA) [14] tackles this by freezing the base weights and introducing a small set of trainable adapter parameters, drastically reducing memory and compute requirements for fine-tuning. 2.2 Quantization for LoRA Fine-Tuning Quantized LoRA fine-tuning further cuts memory usage by quantizing the base model weights without hurting performance.\n\n--- Segment 5 ---\nLow-Rank Adaptation (LoRA) [14] tackles this by freezing the base weights and introducing a small set of trainable adapter parameters, drastically reducing memory and compute requirements for fine-tuning. 2.2 Quantization for LoRA Fine-Tuning Quantized LoRA fine-tuning further cuts memory usage by quantizing the base model weights without hurting performance. QLoRA [9] introduces a NormalFloat format to backpropagate through a 4-bit quantized backbone, while LoftQ [23] and ApiQ [24] jointly optimize quantized base weights and adapter initializations under a unified objective. These advances unlock fine-tuning and deployment of LLMs on low-resource platforms like embedded devices [41, 4] and mobile phones [47, 43]. 2.3 Limitations of Existing Quantized LoRA Methods Despite the early promise of recent work in quantized LoRA like QLoRA and LoftQ, we observe three major limitations that prevent us from fully realizing the potential of memory-efficient LLM fine-tuning. 3 L1: Coarse-Grained Precision Assignment. Existing approaches typically apply a single quantization precision to an entire weight matrix or multiple layers. For instance, QLoRA uses uniform 4-bit quantization across all base weights, while LoftQ adopts a layerwise mixed-precision scheme (e.g., higher precision for earlier layers, lower for later layers). Our findings ( 6) suggest that truly unlocking ultra-low-bit fine-tuning requires a finer-grained assignment strategy, potentially at the sub-layer or sub-matrix level. L2: Discrepancy in Data Distribution. Most quantized LoRA methods use a globally shared data format such as QLoRA s NormalFloat, which assumes a roughly normal distribution. However, Figure 2 reveals that groupwise normalization at a per-channel level often deviates significantly from a global normal distribution. To preserve accuracy, more localized quantization dequantization approaches are needed.\n\n--- Segment 6 ---\nHowever, Figure 2 reveals that groupwise normalization at a per-channel level often deviates significantly from a global normal distribution. To preserve accuracy, more localized quantization dequantization approaches are needed. 1.0 0.5 0.0 0.5 1.0 0 100 200 300 400 500 Layer 0 self_attn_v_proj Channel 1214 1.0 0.5 0.0 0.5 1.0 0 250 500 750 1000 1250 Layer 0 self_attn_q_proj Channel 2082 Histogram Normal PDF 1.0 0.5 0.0 0.5 1.0 0 100 200 300 Layer 0 self_attn_q_proj Channel 723 Normalized Value Frequency Figure 2: Distributions of normalized parameters in different output channels sampled from the first layer of Llama2-7b. L3: Lack of High-Performance Quantization Primitives. Most quantized LoRA methods and related quantization studies [23, 37, 40, 2] rely on simulated quantization 1, lacking native hardware support for sub-4-bit or flexible mixed-precision operations. For instance, LoftQ requires eight A100 GPUs even for smaller LLMs. Such reliance on simulation inflates resource requirements and impedes practical deployment (see Appendix J for related Github issues), as no existing system offers efficient low-bit or adaptive-precision kernels tailored to LoRA. 3 The LowRA Framework In this section, we provide an overview of the LowRA end-to-end workflow, illustrated in Figure 1. The process begins with the pretrained model weights (T1). We feed each layer of these weights into a dedicated mapping and thresholds learner (P1), which produces optimized per-output-channel mappings and thresholds, denoted (T2). These mappings and thresholds, along with the pretrained weights, are then processed by a two-step ILP quantizer (P2) to determine the optimal precision assignments (T3) for each output channel. Next, the output-channel-wise quantize kernel (P3), which supports custom quantization thresholds, uses the derived thresholds (T2) and the assigned precision levels (T3) to quantize the weights. We calculate the quantization errors arising from this step and apply low-rank tensor initialization (P4).\n\n--- Segment 7 ---\nNext, the output-channel-wise quantize kernel (P3), which supports custom quantization thresholds, uses the derived thresholds (T2) and the assigned precision levels (T3) to quantize the weights. We calculate the quantization errors arising from this step and apply low-rank tensor initialization (P4). Techniques for intelligent low-rank initialization include LoftQ [23] and PiSSa [32] (Appendix D) which generates low-rank tensors (T5) designed to absorb quantization errors 1using floating-point values to mimic discrete quantization levels throughout training or fine-tuning. 4 during initialization. In our implementation, we opt for LoftQ [23] as experiments show that they give better performance in the low-bit range. The resulting mixed-precision quantized weights (T4) and the initialized low-rank tensors (T5) are then passed to a fine-tuning module (P5), which relies on an output-channel-wise dequantize kernel (P5.1) to recover the base weights from their quantized form. Following the approach used in LoRA and QLoRA, the base weights (T4) remain frozen, and only the low-rank tensors (T5) are trained. Finally, we obtain the updated low-rank tensors (T6), which are output together with the quantized base weights (and associated quantization state) (T4). 4 Discussion about Design Choices In this section, we discuss various design choices in the LowRA framework, as well as system and hardware support. 4.1 Insights behind LowRA Design Choices Per-Output-Channel Quantization In LLMs, linear layers often exhibit substantially more variation across output channels than across input channels. For instance, in Llama2-7b [44], the average standard deviation along the output channel is 2.20 higher than along the input channel (see Appendix H). As a result, grouping parameters by output channel and assigning a unique precision to each group i.e., per-output-channel quantization more effectively captures their diverse distributions. Groupwise Normalization Each output channel may still exhibit significant internal variability even with per-output-channel quantization. To address this, groupwise normalization is often used to allow each group of elements share a separate scale.\n\n--- Segment 8 ---\nGroupwise Normalization Each output channel may still exhibit significant internal variability even with per-output-channel quantization. To address this, groupwise normalization is often used to allow each group of elements share a separate scale. We follow QLoRA s design of using 64-element normalization scaled by the absmax (i.e., maximum absolute value) in each group [9]. Data-Free Post-Training Quantization Unlike quantization-aware training (QAT) [11, 51, 17, 39], our approach adds no overhead to fine-tuning. By automatically searching for quantization mappings and thresholds, it frees users from manual tuning [39, 54], saving both development and computation resources. Moreover, contrary to some methods that vary compression ratios over time [39, 51], LowRA maintains a consistent compression ratio, ensuring persistent memory savings during fine-tuning. Per-Output-Channel Thresholds and Mappings Figure 3 visualizes the roles of thresholds and mappings in the process of quantization. Thresholds refer to the boundary points ( bin edges ) that partition the continuous domain of normalized parameters into discrete intervals and thus specific bitstring encodings. Mappings, on the other hand, specify the representative values assigned to each encoded bitstring and thus the intervals. As discussed in 2.3, fine-grained designs of quantization mappings and thresholds could lead to significantly more accurate approximation and reconstruction of parameters. LowRA allows each output channel to adopt a different combination of mappings and thresholds for more precise fine-grained quantization. Data-Free One-Shot Post-Training Quantization Most quantization-aware training (QAT) techniques achieve higher task performance by incurring additional training overhead and learning task-specific quantization parameters [11, 51, 17, 39]. Similarly, many post-training quantization 5 -1.00 1.00 Thresholds Mappings Original Quantized -0.92 -0.31 0.46 0.78 -0.76 -0.12 0.63 Figure 3: Roles of mappings and thresholds in quantization. Circles represent thresholds whereas crosses represent mappings. Colored Triangles represent the process of converting a range of original unquantized real values - partitioned by thresholds - to the mapped values corresponding to each quantization level. methods require a calibration set for quantization scheme learning [24, 16].\n\n--- Segment 9 ---\nColored Triangles represent the process of converting a range of original unquantized real values - partitioned by thresholds - to the mapped values corresponding to each quantization level. methods require a calibration set for quantization scheme learning [24, 16]. In contrast, LowRA uses data-free one-shot post-training quantization, enabling reusable quantization schemes and quantized parameters, minimal hyperparameter tuning, and negligible fine-tuning overhead. This design is particularly suited to LoRA fine-tuning because: (1) Task-dependent learning is confined to the adapters, (2) LoRA base weights are often shared across multiple adapters, and (3) LoRA primarily targets resource-constrained fine-tuning scenarios. User-Defined Compression Ratios Quantized LoRA methods see heavy use in tight resource settings - e.g., limited-memory GPUs or on-device scenarios - where specifying a precise compression ratio is pivotal. By tailoring each parameter s bit precision, thresholds, and mappings, LowRA directly aligns compression with real-world resource budgets, ensuring feasibility and efficiency even under strict constraints. Furthermore, because LowRA fixes the ratio in a single pass, it obviates the extensive hyper-parameter tuning needed by alternative methods to find acceptable compression accuracy trade-offs [39, 54]. Using LoftQ as Low-Rank Intializer Researchers have found that the initialization of low-rank tensors is crucial to the effectiveness of LoRA fine-tuning, especially when it comes to ultra-low- bit quantized base weight [23, 32, 46]. LoftQ [23] and PiSSA [32] are two notable initialization techniques for quantized LoRA (see Appendix D for a detailed introduction). While PiSSA purports faster convergence than LoftQ, our experiments consistently show LoftQ outperforming PiSSA. As illustrated by the sample data points in Table 1, PiSSA fails to achieve reasonable task performance at lower bit ranges. This aligns with our intuition that performing quantization rather than SVD first allows the low-rank tensors to better absorb quantization errors. Our findings also corroborate points raised in the LoftQ appendix. Since our main objective is to enable lower-precision fine-tuning and deployment, we opt to use LoftQ as our low-rank initializer. Following the recommendation from LoftQ, we use five alternating steps for initialization.\n\n--- Segment 10 ---\nSince our main objective is to enable lower-precision fine-tuning and deployment, we opt to use LoftQ as our low-rank initializer. Following the recommendation from LoftQ, we use five alternating steps for initialization. Adapting LowRA to Production Use Cases Many production use cases, e.g., batched inference in data centers, require fixed quantization mappings [22, 53]. LowRA can be adapted to such scenarios by keeping only the thresholds learnable, which is shown to be useful in enhancing model performance [28]. To maximize performance with task-agnostic reusable base weight, LowRA can be extended to use the same set of thresholds for multiple adapters but learn mappings for each downstream task. In other words, each adapter can be connected to the base weight together 6 Setup Llama-7b Llama-13b Method Dataset 2-bit 4-bit 2-bit 4-bit PiSSA WikiText-2 ( ) 1919.63 5.53 1825.68 5.05 LoftQ 8.63 5.26 7.27 4.79 Table 1: Perplexities of PiSSA and LoftQ as initialization methods on WikiText-2. Quantization is performed at 2 bits or 4 bits per parameter. Lower values indicate better performance ( ). with a dedicated base weight decoding mapping for that downstream task. Nevertheless, this would demand higher development and computation costs. In our implementation and experiments, we adopt the same set of learned thresholds and learned mappings for a single base weight for the proof of concept. 4.2 System and Hardware Support Building on the limitations noted in 2.3, we implement practical CUDA-based primitives that support both low-bit and mixed-precision LoRA fine-tuning with maximum flexibility (details in Appendix A). Notably, the added kernel generalization incurs only negligible overhead in end-to-end inference, as quantization dequantization constitutes a minimal portion of the total compute cost. 5 Mapping and Threshold Learner In this section, we introduce the mapping and threshold learner in LowRA. Because we want the final base weights to remain task-agnostic and thus reusable across multiple adapters, we adopt a simple approach that minimizes the mean squared error2 (MSE) in each output channel.\n\n--- Segment 11 ---\n5 Mapping and Threshold Learner In this section, we introduce the mapping and threshold learner in LowRA. Because we want the final base weights to remain task-agnostic and thus reusable across multiple adapters, we adopt a simple approach that minimizes the mean squared error2 (MSE) in each output channel. As discussed in 4.1, one could learn separate decoding mappings for each downstream task (or adapter set), but at a higher fine-tuning cost. We therefore propose an efficient design for the mapping threshold learner that avoids this expense. Weighted Lloyd-Max Algorithm. We cast the problem of searching for the optimal quantization mappings and thresholds for each output channel to minimize MSE as a Weighted Lloyd-Max Problem. A detailed description of this algorithm can be found in Appendix G. Weighted Lloyd s for LoRA Quantization. As discussed in 4.1, we perform groupwise normalization to give more scale to quantization within each output channel. To recap, with groupwise normalization (Section 4.1), each block of weights is scaled by the block-wise maximum absolute value (absmax). Specifically, if we denote the set of original weights in a block by {xi} and its maximum absolute value by absmax, then we treat absmax as a per-block weight in the Weighted Lloyd-Max algorithm. By assigning them proportionally larger weights, the algorithm pays more attention to those blocks and adjusts their bin thresholds and centroids accordingly. Consequently, blocks whose values have smaller magnitudes (and thus smaller absmax) are penalized less, striking a balance across all blocks to minimize the overall quantization error in QLoRA. In our application of the Weighted Lloyd s algorithm to LoRA base weight quantization, we initialize the thresholds as those used by NormalFloats [9] for 2-bit and 4-bit precisions and use 2We define SE (Squared Error) as (ÀÜy y)2, i.e., the squared difference between the predicted and ground-truth values. Therefore, MSE is the mean of these squared errors over the dataset. 7 0.0 as the initial threshold for 1-bit quantization34.\n\n--- Segment 12 ---\nTherefore, MSE is the mean of these squared errors over the dataset. 7 0.0 as the initial threshold for 1-bit quantization34. Then, at each iteration, we recompute the quantization mappings as the weighted centroids of the assigned data and recompute the thresholds as the midpoints between consecutive mapping (centroid) values5. We output the last-computed quantization mappings and thresholds when max iteration is reached or the MSE stops going down. In our current implementation, we take the average of all thresholds to preserve distribution and prevent instability in the interaction with the Low-Rank Initializer. 6 Mixed-Precision Quantization: Channelwise Precision Assign- ment In this section, we present how mixed-precision quantization assignment is conducted in LowRA. In light of the aforementioned task-agnostic requirement, a simple yet effective objective for defining this problem is the minimization of the overall Summed Square Error6 (SSE) considering the regular structure of transformer-based architectures [49, 10, 44]. Such formulation can serve as an effective proxy to retain more information for the harder-to-quantize channels in weights. One can observe that finding the optimal mixed-precision scheme (w.r.t. SSE) can be formulated as an ILP. However, due to the large number of output channels in LLMs, a direct solver-based approach becomes prohibitively expensive. For instance, solving more than five layers of LLaMA-2- 7B fails to finish within ten hours. To address this limitation, we propose a two-level ILP workflow (Figure 4, Algorithm 1) that retains the benefits of ILP-based methods while ensuring reasonable complexity. Algorithm 1 Channelwise Precision Assignment 1: Input: N, distinct w(1), . . . , w(K), partition {Ik}, MSE(i, p), total budget Btotal 2: Step 1. Compute Wk P i Ik wi, then Wsum PK k 1 Wk 3: Step 2. Bk Btotal Wk Wsum for k 1, . . . , K 4: for k 1 to K do 5: Step 3. Cluster channels in Ik (e.g. K-Means on MSE features) into Kk clusters 6: Step 4.\n\n--- Segment 13 ---\nCluster channels in Ik (e.g. K-Means on MSE features) into Kk clusters 6: Step 4. Cluster-Level ILP: decide how many channels in each cluster get each bitwidth, subject to Bk 7: Step 5. Intra-Cluster ILP: within each cluster, assign specific channels to bitwidths 8: end for 9: Step 6. Combine final bitwidths into b1, . . . , bN 10: Output: (b1, . . . , bN), total SSE, actual bits used 6.1 Preprocessing for the Pipeline (Step 1-3) To preprocess channels for the hierarchical ILP pipeline, we first compute each channel s MSE under 1-, 2-, and 4-bit quantization. Next, we split channels by parameter count, which in LLMs typically yields two distinct sizes (e.g., 4096 and 11008 for LLaMA-2-7B). Within each group, we then apply three-dimensional K-Means clustering7 (based on the three computed MSE values), forming 128 3Separately defined as NormalFloats lack a 1-bit representation 4See the initial values in Appendix F 5In our implementation, we set number of iterations to 2 6We define SSE (Summed Square Error) as the sum of the squared differences between the original values and their quantized counterparts, i.e., P i xi bxi 2. 7We use 300 as the maximum number of iterations 8 clusters per group in our implementation. 6.2 Cluster-Level ILP (Step 4) Formed clusters first go through the following cluster-level ILP to be assigned budgets of 1-bit, 2-bit, and 4-bit channels. Consider C clusters, each with Sc channels c 1, . . . , C . Let P {1, 2, 4} be the available bit-precisions8.\n\n--- Segment 14 ---\n, C . Let P {1, 2, 4} be the available bit-precisions8. For each cluster c and precision p P, define: costc,p: mean quantization error (e.g., mean-squared error) per channel in cluster c if all channels in that cluster are assigned to precision p, scaled by the number of weight parameters per channel in that cluster 9. yc,p Z 0: decision variable representing the number of channels in cluster c that will be assigned precision p. We define a global bit-budget B (i.e., total permissible bits across all clusters). Let Œ≤(p) be the bit-precision value (e.g., Œ≤(1) 1, Œ≤(2) 2, Œ≤(4) 4). To enforce the bit budget, we multiply Œ≤(p) by the channel parameter count œâc for cluster c, and then by the number of channels yc,p. As we split channels based on the number of parameters within each channel, each channel in a cluster c shares the same œâc. Minimize C X c 1 X p P costc,p yc,p subject to X p P yc,p Sc, c 1, . . . , C, C X c 1 X p P Œ≤(p) œâc yc,p B, yc,p Z 0, 0 yc,p Sc. This formulation seeks to minimize the total weighted quantization error by choosing, for each cluster c, how many of its channels yc,p are assigned to each precision level p. The constraints ensure that every channel of a cluster is allocated exactly once, the total bits used do not exceed the overall budget B, and that the decision variables remain non-negative integers and do not exceed the number of channels in their respective clusters. 6.3 Intra-Cluster ILP (Step 5) Once the cluster-level ILP decides how many channels {yc,p} in each cluster c should be assigned to each bit precision p, a second ILP distributes these assignments to each channel within each cluster. Let Sc denote the total number of channels in cluster c. For channel i {1, . . .\n\n--- Segment 15 ---\n. . , Sc} in cluster c, we define the precomputed mean-squared error at precision p as MSE(i, p) (precomputed quantization error of channel i at precision p). We define binary decision variables xi,p 1 if channel i is assigned bit precision p; otherwise, xi,p 0. 8For LLaMA, restricting to 2 and 4 bits outperformed including 1 bit for bpp 2.0, so we adopt this configuration. 9Contrary to LoftQ s suggestion, per-layer cost weighting based on layer index proved suboptimal in our experiments. 9 Minimize Sc X i 1 X p P MSE(i, p) xi,p subject to X p P xi,p 1 i {1, . . . , Sc}, Sc X i 1 xi,p yc,p p P, xi,p {0, 1} i {1, . . . , Sc}, p P. This formulation constitutes the intra-cluster ILP. The objective minimizes the total quantization error, where MSE(i, p) is the precomputed mean-squared error for channel i at bit precision p. The first constraint ensures that each channel is assigned to exactly one precision. The second constraint enforces that the number of channels assigned to each precision p matches the counts yc,p determined by the cluster-level ILP. Finally, the binary constraint stipulates that each decision variable xi,p is either 0 or 1. This intra-cluster ILP enforces that the required number of channels (from the cluster-level ILP) is assigned to each bit precision and minimizes local MSE within the cluster. Efficiently Leveraging ILP Solvers. In Step 6, we collect the assigned per-channel precisions. By employing this two-step hierarchical approach, we capitalize on the strengths of ILP solvers while maintaining minimal computational overhead. 7 Evaluation We evaluate LowRA across four datasets spanning natural language generation, multi-turn conver- sation, and long-context text summarization, demonstrating that: Better performance at the same precision: LowRA outperforms all baselines below 4-bit and matches their performance at 4-bit ( 7.3). Same performance at lower precision: LowRA achieves comparable performance while reducing precision by 0.86 bits per parameter on average ( 7.3).\n\n--- Segment 16 ---\n7 Evaluation We evaluate LowRA across four datasets spanning natural language generation, multi-turn conver- sation, and long-context text summarization, demonstrating that: Better performance at the same precision: LowRA outperforms all baselines below 4-bit and matches their performance at 4-bit ( 7.3). Same performance at lower precision: LowRA achieves comparable performance while reducing precision by 0.86 bits per parameter on average ( 7.3). First method to fine-tune LoRA under 2 bits: LowRA enables fine-tuning down to 1.75 bits on LLaMA-2-7B, LLaMA-2-13B, and BART-large, and 1.15 bits on LLaMA-30B ( 7.4). Substantial memory savings: LowRA reduces memory usage by 30 50 in fine-tuning and deployment, with minimal performance loss compared to QLoRA ( 7.5). Minimal overhead: The additional one-time preprocessing costs in LowRA are negligible (Appendix E). 7.1 Evaluation Setup Hardware Platform Experiments are conducted on NVIDIA A100 GPUs (80GB memory). Each LLaMA experiment runs on a single dedicated GPU. Each BART-large experiment runs two instances concurrently on a single GPU. Hyperparameters For a fair comparison, we use identical hyperparameters across all methods, consistent with QLoRA [9] and LoftQ [23]. Details on selected hyperparameters are in Appendix I. 10 Channelwise Param Counts Channelwise Quant Costs (MSEs) Channelwise Quant Costs (A) Channelwise Param Counts (A) Channelwise Quant Costs (B) Channelwise Param Counts (B) Split on Param Counts K-Means Clustering K-Means Clustering Cluster-level ILP Per-Cluster Precision Assignments Clusters Group A Clusters Group B Intra-cluster ILP Channelwise Precision Assignment Bit Budget Figure 4: Two-step ILP-based Workflow for Channelwise Precision Assignment Language Models We evaluate LowRA on a range of LLMs: LLaMA-2-7B, LLaMA-2-13B [45], BART-large [20], and LLaMA-30B [44] (to assess ultra-low-bit scalability).\n\n--- Segment 17 ---\nDetails on selected hyperparameters are in Appendix I. 10 Channelwise Param Counts Channelwise Quant Costs (MSEs) Channelwise Quant Costs (A) Channelwise Param Counts (A) Channelwise Quant Costs (B) Channelwise Param Counts (B) Split on Param Counts K-Means Clustering K-Means Clustering Cluster-level ILP Per-Cluster Precision Assignments Clusters Group A Clusters Group B Intra-cluster ILP Channelwise Precision Assignment Bit Budget Figure 4: Two-step ILP-based Workflow for Channelwise Precision Assignment Language Models We evaluate LowRA on a range of LLMs: LLaMA-2-7B, LLaMA-2-13B [45], BART-large [20], and LLaMA-30B [44] (to assess ultra-low-bit scalability). Datasets and Evaluation Metrics We use standard datasets across different NLP tasks: WikiText-2 [33] (language modeling, perplexity), Open-Assistant [19] (multi-turn conversation, perplexity), XSUM [35] (summarization, ROUGE scores), and CNN DailyMail [13] (summarization, ROUGE scores). Each dataset is evaluated using the standard metrics used in prior work. 7.2 Baselines QLoRA QLoRA originally employs a fixed 4-bit quantization for pretrained LLMs and does not support fine-tuning below 4 bits. To enable sub-4-bit QLoRA experiments, we follow the adaptation introduced in LoftQ. Additionally, QLoRA directly quantizes the pretrained weights while preserving their original distribution, initializing the low-rank tensors with zeros and small Gaussian noise. LoftQ LoftQ performs mixed-precision quantization (2-bit 4-bit) and jointly optimizes both quantized LLM weights and low-rank adapter initialization. We match LoftQ s effective batch 11 Method Bit LLaMA-2-7B LLaMA-2-13B BART-large WikiText-2 OASST1 WikiText-2 OASST1 XSUM CNN DailyMail ppl. acc. ppl. ppl. acc. ppl.\n\n--- Segment 18 ---\nacc. ppl. ROUGE1 ROUGE2 ROUGEL QLoRA 4.00 6.22 0.583 3.52 4.79 0.628 3.25 39.07 16.31 31.09 41.18 18.32 27.58 LoftQ 4.00 5.26 0.613 3.48 4.79 0.628 3.23 40.34 17.06 31.92 41.12 18.29 27.54 LowRA 4.00 5.25 0.612 3.48 4.79 0.628 3.23 40.27 17.18 32.06 40.95 18.12 27.54 QLoRA 3.00 7.13 0.566 4.56 6.06 0.588 3.88 17.60 2.68 13.93 15.34 1.12 10.44 LoftQ 3.00 6.87 0.571 4.42 5.91 0.591 3.79 37.23 14.34 29.30 40.47 17.75 26.88 LowRA 3.00 5.84 0.593 3.87 5.24 0.611 3.50 38.84 15.68 30.57 40.85 18.12 27.23 QLoRA 2.50 8.05 0.546 5.17 6.84 0.568 4.36 15.33 1.97 12.55 13.68 1.04 9.99 LoftQ 2.50 7.72 0.552 4.98 6.70 0.572 4.21 34.48 12.26 27.05 39.81 17.19 26.57 LowRA 2.50 6.23 0.582 4.11 5.51 0.601 3.64 37.69 14.76 29.53 40.88 18.06 27.01 QLoRA 2.25 8.67 0.534 5.59 7.31 0.588 4.64 16.37 2.22 12.84 11.90 1.32 10.25 LoftQ 2.25 8.22 0.543 5.24 6.96 0.564 4.46 32.71 10.94 25.37 39.36 16.87 26.29 LowRA 2.25 6.40 0.578 4.21 5.66 0.597 3.73 37.29 14.36 29.12 41.01 18.19 27.23 QLoRA 2.00 9.17 0.526 6.07 7.64 0.551 5.02 DNC 4.84 0.00 4.36 LoftQ 2.00 8.63 0.536 5.68 7.27 0.558 4.75 31.89 10.18 24.59 38.88 16.49 25.85 LowRA 2.00 6.60 0.574 4.35 5.79 0.593 3.84 36.75 13.93 28.61 40.15 17.48 26.67 QLoRA 1.90 LoftQ 1.90 LowRA 1.90 7.13 0.562 4.94 6.16 0.583 4.22 34.05 11.74 26.49 39.19 16.84 26.35 QLoRA 1.80 LoftQ 1.80 LowRA 1.80 7.50 0.553 5.24 6.48 0.575 4.59 33.29 11.19 25.85 39.20 16.69 26.07 QLoRA 1.75 LoftQ 1.75 LowRA 1.75 7.76 0.548 5.43 6.65 0.569 4.76 33.09 11.05 25.69 38.54 16.38 25.99 Table 2: Performance comparison of different methods on LLaMA-2-7B, LLaMA-2-13B, and BART-large.\n\n--- Segment 19 ---\nppl. ROUGE1 ROUGE2 ROUGEL QLoRA 4.00 6.22 0.583 3.52 4.79 0.628 3.25 39.07 16.31 31.09 41.18 18.32 27.58 LoftQ 4.00 5.26 0.613 3.48 4.79 0.628 3.23 40.34 17.06 31.92 41.12 18.29 27.54 LowRA 4.00 5.25 0.612 3.48 4.79 0.628 3.23 40.27 17.18 32.06 40.95 18.12 27.54 QLoRA 3.00 7.13 0.566 4.56 6.06 0.588 3.88 17.60 2.68 13.93 15.34 1.12 10.44 LoftQ 3.00 6.87 0.571 4.42 5.91 0.591 3.79 37.23 14.34 29.30 40.47 17.75 26.88 LowRA 3.00 5.84 0.593 3.87 5.24 0.611 3.50 38.84 15.68 30.57 40.85 18.12 27.23 QLoRA 2.50 8.05 0.546 5.17 6.84 0.568 4.36 15.33 1.97 12.55 13.68 1.04 9.99 LoftQ 2.50 7.72 0.552 4.98 6.70 0.572 4.21 34.48 12.26 27.05 39.81 17.19 26.57 LowRA 2.50 6.23 0.582 4.11 5.51 0.601 3.64 37.69 14.76 29.53 40.88 18.06 27.01 QLoRA 2.25 8.67 0.534 5.59 7.31 0.588 4.64 16.37 2.22 12.84 11.90 1.32 10.25 LoftQ 2.25 8.22 0.543 5.24 6.96 0.564 4.46 32.71 10.94 25.37 39.36 16.87 26.29 LowRA 2.25 6.40 0.578 4.21 5.66 0.597 3.73 37.29 14.36 29.12 41.01 18.19 27.23 QLoRA 2.00 9.17 0.526 6.07 7.64 0.551 5.02 DNC 4.84 0.00 4.36 LoftQ 2.00 8.63 0.536 5.68 7.27 0.558 4.75 31.89 10.18 24.59 38.88 16.49 25.85 LowRA 2.00 6.60 0.574 4.35 5.79 0.593 3.84 36.75 13.93 28.61 40.15 17.48 26.67 QLoRA 1.90 LoftQ 1.90 LowRA 1.90 7.13 0.562 4.94 6.16 0.583 4.22 34.05 11.74 26.49 39.19 16.84 26.35 QLoRA 1.80 LoftQ 1.80 LowRA 1.80 7.50 0.553 5.24 6.48 0.575 4.59 33.29 11.19 25.85 39.20 16.69 26.07 QLoRA 1.75 LoftQ 1.75 LowRA 1.75 7.76 0.548 5.43 6.65 0.569 4.76 33.09 11.05 25.69 38.54 16.38 25.99 Table 2: Performance comparison of different methods on LLaMA-2-7B, LLaMA-2-13B, and BART-large. means this method fails to support this level of precision.\n\n--- Segment 20 ---\nROUGE1 ROUGE2 ROUGEL QLoRA 4.00 6.22 0.583 3.52 4.79 0.628 3.25 39.07 16.31 31.09 41.18 18.32 27.58 LoftQ 4.00 5.26 0.613 3.48 4.79 0.628 3.23 40.34 17.06 31.92 41.12 18.29 27.54 LowRA 4.00 5.25 0.612 3.48 4.79 0.628 3.23 40.27 17.18 32.06 40.95 18.12 27.54 QLoRA 3.00 7.13 0.566 4.56 6.06 0.588 3.88 17.60 2.68 13.93 15.34 1.12 10.44 LoftQ 3.00 6.87 0.571 4.42 5.91 0.591 3.79 37.23 14.34 29.30 40.47 17.75 26.88 LowRA 3.00 5.84 0.593 3.87 5.24 0.611 3.50 38.84 15.68 30.57 40.85 18.12 27.23 QLoRA 2.50 8.05 0.546 5.17 6.84 0.568 4.36 15.33 1.97 12.55 13.68 1.04 9.99 LoftQ 2.50 7.72 0.552 4.98 6.70 0.572 4.21 34.48 12.26 27.05 39.81 17.19 26.57 LowRA 2.50 6.23 0.582 4.11 5.51 0.601 3.64 37.69 14.76 29.53 40.88 18.06 27.01 QLoRA 2.25 8.67 0.534 5.59 7.31 0.588 4.64 16.37 2.22 12.84 11.90 1.32 10.25 LoftQ 2.25 8.22 0.543 5.24 6.96 0.564 4.46 32.71 10.94 25.37 39.36 16.87 26.29 LowRA 2.25 6.40 0.578 4.21 5.66 0.597 3.73 37.29 14.36 29.12 41.01 18.19 27.23 QLoRA 2.00 9.17 0.526 6.07 7.64 0.551 5.02 DNC 4.84 0.00 4.36 LoftQ 2.00 8.63 0.536 5.68 7.27 0.558 4.75 31.89 10.18 24.59 38.88 16.49 25.85 LowRA 2.00 6.60 0.574 4.35 5.79 0.593 3.84 36.75 13.93 28.61 40.15 17.48 26.67 QLoRA 1.90 LoftQ 1.90 LowRA 1.90 7.13 0.562 4.94 6.16 0.583 4.22 34.05 11.74 26.49 39.19 16.84 26.35 QLoRA 1.80 LoftQ 1.80 LowRA 1.80 7.50 0.553 5.24 6.48 0.575 4.59 33.29 11.19 25.85 39.20 16.69 26.07 QLoRA 1.75 LoftQ 1.75 LowRA 1.75 7.76 0.548 5.43 6.65 0.569 4.76 33.09 11.05 25.69 38.54 16.38 25.99 Table 2: Performance comparison of different methods on LLaMA-2-7B, LLaMA-2-13B, and BART-large. means this method fails to support this level of precision. DNC means fine-tuning fails to converge.\n\n--- Segment 21 ---\nmeans this method fails to support this level of precision. DNC means fine-tuning fails to converge. LowRA (using PEFT) not only outperforms QLoRA and LoftQ in terms of performance-precision trade-off, but also enables us to fine-tune LLMs in the sub-2-bit range. Both QLoRA and LoftQ use NormalFloats [9]. LoftQ results on Bart-Large are taken as the best of two strategies: (1) layers are ordered based sheerly on layer-index and (2) encoder layers are ordered before decoder layers. See Appendix K for detailed results. Also, see Appendix B for ablation analysis. sizes but observe discrepancies with its published results due to: (1) reproducibility constraints the original authors did not release experiment seeds or hyperparameters used in mixed-precision trainings (2) hardware differences Our experiments run on a single A100 GPU, whereas LoftQ was trained on 8 A100 GPUs with greater data parallelism - and (3) quantization implementation The original LoftQ experiments rely on simulated quantization, which introduces discrepancies in quantized model weights, as noted by the research community10. In contrast, we employ CUDA-kernel-based quantization and dequantization, ensuring more accurate and hardware-aligned results. 7.3 Analysis of Key Results Table 2 presents the performance comparison of LowRA against QLoRA and LoftQ. Better performance at the same precision: LowRA outperforms QLoRA and LoftQ across 10 12 all sub-4-bit precision levels. In particular, at challenging 2-bit quantization, LowRA achieves a perplexity reduction of: 2.21 (WikiText-2) 1.45 (Open-Assistant) over QLoRA, and 1.76 (WikiText-2) 1.12 (Open-Assistant) over LoftQ. Same performance at lower precision: LowRA enables fine-tuning with 0.98 bits (QLoRA) 0.76 bits (LoftQ) fewer per parameter (on average) without performance loss. For example, 2.5-bit LowRA on WikiText-2 (LLaMA-2-7B) matches 4-bit QLoRA; 1.9-bit LowRA on Open-Assistant (LLaMA-2-7B) matches 2.5-bit LoftQ.\n\n--- Segment 22 ---\nSame performance at lower precision: LowRA enables fine-tuning with 0.98 bits (QLoRA) 0.76 bits (LoftQ) fewer per parameter (on average) without performance loss. For example, 2.5-bit LowRA on WikiText-2 (LLaMA-2-7B) matches 4-bit QLoRA; 1.9-bit LowRA on Open-Assistant (LLaMA-2-7B) matches 2.5-bit LoftQ. 7.4 Fine-Tuning LLMs with Ultra-Low Bits We further explore: (1) Fine-tuning larger LLMs, and (2) fine-tuning under ultra-low-bit precision. Results for LLaMA-30B at 1.15 and 1.25 bits are in Table 3. LowRA is the first method to enable accurate LoRA fine-tuning at ultra-low-bit levels: 1.75 bits on LLaMA-2-7B11, LLaMA-2-13B, and BART-large, or 1.15 bits on LLaMA-30B. 7.5 Memory Implications Following the analysis methodology in QLoRA [9], we evaluate the memory footprint of LowRA at different quantization precisions. Full visualizations are in Appendix C. Our results show that LowRA significantly reduces memory usage for both fine-tuning and inference, making ultra-low-bit LoRA practical on resource-constrained hardware. For inference, reducing precision from 4-bit to 2-bit leads to 40 lower memory usage on LLaMA-2-13B and 30 on LLaMA-2-7B (Figures 8). Compressing LLaMA-30B to 1.15 or 1.25 bits achieves even greater savings, reducing the memory footprint by 50 (Figure 9). For fine-tuning, LowRA also achieves substantial reductions. Moving from 4-bit to 2-bit precision cuts memory consumption by 30 on LLaMA-2-13B and 25 on LLaMA-2-7B (Figures 7). On LLaMA-30B, reducing precision to 1.15 or 1.25 bits per parameter leads to an estimated 45 reduction in fine-tuning memory usage, making it feasible to train larger models under stricter memory constraints.\n\n--- Segment 23 ---\nMoving from 4-bit to 2-bit precision cuts memory consumption by 30 on LLaMA-2-13B and 25 on LLaMA-2-7B (Figures 7). On LLaMA-30B, reducing precision to 1.15 or 1.25 bits per parameter leads to an estimated 45 reduction in fine-tuning memory usage, making it feasible to train larger models under stricter memory constraints. These memory savings are particularly impactful given that 4-bit QLoRA models are already highly compressed. By pushing below 2-bit precision with minimal performance loss, LowRA enables fine-tuning and deployment of LLMs on significantly smaller devices. For instance, a fine-tuned LLaMA-2-7B model can now be deployed on a Raspberry Pi 4 Model B (4GB RAM) [12], making on-device inference feasible even in extreme resource-constrained settings. More strikingly, LowRA is the first method to enable LLaMA-30B fine-tuning on a single NVIDIA Tesla T4 (16GB VRAM) [8], demonstrating its potential for democratizing large-scale LLM adaptation. Bit Dataset Method QLoRA LoftQ LowRA 1.25 WikiText-2 7.46 Open-Assistant 5.44 1.15 WikiText-2 8.00 Open-Assistant 5.73 Table 3: Performance of different methods on LLaMA-33B. LowRA allows us to fine-tune LLMs at a precision level of as low as 1.15 bits, without significantly sacrificing accuracy. 111.5-bit Llama-2-7B gives 9.38 perplexity on Wikitext2 13 8 Conclusion and Future Work As LLMs scale, fine-tuning remains computationally and memory intensive, even with parameter- efficient methods like LoRA. We introduced LowRA, the first framework to enable accurate LoRA fine-tuning below 2 bits per parameter. By addressing key limitations in quantized LoRA, LowRA leverages fine-grained precision assignment, adaptive quantization mappings, and optimized CUDA kernels to minimize memory overhead while preserving performance. Extensive evaluations show that LowRA achieves a superior performance precision trade-off above 2 bits and remains accurate at even 1.15 bits per parameter, reducing memory usage by up to 50 .\n\n--- Segment 24 ---\nBy addressing key limitations in quantized LoRA, LowRA leverages fine-grained precision assignment, adaptive quantization mappings, and optimized CUDA kernels to minimize memory overhead while preserving performance. Extensive evaluations show that LowRA achieves a superior performance precision trade-off above 2 bits and remains accurate at even 1.15 bits per parameter, reducing memory usage by up to 50 . This enables fine-tuning in ultra-resource-constrained environments, making LLMs more accessible for real-world applications. Looking ahead, LowRA paves the way for ultra-low-bit fine-tuning and deployment. We hope it inspires further research and brings efficient LLM fine-tuning and inference to mobile devices, embedded systems, and beyond. References [1] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021. [2] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization. arXiv preprint arXiv:2012.15701, 2020. [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [4] Yuji Chai, Mujin Kwen, David Brooks, and Gu-Yeon Wei. Flexquant: Elastic quantization framework for locally hosted llm on edge devices, 2025. [5] Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy. Punica: Multi-tenant lora serving. Proceedings of Machine Learning and Systems, 6:1 13, 2024. [6] NVIDIA Corporation. Tesla P100 GPU Accelerator. data-center tesla-p100 , 2016.\n\n--- Segment 25 ---\nTesla P100 GPU Accelerator. data-center tesla-p100 , 2016. Accessed: 2025-01-29. [7] NVIDIA Corporation. Tesla V100 GPU Accelerator Datasheet. content technologies volta pdf tesla-volta-v100-datasheet-letter-fnl-web.pdf, 2017. Accessed: 2025-01-29. [8] NVIDIA Corporation. NVIDIA T4 Virtualization Datasheet. com content dam en-zz Solutions design-visualization solutions resources documents1 Datasheet_NVIDIA_T4_Virtualization.pdf, 2021. Accessed: 2025-01-29. [9] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 14 [11] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. [12] Raspberry Pi Foundation. Raspberry Pi 4 Model B. products raspberry-pi-4-model-b , 2019. Accessed: 2025-01-29. [13] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015. [14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n\n--- Segment 26 ---\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [15] Minda Hu, Bowei He, Yufei Wang, Liangyou Li, Chen Ma, and Irwin King. Mitigating large language model hallucination with faithful finetuning. arXiv preprint arXiv:2406.11267, 2024. [16] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In International Conference on Machine Learning, pages 4466 4475. PMLR, 2021. [17] Hyesung Jeon, Yulhwa Kim, and Jae-joon Kim. L4q: Parameter efficient quantization-aware training on large language models via lora-wise lsq. arXiv preprint arXiv:2402.04902, 2024. [18] Virginia Klema and Alan Laub. The singular value decomposition: Its computation and some applications. IEEE Transactions on automatic control, 25(2):164 176, 1980. [19] Andreas K opf, Yannic Kilcher, Dimitri von R utte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich ard Nagyfi, et al. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36, 2024. [20] Mike Lewis. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. [21] Mike Lewis and Facebook AI. BART-Large Model Card. bart-large, 2023. Accessed: [Jan 26th 2025]. [22] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han.\n\n--- Segment 27 ---\nAccessed: [Jan 26th 2025]. [22] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024. [23] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models. arXiv preprint arXiv:2310.08659, 2023. [24] Baohao Liao, Christian Herold, Shahram Khadivi, and Christof Monz. Apiq: Finetuning of 2-bit quantized large language model. arXiv preprint arXiv:2402.05147, 2024. [25] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 15 [26] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations, 2023. [27] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950 1965, 2022. [28] Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform- to-uniform quantization: Towards accurate quantization via generalized straight-through estimation.\n\n--- Segment 28 ---\n[28] Zechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P Xing, and Zhiqiang Shen. Nonuniform- to-uniform quantization: Towards accurate quantization via generalized straight-through estimation. In Proceedings of the IEEE CVF conference on computer vision and pattern recognition, pages 4942 4952, 2022. [29] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129 137, 1982. [30] Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian Khabsa. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv preprint arXiv:2110.07577, 2021. [31] Joel Max. Quantizing for minimum distortion. IRE Transactions on Information Theory, 6(1):7 12, 1960. [32] Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024. [33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. [34] Stuart Mitchell, Michael OSullivan, and Iain Dunning. Pulp: a linear programming toolkit for python. The University of Auckland, Auckland, New Zealand, 65:25, 2011. [35] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018. [36] Oleksiy Ostapenko, Zhan Su, Edoardo Maria Ponti, Laurent Charlin, Nicolas Le Roux, Matheus Pereira, Lucas Caccia, and Alessandro Sordoni. Towards modular llms by building and reusing a library of loras. arXiv preprint arXiv:2405.11157, 2024.\n\n--- Segment 29 ---\nTowards modular llms by building and reusing a library of loras. arXiv preprint arXiv:2405.11157, 2024. [37] Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, and Michele Magno. Accurate lora-finetuning quantization of llms via information retention. arXiv preprint arXiv:2402.05445, 2024. [38] Matthew J Saltzman. Coin-or: an open-source library for optimization. Programming languages and systems in computational economics and finance, pages 3 32, 2002. [39] Pedro Savarese, Xin Yuan, Yanjing Li, and Michael Maire. Not all bits have equal value: Heterogeneous precisions via trainable noise. Advances in Neural Information Processing Systems, 35:35769 35782, 2022. 16 [40] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 8815 8821, 2020. [41] Xuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang Li, Ming Lin, Chao Wu, and Yanzhi Wang. Agile-quant: Activation-guided quantization for faster inference of llms on the edge, 2023. [42] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, et al. Slora: Scalable serving of thousands of lora adapters. Proceedings of Machine Learning and Systems, 6:296 311, 2024. [43] Fuwen Tan, Royson Lee, Lukasz Dudziak, Shell Xu Hu, Sourav Bhattacharya, Timothy Hospedales, Georgios Tzimiropoulos, and Brais Martinez. Mobilequant: Mobile-friendly quantization for on-device language models, 2024.\n\n--- Segment 30 ---\n[43] Fuwen Tan, Royson Lee, Lukasz Dudziak, Shell Xu Hu, Sourav Bhattacharya, Timothy Hospedales, Georgios Tzimiropoulos, and Brais Martinez. Mobilequant: Mobile-friendly quantization for on-device language models, 2024. [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ee Lacroix, Baptiste Rozi ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [46] Shaowen Wang, Linxi Yu, and Jian Li. Lora-ga: Low-rank adaptation with gradient approxi- mation. arXiv preprint arXiv:2407.05000, 2024. [47] Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, and Xipeng Qiu. Bitstack: Any-size compression of large language models in variable memory environments, 2025. [48] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600 nlp tasks. arXiv preprint arXiv:2204.07705, 2022. [49] A Waswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A Gomez, L Kaiser, and I Polosukhin.\n\n--- Segment 31 ---\narXiv preprint arXiv:2204.07705, 2022. [49] A Waswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A Gomez, L Kaiser, and I Polosukhin. Attention is all you need. In NIPS, 2017. [50] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. [51] Huanrui Yang, Lin Duan, Yiran Chen, and Hai Li. Bsq: Exploring bit-level sparsity for mixed-precision neural network quantization. arXiv preprint arXiv:2102.10462, 2021. [52] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. [53] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. Proceedings of Machine Learning and Systems, 6:196 209, 2024. 17 [54] Cyrus Zhou, Vaughn Richard, Pedro Savarese, Zachary Hassman, Michael Maire, Michael DiBrino, and Yanjing Li. Sysmol: A hardware-software co-design framework for ultra-low and fine-grained mixed-precision neural networks. arXiv preprint arXiv:2311.14114, 2023. [55] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.\n\n--- Segment 32 ---\nFine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 18 A LowRA System Support for Low-Bit Fine-Grained LoRA Fine- tuning In this appendix, we provide an overview of the CUDA-based system support we built for supporting low-bit fine-grained quantization and dequantization for LoRA fine-tuning. We specifically introduce the Quantize and Dequantize Kernels for low-bit fine-grained LoRA fine-tuning. We integrate this into the bitsandbytes12 library for usability. 2 4 1 1 2 4 4 2 4 1 1 2 4 2 2 Write per-group absmax absmax array Original Weight Quantized Weight Precs absmax group Quant Decision Trees Feed Into Write to normalize Precs determine write element size Figure 5: Overview of Kernel for Low-Bit Fine-Grained Quantzation. Indexing logic omitted for simplicity. 2 4 1 1 2 4 4 2 4 1 1 2 4 2 2 Write per-group absmax absmax array Precs Precs determine read size Dequant Lookup Tables Index into Write to normalize Original Weight absmax group Quantized Weight scale Figure 6: Overview of Kernel for Low-Bit Fine-Grained Dequantization. Indexing logic omitted for simplicity. A.1 Quantization (Figure 5) During the quantization process, the kernel operates on each block of data from the weight tensor by loading it and computing its maximum magnitude, which is then stored and used for normalization. Next, each value in the block is quantized according to its channel s bit precision (defined by Precs) 12 19 and channel-specific decision boundaries (provided as arrays of decision trees). Finally, the packed, quantized results are written to the output buffer at the appropriate offset. A.2 Dequantization (Figure 6) During the dequantization process, each thread in the kernel calculates its offset into the quantized buffer based on the channel s bit precision (Precs) and then loads the necessary packed bytes. Using the block s absmax array, the thread rescales the dequantized values, which are obtained from a per-channel lookup table indexed by the unpacked quantized values.\n\n--- Segment 33 ---\nA.2 Dequantization (Figure 6) During the dequantization process, each thread in the kernel calculates its offset into the quantized buffer based on the channel s bit precision (Precs) and then loads the necessary packed bytes. Using the block s absmax array, the thread rescales the dequantized values, which are obtained from a per-channel lookup table indexed by the unpacked quantized values. Depending on the precision (1, 2, or 4 bits), the kernel unpacks bits from the packed bytes, retrieves the corresponding float from the channel-specific lookup table, multiplies by absmax, and writes the final results into the output tensor. 20 B Ablation Studies In this appendix, we perform ablation studies on the different components of LowRA. In particular, we compare the fine-tuning results with QLoRA, LoftQ, ours with only the precision assigner, and ours with both the precision assigner and the mapping threshold searcher. Table 4 reports results of Bart-Large on XSUM and CNN DailyMail. Table 5 reports results of Llama-2-7B on Wikitext2.\n\n--- Segment 34 ---\nTable 4 reports results of Bart-Large on XSUM and CNN DailyMail. Table 5 reports results of Llama-2-7B on Wikitext2. Technique Metric XSUM CNN DailyMail 2 2.25 2.5 3 4 2 2.25 2.5 3 4 QLoRA ROUGE1 DNC 16.3727 15.3282 17.6011 39.0742 4.8420 11.8987 13.6767 15.3374 41.1846 ROUGE2 DNC 2.2212 1.9712 2.6801 16.3124 0.0040 1.3188 1.0364 1.1239 18.3249 ROUGEL DNC 12.8367 12.5547 13.9294 31.0891 4.3608 10.2535 9.9929 10.4375 27.5773 LoftQ ROUGE1 31.8941 32.6153 33.7645 36.0222 40.3429 38.8866 39.3648 39.8138 40.4684 41.1247 ROUGE2 10.1775 10.7005 11.7335 13.4883 17.0615 16.4935 16.8711 17.1934 17.7529 18.2853 ROUGEL 24.5908 25.1533 26.2388 28.1558 31.9186 25.8534 26.2877 26.5726 26.8812 27.5372 Ours, PA Only ROUGE1 31.8941 36.4856 37.2861 38.1543 40.3429 38.8866 40.3669 40.6187 41.1820 41.1247 ROUGE2 10.1775 13.6310 14.3775 15.1856 17.0615 16.4935 17.6272 17.8600 18.3966 18.2853 ROUGEL 24.5908 28.4593 29.2036 29.9965 31.9186 25.8534 26.7442 26.9107 27.4180 27.5372 Ours, PA MTSearch ROUGE1 36.7454 37.2915 37.6897 38.8396 40.2669 40.1489 41.0133 40.8819 40.8470 40.9493 ROUGE2 13.9324 14.3641 14.7587 15.6822 17.1800 17.4843 18.1898 18.0609 18.1191 18.1223 ROUGEL 28.6133 29.1175 29.5275 30.5712 32.0614 26.6717 27.2268 27.0113 27.2309 27.5392 Table 4: Comparison of ROUGE scores with Bart-Large on XSUM and CNN DailyMail under different techniques and combinations of techniques.\n\n--- Segment 35 ---\nTable 5 reports results of Llama-2-7B on Wikitext2. Technique Metric XSUM CNN DailyMail 2 2.25 2.5 3 4 2 2.25 2.5 3 4 QLoRA ROUGE1 DNC 16.3727 15.3282 17.6011 39.0742 4.8420 11.8987 13.6767 15.3374 41.1846 ROUGE2 DNC 2.2212 1.9712 2.6801 16.3124 0.0040 1.3188 1.0364 1.1239 18.3249 ROUGEL DNC 12.8367 12.5547 13.9294 31.0891 4.3608 10.2535 9.9929 10.4375 27.5773 LoftQ ROUGE1 31.8941 32.6153 33.7645 36.0222 40.3429 38.8866 39.3648 39.8138 40.4684 41.1247 ROUGE2 10.1775 10.7005 11.7335 13.4883 17.0615 16.4935 16.8711 17.1934 17.7529 18.2853 ROUGEL 24.5908 25.1533 26.2388 28.1558 31.9186 25.8534 26.2877 26.5726 26.8812 27.5372 Ours, PA Only ROUGE1 31.8941 36.4856 37.2861 38.1543 40.3429 38.8866 40.3669 40.6187 41.1820 41.1247 ROUGE2 10.1775 13.6310 14.3775 15.1856 17.0615 16.4935 17.6272 17.8600 18.3966 18.2853 ROUGEL 24.5908 28.4593 29.2036 29.9965 31.9186 25.8534 26.7442 26.9107 27.4180 27.5372 Ours, PA MTSearch ROUGE1 36.7454 37.2915 37.6897 38.8396 40.2669 40.1489 41.0133 40.8819 40.8470 40.9493 ROUGE2 13.9324 14.3641 14.7587 15.6822 17.1800 17.4843 18.1898 18.0609 18.1191 18.1223 ROUGEL 28.6133 29.1175 29.5275 30.5712 32.0614 26.6717 27.2268 27.0113 27.2309 27.5392 Table 4: Comparison of ROUGE scores with Bart-Large on XSUM and CNN DailyMail under different techniques and combinations of techniques. PA refers to the two-level precision assigner.\n\n--- Segment 36 ---\nTechnique Metric XSUM CNN DailyMail 2 2.25 2.5 3 4 2 2.25 2.5 3 4 QLoRA ROUGE1 DNC 16.3727 15.3282 17.6011 39.0742 4.8420 11.8987 13.6767 15.3374 41.1846 ROUGE2 DNC 2.2212 1.9712 2.6801 16.3124 0.0040 1.3188 1.0364 1.1239 18.3249 ROUGEL DNC 12.8367 12.5547 13.9294 31.0891 4.3608 10.2535 9.9929 10.4375 27.5773 LoftQ ROUGE1 31.8941 32.6153 33.7645 36.0222 40.3429 38.8866 39.3648 39.8138 40.4684 41.1247 ROUGE2 10.1775 10.7005 11.7335 13.4883 17.0615 16.4935 16.8711 17.1934 17.7529 18.2853 ROUGEL 24.5908 25.1533 26.2388 28.1558 31.9186 25.8534 26.2877 26.5726 26.8812 27.5372 Ours, PA Only ROUGE1 31.8941 36.4856 37.2861 38.1543 40.3429 38.8866 40.3669 40.6187 41.1820 41.1247 ROUGE2 10.1775 13.6310 14.3775 15.1856 17.0615 16.4935 17.6272 17.8600 18.3966 18.2853 ROUGEL 24.5908 28.4593 29.2036 29.9965 31.9186 25.8534 26.7442 26.9107 27.4180 27.5372 Ours, PA MTSearch ROUGE1 36.7454 37.2915 37.6897 38.8396 40.2669 40.1489 41.0133 40.8819 40.8470 40.9493 ROUGE2 13.9324 14.3641 14.7587 15.6822 17.1800 17.4843 18.1898 18.0609 18.1191 18.1223 ROUGEL 28.6133 29.1175 29.5275 30.5712 32.0614 26.6717 27.2268 27.0113 27.2309 27.5392 Table 4: Comparison of ROUGE scores with Bart-Large on XSUM and CNN DailyMail under different techniques and combinations of techniques. PA refers to the two-level precision assigner. MTSearch refers to mapping and threshold search.\n\n--- Segment 37 ---\nPA refers to the two-level precision assigner. MTSearch refers to mapping and threshold search. Technique Metric Wikitext2 1.5 1.75 1.8 1.9 2 2.25 2.5 3 4 QLoRA Perplexity - - - - 9.17 8.67 8.05 7.13 6.22 Accuracy - - - - 0.526 0.534 0.546 0.566 0.583 LoftQ Perlexity - - - - 8.63 8.22 7.72 6.87 5.26 Accuracy - - - - 0.536 0.543 0.552 0.571 0.613 PA Only Perplexity ND ND ND ND 8.63 8.22 7.75 6.75 5.26 Accuracy ND ND ND ND 0.536 0.542 0.550 0.570 0.613 PA MTSearch Perplexity 9.38 7.76 7.50 7.13 6.60 6.40 6.23 5.84 5.25 Accuracy 0.521 0.548 0.553 0.562 0.574 0.578 0.582 0.593 0.6123 Table 5: Comparison of perplexity and accuracy of Llama2-7B on the Wikitext2 dataset under different techniques and combinations of techniques. ND indicates no data, - indicates the techniques fail to support these precisions. PA refers to the two-level precision assigner. MTSearch refers to mapping and threshold search. We observe generally that the precision assigner gives a significant advantage for Bart-Large on summarization summarization tasks (Table 4), whereas the mapping threshold searcher yields significant gains for Llama-2-7B on Wikitext2 (Table 5). Interestingly, the precision assigner only yields minimal advantage over LoftQ for Llama-2-7B on Wikitext2. In terms of applying the mapping threshold searcher to Bart-Large (Table 4), we see a less significant gain in the 2.5-4.0 precision range in comparison to the gains in the 2.0-2.5 precision range. This is in line with our intuition as at higher precision there are more mappings and thresholds for capturing distributions during quantization and dequantization; thereby, the tolerance for a less accurate combination of mappings and thresholds is higher.\n\n--- Segment 38 ---\nIn terms of applying the mapping threshold searcher to Bart-Large (Table 4), we see a less significant gain in the 2.5-4.0 precision range in comparison to the gains in the 2.0-2.5 precision range. This is in line with our intuition as at higher precision there are more mappings and thresholds for capturing distributions during quantization and dequantization; thereby, the tolerance for a less accurate combination of mappings and thresholds is higher. We encourage future works to build upon LowRA and study the optimal precision assignment and mapping threshold search algorithms for different types of architectures (i.e., encoder-decoder, encoder-only, decoder-only). We will provide easy integration to new advances in our open-sourced repository. 21 C Memory Requirements In this appendix, we provide a detailed breakdown of memory footprints. Figure 7 shows the fine-tuning memory footprint decompositions for Llama-2-7B and Llama-2-13B. Figure 8 shows the inference memory decomposition for Llama-2-7B and Llama-2-13B. Figure 9 shows both inference and fine-tuning memory footprint decomposition for Llama-33B. For finetuning, we follow QLoRA s setup of using a batch size of 1 and sequence length of 512. Number labels on the bars are in MegaBytes (MB). Estimations are linear layer only (not attention).\n\n--- Segment 39 ---\nNumber labels on the bars are in MegaBytes (MB). Estimations are linear layer only (not attention). 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 3309 3348 3425 3502 3695 3888 4274 5046 306 306 306 306 306 306 306 306 288 288 288 288 288 288 288 288 1152 1152 1152 1152 1152 1152 1152 1152 Llama-2-7B 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 5073 5148 5300 5451 5829 6207 6964 8476 479 479 479 479 479 479 479 479 450 450 450 450 450 450 450 450 1800 1800 1800 1800 1800 1800 1800 1800 Llama-2-13B 5.5 6.0 6.5 7.0 7.5 Perplexity 4.75 5.00 5.25 5.50 5.75 6.00 6.25 6.50 Perplexity Model LoRA Weight Grad Optimizer Input Grad Perplexity Figure 7: Decomposition of finetuning memory footprint for Llama-2 7B and 13B under different bits per parameter.\n\n--- Segment 40 ---\nEstimations are linear layer only (not attention). 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 3309 3348 3425 3502 3695 3888 4274 5046 306 306 306 306 306 306 306 306 288 288 288 288 288 288 288 288 1152 1152 1152 1152 1152 1152 1152 1152 Llama-2-7B 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 5073 5148 5300 5451 5829 6207 6964 8476 479 479 479 479 479 479 479 479 450 450 450 450 450 450 450 450 1800 1800 1800 1800 1800 1800 1800 1800 Llama-2-13B 5.5 6.0 6.5 7.0 7.5 Perplexity 4.75 5.00 5.25 5.50 5.75 6.00 6.25 6.50 Perplexity Model LoRA Weight Grad Optimizer Input Grad Perplexity Figure 7: Decomposition of finetuning memory footprint for Llama-2 7B and 13B under different bits per parameter. 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 3309 3348 3425 3502 3695 3888 4274 5046 306 306 306 306 306 306 306 306 Llama-2-7B 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 5073 5148 5300 5451 5829 6207 6964 8476 479 479 479 479 479 479 479 479 Llama-2-13B 5.5 6.0 6.5 7.0 7.5 Perplexity 4.75 5.00 5.25 5.50 5.75 6.00 6.25 6.50 Perplexity Model Adapters Figure 8: Decomposition of inference memory footprint for Llama-2 7B and 13B under different bits per parameter.\n\n--- Segment 41 ---\n1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 3309 3348 3425 3502 3695 3888 4274 5046 306 306 306 306 306 306 306 306 288 288 288 288 288 288 288 288 1152 1152 1152 1152 1152 1152 1152 1152 Llama-2-7B 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 5073 5148 5300 5451 5829 6207 6964 8476 479 479 479 479 479 479 479 479 450 450 450 450 450 450 450 450 1800 1800 1800 1800 1800 1800 1800 1800 Llama-2-13B 5.5 6.0 6.5 7.0 7.5 Perplexity 4.75 5.00 5.25 5.50 5.75 6.00 6.25 6.50 Perplexity Model LoRA Weight Grad Optimizer Input Grad Perplexity Figure 7: Decomposition of finetuning memory footprint for Llama-2 7B and 13B under different bits per parameter. 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 3309 3348 3425 3502 3695 3888 4274 5046 306 306 306 306 306 306 306 306 Llama-2-7B 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 5073 5148 5300 5451 5829 6207 6964 8476 479 479 479 479 479 479 479 479 Llama-2-13B 5.5 6.0 6.5 7.0 7.5 Perplexity 4.75 5.00 5.25 5.50 5.75 6.00 6.25 6.50 Perplexity Model Adapters Figure 8: Decomposition of inference memory footprint for Llama-2 7B and 13B under different bits per parameter. 22 1.15 1.25 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 8395 8778 19302 932 932 932 Inference Memory Usage 1.15 1.25 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 8395 8778 19302 932 932 932 888 888 888 3510 3510 3510 Finetune Memory Usage Model LoRA Weight Grad Optimizer Input Grad Figure 9: Decomposition of memory footprint for Llama-33B under different bits per parameter.\n\n--- Segment 42 ---\n1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 3309 3348 3425 3502 3695 3888 4274 5046 306 306 306 306 306 306 306 306 Llama-2-7B 1.75 1.8 1.9 2.0 2.25 2.5 3.0 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 5073 5148 5300 5451 5829 6207 6964 8476 479 479 479 479 479 479 479 479 Llama-2-13B 5.5 6.0 6.5 7.0 7.5 Perplexity 4.75 5.00 5.25 5.50 5.75 6.00 6.25 6.50 Perplexity Model Adapters Figure 8: Decomposition of inference memory footprint for Llama-2 7B and 13B under different bits per parameter. 22 1.15 1.25 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 8395 8778 19302 932 932 932 Inference Memory Usage 1.15 1.25 4.0 Bits per Parameter (BPP) 0.0 0.2 0.4 0.6 0.8 1.0 Memory Usage (Normalized to Max) 8395 8778 19302 932 932 932 888 888 888 3510 3510 3510 Finetune Memory Usage Model LoRA Weight Grad Optimizer Input Grad Figure 9: Decomposition of memory footprint for Llama-33B under different bits per parameter. 23 D Low-Rank Initializers: LoftQ vs PiSSA Both LoftQ [23] and PiSSa [32] use an iterative two-step process to enhance LoRA finetuning by exploiting low-rank structure and quantizing the residual. In each iteration: 1.\n\n--- Segment 43 ---\n23 D Low-Rank Initializers: LoftQ vs PiSSA Both LoftQ [23] and PiSSa [32] use an iterative two-step process to enhance LoRA finetuning by exploiting low-rank structure and quantizing the residual. In each iteration: 1. Low-Rank Decomposition: A singular value decomposition (SVD) of the current weight (or an updated version of it) is performed to factor out a low-rank approximation. 2. Residual Quantization: The remaining component (i.e., the difference between the original weight and the low-rank approximation) is quantized to preserve overall model capacity with fewer bits. The key distinction lies in how these two steps are ordered: LoftQ first quantizes the residual, which at the beginning is simply the full base weight. Thus, it initializes by treating the entire unmodified weight as a residual to be quantized and only then proceeds with the low-rank factorization in subsequent iterations. PiSSA starts by performing SVD on the unquantized base weight, extracting a low-rank representation before any quantization. Only after factoring out the low-rank component does PiSSa quantize the remaining residual. Below (Table 6) are the experimental results covering the full 2.0-to-4.0 range comparing these two initialization techniques. From our replication, LoftQ consistently outperforms PiSSa in terms of final task performance. We adopt LoftQ s mixed precision scheme as a result. Setup Llama-7b Llama-13b Method Dataset 2.0 2.25 2.5 3.0 4.0 2.0 2.25 2.5 3.0 4.0 PiSSA WikiText-2 ( ) 1919.63 795.88 1938.33 1397.26 5.53 1825.68 1822.62 1769.34 1549.48 5.05 LoftQ 8.63 8.22 7.72 6.87 5.26 7.27 6.96 5.7 5.91 4.79 Table 6: Perplexities of PiSSA and LoftQ as initialization methods on WikiText-2 covering full range from 2.0 to 4.0. Lower values indicate better performance ( ).\n\n--- Segment 44 ---\nSetup Llama-7b Llama-13b Method Dataset 2.0 2.25 2.5 3.0 4.0 2.0 2.25 2.5 3.0 4.0 PiSSA WikiText-2 ( ) 1919.63 795.88 1938.33 1397.26 5.53 1825.68 1822.62 1769.34 1549.48 5.05 LoftQ 8.63 8.22 7.72 6.87 5.26 7.27 6.96 5.7 5.91 4.79 Table 6: Perplexities of PiSSA and LoftQ as initialization methods on WikiText-2 covering full range from 2.0 to 4.0. Lower values indicate better performance ( ). 24 E Overheads of Mapping Thresholds Searcher and Precision As- signer In this appendix, we report the overhead incurred by LowRA s newly added components. Note that LowRA s mapping threshold learner and precision learner can both be done offline. The former only needs to be run for each model architecture, while the latter only needs to be run for each combination of model architecture and user-specified precision requirement. Mapping Threshold Learner Overhead We timed the overhead of our mapping threshold learner on a single NVIDIA A100 SXM GPU with GPU memory. For each precision, we ran 2 iterations of the Lloyd-Max algorithm, which we found sufficient to push down the MSE and enhance task performance. From the average of 10 runs, each run on Bart-Large, Llama2-7B, and Llama2-13B takes 111.46, 309.87, and 464.92 seconds, respectively. Solver Overhead We build our two-level ILP pipeline using the opensourced Coin-Or Branch and Cut (CBC) [38] solver via the Python-based modeling library PuLP [34]. The experiments were run on a server with 2x Intel Xeon Gold 6342 CPUs. The solver uses 8 threads in parallel. We timed the overhead of running this two-level ILP pipeline for each combination of model architecture and precision requirement. From the average of 10 runs, each run on Bart-Large, Llama2-7B, and Llama2-13B takes 48.99 seconds, 319.13 seconds, and 665.93 seconds, respectively.\n\n--- Segment 45 ---\nWe timed the overhead of running this two-level ILP pipeline for each combination of model architecture and precision requirement. From the average of 10 runs, each run on Bart-Large, Llama2-7B, and Llama2-13B takes 48.99 seconds, 319.13 seconds, and 665.93 seconds, respectively. Note that an exact (i.e., one-step) solver for the same problem would fail to finish in a reasonable amount of time. 25 F Initialization of the Mapping and Threshold Learner We attach the mappings and thresholds we use for the weighted Lloyd-Max algorithm in Listing 1. Listing 1 Initializations for bit-precision mappings and thresholds. mappings_4bit_init torch.tensor( [[-1.0, -0.6961928 , -0.5250731 , -0.3949175 , -0.28444138 , -0.18477343 , -0.09105 , 0.0, 0.0795803 , 0.1609302 , 0.2461123 , 0.33791524 , 0.44070983 , 0.562617 , 0.72295684 , 1.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) thresholds_2bit_init torch.tensor( [[-0.5, 0.16895762 , 0.66895762]] , dtype torch.float32 , device device ). repeat(nchannels , 1) mappings_2bit_init torch.tensor( [[-1.0, 0.0, 0.3379 , 1.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) thresholds_1bit_init torch.tensor( [[0.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) mappings_1bit_init torch.tensor( [[-1.0, 1.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) 26 G Weighted Lloyd-Max Algorithm In this subsection, we briefly introduce the Weighted Lloyd-Max Algorithm, extended from the original Lloyd-Max algorithm [29, 31].\n\n--- Segment 46 ---\nrepeat(nchannels , 1) mappings_1bit_init torch.tensor( [[-1.0, 1.0]] , dtype torch.float32 , device device ). repeat(nchannels , 1) 26 G Weighted Lloyd-Max Algorithm In this subsection, we briefly introduce the Weighted Lloyd-Max Algorithm, extended from the original Lloyd-Max algorithm [29, 31]. Let {xi}N i 1 Rd be data points with corresponding weights {wi}N i 1, wi 0. We seek to find K cluster centers {yj}K j 1 Rd minimizing the weighted mean-squared error: min {c(i)},{yj} N X i 1 wi xi yc(i) 2, where c(i) {1, . . . , K} is the cluster index assigned to xi. The weighted Lloyd s algorithm alternates between: 1. Assignment (E-step): Assign each data point xi to the cluster center closest in Euclidean distance: c(i) arg min 1 j K xi yj 2. (E-step) 2. Update (M-step): Recompute each cluster center yj as the weighted centroid of the points assigned to it: yj P i:c(i) j wi xi P i:c(i) j wi . (M-step) These steps are repeated until convergence or until a stopping criterion (e.g., a maximum number of iterations) is met. H Parameter Variances Along Output Channel Dimension vs Along Input Channel Dimension Figure 10 compares weight variance patterns across different layer types by examining the standard deviation along both input and output channel dimensions. We observe that the standard deviation is consistently higher along the output channel dimension than the input channel dimension. down_proj gate_proj k_proj o_proj q_proj up_proj v_proj 0 2 4 6 8 Avg. Std Dev x 10e-5 Out Dim In Dim Figure 10: Standard Deviations Along Output Channel Dimension vs Along Input Channel Dimen- sion, Grouped by Layer Type. 27 I Experiment Hyperparamter Setup In this appendix, we lay out the hyperparameters used for different experiments. Table 7 details the settings for Llama on the Wikitext-2 dataset.\n\n--- Segment 47 ---\n27 I Experiment Hyperparamter Setup In this appendix, we lay out the hyperparameters used for different experiments. Table 7 details the settings for Llama on the Wikitext-2 dataset. Table 8 provides the configuration used for fine-tuning Bart-Large on CNN DailyMail, and Table 9 does the same for XSUM. Finally, Table 10 describes the hyperparameters for Llama on the Open Assistant (oasst1) dataset. Hyperparameter Value model name or path meta-llama Llama-2-7b-hf data seed 42 evaluation strategy steps eval dataset size 1024 max eval samples 1000 per device eval batch size 4 dataloader num workers 3 lora r 64 lora alpha 64 lora modules all bf16 True warmup ratio 0.03 lr scheduler type cosine gradient checkpointing True dataset wikitext dataset config wikitext-2-raw-v1 per device train batch size 16 gradient accumulation steps 4 max steps 126 eval steps 20 learning rate 0.0003 adam beta2 0.999 max grad norm 0.3 weight decay 0.1 seed 0 block size 1024 Table 7: Hyperparameters used for all Llama experiments on Wikitext-2. 28 Hyperparameter Value learning rate 1e-4 seed 11 dataset name cnn dailymail dataset config 3.0.0 pad to max length True max source length 512 num train epochs 15 per device train batch size 8 per device eval batch size 32 gradient accumulation steps 32 model name or path facebook bart-large evaluation strategy epoch predict with generate True Table 8: Hyperparameters for fine-tuning Bart-Large on CNN DailyMail. Hyperparameter Value learning rate 1e-4 seed 11 dataset name xsum dataset config 3.0.0 pad to max length True max source length 512 num train epochs 25 per device train batch size 4 per device eval batch size 32 gradient accumulation steps 32 model name or path facebook bart-large evaluation strategy epoch Table 9: Hyperparameters for fine-tuning Bart-Large on XSUM.\n\n--- Segment 48 ---\n28 Hyperparameter Value learning rate 1e-4 seed 11 dataset name cnn dailymail dataset config 3.0.0 pad to max length True max source length 512 num train epochs 15 per device train batch size 8 per device eval batch size 32 gradient accumulation steps 32 model name or path facebook bart-large evaluation strategy epoch predict with generate True Table 8: Hyperparameters for fine-tuning Bart-Large on CNN DailyMail. Hyperparameter Value learning rate 1e-4 seed 11 dataset name xsum dataset config 3.0.0 pad to max length True max source length 512 num train epochs 25 per device train batch size 4 per device eval batch size 32 gradient accumulation steps 32 model name or path facebook bart-large evaluation strategy epoch Table 9: Hyperparameters for fine-tuning Bart-Large on XSUM. 29 Hyperparameter Value model name or path meta-llama Llama-2-7b-hf data seed 42 evaluation strategy steps eval dataset size 1024 max eval samples 500 per device eval batch size 1 max new tokens 32 dataloader num workers 3 group by length True logging strategy steps remove unused columns False lora r 64 lora alpha 64 lora modules all bf16 True warmup ratio 0.03 lr scheduler type constant gradient checkpointing True dataset oasst1 source max len 16 target max len 512 per device train batch size 4 gradient accumulation steps 4 max steps 1875 eval steps 200 learning rate 0.0002 adam beta2 0.999 max grad norm 0.3 lora dropout 0.1 weight decay 0.0 seed 0 Table 10: Hyperparameters used for all Llama experiments on Open Assistant (oasst1). 30 J Github Issues Related To The Lack of A Practical Quantization Primitive The use of NF fake quantization in LoftQ Discrepancy between real model weights and expected model weights due to fake quantization in LoftQ The use of NF fake quantization in LoftQ Unrealized GPU memory saving due to fake quantization in LoftQ Unrealized LLM model size reduction due to fake quantization in PiSSA K Detailed LoftQ Results on Bart-Large In this appendix, Table 11 presents results for LoftQ-based fine-tuning under two interpretations: (1) ordering layers by index (Layer Index), and (2) treating encoder layers as preceding decoder layers (Encoder First).\n\n--- Segment 49 ---\n29 Hyperparameter Value model name or path meta-llama Llama-2-7b-hf data seed 42 evaluation strategy steps eval dataset size 1024 max eval samples 500 per device eval batch size 1 max new tokens 32 dataloader num workers 3 group by length True logging strategy steps remove unused columns False lora r 64 lora alpha 64 lora modules all bf16 True warmup ratio 0.03 lr scheduler type constant gradient checkpointing True dataset oasst1 source max len 16 target max len 512 per device train batch size 4 gradient accumulation steps 4 max steps 1875 eval steps 200 learning rate 0.0002 adam beta2 0.999 max grad norm 0.3 lora dropout 0.1 weight decay 0.0 seed 0 Table 10: Hyperparameters used for all Llama experiments on Open Assistant (oasst1). 30 J Github Issues Related To The Lack of A Practical Quantization Primitive The use of NF fake quantization in LoftQ Discrepancy between real model weights and expected model weights due to fake quantization in LoftQ The use of NF fake quantization in LoftQ Unrealized GPU memory saving due to fake quantization in LoftQ Unrealized LLM model size reduction due to fake quantization in PiSSA K Detailed LoftQ Results on Bart-Large In this appendix, Table 11 presents results for LoftQ-based fine-tuning under two interpretations: (1) ordering layers by index (Layer Index), and (2) treating encoder layers as preceding decoder layers (Encoder First). For fairness, we select the best-performing interpretation as the LoftQ baseline. Note that for average precision 2.0 and 4.0, both interpretations lead to the same implementation. Interestingly, Encoder First consistently performs better on XSUM, whereas Layer Index generally achieves higher scores on CNN DailyMail.\n\n--- Segment 50 ---\nNote that for average precision 2.0 and 4.0, both interpretations lead to the same implementation. Interestingly, Encoder First consistently performs better on XSUM, whereas Layer Index generally achieves higher scores on CNN DailyMail. Setup Bart-Large Method Dataset 2.0 2.25 2.5 3.0 4.0 Layer Index XSUM( ) 31.89 10.18 24.59 32.62 10.70 25.15 33.76 11.73 26.24 36.02 13.49 28.16 40.34 17.06 31.92 Encoder First 32.72 10.94 25.38 34.49 12.26 27.05 37.23 14.34 29.31 Layer Index CNN DailyMail( ) 38.89 16.49 25.85 39.36 16.87 26.29 39.81 17.19 26.57 40.47 17.75 26.88 41.12 18.29 27.54 Encoder First 39.27 16.84 26.26 39.53 17.05 26.45 39.89 17.36 26.73 Table 11: ROUGE-1 ROUGE-2 ROUGE-L results of Bart-Large fine-tuned with LoftQ. Columns labeled 2.0, 2.25, 2.5, 3.0, and 4.0 refer to the average bits per parameter (bpp) or average precision. Higher scores indicate better performance. 31\n\n